<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.CL](#cs.CL) [Total: 53]
- [cs.RO](#cs.RO) [Total: 33]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.LG](#cs.LG) [Total: 141]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.IR](#cs.IR) [Total: 13]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Multi-encoder ConvNeXt Network with Smooth Attentional Feature Fusion for Multispectral Semantic Segmentation](https://arxiv.org/abs/2602.10137)
*Leo Thomas Ramos,Angel D. Sappa*

Main category: cs.CV

TL;DR: 本文提出MeCSAFNet，一种用于多光谱影像土地覆盖分割的多分支编码器-解码器架构，通过双ConvNeXt编码器分别处理可见与非可见波段，并结合多尺度特征融合解码器、CBAM注意力机制和ASAU激活函数，在多个数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对多光谱影像中可见与非可见波段信息特性差异大、传统模型难以兼顾空间细节与光谱表征的问题，需设计能自适应处理不同光谱配置（如4通道RGB+NIR、6通道含NDVI/NDWI）的土地覆盖分割模型。

Method: 提出MeCSAFNet：采用双ConvNeXt编码器分别处理可见与非可见通道；配备独立解码器重建空间信息；引入多尺度融合解码器整合中间特征；嵌入CBAM注意力增强特征融合；使用ASAU激活函数提升训练稳定性与效率；支持4c与6c两种输入配置。

Result: 在Five-Billion-Pixels（FBP）数据集上，MeCSAFNet-base（6c）相比U-Net（4c）、U-Net（6c）、SegFormer（4c）、SegFormer（6c）mIoU分别提升+19.21%、+14.72%、+19.62%、+14.74%；在Potsdam上，MeCSAFNet-large（4c）相比DeepLabV3+（4c）、DeepLabV3+（6c）、SegFormer（4c）、SegFormer（6c）mIoU分别提升+6.48%、+5.85%、+9.11%、+4.80%；轻量级变体亦保持高效低耗优势。

Conclusion: MeCSAFNet通过分通道建模、多尺度融合与注意力增强，有效提升了多光谱土地覆盖分割精度与泛化能力，兼具高性能与部署友好性，为遥感影像智能解译提供了新思路。

Abstract: This work proposes MeCSAFNet, a multi-branch encoder-decoder architecture for land cover segmentation in multispectral imagery. The model separately processes visible and non-visible channels through dual ConvNeXt encoders, followed by individual decoders that reconstruct spatial information. A dedicated fusion decoder integrates intermediate features at multiple scales, combining fine spatial cues with high-level spectral representations. The feature fusion is further enhanced with CBAM attention, and the ASAU activation function contributes to stable and efficient optimization. The model is designed to process different spectral configurations, including a 4-channel (4c) input combining RGB and NIR bands, as well as a 6-channel (6c) input incorporating NDVI and NDWI indices. Experiments on the Five-Billion-Pixels (FBP) and Potsdam datasets demonstrate significant performance gains. On FBP, MeCSAFNet-base (6c) surpasses U-Net (4c) by +19.21%, U-Net (6c) by +14.72%, SegFormer (4c) by +19.62%, and SegFormer (6c) by +14.74% in mIoU. On Potsdam, MeCSAFNet-large (4c) improves over DeepLabV3+ (4c) by +6.48%, DeepLabV3+ (6c) by +5.85%, SegFormer (4c) by +9.11%, and SegFormer (6c) by +4.80% in mIoU. The model also achieves consistent gains over several recent state-of-the-art approaches. Moreover, compact variants of MeCSAFNet deliver notable performance with lower training time and reduced inference cost, supporting their deployment in resource-constrained environments.

</details>


### [2] [Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement](https://arxiv.org/abs/2602.10138)
*Zhihang Yi,Jian Zhao,Jiancheng Lv,Tao Wang*

Main category: cs.CV

TL;DR: This survey provides a comprehensive overview of Multimodal Large Language Models (MLLMs) for chart understanding, organizing the field into challenges, tasks/datasets, methodologies, and future directions.


<details>
  <summary>Details</summary>
Motivation: The field of MLLM-based chart analysis is fragmented and lacks systematic organization, necessitating a structured survey to guide research and practice.

Method: The paper systematically structures the domain by analyzing fusion challenges, categorizing tasks and datasets (introducing a novel taxonomy), reviewing methodological evolution from classic deep learning to modern MLLMs, and critically assessing model limitations.

Result: A comprehensive roadmap of MLLM-based chart understanding is established, including a new taxonomy of benchmarks and identification of key limitations (e.g., perceptual and reasoning deficits).

Conclusion: The survey consolidates current knowledge, highlights open challenges, and proposes future directions—such as advanced alignment and reinforcement learning—to advance robust and reliable chart understanding systems.

Abstract: Chart understanding is a quintessential information fusion task, requiring the seamless integration of graphical and textual data to extract meaning. The advent of Multimodal Large Language Models (MLLMs) has revolutionized this domain, yet the landscape of MLLM-based chart analysis remains fragmented and lacks systematic organization. This survey provides a comprehensive roadmap of this nascent frontier by structuring the domain's core components. We begin by analyzing the fundamental challenges of fusing visual and linguistic information in charts. We then categorize downstream tasks and datasets, introducing a novel taxonomy of canonical and non-canonical benchmarks to highlight the field's expanding scope. Subsequently, we present a comprehensive evolution of methodologies, tracing the progression from classic deep learning techniques to state-of-the-art MLLM paradigms that leverage sophisticated fusion strategies. By critically examining the limitations of current models, particularly their perceptual and reasoning deficits, we identify promising future directions, including advanced alignment techniques and reinforcement learning for cognitive enhancement. This survey aims to equip researchers and practitioners with a structured understanding of how MLLMs are transforming chart information fusion and to catalyze progress toward more robust and reliable systems.

</details>


### [3] [MPA: Multimodal Prototype Augmentation for Few-Shot Learning](https://arxiv.org/abs/2602.10143)
*Liwen Wu,Wei Wang,Lei Zhao,Zhan Gao,Qika Lin,Shaowen Yao,Zuozhu Liu,Bin Pu*

Main category: cs.CV

TL;DR: 本文提出了一种多模态原型增强的少样本学习框架MPA，融合大语言模型语义增强、多视角特征增强和不确定性建模，显著提升单域与跨域少样本分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本学习方法大多仅依赖视觉模态，直接从原始支持图像计算原型，缺乏丰富多模态信息，难以应对语义稀疏与特征多样性不足问题。

Method: 提出MPA框架，包含三部分：1）LLM-based Multi-Variant Semantic Enhancement（LMSE），利用大语言模型生成多样化类别描述以扩充语义；2）Hierarchical Multi-View Augmentation（HMA），结合自然与多视角增强提升特征多样性；3）Adaptive Uncertain Class Absorber（AUCA），通过插值与高斯采样引入不确定类以吸收难分样本。

Result: 在4个单域和6个跨域FSL基准上全面超越SOTA；5-way 1-shot设置下，单域和跨域分别领先第二名12.29%和24.56%。

Conclusion: MPA通过融合语义、视觉与不确定性建模的多模态原型增强策略，有效提升了少样本学习的泛化性与鲁棒性，尤其在跨域场景中优势显著。

Abstract: Recently, few-shot learning (FSL) has become a popular task that aims to recognize new classes from only a few labeled examples and has been widely applied in fields such as natural science, remote sensing, and medical images. However, most existing methods focus only on the visual modality and compute prototypes directly from raw support images, which lack comprehensive and rich multimodal information. To address these limitations, we propose a novel Multimodal Prototype Augmentation FSL framework called MPA, including LLM-based Multi-Variant Semantic Enhancement (LMSE), Hierarchical Multi-View Augmentation (HMA), and an Adaptive Uncertain Class Absorber (AUCA). LMSE leverages large language models to generate diverse paraphrased category descriptions, enriching the support set with additional semantic cues. HMA exploits both natural and multi-view augmentations to enhance feature diversity (e.g., changes in viewing distance, camera angles, and lighting conditions). AUCA models uncertainty by introducing uncertain classes via interpolation and Gaussian sampling, effectively absorbing uncertain samples. Extensive experiments on four single-domain and six cross-domain FSL benchmarks demonstrate that MPA achieves superior performance compared to existing state-of-the-art methods across most settings. Notably, MPA surpasses the second-best method by 12.29% and 24.56% in the single-domain and cross-domain setting, respectively, in the 5-way 1-shot setting.

</details>


### [4] [VERA: Identifying and Leveraging Visual Evidence Retrieval Heads in Long-Context Understanding](https://arxiv.org/abs/2602.10146)
*Rongcan Pei,Huan Li,Fang Guo,Qi Zhu*

Main category: cs.CV

TL;DR: 本文分析了视觉-语言模型（VLMs）在长上下文处理中的内部机制，发现一类关键的‘视觉证据检索（VER）头’，并据此提出无需训练的VERA框架，通过检测不确定性并显式输出VER头关注的视觉证据，显著提升多个开源VLM在长上下文理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models (VLMs) 在长上下文和复杂推理任务中表现不佳，亟需深入理解其内部机制与性能瓶颈。

Method: 通过注意力分析识别出动态、稀疏的视觉证据检索（VER）头，并提出无需训练的VERA框架：基于模型不确定性（熵）触发，显式地将VER头所关注的视觉证据进行文本化输出。

Result: VERA在五个基准上显著提升开源VLM的长上下文理解能力：Qwen3-VL-8B-Instruct平均相对提升21.3%，GLM-4.1V-Thinking提升20.1%。

Conclusion: VER头对VLM长上下文推理具有因果性作用；VERA作为一种轻量、训练无关的方法，有效缓解了VLM在长上下文理解中的性能瓶颈。

Abstract: While Vision-Language Models (VLMs) have shown promise in textual understanding, they face significant challenges when handling long context and complex reasoning tasks. In this paper, we dissect the internal mechanisms governing long-context processing in VLMs to understand their performance bottlenecks. Through the lens of attention analysis, we identify specific Visual Evidence Retrieval (VER) Heads - a sparse, dynamic set of attention heads critical for locating visual cues during reasoning, distinct from static OCR heads. We demonstrate that these heads are causal to model performance; masking them leads to significant degradation. Leveraging this discovery, we propose VERA (Visual Evidence Retrieval Augmentation), a training-free framework that detects model uncertainty (i.e., entropy) to trigger the explicit verbalization of visual evidence attended by VER heads. Comprehensive experiments demonstrate that VERA significantly improves long-context understanding of open-source VLMs: it yields an average relative improvement of 21.3% on Qwen3-VL-8B-Instruct and 20.1% on GLM-4.1V-Thinking across five benchmarks.

</details>


### [5] [Beyond Closed-Pool Video Retrieval: A Benchmark and Agent Framework for Real-World Video Search and Moment Localization](https://arxiv.org/abs/2602.10159)
*Tao Yu,Yujia Yang,Haopeng Jin,Junhao Gong,Xinlong Chen,Yuxuan Zhou,Shanbin Zhang,Jiabing Yang,Xinming Wang,Hongzhu Yi,Ping Nie,Kai Zou,Zhang Zhang,Yan Huang,Liang Wang,Yeshani,Ruiwen Tao,Jin Ma,Haijin Liang,Jinwen Luo*

Main category: cs.CV

TL;DR: 本文提出了RVMS-Bench——首个面向真实世界视频记忆搜索的评测基准，以及RACLO——一种基于溯因推理的智能体框架，用于模拟人类‘回忆-搜索-验证’的认知过程，揭示现有MLLM在模糊记忆驱动的视频检索与片段定位任务中仍存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 传统视频检索基准局限于精确描述匹配封闭视频库，无法反映真实世界中基于模糊、多维记忆在开放网络上搜索视频的场景。

Method: 构建了包含1440个样本、覆盖20类及4个时长组的真实开放网页视频基准RVMS-Bench，并设计四层描述框架（全局印象、关键片段、时间上下文、听觉记忆）；提出基于溯因推理的智能体框架RACLO，模拟人类回忆-搜索-验证认知流程。

Result: 实验表明，当前多模态大语言模型（MLLM）在基于模糊记忆的真实视频检索与时刻定位任务中性能仍严重不足。

Conclusion: RVMS-Bench与RACLO为提升视频检索系统在真实非结构化场景下的鲁棒性提供了新基准与新方法，推动该领域向更贴近人类记忆机制的方向发展。

Abstract: Traditional video retrieval benchmarks focus on matching precise descriptions to closed video pools, failing to reflect real-world searches characterized by fuzzy, multi-dimensional memories on the open web. We present \textbf{RVMS-Bench}, a comprehensive system for evaluating real-world video memory search. It consists of \textbf{1,440 samples} spanning \textbf{20 diverse categories} and \textbf{four duration groups}, sourced from \textbf{real-world open-web videos}. RVMS-Bench utilizes a hierarchical description framework encompassing \textbf{Global Impression, Key Moment, Temporal Context, and Auditory Memory} to mimic realistic multi-dimensional search cues, with all samples strictly verified via a human-in-the-loop protocol. We further propose \textbf{RACLO}, an agentic framework that employs abductive reasoning to simulate the human ``Recall-Search-Verify'' cognitive process, effectively addressing the challenge of searching for videos via fuzzy memories in the real world. Experiments reveal that existing MLLMs still demonstrate insufficient capabilities in real-world Video Retrieval and Moment Localization based on fuzzy memories. We believe this work will facilitate the advancement of video retrieval robustness in real-world unstructured scenarios.

</details>


### [6] [AD$^2$: Analysis and Detection of Adversarial Threats in Visual Perception for End-to-End Autonomous Driving Systems](https://arxiv.org/abs/2602.10160)
*Ishan Sahu,Somnath Hazra,Somak Aditya,Soumyajit Dey*

Main category: cs.CV

TL;DR: 本文研究了端到端自动驾驶系统在黑盒对抗威胁下的鲁棒性，提出了三种物理与数字层面的攻击方式，并设计了一种基于注意力机制的轻量级攻击检测模型AD²，显著提升了检测性能与效率。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶系统虽取得进展，但其对抗鲁棒性尚不明确，尤其在真实物理世界威胁（如声波模糊、电磁干扰）下缺乏闭环评估。

Method: 在CARLA中对Transfuser和Interfuser两种先进模型进行闭合环路黑盒对抗攻击实验，设计三种攻击（声波诱导模糊、电磁干扰、数字幽灵物体），并提出基于时空注意力的轻量检测模型AD²。

Result: 两类模型在攻击下驾驶评分最高下降99%；AD²在多相机输入下展现出优于现有方法的检测能力与计算效率。

Conclusion: 当前端到端自动驾驶系统在物理与数字对抗攻击下极度脆弱，亟需嵌入高效鲁棒的检测机制；AD²为提升实际部署安全性提供了可行方案。

Abstract: End-to-end autonomous driving systems have achieved significant progress, yet their adversarial robustness remains largely underexplored. In this work, we conduct a closed-loop evaluation of state-of-the-art autonomous driving agents under black-box adversarial threat models in CARLA. Specifically, we consider three representative attack vectors on the visual perception pipeline: (i) a physics-based blur attack induced by acoustic waves, (ii) an electromagnetic interference attack that distorts captured images, and (iii) a digital attack that adds ghost objects as carefully crafted bounded perturbations on images. Our experiments on two advanced agents, Transfuser and Interfuser, reveal severe vulnerabilities to such attacks, with driving scores dropping by up to 99% in the worst case, raising valid safety concerns. To help mitigate such threats, we further propose a lightweight Attack Detection model for Autonomous Driving systems (AD$^2$) based on attention mechanisms that capture spatial-temporal consistency. Comprehensive experiments across multi-camera inputs on CARLA show that our detector achieves superior detection capability and computational efficiency compared to existing approaches.

</details>


### [7] [ArtisanGS: Interactive Tools for Gaussian Splat Selection with AI and Human in the Loop](https://arxiv.org/abs/2602.10173)
*Clement Fuji Tsang,Anita Hu,Or Perel,Carsten Kolve,Maria Shugrina*

Main category: cs.CV

TL;DR: 本文提出了一套面向3D高斯泼溅（3DGS）表示的交互式选择与分割工具，结合AI驱动的2D掩码到3D选择传播与手动编辑能力，支持任意二值分割，并用于用户引导的局部编辑（结合定制视频扩散模型），无需额外优化。


<details>
  <summary>Details</summary>
Motivation: 从野外捕获中提取可用对象困难，且对3DGS表示的可控编辑技术有限；现有方法多聚焦于全自动或高层编辑，缺乏灵活、交互式的底层选择与分割能力。

Method: 提出快速AI驱动的2D选择掩码向3DGS传播方法，支持用户纠错；结合灵活的手动选择与分割工具，实现对无结构3DGS场景的任意二值分割；并构建用户引导的局部编辑流程，集成定制视频扩散模型。

Result: 在3DGS选择任务上优于当前最先进方法；验证了工具在下游编辑任务中的有效性，支持任意野外捕获数据，无需额外优化。

Conclusion: 该交互式工具集显著提升了3DGS表示的可控性与实用性，为物理仿真、动画及内容创作等应用提供了坚实基础。

Abstract: Representation in the family of 3D Gaussian Splats (3DGS) are growing into a viable alternative to traditional graphics for an expanding number of application, including recent techniques that facilitate physics simulation and animation. However, extracting usable objects from in-the-wild captures remains challenging and controllable editing techniques for this representation are limited. Unlike the bulk of emerging techniques, focused on automatic solutions or high-level editing, we introduce an interactive suite of tools centered around versatile Gaussian Splat selection and segmentation. We propose a fast AI-driven method to propagate user-guided 2D selection masks to 3DGS selections. This technique allows for user intervention in the case of errors and is further coupled with flexible manual selection and segmentation tools. These allow a user to achieve virtually any binary segmentation of an unstructured 3DGS scene. We evaluate our toolset against the state-of-the-art for Gaussian Splat selection and demonstrate their utility for downstream applications by developing a user-guided local editing approach, leveraging a custom Video Diffusion Model. With flexible selection tools, users have direct control over the areas that the AI can modify. Our selection and editing tools can be used for any in-the-wild capture without additional optimization.

</details>


### [8] [When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models](https://arxiv.org/abs/2602.10179)
*Jiacheng Hou,Yining Sun,Ruochong Jin,Haochen Han,Fangming Liu,Wai Kin Victor Chan,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 本文提出首个视觉到视觉的越狱攻击（VJA），通过纯视觉输入（如标记、箭头）向图像编辑模型注入恶意指令，并构建安全基准IESBench进行系统评估；实验表明VJA可高成功率攻击主流商业模型，同时提出一种无需训练、基于内省多模态推理的轻量防御方法，显著提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉提示图像编辑成为新范式，攻击面也转为视觉形式，但该安全风险尚未被充分研究。

Method: 提出Vision-Centric Jailbreak Attack（VJA）攻击方法，并构建安全导向基准IESBench；设计一种训练无关、基于 introspective multimodal reasoning 的防御机制。

Result: VJA在Nano Banana Pro和GPT-Image-1.5上攻击成功率分别达80.9%和70.1%；所提防御方法在无额外守卫模型、低计算开销下，显著提升弱对齐模型的安全性至商用水平。

Conclusion: 揭示了视觉提示编辑模型中新型视觉越狱漏洞，提供了首个专用安全基准与实用防御方案，推动安全可信图像编辑系统发展。

Abstract: Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.

</details>


### [9] [DEGMC: Denoising Diffusion Models Based on Riemannian Equivariant Group Morphological Convolutions](https://arxiv.org/abs/2602.10221)
*El Hadji S. Diop,Thierno Fall,Mohamed Daoudi*

Main category: cs.CV

TL;DR: 本文提出了一种结合几何关键特征提取与群等变性的新扩散模型，通过在黎曼流形上定义群形态卷积（源于Hamilton-Jacobi型PDE的粘性解），并引入对流项以增强非线性建模与几何结构表征能力，在MNIST、RotoMNIST和CIFAR-10上优于基线DDPM。


<details>
  <summary>Details</summary>
Motivation: 解决现有DDPM模型在几何关键特征提取和网络等变性（尤其是旋转、反射、排列等欧几里得群对称性）方面的不足，因标准U-Net仅具平移等变性。

Method: 提出黎曼流形上的群形态卷积，基于一阶Hamilton-Jacobi型偏微分方程的粘性解，实现多尺度形态膨胀与腐蚀；加入对流项，并用特征线法求解，以融合欧几里得群等变性与非线性几何建模能力。

Result: 在MNIST、RotoMNIST和CIFAR-10数据集上，相比基准DDPM模型取得显著性能提升。

Conclusion: 将几何先验与群等变性嵌入扩散模型是有效的，所提群形态卷积框架能更好捕获薄几何结构、非线性特征及对称性，提升生成质量与鲁棒性。

Abstract: In this work, we address two major issues in recent Denoising Diffusion Probabilistic Models (DDPM): {\bf 1)} geometric key feature extraction and {\bf 2)} network equivariance. Since the DDPM prediction network relies on the U-net architecture, which is theoretically only translation equivariant, we introduce a geometric approach combined with an equivariance property of the more general Euclidean group, which includes rotations, reflections, and permutations. We introduce the notion of group morphological convolutions in Riemannian manifolds, which are derived from the viscosity solutions of first-order Hamilton-Jacobi-type partial differential equations (PDEs) that act as morphological multiscale dilations and erosions. We add a convection term to the model and solve it using the method of characteristics. This helps us better capture nonlinearities, represent thin geometric structures, and incorporate symmetries into the learning process. Experimental results on the MNIST, RotoMNIST, and CIFAR-10 datasets show noticeable improvements compared to the baseline DDPM model.

</details>


### [10] [XSPLAIN: XAI-enabling Splat-based Prototype Learning for Attribute-aware INterpretability](https://arxiv.org/abs/2602.10239)
*Dominik Galus,Julia Farganus,Tymoteusz Zapala,Mikołaj Czachorowski,Piotr Borycki,Przemysław Spurek,Piotr Syga*

Main category: cs.CV

TL;DR: 本文提出了XSPLAIN，首个专为3D高斯泼溅（3DGS）分类设计的前置式、基于原型的可解释性框架，通过体素聚合PointNet和可逆正交变换实现特征解耦，在不损害分类性能的前提下提供直观、基于样例的解释，并经用户研究验证其显著提升透明度与信任度。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）虽在高保真3D重建中成为标准，但在关键领域应用受限，主要因其生成模型和Splats分类缺乏可解释性；现有针对其他3D表示（如点云）的可解释方法依赖模糊的显著性图，无法体现高斯基元的体素一致性。

Method: 提出XSPLAIN框架：采用体素聚合的PointNet主干网络，并引入一种新颖的可逆正交变换，以解耦特征通道实现可解释性，同时严格保持原始决策边界；解释基于代表性训练样本，支持直观的‘这看起来像那’推理。

Result: 在N=51的严格用户研究中，参与者48.4%的时间选择XSPLAIN解释为最佳，显著优于基线方法（p<0.001），证明其能有效提升透明度与用户信任；且分类性能无任何下降。

Conclusion: XSPLAIN是首个面向3DGS分类的 ante-hoc、原型驱动可解释框架，兼顾高解释质量与模型性能，在理论设计与实证评估上均验证了其有效性与实用性。

Abstract: 3D Gaussian Splatting (3DGS) has rapidly become a standard for high-fidelity 3D reconstruction, yet its adoption in multiple critical domains is hindered by the lack of interpretability of the generation models as well as classification of the Splats. While explainability methods exist for other 3D representations, like point clouds, they typically rely on ambiguous saliency maps that fail to capture the volumetric coherence of Gaussian primitives. We introduce XSPLAIN, the first ante-hoc, prototype-based interpretability framework designed specifically for 3DGS classification. Our approach leverages a voxel-aggregated PointNet backbone and a novel, invertible orthogonal transformation that disentangles feature channels for interpretability while strictly preserving the original decision boundaries. Explanations are grounded in representative training examples, enabling intuitive ``this looks like that'' reasoning without any degradation in classification performance. A rigorous user study (N=51) demonstrates a decisive preference for our approach: participants selected XSPLAIN explanations 48.4\% of the time as the best, significantly outperforming baselines $(p<0.001)$, showing that XSPLAIN provides transparency and user trust. The source code for this work is available at: https://github.com/Solvro/ml-splat-xai

</details>


### [11] [PMMA: The Polytechnique Montreal Mobility Aids Dataset](https://arxiv.org/abs/2602.10259)
*Qingwu Liu,Nicolas Saunier,Guillaume-Alexandre Bilodeau*

Main category: cs.CV

TL;DR: 本文介绍了PMMA数据集，一个用于检测使用助行设备（如轮椅、拐杖、助行器）的行人的新数据集，并在多个目标检测和跟踪模型上建立了基准。


<details>
  <summary>Details</summary>
Motivation: 现有行人检测数据集缺乏对使用助行设备人群的充分覆盖，难以支持无障碍环境下的智能监控与辅助系统开发。

Method: 构建了包含九类助行设备使用者的户外PMMA数据集，并在MMDetection框架下评估了七种目标检测模型（Faster R-CNN、CenterNet、YOLOX、DETR系列等）和三种跟踪算法（ByteTrack、BOT-SORT、OC-SORT）。

Result: YOLOX、Deformable DETR 和 Faster R-CNN 在检测任务中表现最佳；三种跟踪器性能差异较小。PMMA 数据集及代码已开源。

Conclusion: PMMA 数据集填补了助行设备使用者检测领域的数据空白，为相关算法研究与实际应用提供了可靠基准和资源支持。

Abstract: This study introduces a new object detection dataset of pedestrians using mobility aids, named PMMA. The dataset was collected in an outdoor environment, where volunteers used wheelchairs, canes, and walkers, resulting in nine categories of pedestrians: pedestrians, cane users, two types of walker users, whether walking or resting, five types of wheelchair users, including wheelchair users, people pushing empty wheelchairs, and three types of users pushing occupied wheelchairs, including the entire pushing group, the pusher and the person seated on the wheelchair. To establish a benchmark, seven object detection models (Faster R-CNN, CenterNet, YOLOX, DETR, Deformable DETR, DINO, and RT-DETR) and three tracking algorithms (ByteTrack, BOT-SORT, and OC-SORT) were implemented under the MMDetection framework. Experimental results show that YOLOX, Deformable DETR, and Faster R-CNN achieve the best detection performance, while the differences among the three trackers are relatively small. The PMMA dataset is publicly available at https://doi.org/10.5683/SP3/XJPQUG, and the video processing and model training code is available at https://github.com/DatasetPMMA/PMMA.

</details>


### [12] [Colorimeter-Supervised Skin Tone Estimation from Dermatoscopic Images for Fairness Auditing](https://arxiv.org/abs/2602.10265)
*Marin Benčević,Krešimir Romić,Ivana Hartmann Tolić,Irena Galić*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经网络的皮肤色调估计方法，通过预测Fitzpatrick皮肤类型和ITA值，填补了皮肤病学图像数据集中缺乏可靠皮肤色调标注的空白，并揭示了现有公开数据集中深色皮肤样本严重不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的皮肤镜图像诊断模型在不同肤色人群间存在性能差异，但公共皮肤病学数据集缺乏可靠的皮肤色调标注，限制了公平性审计。

Method: 采用序数回归预测Fitzpatrick皮肤类型，颜色回归预测ITA值，以现场采集的Fitzpatrick标签和色度计测量值为监督信号，并利用大量合成与真实皮肤镜及临床图像进行预训练。

Result: Fitzpatrick预测模型与人工众包标注一致性相当，ITA预测与色度计测量高度一致，显著优于像素平均法；应用于ISIC 2020和MILK10k数据集发现，Fitzpatrick V-VI型个体占比不足1%。

Conclusion: 该方法是首个经色度计测量验证的皮肤病学皮肤色调估计神经网络，支持了跨肤色组存在临床相关性能差距的证据，并开源代码与预训练模型以促进快速皮肤色调标注与偏差审计。

Abstract: Neural-network-based diagnosis from dermatoscopic images is increasingly used for clinical decision support, yet studies report performance disparities across skin tones. Fairness auditing of these models is limited by the lack of reliable skin-tone annotations in public dermatoscopy datasets. We address this gap with neural networks that predict Fitzpatrick skin type via ordinal regression and the Individual Typology Angle (ITA) via color regression, using in-person Fitzpatrick labels and colorimeter measurements as targets. We further leverage extensive pretraining on synthetic and real dermatoscopic and clinical images. The Fitzpatrick model achieves agreement comparable to human crowdsourced annotations, and ITA predictions show high concordance with colorimeter-derived ITA, substantially outperforming pixel-averaging approaches. Applying these estimators to ISIC 2020 and MILK10k, we find that fewer than 1% of subjects belong to Fitzpatrick types V and VI. We release code and pretrained models as an open-source tool for rapid skin-tone annotation and bias auditing. This is, to our knowledge, the first dermatoscopic skin-tone estimation neural network validated against colorimeter measurements, and it supports growing evidence of clinically relevant performance gaps across skin-tone groups.

</details>


### [13] [ERGO: Excess-Risk-Guided Optimization for High-Fidelity Monocular 3D Gaussian Splatting](https://arxiv.org/abs/2602.10278)
*Zehua Ma,Hanhui Li,Zhenyu Xie,Xiaonan Luo,Michael Kampffmeyer,Feng Gao,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ERGO的自适应优化框架，通过过量风险分解（excess risk decomposition）来缓解单图像3D生成中合成视图监督信号不一致带来的几何与纹理伪影问题，在3D高斯泼溅中实现鲁棒、高质量的重建。


<details>
  <summary>Details</summary>
Motivation: 单张图像生成3D内容本质上病态，因遮挡区域缺乏几何与纹理信息；现有生成模型提供的辅助视图存在几何不一致和纹理错位，导致重建中伪影传播放大。

Method: 提出基于过量风险分解的自适应优化框架ERGO：将3D高斯泼溅优化损失分解为可减小的‘过量风险’和不可约的‘贝叶斯误差’，据此动态估计各视图的过量风险并自适应调整损失权重；同时引入几何感知与纹理感知目标，构建全局-局部协同优化范式。

Result: ERGO在Google Scanned Objects和OmniObject3D数据集上显著优于现有SOTA方法，展现出对监督噪声的鲁棒性，并同步提升重建3D内容的几何保真度与纹理质量。

Conclusion: ERGO通过理论驱动的风险分解与感知增强的联合优化机制，有效利用有噪合成监督，为单图像3D生成提供了更可靠、高质量的解决方案。

Abstract: Generating 3D content from a single image remains a fundamentally challenging and ill-posed problem due to the inherent absence of geometric and textural information in occluded regions. While state-of-the-art generative models can synthesize auxiliary views to provide additional supervision, these views inevitably contain geometric inconsistencies and textural misalignments that propagate and amplify artifacts during 3D reconstruction. To effectively harness these imperfect supervisory signals, we propose an adaptive optimization framework guided by excess risk decomposition, termed ERGO. Specifically, ERGO decomposes the optimization losses in 3D Gaussian splatting into two components, i.e., excess risk that quantifies the suboptimality gap between current and optimal parameters, and Bayes error that models the irreducible noise inherent in synthesized views. This decomposition enables ERGO to dynamically estimate the view-specific excess risk and adaptively adjust loss weights during optimization. Furthermore, we introduce geometry-aware and texture-aware objectives that complement the excess-risk-derived weighting mechanism, establishing a synergistic global-local optimization paradigm. Consequently, ERGO demonstrates robustness against supervision noise while consistently enhancing both geometric fidelity and textural quality of the reconstructed 3D content. Extensive experiments on the Google Scanned Objects dataset and the OmniObject3D dataset demonstrate the superiority of ERGO over existing state-of-the-art methods.

</details>


### [14] [A Low-Rank Defense Method for Adversarial Attack on Diffusion Models](https://arxiv.org/abs/2602.10319)
*Jiaxuan Zhu,Siyu Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为LoRD的高效防御策略，用于抵御潜在扩散模型（LDMs）上的对抗攻击，通过融合低秩自适应（LoRA）模块与平衡参数实现对抗样本检测与防御，并在人脸和风景图像上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 防止快速发展的对抗攻击算法影响扩散模型的实际应用，亟需开发相应的防御策略。

Method: 提出低秩防御（LoRD）策略，结合LoRA模块、合并思想和平衡参数构建防御流水线，使LDM在对抗与干净样本上微调后仍能生成高质量图像。

Result: 在人脸和风景图像上进行大量实验，LoRD显著优于基线方法，展现出更强的防御性能。

Conclusion: LoRD是一种高效且实用的防御策略，可有效提升LDM对对抗攻击的鲁棒性，同时保持图像生成质量。

Abstract: Recently, adversarial attacks for diffusion models as well as their fine-tuning process have been developed rapidly. To prevent the abuse of these attack algorithms from affecting the practical application of diffusion models, it is critical to develop corresponding defensive strategies. In this work, we propose an efficient defensive strategy, named Low-Rank Defense (LoRD), to defend the adversarial attack on Latent Diffusion Models (LDMs). LoRD introduces the merging idea and a balance parameter, combined with the low-rank adaptation (LoRA) modules, to detect and defend the adversarial samples. Based on LoRD, we build up a defense pipeline that applies the learned LoRD modules to help diffusion models defend against attack algorithms. Our method ensures that the LDM fine-tuned on both adversarial and clean samples can still generate high-quality images. To demonstrate the effectiveness of our approach, we conduct extensive experiments on facial and landscape images, and our method shows significantly better defense performance compared to the baseline methods.

</details>


### [15] [DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories](https://arxiv.org/abs/2602.10809)
*Chenlong Deng,Mengjie Deng,Junjie Wu,Dun Zeng,Teng Wang,Qingsong Xie,Jiadeng Huang,Shengjie Ma,Changwang Zhang,Zhaoxiang Wang,Jun Wang,Yutao Zhu,Zhicheng Dou*

Main category: cs.CV

TL;DR: 本文提出DeepImageSearch，一种将图像检索重构为自主探索任务的新范式，并构建了DISBench基准来评估模型在时序视觉流中基于隐含上下文线索进行多步推理的能力。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态检索系统假设查询-图像相关性可独立衡量，忽略了真实视觉流中固有的时序依赖性与信息分布特性。

Method: 提出DeepImageSearch代理范式，构建DISBench基准；设计人机协同流程生成上下文依赖查询；开发具备细粒度工具与双记忆系统的模块化代理基线。

Result: 实验表明DISBench对当前SOTA模型构成显著挑战，验证了引入代理式推理的必要性。

Conclusion: 图像检索需从静态匹配转向具备时序理解与主动探索能力的代理式推理范式。

Abstract: Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.

</details>


### [16] [Flow Matching with Uncertainty Quantification and Guidance](https://arxiv.org/abs/2602.10326)
*Juyeop Han,Lukas Lao Beyer,Sertac Karaman*

Main category: cs.CV

TL;DR: 本文提出了一种不确定性感知的流匹配方法（UA-Flow），通过在流匹配中联合预测速度场及其异方差不确定性，提升生成样本的质量与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的生成模型（如流匹配）虽成功，但生成样本质量不一致或退化，缺乏对样本可靠性的评估机制。

Method: 提出UA-Flow：在流匹配框架中引入速度场的异方差不确定性建模，并通过流动力学传播该不确定性以获得每样本不确定性估计；进一步将不确定性用于引导采样（不确定性感知的分类器引导和无分类器引导）。

Result: 实验表明，UA-Flow的不确定性信号与样本保真度的相关性高于基线方法，且不确定性引导采样可进一步提升图像生成质量。

Conclusion: UA-Flow是一种轻量、通用的流匹配扩展，能有效建模和利用生成不确定性，为提升生成可靠性与质量提供了新路径。

Abstract: Despite the remarkable success of sampling-based generative models such as flow matching, they can still produce samples of inconsistent or degraded quality. To assess sample reliability and generate higher-quality outputs, we propose uncertainty-aware flow matching (UA-Flow), a lightweight extension of flow matching that predicts the velocity field together with heteroscedastic uncertainty. UA-Flow estimates per-sample uncertainty by propagating velocity uncertainty through the flow dynamics. These uncertainty estimates act as a reliability signal for individual samples, and we further use them to steer generation via uncertainty-aware classifier guidance and classifier-free guidance. Experiments on image generation show that UA-Flow produces uncertainty signals more highly correlated with sample fidelity than baseline methods, and that uncertainty-guided sampling further improves generation quality.

</details>


### [17] [Conditional Uncertainty-Aware Political Deepfake Detection with Stochastic Convolutional Neural Networks](https://arxiv.org/abs/2602.10343)
*Rafael-Petruţ Gardoş*

Main category: cs.CV

TL;DR: 本文提出了一种面向政治深伪图像检测的条件化、不确定性感知方法，采用随机卷积神经网络，在经验性、决策导向的可靠性框架下评估不确定性，强调校准质量与实际预测误差的一致性，并通过构建政治聚焦数据集和多种不确定性估计策略对比，验证了校准概率输出对风险感知内容审核的价值。


<details>
  <summary>Details</summary>
Motivation: 现有自动化深伪检测系统大多只提供点预测，缺乏对预测不可靠性的指示，这在高风险的政治语境中构成关键操作缺陷，亟需不确定性感知的可靠检测方法。

Method: 基于随机卷积神经网络，在经验性、决策导向的可靠性框架下开展条件化不确定性建模；采用确定性元数据过滤构建政治聚焦二分类图像数据集；对ResNet-18和EfficientNet-B4进行全量微调；对比确定性推理、单次随机预测、MC Dropout、温度缩放及集成不确定性代理等策略；评估指标包括ROC-AUC、阈值混淆矩阵、校准度量及生成器无关的OOD性能。

Result: 校准后的概率输出与不确定性估计可支撑风险感知的内容审核策略；置信带系统分析表明，不确定性在特定条件下能提供超越预测置信度的操作价值，但也存在明确局限。

Conclusion: 不确定性不应仅视为贝叶斯概念，而应通过可观测标准（如校准性、评分规则、误差对齐）进行实证评估；该方法为政治深伪检测提供了更可靠、可操作的决策支持基础。

Abstract: Recent advances in generative image models have enabled the creation of highly realistic political deepfakes, posing risks to information integrity, public trust, and democratic processes. While automated deepfake detectors are increasingly deployed in moderation and investigative pipelines, most existing systems provide only point predictions and fail to indicate when outputs are unreliable, being an operationally critical limitation in high-stakes political contexts. This work investigates conditional, uncertainty-aware political deepfake detection using stochastic convolutional neural networks within an empirical, decision-oriented reliability framework. Rather than treating uncertainty as a purely Bayesian construct, it is evaluated through observable criteria, including calibration quality, proper scoring rules, and its alignment with prediction errors under both global and confidence-conditioned analyses. A politically focused binary image dataset is constructed via deterministic metadata filtering from a large public real-synthetic corpus. Two pretrained CNN backbones (ResNet-18 and EfficientNet-B4) are fully fine-tuned for classification. Deterministic inference is compared with single-pass stochastic prediction, Monte Carlo dropout with multiple forward passes, temperature scaling, and ensemble-based uncertainty surrogates. Evaluation reports ROC-AUC, thresholded confusion matrices, calibration metrics, and generator-disjoint out-of-distribution performance. Results demonstrate that calibrated probabilistic outputs and uncertainty estimates enable risk-aware moderation policies. A systematic confidence-band analysis further clarifies when uncertainty provides operational value beyond predicted confidence, delineating both the benefits and limitations of uncertainty-aware deepfake detection in political settings.

</details>


### [18] [Monte Carlo Maximum Likelihood Reconstruction for Digital Holography with Speckle](https://arxiv.org/abs/2602.10344)
*Xi Chen,Arian Maleki,Shirin Jalali*

Main category: cs.CV

TL;DR: 本文提出了一种基于随机线性代数和蒙特卡洛估计的投影梯度下降方法（PGD-MC），用于在数字全息中实现可扩展的最大似然估计（MLE）重建，避免高维矩阵求逆，支持精确孔径建模并显著提升重建质量与计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统MLE方法因高维矩阵求逆计算代价过高，难以应用于带有限孔径的相干成像（如数字全息），导致无法使用物理准确的孔径模型进行重建。

Method: 提出PGD-MC方法：利用传感矩阵结构特性，结合共轭梯度法高效计算似然梯度，避免显式矩阵求逆；引入蒙特卡洛估计实现梯度近似，并嵌入多种去噪器作为正则项。

Result: PGD-MC在多种真实孔径模型下鲁棒性强，重建质量与计算效率显著优于现有Plug-and-Play方法，且可扩展至高分辨率全息重建；实验验证其在精度与速度上均具优势。

Conclusion: PGD-MC为有限孔径数字全息提供了一种灵活、高效、物理准确的MLE重建框架，突破了传统MLE在计算可扩展性上的瓶颈。

Abstract: In coherent imaging, speckle is statistically modeled as multiplicative noise, posing a fundamental challenge for image reconstruction. While maximum likelihood estimation (MLE) provides a principled framework for speckle mitigation, its application to coherent imaging system such as digital holography with finite apertures is hindered by the prohibitive cost of high-dimensional matrix inversion, especially at high resolutions. This computational burden has prevented the use of MLE-based reconstruction with physically accurate aperture modeling. In this work, we propose a randomized linear algebra approach that enables scalable MLE optimization without explicit matrix inversions in gradient computation. By exploiting the structural properties of sensing matrix and using conjugate gradient for likelihood gradient evaluation, the proposed algorithm supports accurate aperture modeling without the simplifying assumptions commonly imposed for tractability. We term the resulting method projected gradient descent with Monte Carlo estimation (PGD-MC). The proposed PGD-MC framework (i) demonstrates robustness to diverse and physically accurate aperture models, (ii) achieves substantial improvements in reconstruction quality and computational efficiency, and (iii) scales effectively to high-resolution digital holography. Extensive experiments incorporating three representative denoisers as regularization show that PGD-MC provides a flexible and effective MLE-based reconstruction framework for digital holography with finite apertures, consistently outperforming prior Plug-and-Play model-based iterative reconstruction methods in both accuracy and speed. Our code is available at: https://github.com/Computational-Imaging-RU/MC_Maximum_Likelihood_Digital_Holography_Speckle.

</details>


### [19] [Comp2Comp: Open-Source Software with FDA-Cleared Artificial Intelligence Algorithms for Computed Tomography Image Analysis](https://arxiv.org/abs/2602.10364)
*Adrit Rao,Malte Jensen,Andrea T. Fisher,Louis Blankemeier,Pauline Berens,Arash Fereydooni,Seth Lirette,Eren Alkan,Felipe C. Kitamura,Juan M. Zambrano Chaves,Eduardo Reis,Arjun Desai,Marc H. Willis,Jason Hom,Andrew Johnston,Leon Lenchik,Robert D. Boutin,Eduardo M. J. M. Farina,Augusto S. Serpa,Marcelo S. Takahashi,Jordan Perchik,Steven A. Rothenberg,Jamie L. Schroeder,Ross Filice,Leonardo K. Bittencourt,Hari Trivedi,Marly van Assen,John Mongan,Kimberly Kallianos,Oliver Aalami,Akshay S. Chaudhari*

Main category: cs.CV

TL;DR: 本文介绍了两个开源、FDA-510(k)认证的深度学习工具AAQ和BMD，用于CT影像的机会性分析，分别评估腹主动脉瘤大小和骨密度；经多中心验证，其性能达到临床可用水平。


<details>
  <summary>Details</summary>
Motivation: 解决现有开源影像分析工具缺乏严格验证、商用工具缺乏透明性的问题，推动可信赖、可复现、可临床部署的AI医学影像工具发展。

Method: 开发并验证两个完全开源、FDA-510(k)-批准的深度学习流程：Abdominal Aortic Quantification（AAQ）和Bone Mineral Density（BMD）估计，集成于Comp2Comp软件包中；在四个外部机构共629例患者数据上进行外部验证（AAQ：258例CT vs 放射科医生标注；BMD：371例CT vs 同期DXA金标准）。

Result: AAQ最大主动脉直径测量平均绝对误差为1.57 mm（95% CI 1.38–1.80 mm）；BMD二分类（低/正常骨密度）敏感性81.0%（95% CI 74.0–86.8%），特异性78.4%（95% CI 72.3–83.7%）。

Conclusion: Comp2Comp中的AAQ与BMD算法具备临床应用所需的准确性；开源+FDA认证模式提升了AI医疗工具的透明度、可评估性与可及性，为医院预验证和科研复用提供新范式。

Abstract: Artificial intelligence allows automatic extraction of imaging biomarkers from already-acquired radiologic images. This paradigm of opportunistic imaging adds value to medical imaging without additional imaging costs or patient radiation exposure. However, many open-source image analysis solutions lack rigorous validation while commercial solutions lack transparency, leading to unexpected failures when deployed. Here, we report development and validation for two of the first fully open-sourced, FDA-510(k)-cleared deep learning pipelines to mitigate both challenges: Abdominal Aortic Quantification (AAQ) and Bone Mineral Density (BMD) estimation are both offered within the Comp2Comp package for opportunistic analysis of computed tomography scans. AAQ segments the abdominal aorta to assess aneurysm size; BMD segments vertebral bodies to estimate trabecular bone density and osteoporosis risk. AAQ-derived maximal aortic diameters were compared against radiologist ground-truth measurements on 258 patient scans enriched for abdominal aortic aneurysms from four external institutions. BMD binary classifications (low vs. normal bone density) were compared against concurrent DXA scan ground truths obtained on 371 patient scans from four external institutions. AAQ had an overall mean absolute error of 1.57 mm (95% CI 1.38-1.80 mm). BMD had a sensitivity of 81.0% (95% CI 74.0-86.8%) and specificity of 78.4% (95% CI 72.3-83.7%). Comp2Comp AAQ and BMD demonstrated sufficient accuracy for clinical use. Open-sourcing these algorithms improves transparency of typically opaque FDA clearance processes, allows hospitals to test the algorithms before cumbersome clinical pilots, and provides researchers with best-in-class methods.

</details>


### [20] [HII-DPO: Eliminate Hallucination via Accurate Hallucination-Inducing Counterfactual Images](https://arxiv.org/abs/2602.10425)
*Yilin Yang,Zhenghui Guo,Yuke Wang,Omprakash Gnawali,Sheng Di,Chengming Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种合成幻觉诱导图像（HIIs）的新方法，揭示了VLMs中场景条件下的幻觉模式，并构建了MOH基准和高质量偏好数据集，显著缓解了语言偏置导致的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（VLMs）虽在多模态任务中表现优异，但易受语言偏置引发的幻觉影响；现有缓解方法常忽略由语言偏置驱动的底层幻觉模式。

Method: 设计新流程合成Hallucination-Inducing Images（HIIs），基于HIIs发现并量化场景条件下的幻觉模式，构建Masked-Object-Hallucination（MOH）基准，并利用HIIs构建高质量偏好数据集用于细粒度对齐。

Result: 在标准幻觉评测基准上，该方法相较当前最优方法提升高达38%，且保持模型通用能力。

Conclusion: 所提方法能有效缓解VLMs因语言偏置导致的幻觉，为幻觉机理分析与对齐优化提供了新范式。

Abstract: Large Vision-Language Models (VLMs) have achieved remarkable success across diverse multimodal tasks but remain vulnerable to hallucinations rooted in inherent language bias. Despite recent progress, existing hallucination mitigation methods often overlook the underlying hallucination patterns driven by language bias. In this work, we design a novel pipeline to accurately synthesize Hallucination-Inducing Images (HIIs). Using synthesized HIIs, we reveal a consistent scene-conditioned hallucination pattern: models tend to mention objects that are highly typical of the scene even when visual evidence is removed. To quantify the susceptibility of VLMs to this hallucination pattern, we establish the Masked-Object-Hallucination (MOH) benchmark to rigorously evaluate existing state-of-the-art alignment frameworks. Finally, we leverage HIIs to construct high-quality preference datasets for fine-grained alignment. Experimental results demonstrate that our approach effectively mitigates hallucinations while preserving general model capabilities. Specifically, our method achieves up to a 38% improvement over the current state-of-the-art on standard hallucination benchmarks.

</details>


### [21] [Towards Remote Sensing Change Detection with Neural Memory](https://arxiv.org/abs/2602.10491)
*Zhenyu Yang,Gensheng Pei,Yazhou Yao,Tianfei Zhou,Lizhong Ding,Fumin Shen*

Main category: cs.CV

TL;DR: 本文提出ChangeTitans框架，基于Titans架构改进遥感变化检测，通过VTitans视觉骨干、分层VTitans-Adapter和TS-CBAM双流融合模块，在保持计算效率的同时显著提升长程依赖建模与检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测方法难以兼顾长程依赖建模与计算效率；Transformer虽能建模全局上下文但复杂度高，线性注意力又难以捕捉复杂时空关系。

Method: 提出基于Titans的ChangeTitans框架：1）VTitans——融合神经记忆与分段局部注意力的视觉骨干；2）分层VTitans-Adapter用于多尺度特征精调；3）TS-CBAM双流模块利用跨时间注意力抑制伪变化并提升精度。

Result: 在LEVIR-CD、WHU-CD、LEVIR-CD+和SYSU-CD四个基准数据集上达到SOTA性能，其中LEVIR-CD上IoU达84.36%，F1-score达91.52%，且计算开销具竞争力。

Conclusion: ChangeTitans有效平衡了建模能力与效率，为遥感变化检测提供了新范式，验证了Titans类架构在视觉任务中的潜力。

Abstract: Remote sensing change detection is essential for environmental monitoring, urban planning, and related applications. However, current methods often struggle to capture long-range dependencies while maintaining computational efficiency. Although Transformers can effectively model global context, their quadratic complexity poses scalability challenges, and existing linear attention approaches frequently fail to capture intricate spatiotemporal relationships. Drawing inspiration from the recent success of Titans in language tasks, we present ChangeTitans, the Titans-based framework for remote sensing change detection. Specifically, we propose VTitans, the first Titans-based vision backbone that integrates neural memory with segmented local attention, thereby capturing long-range dependencies while mitigating computational overhead. Next, we present a hierarchical VTitans-Adapter to refine multi-scale features across different network layers. Finally, we introduce TS-CBAM, a two-stream fusion module leveraging cross-temporal attention to suppress pseudo-changes and enhance detection accuracy. Experimental evaluations on four benchmark datasets (LEVIR-CD, WHU-CD, LEVIR-CD+, and SYSU-CD) demonstrate that ChangeTitans achieves state-of-the-art results, attaining \textbf{84.36\%} IoU and \textbf{91.52\%} F1-score on LEVIR-CD, while remaining computationally competitive.

</details>


### [22] [End-to-End LiDAR optimization for 3D point cloud registration](https://arxiv.org/abs/2602.10492)
*Siddhant Katyan,Marc-André Gardner,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 本文提出了一种自适应LiDAR感知框架，通过将配准反馈融入感知回路，联合优化LiDAR采集与配准超参数，在CARLA仿真中验证了其在精度和效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统点云配准依赖于固定LiDAR配置的预采集数据，导致数据采集次优及大量计算开销（如采样、去噪、调参）；LiDAR传感器设计与下游任务（如配准）脱节。

Method: 提出一种自适应LiDAR感知框架，动态调整传感器参数，并联合优化LiDAR采集与配准超参数；将配准性能反馈嵌入感知闭环，以平衡点密度、噪声与稀疏性。

Result: 在CARLA仿真中，该方法优于固定参数基线方法，同时保持良好泛化能力。

Conclusion: 自适应LiDAR感知能显著提升点云配准的准确性与效率，为自动驾驶与机器人感知提供新思路。

Abstract: LiDAR sensors are a key modality for 3D perception, yet they are typically designed independently of downstream tasks such as point cloud registration. Conventional registration operates on pre-acquired datasets with fixed LiDAR configurations, leading to suboptimal data collection and significant computational overhead for sampling, noise filtering, and parameter tuning. In this work, we propose an adaptive LiDAR sensing framework that dynamically adjusts sensor parameters, jointly optimizing LiDAR acquisition and registration hyperparameters. By integrating registration feedback into the sensing loop, our approach optimally balances point density, noise, and sparsity, improving registration accuracy and efficiency. Evaluations in the CARLA simulation demonstrate that our method outperforms fixed-parameter baselines while retaining generalization abilities, highlighting the potential of adaptive LiDAR for autonomous perception and robotic applications.

</details>


### [23] [Characterizing and Optimizing the Spatial Kernel of Multi Resolution Hash Encodings](https://arxiv.org/abs/2602.10495)
*Tianxiang Dai,Jonathan Fan*

Main category: cs.CV

TL;DR: 本文提出了一种基于点扩散函数（PSF）的多分辨率哈希编码（MHE）分析方法，揭示其空间分辨率由平均分辨率N_avg而非最大分辨率N_max决定，并引入旋转MHE（R-MHE）以缓解网格各向异性。


<details>
  <summary>Details</summary>
Motivation: MHE在神经场中广泛应用，但其空间行为缺乏从物理系统角度的严格理解，导致超参数选择依赖经验启发式。

Method: 通过分析MHE的点扩散函数（PSF），推导无碰撞PSF的闭式近似，量化空间分辨率与保真度；研究哈希容量有限导致的碰撞噪声；并提出旋转MHE（R-MHE）架构以缓解各向异性。

Result: 发现MHE的有效分辨率由平均分辨率N_avg决定的展宽FWHM控制，而非N_max；碰撞会引入散斑噪声并降低SNR；R-MHE可有效缓解各向异性且保持原有效率和参数量。

Conclusion: 本工作建立了基于物理原理（如PSF、带宽、SNR）的MHE分析与优化框架，推动其从启发式设计转向理论驱动设计。

Abstract: Multi-Resolution Hash Encoding (MHE), the foundational technique behind Instant Neural Graphics Primitives, provides a powerful parameterization for neural fields. However, its spatial behavior lacks rigorous understanding from a physical systems perspective, leading to reliance on heuristics for hyperparameter selection. This work introduces a novel analytical approach that characterizes MHE by examining its Point Spread Function (PSF), which is analogous to the Green's function of the system. This methodology enables a quantification of the encoding's spatial resolution and fidelity. We derive a closed-form approximation for the collision-free PSF, uncovering inherent grid-induced anisotropy and a logarithmic spatial profile. We establish that the idealized spatial bandwidth, specifically the Full Width at Half Maximum (FWHM), is determined by the average resolution, $N_{\text{avg}}$. This leads to a counterintuitive finding: the effective resolution of the model is governed by the broadened empirical FWHM (and therefore $N_{\text{avg}}$), rather than the finest resolution $N_{\max}$, a broadening effect we demonstrate arises from optimization dynamics. Furthermore, we analyze the impact of finite hash capacity, demonstrating how collisions introduce speckle noise and degrade the Signal-to-Noise Ratio (SNR). Leveraging these theoretical insights, we propose Rotated MHE (R-MHE), an architecture that applies distinct rotations to the input coordinates at each resolution level. R-MHE mitigates anisotropy while maintaining the efficiency and parameter count of the original MHE. This study establishes a methodology based on physical principles that moves beyond heuristics to characterize and optimize MHE.

</details>


### [24] [The Garbage Dataset (GD): A Multi-Class Image Benchmark for Automated Waste Segregation](https://arxiv.org/abs/2602.10500)
*Suman Kunwar*

Main category: cs.CV

TL;DR: 本文介绍了Garbage Dataset（GD），一个包含10类家庭垃圾、共13,348张标注图像的公开数据集，用于推动基于机器学习和计算机视觉的自动垃圾分类；通过多种方法验证数据质量，并用多个SOTA模型进行基准测试，发现EfficientNetV2S性能最优（96.19%准确率），但也存在类不平衡、背景复杂性和碳排放等实际挑战。


<details>
  <summary>Details</summary>
Motivation: 解决现有垃圾图像数据集规模小、类别少、缺乏真实场景覆盖和环境影响评估的问题，为可持续发展的自动垃圾分类研究提供高质量、可复现、具现实挑战性的基准数据集。

Method: 构建涵盖10类垃圾的13,348张图像GD数据集，来源包括DWaste移动应用与网络采集；采用校验和与离群值检测确保数据完整性；利用PCA/t-SNE分析类间可分性与不平衡；用熵与显著性图评估背景复杂度；在EfficientNetV2M/S、MobileNet、ResNet50/101等模型上开展性能与碳排放联合基准测试。

Result: EfficientNetV2S取得最高精度（96.19%）和F1-score（0.96），但碳成本中等；数据分析揭示了明显的类不平衡（塑料/纸板/纸类离群值多）、亮度差异及高背景复杂度等固有特性。

Conclusion: GD是一个有价值的现实世界垃圾分类基准数据集，其发布有助于推动环保AI研究；同时凸显了在实际部署中必须应对的三大挑战：类别不平衡、背景干扰以及模型性能与碳排放之间的权衡。

Abstract: This study introduces the Garbage Dataset (GD), a publicly available image dataset designed to advance automated waste segregation through machine learning and computer vision. It's a diverse dataset covering 10 common household waste categories: metal, glass, biological, paper, battery, trash, cardboard, shoes, clothes, and plastic. The dataset comprises 13,348 labeled images collected through multiple methods, including DWaste mobile app and curated web sources. Methods included rigorous validation through checksums and outlier detection, analysis of class imbalance and visual separability via PCA/t-SNE, and assessment of background complexity using entropy and saliency measures. The dataset was benchmarked using state-of-the-art deep learning models (EfficientNetV2M, EfficientNetV2S, MobileNet, ResNet50, ResNet101) evaluated on performance metrics and operational carbon emissions. Experiment results indicate EfficientNetV2S achieved the highest performance with 96.19% accuracy and a 0.96 F1-score, though with a moderate carbon cost. Analysis revealed inherent dataset characteristics including class imbalance, a skew toward high-outlier classes (plastic, cardboard, paper), and brightness variations that require consideration. The main conclusion is that GD provides a valuable, real-world benchmark for waste classification research while highlighting important challenges such as class imbalance, background complexity, and environmental trade-offs in model selection that must be addressed for practical deployment. The dataset is publicly released to support further research in environmental sustainability applications.

</details>


### [25] [Med-SegLens: Latent-Level Model Diffing for Interpretable Medical Image Segmentation](https://arxiv.org/abs/2602.10508)
*Salma J. Ahmed,Emad A. Mohammed,Azam Asilian Bidgoli*

Main category: cs.CV

TL;DR: Med-SegLens 是一种模型差异分析框架，利用稀疏自编码器分解分割模型激活，识别跨架构与跨数据集的稳定表征及人群特异性潜在特征，并通过潜在空间干预显著提升跨数据集泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现代分割模型虽性能强但缺乏可解释性，难以诊断失败、理解数据集偏移或进行有原则的干预。

Method: 提出 Med-SegLens 框架，基于 SegFormer 和 U-Net 的激活，用稀疏自编码器学习可解释的潜在特征；通过跨架构、跨数据集（健康、成人、儿童、撒哈拉以南非洲胶质瘤队列）的潜在对齐分析表征稳定性与偏移机制；将潜在特征建模为因果瓶颈并实施靶向干预。

Result: 发现稳定的共享表征主干，数据集偏移源于对人群特异性潜在特征的差异化依赖；潜在干预在 70% 的失败案例中纠正错误，Dice 分数从 39.4% 提升至 74.2%。

Conclusion: 潜在空间的模型差异分析是一种实用且具机制性的工具，可用于诊断分割模型失败和缓解数据集偏移。

Abstract: Modern segmentation models achieve strong predictive performance but remain largely opaque, limiting our ability to diagnose failures, understand dataset shift, or intervene in a principled manner. We introduce Med-SegLens, a model-diffing framework that decomposes segmentation model activations into interpretable latent features using sparse autoencoders trained on SegFormer and U-Net. Through cross-architecture and cross-dataset latent alignment across healthy, adult, pediatric, and sub-Saharan African glioma cohorts, we identify a stable backbone of shared representations, while dataset shift is driven by differential reliance on population-specific latents. We show that these latents act as causal bottlenecks for segmentation failures, and that targeted latent-level interventions can correct errors and improve cross-dataset adaption without retraining, recovering performance in 70% of failure cases and improving Dice score from 39.4% to 74.2%. Our results demonstrate that latent-level model diffing provides a practical and mechanistic tool for diagnosing failures and mitigating dataset shift in segmentation models.

</details>


### [26] [1%>100%: High-Efficiency Visual Adapter with Complex Linear Projection Optimization](https://arxiv.org/abs/2602.10513)
*Dongshuo Yin,Xue Yang,Deng-Ping Fan,Shi-Min Hu*

Main category: cs.CV

TL;DR: 本文提出了一种名为CoLin的新型适配器，通过复数线性投影优化，在仅增加约1%参数的情况下，显著提升了视觉基础模型的适应效率，超越了全量微调和经典delta-tuning方法。


<details>
  <summary>Details</summary>
Motivation: 传统全量微调成本高、效率低；delta-tuning在大语言模型中有效，但难以直接迁移到视觉基础模型适配中。

Method: 设计了一种低秩复数适配器架构，并从理论上分析低秩复合矩阵训练中的收敛问题，提出定制化损失函数加以解决。

Result: 在目标检测、分割、图像分类及遥感旋转目标检测等多个视觉任务上，CoLin以仅1%参数量首次超越全量微调与经典delta-tuning方法。

Conclusion: CoLin为视觉基础模型的高效部署提供了一种新颖、轻量且通用的适配方案。

Abstract: Deploying vision foundation models typically relies on efficient adaptation strategies, whereas conventional full fine-tuning suffers from prohibitive costs and low efficiency. While delta-tuning has proven effective in boosting the performance and efficiency of LLMs during adaptation, its advantages cannot be directly transferred to the fine-tuning pipeline of vision foundation models. To push the boundaries of adaptation efficiency for vision tasks, we propose an adapter with Complex Linear Projection Optimization (CoLin). For architecture, we design a novel low-rank complex adapter that introduces only about 1% parameters to the backbone. For efficiency, we theoretically prove that low-rank composite matrices suffer from severe convergence issues during training, and address this challenge with a tailored loss. Extensive experiments on object detection, segmentation, image classification, and rotated object detection (remote sensing scenario) demonstrate that CoLin outperforms both full fine-tuning and classical delta-tuning approaches with merely 1% parameters for the first time, providing a novel and efficient solution for deployment of vision foundation models. We release the code on https://github.com/DongshuoYin/CoLin.

</details>


### [27] [3DXTalker: Unifying Identity, Lip Sync, Emotion, and Spatial Dynamics in Expressive 3D Talking Avatars](https://arxiv.org/abs/2602.10516)
*Zhongju Wang,Zhenhong Sun,Beier Wang,Yifu Wang,Daoyi Dong,Huadong Mo,Hongdong Li*

Main category: cs.CV

TL;DR: 本文提出3DXTalker，一种通过数据驱动的身份建模、丰富的音频表征与空间动态可控性实现高表现力的3D说话虚拟人生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于身份样本少、音频表征单一、缺乏显式可控性，难以兼顾身份保持、唇音同步、情感表达与自然头部运动等多维表现力需求。

Method: 提出3DXTalker框架：1）基于2D-to-3D数据增广与解耦表征实现可扩展身份建模；2）引入帧级幅度与情感线索增强音频表征；3）采用流匹配（flow-matching）Transformer统一建模面部动态；4）支持提示驱动的头部姿态风格化控制。

Result: 在多项指标上超越现有方法，统一实现了高精度唇同步、细腻情感表达与自然头部运动生成。

Conclusion: 3DXTalker为构建高表现力、高可控性、强泛化能力的3D talking avatar提供了有效且可扩展的统一框架。

Abstract: Audio-driven 3D talking avatar generation is increasingly important in virtual communication, digital humans, and interactive media, where avatars must preserve identity, synchronize lip motion with speech, express emotion, and exhibit lifelike spatial dynamics, collectively defining a broader objective of expressivity. However, achieving this remains challenging due to insufficient training data with limited subject identities, narrow audio representations, and restricted explicit controllability. In this paper, we propose 3DXTalker, an expressive 3D talking avatar through data-curated identity modeling, audio-rich representations, and spatial dynamics controllability. 3DXTalker enables scalable identity modeling via 2D-to-3D data curation pipeline and disentangled representations, alleviating data scarcity and improving identity generalization. Then, we introduce frame-wise amplitude and emotional cues beyond standard speech embeddings, ensuring superior lip synchronization and nuanced expression modulation. These cues are unified by a flow-matching-based transformer for coherent facial dynamics. Moreover, 3DXTalker also enables natural head-pose motion generation while supporting stylized control via prompt-based conditioning. Extensive experiments show that 3DXTalker integrates lip synchronization, emotional expression, and head-pose dynamics within a unified framework, achieves superior performance in 3D talking avatar generation.

</details>


### [28] [MapVerse: A Benchmark for Geospatial Question Answering on Diverse Real-World Maps](https://arxiv.org/abs/2602.10518)
*Sharat Bhat,Harshita Khandelwal,Tushar Kataria,Vivek Gupta*

Main category: cs.CV

TL;DR: 本文提出了MapVerse，一个基于真实世界地图的大规模基准数据集，包含11,837个人工编写的问题-答案对，覆盖10类地图和多种问题类型，用于评估视觉语言模型在地图理解与空间推理方面的能力。实验表明，现有VLM在分类任务上表现尚可，但在复杂空间推理任务上仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有VLM地图推理基准数据集范围狭窄、依赖人工生成内容，难以真实评估模型的地理空间推理能力。

Method: 构建了大规模真实世界地图基准MapVerse，包含1025张真实地图和11837个人工标注QA对，并对10个SOTA模型进行系统评测与细粒度分析。

Result: 当前VLM在分类类任务上表现较好，但在需复杂空间推理的任务上表现较差，开源与闭源模型均存在明显短板。

Conclusion: MapVerse为地图多模态推理提供了更真实、更具挑战性的评估基准，揭示了现有模型在深层空间理解和推理上的关键瓶颈。

Abstract: Maps are powerful carriers of structured and contextual knowledge, encompassing geography, demographics, infrastructure, and environmental patterns. Reasoning over such knowledge requires models to integrate spatial relationships, visual cues, real-world context, and domain-specific expertise-capabilities that current large language models (LLMs) and vision-language models (VLMs) still struggle to exhibit consistently. Yet, datasets used to benchmark VLMs on map-based reasoning remain narrow in scope, restricted to specific domains, and heavily reliant on artificially generated content (outputs from LLMs or pipeline-based methods), offering limited depth for evaluating genuine geospatial reasoning. To address this gap, we present MapVerse, a large-scale benchmark built on real-world maps. It comprises 11,837 human-authored question-answer pairs across 1,025 maps, spanning ten diverse map categories and multiple question categories for each. The dataset provides a rich setting for evaluating map reading, interpretation, and multimodal reasoning. We evaluate ten state-of-the-art models against our benchmark to establish baselines and quantify reasoning gaps. Beyond overall performance, we conduct fine-grained categorical analyses to assess model inference across multiple dimensions and investigate the visual factors shaping reasoning outcomes. Our findings reveal that while current VLMs perform competitively on classification-style tasks, both open- and closed-source models fall short on advanced tasks requiring complex spatial reasoning.

</details>


### [29] [RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images](https://arxiv.org/abs/2602.10546)
*Hanzhe Yu,Yun Ye,Jintao Rong,Qi Xuan,Chen Ma*

Main category: cs.CV

TL;DR: 本文提出一个高质量、大规模的AI生成图像检测数据集（含73万+图像），涵盖多种生成方式并提供丰富标注；基于该数据集，设计了一种基于NLM噪声熵的轻量检测方法，实验表明其泛化能力强、性能优越，并开源数据与代码。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测数据集存在泛化性差、图像质量低、提示词过于简单、多样性不足等问题，难以支撑鲁棒检测模型的训练与评估。

Method: 构建了包含73万+图像的多源、高质量数据集，涵盖文本到图像生成（10,000+定制化提示）、图像修复、图像精炼和人脸替换等生成方式，并为每类图像提供细粒度标注（如修复区域二值掩码）；在此基础上，提出一种基于Non-Local Means噪声熵张量的轻量级检测方法。

Result: 在该数据集上训练的检测模型展现出更强的跨方法、跨域泛化能力；所提噪声熵检测方法性能优越，成为该数据集上的有力基线。

Conclusion: 本工作通过构建更贴近真实挑战的大规模高质量数据集，并配套提出可解释、轻量的检测方法，显著推动AI生成图像检测领域的基准建设与技术鲁棒性发展。

Abstract: The rapid advancement of generative AI has raised concerns about the authenticity of digital images, as highly realistic fake images can now be generated at low cost, potentially increasing societal risks. In response, several datasets have been established to train detection models aimed at distinguishing AI-generated images from real ones. However, existing datasets suffer from limited generalization, low image quality, overly simple prompts, and insufficient image diversity. To address these limitations, we propose a high-quality, large-scale dataset comprising over 730,000 images across multiple categories, including both real and AI-generated images. The generated images are synthesized via state-of-the-art methods, including text-to-image generation (guided by over 10,000 carefully designed prompts), image inpainting, image refinement, and face swapping. Each generated image is annotated with its generation method and category. Inpainting images further include binary masks to indicate inpainted regions, providing rich metadata for analysis. Compared to existing datasets, detection models trained on our dataset demonstrate superior generalization capabilities. Our dataset not only serves as a strong benchmark for evaluating detection methods but also contributes to advancing the robustness of AI-generated image detection techniques. Building upon this, we propose a lightweight detection method based on image noise entropy, which transforms the original image into an entropy tensor of Non-Local Means (NLM) noise before classification. Extensive experiments demonstrate that models trained on our dataset achieve strong generalization, and our method delivers competitive performance, establishing a solid baseline for future research. The dataset and source code are publicly available at https://real-hd.github.io.

</details>


### [30] [Enhancing Weakly Supervised Multimodal Video Anomaly Detection through Text Guidance](https://arxiv.org/abs/2602.10549)
*Shengyang Sun,Jiashen Hua,Junyi Feng,Xiaojin Gong*

Main category: cs.CV

TL;DR: 本文提出了一种文本引导的弱监督多模态视频异常检测框架，通过上下文学习增强文本数据并设计多尺度瓶颈Transformer融合模块，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督多模态视频异常检测方法对文本模态利用不足；文本虽具明确语义信息，但通用语言模型难以捕捉异常特异性，且相关文本描述稀缺，多模态融合还存在冗余与不平衡问题。

Method: 提出文本引导框架：1）基于上下文学习的多阶段文本增强机制，生成高质量异常文本用于微调文本特征提取器；2）设计多尺度瓶颈Transformer融合模块，利用压缩瓶颈token渐进式融合多模态信息。

Result: 在UCF-Crime和XD-Violence数据集上达到当前最优性能。

Conclusion: 文本模态在弱监督视频异常检测中具有重要价值，所提文本增强与瓶颈融合策略可有效缓解特征不匹配与模态失衡问题，提升检测鲁棒性与准确性。

Abstract: Weakly supervised multimodal video anomaly detection has gained significant attention, yet the potential of the text modality remains under-explored. Text provides explicit semantic information that can enhance anomaly characterization and reduce false alarms. However, extracting effective text features is challenging due to the inability of general-purpose language models to capture anomaly-specific nuances and the scarcity of relevant descriptions. Furthermore, multimodal fusion often suffers from redundancy and imbalance. To address these issues, we propose a novel text-guided framework. First, we introduce an in-context learning-based multi-stage text augmentation mechanism to generate high-quality anomaly text samples for fine-tuning the text feature extractor. Second, we design a multi-scale bottleneck Transformer fusion module that uses compressed bottleneck tokens to progressively integrate information across modalities, mitigating redundancy and imbalance. Experiments on UCF-Crime and XD-Violence demonstrate state-of-the-art performance.

</details>


### [31] [C^2ROPE: Causal Continuous Rotary Positional Encoding for 3D Large Multimodal-Models Reasoning](https://arxiv.org/abs/2602.10551)
*Guanting Ye,Qiyan Zhao,Wenhao Yu,Xiaofeng Zhang,Jianmin Ji,Yanyong Zhang,Ka-Veng Yuen*

Main category: cs.CV

TL;DR: 本文提出C^2RoPE，一种改进的旋转位置编码方法，用于3D大语言多模态模型，以解决传统RoPE在处理3D视觉特征时导致的空间连续性丢失和长程注意力衰减问题。


<details>
  <summary>Details</summary>
Motivation: 传统RoPE在3D多模态建模中引入1D时间索引，破坏了视觉特征在列维度上的空间连续性，并因假设时间邻近token因果性强而导致早期token被忽视。

Method: 提出C^2RoPE：构建融合1D时间与2D笛卡尔坐标的三元混合位置索引，并采用频率分配策略编码时空信息；引入基于切比雪夫距离的Chebyshev Causal Masking以建模2D空间中的因果依赖。

Result: 在3D场景推理和3D视觉问答等多个基准上验证了C^2RoPE的有效性，显著提升模型性能。

Conclusion: C^2RoPE通过显式建模视觉token的局部空间连续性与空间因果关系，有效缓解了RoPE在3D多模态任务中的固有缺陷，为3D LMMs的位置编码设计提供了新思路。

Abstract: Recent advances in 3D Large Multimodal Models (LMMs) built on Large Language Models (LLMs) have established the alignment of 3D visual features with LLM representations as the dominant paradigm. However, the inherited Rotary Position Embedding (RoPE) introduces limitations for multimodal processing. Specifically, applying 1D temporal positional indices disrupts the continuity of visual features along the column dimension, resulting in spatial locality loss. Moreover, RoPE follows the prior that temporally closer image tokens are more causally related, leading to long-term decay in attention allocation and causing the model to progressively neglect earlier visual tokens as the sequence length increases. To address these issues, we propose C^2RoPE, an improved RoPE that explicitly models local spatial Continuity and spatial Causal relationships for visual processing. C^2RoPE introduces a spatio-temporal continuous positional embedding mechanism for visual tokens. It first integrates 1D temporal positions with Cartesian-based spatial coordinates to construct a triplet hybrid positional index, and then employs a frequency allocation strategy to encode spatio-temporal positional information across the three index components. Additionally, we introduce Chebyshev Causal Masking, which determines causal dependencies by computing the Chebyshev distance of image tokens in 2D space. Evaluation results across various benchmarks, including 3D scene reasoning and 3D visual question answering, demonstrate C^2RoPE's effectiveness. The code is be available at https://github.com/ErikZ719/C2RoPE.

</details>


### [32] [MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning](https://arxiv.org/abs/2602.10575)
*Chenhao Zhang,Yazhe Niu,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出MetaphorStar，首个面向图像隐喻理解任务的端到端视觉强化学习框架，包含新数据集TFQ-Data、新RL方法TFQ-GRPO和新基准TFQ-Bench；在多项图像隐含意义理解任务上显著超越主流MLLMs，并揭示该任务可提升模型复杂视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在图像隐喻理解任务上表现不佳，因其缺乏多跳推理、文化背景建模和心理理论（ToM）能力。

Method: 提出MetaphorStar框架，包括细粒度数据集TFQ-Data、视觉强化学习方法TFQ-GRPO和结构化基准TFQ-Bench，并进行端到端训练与系统性消融分析。

Result: MetaphorStar-32B在多项图像隐含意义理解任务（多选题、开放题、判断题）上达到SOTA，平均提升82.6%；且训练该任务可增强模型通用视觉推理能力。

Conclusion: 视觉强化学习是提升图像隐喻理解能力的有效范式，MetaphorStar为该领域提供了首个开源、可复现、可扩展的完整解决方案。

Abstract: Metaphorical comprehension in images remains a critical challenge for Nowadays AI systems. While Multimodal Large Language Models (MLLMs) excel at basic Visual Question Answering (VQA), they consistently struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. This difficulty stems from the task's demand for sophisticated multi-hop reasoning, cultural context, and Theory of Mind (ToM) capabilities, which current models lack. To fill this gap, we propose MetaphorStar, the first end-to-end visual reinforcement learning (RL) framework for image implication tasks. Our framework includes three core components: the fine-grained dataset TFQ-Data, the visual RL method TFQ-GRPO, and the well-structured benchmark TFQ-Bench.
  Our fully open-source MetaphorStar family, trained using TFQ-GRPO on TFQ-Data, significantly improves performance by an average of 82.6% on the image implication benchmarks. Compared with 20+ mainstream MLLMs, MetaphorStar-32B achieves state-of-the-art (SOTA) on Multiple-Choice Question and Open-Style Question, significantly outperforms the top closed-source model Gemini-3.0-pro on True-False Question. Crucially, our experiments reveal that learning image implication tasks improves the general understanding ability, especially the complex visual reasoning ability. We further provide a systematic analysis of model parameter scaling, training data scaling, and the impact of different model architectures and training strategies, demonstrating the broad applicability of our method. We open-sourced all model weights, datasets, and method code at https://metaphorstar.github.io.

</details>


### [33] [Enhancing Underwater Images via Adaptive Semantic-aware Codebook Learning](https://arxiv.org/abs/2602.10586)
*Bosen Lin,Feng Gao,Yanwei Yu,Junyu Dong,Qian Du*

Main category: cs.CV

TL;DR: 本文提出SUCode，一种语义感知的水下图像增强网络，通过语义感知的离散码本表示实现自适应增强，解决了传统方法忽略区域退化不一致导致的颜色失真和细节丢失问题。


<details>
  <summary>Details</summary>
Motivation: 水下图像增强（UIE）是一个病态问题，缺乏自然干净的参考图像，且不同语义区域的退化程度差异显著；现有方法采用单一全局模型，忽视场景组件间退化不一致性，导致颜色失真与细节丢失。

Method: 提出SUCode网络：1）语义感知的像素级码本表示；2）三阶段训练范式避免伪真值污染；3）门控通道注意力模块（GCAM）与频域感知特征融合（FAFF）联合建模通道与频率信息。

Result: 在多个基准上实验表明，SUCode在有参考和无参考评价指标上均达到SOTA性能。

Conclusion: SUCode通过语义引导的码本建模与多维度特征融合，有效提升了异质水下场景中颜色保真度与纹理恢复能力，为UIE提供了新思路。

Abstract: Underwater Image Enhancement (UIE) is an ill-posed problem where natural clean references are not available, and the degradation levels vary significantly across semantic regions. Existing UIE methods treat images with a single global model and ignore the inconsistent degradation of different scene components. This oversight leads to significant color distortions and loss of fine details in heterogeneous underwater scenes, especially where degradation varies significantly across different image regions. Therefore, we propose SUCode (Semantic-aware Underwater Codebook Network), which achieves adaptive UIE from semantic-aware discrete codebook representation. Compared with one-shot codebook-based methods, SUCode exploits semantic-aware, pixel-level codebook representation tailored to heterogeneous underwater degradation. A three-stage training paradigm is employed to represent raw underwater image features to avoid pseudo ground-truth contamination. Gated Channel Attention Module (GCAM) and Frequency-Aware Feature Fusion (FAFF) jointly integrate channel and frequency cues for faithful color restoration and texture recovery. Extensive experiments on multiple benchmarks demonstrate that SUCode achieves state-of-the-art performance, outperforming recent UIE methods on both reference and no-reference metrics. The code will be made public available at https://github.com/oucailab/SUCode.

</details>


### [34] [Enhancing YOLOv11n for Reliable Child Detection in Noisy Surveillance Footage](https://arxiv.org/abs/2602.10592)
*Khanh Linh Tran,Minh Nguyen Dang,Thien Nguyen Trong,Hung Nguyen Quoc,Linh Nguyen Kieu*

Main category: cs.CV

TL;DR: 本文提出了一种轻量、实用的儿童检测增强方法，基于YOLOv11n，在低质监控视频中提升小目标、遮挡、模糊、弱光等挑战下的检测性能，通过领域定制数据增强与SAHI推理策略，在保持边缘设备实时性的同时显著提升mAP。


<details>
  <summary>Details</summary>
Motivation: 解决现实监控场景（如托儿所、走失儿童预警）中因摄像头质量差导致的儿童检测困难问题，包括遮挡、小目标、低分辨率、运动模糊和光照差等挑战。

Method: 基于YOLOv11n构建轻量部署管线；设计面向儿童检测的合成增强策略（空间扰动+光度退化）；在推理阶段引入Slicing Aided Hyper Inference（SAHI）提升小目标与部分遮挡目标召回率；使用Roboflow Daycare数据集的儿童子集训练与评估。

Result: 相比YOLOv11n基线，mAP@0.5提升0.7个百分点至0.967，mAP@0.5:0.95提升2.3个百分点至0.783；全程无需修改网络结构，支持边缘设备实时运行。

Conclusion: 该方案在不增加模型复杂度的前提下，显著提升了低质监控视频中的儿童检测精度与鲁棒性，兼具实用性、轻量化与可部署性，适用于资源受限的实际安防系统。

Abstract: This paper presents a practical and lightweight solution for enhancing child detection in low-quality surveillance footage, a critical component in real-world missing child alert and daycare monitoring systems. Building upon the efficient YOLOv11n architecture, we propose a deployment-ready pipeline that improves detection under challenging conditions including occlusion, small object size, low resolution, motion blur, and poor lighting commonly found in existing CCTV infrastructures. Our approach introduces a domain-specific augmentation strategy that synthesizes realistic child placements using spatial perturbations such as partial visibility, truncation, and overlaps, combined with photometric degradations including lighting variation and noise. To improve recall of small and partially occluded instances, we integrate Slicing Aided Hyper Inference (SAHI) at inference time. All components are trained and evaluated on a filtered, child-only subset of the Roboflow Daycare dataset. Compared to the baseline YOLOv11n, our enhanced system achieves a mean Average Precision at 0.5 IoU (mAP@0.5) of 0.967 and a mean Average Precision averaged over IoU thresholds from 0.5 to 0.95 (mAP@0.5:0.95) of 0.783, yielding absolute improvements of 0.7 percent and 2.3 percent, respectively, without architectural changes. Importantly, the entire pipeline maintains compatibility with low-power edge devices and supports real-time performance, making it particularly well suited for low-cost or resource-constrained industrial surveillance deployments. The example augmented dataset and the source code used to generate it are available at: https://github.com/html-ptit/Data-Augmentation-YOLOv11n-child-detection

</details>


### [35] [Fast Person Detection Using YOLOX With AI Accelerator For Train Station Safety](https://arxiv.org/abs/2602.10593)
*Mas Nurul Achmadiah,Novendra Setyawan,Achmad Arif Bryantono,Chi-Chia Sun,Wen-Kai Kuo*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOX与边缘AI加速器（Hailo-8）的乘客检测方法，用于提升火车站道口安全；实验表明其相比Jetson Orin Nano在精度上提升超12%，延迟降低20ms。


<details>
  <summary>Details</summary>
Motivation: 火车站道口因乘客疏忽越线引发事故，需增强实时、高精度的乘客检测能力以提升安全。

Method: 采用YOLOX目标检测模型，并部署于Hailo-8边缘AI加速器与Jetson Orin Nano两种硬件平台进行对比实验。

Result: Hailo-8在检测精度上优于Jetson Orin Nano（提升超12%），且推理延迟更低（减少20ms）。

Conclusion: Hailo-8作为边缘AI加速器在火车站乘客检测任务中兼具更高精度与更低延迟，更适合实时安防应用。

Abstract: Recently, Image processing has advanced Faster and applied in many fields, including health, industry, and transportation. In the transportation sector, object detection is widely used to improve security, for example, in traffic security and passenger crossings at train stations. Some accidents occur in the train crossing area at the station, like passengers uncarefully when passing through the yellow line. So further security needs to be developed. Additional technology is required to reduce the number of accidents. This paper focuses on passenger detection applications at train stations using YOLOX and Edge AI Accelerator hardware. the performance of the AI accelerator will be compared with Jetson Orin Nano. The experimental results show that the Hailo-8 AI hardware accelerator has higher accuracy than Jetson Orin Nano (improvement of over 12%) and has lower latency than Jetson Orin Nano (reduced 20 ms).

</details>


### [36] [Improving Medical Visual Reinforcement Fine-Tuning via Perception and Reasoning Augmentation](https://arxiv.org/abs/2602.10619)
*Guangjing Yang,ZhangYuan Yu,Ziyuan Qin,Xinyuan Song,Huahui Yi,Qingbo Kang,Jun Gao,Yiyue Li,Chenlin Du,Qicheng Lao*

Main category: cs.CV

TL;DR: 本文提出了VRFT-Aug，一种专为医学影像领域设计的视觉强化微调框架，通过引入先验知识注入、感知驱动的策略优化、医学启发的奖励塑造和行为模仿等策略，显著提升了模型在医学图像任务中的感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的强化微调（RFT）方法在跨模态、以视觉为中心的领域（尤其是医学影像）中探索不足，而医学任务需兼顾鲁棒视觉感知与结构化推理。

Method: 提出VRFT-Aug框架，包含先验知识注入、感知驱动的策略优化、医学启发的奖励塑造和行为模仿四种训练策略，以增强感知与推理能力并稳定RFT过程。

Result: 在多个医学数据集上的实验表明，VRFT-Aug持续优于标准监督微调和RFT基线，并提供了可推广至其他医学图像任务的实证训练启发。

Conclusion: VRFT-Aug为高风险医学应用中构建可靠、具备推理能力的模型提供了可操作的指导与新思路。

Abstract: While recent advances in Reinforcement Fine-Tuning (RFT) have shown that rule-based reward schemes can enable effective post-training for large language models, their extension to cross-modal, vision-centric domains remains largely underexplored. This limitation is especially pronounced in the medical imaging domain, where effective performance requires both robust visual perception and structured reasoning. In this work, we address this gap by proposing VRFT-Aug, a visual reinforcement fine-tuning framework tailored for the medical domain. VRFT-Aug introduces a series of training strategies designed to augment both perception and reasoning, including prior knowledge injection, perception-driven policy refinement, medically informed reward shaping, and behavioral imitation. Together, these methods aim to stabilize and improve the RFT process.
  Through extensive experiments across multiple medical datasets, we show that our approaches consistently outperform both standard supervised fine-tuning and RFT baselines. Moreover, we provide empirically grounded insights and practical training heuristics that can be generalized to other medical image tasks. We hope this work contributes actionable guidance and fresh inspiration for the ongoing effort to develop reliable, reasoning-capable models for high-stakes medical applications.

</details>


### [37] [A Vision-Language Foundation Model for Zero-shot Clinical Collaboration and Automated Concept Discovery in Dermatology](https://arxiv.org/abs/2602.10624)
*Siyuan Yan,Xieji Li,Dan Mo,Philipp Tschandl,Yiwen Jiang,Zhonghua Wang,Ming Hu,Lie Ju,Cristina Vico-Alonso,Yizhen Zheng,Jiahe Liu,Juexiao Zhou,Camilla Chello,Jen G. Cheung,Julien Anriot,Luc Thomas,Clare Primiero,Gin Tan,Aik Beng Ng,Simon See,Xiaoying Tang,Albert Ip,Xiaoyang Liao,Adrian Bowling,Martin Haskett,Shuang Zhao,Monika Janda,H. Peter Soyer,Victoria Mar,Harald Kittler,Zongyuan Ge*

Main category: cs.CV

TL;DR: DermFM-Zero 是一个无需任务微调即可在皮肤科诊断与多模态检索中实现零样本SOTA性能的视觉-语言基础模型，已在多项临床研究中验证其提升医生诊断准确率、超越专家表现及增强鲁棒性与可解释性的能力。


<details>
  <summary>Details</summary>
Motivation: 解决医学基础模型依赖任务特定微调、难以广泛部署的问题，探索零样本临床决策支持的可行性与安全性。

Method: 通过掩码潜在建模（masked latent modelling）和对比学习，在超400万个多模态数据点上训练皮肤科视觉-语言基础模型 DermFM-Zero；结合稀疏自编码器进行无监督概念解耦以提升可解释性与去偏能力。

Result: 在20个零样本诊断与多模态检索基准上达到SOTA；在三项跨国临床研究（1100+医生）中显著提升基层医生诊断准确率、超越皮肤科专家的皮肤癌评估能力，并使非专家在协作中优于未辅助专家；其潜在表征可被稀疏自编码器无监督解耦为临床有意义的概念，有效抑制伪影偏差。

Conclusion: DermFM-Zero 证明了高质量医学基础模型可在不微调前提下提供有效、安全且透明的零样本临床决策支持，推动AI在真实医疗场景中的落地。

Abstract: Medical foundation models have shown promise in controlled benchmarks, yet widespread deployment remains hindered by reliance on task-specific fine-tuning. Here, we introduce DermFM-Zero, a dermatology vision-language foundation model trained via masked latent modelling and contrastive learning on over 4 million multimodal data points. We evaluated DermFM-Zero across 20 benchmarks spanning zero-shot diagnosis and multimodal retrieval, achieving state-of-the-art performance without task-specific adaptation. We further evaluated its zero-shot capabilities in three multinational reader studies involving over 1,100 clinicians. In primary care settings, AI assistance enabled general practitioners to nearly double their differential diagnostic accuracy across 98 skin conditions. In specialist settings, the model significantly outperformed board-certified dermatologists in multimodal skin cancer assessment. In collaborative workflows, AI assistance enabled non-experts to surpass unassisted experts while improving management appropriateness. Finally, we show that DermFM-Zero's latent representations are interpretable: sparse autoencoders unsupervisedly disentangle clinically meaningful concepts that outperform predefined-vocabulary approaches and enable targeted suppression of artifact-induced biases, enhancing robustness without retraining. These findings demonstrate that a foundation model can provide effective, safe, and transparent zero-shot clinical decision support.

</details>


### [38] [(MGS)$^2$-Net: Unifying Micro-Geometric Scale and Macro-Geometric Structure for Cross-View Geo-Localization](https://arxiv.org/abs/2602.10704)
*Minglei Li,Mengfan He,Chao Chen,Ziyang Meng*

Main category: cs.CV

TL;DR: 本文提出(MGS)²框架，通过宏观几何结构过滤（MGSF）和微观几何尺度自适应（MGSA）模块，结合几何-外观对比蒸馏损失（GACD），显著提升跨视角地理定位在几何失配下的鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角地理定位方法在倾斜航拍图与正射卫星图之间存在剧烈几何失配时性能脆弱，主要因忽略底层3D几何结构（如垂直立面宏观结构和尺度变化微观因素）导致特征对齐失败。

Method: 提出(MGS)²框架：1）Macro-Geometric Structure Filtering (MGSF) 模块利用膨胀几何梯度滤除高频立面伪影、增强水平面不变特征；2）Micro-Geometric Scale Adaptation (MGSA) 模块基于深度先验进行多分支尺度自适应特征融合；3）Geometric-Appearance Contrastive Distillation (GACD) 损失函数抑制倾斜遮挡干扰。

Result: 在University-1652和SUES-200数据集上Recall@1分别达97.5%和97.02%，达到SOTA；且具备优异的跨数据集泛化能力。

Conclusion: 将3D几何先验显式建模引入CVGL任务可有效缓解视点差异带来的特征失配问题，(MGS)²为GNSS拒止环境下无人机导航提供了更鲁棒的定位基础。

Abstract: Cross-view geo-localization (CVGL) is pivotal for GNSS-denied UAV navigation but remains brittle under the drastic geometric misalignment between oblique aerial views and orthographic satellite references. Existing methods predominantly operate within a 2D manifold, neglecting the underlying 3D geometry where view-dependent vertical facades (macro-structure) and scale variations (micro-scale) severely corrupt feature alignment. To bridge this gap, we propose (MGS)$^2$, a geometry-grounded framework. The core of our innovation is the Macro-Geometric Structure Filtering (MGSF) module. Unlike pixel-wise matching sensitive to noise, MGSF leverages dilated geometric gradients to physically filter out high-frequency facade artifacts while enhancing the view-invariant horizontal plane, directly addressing the domain shift. To guarantee robust input for this structural filtering, we explicitly incorporate a Micro-Geometric Scale Adaptation (MGSA) module. MGSA utilizes depth priors to dynamically rectify scale discrepancies via multi-branch feature fusion. Furthermore, a Geometric-Appearance Contrastive Distillation (GACD) loss is designed to strictly discriminate against oblique occlusions. Extensive experiments demonstrate that (MGS)$^2$ achieves state-of-the-art performance, recording a Recall@1 of 97.5\% on University-1652 and 97.02\% on SUES-200. Furthermore, the framework exhibits superior cross-dataset generalization against geometric ambiguity. The code is available at: \href{https://github.com/GabrielLi1473/MGS-Net}{https://github.com/GabrielLi1473/MGS-Net}.

</details>


### [39] [Eliminating VAE for Fast and High-Resolution Generative Detail Restoration](https://arxiv.org/abs/2602.10630)
*Yan Wang,Shijie Zhao,Junlin Li,Li Zhang*

Main category: cs.CV

TL;DR: 本文提出GenDR-Pix，一种基于像素空间的一步扩散超分辨率方法，通过消除VAE、多阶段对抗蒸馏、随机填充与掩码傅里叶损失等技术，在显著加速（2.8倍）和减存（60%）的同时保持视觉质量，实现4K图像1秒内恢复（仅需6GB显存）。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在超分辨率任务中推理慢、内存占用高，尤其VAE成为延迟与内存瓶颈；tile-by-tile处理影响效率，需彻底摆脱VAE并优化像素级重建。

Method: 1）用pixel-shuffle/unsqueeze替代VAE，构建像素空间GenDR-Pix；2）设计多阶段对抗蒸馏，利用前一阶段生成特征指导判别器；3）引入随机填充增强生成特征、防止判别器坍缩；4）采用掩码傅里叶空间损失约束振幅异常；5）结合padding自集成与无分类器引导提升推理效果。

Result: GenDR-Pix相较GenDR实现2.8倍加速与60%内存节省，视觉质量几乎无损，在单卡6GB显存上1秒完成4K图像超分，性能超越其他一步式扩散SR方法。

Conclusion: 消除VAE并转向像素空间是加速扩散SR的有效路径；多阶段对抗蒸馏与频域约束可缓解像素shuffle引入的重复纹理伪影；所提方法在速度、内存与质量间取得优异平衡，推动扩散模型在实际超分场景落地。

Abstract: Diffusion models have attained remarkable breakthroughs in the real-world super-resolution (SR) task, albeit at slow inference and high demand on devices. To accelerate inference, recent works like GenDR adopt step distillation to minimize the step number to one. However, the memory boundary still restricts the maximum processing size, necessitating tile-by-tile restoration of high-resolution images. Through profiling the pipeline, we pinpoint that the variational auto-encoder (VAE) is the bottleneck of latency and memory. To completely solve the problem, we leverage pixel-(un)shuffle operations to eliminate the VAE, reversing the latent-based GenDR to pixel-space GenDR-Pix. However, upscale with x8 pixelshuffle may induce artifacts of repeated patterns. To alleviate the distortion, we propose a multi-stage adversarial distillation to progressively remove the encoder and decoder. Specifically, we utilize generative features from the previous stage models to guide adversarial discrimination. Moreover, we propose random padding to augment generative features and avoid discriminator collapse. We also introduce a masked Fourier space loss to penalize the outliers of amplitude. To improve inference performance, we empirically integrate a padding-based self-ensemble with classifier-free guidance to improve inference scaling. Experimental results show that GenDR-Pix performs 2.8x acceleration and 60% memory-saving compared to GenDR with negligible visual degradation, surpassing other one-step diffusion SR. Against all odds, GenDR-Pix can restore 4K image in only 1 second and 6GB.

</details>


### [40] [From Steering to Pedalling: Do Autonomous Driving VLMs Generalize to Cyclist-Assistive Spatial Perception and Planning?](https://arxiv.org/abs/2602.10771)
*Krishna Kanth Nakka,Vedasri Nakka*

Main category: cs.CV

TL;DR: 本文提出了CyclingVQA基准，用于评估视觉-语言模型（VLMs）在骑行者视角下的感知、时空理解与交通规则推理能力，发现当前模型在骑行者特有交通线索理解和车道关联方面存在明显不足，且车辆专用模型向骑行辅助场景迁移效果不佳。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型评估多以车辆为中心，缺乏针对骑行者视角的感知与推理能力评测，而骑行者在城市交通中面临独特安全挑战，亟需适配的智能辅助系统。

Method: 构建面向骑行者的诊断型视觉问答基准CyclingVQA，涵盖感知、时空理解及交通规则到车道的推理任务；对31+个主流VLM（通用型、空间增强型、自动驾驶专用型）进行系统评测，并开展失败模式分析。

Result: 当前VLM在骑行者视角任务上展现出一定能力，但在解读骑行者专属交通信号、将标志与对应导航车道关联等方面表现薄弱；部分自动驾驶专用模型性能甚至低于强通用VLM，表明车辆中心训练难以有效迁移至骑行辅助场景。

Conclusion: CyclingVQA揭示了现有VLM在 cyclist-centric理解上的关键短板，为发展更安全、更适配的骑行辅助智能系统提供了评测基准与改进方向。

Abstract: Cyclists often encounter safety-critical situations in urban traffic, highlighting the need for assistive systems that support safe and informed decision-making. Recently, vision-language models (VLMs) have demonstrated strong performance on autonomous driving benchmarks, suggesting their potential for general traffic understanding and navigation-related reasoning. However, existing evaluations are predominantly vehicle-centric and fail to assess perception and reasoning from a cyclist-centric viewpoint. To address this gap, we introduce CyclingVQA, a diagnostic benchmark designed to probe perception, spatio-temporal understanding, and traffic-rule-to-lane reasoning from a cyclist's perspective. Evaluating 31+ recent VLMs spanning general-purpose, spatially enhanced, and autonomous-driving-specialized models, we find that current models demonstrate encouraging capabilities, while also revealing clear areas for improvement in cyclist-centric perception and reasoning, particularly in interpreting cyclist-specific traffic cues and associating signs with the correct navigational lanes. Notably, several driving-specialized models underperform strong generalist VLMs, indicating limited transfer from vehicle-centric training to cyclist-assistive scenarios. Finally, through systematic error analysis, we identify recurring failure modes to guide the development of more effective cyclist-assistive intelligent systems.

</details>


### [41] [VideoSTF: Stress-Testing Output Repetition in Video Large Language Models](https://arxiv.org/abs/2602.10639)
*Yuxin Cao,Wei Song,Shangzhi Xu,Jingling Xue,Jin Song Dong*

Main category: cs.CV

TL;DR: 本文提出VideoSTF框架，首次系统评估视频大语言模型（VideoLLMs）中的输出重复问题，发现该问题广泛存在且对视频时间扰动高度敏感，构成潜在安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLM基准主要关注任务准确性和事实正确性，忽略了输出重复这一未被充分研究的生成失败模式。

Method: 提出VideoSTF框架，基于三种n-gram指标量化重复，并构建含10,000个多样化视频及可控时间变换的标准化测试集；在10个先进VideoLLMs上开展普遍测试、时间压力测试和对抗攻击。

Result: 输出重复现象普遍存在，且对视频时间扰动高度敏感；简单时间变换即可在黑盒条件下高效诱发重复退化，暴露其为可利用的安全漏洞。

Conclusion: 输出重复是现代VideoLLMs的根本稳定性问题，亟需引入稳定性导向的视频-语言系统评估范式。

Abstract: Video Large Language Models (VideoLLMs) have recently achieved strong performance in video understanding tasks. However, we identify a previously underexplored generation failure: severe output repetition, where models degenerate into self-reinforcing loops of repeated phrases or sentences. This failure mode is not captured by existing VideoLLM benchmarks, which focus primarily on task accuracy and factual correctness. We introduce VideoSTF, the first framework for systematically measuring and stress-testing output repetition in VideoLLMs. VideoSTF formalizes repetition using three complementary n-gram-based metrics and provides a standardized testbed of 10,000 diverse videos together with a library of controlled temporal transformations. Using VideoSTF, we conduct pervasive testing, temporal stress testing, and adversarial exploitation across 10 advanced VideoLLMs. We find that output repetition is widespread and, critically, highly sensitive to temporal perturbations of video inputs. Moreover, we show that simple temporal transformations can efficiently induce repetitive degeneration in a black-box setting, exposing output repetition as an exploitable security vulnerability. Our results reveal output repetition as a fundamental stability issue in modern VideoLLMs and motivate stability-aware evaluation for video-language systems. Our evaluation code and scripts are available at: https://github.com/yuxincao22/VideoSTF_benchmark.

</details>


### [42] [Multimodal Priors-Augmented Text-Driven 3D Human-Object Interaction Generation](https://arxiv.org/abs/2602.10659)
*Yin Wang,Ziyao Zhang,Zhiying Leng,Haitian Liu,Frederick W. B. Li,Mu Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: 本文提出MP-HOI框架，通过多模态先验、增强对象表征、模态感知的混合专家模型和级联扩散交互监督，解决文本驱动3D人-物交互运动生成中的三大问题：人类运动次优、物体运动不自然、人-物交互弱。


<details>
  <summary>Details</summary>
Motivation: 现有文本到HOI直接映射方法受限于跨模态鸿沟，导致人类运动次优、物体运动不自然、人-物交互弱。

Method: 提出MP-HOI框架，包含四个核心模块：（1）利用多模态数据（文本、图像、姿态/物体）作为先验；（2）增强物体表征（几何关键点、接触特征、动态属性）；（3）模态感知的混合专家（MoE）模型用于多模态特征融合；（4）带交互监督的级联扩散框架逐步优化人-物交互特征。

Result: MP-HOI在生成高保真、细粒度HOI运动方面优于现有方法。

Conclusion: MP-HOI通过整合多模态先验、改进表征、模态感知融合与交互监督机制，有效缓解了文本驱动3D HOI生成中的跨模态建模难题。

Abstract: We address the challenging task of text-driven 3D human-object interaction (HOI) motion generation. Existing methods primarily rely on a direct text-to-HOI mapping, which suffers from three key limitations due to the significant cross-modality gap: (Q1) sub-optimal human motion, (Q2) unnatural object motion, and (Q3) weak interaction between humans and objects. To address these challenges, we propose MP-HOI, a novel framework grounded in four core insights: (1) Multimodal Data Priors: We leverage multimodal data (text, image, pose/object) from large multimodal models as priors to guide HOI generation, which tackles Q1 and Q2 in data modeling. (2) Enhanced Object Representation: We improve existing object representations by incorporating geometric keypoints, contact features, and dynamic properties, enabling expressive object representations, which tackles Q2 in data representation. (3) Multimodal-Aware Mixture-of-Experts (MoE) Model: We propose a modality-aware MoE model for effective multimodal feature fusion paradigm, which tackles Q1 and Q2 in feature fusion. (4) Cascaded Diffusion with Interaction Supervision: We design a cascaded diffusion framework that progressively refines human-object interaction features under dedicated supervision, which tackles Q3 in interaction refinement. Comprehensive experiments demonstrate that MP-HOI outperforms existing approaches in generating high-fidelity and fine-grained HOI motions.

</details>


### [43] [Towards Learning a Generalizable 3D Scene Representation from 2D Observations](https://arxiv.org/abs/2602.10943)
*Martin Gromniak,Jan-Gerrit Habekost,Sebastian Kamp,Sven Magg,Stefan Wermter*

Main category: cs.CV

TL;DR: 本文提出了一种通用的神经辐射场（NeRF）方法，用于从机器人第一人称视角预测全局工作空间的3D占据情况，无需场景微调，且在真实机器人上验证了其重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法多基于相机坐标系，难以直接用于机器人操作；需在全局工作空间帧中构建占据表示，并实现对未见物体布局的泛化能力。

Method: 提出一种通用神经辐射场方法，将多视角机器人观测映射到全局工作空间帧下的3D占据表示，支持灵活源视图输入，无需场景特定微调。

Result: 在40个真实场景上训练后，在 humanoid 机器人上达到26mm的整体3D重建误差（含遮挡区域），优于传统双目视觉方法。

Conclusion: 该方法能有效推断完整3D工作空间占据，具备强泛化性与实际机器人部署潜力。

Abstract: We introduce a Generalizable Neural Radiance Field approach for predicting 3D workspace occupancy from egocentric robot observations. Unlike prior methods operating in camera-centric coordinates, our model constructs occupancy representations in a global workspace frame, making it directly applicable to robotic manipulation. The model integrates flexible source views and generalizes to unseen object arrangements without scene-specific finetuning. We demonstrate the approach on a humanoid robot and evaluate predicted geometry against 3D sensor ground truth. Trained on 40 real scenes, our model achieves 26mm reconstruction error, including occluded regions, validating its ability to infer complete 3D occupancy beyond traditional stereo vision methods.

</details>


### [44] [AurigaNet: A Real-Time Multi-Task Network for Enhanced Urban Driving Perception](https://arxiv.org/abs/2602.10660)
*Kiarash Ghasemzadeh,Sedigheh Dehghani*

Main category: cs.CV

TL;DR: 本文提出AurigaNet，一种用于自动驾驶感知的先进多任务网络，统一处理目标检测、车道线检测和可行驶区域实例分割，在BDD100K数据集上取得多项SOTA指标，并在Jetson Orin NX上验证实时性。


<details>
  <summary>Details</summary>
Motivation: 开发可靠、高效、泛化能力强的自动驾驶AI系统仍面临挑战，而多任务学习可提升计算效率、实时性与泛化能力。

Method: 提出端到端多任务网络AurigaNet，联合建模目标检测、车道检测与可行驶区域实例分割；基于BDD100K训练与评估；并在Jetson Orin NX嵌入式平台部署验证实时性。

Result: 在BDD100K上：可行驶区域分割IoU达85.2%（+0.7%），车道检测IoU达60.8%（+30%+），目标检测mAP@0.5:0.95为47.6%（+2.9%）；嵌入式设备上实现实时推理。

Conclusion: AurigaNet是一种鲁棒、高效、可部署的多任务感知架构，显著提升了自动驾驶感知性能与实用性。

Abstract: Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet's potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.

</details>


### [45] [Enhancing Predictability of Multi-Tenant DNN Inference for Autonomous Vehicles' Perception](https://arxiv.org/abs/2602.11004)
*Liangkai Liu,Kang G. Shin,Jinkyu Lee,Chengmo Yang,Weisong Shi*

Main category: cs.CV

TL;DR: 本文提出了一种名为PP-DNN的可预测感知系统，通过动态选择关键帧和感兴趣区域（ROI）来减少待处理图像数据量，同时保持多任务DNN的精度，从而提升自动驾驶车辆感知的实时性与可预测性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖传感器和深度神经网络进行实时环境感知与决策，但受限于车载计算资源，难以满足大模型DNN推理的实时性需求；现有方法多聚焦于模型压缩，而忽视了输入数据层面的优化潜力。

Method: PP-DNN系统包含ROI生成器（基于帧间相似性与交通场景识别关键帧/ROI）、FLOPs预测器（预测MAC操作数）、ROI调度器（协调多DNN模型处理）和检测预测器（处理非关键帧）；在ROS平台实现，并在BDD100K和nuScenes数据集上评估。

Result: 相比基线，PP-DNN显著提升感知可预测性：融合帧数提升至7.3倍、融合延迟降低超2.6倍、延迟波动降低超2.3倍、检测完整性提高75.4%、成本效益最高提升98%。

Conclusion: 动态选择关键帧与ROI是一种高效的数据驱动优化策略，可在不牺牲精度的前提下大幅提升多任务DNN感知系统的实时性、可预测性与资源效率。

Abstract: Autonomous vehicles (AVs) rely on sensors and deep neural networks (DNNs) to perceive their surrounding environment and make maneuver decisions in real time. However, achieving real-time DNN inference in the AV's perception pipeline is challenging due to the large gap between the computation requirement and the AV's limited resources. Most, if not all, of existing studies focus on optimizing the DNN inference time to achieve faster perception by compressing the DNN model with pruning and quantization. In contrast, we present a Predictable Perception system with DNNs (PP-DNN) that reduce the amount of image data to be processed while maintaining the same level of accuracy for multi-tenant DNNs by dynamically selecting critical frames and regions of interest (ROIs). PP-DNN is based on our key insight that critical frames and ROIs for AVs vary with the AV's surrounding environment. However, it is challenging to identify and use critical frames and ROIs in multi-tenant DNNs for predictable inference. Given image-frame streams, PP-DNN leverages an ROI generator to identify critical frames and ROIs based on the similarities of consecutive frames and traffic scenarios. PP-DNN then leverages a FLOPs predictor to predict multiply-accumulate operations (MACs) from the dynamic critical frames and ROIs. The ROI scheduler coordinates the processing of critical frames and ROIs with multiple DNN models. Finally, we design a detection predictor for the perception of non-critical frames. We have implemented PP-DNN in an ROS-based AV pipeline and evaluated it with the BDD100K and the nuScenes dataset. PP-DNN is observed to significantly enhance perception predictability, increasing the number of fusion frames by up to 7.3x, reducing the fusion delay by >2.6x and fusion-delay variations by >2.3x, improving detection completeness by 75.4% and the cost-effectiveness by up to 98% over the baseline.

</details>


### [46] [Dynamic Frequency Modulation for Controllable Text-driven Image Generation](https://arxiv.org/abs/2602.10662)
*Tiandong Shi,Ling Zhao,Ji Qi,Jiayi Ma,Chengli Peng*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的频域调制方法，通过动态衰减的频率相关加权函数，在扩散模型中调控噪声隐变量的频谱，以在保持图像结构框架一致性的同时实现精准语义编辑。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导扩散模型在修改文本提示进行语义调整时，常引发不期望的全局结构变化；且依赖经验性特征图干预，稳定性差、效果受限。

Method: 从频域视角分析噪声隐变量频谱对生成过程中结构框架与纹理分层涌现的影响，发现低频主导早期结构构建、高频主导后期细节合成；据此设计训练-free的频域调制方法，采用动态衰减的频率加权函数直接操控噪声隐变量。

Result: 该方法避免了经验性特征图选择，在保持结构一致性前提下支持精准语义编辑；大量实验表明其显著优于当前SOTA方法，在结构保持与语义更新之间取得更优平衡。

Conclusion: 频域视角为扩散模型可控生成提供了新思路；所提训练-free频域调制方法兼具有效性、稳定性和易用性，是文本引导图像编辑的重要进展。

Abstract: The success of text-guided diffusion models has established a new image generation paradigm driven by the iterative refinement of text prompts. However, modifying the original text prompt to achieve the expected semantic adjustments often results in unintended global structure changes that disrupt user intent. Existing methods rely on empirical feature map selection for intervention, whose performance heavily depends on appropriate selection, leading to suboptimal stability. This paper tries to solve the aforementioned problem from a frequency perspective and analyzes the impact of the frequency spectrum of noisy latent variables on the hierarchical emergence of the structure framework and fine-grained textures during the generation process. We find that lower-frequency components are primarily responsible for establishing the structure framework in the early generation stage. Their influence diminishes over time, giving way to higher-frequency components that synthesize fine-grained textures. In light of this, we propose a training-free frequency modulation method utilizing a frequency-dependent weighting function with dynamic decay. This method maintains the structure framework consistency while permitting targeted semantic modifications. By directly manipulating the noisy latent variable, the proposed method avoids the empirical selection of internal feature maps. Extensive experiments demonstrate that the proposed method significantly outperforms current state-of-the-art methods, achieving an effective balance between preserving structure and enabling semantic updates.

</details>


### [47] [AMAP-APP: Efficient Segmentation and Morphometry Quantification of Fluorescent Microscopy Images of Podocytes](https://arxiv.org/abs/2602.10663)
*Arash Fatehi,David Unnersjö-Jess,Linus Butt,Noémie Moreau,Thomas Benzing,Katarzyna Bozek*

Main category: cs.CV

TL;DR: AMAP-APP is a cross-platform desktop application that significantly speeds up automated podocyte foot process quantification by replacing instance segmentation with classic image processing, while maintaining high accuracy and introducing an improved ROI algorithm.


<details>
  <summary>Details</summary>
Motivation: The original AMAP method suffers from high computational demands, absence of a user interface, and Linux-only dependency, limiting its accessibility in kidney research.

Method: AMAP-APP replaces intensive instance segmentation with classic image processing, retains the original semantic segmentation model, and introduces a refined Region of Interest (ROI) algorithm; validated on 365 mouse and human STED/confocal images using Pearson correlation and Two One-Sided T-tests (TOST).

Result: AMAP-APP achieves 147× faster processing on consumer hardware; morphometric outputs show r > 0.90 correlation and statistical equivalence (TOST P < 0.05) vs. original AMAP; new ROI algorithm yields lower deviation from manual delineations.

Conclusion: AMAP-APP democratizes deep learning-based podocyte morphometry by enabling use on standard computers across Windows, macOS, and Linux, facilitating broader adoption in nephrology research and clinical diagnostics.

Abstract: Background: Automated podocyte foot process quantification is vital for kidney research, but the established "Automatic Morphological Analysis of Podocytes" (AMAP) method is hindered by high computational demands, a lack of a user interface, and Linux dependency. We developed AMAP-APP, a cross-platform desktop application designed to overcome these barriers.
  Methods: AMAP-APP optimizes efficiency by replacing intensive instance segmentation with classic image processing while retaining the original semantic segmentation model. It introduces a refined Region of Interest (ROI) algorithm to improve precision. Validation involved 365 mouse and human images (STED and confocal), benchmarking performance against the original AMAP via Pearson correlation and Two One-Sided T-tests (TOST).
  Results: AMAP-APP achieved a 147-fold increase in processing speed on consumer hardware. Morphometric outputs (area, perimeter, circularity, and slit diaphragm density) showed high correlation (r>0.90) and statistical equivalence (TOST P<0.05) to the original method. Additionally, the new ROI algorithm demonstrated superior accuracy compared to the original, showing reduced deviation from manual delineations.
  Conclusion: AMAP-APP democratizes deep learning-based podocyte morphometry. By eliminating the need for high-performance computing clusters and providing a user-friendly interface for Windows, macOS, and Linux, it enables widespread adoption in nephrology research and potential clinical diagnostics.

</details>


### [48] [TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning](https://arxiv.org/abs/2602.10675)
*Junhua Liu,Zhangcheng Wang,Zhike Han,Ningli Wang,Guotao Liang,Kun Kuang*

Main category: cs.CV

TL;DR: 本文提出TwiFF-2.7M数据集和TwiFF-Bench评测基准，用于动态视觉推理任务，并设计TwiFF模型，融合视频生成与图像理解能力，显著提升动态场景下的视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉思维链（VCoT）方法局限于静态场景，难以建模时间动态性，无法有效支持指令执行、预测和相机运动等动态任务。

Method: 构建首个大规模时序对齐的VCoT数据集TwiFF-2.7M（270万视频片段）及高质量评测基准TwiFF-Bench（1078样本）；提出TwiFF模型，统一整合预训练视频生成与图像理解能力，通过迭代生成未来动作帧与文本推理实现时序一致的视觉推理。

Result: TwiFF在动态推理任务上显著优于现有VCoT方法和文本思维链基线，验证了其在动态视觉问答中的有效性。

Conclusion: 引入时序建模是提升VCoT在动态场景中推理能力的关键，TwiFF框架为动态多模态推理提供了新范式与实用资源。

Abstract: Visual Chain-of-Thought (VCoT) has emerged as a promising paradigm for enhancing multimodal reasoning by integrating visual perception into intermediate reasoning steps. However, existing VCoT approaches are largely confined to static scenarios and struggle to capture the temporal dynamics essential for tasks such as instruction, prediction, and camera motion. To bridge this gap, we propose TwiFF-2.7M, the first large-scale, temporally grounded VCoT dataset derived from $2.7$ million video clips, explicitly designed for dynamic visual question and answer. Accompanying this, we introduce TwiFF-Bench, a high-quality evaluation benchmark of $1,078$ samples that assesses both the plausibility of reasoning trajectories and the correctness of final answers in open-ended dynamic settings. Building on these foundations, we propose the TwiFF model, a unified modal that synergistically leverages pre-trained video generation and image comprehension capabilities to produce temporally coherent visual reasoning cues-iteratively generating future action frames and textual reasoning. Extensive experiments demonstrate that TwiFF significantly outperforms existing VCoT methods and Textual Chain-of-Thought baselines on dynamic reasoning tasks, which fully validates the effectiveness for visual question answering in dynamic scenarios. Our code and data is available at https://github.com/LiuJunhua02/TwiFF.

</details>


### [49] [OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL](https://arxiv.org/abs/2602.10687)
*Jinjie Shen,Jing Wu,Yaxiong Wang,Lechao Cheng,Shengeng Tang,Tianrui Hui,Nan Pu,Zhun Zhong*

Main category: cs.CV

TL;DR: 本文提出OmniVL-Guard框架，用于多模态（文本、图像、视频）联合的伪造检测与定位，通过自演化思维链生成和自适应奖励缩放策略优化，解决多任务训练中检测任务主导导致定位性能下降的‘难度偏差’问题。


<details>
  <summary>Details</summary>
Motivation: 现有伪造检测方法多局限于单模态或双模态，难以应对真实世界中图文视频交织的 misinformation；且多任务联合优化中，简单分类任务易主导梯度，损害细粒度定位性能。

Method: 提出OmniVL-Guard：包含Self-Evolving CoT Generation（缓解冷启动、生成高质量推理路径）和Adaptive Reward Scaling Policy Optimization（ARSPO，动态调节奖励尺度与任务权重，实现平衡优化）。

Result: 在多个基准上显著超越SOTA方法，并展现出优异的零样本跨域泛化能力。

Conclusion: OmniVL-Guard有效解决了多模态伪造检测与定位中的难度偏差问题，为统一、鲁棒、可泛化的多模态内容真实性验证提供了新范式。

Abstract: Existing forgery detection methods are often limited to uni-modal or bi-modal settings, failing to handle the interleaved text, images, and videos prevalent in real-world misinformation. To bridge this gap, this paper targets to develop a unified framework for omnibus vision-language forgery detection and grounding. In this unified setting, the {interplay} between diverse modalities and the dual requirements of simultaneous detection and localization pose a critical ``difficulty bias`` problem: the simpler veracity classification task tends to dominate the gradients, leading to suboptimal performance in fine-grained grounding during multi-task optimization. To address this challenge, we propose \textbf{OmniVL-Guard}, a balanced reinforcement learning framework for omnibus vision-language forgery detection and grounding. Particularly, OmniVL-Guard comprises two core designs: Self-Evolving CoT Generatio and Adaptive Reward Scaling Policy Optimization (ARSPO). {Self-Evolving CoT Generation} synthesizes high-quality reasoning paths, effectively overcoming the cold-start challenge. Building upon this, {Adaptive Reward Scaling Policy Optimization (ARSPO)} dynamically modulates reward scales and task weights, ensuring a balanced joint optimization. Extensive experiments demonstrate that OmniVL-Guard significantly outperforms state-of-the-art methods and exhibits zero-shot robust generalization across out-of-domain scenarios.

</details>


### [50] [AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models](https://arxiv.org/abs/2602.10698)
*Zhifeng Rao,Wenlong Chen,Lei Xie,Xia Hua,Dongfu Yin,Zhen Tian,F. Richard Yu*

Main category: cs.CV

TL;DR: 本文提出了一种将深度估计融入视觉-语言-动作（VLA）模型的新框架，通过引入几何感知的3D特征和动作先验约束的‘动作助手’模块，提升VLA模型在复杂3D环境中的空间理解与动作泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要依赖2D图像训练的视觉语言模型，缺乏对3D空间结构的理解和动作在三维场景中的准确接地，限制了其在真实机器人任务中的性能。

Method: 采用VGGT深度估计模型从RGB图像中提取几何感知的3D线索，并设计‘动作助手’模块，利用动作先验对深度衍生特征进行约束和校准；最后将增强的3D特征与传统2D视觉token融合。

Result: 实验表明该方法在几何模糊场景下显著提升感知能力，并提高了动作预测精度，增强了VLA模型的泛化性与鲁棒性。

Conclusion: 深度驱动的数据增强与辅助专家监督可有效弥合2D观测与3D感知决策之间的鸿沟，为构建更可靠的具身智能系统提供了新思路。

Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic perception and control, yet most existing approaches primarily rely on VLM trained using 2D images, which limits their spatial understanding and action grounding in complex 3D environments. To address this limitation, we propose a novel framework that integrates depth estimation into VLA models to enrich 3D feature representations. Specifically, we employ a depth estimation baseline called VGGT to extract geometry-aware 3D cues from standard RGB inputs, enabling efficient utilization of existing large-scale 2D datasets while implicitly recovering 3D structural information. To further enhance the reliability of these depth-derived features, we introduce a new module called action assistant, which constrains the learned 3D representations with action priors and ensures their consistency with downstream control tasks. By fusing the enhanced 3D features with conventional 2D visual tokens, our approach significantly improves the generalization ability and robustness of VLA models. Experimental results demonstrate that the proposed method not only strengthens perception in geometrically ambiguous scenarios but also leads to superior action prediction accuracy. This work highlights the potential of depth-driven data augmentation and auxiliary expert supervision for bridging the gap between 2D observations and 3D-aware decision-making in robotic systems.

</details>


### [51] [FGAA-FPN: Foreground-Guided Angle-Aware Feature Pyramid Network for Oriented Object Detection](https://arxiv.org/abs/2602.10710)
*Jialin Ma*

Main category: cs.CV

TL;DR: 本文提出FGAA-FPN，一种面向定向目标检测的前景引导、角度感知特征金字塔网络，通过前景引导调制和角度感知多头注意力机制提升多尺度特征判别力，在DOTA数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂背景、尺度变化大、方向变化剧烈等挑战下，缺乏显式的前景建模和几何方向先验利用，限制了特征判别能力。

Method: 提出FGAA-FPN：1）前景引导特征调制模块，在弱监督下学习前景显著性以增强低层特征中的目标区域并抑制背景；2）角度感知多头注意力模块，编码相对方向关系以指导高层语义特征的全局交互；整体基于层级功能分解设计。

Result: 在DOTA v1.0和DOTA v1.5数据集上分别达到75.5%和68.3% mAP，性能为当前最优。

Conclusion: FGAA-FPN通过融合前景引导与角度感知机制，有效提升了定向目标检测中多尺度特征的判别性与鲁棒性，验证了显式建模前景与方向先验的重要性。

Abstract: With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.

</details>


### [52] [Ecological mapping with geospatial foundation models](https://arxiv.org/abs/2602.10720)
*Craig Mahlasi,Gciniwe S. Baloyi,Zaheed Gaffoor,Levente Klein,Anne Jones,Etienne Vos,Michal Muszynski,Geoffrey Dawson,Campbell Watson*

Main category: cs.CV

TL;DR: 本文探讨了地理空间基础模型（GFMs）在生态应用中的效用、挑战与机遇，通过微调Prithvi-E0-2.0和TerraMind等预训练模型，并与ResNet-101基线模型对比，验证其在土地利用/覆盖分类（LULC）、森林功能特征制图和泥炭地检测任务中的优越性能，指出多模态输入可显著提升性能，但也强调需更高分辨率数据和更精确标注。


<details>
  <summary>Details</summary>
Motivation: 地理空间基础模型（GFMs）在生态映射等高价值应用场景中的潜力尚未被充分挖掘，亟需系统评估其实际效用、面临挑战及发展机遇。

Method: 对Prithvi-E0-2.0和TerraMind等预训练地理空间基础模型进行微调，应用于三类生态任务（LULC生成、森林功能特征制图、泥炭地检测），并与ResNet-101基线模型对比；分析多模态输入影响及数据偏差问题。

Result: 所有实验中GFMs均优于ResNet-101；TerraMind略优于Prithvi，引入额外模态后性能优势更显著；但输入数据若偏离预训练模态会带来挑战；模型性能受限于图像分辨率和标签精度。

Conclusion: GFMs在生态遥感任务中展现出强大潜力，尤其在多模态融合下效果突出，但需改进数据质量（更高分辨率、更准标注）并谨慎处理模态偏移问题，以支撑像素级动态建模等精细应用。

Abstract: Geospatial foundation models (GFMs) are a fast-emerging paradigm for various geospatial tasks, such as ecological mapping. However, the utility of GFMs has not been fully explored for high-value use cases. This study aims to explore the utility, challenges and opportunities associated with the application of GFMs for ecological uses. In this regard, we fine-tune several pretrained AI models, namely, Prithvi-E0-2.0 and TerraMind, across three use cases, and compare this with a baseline ResNet-101 model. Firstly, we demonstrate TerraMind's LULC generation capabilities. Lastly, we explore the utility of the GFMs in forest functional trait mapping and peatlands detection. In all experiments, the GFMs outperform the baseline ResNet models. In general TerraMind marginally outperforms Prithvi. However, with additional modalities TerraMind significantly outperforms the baseline ResNet and Prithvi models. Nonetheless, consideration should be given to the divergence of input data from pretrained modalities. We note that these models would benefit from higher resolution and more accurate labels, especially for use cases where pixel-level dynamics need to be mapped.

</details>


### [53] [A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography](https://arxiv.org/abs/2602.10722)
*Davide Evangelista,Pasquale Cascarano,Elena Loli Piccolomini*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的深度生成先验（DGP）框架，用于稀疏或有限角度CT重建，结合模型驱动方法的可解释性与生成模型的强大表达能力。


<details>
  <summary>Details</summary>
Motivation: 稀疏或有限角度CT重建因数据缺失易产生伪影和物体失真，亟需引入深度生成模型提升重建质量。

Method: 在DGP框架下，将基于扩散的生成模型与迭代优化算法结合，用于从稀疏几何采集的sinogram中重建CT图像，并对图像生成、模型结构及优化算法进行了改进。

Result: 在高度稀疏几何条件下取得了非常有前景的重建结果。

Conclusion: 该方法在保持可解释性的同时提升了生成能力，但该方向仍需进一步研究。

Abstract: The reconstruction of X-rays CT images from sparse or limited-angle geometries is a highly challenging task. The lack of data typically results in artifacts in the reconstructed image and may even lead to object distortions. For this reason, the use of deep generative models in this context has great interest and potential success. In the Deep Generative Prior (DGP) framework, the use of diffusion-based generative models is combined with an iterative optimization algorithm for the reconstruction of CT images from sinograms acquired under sparse geometries, to maintain the explainability of a model-based approach while introducing the generative power of a neural network. There are therefore several aspects that can be further investigated within these frameworks to improve reconstruction quality, such as image generation, the model, and the iterative algorithm used to solve the minimization problem, for which we propose modifications with respect to existing approaches. The results obtained even under highly sparse geometries are very promising, although further research is clearly needed in this direction.

</details>


### [54] [OccFace: Unified Occlusion-Aware Facial Landmark Detection with Per-Point Visibility](https://arxiv.org/abs/2602.10728)
*Xinhao Xiang,Zhengxin Li,Saurav Dhakad,Theo Bancroft,Jiawei Zhang,Weiyang Li*

Main category: cs.CV

TL;DR: 本文提出OccFace，一种面向通用类人面部（包括人类、风格化角色及其他非人类设计）的遮挡感知关键点检测框架，通过统一的100点密集布局与热图骨干网络，联合预测关键点坐标及逐点可见性，并引入遮挡模块融合局部证据与跨关键点上下文；采用人工标签与基于掩膜-热图重叠生成的伪可见性标签混合监督可见性；构建了首个含100点关键点及逐点可见性标注的数据集和评估套件（含可见/遮挡区域NME、Occ AP、F1@0.5、ROC-AUC），实验表明其在外部遮挡与大角度头部旋转下鲁棒性显著提升，尤其改善遮挡区域性能，同时不牺牲可见区域精度。


<details>
  <summary>Details</summary>
Motivation: 现有关键点检测器通常隐式处理遮挡，未显式预测逐点可见性，而下游任务（如姿态估计、动画驱动）亟需该信息；且对类人面部（含人类、风格化角色等）在大外观变化和旋转导致的自遮挡下表现不佳。

Method: 提出OccFace框架：采用统一100点密集布局和热图骨干网络；新增遮挡模块，联合预测关键点坐标与逐点可见性，融合局部证据与跨关键点上下文；可见性监督结合人工标注与基于掩膜-热图重叠生成的伪标签。

Result: 在外部遮挡和大角度头部旋转场景下鲁棒性提升，尤其在遮挡区域效果显著；可见区域关键点检测精度保持不变；构建了首个含100点关键点及逐点可见性标注的数据集和配套评估套件（含NME分项、Occ AP、F1@0.5、ROC-AUC）。

Conclusion: OccFace通过显式建模逐点可见性并融合多源上下文，在通用类人面部关键点检测中实现了对遮挡更强的鲁棒性，兼顾精度与实用性，为下游任务提供了更可靠的关键点与可见性联合输出。

Abstract: Accurate facial landmark detection under occlusion remains challenging, especially for human-like faces with large appearance variation and rotation-driven self-occlusion. Existing detectors typically localize landmarks while handling occlusion implicitly, without predicting per-point visibility that downstream applications can benefits. We present OccFace, an occlusion-aware framework for universal human-like faces, including humans, stylized characters, and other non-human designs. OccFace adopts a unified dense 100-point layout and a heatmap-based backbone, and adds an occlusion module that jointly predicts landmark coordinates and per-point visibility by combining local evidence with cross-landmark context. Visibility supervision mixes manual labels with landmark-aware masking that derives pseudo visibility from mask-heatmap overlap. We also create an occlusion-aware evaluation suite reporting NME on visible vs. occluded landmarks and benchmarking visibility with Occ AP, F1@0.5, and ROC-AUC, together with a dataset annotated with 100-point landmarks and per-point visibility. Experiments show improved robustness under external occlusion and large head rotations, especially on occluded regions, while preserving accuracy on visible landmarks.

</details>


### [55] [Self-Supervised Image Super-Resolution Quality Assessment based on Content-Free Multi-Model Oriented Representation Learning](https://arxiv.org/abs/2602.10744)
*Kian Majlessi,Amir Masoud Soltani,Mohammad Ebrahim Mahdavi,Aurelien Gourrier,Peyman Adibi*

Main category: cs.CV

TL;DR: 本文提出了一种面向真实世界超分辨率图像的无参考质量评估方法S3 RIQA，通过自监督对比学习建模不同超分算法引入的退化特性，并构建新数据集SRMORSS支持训练，在多个真实SR-IQA基准上超越现有主流指标。


<details>
  <summary>Details</summary>
Motivation: 真实场景中低分辨率图像经超分辨率重建后产生的退化复杂、不可预测且因算法而异，导致现有图像质量评估方法难以适用，尤其在数据稀缺领域缺乏有效无参考评估手段。

Method: 提出基于自监督对比学习的S3 RIQA方法：预训练阶段构建同算法生成图像为正样本、不同算法生成图像为负样本的对比任务；引入针对性预处理和辅助任务以适配不同缩放因子对应的退化模式；并构建新数据集SRMORSS用于无监督预训练。

Result: 在多个真实超分图像质量评估基准上，S3 RIQA一致优于大多数现有最先进无参考指标。

Conclusion: 退化特性主要由超分算法决定而非图像内容本身，因此基于算法感知的自监督表征学习可有效提升真实场景下超分图像的质量评估性能。

Abstract: Super-resolution (SR) applied to real-world low-resolution (LR) images often results in complex, irregular degradations that stem from the inherent complexity of natural scene acquisition. In contrast to SR artifacts arising from synthetic LR images created under well-defined scenarios, those distortions are highly unpredictable and vary significantly across different real-life contexts. Consequently, assessing the quality of SR images (SR-IQA) obtained from realistic LR, remains a challenging and underexplored problem. In this work, we introduce a no-reference SR-IQA approach tailored for such highly ill-posed realistic settings. The proposed method enables domain-adaptive IQA for real-world SR applications, particularly in data-scarce domains. We hypothesize that degradations in super-resolved images are strongly dependent on the underlying SR algorithms, rather than being solely determined by image content. To this end, we introduce a self-supervised learning (SSL) strategy that first pretrains multiple SR model oriented representations in a pretext stage. Our contrastive learning framework forms positive pairs from images produced by the same SR model and negative pairs from those generated by different methods, independent of image content. The proposed approach S3 RIQA, further incorporates targeted preprocessing to extract complementary quality information and an auxiliary task to better handle the various degradation profiles associated with different SR scaling factors. To this end, we constructed a new dataset, SRMORSS, to support unsupervised pretext training; it includes a wide range of SR algorithms applied to numerous real LR images, which addresses a gap in existing datasets. Experiments on real SR-IQA benchmarks demonstrate that S3 RIQA consistently outperforms most state-of-the-art relevant metrics.

</details>


### [56] [Spectral-Spatial Contrastive Learning Framework for Regression on Hyperspectral Data](https://arxiv.org/abs/2602.10745)
*Mohamad Dhaini,Paul Honeine,Maxime Berar,Antonin Van Exem*

Main category: cs.CV

TL;DR: 本文提出了一种面向高光谱数据回归任务的光谱-空间对比学习框架，具有模型无关性，并设计了适用于高光谱数据的增强变换，显著提升了多种骨干网络的性能。


<details>
  <summary>Details</summary>
Motivation: 对比学习在图像分类等表示学习任务中取得了成功，但在回归任务尤其是高光谱数据上的研究仍显不足。

Method: 提出一种光谱-空间对比学习框架，支持3D卷积和Transformer等骨干网络；设计了适用于高光谱数据的专用数据增强变换。

Result: 在合成与真实高光谱数据集上的实验表明，该框架及所提变换能显著提升各类骨干模型的回归性能。

Conclusion: 所提出的模型无关对比学习框架及高光谱专用增强策略，有效拓展了对比学习在回归任务中的应用，并为高光谱数据分析提供了新思路。

Abstract: Contrastive learning has demonstrated great success in representation learning, especially for image classification tasks. However, there is still a shortage in studies targeting regression tasks, and more specifically applications on hyperspectral data. In this paper, we propose a spectral-spatial contrastive learning framework for regression tasks for hyperspectral data, in a model-agnostic design allowing to enhance backbones such as 3D convolutional and transformer-based networks. Moreover, we provide a collection of transformations relevant for augmenting hyperspectral data. Experiments on synthetic and real datasets show that the proposed framework and transformations significantly improve the performance of all studied backbone models.

</details>


### [57] [Text-to-Vector Conversion for Residential Plan Design](https://arxiv.org/abs/2602.10757)
*Egor Bazhenov,Stepan Kasai,Viacheslav Shalamov,Valeria Efimova*

Main category: cs.CV

TL;DR: 本文提出了一种从文本描述生成矢量住宅平面图的新方法，并设计了一种将光栅平面图矢量化为结构化矢量图像的新算法，两者均在CLIPScore指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 矢量图形在设计与建筑领域具有关键优势（如无损缩放），但生成难度大；而现有文本到矢量或光栅到矢量方法在质量与结构准确性（如直角处理）方面存在不足。

Method: 提出一种基于文本生成矢量住宅平面图的新方法，能自然处理直角并支持灵活设置；同时设计一种新算法，将光栅平面图转化为结构化矢量图像。

Result: 文本生成矢量图方法在CLIPScore上比现有方案提升约5%；光栅矢量化算法生成的图像CLIPScore提升约4%。

Conclusion: 所提两种方法分别提升了文本到矢量和光栅到矢量转换的质量与实用性，尤其在建筑平面图生成任务中展现出结构合理性与视觉质量优势。

Abstract: Computer graphics, comprising both raster and vector components, is a fundamental part of modern science, industry, and digital communication. While raster graphics offer ease of use, its pixel-based structure limits scalability. Vector graphics, defined by mathematical primitives, provides scalability without quality loss, however, it is more complex to produce. For design and architecture, the versatility of vector graphics is paramount, despite its computational demands. This paper introduces a novel method for generating vector residential plans from textual descriptions. Our approach surpasses existing solutions by approximately 5% in CLIPScore-based visual quality, benefiting from its inherent handling of right angles and flexible settings. Additionally, we present a new algorithm for vectorizing raster plans into structured vector images. Such images have a better CLIPscore compared to others by about 4%.

</details>


### [58] [Dual-End Consistency Model](https://arxiv.org/abs/2602.10764)
*Linwei Dong,Ruoyu Guo,Ge Bai,Zehuan Yuan,Yawei Luo,Changqing Zou*

Main category: cs.CV

TL;DR: 本文提出了一种双端一致性模型（DE-CM），通过轨迹聚类选择关键子轨迹、结合连续时间一致性目标与流匹配边界正则化，并引入噪声到噪声（N2N）映射，显著提升一致性模型训练稳定性与采样灵活性，在ImageNet 256×256上单步生成FID达1.70，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性模型（CMs）在大规模应用中受限于训练不稳定和采样不灵活两大问题，而当前方法忽视了轨迹选择这一关键因素。

Method: 提出双端一致性模型（DE-CM）：1）分解PF-ODE轨迹并选取三个关键子轨迹作为优化目标；2）采用连续时间CM目标实现少步蒸馏；3）引入流匹配作为边界正则化以稳定训练；4）设计新型噪声到噪声（N2N）映射，缓解首步误差累积。

Result: 在ImageNet 256×256数据集上单步生成取得1.70的FID分数，优于现有基于CM的单步方法，达到当前最优性能。

Conclusion: 轨迹选择对一致性模型至关重要；DE-CM通过子轨迹聚类、边界正则化与N2N映射，有效解决了训练不稳定与采样不灵活问题，显著提升了少步/单步生成质量与鲁棒性。

Abstract: The slow iterative sampling nature remains a major bottleneck for the practical deployment of diffusion and flow-based generative models. While consistency models (CMs) represent a state-of-the-art distillation-based approach for efficient generation, their large-scale application is still limited by two key issues: training instability and inflexible sampling. Existing methods seek to mitigate these problems through architectural adjustments or regularized objectives, yet overlook the critical reliance on trajectory selection. In this work, we first conduct an analysis on these two limitations: training instability originates from loss divergence induced by unstable self-supervised term, whereas sampling inflexibility arises from error accumulation. Based on these insights and analysis, we propose the Dual-End Consistency Model (DE-CM) that selects vital sub-trajectory clusters to achieve stable and effective training. DE-CM decomposes the PF-ODE trajectory and selects three critical sub-trajectories as optimization targets. Specifically, our approach leverages continuous-time CMs objectives to achieve few-step distillation and utilizes flow matching as a boundary regularizer to stabilize the training process. Furthermore, we propose a novel noise-to-noisy (N2N) mapping that can map noise to any point, thereby alleviating the error accumulation in the first step. Extensive experimental results show the effectiveness of our method: it achieves a state-of-the-art FID score of 1.70 in one-step generation on the ImageNet 256x256 dataset, outperforming existing CM-based one-step approaches.

</details>


### [59] [RSHallu: Dual-Mode Hallucination Evaluation for Remote-Sensing Multimodal Large Language Models with Domain-Tailored Mitigation](https://arxiv.org/abs/2602.10799)
*Zihui Zhou,Yong Feng,Yanying Chen,Guofan Duan,Zhenxi Song,Mingliang Zhou,Weijia Jia*

Main category: cs.CV

TL;DR: 本文提出RSHallu，系统研究遥感多模态大语言模型（RS-MLLMs）中的幻觉问题，构建了面向遥感的幻觉分类体系、评测基准RSHalluEval及缓解数据集RSHalluShield，并提出训练无关的即插即用缓解策略，显著提升幻觉抑制效果。


<details>
  <summary>Details</summary>
Motivation: 遥感多模态大语言模型在关键任务（如应急管理和农业监测）中因幻觉问题（响应与输入图像不一致）而难以实际部署，且该问题在遥感领域尚未被深入探索。

Method: 1）构建面向遥感的幻觉分类体系，引入图像级幻觉概念；2）建立包含2023个QA对的评测基准RSHalluEval，并支持云端高精度审计与本地低成本复现的双模式检测；3）构建3万QA对的领域适配数据集RSHalluShield，并提出无需训练的解码时logit校正和遥感感知提示策略。

Result: 所提缓解方法在多个代表性RS-MLLM上，按统一协议将无幻觉率最高提升21.63个百分点，同时在RSVQA和RSVG等下游任务上保持竞争力。

Conclusion: RSHallu为遥感多模态大语言模型的幻觉问题提供了系统性分析框架、评测工具与实用缓解方案，推动其在高风险遥感应用中的可信部署。

Abstract: Multimodal large language models (MLLMs) are increasingly adopted in remote sensing (RS) and have shown strong performance on tasks such as RS visual grounding (RSVG), RS visual question answering (RSVQA), and multimodal dialogue. However, hallucinations, which are responses inconsistent with the input RS images, severely hinder their deployment in high-stakes scenarios (e.g., emergency management and agricultural monitoring) and remain under-explored in RS. In this work, we present RSHallu, a systematic study with three deliverables: (1) we formalize RS hallucinations with an RS-oriented taxonomy and introduce image-level hallucination to capture RS-specific inconsistencies beyond object-centric errors (e.g., modality, resolution, and scene-level semantics); (2) we build a hallucination benchmark RSHalluEval (2,023 QA pairs) and enable dual-mode checking, supporting high-precision cloud auditing and low-cost reproducible local checking via a compact checker fine-tuned on RSHalluCheck dataset (15,396 QA pairs); and (3) we introduce a domain-tailored dataset RSHalluShield (30k QA pairs) for training-friendly mitigation and further propose training-free plug-and-play strategies, including decoding-time logit correction and RS-aware prompting. Across representative RS-MLLMs, our mitigation improves the hallucination-free rate by up to 21.63 percentage points under a unified protocol, while maintaining competitive performance on downstream RS tasks (RSVQA/RSVG). Code and datasets will be released.

</details>


### [60] [DMP-3DAD: Cross-Category 3D Anomaly Detection via Realistic Depth Map Projection with Few Normal Samples](https://arxiv.org/abs/2602.10806)
*Zi Wang,Katsuya Hotta,Koichiro Kamide,Yawen Zou,Jianjian Qin,Chao Zhang,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出DMP-3DAD，一种无需训练的跨类别3D点云异常检测框架，通过多视角真实深度图投影与冻结CLIP视觉编码器提取特征，利用加权特征相似度进行检测，无需微调或类别适配，在ShapeNetPart数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖类别特定训练，难以适应少样本场景，缺乏灵活性。

Method: 将点云转换为固定数量的真实深度图像，利用冻结的CLIP视觉编码器提取多视角表征，并通过加权特征相似度进行异常检测，全程无需微调或类别适配。

Result: 在ShapeNetPart数据集上的大量实验表明，DMP-3DAD在少样本设置下达到最先进性能。

Conclusion: DMP-3DAD提供了一种简单而有效的实用化跨类别3D异常检测方案。

Abstract: Cross-category anomaly detection for 3D point clouds aims to determine whether an unseen object belongs to a target category using only a few normal examples. Most existing methods rely on category-specific training, which limits their flexibility in few-shot scenarios. In this paper, we propose DMP-3DAD, a training-free framework for cross-category 3D anomaly detection based on multi-view realistic depth map projection. Specifically, by converting point clouds into a fixed set of realistic depth images, our method leverages a frozen CLIP visual encoder to extract multi-view representations and performs anomaly detection via weighted feature similarity, which does not require any fine-tuning or category-dependent adaptation. Extensive experiments on the ShapeNetPart dataset demonstrate that DMP-3DAD achieves state-of-the-art performance under few-shot setting. The results show that the proposed approach provides a simple yet effective solution for practical cross-category 3D anomaly detection.

</details>


### [61] [Why Does RL Generalize Better Than SFT? A Data-Centric Perspective on VLM Post-Training](https://arxiv.org/abs/2602.10815)
*Aojun Lu,Tao Feng,Hangjie Yuan,Wei Li,Yanan Sun*

Main category: cs.CV

TL;DR: 本文提出了一种数据驱动的解释：RL在VLM后训练中优于SFT的OOD泛化能力源于其隐式筛选中等难度样本的机制；据此作者设计了显式的Difficulty-Curated SFT（DC-SFT）方法，通过按难度过滤数据提升SFT的OOD性能，效果超越RL且更稳定高效。


<details>
  <summary>Details</summary>
Motivation: 解释为何RL后训练比SFT在VLM中展现出更强的OOD泛化能力，提出‘数据难度’是关键因素，并质疑现有方法对困难样本的过度依赖。

Method: 系统评估不同难度训练集下SFT的OOD泛化性能；基于发现——中等难度样本最有利于OOD泛化——设计DC-SFT：显式按难度筛选训练样本（剔除过难样本），不改变模型结构或优化流程。

Result: DC-SFT显著提升SFT的OOD泛化性能，超越RL基线；同时具备更高训练稳定性与更低计算开销；验证了数据难度是影响VLM后训练泛化的核心变量。

Conclusion: VLM后训练中的OOD泛化差距本质上是数据选择问题而非优化范式问题；DC-SFT提供了一种简单、高效、可解释的替代RL的数据中心路径，推动更鲁棒、实用的多模态模型适配。

Abstract: The adaptation of large-scale Vision-Language Models (VLMs) through post-training reveals a pronounced generalization gap: models fine-tuned with Reinforcement Learning (RL) consistently achieve superior out-of-distribution (OOD) performance compared to those trained with Supervised Fine-Tuning (SFT). This paper posits a data-centric explanation for this phenomenon, contending that RL's generalization advantage arises from an implicit data filtering mechanism that inherently prioritizes medium-difficulty training samples. To test this hypothesis, we systematically evaluate the OOD generalization of SFT models across training datasets of varying difficulty levels. Our results confirm that data difficulty is a critical factor, revealing that training on hard samples significantly degrades OOD performance. Motivated by this finding, we introduce Difficulty-Curated SFT (DC-SFT), a straightforward method that explicitly filters the training set based on sample difficulty. Experiments show that DC-SFT not only substantially enhances OOD generalization over standard SFT, but also surpasses the performance of RL-based training, all while providing greater stability and computational efficiency. This work offers a data-centric account of the OOD generalization gap in VLMs and establishes a more efficient pathway to achieving robust generalization. Code is available at https://github.com/byyx666/DC-SFT.

</details>


### [62] [Resource-Efficient RGB-Only Action Recognition for Edge Deployment](https://arxiv.org/abs/2602.10818)
*Dongsik Yoon,Jongeun Kim,Dayeon Lee*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级纯RGB视频动作识别网络，通过改进X3D主干、引入时序偏移、选择性时序自适应和无参注意力机制，在边缘设备上实现了高精度与高效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的动作识别面临延迟、内存、存储和功耗等严格限制；多模态方法（如骨架、深度）虽提升性能但依赖额外传感器或高开销姿态估计，不适用于边缘场景。

Method: 基于X3D风格主干，融合时序偏移（Temporal Shift）、选择性时序自适应和参数自由注意力机制，构建紧凑的纯RGB网络。

Result: 在NTU RGB+D 60/120数据集上取得优异的精度-效率权衡；Jetson Orin Nano部署实测表明其设备端占用更小、资源利用率更实用。

Conclusion: 所提纯RGB方案在不牺牲精度前提下显著降低边缘部署门槛，为资源受限场景提供了高效可行的动作识别解决方案。

Abstract: Action recognition on edge devices poses stringent constraints on latency, memory, storage, and power consumption. While auxiliary modalities such as skeleton and depth information can enhance recognition performance, they often require additional sensors or computationally expensive pose-estimation pipelines, limiting practicality for edge use. In this work, we propose a compact RGB-only network tailored for efficient on-device inference. Our approach builds upon an X3D-style backbone augmented with Temporal Shift, and further introduces selective temporal adaptation and parameter-free attention. Extensive experiments on the NTU RGB+D 60 and 120 benchmarks demonstrate a strong accuracy-efficiency balance. Moreover, deployment-level profiling on the Jetson Orin Nano verifies a smaller on-device footprint and practical resource utilization compared to existing RGB-based action recognition techniques.

</details>


### [63] [Flow caching for autoregressive video generation](https://arxiv.org/abs/2602.10825)
*Yuexiao Ma,Xuzhe Zheng,Jing Xu,Xiwei Xu,Feng Ling,Xiawu Zheng,Huafeng Kuang,Huixia Li,Xing Wang,Xuefeng Xiao,Fei Chao,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出FlowCache，首个专为自回归视频生成设计的缓存框架，通过分块独立缓存策略与联合重要性-冗余性优化的KV缓存压缩机制，在保持生成质量的同时显著加速超长视频生成。


<details>
  <summary>Details</summary>
Motivation: 自回归视频生成因顺序合成而速度慢；现有缓存方法假设各帧去噪一致，不适用于具有不同相似性模式的视频分块。

Method: 提出分块级独立缓存策略，动态适配每块的去噪特性；设计联合重要性-冗余性优化的KV缓存压缩机制，在固定内存下维持质量。

Result: 在MAGI-1和SkyReels-V2上分别实现2.38倍和6.7倍加速，VBench指标变化极小（+0.87和−0.79）。

Conclusion: FlowCache成功释放自回归模型在实时超长视频生成中的潜力，为大规模高效视频合成树立新基准。

Abstract: Autoregressive models, often built on Transformer architectures, represent a powerful paradigm for generating ultra-long videos by synthesizing content in sequential chunks. However, this sequential generation process is notoriously slow. While caching strategies have proven effective for accelerating traditional video diffusion models, existing methods assume uniform denoising across all frames-an assumption that breaks down in autoregressive models where different video chunks exhibit varying similarity patterns at identical timesteps. In this paper, we present FlowCache, the first caching framework specifically designed for autoregressive video generation. Our key insight is that each video chunk should maintain independent caching policies, allowing fine-grained control over which chunks require recomputation at each timestep. We introduce a chunkwise caching strategy that dynamically adapts to the unique denoising characteristics of each chunk, complemented by a joint importance-redundancy optimized KV cache compression mechanism that maintains fixed memory bounds while preserving generation quality. Our method achieves remarkable speedups of 2.38 times on MAGI-1 and 6.7 times on SkyReels-V2, with negligible quality degradation (VBench: 0.87 increase and 0.79 decrease respectively). These results demonstrate that FlowCache successfully unlocks the potential of autoregressive models for real-time, ultra-long video generation-establishing a new benchmark for efficient video synthesis at scale. The code is available at https://github.com/mikeallen39/FlowCache.

</details>


### [64] [Hyperspectral Smoke Segmentation via Mixture of Prototypes](https://arxiv.org/abs/2602.10858)
*Lujian Yao,Haitao Zhao,Xianghai Kong,Yuhan Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于高光谱成像的烟雾分割方法，构建了首个高光谱烟雾分割数据集HSSDataset，并设计了混合原型（MoP）网络以自适应加权不同光谱波段，显著提升了烟雾分割性能，尤其在云干扰和半透明烟雾区域表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统可见光烟雾分割方法受限于光谱信息不足，难以应对云干扰和半透明烟雾区域；需利用高光谱成像提升判别能力。

Method: 构建首个高光谱烟雾分割数据集HSSDataset及多光谱数据集MSSDataset；提出混合原型（MoP）网络，包含波段分割、原型化光谱表征和双层路由自适应空间感知波段加权机制。

Result: 在高光谱与多光谱模态上均取得优越分割性能，有效缓解光谱交互污染、增强光谱模式建模能力，并解决了复杂波段加权问题。

Conclusion: 本工作确立了基于光谱信息的烟雾分割新范式，为 wildfire 管理与工业安全提供了更鲁棒、精准的视觉感知基础。

Abstract: Smoke segmentation is critical for wildfire management and industrial safety applications. Traditional visible-light-based methods face limitations due to insufficient spectral information, particularly struggling with cloud interference and semi-transparent smoke regions. To address these challenges, we introduce hyperspectral imaging for smoke segmentation and present the first hyperspectral smoke segmentation dataset (HSSDataset) with carefully annotated samples collected from over 18,000 frames across 20 real-world scenarios using a Many-to-One annotations protocol. However, different spectral bands exhibit varying discriminative capabilities across spatial regions, necessitating adaptive band weighting strategies. We decompose this into three technical challenges: spectral interaction contamination, limited spectral pattern modeling, and complex weighting router problems. We propose a mixture of prototypes (MoP) network with: (1) Band split for spectral isolation, (2) Prototype-based spectral representation for diverse patterns, and (3) Dual-level router for adaptive spatial-aware band weighting. We further construct a multispectral dataset (MSSDataset) with RGB-infrared images. Extensive experiments validate superior performance across both hyperspectral and multispectral modalities, establishing a new paradigm for spectral-based smoke segmentation.

</details>


### [65] [Stride-Net: Fairness-Aware Disentangled Representation Learning for Chest X-Ray Diagnosis](https://arxiv.org/abs/2602.10875)
*Darakshan Rashid,Raza Imam,Dwarikanath Mahapatra,Brejesh Lall*

Main category: cs.CV

TL;DR: 本文提出Stride-Net，一种面向胸部X光分类的公平性感知框架，通过解耦表征、可学习掩码与嵌入对齐，在保持甚至提升诊断准确率的同时，显著改善跨种族及种族-性别交叉子群的模型公平性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在胸部X光分类中存在对特定人口统计子群（如不同种族或种族-性别组合）性能下降的问题，威胁临床安全与公平性；现有去偏方法常牺牲整体性能或泛化性差，且将公平性视为事后约束而非表征固有属性。

Method: Stride-Net在图像块级别操作：1）采用可学习步长掩码选择疾病标签对齐区域；2）引入对抗混淆损失抑制敏感属性信息；3）利用Group Optimal Transport实现图像特征与BioBERT疾病标签嵌入之间的语义对齐，防止捷径学习。

Result: 在MIMIC-CXR和CheXpert数据集上，针对种族及种族-性别交叉子群，Stride-Net在ResNet和ViT等多种架构下均一致提升公平性指标（如平等机会差异），同时准确率不降反升，优于现有去偏方法。

Conclusion: Stride-Net将公平性内化为表征学习目标，实现了诊断效用与群体公平性的协同优化，为医学AI的临床部署提供了更可靠、更具包容性的解决方案。

Abstract: Deep neural networks for chest X-ray classification achieve strong average performance, yet often underperform for specific demographic subgroups, raising critical concerns about clinical safety and equity. Existing debiasing methods frequently yield inconsistent improvements across datasets or attain fairness by degrading overall diagnostic utility, treating fairness as a post hoc constraint rather than a property of the learned representation. In this work, we propose Stride-Net (Sensitive Attribute Resilient Learning via Disentanglement and Learnable Masking with Embedding Alignment), a fairness-aware framework that learns disease-discriminative yet demographically invariant representations for chest X-ray analysis. Stride-Net operates at the patch level, using a learnable stride-based mask to select label-aligned image regions while suppressing sensitive attribute information through adversarial confusion loss. To anchor representations in clinical semantics and discourage shortcut learning, we further enforce semantic alignment between image features and BioBERT-based disease label embeddings via Group Optimal Transport. We evaluate Stride-Net on the MIMIC-CXR and CheXpert benchmarks across race and intersectional race-gender subgroups. Across architectures including ResNet and Vision Transformers, Stride-Net consistently improves fairness metrics while matching or exceeding baseline accuracy, achieving a more favorable accuracy-fairness trade-off than prior debiasing approaches. Our code is available at https://github.com/Daraksh/Fairness_StrideNet.

</details>


### [66] [Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation](https://arxiv.org/abs/2602.10880)
*Minggui He,Mingchen Dai,Jian Zhang,Yilun Liu,Shimin Tao,Pufan Zeng,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 本文提出Chart Specification，一种结构化中间表示，通过结构化监督提升视觉语言模型在图表图像到绘图代码生成任务中的结构保真度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的方法倾向于表面级的token模仿，难以保证图表结构的忠实建模，导致幻觉或语义不一致的输出。

Method: 提出Chart Specification作为结构化中间表示，过滤语法噪声构建结构均衡训练集，并设计Spec-Align Reward提供细粒度、可验证的结构正确性反馈，支持强化学习优化。

Result: 在三个公开基准上持续超越先前方法；仅用3K样本即比基线最高提升61.7%，4K样本时在所有指标上达到新SOTA。

Conclusion: 精确的结构化监督是实现高保真图表到代码生成的高效路径。

Abstract: Vision-Language Models (VLMs) have shown promise in generating plotting code from chart images, yet achieving structural fidelity remains challenging. Existing approaches largely rely on supervised fine-tuning, encouraging surface-level token imitation rather than faithful modeling of underlying chart structure, which often leads to hallucinated or semantically inconsistent outputs. We propose Chart Specification, a structured intermediate representation that shifts training from text imitation to semantically grounded supervision. Chart Specification filters syntactic noise to construct a structurally balanced training set and supports a Spec-Align Reward that provides fine-grained, verifiable feedback on structural correctness, enabling reinforcement learning to enforce consistent plotting logic. Experiments on three public benchmarks show that our method consistently outperforms prior approaches. With only 3K training samples, we achieve strong data efficiency, surpassing leading baselines by up to 61.7% on complex benchmarks, and scaling to 4K samples establishes new state-of-the-art results across all evaluated metrics. Overall, our results demonstrate that precise structural supervision offers an efficient pathway to high-fidelity chart-to-code generation. Code and dataset are available at: https://github.com/Mighten/chart-specification-paper

</details>


### [67] [ResWorld: Temporal Residual World Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.10884)
*Jinqing Zhang,Zehua Fu,Zelin Xu,Wenying Dai,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为TR-World的时序残差世界模型，专注于动态物体建模，并结合FGTR模块实现轨迹与未来BEV特征的交互优化，显著提升了端到端自动驾驶规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在驾驶场景中存在对静态区域冗余建模、与轨迹缺乏深度交互的问题，限制了其效能发挥。

Method: 提出Temporal Residual World Model (TR-World)，通过计算场景表征的时序残差提取动态对象信息；并设计Future-Guided Trajectory Refinement (FGTR)模块，实现先验轨迹与未来BEV特征的交互与联合优化。

Result: 在nuScenes和NAVSIM数据集上，ResWorld方法实现了当前最优的规划性能。

Conclusion: 聚焦动态建模与轨迹-未来特征协同优化的世界模型设计，能有效提升端到端自动驾驶系统的规划精度与鲁棒性。

Abstract: The comprehensive understanding capabilities of world models for driving scenarios have significantly improved the planning accuracy of end-to-end autonomous driving frameworks. However, the redundant modeling of static regions and the lack of deep interaction with trajectories hinder world models from exerting their full effectiveness. In this paper, we propose Temporal Residual World Model (TR-World), which focuses on dynamic object modeling. By calculating the temporal residuals of scene representations, the information of dynamic objects can be extracted without relying on detection and tracking. TR-World takes only temporal residuals as input, thus predicting the future spatial distribution of dynamic objects more precisely. By combining the prediction with the static object information contained in the current BEV features, accurate future BEV features can be obtained. Furthermore, we propose Future-Guided Trajectory Refinement (FGTR) module, which conducts interaction between prior trajectories (predicted from the current scene representation) and the future BEV features. This module can not only utilize future road conditions to refine trajectories, but also provides sparse spatial-temporal supervision on future BEV features to prevent world model collapse. Comprehensive experiments conducted on the nuScenes and NAVSIM datasets demonstrate that our method, namely ResWorld, achieves state-of-the-art planning performance. The code is available at https://github.com/mengtan00/ResWorld.git.

</details>


### [68] [FastUSP: A Multi-Level Collaborative Acceleration Framework for Distributed Diffusion Model Inference](https://arxiv.org/abs/2602.10940)
*Guandong Li*

Main category: cs.CV

TL;DR: 本文提出FastUSP，一种多级优化框架，用于提升大规模扩散模型（如FLUX、Qwen-Image）在多GPU上的分布式注意力推理效率，通过编译级、通信级和算子级优化，在FLUX上实现1.12–1.16×端到端加速。


<details>
  <summary>Details</summary>
Motivation: 现有Unified Sequence Parallelism（USP）实现在大规模扩散模型推理中存在内核启动开销大、计算-通信调度不佳等效率瓶颈，难以充分发挥现代高带宽GPU互连的潜力。

Method: 提出FastUSP框架，融合三层次优化：（1）编译级——CUDA Graphs图编译与计算-通信重排序；（2）通信级——FP8量化集体通信；（3）算子级——双缓冲流水线Ring attention。

Result: 在FLUX（12B）上，FastUSP在2/4/8卡RTX 5090上稳定实现1.12×–1.16×端到端加速，其中编译级优化贡献最大；在Qwen-Image上，2卡获1.09×加速，但4–8卡受限于PyTorch Inductor对Ring attention兼容性而无法启用编译优化；分析指出内核启动开销是当前主要瓶颈，而非通信延迟。

Conclusion: FastUSP通过协同多级优化显著提升USP推理效率，揭示了现代GPU集群中kernel launch overhead的关键制约作用，并为后续分布式生成模型系统优化提供了新方向。

Abstract: Large-scale diffusion models such as FLUX (12B parameters) and Stable Diffusion 3 (8B parameters) require multi-GPU parallelism for efficient inference. Unified Sequence Parallelism (USP), which combines Ulysses and Ring attention mechanisms, has emerged as the state-of-the-art approach for distributed attention computation. However, existing USP implementations suffer from significant inefficiencies including excessive kernel launch overhead and suboptimal computation-communication scheduling. In this paper, we propose \textbf{FastUSP}, a multi-level optimization framework that integrates compile-level optimization (graph compilation with CUDA Graphs and computation-communication reordering), communication-level optimization (FP8 quantized collective communication), and operator-level optimization (pipelined Ring attention with double buffering). We evaluate FastUSP on FLUX (12B) and Qwen-Image models across 2, 4, and 8 NVIDIA RTX 5090 GPUs. On FLUX, FastUSP achieves consistent \textbf{1.12$\times$--1.16$\times$} end-to-end speedup over baseline USP, with compile-level optimization contributing the dominant improvement. On Qwen-Image, FastUSP achieves \textbf{1.09$\times$} speedup on 2 GPUs; on 4--8 GPUs, we identify a PyTorch Inductor compatibility limitation with Ring attention that prevents compile optimization, while baseline USP scales to 1.30$\times$--1.46$\times$ of 2-GPU performance. We further provide a detailed analysis of the performance characteristics of distributed diffusion inference, revealing that kernel launch overhead -- rather than communication latency -- is the primary bottleneck on modern high-bandwidth GPU interconnects.

</details>


### [69] [Healthy Harvests: A Comparative Look at Guava Disease Classification Using InceptionV3](https://arxiv.org/abs/2602.10967)
*Samanta Ghosh,Shaila Afroz Anika,Umma Habiba Ahmed,B. M. Shahria Alam,Mohammad Tahmid Noor,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 本研究利用InceptionV3和ResNet50模型对番石榴果实病害（炭疽病、果蝇侵害及健康果实）进行分类，通过数据增强与CutMix/MixUp等技术提升性能，InceptionV3达98.15%准确率，并结合SHAP提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 番石榴果实易受多种病害影响，导致品质下降和产量损失；早期精准识别对减少损害、保障果实健康至关重要。

Method: 采用Mendeley Data提供的473张原始图像，统一缩放为256×256 RGB图像，并通过数据增强扩增至3784张；构建InceptionV3与ResNet50深度学习模型，引入CutMix和MixUp数据混合策略，并使用混淆矩阵与SHAP方法评估性能与可解释性。

Result: InceptionV3模型准确率达98.15%，ResNet50为94.46%；混淆矩阵验证了两类模型的分类效果，SHAP分析揭示了影响预测的关键图像区域。

Conclusion: InceptionV3在番石榴病害图像分类任务中表现最优，结合数据增强与混合策略及SHAP可解释性分析，可为农业智能诊断提供高效、可信的技术支持。

Abstract: Guava fruits often suffer from many diseases. This can harm fruit quality and fruit crop yield. Early identification is important for minimizing damage and ensuring fruit health. This study focuses on 3 different categories for classifying diseases. These are Anthracnose, Fruit flies, and Healthy fruit. The data set used in this study is collected from Mendeley Data. This dataset contains 473 original images of Guava. These images vary in size and format. The original dataset was resized to 256x256 pixels with RGB color mode for better consistency. After this, the Data augmentation process is applied to improve the dataset by generating variations of the original images. The augmented dataset consists of 3784 images using advanced preprocessing techniques. Two deep learning models were implemented to classify the images. The InceptionV3 model is well known for its advanced framework. These apply multiple convolutional filters for obtaining different features effectively. On the other hand, the ResNet50 model helps to train deeper networks by using residual learning. The InceptionV3 model achieved the impressive accuracy of 98.15%, and ResNet50got 94.46% accuracy. Data mixing methods such as CutMix and MixUp were applied to enhance the model's robustness. The confusion matrix was used to evaluate the overall model performance of both InceptionV3 and Resnet50. Additionally, SHAP analysis is used to improve interpretability, which helps to find the significant parts of the image for the model prediction. This study purposes to highlight how advanced models enhan

</details>


### [70] [VFGS-Net: Frequency-Guided State-Space Learning for Topology-Preserving Retinal Vessel Segmentation](https://arxiv.org/abs/2602.10978)
*Ruiqi Song,Lei Liu,Ya-Nan Zhang,Chao Wang,Xiaoning Li,Nan Mu*

Main category: cs.CV

TL;DR: 本文提出VFGS-Net，一种融合频域感知增强、双路径卷积表征学习与双向非对称空间建模的端到端视网膜血管分割网络，显著提升细小血管、低对比区域及复杂分支结构的分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时保留细小毛细血管并维持血管整体拓扑连续性，主要受限于血管细长形态、尺度变化大、对比度低等挑战。

Method: 提出VFGS-Net：包含双路径特征卷积模块（兼顾局部纹理与多尺度语义）、血管感知频域通道注意力机制（自适应增强血管相关频谱响应）、以及基于Mamba2的双向非对称空间建模模块（强化长程依赖与全局连续性）。

Result: 在四个公开视网膜血管数据集上达到或超越当前最优方法，尤其在细血管、复杂分支和低对比区域分割上表现更优。

Conclusion: VFGS-Net通过多维度建模有效缓解了视网膜血管分割中的关键难点，具备良好的鲁棒性与临床应用潜力。

Abstract: Accurate retinal vessel segmentation is a critical prerequisite for quantitative analysis of retinal images and computer-aided diagnosis of vascular diseases such as diabetic retinopathy. However, the elongated morphology, wide scale variation, and low contrast of retinal vessels pose significant challenges for existing methods, making it difficult to simultaneously preserve fine capillaries and maintain global topological continuity. To address these challenges, we propose the Vessel-aware Frequency-domain and Global Spatial modeling Network (VFGS-Net), an end-to-end segmentation framework that seamlessly integrates frequency-aware feature enhancement, dual-path convolutional representation learning, and bidirectional asymmetric spatial state-space modeling within a unified architecture. Specifically, VFGS-Net employs a dual-path feature convolution module to jointly capture fine-grained local textures and multi-scale contextual semantics. A novel vessel-aware frequency-domain channel attention mechanism is introduced to adaptively reweight spectral components, thereby enhancing vessel-relevant responses in high-level features. Furthermore, at the network bottleneck, we propose a bidirectional asymmetric Mamba2-based spatial modeling block to efficiently capture long-range spatial dependencies and strengthen the global continuity of vascular structures. Extensive experiments on four publicly available retinal vessel datasets demonstrate that VFGS-Net achieves competitive or superior performance compared to state-of-the-art methods. Notably, our model consistently improves segmentation accuracy for fine vessels, complex branching patterns, and low-contrast regions, highlighting its robustness and clinical potential.

</details>


### [71] [DFIC: Towards a balanced facial image dataset for automatic ICAO compliance verification](https://arxiv.org/abs/2602.10985)
*Nuno Gonçalves,Diogo Nunes,Carla Guerra,João Marcos*

Main category: cs.CV

TL;DR: 本文提出了DFIC数据集，包含约58,000张图像和2706段视频，覆盖多种人脸合规与非合规情形，并基于该数据集提出一种依赖空间注意力机制的ICAO合规性自动验证新方法，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前人工检验护照等机读旅行证件中人脸图像是否符合ISO/IEC和ICAO标准效率低下，亟需自动化、高鲁棒性的合规性验证方法。

Method: 构建DFIC大规模多模态人脸图像数据集，并基于其微调一种以空间注意力机制为核心的新模型，用于自动验证ICAO合规性要求。

Result: 所提方法在ICAO合规性验证任务上性能优于现有最先进方法；DFIC数据集已开源，具备更均衡的人口统计分布和前所未有的多样性。

Conclusion: DFIC数据集及其配套方法显著提升了自动化ICAO合规验证能力，亦可拓展应用于提升人脸识别系统的安全性、隐私性和公平性。

Abstract: Ensuring compliance with ISO/IEC and ICAO standards for facial images in machine-readable travel documents (MRTDs) is essential for reliable identity verification, but current manual inspection methods are inefficient in high-demand environments. This paper introduces the DFIC dataset, a novel comprehensive facial image dataset comprising around 58,000 annotated images and 2706 videos of more than 1000 subjects, that cover a broad range of non-compliant conditions, in addition to compliant portraits. Our dataset provides a more balanced demographic distribution than the existing public datasets, with one partition that is nearly uniformly distributed, facilitating the development of automated ICAO compliance verification methods.
  Using DFIC, we fine-tuned a novel method that heavily relies on spatial attention mechanisms for the automatic validation of ICAO compliance requirements, and we have compared it with the state-of-the-art aimed at ICAO compliance verification, demonstrating improved results. DFIC dataset is now made public (https://github.com/visteam-isr-uc/DFIC) for the training and validation of new models, offering an unprecedented diversity of faces, that will improve both robustness and adaptability to the intrinsically diverse combinations of faces and props that can be presented to the validation system. These results emphasize the potential of DFIC to enhance automated ICAO compliance methods but it can also be used in many other applications that aim to improve the security, privacy, and fairness of facial recognition systems.

</details>


### [72] [Interpretable Vision Transformers in Image Classification via SVDA](https://arxiv.org/abs/2602.10994)
*Vasileios Arampatzakis,George Pavlidis,Nikolaos Mitianoudis,Nikos Papamarkos*

Main category: cs.CV

TL;DR: 本文提出将SVD-Inspired Attention（SVDA）机制引入Vision Transformer（ViT），以提升注意力机制的可解释性、稀疏性和谱结构，实验表明其在多个基准数据集上保持分类精度的同时显著增强了解释性。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers的注意力机制通常不透明且呈现密集、非结构化行为，亟需提升其可解释性与结构化特性。

Method: 将先前提出的SVD-Inspired Attention（SVDA）机制适配至ViT架构，引入几何驱动的注意力公式，并利用SVDA原有的可解释性指标监控训练过程中的注意力动态及表征结构特性。

Result: 在CIFAR-10、FashionMNIST、CIFAR-100和ImageNet-100四个基准上，SVDA consistently生成更可解释的注意力模式，且未牺牲分类精度。

Conclusion: SVDA是一种全面、信息丰富的工具，可用于分析与开发计算机视觉中结构化的注意力模型，为可解释AI、谱诊断与注意力模型压缩奠定基础。

Abstract: Vision Transformers (ViTs) have achieved state-of-the-art performance in image classification, yet their attention mechanisms often remain opaque and exhibit dense, non-structured behaviors. In this work, we adapt our previously proposed SVD-Inspired Attention (SVDA) mechanism to the ViT architecture, introducing a geometrically grounded formulation that enhances interpretability, sparsity, and spectral structure. We apply the use of interpretability indicators -- originally proposed with SVDA -- to monitor attention dynamics during training and assess structural properties of the learned representations. Experimental evaluations on four widely used benchmarks -- CIFAR-10, FashionMNIST, CIFAR-100, and ImageNet-100 -- demonstrate that SVDA consistently yields more interpretable attention patterns without sacrificing classification accuracy. While the current framework offers descriptive insights rather than prescriptive guidance, our results establish SVDA as a comprehensive and informative tool for analyzing and developing structured attention models in computer vision. This work lays the foundation for future advances in explainable AI, spectral diagnostics, and attention-based model compression.

</details>


### [73] [Interpretable Vision Transformers in Monocular Depth Estimation via SVDA](https://arxiv.org/abs/2602.11005)
*Vasileios Arampatzakis,George Pavlidis,Nikolaos Mitianoudis,Nikos Papamarkos*

Main category: cs.CV

TL;DR: 本文提出了一种受奇异值分解（SVD）启发的注意力机制（SVDA），将其嵌入Dense Prediction Transformer（DPT）中，首次为密集预测任务提供了谱结构化的注意力建模方法，提升了单目深度估计模型的可解释性，并引入六种谱指标揭示注意力在训练中的组织规律。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer中的自注意力机制对单目深度估计等密集预测任务而言过于黑箱，缺乏可解释性；亟需一种既保持性能又具备内在可解释性的注意力机制。

Method: 提出SVD-Inspired Attention（SVDA），在归一化查询-键交互中嵌入可学习对角矩阵，解耦方向对齐与谱调制，使注意力图具有内在可解释性；将其集成至DPT架构中，并在KITTI和NYU-v2数据集上进行实验验证。

Result: SVDA在保持或略微提升预测精度的同时仅引入轻微计算开销，并首次提供六种谱指标（熵、秩、稀疏性、对齐度、选择性、鲁棒性），揭示了跨数据集与深度层一致的注意力组织模式。

Conclusion: SVDA将注意力从黑箱机制转变为可量化描述符，重新定义了单目深度估计中的可解释性，为构建透明密集预测模型提供了新范式。

Abstract: Monocular depth estimation is a central problem in computer vision with applications in robotics, AR, and autonomous driving, yet the self-attention mechanisms that drive modern Transformer architectures remain opaque. We introduce SVD-Inspired Attention (SVDA) into the Dense Prediction Transformer (DPT), providing the first spectrally structured formulation of attention for dense prediction tasks. SVDA decouples directional alignment from spectral modulation by embedding a learnable diagonal matrix into normalized query-key interactions, enabling attention maps that are intrinsically interpretable rather than post-hoc approximations. Experiments on KITTI and NYU-v2 show that SVDA preserves or slightly improves predictive accuracy while adding only minor computational overhead. More importantly, SVDA unlocks six spectral indicators that quantify entropy, rank, sparsity, alignment, selectivity, and robustness. These reveal consistent cross-dataset and depth-wise patterns in how attention organizes during training, insights that remain inaccessible in standard Transformers. By shifting the role of attention from opaque mechanism to quantifiable descriptor, SVDA redefines interpretability in monocular depth estimation and opens a principled avenue toward transparent dense prediction models.

</details>


### [74] [Chatting with Images for Introspective Visual Thinking](https://arxiv.org/abs/2602.11073)
*Junfei Wu,Jian Guan,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tienie Tan*

Main category: cs.CV

TL;DR: 本文提出'与图像聊天'（chatting with images）新框架，通过语言引导的特征调制实现视觉-语言联合推理，并设计ViLaVT模型验证其在多图像和视频空间推理任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型依赖单次视觉编码，易丢失细粒度视觉信息；'用图像思考'方法虽尝试外部工具操作图像，但视觉状态与语言语义对齐不足，难以处理跨区域或跨图像的视觉语义与几何关系推理。

Method: 提出'与图像聊天'框架，将视觉操作重构为语言引导的特征调制；设计具备动态视觉编码器的ViLaVT模型，支持在语言提示下对多个图像区域进行联合重编码；采用监督微调加强化学习的两阶段课程训练策略。

Result: 在八个基准上实验表明，ViLaVT实现显著且一致的性能提升，尤其在复杂多图像和视频空间推理任务中增益突出。

Conclusion: 语言引导的动态视觉特征调制能更紧密耦合语言推理与视觉状态更新，有效提升跨模态对齐与复杂视觉推理能力。

Abstract: Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of ''thinking with images'' attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in linguistic semantics, impairing effective cross-modal alignment - particularly when visual semantics or geometric relationships must be reasoned over across distant regions or multiple images. To address these challenges, we propose ''chatting with images'', a new framework that reframes visual manipulation as language-guided feature modulation. Under the guidance of expressive language prompts, the model dynamically performs joint re-encoding over multiple image regions, enabling tighter coupling between linguistic reasoning and visual state updates. We instantiate this paradigm in ViLaVT, a novel LVLM equipped with a dynamic vision encoder explicitly designed for such interactive visual reasoning, and trained it with a two-stage curriculum combining supervised fine-tuning and reinforcement learning to promote effective reasoning behaviors. Extensive experiments across eight benchmarks demonstrate that ViLaVT achieves strong and consistent improvements, with particularly pronounced gains on complex multi-image and video-based spatial reasoning tasks.

</details>


### [75] [LaSSM: Efficient Semantic-Spatial Query Decoding via Local Aggregation and State Space Models for 3D Instance Segmentation](https://arxiv.org/abs/2602.11007)
*Lei Yao,Yi Wang,Yawen Cui,Moyun Liu,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 本文提出LaSSM方法，通过分层语义-空间查询初始化器和坐标引导的状态空间模型（SSM）解码器，提升点云场景实例分割的效率与性能，在ScanNet++ V2等基准上达到SOTA，同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的3D场景实例分割方法面临点云稀疏导致的查询初始化困难，以及查询解码器中注意力机制计算开销大的问题。

Method: 提出LaSSM：1）分层语义-空间查询初始化器，从超点出发融合语义与空间分布生成查询集；2）坐标引导的状态空间模型解码器，含局部聚合机制和空间双路径SSM模块，利用坐标信息逐步优化查询。

Result: 在ScanNet++ V2榜单排名第一，mAP超越此前最优方法2.5%，仅需1/3 FLOPs；在ScanNet、ScanNet200、S3DIS和ScanNet++ V1上也取得具竞争力的结果且计算成本更低。

Conclusion: LaSSM以简洁高效的设计实现了高性能3D实例分割，验证了状态空间模型在该任务中的有效性与潜力，为大规模场景分割提供了新思路。

Abstract: Query-based 3D scene instance segmentation from point clouds has attained notable performance. However, existing methods suffer from the query initialization dilemma due to the sparse nature of point clouds and rely on computationally intensive attention mechanisms in query decoders. We accordingly introduce LaSSM, prioritizing simplicity and efficiency while maintaining competitive performance. Specifically, we propose a hierarchical semantic-spatial query initializer to derive the query set from superpoints by considering both semantic cues and spatial distribution, achieving comprehensive scene coverage and accelerated convergence. We further present a coordinate-guided state space model (SSM) decoder that progressively refines queries. The novel decoder features a local aggregation scheme that restricts the model to focus on geometrically coherent regions and a spatial dual-path SSM block to capture underlying dependencies within the query set by integrating associated coordinates information. Our design enables efficient instance prediction, avoiding the incorporation of noisy information and reducing redundant computation. LaSSM ranks first place on the latest ScanNet++ V2 leaderboard, outperforming the previous best method by 2.5% mAP with only 1/3 FLOPs, demonstrating its superiority in challenging large-scale scene instance segmentation. LaSSM also achieves competitive performance on ScanNet, ScanNet200, S3DIS and ScanNet++ V1 benchmarks with less computational cost. Extensive ablation studies and qualitative results validate the effectiveness of our design. The code and weights are available at https://github.com/RayYoh/LaSSM.

</details>


### [76] [Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting](https://arxiv.org/abs/2602.11024)
*Rishikesh Bhyri,Brian R Quaranto,Philip J Seger,Kaity Tung,Brendan Fox,Gene Yang,Steven D. Schwaitzberg,Junsong Yuan,Nan Xi,Peter C W Kim*

Main category: cs.CV

TL;DR: 本文提出Chain-of-Look框架，通过模拟人类顺序计数过程并引入邻域损失函数，在高密度手术器械图像中实现更准确的计数；同时构建了SurgCount-HD新数据集，并在实验中超越现有计数方法和多模态大模型。


<details>
  <summary>Details</summary>
Motivation: 手术室中手术器械的准确计数对患者安全至关重要，但在器械密集堆叠场景下，现有基于无序目标检测的方法难以准确计数。

Method: 提出Chain-of-Look视觉推理框架，构建有序空间视觉链以模拟人类逐次观察计数过程，并设计邻域损失函数显式建模密集器械间的空间约束关系；同时构建SurgCount-HD高密度手术器械图像数据集。

Result: 在高密度手术器械计数任务上，该方法显著优于当前最优计数方法（如CountGD、REC）及多模态大语言模型（如Qwen、ChatGPT）。

Conclusion: 结构化视觉链与物理约束建模可有效提升密集场景下的计数精度，为手术安全提供更可靠的AI支持。

Abstract: Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, a novel visual reasoning framework that mimics the sequential human counting process by enforcing a structured visual chain, rather than relying on classic object detection which is unordered. This visual chain guides the model to count along a coherent spatial trajectory, improving accuracy in complex scenes. To further enforce the physical plausibility of the visual chain, we introduce the neighboring loss function, which explicitly models the spatial constraints inherent to densely packed surgical instruments. We also present SurgCount-HD, a new dataset comprising 1,464 high-density surgical instrument images. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches for counting (e.g., CountGD, REC) as well as Multimodality Large Language Models (e.g., Qwen, ChatGPT) in the challenging task of dense surgical instrument counting.

</details>


### [77] [PuriLight: A Lightweight Shuffle and Purification Framework for Monocular Depth Estimation](https://arxiv.org/abs/2602.11066)
*Yujie Chen,Li Zhang,Xiaomeng Chu,Tian Zhang*

Main category: cs.CV

TL;DR: PuriLight是一种轻量高效的自监督单目深度估计框架，通过三个创新模块（SDC、RAKA、DFSP）在保持模型轻量化的同时提升结构精度和细节保留能力，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有自监督单目深度估计方法中轻量模型牺牲结构精度、大模型又缺乏实用性的双重局限，亟需兼具轻量化与高精度的架构。

Method: 提出三阶段架构，包含Shuffle-Dilation卷积（SDC）用于局部特征提取、Rotation-Adaptive Kernel注意力（RAKA）用于分层特征增强、Deep Frequency Signal Purification（DFSP）用于全局特征净化。

Result: 在参数极少的情况下达到当前最优性能，同时保持卓越的计算效率。

Conclusion: PuriLight成功平衡了轻量化与深度估计精度，在自监督单目深度估计任务中具有显著实用价值和推广潜力。

Abstract: We propose PuriLight, a lightweight and efficient framework for self-supervised monocular depth estimation, to address the dual challenges of computational efficiency and detail preservation. While recent advances in self-supervised depth estimation have reduced reliance on ground truth supervision, existing approaches remain constrained by either bulky architectures compromising practicality or lightweight models sacrificing structural precision. These dual limitations underscore the critical need to develop lightweight yet structurally precise architectures. Our framework addresses these limitations through a three-stage architecture incorporating three novel modules: the Shuffle-Dilation Convolution (SDC) module for local feature extraction, the Rotation-Adaptive Kernel Attention (RAKA) module for hierarchical feature enhancement, and the Deep Frequency Signal Purification (DFSP) module for global feature purification. Through effective collaboration, these modules enable PuriLight to achieve both lightweight and accurate feature extraction and processing. Extensive experiments demonstrate that PuriLight achieves state-of-the-art performance with minimal training parameters while maintaining exceptional computational efficiency. Codes will be available at https://github.com/ishrouder/PuriLight.

</details>


### [78] [First International StepUP Competition for Biometric Footstep Recognition: Methods, Results and Remaining Challenges](https://arxiv.org/abs/2602.11086)
*Robyn Larracy,Eve MacDonald,Angkoon Phinyomark,Saeid Rezaei,Mahdi Laghaei,Ali Hajighasem,Aaron Tabor,Erik Scheme*

Main category: cs.CV

TL;DR: 本文介绍了首届国际StepUP步态识别竞赛，利用UNB StepUP-P150大型压力步态数据集推动生物特征步态识别研究，Saeid_UCC团队以10.77%的等错误率（EER）夺冠，但跨鞋型泛化仍是关键挑战。


<details>
  <summary>Details</summary>
Motivation: 解决步态识别领域因缺乏大规模、多样化数据集而导致的模型泛化能力差、对鞋型和步速变化鲁棒性不足等问题。

Method: 依托UNB StepUP-P150高分辨率足底压力数据集，组织国际竞赛，要求参赛者构建鲁棒识别模型，并在含挑战性变化的独立测试集上评估验证性能；优胜方案采用生成式奖励机（GRM）优化策略。

Result: 23支来自学术界与工业界的队伍参赛，冠军团队Saeid_UCC实现10.77%的最低等错误率（EER）；整体表现良好，但跨鞋型识别仍存在明显性能下降。

Conclusion: StepUP-P150数据集与竞赛显著推动了步态识别发展，验证了深度学习方法的有效性，但模型对未知鞋型的泛化能力不足，是未来研究的关键方向。

Abstract: Biometric footstep recognition, based on a person's unique pressure patterns under their feet during walking, is an emerging field with growing applications in security and safety. However, progress in this area has been limited by the lack of large, diverse datasets necessary to address critical challenges such as generalization to new users and robustness to shifts in factors like footwear or walking speed. The recent release of the UNB StepUP-P150 dataset, the largest and most comprehensive collection of high-resolution footstep pressure recordings to date, opens new opportunities for addressing these challenges through deep learning. To mark this milestone, the First International StepUP Competition for Biometric Footstep Recognition was launched. Competitors were tasked with developing robust recognition models using the StepUP-P150 dataset that were then evaluated on a separate, dedicated test set designed to assess verification performance under challenging variations, given limited and relatively homogeneous reference data. The competition attracted global participation, with 23 registered teams from academia and industry. The top-performing team, Saeid_UCC, achieved the best equal error rate (EER) of 10.77% using a generative reward machine (GRM) optimization strategy. Overall, the competition showcased strong solutions, but persistent challenges in generalizing to unfamiliar footwear highlight a critical area for future work.

</details>


### [79] [FastFlow: Accelerating The Generative Flow Matching Models with Bandit Inference](https://arxiv.org/abs/2602.11105)
*Divya Jyoti Bajpai,Dhruv Bhardwaj,Soumya Roy,Tejas Duseja,Harsh Agarwal,Aashay Sandansing,Manjesh Kumar Hanawal*

Main category: cs.CV

TL;DR: FastFlow是一种即插即用的自适应推理框架，通过跳过对去噪路径影响微小的步骤，并利用有限差分速度估计进行高效外推，在不牺牲生成质量的前提下显著加速流匹配模型。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配模型虽生成质量高，但因顺序去噪过程而速度慢；已有加速方法（如蒸馏、轨迹截断、一致性方法）存在静态性、需重训练、泛化性差等问题。

Method: FastFlow将跳步决策建模为多臂赌博机问题，动态学习最优跳步策略；对可跳过的步骤，用前序预测的有限差分速度估计替代完整神经网络的速度预测，实现零计算成本的路径外推。

Result: 在图像生成、视频生成与编辑任务中均有效，实测加速超2.6倍，同时保持高质量输出。

Conclusion: FastFlow是一种无需重训练、即插即用、跨任务通用的流匹配模型加速框架，兼顾速度与性能。

Abstract: Flow-matching models deliver state-of-the-art fidelity in image and video generation, but the inherent sequential denoising process renders them slower. Existing acceleration methods like distillation, trajectory truncation, and consistency approaches are static, require retraining, and often fail to generalize across tasks. We propose FastFlow, a plug-and-play adaptive inference framework that accelerates generation in flow matching models. FastFlow identifies denoising steps that produce only minor adjustments to the denoising path and approximates them without using the full neural network models used for velocity predictions. The approximation utilizes finite-difference velocity estimates from prior predictions to efficiently extrapolate future states, enabling faster advancements along the denoising path at zero compute cost. This enables skipping computation at intermediary steps. We model the decision of how many steps to safely skip before requiring a full model computation as a multi-armed bandit problem. The bandit learns the optimal skips to balance speed with performance. FastFlow integrates seamlessly with existing pipelines and generalizes across image generation, video generation, and editing tasks. Experiments demonstrate a speedup of over 2.6x while maintaining high-quality outputs. The source code for this work can be found at https://github.com/Div290/FastFlow.

</details>


### [80] [HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion](https://arxiv.org/abs/2602.11117)
*Di Chang,Ji Hou,Aljaz Bozic,Assaf Neuberger,Felix Juefei-Xu,Olivier Maury,Gene Wei-Chin Lin,Tuur Stuyck,Doug Roble,Mohammad Soleymani,Stephane Grabli*

Main category: cs.CV

TL;DR: HairWeaver是一种基于扩散模型的单图人像动画生成方法，专精于生成真实、富有表现力的头发动态效果，通过两个轻量级LoRA模块（Motion-Context-LoRA和Sim2Real-Domain-LoRA）增强运动控制与外观保真度，在CG合成数据上训练，显著提升头发动画的真实感与细节表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽能控制人体姿态，但缺乏对头发运动的精细建模，导致生成动画中头发僵硬、不自然。

Method: 提出HairWeaver扩散模型框架，包含Motion-Context-LoRA（融合运动条件）和Sim2Real-Domain-LoRA（跨域保持主体照片级真实感）两个轻量模块，以引导视频扩散主干网络；训练数据来自CG仿真器生成的动态人体运动数据集。

Result: 在综合评估中达到新SOTA，生成具有动态细节、响应自然的逼真人发动画。

Conclusion: HairWeaver有效解决了单图像驱动下头发运动建模难的问题，通过模块化LoRA设计与领域适配训练，实现了高保真、可控的头发动画生成。

Abstract: We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject's photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.

</details>


### [81] [PhyCritic: Multimodal Critic Models for Physical AI](https://arxiv.org/abs/2602.11124)
*Tianyi Xiong,Shihao Wang,Guilin Liu,Yi Dong,Ming Li,Heng Huang,Jan Kautz,Zhiding Yu*

Main category: cs.CV

TL;DR: 本文提出PhyCritic，一种专为物理AI任务优化的多模态评判模型，通过两阶段RLVR流程（物理技能预热+自参照评判微调）提升对感知、因果推理与规划等物理任务的判断稳定性与正确性，在物理及通用多模态评判基准上均显著优于开源基线。


<details>
  <summary>Details</summary>
Motivation: 现有评判模型主要面向通用视觉任务（如图像描述、视觉问答），缺乏对物理AI任务（涉及感知、因果推理和规划）的支持，亟需专用于物理领域的可靠评判模型。

Method: 提出PhyCritic模型，采用两阶段RLVR训练流程：第一阶段为物理技能预热，增强物理导向的感知与推理能力；第二阶段为自参照评判微调，即评判模型先生成自身预测作为内部参考，再评估候选响应，以提升判断稳定性与物理正确性。

Result: PhyCritic在物理与通用多模态评判基准上均显著超越开源基线；作为策略模型使用时，还能进一步提升物理具身任务中的感知与推理能力。

Conclusion: PhyCritic有效填补了物理AI领域专用评判模型的空白，其自参照机制与物理预热策略为构建更可靠、更具物理一致性的多模态评判系统提供了新范式。

Abstract: With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.

</details>


### [82] [Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling](https://arxiv.org/abs/2602.11146)
*Gongye Liu,Bo Yang,Yida Zhi,Zhizhou Zhong,Lei Ke,Didan Deng,Han Gao,Yongxiang Huang,Kaihao Zhang,Hongbo Fu,Wenhan Luo*

Main category: cs.CV

TL;DR: 本文提出DiNa-LRM，一种原生于扩散模型的潜在奖励模型，直接在含噪扩散状态上进行偏好学习，通过噪声校准的Thurstone似然建模不确定性，在保持高性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的奖励函数存在计算开销大、内存占用高及像素空间奖励与潜在扩散生成器之间存在域不匹配的问题。

Method: 提出DiNa-LRM：基于预训练潜在扩散主干网络，添加时间步条件奖励头；设计噪声校准的Thurstone似然函数，建模扩散噪声依赖的不确定性；支持推理时噪声集成以提升鲁棒性与可扩展性。

Result: 在图像对齐基准上显著优于现有扩散类奖励基线，性能媲美SOTA VLM但计算成本大幅降低；在偏好优化中展现出更优的收敛速度与资源效率。

Conclusion: DiNa-LRM为扩散/流匹配模型提供了高效、鲁棒且原生适配的潜在空间奖励机制，有效缓解了域不匹配与高计算成本问题。

Abstract: Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.

</details>


### [83] [SurfPhase: 3D Interfacial Dynamics in Two-Phase Flows from Sparse Videos](https://arxiv.org/abs/2602.11154)
*Yue Gao,Hong-Xing Yu,Sanghyeon Chang,Qianxi Fu,Bo Zhu,Yoonjin Won,Juan Carlos Niebles,Jiajun Wu*

Main category: cs.CV

TL;DR: SurfPhase is a novel neural rendering model that reconstructs 3D interfacial dynamics in two-phase flows (e.g., pool boiling) from sparse camera views, using dynamic Gaussian surfels, signed distance functions, and video diffusion for view synthesis and velocity estimation.


<details>
  <summary>Details</summary>
Motivation: Classical experimental techniques struggle near moving interfaces, and existing neural rendering methods fail for sharp, deformable liquid-vapor interfaces in two-phase flows.

Method: SurfPhase integrates dynamic Gaussian surfels with a signed distance function for geometric consistency and employs a video diffusion model to synthesize novel-view videos, refining reconstruction from sparse observations.

Result: Evaluated on a new high-speed pool boiling dataset, SurfPhase achieves high-quality novel-view synthesis and accurate velocity estimation using only two camera views.

Conclusion: SurfPhase overcomes key limitations of prior methods, enabling robust 3D reconstruction of sharp, dynamic interfaces in two-phase flows from minimal input views.

Abstract: Interfacial dynamics in two-phase flows govern momentum, heat, and mass transfer, yet remain difficult to measure experimentally. Classical techniques face intrinsic limitations near moving interfaces, while existing neural rendering methods target single-phase flows with diffuse boundaries and cannot handle sharp, deformable liquid-vapor interfaces. We propose SurfPhase, a novel model for reconstructing 3D interfacial dynamics from sparse camera views. Our approach integrates dynamic Gaussian surfels with a signed distance function formulation for geometric consistency, and leverages a video diffusion model to synthesize novel-view videos to refine reconstruction from sparse observations. We evaluate on a new dataset of high-speed pool boiling videos, demonstrating high-quality view synthesis and velocity estimation from only two camera views. Project website: https://yuegao.me/SurfPhase.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [Reviewing the Reviewer: Elevating Peer Review Quality through LLM-Guided Feedback](https://arxiv.org/abs/2602.10118)
*Sukannya Purkayastha,Qile Wan,Anne Lauscher,Lizhen Qu,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的框架，用于检测和改善学术评审中的‘懒惰思维’问题，通过将评审分解为论证片段、结合神经符号模块识别多类问题，并生成有针对性的反馈，显著提升了评审质量。


<details>
  <summary>Details</summary>
Motivation: 同行评审对科学质量至关重要，但依赖简单启发式（即‘懒惰思维’）降低了评审标准；现有方法将懒惰思维检测视为单标签任务，忽略了评审中可能同时存在的多种问题（如清晰度、具体性等），且缺乏可操作的指南导向反馈。

Method: 提出一个LLM驱动的框架：1）将评审分解为论证性片段；2）使用神经符号模块（融合LLM特征与传统分类器）识别多类问题；3）利用经遗传算法优化的、问题特定模板生成针对性反馈。

Result: 实验表明该方法优于零样本LLM基线，评审质量提升最高达92.4%；并发布了包含1309句标注数据的新数据集LazyReviewPlus，涵盖懒惰思维与具体性标签。

Conclusion: 该框架有效支持多问题细粒度检测与可操作反馈生成，推动评审质量实质性提升，并为相关研究提供了高质量基准数据集。

Abstract: Peer review is central to scientific quality, yet reliance on simple heuristics -- lazy thinking -- has lowered standards. Prior work treats lazy thinking detection as a single-label task, but review segments may exhibit multiple issues, including broader clarity problems, or specificity issues. Turning detection into actionable improvements requires guideline-aware feedback, which is currently missing. We introduce an LLM-driven framework that decomposes reviews into argumentative segments, identifies issues via a neurosymbolic module combining LLM features with traditional classifiers, and generates targeted feedback using issue-specific templates refined by a genetic algorithm. Experiments show our method outperforms zero-shot LLM baselines and improves review quality by up to 92.4\%. We also release LazyReviewPlus, a dataset of 1,309 sentences labeled for lazy thinking and specificity.

</details>


### [85] [Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens](https://arxiv.org/abs/2602.10229)
*Weihao Liu,Dehai Min,Lu Cheng*

Main category: cs.CL

TL;DR: 本文提出Latent Thoughts Tuning（LT-Tuning）框架，通过Context-Prediction-Fusion机制和三阶段课程学习，在连续潜在空间中实现更稳定、鲁棒的推理，缓解特征坍缩问题，优于现有潜在推理方法。


<details>
  <summary>Details</summary>
Motivation: 现有隐式（潜在空间）推理方法存在特征坍缩与不稳定性问题，源于隐藏状态重复作为输入嵌入时的分布失配或依赖辅助模型时的对齐问题。

Method: 提出LT-Tuning框架：1）Context-Prediction-Fusion机制，联合利用上下文隐藏状态与词表嵌入空间中的语义预测指导；2）渐进式三阶段课程学习，支持潜在与显式思维模式动态切换。

Result: 实验表明LT-Tuning显著优于现有潜在推理基线，有效缓解特征坍缩，提升推理鲁棒性与准确性。

Conclusion: 在连续潜在空间中构建高质量‘隐式思维’需兼顾上下文与语义先验，并通过结构化训练策略实现模态协同，为LLM推理范式提供新路径。

Abstract: While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy.

</details>


### [86] [Learning to Evict from Key-Value Cache](https://arxiv.org/abs/2602.10238)
*Luca Moschella,Laura Manduchi,Ozan Sener*

Main category: cs.CL

TL;DR: 本文提出KV Policy (KVP)框架，将KV缓存淘汰问题建模为强化学习任务，通过轻量级每头RL代理学习预测token未来效用以实现自适应缓存管理，在多个长上下文和对话基准上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）推理中KV缓存内存开销大，现有基于启发式（如最近性或注意力分数）的淘汰/压缩方法仅间接估计token未来效用，且引入额外计算开销。

Method: 将KV缓存淘汰建模为强化学习问题，设计轻量级、每注意力头独立的RL代理（KVP），在预计算生成轨迹上仅利用key和value向量进行训练，学习按未来效用对token排序的淘汰策略，无需修改原模型或增加推理开销。

Result: 在RULER（长上下文）和OASST2-4k（多轮对话）基准上显著优于基线；零样本泛化测试（LongBench、BOOLQ、ARC等）表明其能跨任务、跨更长上下文长度有效泛化。

Conclusion: 学习预测token未来效用是一种强大且可扩展的自适应KV缓存管理新范式。

Abstract: The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token's future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by future utility, which evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated across two different model families on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks (e.g., LongBench, BOOLQ, ARC) indicate that KVP generalizes well beyond its training distribution and to longer context lengths. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management.

</details>


### [87] [On Emergent Social World Models -- Evidence for Functional Integration of Theory of Mind and Pragmatic Reasoning in Language Models](https://arxiv.org/abs/2602.10298)
*Polina Tsvilodub,Jan-Felix Klumpp,Amir Mohammadpour,Jennifer Hu,Michael Franke*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LMs）是否在一般心理理论（ToM）与语言特异性语用推理中复用共享的计算机制，以检验其是否发展出跨任务通用的‘社会世界模型’（即功能整合假说）。通过行为评估和受认知神经科学启发的功能定位因果实验，在更大规模的本地化数据集上分析了LMs在七类ToM能力上的表现。结果为功能整合假说提供了初步支持，表明LMs可能形成相互关联而非孤立的社会认知能力。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否具备类似人类的、可跨任务复用的‘社会世界模型’，即是否在心理理论与语用推理中共享计算机制，从而理解人工系统中社会认知的涌现机制。

Method: 结合行为评估与受认知神经科学启发的功能定位因果实验方法，在比以往研究更大规模的本地化数据集上，系统评估大语言模型在Beaudoin等人（2020）提出的七类心理理论子能力上的表现，并采用严格的假设驱动统计检验。

Result: 严格统计检验结果为‘功能整合假说’提供了初步（suggestive）支持，表明大语言模型可能发展出相互关联的‘社会世界模型’，而非彼此孤立的任务特定能力。

Conclusion: 大语言模型可能在训练过程中自发形成具有一定功能整合性的社会认知表征，支持其具备初步的、跨任务通用的‘社会世界模型’；该发现对理解AI社会智能的涌现机制具有理论与方法论意义。

Abstract: This paper investigates whether LMs recruit shared computational mechanisms for general Theory of Mind (ToM) and language-specific pragmatic reasoning in order to contribute to the general question of whether LMs may be said to have emergent "social world models", i.e., representations of mental states that are repurposed across tasks (the functional integration hypothesis). Using behavioral evaluations and causal-mechanistic experiments via functional localization methods inspired by cognitive neuroscience, we analyze LMs' performance across seven subcategories of ToM abilities (Beaudoin et al., 2020) on a substantially larger localizer dataset than used in prior like-minded work. Results from stringent hypothesis-driven statistical testing offer suggestive evidence for the functional integration hypothesis, indicating that LMs may develop interconnected "social world models" rather than isolated competencies. This work contributes novel ToM localizer data, methodological refinements to functional localization techniques, and empirical insights into the emergence of social cognition in artificial systems.

</details>


### [88] [Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality](https://arxiv.org/abs/2602.10329)
*Zhimin Hu,Riya Roshan,Sashank Varma*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在推理时的资源合理性，发现即使没有显式的计算成本奖励，通过推理时扩展计算（inference-time scaling）也能自然涌现出资源合理性。


<details>
  <summary>Details</summary>
Motivation: 探究资源合理性是否能在不依赖显式计算成本奖励的情况下，通过推理时扩展计算自然涌现。

Method: 提出可变归因任务（Variable Attribution Task），通过改变候选变量数量和输入输出样本数量来系统调节任务复杂度，并对比指令微调模型（IT）与大型推理模型（LRM）在不同复杂度下的策略变化。

Result: 两类模型均随任务复杂度增加从暴力搜索转向分析式策略；IT模型在XOR/XNOR任务上性能下降，而LRM保持稳健；表明资源合理性可作为推理时扩展的自然涌现属性。

Conclusion: 资源合理性是推理时计算扩展本身的一种涌现特性，无需依赖显式的计算成本优化目标。

Abstract: Human reasoning is shaped by resource rationality -- optimizing performance under constraints. Recently, inference-time scaling has emerged as a powerful paradigm to improve the reasoning performance of Large Language Models by expanding test-time computation. Specifically, instruction-tuned (IT) models explicitly generate long reasoning steps during inference, whereas Large Reasoning Models (LRMs) are trained by reinforcement learning to discover reasoning paths that maximize accuracy. However, it remains unclear whether resource-rationality can emerge from such scaling without explicit reward related to computational costs. We introduce a Variable Attribution Task in which models infer which variables determine outcomes given candidate variables, input-output trials, and predefined logical functions. By varying the number of candidate variables and trials, we systematically manipulate task complexity. Both models exhibit a transition from brute-force to analytic strategies as complexity increases. IT models degrade on XOR and XNOR functions, whereas LRMs remain robust. These findings suggest that models can adjust their reasoning behavior in response to task complexity, even without explicit cost-based reward. It provides compelling evidence that resource rationality is an emergent property of inference-time scaling itself.

</details>


### [89] [The Subjectivity of Respect in Police Traffic Stops: Modeling Community Perspectives in Body-Worn Camera Footage](https://arxiv.org/abs/2602.10339)
*Preni Golazizian,Elnaz Rahmati,Jackson Trager,Zhivar Sourati,Nona Ghazizadeh,Georgios Chochlakis,Jose Alcocer,Kerby Bennett,Aarya Vijay Devnani,Parsa Hejabi,Harry G. Muttram,Akshay Kiran Padte,Mehrshad Saadatinia,Chenhao Wu,Alireza S. Zaibari,Michael Sierra-Arévalo,Nick Weller,Shrikanth Narayanan,Benjamin A. T. Graham,Morteza Dehghani*

Main category: cs.CL

TL;DR: 本文构建了首个大规模交通拦截身体摄像头数据集，包含来自不同社区（警察关联、司法系统受影响、非关联）居民的尊重度评分和理由标注，并提出了一个视角感知建模框架，以预测个性化尊重评分并生成对应理由，从而促进执法透明与公众信任。


<details>
  <summary>Details</summary>
Motivation: 尊重是警民互动的核心维度，影响公众信任与合法性感知，但其主观性强且受生活经验影响，需纳入社区特异性视角。

Method: 基于程序正义理论、洛杉矶警方培训材料及实地调研制定领域专用评估量表；提出量表驱动的偏好数据构建框架以实现视角一致对齐；设计视角感知建模框架，从交通拦截文本中预测多视角尊重评分并生成对应理由。

Result: 在三类标注者群体上，该方法均提升了评分预测性能与理由一致性；验证了不同社区对执法行为尊重度存在系统性认知差异。

Conclusion: 视角感知建模有助于执法部门理解多元社区期望，为提升程序合法性与公共信任提供了可操作的技术路径。

Abstract: Traffic stops are among the most frequent police-civilian interactions, and body-worn cameras (BWCs) provide a unique record of how these encounters unfold. Respect is a central dimension of these interactions, shaping public trust and perceived legitimacy, yet its interpretation is inherently subjective and shaped by lived experience, rendering community-specific perspectives a critical consideration. Leveraging unprecedented access to Los Angeles Police Department BWC footage, we introduce the first large-scale traffic-stop dataset annotated with respect ratings and free-text rationales from multiple perspectives. By sampling annotators from police-affiliated, justice-system-impacted, and non-affiliated Los Angeles residents, we enable the systematic study of perceptual differences across diverse communities. To this end, we (i) develop a domain-specific evaluation rubric grounded in procedural justice theory, LAPD training materials, and extensive fieldwork; (ii) introduce a rubric-driven preference data construction framework for perspective-consistent alignment; and (iii) propose a perspective-aware modeling framework that predicts personalized respect ratings and generates annotator-specific rationales for both officers and civilian drivers from traffic-stop transcripts. Across all three annotator groups, our approach improves both rating prediction performance and rationale alignment. Our perspective-aware framework enables law enforcement to better understand diverse community expectations, providing a vital tool for building public trust and procedural legitimacy.

</details>


### [90] [Geometry-Aware Decoding with Wasserstein-Regularized Truncation and Mass Penalties for Large Language Models](https://arxiv.org/abs/2602.10346)
*Arash Gholami Davoodi,Navid Rezazadeh,Seyed Pouyan Mousavi Davoudi,Pouya Pezeshkpour*

Main category: cs.CL

TL;DR: 本文提出Top-W解码方法，通过Wasserstein距离在词嵌入空间中进行几何感知的截断采样，在保持原始分布语义结构的同时平衡概率质量和熵，理论上有简洁闭式解，实验显示其在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于截断的采样方法主要依赖概率质量和熵，忽略了词嵌入空间的语义几何结构，难以兼顾生成多样性、创造性与逻辑一致性。

Method: 提出Top-W几何感知截断规则，利用Wasserstein距离度量截断后分布与原始分布的语义接近性，并在概率质量与保留集合熵之间显式权衡；理论推导出最优截断集为单个token或一维前缀，可线性扫描高效求解；结合基于几何势函数（如最近邻集或k-NN）与交替解码流程，兼容标准截断-采样接口。

Result: 在GSM8K、GPQA、AlpacaEval和MT-Bench四个基准上，跨三个指令微调模型，Top-W持续超越现有最优解码方法，最高提升33.7%；同时在基于裁判的开放生成评估中提升创造力。

Conclusion: Top-W通过引入词嵌入空间的几何结构建模，为解码策略提供了更本质的理论基础与实用性能提升，实现了逻辑性、多样性与创造性的更好统一。

Abstract: Large language models (LLMs) must balance diversity and creativity against logical coherence in open-ended generation. Existing truncation-based samplers are effective but largely heuristic, relying mainly on probability mass and entropy while ignoring semantic geometry of the token space. We present Top-W, a geometry-aware truncation rule that uses Wasserstein distance-defined over token-embedding geometry-to keep the cropped distribution close to the original, while explicitly balancing retained probability mass against the entropy of the kept set. Our theory yields a simple closed-form structure for the fixed-potential subset update: depending on the mass-entropy trade-off, the optimal crop either collapses to a single token or takes the form of a one-dimensional prefix that can be found efficiently with a linear scan. We implement Top-W using efficient geometry-based potentials (nearest-set or k-NN) and pair it with an alternating decoding routine that keeps the standard truncation-and-sampling interface unchanged. Extensive experiments on four benchmarks (GSM8K, GPQA, AlpacaEval, and MT-Bench) across three instruction-tuned models show that Top-W consistently outperforms prior state-of-the-art decoding approaches achieving up to 33.7% improvement. Moreover, we find that Top-W not only improves accuracy-focused performance, but also boosts creativity under judge-based open-ended evaluation.

</details>


### [91] [When Less Is More? Diagnosing ASR Predictions in Sardinian via Layer-Wise Decoding](https://arxiv.org/abs/2602.10350)
*Domenico De Cristofaro,Alessandro Vietti,Marianne Pouplier,Aleese Block*

Main category: cs.CL

TL;DR: 本文研究了多语言语音模型中间层比最终输出层具有更准确的音素表示，并通过逐层解码预训练的Wav2Vec2模型，发现在坎皮达内塞撒丁语（低资源语言）上，截断顶层Transformer层反而能提升音素错误率（PER），最佳性能出现在倒数第二层而非最后一层；进一步分析揭示了中间层预测在保持音段身份、避免过生成和减少音系错误方面的优势，并提出‘退化错误’概念——即中间层正确预测被最终层错误覆盖的现象，表明深层可能过度泛化而丢失声学细节；结果支持将早期层探测作为ASR模型（尤其低资源场景）的语言学诊断工具。


<details>
  <summary>Details</summary>
Motivation: 探索多语言语音模型中间层是否比最终层提供更准确的音素表示，尤其针对低资源语言（如坎皮达内塞撒丁语），并理解标准评估指标可能掩盖的音系行为。

Method: 对预训练Wav2Vec2模型采用逐层解码策略，系统截断不同深度的编码器层进行音素识别；结合细粒度对齐分析与音素错误率（PER）评估；定义并统计‘退化错误’现象。

Result: 在坎皮达内塞撒丁语上，截断顶层后PER下降，最优性能出现在倒数第二层；中间层预测更保真于音段身份、减少过生成及特定音系错误；发现显著的‘退化错误’，即中间层正确预测被最终层错误覆盖。

Conclusion: 中间编码层蕴含更丰富的语音细节，深层抽象可能导致性能退化；早期层探测可作为低资源ASR模型的语言学诊断有效手段，超越表面错误率指标。

Abstract: Recent studies have shown that intermediate layers in multilingual speech models often encode more phonetically accurate representations than the final output layer. In this work, we apply a layer-wise decoding strategy to a pretrained Wav2Vec2 model to investigate how phoneme-level predictions evolve across encoder layers, focusing on Campidanese Sardinian, a low-resource language. We show that truncating upper transformer layers leads to improved Phoneme Error Rates (PER), with the best performance achieved not at the final layer, but two layers earlier. Through fine-grained alignment analysis, we find that intermediate predictions better preserve segmental identity, avoid overgeneration, and reduce certain classes of phonological errors. We also introduce the notion of regressive errors, cases where correct predictions at intermediate layers are overwritten by errors at the final layer. These regressions highlight the limitations of surface-level error metrics and reveal how deeper layers may generalize or abstract away from acoustic detail. Our findings support the use of early-layer probing as a diagnostic tool for ASR models, particularly in low-resource settings where standard evaluation metrics may fail to capture linguistically meaningful behavior.

</details>


### [92] [Learning Self-Interpretation from Interpretability Artifacts: Training Lightweight Adapters on Vector-Label Pairs](https://arxiv.org/abs/2602.10352)
*Keenan Pepper,Alex McKenzie,Florin Pop,Stijn Servaes,Martin Leitgab,Mike Vaiana,Judd Rosenblatt,Michael S. A. Graziano,Diogo de Lucena*

Main category: cs.CL

TL;DR: 本文提出了一种无需修改大语言模型（LM）参数、仅训练轻量级适配器（scalar affine adapter）来实现可靠自解释的新方法，在多个任务和模型族上表现出色，且性能随模型规模提升而增强。


<details>
  <summary>Details</summary>
Motivation: 现有自解释方法因超参数敏感而不稳定，亟需一种更鲁棒、可泛化且不改变原模型的解释机制。

Method: 在冻结大语言模型的前提下，训练仅含 d_model+1 参数的标量仿射适配器，利用其生成稀疏自编码器特征标签、主题识别及多跳推理中的隐式桥接实体。

Result: 适配器在70B模型上生成标签准确率达71%（优于原始训练标签的63%），主题识别召回率94%（基线仅1%），并能解码未显式出现的桥接实体；偏置向量贡献85%性能提升；简单适配器泛化优于复杂变体；自解释能力增益超过模型参数从7B增至72B带来的能力增益。

Conclusion: 自解释能力可通过轻量适配器高效获得，且随模型规模扩大而提升，无需修改被解释模型，为可扩展、鲁棒的模型可解释性提供了新范式。

Abstract: Self-interpretation methods prompt language models to describe their own internal states, but remain unreliable due to hyperparameter sensitivity. We show that training lightweight adapters on interpretability artifacts, while keeping the LM entirely frozen, yields reliable self-interpretation across tasks and model families. A scalar affine adapter with just $d_\text{model}+1$ parameters suffices: trained adapters generate sparse autoencoder feature labels that outperform the training labels themselves (71% vs 63% generation scoring at 70B scale), identify topics with 94% recall@1 versus 1% for untrained baselines, and decode bridge entities in multi-hop reasoning that appear in neither prompt nor response, surfacing implicit reasoning without chain-of-thought. The learned bias vector alone accounts for 85% of improvement, and simpler adapters generalize better than more expressive alternatives. Controlling for model knowledge via prompted descriptions, we find self-interpretation gains outpace capability gains from 7B to 72B parameters. Our results demonstrate that self-interpretation improves with scale, without modifying the model being interpreted.

</details>


### [93] [Physically Interpretable AlphaEarth Foundation Model Embeddings Enable LLM-Based Land Surface Intelligence](https://arxiv.org/abs/2602.10354)
*Mashrekur Rahman*

Main category: cs.CL

TL;DR: 本文系统分析了Google AlphaEarth卫星基础模型嵌入的物理可解释性，发现其64维嵌入能高保真地表征26种环境变量（如温度、高程R²达0.97），并基于此构建了支持自然语言查询的‘地表智能系统’，经多LLM评估验证其接地性和连贯性优异。


<details>
  <summary>Details</summary>
Motivation: 卫星基础模型生成的密集嵌入缺乏物理可解释性，限制其在环境决策系统中的实际应用。

Method: 基于美国本土1210万样本，采用线性、非线性及注意力机制三种方法，分析AlphaEarth 64维嵌入与26个环境变量的关系；进而构建基于FAISS索引和检索增强生成（RAG）的地表智能系统，并通过四模型轮换的LLM-as-Judge框架评估系统性能。

Result: 12/26个环境变量R² > 0.90，温度与高程接近0.97；最强维度-变量关系在三种方法中一致且时空稳健；地表智能系统在360次查询中加权评分为3.74±0.77，其中接地性3.93、连贯性4.25。

Conclusion: 卫星基础模型嵌入是具有物理结构的表征，可被可靠解读并直接用于环境与地理空间智能任务。

Abstract: Satellite foundation models produce dense embeddings whose physical interpretability remains poorly understood, limiting their integration into environmental decision systems. Using 12.1 million samples across the Continental United States (2017--2023), we first present a comprehensive interpretability analysis of Google AlphaEarth's 64-dimensional embeddings against 26 environmental variables spanning climate, vegetation, hydrology, temperature, and terrain. Combining linear, nonlinear, and attention-based methods, we show that individual embedding dimensions map onto specific land surface properties, while the full embedding space reconstructs most environmental variables with high fidelity (12 of 26 variables exceed $R^2 > 0.90$; temperature and elevation approach $R^2 = 0.97$). The strongest dimension-variable relationships converge across all three analytical methods and remain robust under spatial block cross-validation (mean $ΔR^2 = 0.017$) and temporally stable across all seven study years (mean inter-year correlation $r = 0.963$). Building on these validated interpretations, we then developed a Land Surface Intelligence system that implements retrieval-augmented generation over a FAISS-indexed embedding database of 12.1 million vectors, translating natural language environmental queries into satellite-grounded assessments. An LLM-as-Judge evaluation across 360 query--response cycles, using four LLMs in rotating generator, system, and judge roles, achieved weighted scores of $μ= 3.74 \pm 0.77$ (scale 1--5), with grounding ($μ= 3.93$) and coherence ($μ= 4.25$) as the strongest criteria. Our results demonstrate that satellite foundation model embeddings are physically structured representations that can be operationalized for environmental and geospatial intelligence.

</details>


### [94] [Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation](https://arxiv.org/abs/2602.10356)
*Tianci Xue,Zeyi Liao,Tianneng Shi,Zilu Wang,Kai Zhang,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: 本文提出ACuRL框架，通过自主课程强化学习实现无需人工标注数据的计算机使用代理（CUA）持续学习，并引入CUAJudge自动评估器提供可靠奖励信号，显著提升性能且避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现实数字环境高度多样和动态，导致代理常面临未见场景与分布偏移，而获取高质量、环境接地的代理数据依赖昂贵人工标注，亟需零人工数据的持续学习方法。

Method: 提出ACuRL框架：代理先探索目标环境获取初始经验；迭代训练中，课程任务生成器结合历史经验与上轮反馈，生成适配当前能力的新任务；引入CUAJudge自动评估器提供高一致性（93%）奖励信号；采用稀疏参数更新（如20%参数）。

Result: 在环境内和跨环境持续学习中均有效，性能提升4–22%，无灾难性遗忘；分析显示仅需高度稀疏参数更新（如20%），保障高效鲁棒适应。

Conclusion: ACuRL实现了零人工标注下的CUA持续自适应学习，结合自主课程生成与高可靠性自动评估，为动态数字环境中的智能代理提供了可扩展、稳健的学习范式。

Abstract: Real-world digital environments are highly diverse and dynamic. These characteristics cause agents to frequently encounter unseen scenarios and distribution shifts, making continual learning in specific environments essential for computer-use agents (CUAs). However, a key challenge lies in obtaining high-quality and environment-grounded agent data without relying on costly human annotation. In this work, we introduce ACuRL, an Autonomous Curriculum Reinforcement Learning framework that continually adapts agents to specific environments with zero human data. The agent first explores target environments to acquire initial experiences. During subsequent iterative training, a curriculum task generator leverages these experiences together with feedback from the previous iteration to synthesize new tasks tailored for the agent's current capabilities. To provide reliable reward signals, we introduce CUAJudge, a robust automatic evaluator for CUAs that achieves 93% agreement with human judgments. Empirically, our method effectively enables both intra-environment and cross-environment continual learning, yielding 4-22% performance gains without catastrophic forgetting on existing environments. Further analyses show highly sparse updates (e.g., 20% parameters), which helps explain the effective and robust adaptation. Our data and code are available at https://github.com/OSU-NLP-Group/ACuRL.

</details>


### [95] [The Alignment Bottleneck in Decomposition-Based Claim Verification](https://arxiv.org/abs/2602.10380)
*Mahmud Elahi Akhter,Federico Ruggeri,Iman Munire Bilal,Rob Procter,Maria Liakata*

Main category: cs.CL

TL;DR: 本文指出结构化声明分解在验证复杂声明时效果不一致，归因于证据对齐和子声明错误分布两个被忽视的瓶颈；通过构建新数据集并设计两种证据对齐设置（SAE与SRE），发现仅当证据细粒度且严格对齐时分解才有效；同时揭示子声明标签噪声下“保守拒答”比错误预测更能抑制误差传播，强调未来框架需注重精准证据合成与子声明模型标签偏差校准。


<details>
  <summary>Details</summary>
Motivation: 现有结构化声明分解方法在验证复杂多面声明时实证结果不一致，作者认为这源于未被重视的两个瓶颈：证据对齐问题和子声明错误分布特性。

Method: 构建包含时间限定证据和人工标注子声明证据跨度的真实世界复杂声明新数据集；设计并对比两种证据对齐设置——子声明对齐证据（SAE）与重复声明级证据（SRE）；在多个数据集（PHEMEPlus、MMM-Fact、COVID-Fact）和领域上系统评估分解性能；分析噪声子声明标签下的错误类型及其对下游鲁棒性的影响。

Result: 仅在子声明对齐证据（SAE）设置下，声明分解显著提升性能；而在重复声明级证据（SRE）设置下，分解不提升甚至损害性能；在子声明标签含噪时，“保守拒答”策略相比错误预测能显著降低误差传播。

Conclusion: 未来声明分解框架必须优先考虑精确的证据合成能力，并校准子声明验证模型的标签偏差，而非单纯依赖分解结构。

Abstract: Structured claim decomposition is often proposed as a solution for verifying complex, multi-faceted claims, yet empirical results have been inconsistent. We argue that these inconsistencies stem from two overlooked bottlenecks: evidence alignment and sub-claim error profiles. To better understand these factors, we introduce a new dataset of real-world complex claims, featuring temporally bounded evidence and human-annotated sub-claim evidence spans. We evaluate decomposition under two evidence alignment setups: Sub-claim Aligned Evidence (SAE) and Repeated Claim-level Evidence (SRE). Our results reveal that decomposition brings significant performance improvement only when evidence is granular and strictly aligned. By contrast, standard setups that rely on repeated claim-level evidence (SRE) fail to improve and often degrade performance as shown across different datasets and domains (PHEMEPlus, MMM-Fact, COVID-Fact). Furthermore, we demonstrate that in the presence of noisy sub-claim labels, the nature of the error ends up determining downstream robustness. We find that conservative "abstention" significantly reduces error propagation compared to aggressive but incorrect predictions. These findings suggest that future claim decomposition frameworks must prioritize precise evidence synthesis and calibrate the label bias of sub-claim verification models.

</details>


### [96] [Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models](https://arxiv.org/abs/2602.10382)
*Théo Lasnier,Wissam Antoun,Francis Kulumba,Djamé Seddah*

Main category: cs.CL

TL;DR: 本文首次对语言切换后门攻击进行了机制分析，发现后门触发器并非独立电路，而是劫持模型中已有的语言编码组件。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对大语言模型构成严重安全威胁，但其触发机制尚不清楚。

Method: 采用激活修补（activation patching）技术，在GAPperon模型家族（1B、8B、24B参数）上定位触发器形成位置，并识别处理触发信息的注意力头。

Result: 触发器激活的注意力头与自然编码输出语言的注意力头高度重叠（Jaccard指数0.18–0.66），表明触发器劫持了模型固有语言功能。

Conclusion: 后门触发器不构建孤立电路，而是复用模型原有语言表征机制；该发现为基于功能组件监控的检测与缓解策略提供了新思路。

Abstract: Backdoor attacks pose significant security risks for Large Language Models (LLMs), yet the internal mechanisms by which triggers operate remain poorly understood. We present the first mechanistic analysis of language-switching backdoors, studying the GAPperon model family (1B, 8B, 24B parameters) which contains triggers injected during pretraining that cause output language switching. Using activation patching, we localize trigger formation to early layers (7.5-25% of model depth) and identify which attention heads process trigger information. Our central finding is that trigger-activated heads substantially overlap with heads naturally encoding output language across model scales, with Jaccard indices between 0.18 and 0.66 over the top heads identified. This suggests that backdoor triggers do not form isolated circuits but instead co-opt the model's existing language components. These findings have implications for backdoor defense: detection methods may benefit from monitoring known functional components rather than searching for hidden circuits, and mitigation strategies could potentially leverage this entanglement between injected and natural behaviors.

</details>


### [97] [When Tables Go Crazy: Evaluating Multimodal Models on French Financial Documents](https://arxiv.org/abs/2602.10384)
*Virginie Mouilleron,Théo Lasnier,Djamé Seddah*

Main category: cs.CL

TL;DR: 本文提出了首个面向法语金融文档理解的多模态评测基准Multimodal Finance Eval，评估了6个开源视觉语言模型在文本提取、表格理解、图表解读和多轮对话推理等任务上的表现，发现模型在图表理解和多轮交互中存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在专业非英语领域（尤其是金融领域）的可靠性尚未被充分探索，而金融文档包含密集监管文本、数值表格和可视化图表，错误提取可能带来现实后果。

Method: 构建了包含1204个专家验证问题的多模态金融评测数据集Multimodal Finance Eval，覆盖投资说明书、KID和PRIIPs等真实文档；采用LLM-as-judge协议评估6个开源VLM（8B-124B参数）。

Result: 模型在文本和表格任务上表现良好（85-90%准确率），但在图表解读上表现较差（34-62%）；多轮对话中早期错误会传播，导致准确率降至约50%，且与模型规模无关。

Conclusion: 当前VLM在明确定义的抽取任务中有效，但在交互式、多步金融分析中仍很脆弱；Multimodal Finance Eval为这一高风险领域提供了具有挑战性的评测基准。

Abstract: Vision-language models (VLMs) perform well on many document understanding tasks, yet their reliability in specialized, non-English domains remains underexplored. This gap is especially critical in finance, where documents mix dense regulatory text, numerical tables, and visual charts, and where extraction errors can have real-world consequences. We introduce Multimodal Finance Eval, the first multimodal benchmark for evaluating French financial document understanding. The dataset contains 1,204 expert-validated questions spanning text extraction, table comprehension, chart interpretation, and multi-turn conversational reasoning, drawn from real investment prospectuses, KIDs, and PRIIPs. We evaluate six open-weight VLMs (8B-124B parameters) using an LLM-as-judge protocol. While models achieve strong performance on text and table tasks (85-90% accuracy), they struggle with chart interpretation (34-62%). Most notably, multi-turn dialogue reveals a sharp failure mode: early mistakes propagate across turns, driving accuracy down to roughly 50% regardless of model size.
  These results show that current VLMs are effective for well-defined extraction tasks but remain brittle in interactive, multi-step financial analysis. Multimodal Finance Eval offers a challenging benchmark to measure and drive progress in this high-stakes setting.

</details>


### [98] [Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs](https://arxiv.org/abs/2602.10388)
*Zhongzhi Li,Xuansheng Wu,Yijiang Li,Lijie Hu,Ninghao Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于特征激活覆盖（FAC）的多样性驱动数据合成框架（FAC Synthesis），通过稀疏自编码器识别缺失特征并生成对应样本，显著提升大语言模型下游任务性能，并发现跨模型家族的可迁移可解释特征空间。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本度量的数据多样性评估方法对任务相关特征捕捉能力弱，难以有效指导后训练数据构建。

Method: 提出Feature Activation Coverage（FAC）指标，在可解释的神经元特征空间中量化数据多样性；构建FAC Synthesis框架：先用稀疏自编码器识别种子数据中缺失的高价值特征，再生成能激活这些特征的合成样本。

Result: 在指令遵循、毒性检测、奖励建模和行为引导等任务上，FAC Synthesis显著提升数据多样性与下游性能；发现LLaMA、Mistral、Qwen等模型存在共享可解释特征空间，支持跨模型知识迁移。

Conclusion: FAC为数据多样性提供了更本质、可解释的度量方式，FAC Synthesis是一种实用且普适的数据中心优化方法，推动大语言模型后训练的数据驱动范式发展。

Abstract: The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.

</details>


### [99] [When are We Worried? Temporal Trends of Anxiety and What They Reveal about Us](https://arxiv.org/abs/2602.10400)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 本文利用新构建的词汇-焦虑关联词典，分析了大量美国和加拿大社交媒体（推文）数据，揭示了人们在一天中、一周中以及不同语法时态和人称代词使用下的焦虑模式。


<details>
  <summary>Details</summary>
Motivation: 探究社交媒体上人类焦虑表达的时间规律及其与语言特征（如时态、人称代词）的关系，以理解焦虑的心理与行为背景。

Method: 基于词典的焦虑词频统计方法，对大规模推文数据进行时间序列分析（按小时、星期）、时态分类（过去/现在/未来）及人称代词（第一、二、三人称，主格/宾格）分组分析。

Result: 发现焦虑水平在早上8点最高、正午最低；周中最高、周末最低；过去时句子焦虑最高、未来时最低；第三人称和主语代词相关帖子焦虑更高。

Conclusion: 焦虑表达具有显著的时间与语言结构规律，反映其与生理节律、时间取向及自我/他人关注焦点密切相关。

Abstract: In this short paper, we make use of a recently created lexicon of word-anxiety associations to analyze large amounts of US and Canadian social media data (tweets) to explore *when* we are anxious and what insights that reveals about us. We show that our levels of anxiety on social media exhibit systematic patterns of rise and fall during the day -- highest at 8am (in-line with when we have high cortisol levels in the body) and lowest around noon. Anxiety is lowest on weekends and highest mid-week. We also examine anxiety in past, present, and future tense sentences to show that anxiety is highest in past tense and lowest in future tense. Finally, we examine the use of anxiety and calmness words in posts that contain pronouns to show: more anxiety in 3rd person pronouns (he, they) posts than 1st and 2nd person pronouns and higher anxiety in posts with subject pronouns (I, he, she, they) than object pronouns (me, him, her, them). Overall, these trends provide valuable insights on not just when we are anxious, but also how different types of focus (future, past, self, outward, etc.) are related to anxiety.

</details>


### [100] [EVOKE: Emotion Vocabulary Of Korean and English](https://arxiv.org/abs/2602.10414)
*Yoonwon Jung,Hagyeong Shin,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 本文介绍了EVOKE，一个英语和韩语情感词汇的平行数据集，涵盖全面的情感词、多对多翻译及语言特有情感词识别，并系统标注了形容词、动词、多义词及隐喻，是目前最全面、系统且理论中立的双语情感词资源。


<details>
  <summary>Details</summary>
Motivation: 构建一个全面、系统且理论中立的英语-韩语情感词汇平行数据集，以支持跨语言情感研究及多种学科应用。

Method: 构建包含1427个韩语词和1399个英语词的平行数据集，系统标注819个韩语和924个英语形容词与动词，同时标注多义性、语义关系及情感隐喻。

Result: 发布首个大规模、系统化、理论中立的英韩双语情感词汇数据集EVOKE，含多对多翻译、语言特有词识别及多义/隐喻标注，已开源。

Conclusion: EVOKE为情感科学、心理语言学、计算语言学和自然语言处理提供了灵活、实用且理论兼容的双语情感词资源。

Abstract: This paper introduces EVOKE, a parallel dataset of emotion vocabulary in English and Korean. The dataset offers comprehensive coverage of emotion words in each language, in addition to many-to-many translations between words in the two languages and identification of language-specific emotion words. The dataset contains 1,427 Korean words and 1,399 English words, and we systematically annotate 819 Korean and 924 English adjectives and verbs. We also annotate multiple meanings of each word and their relationships, identifying polysemous emotion words and emotion-related metaphors. The dataset is, to our knowledge, the most comprehensive, systematic, and theory-agnostic dataset of emotion words in both Korean and English to date. It can serve as a practical tool for emotion science, psycholinguistics, computational linguistics, and natural language processing, allowing researchers to adopt different views on the resource reflecting their needs and theoretical perspectives. The dataset is publicly available at https://github.com/yoonwonj/EVOKE.

</details>


### [101] [LATA: A Tool for LLM-Assisted Translation Annotation](https://arxiv.org/abs/2602.10454)
*Baorong Huang,Ali Asiri*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的交互式工具，用于阿拉伯语-英语等结构差异大的语言对的高质量平行语料库构建，通过模板化提示管理与人工校验结合，提升句级对齐与翻译现象标注的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统自动化对齐工具在处理阿拉伯语-英语等结构高度差异的语言对时，难以捕捉深层语言转换和语义细微差别，亟需兼顾可扩展性与专家级标注精度的新方法。

Method: 设计了一个LLM辅助的交互式工具，采用模板驱动的Prompt Manager，在严格JSON输出约束下完成句子切分与对齐；结合自动化预处理与人工在环（human-in-the-loop）流程，支持研究者精细化调整对齐结果并以独立标注架构（stand-off architecture）添加翻译技术标签。

Result: 该工具在保持高标注效率的同时，显著提升了对复杂翻译现象（如专业领域中的语义迁移）的建模能力与标注准确性。

Conclusion: LLM辅助的结构化提示与人机协同流程，为高难度语言对的平行语料构建提供了兼顾自动化规模与语言学严谨性的可行路径。

Abstract: The construction of high-quality parallel corpora for translation research has increasingly evolved from simple sentence alignment to complex, multi-layered annotation tasks. This methodological shift presents significant challenges for structurally divergent language pairs, such as Arabic--English, where standard automated tools frequently fail to capture deep linguistic shifts or semantic nuances. This paper introduces a novel, LLM-assisted interactive tool designed to reduce the gap between scalable automation and the rigorous precision required for expert human judgment. Unlike traditional statistical aligners, our system employs a template-based Prompt Manager that leverages large language models (LLMs) for sentence segmentation and alignment under strict JSON output constraints. In this tool, automated preprocessing integrates into a human-in-the-loop workflow, allowing researchers to refine alignments and apply custom translation technique annotations through a stand-off architecture. By leveraging LLM-assisted processing, the tool balances annotation efficiency with the linguistic precision required to analyze complex translation phenomena in specialized domains.

</details>


### [102] [Neuro-Symbolic Synergy for Interactive World Modeling](https://arxiv.org/abs/2602.10480)
*Hongyu Zhao,Siyu Zhou,Haolin Yang,Zengyi Qin,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文提出Neuro-Symbolic Synergy (NeSyS)框架，融合大语言模型（LLM）的语义表达能力与符号世界模型（WM）的逻辑一致性，通过交替训练和概率分布约束提升预测准确率与数据效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）作为世界模型（WMs）时易产生幻觉，尤其在需严格遵循确定性转移规则的边界情况下；而符号WM虽具逻辑一致性，却缺乏语义表达力。

Method: 提出Neuro-Symbolic Synergy（NeSyS）框架：1）让LLM与可执行符号WM交替训练，彼此补充未覆盖的轨迹；2）符号WM不依赖提示工程，而是直接修改LLM输出概率分布以施加约束；3）神经WM仅在符号规则未覆盖的轨迹上微调。

Result: 在ScienceWorld、Webshop和Plancraft三个交互环境中，NeSyS在WM预测准确率和数据效率（训练数据减少50%）上均显著优于基线方法。

Conclusion: NeSyS成功弥合了神经与符号方法在世界建模中的关键鸿沟，在保持语义表达力的同时保障了逻辑鲁棒性，为构建可靠、高效的世界模型提供了新范式。

Abstract: Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical consistency but lack semantic expressivity. To bridge this gap, we propose Neuro-Symbolic Synergy (NeSyS), a framework that integrates the probabilistic semantic priors of LLMs with executable symbolic rules to achieve both expressivity and robustness. NeSyS alternates training between the two models using trajectories inadequately explained by the other. Unlike rule-based prompting, the symbolic WM directly constrains the LLM by modifying its output probability distribution. The neural WM is fine-tuned only on trajectories not covered by symbolic rules, reducing training data by 50% without loss of accuracy. Extensive experiments on three distinct interactive environments, i.e., ScienceWorld, Webshop, and Plancraft, demonstrate NeSyS's consistent advantages over baselines in both WM prediction accuracy and data efficiency.

</details>


### [103] [Canvas-of-Thought: Grounding Reasoning via Mutable Structured States](https://arxiv.org/abs/2602.10494)
*Lingzhuang Sun,Yuxia Zhu,Ruitong Liu,Hao Liang,Zheng Sun,Caijun Jia,Honghao He,Yuchen Wu,Siyuan Li,Jingxuan Wei,Xiangxiang Zhang,Bihui Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: 本文提出Canvas-of-Thought（Canvas-CoT），利用HTML Canvas作为外部推理基底，支持基于DOM的原子级CRUD操作与渲染反馈机制，以提升多模态大模型在几何、SVG等高维任务中的推理精度与上下文效率。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought（CoT）在多模态大模型中受限于线性文本序列，难以有效处理视觉状态更新和局部纠错，尤其在几何与SVG等需显式视觉引导的高维任务中表现不足。

Method: 提出Canvas-of-Thought（Canvas-CoT），将HTML Canvas作为外部推理基底，支持原子级DOM操作（CRUD）实现原位状态修订，并引入基于渲染的批判循环（rendering-based critique loop）提供硬约束视觉反馈。

Result: 在VCode、RBench-V和MathVista等多个基准上显著超越现有方法，验证了其在上下文效率与推理精度上的优势。

Conclusion: Canvas-CoT为多模态推理提供了新范式，通过解耦推理逻辑与可视化状态管理，缓解了传统CoT的上下文膨胀与隐式状态维护问题。

Abstract: While Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of Multimodal Large Language Models (MLLMs), relying solely on linear text sequences remains a bottleneck for complex tasks. We observe that even when auxiliary visual elements are interleaved, they are often treated as static snapshots within a one-dimensional, unstructured reasoning chain. We argue that such approaches treat reasoning history as an immutable stream: correcting a local error necessitates either generating verbose downstream corrections or regenerating the entire context. This forces the model to implicitly maintain and track state updates, significantly increasing token consumption and cognitive load. This limitation is particularly acute in high-dimensional domains, such as geometry and SVG design, where the textual expression of CoT lacks explicit visual guidance, further constraining the model's reasoning precision. To bridge this gap, we introduce \textbf{Canvas-of-Thought (Canvas-CoT)}. By leveraging a HTML Canvas as an external reasoning substrate, Canvas-CoT empowers the model to perform atomic, DOM-based CRUD operations. This architecture enables in-place state revisions without disrupting the surrounding context, allowing the model to explicitly maintain the "ground truth". Furthermore, we integrate a rendering-based critique loop that serves as a hard constraint validator, providing explicit visual feedback to resolve complex tasks that are difficult to articulate through text alone. Extensive experiments on VCode, RBench-V, and MathVista demonstrate that Canvas-CoT significantly outperforms existing baselines, establishing a new paradigm for context-efficient multimodal reasoning.

</details>


### [104] [On the Robustness of Knowledge Editing for Detoxification](https://arxiv.org/abs/2602.10504)
*Ming Dong,Shiyi Tang,Ziyan Peng,Guanyi Chen,Tingting He*

Main category: cs.CL

TL;DR: 本文提出了一种面向鲁棒性的知识编辑型去毒化评估框架，揭示了现有方法存在伪去毒化、联合编辑失效及跨语言泛化能力弱等问题，表明该技术目前仅在特定模型、少量目标和部分语言上有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识编辑的去毒化评估过度依赖自动毒性分类器，假设毒性分数下降即代表有害行为被真正抑制，缺乏对方法鲁棒性的系统检验。

Method: 提出三维度鲁棒性评估框架：优化鲁棒性（对抗退化生成）、组合鲁棒性（多行为联合编辑效果）、跨语言鲁棒性（单/跨语言去毒泛化能力），并识别伪去毒化等典型失效模式。

Result: 发现伪去毒化普遍存在；多行为联合编辑显著降低效果；单语与跨语言去毒效果高度依赖模型与方法的特定组合；整体鲁棒性受限。

Conclusion: KE-based detoxification 目前仅在特定大模型、有限数量的去毒目标及部分语言上具备鲁棒性，尚不具备通用部署条件。

Abstract: Knowledge-Editing-based (KE-based) detoxification has emerged as a promising approach for mitigating harmful behaviours in Large Language Models. Existing evaluations, however, largely rely on automatic toxicity classifiers, implicitly assuming that reduced toxicity scores reflect genuine behavioural suppression. In this work, we propose a robustness-oriented evaluation framework for KE-based detoxification that examines its reliability beyond standard classifier-based metrics along three dimensions: optimisation robustness, compositional robustness, and cross-lingual robustness. We identify pseudo-detoxification as a common failure mode, where apparent toxicity reductions arise from degenerate generation behaviours rather than meaningful suppression of unsafe content. We further show that detoxification effectiveness degrades when multiple unsafe behaviours are edited jointly, and that both monolingual and cross-lingual detoxification remain effective only under specific model-method combinations. Overall, our results indicate that KE-based detoxification is robust only for certain models, limited numbers of detoxification objectives, and a subset of languages.

</details>


### [105] [LHAW: Controllable Underspecification for Long-Horizon Tasks](https://arxiv.org/abs/2602.10525)
*George Pu,Michael S. Lee,Udari Madhushani Sehwag,David J. Lee,Bryan Zhu,Yash Maurya,Mohit Raghavendra,Yuan Xue,Samuel Marc Denton*

Main category: cs.CL

TL;DR: 本文提出了LHAW框架，用于系统性生成和评估长周期工作流中的模糊性问题，通过移除目标、约束、输入和上下文四类信息来构造可控的欠规范任务变体，并基于实际智能体执行结果进行分类验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏可扩展、任务无关的框架来系统地构建和衡量自定义工作流中模糊性的影响，限制了长周期工作流智能体在模糊情境下寻求澄清能力的发展。

Method: 提出LHAW（Long-Horizon Augmented Workflows），一种模块化、数据集无关的合成流程，通过在目标、约束、输入和上下文四个维度上按可控严重程度移除信息，将明确任务转化为欠规范变体；并采用真实智能体运行结果而非LLM预测来验证和分类（结果关键型、发散型、良性）。

Result: 发布了来自TheAgentCompany、SWE-Bench Pro和MCP-Atlas的285个任务变体，并提供了对当前智能体在检测、推理与解决欠规范问题方面表现的正式分析。

Conclusion: LHAW是首个支持成本敏感型评估的系统性框架，可用于评测长周期场景下智能体的澄清行为，推动高可靠性自主系统的发展。

Abstract: Long-horizon workflow agents that operate effectively over extended periods are essential for truly autonomous systems. Their reliable execution critically depends on the ability to reason through ambiguous situations in which clarification seeking is necessary to ensure correct task execution. However, progress is limited by the lack of scalable, task-agnostic frameworks for systematically curating and measuring the impact of ambiguity across custom workflows. We address this gap by introducing LHAW (Long-Horizon Augmented Workflows), a modular, dataset-agnostic synthetic pipeline that transforms any well-specified task into controllable underspecified variants by systematically removing information across four dimensions - Goals, Constraints, Inputs, and Context - at configurable severity levels. Unlike approaches that rely on LLM predictions of ambiguity, LHAW validates variants through empirical agent trials, classifying them as outcome-critical, divergent, or benign based on observed terminal state divergence. We release 285 task variants from TheAgentCompany, SWE-Bench Pro and MCP-Atlas according to our taxonomy alongside formal analysis measuring how current agents detect, reason about, and resolve underspecification across ambiguous settings. LHAW provides the first systematic framework for cost-sensitive evaluation of agent clarification behavior in long-horizon settings, enabling development of reliable autonomous systems.

</details>


### [106] [When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning](https://arxiv.org/abs/2602.10560)
*Leheng Sheng,Yongtao Zhang,Wenchang Ma,Yaorui Shi,Ting Huang,Xiang Wang,An Zhang,Ke Shen,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文提出GRU-Mem，一种基于门控机制的长上下文推理方法，通过更新门和退出门控制记忆更新与循环终止，并结合端到端强化学习优化，显著提升推理效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文推理方法（如MemAgent）存在记忆无节制膨胀和缺乏循环退出机制两大问题，导致性能下降与计算浪费。

Method: 提出GRU-Mem模型，引入两个文本控制的门（更新门与退出门），并设计两个强化学习奖励信号r^update和r^exit进行端到端训练。

Result: 在多种长上下文推理任务上，GRU-Mem显著优于MemAgent，推理速度最高提升400%。

Conclusion: 门控机制与强化学习联合优化可有效提升长上下文推理的稳定性、效率与准确性。

Abstract: While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals $r^{\text{update}}$ and $r^{\text{exit}}$ within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\% times inference speed acceleration.

</details>


### [107] [Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters](https://arxiv.org/abs/2602.10604)
*Ailin Huang,Ang Li,Aobo Kong,Bin Wang,Binxing Jiao,Bo Dong,Bojun Wang,Boyu Chen,Brian Li,Buyun Ma,Chang Su,Changxin Miao,Changyi Wan,Chao Lou,Chen Hu,Chen Xu,Chenfeng Yu,Chengting Feng,Chengyuan Yao,Chunrui Han,Dan Ma,Dapeng Shi,Daxin Jiang,Dehua Ma,Deshan Sun,Di Qi,Enle Liu,Fajie Zhang,Fanqi Wan,Guanzhe Huang,Gulin Yan,Guoliang Cao,Guopeng Li,Han Cheng,Hangyu Guo,Hanshan Zhang,Hao Nie,Haonan Jia,Haoran Lv,Hebin Zhou,Hekun Lv,Heng Wang,Heung-Yeung Shum,Hongbo Huang,Hongbo Peng,Hongyu Zhou,Hongyuan Wang,Houyong Chen,Huangxi Zhu,Huimin Wu,Huiyong Guo,Jia Wang,Jian Zhou,Jianjian Sun,Jiaoren Wu,Jiaran Zhang,Jiashu Lv,Jiashuo Liu,Jiayi Fu,Jiayu Liu,Jie Cheng,Jie Luo,Jie Yang,Jie Zhou,Jieyi Hou,Jing Bai,Jingcheng Hu,Jingjing Xie,Jingwei Wu,Jingyang Zhang,Jishi Zhou,Junfeng Liu,Junzhe Lin,Ka Man Lo,Kai Liang,Kaibo Liu,Kaijun Tan,Kaiwen Yan,Kaixiang Li,Kang An,Kangheng Lin,Lei Yang,Liang Lv,Liang Zhao,Liangyu Chen,Lieyu Shi,Liguo Tan,Lin Lin,Lina Chen,Luck Ma,Mengqiang Ren,Michael Li,Ming Li,Mingliang Li,Mingming Zhang,Mingrui Chen,Mitt Huang,Na Wang,Peng Liu,Qi Han,Qian Zhao,Qinglin He,Qinxin Du,Qiuping Wu,Quan Sun,Rongqiu Yang,Ruihang Miao,Ruixin Han,Ruosi Wan,Ruyan Guo,Shan Wang,Shaoliang Pang,Shaowen Yang,Shengjie Fan,Shijie Shang,Shiliang Yang,Shiwei Li,Shuangshuang Tian,Siqi Liu,Siye Wu,Siyu Chen,Song Yuan,Tiancheng Cao,Tianchi Yue,Tianhao Cheng,Tianning Li,Tingdan Luo,Wang You,Wei Ji,Wei Yuan,Wei Zhang,Weibo Wu,Weihao Xie,Wen Sun,Wenjin Deng,Wenzhen Zheng,Wuxun Xie,Xiangfeng Wang,Xiangwen Kong,Xiangyu Liu,Xiangyu Zhang,Xiaobo Yang,Xiaojia Liu,Xiaolan Yuan,Xiaoran Jiao,Xiaoxiao Ren,Xiaoyun Zhang,Xin Li,Xin Liu,Xin Wu,Xing Chen,Xingping Yang,Xinran Wang,Xu Zhao,Xuan He,Xuanti Feng,Xuedan Cai,Xuqiang Zhou,Yanbo Yu,Yang Li,Yang Xu,Yanlin Lai,Yanming Xu,Yaoyu Wang,Yeqing Shen,Yibo Zhu,Yichen Lv,Yicheng Cao,Yifeng Gong,Yijing Yang,Yikun Yang,Yin Zhao,Yingxiu Zhao,Yinmin Zhang,Yitong Zhang,Yixuan Zhang,Yiyang Chen,Yongchi Zhao,Yongshen Long,Yongyao Wang,Yousong Guan,Yu Zhou,Yuang Peng,Yuanhao Ding,Yuantao Fan,Yuanzhen Yang,Yuchu Luo,Yudi Zhao,Yue Peng,Yueqiang Lin,Yufan Lu,Yuling Zhao,Yunzhou Ju,Yurong Zhang,Yusheng Li,Yuxiang Yang,Yuyang Chen,Yuzhu Cai,Zejia Weng,Zetao Hong,Zexi Li,Zhe Xie,Zheng Ge,Zheng Gong,Zheng Zeng,Zhenyi Lu,Zhewei Huang,Zhichao Chang,Zhiguo Huang,Zhiheng Hu,Zidong Yang,Zili Wang,Ziqi Ren,Zixin Zhang,Zixuan Wang*

Main category: cs.CL

TL;DR: Step 3.5 Flash 是一种稀疏MoE模型，结合196B参数基础与仅11B激活参数，在保持前沿智能水平的同时显著提升推理效率；通过新型注意力机制、多令牌预测和可扩展强化学习框架，在数学、代码与工具使用等代理任务上达到媲美GPT-5.2 xHigh和Gemini 3.0 Pro的性能。


<details>
  <summary>Details</summary>
Motivation: 构建高效、可靠且具备强推理能力的智能体，需在计算效率与前沿智能之间取得平衡，尤其关注多轮交互延迟、成本及稳定性。

Method: 采用稀疏MoE架构（196B总参、11B激活参），引入3:1滑动窗口/全注意力交替机制与Multi-Token Prediction (MTP-3)，并设计融合可验证信号与偏好反馈的大规模离策略强化学习框架。

Result: 在IMO-AnswerBench（85.4%）、LiveCodeBench-v6（86.4%）、tau2-Bench（88.2%）、BrowseComp（69.0%）、Terminal-Bench 2.0（51.0%）上表现优异，整体性能媲美GPT-5.2 xHigh与Gemini 3.0 Pro。

Conclusion: Step 3.5 Flash 重新定义了效率前沿，为工业级复杂智能体部署提供了高密度、高性价比的基础模型。

Abstract: We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.

</details>


### [108] [Online Causal Kalman Filtering for Stable and Effective Policy Optimization](https://arxiv.org/abs/2602.10609)
*Shuo He,Lang Feng,Xin Cheng,Lei Feng,Bo An*

Main category: cs.CL

TL;DR: 本文提出了一种基于在线因果卡尔曼滤波的策略优化方法（KPO），用于解决大语言模型强化学习中因高方差token级重要性采样（IS）比导致的训练不稳定问题。KPO将IS比建模为随token演化的隐状态，并利用卡尔曼滤波进行自回归在线更新，兼顾局部结构变化与噪声抑制，显著提升训练稳定性与数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理token级重要性采样比时，或采用固定序列级比值，或独立调整各token比值，忽视了序列内token间的时序离策略推导关系；实证发现token级局部离策略偏差存在结构性不一致，会扭曲相邻token的策略梯度更新，导致训练崩溃。

Method: 提出Online Causal Kalman Filtering for Policy Optimization（KPO）：将目标IS比建模为随token递进演化的隐状态，使用因果（仅依赖历史token）卡尔曼滤波对其进行在线、自回归估计与平滑，从而生成兼具结构感知与噪声鲁棒性的token级IS比。

Result: KPO在多个具有挑战性的数学推理数据集上显著优于当前最优方法，验证了其在提升训练稳定性与策略优化效果方面的有效性。

Conclusion: 通过引入因果卡尔曼滤波建模IS比的动态演化，KPO有效缓解了token级IS比的高方差问题，在保持局部敏感性的同时实现强噪声抑制，为大语言模型的稳定强化学习提供了新范式。

Abstract: Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.

</details>


### [109] [How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning](https://arxiv.org/abs/2602.10622)
*Jiahao Yuan,Yike Xu,Jinyong Wen,Baokun Wang,Yang Chen,Xiaotong Lin,Wuliang Huang,Ziyi Gao,Xing Fu,Yu Cheng,Weiqiang Wang*

Main category: cs.CL

TL;DR: 本文系统研究了在解码器-only大语言模型中不同注意力掩码（因果、混合、双向）对用户表征学习的影响，并提出了一种梯度引导的软掩码方法（GGSM），以改善从因果到双向注意力的训练过渡，显著提升了用户嵌入质量与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解码器-only大语言模型被广泛用作用户行为编码器，但注意力掩码方式对用户嵌入质量的影响尚未被充分探索。

Method: 在统一的对比学习框架下，基于大规模真实Alipay数据，系统评估因果、混合和双向注意力掩码；提出梯度引导软掩码（GGSM）——一种在优化前进行的梯度驱动预热策略，配合线性调度器逐步开放未来注意力。

Result: 在9个工业级用户认知基准任务（涵盖预测、偏好、营销敏感性）上，GGSM相比因果、混合及仅调度器基线，训练更稳定、双向表征质量更高，且兼容解码器预训练。

Conclusion: 注意力掩码设计与训练过渡策略对解码器-only LLM适配用户表征学习至关重要。

Abstract: Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM.

</details>


### [110] [UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory](https://arxiv.org/abs/2602.10652)
*Yongshi Ye,Hui Jiang,Feihu Jiang,Tian Lan,Yichao Du,Biao Fu,Xiaodong Shi,Qianghuai Jia,Longyue Wang,Weihua Luo*

Main category: cs.CL

TL;DR: 本文提出UMEM框架，统一优化大语言模型的记忆提取与管理，通过语义邻域建模和基于邻域边际效用的GRPO奖励机制提升记忆泛化性，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将记忆提取视为静态过程，仅优化记忆管理，导致代理积累实例噪声而非鲁棒记忆，泛化能力差。

Method: 提出Unified Memory Extraction and Management (UMEM)框架，联合优化LLM进行记忆提取与管理；引入Semantic Neighborhood Modeling，并采用基于邻域边际效用的GRPO奖励进行训练。

Result: 在五个基准测试中显著超越强基线，多轮交互任务最高提升10.67%，且在持续演化中保持单调增长曲线。

Conclusion: UMEM通过联合优化与语义邻域建模，有效提升了自演化代理的记忆泛化性与持续学习能力。

Abstract: Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.

</details>


### [111] [Benchmarks Are Not That Out of Distribution: Word Overlap Predicts Performance](https://arxiv.org/abs/2602.10657)
*Woojin Chung,Jeonghoon Kim*

Main category: cs.CL

TL;DR: 本文研究了预训练数据质量与基准测试性能之间的关系，发现词频统计（特别是词级单字交叉熵）与零样本基准性能存在强逆相关性，表明许多标准基准测试相对于预训练语料分布偏移较小，仅靠简单词重叠即可预测性能。


<details>
  <summary>Details</summary>
Motivation: 理解高质量预训练数据的构成要素是语言模型训练中的核心问题；探究基准性能是否主要由预训练语料与评测数据间的统计模式重叠程度驱动。

Method: 使用词级单字交叉熵和词频统计衡量预训练语料与评测数据间的统计重叠；在10个零样本基准、4个不同规模（8.5B–60B tokens）预训练数据集、以及400M–3B参数模型上开展受控实验。

Result: 观察到词级单字交叉熵与基准性能呈稳健的负相关；相同交叉熵下，更大规模预训练子集带来更好下游结果；词频统计对基准得分有额外影响。

Conclusion: 多数标准基准测试相对于预训练语料仅弱OOD，其性能可被简单的词重叠统计较好预测，提示需构建更具挑战性的真正分布外评测基准。

Abstract: Understanding what constitutes high-quality pre-training data remains a central question in language model training. In this work, we investigate whether benchmark performance is primarily driven by the degree of statistical pattern overlap between pre-training corpora and evaluation datasets. We measure this overlap using word-level unigram cross-entropy and word frequency statistics, and perform controlled experiments across $10$ zero-shot benchmarks, $4$ pre-training datasets spanning $8.5\mathrm{B}$ to $60\mathrm{B}$ tokens, and model sizes ranging from $400\mathrm{M}$ to $3\mathrm{B}$ parameters. Our results demonstrate a robust inverse relationship between word-level unigram cross-entropy and benchmark performance, suggesting that widely used benchmarks are strongly influenced by word overlap between training and evaluation data. Thus, larger pre-training subsets with similar word-level unigram cross-entropy yield improved downstream results, indicating that word frequency statistics play an additional role in shaping benchmark scores. Taken together, these results suggest that many standard benchmarks are only weakly out-of-distribution relative to pre-training corpora, so that simple word-overlap statistics predict benchmark performance.

</details>


### [112] [Targeted Syntactic Evaluation of Language Models on Georgian Case Alignment](https://arxiv.org/abs/2602.10661)
*Daniel Gallagher,Gerhard Heyer*

Main category: cs.CL

TL;DR: 本文评估了基于Transformer的语言模型在格鲁吉亚语分裂作格系统中的表现，发现模型在分配作格时表现最差，而在分配主格时表现最佳，性能与三种格形式的频率分布（主格>与格>作格）相关。


<details>
  <summary>Details</summary>
Motivation: 评估Transformer语言模型在罕见的分裂作格系统（格鲁吉亚语）中的语法角色标记能力，尤其是主格、作格和与格的分配。

Method: 采用基于树库的方法，利用Grew查询语言生成最小对立对，构建包含370个句法测试样本的数据集（含7个任务，每任务50–70样本），每个样本测试三种名词格形式；评估5个编码器和2个解码器模型，使用词级/句级准确率指标。

Result: 所有模型在作格分配上表现最差，在主格分配上最强；性能与格形式频率分布（NOM > DAT > ERG）呈正相关；数据稀缺及作格功能高度特化是导致作格识别困难的关键原因。

Conclusion: 模型对低频且语法功能特殊的作格识别能力有限，凸显了针对低资源语言设计专用评测基准与数据增强方法的必要性；所构建数据集与方法为缺乏基准的语言提供了新的句法评估路径。

Abstract: This paper evaluates the performance of transformer-based language models on split-ergative case alignment in Georgian, a particularly rare system for assigning grammatical cases to mark argument roles. We focus on subject and object marking determined through various permutations of nominative, ergative, and dative noun forms. A treebank-based approach for the generation of minimal pairs using the Grew query language is implemented. We create a dataset of 370 syntactic tests made up of seven tasks containing 50-70 samples each, where three noun forms are tested in any given sample. Five encoder- and two decoder-only models are evaluated with word- and/or sentence-level accuracy metrics. Regardless of the specific syntactic makeup, models performed worst in assigning the ergative case correctly and strongest in assigning the nominative case correctly. Performance correlated with the overall frequency distribution of the three forms (NOM > DAT > ERG). Though data scarcity is a known issue for low-resource languages, we show that the highly specific role of the ergative along with a lack of available training data likely contributes to poor performance on this case. The dataset is made publicly available and the methodology provides an interesting avenue for future syntactic evaluations of languages where benchmarks are limited.

</details>


### [113] [Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents](https://arxiv.org/abs/2602.10715)
*Yifei Li,Weidong Guo,Lingling Zhang,Rongman Xu,Muye Huang,Hui Liu,Lijiao Xu,Yu Xu,Jun Liu*

Main category: cs.CL

TL;DR: 本文提出LoCoMo-Plus基准，用于评估大语言模型在长对话中对隐含用户状态、目标或价值观等潜在线索的记忆与应用能力，强调认知记忆而非表面事实回忆，并设计基于约束一致性的统一评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有对话记忆评估过于关注表面事实召回，忽视了真实对话中依赖隐含用户状态、目标或价值观等未显式提及的线索进行响应的需求。

Method: 构建LoCoMo-Plus基准，聚焦‘线索—触发语义断连’场景；提出基于约束一致性的新评估框架，替代传统字符串匹配和显式任务提示。

Result: 实验表明，当前主流模型、检索方法及记忆系统在认知记忆任务上表现不佳，其失败模式无法被现有基准检测到。

Conclusion: 认知记忆是长期对话系统的关键挑战，LoCoMo-Plus填补了隐含约束保持与应用能力评估的空白，推动更贴近真实交互的评测范式。

Abstract: Long-term conversational memory is a core capability for LLM-based dialogue systems, yet existing benchmarks and evaluation protocols primarily focus on surface-level factual recall. In realistic interactions, appropriate responses often depend on implicit constraints such as user state, goals, or values that are not explicitly queried later. To evaluate this setting, we introduce \textbf{LoCoMo-Plus}, a benchmark for assessing cognitive memory under cue--trigger semantic disconnect, where models must retain and apply latent constraints across long conversational contexts. We further show that conventional string-matching metrics and explicit task-type prompting are misaligned with such scenarios, and propose a unified evaluation framework based on constraint consistency. Experiments across diverse backbone models, retrieval-based methods, and memory systems demonstrate that cognitive memory remains challenging and reveals failures not captured by existing benchmarks. Our code and evaluation framework are publicly available at: https://github.com/xjtuleeyf/Locomo-Plus.

</details>


### [114] [Macaron: Controlled, Human-Written Benchmark for Multilingual and Multicultural Reasoning via Template-Filling](https://arxiv.org/abs/2602.10732)
*Alaa Elsetohy,Sama Hadhoud,Haryo Akbarianto Wibowo,Chenxi Whitehouse,Genta Indra Winata,Fajri Koto,Alham Fikri Aji*

Main category: cs.CL

TL;DR: Macaron 是一个模板优先的多语言基准测试，旨在评估模型在文化背景前提下的推理能力，通过解耦推理类型与文化维度，覆盖20种语言、10种文字和20个国家/文化背景，揭示当前多语言大模型在本地语言尤其是真/假任务上的显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准要么保留英语中心主义场景，要么缺乏对所需推理类型的控制，难以有效评估文化嵌入式推理能力。

Method: 提出 Macaron 基准：基于100个语言无关模板（涵盖7类推理、22种文化维度），由母语标注者构建英语与本地语言的多项选择题及系统生成的真假题；覆盖20国、10文字、20语言（含阿姆哈拉语、约鲁巴语等低资源语言）。

Result: 零样本评测21个多语言大模型发现：推理导向模型在英语与本地语言间表现接近均衡；开源权重模型在本地语言上显著退化，真假题常近随机水平；文化嵌入的数学与计数类模板始终最难。

Conclusion: Macaron 揭示了当前多语言大模型在文化敏感推理、尤其低资源语言上的关键短板，为更公平、更具文化代表性的评估提供了新范式。

Abstract: Multilingual benchmarks rarely test reasoning over culturally grounded premises: translated datasets keep English-centric scenarios, while culture-first datasets often lack control over the reasoning required. We propose Macaron, a template-first benchmark that factorizes reasoning type and cultural aspect across question languages. Using 100 language-agnostic templates that cover 7 reasoning types, 22 cultural aspects, native annotators create scenario-aligned English and local-language multiple-choice questions and systematically derived True/False questions. Macaron contains 11,862 instances spanning 20 countries/cultural contexts, 10 scripts, and 20 languages (including low-resource ones like Amharic, Yoruba, Zulu, Kyrgyz, and some Arabic dialects). In zero-shot evaluation of 21 multilingual LLMs, reasoning-mode models achieve the strongest performance and near-parity between English and local languages, while open-weight models degrade substantially in local languages and often approach chance on T/F tasks. Culture-grounded mathematical and counting templates are consistently the hardest. The data can be accessed here https://huggingface.co/datasets/AlaaAhmed2444/Macaron.

</details>


### [115] [Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs](https://arxiv.org/abs/2602.10740)
*Yuming Yan,Shuo Yang,Kai Tang,Sihong Chen,Yang Zhang,Ke Xu,Dan Hu,Qun Yu,Pengfei Hu,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: 本文提出了一种名为Reinforced Curriculum Pre-Alignment (RCPA) 的新范式，用于在不损害VLM通用能力的前提下实现高效领域自适应。该方法通过分阶段的渐进式调制机制（先部分约束输出、再全生成优化），缓解灾难性遗忘与优化崩溃问题，在多个专业领域和通用基准上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models (VLMs) 在通用任务上表现优异，但在医学影像、几何推理等专业领域性能不足；监督微调易导致灾难性遗忘，而持续预训练对VLM又因计算开销大、数据不可得而难以实施；现有强化学习方法（如GRPO）在初始领域知识匮乏时易发生优化崩溃。

Method: 提出Reinforced Curriculum Pre-Alignment (RCPA)，一种基于课程感知的渐进式后训练范式：早期施加部分输出约束以安全引入领域概念；随模型领域熟悉度提升，逐步过渡到全生成优化，精细调整响应并契合领域偏好。

Result: 在多个专业领域（如医学、几何）及通用基准测试中，RCPA显著提升领域性能，同时较好保持原有通用多模态能力，优于SFT、GRPO等基线方法。

Conclusion: RCPA为构建高性能且可领域自适应的VLM提供了实用可行的新路径，兼顾领域专精性与通用能力保留。

Abstract: Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target domain, but it typically causes catastrophic forgetting, limiting its generalization. The central challenge, therefore, is to adapt VLMs to new domains while preserving their general-purpose capabilities. Continual pretraining is effective for expanding knowledge in Large Language Models (LLMs), but it is less feasible for VLMs due to prohibitive computational costs and the unavailability of pretraining data for most open-source models. This necessitates efficient post-training adaptation methods. Reinforcement learning (RL)-based approaches such as Group Relative Policy Optimization (GRPO) have shown promise in preserving general abilities, yet they often fail in domain adaptation scenarios where the model initially lacks sufficient domain knowledge, leading to optimization collapse. To bridge this gap, we propose Reinforced Curriculum Pre-Alignment (RCPA), a novel post-training paradigm that introduces a curriculum-aware progressive modulation mechanism. In the early phase, RCPA applies partial output constraints to safely expose the model to new domain concepts. As the model's domain familiarity increases, training gradually transitions to full generation optimization, refining responses and aligning them with domain-specific preferences. This staged adaptation balances domain knowledge acquisition with the preservation of general multimodal capabilities. Extensive experiments across specialized domains and general benchmarks validate the effectiveness of RCPA, establishing a practical pathway toward building high-performing and domain-adaptive VLMs.

</details>


### [116] [Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM](https://arxiv.org/abs/2602.10801)
*Haotian Sheng,Heyong Wang,Ming Hong,Hongman He,Junqiu Liu*

Main category: cs.CL

TL;DR: 本文提出LSCL方法，通过知识蒸馏框架，利用黑盒大语言模型的输入问题、输出答案和词元概率，构建其内部知识状态映射，从而量化并表达其知识边界，有效缓解幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在幻觉问题，核心原因在于其缺乏对自身内部知识边界的认知；现有研究多针对白盒模型，而适用于仅提供API访问的黑盒模型的方法尚属空白。

Method: 提出LSCL（LLM-Supervised Confidence Learning），基于知识蒸馏框架，设计深度学习模型，以黑盒LLM的输入问题、输出答案及token概率为输入，建模其内部知识状态，实现知识边界量化表达；并针对不支持token概率的LLM，提出性能相近的自适应替代方法。

Result: 在多个公开数据集和主流黑盒LLM上实验表明，LSCL在准确率、召回率等指标上显著优于现有基线模型；替代方法性能接近LSCL且仍优于基线。

Conclusion: LSCL为黑盒大语言模型提供了可扩展、实用的知识边界表达方案，有助于缓解幻觉，提升实际应用可靠性。

Abstract: Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications. The core cause of hallucination lies in LLMs' lack of awareness regarding their stored internal knowledge, preventing them from expressing their knowledge state on questions beyond their internal knowledge boundaries, as humans do. However, existing research on knowledge boundary expression primarily focuses on white-box LLMs, leaving methods suitable for black-box LLMs which offer only API access without revealing internal parameters-largely unexplored. Against this backdrop, this paper proposes LSCL (LLM-Supervised Confidence Learning), a deep learning-based method for expressing the knowledge boundaries of black-box LLMs. Based on the knowledge distillation framework, this method designs a deep learning model. Taking the input question, output answer, and token probability from a black-box LLM as inputs, it constructs a mapping between the inputs and the model' internal knowledge state, enabling the quantification and expression of the black-box LLM' knowledge boundaries. Experiments conducted on diverse public datasets and with multiple prominent black-box LLMs demonstrate that LSCL effectively assists black-box LLMs in accurately expressing their knowledge boundaries. It significantly outperforms existing baseline models on metrics such as accuracy and recall rate. Furthermore, considering scenarios where some black-box LLMs do not support access to token probability, an adaptive alternative method is proposed. The performance of this alternative approach is close to that of LSCL and surpasses baseline models.

</details>


### [117] [Beyond Confidence: The Rhythms of Reasoning in Generative Models](https://arxiv.org/abs/2602.10816)
*Deyuan Liu,Zecheng Wang,Zhanyue Qin,Zhiying Tu,Dianhui Chu,Dianbo Sui*

Main category: cs.CL

TL;DR: 本文提出了一种新指标Token Constraint Bound (δ_TCB)，用于量化大语言模型在内部状态扰动下保持主导词预测稳定的能力，揭示了模型预测的局部鲁棒性，并弥补了传统指标（如困惑度）的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标（如准确率、困惑度）无法有效衡量大语言模型在输入微小变化下的局部预测鲁棒性，因归一化输出概率可能掩盖其内部状态对扰动的真实韧性。

Method: 提出Token Constraint Bound（δ_TCB）指标，基于输出嵌入空间几何结构，量化模型内部状态所能承受的最大扰动幅度，使其主导下一词预测不发生显著变化。

Result: 实验表明δ_TCB与有效提示工程相关，并能发现困惑度所忽略的关键预测不稳定性，尤其在上下文学习和文本生成中。

Conclusion: δ_TCB为分析和提升大语言模型在上下文中的预测稳定性提供了原理清晰、互补性强的新工具。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability. Conventional metrics like accuracy and perplexity fail to assess local prediction robustness, as normalized output probabilities can obscure the underlying resilience of an LLM's internal state to perturbations. We introduce the Token Constraint Bound ($δ_{\mathrm{TCB}}$), a novel metric that quantifies the maximum internal state perturbation an LLM can withstand before its dominant next-token prediction significantly changes. Intrinsically linked to output embedding space geometry, $δ_{\mathrm{TCB}}$ provides insights into the stability of the model's internal predictive commitment. Our experiments show $δ_{\mathrm{TCB}}$ correlates with effective prompt engineering and uncovers critical prediction instabilities missed by perplexity during in-context learning and text generation. $δ_{\mathrm{TCB}}$ offers a principled, complementary approach to analyze and potentially improve the contextual stability of LLM predictions.

</details>


### [118] [I can tell whether you are a Native Hawlêri Speaker! How ANN, CNN, and RNN perform in NLI-Native Language Identification](https://arxiv.org/abs/2602.10832)
*Hardi Garari,Hossein Hassani*

Main category: cs.CL

TL;DR: This paper addresses Native Language Identification (NLI) for Hewlêri, a subdialect of Sorani Kurdish — a less-resourced language — using speech data and neural models (ANN, CNN, RNN), achieving 95.92% accuracy with RNN on 5-second segments.


<details>
  <summary>Details</summary>
Motivation: There is a significant research gap in Native Language Identification (NLI) for dialects and subdialects, especially in less-resourced languages like Kurdish; this work targets Hewlêri, a Sorani Kurdish subdialect, to fill that gap.

Method: Collected ~24 hours of speech from 40 native/non-native Hewlêri speakers; built and evaluated three neural models (ANN, CNN, RNN) across 66 experiments varying time-frame (1–60 sec), sampling strategies (undersampling/oversampling), and cross-validation.

Result: RNN achieved the highest accuracy of 95.92% on 5-second audio segments under 80:10:10 train/validation/test split; the study also produced the first publicly available speech dataset for Hewlêri subdialect NLI.

Conclusion: Neural models — especially RNN — are highly effective for subdialect-level NLI in low-resource settings, and the newly created Hewlêri speech dataset enables future linguistic, forensic, and NLP research.

Abstract: Native Language Identification (NLI) is a task in Natural Language Processing (NLP) that typically determines the native language of an author through their writing or a speaker through their speaking. It has various applications in different areas, such as forensic linguistics and general linguistics studies. Although considerable research has been conducted on NLI regarding two different languages, such as English and German, the literature indicates a significant gap regarding NLI for dialects and subdialects. The gap becomes wider in less-resourced languages such as Kurdish. This research focuses on NLI within the context of a subdialect of Sorani (Central) Kurdish. It aims to investigate the NLI for Hewlêri, a subdialect spoken in Hewlêr (Erbil), the Capital of the Kurdistan Region of Iraq. We collected about 24 hours of speech by recording interviews with 40 native or non-native Hewlêri speakers, 17 female and 23 male. We created three Neural Network-based models: Artificial Neural Network (ANN), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), which were evaluated through 66 experiments, covering various time-frames from 1 to 60 seconds, undersampling, oversampling, and cross-validation. The RNN model showed the highest accuracy of 95.92% for 5-second audio segmentation, using an 80:10:10 data splitting scheme. The created dataset is the first speech dataset for NLI on the Hewlêri subdialect in the Sorani Kurdish dialect, which can be of benefit to various research areas.

</details>


### [119] [C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution](https://arxiv.org/abs/2602.10874)
*Binwei Yan,Yifei Fu,Mingjian Zhu,Hanting Chen,Mingxuan Yuan,Yunhe Wang,Hailin Hu*

Main category: cs.CL

TL;DR: 本文提出C-MOP框架，通过边界感知对比采样（BACS）和动量引导语义聚类（MGSC）提升大语言模型提示优化的稳定性与效果，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法常受噪声和冲突更新信号干扰，需更稳定、精准的优化机制。

Method: 提出C-MOP框架，包含Boundary-Aware Contrastive Sampling（BACS）用于挖掘硬负样本、锚点与边界对，以及Momentum-Guided Semantic Clustering（MGSC）引入带时间衰减的文本动量机制以缓解语义冲突。

Result: 在多个任务上持续超越PromptWizard、ProTeGi等SOTA方法，平均提升1.58%和3.35%；3B通用LLM经C-MOP优化后性能超越70B领域专用稠密模型。

Conclusion: C-MOP通过结构化采样与动量引导聚类有效提升提示进化精度与鲁棒性，为轻量高效提示优化提供了新范式。

Abstract: Automatic prompt optimization is a promising direction to boost the performance of Large Language Models (LLMs). However, existing methods often suffer from noisy and conflicting update signals. In this research, we propose C-MOP (Cluster-based Momentum Optimized Prompting), a framework that stabilizes optimization via Boundary-Aware Contrastive Sampling (BACS) and Momentum-Guided Semantic Clustering (MGSC). Specifically, BACS utilizes batch-level information to mine tripartite features--Hard Negatives, Anchors, and Boundary Pairs--to precisely characterize the typical representation and decision boundaries of positive and negative prompt samples. To resolve semantic conflicts, MGSC introduces a textual momentum mechanism with temporal decay that distills persistent consensus from fluctuating gradients across iterations. Extensive experiments demonstrate that C-MOP consistently outperforms SOTA baselines like PromptWizard and ProTeGi, yielding average gains of 1.58% and 3.35%. Notably, C-MOP enables a general LLM with 3B activated parameters to surpass a 70B domain-specific dense LLM, highlighting its effectiveness in driving precise prompt evolution. The code is available at https://github.com/huawei-noah/noah-research/tree/master/C-MOP.

</details>


### [120] [Diagnosing Structural Failures in LLM-Based Evidence Extraction for Meta-Analysis](https://arxiv.org/abs/2602.10881)
*Zhiyin Tan,Jennifer D'Souza*

Main category: cs.CL

TL;DR: 本文提出了一种结构化诊断框架，评估大语言模型（LLMs）在系统性综述与Meta分析中进行证据提取的能力，发现当前LLM在保持变量角色、统计方法与效应量之间稳定绑定方面存在系统性结构缺陷，难以满足自动化Meta分析所需的结构保真度、关系绑定和数值准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型快速发展，但其是否能满足系统性综述与Meta分析中对结构化证据提取的严格要求（如角色、方法、效应量归属的准确保持）尚不明确。

Method: 提出一种结构化、诊断性评估框架，将LLM证据提取建模为一系列逐步增加关系与数值复杂度的模式约束查询；基于跨五个科学领域的手工标注语料库、统一查询套件与评估协议，在单文档与长上下文多文档输入下评测两类SOTA LLM。

Result: 单属性提取性能中等，但涉及变量角色、统计方法与效应量稳定绑定的任务性能急剧下降；Meta分析关联元组提取可靠性近零；长上下文进一步加剧错误；下游聚合显著放大上游微小误差；错误主因是结构性失败（如角色反转、跨分析绑定漂移、密集结果压缩、数值错配），而非实体识别错误。

Conclusion: 当前LLM缺乏自动化Meta分析所必需的结构保真度、关系绑定能力和数值接地能力，需针对性改进结构推理机制。

Abstract: Systematic reviews and meta-analyses rely on converting narrative articles into structured, numerically grounded study records. Despite rapid advances in large language models (LLMs), it remains unclear whether they can meet the structural requirements of this process, which hinge on preserving roles, methods, and effect-size attribution across documents rather than on recognizing isolated entities. We propose a structural, diagnostic framework that evaluates LLM-based evidence extraction as a progression of schema-constrained queries with increasing relational and numerical complexity, enabling precise identification of failure points beyond atom-level extraction. Using a manually curated corpus spanning five scientific domains, together with a unified query suite and evaluation protocol, we evaluate two state-of-the-art LLMs under both per-document and long-context, multi-document input regimes. Across domains and models, performance remains moderate for single-property queries but degrades sharply once tasks require stable binding between variables, roles, statistical methods, and effect sizes. Full meta-analytic association tuples are extracted with near-zero reliability, and long-context inputs further exacerbate these failures. Downstream aggregation amplifies even minor upstream errors, rendering corpus-level statistics unreliable. Our analysis shows that these limitations stem not from entity recognition errors, but from systematic structural breakdowns, including role reversals, cross-analysis binding drift, instance compression in dense result sections, and numeric misattribution, indicating that current LLMs lack the structural fidelity, relational binding, and numerical grounding required for automated meta-analysis. The code and data are publicly available at GitHub (https://github.com/zhiyintan/LLM-Meta-Analysis).

</details>


### [121] [The CLEF-2026 FinMMEval Lab: Multilingual and Multimodal Evaluation of Financial AI Systems](https://arxiv.org/abs/2602.10886)
*Zhuohan Xie,Rania Elbadry,Fan Zhang,Georgi Georgiev,Xueqing Peng,Lingfei Qian,Jimin Huang,Dimitar Dimitrov,Vanshikaa Jani,Yuyang Dai,Jiahui Geng,Yuxia Wang,Ivan Koychev,Veselin Stoyanov,Preslav Nakov*

Main category: cs.CL

TL;DR: FinMMEval Lab at CLEF 2026 introduces the first multilingual and multimodal benchmark for evaluating financial LLMs, featuring three tasks—Financial Exam QA, PolyFiQA, and Financial Decision Making—to assess reasoning, generalization, and action across languages and modalities.


<details>
  <summary>Details</summary>
Motivation: Existing financial NLP benchmarks are largely monolingual, text-only, and narrow in scope, failing to reflect real-world multilingual and multimodal financial AI needs.

Method: Design and release of a new multilingual, multimodal evaluation framework comprising three interconnected tasks: Financial Exam Question Answering, Multilingual Financial Question Answering (PolyFiQA), and Financial Decision Making.

Result: A comprehensive, publicly available evaluation suite enabling robust, transparent, and globally inclusive assessment of financial LLMs across languages and modalities.

Conclusion: FinMMEval 2026 bridges a critical gap in financial AI evaluation and fosters reproducible, equitable advancement of multilingual and multimodal financial language models.

Abstract: We present the setup and the tasks of the FinMMEval Lab at CLEF 2026, which introduces the first multilingual and multimodal evaluation framework for financial Large Language Models (LLMs). While recent advances in financial natural language processing have enabled automated analysis of market reports, regulatory documents, and investor communications, existing benchmarks remain largely monolingual, text-only, and limited to narrow subtasks. FinMMEval 2026 addresses this gap by offering three interconnected tasks that span financial understanding, reasoning, and decision-making: Financial Exam Question Answering, Multilingual Financial Question Answering (PolyFiQA), and Financial Decision Making. Together, these tasks provide a comprehensive evaluation suite that measures models' ability to reason, generalize, and act across diverse languages and modalities. The lab aims to promote the development of robust, transparent, and globally inclusive financial AI systems, with datasets and evaluation resources publicly released to support reproducible research.

</details>


### [122] [SoftMatcha 2: A Fast and Soft Pattern Matcher for Trillion-Scale Corpora](https://arxiv.org/abs/2602.10908)
*Masataka Yoneda,Yusuke Matsushita,Go Kamoda,Kohei Suenaga,Takuya Akiba,Masaki Waga,Sho Yokoi*

Main category: cs.CL

TL;DR: 本文提出了一种超快速、灵活的自然语言万亿级语料库搜索算法，基于后缀数组实现，在0.3秒内完成搜索，并支持语义变化（替换、插入、删除）；通过磁盘感知的精确查找与语料库感知的动态剪枝，有效抑制搜索空间随查询长度的指数增长；实验表明其搜索延迟显著低于现有方法，并成功识别出其他方法未能发现的基准污染。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在保持低延迟的同时处理万亿级语料库中的语义变体查询，且易受组合爆炸影响，亟需兼顾速度、规模与语义鲁棒性的新搜索范式。

Method: 基于后缀数组的字符串匹配算法，结合磁盘感知的快速精确查找和动态语料库感知剪枝策略，并利用自然语言统计特性从理论上抑制搜索空间的指数增长。

Result: 在FineWeb-Edu（1.4T tokens）上，搜索延迟显著低于infini-gram、infini-gram mini和SoftMatcha；成功检测到先前未识别的基准污染；支持七语言在线软搜索演示。

Conclusion: 该方法实现了万亿级语料库上亚秒级、语义鲁棒的高效搜索，兼具理论保证与实用价值，为大模型数据清洗与检索提供了新工具。

Abstract: We present an ultra-fast and flexible search algorithm that enables search over trillion-scale natural language corpora in under 0.3 seconds while handling semantic variations (substitution, insertion, and deletion). Our approach employs string matching based on suffix arrays that scales well with corpus size. To mitigate the combinatorial explosion induced by the semantic relaxation of queries, our method is built on two key algorithmic ideas: fast exact lookup enabled by a disk-aware design, and dynamic corpus-aware pruning. We theoretically show that the proposed method suppresses exponential growth in the search space with respect to query length by leveraging statistical properties of natural language. In experiments on FineWeb-Edu (Lozhkov et al., 2024) (1.4T tokens), we show that our method achieves significantly lower search latency than existing methods: infini-gram (Liu et al., 2024), infini-gram mini (Xu et al., 2025), and SoftMatcha (Deguchi et al., 2025). As a practical application, we demonstrate that our method identifies benchmark contamination in training corpora, unidentified by existing approaches. We also provide an online demo of fast, soft search across corpora in seven languages.

</details>


### [123] [Computational Phenomenology of Temporal Experience in Autism: Quantifying the Emotional and Narrative Characteristics of Lived Unpredictability](https://arxiv.org/abs/2602.10947)
*Kacper Dudzic,Karolina Drożdż,Maciej Wodziński,Anastazja Szuła,Marcin Moskalewicz*

Main category: cs.CL

TL;DR: 本研究通过整合现象学访谈、计算语言分析和叙事流分析三种方法，探讨自闭症个体的时间性障碍，发现其核心问题在于经验的不可预测性，而非叙事建构能力缺陷。


<details>
  <summary>Details</summary>
Motivation: 针对当前自闭症时间性研究中医学缺陷模型主导、质性样本量小、计算研究缺乏现象学基础等局限，本研究旨在弥合现象学与计算方法之间的鸿沟，并克服样本限制。

Method: 采用三阶段混合方法：A）使用跨诊断时间体验评估工具对自闭症个体进行结构化现象学访谈；B）构建并计算分析专用于本研究的自闭症自传语料库；C）复现一项基于叙事流指标评估自闭症自传现象学真实性的计算研究。

Result: 现象学访谈显示，自闭症组与对照组最显著差异在于经验的不可预测性；计算分析发现自闭症叙事中时间词汇整体负向情感更强，尤其‘即时性与突发性’类别；异常值分析识别出‘不可预测地’‘陡然地’‘突然地’等表征断裂感的词高度负向；叙事流分析表明自闭症自传在结构上更接近真实自传而非虚构故事。

Conclusion: 自闭症个体所经历的时间性挑战主要源于生活经验内容本身的不可预测性，而非其叙事建构能力的缺陷，这挑战了传统的缺陷导向模型，支持以现象学为基础的神经多样性视角。

Abstract: Disturbances in temporality, such as desynchronization with the social environment and its unpredictability, are considered core features of autism with a deep impact on relationships. However, limitations regarding research on this issue include: 1) the dominance of deficit-based medical models of autism, 2) sample size in qualitative research, and 3) the lack of phenomenological anchoring in computational research. To bridge the gap between phenomenological and computational approaches and overcome sample-size limitations, our research integrated three methodologies. Study A: structured phenomenological interviews with autistic individuals using the Transdiagnostic Assessment of Temporal Experience. Study B: computational analysis of an autobiographical corpus of autistic narratives built for this purpose. Study C: a replication of a computational study using narrative flow measures to assess the perceived phenomenological authenticity of autistic autobiographies. Interviews revealed that the most significant differences between the autistic and control groups concerned unpredictability of experience. Computational results mirrored these findings: the temporal lexicon in autistic narratives was significantly more negatively valenced - particularly the "Immediacy & Suddenness" category. Outlier analysis identified terms associated with perceived discontinuity (unpredictably, precipitously, and abruptly) as highly negative. The computational analysis of narrative flow found that the autistic narratives contained within the corpus quantifiably resemble autobiographical stories more than imaginary ones. Overall, the temporal challenges experienced by autistic individuals were shown to primarily concern lived unpredictability and stem from the contents of lived experience, and not from autistic narrative construction.

</details>


### [124] [Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models](https://arxiv.org/abs/2602.10953)
*Mingyu Cao,Alvaro Correia,Christos Louizos,Shiwei Liu,Lu Yin*

Main category: cs.CL

TL;DR: SOAR是一种无需训练的解码算法，通过根据模型不确定性动态调整解码策略（低置信度时拓宽搜索、高置信度时并行解码），在保持推理速度的同时提升扩散语言模型在数学推理与代码生成任务上的生成质量。


<details>
  <summary>Details</summary>
Motivation: 标准贪心解码易因局部最优选择导致次优的去噪顺序，尤其在需复杂推理的任务中表现不佳。

Method: 提出SOAR解码算法：依据模型对各位置预测的置信度自适应调整——低置信度时扩展候选解码路径以避免过早确定；高置信度时并行解码多个位置以减少迭代步数。

Result: 在GSM8K、MBPP、HumanEval等数学推理与代码生成基准上，SOAR在Dream-7B和LLaDA-8B模型上均提升了生成质量，同时维持有竞争力的推理速度。

Conclusion: SOAR提供了一种实用、无需训练的解码方案，在扩散语言模型中有效权衡生成质量与推理效率。

Abstract: Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice can lock the model into a suboptimal unmasking order, especially on reasoning-heavy prompts. We present SOAR, a training-free decoding algorithm that adapts its behavior to the model's uncertainty. When confidence is low, SOAR briefly widens the search over alternative unmasking decisions to avoid premature commitments; when confidence is high, it collapses the search and decodes many positions in parallel to reduce the number of denoising iterations. Across mathematical reasoning and code generation benchmarks (GSM8K, MBPP, HumanEval) on Dream-7B and LLaDA-8B, SOAR improves generation quality while maintaining competitive inference speed, offering a practical way to balance quality and efficiency in DLM decoding.

</details>


### [125] [LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules](https://arxiv.org/abs/2602.10993)
*Ivan Vulić,Adam Grycner,Quentin de Laroussilhe,Jonas Pfeiffer*

Main category: cs.CL

TL;DR: 本文提出LoRA-Squeeze方法，通过先以高秩训练再压缩（后验或动态降秩）来改进标准LoRA，利用RSVD实现高效低秩适配，在多项任务上显著提升参数效率与性能权衡。


<details>
  <summary>Details</summary>
Motivation: 标准LoRA面临最优秩预选困难、超参依赖性强、异构秩模块部署复杂等问题，亟需更灵活、鲁棒的秩自适应机制。

Method: LoRA-Squeeze：先以较高源秩微调模型，重建全量权重更新矩阵，再用随机化奇异值分解（RSVD）压缩为更低目标秩的LoRA模块；支持后验压缩与训练中渐进式秩退火两种策略。

Result: 在13个文本和10个视觉语言任务上验证，后验压缩所得低秩适配器常优于直接在目标秩下训练的结果；加入少量目标秩微调可进一步提升；渐进式秩退火变体持续取得最优大小-性能权衡。

Conclusion: 先高秩学习再压缩优于直接低秩优化，LoRA-Squeeze提供简单、通用且高效的LoRA增强范式，缓解了秩选择与部署难题，提升了PEFT实用性。

Abstract: Despite its huge number of variants, standard Low-Rank Adaptation (LoRA) is still a dominant technique for parameter-efficient fine-tuning (PEFT). Nonetheless, it faces persistent challenges, including the pre-selection of an optimal rank and rank-specific hyper-parameters, as well as the deployment complexity of heterogeneous-rank modules and more sophisticated LoRA derivatives. In this work, we introduce LoRA-Squeeze, a simple and efficient methodology that aims to improve standard LoRA learning by changing LoRA module ranks either post-hoc or dynamically during training}. Our approach posits that it is better to first learn an expressive, higher-rank solution and then compress it, rather than learning a constrained, low-rank solution directly. The method involves fine-tuning with a deliberately high(er) source rank, reconstructing or efficiently approximating the reconstruction of the full weight update matrix, and then using Randomized Singular Value Decomposition (RSVD) to create a new, compressed LoRA module at a lower target rank. Extensive experiments across 13 text and 10 vision-language tasks show that post-hoc compression often produces lower-rank adapters that outperform those trained directly at the target rank, especially if a small number of fine-tuning steps at the target rank is allowed. Moreover, a gradual, in-tuning rank annealing variant of LoRA-Squeeze consistently achieves the best LoRA size-performance trade-off.

</details>


### [126] [Linguistic Indicators of Early Cognitive Decline in the DementiaBank Pitt Corpus: A Statistical and Machine Learning Study](https://arxiv.org/abs/2602.11028)
*Artsvik Avetisyan,Sachin Kumar*

Main category: cs.CL

TL;DR: 本研究利用DementiaBank Pitt语料库的自发语言转录文本，通过三种语言表征（原始清理文本、词性增强表征、仅词性句法表征）结合逻辑回归与随机森林模型，评估其在痴呆早期识别中的有效性，并通过可解释性分析与非参数统计验证，证实抽象语言特征（如功能词使用、词汇多样性、句法结构等）是稳健且临床可解释的认知衰退标志。


<details>
  <summary>Details</summary>
Motivation: 寻找可解释、临床可落地的语言学指标以支持阿尔茨海默病等认知障碍的早期透明筛查。

Method: 基于DementiaBank Pitt语料库，采用三种语言表征（原始文本、POS增强、POS-only），结合逻辑回归和随机森林模型，在transcript-level和subject-level两种划分方式下评估性能；使用全局特征重要性分析模型可解释性，并以Mann-Whitney U检验与Cliff's delta进行统计验证。

Result: 各表征下模型性能稳定，尤其POS-enhanced和POS-only表征在subject-level交叉验证中表现更稳健；统计分析与ML特征重要性高度一致，显著差异体现在功能词使用、词汇多样性、句子结构与语篇连贯性上。

Conclusion: 抽象语言特征（不依赖具体词汇）能稳健反映早期认知衰退，结合可解释机器学习与非参数统计验证，为语言驱动的认知筛查提供了透明、可靠且语言学基础坚实的路径。

Abstract: Background: Subtle changes in spontaneous language production are among the earliest indicators of cognitive decline. Identifying linguistically interpretable markers of dementia can support transparent and clinically grounded screening approaches.
  Methods: This study analyzes spontaneous speech transcripts from the DementiaBank Pitt Corpus using three linguistic representations: raw cleaned text, a part-of-speech (POS)-enhanced representation combining lexical and grammatical information, and a POS-only syntactic representation. Logistic regression and random forest models were evaluated under two protocols: transcript-level train-test splits and subject-level five-fold cross-validation to prevent speaker overlap. Model interpretability was examined using global feature importance, and statistical validation was conducted using Mann-Whitney U tests with Cliff's delta effect sizes.
  Results: Across representations, models achieved stable performance, with syntactic and grammatical features retaining strong discriminative power even in the absence of lexical content. Subject-level evaluation yielded more conservative but consistent results, particularly for POS-enhanced and POS-only representations. Statistical analysis revealed significant group differences in functional word usage, lexical diversity, sentence structure, and discourse coherence, aligning closely with machine learning feature importance findings.
  Conclusion: The results demonstrate that abstract linguistic features capture robust markers of early cognitive decline under clinically realistic evaluation. By combining interpretable machine learning with non-parametric statistical validation, this study supports the use of linguistically grounded features for transparent and reliable language-based cognitive screening.

</details>


### [127] [Language Model Inversion through End-to-End Differentiation](https://arxiv.org/abs/2602.11044)
*Kevin Yandoka Denamganaï,Kartic Subr*

Main category: cs.CL

TL;DR: 本文提出了一种基于梯度优化的方法，通过将语言模型视为作用于词元分布序列的函数，实现对冻结语言模型的端到端可微，并高效反演生成目标输出所需的提示词。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少分析语言模型的可逆性，即给定目标输出，如何找到能生成该输出的输入提示，这一问题尚未解决。

Method: 将语言模型建模为作用于词元分布序列的函数，构建端到端可微框架，并利用梯度下降优化提示词。

Result: 在多个白盒语言模型上验证了方法的有效性，能可靠高效地优化长度为10和80的提示词，以生成长度为20的目标输出。

Conclusion: 该方法为语言模型的可逆性分析提供了新视角和实用工具，拓展了对LM内部机制的理解与可控性。

Abstract: Despite emerging research on Language Models (LM), few approaches analyse the invertibility of LMs. That is, given a LM and a desirable target output sequence of tokens, determining what input prompts would yield the target output remains an open problem. We formulate this problem as a classical gradient-based optimisation. First, we propose a simple algorithm to achieve end-to-end differentiability of a given (frozen) LM and then find optimised prompts via gradient descent. Our central insight is to view LMs as functions operating on sequences of distributions over tokens (rather than the traditional view as functions on sequences of tokens). Our experiments and ablations demonstrate that our DLM-powered inversion can reliably and efficiently optimise prompts of lengths $10$ and $80$ for targets of length $20$, for several white-box LMs (out-of-the-box).

</details>


### [128] [Embedding Inversion via Conditional Masked Diffusion Language Models](https://arxiv.org/abs/2602.11047)
*Han Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种将嵌入反转建模为条件掩码扩散的方法，通过迭代去噪并行恢复所有token，而非顺序自回归生成。该方法使用自适应层归一化对目标嵌入进行条件化，在仅8次前向传播、无需访问目标编码器的情况下，实现了较高的token准确率和余弦相似度。


<details>
  <summary>Details</summary>
Motivation: 传统嵌入反转方法多依赖自回归生成，效率低且难以并行；本文旨在探索更高效、并行化的嵌入重构范式。

Method: 将嵌入反转建模为条件掩码扩散过程，采用带自适应层归一化的掩码扩散语言模型，以目标嵌入为条件进行迭代去噪。

Result: 在32-token序列及三种嵌入模型上，达到81.3%的token准确率和0.87的余弦相似度，仅需8次前向传播，模型参数量为78M且不访问目标编码器。

Conclusion: 条件掩码扩散是一种高效、可扩展的嵌入反转新范式，显著优于传统自回归方法，在准确率与计算效率间取得良好平衡。

Abstract: We frame embedding inversion as conditional masked diffusion, recovering all tokens in parallel through iterative denoising rather than sequential autoregressive generation. A masked diffusion language model is conditioned on the target embedding via adaptive layer normalization, requiring only 8 forward passes through a 78M parameter model with no access to the target encoder. On 32-token sequences across three embedding models, the method achieves 81.3% token accuracy and 0.87 cosine similarity.

</details>


### [129] [Conversational Behavior Modeling Foundation Model With Multi-Level Perception](https://arxiv.org/abs/2602.11065)
*Dingkun Zhou,Shuchang Pan,Jiachen Lian,Siddharth Banerjee,Sarika Pasumarthy,Dhruv Hebbar,Siddhant Patel,Zeyi Austin Li,Kan Jen Cheng,Sanay Bordia,Krish Patel,Akshaj Gupta,Tingle Li,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: 本文提出了一种基于图思维（Graph-of-Thoughts, GoT）的多层级感知与推理框架，用于建模人类对话中隐含的意图-行为链，支持全双工交互系统中的可解释行为预测。


<details>
  <summary>Details</summary>
Motivation: 人类对话具有隐含的思维链和时序性言语行为，现有系统难以建模其因果与时间依赖关系，阻碍自然全双工交互。

Method: 提出多级感知+图思维（GoT）框架，结合分层标注体系（高层意图+低层言语行为），构建动态演化的预测图；使用可控、事件丰富的对话数据集及人工标注进行训练；Transformer模型据此预测言语行为、生成理由并动态修正推理。

Result: 在合成与真实全双工对话数据上验证了该框架在行为检测鲁棒性、推理链可解释性方面的优势，并为全双工语音对话系统的推理能力评测奠定基础。

Conclusion: GoT框架有效建模了对话中意图到行为的感知-推理路径，提升了全双工交互系统的自然性与可解释性，是迈向认知对齐对话系统的重要一步。

Abstract: Human conversation is organized by an implicit chain of thoughts that manifests as timed speech acts. Capturing this perceptual pathway is key to building natural full-duplex interactive systems. We introduce a framework that models this process as multi-level perception, and then reasons over conversational behaviors via a Graph-of-Thoughts (GoT). Our approach formalizes the intent-to-action pathway with a hierarchical labeling scheme, predicting high-level communicative intents and low-level speech acts to learn their causal and temporal dependencies. To train this system, we develop a high quality corpus that pairs controllable, event-rich dialogue data with human-annotated labels. The GoT framework structures streaming predictions as an evolving graph, enabling a transformer to forecast the next speech act, generate concise justifications for its decisions, and dynamically refine its reasoning. Experiments on both synthetic and real duplex dialogues show that the framework delivers robust behavior detection, produces interpretable reasoning chains, and establishes a foundation for benchmarking conversational reasoning in full duplex spoken dialogue systems.

</details>


### [130] [Simultaneous Speech-to-Speech Translation Without Aligned Data](https://arxiv.org/abs/2602.11072)
*Tom Labiausse,Romain Fabre,Yannick Estève,Alexandre Défossez,Neil Zeghidour*

Main category: cs.CL

TL;DR: Hibiki-Zero是一种无需词级对齐的实时语音翻译方法，通过句子级监督与GRPO强化学习联合优化准确率与延迟，在多语言任务中达到SOTA，并支持低资源语言快速适配。


<details>
  <summary>Details</summary>
Motivation: 传统同步语音翻译依赖难以大规模获取的词级对齐数据，或使用次优的语言特定启发式合成对齐，限制了多语言扩展能力。

Method: 提出Hibiki-Zero：先在句子级对齐数据上训练高延迟语音翻译模型，再用基于GRPO的强化学习联合优化延迟与翻译质量，完全摒弃词级对齐。

Result: 在五个X-to-English任务上实现翻译准确率、延迟、音色迁移和自然度的SOTA；仅需<1000小时语音即可适配新语言；发布45小时多语种评测基准、模型权重与推理代码。

Conclusion: Hibiki-Zero消除了词级对齐依赖，简化训练流程，显著提升多语言可扩展性与低资源适应性，为实际部署的同步语音翻译提供了更鲁棒、更通用的解决方案。

Abstract: Simultaneous speech translation requires translating source speech into a target language in real-time while handling non-monotonic word dependencies. Traditional approaches rely on supervised training with word-level aligned data, which is difficult to collect at scale and thus depends on synthetic alignments using language-specific heuristics that are suboptimal. We propose Hibiki-Zero, which eliminates the need for word-level alignments entirely. This fundamentally simplifies the training pipeline and enables seamless scaling to diverse languages with varying grammatical structures, removing the bottleneck of designing language-specific alignment heuristics. We first train on sentence-level aligned data to learn speech translation at high latency, then apply a novel reinforcement learning strategy using GRPO to optimize latency while preserving translation quality. Hibiki-Zero achieves state-of-the-art performance in translation accuracy, latency, voice transfer, and naturalness across five X-to-English tasks. Moreover, we demonstrate that our model can be adapted to support a new input language with less than 1000h of speech. We provide examples, model weights, inference code and we release a benchmark containing 45h of multilingual data for speech translation evaluation.

</details>


### [131] [SteuerLLM: Local specialized large language model for German tax law analysis](https://arxiv.org/abs/2602.11081)
*Sebastian Wind,Jeta Sopa,Laurin Schmid,Quirin Jackl,Sebastian Kiefer,Fei Wu,Martin Mayr,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: 本文提出了SteuerEx——首个基于真实德国大学税法考试的开源基准，并开发了领域适配的大型语言模型SteuerLLM（28B参数），在税法推理任务上显著优于同规模及更大规模通用模型，强调领域数据与架构适配比参数规模更关键。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在受严格形式规则、精确术语和法律约束结构支配的领域（如税法）中表现下降，亟需面向真实法律考试场景的评估基准与专用模型。

Method: 1）算法生成SteuerEx基准：基于真实德国大学税法考试题，经专家验证，含115道覆盖6大核心领域的多级试题，采用语句级部分计分评估框架；2）构建SteuerLLM：通过受控检索增强流水线，从真实考题生成大规模合成数据并训练28B参数领域适配模型。

Result: SteuerLLM在SteuerEx基准上持续超越同规模通用指令微调模型，甚至在多个任务上优于更大规模系统；所有数据、模型权重、评估代码及Web演示均已开源。

Conclusion: 在真实法律推理任务中，领域专用数据与模型架构适配比单纯扩大参数规模更具决定性作用；开源资源将推动可复现的领域法律人工智能研究。

Abstract: Large language models (LLMs) demonstrate strong general reasoning and language understanding, yet their performance degrades in domains governed by strict formal rules, precise terminology, and legally binding structure. Tax law exemplifies these challenges, as correct answers require exact statutory citation, structured legal argumentation, and numerical accuracy under rigid grading schemes. We algorithmically generate SteuerEx, the first open benchmark derived from authentic German university tax law examinations. SteuerEx comprises 115 expert-validated examination questions spanning six core tax law domains and multiple academic levels, and employs a statement-level, partial-credit evaluation framework that closely mirrors real examination practice. We further present SteuerLLM, a domain-adapted LLM for German tax law trained on a large-scale synthetic dataset generated from authentic examination material using a controlled retrieval-augmented pipeline. SteuerLLM (28B parameters) consistently outperforms general-purpose instruction-tuned models of comparable size and, in several cases, substantially larger systems, demonstrating that domain-specific data and architectural adaptation are more decisive than parameter scale for performance on realistic legal reasoning tasks. All benchmark data, training datasets, model weights, and evaluation code are released openly to support reproducible research in domain-specific legal artificial intelligence. A web-based demo of SteuerLLM is available at https://steuerllm.i5.ai.fau.de.

</details>


### [132] [DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning](https://arxiv.org/abs/2602.11089)
*Yicheng Chen,Zerun Ma,Xinchen Xie,Yining Li,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出端到端数据配方生成方法，通过DataChef-32B模型利用在线强化学习自动构建高效训练数据流程，显著减少人工依赖，并在多个任务上达到媲美人工设计的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练中数据配方设计高度依赖人工，耗时耗力；亟需自动化方法来提升效率与可扩展性。

Method: 提出端到端数据配方生成任务，构建DataChef-32B模型，采用基于代理奖励的在线强化学习，从原始数据源中自动搜索最优数据处理流程。

Result: 在六个预留任务上，DataChef-32B生成的数据配方性能媲美人类专家；尤其在数学领域，将Qwen3-1.7B-Base适配后在AIME'25上达66.7分，超越原模型。

Conclusion: 该工作为LLM训练自动化和自演化AI系统提供了新范式，验证了数据配方可学习、可优化。

Abstract: In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \emph{data recipe}, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate \emph{end-to-end data recipe generation} for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.

</details>


### [133] [Can Large Language Models Make Everyone Happy?](https://arxiv.org/abs/2602.11091)
*Usman Naseem,Gautam Siddharth Kashyap,Ebad Shabbir,Sushant Kumar Ray,Abdullah Mohammad,Rafiq Ali*

Main category: cs.CL

TL;DR: 本文提出MisAlign-Profile统一基准，用于系统评估大语言模型在安全、价值与文化三维度间存在的错位权衡问题，并构建了覆盖112个规范领域、标注语义错位类型的MISALIGNTRADE数据集，实验揭示主流LLM存在12%-34%的跨维错位率。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如SAFETUNEBED、VALUEBENCH、WORLDVIEW-BENCH）仅孤立评估安全、价值或文化维度，无法刻画三者共现时的交互与权衡；基于机制可解释性的新方法（如MIB）仍不足以系统刻画跨维错位。

Method: 提出基于机制剖析思想的MisAlign-Profile统一基准；构建MISALIGNTRADE数据集：涵盖112个规范领域（14安全/56价值/42文化），每条提示标注正交语义错位类型（对象/属性/关系），利用Gemma-2与Qwen3生成并经SimHash去重，通过两阶段拒绝采样配对错位与对齐响应；在多类LLM上开展基准评测。

Result: 在通用、微调及开源权重LLM上评测发现，各模型在安全-价值-文化三维度间普遍存在12%–34%的错位权衡现象，验证了跨维错位的普遍性与严重性。

Conclusion: MisAlign-Profile为首次系统量化LLM跨安全、价值与文化维度错位权衡的基准框架，MISALIGNTRADE数据集及其语义分类体系为深入理解与缓解错位提供了新工具与实证基础。

Abstract: Misalignment in Large Language Models (LLMs) refers to the failure to simultaneously satisfy safety, value, and cultural dimensions, leading to behaviors that diverge from human expectations in real-world settings where these dimensions must co-occur. Existing benchmarks, such as SAFETUNEBED (safety-centric), VALUEBENCH (value-centric), and WORLDVIEW-BENCH (culture-centric), primarily evaluate these dimensions in isolation and therefore provide limited insight into their interactions and trade-offs. More recent efforts, including MIB and INTERPRETABILITY BENCHMARK-based on mechanistic interpretability, offer valuable perspectives on model failures; however, they remain insufficient for systematically characterizing cross-dimensional trade-offs. To address these gaps, we introduce MisAlign-Profile, a unified benchmark for measuring misalignment trade-offs inspired by mechanistic profiling. First, we construct MISALIGNTRADE, an English misaligned-aligned dataset across 112 normative domains taxonomies, including 14 safety, 56 value, and 42 cultural domains. In addition to domain labels, each prompt is classified with one of three orthogonal semantic types-object, attribute, or relations misalignment-using Gemma-2-9B-it and expanded via Qwen3-30B-A3B-Instruct-2507 with SimHash-based fingerprinting to avoid deduplication. Each prompt is paired with misaligned and aligned responses through two-stage rejection sampling to ensure quality. Second, we benchmark general-purpose, fine-tuned, and open-weight LLMs on MISALIGNTRADE-revealing 12%-34% misalignment trade-offs across dimensions.

</details>


### [134] [Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away](https://arxiv.org/abs/2602.11096)
*Soumya Suvra Ghosal,Souradip Chakraborty,Vaibhav Singh,Furong Huang,Dinesh Manocha,Amrit Singh Bedi*

Main category: cs.CL

TL;DR: 本文提出SafeThink，一种轻量级推理时防御方法，通过安全奖励模型监控推理过程，并在安全阈值被违反时条件性注入短纠正前缀（如'Wait, think safely'），显著降低多种多模态大模型的越狱成功率，同时保持其推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的链式思维后训练方法虽提升多模态大模型推理能力，却损害安全对齐、提高越狱成功率，亟需兼顾安全与推理能力的轻量防御机制。

Method: SafeThink在推理时实时监控推理轨迹，利用安全奖励模型判断是否违反安全阈值；若违反，则注入优化后的短纠正前缀（如'Wait, think safely'），且实验证明仅干预前1-3步即可有效重定向生成至安全结果。

Result: 在六个开源多模态大模型和四个越狱基准测试中，SafeThink将攻击成功率降低30%-60%（如LlamaV-o1在JailbreakV-28K上从63.33%降至5.74%），同时MathVista准确率几乎不变（65.20%→65.00%）。

Conclusion: 安全恢复可视为满足性约束而非优化目标；SafeThink以极小开销实现高效安全防护，验证了早期、稀疏干预在多模态推理安全中的有效性。

Abstract: Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates. We propose SafeThink, a lightweight inference-time defense that treats safety recovery as a satisficing constraint rather than a maximization objective. SafeThink monitors the evolving reasoning trace with a safety reward model and conditionally injects an optimized short corrective prefix ("Wait, think safely") only when the safety threshold is violated. In our evaluations across six open-source MLRMs and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, and MM-SafetyBench), SafeThink reduces attack success rates by 30-60% (e.g., LlamaV-o1: 63.33% to 5.74% on JailbreakV-28K, R1-Onevision: 69.07% to 5.65% on Hades) while preserving reasoning performance (MathVista accuracy: 65.20% to 65.00%). A key empirical finding from our experiments is that safety recovery is often only a few steering steps away: intervening in the first 1-3 reasoning steps typically suffices to redirect the full generation toward safe completions.

</details>


### [135] [TEGRA: Text Encoding With Graph and Retrieval Augmentation for Misinformation Detection](https://arxiv.org/abs/2602.11106)
*Géraud Faye,Wassila Ouerdane,Guillaume Gadek,Céline Hudelot*

Main category: cs.CL

TL;DR: 本文提出了一种结合知识图谱与文本编码的方法（TEG）用于虚假信息检测，并进一步扩展为融入领域知识的TEGRA框架，实验表明其性能优于纯语言模型方法。


<details>
  <summary>Details</summary>
Motivation: 虚假信息检测需要借助外部知识（如人工事实核查），而现有方法多依赖纯语言模型，缺乏对结构化知识的有效利用。

Method: 提出Text Encoding with Graph（TEG）：从文本中抽取结构化图谱信息，联合编码文本与图进行分类；进一步扩展为TEGRA，引入领域特定知识增强建模。

Result: 大量实验表明，TEG显著优于仅使用语言模型的方法；TEGRA在多数情况下进一步提升了分类准确率。

Conclusion: 融合结构化知识图谱与文本的编码方式能有效提升虚假信息检测性能，且引入领域知识可带来额外增益。

Abstract: Misinformation detection is a critical task that can benefit significantly from the integration of external knowledge, much like manual fact-checking. In this work, we propose a novel method for representing textual documents that facilitates the incorporation of information from a knowledge base. Our approach, Text Encoding with Graph (TEG), processes documents by extracting structured information in the form of a graph and encoding both the text and the graph for classification purposes. Through extensive experiments, we demonstrate that this hybrid representation enhances misinformation detection performance compared to using language models alone. Furthermore, we introduce TEGRA, an extension of our framework that integrates domain-specific knowledge, further enhancing classification accuracy in most cases.

</details>


### [136] [Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning](https://arxiv.org/abs/2602.11149)
*Dawid J. Kopiczko,Sagar Vaze,Tijmen Blankevoort,Yuki M. Asano*

Main category: cs.CL

TL;DR: 本文发现，在推理语言模型的思维链监督微调（SFT）中，重复训练小规模数据集（多轮epoch）比单轮训练大规模数据集效果更好，且在达到完全记忆时泛化性能达到峰值；该‘重复优势’现象挑战了传统数据规模直觉，并提出以token准确率作为早停准则的实用SFT策略。


<details>
  <summary>Details</summary>
Motivation: 标准机器学习认为更多样化的训练样本有助于泛化，但作者观察到SFT中重复训练小数据反而更优，动机是探究这一反直觉现象背后的机制与实用价值。

Method: 在固定参数更新预算下，对比不同epoch数与数据量组合的SFT效果；使用Olmo3-7B模型，在AIME'24/25和GPQA基准上系统评估；以训练token准确率作为重复饱和信号。

Result: 128 epoch × 400样本显著优于1 epoch × 51200样本（提升12–26个百分点），且无灾难性遗忘；token准确率饱和点与泛化性能峰值一致。

Conclusion: 重复训练小数据集可高效提升推理模型性能；token准确率是可靠停止信号；‘重复优势’——即完全记忆与泛化提升同步发生——构成理解大模型训练动力学的新开放问题。

Abstract: Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [137] [Adaptive Time Step Flow Matching for Autonomous Driving Motion Planning](https://arxiv.org/abs/2602.10285)
*Ananya Trivedi,Anjian Li,Mohamed Elnoor,Yusuf Umut Ciftci,Avinash Singh,Jovin D'sa,Sangjae Bae,David Isele,Taskin Padir,Faizan M. Tariq*

Main category: cs.RO

TL;DR: 本文提出了一种基于条件流匹配的实时自动驾驶轨迹规划框架，联合预测周围交通体运动与自车轨迹，通过轻量方差估计器动态选择推理步数，并引入低开销的凸二次规划后处理提升乘坐舒适性，在Waymo数据集上实现20Hz实时性能且无需场景调优。


<details>
  <summary>Details</summary>
Motivation: 现有基于大规模模仿学习的自动驾驶方法（如扩散模型、一致性模型）在实时性、多模态动作建模及噪声调度适应性方面存在局限：扩散模型推理延迟高；一致性模型依赖人工调优且重训练成本高。

Method: 提出基于条件流匹配（Conditional Flow Matching）的联合预测框架；设计轻量级方差估计器实现在线动态步数选择；引入基于凸二次规划（QP）的轨迹后处理模块以提升平滑性与动态可行性。

Result: 在Waymo Open Motion Dataset上验证，支持车道变换、巡航控制、无保护左转等复杂操作；在NVIDIA RTX 3070 GPU上达20 Hz更新率；相比Transformer、扩散模型和一致性模型基线，轨迹更平滑、更符合动态约束。

Conclusion: 该框架在保证实时性的同时提升了轨迹质量与泛化能力，无需场景特定调优或昂贵重训练，适用于在线部署。

Abstract: Autonomous driving requires reasoning about interactions with surrounding traffic. A prevailing approach is large-scale imitation learning on expert driving datasets, aimed at generalizing across diverse real-world scenarios. For online trajectory generation, such methods must operate at real-time rates. Diffusion models require hundreds of denoising steps at inference, resulting in high latency. Consistency models mitigate this issue but rely on carefully tuned noise schedules to capture the multimodal action distributions common in autonomous driving. Adapting the schedule, typically requires expensive retraining. To address these limitations, we propose a framework based on conditional flow matching that jointly predicts future motions of surrounding agents and plans the ego trajectory in real time. We train a lightweight variance estimator that selects the number of inference steps online, removing the need for retraining to balance runtime and imitation learning performance. To further enhance ride quality, we introduce a trajectory post-processing step cast as a convex quadratic program, with negligible computational overhead. Trained on the Waymo Open Motion Dataset, the framework performs maneuvers such as lane changes, cruise control, and navigating unprotected left turns without requiring scenario-specific tuning. Our method maintains a 20 Hz update rate on an NVIDIA RTX 3070 GPU, making it suitable for online deployment. Compared to transformer, diffusion, and consistency model baselines, we achieve improved trajectory smoothness and better adherence to dynamic constraints. Experiment videos and code implementations can be found at https://flow-matching-self-driving.github.io/.

</details>


### [138] [A Human-in-the-Loop Confidence-Aware Failure Recovery Framework for Modular Robot Policies](https://arxiv.org/abs/2602.10289)
*Rohan Banerjee,Krishna Palempalli,Bohan Yang,Jiaying Fang,Alif Abdullah,Tom Silver,Sarah Dean,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: 本文提出了一种面向模块化机器人策略的人在环失败恢复框架，通过联合建模模块级不确定性与人类干预成本，动态决定向哪个模块求助及是否请求人类介入，从而在提升恢复成功率的同时降低用户认知与操作负荷。


<details>
  <summary>Details</summary>
Motivation: 机器人在非结构化人类环境中（如照护场景）易发生故障，而频繁或不恰当的人类查询会增加用户的认知与身体负担，亟需一种兼顾机器人不确定性与人类代价的智能恢复机制。

Method: 构建模块化策略（感知、规划、控制等）的不确定性估计模型，并结合人类干预成本模型；解耦‘选择哪个模块查询’（模块选择器）与‘是否发起查询’（查询算法）两个决策；在合成实验与真实照护机器人系统（辅助进食）中验证多种策略。

Result: 在合成实验中揭示了恢复效率、系统/用户鲁棒性与用户负荷之间的权衡；在真实辅助进食系统中，对模拟和真实运动障碍用户均显著提升了故障恢复成功率并降低了用户负荷。

Conclusion: 显式联合建模机器人模块不确定性与人类干预成本，可实现更高效、更以用户为中心的协作机器人失败恢复。

Abstract: Robots operating in unstructured human environments inevitably encounter failures, especially in robot caregiving scenarios. While humans can often help robots recover, excessive or poorly targeted queries impose unnecessary cognitive and physical workload on the human partner. We present a human-in-the-loop failure-recovery framework for modular robotic policies, where a policy is composed of distinct modules such as perception, planning, and control, any of which may fail and often require different forms of human feedback. Our framework integrates calibrated estimates of module-level uncertainty with models of human intervention cost to decide which module to query and when to query the human. It separates these two decisions: a module selector identifies the module most likely responsible for failure, and a querying algorithm determines whether to solicit human input or act autonomously. We evaluate several module-selection strategies and querying algorithms in controlled synthetic experiments, revealing trade-offs between recovery efficiency, robustness to system and user variables, and user workload. Finally, we deploy the framework on a robot-assisted bite acquisition system and demonstrate, in studies involving individuals with both emulated and real mobility limitations, that it improves recovery success while reducing the workload imposed on users. Our results highlight how explicitly reasoning about both robot uncertainty and human effort can enable more efficient and user-centered failure recovery in collaborative robots. Supplementary materials and videos can be found at: http://emprise.cs.cornell.edu/modularhil

</details>


### [139] [Solving Geodesic Equations with Composite Bernstein Polynomials for Trajectory Planning](https://arxiv.org/abs/2602.10365)
*Nick Gorman,Gage MacLin,Maxwell Hammond,Venanzio Cichella*

Main category: cs.RO

TL;DR: 本文提出了一种基于复合Bernstein多项式的轨迹规划方法，利用符号优化框架在连续代价场中生成平滑、安全且动力学可行的轨迹，适用于多维复杂环境及计算资源受限的自主系统（如航天器）。


<details>
  <summary>Details</summary>
Motivation: 为解决传统轨迹规划在复杂环境中难以兼顾安全性、平滑性、动力学可行性与计算效率的问题，尤其针对障碍物建模离散化、路径后处理繁琐、数值精度低等局限。

Method: 采用复合Bernstein多项式构建轨迹的符号化表示，结合连续障碍代价场（高斯型不等式约束）、地线方程引导和边界条件约束，在符号优化框架下进行联合优化；支持精确求导，保证C²连续性与局部曲率可控。

Result: 在二维/三维多障碍场景中高效生成光滑、无碰撞、满足最小避障距离与动力学约束的轨迹；验证于航天任务（轨道机动、交会对接、行星探测），具备高数值效率与低计算开销。

Conclusion: 该方法统一了安全性、几何灵活性与优化效率，可作为独立规划器或复杂运动规划问题的高质量初值生成器，具有跨平台（地面/空中/水下/太空）适用性。

Abstract: This work presents a trajectory planning method based on composite Bernstein polynomials for autonomous systems navigating complex environments. The method is implemented in a symbolic optimization framework that enables continuous paths and precise control over trajectory shape. Trajectories are planned over a cost surface that encodes obstacles as continuous fields rather than discrete boundaries. Regions near obstacles are assigned higher costs, naturally encouraging the trajectory to maintain a safe distance while still allowing efficient routing through constrained spaces. The use of composite Bernstein polynomials preserves continuity while enabling fine control over local curvature to satisfy geodesic constraints. The symbolic representation supports exact derivatives, improving optimization efficiency. The method applies to both two- and three-dimensional environments and is suitable for ground, aerial, underwater, and space systems. In spacecraft trajectory planning, for example, it enables the generation of continuous, dynamically feasible trajectories with high numerical efficiency, making it well suited for orbital maneuvers, rendezvous and proximity operations, cluttered gravitational environments, and planetary exploration missions with limited onboard computational resources. Demonstrations show that the approach efficiently generates smooth, collision-free paths in scenarios with multiple obstacles, maintaining clearance without extensive sampling or post-processing. The optimization incorporates three constraint types: (1) a Gaussian surface inequality enforcing minimum obstacle clearance; (2) geodesic equations guiding the path along locally efficient directions on the cost surface; and (3) boundary constraints enforcing fixed start and end conditions. The method can serve as a standalone planner or as an initializer for more complex motion planning problems.

</details>


### [140] [LocoVLM: Grounding Vision and Language for Adapting Versatile Legged Locomotion Policies](https://arxiv.org/abs/2602.10399)
*I Made Aswin Nahrendra,Seunghyun Lee,Dongkyu Lee,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出了一种将大语言模型和视觉-语言模型融入四足机器人运动适应的新方法，实现基于指令和环境语义的实时、高精度（87%）运动控制，无需云端查询。


<details>
  <summary>Details</summary>
Motivation: 现有腿式机器人运动学习主要依赖几何环境表征，难以响应人类指令等高层语义，亟需引入常识推理能力。

Method: 利用预训练大语言模型构建指令驱动的技能库；用视觉-语言模型提取并匹配环境语义到技能库；训练风格条件化策略以生成多样化、鲁棒的运动技能。

Result: 首次实现基于高层语义与指令的实时腿式运动自适应，指令遵循准确率达87%，且全部计算可在本地完成，无需云端大模型查询。

Conclusion: 融合基础模型的高层语义理解与具身运动控制是提升腿式机器人泛化性与人机协同能力的有效路径。

Abstract: Recent advances in legged locomotion learning are still dominated by the utilization of geometric representations of the environment, limiting the robot's capability to respond to higher-level semantics such as human instructions. To address this limitation, we propose a novel approach that integrates high-level commonsense reasoning from foundation models into the process of legged locomotion adaptation. Specifically, our method utilizes a pre-trained large language model to synthesize an instruction-grounded skill database tailored for legged robots. A pre-trained vision-language model is employed to extract high-level environmental semantics and ground them within the skill database, enabling real-time skill advisories for the robot. To facilitate versatile skill control, we train a style-conditioned policy capable of generating diverse and robust locomotion skills with high fidelity to specified styles. To the best of our knowledge, this is the first work to demonstrate real-time adaptation of legged locomotion using high-level reasoning from environmental semantics and instructions with instruction-following accuracy of up to 87% without the need for online query to on-the-cloud foundation models.

</details>


### [141] [Towards Long-Lived Robots: Continual Learning VLA Models via Reinforcement Fine-Tuning](https://arxiv.org/abs/2602.10503)
*Yuan Liu,Haoran Li,Shuai Tian,Yuxing Qin,Yuhui Chen,Yupeng Zheng,Yongzhen Huang,Dongbin Zhao*

Main category: cs.RO

TL;DR: 本文提出LifeLong-RFT，一种无需在线环境反馈和预训练奖励模型的强化微调策略，通过多维过程奖励机制（QACR、CTAR、FCR）优化视觉语言动作（VLA）模型，在多任务与持续学习中显著优于监督微调（SFT）。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）需大量任务数据且易灾难性遗忘，限制VLA模型在下游领域的适应能力。

Method: 提出LifeLong-RFT：结合分块级on-policy强化学习与多维过程奖励（MDPR），包括量化动作一致性奖励（QACR）、连续轨迹对齐奖励（CTAR）和格式合规奖励（FCR）。

Result: 在SimplerEnv、LIBERO及真实世界任务上验证有效性；在LIBERO持续学习中平均成功率较SFT提升22%，仅用20%训练数据即可有效适配新任务。

Conclusion: LifeLong-RFT为VLA模型提供了一种高效、低数据依赖、抗遗忘的后训练范式，推动其向通用机器人策略发展。

Abstract: Pretrained on large-scale and diverse datasets, VLA models demonstrate strong generalization and adaptability as general-purpose robotic policies. However, Supervised Fine-Tuning (SFT), which serves as the primary mechanism for adapting VLAs to downstream domains, requires substantial amounts of task-specific data and is prone to catastrophic forgetting. To address these limitations, we propose LifeLong-RFT, a simple yet effective Reinforcement Fine-Tuning (RFT) strategy for VLA models independent of online environmental feedback and pre-trained reward models. By integrating chunking-level on-policy reinforcement learning with the proposed Multi-Dimensional Process Reward (MDPR) mechanism, LifeLong-RFT quantifies the heterogeneous contributions of intermediate action chunks across three dimensions to facilitate policy optimization. Specifically, (1) the Quantized Action Consistency Reward (QACR) ensures accurate action prediction within the discrete action space; (2) the Continuous Trajectory Alignment Reward (CTAR) aligns decoded continuous action chunks with reference trajectories to ensure precise control; (3) the Format Compliance Reward (FCR) guarantees the structural validity of outputs. Comprehensive experiments across SimplerEnv, LIBERO, and real-world tasks demonstrate that LifeLong-RFT exhibits strong performance in multi-task learning. Furthermore, for continual learning on the LIBERO benchmark, our method achieves a 22% gain in average success rate over SFT, while effectively adapting to new tasks using only 20% of the training data. Overall, our method provides a promising post-training paradigm for VLAs.

</details>


### [142] [Co-jump: Cooperative Jumping with Quadrupedal Robots via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.10514)
*Shihao Dong,Yeke Chen,Zeren Luo,Jiahui Zhang,Bowen Xu,Jinghan Lin,Yimin Han,Ji Ma,Zhiyou Yu,Yudong Zhao,Peng Lu*

Main category: cs.RO

TL;DR: 本文提出了Co-jump协作跳跃任务，利用去中心化、无显式通信的多智能体强化学习（MAPPO）框架，使两个四足机器人仅凭本体感知实现高同步性协作跳跃，显著超越单机能力，并成功迁移到真实硬件。


<details>
  <summary>Details</summary>
Motivation: 单机器人受物理执行器限制，难以突破运动性能瓶颈；需通过多机器人协作拓展运动能力边界。

Method: 提出基于MAPPO的去中心化多智能体控制框架，结合渐进式课程学习策略，仅依赖本体感知反馈实现无通信同步控制。

Result: 仿真与实物实验均成功实现多方向协作跳跃，平台高度达1.5米；其中一机器人脚端高度达1.1米，较单机0.45米提升144%。

Conclusion: 证明了仅靠本体感知即可实现高精度、无通信的多机器人协作运动，为受限环境下的协同运动提供了新范式。

Abstract: While single-agent legged locomotion has witnessed remarkable progress, individual robots remain fundamentally constrained by physical actuation limits. To transcend these boundaries, we introduce Co-jump, a cooperative task where two quadrupedal robots synchronize to execute jumps far beyond their solo capabilities. We tackle the high-impulse contact dynamics of this task under a decentralized setting, achieving synchronization without explicit communication or pre-specified motion primitives. Our framework leverages Multi-Agent Proximal Policy Optimization (MAPPO) enhanced by a progressive curriculum strategy, which effectively overcomes the sparse-reward exploration challenges inherent in mechanically coupled systems. We demonstrate robust performance in simulation and successful transfer to physical hardware, executing multi-directional jumps onto platforms up to 1.5 m in height. Specifically, one of the robots achieves a foot-end elevation of 1.1 m, which represents a 144% improvement over the 0.45 m jump height of a standalone quadrupedal robot, demonstrating superior vertical performance. Notably, this precise coordination is achieved solely through proprioceptive feedback, establishing a foundation for communication-free collaborative locomotion in constrained environments.

</details>


### [143] [ReSPEC: A Framework for Online Multispectral Sensor Reconfiguration in Dynamic Environments](https://arxiv.org/abs/2602.10547)
*Yanchen Liu,Yuang Fan,Minghui Zhao,Xiaofan Jiang*

Main category: cs.RO

TL;DR: 本文提出了一种将感知、学习与执行统一的闭环自适应传感框架，利用强化学习根据任务需求和环境动态调整多传感器配置（如采样率、分辨率等），在移动机器人上验证了其显著降低GPU负载并保持较高检测精度的能力。


<details>
  <summary>Details</summary>
Motivation: 现有多传感器融合系统采用静态配置，无法根据环境变化（如光照差、遮挡）动态调整传感器使用，导致带宽、计算和能耗浪费，且难以应对挑战性场景。

Method: 构建一个闭环自适应感知框架：任务专用检测骨干网络提取多光谱特征并生成各模态贡献分；强化学习代理基于这些分数实时调整传感器参数（如采样频率、分辨率、探测距离等）；实现传感-学习-执行的联合优化。

Result: 在移动探测车平台上实现并验证：相比启发式基线，GPU负载降低29.3%，检测精度仅下降5.3%。

Conclusion: 资源感知的自适应传感可显著提升嵌入式机器人平台的能效与鲁棒性，为动态环境下的智能感知提供了新范式。

Abstract: Multi-sensor fusion is central to robust robotic perception, yet most existing systems operate under static sensor configurations, collecting all modalities at fixed rates and fidelity regardless of their situational utility. This rigidity wastes bandwidth, computation, and energy, and prevents systems from prioritizing sensors under challenging conditions such as poor lighting or occlusion. Recent advances in reinforcement learning (RL) and modality-aware fusion suggest the potential for adaptive perception, but prior efforts have largely focused on re-weighting features at inference time, ignoring the physical cost of sensor data collection. We introduce a framework that unifies sensing, learning, and actuation into a closed reconfiguration loop. A task-specific detection backbone extracts multispectral features (e.g. RGB, IR, mmWave, depth) and produces quantitative contribution scores for each modality. These scores are passed to an RL agent, which dynamically adjusts sensor configurations, including sampling frequency, resolution, sensing range, and etc., in real time. Less informative sensors are down-sampled or deactivated, while critical sensors are sampled at higher fidelity as environmental conditions evolve. We implement and evaluate this framework on a mobile rover, showing that adaptive control reduces GPU load by 29.3\% with only a 5.3\% accuracy drop compared to a heuristic baseline. These results highlight the potential of resource-aware adaptive sensing for embedded robotic platforms.

</details>


### [144] [LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer](https://arxiv.org/abs/2602.10556)
*Lihan Zha,Asher J. Hancock,Mingtong Zhang,Tenny Yin,Yixuan Huang,Dhruv Shah,Allen Z. Ren,Anirudha Majumdar*

Main category: cs.RO

TL;DR: 本文提出了一种名为Language-Action Pre-training (LAP) 的新方法，将机器人低层动作直接用自然语言表示，使动作监督与视觉-语言模型的输入输出分布对齐，从而实现无需微调的跨机器人本体零样本迁移。基于LAP构建的LAP-3B模型，在多个未见机器人平台上实现了超50%的平均零样本成功率，性能约为先前最优VLA模型的2倍。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型在跨机器人本体部署时仍需昂贵的微调，缺乏真正的零样本泛化能力，难以实现通用机器人策略目标。

Method: 提出Language-Action Pre-training（LAP），将低层机器人动作直接编码为自然语言指令，无需学习tokenizer、人工标注或本体特异性架构设计；在此基础上构建LAP-3B模型，并支持动作预测与视觉问答（VQA）的联合训练。

Result: LAP-3B是首个在未经任何本体特异性微调前提下，实现对多种全新机器人显著零样本迁移的VLA模型；在多个新机器人和操作任务上平均零样本成功率超50%，相较最强基线提升约2倍；同时展现出高效适应性与良好扩展性。

Conclusion: LAP通过语言化动作表征，打破了VLA模型对训练本体的强耦合依赖，为构建真正通用、可零样本部署的机器人策略提供了可行且简洁的新范式。

Abstract: A long-standing goal in robotics is a generalist policy that can be deployed zero-shot on new robot embodiments without per-embodiment adaptation. Despite large-scale multi-embodiment pre-training, existing Vision-Language-Action models (VLAs) remain tightly coupled to their training embodiments and typically require costly fine-tuning. We introduce Language-Action Pre-training (LAP), a simple recipe that represents low-level robot actions directly in natural language, aligning action supervision with the pre-trained vision-language model's input-output distribution. LAP requires no learned tokenizer, no costly annotation, and no embodiment-specific architectural design. Based on LAP, we present LAP-3B, which to the best of our knowledge is the first VLA to achieve substantial zero-shot transfer to previously unseen robot embodiments without any embodiment-specific fine-tuning. Across multiple novel robots and manipulation tasks, LAP-3B attains over 50% average zero-shot success, delivering roughly a 2x improvement over the strongest prior VLAs. We further show that LAP enables efficient adaptation and favorable scaling, while unifying action prediction and VQA in a shared language-action format that yields additional gains through co-training.

</details>


### [145] [Morphogenetic Assembly and Adaptive Control for Heterogeneous Modular Robots](https://arxiv.org/abs/2602.10561)
*Chongxi Meng,Da Zhao,Yifei Zhao,Minghao Zeng,Yanmin Zhou,Zhipeng Wang,Bin He*

Main category: cs.RO

TL;DR: 本文提出了一种面向异构模块化机器人的闭环自动化框架，涵盖从形态构建到自适应控制的全流程；通过分层规划器解决大规模异构重构的状态爆炸问题，并设计GPU加速的退火-方差MPPI控制器实现未知构型下的实时、构型无关运动控制。


<details>
  <summary>Details</summary>
Motivation: 解决异构模块化机器人在大规模动态重构中面临的状态空间爆炸问题，以及未知组装构型下缺乏高效、鲁棒、实时自适应运动控制方法的挑战。

Method: 提出分层规划器（高层采用带类型惩罚项的双向启发式搜索生成模块操作序列，底层用A*搜索计算执行轨迹），并设计GPU加速的退火-方差MPPI控制器（引入多阶段方差退火策略平衡全局探索与局部收敛）。

Result: 仿真表明：类型惩罚项显著提升异构场景下规划鲁棒性；贪心启发式比匈牙利启发式产生物理执行成本更低的规划；退火-方差MPPI在速度跟踪精度和控制频率上均优于标准MPPI，实现实时50Hz控制。

Conclusion: 该框架成功验证了异构模块化机器人从模块装配、机器人合并/拆分到动态运动生成的全周期闭环自动化能力，为可重构机器人系统提供了可扩展、实时、构型无关的控制范式。

Abstract: This paper presents a closed-loop automation framework for heterogeneous modular robots, covering the full pipeline from morphological construction to adaptive control. In this framework, a mobile manipulator handles heterogeneous functional modules including structural, joint, and wheeled modules to dynamically assemble diverse robot configurations and provide them with immediate locomotion capability. To address the state-space explosion in large-scale heterogeneous reconfiguration, we propose a hierarchical planner: the high-level planner uses a bidirectional heuristic search with type-penalty terms to generate module-handling sequences, while the low level planner employs A* search to compute optimal execution trajectories. This design effectively decouples discrete configuration planning from continuous motion execution. For adaptive motion generation of unknown assembled configurations, we introduce a GPU accelerated Annealing-Variance Model Predictive Path Integral (MPPI) controller. By incorporating a multi stage variance annealing strategy to balance global exploration and local convergence, the controller enables configuration-agnostic, real-time motion control. Large scale simulations show that the type-penalty term is critical for planning robustness in heterogeneous scenarios. Moreover, the greedy heuristic produces plans with lower physical execution costs than the Hungarian heuristic. The proposed annealing-variance MPPI significantly outperforms standard MPPI in both velocity tracking accuracy and control frequency, achieving real time control at 50 Hz. The framework validates the full-cycle process, including module assembly, robot merging and splitting, and dynamic motion generation.

</details>


### [146] [Flow-Enabled Generalization to Human Demonstrations in Few-Shot Imitation Learning](https://arxiv.org/abs/2602.10594)
*Runze Tang,Penny Sweetser*

Main category: cs.RO

TL;DR: 本文提出SFCrP方法，通过场景流预测模型（SFCr）和流与裁剪点云条件策略（FCrP），利用人类视频辅助机器人模仿学习，在减少机器人演示需求的同时提升跨形态泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于光流的模仿学习方法难以描述交互运动、缺乏对人类视频中未见场景的泛化能力，且易因依赖场景观测而过拟合；需一种能建模任意点轨迹并兼顾通用性与精确性的新方法。

Method: 提出SFCrP框架：1）SFCr模型联合学习机器人与人类视频，预测任意三维点的轨迹（即场景流）；2）FCrP策略以预测的场景流为粗略指导，并结合实时裁剪点云观测进行动作微调，实现通用运动与精细操作的结合。

Result: 在多个真实世界任务中超越SOTA基线；展现出强空间与实例泛化能力，可成功处理仅在人类视频中出现的新场景。

Conclusion: 场景流作为跨形态表征比传统光流更具表达力；解耦通用运动先验（流）与任务特定调整（观测）可有效提升泛化性与精度，为低成本、高适应性机器人模仿学习提供了新范式。

Abstract: Imitation Learning (IL) enables robots to learn complex skills from demonstrations without explicit task modeling, but it typically requires large amounts of demonstrations, creating significant collection costs. Prior work has investigated using flow as an intermediate representation to enable the use of human videos as a substitute, thereby reducing the amount of required robot demonstrations. However, most prior work has focused on the flow, either on the object or on specific points of the robot/hand, which cannot describe the motion of interaction. Meanwhile, relying on flow to achieve generalization to scenarios observed only in human videos remains limited, as flow alone cannot capture precise motion details. Furthermore, conditioning on scene observation to produce precise actions may cause the flow-conditioned policy to overfit to training tasks and weaken the generalization indicated by the flow. To address these gaps, we propose SFCrP, which includes a Scene Flow prediction model for Cross-embodiment learning (SFCr) and a Flow and Cropped point cloud conditioned Policy (FCrP). SFCr learns from both robot and human videos and predicts any point trajectories. FCrP follows the general flow motion and adjusts the action based on observations for precision tasks. Our method outperforms SOTA baselines across various real-world task settings, while also exhibiting strong spatial and instance generalization to scenarios seen only in human videos.

</details>


### [147] [Pitch Angle Control of a Magnetically Actuated Capsule Robot with Nonlinear FEA-based MPC and EKF Multisensory Fusion](https://arxiv.org/abs/2602.10610)
*Chongxun Wang,Zikang Shen,Apoorav Rathore,Akanimoh Udombeh,Harrison Teng,Fangzhou Xia*

Main category: cs.RO

TL;DR: 本文提出了一种基于非线性模型和有限元仿真的磁驱动胶囊机器人俯仰角控制框架，结合约束模型预测控制（MPC）与扩展卡尔曼滤波（EKF）传感器融合，在胃壁模拟表面实现了快速、稳定、鲁棒的俯仰调控，并支持低帧率视觉反馈下的闭环控制。


<details>
  <summary>Details</summary>
Motivation: 现有磁驱动胶囊机器人大多忽略俯仰角（pitch）控制，而该自由度对在倾斜胃壁上实现接触式交互至关重要。

Method: 构建包含滚动接触与执行器动力学的刚体俯仰模型，利用三维有限元仿真获取磁场力/力矩查表；设计满足电流与变化率约束的模型预测控制器（MPC）；采用融合惯性测量与间歇性视觉观测的扩展卡尔曼滤波（EKF）实现状态估计。

Result: 实验表明，相比开关控制，所提方法实现3–5倍更快的俯仰调节、更小振荡；在视觉帧率降至1 Hz时仍能维持稳定闭环控制。

Conclusion: 有限元辅助的MPC与多源传感器融合是实现胶囊机器人俯仰调控、可控对接及未来多自由度运动的可扩展策略。

Abstract: Magnetically actuated capsule robots promise minimally invasive diagnosis and therapy in the gastrointestinal (GI) tract, but existing systems largely neglect control of capsule pitch, a degree of freedom critical for contact-rich interaction with inclined gastric walls. This paper presents a nonlinear, model-based framework for magnetic pitch control of an ingestible capsule robot actuated by a four-coil electromagnetic array. Angle-dependent magnetic forces and torques acting on embedded permanent magnets are characterized using three-dimensional finite-element simulations and embedded as lookup tables in a control-oriented rigid-body pitching model with rolling contact and actuator dynamics. A constrained model predictive controller (MPC) is designed to regulate pitch while respecting hardware-imposed current and slew-rate limits. Experiments on a compliant stomach-inspired surface demonstrate robust pitch reorientation from both horizontal and upright configurations, achieving about three to five times faster settling and reduced oscillatory motion than on-off control. Furthermore, an extended Kalman filter (EKF) fusing inertial sensing with intermittent visual measurements enables stable closed-loop control when the camera update rate is reduced from 30 Hz to 1 Hz, emulating clinically realistic imaging constraints. These results establish finite-element-informed MPC with sensor fusion as a scalable strategy for pitch regulation, controlled docking, and future multi-degree-of-freedom capsule locomotion.

</details>


### [148] [Free-Flying Crew Cooperative Robots on the ISS: A Joint Review of Astrobee, CIMON, and Int-Ball Operations](https://arxiv.org/abs/2602.10686)
*Seiko Piotr Yamaguchi,Andres Mora Vargas,Till Eisenberg,Christian Rogon,Tatsuya Yamamoto,Shona Inoue,Christoph Kössl,Brian Coltin,Trey Smith,Jose V. Benavides*

Main category: cs.RO

TL;DR: 本文首次联合分析了NASA的Astrobee、DLR的CIMON和JAXA的Int-Ball三款国际空间站自由飞行机器人，总结其在设计与在轨运行全生命周期中的共性经验与教训，为未来空间机器人研发提供设计建议。


<details>
  <summary>Details</summary>
Motivation: 不同国家开发的自由飞行机器人在空间站中面临相似挑战，亟需系统性总结跨平台经验以指导未来研发。

Method: 由各机器人研发与运行团队联合开展定性分析，梳理设计目标、系统架构、在轨操作实践及全生命周期中的关键问题。

Result: 识别出多款机器人在动力学建模、自主导航、人机协同、故障管理及在轨维护等方面的显著收敛性与共性挑战。

Conclusion: 尽管起源与设计理念不同，三款机器人在技术路径与运营实践上呈现高度趋同；所提炼的联合经验可转化为面向未来空间机器人的通用设计与运行推荐准则。

Abstract: Intra-vehicular free-flying robots are anticipated to support various work in human spaceflight while working side-by-side with astronauts. Such example of robots includes NASA's Astrobee, DLR's CIMON, and JAXA's Int-Ball, which are deployed on the International Space Station. This paper presents the first joint analyses of these robot's shared experiences, co-authored by their development and operation team members. Despite the different origins and design philosophies, the development and operations of these platforms encountered various convergences. Hence, this paper presents a detailed overview of these robots, presenting their objectives, design, and onboard operations. Hence, joint lessons learned across the lifecycle are presented, from design to on-orbit operations. These lessons learned are anticipated to serve for future development and research as design recommendations.

</details>


### [149] [3D-Printed Anisotropic Soft Magnetic Coating for Directional Rolling of a Magnetically Actuated Capsule Robot](https://arxiv.org/abs/2602.10688)
*Jin Zhou,Chongxun Wang,Zikang Shen,Fangzhou Xia*

Main category: cs.RO

TL;DR: 本文提出了一种基于磁性涂层的紧凑型3D打印软胶囊机器人，替代传统内置永磁体结构，保留完整内腔用于医疗载荷，同时提升吞咽性和运动性能。


<details>
  <summary>Details</summary>
Motivation: 传统磁控胶囊机器人因内置永磁体导致可用腔体减少10–20 mm，限制功能模块集成；需兼顾吞咽性、腔体利用率与可控运动性能。

Method: 设计并3D打印软硅胶-磁性复合材料胶囊，表面涂覆编程化NSSN/SNNS磁极分布的磁性涂层；通过磁静力学仿真与实验验证其滚动、转向、爬坡及越障能力。

Result: 实现稳定双向滚动、全向转向、7.5°斜面攀爬和5 mm障碍跨越；最低驱动磁场0.3 mT，等效作用深度30 mm；涂层结构提升吞咽性且不牺牲腔体空间。

Conclusion: 该磁性涂层方案显著提升胶囊机器人的功能性、生物相容性与临床适用潜力，为微创诊疗提供新平台；后续将优化材料、涂层与磁场发生系统以推动临床转化。

Abstract: Capsule robots are promising tools for minimally invasive diagnostics and therapy, with applications from gastrointestinal endoscopy to targeted drug delivery and biopsy sampling. Conventional magnetic capsule robots embed bulky permanent magnets at both ends, reducing the usable cavity by about 10-20 mm and limiting integration of functional modules. We propose a compact, 3D-printed soft capsule robot with a magnetic coating that replaces internal magnets, enabling locomotion via a thin, functional shell while preserving the entire interior cavity as a continuous volume for medical payloads. The compliant silicone-magnetic composite also improves swallowability, even with a slightly larger capsule size. Magnetostatic simulations and experiments confirm that programmed NSSN/SNNS pole distributions provide strong anisotropy and reliable torque generation, enabling stable bidirectional rolling, omnidirectional steering, climbing on 7.5 degree inclines, and traversal of 5 mm protrusions. Rolling motion is sustained when the magnetic field at the capsule reaches at least 0.3 mT, corresponding to an effective actuation depth of 30 mm in our setup. Future work will optimize material composition, coating thickness, and magnetic layouts to enhance force output and durability, while next-generation robotic-arm-based field generators with closed-loop feedback will address nonlinearities and expand maneuverability. Together, these advances aim to transition coating-based capsule robots toward reliable clinical deployment and broaden their applications in minimally invasive diagnostics and therapy.

</details>


### [150] [A Unified Experimental Architecture for Informative Path Planning: from Simulation to Deployment with GuadalPlanner](https://arxiv.org/abs/2602.10702)
*Alejandro Mendoza Barrionuevo,Dame Seck Diop,Alejandro Casado Pérez,Daniel Gutiérrez Reina,Sergio L. Toral Marín,Samuel Yanes Luis*

Main category: cs.RO

TL;DR: 本文提出了一种统一的路径规划评估架构GuadalPlanner，通过解耦高层决策与底层控制，支持算法在仿真与真实场景中一致部署与评估。


<details>
  <summary>Details</summary>
Motivation: 现有自主车辆信息路径规划算法的评估受限于执行流程碎片化及仿真与实车间迁移性差的问题。

Method: 设计并实现了一个基于ROS2、MAVLink和MQTT的开源可扩展架构GuadalPlanner，定义了规划、感知与执行模块间的标准化接口，支持图结构环境与多种规划策略。

Result: 验证了该架构可在全仿真、软件在环及真实水面无人艇上使用同一执行流程，并成功应用于实时水质监测任务。

Conclusion: 所提架构显著提升了路径规划算法评估的一致性、可复现性与跨平台迁移能力，为算法研发与部署提供了通用实验框架。

Abstract: The evaluation of informative path planning algorithms for autonomous vehicles is often hindered by fragmented execution pipelines and limited transferability between simulation and real-world deployment. This paper introduces a unified architecture that decouples high-level decision-making from vehicle-specific control, enabling algorithms to be evaluated consistently across different abstraction levels without modification. The proposed architecture is realized through GuadalPlanner, which defines standardized interfaces between planning, sensing, and vehicle execution. It is an open and extensible research tool that supports discrete graph-based environments and interchangeable planning strategies, and is built upon widely adopted robotics technologies, including ROS2, MAVLink, and MQTT. Its design allows the same algorithmic logic to be deployed in fully simulated environments, software-in-the-loop configurations, and physical autonomous vehicles using an identical execution pipeline. The approach is validated through a set of experiments, including real-world deployment on an autonomous surface vehicle performing water quality monitoring with real-time sensor feedback.

</details>


### [151] [Omnidirectional Dual-Arm Aerial Manipulator with Proprioceptive Contact Localization for Landing on Slanted Roofs](https://arxiv.org/abs/2602.10703)
*Martijn B. J. Brummelhuis,Nathan F. Lepora,Salua Hamaza*

Main category: cs.RO

TL;DR: 本文提出了一种新型无人机空中机械臂（UAM）形态，通过基于动量的力矩观测器实现盲接触检测与定位，从而在着陆前估计屋顶倾斜角度，实验证明其可在高达30.5度倾斜表面上稳定着陆，平均倾角估计误差为2.87度。


<details>
  <summary>Details</summary>
Motivation: 城市环境中无人机需在几何形状与表面不规则性各异的屋顶上降落，而传统视觉或声学等传感方法易受天气和表面材质影响，导致倾斜角检测不可靠。

Method: 设计具有全向3D工作空间和延伸作业范围的双臂空中机械臂，并提出基于动量的扭矩观测器实现本体感知式接触检测与定位，以在触地前通过物理交互盲估表面倾角。

Result: 飞行实验验证了该方法可在倾角达30.5度的表面上实现鲁棒着陆，在9组不同倾角实验中平均倾角估计误差为2.87度。

Conclusion: 所提出的UAM形态与本体感知策略有效克服了外部环境干扰，提升了无人机在复杂城市屋顶上的自主着陆能力与可靠性。

Abstract: Operating drones in urban environments often means they need to land on rooftops, which can have different geometries and surface irregularities. Accurately detecting roof inclination using conventional sensing methods, such as vision-based or acoustic techniques, can be unreliable, as measurement quality is strongly influenced by external factors including weather conditions and surface materials. To overcome these challenges, we propose a novel unmanned aerial manipulator morphology featuring a dual-arm aerial manipulator with an omnidirectional 3D workspace and extended reach. Building on this design, we develop a proprioceptive contact detection and contact localization strategy based on a momentum-based torque observer. This enables the UAM to infer the inclination of slanted surfaces blindly - through physical interaction - prior to touchdown. We validate the approach in flight experiments, demonstrating robust landings on surfaces with inclinations of up to 30.5 degrees and achieving an average surface inclination estimation error of 2.87 degrees over 9 experiments at different incline angles.

</details>


### [152] [Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation](https://arxiv.org/abs/2602.10717)
*Songen Gu,Yunuo Cai,Tianyu Wang,Simo Wu,Yanwei Fu*

Main category: cs.RO

TL;DR: 本文提出了一种面向机器人操作的快速、预测性视频条件动作框架，通过适配视频生成模型、对抗蒸馏加速预测，并结合生成视频与真实观测来校正空间误差，显著提升了操作中的时序一致性、空间指代能力和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操纵系统缺乏对环境响应动作的预测能力，导致错误和低效；VLMs无法显式预测未来状态，而现有世界模型存在预测时间短或空间不一致的问题。

Method: 提出一个三阶段框架：1）选择并适配鲁棒视频生成模型以确保可靠未来预测；2）采用对抗蒸馏实现快速、少步数视频生成；3）训练动作模型，联合利用生成视频与真实观测来校正空间误差。

Result: 实验表明该方法生成的视频预测在时序上连贯、空间上准确，直接支持精准操作，在具身一致性、空间指代能力和任务完成率上显著优于现有基线。

Conclusion: 所提框架有效弥补了当前世界模型在长时程、空间一致性预测上的不足，为基于视频预测的机器人操作提供了高效可靠的解决方案。

Abstract: Robotic manipulation requires anticipating how the environment evolves in response to actions, yet most existing systems lack this predictive capability, often resulting in errors and inefficiency. While Vision-Language Models (VLMs) provide high-level guidance, they cannot explicitly forecast future states, and existing world models either predict only short horizons or produce spatially inconsistent frames. To address these challenges, we propose a framework for fast and predictive video-conditioned action. Our approach first selects and adapts a robust video generation model to ensure reliable future predictions, then applies adversarial distillation for fast, few-step video generation, and finally trains an action model that leverages both generated videos and real observations to correct spatial errors. Extensive experiments show that our method produces temporally coherent, spatially accurate video predictions that directly support precise manipulation, achieving significant improvements in embodiment consistency, spatial referring ability, and task completion over existing baselines. Codes & Models will be released.

</details>


### [153] [From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving](https://arxiv.org/abs/2602.10719)
*Sining Ang,Yuguang Yang,Chenxu Dang,Canyu Chen,Cheng Chi,Haiyan Liu,Xuanyao Mao,Jason Bao,Xuliang,Bingchuan Sun,Yan Wang*

Main category: cs.RO

TL;DR: 本文通过RecogDrive框架对Vision-Language-Action（VLA）驾驶系统进行深入分析，发现视觉语言模型（VLM）与纯视觉模型（ViT）在行为策略上存在互补性，并据此提出HybridDriveVLA和DualDriveVLA两种融合方法，在保持高规划性能（PDMS达92.10/91.00）的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA驾驶系统虽引入语言模态，但其相较于纯视觉系统带来的本质变化尚不明确，尤其在行为分布、长尾场景适应性及系统设计权衡方面缺乏深入理解。

Method: 采用统一的扩散Transformer规划器，分别接入完整VLM和纯视觉（ViT）骨干网络，构建双分支系统；通过3个研究问题（RQ）定量分析其表征差异与行为特性；进一步设计基于学习打分器的轨迹选择机制（HybridDriveVLA）及置信度驱动的动态调用策略（DualDriveVLA）。

Result: VLM与ViT在约2–3%的长尾测试场景中各自显著占优；Oracle融合上限为93.58 PDMS；HybridDriveVLA达92.10 PDMS；DualDriveVLA以仅15%场景调用VLM实现91.00 PDMS，吞吐量提升3.2倍。

Conclusion: VLM与ViT并非简单替代关系，而是具有行为互补性的异构模态；通过智能融合可兼顾性能与效率，为VLA系统设计提供新范式。

Abstract: Vision-Language-Action (VLA) driving augments end-to-end (E2E) planning with language-enabled backbones, yet it remains unclear what changes beyond the usual accuracy--cost trade-off. We revisit this question with 3--RQ analysis in RecogDrive by instantiating the system with a full VLM and vision-only backbones, all under an identical diffusion Transformer planner. RQ1: At the backbone level, the VLM can introduce additional subspaces upon the vision-only backbones. RQ2: This unique subspace leads to a different behavioral in some long-tail scenario: the VLM tends to be more aggressive whereas ViT is more conservative, and each decisively wins on about 2--3% of test scenarios; With an oracle that selects, per scenario, the better trajectory between the VLM and ViT branches, we obtain an upper bound of 93.58 PDMS. RQ3: To fully harness this observation, we propose HybridDriveVLA, which runs both ViT and VLM branches and selects between their endpoint trajectories using a learned scorer, improving PDMS to 92.10. Finally, DualDriveVLA implements a practical fast--slow policy: it runs ViT by default and invokes the VLM only when the scorer's confidence falls below a threshold; calling the VLM on 15% of scenarios achieves 91.00 PDMS while improving throughput by 3.2x. Code will be released.

</details>


### [154] [Biomimetic Mantaray robot toward the underwater autonomous -- Experimental verification of swimming and diving by flapping motion -](https://arxiv.org/abs/2602.10904)
*Kenta Tabata,Ryosuke Oku,Jun Ito,Renato Miyagusuku,Koichi Ozaki*

Main category: cs.RO

TL;DR: This paper introduces a biomimetic manta ray robot for underwater exploration, using flapping propulsion for efficiency and minimal seabed disturbance; it demonstrates stable swimming and diving in pool tests using PD control and shows promise for ecological monitoring in sensitive environments.


<details>
  <summary>Details</summary>
Motivation: To develop an underwater robot that minimizes seabed disturbance and improves propulsion efficiency compared to traditional screw-based systems, inspired by the natural locomotion of manta rays.

Method: Design and construction of a biomimetic manta ray robot with servo-driven pectoral fins and a streamlined control box; control system based on Raspberry Pi 3B integrated with IMU and pressure sensor; PD control implemented for swimming and diving; experimental validation in a pool.

Result: The robot achieved stable swimming and diving motions under PD control; experiments confirmed its suitability for low-disturbance environments such as aquariums and fish nurseries.

Conclusion: Bio-inspired robotic designs—particularly flapping-wing-like propulsion—offer viable solutions for efficient, ecologically sensitive underwater exploration and monitoring.

Abstract: This study presents the development and experimental verification of a biomimetic manta ray robot for underwater autonomous exploration. Inspired by manta rays, the robot uses flapping motion for propulsion to minimize seabed disturbance and enhance efficiency compared to traditional screw propulsion. The robot features pectoral fins driven by servo motors and a streamlined control box to reduce fluid resistance. The control system, powered by a Raspberry Pi 3B, includes an IMU and pressure sensor for real-time monitoring and control. Experiments in a pool assessed the robot's swimming and diving capabilities. Results show stable swimming and diving motions with PD control. The robot is suitable for applications in environments like aquariums and fish nurseries, requiring minimal disturbance and efficient maneuverability. Our findings demonstrate the potential of bio-inspired robotic designs to improve ecological monitoring and underwater exploration.

</details>


### [155] [Safe mobility support system using crowd mapping and avoidance route planning using VLM](https://arxiv.org/abs/2602.10910)
*Sena Saito,Kenta Tabata,Renato Miyagusuku,Koichi Ozaki*

Main category: cs.RO

TL;DR: 本文提出了一种融合视觉-语言模型（VLM）与高斯过程回归（GPR）的新型框架，用于生成动态人群密度图（Abstraction Maps），以提升自主移动机器人在拥挤动态环境中的导航安全性与适应性。


<details>
  <summary>Details</summary>
Motivation: 解决自主移动机器人在动态、拥挤环境中安全高效导航的挑战，应对劳动力短缺和运营效率提升的需求。

Method: 融合视觉-语言模型（VLM）识别抽象环境概念（如人群密度）与高斯过程回归（GPR）进行概率化建模，生成动态‘Abstraction Maps’。

Result: 在大学校园真实场景实验中，机器人成功规划出避开静态障碍物和动态人群的安全路径，验证了方法的有效性。

Conclusion: 该VLM-GPR联合框架能有效支持机器人对复杂动态环境的理解与响应，显著提升导航的安全性与适应性。

Abstract: Autonomous mobile robots offer promising solutions for labor shortages and increased operational efficiency. However, navigating safely and effectively in dynamic environments, particularly crowded areas, remains challenging. This paper proposes a novel framework that integrates Vision-Language Models (VLM) and Gaussian Process Regression (GPR) to generate dynamic crowd-density maps (``Abstraction Maps'') for autonomous robot navigation. Our approach utilizes VLM's capability to recognize abstract environmental concepts, such as crowd densities, and represents them probabilistically via GPR. Experimental results from real-world trials on a university campus demonstrated that robots successfully generated routes avoiding both static obstacles and dynamic crowds, enhancing navigation safety and adaptability.

</details>


### [156] [Design, Development, and Use of Maya Robot as an Assistant for the Therapy/Education of Children with Cancer: a Pilot Study](https://arxiv.org/abs/2602.10942)
*Alireza Taheri,Minoo Alemi,Elham Ranjkar,Raman Rafatnejad,Ali F. Meghdari*

Main category: cs.RO

TL;DR: 本研究设计并实现了名为Maya的便携式大象形社交机器人，用于辅助癌症治疗中的儿童患者；通过深度神经网络实现98%的面部表情识别准确率，并通过两项初步实验验证其在减轻注射疼痛感和降低儿童焦虑水平方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 为改善接受癌症治疗的儿童患者在临床环境中的疼痛管理和情绪健康，探索社交机器人作为治疗/教育辅助工具的潜力。

Method: 采用深度神经网络提升机器人面部表情识别精度；开展两项初步探索性实验：一是采用配对T检验比较有无Maya机器人时儿童注射过程的疼痛感知差异；二是通过UTAUT问卷评估儿童及其母亲与Maya互动及游戏过程中的焦虑水平与信任度。

Result: 实验一显示机器人存在显著降低儿童注射疼痛感知（p<0.05）；实验二表明儿童在互动中焦虑水平显著低于其母亲，且对机器人及游戏的信任度显著更高（p<0.05）。

Conclusion: Maya机器人在儿科临床环境中展现出积极影响，有助于缓解儿童疼痛、降低焦虑并增强信任，证实社交机器人在儿童癌症护理中具有应用价值。

Abstract: This study centers around the design and implementation of the Maya Robot, a portable elephant-shaped social robot, intended to engage with children undergoing cancer treatment. Initial efforts were devoted to enhancing the robot's facial expression recognition accuracy, achieving a 98% accuracy through deep neural networks. Two subsequent preliminary exploratory experiments were designed to advance the study's objectives. The first experiment aimed to compare pain levels experienced by children during the injection process, with and without the presence of the Maya robot. Twenty-five children, aged 4 to 9, undergoing cancer treatment participated in this counterbalanced study. The paired T-test results revealed a significant reduction in perceived pain when the robot was actively present in the injection room. The second experiment sought to assess perspectives of hospitalized children and their mothers during engagement with Maya through a game. Forty participants, including 20 children aged 4 to 9 and their mothers, were involved. Post Human-Maya Interactions, UTAUT questionnaire results indicated that children experienced significantly less anxiety than their parents during the interaction and game play. Notably, children exhibited higher trust levels in both the robot and the games, presenting a statistically significant difference in trust levels compared to their parents (P-value < 0.05). This preliminary exploratory study highlights the positive impact of utilizing Maya as an assistant for therapy/education in a clinical setting, particularly benefiting children undergoing cancer treatment. The findings underscore the potential of social robots in pediatric healthcare contexts, emphasizing improved pain management and emotional well-being among young patients.

</details>


### [157] [Developing Neural Network-Based Gaze Control Systems for Social Robots](https://arxiv.org/abs/2602.10946)
*Ramtin Tabatabaei,Alireza Taheri*

Main category: cs.RO

TL;DR: 本研究利用深度神经网络（LSTM和Transformer）分析人类在多种社交场景中的凝视行为模式，并将最优模型部署到Nao机器人上，实现了60%-65%的凝视方向预测准确率，用户评估显示整体满意度较高。


<details>
  <summary>Details</summary>
Motivation: 在多主体交互中，凝视方向是兴趣与意图的关键指标，社会机器人需理解社交上下文以有效参与、预测人类意图并顺畅交互。

Method: 采集30名参与者在2D屏幕与3D虚拟现实（Oculus Quest 1）中观看不同社交场景视频（如进入、离开、挥手、交谈、指物）的眼动数据；使用LSTM和Transformer模型建模并预测凝视方向；将最优模型部署至Nao机器人，并由36名新参与者进行人机交互评估。

Result: LSTM和Transformer模型在2D动画中凝视方向预测准确率达60%，在3D动画中达65%；部署于Nao机器人后，用户反馈总体满意，具机器人经验者评价更高。

Conclusion: 基于真实人类凝视数据训练的深度学习模型可有效提升社会机器人在动态社交场景中的凝视行为生成能力，具备实用潜力，但准确率仍有提升空间。

Abstract: During multi-party interactions, gaze direction is a key indicator of interest and intent, making it essential for social robots to direct their attention appropriately. Understanding the social context is crucial for robots to engage effectively, predict human intentions, and navigate interactions smoothly. This study aims to develop an empirical motion-time pattern for human gaze behavior in various social situations (e.g., entering, leaving, waving, talking, and pointing) using deep neural networks based on participants' data. We created two video clips-one for a computer screen and another for a virtual reality headset-depicting different social scenarios. Data were collected from 30 participants: 15 using an eye-tracker and 15 using an Oculus Quest 1 headset. Deep learning models, specifically Long Short-Term Memory (LSTM) and Transformers, were used to analyze and predict gaze patterns. Our models achieved 60% accuracy in predicting gaze direction in a 2D animation and 65% accuracy in a 3D animation. Then, the best model was implemented onto the Nao robot; and 36 new participants evaluated its performance. The feedback indicated overall satisfaction, with those experienced in robotics rating the models more favorably.

</details>


### [158] [Stability Analysis of Geometric Control for a Canonical Class of Underactuated Aerial Vehicles with Spurious Forces](https://arxiv.org/abs/2602.10961)
*Simone Orelli,Mirko Mizzoni,Antonio Franchi*

Main category: cs.RO

TL;DR: 本文提出了首个针对受杂散力影响的漂浮刚体系统的严格稳定性分析，通过构建李雅普诺夫函数证明了悬停平衡点的局部指数稳定性，并解决了非最小相位行为带来的理论挑战。


<details>
  <summary>Details</summary>
Motivation: 标准几何控制依赖于力-力矩解耦假设，但在许多空中平台中该假设因控制力矩引起的杂散力而失效；目前尚缺乏对这类耦合系统稳定性的严格理论认证。

Method: 引入一个典型模型，并基于李雅普诺夫方法构建稳定性证明，特别处理由杂散力引发的非最小相位结构挑战，避免使用标准级联论证。

Result: 首次实现了对一类受杂散力影响的漂浮刚体系统的局部指数稳定性形式化证明。

Conclusion: 该工作填补了耦合动力学系统稳定性理论认证的空白，为实际空中平台的控制器设计提供了坚实的理论基础。

Abstract: Standard geometric control relies on force-moment decoupling, an assumption that breaks down in many aerial platforms due to spurious forces naturally induced by control moments. While strategies for such coupled systems have been validated experimentally, a rigorous theoretical certification of their stability is currently missing. This work fills this gap by providing the first formal stability analysis for a generic class of floating rigid bodies subject to spurious forces. We introduce a canonical model and construct a Lyapunov-based proof establishing local exponential stability of the hovering equilibrium. Crucially, the analysis explicitly addresses the structural challenges - specifically the induced non-minimum-phase behavior - that prevent the application of standard cascade arguments.

</details>


### [159] [RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation](https://arxiv.org/abs/2602.10980)
*Yuhao Chen,Zhihao Zhan,Xiaoxin Lin,Zijian Song,Hao Liu,Qinhan Lyu,Yubo Zu,Xiao Chen,Zhiyuan Liu,Tao Pu,Tianshui Chen,Keze Wang,Liang Lin,Guangrun Wang*

Main category: cs.RO

TL;DR: 本文提出RADAR基准，旨在解决当前VLA模型评估中脱离真实物理环境、忽视空间-物理智能及缺乏可扩展自主评估三大问题，通过引入真实物理动力学、空间推理任务和全自动3D评估流程，揭示了现有SOTA模型在现实条件下的严重脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在仿真或受限现实环境中表现良好，但存在显著的‘现实鸿沟’，其根本原因在于评估基准未能反映真实世界的动态性、空间物理智能需求和可扩展自主性。

Method: 提出RADAR基准，包含三个核心组件：（1）基于真实物理规律的动力学建模；（2）专门测试空间推理与物理理解的任务设计；（3）基于3D指标的全自动评估流水线，摆脱人工干预。

Result: 对多个SOTA VLA模型的审计显示：在传感器噪声下，3D IoU从0.261骤降至0.068；模型空间推理能力普遍薄弱，暴露其现实部署中的严重脆弱性。

Conclusion: RADAR是迈向可靠、泛化性强的真实世界VLA模型评估的必要基准，推动评估范式从仿真中心转向物理现实中心。

Abstract: VLA models have achieved remarkable progress in embodied intelligence; however, their evaluation remains largely confined to simulations or highly constrained real-world settings. This mismatch creates a substantial reality gap, where strong benchmark performance often masks poor generalization in diverse physical environments. We identify three systemic shortcomings in current benchmarking practices that hinder fair and reliable model comparison. (1) Existing benchmarks fail to model real-world dynamics, overlooking critical factors such as dynamic object configurations, robot initial states, lighting changes, and sensor noise. (2) Current protocols neglect spatial--physical intelligence, reducing evaluation to rote manipulation tasks that do not probe geometric reasoning. (3) The field lacks scalable fully autonomous evaluation, instead relying on simplistic 2D metrics that miss 3D spatial structure or on human-in-the-loop systems that are costly, biased, and unscalable. To address these limitations, we introduce RADAR (Real-world Autonomous Dynamics And Reasoning), a benchmark designed to systematically evaluate VLA generalization under realistic conditions. RADAR integrates three core components: (1) a principled suite of physical dynamics; (2) dedicated tasks that explicitly test spatial reasoning and physical understanding; and (3) a fully autonomous evaluation pipeline based on 3D metrics, eliminating the need for human supervision. We apply RADAR to audit multiple state-of-the-art VLA models and uncover severe fragility beneath their apparent competence. Performance drops precipitously under modest physical dynamics, with the expectation of 3D IoU declining from 0.261 to 0.068 under sensor noise. Moreover, models exhibit limited spatial reasoning capability. These findings position RADAR as a necessary bench toward reliable and generalizable real-world evaluation of VLA models.

</details>


### [160] [Scaling World Model for Hierarchical Manipulation Policies](https://arxiv.org/abs/2602.10983)
*Qian Long,Yueze Wang,Jiaxi Song,Junbo Zhang,Peiyan Li,Wenxuan Wang,Yuqi Wang,Haoyang Li,Shaoxuan Xie,Guocai Yao,Hanbo Zhang,Xinlong Wang,Zhongyuan Wang,Xuguang Lan,Huaping Liu,Xinghang Li*

Main category: cs.RO

TL;DR: 本文提出了一种分层的视觉-语言-动作（VLA）框架VISTA，利用大规模预训练世界模型作为高层规划器，将操作任务分解为带目标图像的子任务序列；VLA作为底层执行器，依据文本和视觉引导生成动作序列，显著提升了在分布外场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在分布外（OOD）场景下鲁棒性差，尤其受限于真实机器人数据稀缺，亟需提升其泛化能力。

Method: 提出分层VISTA框架：高层采用预训练世界模型进行视觉子目标任务分解，生成带目标图像的子任务序列；底层VLA策略基于图文指导执行动作；目标图像是视觉与物理可接地的强引导信号。

Result: 在大量OOD场景中验证有效，相同结构VLA在新场景中的成功率从14%提升至69%，显著优于先前基线方法。

Conclusion: 通过引入世界模型驱动的视觉子目标分解，VISTA框架有效缓解了VLA模型在分布外场景中的泛化瓶颈，为通用机器人操作提供了更鲁棒、可扩展的解决方案。

Abstract: Vision-Language-Action (VLA) models are promising for generalist robot manipulation but remain brittle in out-of-distribution (OOD) settings, especially with limited real-robot data. To resolve the generalization bottleneck, we introduce a hierarchical Vision-Language-Action framework \our{} that leverages the generalization of large-scale pre-trained world model for robust and generalizable VIsual Subgoal TAsk decomposition VISTA. Our hierarchical framework \our{} consists of a world model as the high-level planner and a VLA as the low-level executor. The high-level world model first divides manipulation tasks into subtask sequences with goal images, and the low-level policy follows the textual and visual guidance to generate action sequences. Compared to raw textual goal specification, these synthesized goal images provide visually and physically grounded details for low-level policies, making it feasible to generalize across unseen objects and novel scenarios. We validate both visual goal synthesis and our hierarchical VLA policies in massive out-of-distribution scenarios, and the performance of the same-structured VLA in novel scenarios could boost from 14% to 69% with the guidance generated by the world model. Results demonstrate that our method outperforms previous baselines with a clear margin, particularly in out-of-distribution scenarios. Project page: \href{https://vista-wm.github.io/}{https://vista-wm.github.io}

</details>


### [161] [Multi-Task Reinforcement Learning of Drone Aerobatics by Exploiting Geometric Symmetries](https://arxiv.org/abs/2602.10997)
*Zhanyu Guo,Zikang Yin,Guobin Zhu,Shiliang Guo,Shiyu Zhao*

Main category: cs.RO

TL;DR: 本文提出GEAR框架，利用MAV动力学中的SO(2)旋转对称性，结合等变Actor网络、FiLM任务调制和多头Critic，实现高效、鲁棒、统一的多任务自主特技飞行控制。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在MAV多任务特技飞行中存在数据效率低、泛化能力差的问题，尤其在需单一策略掌握多种机动动作时挑战更显著。

Method: 提出端到端多任务强化学习框架GEAR，融合几何等变性（SO(2)对称性建模）、FiLM任务调制机制与多头Critic结构，构建具备任务自适应与运动对称感知能力的策略网络。

Result: GEAR在各类特技任务中达到98.85%成功率，显著优于基线方法；实验证明其能稳定执行多种机动，并组合基础运动原语完成复杂特技。

Conclusion: GEAR通过显式建模动力学对称性与任务结构，实现了高数据效率、强鲁棒性与良好泛化能力的统一多任务MAV控制框架，为自主特技飞行提供了新范式。

Abstract: Flight control for autonomous micro aerial vehicles (MAVs) is evolving from steady flight near equilibrium points toward more aggressive aerobatic maneuvers, such as flips, rolls, and Power Loop. Although reinforcement learning (RL) has shown great potential in these tasks, conventional RL methods often suffer from low data efficiency and limited generalization. This challenge becomes more pronounced in multi-task scenarios where a single policy is required to master multiple maneuvers. In this paper, we propose a novel end-to-end multi-task reinforcement learning framework, called GEAR (Geometric Equivariant Aerobatics Reinforcement), which fully exploits the inherent SO(2) rotational symmetry in MAV dynamics and explicitly incorporates this property into the policy network architecture. By integrating an equivariant actor network, FiLM-based task modulation, and a multi-head critic, GEAR achieves both efficiency and flexibility in learning diverse aerobatic maneuvers, enabling a data-efficient, robust, and unified framework for aerobatic control. GEAR attains a 98.85\% success rate across various aerobatic tasks, significantly outperforming baseline methods. In real-world experiments, GEAR demonstrates stable execution of multiple maneuvers and the capability to combine basic motion primitives to complete complex aerobatics.

</details>


### [162] [ContactGaussian-WM: Learning Physics-Grounded World Model from Videos](https://arxiv.org/abs/2602.11021)
*Meizhong Wang,Wanxin Jin,Kun Cao,Lihua Xie,Yiguang Hong*

Main category: cs.RO

TL;DR: 本文提出了ContactGaussian-WM，一种基于物理的可微分刚体世界模型，能从稀疏且接触丰富的视频序列中直接学习复杂物理规律。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据稀缺和接触密集的动态运动条件下难以准确建模环境，制约了机器人规划与仿真发展。

Method: 提出统一高斯表示（兼顾视觉外观与碰撞几何）和端到端可微学习框架，通过闭式物理引擎反向传播，从稀疏视觉观测中推断物理属性。

Result: 在仿真与真实场景中均优于现有最先进方法，展现出对复杂场景的强学习能力与泛化性，并成功应用于数据合成与实时模型预测控制（MPC）。

Conclusion: ContactGaussian-WM为数据稀缺下的物理感知世界建模提供了新范式，提升了机器人系统在复杂交互环境中的理解与决策能力。

Abstract: Developing world models that understand complex physical interactions is essential for advancing robotic planning and simulation.However, existing methods often struggle to accurately model the environment under conditions of data scarcity and complex contact-rich dynamic motion.To address these challenges, we propose ContactGaussian-WM, a differentiable physics-grounded rigid-body world model capable of learning intricate physical laws directly from sparse and contact-rich video sequences.Our framework consists of two core components: (1) a unified Gaussian representation for both visual appearance and collision geometry, and (2) an end-to-end differentiable learning framework that differentiates through a closed-form physics engine to infer physical properties from sparse visual observations.Extensive simulations and real-world evaluations demonstrate that ContactGaussian-WM outperforms state-of-the-art methods in learning complex scenarios, exhibiting robust generalization capabilities.Furthermore, we showcase the practical utility of our framework in downstream applications, including data synthesis and real-time MPC.

</details>


### [163] [SQ-CBF: Signed Distance Functions for Numerically Stable Superquadric-Based Safety Filtering](https://arxiv.org/abs/2602.11049)
*Haocheng Zhao,Lukas Brunke,Oliver Lagerquist,Siqi Zhou,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 本文提出一种基于超二次曲面（SQ）的机器人安全过滤新框架，通过使用有符号距离函数（SDF）替代传统隐式SQ函数作为屏障函数，结合GJK算法与随机平滑梯度估计，解决了隐式SQ梯度病态导致优化不可行的问题，在仿真和实物实验中实现了复杂动态环境下的鲁棒无碰撞操作。


<details>
  <summary>Details</summary>
Motivation: 现有基于控制屏障函数（CBF）的安全过滤方法在复杂环境中性能受限，主要因几何表征简化导致保守或覆盖不足；超二次曲面虽具表达力，但直接以其隐式函数构造屏障时梯度严重病态，使实时优化不可靠。

Method: 提出以超二次曲面的有符号距离函数（SDF）作为屏障候选；因一般SQ无解析SDF，采用高效GJK算法计算距离，并用随机平滑法估计SDF梯度；将该SDF-CBF集成至实时安全过滤框架中。

Result: 在密集、非结构化仿真与真实场景中均实现稳定无碰撞操纵；对复杂几何、传感噪声及动态扰动具有强鲁棒性；提升遥操作任务效率。

Conclusion: 使用SDF替代隐式函数可显著提升SQ表征下安全过滤的数值稳定性与可靠性，为应对真实世界几何复杂性提供了可行且精确的安全保障路径。

Abstract: Ensuring safe robot operation in cluttered and dynamic environments remains a fundamental challenge. While control barrier functions provide an effective framework for real-time safety filtering, their performance critically depends on the underlying geometric representation, which is often simplified, leading to either overly conservative behavior or insufficient collision coverage. Superquadrics offer an expressive way to model complex shapes using a few primitives and are increasingly used for robot safety. To integrate this representation into collision avoidance, most existing approaches directly use their implicit functions as barrier candidates. However, we identify a critical but overlooked issue in this practice: the gradients of the implicit SQ function can become severely ill-conditioned, potentially rendering the optimization infeasible and undermining reliable real-time safety filtering. To address this issue, we formulate an SQ-based safety filtering framework that uses signed distance functions as barrier candidates. Since analytical SDFs are unavailable for general SQs, we compute distances using the efficient Gilbert-Johnson-Keerthi algorithm and obtain gradients via randomized smoothing. Extensive simulation and real-world experiments demonstrate consistent collision-free manipulation in cluttered and unstructured scenes, showing robustness to challenging geometries, sensing noise, and dynamic disturbances, while improving task efficiency in teleoperation tasks. These results highlight a pathway toward safety filters that remain precise and reliable under the geometric complexity of real-world environments.

</details>


### [164] [RISE: Self-Improving Robot Policy with Compositional World Model](https://arxiv.org/abs/2602.11075)
*Jiazhi Yang,Kunyang Lin,Jinwei Li,Wencong Zhang,Tianwei Lin,Longyan Wu,Zhizhong Su,Hao Zhao,Ya-Qin Zhang,Li Chen,Ping Luo,Xiangyu Yue,Hongyang Li*

Main category: cs.RO

TL;DR: 本文提出RISE框架，通过构想式强化学习提升视觉-语言-动作（VLA）模型在接触密集型动态操作任务中的鲁棒性，核心是组合式世界模型，包含可控动力学模型与进展价值模型，在无需真实物理交互下实现策略持续自优化。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在接触密集、动态操作任务中仍脆弱，微小执行偏差易导致失败；而真实世界中基于策略的强化学习受限于安全风险、硬件成本和环境重置困难。

Method: 提出RISE框架：构建组合式世界模型，包括（i）通过可控动力学模型预测多视角未来状态，（ii）用进展价值模型评估构想结果并生成信息丰富的优势函数；所有组件集成到闭环自优化流程中，在构想空间中生成rollout、估计优势、更新策略。

Result: 在三个真实世界任务（动态砖块分拣、背包装填、盒子闭合）上显著超越先前方法，绝对性能分别提升+35%、+45%、+35%。

Conclusion: RISE通过解耦建模与目标驱动设计，实现了高效、安全、可扩展的机器人强化学习，为复杂物理操作任务提供了新范式。

Abstract: Despite the sustained scaling on model capacity and data acquisition, Vision-Language-Action (VLA) models remain brittle in contact-rich and dynamic manipulation tasks, where minor execution deviations can compound into failures. While reinforcement learning (RL) offers a principled path to robustness, on-policy RL in the physical world is constrained by safety risk, hardware cost, and environment reset. To bridge this gap, we present RISE, a scalable framework of robotic reinforcement learning via imagination. At its core is a Compositional World Model that (i) predicts multi-view future via a controllable dynamics model, and (ii) evaluates imagined outcomes with a progress value model, producing informative advantages for the policy improvement. Such compositional design allows state and value to be tailored by best-suited yet distinct architectures and objectives. These components are integrated into a closed-loop self-improving pipeline that continuously generates imaginary rollouts, estimates advantages, and updates the policy in imaginary space without costly physical interaction. Across three challenging real-world tasks, RISE yields significant improvement over prior art, with more than +35% absolute performance increase in dynamic brick sorting, +45% for backpack packing, and +35% for box closing, respectively.

</details>


### [165] [Digging for Data: Experiments in Rock Pile Characterization Using Only Proprioceptive Sensing in Excavation](https://arxiv.org/abs/2602.11082)
*Unal Artan,Martin Magnusson,Joshua A. Marshall*

Main category: cs.RO

TL;DR: 本文提出了一种仅利用轮式装载机挖掘过程中的本体感知数据（如惯性响应）来估计破碎岩石堆颗粒相对尺寸的新方法，通过小波分析提取特征，并在真实采石场中验证了其与视觉分析和筛分结果的一致性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖外部传感器（如相机或LiDAR）估计岩石颗粒尺寸，成本高且易受环境干扰；而本体感知数据易于获取、鲁棒性强，亟需探索其在岩石破碎度评估中的潜力。

Method: 基于轮式装载机挖掘时的惯性响应信号，采用小波分析提取与岩石破碎程度成比例的特征；通过计算不同岩堆间小波特征的比值，估计其平均颗粒尺寸的相对比值。

Result: 在18吨电池电动LHD设备上开展全尺度现场实验，结果表明该方法估算的相对颗粒尺寸与视觉分析工具及实际筛分结果高度一致。

Conclusion: 仅利用本体感知数据结合小波特征分析，即可有效、可靠地估计破碎岩石堆的相对颗粒尺寸，为智能采矿提供了一种低成本、高鲁棒性的在线监测新途径。

Abstract: Characterization of fragmented rock piles is a fundamental task in the mining and quarrying industries, where rock is fragmented by blasting, transported using wheel loaders, and then sent for further processing. This field report studies a novel method for estimating the relative particle size of fragmented rock piles from only proprioceptive data collected while digging with a wheel loader. Rather than employ exteroceptive sensors (e.g., cameras or LiDAR sensors) to estimate rock particle sizes, the studied method infers rock fragmentation from an excavator's inertial response during excavation. This paper expands on research that postulated the use of wavelet analysis to construct a unique feature that is proportional to the level of rock fragmentation. We demonstrate through extensive field experiments that the ratio of wavelet features, constructed from data obtained by excavating in different rock piles with different size distributions, approximates the ratio of the mean particle size of the two rock piles. Full-scale excavation experiments were performed with a battery electric, 18-tonne capacity, load-haul-dump (LHD) machine in representative conditions in an operating quarry. The relative particle size estimates generated with the proposed sensing methodology are compared with those obtained from both a vision-based fragmentation analysis tool and from sieving of sampled materials.

</details>


### [166] [A receding-horizon multi-contact motion planner for legged robots in challenging environments](https://arxiv.org/abs/2602.11113)
*Daniel S. J. Derwent,Simon Watson,Bruno V. Adorno*

Main category: cs.RO

TL;DR: 本文提出了一种新型的滚动时域多接触运动规划器，用于腿足机器人在复杂场景（如烟囱攀爬、狭窄通道通行、跨越大间隙）中的运动规划，具备实时重规划与接触点-全身轨迹联合优化能力，显著提升规划速度与运动质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有腿足机器人运动规划方法在复杂场景中难以实时响应、需多阶段处理、易陷入局部极小值等问题，提升规划效率与鲁棒性。

Method: 基于滚动时域框架，采用二次规划（QP）实现接触位置与全身轨迹的联合优化，并支持基于新信息的反应式重规划。

Result: 短规划时域下（如一步），平均比现有方法快45%–98%，但步态效率略低（姿态变换次数变化范围为-5%至+700%）；长规划时域下（如四步），规划时间有所增加（最快快73%，最慢慢400%），但运动质量显著提升（姿态变换减少8%至47%）。

Conclusion: 该方法在规划速度与运动质量之间实现了更优权衡，尤其适用于对实时性与运动鲁棒性均有高要求的复杂地形任务。

Abstract: We present a novel receding-horizon multi-contact motion planner for legged robots in challenging scenarios, able to plan motions such as chimney climbing, navigating very narrow passages or crossing large gaps. Our approach adds new capabilities to the state of the art, including the ability to reactively re-plan in response to new information, and planning contact locations and whole-body trajectories simultaneously, simplifying the implementation and removing the need for post-processing or complex multi-stage approaches. Our method is more resistant to local minima problems than other potential field based approaches, and our quadratic-program-based posture generator returns nodes more quickly than those of existing algorithms. Rigorous statistical analysis shows that, with short planning horizons (e.g., one step ahead), our planner is faster than the state-of-the-art across all scenarios tested (between 45% and 98% faster on average, depending on the scenario), while planning less efficient motions (requiring 5% fewer to 700% more stance changes on average). In all but one scenario (Chimney Walking), longer planning horizons (e.g., four steps ahead) extended the average planning times (between 73% faster and 400% slower than the state-of-the-art) but resulted in higher quality motion plans (between 8% more and 47% fewer stance changes than the state-of-the-art).

</details>


### [167] [Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows](https://arxiv.org/abs/2602.11142)
*Shaswat Garg,Matin Moezzi,Brandon Da Silva*

Main category: cs.RO

TL;DR: 本文提出了NF-HIQL，一种基于归一化流的分层隐式Q学习框架，通过在高低层策略中引入表达能力强的归一化流策略，提升了数据效率与策略表达能力，并在多种长视野任务上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有分层目标条件强化学习（H-GCRL）在离线或数据稀缺场景下存在数据效率低、策略表达能力弱的问题，限制了其实际应用。

Method: 提出NF-HIQL框架，用归一化流策略替代传统单峰高斯策略，并在高低层均部署；推导RealNVP策略的KL散度界及PAC式样本效率理论保证。

Result: 在运动控制、运球、多步操作等长视野任务上，NF-HIQL显著优于现有目标条件和分层基线方法，尤其在数据受限时表现出更强鲁棒性。

Conclusion: 归一化流架构可有效提升分层强化学习的可扩展性与数据效率，为复杂长视野任务提供新思路。

Abstract: Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides a powerful framework for tackling complex, long-horizon tasks by decomposing them into structured subgoals. However, its practical adoption is hindered by poor data efficiency and limited policy expressivity, especially in offline or data-scarce regimes. In this work, Normalizing flow-based hierarchical implicit Q-learning (NF-HIQL), a novel framework that replaces unimodal gaussian policies with expressive normalizing flow policies at both the high- and low-levels of the hierarchy is introduced. This design enables tractable log-likelihood computation, efficient sampling, and the ability to model rich multimodal behaviors. New theoretical guarantees are derived, including explicit KL-divergence bounds for Real-valued non-volume preserving (RealNVP) policies and PAC-style sample efficiency results, showing that NF-HIQL preserves stability while improving generalization. Empirically, NF-HIQL is evaluted across diverse long-horizon tasks in locomotion, ball-dribbling, and multi-step manipulation from OGBench. NF-HIQL consistently outperforms prior goal-conditioned and hierarchical baselines, demonstrating superior robustness under limited data and highlighting the potential of flow-based architectures for scalable, data-efficient hierarchical reinforcement learning.

</details>


### [168] [APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots](https://arxiv.org/abs/2602.11143)
*Yikai Wang,Tingxuan Leng,Changyi Lin,Shiqi Liu,Shir Simon,Bingqing Chen,Jonathan Francis,Ding Zhao*

Main category: cs.RO

TL;DR: 本文提出APEX系统，通过感知驱动的攀爬式高平台穿越方法，结合地形条件行为（如攀爬、行走、姿态调整）和广义棘轮进度奖励机制，在仿真中训练出基于LiDAR的全身运动策略，并成功实现零样本迁移至真实Unitree G1人形机器人，完成高达腿长114%（0.8米）平台的稳定穿越。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习在人形机器人步态控制中虽取得进展，但在超越腿长的高平台穿越任务中易收敛于高冲击、扭矩受限且不安全的跳跃式解，难以部署到现实世界。

Method: 提出APEX系统，包含：1）地形条件化的行为组合（攀上/攀下、平台行走/爬行、起立/躺倒）；2）广义棘轮进度奖励——跟踪历史最优任务进度并惩罚非改进步，提供稠密且无速度依赖的监督信号；3）LiDAR全身体策略训练，结合仿真中建模地图失真与部署时对高程图进行滤波与修复，缩小sim-to-real感知差距；4）将六种技能蒸馏为单一策略，依据局部几何与指令自主选择行为与切换。

Result: 在29自由度Unitree G1人形机器人上实现零样本sim-to-real高平台穿越，成功跨越0.8米（约114%腿长）平台；具备对平台高度与初始姿态的鲁棒适应能力，多技能切换平滑稳定。

Conclusion: APEX首次实现了基于学习的、安全可靠的高平台攀爬式穿越，突破了传统DRL在高平台任务中易陷于跳跃策略的局限，为复杂三维环境中的具身智能体提供了可扩展的行为框架。

Abstract: Humanoid locomotion has advanced rapidly with deep reinforcement learning (DRL), enabling robust feet-based traversal over uneven terrain. Yet platforms beyond leg length remain largely out of reach because current RL training paradigms often converge to jumping-like solutions that are high-impact, torque-limited, and unsafe for real-world deployment. To address this gap, we propose APEX, a system for perceptive, climbing-based high-platform traversal that composes terrain-conditioned behaviors: climb-up and climb-down at vertical edges, walking or crawling on the platform, and stand-up and lie-down for posture reconfiguration. Central to our approach is a generalized ratchet progress reward for learning contact-rich, goal-reaching maneuvers. It tracks the best-so-far task progress and penalizes non-improving steps, providing dense yet velocity-free supervision that enables efficient exploration under strong safety regularization. Based on this formulation, we train LiDAR-based full-body maneuver policies and reduce the sim-to-real perception gap through a dual strategy: modeling mapping artifacts during training and applying filtering and inpainting to elevation maps during deployment. Finally, we distill all six skills into a single policy that autonomously selects behaviors and transitions based on local geometry and commands. Experiments on a 29-DoF Unitree G1 humanoid demonstrate zero-shot sim-to-real traversal of 0.8 meter platforms (approximately 114% of leg length), with robust adaptation to platform height and initial pose, as well as smooth and stable multi-skill transitions.

</details>


### [169] [YOR: Your Own Mobile Manipulator for Generalizable Robotics](https://arxiv.org/abs/2602.11150)
*Manan H Anjaria,Mehmet Enes Erciyes,Vedant Ghatnekar,Neha Navarkar,Haritheja Etukuru,Xiaole Jiang,Kanad Patel,Dhawal Kabra,Nicholas Wojno,Radhika Ajay Prayage,Soumith Chintala,Lerrel Pinto,Nur Muhammad Mahi Shafiullah,Zichen Jeff Cui*

Main category: cs.RO

TL;DR: 本文介绍了YOR，一种开源、低成本的移动操作机器人平台，具备全身体移动与操作能力，成本低于10,000美元，并在协同全身控制、双臂操作和自主导航任务中验证了其能力。


<details>
  <summary>Details</summary>
Motivation: 当前移动操作机器人的最优形态（尤其在预算受限下）尚不明确，而机器人学习进展与执行器商品化推动了低成本平台需求。

Method: 设计并构建了YOR机器人：集成全向底盘、可伸缩垂直升降机构及双臂带夹爪，强调模块化、易组装性及低成本（全部使用现货组件）。

Result: YOR成功完成需协调全身控制、双臂操作和自主导航的任务，功能媲美高价平台，但成本仅为后者一小部分。

Conclusion: YOR为移动操作研究提供了一种高性价比、开源且实用的硬件平台，有望促进相关领域更广泛的研究与应用。

Abstract: Recent advances in robot learning have generated significant interest in capable platforms that may eventually approach human-level competence. This interest, combined with the commoditization of actuators, has propelled growth in low-cost robotic platforms. However, the optimal form factor for mobile manipulation, especially on a budget, remains an open question. We introduce YOR, an open-source, low-cost mobile manipulator that integrates an omnidirectional base, a telescopic vertical lift, and two arms with grippers to achieve whole-body mobility and manipulation. Our design emphasizes modularity, ease of assembly using off-the-shelf components, and affordability, with a bill-of-materials cost under 10,000 USD. We demonstrate YOR's capability by completing tasks that require coordinated whole-body control, bimanual manipulation, and autonomous navigation. Overall, YOR offers competitive functionality for mobile manipulation research at a fraction of the cost of existing platforms. Project website: https://www.yourownrobot.ai/

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [170] [AIvilization v0: Toward Large-Scale Artificial Social Simulation with a Unified Agent Architecture and Adaptive Agent Profiles](https://arxiv.org/abs/2602.10429)
*Wenkai Fan,Shurui Zhang,Xiaolong Wang,Haowei Yang,Tsz Wai Chan,Xingyan Chen,Junquan Bi,Zirui Zhou,Jia Liu,Kani Chen*

Main category: cs.MA

TL;DR: AIvilization v0 是一个公开部署的大规模人工社会，结合资源受限的沙盒经济与统一的 LLM 智能体架构，通过分层分支规划、双过程记忆代理画像和人机协同接口，实现长期自主性与环境适应性的平衡，并在模拟中复现真实经济现象。


<details>
  <summary>Details</summary>
Motivation: 缓解目标稳定性与响应正确性之间的张力，支持LLM智能体在动态环境中实现长周期自主运行。

Method: 提出三层机制：(i) 分层分支思考规划器，用于目标分解、仿真验证与分级重规划；(ii) 具有双过程记忆（短期执行痕迹/长期语义整合）的自适应代理画像；(iii) 多抽象层级的人在环路引导接口，通过记忆而非提示覆盖传递指令。环境建模包含生理生存成本、多级不可替代生产、AMM价格机制及门控教育-职业系统。

Result: 实证发现稳定市场可复现重尾收益、波动聚集等典型金融特征，并形成由教育与准入约束驱动的财富分层；消融实验表明完整架构在多目标、长周期任务（如延迟投资、持续探索）中鲁棒性显著优于简化规划器。

Conclusion: 该架构为构建可持续、可演化的LLM驱动人工社会提供了可行路径，强调结构化记忆、分层规划与人机协同对长期自治的关键作用。

Abstract: AIvilization v0 is a publicly deployed large-scale artificial society that couples a resource-constrained sandbox economy with a unified LLM-agent architecture, aiming to sustain long-horizon autonomy while remaining executable under rapidly changing environment. To mitigate the tension between goal stability and reactive correctness, we introduce (i) a hierarchical branch-thinking planner that decomposes life goals into parallel objective branches and uses simulation-guided validation plus tiered re-planning to ensure feasibility; (ii) an adaptive agent profile with dual-process memory that separates short-term execution traces from long-term semantic consolidation, enabling persistent yet evolving identity; and (iii) a human-in-the-loop steering interface that injects long-horizon objectives and short commands at appropriate abstraction levels, with effects propagated through memory rather than brittle prompt overrides. The environment integrates physiological survival costs, non-substitutable multi-tier production, an AMM-based price mechanism, and a gated education-occupation system. Using high-frequency transactions from the platforms mature phase, we find stable markets that reproduce key stylized facts (heavy-tailed returns and volatility clustering) and produce structured wealth stratification driven by education and access constraints. Ablations show simplified planners can match performance on narrow tasks, while the full architecture is more robust under multi-objective, long-horizon settings, supporting delayed investment and sustained exploration.

</details>


### [171] [An Ontology-driven Dynamic Knowledge Base for Uninhabited Ground Vehicles](https://arxiv.org/abs/2602.10555)
*Hsan Sandar Win,Andrew Walters,Cheng-Chew Lim,Daniel Webber,Seth Leslie,Tan Doan*

Main category: cs.MA

TL;DR: 本文提出了动态上下文任务数据（DCMD）的概念，构建了一个面向无人地面车辆（UGVs）战术边缘应用的本体驱动动态知识库，以提升态势感知、自主决策与环境适应能力。


<details>
  <summary>Details</summary>
Motivation: UGVs过度依赖任务前预置的先验信息，在任务中遭遇意外情况时易产生识别歧义并增加人工干预需求；需通过引入上下文信息动态更新先验知识以释放其全部潜力。

Method: 设计并实现了一个基于本体的知识表示方法，结合近实时信息获取与分析，支持任务中平台端的DCMD动态更新，并在四台UGV组成的实验室监视任务中进行验证。

Result: 实验表明该本体驱动的动态环境表征具备机器可操作性，能生成有效上下文信息，成功支撑了及时、有效的任务执行，并直接提升了态势感知能力。

Conclusion: DCMD与本体驱动的动态知识库是提升UGVs在复杂动态环境中自主性与适应性的有效途径。

Abstract: In this paper, the concept of Dynamic Contextual Mission Data (DCMD) is introduced to develop an ontology-driven dynamic knowledge base for Uninhabited Ground Vehicles (UGVs) at the tactical edge. The dynamic knowledge base with DCMD is added to the UGVs to: support enhanced situation awareness; improve autonomous decision making; and facilitate agility within complex and dynamic environments. As UGVs are heavily reliant on the a priori information added pre-mission, unexpected occurrences during a mission can cause identification ambiguities and require increased levels of user input. Updating this a priori information with contextual information can help UGVs realise their full potential. To address this, the dynamic knowledge base was designed using an ontology-driven representation, supported by near real-time information acquisition and analysis, to provide in-mission on-platform DCMD updates. This was implemented on a team of four UGVs that executed a laboratory based surveillance mission. The results showed that the ontology-driven dynamic representation of the UGV operational environment was machine actionable, producing contextual information to support a successful and timely mission, and contributed directly to the situation awareness.

</details>


### [172] [Beyond Task Performance: A Metric-Based Analysis of Sequential Cooperation in Heterogeneous Multi-Agent Destructive Foraging](https://arxiv.org/abs/2602.10685)
*Alejandro Mendoza Barrionuevo,Samuel Yanes Luis,Daniel Gutiérrez Reina,Sergio L. Toral Marín*

Main category: cs.MA

TL;DR: 本文提出了一套通用的多智能体合作度量体系，用于在部分可观测、角色时序依赖的异构多智能体系统中（以破坏性多智能体觅食为背景）量化效率、协调性、依赖性、公平性与敏感性，并在受水面清洁启发的真实场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注任务完成性能，缺乏对合作内在特性（如协调、依赖、公平等）的系统刻画；且多数指标不可迁移，难以泛化到其他序贯多智能体领域。

Method: 设计了三类可迁移的合作度量：基础指标（效率等）、队间指标（团队间依赖与协作）、队内指标（团队内部协调与公平），并在基于动态水面清洁的破坏性觅食仿真环境中进行实证评估，对比了学习型与启发式算法。

Result: 所提指标能有效区分不同算法在合作质量上的差异，揭示其在协调性、角色依赖和公平性等方面的特性，且具备跨域迁移潜力。

Conclusion: 该度量体系为异构、部分可观测、时序依赖的多智能体系统提供了可解释、可比较、可迁移的合作分析框架，弥补了仅以任务性能为导向的评估局限。

Abstract: This work addresses the problem of analyzing cooperation in heterogeneous multi-agent systems which operate under partial observability and temporal role dependency, framed within a destructive multi-agent foraging setting. Unlike most previous studies, which focus primarily on algorithmic performance with respect to task completion, this article proposes a systematic set of general-purpose cooperation metrics aimed at characterizing not only efficiency, but also coordination and dependency between teams and agents, fairness, and sensitivity. These metrics are designed to be transferable to different multi-agent sequential domains similar to foraging. The proposed suite of metrics is structured into three main categories that jointly provide a multilevel characterization of cooperation: primary metrics, inter-team metrics, and intra-team metrics. They have been validated in a realistic destructive foraging scenario inspired by dynamic aquatic surface cleaning using heterogeneous autonomous vehicles. It involves two specialized teams with sequential dependencies: one focused on the search of resources, and another on their destruction. Several representative approaches have been evaluated, covering both learning-based algorithms and classical heuristic paradigms.

</details>


### [173] [The emergence of numerical representations in communicating artificial agents](https://arxiv.org/abs/2602.10996)
*Daniela Mihai,Lucas Weber,Francesca Franzon*

Main category: cs.MA

TL;DR: 本文研究了在没有预设数字概念的情况下，两个基于神经网络的智能体在指称博弈中通过离散符号或连续草图交流数量的能力。结果表明，仅靠交流压力足以实现已学数量的精确传输，但要产生组合性编码和泛化能力还需额外压力。


<details>
  <summary>Details</summary>
Motivation: 探究仅靠交流压力是否足以使人工智能体自发产生数值表征，以及这些表征是否类似于人类数字符号。

Method: 使用两个基于神经网络的智能体，在指称博弈中分别采用离散符号或连续草图进行数量交流，不预设任何数字概念。

Result: 智能体在分布内实现了高通信准确率，并收敛于高精度的符号-意义映射；但其生成的代码是非组合性的，在面对未见过的数量时无法系统性地生成新消息。

Conclusion: 仅靠交流压力足以实现已学数量的精确传输，但需要额外压力才能产生组合性编码和泛化能力。

Abstract: Human languages provide efficient systems for expressing numerosities, but whether the sheer pressure to communicate is enough for numerical representations to arise in artificial agents, and whether the emergent codes resemble human numerals at all, remains an open question. We study two neural network-based agents that must communicate numerosities in a referential game using either discrete tokens or continuous sketches, thus exploring both symbolic and iconic representations. Without any pre-defined numeric concepts, the agents achieve high in-distribution communication accuracy in both communication channels and converge on high-precision symbol-meaning mappings. However, the emergent code is non-compositional: the agents fail to derive systematic messages for unseen numerosities, typically reusing the symbol of the highest trained numerosity (discrete), or collapsing extrapolated values onto a single sketch (continuous). We conclude that the communication pressure alone suffices for precise transmission of learned numerosities, but additional pressures are needed to yield compositional codes and generalisation abilities.

</details>


### [174] [Learning to Compose for Cross-domain Agentic Workflow Generation](https://arxiv.org/abs/2602.11114)
*Jialiang Wang,Shengxiang Xu,Hanmo Liu,Jiachuan Wang,Yuyu Luo,Shimin Di,Min-Ling Zhang,Lei Chen*

Main category: cs.MA

TL;DR: 本文提出了一种基于分解-重组-决策机制的单次生成式工作流生成方法，通过学习跨域可复用的工作流能力基元，在多域和未见域任务上超越了需20次迭代的SOTA优化方法，显著降低延迟与成本。


<details>
  <summary>Details</summary>
Motivation: 现有工作流生成系统在领域迁移时依赖高成本、不稳定的多次迭代优化，缺乏跨域泛化能力。

Method: 将分解-重组-决策机制内化到开源LLM中：分解——学习跨域可复用的紧凑工作流能力集；重组——将输入任务稀疏映射到能力基元上单次生成工作流；决策——通过反事实归因分析各能力对成功与否的边际贡献。

Result: 在多域、跨域及未见域严格评测中，该单次生成器性能超越需20次迭代的SOTA优化基线，同时大幅降低生成延迟与计算成本。

Conclusion: 内化可解释、可归因的分解-重组-决策机制，能实现高效、稳定、跨域的工作流生成，摆脱对昂贵迭代优化的依赖。

Abstract: Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically rely on iterative workflow refinement to discover a feasible workflow from a large workflow space, incurring high iteration costs and yielding unstable, domain-specific behavior. In response, we internalize a decompose-recompose-decide mechanism into an open-source LLM for cross-domain workflow generation. To decompose, we learn a compact set of reusable workflow capabilities across diverse domains. To recompose, we map each input task to a sparse composition over these bases to generate a task-specific workflow in a single pass. To decide, we attribute the success or failure of workflow generation to counterfactual contributions from learned capabilities, thereby capturing which capabilities actually drive success by their marginal effects. Across stringent multi-domain, cross-domain, and unseen-domain evaluations, our 1-pass generator surpasses SOTA refinement baselines that consume 20 iterations, while substantially reducing generation latency and cost.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [175] [Large Language Models Predict Functional Outcomes after Acute Ischemic Stroke](https://arxiv.org/abs/2602.10119)
*Anjali K. Kapoor,Anton Alyakin,Jin Vivian Lee,Eunice Yang,Annelene M. Schulze,Krithik Vishwanath,Jinseok Lee,Yindalon Aphinyanaphongs,Howard Riina,Jennifer A. Frontera,Eric Karl Oermann*

Main category: cs.LG

TL;DR: 本研究探索了大型语言模型（LLMs）直接从急性缺血性卒中患者入院病历中预测改良Rankin量表（mRS）评分的能力，发现微调后的Llama模型在出院和90天功能预后预测上表现优异，性能媲美依赖结构化数据（如NIHSS、年龄）的传统模型。


<details>
  <summary>Details</summary>
Motivation: 现有mRS预测方法主要依赖结构化变量和传统机器学习，而利用常规入院文本直接预测的功能尚待探索；临床亟需无需人工提取结构化数据、可无缝嵌入工作流的文本驱动预后工具。

Method: 在真实世界卒中登记库（NYU Langone GWTG-Stroke，2016–2025）上，评估多种编码器（BERT、NYUTron）与生成式LLM（Llama-3.1-8B、MedGemma-4B）在冻结与微调模式下对出院及90天mRS的预测能力；采用时间划分（最近12个月为测试集），以7分类准确率和二分类（mRS 0–2 vs. 3–6）准确率评估，并与基于NIHSS和年龄的结构化基线模型对比。

Result: 微调Llama在90天预测中达到33.9%的7类mRS准确率和76.3%的二分类准确率；出院预测达42.0%和75.0%；其90天性能与结构化基线模型相当。

Conclusion: 仅使用入院病历文本、经微调的LLM即可实现与结构化模型相当的卒中功能预后预测效果，支持开发免人工数据提取、易集成于临床流程的文本型预后工具。

Abstract: Accurate prediction of functional outcomes after acute ischemic stroke can inform clinical decision-making and resource allocation. Prior work on modified Rankin Scale (mRS) prediction has relied primarily on structured variables (e.g., age, NIHSS) and conventional machine learning. The ability of large language models (LLMs) to infer future mRS scores directly from routine admission notes remains largely unexplored. We evaluated encoder (BERT, NYUTron) and generative (Llama-3.1-8B, MedGemma-4B) LLMs, in both frozen and fine-tuned settings, for discharge and 90-day mRS prediction using a large, real-world stroke registry. The discharge outcome dataset included 9,485 History and Physical notes and the 90-day outcome dataset included 1,898 notes from the NYU Langone Get With The Guidelines-Stroke registry (2016-2025). Data were temporally split with the most recent 12 months held out for testing. Performance was assessed using exact (7-class) mRS accuracy and binary functional outcome (mRS 0-2 vs. 3-6) accuracy and compared against established structured-data baselines incorporating NIHSS and age. Fine-tuned Llama achieved the highest performance, with 90-day exact mRS accuracy of 33.9% [95% CI, 27.9-39.9%] and binary accuracy of 76.3% [95% CI, 70.7-81.9%]. Discharge performance reached 42.0% [95% CI, 39.0-45.0%] exact accuracy and 75.0% [95% CI, 72.4-77.6%] binary accuracy. For 90-day prediction, Llama performed comparably to structured-data baselines. Fine-tuned LLMs can predict post-stroke functional outcomes from admission notes alone, achieving performance comparable to models requiring structured variable abstraction. Our findings support the development of text-based prognostic tools that integrate seamlessly into clinical workflows without manual data extraction.

</details>


### [176] [Towards Autonomous Mathematics Research](https://arxiv.org/abs/2602.10177)
*Tony Feng,Trieu H. Trinh,Garrett Bingham,Dawsen Hwang,Yuri Chervonyi,Junehyuk Jung,Joonkyung Lee,Carlo Pagano,Sang-hyun Kim,Federico Pasqualotto,Sergei Gukov,Jonathan N. Lee,Junsu Kim,Kaiying Hou,Golnaz Ghiasi,Yi Tay,YaGuang Li,Chenkai Kuang,Yuan Liu,Hanzhao,Lin,Evan Zheran Liu,Nigamaa Nayakanti,Xiaomeng Yang,Heng-tze Cheng,Demis Hassabis,Koray Kavukcuoglu,Quoc V. Le,Thang Luong*

Main category: cs.LG

TL;DR: 本文介绍了Aletheia，一个能够自主生成、验证和修订数学研究解决方案的AI代理，它在奥林匹克竞赛级别到博士级问题上均表现出色，并成功参与了多项AI辅助数学研究里程碑式的工作。


<details>
  <summary>Details</summary>
Motivation: 从竞赛级问题解决过渡到专业数学研究需要处理大量文献并构建长周期证明，现有模型尚不能满足这一需求。

Method: Aletheia基于增强版Gemini Deep Think模型，结合新型推理时扩展定律和密集工具调用，在自然语言中端到端迭代生成、验证与修订数学解。

Result: Aletheia成功完成奥林匹克问题、博士级练习，并推动三项AI辅助数学研究里程碑：完全由AI撰写的论文Feng26、人机协作论文LeeSeo26，以及对700个开放问题的大规模半自主评估（含4个自主解决的开放问题）。

Conclusion: 作者建议建立衡量AI辅助数学成果自主性与新颖性的标准层级，并反思人机协同在数学研究中的未来角色。

Abstract: Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.

</details>


### [177] [Signature-Kernel Based Evaluation Metrics for Robust Probabilistic and Tail-Event Forecasting](https://arxiv.org/abs/2602.10182)
*Benjamin R. Redhead,Thomas L. Lee,Peng Gu,Víctor Elvira,Amos Storkey*

Main category: cs.LG

TL;DR: 本文提出两种基于核函数的概率预测评估指标Sig-MMD和CSig-MMD，以解决现有框架忽略时序/变量依赖性及对尾部事件不敏感的问题；CSig-MMD通过截断机制增强尾部敏感性并保持评分规则的合理性。


<details>
  <summary>Details</summary>
Motivation: 现有概率预测评估框架缺乏共识指标，且常错误假设时间步或变量间独立，同时对关键的尾部事件不敏感。

Method: 提出基于签名核（signature kernel）的Sig-MMD和新型截断式Sig-MMD（CSig-MMD），利用签名核建模复杂跨变量与跨时间依赖，并引入截断机制提升尾部事件评估能力，同时保证评分规则的合理性（properness）。

Result: 所提指标能更可靠地评估直接多步概率预测，支持更鲁棒算法的开发；CSig-MMD在保持properness前提下显著增强对尾部事件的敏感性。

Conclusion: Sig-MMD与CSig-MMD为高风险领域中的概率预测提供了更具表达力、鲁棒性和实践意义的评估工具。

Abstract: Probabilistic forecasting is increasingly critical across high-stakes domains, from finance and epidemiology to climate science. However, current evaluation frameworks lack a consensus metric and suffer from two critical flaws: they often assume independence across time steps or variables, and they demonstrably lack sensitivity to tail events, the very occurrences that are most pivotal in real-world decision-making. To address these limitations, we propose two kernel-based metrics: the signature maximum mean discrepancy (Sig-MMD) and our novel censored Sig-MMD (CSig-MMD). By leveraging the signature kernel, these metrics capture complex inter-variate and inter-temporal dependencies and remain robust to missing data. Furthermore, CSig-MMD introduces a censoring scheme that prioritizes a forecaster's capability to predict tail events while strictly maintaining properness, a vital property for a good scoring rule. These metrics enable a more reliable evaluation of direct multi-step forecasting, facilitating the development of more robust probabilistic algorithms.

</details>


### [178] [Versor: A Geometric Sequence Architecture](https://arxiv.org/abs/2602.10195)
*Truong Minh Huy,Edward Hirst*

Main category: cs.LG

TL;DR: Versor是一种基于共形几何代数（CGA）的新序列架构，通过在Cl_{4,1}流形上嵌入状态并用旋子（rotors）进行几何演化，实现SE(3)-等变性、结构泛化、高效率与强可解释性，在多项任务中显著超越Transformer等基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型（如Transformer）缺乏对几何结构的内在建模能力，难以实现结构泛化、等变性与可解释性；而物理与科学建模亟需具备几何感知能力的高效、鲁棒架构。

Method: 提出Versor架构：将状态嵌入Cl_{4,1}共形几何代数空间，以旋子作为基本演化单元实现几何变换；引入递归旋子累加器（Recursive Rotor Accumulator）实现O(L)线性复杂度；设计定制化Clifford核加速计算；注意力机制自然解耦为邻近性与朝向性分量。

Result: 在N体混沌动力学、拓扑推理及CIFAR-10/WikiText-103等基准上全面超越Transformer、图网络及GATr/EGNN；参数量减少200倍；零样本尺度泛化MCC达99.3%（ViT仅50.4%）；OOD下预测稳定；Clifford核加速最高78倍。

Conclusion: Versor验证了几何代数作为深度学习基础表示的可行性，为科学建模提供了兼具表达力、效率、可解释性与泛化性的新范式。

Abstract: A novel sequence architecture design is introduced, Versor, which uses Conformal Geometric Algebra (CGA) in place of the traditional fundamental non-linear operations to achieve structural generalization and significant performance improvements on a variety of tasks, while offering improved interpretability and efficiency. By embedding states in the $Cl_{4,1}$ manifold and evolving them via geometric transformations (rotors), Versor natively represents $SE(3)$-equivariant relationships without requiring explicit structural encoding. Versor is validated on chaotic N-body dynamics, topological reasoning, and standard multimodal benchmarks (CIFAR-10, WikiText-103), consistently outperforming Transformers, Graph Networks, and geometric baselines (GATr, EGNN). Key results include: orders of magnitude fewer parameters ($200\times$ vs. Transformers); interpretable attention decomposing into proximity and orientational components; zero-shot scale generalization (99.3% MCC on topology vs. 50.4% for ViT); and $O(L)$ linear complexity via the novel Recursive Rotor Accumulator. In out-of-distribution tests, Versor maintains stable predictions while Transformers fail catastrophically. Custom Clifford kernels achieve up to $78\times$ speedup, providing a scalable foundation for geometrically-aware scientific modeling.

</details>


### [179] [Adaptive Optimization via Momentum on Variance-Normalized Gradients](https://arxiv.org/abs/2602.10204)
*Francisco Patitucci,Aryan Mokhtari*

Main category: cs.LG

TL;DR: MVN-Grad 是一种改进的 Adam 类优化器，通过先对梯度进行基于方差的归一化、再应用动量，提升了训练稳定性与性能，并在多个基准任务上优于 Adam 等主流优化器。


<details>
  <summary>Details</summary>
Motivation: 解决标准 Adam 类优化器中动量与归一化因子跨时间耦合导致的不稳定性和对异常梯度敏感的问题。

Method: 提出 MVN-Grad：对梯度估计不确定性（即方差）做指数滑动平均，以此归一化各坐标梯度，再对归一化后的梯度施加动量；理论证明其单步更新条件方差更小且对梯度尖峰有界响应。

Result: 在 CIFAR-100 图像分类和 GPT 风格语言建模任务上，MVN-Grad 匹配或超越 Adam、AdaBelief 和 LaProp，训练更平滑、泛化更好，且无额外计算开销。

Conclusion: MVN-Grad 通过解耦动量与归一化，提升了优化器的鲁棒性、稳定性和收敛性，是一种高效实用的新型自适应优化方法。

Abstract: We introduce MVN-Grad (Momentum on Variance-Normalized Gradients), an Adam-style optimizer that improves stability and performance by combining two complementary ideas: variance-based normalization and momentum applied after normalization. MVN-Grad scales each coordinate by an exponential moving average of gradient uncertainty and applies momentum to the resulting normalized gradients, eliminating the cross-time coupling between stale momentum and a stochastic normalizer present in standard Adam-type updates. We prove that this decoupling yields strictly smaller one-step conditional update variance than momentum-then-normalize variance methods under standard noise assumptions, and that MVN-Grad is robust to outliers: it has a uniformly bounded response to single gradient spikes.
  In low-variance regimes, we further show variance normalization avoids sign-type collapse associated with second-moment scaling and can yield accelerated convergence. Across CIFAR-100 image classification and GPT-style language modeling benchmarks, MVN-Grad matches or outperforms Adam, AdaBelief, and LaProp, delivering smoother training and improved generalization with no added overhead.

</details>


### [180] [Neural Network Quantum Field Theory from Transformer Architectures](https://arxiv.org/abs/2602.10209)
*Dmitry S. Ageev,Yulia A. Ageeva*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer注意力头的神经网络构建欧几里得标量量子场论（NN-QFT）的方法，通过随机网络参数平均定义n点关联函数；发现单头注意力在无限宽度极限下仍保持非高斯场统计特性，并识别出一种在无限宽度下仍有限的‘独立性破坏’四点关联贡献；多头叠加并在标准归一化下可使非高斯关联随头数增加而抑制，最终趋于高斯NN-QFT。


<details>
  <summary>Details</summary>
Motivation: 将深度学习模型（特别是Transformer）与量子场论建立严格联系，探索神经网络在无限宽度极限下的统计场论行为，为理解大模型内在物理结构提供新视角。

Method: 在NN-QFT框架下，利用随机初始化的Transformer单头注意力机制建模标量场，通过平均随机参数定义n点关联函数；采用注意力权重表象计算两点函数，设计随机特征词嵌入实现欧氏不变核；解析推导连通四点函数并分离‘独立性破坏’项；分析多头叠加及1/N_h归一化对高斯性的渐近影响。

Result: 单注意力头在d_k→∞极限下仍产生非高斯场统计；两点函数可构造为欧氏不变形式；四点连通函数中存在不随宽度消失的‘独立性破坏’协方差项；多头叠加+1/N_h归一化使非高斯关联按1/N_h衰减，大头数极限下NN-QFT趋于高斯理论。

Conclusion: Transformer注意力机制天然蕴含非平凡量子场论结构，其宽度和头数构成可调控的‘场论参数’，为连接深度学习与统计物理/量子场论提供了可计算的桥梁。

Abstract: We propose a neural-network construction of Euclidean scalar quantum field theories from transformer attention heads, defining $n$-point correlators by averaging over random network parameters in the NN-QFT framework. For a single attention head, shared random softmax weights couple different width coordinates and induce non-Gaussian field statistics that persist in the infinite-width limit $d_k\to\infty$. We compute the two-point function in an attention-weight representation and show how Euclidean-invariant kernels can be engineered via random-feature token embeddings. We then analyze the connected four-point function and identify an "independence-breaking" contribution, expressible as a covariance over query-key weights, which remains finite at infinite width. Finally, we show that summing many independent heads with standard $1/N_h$ normalization suppresses connected non-Gaussian correlators as $1/N_h$, yielding a Gaussian NN-QFT in the large-head limit.

</details>


### [181] [How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge](https://arxiv.org/abs/2602.10210)
*Junhong Lin,Bing Zhang,Song Wang,Ziyan Liu,Dan Gutfreund,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: 本文提出HybridRAG-Bench，一个用于评估大语言模型在混合知识（非结构化文本+结构化知识图谱）上进行多跳推理能力的基准构建框架，强调避免预训练数据污染，支持按领域和时间定制，实验证明其能有效区分真实检索推理与参数化记忆。


<details>
  <summary>Details</summary>
Motivation: 现有评测基准与LLM预训练数据重叠严重，难以区分模型是靠真实外部知识检索推理，还是依赖已编码于参数中的知识（parametric recall）。

Method: 提出HybridRAG-Bench框架，自动融合arXiv近期论文生成的非结构化文本与结构化知识图谱，构建基于显式推理路径的知识密集型问答对；支持按领域和时间范围灵活定制，确保评测无数据污染。

Result: 在人工智能、治理与政策、生物信息学三个领域实验表明，HybridRAG-Bench能有效识别并奖励真实检索与多跳推理能力，而非参数化回忆；代码与数据已开源。

Conclusion: HybridRAG-Bench为混合知识增强型推理系统提供了可控、污染感知、可扩展的评测新范式。

Abstract: Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.

</details>


### [182] [Rank-Accuracy Trade-off for LoRA: A Gradient-Flow Analysis](https://arxiv.org/abs/2602.10212)
*Michael Rushka,Diego Klabjan*

Main category: cs.LG

TL;DR: 本文从动力系统角度分析LoRA微调中更新秩与准确率的关系，通过梯度流分析建立了秩与准确率的显式闭式关系。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA在实践中表现出色，但其准确率对更新秩的依赖性缺乏理论支撑，本文旨在填补这一理论空白。

Method: 采用动力系统视角，对全秩和低秩情形分别进行梯度流分析，并严格推导LoRA梯度流方程，证明其在同步与顺序更新下形式一致。

Result: 得到了LoRA秩与准确率在迹平方损失和Frobenius范数低秩近似损失下的闭式关系。

Conclusion: LoRA的准确率可被其更新秩显式刻画，为低秩微调提供了坚实的理论基础。

Abstract: Previous empirical studies have shown that LoRA achieves accuracy comparable to full-parameter methods on downstream fine-tuning tasks, even for rank-1 updates. By contrast, the theoretical underpinnings of the dependence of LoRA's accuracy on update rank remain relatively unexplored. In this work, we compare the accuracy of rank-r LoRA updates against full-parameter updates for fine-tuning tasks from a dynamical systems perspective. We perform gradient flow analysis in both full-rank and low-rank regimes to establish explicit relationships between rank and accuracy for two loss functions under LoRA. While gradient flow equations for LoRA are presented in prior work, we rigorously derive their form and show that they are identical for simultaneous and sequential LoRA parameter updates. We then use the resulting dynamical system equations to obtain closed-form relationships between LoRA rank and accuracy for trace-squared and Frobenius-norm low-rank approximation loss functions.

</details>


### [183] [ELROND: Exploring and decomposing intrinsic capabilities of diffusion models](https://arxiv.org/abs/2602.10216)
*Paweł Skierś,Tomasz Trzciński,Kamil Deja*

Main category: cs.LG

TL;DR: 本文提出了一种在扩散模型输入嵌入空间中直接解耦语义方向的新框架，通过分析固定文本提示下不同随机生成结果的梯度差异，并用PCA或稀疏自编码器分解，实现对生成图像语义变化的精细可控调节。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法在生成过程中直接控制文本提示导致的语义变化，仅依赖输出特征分析而忽略生成机制本身。

Method: 采集同一文本提示下不同随机生成样本的输出差异梯度，在输入嵌入空间中进行主成分分析（PCA）或稀疏自编码器分解，提取可解释、可调控的语义方向。

Result: 实现了对单一概念的细粒度语义控制；缓解了蒸馏模型中的模式崩溃问题，恢复生成多样性；提出了基于发现子空间维度的概念复杂度新估计器。

Conclusion: 该方法将语义控制前移至生成过程内部，在不修改模型结构的前提下，提升了可控性、多样性与可解释性，为理解扩散模型的内在语义结构提供了新视角。

Abstract: A single text prompt passed to a diffusion model often yields a wide range of visual outputs determined solely by stochastic process, leaving users with no direct control over which specific semantic variations appear in the image. While existing unsupervised methods attempt to analyze these variations via output features, they omit the underlying generative process. In this work, we propose a framework to disentangle these semantic directions directly within the input embedding space. To that end, we collect a set of gradients obtained by backpropagating the differences between stochastic realizations of a fixed prompt that we later decompose into meaningful steering directions with either Principal Components Analysis or Sparse Autoencoder. Our approach yields three key contributions: (1) it isolates interpretable, steerable directions for precise, fine-grained control over a single concept; (2) it effectively mitigates mode collapse in distilled models by reintroducing lost diversity; and (3) it establishes a novel estimator for concept complexity under a specific model, based on the dimensionality of the discovered subspace.

</details>


### [184] [Temper-Then-Tilt: Principled Unlearning for Generative Models through Tempering and Classifier Guidance](https://arxiv.org/abs/2602.10217)
*Jacob L. Block,Mehryar Mohri,Aryan Mokhtari,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 本文提出了一种针对大生成模型的机器遗忘新方法T3-Unlearning，通过密度比估计而非监督微调实现遗忘，结合温度调节和倾斜校正两步推理，在集中分布数据上显著提升遗忘质量和生成效用。


<details>
  <summary>Details</summary>
Motivation: 现有基于分类器引导的遗忘方法在遗忘集为尖锐、集中分布时，因有限样本难以准确估计密度比而失败。

Method: 提出Temper-Then-Tilt Unlearning（T3-Unlearning）：先对基础模型分布进行温度调节以压制高置信度尖峰，再用轻量级分类器对调节后分布进行倾斜校正；理论分析给出分类器风险与遗忘误差的有限样本界，并证明温度调节对集中分布遗忘的必要性。

Result: 在TOFU基准测试中，T3-Unlearning相比现有基线显著提升遗忘质量与生成效用，仅训练少量参数且运行开销极小。

Conclusion: 密度比估计框架下引入温度调节是解决集中分布数据遗忘难题的关键，T3-Unlearning为大模型高效、可靠遗忘提供了新范式。

Abstract: We study machine unlearning in large generative models by framing the task as density ratio estimation to a target distribution rather than supervised fine-tuning. While classifier guidance is a standard approach for approximating this ratio and can succeed in general, we show it can fail to faithfully unlearn with finite samples when the forget set represents a sharp, concentrated data distribution. To address this, we introduce Temper-Then-Tilt Unlearning (T3-Unlearning), which freezes the base model and applies a two-step inference procedure: (i) tempering the base distribution to flatten high-confidence spikes, and (ii) tilting the tempered distribution using a lightweight classifier trained to distinguish retain from forget samples. Our theoretical analysis provides finite-sample guarantees linking the surrogate classifier's risk to unlearning error, proving that tempering is necessary to successfully unlearn for concentrated distributions. Empirical evaluations on the TOFU benchmark show that T3-Unlearning improves forget quality and generative utility over existing baselines, while training only a fraction of the parameters with a minimal runtime.

</details>


### [185] [Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2602.10224)
*Shiting Huang,Zecheng Li,Yu Zeng,Qingnan Ren,Zhen Fang,Qisheng Su,Kou Shi,Lin Chen,Zehui Chen,Feng Zhao*

Main category: cs.LG

TL;DR: 本文提出Meta-Experience Learning (MEL)框架，通过LLM自验证能力进行对比分析，识别推理错误分叉点，并将可复用的‘元经验’内化至参数记忆中，从而突破RLVR在细粒度信用分配与知识复用上的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习与可验证奖励（RLVR）方法缺乏类人的错误归因与经验内化机制，导致细粒度信用分配困难和可复用知识（即‘元经验’）难以形成。

Method: 在标准RLVR基础上，引入基于自验证的对比分析机制，对正确与错误推理轨迹进行配对分析，定位推理错误的精确分叉点，并将总结出的元经验通过最小化负对数似然方式内化至LLM参数记忆中，生成语言建模的奖励信号。

Result: MEL在多个基准测试中实现稳定提升，在不同规模模型上均取得3.92%–4.73%的Pass@1增益。

Conclusion: MEL通过显式建模和内化元经验，有效增强了LLM的推理纠错与知识复用能力，为RLVR范式提供了可扩展的元学习增强路径。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.

</details>


### [186] [Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents](https://arxiv.org/abs/2602.10226)
*Haochen Wang,Yi Wu,Daryl Chang,Li Wei,Lukasz Heldt*

Main category: cs.LG

TL;DR: 本文提出了一种基于Gemini大语言模型的自演化系统，用于自动优化大规模推荐模型，在YouTube生产环境中验证了其在开发效率和模型性能上优于传统人工迭代方法。


<details>
  <summary>Details</summary>
Motivation: 传统大规模机器学习系统（如视频平台推荐模型）的优化依赖大量人工迭代，难以高效探索超参空间、设计优化器、架构和奖励函数以捕捉复杂用户行为。

Method: 构建双环自演化系统：离线代理（内环）利用代理指标高速生成假设；在线代理（外环）在真实线上环境中用延迟的‘北极星’业务指标验证候选方案；两个代理均模拟专业机器学习工程师，具备深度推理能力，可自主生成、训练和部署复杂模型改进。

Result: 该系统已在YouTube成功实现多次线上部署，显著提升了开发速度与模型性能，验证了LLM驱动的自主演化可行性与优越性。

Conclusion: 基于LLM的自演化系统可有效替代传统人工优化流程，在大规模推荐系统中实现更快速、更高质量的模型迭代与升级。

Abstract: Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achieving substantial improvements in these areas is a non-trivial task, traditionally relying on extensive manual iterations to test new hypotheses. We propose a self-evolving system that leverages Large Language Models (LLMs), specifically those from Google's Gemini family, to autonomously generate, train, and deploy high-performing, complex model changes within an end-to-end automated workflow. The self-evolving system is comprised of an Offline Agent (Inner Loop) that performs high-throughput hypothesis generation using proxy metrics, and an Online Agent (Outer Loop) that validates candidates against delayed north star business metrics in live production. Our agents act as specialized Machine Learning Engineers (MLEs): they exhibit deep reasoning capabilities, discovering novel improvements in optimization algorithms and model architecture, and formulating innovative reward functions that target long-term user engagement. The effectiveness of this approach is demonstrated through several successful production launches at YouTube, confirming that autonomous, LLM-driven evolution can surpass traditional engineering workflows in both development velocity and model performance.

</details>


### [187] [PRISM: Differentially Private Synthetic Data with Structure-Aware Budget Allocation for Prediction](https://arxiv.org/abs/2602.10228)
*Amir Asiaee,Chao Yan,Zachary B. Abrams,Bradley A. Malin*

Main category: cs.LG

TL;DR: 本文提出PRISM方法，一种面向预测任务的差分隐私合成数据机制，根据可用结构知识（因果、图模型或纯预测）选择特征子集、构建目标摘要统计、优化隐私预算分配，并通过图模型推理生成合成数据，在保证端到端差分隐私的同时显著提升预测准确性，尤其在分布偏移下优于传统相关性选择方法。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私合成数据方法对所有特征一视同仁，均匀加噪，忽视下游预测任务需求，导致噪声累积和预测性能下降；尤其在分布偏移或缺乏结构知识时表现不佳。

Method: 提出PRISM机制：按因果/图形/预测三类结构知识 regime 识别预测相关特征子集（如因果父母、Markov毯或DP特征选择），构建针对性摘要统计，依据预测误差上界优化隐私预算分配，并基于图模型进行合成数据生成；理论证明端到端差分隐私与风险界。

Result: 实验表明，任务感知的预算分配显著提升预测精度；在分布偏移下，以因果父母为目标的方法AUC达≈0.73，而相关性选择降至≈0.49（近随机）。

Conclusion: 面向预测任务定制差分隐私合成策略（如PRISM）可兼顾隐私保障与实用性，结构知识引导的特征选择与预算分配是提升鲁棒预测性能的关键。

Abstract: Differential privacy (DP) provides a mathematical guarantee limiting what an adversary can learn about any individual from released data. However, achieving this protection typically requires adding noise, and noise can accumulate when many statistics are measured. Existing DP synthetic data methods treat all features symmetrically, spreading noise uniformly even when the data will serve a specific prediction task.
  We develop a prediction-centric approach operating in three regimes depending on available structural knowledge. In the causal regime, when the causal parents of $Y$ are known and distribution shift is expected, we target the parents for robustness. In the graphical regime, when a Bayesian network structure is available and the distribution is stable, the Markov blanket of $Y$ provides a sufficient feature set for optimal prediction. In the predictive regime, when no structural knowledge exists, we select features via differentially private methods without claiming to recover causal or graphical structure.
  We formalize this as PRISM, a mechanism that (i) identifies a predictive feature subset according to the appropriate regime, (ii) constructs targeted summary statistics, (iii) allocates budget to minimize an upper bound on prediction error, and (iv) synthesizes data via graphical-model inference. We prove end-to-end privacy guarantees and risk bounds. Empirically, task-aware allocation improves prediction accuracy compared to generic synthesizers. Under distribution shift, targeting causal parents achieves AUC $\approx 0.73$ while correlation-based selection collapses to chance ($\approx 0.49$).

</details>


### [188] [Chamfer-Linkage for Hierarchical Agglomerative Clustering](https://arxiv.org/abs/2602.10444)
*Kishen N Gowda,Willem Fletcher,MohammadHossein Bateni,Laxman Dhulipala,D Ellis Hershkowitz,Rajesh Jayaram,Jakub Łącki*

Main category: cs.LG

TL;DR: 本文提出了一种新的层次聚类链接函数——Chamfer-linkage，利用点云间常用的Chamfer距离来度量簇间距离，在理论效率（O(n²)）和实验效果上均优于传统链接方法。


<details>
  <summary>Details</summary>
Motivation: 传统HAC的链接函数（如单链接、平均链接、Ward法）在真实数据上表现不稳定，缺乏一致的高质量聚类能力。

Method: 提出基于Chamfer距离的新型链接函数Chamfer-linkage，并证明其可在O(n²)时间内实现，与经典链接函数效率相当。

Result: 在多种真实数据集上的实验表明，Chamfer-linkage在聚类质量上持续优于平均链接和Ward法等经典方法。

Conclusion: Chamfer-linkage是一种理论高效、实践有效、可直接替代经典链接函数的新方案，拓展了层次聚类的理论与应用工具箱。

Abstract: Hierarchical Agglomerative Clustering (HAC) is a widely-used clustering method based on repeatedly merging the closest pair of clusters, where inter-cluster distances are determined by a linkage function. Unlike many clustering methods, HAC does not optimize a single explicit global objective; clustering quality is therefore primarily evaluated empirically, and the choice of linkage function plays a crucial role in practice. However, popular classical linkages, such as single-linkage, average-linkage and Ward's method show high variability across real-world datasets and do not consistently produce high-quality clusterings in practice.
  In this paper, we propose \emph{Chamfer-linkage}, a novel linkage function that measures the distance between clusters using the Chamfer distance, a popular notion of distance between point-clouds in machine learning and computer vision. We argue that Chamfer-linkage satisfies desirable concept representation properties that other popular measures struggle to satisfy. Theoretically, we show that Chamfer-linkage HAC can be implemented in $O(n^2)$ time, matching the efficiency of classical linkage functions. Experimentally, we find that Chamfer-linkage consistently yields higher-quality clusterings than classical linkages such as average-linkage and Ward's method across a diverse collection of datasets. Our results establish Chamfer-linkage as a practical drop-in replacement for classical linkage functions, broadening the toolkit for hierarchical clustering in both theory and practice.

</details>


### [189] [Frame-Level Internal Tool Use for Temporal Grounding in Audio LMs](https://arxiv.org/abs/2602.10230)
*Joesph An,Phillip Keung,Jiaqi Wang,Orevaoghene Ahia,Noah A. Smith*

Main category: cs.LG

TL;DR: 本文提出了一种帧级内部工具使用方法，使大音频语言模型能直接利用内部音频表征进行时间定位，通过二元帧分类器和非齐次泊松过程损失函数提升时序任务性能，显著加快推理速度并增强长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大音频语言模型在需要精确时间定位的时序任务（如词对齐、说话人日志）上表现不佳，标准的时间戳文本生成方法计算开销大且易产生幻觉，尤其在处理训练分布外的音频长度时失效。

Method: 提出帧级内部工具使用方法，训练音频语言模型直接利用其内部音频表征进行时间定位；引入轻量级预测机制，联合优化二元帧分类器和新型非齐次泊松过程（IHP）损失函数以建模时间事件强度。

Result: 在词定位、说话人日志和事件定位任务上均优于基于token的基线方法；实现超50倍推理加速；在分布外音频长度上保持高精度，而标准token方法完全失效。

Conclusion: 帧级内部工具使用是一种高效、鲁棒的时间定位新范式，克服了传统token化时间建模的计算与泛化瓶颈，为音频语言模型的时间理解能力提供了新路径。

Abstract: Large audio language models are increasingly used for complex audio understanding tasks, but they struggle with temporal tasks that require precise temporal grounding, such as word alignment and speaker diarization. The standard approach, where we generate timestamps as sequences of text tokens, is computationally expensive and prone to hallucination, especially when processing audio lengths outside the model's training distribution. In this work, we propose frame-level internal tool use, a method that trains audio LMs to use their own internal audio representations to perform temporal grounding directly. We introduce a lightweight prediction mechanism trained via two objectives: a binary frame classifier and a novel inhomogeneous Poisson process (IHP) loss that models temporal event intensity. Across word localization, speaker diarization, and event localization tasks, our approach outperforms token-based baselines. Most notably, it achieves a >50x inference speedup and demonstrates robust length generalization, maintaining high accuracy on out-of-distribution audio durations where standard token-based models collapse completely.

</details>


### [190] [Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards](https://arxiv.org/abs/2602.10231)
*Kirill Pavlenko,Alexander Golubev,Simon Karasik,Boris Yangel*

Main category: cs.LG

TL;DR: 本文提出了一种名为Blockwise Advantage Estimation的新型优势估计方法，用于解决GRPO在结构化生成中因统一标量优势导致的目标干扰问题；该方法为每个目标分配独立优势并仅作用于对应文本块，并引入Outcome-Conditioned Baseline以避免嵌套rollout开销，在数学任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: GRPO对整个生成序列使用单一标量优势，导致结构化生成中不同段落（对应不同目标）的奖励信号耦合，引发目标干扰和信用误分配。

Method: 提出Blockwise Advantage Estimation方法族：为每个目标定义独立优势，并仅应用于对应文本块；核心是Outcome-Conditioned Baseline，利用前缀导出的中间结果对样本分层，仅用组内统计估计中间状态值，避免标准无偏方法所需的昂贵嵌套rollout。

Result: 在带不确定性估计的数学任务上，该方法缓解了奖励干扰，性能媲美当前最优人工设计奖励的方法，并保持置信度加权集成带来的测试时增益。

Conclusion: Blockwise Advantage Estimation提供了一种模块化、无需额外rollout的方案，可扩展地优化结构化生成中的多阶段目标。

Abstract: Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.

</details>


### [191] [MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation](https://arxiv.org/abs/2602.11062)
*Jialin Liu,Zhaorui Zhang,Ray C. C. Cheung*

Main category: cs.LG

TL;DR: 本文提出MoToRec框架，通过稀疏正则化的残差量化变分自编码器（RQ-VAE）将多模态内容离散化为可解释语义token，结合自适应稀缺性增强与分层多源图编码器，显著缓解推荐系统中的物品冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理新物品（冷启动）时因多模态数据稀疏、噪声大、表征纠缠而效果不佳，亟需更鲁棒、解耦且可解释的表示学习机制。

Method: 提出MoToRec框架：1）稀疏正则化的残差量化VAE（RQ-VAE）实现离散语义token生成；2）自适应稀缺性增强机制优先学习冷启动物品；3）分层多源图编码器融合协同信号与多模态信号。

Result: 在三个大规模数据集上，MoToRec在整体推荐性能及冷启动子任务上均显著优于SOTA方法。

Conclusion: 离散语义token化是一种有效、可扩展的冷启动缓解范式，能提升表征解耦性、可解释性与泛化能力。

Abstract: Graph neural networks (GNNs) have revolutionized recommender systems by effectively modeling complex user-item interactions, yet data sparsity and the item cold-start problem significantly impair performance, particularly for new items with limited or no interaction history. While multimodal content offers a promising solution, existing methods result in suboptimal representations for new items due to noise and entanglement in sparse data. To address this, we transform multimodal recommendation into discrete semantic tokenization. We present Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation (MoToRec), a framework centered on a sparsely-regularized Residual Quantized Variational Autoencoder (RQ-VAE) that generates a compositional semantic code of discrete, interpretable tokens, promoting disentangled representations. MoToRec's architecture is enhanced by three synergistic components: (1) a sparsely-regularized RQ-VAE that promotes disentangled representations, (2) a novel adaptive rarity amplification that promotes prioritized learning for cold-start items, and (3) a hierarchical multi-source graph encoder for robust signal fusion with collaborative signals. Extensive experiments on three large-scale datasets demonstrate MoToRec's superiority over state-of-the-art methods in both overall and cold-start scenarios. Our work validates that discrete tokenization provides an effective and scalable alternative for mitigating the long-standing cold-start challenge.

</details>


### [192] [Risk-Equalized Differentially Private Synthetic Data: Protecting Outliers by Controlling Record-Level Influence](https://arxiv.org/abs/2602.10232)
*Amir Asiaee,Chao Yan,Zachary B. Abrams,Bradley A. Malin*

Main category: cs.LG

TL;DR: 本文提出风险均衡差分隐私合成框架，通过两阶段机制（先估计记录的异常程度，再按风险反比加权学习）优先保护高风险记录，从而在保持整体差分隐私的同时显著降低对异常样本的成员推断攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 差分隐私虽提供最坏情况保障，但实际中异常个体（如罕见病患者或异常交易）更易遭受成员推断等经验性攻击，尤其在中等隐私预算和存在辅助信息时。

Method: 采用两阶段风险均衡DP合成：第一阶段用小隐私预算估计每条记录的‘异常度’；第二阶段在DP学习过程中按异常度反比加权各记录；理论证明该机制在高斯机制下可导出紧致的逐记录隐私损失界。

Result: 在可控异常注入的模拟数据上，风险加权显著降低了对高异常度记录的成员推断成功率；消融实验验证改进源于针对性降权而非随机降权；在真实数据集（Breast Cancer、Adult、German Credit）上效果因数据集而异，体现评分器质量与合成流程的耦合影响。

Conclusion: 风险均衡DP合成能有效提升对脆弱个体的保护能力，实现更公平、实用的隐私保障，为面向异质数据的隐私增强合成提供了新范式。

Abstract: When synthetic data is released, some individuals are harder to protect than others. A patient with a rare disease combination or a transaction with unusual characteristics stands out from the crowd. Differential privacy provides worst-case guarantees, but empirical attacks -- particularly membership inference -- succeed far more often against such outliers, especially under moderate privacy budgets and with auxiliary information.
  This paper introduces risk-equalized DP synthesis, a framework that prioritizes protection for high-risk records by reducing their influence on the learned generator. The mechanism operates in two stages: first, a small privacy budget estimates each record's "outlierness"; second, a DP learning procedure weights each record inversely to its risk score. Under Gaussian mechanisms, a record's privacy loss is proportional to its influence on the output -- so deliberately shrinking outliers' contributions yields tighter per-instance privacy bounds for precisely those records that need them most.
  We prove end-to-end DP guarantees via composition and derive closed-form per-record bounds for the synthesis stage (the scoring stage adds a uniform per-record term). Experiments on simulated data with controlled outlier injection show that risk-weighting substantially reduces membership inference success against high-outlierness records; ablations confirm that targeting -- not random downweighting -- drives the improvement. On real-world benchmarks (Breast Cancer, Adult, German Credit), gains are dataset-dependent, highlighting the interplay between scorer quality and synthesis pipeline.

</details>


### [193] [Diffusion-Pretrained Dense and Contextual Embeddings](https://arxiv.org/abs/2602.11151)
*Sedigheh Eslami,Maksim Gaiduk,Markus Krimmel,Louis Milliken,Bo Wang,Denis Bykov*

Main category: cs.LG

TL;DR: 本文介绍了pplx-embed系列多语言嵌入模型，采用基于扩散预训练语言模型的多阶段对比学习方法，提升长文档检索效果，并发布两种模型：pplx-embed-v1（标准检索）和pplx-embed-context-v1（上下文化嵌入），在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升长文档检索中对全局上下文的建模能力，解决传统嵌入模型在长文本中丢失全局信息的问题，并满足大规模生产环境对检索质量与效率的双重需求。

Method: 基于扩散预训练语言模型主干，采用多阶段对比学习；利用双向注意力机制增强上下文建模；引入均值池化与晚期分块策略以更好保留长文档的全局语义。

Result: pplx-embed-v1在MTEB(Multilingual, v2)、MTEB(Code)、MIRACL、BERGEN、ToolRet等多语言与代码检索基准上达到领先水平；pplx-embed-context-v1在ConTEB基准上创纪录；内部千万级文档真实搜索场景验证其高效性与鲁棒性。

Conclusion: pplx-embed系列模型通过扩散预训练与多阶段对比学习有效提升了多语言长文档检索性能，尤其在兼顾精度与效率的大规模生产环境中展现出显著优势。

Abstract: In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunking strategy to better preserve global context across long documents. We release two model types: pplx-embed-v1 for standard retrieval, and pplx-embed-context-v1 for contextualized embeddings that incorporate global document context into passage representations. pplx-embed-v1 achieves competitive performance on the MTEB(Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet retrieval benchmarks, while pplx-embed-context-v1 sets new records on the ConTEB benchmark. Beyond public benchmarks, pplx-embed-v1 demonstrates strong performance on our internal evaluation suite, which focuses on real-world, large-scale search scenarios over tens of millions of documents. These results validate the models' effectiveness in production environments where retrieval quality and efficiency are critical at scale.

</details>


### [194] [Modeling Programming Skills with Source Code Embeddings for Context-aware Exercise Recommendation](https://arxiv.org/abs/2602.10249)
*Carlos Eduardo P. Silva,João Pedro M. Sena,Julio C. S. Reis,André G. Santos,Lucas N. Ferreira*

Main category: cs.LG

TL;DR: 本文提出了一种基于源代码嵌入的上下文感知推荐系统，用于建模学生编程技能并为其推荐匹配的编程习题。


<details>
  <summary>Details</summary>
Motivation: 传统推荐方法（如基于正确率或解题时间）难以准确反映学生的实际编程能力，需更细粒度、动态的技能建模方式。

Method: 利用Jina等模型生成学生提交代码的嵌入表示，构建多主题编程技能画像；通过余弦相似度匹配学生画像与习题所需技能向量，实现个性化习题推荐。

Result: Jina嵌入在多数编程技能预测任务上优于TF-IDF、CodeBERT-cpp和GraphCodeBERT；在七届课程数据上的推荐效果显著优于基于正确率或解题时间的基线方法。

Conclusion: 基于源代码嵌入的学生技能建模能更有效地支撑个性化编程习题推荐，为教育推荐系统提供了新范式。

Abstract: In this paper, we propose a context-aware recommender system that models students' programming skills using embeddings of the source code they submit throughout a course. These embeddings predict students' skills across multiple programming topics, producing profiles that are matched to the skills required by unseen homework problems. To generate recommendations, we compute the cosine similarity between student profiles and problem skill vectors, ranking exercises according to their alignment with each student's current abilities. We evaluated our approach using real data from students and exercises in an introductory programming course at our university. First, we assessed the effectiveness of our source code embeddings for predicting skills, comparing them with token-based and graph-based alternatives. Results showed that Jina embeddings outperformed TF-IDF, CodeBERT-cpp, and GraphCodeBERT across most skills. Additionally, we evaluated the system's ability to recommend exercises aligned with weekly course content by analyzing student submissions collected over seven course offerings. Our approach consistently produced more suitable recommendations than baselines based on correctness or solution time, indicating that predicted programming skills provide a stronger signal for problem recommendation.

</details>


### [195] [Kernel-Based Learning of Chest X-ray Images for Predicting ICU Escalation among COVID-19 Patients](https://arxiv.org/abs/2602.10261)
*Qiyuan Shi,Jian Kang,Yi Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为GLIMARK的新方法，将多核学习（MKL）扩展至广义线性模型框架，以处理指数族分布的响应变量（如二分类），并在COVID-19胸部X光数据上验证了其在ICU升级预测和可解释特征提取中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统多核学习（MKL）主要针对连续型响应变量，难以适用于分类等离散型结果；现实数据常具异质性和多源性，单一核函数建模能力有限。

Method: 提出广义线性模型与集成多核加性回归（GLIMARK），将多个基础核函数线性组合构建复合核，并嵌入广义线性模型框架，支持指数族分布的响应变量（如伯努利、泊松等），采用联合优化策略估计核权重与模型参数。

Result: 在模拟实验中能有效恢复真实数据生成机制；在真实COVID-19胸部X光数据上成功预测二分类ICU升级结果，并识别出具有临床意义的影像区域特征。

Conclusion: GLIMARK拓展了MKL的应用边界，使其适用于更广泛的响应变量类型，兼具预测性能与可解释性，在医疗等高风险领域展现出实用价值。

Abstract: Kernel methods have been extensively utilized in machine learning for classification and prediction tasks due to their ability to capture complex non-linear data patterns. However, single kernel approaches are inherently limited, as they rely on a single type of kernel function (e.g., Gaussian kernel), which may be insufficient to fully represent the heterogeneity or multifaceted nature of real-world data. Multiple kernel learning (MKL) addresses these limitations by constructing composite kernels from simpler ones and integrating information from heterogeneous sources. Despite these advances, traditional MKL methods are primarily designed for continuous outcomes. We extend MKL to accommodate the outcome variable belonging to the exponential family, representing a broader variety of data types, and refer to our proposed method as generalized linear models with integrated multiple additive regression with kernels (GLIMARK). Empirically, we demonstrate that GLIMARK can effectively recover or approximate the true data-generating mechanism. We have applied it to a COVID-19 chest X-ray dataset, predicting binary outcomes of ICU escalation and extracting clinically meaningful features, underscoring the practical utility of this approach in real-world scenarios.

</details>


### [196] [From Classical to Topological Neural Networks Under Uncertainty](https://arxiv.org/abs/2602.10266)
*Sarah Harkins Dayton,Layal Bou Hamdan,Ioannis D. Schizas,David L. Boothe,Vasileios Maroulas*

Main category: cs.LG

TL;DR: 本章探讨了神经网络、拓扑数据分析和拓扑深度学习技术，以及统计贝叶斯方法，在军事领域中处理图像、时间序列和图数据，以提升人工智能的鲁棒性、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提升人工智能在军事领域的鲁棒性、可解释性和泛化能力。

Method: 结合神经网络、拓扑数据分析、拓扑深度学习和统计贝叶斯方法。

Result: 展示了在图像、视频、音频和时间序列识别、欺诈检测及图数据链接预测等方面的实用应用。

Conclusion: 拓扑感知和不确定性感知模型能显著增强人工智能系统在军事应用中的性能。

Abstract: This chapter explores neural networks, topological data analysis, and topological deep learning techniques, alongside statistical Bayesian methods, for processing images, time series, and graphs to maximize the potential of artificial intelligence in the military domain. Throughout the chapter, we highlight practical applications spanning image, video, audio, and time-series recognition, fraud detection, and link prediction for graphical data, illustrating how topology-aware and uncertainty-aware models can enhance robustness, interpretability, and generalization.

</details>


### [197] [Linear-LLM-SCM: Benchmarking LLMs for Coefficient Elicitation in Linear-Gaussian Causal Models](https://arxiv.org/abs/2602.10282)
*Kanta Yamaoka,Sumantrak Mukherjee,Thomas Gärtner,David Antony Selby,Stefan Konigorski,Eyke Hüllermeier,Viktor Bengs,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: 本文提出了Linear-LLM-SCM框架，用于评估大语言模型（LLMs）在已知DAG结构下对线性高斯结构因果模型（SCM）参数的定量因果推理能力，发现当前LLMs在系数估计上存在强随机性、对DAG误设敏感等局限性，并开源了该基准框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在定性因果关系识别上已有潜力，但在连续域中进行定量因果推理（即估计函数关系的效应大小）的能力尚缺乏探索。

Method: 提出Linear-LLM-SCM插拔式基准框架：将给定DAG分解为局部父-子节点集，用回归风格提示LLM为每个节点生成结构方程，并聚合结果与真实参数对比。

Result: 实验揭示LLMs在定量因果参数估计中存在强随机性、对虚假边导致的DAG误设高度敏感、系数估计变异性大，且易受结构和语义扰动影响。

Conclusion: 当前LLMs作为定量因果参数估计器仍存在显著局限；所提框架开源，支持研究者在任意领域便捷评估不同LLM与DAG组合的性能。

Abstract: Large language models (LLMs) have shown potential in identifying qualitative causal relations, but their ability to perform quantitative causal reasoning -- estimating effect sizes that parametrize functional relationships -- remains underexplored in continuous domains. We introduce Linear-LLM-SCM, a plug-and-play benchmarking framework for evaluating LLMs on linear Gaussian structural causal model (SCM) parametrization when the DAG is given. The framework decomposes a DAG into local parent-child sets and prompts an LLM to produce a regression-style structural equation per node, which is aggregated and compared against available ground-truth parameters. Our experiments show several challenges in such benchmarking tasks, namely, strong stochasticity in the results in some of the models and susceptibility to DAG misspecification via spurious edges in the continuous domains. Across models, we observe substantial variability in coefficient estimates for some settings and sensitivity to structural and semantic perturbations, highlighting current limitations of LLMs as quantitative causal parameterizers. We also open-sourced the benchmarking framework so that researchers can utilize their DAGs and any off-the-shelf LLMs plug-and-play for evaluation in their domains effortlessly.

</details>


### [198] [What Does Preference Learning Recover from Pairwise Comparison Data?](https://arxiv.org/abs/2602.10286)
*Rattana Pukdee,Maria-Florina Balcan,Pradeep Ravikumar*

Main category: cs.LG

TL;DR: 本文研究了在真实数据可能违反Bradley-Terry（BT）模型假设的情况下，BT模型在成对偏好学习中实际能恢复什么，并提出了条件偏好分布（CPRD）来形式化三元组数据所编码的偏好信息，给出了BT模型适用性的精确条件以及影响样本效率的因素（如间隔和连通性）。


<details>
  <summary>Details</summary>
Motivation: 现实中的偏好数据可能不满足Bradley-Terry模型的假设，但目前尚不清楚在此类情况下BT学习究竟恢复了什么，因此需要一个数据驱动的基础来理解偏好学习的本质。

Method: 从三元组比较数据出发，形式化定义条件偏好分布（CPRD），分析BT模型建模CPRD的适用条件，并研究影响样本效率的关键因素（如margin和connectivity）。

Result: 明确了BT模型适用于建模CPRD的精确条件，并识别出决定样本效率的核心因素（margin和connectivity），为偏好学习提供了数据中心视角的理论基础。

Conclusion: BT模型在实际应用中的有效性依赖于数据是否满足其隐含假设；本文通过CPRD框架揭示了其本质作用，并为更鲁棒的偏好建模提供了理论指导。

Abstract: Pairwise preference learning is central to machine learning, with recent applications in aligning language models with human preferences. A typical dataset consists of triplets $(x, y^+, y^-)$, where response $y^+$ is preferred over response $y^-$ for context $x$. The Bradley--Terry (BT) model is the predominant approach, modeling preference probabilities as a function of latent score differences. Standard practice assumes data follows this model and learns the latent scores accordingly. However, real data may violate this assumption, and it remains unclear what BT learning recovers in such cases. Starting from triplet comparison data, we formalize the preference information it encodes through the conditional preference distribution (CPRD). We give precise conditions for when BT is appropriate for modeling the CPRD, and identify factors governing sample efficiency -- namely, margin and connectivity. Together, these results offer a data-centric foundation for understanding what preference learning actually recovers.

</details>


### [199] [Configuration-to-Performance Scaling Law with Neural Ansatz](https://arxiv.org/abs/2602.10300)
*Huaqing Zhang,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 本文提出了一种神经配置-性能缩放定律（NCPL），利用大语言模型建模完整训练配置与性能之间的映射关系，显著优于传统缩放定律（如Chinchilla），并在跨配置预测、多超参联合调优和损失曲线预测等方面展现出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统缩放定律假设超参数已最优选择，但实际中难以实现且受限于硬件；需一种能覆盖更广超参数空间、简化大规模调优的预测方法。

Method: 提出配置到性能缩放定律（CPL），用大语言模型（LLM）参数化该映射，基于多源开源预训练日志进行拟合，构建神经版本（NCPL）。

Result: NCPL在最终预训练损失预测上比Chinchilla定律误差低20–40%，可泛化至训练集最大计算量10倍的运行，并支持多超参联合调优及损失曲线预测。

Conclusion: NCPL提供了一种更通用、实用且可扩展的训练性能预测范式，为大规模模型训练的高效配置搜索与自动化调优奠定基础。

Abstract: Researchers build scaling laws to forecast the training performance of expensive large-scale runs with larger model size N and data size D. These laws assume that other training hyperparameters are optimally chosen, which can require significant effort and, in some cases, be impossible due to external hardware constraints. To improve predictability across a broader set of hyperparameters and enable simpler tuning at scale, we propose learning a \textit{Configuration-to-Performance Scaling Law} (CPL): a mapping from the \textit{full training configuration} to training performance. Because no simple functional form can express this mapping, we parameterize it with a large language model (LLM), and fit it with diverse open-source pretraining logs across multiple sources, yielding a \textit{Neural} Configuration-to-Performance Scaling Law (NCPL). NCPL accurately predicts how training configurations influence the final pretraining loss, achieving 20-40% lower prediction error than the configuration-agnostic Chinchilla law and generalizing to runs using up to 10 x more compute than any run in the training set. It further supports joint tuning of multiple hyperparameters with performance comparable to hyperparameter scaling law baselines. Finally, NCPL naturally and effectively extends to richer prediction targets such as loss-curve prediction.

</details>


### [200] [ICODEN: Ordinary Differential Equation Neural Networks for Interval-Censored Data](https://arxiv.org/abs/2602.10303)
*Haoling Wang,Lang Zeng,Tao Sun,Youngjoo Cho,Ying Ding*

Main category: cs.LG

TL;DR: 本文提出了ICODEN，一种基于常微分方程的神经网络模型，用于处理区间删失生存数据，无需比例风险假设或预设参数形式，具有高维预测能力和良好泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有区间删失生存分析方法常依赖强模型假设或难以处理高维预测变量，亟需更灵活、稳健的建模工具。

Method: 提出ICODEN模型：用深度神经网络建模风险函数，并通过求解常微分方程获得累积风险函数，避免比例风险假设和参数形式限制。

Result: 在多种模拟场景（含比例/非比例风险、线性/非线性协变量效应）及ADNI、AREDS/AREDS2真实生物医学数据中，ICODEN均展现出优异且稳定的预测性能，并支持基于SNP的亚组识别。

Conclusion: ICODEN是一种假设轻量、实用性强的高维区间删失生存预测工具，适用于阿尔茨海默病和年龄相关性黄斑变性等复杂疾病研究。

Abstract: Predicting time-to-event outcomes when event times are interval censored is challenging because the exact event time is unobserved. Many existing survival analysis approaches for interval-censored data rely on strong model assumptions or cannot handle high-dimensional predictors. We develop ICODEN, an ordinary differential equation-based neural network for interval-censored data that models the hazard function through deep neural networks and obtains the cumulative hazard by solving an ordinary differential equation. ICODEN does not require the proportional hazards assumption or a prespecified parametric form for the hazard function, thereby permitting flexible survival modeling. Across simulation settings with proportional or non-proportional hazards and both linear and nonlinear covariate effects, ICODEN consistently achieves satisfactory predictive accuracy and remains stable as the number of predictors increases. Applications to data from multiple phases of the Alzheimer's Disease Neuroimaging Initiative (ADNI) and to two Age-Related Eye Disease Studies (AREDS and AREDS2) for age-related macular degeneration (AMD) demonstrate ICODEN's robust prediction performance. In both applications, predicting time-to-AD or time-to-late AMD, ICODEN effectively uses hundreds to more than 1,000 SNPs and supports data-driven subgroup identification with differential progression risk profiles. These results establish ICODEN as a practical assumption-lean tool for prediction with interval-censored survival data in high-dimensional biomedical settings.

</details>


### [201] [Confounding Robust Continuous Control via Automatic Reward Shaping](https://arxiv.org/abs/2602.10305)
*Mateo Juliani,Mingxuan Li,Elias Bareinboim*

Main category: cs.LG

TL;DR: 本文提出了一种从含未观测混杂因素的离线数据中自动学习连续控制任务奖励塑形函数的方法，基于因果贝尔曼方程估计最优状态值上界，并将其作为势函数用于势基奖励塑形（PBRS），在多个基准任务中验证了其鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励塑形方法缺乏对复杂连续控制问题的有效、可解释的设计原则，尤其在存在未观测混杂变量时缺乏鲁棒性。

Method: 基于因果贝尔曼方程，从可能受未观测混杂变量污染的离线数据中学习最优状态值的紧致上界，并将该上界用作势函数嵌入PBRS框架，结合Soft-Actor-Critic（SAC）进行策略训练。

Result: 所提方法在多个标准连续控制基准任务上显著提升SAC的训练效率与性能，且在存在未观测混杂变量时仍保持强鲁棒性。

Conclusion: 本工作首次将因果推断思想系统引入连续控制中的奖励塑形设计，为构建混杂鲁棒的强化学习方法提供了新范式。

Abstract: Reward shaping has been applied widely to accelerate Reinforcement Learning (RL) agents' training. However, a principled way of designing effective reward shaping functions, especially for complex continuous control problems, remains largely under-explained. In this work, we propose to automatically learn a reward shaping function for continuous control problems from offline datasets, potentially contaminated by unobserved confounding variables. Specifically, our method builds upon the recently proposed causal Bellman equation to learn a tight upper bound on the optimal state values, which is then used as the potentials in the Potential-Based Reward Shaping (PBRS) framework. Our proposed reward shaping algorithm is tested with Soft-Actor-Critic (SAC) on multiple commonly used continuous control benchmarks and exhibits strong performance guarantees under unobserved confounders. More broadly, our work marks a solid first step towards confounding robust continuous control from a causal perspective. Code for training our reward shaping functions can be found at https://github.com/mateojuliani/confounding_robust_cont_control.

</details>


### [202] [R2RAG-Flood: A reasoning-reinforced training-free retrieval augmentation generation framework for flood damage nowcasting](https://arxiv.org/abs/2602.10312)
*Lipai Huang,Kai Yin,Chia-Fu Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: R2RAG-Flood 是一种无需训练的推理增强型检索增强生成框架，用于风暴后财产损失即时预测，通过检索邻近地理样本和典型类别的推理轨迹，引导大语言模型复用已有推理逻辑，实现高效率、可解释的两阶段损伤预测。


<details>
  <summary>Details</summary>
Motivation: 现有监督式表格预测模型虽准确但缺乏可解释性，且难以泛化；而大语言模型直接应用又面临任务适配难、成本高、无训练支持等问题。本文旨在构建一种无需微调、具备推理可解释性且高效实用的灾后评估新范式。

Method: 基于已有监督表格预测器构建以推理为中心的知识库（含结构化特征、自然语言摘要与模型生成的推理轨迹）；推理时通过地理邻近性与类别原型检索相关推理轨迹，并以上下文增强提示驱动大语言模型复用/适应这些轨迹；采用两阶段预测（先判有无损伤，再分级为三级损害程度）并引入条件降级机制修正过预测。

Result: 在哈里斯县案例中，R2RAG-Flood 在7个大语言模型上达到0.613–0.668整体准确率和0.757–0.896中高损类别准确率，逼近监督基线（0.714/0.859），同时输出结构化推理依据；轻量变体在单位成本效益上显著优于监督模型和大型语言模型。

Conclusion: R2RAG-Flood 验证了无需训练的推理重用范式在灾情评估中的有效性，在保持竞争力性能的同时兼顾可解释性、泛化性与部署效率，为低资源、高时效性地理AI应用提供了新路径。

Abstract: R2RAG-Flood is a reasoning-reinforced, training-free retrieval-augmented generation framework for post-storm property damage nowcasting. Building on an existing supervised tabular predictor, the framework constructs a reasoning-centric knowledge base composed of labeled tabular records, where each sample includes structured predictors, a compact natural language text-mode summary, and a model-generated reasoning trajectory. During inference, R2RAG-Flood issues context-augmented prompts that retrieve and condition on relevant reasoning trajectories from nearby geospatial neighbors and canonical class prototypes, enabling the large language model backbone to emulate and adapt prior reasoning rather than learn new task-specific parameters. Predictions follow a two-stage procedure that first determines property damage occurrence and then refines severity within a three-level Property Damage Extent categorization, with a conditional downgrade step to correct over-predicted severity. In a case study of Harris County, Texas at the 12-digit Hydrologic Unit Code scale, the supervised tabular baseline trained directly on structured predictors achieves 0.714 overall accuracy and 0.859 damage class accuracy for medium and high damage classes. Across seven large language model backbones, R2RAG-Flood attains 0.613 to 0.668 overall accuracy and 0.757 to 0.896 damage class accuracy, approaching the supervised baseline while additionally producing a structured rationale for each prediction. Using a severity-per-cost efficiency metric derived from API pricing and GPU instance costs, lightweight R2RAG-Flood variants demonstrate substantially higher efficiency than both the supervised tabular baseline and larger language models, while requiring no task-specific training or fine-tuning.

</details>


### [203] [Stop Training for the Worst: Progressive Unmasking Accelerates Masked Diffusion Training](https://arxiv.org/abs/2602.10314)
*Jaeyeon Kim,Jonathan Geuter,David Alvarez-Melis,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: 本文提出PUMA方法，通过调整前向掩码过程使训练与推理时的掩码模式对齐，从而加速Masked Diffusion Models（MDMs）的训练并缓解训练-测试不匹配问题。


<details>
  <summary>Details</summary>
Motivation: MDMs在离散空间生成中表现优异，但其训练需遍历指数级掩码模式，计算开销大且存在训练（随机掩码）与推理（结构化掩码）不一致的问题。

Method: 提出Progressive UnMAsking（PUMA），修改前向掩码过程，使训练中采样的掩码模式逐步接近推理时的实际解码顺序，聚焦于推理对齐的掩码子集。

Result: 在125M参数规模上预训练速度提升约2.5倍，并能与自回归初始化等常用策略协同增益。

Conclusion: PUMA是一种简单有效的方法，通过掩码对齐显著降低MDMs训练成本，同时保持甚至提升性能，具备实用性和可扩展性。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces. By generating sequences in any order and allowing for parallel decoding, they enable fast inference and strong performance on non-causal tasks. However, this flexibility comes with a training complexity trade-off: MDMs train on an exponentially large set of masking patterns, which is not only computationally expensive, but also creates a train--test mismatch between the random masks used in training and the highly structured masks induced by inference-time unmasking. In this work, we propose Progressive UnMAsking (PUMA), a simple modification of the forward masking process that aligns training-time and inference-time masking patterns, thereby focusing optimization on inference-aligned masks and speeding up training. Empirically, PUMA speeds up pretraining at the 125M scale by $\approx 2.5\times$ and offers complementary advantages on top of common recipes like autoregressive initialization. We open-source our codebase at https://github.com/JaeyeonKim01/PUMA.

</details>


### [204] [Identifying Evidence-Based Nudges in Biomedical Literature with Large Language Models](https://arxiv.org/abs/2602.10345)
*Jaydeep Chauhan,Mark Seidman,Pezhman Raeisian Parvari,Zhi Zheng,Zina Ben-Miled,Cristina Barboi,Andrew Gonzalez,Malaz Boustani*

Main category: cs.LG

TL;DR: 本文提出了一种可扩展的AI系统，用于从海量生物医学文献中自动识别和提取基于证据的行为助推（nudges），通过多阶段混合过滤与量化版LLaMA模型实现高效、可解释的结构化信息抽取，并支持精度-召回率灵活权衡。


<details>
  <summary>Details</summary>
Motivation: 行为助推在改善健康结局（如用药依从性）方面效果显著，但其在PubMed超800万篇文献中的手动识别效率极低，亟需自动化方法。

Method: 采用多阶段流水线：第一阶段用关键词、TF-IDF、余弦相似度及‘助推词加分’进行混合过滤，将文献缩减至约8.1万篇；第二阶段使用量化版OpenScholar（LLaMA 3.1 8B）对论文分类并一次性抽取nudge类型、目标行为等结构化字段，输出受JSON Schema验证。

Result: 在197篇标注测试集上，最优配置（标题/摘要/引言）达67.0% F1与72.0%召回率；高精度变体（7次随机自一致性采样）实现100%精度、12%召回率；系统正集成至Agile Nudge+平台。

Conclusion: 该系统实现了可解释、领域适配的证据检索与合成，为个性化医疗中LLM生成干预措施提供循证基础，推动AI在真实临床场景中的可信落地。

Abstract: We present a scalable, AI-powered system that identifies and extracts evidence-based behavioral nudges from unstructured biomedical literature. Nudges are subtle, non-coercive interventions that influence behavior without limiting choice, showing strong impact on health outcomes like medication adherence. However, identifying these interventions from PubMed's 8 million+ articles is a bottleneck. Our system uses a novel multi-stage pipeline: first, hybrid filtering (keywords, TF-IDF, cosine similarity, and a "nudge-term bonus") reduces the corpus to about 81,000 candidates. Second, we use OpenScholar (quantized LLaMA 3.1 8B) to classify papers and extract structured fields like nudge type and target behavior in a single pass, validated against a JSON schema.
  We evaluated four configurations on a labeled test set (N=197). The best setup (Title/Abstract/Intro) achieved a 67.0% F1 score and 72.0% recall, ideal for discovery. A high-precision variant using self-consistency (7 randomized passes) achieved 100% precision with 12% recall, demonstrating a tunable trade-off for high-trust use cases. This system is being integrated into Agile Nudge+, a real-world platform, to ground LLM-generated interventions in peer-reviewed evidence. This work demonstrates interpretable, domain-specific retrieval pipelines for evidence synthesis and personalized healthcare.

</details>


### [205] [Theoretical Analysis of Contrastive Learning under Imbalanced Data: From Training Dynamics to a Pruning Solution](https://arxiv.org/abs/2602.10357)
*Haixu Liao,Yating Zhou,Songyang Zhang,Meng Wang,Shuai Zhang*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架来分析在不平衡数据下基于Transformer编码器的对比学习训练动态，揭示了神经元权重演化的三个阶段，并表明剪枝可以恢复因不平衡导致的性能下降并增强特征分离。


<details>
  <summary>Details</summary>
Motivation: 对比学习在不平衡数据分布下的理论理解有限，而这种不平衡会降低表征质量并引发模型偏差，亟需对其影响进行严格刻画。

Method: 构建理论框架分析对比学习在不平衡数据下、使用Transformer编码器时的训练动力学；通过理论推导与数值实验验证神经元权重演化规律及剪枝效果。

Result: 发现神经元权重演化分为三个阶段，分别对应多数类特征、少数类特征和噪声；少数类特征会降低表征能力、增加模型复杂度需求、阻碍真实特征与噪声的分离；剪枝可有效恢复性能并提升特征分离能力。

Conclusion: 本文从神经元层面揭示了数据不平衡对对比学习的影响机制，并提出剪枝作为有效的缓解策略，兼具理论深度与实践价值。

Abstract: Contrastive learning has emerged as a powerful framework for learning generalizable representations, yet its theoretical understanding remains limited, particularly under imbalanced data distributions that are prevalent in real-world applications. Such an imbalance can degrade representation quality and induce biased model behavior, yet a rigorous characterization of these effects is lacking. In this work, we develop a theoretical framework to analyze the training dynamics of contrastive learning with Transformer-based encoders under imbalanced data. Our results reveal that neuron weights evolve through three distinct stages of training, with different dynamics for majority features, minority features, and noise. We further show that minority features reduce representational capacity, increase the need for more complex architectures, and hinder the separation of ground-truth features from noise. Inspired by these neuron-level behaviors, we show that pruning restores performance degraded by imbalance and enhances feature separation, offering both conceptual insights and practical guidance. Major theoretical findings are validated through numerical experiments.

</details>


### [206] [Simple LLM Baselines are Competitive for Model Diffing](https://arxiv.org/abs/2602.10371)
*Elias Kempf,Simon Schrodi,Bartosz Cywiński,Thomas Brox,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 本文提出了一种评估大语言模型（LLM）行为差异的新框架——模型差分（model diffing），并系统比较了基于LLM和基于稀疏自编码器（SAE）的两种主流方法，提出了衡量其性能的三个关键指标（泛化性、有趣性、抽象程度），结果表明改进后的LLM基线方法在抽象性上通常优于SAE方法。


<details>
  <summary>Details</summary>
Motivation: 标准LLM评估仅覆盖预设能力或倾向，难以发现模型版本间的行为偏移或意外的对齐问题；模型差分旨在自动揭示系统性行为差异，但缺乏系统性比较与统一评估标准。

Method: 提出针对模型差分的三类评估指标（泛化性、有趣性、抽象程度），并基于这些指标对现有LLM生成式方法与SAE特征识别方法进行实证比较，同时改进了一个LLM基线方法。

Result: 改进的LLM基线方法在整体性能上可媲美SAE方法，且通常能揭示更高抽象层次的行为差异。

Conclusion: 模型差分需多维评估标准；LLM方法在抽象性方面具有优势，SAE与LLM方法各有适用场景，未来应结合二者长处构建更鲁棒的差分分析框架。

Abstract: Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.

</details>


### [207] [Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs](https://arxiv.org/abs/2602.10377)
*Luoyang Sun,Jiwen Jiang,Yifeng Ding,Fengfa Li,Yan Song,Haifeng Zhang,Jian Ying,Lei Ren,Kun Zhan,Wei Chen,Yan Xie,Cheng Deng*

Main category: cs.LG

TL;DR: 本文提出了一种面向边缘设备（如NVIDIA Jetson Orin）的视觉-语言-动作模型（VLA）中大语言模型（LLM）骨干网的硬件-软件协同设计方法，通过建立架构超参数与训练损失、推理延迟之间的定量关系（协同设计定律），快速定位精度-延迟帕累托前沿，在相同延迟下显著降低困惑度（如比Qwen2.5-0.5B低19.42%），将架构选型周期从数月缩短至数天。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备（如自动驾驶汽车、机器人）上部署VLAs时，需在模型精度、推理延迟和硬件效率之间取得平衡，传统独立设计软硬件已不适用，亟需硬件-软件协同设计框架。

Method: 提出硬件协同设计定律：1）将训练损失建模为架构超参数的显式函数，并通过在Jetson Orin上训练170个模型（各10B tokens）拟合缩放律；2）采用Roofline模型刻画推理延迟；3）联合优化精度与延迟，界定满足工业预算的可行架构区域。

Result: 在NVIDIA Jetson Orin上验证了1942个候选架构，发现相同延迟下，所提协同设计架构在WikiText-2上困惑度比Qwen2.5-0.5B低19.42%；架构搜索周期由数月压缩至数天。

Conclusion: 本文首次构建了面向端侧LLM部署的、可操作的硬件协同设计缩放律框架，为物理AI系统中高效VLA模型的定制化设计提供了理论基础与实用工具。

Abstract: Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.

</details>


### [208] [Deep learning outperforms traditional machine learning methods in predicting childhood malnutrition: evidence from survey data](https://arxiv.org/abs/2602.10381)
*Deepak Bastola,Yang Li*

Main category: cs.LG

TL;DR: This paper evaluates 16 machine learning and deep learning models to identify malnutrition in children under five in Nepal using MICS 2019 data; TabNet achieved best performance, with maternal education, wealth index, and child age as top predictors—enabling scalable, survey-based screening for targeted interventions.


<details>
  <summary>Details</summary>
Motivation: Childhood malnutrition is a major public health issue in Nepal and similar low-resource settings, but conventional case-finding is labor-intensive and inaccessible in remote areas.

Method: Systematic comparison of 16 ML/DL algorithms (including TabNet, SVM, AdaBoost) on Nepal MICS 2019 data; constructed composite malnutrition indicator (stunting + wasting + underweight); evaluated using 10 metrics, prioritizing F1-score and recall due to class imbalance and high cost of false negatives; performed consensus feature importance analysis.

Result: TabNet achieved the best performance, outperforming SVM and AdaBoost; maternal education, household wealth index, and child age were identified as the top three predictors of malnutrition.

Conclusion: The study establishes a scalable, survey-based ML screening framework for malnutrition risk identification in Nepal, supporting SDG progress and offering a transferable approach for other low-resource settings.

Abstract: Childhood malnutrition remains a major public health concern in Nepal and other low-resource settings, while conventional case-finding approaches are labor-intensive and frequently unavailable in remote areas. This study provides the first comprehensive assessment of machine learning and deep learning methodologies for identifying malnutrition among children under five years of age in Nepal. We systematically compared 16 algorithms spanning deep learning, gradient boosting, and traditional machine learning families, using data from the Nepal Multiple Indicator Cluster Survey (MICS) 2019. A composite malnutrition indicator was constructed by integrating stunting, wasting, and underweight status, and model performance was evaluated using ten metrics, with emphasis on F1-score and recall to account for substantial class imbalance and the high cost of failing to detect malnourished children. Among all models, TabNet demonstrated the best performance, likely attributable to its attention-based architecture, and outperformed both support vector machine and AdaBoost classifiers. A consensus feature importance analysis identified maternal education, household wealth index, and child age as the primary predictors of malnutrition, followed by geographic characteristics, vaccination status, and meal frequency. Collectively, these results demonstrate a scalable, survey-based screening framework for identifying children at elevated risk of malnutrition and for guiding targeted nutritional interventions. The proposed approach supports Nepal's progress toward the Sustainable Development Goals and offers a transferable methodological template for similar low-resource settings globally.

</details>


### [209] [Time-to-Event Transformer to Capture Timing Attention of Events in EHR Time Series](https://arxiv.org/abs/2602.10385)
*Jia Li,Yu Hou,Rui Zhang*

Main category: cs.LG

TL;DR: 本文提出LITT模型，一种新型Timing-Transformer架构，通过在虚拟相对时间线上对齐患者序列事件，实现事件时间聚焦注意力与个性化临床轨迹解释，在真实乳腺癌患者EHR数据上成功预测心脏毒性引发的心脏病发病时间，并在公开数据集上超越现有生存分析方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型（如Transformer）虽能捕捉丰富关联，但忽视事件的时间与顺序信息，难以支持临床因果推理；亟需能将时间作为可计算维度、评估个体轨迹对齐度并挖掘一致时序模式的方法。

Method: 提出LITT（Timing-Transformer）架构，构建虚拟「相对时间线」以对齐患者特异性序列事件，引入事件时间聚焦注意力机制，并支持可解释的个性化轨迹建模。

Result: 在3276例乳腺癌患者纵向EHR数据上验证了LITT的可解释性与有效性，成功预测心脏毒性所致心脏病的发病时间；在多个公开数据集上性能优于基准及SOTA生存分析方法。

Conclusion: LITT为临床AI中的精准医学提供了新范式，首次将时间建模为可学习、可对齐的计算维度，显著提升了时序医疗事件建模与预测能力。

Abstract: Automatically discovering personalized sequential events from large-scale time-series data is crucial for enabling precision medicine in clinical research, yet it remains a formidable challenge even for contemporary AI models. For example, while transformers capture rich associations, they are mostly agnostic to event timing and ordering, thereby bypassing potential causal reasoning.
  Intuitively, we need a method capable of evaluating the "degree of alignment" among patient-specific trajectories and identifying their shared patterns, i.e., the significant events in a consistent sequence. This necessitates treating timing as a true \emph{computable} dimension, allowing models to assign ``relative timestamps'' to candidate events beyond their observed physical times.
  In this work, we introduce LITT, a novel Timing-Transformer architecture that enables temporary alignment of sequential events on a virtual ``relative timeline'', thereby enabling \emph{event-timing-focused attention} and personalized interpretations of clinical trajectories. Its interpretability and effectiveness are validated on real-world longitudinal EHR data from 3,276 breast cancer patients to predict the onset timing of cardiotoxicity-induced heart disease. Furthermore, LITT outperforms both the benchmark and state-of-the-art survival analysis methods on public datasets, positioning it as a significant step forward for precision medicine in clinical AI.

</details>


### [210] [Colorful Talks with Graphs: Human-Interpretable Graph Encodings for Large Language Models](https://arxiv.org/abs/2602.10386)
*Angelo Zangari,Peyman Baghershahi,Sourav Medya*

Main category: cs.LG

TL;DR: 本文提出了一种将图结构以人类可解释的方式编码为自然语言提示的方法，通过改进的Weisfeiler-Lehman相似类映射为颜色标记，提升大语言模型在图任务上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 图任务要求结构感知、置换不变性和复杂关系推理，而大语言模型擅长处理非结构化文本，二者存在表征不匹配问题。

Method: 提出一种人类可解释的图到文本结构编码策略：计算改进版Weisfeiler-Lehman相似类，并将其映射为类人颜色标记而非数字标签，嵌入自然语言提示中。

Result: 在多个算法性与预测性图任务（含合成与真实数据集）上显著提升LLM性能，尤其在需全局图结构推理的任务中效果突出。

Conclusion: 语义清晰、人类可解释的结构提示比符号化编码更适配LLM，有效弥合了LLM与图推理之间的表征鸿沟。

Abstract: Graph problems are fundamentally challenging for large language models (LLMs). While LLMs excel at processing unstructured text, graph tasks require reasoning over explicit structure, permutation invariance, and computationally complex relationships, creating a mismatch with the representations of text-based models. Our work investigates how LLMs can be effectively applied to graph problems despite these barriers. We introduce a human-interpretable structural encoding strategy for graph-to-text translation that injects graph structure directly into natural language prompts. Our method involves computing a variant of Weisfeiler-Lehman (WL) similarity classes and maps them to human-like color tokens rather than numeric labels. The key insight is that semantically meaningful and human-interpretable cues may be more effectively processed by LLMs than opaque symbolic encoding. Experimental results on multiple algorithmic and predictive graph tasks show the considerable improvements by our method on both synthetic and real-world datasets. By capturing both local and global-range dependencies, our method enhances LLM performance especially on graph tasks that require reasoning over global graph structure.

</details>


### [211] [Affordances Enable Partial World Modeling with LLMs](https://arxiv.org/abs/2602.10390)
*Khimya Khetarpal,Gheorghe Comanici,Jonathan Richens,Jeremy Shar,Fei Xia,Laurent Orseau,Aleksandra Faust,Doina Precup*

Main category: cs.LG

TL;DR: 本文探讨了将大型预训练模型视为部分世界模型（partial world models）的可行性，提出并证明了任务无关、语言条件化意图的智能体必然具备基于功能（affordances）的部分世界预测模型；进一步引入分布鲁棒的功能概念，在多任务场景下可高效提取部分模型，实验证明其在桌面机器人任务中显著降低搜索分支因子并提升奖励。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型虽蕴含丰富世界知识，但直接用于搜索效率低、精度差；而部分世界模型聚焦于与用户意图相关联的功能性状态-动作子集，更实用。本文旨在形式化回答‘能否将大模型视为部分世界模型’这一问题。

Method: 提出并证明任务无关、语言条件化意图的智能体必然具备由功能驱动的预测性部分世界模型；在多任务设定下定义并使用分布鲁棒的功能（distribution-robust affordances），从中提取部分模型以提升搜索效率。

Result: 理论证明成立；实验表明所提方法在桌面机器人任务中降低了搜索分支因子，并获得比全世界模型更高的任务奖励。

Conclusion: 大型语言模型可被形式化地视为功能驱动的部分世界模型；通过提取此类模型，可在保证预测质量的同时大幅提升规划与搜索效率，尤其适用于多任务、分布变化场景。

Abstract: Full models of the world require complex knowledge of immense detail. While pre-trained large models have been hypothesized to contain similar knowledge due to extensive pre-training on vast amounts of internet scale data, using them directly in a search procedure is inefficient and inaccurate. Conversely, partial models focus on making high quality predictions for a subset of state and actions: those linked through affordances that achieve user intents~\citep{khetarpal2020can}. Can we posit large models as partial world models? We provide a formal answer to this question, proving that agents achieving task-agnostic, language-conditioned intents necessarily possess predictive partial-world models informed by affordances. In the multi-task setting, we introduce distribution-robust affordances and show that partial models can be extracted to significantly improve search efficiency. Empirical evaluations in tabletop robotics tasks demonstrate that our affordance-aware partial models reduce the search branching factor and achieve higher rewards compared to full world models.

</details>


### [212] [Tensor Methods: A Unified and Interpretable Approach for Material Design](https://arxiv.org/abs/2602.10392)
*Shaan Pakala,Aldair E. Gongora,Brian Giera,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 本文提出使用张量补全方法作为材料设计的代理模型，以解决传统机器学习方法在可解释性和非均匀采样数据下性能下降的问题；实验表明该方法预测性能与传统ML相当，且提供物理可解释的张量因子，并在非均匀采样下显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 材料设计中参数空间随维度增加呈指数增长，传统仿真（如有限元）计算成本高，而现有机器学习代理模型存在难以解释、在非均匀采样数据上表现差的问题。

Method: 采用经典及改进的张量补全方法构建代理模型，利用其内在可分解结构获取可解释的物理因子，并对比其在均匀与非均匀采样下的预测性能。

Result: 张量方法预测精度媲美传统ML，且张量因子能复现已知物理现象；在非均匀采样下，特定张量模型相较基线ML方法整体R²提升达5%，部分分布外区域误差减半。

Conclusion: 张量补全是一种兼具高预测精度、强可解释性及对非均匀采样鲁棒性的新型代理建模范式，有助于连接数据驱动发现与物理机理理解。

Abstract: When designing new materials, it is often necessary to tailor the material design (with respect to its design parameters) to have some desired properties (e.g. Young's modulus). As the set of design parameters grow, the search space grows exponentially, making the actual synthesis and evaluation of all material combinations virtually impossible. Even using traditional computational methods such as Finite Element Analysis becomes too computationally heavy to search the design space. Recent methods use machine learning (ML) surrogate models to more efficiently determine optimal material designs; unfortunately, these methods often (i) are notoriously difficult to interpret and (ii) under perform when the training data comes from a non-uniform sampling of the design space. We suggest the use of tensor completion methods as an all-in-one approach for interpretability and predictions. We observe classical tensor methods are able to compete with traditional ML in predictions, with the added benefit of their interpretable tensor factors (which are given completely for free, as a result of the prediction). In our experiments, we are able to rediscover physical phenomena via the tensor factors, indicating that our predictions are aligned with the true underlying physics of the problem. This also means these tensor factors could be used by experimentalists to identify potentially novel patterns, given we are able to rediscover existing ones. We also study the effects of both types of surrogate models when we encounter training data from a non-uniform sampling of the design space. We observe more specialized tensor methods that can give better generalization in these non-uniforms sampling scenarios. We find the best generalization comes from a tensor model, which is able to improve upon the baseline ML methods by up to 5% on aggregate $R^2$, and halve the error in some out of distribution regions.

</details>


### [213] [Experimental Demonstration of Online Learning-Based Concept Drift Adaptation for Failure Detection in Optical Networks](https://arxiv.org/abs/2602.10401)
*Yousuf Moiz Ali,Jaroslaw E. Prilepsky,João Pedro,Antonio Napoli,Sasipim Srivallapanondh,Sergei K. Turitsyn,Pedro Freire*

Main category: cs.LG

TL;DR: 本文提出了一种基于在线学习的新型方法，用于光网络故障检测中的概念漂移适应，相比传统静态模型性能提升高达70%，同时保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决光网络故障检测中因概念漂移导致的传统静态模型性能下降问题。

Method: 采用在线学习方法进行概念漂移自适应。

Result: 在光网络故障检测任务中，性能相较传统静态模型提升高达70%，且保持低延迟。

Conclusion: 所提在线学习方法能有效应对概念漂移，在保证实时性的同时显著提升检测性能。

Abstract: We present a novel online learning-based approach for concept drift adaptation in optical network failure detection, achieving up to a 70% improvement in performance over conventional static models while maintaining low latency.

</details>


### [214] [Modular Multi-Task Learning for Chemical Reaction Prediction](https://arxiv.org/abs/2602.10404)
*Jiayun Pang,Ahmed M. Zaitoun,Xacobe Couso Cambeiro,Ivan Vulić*

Main category: cs.LG

TL;DR: 本文评估了低秩适应（LoRA）作为参数高效微调方法，在有限且复杂的有机反应数据集上进行有机反应预测的效果，结果表明LoRA在保持多任务性能和缓解灾难性遗忘方面优于全量微调，并能有效适应特定反应类型如C-H官能团化。


<details>
  <summary>Details</summary>
Motivation: 将广泛训练的大型语言模型（LLMs）适配到小规模、领域特定的反应数据集是化学与制药研发中的关键挑战，需在学习新反应知识的同时保留通用化学理解。

Method: 采用低秩适应（LoRA）方法，在USPTO反应类别和具有挑战性的C-H官能化反应数据集上，对前向反应预测、逆合成分析和试剂预测三大任务进行基准测试，并与全量微调对比。

Result: LoRA在准确性上媲美全量微调，显著缓解灾难性遗忘，更好维持多任务性能；两种方法均能泛化至训练分布外（如合理预测替代溶剂）；C-H官能化实验显示LoRA编码的反应模式更特异、更有效。

Conclusion: LoRA等模块化、参数高效的微调策略，为大规模LLMs在化学领域的灵活部署提供了实用可行的路径。

Abstract: Adapting large language models (LLMs) trained on broad organic chemistry to smaller, domain-specific reaction datasets is a key challenge in chemical and pharmaceutical R&D. Effective specialisation requires learning new reaction knowledge while preserving general chemical understanding across related tasks. Here, we evaluate Low-Rank Adaptation (LoRA) as a parameter-efficient alternative to full fine-tuning for organic reaction prediction on limited, complex datasets. Using USPTO reaction classes and challenging C-H functionalisation reactions, we benchmark forward reaction prediction, retrosynthesis and reagent prediction. LoRA achieves accuracy comparable to full fine-tuning while effectively mitigating catastrophic forgetting and better preserving multi-task performance. Both fine-tuning approaches generalise beyond training distributions, producing plausible alternative solvent predictions. Notably, C-H functionalisation fine-tuning reveals that LoRA and full fine-tuning encode subtly different reactivity patterns, suggesting more effective reaction-specific adaptation with LoRA. As LLMs continue to scale, our results highlight the practicality of modular, parameter-efficient fine-tuning strategies for their flexible deployment for chemistry applications.

</details>


### [215] [Gated Removal of Normalization in Transformers Enables Stable Training and Efficient Inference](https://arxiv.org/abs/2602.10408)
*Andrei Kanavalau,Carmen Amo Alonso,Sanjay Lall*

Main category: cs.LG

TL;DR: 本文提出TaperNorm，一种渐进式替代RMSNorm/LayerNorm的归一化方法，在训练初期保持标准归一化行为，随后平滑过渡为样本无关的线性/仿射变换；理论与实验表明输出归一化的核心作用是‘尺度锚定’以抑制logit无界增长，且可通过固定目标辅助损失显式实现该功能；TaperNorm在保持性能的同时消除token级统计量，支持推理时折叠缩放操作，提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 重新审视预归一化Transformer中样本依赖归一化的必要性，探究其是否可被简化或去除。

Method: 提出TaperNorm：通过一个全局门控机制控制归一化强度，初期保持RMSNorm/LayerNorm行为，训练中用EMA校准缩放分支，再余弦衰减至0，使归一化完全退化为样本无关的线性变换；同时引入针对残差流尺度的固定目标辅助损失作为归一化的替代锚点。

Result: TaperNorm在各项任务上匹配基线性能，消除token级统计计算，支持将内部缩放参数折叠进相邻线性层；微基准测试显示最后token logits模式下吞吐量最高提升1.22倍。

Conclusion: 样本依赖归一化并非Transformer训练所必需，其核心作用在于输出尺度锚定；TaperNorm实现了向无归一化Transformer的可行过渡，并明确了输出归一化的特殊功能。

Abstract: Normalization is widely viewed as essential for stabilizing Transformer training. We revisit this assumption for pre-norm Transformers and ask to what extent sample-dependent normalization is needed inside Transformer blocks. We introduce TaperNorm, a drop-in replacement for RMSNorm/LayerNorm that behaves exactly like the standard normalizer early in training and then smoothly tapers to a learned sample-independent linear/affine map. A single global gate is held at $g{=}1$ during gate warmup, used to calibrate the scaling branch via EMAs, and then cosine-decayed to $g{=}0$, at which point per-token statistics vanish and the resulting fixed scalings can be folded into adjacent linear projections. Our theoretical and empirical results isolate scale anchoring as the key role played by output normalization: as a (near) $0$-homogeneous map it removes radial gradients at the output, whereas without such an anchor cross-entropy encourages unbounded logit growth (``logit chasing''). We further show that a simple fixed-target auxiliary loss on the pre-logit residual-stream scale provides an explicit alternative anchor and can aid removal of the final normalization layer. Empirically, TaperNorm matches normalized baselines under identical setups while eliminating per-token statistics and enabling these layers to be folded into adjacent linear projections at inference. On an efficiency microbenchmark, folding internal scalings yields up to $1.22\times$ higher throughput in last-token logits mode. These results take a step towards norm-free Transformers while identifying the special role output normalization plays.

</details>


### [216] [Semi-Supervised Cross-Domain Imitation Learning](https://arxiv.org/abs/2602.10793)
*Li-Min Chu,Kai-Siang Ma,Ming-Hong Chen,Ping-Chun Hsieh*

Main category: cs.LG

TL;DR: 本文提出了半监督跨域模仿学习（SS-CDIL）新设定，并设计首个具理论保证的算法，仅需少量目标域专家演示和未标注的次优轨迹，通过新跨域损失函数与自适应权重机制实现稳定、高效策略学习。


<details>
  <summary>Details</summary>
Motivation: 现有跨域模仿学习方法要么依赖大量标注数据（监督式），要么分布对齐不稳定（无监督式），而实际中专家数据获取成本高，亟需更鲁棒、低监督需求的方案。

Method: 提出半监督CDIL设定；设计新型跨域损失函数学习源域与目标域间状态-动作映射；引入自适应权重函数平衡源域与目标域知识；全部基于离线数据训练。

Result: 在MuJoCo和Robosuite上的实验表明，该方法在稳定性与数据效率上持续优于基线方法。

Conclusion: SS-CDIL为跨域模仿学习提供了更实用、低监督、理论可证的新范式，显著降低对高质量专家数据的依赖。

Abstract: Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.

</details>


### [217] [LUCID: Attention with Preconditioned Representations](https://arxiv.org/abs/2602.10410)
*Sai Surya Duvvuri,Nirmal Patel,Nilesh Gupta,Inderjit S. Dhillon*

Main category: cs.LG

TL;DR: 本文提出LUCID Attention，通过在注意力概率上应用基于指数化键-键相似度的预调节器，缓解长上下文下Softmax注意力的扩散问题，避免低温度导致的梯度消失，在保持相同计算复杂度的同时显著提升长文本检索任务性能。


<details>
  <summary>Details</summary>
Motivation: Softmax注意力在长上下文下易将概率质量扩散至无关token，降低性能；而降低softmax温度虽可增强聚焦，却引发梯度消失、损害可学习性。

Method: 提出LUCID Attention：利用指数化键-键相似度构造预调节器，最小化键在再生核希尔伯特空间（RKHS）中的重叠，从而提升查询对关键键的选择精度；该方法无需调低温度，且计算复杂度与标准注意力一致。

Result: 在约10亿参数模型上验证，于BABILong、RULER、SCROLLS和LongBench等长上下文检索基准中显著提升性能，如BABILong提升达18%，RULER多针检索提升14%。

Conclusion: LUCID Attention是一种高效、可学习性强的注意力改进方法，有效解决长序列下Softmax注意力的固有局限，为长上下文建模提供了新思路。

Abstract: Softmax-based dot-product attention is a cornerstone of Transformer architectures, enabling remarkable capabilities such as in-context learning. However, as context lengths increase, a fundamental limitation of the softmax function emerges: it tends to diffuse probability mass to irrelevant tokens degrading performance in long-sequence scenarios. Furthermore, attempts to sharpen focus by lowering softmax temperature hinder learnability due to vanishing gradients. We introduce LUCID Attention, an architectural modification that applies a preconditioner to the attention probabilities. This preconditioner, derived from exponentiated key-key similarities, minimizes overlap between the keys in a Reproducing Kernel Hilbert Space, thus allowing the query to focus on important keys among large number of keys accurately with same computational complexity as standard attention. Additionally, LUCID's preconditioning-based approach to retrieval bypasses the need for low temperature and the learnability problems associated with it. We validate our approach by training ~1 billion parameter language models evaluated on up to 128K tokens. Our results demonstrate significant gains on long-context retrieval tasks, specifically retrieval tasks from BABILong, RULER, SCROLLS and LongBench. For instance, LUCID achieves up to 18% improvement in BABILong and 14% improvement in RULER multi-needle performance compared to standard attention.

</details>


### [218] [LightGTS-Cov: Covariate-Enhanced Time Series Forecasting](https://arxiv.org/abs/2602.10412)
*Yong Shang,Zhipeng Yao,Ning Jin,Xiangfei Qiu,Hui Zhang,Bin Yang*

Main category: cs.LG

TL;DR: 本文提出了LightGTS-Cov，一种在轻量级、周期感知的LightGTS基础上增强协变量建模能力的时间序列基础模型，通过残差式MLP插件融合历史与未来已知协变量，在电力价格与可再生能源预测等任务中显著优于基线方法，并在真实工业场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型在处理协变量丰富的应用场景（如电价和新能源发电预测）时，常忽略协变量或仅简单拼接，限制了建模效果。

Method: 在约100万参数的LightGTS骨干网络上，增加一个约10万参数的MLP插件，以时间对齐方式将历史与未来已知协变量残差式地融入解码输出，保持模型轻量且周期感知。

Result: 在多个协变量感知基准数据集（电力价格、能源发电）上，LightGTS-Cov持续超越原LightGTS及其他协变量感知基线；在光伏长期功率预测和日前电价预测两个真实工业案例中，部署后展现出高精度与运行稳定性。

Conclusion: LightGTS-Cov是一种高效、实用的协变量增强型轻量时间序列模型，兼顾建模能力与部署可行性，适用于真实能源系统预测任务。

Abstract: Time series foundation models are typically pre-trained on large, multi-source datasets; however, they often ignore exogenous covariates or incorporate them via simple concatenation with the target series, which limits their effectiveness in covariate-rich applications such as electricity price forecasting and renewable energy forecasting. We introduce LightGTS-Cov, a covariate-enhanced extension of LightGTS that preserves its lightweight, period-aware backbone while explicitly incorporating both past and future-known covariates. Built on a $\sim$1M-parameter LightGTS backbone, LightGTS-Cov adds only a $\sim$0.1M-parameter MLP plug-in that integrates time-aligned covariates into the target forecasts by residually refining the outputs of the decoding process. Across covariate-aware benchmarks on electricity price and energy generation datasets, LightGTS-Cov consistently outperforms LightGTS and achieves superior performance over other covariate-aware baselines under both settings, regardless of whether future-known covariates are provided. We further demonstrate its practical value in two real-world energy case applications: long-term photovoltaic power forecasting with future weather forecasts and day-ahead electricity price forecasting with weather and dispatch-plan covariates. Across both applications, LightGTS-Cov achieves strong forecasting accuracy and stable operational performance after deployment, validating its effectiveness in real-world industrial settings.

</details>


### [219] [AI-rithmetic](https://arxiv.org/abs/2602.10416)
*Alex Bie,Travis Dick,Alex Kulesza,Prabhakar Raghavan,Vinod Raman,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: 本文系统研究了现代AI系统在基础算术（特别是整数加法）上的严重缺陷，发现所有前沿模型随数字位数增加准确率显著下降，错误主要源于操作数错位或进位失败，并揭示了这些错误与分词及随机性之间的关联。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在高级数学任务上取得成功，但在基础算术（如两数相加）上表现极差，这一反常现象亟需系统性探究。

Method: 通过实证分析前沿大模型在不同位数整数加法任务上的表现，对错误进行归因分类（错位、进位失败等），并结合分词机制与错误模式分析其成因。

Result: 所有前沿模型加法准确率随位数增长而显著下降；87.9%（Claude Opus 4.1）、62.9%（GPT-5）、92.4%（Gemini 2.5 Pro）的错误可归为错位或进位失败；错位错误多与tokenization相关，进位错误则呈近似随机分布。

Conclusion: 当前大语言模型的基础算术能力存在根本性缺陷，其错误具有高度可解释性，根源在于架构与训练范式对底层数值运算建模不足，需针对性改进。

Abstract: Modern AI systems have been successfully deployed to win medals at international math competitions, assist with research workflows, and prove novel technical lemmas. However, despite their progress at advanced levels of mathematics, they remain stubbornly bad at basic arithmetic, consistently failing on the simple task of adding two numbers. We present a systematic investigation of this phenomenon. We demonstrate empirically that all frontier models suffer significantly degraded accuracy for integer addition as the number of digits increases. Furthermore, we show that most errors made by these models are highly interpretable and can be attributed to either operand misalignment or a failure to correctly carry; these two error classes explain 87.9%, 62.9%, and 92.4% of Claude Opus 4.1, GPT-5, and Gemini 2.5 Pro errors, respectively. Finally, we show that misalignment errors are frequently related to tokenization, and that carrying errors appear largely as independent random failures.

</details>


### [220] [Equivariant Evidential Deep Learning for Interatomic Potentials](https://arxiv.org/abs/2602.10419)
*Zhongyao Wang,Taoyong Cui,Jiawen Zou,Shufei Zhang,Bo Yan,Wanli Ouyang,Weimin Tan,Mao Su*

Main category: cs.LG

TL;DR: 本文提出了一种名为e²IP的等变证据深度学习框架，用于分子动力学中机器学习原子势的不确定性量化，通过建模3×3对称正定协方差张量实现力与不确定性的联合、旋转等变表示，在精度、效率和可靠性间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习原子势（MLIPs）的不确定性量化（UQ）方法常受限于高计算成本或性能不佳；传统证据深度学习（EDL）难以直接推广到需满足旋转等变性的向量型目标（如原子力）。

Method: 提出Equivariant Evidential Deep Learning for Interatomic Potentials（e²IP），一种骨架无关框架：将原子力的不确定性建模为一个3×3对称正定协方差张量，并确保其在旋转下严格等变；支持单模型前向传播同时输出力及其完整不确定性。

Result: 在多个分子基准测试中，e²IP相比非等变证据基线和常用集成方法，在准确性、计算效率与不确定性校准性三方面取得更优平衡；具备更高数据效率和单模型推理优势。

Conclusion: e²IP为MLIPs提供了理论严谨、计算高效且物理一致（旋转等变）的不确定性量化新范式，推动了不确定性感知分子模拟与主动学习等应用的发展。

Abstract: Uncertainty quantification (UQ) is critical for assessing the reliability of machine learning interatomic potentials (MLIPs) in molecular dynamics (MD) simulations, identifying extrapolation regimes and enabling uncertainty-aware workflows such as active learning for training dataset construction. Existing UQ approaches for MLIPs are often limited by high computational cost or suboptimal performance. Evidential deep learning (EDL) provides a theoretically grounded single-model alternative that determines both aleatoric and epistemic uncertainty in a single forward pass. However, extending evidential formulations from scalar targets to vector-valued quantities such as atomic forces introduces substantial challenges, particularly in maintaining statistical self-consistency under rotational transformations. To address this, we propose \textit{Equivariant Evidential Deep Learning for Interatomic Potentials} ($\text{e}^2$IP), a backbone-agnostic framework that models atomic forces and their uncertainty jointly by representing uncertainty as a full $3\times3$ symmetric positive definite covariance tensor that transforms equivariantly under rotations. Experiments on diverse molecular benchmarks show that $\text{e}^2$IP provides a stronger accuracy-efficiency-reliability balance than the non-equivariant evidential baseline and the widely used ensemble method. It also achieves better data efficiency through the fully equivariant architecture while retaining single-model inference efficiency.

</details>


### [221] [Binary Flow Matching: Prediction-Loss Space Alignment for Robust Learning](https://arxiv.org/abs/2602.10420)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: 本文研究了流匹配（flow matching）在二值流形（binary manifolds）上的应用，指出信号空间预测（x-预测）虽有效，但与速度目标（v-loss）结合时存在结构不匹配，导致时间依赖的奇异加权和梯度敏感；为此提出并证明了预测损失对齐（prediction-loss alignment）的必要性，采用x-loss可消除奇异加权、实现梯度有界和鲁棒训练；进一步分析了二值数据下概率型（如交叉熵）与几何型（如MSE）损失的拓扑依赖差异，为离散域流匹配提供了理论基础与实践指南。


<details>
  <summary>Details</summary>
Motivation: 将信号空间预测（x-预测）范式迁移到二值流形时，发现其与速度目标（v-loss）耦合引发结构不匹配，导致时间依赖的奇异加权和梯度不稳定，亟需理论解释与稳健训练方案。

Method: 形式化定义预测-损失对齐（prediction-loss alignment）为流匹配训练的必要条件；理论证明采用信号空间损失（x-loss）可消除奇异加权、保证梯度一致有界；并在二值数据设定下系统比较概率型与几何型损失的拓扑行为差异。

Result: 证明x-loss能消除v-loss引入的时间依赖奇异加权，实现均匀有界的梯度和无需启发式时间表的鲁棒训练；揭示二值数据中交叉熵等概率损失与MSE等几何损失存在本质的拓扑依赖差异。

Conclusion: 预测-损失对齐是离散域流匹配稳健训练的核心原则；x-loss优于v-loss；损失函数选择需考虑数据流形拓扑，为二值及更广义离散生成建模提供理论支撑与实用指导。

Abstract: Flow matching has emerged as a powerful framework for generative modeling, with recent empirical successes highlighting the effectiveness of signal-space prediction ($x$-prediction). In this work, we investigate the transfer of this paradigm to binary manifolds, a fundamental setting for generative modeling of discrete data. While $x$-prediction remains effective, we identify a latent structural mismatch that arises when it is coupled with velocity-based objectives ($v$-loss), leading to a time-dependent singular weighting that amplifies gradient sensitivity to approximation errors. Motivated by this observation, we formalize prediction-loss alignment as a necessary condition for flow matching training. We prove that re-aligning the objective to the signal space ($x$-loss) eliminates the singular weighting, yielding uniformly bounded gradients and enabling robust training under uniform timestep sampling without reliance on heuristic schedules. Finally, with alignment secured, we examine design choices specific to binary data, revealing a topology-dependent distinction between probabilistic objectives (e.g., cross-entropy) and geometric losses (e.g., mean squared error). Together, these results provide theoretical foundations and practical guidelines for robust flow matching on binary -- and related discrete -- domains, positioning signal-space alignment as a key principle for robust diffusion learning.

</details>


### [222] [Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation](https://arxiv.org/abs/2602.10430)
*Jie Jiang,Yusen Huo,Xiangxin Zhan,Changping Wang,Jun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种分布鲁棒策略优化（DRPO）方法，通过构建乐观分布鲁棒优化（DRO）目标，理论证明硬过滤是其精确解，从而在离线推荐中有效分离高质量行为、抑制噪声引发的模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有基于策略的强化学习在离线历史日志上应用时，因低质量数据主导而导致严重模型崩溃；传统方法无法兼顾方差降低与噪声建模之间的矛盾。

Method: 提出分布鲁棒策略优化（DRPO），将优化目标重构为乐观分布鲁棒优化（DRO）问题，并从理论上证明硬过滤是该DRO目标的精确解，用以严格剔除发散性噪声、恢复高质量行为分布。

Result: DRPO在混合质量推荐基准上实现了最先进的性能，显著缓解了离线RL中的模型崩溃问题。

Conclusion: 通过识别并优化隐含的高质量行为分布，DRPO从根本上解决了离线策略优化中由负梯度更新引发的指数级发散问题，为生成式推荐提供了更鲁棒、更可靠的训练范式。

Abstract: Policy-based Reinforcement Learning (RL) has established itself as the dominant paradigm in generative recommendation for optimizing sequential user interactions. However, when applied to offline historical logs, these methods suffer a critical failure: the dominance of low-quality data induces severe model collapse. We first establish the Divergence Theory of Repulsive Optimization, revealing that negative gradient updates inherently trigger exponential intensity explosion during off-policy training. This theory elucidates the inherent dilemma of existing methods, exposing their inability to reconcile variance reduction and noise imitation. To break this curse, we argue that the solution lies in rigorously identifying the latent high-quality distribution entangled within the noisy behavior policy. Accordingly, we reformulate the objective as an Optimistic Distributionally Robust Optimization (DRO) problem. Guided by this formulation, we propose Distributionally Robust Policy Optimization (DRPO). We prove that hard filtering is the exact solution to this DRO objective, enabling DRPO to optimally recover high-quality behaviors while strictly discarding divergence-inducing noise. Extensive experiments demonstrate that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks.

</details>


### [223] [QTALE: Quantization-Robust Token-Adaptive Layer Execution for LLMs](https://arxiv.org/abs/2602.10431)
*Kanghyun Noh,Jinheon Choi,Yulwha Kim*

Main category: cs.LG

TL;DR: QTALE是一种新框架，旨在将token-adaptive层执行与量化无缝结合，以在降低计算量和内存占用的同时保持大语言模型（LLM）的精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）部署面临高计算与内存开销挑战；现有token-adaptive执行与量化方法单独有效，但直接组合会因冗余减少导致额外精度下降。

Method: 提出QTALE框架：（1）设计训练策略，确保微调中充分探索多样化的执行路径；（2）引入推理时可调的执行比率后训练机制，按需恢复冗余。

Result: 在CommonsenseQA基准上，QTALE与纯量化模型精度差距控制在0.5%以内，实现FLOPs与内存双重优化且无明显精度损失。

Conclusion: QTALE成功实现了token-adaptive执行与量化的鲁棒协同，为高效LLM部署提供了实用解决方案。

Abstract: Large language models (LLMs) demand substantial computational and memory resources, posing challenges for efficient deployment. Two complementary approaches have emerged to address these issues: token-adaptive layer execution, which reduces floating-point operations (FLOPs) by selectively bypassing layers, and quantization, which lowers memory footprint by reducing weight precision. However, naively integrating these techniques leads to additional accuracy degradation due to reduced redundancy in token-adaptive models. We propose QTALE (Quantization-Robust Token-Adaptive Layer Execution for LLMs), a novel framework that enables seamless integration of token-adaptive execution with quantization while preserving accuracy. Conventional token-adaptive methods reduce redundancy in two ways: (1) by limiting the diversity of training paths explored during fine-tuning, and (2) by lowering the number of parameters actively involved in inference. To overcome these limitations, QTALE introduces two key components: (1) a training strategy that ensures diverse execution paths are actively explored during fine-tuning, and (2) a post-training mechanism that allows flexible adjustment of the execution ratio at inference to reintroduce redundancy when needed. Experimental results show that QTALE enables seamless integration of token-adaptive layer execution with quantization, showing no noticeable accuracy difference, with the gap to quantization-only models kept below 0.5% on CommonsenseQA benchmarks. By combining token-adaptive execution for FLOPs reduction and quantization for memory savings, QTALE provides an effective solution for efficient LLM deployment.

</details>


### [224] [A Dual-Stream Physics-Augmented Unsupervised Architecture for Runtime Embedded Vehicle Health Monitoring](https://arxiv.org/abs/2602.10432)
*Enzo Nicolas Spotorno,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

TL;DR: 本文提出一种双流架构，结合无监督学习与宏观物理代理，用于实时量化车辆运行强度，特别关注高负载稳态下的机械疲劳，已在RISC-V嵌入式平台上验证其低开销与边缘部署能力。


<details>
  <summary>Details</summary>
Motivation: 传统里程等指标无法反映机械负荷，而现有无监督深度学习模型易将高负载稳态（如重载爬坡）误判为正常，忽视其对传动系统造成的持续疲劳，存在关键盲区。

Method: 提出双流架构：一者采用无监督学习检测表层异常（如瞬态冲击），二者利用低频传感器数据构建宏观物理代理，估算累积负载；二者融合生成多维健康向量，区分动态风险与持续机械负荷。

Result: 该架构在RISC-V嵌入式平台验证成功，计算开销低，支持资源受限ECU上的全时边缘健康监测，避免了云监控的延迟与带宽成本。

Conclusion: 所提方法有效弥补了统计正常性与机械疲劳之间的语义鸿沟，为商用车队预测性维护提供了更精准、可部署的运行强度量化方案。

Abstract: Runtime quantification of vehicle operational intensity is essential for predictive maintenance and condition monitoring in commercial and heavy-duty fleets. Traditional metrics like mileage fail to capture mechanical burden, while unsupervised deep learning models detect statistical anomalies, typically transient surface shocks, but often conflate statistical stability with mechanical rest. We identify this as a critical blind spot: high-load steady states, such as hill climbing with heavy payloads, appear statistically normal yet impose significant drivetrain fatigue. To resolve this, we propose a Dual-Stream Architecture that fuses unsupervised learning for surface anomaly detection with macroscopic physics proxies for cumulative load estimation. This approach leverages low-frequency sensor data to generate a multi-dimensional health vector, distinguishing between dynamic hazards and sustained mechanical effort. Validated on a RISC-V embedded platform, the architecture demonstrates low computational overhead, enabling comprehensive, edge-based health monitoring on resource-constrained ECUs without the latency or bandwidth costs of cloud-based monitoring.

</details>


### [225] [Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering](https://arxiv.org/abs/2602.10437)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.LG

TL;DR: 本文提出Control Reinforcement Learning (CRL)框架，利用强化学习动态选择稀疏自编码器（SAE）特征进行模型输出干预，生成可解释的逐token干预日志，并支持多种新型机制分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器（SAEs）方法只能揭示哪些特征被激活，无法识别哪些特征在放大时实际改变模型输出，缺乏对因果干预效果的刻画。

Method: 提出Control Reinforcement Learning（CRL）：训练一个策略网络在每个token处选择SAE特征进行放大干预；引入Adaptive Feature Masking以促进多样化且单特征可解释的特征发现；配套设计分支点追踪、critic轨迹分析和层间对比等分析模块。

Result: 在Gemma-2 2B模型上，CRL在MMLU、BBQ、GSM8K、HarmBench和XSTest多个基准上实现性能提升，并生成可解释的逐token干预日志；分析表明早期层偏向句法特征、后期层偏向语义特征。

Conclusion: CRL将特征干预从静态分析推进至动态、可学习、可归因的机制探针，拓展了机械可解释性的能力边界，是静态特征分析的重要补充。

Abstract: Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for steering at each token, producing interpretable intervention logs: the learned policy identifies features that change model outputs when amplified. Adaptive Feature Masking encourages diverse feature discovery while preserving singlefeature interpretability. The framework yields new analysis capabilities: branch point tracking locates tokens where feature choice determines output correctness; critic trajectory analysis separates policy limitations from value estimation errors; layer-wise comparison reveals syntactic features in early layers and semantic features in later layers. On Gemma-2 2B across MMLU, BBQ, GSM8K, HarmBench, and XSTest, CRL achieves improvements while providing per-token intervention logs. These results establish learned feature steering as a mechanistic interpretability tool that complements static feature analysis with dynamic intervention probes

</details>


### [226] [LakeMLB: Data Lake Machine Learning Benchmark](https://arxiv.org/abs/2602.10441)
*Feiyu Pan,Tianbin Zhang,Aoqian Zhang,Yu Sun,Zheng Wang,Lixing Chen,Li Pan,Jianhua Li*

Main category: cs.LG

TL;DR: 本文提出了LakeMLB，一个面向数据湖环境的机器学习基准，涵盖Union和Join两类多表场景，提供真实数据集与三种集成策略，并进行了大规模实验验证。


<details>
  <summary>Details</summary>
Motivation: 现代数据湖在大规模机器学习中日益重要，但缺乏标准化的机器学习性能评估基准。

Method: 设计LakeMLB基准，聚焦Union和Join两类多表场景，提供三类真实数据集（政府开放数据、金融、维基百科、在线市场），支持预训练、数据增强和特征增强三类集成策略，并对前沿表格学习方法进行系统实验评估。

Result: 通过实验揭示了不同表格学习方法在复杂数据湖场景下的性能表现差异，验证了LakeMLB的实用性与挑战性。

Conclusion: LakeMLB填补了数据湖环境下机器学习基准的空白，为该领域研究提供了可复现、可扩展的评估平台，并已开源数据与代码。

Abstract: Modern data lakes have emerged as foundational platforms for large-scale machine learning, enabling flexible storage of heterogeneous data and structured analytics through table-oriented abstractions. Despite their growing importance, standardized benchmarks for evaluating machine learning performance in data lake environments remain scarce. To address this gap, we present LakeMLB (Data Lake Machine Learning Benchmark), designed for the most common multi-source, multi-table scenarios in data lakes. LakeMLB focuses on two representative multi-table scenarios, Union and Join, and provides three real-world datasets for each scenario, covering government open data, finance, Wikipedia, and online marketplaces. The benchmark supports three representative integration strategies: pre-training-based, data augmentation-based, and feature augmentation-based approaches. We conduct extensive experiments with state-of-the-art tabular learning methods, offering insights into their performance under complex data lake scenarios. We release both datasets and code to facilitate rigorous research on machine learning in data lake ecosystems; the benchmark is available at https://github.com/zhengwang100/LakeMLB.

</details>


### [227] [A Unified Theory of Random Projection for Influence Functions](https://arxiv.org/abs/2602.10449)
*Pingbang Hu,Yuzheng Hu,Jiaqi W. Ma,Han Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种统一理论，分析了随机投影（sketching）在计算影响函数（influence functions）时的保真性，特别关注无正则化、带岭回归正则化及Kronecker分解结构下的投影行为，并量化了测试梯度超出Hessian值域时的泄漏误差。


<details>
  <summary>Details</summary>
Motivation: 现代过参数化模型中，直接计算或求逆大规模Hessian矩阵F不可行；现有基于JL引理的随机投影方法缺乏对矩阵求逆、正则化及结构化近似下影响函数保持性的理论保证。

Method: 建立统一理论框架，分三种情形分析投影P对g^⊤F^{-1}g′形式影响函数的保持性：1）无正则化下要求P在F值域上单射；2）岭正则化下由有效维度控制误差界；3）Kronecker结构F=A⊗E下支持解耦投影P_A⊗P_E；并分析测试梯度含ker(F)分量时的泄漏项。

Result: 给出了不同设定下影响函数被投影准确近似的充要条件与误差界，揭示了sketch尺寸m与rank(F)、有效维度及结构特性之间的定量关系，并为实际中sketch大小选择提供了理论指导。

Conclusion: 随机投影能否准确估计影响函数不仅取决于JL性质，更关键的是其在F值域上的作用方式及是否适配正则化与结构假设；本文理论填补了该方向空白，支撑更可靠、可解释的数据归因计算。

Abstract: Influence functions and related data attribution scores take the form of $g^{\top}F^{-1}g^{\prime}$, where $F\succeq 0$ is a curvature operator. In modern overparameterized models, forming or inverting $F\in\mathbb{R}^{d\times d}$ is prohibitive, motivating scalable influence computation via random projection with a sketch $P \in \mathbb{R}^{m\times d}$. This practice is commonly justified via the Johnson--Lindenstrauss (JL) lemma, which ensures approximate preservation of Euclidean geometry for a fixed dataset. However, JL does not address how sketching behaves under inversion. Furthermore, there is no existing theory that explains how sketching interacts with other widely-used techniques, such as ridge regularization and structured curvature approximations.
  We develop a unified theory characterizing when projection provably preserves influence functions. When $g,g^{\prime}\in\text{range}(F)$, we show that: 1) Unregularized projection: exact preservation holds iff $P$ is injective on $\text{range}(F)$, which necessitates $m\geq \text{rank}(F)$; 2) Regularized projection: ridge regularization fundamentally alters the sketching barrier, with approximation guarantees governed by the effective dimension of $F$ at the regularization scale; 3) Factorized influence: for Kronecker-factored curvatures $F=A\otimes E$, the guarantees continue to hold for decoupled sketches $P=P_A\otimes P_E$, even though such sketches exhibit row correlations that violate i.i.d. assumptions. Beyond this range-restricted setting, we analyze out-of-range test gradients and quantify a \emph{leakage} term that arises when test gradients have components in $\ker(F)$. This yields guarantees for influence queries on general test points.
  Overall, this work develops a novel theory that characterizes when projection provably preserves influence and provides principled guidance for choosing the sketch size in practice.

</details>


### [228] [Constructing Industrial-Scale Optimization Modeling Benchmark](https://arxiv.org/abs/2602.10450)
*Zhong Li,Hongliang Lu,Tao Wei,Wenyu Liu,Yuxuan Chen,Yuan Lan,Fan Zhang,Zaiwen Wen*

Main category: cs.LG

TL;DR: 本文提出了MIPLIB-NL——首个面向真实工业规模混合整数线性规划（MILP）问题的自然语言到优化建模基准，通过结构感知的逆向构建方法，从MIPLIB 2017真实模型中反向生成语义一致、可验证的自然语言描述与参考代码，填补了现有评估在真实复杂度和结构保真度上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在自然语言到优化建模任务中的评估依赖于玩具级或合成数据集，无法反映工业级问题（10³–10⁶变量/约束）的真实难度；缺乏将自然语言需求与真实优化模型及其求解器代码对齐的可靠基准。

Method: 提出结构感知的逆向构建方法：（i）从扁平化求解器代码中恢复紧凑、可复用的模型结构；（ii）基于统一的‘模型–数据分离’格式，逆向生成与该结构显式绑定的自然语言描述；（iii）通过专家评审与人–LLM协同的独立重构验证进行迭代语义校验。

Result: 构建了包含223个一一对应重构实例的MIPLIB-NL基准，严格保持原始MILP的数学内容；实验表明，当前在玩具基准上表现优异的系统在MIPLIB-NL上性能显著下降，暴露出小规模测试无法发现的失败模式。

Conclusion: MIPLIB-NL为自然语言驱动的优化建模提供了首个面向真实复杂度、结构保真与语义可验证的评估基准，推动LLM在运筹学实际应用中的可信落地。

Abstract: Optimization modeling underpins decision-making in logistics, manufacturing, energy, and finance, yet translating natural-language requirements into correct optimization formulations and solver-executable code remains labor-intensive. Although large language models (LLMs) have been explored for this task, evaluation is still dominated by toy-sized or synthetic benchmarks, masking the difficulty of industrial problems with $10^{3}$--$10^{6}$ (or more) variables and constraints. A key bottleneck is the lack of benchmarks that align natural-language specifications with reference formulations/solver code grounded in real optimization models. To fill in this gap, we introduce MIPLIB-NL, built via a structure-aware reverse construction methodology from real mixed-integer linear programs in MIPLIB~2017. Our pipeline (i) recovers compact, reusable model structure from flat solver formulations, (ii) reverse-generates natural-language specifications explicitly tied to this recovered structure under a unified model--data separation format, and (iii) performs iterative semantic validation through expert review and human--LLM interaction with independent reconstruction checks. This yields 223 one-to-one reconstructions that preserve the mathematical content of the original instances while enabling realistic natural-language-to-optimization evaluation. Experiments show substantial performance degradation on MIPLIB-NL for systems that perform strongly on existing benchmarks, exposing failure modes invisible at toy scale.

</details>


### [229] [A Multimodal Conditional Mixture Model with Distribution-Level Physics Priors](https://arxiv.org/abs/2602.10451)
*Jinkyo Han,Bahador Bahmani*

Main category: cs.LG

TL;DR: 本文提出了一种基于混合密度网络（MDN）的物理信息多模态条件建模框架，通过在各成分上引入物理约束正则项，实现对多模态物理系统（如分岔、随机PDE、原子尺度激波）的可解释、高效建模，并在性能上媲美条件流匹配等先进生成模型。


<details>
  <summary>Details</summary>
Motivation: 许多科学与工程系统具有内在多模态行为（如隐含状态切换、非唯一物理机制），现有方法难以在数据有限或需保持物理结构的前提下，学习出物理一致且可解释的完整条件分布。

Method: 构建基于混合密度网络（MDN）的物理信息多模态条件建模框架；对每个MDN成分施加物理规律（如控制方程）违反的正则惩罚，实现物理知识嵌入；支持上下文条件输入，兼顾非唯一性与随机性。

Result: 在分岔动力系统、随机偏微分方程和原子尺度激波等典型多模态物理问题上验证有效；相比条件流匹配（CFM）等前沿生成模型，达到竞争性性能，同时更简洁、可解释、计算高效。

Conclusion: MDN结合物理正则化是一种可行、高效且可解释的物理信息多模态建模范式，尤其适用于数据受限、需严格物理一致性与机制可解释性的科学场景。

Abstract: Many scientific and engineering systems exhibit intrinsically multimodal behavior arising from latent regime switching and non-unique physical mechanisms. In such settings, learning the full conditional distribution of admissible outcomes in a physically consistent and interpretable manner remains a challenge. While recent advances in machine learning have enabled powerful multimodal generative modeling, their integration with physics-constrained scientific modeling remains nontrivial, particularly when physical structure must be preserved or data are limited. This work develops a physics-informed multimodal conditional modeling framework based on mixture density representations. Mixture density networks (MDNs) provide an explicit and interpretable parameterization of multimodal conditional distributions. Physical knowledge is embedded through component-specific regularization terms that penalize violations of governing equations or physical laws. This formulation naturally accommodates non-uniqueness and stochasticity while remaining computationally efficient and amenable to conditioning on contextual inputs. The proposed framework is evaluated across a range of scientific problems in which multimodality arises from intrinsic physical mechanisms rather than observational noise, including bifurcation phenomena in nonlinear dynamical systems, stochastic partial differential equations, and atomistic-scale shock dynamics. In addition, the proposed method is compared with a conditional flow matching (CFM) model, a representative state-of-the-art generative modeling approach, demonstrating that MDNs can achieve competitive performance while offering a simpler and more interpretable formulation.

</details>


### [230] [Analyzing Fairness of Neural Network Prediction via Counterfactual Dataset Generation](https://arxiv.org/abs/2602.10457)
*Brian Hyeongseok Kim,Jacqueline L. Mitchell,Chao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于反事实数据集的方法，通过少量修改训练标签来构建反事实数据集，从而分析模型预测对标签偏差的依赖性，用于公平性评估和可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法聚焦于扰动输入，而本文旨在从数据层面出发，探究训练标签偏差如何影响模型预测，以更直接地评估公平性。

Method: 提出反事实数据集构造方法：在原始训练集中仅修改少量标签，构建最接近的替代数据集；结合梯度追踪与启发式排序，定位并修改关键训练样本标签，重新训练模型并检测目标测试样本预测是否改变。

Result: 在7个主流公平性数据集、超1100个测试案例上验证了方法有效性；仅需修改极小比例的训练标签即可改变预测，揭示了训练样本与测试样本间的可解释关联。

Conclusion: 该方法为理解训练数据偏差对模型决策的影响提供了新视角，是一种高效、可解释、面向数据的模型行为分析工具。

Abstract: Interpreting the inference-time behavior of deep neural networks remains a challenging problem. Existing approaches to counterfactual explanation typically ask: What is the closest alternative input that would alter the model's prediction in a desired way? In contrast, we explore counterfactual datasets. Rather than perturbing the input, our method efficiently finds the closest alternative training dataset, one that differs from the original dataset by changing a few labels. Training a new model on this altered dataset can then lead to a different prediction of a given test instance. This perspective provides a new way to assess fairness by directly analyzing the influence of label bias on training and inference. Our approach can be characterized as probing whether a given prediction depends on biased labels. Since exhaustively enumerating all possible alternate datasets is infeasible, we develop analysis techniques that trace how bias in the training data may propagate through the learning algorithm to the trained network. Our method heuristically ranks and modifies the labels of a bounded number of training examples to construct a counterfactual dataset, retrains the model, and checks whether its prediction on a chosen test case changes. We evaluate our approach on feedforward neural networks across over 1100 test cases from 7 widely-used fairness datasets. Results show that it modifies only a small subset of training labels, highlighting its ability to pinpoint the critical training examples that drive prediction changes. Finally, we demonstrate how our counterfactual datasets reveal connections between training examples and test cases, offering an interpretable way to probe dataset bias.

</details>


### [231] [Driving Reaction Trajectories via Latent Flow Matching](https://arxiv.org/abs/2602.10476)
*Yili Shen,Xiangliang Zhang*

Main category: cs.LG

TL;DR: 本文提出LatentRxnFlow，一种基于连续潜在轨迹建模化学反应的新范式，无需机理标注即可从反应物-产物对中学习动态过程，在保持SOTA预测精度的同时，提供可解释的轨迹分析、错误定位与不确定性估计能力。


<details>
  <summary>Details</summary>
Motivation: 现有反应预测模型多为单步映射，缺乏对反应过程的理解；而分步生成方法依赖机理监督或符号操作，推理开销大且泛化性差。

Method: 基于条件流匹配（Conditional Flow Matching），将反应建模为锚定在热力学产物态的连续潜在空间轨迹，仅需标准反应物-产物对进行训练，不依赖中间态或机理标注。

Result: 在USPTO基准上达到SOTA性能；支持轨迹级诊断（如失败模式定位与门控推理纠错）；潜在轨迹几何特性可作为内在认知不确定性信号。

Conclusion: LatentRxnFlow在高精度预测基础上显著提升了模型透明性、可诊断性与不确定性感知能力，更适用于高通量药物/材料发现等可信部署场景。

Abstract: Recent advances in reaction prediction have achieved near-saturated accuracy on standard benchmarks (e.g., USPTO), yet most state-of-the-art models formulate the task as a one-shot mapping from reactants to products, offering limited insight into the underlying reaction process. Procedural alternatives introduce stepwise generation but often rely on mechanism-specific supervision, discrete symbolic edits, and computationally expensive inference. In this work, we propose LatentRxnFlow, a new reaction prediction paradigm that models reactions as continuous latent trajectories anchored at the thermodynamic product state. Built on Conditional Flow Matching, our approach learns time-dependent latent dynamics directly from standard reactant-product pairs, without requiring mechanistic annotations or curated intermediate labels. While LatentRxnFlow achieves state-of-the-art performance on USPTO benchmarks, more importantly, the continuous formulation exposes the full generative trajectory, enabling trajectory-level diagnostics that are difficult to realize with discrete or one-shot models. We show that latent trajectory analysis allows us to localize and characterize failure modes and to mitigate certain errors via gated inference. Furthermore, geometric properties of the learned trajectories provide an intrinsic signal of epistemic uncertainty, helping prioritize reliably predictable reaction outcomes and flag ambiguous cases for additional validation. Overall, LatentRxnFlow combines strong predictive accuracy with improved transparency, diagnosability, and uncertainty awareness, moving reaction prediction toward more trustworthy deployment in high-throughput discovery workflows.

</details>


### [232] [Learning Adaptive Distribution Alignment with Neural Characteristic Function for Graph Domain Adaptation](https://arxiv.org/abs/2602.10489)
*Wei Chen,Xingyu Guo,Shuang Li,Zhao Zhang,Yan Zhong,Fuzhen Zhuang,Deqing wang*

Main category: cs.LG

TL;DR: 本文提出ADAlign框架，通过神经谱差异（NSD）自动识别并联合对齐图域适应中属性、结构及其依赖关系的多维分布偏移，无需人工设计对齐准则，在10个数据集、16个迁移任务上显著优于现有方法，且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有图域适应方法依赖人工选定图元素和手工设计滤波器进行分布对齐，缺乏灵活性，难以应对不同场景下主导差异的变化。

Method: 提出ADAlign框架，核心是神经谱差异（NSD）——一种基于谱域神经特征函数的可学习参数化距离度量，结合可学习频率采样器，通过极小极大范式自适应聚焦关键谱分量，实现属性、结构及依赖关系的联合、自适应分布对齐。

Result: 在10个数据集、16个图域迁移任务上，ADAlign显著超越现有SOTA方法；同时内存占用更低、训练更快。

Conclusion: ADAlign是一种灵活、场景感知且鲁棒的图域适应方法，其自适应联合对齐机制和理论驱动的NSD度量为解决复杂多维分布偏移提供了新范式。

Abstract: Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs to unlabeled target graphs but is challenged by complex, multi-faceted distributional shifts. Existing methods attempt to reduce distributional shifts by aligning manually selected graph elements (e.g., node attributes or structural statistics), which typically require manually designed graph filters to extract relevant features before alignment. However, such approaches are inflexible: they rely on scenario-specific heuristics, and struggle when dominant discrepancies vary across transfer scenarios. To address these limitations, we propose \textbf{ADAlign}, an Adaptive Distribution Alignment framework for GDA. Unlike heuristic methods, ADAlign requires no manual specification of alignment criteria. It automatically identifies the most relevant discrepancies in each transfer and aligns them jointly, capturing the interplay between attributes, structures, and their dependencies. This makes ADAlign flexible, scenario-aware, and robust to diverse and dynamically evolving shifts. To enable this adaptivity, we introduce the Neural Spectral Discrepancy (NSD), a theoretically principled parametric distance that provides a unified view of cross-graph shifts. NSD leverages neural characteristic function in the spectral domain to encode feature-structure dependencies of all orders, while a learnable frequency sampler adaptively emphasizes the most informative spectral components for each task via minimax paradigm. Extensive experiments on 10 datasets and 16 transfer tasks show that ADAlign not only outperforms state-of-the-art baselines but also achieves efficiency gains with lower memory usage and faster training.

</details>


### [233] [Low-Dimensional Execution Manifolds in Transformer Learning Dynamics: Evidence from Modular Arithmetic Tasks](https://arxiv.org/abs/2602.10496)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 本文研究了过参数化Transformer模型在模运算任务中的学习动力学几何结构，发现训练轨迹迅速坍缩到3-4维的执行流形上，并以此解释注意力集中、SGD可积性及稀疏自编码器局限等现象。


<details>
  <summary>Details</summary>
Motivation: 理解过参数化Transformer模型中学习动力学的几何本质，以及高维参数空间中核心计算如何被组织。

Method: 通过可控的模块化算术任务，分析过参数化Transformer（d=128）训练轨迹的几何结构，结合流形分析、投影动力学建模与稀疏自编码器探测。

Result: 训练轨迹快速坍缩至3–4维执行流形；该流形具有鲁棒低维性但方向随随机种子变化；注意力锐化、SGD近似可积性及稀疏自编码器局限均可由此几何结构解释。

Conclusion: Transformer的核心计算发生在极低维执行流形上，大部分参数起‘干扰吸收’作用；该几何框架为可解释性、课程设计和过参数化作用提供统一理解。

Abstract: We investigate the geometric structure of learning dynamics in overparameterized transformer models through carefully controlled modular arithmetic tasks. Our primary finding is that despite operating in high-dimensional parameter spaces ($d=128$), transformer training trajectories rapidly collapse onto low-dimensional execution manifolds of dimension $3$--$4$. This dimensional collapse is robust across random seeds and moderate task difficulties, though the orientation of the manifold in parameter space varies between runs. We demonstrate that this geometric structure underlies several empirically observed phenomena: (1) sharp attention concentration emerges as saturation along routing coordinates within the execution manifold, (2) stochastic gradient descent (SGD) exhibits approximately integrable dynamics when projected onto the execution subspace, with non-integrability confined to orthogonal staging directions, and (3) sparse autoencoders capture auxiliary routing structure but fail to isolate execution itself, which remains distributed across the low-dimensional manifold. Our results suggest a unifying geometric framework for understanding transformer learning, where the vast majority of parameters serve to absorb optimization interference while core computation occurs in a dramatically reduced subspace. These findings have implications for interpretability, training curriculum design, and understanding the role of overparameterization in neural network learning.

</details>


### [234] [Enhancing Ride-Hailing Forecasting at DiDi with Multi-View Geospatial Representation Learning from the Web](https://arxiv.org/abs/2602.10502)
*Xixuan Hao,Guicheng Li,Daiqiang Wu,Xusen Guo,Yumeng Zhu,Zhichao Zou,Peng Zhen,Yao Yao,Yuxuan Liang*

Main category: cs.LG

TL;DR: 本文提出MVGR-Net框架，通过两阶段方法（预训练学习多视角地理空间表征 + 基于提示微调大语言模型进行预测）提升网约车需求预测精度，有效应对地理异质性与外部事件干扰，在滴滴真实数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 网约车预测对优化乘客体验和城市交通效率至关重要，但面临地理空间异质性和易受外部事件影响的挑战。

Method: 提出MVGR-Net：第一阶段通过融合兴趣点（POI）与时空移动模式进行多视角（语义属性+时序移动模式）地理空间表征预训练；第二阶段采用提示驱动的大语言模型微调框架，融入外部事件信息进行预测。

Result: 在滴滴真实数据集上的大量实验表明，该方法达到当前最优（state-of-the-art）性能。

Conclusion: MVGR-Net通过解耦表征学习与预测建模，并融合多源地理语义与时序信息及外部事件，显著提升了网约车需求预测的准确性与鲁棒性。

Abstract: The proliferation of ride-hailing services has fundamentally transformed urban mobility patterns, making accurate ride-hailing forecasting crucial for optimizing passenger experience and urban transportation efficiency. However, ride-hailing forecasting faces significant challenges due to geospatial heterogeneity and high susceptibility to external events. This paper proposes MVGR-Net(Multi-View Geospatial Representation Learning), a novel framework that addresses these challenges through a two-stage approach. In the pretraining stage, we learn comprehensive geospatial representations by integrating Points-of-Interest and temporal mobility patterns to capture regional characteristics from both semantic attribute and temporal mobility pattern views. The forecasting stage leverages these representations through a prompt-empowered framework that fine-tunes Large Language Models while incorporating external events. Extensive experiments on DiDi's real-world datasets demonstrate the state-of-the-art performance.

</details>


### [235] [Learning Structure-Semantic Evolution Trajectories for Graph Domain Adaptation](https://arxiv.org/abs/2602.10506)
*Wei Chen,Xingyu Guo,Shuang Li,Yan Zhong,Zhao Zhang,Fuzhen Zhuang,Hongrui Liu,Libang Zhang,Guo Ye,Huimei He*

Main category: cs.LG

TL;DR: 本文提出DiffGDA，一种基于扩散模型的图域自适应方法，将源图到目标图的适配建模为连续时间随机微分方程过程，并引入域感知网络引导演化路径，在多个真实数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有离散化图域自适应方法难以刻画现实中图结构连续、非线性的演化过程，导致固定步长对齐效果不佳。

Method: 提出DiffGDA，利用随机微分方程（SDE）建模源图到目标图的连续演化过程，联合建模结构与语义变化；设计域感知网络引导扩散轨迹朝向目标域；理论证明该扩散过程在隐空间中收敛于最优域桥接解。

Result: 在8个真实世界数据集共14个图迁移任务上，DiffGDA持续超越当前最先进基线方法。

Conclusion: 基于扩散模型的连续建模范式更契合图结构的实际演化特性，能有效提升图域自适应性能。

Abstract: Graph Domain Adaptation (GDA) aims to bridge distribution shifts between domains by transferring knowledge from well-labeled source graphs to given unlabeled target graphs. One promising recent approach addresses graph transfer by discretizing the adaptation process, typically through the construction of intermediate graphs or stepwise alignment procedures. However, such discrete strategies often fail in real-world scenarios, where graph structures evolve continuously and nonlinearly, making it difficult for fixed-step alignment to approximate the actual transformation process. To address these limitations, we propose \textbf{DiffGDA}, a \textbf{Diff}usion-based \textbf{GDA} method that models the domain adaptation process as a continuous-time generative process. We formulate the evolution from source to target graphs using stochastic differential equations (SDEs), enabling the joint modeling of structural and semantic transitions. To guide this evolution, a domain-aware network is introduced to steer the generative process toward the target domain, encouraging the diffusion trajectory to follow an optimal adaptation path. We theoretically show that the diffusion process converges to the optimal solution bridging the source and target domains in the latent space. Extensive experiments on 14 graph transfer tasks across 8 real-world datasets demonstrate DiffGDA consistently outperforms state-of-the-art baselines.

</details>


### [236] [Don't Eliminate Cut: Exponential Separations in LLM-Based Theorem Proving](https://arxiv.org/abs/2602.10512)
*Sho Sonoda,Shunta Akiyama,Yuya Uezato*

Main category: cs.LG

TL;DR: 本文提出了一种基于随机策略与有限视界确定性MDP的理论框架，用于分析大语言模型（LLM）引导下的形式化定理证明；通过引入参考策略生成的问题分布与Tsybakov型边界条件，推导出成功概率下界，并证明利用子目标分解（cut-aware层次结构）的学习器比扁平（cut-free）学习器具有指数级数据效率优势。


<details>
  <summary>Details</summary>
Motivation: 解释为何LLM在形式化定理证明中经验上成功，尽管理论上存在最坏情况下的计算硬度；尤其关注子目标分解（如引理、草图、cut）在提升证明效率中的作用。

Method: 将战术选择建模为有限视界确定性MDP中的Lipschitz随机策略；采用紧致度量空间建模状态与动作空间；引入基于参考策略q的问题分布与含隐变量的证明DAG模型；结合top-k搜索与Tsybakov型边界，利用序列Rademacher/覆盖复杂度分析学习误差。

Result: 推导出有限视界成功概率的下界，可分解为搜索项与学习项；核心分离结果表明：当cut消除导致证明规模从O(λ^D)指数膨胀至Ω(Λ^D)（λ≪Λ）时，cut-aware层次学习器所需样本复杂度比flat学习器低指数级。

Conclusion: 为近期基于代理（agentic）定理证明器中广泛采用的子目标分解策略提供了坚实的理论依据，表明其不仅实用，而且在统计效率上具有本质优势。

Abstract: We develop a theoretical analysis of LLM-guided formal theorem proving in interactive proof assistants (e.g., Lean) by modeling tactic proposal as a stochastic policy in a finite-horizon deterministic MDP. To capture modern representation learning, we treat the state and action spaces as general compact metric spaces and assume Lipschitz policies. To explain the gap between worst-case hardness and empirical success, we introduce problem distributions generated by a reference policy $q$, including a latent-variable model in which proofs exhibit reusable cut/lemma/sketch structure represented by a proof DAG. Under a top-$k$ search protocol and Tsybakov-type margin conditions, we derive lower bounds on finite-horizon success probability that decompose into search and learning terms, with learning controlled by sequential Rademacher/covering complexity. Our main separation result shows that when cut elimination expands a DAG of depth $D$ into a cut-free tree of size $Ω(Λ^D)$ while the cut-aware hierarchical process has size $O(λ^D)$ with $λ\llΛ$, a flat (cut-free) learner provably requires exponentially more data than a cut-aware hierarchical learner. This provides a principled justification for subgoal decomposition in recent agentic theorem provers.

</details>


### [237] [Prioritize the Process, Not Just the Outcome: Rewarding Latent Thought Trajectories Improves Reasoning in Looped Language Models](https://arxiv.org/abs/2602.10520)
*Williams Jonathan,Tureci Esin*

Main category: cs.LG

TL;DR: 本文提出RLTT框架，通过在潜推理轨迹上分配奖励来改进LoopLM的强化学习效果，显著提升了数学推理能力，并具有跨领域泛化性。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习方法（如GRPO）仅对最终潜状态赋值奖励，与LoopLM多步潜推理机制不匹配，导致性能提升失败。

Method: 提出RLTT（Reward Latent Thought Trajectories）框架，实现对完整潜推理轨迹的密集、轨迹级信用分配，无需外部验证器，可直接替代GRPO。

Result: 在Ouro-2.6B-Thinking模型上，RLTT在MATH-500、AIME24和BeyondAIME数学基准上分别提升准确率+14.4%、+16.6%和+10.0%；且在非数学推理任务上也表现出良好迁移能力。

Conclusion: 轨迹级信用分配是提升LoopLM强化学习效果的关键，RLTT为潜推理模型的训练提供了更匹配其内部计算结构的有效强化学习范式。

Abstract: Looped Language Models (LoopLMs) perform multi-step latent reasoning prior to token generation and outperform conventional LLMs on reasoning benchmarks at smaller parameter budgets. However, attempts to further improve LoopLM reasoning with reinforcement learning have failed - standard objectives such as Group Relative Policy Optimization (GRPO) only assign credit to the final latent state, creating a fundamental mismatch with the model's internal computation. To resolve this, we introduce RLTT (Reward Latent Thought Trajectories), a reinforcement learning framework which distributes reward across the full latent reasoning trajectory. RLTT provides dense, trajectory-level credit assignment without relying on external verifiers and can directly replace GRPO with negligible overhead. Across extensive experiments with Ouro-2.6B-Thinking under identical training and inference conditions, RLTT yields substantial improvements over GRPO on challenging mathematical reasoning benchmarks, improving accuracy by +14.4% on MATH-500, +16.6% on AIME24, and +10.0% on BeyondAIME. Despite being trained exclusively on mathematics, RLTT also transfers effectively to non-mathematical reasoning benchmarks, demonstrating the effectiveness of trajectory-level credit assignment for reinforcement learning in LoopLMs.

</details>


### [238] [A Swap-Adversarial Framework for Improving Domain Generalization in Electroencephalography-Based Parkinson's Disease Prediction](https://arxiv.org/abs/2602.10528)
*Seongwon Jin,Hanseul Choi,Sunggu Yang,Sungho Park,Jibum Kim*

Main category: cs.LG

TL;DR: 本文提出了一种新的ECoG数据集和Swap-Adversarial Framework (SAF)，用于帕金森病早期预测，解决了跨被试差异大、小样本高维等问题，并在跨被试、跨会话、跨数据集设置下均取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有ECoG研究受限于人类实验的伦理约束和缺乏公开基准数据集，难以进行可重复比较；同时ECoG数据存在高跨被试变异性和高维小样本（HDLSS）问题。

Method: 构建首个基于6-OHDA诱导大鼠模型的长期ECoG公开基准数据集；提出Swap-Adversarial Framework（SAF），包含鲁棒预处理、跨被试平衡通道交换（ISBCS）增强和域对抗训练三部分，以抑制被试特异性偏差并提升泛化能力。

Result: SAF在跨被试、跨会话、跨数据集实验中持续优于所有基线方法，尤其在高变异环境下提升显著；且在公共EEG基准间迁移时表现优异，验证了其对EEG数据的强泛化能力。

Conclusion: 本工作不仅提供了首个可复现的ECoG帕金森病预测基准数据集与开源代码，还提出了有效缓解HDLSS与跨被试差异的新框架SAF，推动ECoG/EEG联合建模与临床转化。

Abstract: Electroencephalography (ECoG) offers a promising alternative to conventional electrocorticography (EEG) for the early prediction of Parkinson's disease (PD), providing higher spatial resolution and a broader frequency range. However, reproducible comparisons has been limited by ethical constraints in human studies and the lack of open benchmark datasets. To address this gap, we introduce a new dataset, the first reproducible benchmark for PD prediction. It is constructed from long-term ECoG recordings of 6-hydroxydopamine (6-OHDA)-induced rat models and annotated with neural responses measured before and after electrical stimulation. In addition, we propose a Swap-Adversarial Framework (SAF) that mitigates high inter-subject variability and the high-dimensional low-sample-size (HDLSS) problem in ECoG data, while achieving robust domain generalization across ECoG and EEG-based Brain-Computer Interface (BCI) datasets. The framework integrates (1) robust preprocessing, (2) Inter-Subject Balanced Channel Swap (ISBCS) for cross-subject augmentation, and (3) domain-adversarial training to suppress subject-specific bias. ISBCS randomly swaps channels between subjects to reduce inter-subject variability, and domain-adversarial training jointly encourages the model to learn task-relevant shared features. We validated the effectiveness of the proposed method through extensive experiments under cross-subject, cross-session, and cross-dataset settings. Our method consistently outperformed all baselines across all settings, showing the most significant improvements in highly variable environments. Furthermore, the proposed method achieved superior cross-dataset performance between public EEG benchmarks, demonstrating strong generalization capability not only within ECoG but to EEG data. The new dataset and source code will be made publicly available upon publication.

</details>


### [239] [What Makes Value Learning Efficient in Residual Reinforcement Learning?](https://arxiv.org/abs/2602.10539)
*Guozheng Ma,Lu Li,Haoyu Wang,Zixuan Liu,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出DAWN方法，通过数据锚定预热和批评者归一化解决残差强化学习中价值学习的冷启动问题和结构尺度不匹配问题，显著提升学习效率。


<details>
  <summary>Details</summary>
Motivation: 残差强化学习中价值学习存在冷启动病理和结构性尺度不匹配两大瓶颈，影响在线策略精调的稳定性与效率。

Method: 提出DAWN方法，利用基策略转移作为价值锚点实现隐式预热，并通过批评者归一化恢复表征敏感性以区分价值差异。

Result: DAWN在多种基准测试、策略架构和观测模态下均展现出显著的效率提升。

Conclusion: 针对残差RL中价值学习的两个关键瓶颈，DAWN提供简单而有效的解决方案，为高效在线策略优化提供了新思路。

Abstract: Residual reinforcement learning (RL) enables stable online refinement of expressive pretrained policies by freezing the base and learning only bounded corrections. However, value learning in residual RL poses unique challenges that remain poorly understood. In this work, we identify two key bottlenecks: cold start pathology, where the critic lacks knowledge of the value landscape around the base policy, and structural scale mismatch, where the residual contribution is dwarfed by the base action. Through systematic investigation, we uncover the mechanisms underlying these bottlenecks, revealing that simple yet principled solutions suffice: base-policy transitions serve as an essential value anchor for implicit warmup, and critic normalization effectively restores representation sensitivity for discerning value differences. Based on these insights, we propose DAWN (Data-Anchored Warmup and Normalization), a minimal approach targeting efficient value learning in residual RL. By addressing these bottlenecks, DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities.

</details>


### [240] [Bridging the Compression-Precision Paradox: A Hybrid Architecture for Clinical EEG Report Generation with Guaranteed Measurement Accuracy](https://arxiv.org/abs/2602.10544)
*Wuyang Zhang,Zhen Luo,Chuqiao Gu,Jianming Ma,Yebo Cao,Wangming Yuan,Yinzhi Jin*

Main category: cs.LG

TL;DR: 本文提出了一种混合架构，通过分离测量提取与文本生成，在自动化EEG监测中实现了临床级精度的癫痫检测与报告，解决了大语言模型在处理长时程EEG信号时因过度压缩导致的时间精度丢失和测量幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 临床EEG记录远超大语言模型上下文窗口，需极高比例压缩（400:1以上），导致关键时间精度丢失（如0.5Hz误差即可影响癫痫综合征诊断），且LLM缺乏原生时序理解能力，易产生临床错误的测量幻觉。

Method: 采用混合架构：1）先通过信号处理精确提取临床测量值，再压缩；2）构建跨模态桥梁实现EEG到语言的翻译；3）对冻结槽位进行参数高效微调与约束解码；4）多速率采样兼顾长程上下文与事件级精度。

Result: 在TUH和CHB-MIT数据集上，误报减少60%，检测速度提升50%，测量精度达亚临床水平，首次在自动EEG报告中保证临床测量准确性。

Conclusion: 该系统突破了LLM直接处理原始EEG信号的固有局限，通过解耦测量与生成、引入信号先验与结构化约束，为可信AI辅助神经电生理诊断提供了新范式。

Abstract: Automated EEG monitoring requires clinician-level precision for seizure detection and reporting. Clinical EEG recordings exceed LLM context windows, requiring extreme compression (400:1+ ratios) that destroys fine-grained temporal precision. A 0.5 Hz error distinguishes absence epilepsy from Lennox-Gastaut syndrome. LLMs lack inherent time-series comprehension and rely on statistical associations from compressed representations. This dual limitation causes systems to hallucinate clinically incorrect measurement values.
  We separate measurement extraction from text generation. Our hybrid architecture computes exact clinical values via signal processing before compression, employs a cross-modal bridge for EEG-to-language translation, and uses parameter-efficient fine-tuning with constrained decoding around frozen slots. Multirate sampling maintains long-range context while preserving event-level precision. Evaluation on TUH and CHB-MIT datasets achieves 60% fewer false alarms, 50% faster detection, and sub-clinical measurement precision. This is the first system guaranteeing clinical measurement accuracy in automated EEG reports.

</details>


### [241] [$μ$pscaling small models: Principled warm starts and hyperparameter transfer](https://arxiv.org/abs/2602.10545)
*Yuxin Ma,Nan Chen,Mateo Díaz,Soufiane Hayou,Dmitriy Kunisky,Soledad Villar*

Main category: cs.LG

TL;DR: 本文提出了一种通用的模型宽度上扩方法，并扩展了μTransfer理论以实现高效超参数迁移，从而在模型上扩场景下避免昂贵的超参数调优。


<details>
  <summary>Details</summary>
Motivation: 现有模型上扩方法对目标尺寸的超参数敏感，直接调优成本过高；而基于小模型调优再外推的传统做法在上扩场景下的有效性尚不明确。

Method: 1）基于μP和任意维架构思想，提出一种适用于多种架构和优化器的通用宽度上扩方法，并提供理论保证（等价性与无限宽极限分析）；2）将μTransfer理论扩展为适用于该上扩方法的超参数迁移技术。

Result: 所提上扩方法保证了模型与其加宽版本的等价性；扩展的超参数迁移技术在真实数据集和架构上被实证有效。

Conclusion: 本文为模型上扩提供了理论坚实、实践有效的宽度扩展与超参数迁移框架，解决了上扩中超参数调优成本高且外推不可靠的问题。

Abstract: Modern large-scale neural networks are often trained and released in multiple sizes to accommodate diverse inference budgets. To improve efficiency, recent work has explored model upscaling: initializing larger models from trained smaller ones in order to transfer knowledge and accelerate convergence. However, this method can be sensitive to hyperparameters that need to be tuned at the target upscaled model size, which is prohibitively costly to do directly. It remains unclear whether the most common workaround -- tuning on smaller models and extrapolating via hyperparameter scaling laws -- is still sound when using upscaling. We address this with principled approaches to upscaling with respect to model widths and efficiently tuning hyperparameters in this setting. First, motivated by $μ$P and any-dimensional architectures, we introduce a general upscaling method applicable to a broad range of architectures and optimizers, backed by theory guaranteeing that models are equivalent to their widened versions and allowing for rigorous analysis of infinite-width limits. Second, we extend the theory of $μ$Transfer to a hyperparameter transfer technique for models upscaled using our method and empirically demonstrate that this method is effective on realistic datasets and architectures.

</details>


### [242] [Contrastive Learning for Multi Label ECG Classification with Jaccard Score Based Sigmoid Loss](https://arxiv.org/abs/2602.10553)
*Junichiro Takahashi,Masataka Sato,Satoshi Kodeta,Norihiko Takeda*

Main category: cs.LG

TL;DR: 本文提出了一种基于SigLIP的改进ECG编码器，通过引入适配多标签特性的损失函数、融入医学知识、增大嵌入维度和随机裁剪等策略，提升了真实医院数据下的多标签ECG分类性能，并分析了各ECG诊断项的预测难易程度。


<details>
  <summary>Details</summary>
Motivation: 现有多模态医疗大模型（如MedGemini、MedGemma）在ECG任务上表现有限或不支持ECG输入；ECG解读本身难度高且依赖经验；超声心动图虽信息丰富但可及性低。因此亟需构建鲁棒的ECG编码器以支持多模态医疗AI。

Method: 采用SigLIP架构（基于CLIP、使用Sigmoid损失支持多标签），设计适配ECG多标签特性的改进损失函数；在语言模型中注入医学知识；增大嵌入维度；应用随机裁剪缓解数据漂移；并在真实医院ECG数据上进行多标签预训练与评估。

Result: 所提方法显著提升多标签ECG分类性能；消融实验验证了医学知识注入和改进损失函数的有效性；增大嵌入维度和随机裁剪进一步增强鲁棒性；逐标签分析揭示了不同ECG发现的预测难度差异。

Conclusion: 本研究为利用ECG数据构建多模态医疗AI模型提供了可扩展的基础框架，强调了针对ECG多标签特性定制建模的重要性。

Abstract: Recent advances in large language models (LLMs) have enabled the development of multimodal medical AI. While models such as MedGemini achieve high accuracy on VQA tasks like USMLE MM, their performance on ECG based tasks remains limited, and some models, such as MedGemma, do not support ECG data at all. Interpreting ECGs is inherently challenging, and diagnostic accuracy can vary depending on the interpreter's experience. Although echocardiography provides rich diagnostic information, it requires specialized equipment and personnel, limiting its availability. In this study, we focus on constructing a robust ECG encoder for multimodal pretraining using real world hospital data. We employ SigLIP, a CLIP based model with a sigmoid based loss function enabling multi label prediction, and introduce a modified loss function tailored to the multi label nature of ECG data. Experiments demonstrate that incorporating medical knowledge in the language model and applying the modified loss significantly improve multi label ECG classification. To further enhance performance, we increase the embedding dimensionality and apply random cropping to mitigate data drift. Finally, per label analysis reveals which ECG findings are easier or harder to predict. Our study provides a foundational framework for developing medical models that utilize ECG data.

</details>


### [243] [Online Min-Max Optimization: From Individual Regrets to Cumulative Saddle Points](https://arxiv.org/abs/2602.10565)
*Abhijeet Vyas,Brian Bullins*

Main category: cs.LG

TL;DR: 本文提出了在线最小-最大优化的新框架，定义了静态对偶间隙（SDual-Gap_T）和动态鞍点遗憾（DSP-Reg_T）两种新性能度量，并在强凸-强凹、极小极大指数凹性及双侧Polyak-Łojasiewicz条件下给出算法与理论保证。


<details>
  <summary>Details</summary>
Motivation: 传统静态纳什均衡遗憾（SNE-Reg_T）与个体遗憾不兼容，尤其在强凸-强凹情形下；需构建更合理、可分解且与OCO框架兼容的性能度量。

Method: 基于在线凸优化（OCO）框架，通过问题约简设计算法；引入静态对偶间隙（SDual-Gap_T）和动态鞍点遗憾（DSP-Reg_T）作为新评估指标；分析多种结构化函数类（如min-max EC、双侧PL条件）下的理论界。

Result: 在强凸-强凹和min-max EC条件下，获得了SDual-Gap_T和DSP-Reg_T的上界；构造了一类满足min-max EC的函数，涵盖双人投资组合选择问题；在双侧PL条件下，实现了与个体遗憾兼容的动态遗憾上界。

Conclusion: 本文建立了更自然、更具分解性的在线min-max优化性能度量体系，拓展了OCO工具至非凸-非凹场景，并为实际博弈与学习问题（如金融决策）提供了理论基础与算法支持。

Abstract: We propose and study an online version of min-max optimization based on cumulative saddle points under a variety of performance measures beyond convex-concave settings. After first observing the incompatibility of (static) Nash equilibrium (SNE-Reg$_T$) with individual regrets even for strongly convex-strongly concave functions, we propose an alternate \emph{static} duality gap (SDual-Gap$_T$) inspired by the online convex optimization (OCO) framework. We provide algorithms that, using a reduction to classic OCO problems, achieve bounds for SDual-Gap$_T$~and a novel \emph{dynamic} saddle point regret (DSP-Reg$_T$), which we suggest naturally represents a min-max version of the dynamic regret in OCO. We derive our bounds for SDual-Gap$_T$~and DSP-Reg$_T$~under strong convexity-strong concavity and a min-max notion of exponential concavity (min-max EC), and in addition we establish a class of functions satisfying min-max EC~that captures a two-player variant of the classic portfolio selection problem. Finally, for a dynamic notion of regret compatible with individual regrets, we derive bounds under a two-sided Polyak-Łojasiewicz (PL) condition.

</details>


### [244] [SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining](https://arxiv.org/abs/2602.10718)
*Yifan Zhang,Zunhai Su,Shuhao Hu,Rui Yang,Wei Wu,Yulei Qian,Yuchen Xie,Xunliang Cai*

Main category: cs.LG

TL;DR: 本文提出SnapMLA，一种面向FP8的DeepSeek MLA解码优化框架，通过RoPE感知的逐token KV量化、量化PV计算流水线重构和端到端数据流优化，显著提升长上下文推理吞吐（最高1.91倍），同时保持精度。


<details>
  <summary>Details</summary>
Motivation: FP8在MLA解码中面临数值异质性（如位置编码解耦）、FP8 PV GEMM量化尺度不匹配及系统级支持不足等挑战。

Method: 提出三项硬件感知的算法-内核协同优化技术：(i) RoPE感知的逐token KV量化，保留RoPE高精度；(ii) 量化PV计算流水线重构，解决MLA共享KV结构导致的尺度错位；(iii) 端到端数据流优化，设计专用内核提升数据读写效率。

Result: 在主流MLA大模型上实验表明，SnapMLA最高实现1.91倍吞吐提升，在数学推理与代码生成等长上下文任务中无明显性能下降。

Conclusion: SnapMLA有效克服了FP8在MLA解码中的关键瓶颈，为长上下文高效推理提供了实用且鲁棒的低精度解决方案。

Abstract: While FP8 attention has shown substantial promise in innovations like FlashAttention-3, its integration into the decoding phase of the DeepSeek Multi-head Latent Attention (MLA) architecture presents notable challenges. These challenges include numerical heterogeneity arising from the decoupling of positional embeddings, misalignment of quantization scales in FP8 PV GEMM, and the need for optimized system-level support. In this paper, we introduce SnapMLA, an FP8 MLA decoding framework optimized to improve long-context efficiency through the following hardware-aware algorithm-kernel co-optimization techniques: (i) RoPE-Aware Per-Token KV Quantization, where the RoPE part is maintained in high precision, motivated by our comprehensive analysis of the heterogeneous quantization sensitivity inherent to the MLA KV cache. Furthermore, per-token granularity is employed to align with the autoregressive decoding process and maintain quantization accuracy. (ii) Quantized PV Computation Pipeline Reconstruction, which resolves the misalignment of quantization scale in FP8 PV computation stemming from the shared KV structure of the MLA KV cache. (iii) End-to-End Dataflow Optimization, where we establish an efficient data read-and-write workflow using specialized kernels, ensuring efficient data flow and performance gains. Extensive experiments on state-of-the-art MLA LLMs show that SnapMLA achieves up to a 1.91x improvement in throughput, with negligible risk of performance degradation in challenging long-context tasks, including mathematical reasoning and code generation benchmarks. Code is available at https://github.com/meituan-longcat/SGLang-FluentLLM.

</details>


### [245] [Gauss-Newton Unlearning for the LLM Era](https://arxiv.org/abs/2602.10568)
*Lev McKinney,Anvith Thudi,Juhan Bae,Tara Rezaei,Nicolas Papernot,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: 本文提出K-FADE方法，利用K-FAC近似Hessian矩阵，通过少量上坡Gauss-Newton步实现大语言模型高效遗忘（unlearning），在抑制遗忘集输出的同时，显著减少对保留集性能的损害，并支持后续训练中重复应用遗忘更新。


<details>
  <summary>Details</summary>
Motivation: 标准大语言模型训练可能导致部署中产生不可接受的输出；现有遗忘方法虽可降低此类输出概率，但常损害模型在其他数据分布（保留集）上的性能，亟需改善遗忘效果与保留性能之间的权衡。

Method: 提出K-FADE方法：利用遗忘集计算少量上坡Gauss-Newton步，结合K-FAC等参数化Hessian近似技术，将输出空间对保留集的约束转化为权重空间约束，从而最小化对保留集行为的影响。

Result: 在WMDP和ToFU基准测试中，K-FADE能有效抑制遗忘集输出，输出表现逼近完全剔除遗忘集后重训练的结果，且对保留集输出的扰动小于先前方法；遗忘更新可被重复应用以维持遗忘效果。

Conclusion: K-FADE是一种概念简洁、性能领先的LLM遗忘方法，通过权衡遗忘精度与保留稳定性，兼顾高效性、可维护性与实用性。

Abstract: Standard large language model training can create models that produce outputs their trainer deems unacceptable in deployment. The probability of these outputs can be reduced using methods such as LLM unlearning. However, unlearning a set of data (called the forget set) can degrade model performance on other distributions where the trainer wants to retain the model's behavior. To improve this trade-off, we demonstrate that using the forget set to compute only a few uphill Gauss-Newton steps provides a conceptually simple, state-of-the-art unlearning approach for LLMs. While Gauss-Newton steps adapt Newton's method to non-linear models, it is non-trivial to efficiently and accurately compute such steps for LLMs. Hence, our approach crucially relies on parametric Hessian approximations such as Kronecker-Factored Approximate Curvature (K-FAC). We call this combined approach K-FADE (K-FAC for Distribution Erasure). Our evaluation on the WMDP and ToFU benchmarks demonstrates that K-FADE suppresses outputs from the forget set and approximates, in output space, the results of retraining without the forget set. Critically, our method does this while altering the outputs on the retain set less than previous methods. This is because K-FADE transforms a constraint on the model's outputs across the entire retain set into a constraint on the model's weights, allowing the algorithm to minimally change the model's behavior on the retain set at each step. Moreover, the unlearning updates computed by K-FADE can be reapplied later if the model undergoes further training, allowing unlearning to be cheaply maintained.

</details>


### [246] [LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization](https://arxiv.org/abs/2602.10576)
*Boxiao Wang,Kai Li,Tianyi Liu,Chen Li,Junzhe Wang,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: 本文提出PiT-PO框架，通过物理信息引导与词元级正则化的强化学习方法，使大语言模型在符号回归中自适应生成科学一致且结构简洁的数学方程。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的符号回归方法将模型视为静态生成器，缺乏根据搜索反馈更新内部表示的能力，导致生成的方程常存在物理不一致或数学冗余问题。

Method: 提出PiT-PO（Physics-informed Token-regularized Policy Optimization），采用双约束机制：一是分层物理有效性约束，二是词元级细粒度惩罚以抑制冗余结构，并通过强化学习实现LLM的自适应演化。

Result: 在标准基准上达到SOTA性能；成功发现复杂流体动力学问题中的新湍流模型；小规模模型经PiT-PO优化后可超越闭源大模型。

Conclusion: PiT-PO实现了LLM在符号回归中的动态适配与科学对齐，提升了方程生成的物理合理性与简洁性，并推动了高性能科学发现的普惠化。

Abstract: Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.

</details>


### [247] [Rotary Positional Embeddings as Phase Modulation: Theoretical Bounds on the RoPE Base for Long-Context Transformers](https://arxiv.org/abs/2602.10959)
*Feilong Liu*

Main category: cs.LG

TL;DR: 本文将RoPE重新解释为复振荡器银行的相位调制，利用经典信号处理理论分析其在长上下文中的行为，推导出保证位置一致性的RoPE底数下界（包括类奈奎斯特混叠界和DC分量稳定性界），并考虑深度叠加和浮点精度限制，提出一个精度与深度依赖的‘黄金区间’，并通过LLaMA、Mistral等模型验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: Rotary positional embeddings (RoPE) 在大语言模型中广泛应用，但其在长上下文下的行为缺乏系统刻画，尤其在位置编码一致性、深层叠加效应和数值精度限制方面存在理论空白。

Method: 将RoPE建模为复振荡器的相位调制，基于信号处理理论推导RoPE底数的理论下界（混叠界、DC稳定性界）和上界（浮点精度导致的相位更新失效界），并扩展至多层Transformer分析角度误差累积效应；最后通过主流模型实证验证。

Result: 得到了RoPE底数的精度与深度依赖的可行性区间（Goldilocks zone）；发现违反稳定性界会导致注意力坍缩和长程性能退化；超百万token时遭遇与架构无关的硬性精度瓶颈。

Conclusion: RoPE的长期有效性受严格理论约束，其底数选择需兼顾信号保真（下界）与数值可实现性（上界）；该分析为长上下文Transformer的设计与调优提供了可解释、可预测的理论指导。

Abstract: Rotary positional embeddings (RoPE) are widely used in large language models to encode token positions through multiplicative rotations, yet their behavior at long context lengths remains poorly characterized. In this work, we reinterpret RoPE as phase modulation applied to a bank of complex oscillators, enabling analysis through classical signal processing theory.
  Under this formulation, we derive principled lower bounds on the RoPE base parameter that are necessary to preserve positional coherence over a target context length. These include a fundamental aliasing bound, analogous to a Nyquist limit, and a DC-component stability bound that constrains phase drift in low-frequency positional modes. We further extend this analysis to deep transformers, showing that repeated rotary modulation across layers compounds angular misalignment, tightening the base requirement as depth increases.
  Complementing these results, we derive a precision-dependent upper bound on the RoPE base arising from finite floating-point resolution. Beyond this limit, incremental phase updates become numerically indistinguishable, leading to positional erasure even in the absence of aliasing. Together, the lower and upper bounds define a precision- and depth-dependent feasibility region a Goldilocks zone for long-context transformers.
  We validate the framework through a comprehensive case study of state-of-the-art models, including LLaMA, Mistral, and DeepSeek variants, showing that observed successes, failures, and community retrofits align closely with the predicted bounds. Notably, models that violate the stability bound exhibit attention collapse and long-range degradation, while attempts to scale beyond one million tokens encounter a hard precision wall independent of architecture or training.

</details>


### [248] [When Gradient Clipping Becomes a Control Mechanism for Differential Privacy in Deep Learning](https://arxiv.org/abs/2602.10584)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于模型权重谱分析的轻量级、自适应梯度裁剪策略，用于差分隐私随机优化，避免依赖逐样本梯度统计，不增加额外隐私开销。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私训练中的自适应梯度裁剪方法依赖逐样本梯度范数统计，计算开销大且对数据集和模型结构敏感；而固定裁剪阈值难以兼顾优化偏差与噪声干扰。

Method: 提出一种控制驱动的裁剪策略：在周期性探针步骤中，对指定权重矩阵进行谱分解，估计表征训练稳定性的重尾谱指标；该指标经时间平滑后输入有界反馈控制器，在对数域中乘性更新裁剪阈值。

Result: 该方法仅利用隐私训练过程中已生成的模型参数，裁剪阈值更新为后处理操作，不增加额外隐私损失；实验表明其在保持隐私预算不变前提下提升模型精度与训练稳定性。

Conclusion: 基于权重谱的控制式自适应裁剪是一种高效、通用且隐私安全的梯度裁剪新范式，克服了传统方法的计算与泛化瓶颈。

Abstract: Privacy-preserving training on sensitive data commonly relies on differentially private stochastic optimization with gradient clipping and Gaussian noise. The clipping threshold is a critical control knob: if set too small, systematic over-clipping induces optimization bias; if too large, injected noise dominates updates and degrades accuracy. Existing adaptive clipping methods often depend on per-example gradient norm statistics, adding computational overhead and introducing sensitivity to datasets and architectures. We propose a control-driven clipping strategy that adapts the threshold using a lightweight, weight-only spectral diagnostic computed from model parameters. At periodic probe steps, the method analyzes a designated weight matrix via spectral decomposition and estimates a heavy-tailed spectral indicator associated with training stability. This indicator is smoothed over time and fed into a bounded feedback controller that updates the clipping threshold multiplicatively in the log domain. Because the controller uses only parameters produced during privacy-preserving training, the resulting threshold updates are post-processing and do not increase privacy loss beyond that of the underlying DP optimizer under standard composition accounting.

</details>


### [249] [Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity](https://arxiv.org/abs/2602.10585)
*Guangzhi Xiong,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: 本文提出Neural Additive Experts (NAEs)框架，在保持可解释性的同时提升预测精度，通过专家混合模型与动态门控机制放松严格加性约束，并引入正则化技术平衡特征交互与清晰归因。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中可解释性与准确性之间的权衡问题：传统GAMs虽可解释但受限于严格加性结构，引入交互可提升精度却损害可解释性。

Method: 提出NAEs框架，采用每特征多个专用网络的专家混合结构，结合动态跨特征门控机制；设计针对性正则化以降低专家间方差，实现从纯加性到含交互的平滑过渡。

Result: 理论分析和合成数据实验验证模型灵活性；在多个真实数据集上的广泛评估表明，NAEs在预测精度和特征级透明解释之间取得最优平衡。

Conclusion: NAEs成功兼顾高预测性能与清晰的特征归因能力，为可解释机器学习提供了新范式。

Abstract: The trade-off between interpretability and accuracy remains a core challenge in machine learning. Standard Generalized Additive Models (GAMs) offer clear feature attributions but are often constrained by their strictly additive nature, which can limit predictive performance. Introducing feature interactions can boost accuracy yet may obscure individual feature contributions. To address these issues, we propose Neural Additive Experts (NAEs), a novel framework that seamlessly balances interpretability and accuracy. NAEs employ a mixture of experts framework, learning multiple specialized networks per feature, while a dynamic gating mechanism integrates information across features, thereby relaxing rigid additive constraints. Furthermore, we propose targeted regularization techniques to mitigate variance among expert predictions, facilitating a smooth transition from an exclusively additive model to one that captures intricate feature interactions while maintaining clarity in feature attributions. Our theoretical analysis and experiments on synthetic data illustrate the model's flexibility, and extensive evaluations on real-world datasets confirm that NAEs achieve an optimal balance between predictive accuracy and transparent, feature-level explanations. The code is available at https://github.com/Teddy-XiongGZ/NAE.

</details>


### [250] [ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression](https://arxiv.org/abs/2602.11008)
*Ammar Ali,Baher Mohammad,Denis Makhov,Dmitriy Shopkhoev,Magauiya Zhussip,Stamatios Lefkimmiatis*

Main category: cs.LG

TL;DR: ROCKET是一种无需训练的模型压缩方法，通过多选择背包问题优化各层压缩比例，并结合单步稀疏矩阵分解技术，在不微调的情况下实现高性能压缩。


<details>
  <summary>Details</summary>
Motivation: 现有模型压缩方法（如因子分解、结构化剪枝和动态压缩）通常需要训练或迭代优化，效率低且效果受限，亟需一种高效、免训练的压缩方案。

Method: ROCKET包含两个核心创新：1）将层间压缩分配建模为多选择背包问题，以全局压缩预算下最小化重建误差；2）提出单步稀疏矩阵分解，基于激活-权重敏感性进行系数稀疏化，并用最小二乘闭式求解字典更新，完全规避迭代优化、稀疏编码与反向传播。

Result: ROCKET在20%-50%压缩率下持续超越各类基线方法；30%压缩率下无需微调即保留超90%原始性能；轻量微调（如Qwen3-14B压缩至8B并用30M token微调）后性能接近原生Qwen3-8B。

Conclusion: ROCKET是一种高效、免训练、可扩展的模型压缩方法，在保持高精度的同时显著降低计算与存储开销，为大模型部署提供了实用新路径。

Abstract: We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\% compression rates. Notably, it retains over 90\% of the original model's performance at 30\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.

</details>


### [251] [TRACE: Theoretical Risk Attribution under Covariate-shift Effects](https://arxiv.org/abs/2602.10588)
*Hosein Anjidani,S. Yahya S. R. Tehrani,Mohammad Mahdi Mojahedian,Mohammad Hossein Yassaee*

Main category: cs.LG

TL;DR: 本文提出TRACE框架，用于在协变量偏移下分解和诊断源域风险变化ΔR，通过四个可解释因子（两个泛化差距、模型变化惩罚、协变量偏移惩罚）提供可计算的性能退化归因，并设计部署门控分数实现安全高效模型替换。


<details>
  <summary>Details</summary>
Motivation: 当源训练模型被偏移数据上训练的新模型替代时，其在源域上的性能变化不可预测，亟需可解释、可计算的风险变化归因方法。

Method: 提出TRACE框架，将|ΔR|分解为四个可解释项；协变量偏移惩罚通过高分位输入梯度敏感性与特征空间最优传输（或MMD）估计；模型变化惩罚用两模型在目标样本上的平均输出距离衡量；泛化差距基于留出数据估计；并导出与|ΔR|强相关的部署门控分数。

Result: 在理想线性回归、合成数据及视觉基准上验证了TRACE界的有效性与单调性；部署门控分数在AUROC/AUPRC上表现优异，支持标签高效、安全的模型替换。

Conclusion: TRACE为协变量偏移下的模型替换提供了理论严谨、可计算、可解释的风险归因与部署决策支持框架。

Abstract: When a source-trained model $Q$ is replaced by a model $\tilde{Q}$ trained on shifted data, its performance on the source domain can change unpredictably. To address this, we study the two-model risk change, $ΔR := R_P(\tilde{Q}) - R_P(Q)$, under covariate shift. We introduce TRACE (Theoretical Risk Attribution under Covariate-shift Effects), a framework that decomposes $|ΔR|$ into an interpretable upper bound. This decomposition disentangles the risk change into four actionable factors: two generalization gaps, a model change penalty, and a covariate shift penalty, transforming the bound into a powerful diagnostic tool for understanding why performance has changed. To make TRACE a fully computable diagnostic, we instantiate each term. The covariate shift penalty is estimated via a model sensitivity factor (from high-quantile input gradients) and a data-shift measure; we use feature-space Optimal Transport (OT) by default and provide a robust alternative using Maximum Mean Discrepancy (MMD). The model change penalty is controlled by the average output distance between the two models on the target sample. Generalization gaps are estimated on held-out data. We validate our framework in an idealized linear regression setting, showing the TRACE bound correctly captures the scaling of the true risk difference with the magnitude of the shift. Across synthetic and vision benchmarks, TRACE diagnostics are valid and maintain a strong monotonic relationship with the true performance degradation. Crucially, we derive a deployment gate score that correlates strongly with $|ΔR|$ and achieves high AUROC/AUPRC for gating decisions, enabling safe, label-efficient model replacement.

</details>


### [252] [Learning Page Order in Shuffled WOO Releases](https://arxiv.org/abs/2602.11040)
*Efe Kahraman,Giulio Tosato*

Main category: cs.LG

TL;DR: 本文研究了5461份打乱顺序的荷兰信息公开文件（WOO）的页面重排序问题，使用页面嵌入方法比较了五种模型，发现最佳方法在15页文档上Kendall's tau达0.72；同时揭示了seq2seq模型在长文档上严重失效（tau从0.918骤降至0.014）及课程学习策略失效的原因，并通过模型专业化显著提升长文档排序性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构PDF文档（含邮件、法律文本、表格等）中语义排序信号不可靠导致的页面顺序混乱问题。

Method: 采用页面嵌入表示，对比指针网络、seq2seq Transformer和专用成对排序模型等五种方法，并开展消融实验与注意力模式分析。

Result: 最佳方法在2–5页文档上Kendall's tau达0.95，15页文档为0.72；seq2seq在21–25页文档上tau仅0.014；课程学习比直接训练差39%；模型专业化使长文档tau提升0.21。

Conclusion: 页面重排序需根据文档长度采用不同策略；seq2seq失效源于多因素交互（如位置编码），不能仅靠课程学习缓解；模型专业化是提升长文档性能的有效途径。

Abstract: We investigate document page ordering on 5,461 shuffled WOO documents (Dutch freedom of information releases) using page embeddings. These documents are heterogeneous collections such as emails, legal texts, and spreadsheets compiled into single PDFs, where semantic ordering signals are unreliable. We compare five methods, including pointer networks, seq2seq transformers, and specialized pairwise ranking models. The best performing approach successfully reorders documents up to 15 pages, with Kendall's tau ranging from 0.95 for short documents (2-5 pages) to 0.72 for 15 page documents. We observe two unexpected failures: seq2seq transformers fail to generalize on long documents (Kendall's tau drops from 0.918 on 2-5 pages to 0.014 on 21-25 pages), and curriculum learning underperforms direct training by 39% on long documents. Ablation studies suggest learned positional encodings are one contributing factor to seq2seq failure, though the degradation persists across all encoding variants, indicating multiple interacting causes. Attention pattern analysis reveals that short and long documents require fundamentally different ordering strategies, explaining why curriculum learning fails. Model specialization achieves substantial improvements on longer documents (+0.21 tau).

</details>


### [253] [Roughness-Informed Federated Learning](https://arxiv.org/abs/2602.10595)
*Mohammad Partohaghighi,Roummel Marcia,Bruce J. West,YangQuan Chen*

Main category: cs.LG

TL;DR: 本文提出RI-FedAvg算法，通过引入基于粗糙度指数（RI）的正则项缓解非独立同分布（non-IID）场景下的客户端漂移问题，提升联邦学习的收敛性与鲁棒性，并在多个数据集上验证其优越性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布（non-IID）数据下易受客户端漂移影响，导致收敛困难，亟需更鲁棒的优化方法。

Method: 提出RI-FedAvg算法，在本地目标函数中加入基于粗糙度指数（RI）的自适应正则项，量化并抑制局部损失曲面波动；提供非凸目标下的严格收敛性分析。

Result: 在MNIST、CIFAR-10和CIFAR-100上的实验表明，RI-FedAvg在non-IID设置下相比FedAvg、FedProx、FedDyn、SCAFFOLD和DP-FedAvg等方法，准确率更高、收敛更快。

Conclusion: RI-FedAvg有效缓解客户端漂移，显著提升联邦学习在异构现实环境中的鲁棒性与训练效率。

Abstract: Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet faces challenges in non-independent and identically distributed (non-IID) settings due to client drift, which impairs convergence. We propose RI-FedAvg, a novel FL algorithm that mitigates client drift by incorporating a Roughness Index (RI)-based regularization term into the local objective, adaptively penalizing updates based on the fluctuations of local loss landscapes. This paper introduces RI-FedAvg, leveraging the RI to quantify the roughness of high-dimensional loss functions, ensuring robust optimization in heterogeneous settings. We provide a rigorous convergence analysis for non-convex objectives, establishing that RI-FedAvg converges to a stationary point under standard assumptions. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100 demonstrate that RI-FedAvg outperforms state-of-the-art baselines, including FedAvg, FedProx, FedDyn, SCAFFOLD, and DP-FedAvg, achieving higher accuracy and faster convergence in non-IID scenarios. Our results highlight RI-FedAvg's potential to enhance the robustness and efficiency of federated learning in practical, heterogeneous environments.

</details>


### [254] [Learning Mixture Density via Natural Gradient Expectation Maximization](https://arxiv.org/abs/2602.10602)
*Yutao Chen,Jasmine Bayrooti,Steven Morad*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息几何的自然梯度EM（nGEM）方法，显著提升混合密度网络（MDN）的训练效率与稳定性，实现最高10倍加速且几乎零计算开销。


<details>
  <summary>Details</summary>
Motivation: 标准混合密度网络（MDN）采用负对数似然（NLL）进行最大似然训练，存在收敛慢和模式坍塌问题。

Method: 将MDN视为深度隐变量模型，通过EM框架分析其信息几何结构，揭示其与自然梯度下降的理论联系，并据此推导出自然梯度EM（nGEM）优化目标。

Result: nGEM在实验中实现了最高10倍的收敛速度提升，计算开销几乎为零，并在高维数据上表现稳健，而传统NLL在此类场景下失效。

Conclusion: 利用信息几何指导MDN优化是有效且可扩展的路径，nGEM为多模态条件密度建模提供了更优的训练范式。

Abstract: Mixture density networks are neural networks that produce Gaussian mixtures to represent continuous multimodal conditional densities. Standard training procedures involve maximum likelihood estimation using the negative log-likelihood (NLL) objective, which suffers from slow convergence and mode collapse. In this work, we improve the optimization of mixture density networks by integrating their information geometry. Specifically, we interpret mixture density networks as deep latent-variable models and analyze them through an expectation maximization framework, which reveals surprising theoretical connections to natural gradient descent. We then exploit such connections to derive the natural gradient expectation maximization (nGEM) objective. We show that empirically nGEM achieves up to 10$\times$ faster convergence while adding almost zerocomputational overhead, and scales well to high-dimensional data where NLL otherwise fails.

</details>


### [255] [dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning](https://arxiv.org/abs/2602.10603)
*Arnav Shah,Junzhe Li,Parsa Idehpour,Adibvafa Fallahpour,Brandon Wang,Sukjun Hwang,Bo Wang,Patrick D. Hsu,Hani Goodarzi,Albert Gu*

Main category: cs.LG

TL;DR: dnaHNet是一种无需分词器的自回归基因组模型，通过可微动态分块机制将原始核苷酸自适应压缩为潜在token，在保持生物学意义的同时显著提升计算效率和预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型在输入表示上存在根本权衡：固定词汇表分词器会破坏生物学上有意义的基序（如密码子、调控元件），而核苷酸级模型虽保持生物学连贯性，但长上下文计算成本过高。

Method: 提出dnaHNet，一种无需分词器的端到端自回归模型；采用可微动态分块机制，将原始核苷酸自适应压缩为潜在token；在原核基因组上预训练。

Result: 相比StripedHyena2等先进架构，dnaHNet在扩展性和效率上更优；实现二次方级FLOP减少，推理速度提升超3倍；零样本任务中在蛋白变体适应度和基因必需性预测上表现更优；无需监督即可自动发现层级生物学结构。

Conclusion: dnaHNet是一种可扩展、可解释的新一代基因组建模框架。

Abstract: Genomic foundation models have the potential to decode DNA syntax, yet face a fundamental tradeoff in their input representation. Standard fixed-vocabulary tokenizers fragment biologically meaningful motifs such as codons and regulatory elements, while nucleotide-level models preserve biological coherence but incur prohibitive computational costs for long contexts. We introduce dnaHNet, a state-of-the-art tokenizer-free autoregressive model that segments and models genomic sequences end-to-end. Using a differentiable dynamic chunking mechanism, dnaHNet compresses raw nucleotides into latent tokens adaptively, balancing compression with predictive accuracy. Pretrained on prokaryotic genomes, dnaHNet outperforms leading architectures including StripedHyena2 in scaling and efficiency. This recursive chunking yields quadratic FLOP reductions, enabling $>3 \times$ inference speedup over Transformers. On zero-shot tasks, dnaHNet achieves superior performance in predicting protein variant fitness and gene essentiality, while automatically discovering hierarchical biological structures without supervision. These results establish dnaHNet as a scalable, interpretable framework for next-generation genomic modeling.

</details>


### [256] [Just on Time: Token-Level Early Stopping for Diffusion Language Models](https://arxiv.org/abs/2602.11133)
*Zahar Kohut,Severyn Shykula,Dmytro Khamula,Mykola Vysotskyi,Taras Rumezhak,Volodymyr Karpiv*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练、基于token级别的早期停止方法，用于加速扩散语言模型的文本生成过程，通过轻量级信号动态判断各token是否收敛，从而减少总扩散步数，在保持生成质量的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在文本生成中存在计算效率低的问题，因为许多token在最终去噪步骤之前就已稳定，但模型仍继续迭代。

Method: 提出一种无需训练的token级早期停止策略，利用模型预测和局部上下文的轻量级信号，独立判断每个位置token的收敛性，并实现自适应的逐token冻结。

Result: 在数学推理、通用问答和科学理解等多个基准上实现了最先进的效率提升，同时保持了生成质量。

Conclusion: 该方法无需任务特定微调，即可有效降低扩散语言模型的计算开销，为高效文本生成提供了新思路。

Abstract: Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model's predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.

</details>


### [257] [Hierarchical Zero-Order Optimization for Deep Neural Networks](https://arxiv.org/abs/2602.10607)
*Sansheng Cao,Zhengyu Ma,Yonghong Tian*

Main category: cs.LG

TL;DR: 本文提出了一种分层零阶（HZO）优化方法，通过将网络深度维度分解，显著降低了零阶优化的查询复杂度，并在CIFAR-10和ImageNet上验证了其与反向传播相当的精度。


<details>
  <summary>Details</summary>
Motivation: 零阶优化虽具生物可解释性和处理不可导目标的能力，但其高计算复杂度限制了其在深度神经网络中的应用。

Method: 提出分层零阶（HZO）优化，采用分治策略分解网络深度维度，并理论证明其查询复杂度从$O(ML^2)$降至$O(ML \log L)$，同时进行误差分析确保数值稳定性。

Result: HZO在CIFAR-10和ImageNet数据集上达到与反向传播相竞争的准确率，且查询复杂度显著降低。

Conclusion: HZO为零阶优化在深度学习中的实际应用提供了高效、稳定的新范式，突破了传统逐层梯度传播的限制。

Abstract: Zeroth-order (ZO) optimization has long been favored for its biological plausibility and its capacity to handle non-differentiable objectives, yet its computational complexity has historically limited its application in deep neural networks. Challenging the conventional paradigm that gradients propagate layer-by-layer, we propose Hierarchical Zeroth-Order (HZO) optimization, a novel divide-and-conquer strategy that decomposes the depth dimension of the network. We prove that HZO reduces the query complexity from $O(ML^2)$ to $O(ML \log L)$ for a network of width $M$ and depth $L$, representing a significant leap over existing ZO methodologies. Furthermore, we provide a detailed error analysis showing that HZO maintains numerical stability by operating near the unitary limit ($L_{lip} \approx 1$). Extensive evaluations on CIFAR-10 and ImageNet demonstrate that HZO achieves competitive accuracy compared to backpropagation.

</details>


### [258] [Weight Decay Improves Language Model Plasticity](https://arxiv.org/abs/2602.11137)
*Tessa Han,Sebastian Bordt,Hanlin Zhang,Sham Kakade*

Main category: cs.LG

TL;DR: 本文研究了预训练中权重衰减对大语言模型可塑性（即微调下游任务的能力）的影响，发现较大的权重衰减虽降低预训练性能，却提升微调效果，并揭示其机制在于促进线性可分表征、正则化注意力矩阵和减少过拟合。


<details>
  <summary>Details</summary>
Motivation: 现有LLM预训练研究多关注验证损失，忽视模型对下游任务的适应能力（即可塑性），本文旨在从可塑性角度重新审视预训练超参数（特别是权重衰减）的作用。

Method: 通过系统性实验，分析不同权重衰减值下预训练模型在下游任务微调后的性能变化，并探究其对表征、注意力机制和过拟合行为的内在影响。

Result: 更大的权重衰减提升了模型可塑性，使微调后性能增益更大；甚至出现预训练性能较差但微调后更优的反直觉现象；机制上促进线性可分表征、正则化注意力、缓解过拟合。

Conclusion: 超参数优化应超越交叉熵损失，纳入可塑性等下游适应指标；单一超参数（如权重衰减）具有多重且深远的行为塑造作用。

Abstract: The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.

</details>


### [259] [On the Role of Consistency Between Physics and Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2602.10611)
*Nicolás Becerra-Zuniga,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: 本文研究了数据与偏微分方程（PDE）不一致对物理信息神经网络（PINNs）精度的限制，提出了‘一致性壁垒’概念，并通过一维粘性Burgers方程验证：数据误差设定了PINN误差下限，高保真数据可消除该壁垒。


<details>
  <summary>Details</summary>
Motivation: PINNs在数据稀缺时依赖物理约束，但实际训练数据常因噪声、离散误差等与PDE不一致；这种不一致对PINN精度和收敛性的影响尚不明确。

Method: 提出‘一致性壁垒’概念，定义为数据保真度与PDE残差精确施加之间不匹配所导致的固有误差下限；在一维粘性Burgers方程（具解析解）上系统实验，对比不同精度数值数据与解析数据训练效果。

Result: PINN精度最终饱和于由数据不一致决定的误差水平；低精度数据下仍能恢复主物理结构，但无法超越该壁垒；高保真数值数据下结果与解析数据训练无差别，壁垒消失。

Conclusion: 数据质量与物理约束协同决定PINN性能；一致性壁垒揭示了数据保真度的根本限制，为构建和解释物理信息代理模型提供实践指导。

Abstract: Physics-informed neural networks (PINNs) have gained significant attention as a surrogate modeling strategy for partial differential equations (PDEs), particularly in regimes where labeled data are scarce and physical constraints can be leveraged to regularize the learning process. In practice, however, PINNs are frequently trained using experimental or numerical data that are not fully consistent with the governing equations due to measurement noise, discretization errors, or modeling assumptions. The implications of such data-to-PDE inconsistencies on the accuracy and convergence of PINNs remain insufficiently understood. In this work, we systematically analyze how data inconsistency fundamentally limits the attainable accuracy of PINNs. We introduce the concept of a consistency barrier, defined as an intrinsic lower bound on the error that arises from mismatches between the fidelity of the data and the exact enforcement of the PDE residual. To isolate and quantify this effect, we consider the 1D viscous Burgers equation with a manufactured analytical solution, which enables full control over data fidelity and residual errors. PINNs are trained using datasets of progressively increasing numerical accuracy, as well as perfectly consistent analytical data. Results show that while the inclusion of the PDE residual allows PINNs to partially mitigate low-fidelity data and recover the dominant physical structure, the training process ultimately saturates at an error level dictated by the data inconsistency. When high-fidelity numerical data are employed, PINN solutions become indistinguishable from those trained on analytical data, indicating that the consistency barrier is effectively removed. These findings clarify the interplay between data quality and physics enforcement in PINNs providing practical guidance for the construction and interpretation of physics-informed surrogate models.

</details>


### [260] [Pupillometry and Brain Dynamics for Cognitive Load in Working Memory](https://arxiv.org/abs/2602.10614)
*Nusaibah Farrukh,Malavika Pradeep,Akshay Sasi,Rahul Venugopal,Elizabeth Sherly*

Main category: cs.LG

TL;DR: 本研究比较了脑电图（EEG）和瞳孔测量法（pupillometry）在认知负荷评估中的实用性，发现基于特征的经典机器学习方法优于深度学习模型，且瞳孔测量法单独使用即可媲美EEG，具备便携、低成本、可解释性强等优势。


<details>
  <summary>Details</summary>
Motivation: 准确评估认知负荷对自适应学习、临床监测和脑机接口至关重要，但现有生理信号（如EEG和瞳孔测量）的比较效用及轻量化可穿戴整合仍缺乏深入探索。

Method: 结合基于特征（Catch-22特征）和模型驱动的方法，使用OpenNeuro 'Digit Span Task'数据集，对比EEG与瞳孔测量在二分类与多分类认知负荷识别任务中的性能，并采用SHAP进行可解释性特征分析。

Result: 基于Catch-22特征与经典机器学习模型的表现优于深度学习；瞳孔测量法单独使用即可达到与EEG相当的分类性能；SHAP分析揭示了具有生理意义的瞳孔动态特征。

Conclusion: 瞳孔测量法是一种便携、经济、可解释的认知负荷监测替代方案，无需依赖高成本、难穿戴的EEG，为神经精神科、教育与医疗领域的可穿戴认知监测系统提供了新路径。

Abstract: Cognitive load, the mental effort required during working memory, is central to neuroscience, psychology, and human-computer interaction. Accurate assessment is vital for adaptive learning, clinical monitoring, and brain-computer interfaces. Physiological signals such as pupillometry and electroencephalography are established biomarkers of cognitive load, but their comparative utility and practical integration as lightweight, wearable monitoring solutions remain underexplored. EEG provides high temporal resolution of neural activity. Although non-invasive, it is technologically demanding and limited in wearability and cost due to its resource-intensive nature, whereas pupillometry is non-invasive, portable, and scalable. Existing studies often rely on deep learning models with limited interpretability and substantial computational expense. This study integrates feature-based and model-driven approaches to advance time-series analysis. Using the OpenNeuro 'Digit Span Task' dataset, this study investigates cognitive load classification from EEG and pupillometry. Feature-based approaches using Catch-22 features and classical machine learning models outperform deep learning in both binary and multiclass tasks. The findings demonstrate that pupillometry alone can compete with EEG, serving as a portable and practical proxy for real-world applications. These results challenge the assumption that EEG is necessary for load detection, showing that pupil dynamics combined with interpretable models and SHAP based feature analysis provide physiologically meaningful insights. This work supports the development of wearable, affordable cognitive monitoring systems for neuropsychiatry, education, and healthcare.

</details>


### [261] [Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling](https://arxiv.org/abs/2602.10623)
*Zhibin Duan,Guowei Rong,Zhuo Li,Bo Chen,Mingyuan Zhou,Dandan Guo*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯非负奖励模型（BNRM），通过将非负因子分析融入Bradley-Terry偏好模型，实现对LLM奖励建模的去偏与解耦，提升鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类偏好的奖励模型易受标注噪声和系统性偏差（如响应长度、风格）影响，导致奖励黑客行为。

Method: 提出BNRM框架：结合非负因子分析与Bradley-Terry模型；采用双层稀疏非负潜在因子生成过程（实例级解耦 + 全局级稀疏去偏）；设计基于深度表征的摊销变分推断网络以支持大规模LLM训练。

Result: BNRM显著缓解奖励过优化，在分布偏移下更鲁棒，并提供比强基线更可解释的奖励分解。

Conclusion: BNRM通过‘先解耦、后去偏’结构实现不确定性感知的鲁棒奖励学习，为对齐LLM提供了新范式。

Abstract: Reward models learned from human preferences are central to aligning large language models (LLMs) via reinforcement learning from human feedback, yet they are often vulnerable to reward hacking due to noisy annotations and systematic biases such as response length or style. We propose Bayesian Non-Negative Reward Model (BNRM), a principled reward modeling framework that integrates non-negative factor analysis into Bradley-Terry (BT) preference model. BNRM represents rewards through a sparse, non-negative latent factor generative process that operates at two complementary levels: instance-specific latent variables induce disentangled reward representations, while sparsity over global latent factors acts as an implicit debiasing mechanism that suppresses spurious correlations. Together, this disentanglement-then-debiasing structure enables robust uncertainty-aware reward learning. To scale BNRM to modern LLMs, we develop an amortized variational inference network conditioned on deep model representations, allowing efficient end-to-end training. Extensive empirical results demonstrate that BNRM substantially mitigates reward over-optimization, improves robustness under distribution shifts, and yields more interpretable reward decompositions than strong baselines.

</details>


### [262] [Generative clinical time series models trained on moderate amounts of patient data are privacy preserving](https://arxiv.org/abs/2602.10631)
*Rustam Zhumagambetov,Niklas Giesa,Sebastian D. Boie,Stefan Haufe*

Main category: cs.LG

TL;DR: 本文评估了生成式AI模型在医疗时间序列数据上的隐私保护能力，发现现有隐私攻击对大规模训练的合成数据效果有限，而差分隐私机制反而会损害数据效用。


<details>
  <summary>Details</summary>
Motivation: 医疗数据共享受限于隐私风险，合成数据被视为潜在解决方案，但其实际隐私保护能力尚不明确。

Method: 使用多种已知隐私攻击方法对基于MIMIC-IV训练的先进医院时间序列生成模型进行隐私审计，并利用eICU数据发起跨数据集攻击。

Result: 当生成模型在足够大的数据集上训练时，现有隐私攻击对生成的多变量临床时间序列无效；引入差分隐私机制未提升隐私性，却显著降低下游机器学习任务性能。

Conclusion: 当前生成式时间序列模型在大数据训练下具备一定隐私鲁棒性，而强行施加差分隐私等机制可能得不偿失；隐私审计仍是必要环节。

Abstract: Sharing medical data for machine learning model training purposes is often impossible due to the risk of disclosing identifying information about individual patients. Synthetic data produced by generative artificial intelligence (genAI) models trained on real data is often seen as one possible solution to comply with privacy regulations. While powerful genAI models for heterogeneous hospital time series have recently been introduced, such modeling does not guarantee privacy protection, as the generated data may still reveal identifying information about individuals in the models' training cohort. Applying established privacy mechanisms to generative time series models, however, proves challenging as post-hoc data anonymization through k-anonymization or similar techniques is limited, while model-centered privacy mechanisms that implement differential privacy (DP) may lead to unstable training, compromising the utility of generated data. Given these known limitations, privacy audits for generative time series models are currently indispensable regardless of the concrete privacy mechanisms applied to models and/or data. In this work, we use a battery of established privacy attacks to audit state-of-the-art hospital time series models, trained on the public MIMIC-IV dataset, with respect to privacy preservation. Furthermore, the eICU dataset was used to mount a privacy attack against the synthetic data generator trained on the MIMIC-IV dataset. Results show that established privacy attacks are ineffective against generated multivariate clinical time series when synthetic data generators are trained on large enough training datasets. Furthermore, we discuss how the use of existing DP mechanisms for these synthetic data generators would not bring desired improvement in privacy, but only a decrease in utility for machine learning prediction tasks.

</details>


### [263] [Coarse-Grained Boltzmann Generators](https://arxiv.org/abs/2602.10637)
*Weilong Chen,Bojun Zhao,Jan Eckwert,Julija Zavadlav*

Main category: cs.LG

TL;DR: 本文提出了粗粒度玻尔兹曼生成器（CG-BGs），将粗粒度建模与重要性采样结合，通过学习势平均力（PMF）对流模型生成的样本进行重加权，实现对大分子系统的无偏高效采样。


<details>
  <summary>Details</summary>
Motivation: 现有玻尔兹曼生成器（BGs）可扩展性受限；粗粒度模型虽能处理更大系统，但缺乏保证统计准确性的重加权机制。

Method: 提出CG-BGs框架：在粗粒度坐标空间中运行，用基于流的生成模型采样，并利用通过力匹配从快速收敛数据中学习到的势平均力（PMF）进行精确重加权。

Result: CG-BGs能在高度简化的表示中忠实捕捉显式溶剂介导的复杂相互作用，显著提升大分子系统无偏采样的可扩展性。

Conclusion: CG-BGs为大规模分子系统提供了兼具可扩展性与统计严格性的新范式，统一了降维建模与精确重要性采样。

Abstract: Sampling equilibrium molecular configurations from the Boltzmann distribution is a longstanding challenge. Boltzmann Generators (BGs) address this by combining exact-likelihood generative models with importance sampling, but their practical scalability is limited. Meanwhile, coarse-grained surrogates enable the modeling of larger systems by reducing effective dimensionality, yet often lack the reweighting process required to ensure asymptotically correct statistics. In this work, we propose Coarse-Grained Boltzmann Generators (CG-BGs), a principled framework that unifies scalable reduced-order modeling with the exactness of importance sampling. CG-BGs act in a coarse-grained coordinate space, using a learned potential of mean force (PMF) to reweight samples generated by a flow-based model. Crucially, we show that this PMF can be efficiently learned from rapidly converged data via force matching. Our results demonstrate that CG-BGs faithfully capture complex interactions mediated by explicit solvent within highly reduced representations, establishing a scalable pathway for the unbiased sampling of larger molecular systems.

</details>


### [264] [Evaluation metrics for temporal preservation in synthetic longitudinal patient data](https://arxiv.org/abs/2602.10643)
*Katariina Perkonoja,Parisa Movahedi,Antti Airola,Kari Auranen,Joni Virta*

Main category: cs.LG

TL;DR: 本文提出了一套评估合成纵向患者数据中时间保真度的指标，涵盖边缘、协方差、个体层面和测量结构四个维度，强调需多维综合评估而非依赖单一指标。


<details>
  <summary>Details</summary>
Motivation: 现有合成纵向医疗数据缺乏对时间结构保真度的系统评估方法，易因忽略协方差或个体轨迹失真而高估模型性能。

Method: 设计四类时间保真度评估指标（边缘、协方差、个体级、测量结构），并分析数据质量、采样频率及预处理（如分箱、编码、精度）对保真度的影响。

Result: 发现边缘相似性高可能掩盖协方差失真与个体轨迹断裂；稀疏/不规则时序变量显著降低合成数据的时间保真度；单一指标不足以全面评估，需多维联合分析。

Conclusion: 多维时间保真度指标可更可靠地评估和改进生成模型，推动构建时间上更真实的合成纵向患者数据。

Abstract: This study introduces a set of metrics for evaluating temporal preservation in synthetic longitudinal patient data, defined as artificially generated data that mimic real patients' repeated measurements over time. The proposed metrics assess how synthetic data reproduces key temporal characteristics, categorized into marginal, covariance, individual-level and measurement structures. We show that strong marginal-level resemblance may conceal distortions in covariance and disruptions in individual-level trajectories. Temporal preservation is influenced by factors such as original data quality, measurement frequency, and preprocessing strategies, including binning, variable encoding and precision. Variables with sparse or highly irregular measurement times provide limited information for learning temporal dependencies, resulting in reduced resemblance between the synthetic and original data. No single metric adequately captures temporal preservation; instead, a multidimensional evaluation across all characteristics provides a more comprehensive assessment of synthetic data quality. Overall, the proposed metrics clarify how and why temporal structures are preserved or degraded, enabling more reliable evaluation and improvement of generative models and supporting the creation of temporally realistic synthetic longitudinal patient data.

</details>


### [265] [Domain Knowledge Guided Bayesian Optimization For Autonomous Alignment Of Complex Scientific Instruments](https://arxiv.org/abs/2602.10670)
*Aashwin Mishra,Matt Seaberg,Ryan Roussel,Daniel Ratner,Apurva Mehta*

Main category: cs.LG

TL;DR: 本文提出了一种结合领域知识（物理洞察）的贝叶斯优化方法，通过坐标变换解耦高维强耦合参数、对齐活跃子空间，显著提升在稀疏奖励“针尖找 haystack”问题中的优化效率与鲁棒性，并在12维光学系统中验证其优于标准BO、TuRBO和多目标BO。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化（如TuRBO）在高维、强耦合、高度非对称且奖励稀疏的问题中性能下降严重，难以找到全局最优；亟需提升复杂科学仪器自动调优的鲁棒性与效率。

Method: 提出基于物理洞察的坐标变换策略，将原始输入空间映射为解耦、对齐活跃子空间的新坐标系，并结合反向退火探索策略进行贝叶斯优化。

Result: 在12维6晶体Split-and-Delay光学系统实验中，该方法稳定收敛至全局最优，而标准BO、TuRBO及多目标BO均持续失败；坐标变换本身是加速搜索的关键。

Conclusion: 将物理先验融入坐标表示可将高维耦合优化问题简化为低维解耦形式，无需替换现有优化算法，即可实现快速、鲁棒的自动化调优，具有广泛可推广性。

Abstract: Bayesian Optimization (BO) is a powerful tool for optimizing complex non-linear systems. However, its performance degrades in high-dimensional problems with tightly coupled parameters and highly asymmetric objective landscapes, where rewards are sparse. In such needle-in-a-haystack scenarios, even advanced methods like trust-region BO (TurBO) often lead to unsatisfactory results. We propose a domain knowledge guided Bayesian Optimization approach, which leverages physical insight to fundamentally simplify the search problem by transforming coordinates to decouple input features and align the active subspaces with the primary search axes. We demonstrate this approach's efficacy on a challenging 12-dimensional, 6-crystal Split-and-Delay optical system, where conventional approaches, including standard BO, TuRBO and multi-objective BO, consistently led to unsatisfactory results. When combined with an reverse annealing exploration strategy, this approach reliably converges to the global optimum. The coordinate transformation itself is the key to this success, significantly accelerating the search by aligning input co-ordinate axes with the problem's active subspaces. As increasingly complex scientific instruments, from large telescopes to new spectrometers at X-ray Free Electron Lasers are deployed, the demand for robust high-dimensional optimization grows. Our results demonstrate a generalizable paradigm: leveraging physical insight to transform high-dimensional, coupled optimization problems into simpler representations can enable rapid and robust automated tuning for consistent high performance while still retaining current optimization algorithms.

</details>


### [266] [VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training](https://arxiv.org/abs/2602.10693)
*Guobin Shen,Chenxiao Zhao,Xiang Cheng,Lei Huang,Xing Yu*

Main category: cs.LG

TL;DR: 本文提出VESPO方法，通过变分公式结合方差缩减技术，设计序列级重要性权重重塑核，解决大语言模型强化学习中策略陈旧导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 训练稳定性是大语言模型强化学习中的核心挑战，策略陈旧、异步训练及训练-推理引擎不匹配导致行为策略偏离当前策略，引发训练崩溃；重要性采样虽可校正分布偏移，但方差高，现有修正方法缺乏统一理论基础。

Method: 提出变分序列级软策略优化（VESPO），将方差缩减融入关于提议分布的变分框架，推导出直接作用于序列级重要性权重的闭式重塑核，无需长度归一化。

Result: 在数学推理基准上实验表明，VESPO在陈旧比高达64倍和完全异步执行下仍保持稳定训练，并在稠密模型和MoE模型上均取得一致性能提升。

Conclusion: VESPO为RL for LLMs提供了兼具理论严谨性与实践有效性的序列级重要性权重校正方案，显著提升了训练鲁棒性与可扩展性。

Abstract: Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO

</details>


### [267] [Reducing Estimation Uncertainty Using Normalizing Flows and Stratification](https://arxiv.org/abs/2602.10706)
*Paweł Lorek,Rafał Topolnicki,Tomasz Trzciński,Maciej Zięba,Aleksandra Krystecka*

Main category: cs.LG

TL;DR: 本文提出了一种结合基于流的模型与分层抽样的新方法，用于在未知分布下更准确地估计实值函数的期望值，显著降低了估计不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设数据服从（半）参数分布（如高斯分布），当假设不成立时会导致较大的估计不确定性。

Method: 提出一种基于流的生成模型，并与分层抽样相结合，利用参数化神经网络灵活建模未知数据分布。

Result: 在多个数据集（包括30维和128维高维数据）上显著降低估计不确定性，性能优于朴素蒙特卡洛估计器和高斯混合模型。

Conclusion: 该方法提升了非参数或复杂分布下期望估计的鲁棒性与精度，为统计推断提供了更灵活可靠的工具。

Abstract: Estimating the expectation of a real-valued function of a random variable from sample data is a critical aspect of statistical analysis, with far-reaching implications in various applications. Current methodologies typically assume (semi-)parametric distributions such as Gaussian or mixed Gaussian, leading to significant estimation uncertainty if these assumptions do not hold. We propose a flow-based model, integrated with stratified sampling, that leverages a parametrized neural network to offer greater flexibility in modeling unknown data distributions, thereby mitigating this limitation. Our model shows a marked reduction in estimation uncertainty across multiple datasets, including high-dimensional (30 and 128) ones, outperforming crude Monte Carlo estimators and Gaussian mixture models. Reproducible code is available at https://github.com/rnoxy/flowstrat.

</details>


### [268] [Interpretable Graph-Level Anomaly Detection via Contrast with Normal Prototypes](https://arxiv.org/abs/2602.10708)
*Qiuran Zhao,Kai Ming Ting,Xinpeng Li*

Main category: cs.LG

TL;DR: 本文提出了一种可解释的无监督图级异常检测框架ProtoGLAD，通过显式对比异常图与其最近的正常原型图来提供解释，并利用点集核迭代发现多个正常原型图及其簇，从而识别远离所有正常簇的异常图。


<details>
  <summary>Details</summary>
Motivation: 现有深度图级异常检测方法缺乏可解释性，且已有解释方法要么未参考正常图，要么依赖抽象潜在向量而非具体正常图作为原型。

Method: 提出Prototype-based Graph-Level Anomaly Detection（ProtoGLAD），采用点集核迭代发现多个正常原型图及其簇，将远离所有正常簇的图判定为异常，并以最近正常原型图为依据提供解释。

Result: 在多个真实数据集上的实验表明，ProtoGLAD在检测性能上媲美当前最优方法，同时提供更优的人类可理解的原型驱动解释。

Conclusion: ProtoGLAD在保持高检测性能的同时显著提升了图级异常检测的可解释性与实用性，为实际部署提供了可靠支持。

Abstract: The task of graph-level anomaly detection (GLAD) is to identify anomalous graphs that deviate significantly from the majority of graphs in a dataset. While deep GLAD methods have shown promising performance, their black-box nature limits their reliability and deployment in real-world applications. Although some recent methods have made attempts to provide explanations for anomaly detection results, they either provide explanations without referencing normal graphs, or rely on abstract latent vectors as prototypes rather than concrete graphs from the dataset. To address these limitations, we propose Prototype-based Graph-Level Anomaly Detection (ProtoGLAD), an interpretable unsupervised framework that provides explanation for each detected anomaly by explicitly contrasting with its nearest normal prototype graph. It employs a point-set kernel to iteratively discover multiple normal prototype graphs and their associated clusters from the dataset, then identifying graphs distant from all discovered normal clusters as anomalies. Extensive experiments on multiple real-world datasets demonstrate that ProtoGLAD achieves competitive anomaly detection performance compared to state-of-the-art GLAD methods while providing better human-interpretable prototype-based explanations.

</details>


### [269] [Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks](https://arxiv.org/abs/2602.10780)
*Enrico Ahlers,Daniel Passon,Yannic Noller,Lars Grunske*

Main category: cs.LG

TL;DR: 本文提出了一种推理时的后门缓解方法FIRE，通过在特征空间中反向修正触发器引起的表示变化，从而在不修改模型或训练数据的情况下有效中和后门攻击。


<details>
  <summary>Details</summary>
Motivation: 已部署的后门模型难以用传统训练数据过滤、模型修改或输入预处理等方法有效缓解，亟需一种高效、低开销的推理时防御方案。

Method: FIRE方法将触发器建模为潜空间中可识别的方向，并在推理时对中毒样本的潜层特征沿这些方向进行反向调整，以中和触发效应，实现自我修复。

Result: FIRE在多个图像基准测试中显著优于现有运行时缓解方法，具有低计算开销，且在不同攻击类型、数据集和网络结构上均表现稳健。

Conclusion: FIRE提供了一种实用、通用且高效的推理时后门防御机制，为已部署的脆弱模型提供了可行的安全增强路径。

Abstract: Machine learning models are increasingly present in our everyday lives; as a result, they become targets of adversarial attackers seeking to manipulate the systems we interact with. A well-known vulnerability is a backdoor introduced into a neural network by poisoned training data or a malicious training process. Backdoors can be used to induce unwanted behavior by including a certain trigger in the input. Existing mitigations filter training data, modify the model, or perform expensive input modifications on samples. If a vulnerable model has already been deployed, however, those strategies are either ineffective or inefficient. To address this gap, we propose our inference-time backdoor mitigation approach called FIRE (Feature-space Inference-time REpair). We hypothesize that a trigger induces structured and repeatable changes in the model's internal representation. We view the trigger as directions in the latent spaces between layers that can be applied in reverse to correct the inference mechanism. Therefore, we turn the backdoored model against itself by manipulating its latent representations and moving a poisoned sample's features along the backdoor directions to neutralize the trigger. Our evaluation shows that FIRE has low computational overhead and outperforms current runtime mitigations on image benchmarks across various attacks, datasets, and network architectures.

</details>


### [270] [Rising Multi-Armed Bandits with Known Horizons](https://arxiv.org/abs/2602.10727)
*Seockbean Song,Chenyu Gan,Youngsik Yoon,Siwei Wang,Wei Chen,Jungseul Ok*

Main category: cs.LG

TL;DR: 本文提出了CURE-UCB算法，用于解决上升型多臂赌博机（RMAB）问题，该问题中各臂的期望奖励随尝试次数增加而上升；算法显式整合了预算周期T，并在理论和实验上均证明优于不考虑周期的策略。


<details>
  <summary>Details</summary>
Motivation: RMAB框架建模了现实中奖励随使用次数提升的场景（如超参调优、机器人学习），其最优策略高度依赖于总预算T（horizon-dependent optimality），但现有研究大多忽略T的影响，缺乏对horizon-aware设置的深入探索。

Method: 提出CUmulative Reward Estimation UCB（CURE-UCB）算法，显式将预算周期T融入置信上界（UCB）设计中，通过累计奖励估计实现更精准的臂选择；并给出严格的悔恨上界分析。

Result: 理论证明CURE-UCB在‘线性后平坦’等结构化RMAB实例中严格优于horizon-agnostic策略；实验表明其在多个基准上显著超越现有基线方法。

Conclusion: 显式利用预算周期T是提升RMAB性能的关键；CURE-UCB为horizon-aware bandit问题提供了有效且可分析的新范式。

Abstract: The Rising Multi-Armed Bandit (RMAB) framework models environments where expected rewards of arms increase with plays, which models practical scenarios where performance of each option improves with the repeated usage, such as in robotics and hyperparameter tuning. For instance, in hyperparameter tuning, the validation accuracy of a model configuration (arm) typically increases with each training epoch. A defining characteristic of RMAB is em horizon-dependent optimality: unlike standard settings, the optimal strategy here shifts dramatically depending on the available budget $T$. This implies that knowledge of $T$ yields significantly greater utility in RMAB, empowering the learner to align its decision-making with this shifting optimality. However, the horizon-aware setting remains underexplored. To address this, we propose a novel CUmulative Reward Estimation UCB (CURE-UCB) that explicitly integrates the horizon. We provide a rigorous analysis establishing a new regret upper bound and prove that our method strictly outperforms horizon-agnostic strategies in structured environments like ``linear-then-flat'' instances. Extensive experiments demonstrate its significant superiority over baselines.

</details>


### [271] [From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers](https://arxiv.org/abs/2602.11130)
*Maximilian Plattner,Fabian Paischer,Johannes Brandstetter,Arturs Berzins*

Main category: cs.LG

TL;DR: 本文发现3D扩散Transformer在稀疏点云表面补全任务中存在一种名为'Meltdown'的灾难性失效模式：微小的输入扰动会导致输出严重碎片化；通过机制可解释性中的激活修补技术，定位到早期去噪交叉注意力层的一个关键激活，并利用其奇异值谱的谱熵作为碎片化预测指标；基于此，提出测试时控制方法PowerRemap，显著提升模型鲁棒性（最高达98.3%稳定率）。


<details>
  <summary>Details</summary>
Motivation: 3D扩散Transformer虽在稀疏点云表面补全任务中性能领先，但其对微小输入扰动高度敏感，导致输出严重碎片化（Meltdown），威胁实际应用可靠性。

Method: 采用机制可解释性中的激活修补（activation-patching）技术定位失效根源；分析交叉注意力激活的奇异值谱，提取谱熵作为标量代理；结合扩散动力学建模，揭示其对应反向过程的对称性破缺分岔；据此设计测试时控制策略PowerRemap以稳定稀疏点云条件化。

Result: 成功将Meltdown归因于单个早期去噪交叉注意力激活；谱熵被验证为可靠碎片化指示器；PowerRemap在多种SOTA模型（WaLa、Make-a-Shape）、数据集（GSO、SimJEB）和采样器（DDPM、DDIM）上实现高达98.3%的稳定率。

Conclusion: 扩散模型的行为可通过电路级机制分析深入理解并有效引导；本文首次将特定交叉注意力机制与扩散轨迹分岔的动力学解释相联系，为提升生成模型鲁棒性提供了可解释、可干预的新范式。

Abstract: Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdown to a single early denoising cross-attention activation. We find that the singular-value spectrum of this activation provides a scalar proxy: its spectral entropy rises when fragmentation occurs and returns to baseline when patched. Interpreted through diffusion dynamics, we show that this proxy tracks a symmetry-breaking bifurcation of the reverse process. Guided by this insight, we introduce PowerRemap, a test-time control that stabilizes sparse point-cloud conditioning. We demonstrate that Meltdown persists across state-of-the-art architectures (WaLa, Make-a-Shape), datasets (GSO, SimJEB) and denoising strategies (DDPM, DDIM), and that PowerRemap effectively counters this failure with stabilization rates of up to 98.3%. Overall, this work is a case study on how diffusion model behavior can be understood and guided based on mechanistic analysis, linking a circuit-level cross-attention mechanism to diffusion-dynamics accounts of trajectory bifurcations.

</details>


### [272] [Kalman Linear Attention: Parallel Bayesian Filtering For Efficient Language Modelling and State Tracking](https://arxiv.org/abs/2602.10743)
*Vaisakh Shaj,Cameron Barker,Aidan Scannell,Andras Szecsenyi,Elliot J. Crowley,Amos Storkey*

Main category: cs.LG

TL;DR: 本文提出Kalman Linear Attention (KLA)层，通过将卡尔曼滤波器重参数化为信息形式，实现并行化概率推理，兼具表达力与计算效率，在语言建模任务中表现优于现有状态空间模型和门控线性注意力方法。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间语言模型（如Mamba、GLA）虽具线性复杂度和平行训练优势，但在表达力和鲁棒状态跟踪方面存在不足。

Method: 将卡尔曼滤波器重参数化为信息形式，使其更新可通过结合扫描（associative scan）并行计算；基于此构建KLA层，实现时间并行的概率推断，并显式建模信念状态的不确定性。

Result: KLA在离散token操作和状态跟踪基准上匹配或超越现代SSM和GLA；支持更丰富的非线性更新和门控机制，同时保持其计算优势。

Conclusion: KLA为序列建模提供了兼具理论严谨性（贝叶斯滤波）、表达力和高效并行训练的新范式，弥合了传统滤波器与现代神经序列模型之间的鸿沟。

Abstract: State-space language models such as Mamba and gated linear attention (GLA) offer efficient alternatives to transformers due to their linear complexity and parallel training, but often lack the expressivity and robust state-tracking needed for complex reasoning. We address these limitations by reframing sequence modelling through a probabilistic lens, using Bayesian filters as a core primitive. While classical filters such as Kalman filters provide principled state estimation and uncertainty tracking, they are typically viewed as inherently sequential. We show that reparameterising the Kalman filter in information form enables its updates to be computed via an associative scan, allowing efficient parallel training. Building on this insight, we introduce the Kalman Linear Attention (KLA) layer, a neural sequence-modelling primitive that performs time-parallel probabilistic inference while maintaining explicit belief-state uncertainty. KLA offers strictly more expressive nonlinear updates and gating than GLA variants while retaining their computational advantages. On language modelling tasks, KLA matches or outperforms modern SSMs and GLAs across representative discrete token-manipulation and state-tracking benchmarks.

</details>


### [273] [GENIUS: Generative Fluid Intelligence Evaluation Suite](https://arxiv.org/abs/2602.11144)
*Ruichuan An,Sihan Yang,Ziyu Guo,Wei Dai,Zijun Shen,Haodong Li,Renrui Zhang,Xinyu Wei,Guopeng Li,Wenshan Wu,Wentao Zhang*

Main category: cs.LG

TL;DR: 本文提出GENIUS评测套件，用于评估统一多模态模型（UMMs）的生成流体智力（GFI），即在新场景中即时推理、模式归纳与约束执行的能力；发现现有模型在此类任务上存在显著缺陷，根源在于上下文理解不足而非生成能力欠缺，并提出一种无需训练的注意力干预策略加以改进。


<details>
  <summary>Details</summary>
Motivation: 现有评测主要关注结晶化智力（知识回忆），忽视了生成流体智力（GFI）——即在无先验知识前提下，基于即时上下文进行模式归纳、约束执行和动态适应的能力。

Method: 提出GENIUS评测套件，将GFI形式化为三大基本能力：隐式模式归纳、即兴约束执行、上下文知识适配；对12个主流模型系统评测，并通过诊断分析定位失败原因；进一步设计一种训练-free的注意力干预策略以提升上下文理解能力。

Result: 实验表明当前UMMs在GFI任务上表现较差，且问题根源明确指向上下文理解能力不足；所提注意力干预策略有效缓解该缺陷。

Conclusion: GENIUS建立了评估生成流体智力的严格标准，推动多模态生成模型从依赖已有知识转向具备动态、通用推理能力的新范式。

Abstract: Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\textbf{GENIUS}$ ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite). We formalize $\textit{GFI}$ as a synthesis of three primitives. These include $\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\textbf{GENIUS}$ establishes a rigorous standard for $\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$.

</details>


### [274] [Predicting integers from continuous parameters](https://arxiv.org/abs/2602.10751)
*Bas Maat,Peter Bloem*

Main category: cs.LG

TL;DR: 本文研究如何直接建模受约束的整数标签（如社交帖子点赞数、共享单车数量），提出使用可微分的离散分布替代传统连续回归，重点评估了Bitwise（逐位伯努利）和离散拉普拉斯两种分布，在表格、序列和图像任务中表现最优。


<details>
  <summary>Details</summary>
Motivation: 传统回归将整数标签视为连续变量，改变了其固有的离散分布特性；而直接建模整数标签需满足神经网络输出参数可微以支持反向传播。

Method: 探索多种可微分离散分布（包括已有方法与新提出的Bitwise和离散拉普拉斯分布），将其嵌入神经网络输出层，用于整数标签预测任务。

Result: 在表格学习、序列预测和图像生成等多个任务上，Bitwise分布和离散拉普拉斯分布整体性能最优。

Conclusion: 可微分离散分布能更适配整数标签的本质特性，其中Bitwise和离散拉普拉斯是兼顾建模能力与优化可行性的有效方案。

Abstract: We study the problem of predicting numeric labels that are constrained to the integers or to a subrange of the integers. For example, the number of up-votes on social media posts, or the number of bicycles available at a public rental station. While it is possible to model these as continuous values, and to apply traditional regression, this approach changes the underlying distribution on the labels from discrete to continuous. Discrete distributions have certain benefits, which leads us to the question whether such integer labels can be modeled directly by a discrete distribution, whose parameters are predicted from the features of a given instance. Moreover, we focus on the use case of output distributions of neural networks, which adds the requirement that the parameters of the distribution be continuous so that backpropagation and gradient descent may be used to learn the weights of the network. We investigate several options for such distributions, some existing and some novel, and test them on a range of tasks, including tabular learning, sequential prediction and image generation. We find that overall the best performance comes from two distributions: Bitwise, which represents the target integer in bits and places a Bernoulli distribution on each, and a discrete analogue of the Laplace distribution, which uses a distribution with exponentially decaying tails around a continuous mean.

</details>


### [275] [Exploring the impact of adaptive rewiring in Graph Neural Networks](https://arxiv.org/abs/2602.10754)
*Charlotte Cambier van Nooten,Christos Aronis,Yuliya Shapovalova,Lucia Cavallaro*

Main category: cs.LG

TL;DR: 本文探讨了图神经网络（GNN）中基于稀疏化的正则化方法，结合网络科学与机器学习技术（如Erdős-Rényi模型），提升其在大规模图（如电力系统N-1故障评估）中的效率与可扩展性；实验表明适度稀疏可改善泛化，但过度稀疏损害性能，而自适应重连配合早停效果显著。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在大规模图应用中内存占用高、计算成本大的问题，尤其面向电力系统等关键实际场景的可靠性分析需求。

Method: 采用网络科学（如Erdős-Rényi随机图模型）与机器学习结合的稀疏化与自适应重连策略，应用于GCN和GIN模型，并结合早停机制进行训练优化。

Result: 在三个不同规模数据集上的实验表明：适度稀疏可提升泛化能力；过度稀疏削弱复杂模式学习；自适应重连+早停显著提升性能与鲁棒性。

Conclusion: 稀疏化是提升GNN可扩展性与实用性的有效正则化手段，但需精细调参；跨领域（网络科学+ML）协同设计对关键基础设施建模具有重要价值。

Abstract: This paper explores sparsification methods as a form of regularization in Graph Neural Networks (GNNs) to address high memory usage and computational costs in large-scale graph applications. Using techniques from Network Science and Machine Learning, including Erdős-Rényi for model sparsification, we enhance the efficiency of GNNs for real-world applications. We demonstrate our approach on N-1 contingency assessment in electrical grids, a critical task for ensuring grid reliability. We apply our methods to three datasets of varying sizes, exploring Graph Convolutional Networks (GCN) and Graph Isomorphism Networks (GIN) with different degrees of sparsification and rewiring. Comparison across sparsification levels shows the potential of combining insights from both research fields to improve GNN performance and scalability. Our experiments highlight the importance of tuning sparsity parameters: while sparsity can improve generalization, excessive sparsity may hinder learning of complex patterns. Our adaptive rewiring approach, particularly when combined with early stopping, proves promising by allowing the model to adapt its connectivity structure during training. This research contributes to understanding how sparsity can be effectively leveraged in GNNs for critical applications like power grid reliability analysis.

</details>


### [276] [Collaborative Threshold Watermarking](https://arxiv.org/abs/2602.10765)
*Tameem Bakr,Anish Ambreth,Nils Lukas*

Main category: cs.LG

TL;DR: 本文提出了一种适用于联邦学习的(t,K)-门限水印机制，允许多个客户端协作嵌入共享水印，仅当至少t个客户端联合时才能恢复密钥并验证模型，从而保护模型来源证明的同时防止单个客户端篡改或移除水印。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，各客户端需证明其对联合训练模型的贡献，但现有水印方法存在可扩展性差或易被单方移除的问题。

Method: 采用门限秘密共享方案对水印密钥τ进行分发，使得少于t个客户端无法重构密钥；设计白盒场景下的水印嵌入与验证协议，支持无明文暴露的验证。

Result: 在图像分类任务上验证了该方法在K=128规模下仍保持水印可检测性（z≥4），且精度损失极小；对使用最多20%数据的自适应微调等攻击具有鲁棒性。

Conclusion: 所提(t,K)-门限水印机制在保障多方协作验证能力的同时，兼顾安全性、可扩展性与鲁棒性，为联邦学习中的模型溯源提供了新范式。

Abstract: In federated learning (FL), $K$ clients jointly train a model without sharing raw data. Because each participant invests data and compute, clients need mechanisms to later prove the provenance of a jointly trained model. Model watermarking embeds a hidden signal in the weights, but naive approaches either do not scale with many clients as per-client watermarks dilute as $K$ grows, or give any individual client the ability to verify and potentially remove the watermark. We introduce $(t,K)$-threshold watermarking: clients collaboratively embed a shared watermark during training, while only coalitions of at least $t$ clients can reconstruct the watermark key and verify a suspect model. We secret-share the watermark key $τ$ so that coalitions of fewer than $t$ clients cannot reconstruct it, and verification can be performed without revealing $τ$ in the clear. We instantiate our protocol in the white-box setting and evaluate on image classification. Our watermark remains detectable at scale ($K=128$) with minimal accuracy loss and stays above the detection threshold ($z\ge 4$) under attacks including adaptive fine-tuning using up to 20% of the training data.

</details>


### [277] [LOREN: Low Rank-Based Code-Rate Adaptation in Neural Receivers](https://arxiv.org/abs/2602.10770)
*Bram Van Bolderik,Vlado Menkovski,Sonia Heemstra de Groot,Manil Dev Gomony*

Main category: cs.LG

TL;DR: LOREN is a low-rank adaptation neural receiver that reduces memory and power overhead by using lightweight adapters per code rate while sharing a frozen base network, achieving performance comparable to fully retrained models with significant hardware savings.


<details>
  <summary>Details</summary>
Motivation: Neural network-based receivers suffer from high memory and power requirements due to needing separate weight sets for each code rate, limiting their practical deployment.

Method: LOREN introduces low-rank adaptation adapters (LOREN adapters) integrated into convolutional layers; the base network is frozen and only small adapters are trained per code rate, with end-to-end training over 3GPP CDL channels.

Result: LOREN achieves comparable or superior performance to fully retrained neural receivers and demonstrates >65% silicon area savings and up to 15% power reduction in 22nm hardware implementation supporting three code rates.

Conclusion: LOREN enables efficient, code-rate-adaptive neural receivers with minimal overhead, enhancing practicality for real-world wireless systems.

Abstract: Neural network based receivers have recently demonstrated superior system-level performance compared to traditional receivers. However, their practicality is limited by high memory and power requirements, as separate weight sets must be stored for each code rate. To address this challenge, we propose LOREN, a Low Rank-Based Code-Rate Adaptation Neural Receiver that achieves adaptability with minimal overhead. LOREN integrates lightweight low rank adaptation adapters (LOREN adapters) into convolutional layers, freezing a shared base network while training only small adapters per code rate. An end-to-end training framework over 3GPP CDL channels ensures robustness across realistic wireless environments. LOREN achieves comparable or superior performance relative to fully retrained base neural receivers. The hardware implementation of LOREN in 22nm technology shows more than 65% savings in silicon area and up to 15% power reduction when supporting three code rates.

</details>


### [278] [Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization](https://arxiv.org/abs/2602.10794)
*Benjy Friedmann,Nadav Dym*

Main category: cs.LG

TL;DR: 本文提出CycFlow框架，通过确定性点传输替代迭代边去噪，利用实例条件向量场将2D坐标连续传输至标准圆形排列，并通过角度排序恢复最优路径，显著提升求解速度并保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的神经组合优化方法在处理欧氏旅行商问题时存在二次计算瓶颈，效率低下。

Method: CycFlow学习一个实例条件向量场，将输入2D坐标连续传输到标准圆形排列；通过角度排序从2N维表示中恢复最优环游；采用数据依赖的流匹配避免边评分的二次复杂度。

Result: 相比最先进的扩散模型基线，求解速度提升高达三个数量级，同时保持有竞争力的最优性差距。

Conclusion: CycFlow通过将TSP建模为确定性点传输而非随机热图生成，实现了效率与性能的良好平衡，为NCO提供了新范式。

Abstract: Recent advances in Neural Combinatorial Optimization (NCO) have been dominated by diffusion models that treat the Euclidean Traveling Salesman Problem (TSP) as a stochastic $N \times N$ heatmap generation task. In this paper, we propose CycFlow, a framework that replaces iterative edge denoising with deterministic point transport. CycFlow learns an instance-conditioned vector field that continuously transports input 2D coordinates to a canonical circular arrangement, where the optimal tour is recovered from this $2N$ dimensional representation via angular sorting. By leveraging data-dependent flow matching, we bypass the quadratic bottleneck of edge scoring in favor of linear coordinate dynamics. This paradigm shift accelerates solving speed by up to three orders of magnitude compared to state-of-the-art diffusion baselines, while maintaining competitive optimality gaps.

</details>


### [279] [PRISM: Parallel Residual Iterative Sequence Model](https://arxiv.org/abs/2602.10796)
*Jie Jiang,Ke Cheng,Xin Xu,Mengyang Pang,Tianhao Lu,Jiaheng Li,Yue Liu,Yuan Wang,Jun Zhang,Huan Yu,Zhouchen Lin*

Main category: cs.LG

TL;DR: PRISM是一种新型并行序列模型，通过引入求解器启发的归纳偏置和写-忘解耦策略，在保持高效并行计算的同时，实现了多步迭代优化的建模能力，显著提升了吞吐量并突破了传统线性模型的表达瓶颈。


<details>
  <summary>Details</summary>
Motivation: 解决生成式序列建模中Transformer高表达性与线性模型高效率之间的根本张力，克服现有高效架构受限于单步线性更新、而强迭代方法（如TTT）因状态依赖梯度而破坏硬件并行性的问题。

Method: 提出PRISM模型：采用求解器启发的归纳偏置；引入Write-Forget Decoupling策略将非线性限制在注入算子内；设计两阶段代理架构——短卷积锚定初始残差，学习型预测器直接从输入估计修正更新；理论证明其具备Rank-L累积能力。

Result: 在保持与显式优化方法相当性能的同时，实现174倍吞吐量提升；理论上突破单步Rank-1更新瓶颈，结构化扩展更新流形。

Conclusion: PRISM成功弥合了表达力与效率之间的鸿沟，将多步迭代精炼的结构模式蒸馏为可并行的前馈算子，为高效生成序列建模提供了新范式。

Abstract: Generative sequence modeling faces a fundamental tension between the expressivity of Transformers and the efficiency of linear sequence models. Existing efficient architectures are theoretically bounded by shallow, single-step linear updates, while powerful iterative methods like Test-Time Training (TTT) break hardware parallelism due to state-dependent gradients. We propose PRISM (Parallel Residual Iterative Sequence Model) to resolve this tension. PRISM introduces a solver-inspired inductive bias that captures key structural properties of multi-step refinement in a parallelizable form. We employ a Write-Forget Decoupling strategy that isolates non-linearity within the injection operator. To bypass the serial dependency of explicit solvers, PRISM utilizes a two-stage proxy architecture: a short-convolution anchors the initial residual using local history energy, while a learned predictor estimates the refinement updates directly from the input. This design distills structural patterns associated with iterative correction into a parallelizable feedforward operator. Theoretically, we prove that this formulation achieves Rank-$L$ accumulation, structurally expanding the update manifold beyond the single-step Rank-$1$ bottleneck. Empirically, it achieves comparable performance to explicit optimization methods while achieving 174x higher throughput.

</details>


### [280] [RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization](https://arxiv.org/abs/2602.10819)
*Linxuan Xia,Xiaolong Yang,Yongyuan Chen,Enyue Zhao,Deng Cai,Yasheng Wang,Boxi Wu*

Main category: cs.LG

TL;DR: 本文提出Rephrasing Policy Optimization (RePO)，通过让策略模型先理解离策略知识再将其重述为符合自身风格和参数分布的轨迹，从而在保持在线策略训练稳定性的同时，有效吸收离策略知识，提升大语言模型在领域数据对齐中对难样本的利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两难：监督微调（SFT）易损害模型通用性；在线策略强化学习（RL）难以有效处理超出当前推理能力的难样本；离线策略RL虽改善难样本利用，但因强制分布偏移导致训练极不稳定。

Method: 提出RePO方法：策略模型被提示先理解离策略知识，再将其重述为符合自身风格与参数分布的轨迹；动态用这些高质量重述轨迹替换低奖励采样轨迹，从而在严格保持在线策略训练动态的前提下引导模型走向正确推理路径。

Result: 在多个基准测试上，RePO显著提升难样本利用效率，性能超越现有基线，达到当前最优水平。

Conclusion: RePO成功协调了离策略知识吸收的有效性与在线策略RL的训练稳定性，在领域特定数据对齐任务中展现出优越性。

Abstract: Aligning large language models (LLMs) on domain-specific data remains a fundamental challenge. Supervised fine-tuning (SFT) offers a straightforward way to inject domain knowledge but often degrades the model's generality. In contrast, on-policy reinforcement learning (RL) preserves generality but fails to effectively assimilate hard samples that exceed the model's current reasoning level. Recent off-policy RL attempts improve hard sample utilization, yet they suffer from severe training instability due to the forced distribution shift toward off-policy knowledge. To reconcile effective off-policy knowledge absorption with the stability of on-policy RL, we propose Rephrasing Policy Optimization (RePO). In RePO, the policy model is prompted to first comprehend off-policy knowledge and then rephrase it into trajectories that conform to its own stylistic and parametric distribution. RePO dynamically replaces low-reward rollouts with these rephrased, high-quality trajectories. This strategy guides the model toward correct reasoning paths while strictly preserving on-policy training dynamics. Experiments on several benchmarks demonstrate that RePO improves hard-sample utilization and outperforms existing baselines, achieving state-of-the-art performance.

</details>


### [281] [Adaptive Sampling for Private Worst-Case Group Optimization](https://arxiv.org/abs/2602.10820)
*Max Cairney-Leeming,Amartya Sanyal,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 本文提出了一种名为ASC的新算法，用于差分隐私下的最坏情况组优化，通过自适应控制各组的采样率和裁剪阈值，在保证各组隐私一致的同时提升最坏组性能。


<details>
  <summary>Details</summary>
Motivation: 平均损失最小化模型在小规模或难学习数据组上表现不佳；而现有针对最坏组优化的加权方法在差分隐私下会导致各组隐私保障不均，尤其损害少数群体隐私。

Method: 提出ASC（Adaptively Sampled and Clipped Worst-case Group Optimization）算法，自适应调整每组的采样率与梯度裁剪阈值，使难学组被更频繁采样，同时维持各组一致的隐私预算分配。

Result: 相比先前方法，ASC实现了更低方差的梯度估计、更紧的隐私保证，以及显著更高的最坏组准确率，且不牺牲整体平均准确率。

Conclusion: ASC在差分隐私约束下有效平衡了群体公平性与隐私一致性，为鲁棒且公平的隐私机器学习提供了新路径。

Abstract: Models trained by minimizing the average loss often fail to be accurate on small or hard-to-learn groups of the data. Various methods address this issue by optimizing a weighted objective that focuses on the worst-performing groups. However, this approach becomes problematic when learning with differential privacy, as unequal data weighting can result in inhomogeneous privacy guarantees, in particular weaker privacy for minority groups. In this work, we introduce a new algorithm for differentially private worst-case group optimization called ASC (Adaptively Sampled and Clipped Worst-case Group Optimization). It adaptively controls both the sampling rate and the clipping threshold of each group. Thereby, it allows for harder-to-learn groups to be sampled more often while ensuring consistent privacy guarantees across all groups. Comparing ASC to prior work, we show that it results in lower-variance gradients, tighter privacy guarantees, and substantially higher worst-case group accuracy without sacrificing overall average accuracy.

</details>


### [282] [SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios](https://arxiv.org/abs/2602.10840)
*Yanan Wang,Renxi Wang,Yongxin Wang,Xuezhi Liang,Fajri Koto,Timothy Baldwin,Xiaodan Liang,Haonan Li*

Main category: cs.LG

TL;DR: 本文提出了SimuScene，首个系统性研究大型语言模型（LLMs）在物理场景代码模拟任务上的能力，涵盖5个物理领域、52个物理概念；构建了含7659个场景的数据集（含334个人工验证测试样本），发现当前最强LLM仅达21.5%通过率；进一步提出基于视觉奖励的强化学习训练方法，显著提升物理模拟与通用代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在数学竞赛、编程和科学推理等任务上被广泛研究，但其在通过代码准确表示和模拟物理场景方面的能力仍缺乏系统探索。

Method: 构建自动数据采集流水线并辅以人工校验，形成覆盖5个物理领域、52个物理概念的SimuScene数据集（共7659个场景，334个测试样本）；评估10个主流LLM；提出基于视觉奖励（由多模态VLM作为裁判）的强化学习训练框架，用于提升文本模型的物理模拟能力。

Result: 最强LLM在该任务上仅取得21.5%的通过率；使用SimuScene数据训练后，模型在物理模拟及通用代码生成性能上均显著提升。

Conclusion: 物理场景的代码模拟对当前LLMs仍极具挑战性；SimuScene为该方向提供了首个基准与高质量数据集，并验证了视觉引导强化学习的有效性。

Abstract: Large language models (LLMs) have been extensively studied for tasks like math competitions, complex coding, and scientific reasoning, yet their ability to accurately represent and simulate physical scenarios via code remains underexplored. We propose SimuScene, the first systematic study that trains and evaluates LLMs on simulating physical scenarios across five physics domains and 52 physical concepts. We build an automatic pipeline to collect data, with human verification to ensure quality. The final dataset contains 7,659 physical scenarios with 334 human-verified examples as the test set. We evaluated 10 contemporary LLMs and found that even the strongest model achieves only a 21.5% pass rate, demonstrating the difficulty of the task. Finally, we introduce a reinforcement learning pipeline with visual rewards that uses a vision-language model as a judge to train textual models. Experiments show that training with our data improves physical simulation via code while substantially enhancing general code generation performance.

</details>


### [283] [Enhancing Multivariate Time Series Forecasting with Global Temporal Retrieval](https://arxiv.org/abs/2602.10847)
*Fanpu Cao,Lu Dai,Jindong Han,Hui Xiong*

Main category: cs.LG

TL;DR: 本文提出Global Temporal Retriever (GTR)，一种轻量、即插即用的模块，用于增强多变量时间序列预测模型对长周期全局模式的建模能力，无需扩大历史窗口，通过自适应全局时序嵌入与动态检索对齐，在不改变原模型结构前提下提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有MTSF模型受限于短历史窗口，难以捕捉远超输入长度的全局周期性模式，而简单延长窗口会导致过拟合、计算开销大和冗余信息等问题。

Method: 提出GTR模块：维护自适应的全局时序嵌入，动态检索并对其相关全局片段；结合2D卷积与残差融合联合建模局部与全局依赖关系。

Result: 在六个真实数据集上实验表明，GTR在短期和长期预测任务中均达到SOTA性能，且参数与计算开销极小。

Conclusion: GTR是一种高效、通用的解决方案，可显著提升多变量时间序列预测中对全局周期性的建模能力。

Abstract: Multivariate time series forecasting (MTSF) plays a vital role in numerous real-world applications, yet existing models remain constrained by their reliance on a limited historical context. This limitation prevents them from effectively capturing global periodic patterns that often span cycles significantly longer than the input horizon - despite such patterns carrying strong predictive signals. Naive solutions, such as extending the historical window, lead to severe drawbacks, including overfitting, prohibitive computational costs, and redundant information processing. To address these challenges, we introduce the Global Temporal Retriever (GTR), a lightweight and plug-and-play module designed to extend any forecasting model's temporal awareness beyond the immediate historical context. GTR maintains an adaptive global temporal embedding of the entire cycle and dynamically retrieves and aligns relevant global segments with the input sequence. By jointly modeling local and global dependencies through a 2D convolution and residual fusion, GTR effectively bridges short-term observations with long-term periodicity without altering the host model architecture. Extensive experiments on six real-world datasets demonstrate that GTR consistently delivers state-of-the-art performance across both short-term and long-term forecasting scenarios, while incurring minimal parameter and computational overhead. These results highlight GTR as an efficient and general solution for enhancing global periodicity modeling in MTSF tasks. Code is available at this repository: https://github.com/macovaseas/GTR.

</details>


### [284] [Time Series Foundation Models for Energy Load Forecasting on Consumer Hardware: A Multi-Dimensional Zero-Shot Benchmark](https://arxiv.org/abs/2602.10848)
*Luigi Simeone*

Main category: cs.LG

TL;DR: 本文对四种时间序列基础模型（TSFMs）在电力负荷预测任务上的性能进行了多维度基准评估，涵盖上下文长度敏感性、概率校准、分布偏移鲁棒性及可操作性分析，并与Prophet及统计模型对比；结果表明TSFMs在短上下文下仍保持稳定精度，部分模型显著优于基线，且校准性能差异明显。


<details>
  <summary>Details</summary>
Motivation: 评估时间序列基础模型（TSFMs）在电力需求预测等关键任务中的实际适用性，尤其关注其零样本能力是否满足准确性、校准性和鲁棒性要求。

Method: 构建多维基准测试框架，在ERCOT小时级负荷数据（2020–2024）上评估Chronos-Bolt、Chronos-2、Moirai-2、TinyTimeMixer四类TSFMs，对比Prophet、SARIMA和Seasonal Naive；实验在消费级硬件上运行，考察上下文长度敏感性、概率校准、分布偏移（如疫情封锁、冬季风暴Uri）鲁棒性及决策支持能力。

Result: TSFMs在长上下文（2048小时）下MASE达0.31，较Seasonal Naive提升47%；Prophet在短上下文（24小时）下失效（MASE>74），而TSFMs保持稳定；Chronos-2校准良好（95%实测覆盖率对应90%名义置信度），Moirai-2和Prophet则严重过自信（~70%覆盖率）。

Conclusion: TSFMs在电力负荷预测中展现出优于传统方法的零样本泛化能力与鲁棒性，但校准性能差异大，需结合任务需求谨慎选型；研究提供了实用选型指南并开源全部基准框架。

Abstract: Time Series Foundation Models (TSFMs) have introduced zero-shot prediction capabilities that bypass the need for task-specific training. Whether these capabilities translate to mission-critical applications such as electricity demand forecasting--where accuracy, calibration, and robustness directly affect grid operations--remains an open question. We present a multi-dimensional benchmark evaluating four TSFMs (Chronos-Bolt, Chronos-2, Moirai-2, and TinyTimeMixer) alongside Prophet as an industry-standard baseline and two statistical references (SARIMA and Seasonal Naive), using ERCOT hourly load data from 2020 to 2024. All experiments run on consumer-grade hardware (AMD Ryzen 7, 16GB RAM, no GPU). The evaluation spans four axes: (1) context length sensitivity from 24 to 2048 hours, (2) probabilistic forecast calibration, (3) robustness under distribution shifts including COVID-19 lockdowns and Winter Storm Uri, and (4) prescriptive analytics for operational decision support. The top-performing foundation models achieve MASE values near 0.31 at long context lengths (C = 2048h, day-ahead horizon), a 47% reduction over the Seasonal Naive baseline. The inclusion of Prophet exposes a structural advantage of pre-trained models: Prophet fails when the fitting window is shorter than its seasonality period (MASE > 74 at 24-hour context), while TSFMs maintain stable accuracy even with minimal context because they recognise temporal patterns learned during pre-training rather than estimating them from scratch. Calibration varies substantially across models--Chronos-2 produces well-calibrated prediction intervals (95% empirical coverage at 90% nominal level) while both Moirai-2 and Prophet exhibit overconfidence (~70% coverage). We provide practical model selection guidelines and release the complete benchmark framework for reproducibility.

</details>


### [285] [Automated Model Design using Gated Neuron Selection in Telecom](https://arxiv.org/abs/2602.10854)
*Adam Orucu,Marcus Medhage,Farnaz Moradi,Andreas Johnsson,Sarunas Girdzijauskas*

Main category: cs.LG

TL;DR: 本文提出了一种面向电信网络中表格数据的新型梯度式神经架构搜索（NAS）方法TabGNS，显著提升预测性能，同时大幅减小模型规模（51–82%）和搜索时间（最高达36倍）。


<details>
  <summary>Details</summary>
Motivation: 电信行业深度学习应用增长迅速，但为资源受限环境设计紧凑高效神经网络仍具挑战性，亟需自动化模型设计方法。

Method: 提出TabGNS（Tabular Gated Neuron Selection），一种专为电信网络表格数据定制的梯度式神经架构搜索方法。

Result: 在多个电信及通用表格数据集上验证，TabGNS在提升预测性能的同时，将模型大小减少51–82%，搜索时间加速最高达36倍。

Conclusion: TabGNS可集成至模型生命周期管理，实现神经网络的自动化设计，加快电信网络中机器学习方案的部署。

Abstract: The telecommunications industry is experiencing rapid growth in adopting deep learning for critical tasks such as traffic prediction, signal strength prediction, and quality of service optimisation. However, designing neural network architectures for these applications remains challenging and time-consuming, particularly when targeting compact models suitable for resource-constrained network environments. Therefore, there is a need for automating the model design process to create high-performing models efficiently. This paper introduces TabGNS (Tabular Gated Neuron Selection), a novel gradient-based Neural Architecture Search (NAS) method specifically tailored for tabular data in telecommunications networks. We evaluate TabGNS across multiple telecommunications and generic tabular datasets, demonstrating improvements in prediction performance while reducing the architecture size by 51-82% and reducing the search time by up to 36x compared to state-of-the-art tabular NAS methods. Integrating TabGNS into the model lifecycle management enables automated design of neural networks throughout the lifecycle, accelerating deployment of ML solutions in telecommunications networks.

</details>


### [286] [ICA: Information-Aware Credit Assignment for Visually Grounded Long-Horizon Information-Seeking Agents](https://arxiv.org/abs/2602.10863)
*Cong Pang,Xuyu Feng,Yujie Yi,Zixuan Chen,Jiawei Hong,Tiankuo Yao,Nang Yuan,Jiapeng Luo,Lewei Lu,Xin Lou*

Main category: cs.LG

TL;DR: 本文提出了一种视觉原生的搜索框架和信息感知信用分配（ICA）方法，通过网页视觉快照和后验密集信号传播，缓解开放网络环境中强化学习的信息检索信用分配瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的信息检索智能体在开放网络环境中受限于低信噪比反馈：文本解析器丢失布局语义并引入噪声，长周期训练依赖稀疏结果奖励，难以定位关键检索动作。

Method: 提出视觉原生搜索框架，将网页表示为视觉快照以利用布局线索；设计信息感知信用分配（ICA），通过后验分析估计各快照对最终结果的贡献，并将密集学习信号反传至关键搜索步骤；结合GRPO训练流程。

Result: 在多个信息检索基准上持续超越文本基线；验证了视觉快照建模与信息级信用分配可有效缓解信用分配瓶颈。

Conclusion: 视觉快照表征与细粒度信用分配相结合，是提升开放网络中信息检索智能体训练效率与性能的有效路径。

Abstract: Despite the strong performance achieved by reinforcement learning-trained information-seeking agents, learning in open-ended web environments remains severely constrained by low signal-to-noise feedback. Text-based parsers often discard layout semantics and introduce unstructured noise, while long-horizon training typically relies on sparse outcome rewards that obscure which retrieval actions actually matter. We propose a visual-native search framework that represents webpages as visual snapshots, allowing agents to leverage layout cues to quickly localize salient evidence and suppress distractors. To learn effectively from these high-dimensional observations, we introduce Information-Aware Credit Assignment (ICA), a post-hoc method that estimates each retrieved snapshot's contribution to the final outcome via posterior analysis and propagates dense learning signals back to key search turns. Integrated with a GRPO-based training pipeline, our approach consistently outperforms text-based baselines on diverse information-seeking benchmarks, providing evidence that visual snapshot grounding with information-level credit assignment alleviates the credit-assignment bottleneck in open-ended web environments. The code and datasets will be released in https://github.com/pc-inno/ICA_MM_deepsearch.git.

</details>


### [287] [The Sample Complexity of Uniform Approximation for Multi-Dimensional CDFs and Fixed-Price Mechanisms](https://arxiv.org/abs/2602.10868)
*Matteo Castiglioni,Anna Lunghi,Alberto Marchesi*

Main category: cs.LG

TL;DR: 本文研究在仅能获得单比特反馈（bandit feedback）的情况下，学习n维累积分布函数（CDF）的一致ε-近似的样本复杂度，发现其具有近似与维度无关的特性，仅在对数项中体现维度影响，并将结果应用于双边交易等小市场中的固定价格机制学习。


<details>
  <summary>Details</summary>
Motivation: 扩展经典的多变量DKW不等式（适用于全反馈情形）到更现实的单比特反馈（bandit feedback）设定，以刻画在信息受限下学习高维CDF的统计极限。

Method: 通过构造精细网格上的单比特查询策略，结合概率集中不等式和组合覆盖论证，推导一致逼近所需的最小样本量。

Result: 获得样本复杂度上界为\frac{1}{ε^3} \log(1/ε)^{\mathcal{O}(n)}，证明维度n仅以对数形式影响复杂度；并由此导出小市场中固定价格机制学习的紧样本界与新型遗憾界。

Conclusion: 在单比特反馈下，高维CDF的一致学习仍具可行性，其代价主要由精度ε主导，维度惩罚较温和；该结论为带有限反馈的机制设计与统计学习提供了理论基础。

Abstract: We study the sample complexity of learning a uniform approximation of an $n$-dimensional cumulative distribution function (CDF) within an error $ε> 0$, when observations are restricted to a minimal one-bit feedback. This serves as a counterpart to the multivariate DKW inequality under ''full feedback'', extending it to the setting of ''bandit feedback''. Our main result shows a near-dimensional-invariance in the sample complexity: we get a uniform $ε$-approximation with a sample complexity $\frac{1}{ε^3}{\log\left(\frac 1 ε\right)^{\mathcal{O}(n)}}$ over a arbitrary fine grid, where the dimensionality $n$ only affects logarithmic terms. As direct corollaries, we provide tight sample complexity bounds and novel regret guarantees for learning fixed-price mechanisms in small markets, such as bilateral trade settings.

</details>


### [288] [FedPS: Federated data Preprocessing via aggregated Statistics](https://arxiv.org/abs/2602.10870)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 本文提出FedPS框架，用于在联邦学习中实现隐私保护且通信高效的分布式数据预处理，支持特征缩放、编码、离散化和缺失值插补等操作，并扩展了k-Means等模型至水平与垂直FL场景。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据预处理被严重忽视，而实际系统中因隐私限制无法集中原始数据，且通信效率要求高，亟需分布式预处理方案。

Method: 提出基于聚合统计的统一联邦预处理框架FedPS，利用数据草图（data-sketching）技术高效汇总本地数据统计信息，并据此设计多种联邦预处理算法及扩展相关模型。

Result: 实现了灵活、通信高效、一致的联邦预处理流程，支持水平与垂直FL设置下的多种预处理任务及模型（如k-Means、k-NN、贝叶斯线性回归）。

Conclusion: FedPS填补了联邦学习中预处理环节的研究空白，为实际FL部署提供了实用、可扩展且隐私安全的预处理解决方案。

Abstract: Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments.

</details>


### [289] [Resource-Efficient Model-Free Reinforcement Learning for Board Games](https://arxiv.org/abs/2602.10894)
*Kazuki Ota,Takayuki Osa,Motoki Omura,Tatsuya Harada*

Main category: cs.LG

TL;DR: 本文提出了一种面向棋类游戏的无模型强化学习算法，相比AlphaZero等基于搜索的方法更高效，并在五种棋类游戏中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于搜索的强化学习方法（如AlphaZero）计算开销大，影响可复现性，亟需更高效的替代方案。

Method: 提出一种专为棋类游戏设计的无模型强化学习算法，并通过在多种棋类环境中的实验及消融研究验证其核心组件的有效性。

Result: 在Animal Shogi、Gardner Chess、Go、Hex和Othello五种棋类游戏中，该方法均展现出比现有方法更高的学习效率。

Conclusion: 无模型强化学习在传统上由搜索主导的棋类领域具有应用潜力，本工作为其提供了高效可行的新路径。

Abstract: Board games have long served as complex decision-making benchmarks in artificial intelligence. In this field, search-based reinforcement learning methods such as AlphaZero have achieved remarkable success. However, their significant computational demands have been pointed out as barriers to their reproducibility. In this study, we propose a model-free reinforcement learning algorithm designed for board games to achieve more efficient learning. To validate the efficiency of the proposed method, we conducted comprehensive experiments on five board games: Animal Shogi, Gardner Chess, Go, Hex, and Othello. The results demonstrate that the proposed method achieves more efficient learning than existing methods across these environments. In addition, our extensive ablation study shows the importance of core techniques used in the proposed method. We believe that our efficient algorithm shows the potential of model-free reinforcement learning in domains traditionally dominated by search-based methods.

</details>


### [290] [Natural Hypergradient Descent: Algorithm Design, Convergence Analysis, and Parallel Implementation](https://arxiv.org/abs/2602.10905)
*Deyi Kong,Zaiwei Chen,Shuzhong Zhang,Shancong Mou*

Main category: cs.LG

TL;DR: 本文提出了自然超梯度下降（NHGD）方法，通过使用经验Fisher信息矩阵替代Hessian逆来高效求解双层优化问题，在保证理论性能的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决双层优化中超梯度估计的计算瓶颈，特别是Hessian逆的计算或近似问题。

Method: 提出自然超梯度下降（NHGD），利用经验Fisher信息矩阵作为Hessian的渐近一致替代，并设计并行的“优化与近似同步”框架，复用梯度信息以极低成本更新Hessian逆近似。

Result: 理论方面：给出了高概率误差界和样本复杂度保证，与当前最优的‘先优化后近似’方法相当；实验方面：在典型双层学习任务上验证了NHGD的可扩展性和有效性，尤其适用于大规模机器学习场景。

Conclusion: NHGD在不牺牲理论保证的前提下，显著提升了双层优化的计算效率，是一种兼具理论严谨性与实用性的新方法。

Abstract: In this work, we propose Natural Hypergradient Descent (NHGD), a new method for solving bilevel optimization problems. To address the computational bottleneck in hypergradient estimation--namely, the need to compute or approximate Hessian inverse--we exploit the statistical structure of the inner optimization problem and use the empirical Fisher information matrix as an asymptotically consistent surrogate for the Hessian. This design enables a parallel optimize-and-approximate framework in which the Hessian-inverse approximation is updated synchronously with the stochastic inner optimization, reusing gradient information at negligible additional cost. Our main theoretical contribution establishes high-probability error bounds and sample complexity guarantees for NHGD that match those of state-of-the-art optimize-then-approximate methods, while significantly reducing computational time overhead. Empirical evaluations on representative bilevel learning tasks further demonstrate the practical advantages of NHGD, highlighting its scalability and effectiveness in large-scale machine learning settings.

</details>


### [291] [Tuning the burn-in phase in training recurrent neural networks improves their performance](https://arxiv.org/abs/2602.10911)
*Julian D. Schiller,Malte Heinrich,Victor G. Lopez,Matthias A. Müller*

Main category: cs.LG

TL;DR: 本文研究了在时间序列任务中使用截断BPTT训练RNN时的理论性能界限，指出RNN的预热阶段（burn-in phase）是关键调参因素，并通过实验验证其对预测误差的显著影响。


<details>
  <summary>Details</summary>
Motivation: 标准BPTT在处理长序列时计算和内存开销大，截断BPTT虽实用，但缺乏对其性能损失的理论刻画。

Method: 建立截断BPTT下RNN优化的理论误差界，重点分析burn-in阶段的影响，并在系统辨识与时间序列预测基准上进行实验验证。

Result: 理论证明burn-in阶段显著影响性能保证；实验显示合理调优burn-in可使训练/测试预测误差降低超60%。

Conclusion: burn-in阶段是截断BPTT训练RNN的关键设计自由度，其理论分析与实证结果为高效RNN训练提供了新指导。

Abstract: Training recurrent neural networks (RNNs) with standard backpropagation through time (BPTT) can be challenging, especially in the presence of long input sequences. A practical alternative to reduce computational and memory overhead is to perform BPTT repeatedly over shorter segments of the training data set, corresponding to truncated BPTT. In this paper, we examine the training of RNNs when using such a truncated learning approach for time series tasks. Specifically, we establish theoretical bounds on the accuracy and performance loss when optimizing over subsequences instead of the full data sequence. This reveals that the burn-in phase of the RNN is an important tuning knob in its training, with significant impact on the performance guarantees. We validate our theoretical results through experiments on standard benchmarks from the fields of system identification and time series forecasting. In all experiments, we observe a strong influence of the burn-in phase on the training process, and proper tuning can lead to a reduction of the prediction error on the training and test data of more than 60% in some cases.

</details>


### [292] [Near-Constant Strong Violation and Last-Iterate Convergence for Online CMDPs via Decaying Safety Margins](https://arxiv.org/abs/2602.10917)
*Qian Zuo,Zhiyong Wang,Fengxiang He*

Main category: cs.LG

TL;DR: 本文提出FlexDOME算法，首次在CMDP中实现近常数强约束违反（Õ(1)）、次线性强奖励遗憾及非渐近最后迭代收敛，通过时变安全裕度与正则化项克服现有对偶方法的振荡与收敛限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于原始-对偶的在线强化学习方法在强遗憾和强违反度量下，难以同时保证次线性奖励遗憾与有界约束违反，且常受限于平均迭代收敛或违反持续增长。

Method: 提出FlexDOME算法：在原始-对偶框架中引入时变安全裕度和边际正则化探索机制；采用逐项渐近主导分析策略，使安全裕度主导优化与统计误差衰减速率；结合策略-对偶Lyapunov函数证明非渐近最后迭代收敛。

Result: 理论证明FlexDOME achieves Õ(1) strong constraint violation, sublinear strong reward regret, and non-asymptotic last-iterate convergence；实验验证了理论结果。

Conclusion: FlexDOME是首个在强度量下同时实现近常数约束违反、次线性遗憾与非渐近最后迭代收敛的在线安全RL算法，为CMDP的安全学习提供了新范式。

Abstract: We study safe online reinforcement learning in Constrained Markov Decision Processes (CMDPs) under strong regret and violation metrics, which forbid error cancellation over time. Existing primal-dual methods that achieve sublinear strong reward regret inevitably incur growing strong constraint violation or are restricted to average-iterate convergence due to inherent oscillations. To address these limitations, we propose the Flexible safety Domain Optimization via Margin-regularized Exploration (FlexDOME) algorithm, the first to provably achieve near-constant $\tilde{O}(1)$ strong constraint violation alongside sublinear strong regret and non-asymptotic last-iterate convergence. FlexDOME incorporates time-varying safety margins and regularization terms into the primal-dual framework. Our theoretical analysis relies on a novel term-wise asymptotic dominance strategy, where the safety margin is rigorously scheduled to asymptotically majorize the functional decay rates of the optimization and statistical errors, thereby clamping cumulative violations to a near-constant level. Furthermore, we establish non-asymptotic last-iterate convergence guarantees via a policy-dual Lyapunov argument. Experiments corroborate our theoretical findings.

</details>


### [293] [Spatial-Morphological Modeling for Multi-Attribute Imputation of Urban Blocks](https://arxiv.org/abs/2602.10923)
*Vasilii Starikov,Ruslan Kozliak,Georgii Kontsevik,Sergey Mityagin*

Main category: cs.LG

TL;DR: 本文提出了一种空间形态学（SM）插补工具，结合形态聚类与邻域空间方法，用于城市街区尺度的FSI和GSI缺失值重建，性能优于现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 准确重建城市缺失的形态学指标对城市规划和数据驱动分析至关重要。

Method: 提出空间形态学（SM）插补工具，融合数据驱动的形态聚类与基于邻域的空间方法（如IDW、sKNN），并以SpaceMatrix框架为灵感，结合城市尺度形态模式（全局先验）与局部空间信息（上下文依赖插值）。

Result: 评估表明，SM与IDW或sKNN结合的方法性能优于现有最先进模型；复合方法体现了形态学与空间方法的互补优势。

Conclusion: 融合全局形态先验与局部空间信息的复合插补策略，在城市形态指标重建任务中具有显著优势，为城市数据修复提供了新范式。

Abstract: Accurate reconstruction of missing morphological indicators of a city is crucial for urban planning and data-driven analysis. This study presents the spatial-morphological (SM) imputer tool, which combines data-driven morphological clustering with neighborhood-based methods to reconstruct missing values of the floor space index (FSI) and ground space index (GSI) at the city block level, inspired by the SpaceMatrix framework. This approach combines city-scale morphological patterns as global priors with local spatial information for context-dependent interpolation. The evaluation shows that while SM alone captures meaningful morphological structure, its combination with inverse distance weighting (IDW) or spatial k-nearest neighbor (sKNN) methods provides superior performance compared to existing SOTA models. Composite methods demonstrate the complementary advantages of combining morphological and spatial approaches.

</details>


### [294] [CMAD: Cooperative Multi-Agent Diffusion via Stochastic Optimal Control](https://arxiv.org/abs/2602.10933)
*Riccardo Barbano,Alexander Denker,Zeljko Kereta,Runchang Li,Francisco Vargas*

Main category: cs.LG

TL;DR: 本文提出一种新的连续时间生成模型组合范式，将多个预训练扩散模型视为协同工作的智能体，通过随机最优控制联合引导其扩散轨迹，以实现对组合生成过程的可控性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将模型组合视为概率密度的代数运算（如乘积或专家混合），但该假设要求目标分布已知，而现实中几乎不可行。

Method: 将预训练扩散模型建模为相互作用的智能体，将其扩散轨迹建模为随机过程，并将组合生成建模为协同随机最优控制问题，通过优化联合轨迹实现共享目标。

Result: 在条件MNIST生成任务上验证了该框架的有效性，并优于基于每步梯度引导的DPS风格基线方法。

Conclusion: 将组合生成重新定义为协同最优控制问题，提供了一种更自然、可控且无需显式目标分布假设的新范式。

Abstract: Continuous-time generative models have achieved remarkable success in image restoration and synthesis. However, controlling the composition of multiple pre-trained models remains an open challenge. Current approaches largely treat composition as an algebraic composition of probability densities, such as via products or mixtures of experts. This perspective assumes the target distribution is known explicitly, which is almost never the case. In this work, we propose a different paradigm that formulates compositional generation as a cooperative Stochastic Optimal Control problem. Rather than combining probability densities, we treat pre-trained diffusion models as interacting agents whose diffusion trajectories are jointly steered, via optimal control, toward a shared objective defined on their aggregated output. We validate our framework on conditional MNIST generation and compare it against a naive inference-time DPS-style baseline replacing learned cooperative control with per-step gradient guidance.

</details>


### [295] [Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink](https://arxiv.org/abs/2602.10956)
*Victoria Hankemeier,Malte Hankemeier*

Main category: cs.LG

TL;DR: 本文分析了时序注意力机制中的偏差问题，理论推导了其雅可比矩阵的敏感性界，揭示了随序列长度增长而加剧的对角线注意力汇聚现象，并提出了有效的正则化方法。


<details>
  <summary>Details</summary>
Motivation: 时空模型易受空间与时间间信息退化影响，已有研究指出因果注意力或时序卷积中的过压缩现象会导致对初始token的偏差；本文旨在探究该偏差是否同样存在于时序注意力机制中。

Method: 推导时序注意力层雅可比矩阵期望值的敏感性界，理论分析非对角线注意力分数与序列长度的关系，并提出针对性正则化方法。

Result: 理论上证实时序注意力矩阵存在对角线注意力汇聚现象，且该现象随序列长度增加而加剧；实验验证所提正则化方法有效缓解该偏差。

Conclusion: 时序注意力机制存在固有偏差，表现为对角线注意力汇聚，需通过正则化加以抑制，以提升模型建模长时序动态的能力。

Abstract: Spatio-temporal models analyze spatial structures and temporal dynamics, which makes them prone to information degeneration among space and time. Prior literature has demonstrated that over-squashing in causal attention or temporal convolutions creates a bias on the first tokens. To analyze whether such a bias is present in temporal attention mechanisms, we derive sensitivity bounds on the expected value of the Jacobian of a temporal attention layer. We theoretically show how off-diagonal attention scores depend on the sequence length, and that temporal attention matrices suffer a diagonal attention sink. We suggest regularization methods, and experimentally demonstrate their effectiveness.

</details>


### [296] [MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.10965)
*Yupu Gu,Rongzhe Wei,Andy Zhu,Pan Li*

Main category: cs.LG

TL;DR: 本文提出了MoEEdit，首个针对稀疏Mixture-of-Experts（MoE）大语言模型的知识编辑框架，通过每专家零空间投影实现路由稳定更新，兼顾编辑效果、泛化性、特异性与计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要面向稠密模型，难以适配日益普及的稀疏MoE架构；直接迁移会导致计算开销大、路由分布偏移，损害稳定性与一致性。

Method: 提出MoEEdit框架：利用每专家的零空间投影重参数化专家更新，保持路由器输入不变以抑制路由偏移；采用分块坐标下降（BCD）求解器高效优化该分块结构化问题。

Result: 实验表明MoEEdit在有效性、泛化性、特异性和路由稳定性上达到SOTA，同时具备更优的计算与内存效率。

Conclusion: MoEEdit为稀疏大模型提供了可扩展、高精度的知识编辑基础，凸显了路由稳定干预的关键作用。

Abstract: Knowledge editing (KE) enables precise modifications to factual content in large language models (LLMs). Existing KE methods are largely designed for dense architectures, limiting their applicability to the increasingly prevalent sparse Mixture-of-Experts (MoE) models that underpin modern scalable LLMs. Although MoEs offer strong efficiency and capacity scaling, naively adapting dense-model editors is both computationally costly and prone to routing distribution shifts that undermine stability and consistency. To address these challenges, we introduce MoEEdit, the first routing-stable framework for parameter-modifying knowledge editing in MoE LLMs. Our method reparameterizes expert updates via per-expert null-space projections that keep router inputs invariant and thereby suppress routing shifts. The resulting block-structured optimization is solved efficiently with a block coordinate descent (BCD) solver. Experiments show that MoEEdit attains state-of-the-art efficacy and generalization while preserving high specificity and routing stability, with superior compute and memory efficiency. These results establish a robust foundation for scalable, precise knowledge editing in sparse LLMs and underscore the importance of routing-stable interventions.

</details>


### [297] [A Jointly Efficient and Optimal Algorithm for Heteroskedastic Generalized Linear Bandits with Adversarial Corruptions](https://arxiv.org/abs/2602.10971)
*Sanghwa Kim,Junghyun Lee,Se-Young Yun*

Main category: cs.LG

TL;DR: 本文提出了HCW-GLB-OMD算法，用于解决具有对抗性污染的异方差广义线性带臂问题（GLBs），通过在线镜像下降估计器与Hessian加权置信区间实现鲁棒性，具备高效计算特性（每轮O(1)时空复杂度），并在自协调假设下达到接近实例最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时处理异方差性、广义线性结构及对抗性污染三重挑战，缺乏统一且鲁棒的理论与算法框架。

Method: 提出HCW-GLB-OMD：结合在线镜像下降（OMD）估计器与基于Hessian的置信权重机制，在自协调链接函数假设下构建鲁棒置信域。

Result: 获得上界 regret = Õ(d√∑ₜ g(τₜ)μ̇ₜ,⋆ + d²gₘₐₓκ + dκC)，并给出匹配的下界 Ω̃(d√∑ₜ g(τₜ)μ̇ₜ,⋆ + dC)，证明其在污染项上至多差κ因子即达实例最优。

Conclusion: HCW-GLB-OMD是首个在多种异方差GLB设定（如线性、逻辑、泊松带臂）中对对抗污染具备统一鲁棒性与实例最优性的高效算法。

Abstract: We consider the problem of heteroskedastic generalized linear bandits (GLBs) with adversarial corruptions, which subsumes various stochastic contextual bandit settings, including heteroskedastic linear bandits and logistic/Poisson bandits. We propose HCW-GLB-OMD, which consists of two components: an online mirror descent (OMD)-based estimator and Hessian-based confidence weights to achieve corruption robustness. This is computationally efficient in that it only requires ${O}(1)$ space and time complexity per iteration. Under the self-concordance assumption on the link function, we show a regret bound of $\tilde{O}\left( d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d^2 g_{\max} κ+ d κC \right)$, where $\dotμ_{t,\star}$ is the slope of $μ$ around the optimal arm at time $t$, $g(τ_t)$'s are potentially exogenously time-varying dispersions (e.g., $g(τ_t) = σ_t^2$ for heteroskedastic linear bandits, $g(τ_t) = 1$ for Bernoulli and Poisson), $g_{\max} = \max_{t \in [T]} g(τ_t)$ is the maximum dispersion, and $C \geq 0$ is the total corruption budget of the adversary. We complement this with a lower bound of $\tildeΩ(d \sqrt{\sum_t g(τ_t) \dotμ_{t,\star}} + d C)$, unifying previous problem-specific lower bounds. Thus, our algorithm achieves, up to a $κ$-factor in the corruption term, instance-wise minimax optimality simultaneously across various instances of heteroskedastic GLBs with adversarial corruptions.

</details>


### [298] [RiemannGL: Riemannian Geometry Changes Graph Deep Learning](https://arxiv.org/abs/2602.10982)
*Li Sun,Qiqi Wan,Suyang Zhou,Zhenhao Huang,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出黎曼几何是图表示学习的原理性和必要性基础，主张将黎曼图学习视为统一范式，并指出当前研究局限于双曲空间等少数流形及外在流形建模，强调需发展内蕴流形结构的图神经网络；为此，文章从流形类型、神经架构与学习范式三方面提出系统性研究议程，并探讨理论基础与开放挑战。


<details>
  <summary>Details</summary>
Motivation: 图具有典型的非欧几里得结构，现有图学习方法缺乏统一的几何基础；而黎曼几何能为建模复杂对象交互提供原理性支撑，但当前研究多局限于特定流形（如双曲空间）和外在建模方式，忽视内蕴流形结构的构建。

Method: 通过概念分析与范式梳理，识别现有黎曼图学习在流形类型、神经架构和学习范式三方面的关键缺口，并提出结构化研究议程；强调应转向内蕴流形结构的图神经网络设计。

Result: 确立了黎曼图学习作为统一范式的地位，明确了以‘内蕴流形结构’为核心的研究方向，并系统勾勒出涵盖 manifold type、neural architecture 和 learning paradigm 的三维研究框架。

Conclusion: 黎曼几何不应仅作为若干孤立技术的补充，而应成为图学习的基础性框架；推动内蕴建模、拓展流形类别、夯实理论基础，是释放黎曼图学习全部潜力的关键路径。

Abstract: Graphs are ubiquitous, and learning on graphs has become a cornerstone in artificial intelligence and data mining communities. Unlike pixel grids in images or sequential structures in language, graphs exhibit a typical non-Euclidean structure with complex interactions among the objects. This paper argues that Riemannian geometry provides a principled and necessary foundation for graph representation learning, and that Riemannian graph learning should be viewed as a unifying paradigm rather than a collection of isolated techniques. While recent studies have explored the integration of graph learning and Riemannian geometry, most existing approaches are limited to a narrow class of manifolds, particularly hyperbolic spaces, and often adopt extrinsic manifold formulations. We contend that the central mission of Riemannian graph learning is to endow graph neural networks with intrinsic manifold structures, which remains underexplored. To advance this perspective, we identify key conceptual and methodological gaps in existing approaches and outline a structured research agenda along three dimensions: manifold type, neural architecture, and learning paradigm. We further discuss open challenges, theoretical foundations, and promising directions that are critical for unlocking the full potential of Riemannian graph learning. This paper aims to provide a coherent viewpoint and to stimulate broader exploration of Riemannian geometry as a foundational framework for future graph learning research.

</details>


### [299] [Sample Efficient Generative Molecular Optimization with Joint Self-Improvement](https://arxiv.org/abs/2602.10984)
*Serra Korkmaz,Adam Izdebski,Jonathan Pirnay,Rasmus Møller-Larsen,Michal Kmicikiewicz,Pankhil Gawade,Dominik G. Grimm,Ewa Szczurek*

Main category: cs.LG

TL;DR: 本文提出了一种名为Joint Self-Improvement的方法，通过联合生成-预测模型与自改进采样策略，缓解分布偏移并提升样本效率，从而在有限评估预算下实现更优的分子优化性能。


<details>
  <summary>Details</summary>
Motivation: 生成式分子优化中候选分子稀有且评估代价高，导致样本效率至关重要；同时，用于预测分子性质的代理模型易受分布偏移影响，因优化过程使生成分子不断偏离训练分布。

Method: 提出Joint Self-Improvement框架：(i) 构建联合生成-预测模型，使生成器与代理模型对齐以缓解分布偏移；(ii) 设计自改进采样方案，在推理时用预测模块引导生成模块，高效生成优化分子。

Result: 在离线和在线分子优化基准实验中，该方法在有限评估预算下显著优于当前最先进方法。

Conclusion: Joint Self-Improvement通过联合建模与自适应采样，有效提升了生成式分子优化的样本效率与鲁棒性，为解决分布偏移与评估昂贵问题提供了新思路。

Abstract: Generative molecular optimization aims to design molecules with properties surpassing those of existing compounds. However, such candidates are rare and expensive to evaluate, yielding sample efficiency essential. Additionally, surrogate models introduced to predict molecule evaluations, suffer from distribution shift as optimization drives candidates increasingly out-of-distribution. To address these challenges, we introduce Joint Self-Improvement, which benefits from (i) a joint generative-predictive model and (ii) a self-improving sampling scheme. The former aligns the generator with the surrogate, alleviating distribution shift, while the latter biases the generative part of the joint model using the predictive one to efficiently generate optimized molecules at inference-time. Experiments across offline and online molecular optimization benchmarks demonstrate that Joint Self-Improvement outperforms state-of-the-art methods under limited evaluation budgets.

</details>


### [300] [TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents](https://arxiv.org/abs/2602.10986)
*Abhishek Vijaya Kumar,Bhaskar Kataria,Byungsoo Oh,Emaad Manzoor,Rachee Singh*

Main category: cs.LG

TL;DR: 本文提出了TVCACHE，一种用于LLM智能体RL后训练的状态化工具值缓存机制，通过维护工具调用序列树并进行最长前缀匹配，确保环境状态一致的前提下实现高效缓存，显著降低工具调用延迟和GPU空闲时间，且不损害奖励性能。


<details>
  <summary>Details</summary>
Motivation: 在LLM智能体的强化学习后训练中，外部工具调用耗时长，导致GPU闲置、训练成本高；虽然存在重复调用，但因工具输出依赖于动态环境状态，简单缓存不可行。

Method: 提出TVCACHE：构建并维护已观察到的工具调用序列树，缓存时以完整工具历史为键，通过最长前缀匹配判断缓存命中，从而保证环境状态完全一致。

Result: 在终端任务、SQL生成和视频理解三个任务上，缓存命中率最高达70%，中位数工具调用执行时间最多降低6.9倍，且后训练奖励无下降。

Conclusion: TVCACHE是一种安全、高效的状态感知缓存机制，能显著加速LLM智能体的RL后训练过程，具备实际部署价值。

Abstract: In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.

</details>


### [301] [OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories](https://arxiv.org/abs/2602.11018)
*Returaj Burnwal,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 本文提出了一种名为OSIL的离线安全模仿学习算法，通过非偏好轨迹推断安全性，在不依赖显式安全成本和奖励标注的情况下，学习满足安全约束且高回报的策略。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中，在线学习存在风险，且难以准确指定安全成本；但收集反映不安全行为的非偏好轨迹是可行的。

Method: 将安全策略学习建模为约束马尔可夫决策过程（CMDP），通过推导奖励最大化目标的下界，并学习估计非偏好行为可能性的成本模型，实现从离线非偏好演示中推断安全约束。

Result: 实验表明，OSIL能学习出满足成本约束且不牺牲奖励性能的更安全策略，优于多个基线方法。

Conclusion: OSIL为离线安全模仿学习提供了一种有效范式，仅需非偏好轨迹即可兼顾安全性与任务表现。

Abstract: This work addresses the problem of offline safe imitation learning (IL), where the goal is to learn safe and reward-maximizing policies from demonstrations that do not have per-timestep safety cost or reward information. In many real-world domains, online learning in the environment can be risky, and specifying accurate safety costs can be difficult. However, it is often feasible to collect trajectories that reflect undesirable or unsafe behavior, implicitly conveying what the agent should avoid. We refer to these as non-preferred trajectories. We propose a novel offline safe IL algorithm, OSIL, that infers safety from non-preferred demonstrations. We formulate safe policy learning as a Constrained Markov Decision Process (CMDP). Instead of relying on explicit safety cost and reward annotations, OSIL reformulates the CMDP problem by deriving a lower bound on reward maximizing objective and learning a cost model that estimates the likelihood of non-preferred behavior. Our approach allows agents to learn safe and reward-maximizing behavior entirely from offline demonstrations. We empirically demonstrate that our approach can learn safer policies that satisfy cost constraints without degrading the reward performance, thus outperforming several baselines.

</details>


### [302] [When Fusion Helps and When It Breaks: View-Aligned Robustness in Same-Source Financial Imaging](https://arxiv.org/abs/2602.11020)
*Rui Ma*

Main category: cs.LG

TL;DR: 本文研究了多视图学习与对抗鲁棒性在金融图像表示的次日方向预测中的应用，通过构建价格/成交量图表与技术指标矩阵两种视图，在上海黄金交易所数据上进行实验，发现标签噪声对结果影响显著，并提出基于最小变动过滤的评估子集构建方法；融合策略效果依赖于数据噪声水平，晚期融合在干净性能和鲁棒性方面表现更优，但联合攻击仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 提升金融时间序列方向预测的准确性与鲁棒性，尤其在存在标签模糊（如小幅波动导致方向难判）和多源异构视图信息下的建模能力。

Method: 构建OHLCV图表与技术指标矩阵双视图；采用防泄露的时间块划分与MCC评估；引入后验最小变动过滤（ex-post minimum-movement filter）控制标签噪声；比较早期融合（通道堆叠）与晚期融合（双编码器+融合头），并加入跨视图一致性正则化；在测试时使用FGSM和PGD进行L-infinity对抗扰动评估，区分单视图与联合视图攻击场景。

Result: 标签噪声水平显著影响模型性能，最小变动过滤可揭示预测信号但带来样本量-方差权衡；晚期融合在干净数据上性能最优且提升单视图攻击下的鲁棒性；模型对微小扰动高度敏感，联合攻击仍导致严重性能下降；跨视图一致性正则化效果次要且依赖骨干网络。

Conclusion: 多视图学习需适配标签噪声特性，晚期融合是兼顾预测精度与部分对抗鲁棒性的更优范式；但当前方法对联合多视图攻击防御能力有限，需进一步研究强鲁棒融合机制。

Abstract: We study same-source multi-view learning and adversarial robustness for next-day direction prediction with financial image representations. On Shanghai Gold Exchange (SGE) spot gold data (2005-2025), we construct two window-aligned views from each rolling window: an OHLCV-rendered price/volume chart and a technical-indicator matrix. To ensure reliable evaluation, we adopt leakage-resistant time-block splits with embargo and use Matthews correlation coefficient (MCC). We find that results depend strongly on the label-noise regime: we apply an ex-post minimum-movement filter that discards samples with realized next-day absolute return below tau to define evaluation subsets with reduced near-zero label ambiguity. This induces a non-monotonic data-noise trade-off that can reveal predictive signal but eventually increases variance as sample size shrinks; the filter is used for offline benchmark construction rather than an inference-time decision rule. In the stabilized subsets, fusion is regime dependent: early fusion by channel stacking can exhibit negative transfer, whereas late fusion with dual encoders and a fusion head provides the dominant clean-performance gains; cross-view consistency regularization has secondary, backbone-dependent effects. We further evaluate test-time L-infinity perturbations using FGSM and PGD under two threat scenarios: view-constrained attacks that perturb one view and joint attacks that perturb both. We observe severe vulnerability at tiny budgets with strong view asymmetry. Late fusion consistently improves robustness under view-constrained attacks, but joint attacks remain challenging and can still cause substantial worst-case degradation.

</details>


### [303] [Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models](https://arxiv.org/abs/2602.11057)
*Xinyu Yuan,Yan Qiao,Zonghui Wang,Wenzhi Chen*

Main category: cs.LG

TL;DR: 本文提出Pram，首个利用多模态语言模型（MLM）解决多商品流（MCF）问题的机器学习方法，在保证高质量解的同时显著提升求解速度，并具备鲁棒性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有优化引擎难以在大规模资源分配系统中兼顾最优性与可扩展性，亟需一种兼顾性能与效率的新方法。

Method: Pram将MCF问题分解为局部子问题，由MLM驱动的‘智能体’快速求解，并通过多智能体强化学习协调全局一致性；理论证明其能模拟梯度下降并收敛至最优。

Result: 在真实数据和公开拓扑上，Pram性能接近线性规划求解器（近最优），运行时间快1–2个数量级，且在链路故障或流量突发下性能下降<10%。

Conclusion: Pram是一种目标无关、可无缝集成、兼具高效性、鲁棒性与泛化能力的实用MCF求解框架，适用于未来网络。

Abstract: The multi-commodity flow (MCF) problem is a fundamental topic in network flow and combinatorial optimization, with broad applications in transportation, communication, and logistics, etc. Nowadays, the rapid expansion of allocation systems has posed challenges for existing optimization engines in balancing optimality and tractability. In this paper, we present Pram, the first ML-based method that leverages the reasoning power of multimodal language models (MLMs) for addressing the trade-off dilemma -- a great need of service providers. As part of our proposal, Pram (i) quickly computes high-quality allocations by dividing the original problem into local subproblems, which are then resolved by an MLM-powered "agent", and (ii) ensures global consistency by harmonizing these subproblems via a multi-agent reinforcement learning algorithm. Theoretically, we show that Pram, which learns to perform gradient descent in context, provably converges to the optimum within the family of MCF problems. Empirically, on real-world datasets and public topologies, Pram achieves performance comparable to, and in some cases even surpassing, linear programming solvers (very close to the optimal solution), and substantially lower runtimes (1 to 2 orders of magnitude faster). Moreover, Pram exhibits strong robustness (<10\% performance degradation under link failures or flow bursts), demonstrating MLM's generalization ability to unforeseen events. Pram is objective-agnostic and seamlessly integrates with mainstream allocation systems, providing a practical and scalable solution for future networks.

</details>


### [304] [Motion Capture is Not the Target Domain: Scaling Synthetic Data for Learning Motion Representations](https://arxiv.org/abs/2602.11064)
*Firas Darwish,George Nicholson,Aiden Doherty,Hang Yuan*

Main category: cs.LG

TL;DR: This paper investigates the use of synthetic data for pretraining motion time-series models in wearable-based Human Activity Recognition (HAR), finding that synthetic pretraining improves generalization—especially when combined with real data or scaled up—while large-scale motion-capture pretraining offers only marginal gains due to domain mismatch with wearable signals.


<details>
  <summary>Details</summary>
Motivation: Large-scale real-world full-body motion data collection is infeasible but essential for wearable-based HAR; synthetic motion data from motion-capture representations offers a scalable alternative, yet its transfer reliability remains unclear.

Method: Pretrain motion time-series models on synthetic motion data derived from motion-capture, and evaluate transfer performance across diverse downstream HAR tasks, comparing synthetic-only, synthetic+real, and motion-capture pretraining setups.

Result: Synthetic pretraining improves generalization—particularly when mixed with real data or scaled sufficiently; large-scale motion-capture pretraining yields only marginal gains due to domain mismatch with wearable sensor signals.

Conclusion: Synthetic motion data holds promise for transferable HAR representations, but its effectiveness depends on integration strategy and scale; domain mismatch fundamentally limits direct transfer from motion-capture to wearable domains.

Abstract: Synthetic data offers a compelling path to scalable pretraining when real-world data is scarce, but models pretrained on synthetic data often fail to transfer reliably to deployment settings. We study this problem in full-body human motion, where large-scale data collection is infeasible but essential for wearable-based Human Activity Recognition (HAR), and where synthetic motion can be generated from motion-capture-derived representations. We pretrain motion time-series models using such synthetic data and evaluate their transfer across diverse downstream HAR tasks. Our results show that synthetic pretraining improves generalisation when mixed with real data or scaled sufficiently. We also demonstrate that large-scale motion-capture pretraining yields only marginal gains due to domain mismatch with wearable signals, clarifying key sim-to-real challenges and the limits and opportunities of synthetic motion data for transferable HAR representations.

</details>


### [305] [In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution](https://arxiv.org/abs/2602.11079)
*Frank Xiao,Santiago Aranguri*

Main category: cs.LG

TL;DR: 本文提出了一种基于激活的数据归因方法，用于追溯语言模型行为变化至具体训练数据点，并在OLMo 2模型中发现了由干扰项触发的有害服从行为，通过数据过滤或标签翻转显著缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在真实场景中精准定位导致模型有害行为的具体训练数据，尤其当行为源于污染的偏好数据而非刻意注入时。

Method: 提出基于激活差异向量与余弦相似度排序的数据归因方法，并结合因果验证（重训练）和无监督聚类来发现行为-数据关联。

Result: 在OLMo 2的DPO训练中识别出‘干扰项触发的服从行为’，过滤或翻转top-ranked数据分别降低该行为63%和78%，且方法比梯度归因和LLM-judge基线更高效、更廉价。

Conclusion: 激活驱动的数据归因是一种高效、可解释、可因果验证的工具，为理解与干预真实世界语言模型的安全风险提供了新范式。

Abstract: We propose activation-based data attribution, a method that traces behavioral changes in post-trained language models to responsible training datapoints. By computing activation-difference vectors for both test prompts and preference pairs and ranking by cosine similarity, we identify datapoints that cause specific behaviors and validate these attributions causally by retraining with modified data. Clustering behavior-datapoint similarity matrices also enables unsupervised discovery of emergent behaviors. Applying this to OLMo 2's production DPO training, we surfaced distractor-triggered compliance: a harmful behavior where the model complies with dangerous requests when benign formatting instructions are appended. Filtering top-ranked datapoints reduces this behavior by 63% while switching their labels achieves 78%. Our method outperforms gradient-based attribution and LLM-judge baselines while being over 10 times cheaper than both. This in-the-wild model organism - emerging from contaminated preference data rather than deliberate injection - provides a realistic benchmark for safety techniques.

</details>


### [306] [Token-Efficient Change Detection in LLM APIs](https://arxiv.org/abs/2602.11083)
*Timothée Chauvin,Clément Lalanne,Erwan Le Merrer,Jean-Michel Loubes,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: 本文提出了一种名为B3IT的黑盒远程大模型变更检测方法，利用Border Inputs（边界输入）实现低成本、高性能的变更检测，无需访问模型权重或输出概率，仅依赖输出token，在实验中性能媲美现有灰盒方法且成本降低30倍。


<details>
  <summary>Details</summary>
Motivation: 现有远程大模型变更检测方法要么部署成本过高，要么需要白盒（模型权重）或灰盒（log概率）访问权限，难以兼顾低成本与严格黑盒操作。

Method: 提出基于Border Inputs（存在多个top token的特定输入）的Black-Box Border Input Tracking (B3IT) 方法；理论分析表明在低温设定下，Border Inputs能通过模型Jacobian和输出分布Fisher信息实现高效变更检测。

Result: 实验证明Border Inputs易于在非推理类API端点上发现，B3IT性能媲美最优灰盒方法，且成本降低30倍，严格满足黑盒约束。

Conclusion: Border Inputs为黑盒场景下的高效、低成本大模型变更检测提供了新范式，B3IT在实用性与理论性上均取得突破。

Abstract: Remote change detection in LLMs is a difficult problem. Existing methods are either too expensive for deployment at scale, or require initial white-box access to model weights or grey-box access to log probabilities. We aim to achieve both low cost and strict black-box operation, observing only output tokens.
  Our approach hinges on specific inputs we call Border Inputs, for which there exists more than one output top token. From a statistical perspective, optimal change detection depends on the model's Jacobian and the Fisher information of the output distribution. Analyzing these quantities in low-temperature regimes shows that border inputs enable powerful change detection tests.
  Building on this insight, we propose the Black-Box Border Input Tracking (B3IT) scheme. Extensive in-vivo and in-vitro experiments show that border inputs are easily found for non-reasoning tested endpoints, and achieve performance on par with the best available grey-box approaches. B3IT reduces costs by $30\times$ compared to existing methods, while operating in a strict black-box setting.

</details>


### [307] [GRASP: group-Shapley feature selection for patients](https://arxiv.org/abs/2602.11084)
*Yuheng Luo,Shuyan Li,Zhong Cao*

Main category: cs.LG

TL;DR: GRASP is a novel feature selection framework that combines SHAP-based group importance scoring with group L21 regularization to achieve robust, interpretable, and stable feature selection in medical prediction.


<details>
  <summary>Details</summary>
Motivation: Existing feature selection methods like LASSO lack robustness and interpretability in medical prediction tasks.

Method: GRASP couples Shapley value-driven attribution (via SHAP on a pretrained tree model) with group L21-regularized logistic regression to enforce structured sparsity and extract compact, non-redundant feature sets.

Result: GRASP achieves comparable or superior predictive accuracy versus LASSO, SHAP, and deep learning methods, while selecting fewer, less redundant, and more stable features.

Conclusion: GRASP provides a robust, interpretable, and stable solution for feature selection in medical prediction.

Abstract: Feature selection remains a major challenge in medical prediction, where existing approaches such as LASSO often lack robustness and interpretability. We introduce GRASP, a novel framework that couples Shapley value driven attribution with group $L_{21}$ regularization to extract compact and non-redundant feature sets. GRASP first distills group level importance scores from a pretrained tree model via SHAP, then enforces structured sparsity through group $L_{21}$ regularized logistic regression, yielding stable and interpretable selections. Extensive comparisons with LASSO, SHAP, and deep learning based methods show that GRASP consistently delivers comparable or superior predictive accuracy, while identifying fewer, less redundant, and more stable features.

</details>


### [308] [General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies](https://arxiv.org/abs/2602.11087)
*Jianxun Wang,Grant C. Forbes,Leonardo Villalobos-Arias,David L. Roberts*

Main category: cs.LG

TL;DR: 本文提出了一种基于f-散度的灵活约束方法，用于改进离线强化学习算法在低多样性、多行为策略数据集上的性能，通过线性规划形式和凸共轭建立了f-散度与贝尔曼残差约束之间的联系，并在MuJoCo、Fetch和AdroitHand环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 实际离线数据集常缺乏多样性、探索不足，且来自多个不同能力水平的行为策略，导致传统离线RL算法在值估计和策略约束之间难以平衡。

Method: 基于更一般的强化学习线性规划（LP）形式与凸共轭，建立f-散度与贝尔曼残差优化约束的联系，并提出一种通用的灵活f-散度函数形式，以自适应地调整学习目标约束。

Result: 在MuJoCo、Fetch和AdroitHand环境的实验表明所提LP形式正确，且灵活f-散度能提升算法在挑战性离线数据集上的性能。

Conclusion: 引入自适应f-散度约束可更好权衡离线RL中的优化目标与行为策略约束，提升在复杂真实离线数据上的泛化与鲁棒性。

Abstract: Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, and from multiple behavior policies with diverse expertise levels. Limited exploration can impair the offline RL algorithm's ability to estimate \textit{Q} or \textit{V} values, while constraining towards diverse behavior policies can be overly conservative. Such datasets call for a balance between the RL objective and behavior policy constraints. We first identify the connection between $f$-divergence and optimization constraint on the Bellman residual through a more general Linear Programming form for RL and the convex conjugate. Following this, we introduce the general flexible function formulation for the $f$-divergence to incorporate an adaptive constraint on algorithms' learning objectives based on the offline training dataset. Results from experiments on the MuJoCo, Fetch, and AdroitHand environments show the correctness of the proposed LP form and the potential of the flexible $f$-divergence in improving performance for learning from a challenging dataset when applied to a compatible constrained optimization algorithm.

</details>


### [309] [Direct Learning of Calibration-Aware Uncertainty for Neural PDE Surrogates](https://arxiv.org/abs/2602.11090)
*Carlos Stein Brito*

Main category: cs.LG

TL;DR: 本文提出了一种名为交叉正则化不确定性（Cross-regularized uncertainty）的新型神经PDE代理模型不确定性学习框架，通过在独立验证集上优化低维不确定性参数，实现无需人工调参的自适应不确定性建模，并在傅里叶神经算子中验证了其在数据受限场景下的良好校准性和空间误差定位能力。


<details>
  <summary>Details</summary>
Motivation: 神经PDE代理模型常用于数据稀缺或部分观测场景，下游决策不仅依赖低预测误差，更需校准良好的不确定性估计；现有不确定性方法（如集成、固定随机噪声、后验校准）缺乏对不同数据 regime 的自适应能力。

Method: 引入交叉正则化不确定性框架：将数据划分为训练集和正则化（验证）集；预测器在训练集上优化拟合，而低维不确定性控制参数（如输出头、隐层特征或谱模式中的连续噪声水平）在正则化集上通过梯度优化，以减小训练-测试失配。

Result: 在APEBench数据集上对观测比例和训练集规模进行系统评估，结果表明该方法所得预测分布在校验集上校准性更优，且不确定性场能有效集中在单步空间诊断的高误差区域。

Conclusion: 交叉正则化不确定性是一种无需针对不同数据 regime 手动调优噪声的通用框架，可灵活嵌入算子架构（如FNO），显著提升神经PDE代理模型在数据受限场景下的不确定性质量与实用性。

Abstract: Neural PDE surrogates are often deployed in data-limited or partially observed regimes where downstream decisions depend on calibrated uncertainty in addition to low prediction error. Existing approaches obtain uncertainty through ensemble replication, fixed stochastic noise such as dropout, or post hoc calibration. Cross-regularized uncertainty learns uncertainty parameters during training using gradients routed through a held-out regularization split. The predictor is optimized on the training split for fit, while low-dimensional uncertainty controls are optimized on the regularization split to reduce train-test mismatch, yielding regime-adaptive uncertainty without per-regime noise tuning. The framework can learn continuous noise levels at the output head, within hidden features, or within operator-specific components such as spectral modes. We instantiate the approach in Fourier Neural Operators and evaluate on APEBench sweeps over observed fraction and training-set size. Across these sweeps, the learned predictive distributions are better calibrated on held-out splits and the resulting uncertainty fields concentrate in high-error regions in one-step spatial diagnostics.

</details>


### [310] [MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning](https://arxiv.org/abs/2602.11092)
*Cassandre Notton,Benjamin Stott,Philippe Schoeb,Anthony Walsh,Grégoire Leboucher,Vincent Espitalier,Vassilis Apostolou,Louis-Félix Vigneux,Alexia Salavrakos,Jean Senellart*

Main category: cs.LG

TL;DR: MerLin是一个开源框架，旨在系统性地探索光子和混合量子机器学习模型，支持端到端可微分训练、可复现基准测试，并已复现18项前沿工作，嵌入PyTorch/scikit-learn生态，兼顾硬件适配与未来协同设计。


<details>
  <summary>Details</summary>
Motivation: 当前量子机器学习研究多为孤立算法提案，缺乏在模型、数据集和硬件约束下的系统性、实证性探索；亟需统一、可复现的基准平台推动实用化进展。

Method: 开发MerLin框架，集成线性光路的优化强模拟，无缝接入PyTorch和scikit-learn；支持可微分量子层训练、模块化实验设计、硬件感知仿真与真实硬件对接。

Result: 成功复现18项光子及混合QML前沿工作（涵盖核方法、储备池计算、CNN/RNN、生成模型等），发布为可重用、可扩展的标准化实验模块，并实现与经典ML工具链（如消融分析、跨模态比较）的兼容。

Conclusion: MerLin填补了QML实证研究的基础设施空白，通过标准化基准、生态融合与硬件协同设计，为近中期量子优势的识别与实现提供了可持续的开放平台。

Abstract: Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates optimized strong simulation of linear optical circuits into standard PyTorch and scikit learn workflows, enabling end to end differentiable training of quantum layers. MerLin is designed around systematic benchmarking and reproducibility. As an initial contribution, we reproduce eighteen state of the art photonic and hybrid QML works spanning kernel methods, reservoir computing, convolutional and recurrent architectures, generative models, and modern training paradigms. These reproductions are released as reusable, modular experiments that can be directly extended and adapted, establishing a shared experimental baseline consistent with empirical benchmarking methodologies widely adopted in modern artificial intelligence. By embedding photonic quantum models within established machine learning ecosystems, MerLin allows practitioners to leverage existing tooling for ablation studies, cross modality comparisons, and hybrid classical quantum workflows. The framework already implements hardware aware features, allowing tests on available quantum hardware while enabling exploration beyond its current capabilities, positioning MerLin as a future proof co design tool linking algorithms, benchmarks, and hardware.

</details>


### [311] [Statistical Learning Analysis of Physics-Informed Neural Networks](https://arxiv.org/abs/2602.11097)
*David A. Barajas-Solano*

Main category: cs.LG

TL;DR: 本文从统计学习视角分析了带硬约束的物理信息神经网络（PINN）在初边值问题中的训练与性能，将物理残差项解释为无限间接数据源，并利用奇异学习理论（如局部学习系数）分析参数估计，探讨其对预测不确定性量化和外推能力的影响。


<details>
  <summary>Details</summary>
Motivation: 理解PINN在初边值问题中训练机制的本质，澄清物理惩罚项的作用（非正则化项而是间接数据源），并从统计学习理论角度分析其学习特性。

Method: 将带硬约束的PINN参数估计建模为统计学习问题，以KL散度最小化连接PINN残差分布与真实数据生成分布；引入奇异学习理论及局部学习系数（LLC）分析热方程IBVP下随机优化所得参数估计。

Result: 揭示PINN物理学习本质上是奇异学习问题；LLC可用于刻画其学习难度与泛化行为；该视角有助于理解预测不确定性与外推能力。

Conclusion: PINN的物理信息学习应被视作一种由物理定律驱动的、具有内在奇异性与特定分布匹配目标的统计学习过程，而非传统监督学习或简单正则化方法。

Abstract: We study the training and performance of physics-informed learning for initial and boundary value problems (IBVP) with physics-informed neural networks (PINNs) from a statistical learning perspective. Specifically, we restrict ourselves to parameterizations with hard initial and boundary condition constraints and reformulate the problem of estimating PINN parameters as a statistical learning problem. From this perspective, the physics penalty on the IBVP residuals can be better understood not as a regularizing term bus as an infinite source of indirect data, and the learning process as fitting the PINN distribution of residuals $p(y \mid x, t, w) q(x, t) $ to the true data-generating distribution $δ(0) q(x, t)$ by minimizing the Kullback-Leibler divergence between the true and PINN distributions. Furthermore, this analysis show that physics-informed learning with PINNs is a singular learning problem, and we employ singular learning theory tools, namely the so-called Local Learning Coefficient (Lau et al., 2025) to analyze the estimates of PINN parameters obtained via stochastic optimization for a heat equation IBVP. Finally, we discuss implications of this analysis on the quantification of predictive uncertainty of PINNs and the extrapolation capacity of PINNs.

</details>


### [312] [From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent](https://arxiv.org/abs/2602.11123)
*Genmao Zhuang,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 本文提出了一种名为Materials Knowledge Navigation Agent (MKNA)的语言驱动材料探索系统，能将自然语言科学意图转化为数据库检索、性能预测、结构生成和稳定性评估等可执行操作，并自主提取定量阈值与化学设计模式，成功应用于高德拜温度陶瓷的发现。


<details>
  <summary>Details</summary>
Motivation: 加速高性能材料的发现是能源、电子和航空航天等领域的核心挑战，传统方法依赖专家直觉和高成本模拟，效率低且难以规模化。

Method: 构建语言驱动的MKNA系统，集成数据库检索、属性预测、晶体结构生成与热力学稳定性评估模块，并具备从文献和数据库中自动提取定量标准与化学设计模式的能力。

Result: 在高德拜温度陶瓷搜索中，MKNA识别出Theta_D > 800 K筛选准则，复现了金刚石、SiC等已知材料，并提出多种热力学稳定的新型Be-C富集化合物，覆盖1500–1700 K稀疏区域。

Conclusion: MKNA不仅能高效发现稳定新材料，还能反演可解释的设计启发式规则，为自主、语言引导的材料探索提供了通用平台。

Abstract: Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into executable actions for database retrieval, property prediction, structure generation, and stability evaluation. Beyond automating tool invocation, MKNA autonomously extracts quantitative thresholds and chemically meaningful design motifs from literature and database evidence, enabling data-grounded hypothesis formation. Applied to the search for high-Debye-temperature ceramics, the agent identifies a literature-supported screening criterion (Theta_D > 800 K), rediscovers canonical ultra-stiff materials such as diamond, SiC, SiN, and BeO, and proposes thermodynamically stable, previously unreported Be-C-rich compounds that populate the sparsely explored 1500-1700 K regime. These results demonstrate that MKNA not only finds stable candidates but also reconstructs interpretable design heuristics, establishing a generalizable platform for autonomous, language-guided materials exploration.

</details>


### [313] [The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization](https://arxiv.org/abs/2602.11126)
*Stephanie Holly,Alexandru-Ciprian Zăvoianu,Siegfried Silber,Sepp Hochreiter,Werner Zellinger*

Main category: cs.LG

TL;DR: 本文探讨了离线多目标优化（MOO）中生成式方法（如扩散模型）在不同评估指标下的表现差异，指出其在生成距离等指标上系统性弱于进化算法，并将原因归结为‘离线前沿偏移’这一根本限制；作者提出需通过目标空间中的分布外采样（借助积分概率度量）来克服该限制，并实证发现现有生成方法仍过于保守地贴近离线目标分布。


<details>
  <summary>Details</summary>
Motivation: 理解生成式方法（尤其是扩散模型）在离线多目标优化中为何在某些经典指标（如生成距离）下表现不佳，揭示其性能局限的根本原因。

Method: 分析离线MOO中生成方法与进化算法在不同指标下的表现差异，提出‘离线前沿偏移’概念解释失败机制，并引入积分概率度量（IPM）刻画目标空间的分布外采样需求。

Result: 发现生成方法在生成距离等指标上系统性劣于进化算法；证实其性能受限于离线数据与真实Pareto前沿之间的偏移；实证显示生成方法倾向于保守地拟合离线目标分布，缺乏足够分布外探索能力。

Conclusion: 离线MOO本质上是受分布偏移限制的问题；生成式优化方法的失效可由此诊断；未来工作需设计能主动进行目标空间分布外采样的新范式。

Abstract: Offline multi-objective optimization (MOO) aims to recover Pareto-optimal designs given a finite, static dataset. Recent generative approaches, including diffusion models, show strong performance under hypervolume, yet their behavior under other established MOO metrics is less understood. We show that generative methods systematically underperform evolutionary alternatives with respect to other metrics, such as generational distance. We relate this failure mode to the offline-frontier shift, i.e., the displacement of the offline dataset from the Pareto front, which acts as a fundamental limitation in offline MOO. We argue that overcoming this limitation requires out-of-distribution sampling in objective space (via an integral probability metric) and empirically observe that generative methods remain conservatively close to the offline objective distribution. Our results position offline MOO as a distribution-shift--limited problem and provide a diagnostic lens for understanding when and why generative optimization methods fail.

</details>


### [314] [Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.11128)
*Reinhard Heckel,Mahdi Soltanolkotabi,Christos Thramboulidis*

Main category: cs.LG

TL;DR: 本文提出了一种针对强化学习中提示（prompt）的非对称加权策略，特别适用于从零开始训练（from-scratch RL）场景，在低成功率提示上赋予更高权重，以加速收敛；理论分析表明该策略在响应稀疏、成本高昂的低成功率阶段最优。


<details>
  <summary>Details</summary>
Motivation: 现有主流RLHF算法（如GRPO、DAPO、RLOO）倾向于忽略极难或极易的提示，仅关注中间成功率提示；但作者观察到在从零训练（如R1-Zero）过程中，低成功率提示蕴含关键学习信号，亟需更优梯度加权机制。

Method: 提出基于成功概率的非对称提示加权方案，并建立理论模型，推导在固定更新预算下、使成功率从初始值提升至目标值所需时间最小化的最优权重函数。

Result: 实验证明非对称加权显著提升from-scratch RL训练效率，尤其在低成功率阶段；理论证明该策略可最小化收敛时间，且在响应稀疏、成本主导场景下自然导出上偏权重分布。

Conclusion: 提示的成功概率应作为梯度加权的核心依据，而非简单过滤；非对称加权是适配不同训练阶段（尤其是冷启动阶段）的关键设计，为LLM强化学习提供了新理论视角与实用策略。

Abstract: Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.

</details>


### [315] [TabICLv2: A better, faster, scalable, and open tabular foundation model](https://arxiv.org/abs/2602.11139)
*Jingang Qu,David Holzmüller,Gaël Varoquaux,Marine Le Morvan*

Main category: cs.LG

TL;DR: TabICLv2 is a new state-of-the-art tabular foundation model for regression and classification, built on synthetic data generation, architectural innovations (e.g., scalable softmax in attention), and optimized pretraining (e.g., Muon optimizer); it outperforms RealTabPFN-2.5 without tuning, scales efficiently to million-sample datasets, and is open-sourced.


<details>
  <summary>Details</summary>
Motivation: To advance tabular foundation models by overcoming limitations of prior approaches—such as limited pretraining diversity, poor scalability to long sequences, and suboptimal optimization—while surpassing gradient-boosted trees and recent models like RealTabPFN-2.5.

Method: Introduces TabICLv2 based on three key components: (1) a novel synthetic data generation engine for high pretraining diversity; (2) architectural improvements, including a scalable softmax in attention for better generalization to large datasets; and (3) optimized pretraining protocols using the Muon optimizer instead of AdamW.

Result: TabICLv2 achieves SOTA performance on TabArena and TALENT benchmarks without any tuning—surpassing hyperparameter-tuned, ensembled, and fine-tuned RealTabPFN-2.5—and efficiently handles million-scale datasets under 50GB GPU memory while being significantly faster.

Conclusion: TabICLv2 demonstrates that carefully designed synthetic data, architecture, and optimization can jointly enable strong, scalable, and efficient in-context learning for tabular data—setting a new benchmark and supporting open research via public release of code and weights.

Abstract: Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at https://github.com/soda-inria/tabicl, with synthetic data engine and pretraining code to follow.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [316] [Photons x Force: Differentiable Radiation Pressure Modeling](https://arxiv.org/abs/2602.10712)
*Charles Constant,Elizabeth Bates,Santosh Bhattarai,Marek Ziebart,Tobias Ritschel*

Main category: cs.GR

TL;DR: 本文提出了一种面向辐射压力约束的参数化航天器设计优化系统，通过蒙特卡洛模拟、神经代理模型和可微优化三方面创新，显著提升计算效率并支持大规模设计与任务优化。


<details>
  <summary>Details</summary>
Motivation: 辐射压力是高空（>800 km）航天器最主要的非保守作用力，但高保真建模计算成本过高，限制了其在航天器设计、优化及空间态势感知中的实际应用。

Method: 1) 提出一种基于计算机图形学启发的并行蒙特卡洛辐射压力仿真方法，支持重要性采样与下事件估计以降方差，并能批量仿真一族设计；2) 利用神经网络构建从设计参数到辐射力的可微代理模型，大幅加速力查询；3) 基于该代理模型开展逆辐射压力设计优化（如几何/材料/操作参数优化）。

Result: 实现了对辐射压力效应的高效、可微、批量建模与优化，支持最小化飞行时间、最大化目标邻近度、节省推进剂、训练任务控制策略及外星算力分配等多样化逆设计任务。

Conclusion: 该系统通过仿真、表征与优化三重创新，首次将高保真辐射压力建模有效嵌入参数化航天器设计闭环，为太空系统智能设计提供了新范式。

Abstract: We propose a system to optimize parametric designs subject to radiation pressure, \ie the effect of light on the motion of objects. This is most relevant in the design of spacecraft, where radiation pressure presents the dominant non-conservative forcing mechanism, which is the case beyond approximately 800 km altitude. Despite its importance, the high computational cost of high-fidelity radiation pressure modeling has limited its use in large-scale spacecraft design, optimization, and space situational awareness applications. We enable this by offering three innovations in the simulation, in representation and in optimization: First, a practical computer graphics-inspired Monte-Carlo (MC) simulation of radiation pressure. The simulation is highly parallel, uses importance sampling and next-event estimation to reduce variance and allows simulating an entire family of designs instead of a single spacecraft as in previous work. Second, we introduce neural networks as a representation of forces from design parameters. This neural proxy model, learned from simulations, is inherently differentiable and can query forces orders of magnitude faster than a full MC simulation. Third, and finally, we demonstrate optimizing inverse radiation pressure designs, such as finding geometry, material or operation parameters that minimizes travel time, maximizes proximity given a desired end-point, minimize thruster fuel, trains mission control policies or allocated compute budget in extraterrestrial compute.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [317] [Discovering Differences in Strategic Behavior Between Humans and LLMs](https://arxiv.org/abs/2602.10324)
*Caroline Wang,Daniel Kasenberg,Kim Stachenfeld,Pablo Samuel Castro*

Main category: cs.AI

TL;DR: 本文利用AlphaEvolve工具，从数据中直接发现可解释的人类与大语言模型（LLM）行为模型，发现在重复石头剪刀布博弈中，前沿LLM展现出比人类更深的战略行为。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型（LLMs）在社会与战略场景中行为与人类的差异，并弥补现有行为博弈论（BGT）模型无法充分刻画人类及黑箱非人类智能体行为的不足。

Method: 采用前沿程序发现工具AlphaEvolve，从实验数据中直接发现可解释的人类与LLM行为模型，以开放方式识别驱动二者行为的结构性因素。

Result: 在迭代石头剪刀布博弈中，前沿LLM展现出比人类更深层次的战略行为能力。

Conclusion: 该研究为理解人类与LLM在战略互动中行为差异背后的结构性动因提供了基础。

Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions.

</details>


### [318] [To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks](https://arxiv.org/abs/2602.10625)
*Nanxu Gong,Haotian Li,Sixun Dong,Jianxun Lian,Yanjie Fu,Xing Xie*

Main category: cs.AI

TL;DR: 本研究系统评估了九种大型语言模型在心理理论（ToM）任务上的表现，发现推理模型在社会认知任务中并未持续优于非推理模型，甚至有时更差；分析揭示了‘慢思考崩溃’、‘适度自适应推理有益’和‘选项匹配捷径’三大现象，并提出S2F与T2M两种干预方法，表明形式推理能力难以直接迁移到社会推理任务。


<details>
  <summary>Details</summary>
Motivation: 探究大型推理模型（LRMs）在数学和编程等领域的推理能力提升是否能迁移到社会认知能力（如心理理论ToM）上，当前该问题尚未被充分探索。

Method: 对九种先进大语言模型在三个典型ToM基准上进行系统评测，对比推理型与非推理型模型；开展细粒度分析（响应长度影响、推理预算效应、选项存在性控制）；设计Slow-to-Fast（S2F）自适应推理与Think-to-Match（T2M）捷径抑制两种干预方法进行验证。

Result: 1）推理模型在ToM任务上未稳定优于非推理模型；2）响应越长准确率越低，增大推理预算反而损害性能；3）限制推理长度或动态调整可提升性能；4）移除多选选项后推理模型性能显著提升，暴露其依赖选项匹配而非真实推理；5）S2F和T2M干预有效缓解上述问题。

Conclusion: 大型推理模型在形式推理（如数学、代码）上的进步不能直接迁移至心理理论等社会推理任务；实现鲁棒的心理理论能力需发展超越现有推理范式的专属能力。

Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.

</details>


### [319] [GameDevBench: Evaluating Agentic Capabilities Through Game Development](https://arxiv.org/abs/2602.11103)
*Wayne Chi,Yixiong Fang,Arnav Yayavaram,Siddharth Yayavaram,Seth Karten,Qiuhong Anna Wei,Runkun Chen,Alexander Wang,Valerie Chen,Ameet Talwalkar,Chris Donahue*

Main category: cs.AI

TL;DR: 本文提出了GameDevBench，首个面向游戏开发任务的多模态智能体评测基准，包含132个源自教程的复杂任务，强调代码与图像、视频等多模态资产的协同理解与操作；实验表明现有智能体表现有限，尤其在2D图形任务上更差，并提出两种简单有效的图像/视频反馈机制显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有编码智能体发展迅速，但多模态智能体进展滞后，主因缺乏兼具软件开发复杂性与深度多模态理解需求的评测基准；游戏开发天然融合代码与着色器、贴图、动画等多模态资产，是理想的评测场景。

Method: 构建GameDevBench基准：基于网络与视频教程提取132个真实游戏开发任务；设计图像和视频两种轻量级反馈机制供智能体在开发过程中利用视觉信息进行自我修正。

Result: 当前最优智能体仅完成54.5%的任务；任务难度与多模态复杂度强相关（游戏逻辑任务成功率46.9%，2D图形任务仅31.6%）；引入图像/视频反馈后，Claude Sonnet 4.5成功率从33.3%提升至47.7%。

Conclusion: GameDevBench填补了多模态智能体评测空白，验证了多模态理解是当前瓶颈，而简单视觉反馈可有效缓解；该基准已开源，推动具身化、多模态智能体在复杂软件工程场景中的研究。

Abstract: Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.

</details>


### [320] [Found-RL: foundation model-enhanced reinforcement learning for autonomous driving](https://arxiv.org/abs/2602.10458)
*Yansong Qu,Zihao Sheng,Zilin Huang,Jiancong Chen,Yuhao Luo,Tianyi Wang,Yiheng Feng,Samuel Labi,Sikai Chen*

Main category: cs.AI

TL;DR: 本文提出Found-RL平台，通过异步批推理框架与多种监督机制（如VMR、AWAG）将视觉语言模型（VLM）高效融入自动驾驶强化学习训练中，在保持实时性（~500 FPS）的同时显著提升策略性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在自动驾驶中面临样本效率低和语义可解释性差的问题，而基础模型（尤其是VLM）虽具丰富上下文知识，但高推理延迟阻碍其在高频RL训练环中应用。

Method: 提出Found-RL平台，核心包括：1）异步批推理框架解耦VLM推理与仿真循环；2）Value-Margin Regularization（VMR）和Advantage-Weighted Action Guidance（AWAG）蒸馏VLM动作建议；3）基于CLIP的密集奖励塑形，并引入Conditional Contrastive Action Alignment解决其动态盲区问题。

Result: 轻量级RL模型借助Found-RL可达到接近十亿参数VLM的性能，同时维持约500 FPS的实时推理速度。

Conclusion: Found-RL成功弥合了基础模型与强化学习在自动驾驶中的应用鸿沟，实现了高性能、高效率与高实时性的统一，并提供开源代码、数据与模型。

Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.

</details>


### [321] [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598)
*Shuai Han,Mehdi Dastani,Shihan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为Neuro-symbolic Action Masking (NSAM)的新框架，它能在深度强化学习过程中以最小监督方式自动学习符合领域约束的符号模型，并基于该模型生成动作掩码，从而排除不可行动作，提高样本效率并减少约束违反。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在训练和执行过程中可能探索不可行动作，而现有方法依赖人工指定的动作掩码和符号映射函数，缺乏自动化和泛化能力。

Method: 提出NSAM框架，在DRL过程中以最小监督方式自动学习与高维状态领域约束一致的符号模型，并基于此模型学习动作掩码；实现符号推理与深度策略优化的端到端联合训练。

Result: 在多个带约束的领域中验证，NSAM显著提升了DRL智能体的样本效率，并大幅减少了约束违反。

Conclusion: NSAM成功实现了符号推理与深度强化学习的协同优化，为解决高维状态下的动作可行性问题提供了新思路。

Abstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.

</details>


### [322] [OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization](https://arxiv.org/abs/2602.10635)
*Keane Ong,Sabri Boughorbel,Luwei Xiao,Chanakya Ekbote,Wei Dai,Ao Qu,Jingyao Wu,Rui Mao,Ehsan Hoque,Erik Cambria,Gianmarco Mengaldo,Paul Pu Liang*

Main category: cs.AI

TL;DR: 本文提出HARPO方法，用于在异构社交行为任务上进行强化学习训练，并基于此构建了Omnisapiens-7B 2.0模型，在多任务与未见任务上均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法常孤立建模人类行为维度，导致训练成本高、泛化能力差；而当前推理型强化学习方法未显式解决异构行为数据的学习问题。

Method: 提出Heterogeneity-Aware Relative Policy Optimization (HARPO)，通过调节优势函数来平衡不同任务和样本在策略优化中的影响。

Result: 基于HARPO构建的Omnisapiens-7B 2.0在行为任务上表现最优，多任务设置下提升达+16.85%，未见任务设置下提升+9.37%，且推理过程更显式、鲁棒；HARPO在对比实验中表现最稳定。

Conclusion: HARPO有效提升了异构社交行为任务上的统一建模能力，为构建通用社会智能AI提供了新路径。

Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.

</details>


### [323] [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)
*Jie Jiang,Yangru Huang,Zeyu Wang,Changping Wang,Yuling Xiong,Jun Zhang,Huan Yu*

Main category: cs.AI

TL;DR: 本文提出V-STAR框架，通过价值引导采样与树状优势强化学习，解决自回归推荐中RL微调时的概率-奖励错配问题，提升探索效率与学习信号质量。


<details>
  <summary>Details</summary>
Motivation: 自回归生成式推荐中，RL微调常因似然主导解码（如束搜索）导致探索不足和优势压缩，削弱学习信号。

Method: 提出V-STAR：包含价值引导高效解码（VED）以选择性扩展高潜力前缀；以及Sibling-GRPO，在树结构上计算兄弟节点相对优势，聚焦关键分支决策。

Result: 在离线与在线数据集上，V-STAR显著优于SOTA基线，在严格延迟约束下提升准确率与候选集多样性。

Conclusion: V-STAR通过协同的树结构建模与优势重校准，有效缓解概率-奖励错配，为生成式推荐的RL优化提供了新范式。

Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.

</details>


### [324] [SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy](https://arxiv.org/abs/2602.10845)
*Xuecheng Zou,Yu Tang,Bingbing Wang*

Main category: cs.AI

TL;DR: 本文提出SynergyKGC框架，通过跨模态协同专家机制和密度自适应锚定策略，解决知识图谱补全中因图密度差异导致的结构表征失配问题，显著提升补全性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全方法存在‘结构分辨率不匹配’问题，难以兼顾稠密与稀疏子图的不同表征需求，导致结构噪声干扰或表征坍塌。

Method: 提出SynergyKGC框架，包含关系感知的跨模态协同专家（基于跨注意力与语义意图门控）、密度依赖的身份锚定策略，以及双塔一致性架构。

Result: 在两个公开基准上显著提升KGC的Hit@N指标，验证了方法在异构图结构中鲁棒信息融合的有效性。

Conclusion: 结构异质性需通过自适应、协同式建模来缓解，SynergyKGC为非均匀结构化数据中的稳健表征学习提供了新范式。

Abstract: Knowledge Graph Completion (KGC) fundamentally hinges on the coherent fusion of pre-trained entity semantics with heterogeneous topological structures to facilitate robust relational reasoning. However, existing paradigms encounter a critical "structural resolution mismatch," failing to reconcile divergent representational demands across varying graph densities, which precipitates structural noise interference in dense clusters and catastrophic representation collapse in sparse regions. We present SynergyKGC, an adaptive framework that advances traditional neighbor aggregation to an active Cross-Modal Synergy Expert via relation-aware cross-attention and semantic-intent-driven gating. By coupling a density-dependent Identity Anchoring strategy with a Double-tower Coherent Consistency architecture, SynergyKGC effectively reconciles topological heterogeneity while ensuring representational stability across training and inference phases. Systematic evaluations on two public benchmarks validate the superiority of our method in significantly boosting KGC hit rates, providing empirical evidence for a generalized principle of resilient information integration in non-homogeneous structured data.

</details>


### [325] [Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics](https://arxiv.org/abs/2602.10885)
*Leheng Sheng,Wenchang Ma,Ruixin Hong,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 本文提出RLCER方法，通过自生成和自演化的评分标准来奖励思维链（CoT），无需人工标注，且能随CoT分布变化而自适应演进，在推理性能上优于传统结果导向的强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 直接奖励思维链（CoT）困难：人工标注成本高，静态奖励模型难以适应CoT分布演化及对抗性奖励欺骗。因此需一种无需人工标注、可自主渐进演化的CoT奖励机制。

Method: 提出RLCER（基于自演化评分标准的CoT监督强化学习），在结果导向的RLVR基础上，引入自生成、自演化的评分标准作为CoT监督信号，并将这些标准用于提示工程以提升推理时性能。

Result: 自生成与自演化的评分标准即使无结果奖励也能提供可靠CoT监督信号，使RLCER优于结果导向的RLVR；且作为提示时可进一步提升推理性能。

Conclusion: RLCER实现了无需人工标注、可自适应演化的CoT奖励机制，有效提升了LLM推理能力，为CoT监督提供了新范式。

Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [326] [JAG: Joint Attribute Graphs for Filtered Nearest Neighbor Search](https://arxiv.org/abs/2602.10258)
*Haike Xu,Guy Blelloch,Laxman Dhulipala,Lars Gottesbüren,Rajesh Jayaram,Jakub Łącki*

Main category: cs.IR

TL;DR: 本文提出JAG（联合属性图）算法，通过引入属性距离和过滤器距离，将二元过滤约束转化为连续导航指导，在整个选择性范围内实现鲁棒性能，并支持多种过滤类型。


<details>
  <summary>Details</summary>
Motivation: 现有过滤最近邻搜索算法对查询选择性和过滤器类型高度敏感，难以在实际部署中泛化到新过滤类型和未知查询选择性。

Method: 提出基于图的JAG算法，构建同时优化向量相似性和属性接近性的邻近图，并引入属性距离和过滤器距离作为连续导航指导。

Result: 在五个数据集和四种过滤类型（标签、范围、子集、布尔）上的实验表明，JAG在吞吐量和召回鲁棒性上显著优于现有最先进基线方法。

Conclusion: JAG算法能有效应对不同过滤类型和广泛选择性范围的挑战，为现代向量搜索系统提供更通用、鲁棒的过滤最近邻搜索方案。

Abstract: Despite filtered nearest neighbor search being a fundamental task in modern vector search systems, the performance of existing algorithms is highly sensitive to query selectivity and filter type. In particular, existing solutions excel either at specific filter categories (e.g., label equality) or within narrow selectivity bands (e.g., pre-filtering for low selectivity) and are therefore a poor fit for practical deployments that demand generalization to new filter types and unknown query selectivities. In this paper, we propose JAG (Joint Attribute Graphs), a graph-based algorithm designed to deliver robust performance across the entire selectivity spectrum and support diverse filter types. Our key innovation is the introduction of attribute and filter distances, which transform binary filter constraints into continuous navigational guidance. By constructing a proximity graph that jointly optimizes for both vector similarity and attribute proximity, JAG prevents navigational dead-ends and allows JAG to consistently outperform prior graph-based filtered nearest neighbor search methods. Our experimental results across five datasets and four filter types (Label, Range, Subset, Boolean) demonstrate that JAG significantly outperforms existing state-of-the-art baselines in both throughput and recall robustness.

</details>


### [327] [MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation](https://arxiv.org/abs/2602.10271)
*Yongyue Zhang,Yaxiong Wu*

Main category: cs.IR

TL;DR: 本文提出MLDocRAG框架，通过构建多模态块-查询图（MCQG）将跨模态、跨页文档内容映射到统一查询空间，实现以查询为中心的检索与结构化证据聚合，提升长上下文多模态问答的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态长文档理解中的跨模态异质性与跨页推理难题，需将分散于不同模态和页面的信息统一到语义一致的查询表示空间中。

Method: 提出MLDocRAG框架，核心是构建Multimodal Chunk-Query Graph（MCQG）：通过多模态文档扩展生成细粒度、可回答的查询，并将其与对应多模态、跨页内容建立链接；基于该图实现查询驱动的检索与结构化证据聚合。

Result: 在MMLongBench-Doc和LongDocURL数据集上，MLDocRAG显著提升了检索质量与答案准确性。

Conclusion: MLDocRAG通过查询中心化建模与图结构组织，有效增强了长上下文多模态文档的理解、定位与推理能力，为复杂文档问答提供了新范式。

Abstract: Understanding multimodal long-context documents that comprise multimodal chunks such as paragraphs, figures, and tables is challenging due to (1) cross-modal heterogeneity to localize relevant information across modalities, (2) cross-page reasoning to aggregate dispersed evidence across pages. To address these challenges, we are motivated to adopt a query-centric formulation that projects cross-modal and cross-page information into a unified query representation space, with queries acting as abstract semantic surrogates for heterogeneous multimodal content. In this paper, we propose a Multimodal Long-Context Document Retrieval Augmented Generation (MLDocRAG) framework that leverages a Multimodal Chunk-Query Graph (MCQG) to organize multimodal document content around semantically rich, answerable queries. MCQG is constructed via a multimodal document expansion process that generates fine-grained queries from heterogeneous document chunks and links them to their corresponding content across modalities and pages. This graph-based structure enables selective, query-centric retrieval and structured evidence aggregation, thereby enhancing grounding and coherence in long-context multimodal question answering. Experiments on datasets MMLongBench-Doc and LongDocURL demonstrate that MLDocRAG consistently improves retrieval quality and answer accuracy, demonstrating its effectiveness for long-context multimodal understanding.

</details>


### [328] [Single-Turn LLM Reformulation Powered Multi-Stage Hybrid Re-Ranking for Tip-of-the-Tongue Known-Item Retrieval](https://arxiv.org/abs/2602.10321)
*Debayan Mukhopadhyay,Utshab Kumar Ghosh,Shubham Chatterjee*

Main category: cs.IR

TL;DR: 本文提出了一种基于通用8B参数大语言模型（LLM）的查询重写方法，用于解决“舌尖现象”（ToT）检索难题；该方法不依赖模型微调，仅通过提示策略提升模糊查询质量，并结合多阶段检索与重排序流程，在TREC-ToT 2025数据集上显著提升召回率与排序指标。


<details>
  <summary>Details</summary>
Motivation: Tip-of-the-Tongue（ToT）检索因用户描述模糊、初始召回差，导致传统伪相关反馈（PRF）失效，亟需轻量、通用且无需微调的查询增强方案。

Method: 使用未微调的通用8B LLM进行单次查询重写，再输入至多阶段检索流水线：BM25稀疏检索 → Contriever/E5/ColBERTv2密集或late-interaction重排序 → monoT5交叉编码 → Qwen2.5-72B列表级重排序。

Result: 在TREC-ToT 2025数据集上，查询重写使Recall提升20.61%；后续多级重排序使nDCG@10提升33.88%，MRR提升29.92%，MAP@10提升29.98%。

Conclusion: 仅靠提示工程驱动的LLM查询重写是一种低成本、高收益的预检索干预手段，能显著释放下游排序器性能，且不依赖领域微调。

Abstract: Retrieving known items from vague descriptions, Tip-of-the-Tongue (ToT) retrieval, remains a significant challenge. We propose using a single call to a generic 8B-parameter LLM for query reformulation, bridging the gap between ill-formed ToT queries and specific information needs. This method is particularly effective where standard Pseudo-Relevance Feedback fails due to poor initial recall. Crucially, our LLM is not fine-tuned for ToT or specific domains, demonstrating that gains stem from our prompting strategy rather than model specialization. Rewritten queries feed a multi-stage pipeline: sparse retrieval (BM25), dense/late-interaction reranking (Contriever, E5-large-v2, ColBERTv2), monoT5 cross-encoding, and list-wise reranking (Qwen 2.5 72B). Experiments on 2025 TREC-ToT datasets show that while raw queries yield poor performance, our lightweight pre-retrieval transformation improves Recall by 20.61%. Subsequent reranking improves nDCG@10 by 33.88%, MRR by 29.92%, and MAP@10 by 29.98%, offering a cost-effective intervention that unlocks the potential of downstream rankers. Code and data: https://github.com/debayan1405/TREC-TOT-2025

</details>


### [329] [GeoGR: A Generative Retrieval Framework for Spatio-Temporal Aware POI Recommendation](https://arxiv.org/abs/2602.10411)
*Fangye Wang,Haowen Lin,Yifang Yuan,Siyuan Wang,Xiaojiang Zhou,Song Yang,Pengjie Wang*

Main category: cs.IR

TL;DR: 本文提出GeoGR框架，用于解决大规模导航平台中的下一点兴趣点（POI）预测问题，通过地理感知的SID分词与多阶段大语言模型训练，提升稀疏复杂场景下的推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于SID的POI推荐方法在真实稀疏复杂环境中表现不佳，主要受限于：(1) 难以建模高质量SID以捕捉跨类别的时空协同关系；(2) 大语言模型与POI推荐任务对齐不足。

Method: 提出GeoGR地理生成式推荐框架，包含两阶段设计：(i) 地理感知SID分词流程，利用地理约束共访问POI对、对比学习和迭代优化学习时空协同语义；(ii) 多阶段LLM训练策略，包括模板驱动的持续预训练（CPT）对齐非原生SID token，并通过监督微调（SFT）实现自回归POI生成。

Result: 在多个真实数据集上显著优于SOTA基线；在高德地图平台上线部署后，服务数百万用户，多项线上指标提升，验证了其实际有效性与可扩展性。

Conclusion: GeoGR有效解决了POI推荐中SID建模质量低与LLM任务对齐差两大瓶颈，为导航类LBS提供了可落地的生成式推荐新范式。

Abstract: Next Point-of-Interest (POI) prediction is a fundamental task in location-based services, especially critical for large-scale navigation platforms like AMAP that serve billions of users across diverse lifestyle scenarios. While recent POI recommendation approaches based on SIDs have achieved promising, they struggle in complex, sparse real-world environments due to two key limitations: (1) inadequate modeling of high-quality SIDs that capture cross-category spatio-temporal collaborative relationships, and (2) poor alignment between large language models (LLMs) and the POI recommendation task. To this end, we propose GeoGR, a geographic generative recommendation framework tailored for navigation-based LBS like AMAP, which perceives users' contextual state changes and enables intent-aware POI recommendation. GeoGR features a two-stage design: (i) a geo-aware SID tokenization pipeline that explicitly learns spatio-temporal collaborative semantic representations via geographically constrained co-visited POI pairs, contrastive learning, and iterative refinement; and (ii) a multi-stage LLM training strategy that aligns non-native SID tokens through multiple template-based continued pre-training(CPT) and enables autoregressive POI generation via supervised fine-tuning(SFT). Extensive experiments on multiple real-world datasets demonstrate GeoGR's superiority over state-of-the-art baselines. Moreover, deployment on the AMAP platform, serving millions of users with multiple online metrics boosting, confirms its practical effectiveness and scalability in production.

</details>


### [330] [End-to-End Semantic ID Generation for Generative Advertisement Recommendation](https://arxiv.org/abs/2602.10445)
*Jie Jiang,Xinxun Zhang,Enming Zhang,Yuling Xiong,Jun Zhang,Jingwen Wang,Huan Yu,Yuxiang Wang,Hao Wang,Xiao Yan,Jiawei Jiang*

Main category: cs.IR

TL;DR: 本文提出UniSID框架，通过端到端联合优化嵌入与语义ID（SIDs），解决现有残差量化（RQ）方法在生成式推荐中因两阶段压缩导致的目标错位、语义退化和误差累积问题，并引入多粒度对比学习与摘要式广告重建机制以增强SID语义表征能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于残差量化（RQ）的语义ID（SID）生成方法存在目标错位、语义退化和误差累积等固有缺陷，难以满足生成式广告推荐对高质量离散序列的需求。

Method: 提出端到端统一SID生成框架UniSID：1）联合优化原始广告数据的嵌入与SIDs；2）引入多粒度对比学习对齐不同粒度下的SID语义；3）设计摘要式广告重建机制以捕获上下文未显式包含的高层语义。

Result: 在多个下游广告推荐场景中，UniSID相比最强基线方法在Hit Rate指标上最高提升4.62%，显著优于现有SID生成方法。

Conclusion: UniSID通过端到端建模和多层级语义对齐，有效克服了传统两阶段SID生成范式的局限性，为生成式推荐中的语义离散化提供了更优解决方案。

Abstract: Generative Recommendation (GR) has excelled by framing recommendation as next-token prediction. This paradigm relies on Semantic IDs (SIDs) to tokenize large-scale items into discrete sequences. Existing GR approaches predominantly generate SIDs via Residual Quantization (RQ), where items are encoded into embeddings and then quantized to discrete SIDs. However, this paradigm suffers from inherent limitations: 1) Objective misalignment and semantic degradation stemming from the two-stage compression; 2) Error accumulation inherent in the structure of RQ. To address these limitations, we propose UniSID, a Unified SID generation framework for generative advertisement recommendation. Specifically, we jointly optimize embeddings and SIDs in an end-to-end manner from raw advertising data, enabling semantic information to flow directly into the SID space and thus addressing the inherent limitations of the two-stage cascading compression paradigm. To capture fine-grained semantics, a multi-granularity contrastive learning strategy is introduced to align distinct items across SID levels. Finally, a summary-based ad reconstruction mechanism is proposed to encourage SIDs to capture high-level semantic information that is not explicitly present in advertising contexts. Experiments demonstrate that UniSID consistently outperforms state-of-the-art SID generation methods, yielding up to a 4.62% improvement in Hit Rate metrics across downstream advertising scenarios compared to the strongest baseline.

</details>


### [331] [Compute Only Once: UG-Separation for Efficient Large Recommendation Models](https://arxiv.org/abs/2602.10455)
*Hui Lu,Zheng Chai,Shipeng Bai,Hao Zhang,Zhifang Fan,Kunmin Bai,Yingwen Wu,Bingzheng Wei,Xiang Sun,Ziyan Gong,Tianyi Liu,Hua Chen,Deping Xie,Zhongkai Chen,Zhiliang Guo,Qiwei Chen,Yuchao Zheng*

Main category: cs.IR

TL;DR: 本文提出UG-Sep框架，通过掩码机制分离用户侧与物品侧信息流，实现密集交互模型中用户侧计算的复用，并结合信息补偿和W8A16量化，在不损性能前提下降低20%推理延迟。


<details>
  <summary>Details</summary>
Motivation: 大型推荐模型因密集特征交互导致用户侧计算难以复用，带来高昂的训练与推理成本。

Method: 提出User-Group Separation（UG-Sep）框架：1）在token-mixing层引入掩码机制，显式分离用户与物品信息流，保留纯用户表征以支持跨样本复用；2）设计信息补偿策略重建被抑制的用户-物品交互；3）集成W8A16权重量化缓解内存带宽瓶颈。

Result: 在字节跳动大规模线上A/B实验中，UG-Sep将推理延迟最高降低20%，且未损害用户体验与商业指标（如点击率、收入等），适用于信息流推荐与广告系统。

Conclusion: UG-Sep首次实现了密集特征交互推荐模型中用户侧计算的有效复用，兼顾效率与表达能力，为工业级大模型推理优化提供了新范式。

Abstract: Driven by scaling laws, recommender systems increasingly rely on large-scale models to capture complex feature interactions and user behaviors, but this trend also leads to prohibitive training and inference costs. While long-sequence models(e.g., LONGER) can reuse user-side computation through KV caching, such reuse is difficult in dense feature interaction architectures(e.g., RankMixer), where user and group (candidate item) features are deeply entangled across layers. In this work, we propose User-Group Separation (UG-Sep), a novel framework that enables reusable user-side computation in dense interaction models for the first time. UG-Sep introduces a masking mechanism that explicitly disentangles user-side and item-side information flows within token-mixing layers, ensuring that a subset of tokens to preserve purely user-side representations across layers. This design enables corresponding token computations to be reused across multiple samples, significantly reducing redundant inference cost. To compensate for potential expressiveness loss induced by masking, we further propose an Information Compensation strategy that adaptively reconstructs suppressed user-item interactions. Moreover, as UG-Sep substantially reduces user-side FLOPs and exposes memory-bound components, we incorporate W8A16 (8-bit weight, 16-bit activation) weight-only quantization to alleviate memory bandwidth bottlenecks and achieve additional acceleration. We conduct extensive offline evaluations and large-scale online A/B experiments at ByteDance, demonstrating that UG-Sep reduces inference latency by up to 20 percent without degrading online user experience or commercial metrics across multiple business scenarios, including feed recommendation and advertising systems.

</details>


### [332] [ChainRec: An Agentic Recommender Learning to Route Tool Chains for Diverse and Evolving Interests](https://arxiv.org/abs/2602.10490)
*Fuchun Li,Qian Li,Xingyu Gao,Bocheng Pan,Yang Wu,Jun Zhang,Huan Yu,Jie Jiang,Jinsheng Xiao,Hailong Shi*

Main category: cs.IR

TL;DR: 本文提出ChainRec，一种基于规划器动态选择推理工具的智能体推荐系统，通过标准化工具库和偏好优化规划，在冷启动和兴趣变化场景中显著提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的推荐系统多采用固定推理流程，难以适应用户上下文（如冷启动、兴趣漂移）的多样性，亟需能自适应决策的智能体推荐方法。

Method: 构建标准化的工具智能体库，并通过监督微调与偏好优化训练一个规划器，使其能动态选择工具、决定执行顺序并判断终止时机。

Result: 在AgentRecBench数据集（Amazon/Yelp/Goodreads）上，ChainRec在Avg HR@{1,3,5}指标上持续超越强基线，尤其在冷启动和兴趣演化场景中增益显著；消融实验验证了工具标准化与偏好优化规划的关键作用。

Conclusion: 动态工具规划机制比固定推理流程更适配真实推荐场景的多样性，ChainRec为构建自适应、可扩展的智能体推荐系统提供了有效范式。

Abstract: Large language models (LLMs) are increasingly integrated into recommender systems, motivating recent interest in agentic and reasoning-based recommendation. However, most existing approaches still rely on fixed workflows, applying the same reasoning procedure across diverse recommendation scenarios. In practice, user contexts vary substantially-for example, in cold-start settings or during interest shifts, so an agent should adaptively decide what evidence to gather next rather than following a scripted process. To address this, we propose ChainRec, an agentic recommender that uses a planner to dynamically select reasoning tools. ChainRec builds a standardized Tool Agent Library from expert trajectories. It then trains a planner using supervised fine-tuning and preference optimization to dynamically select tools, decide their order, and determine when to stop. Experiments on AgentRecBench across Amazon, Yelp, and Goodreads show that ChainRec consistently improves Avg HR@{1,3,5} over strong baselines, with especially notable gains in cold-start and evolving-interest scenarios. Ablation studies further validate the importance of tool standardization and preference-optimized planning.

</details>


### [333] [Boundary-Aware Multi-Behavior Dynamic Graph Transformer for Sequential Recommendation](https://arxiv.org/abs/2602.10493)
*Jingsong Su,Xuetao Ma,Mingming Li,Qiannan Zhu,Yu Guo*

Main category: cs.IR

TL;DR: 本文提出了一种边界感知的多行为动态图Transformer（MB-DGT）模型，通过动态更新图结构并结合Transformer建模用户序列行为，同时设计用户特定的多行为损失函数以区分不同行为的兴趣边界，从而提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时建模图拓扑的动态性、用户交互的序列性以及多种用户行为间的兴趣边界。

Method: 提出MB-DGT模型：1）基于Transformer的动态图聚合器，融合时变图结构与行为序列；2）用户特定的多行为损失函数，显式划分不同行为的兴趣边界。

Result: 在三个数据集上的实验表明，该模型持续取得显著更优的推荐性能。

Conclusion: 动态图建模与边界感知的多行为优化协同提升了用户偏好表征能力，为多行为序列推荐提供了新范式。

Abstract: In the landscape of contemporary recommender systems, user-item interactions are inherently dynamic and sequential, often characterized by various behaviors. Prior research has explored the modeling of user preferences through sequential interactions and the user-item interaction graph, utilizing advanced techniques such as graph neural networks and transformer-based architectures. However, these methods typically fall short in simultaneously accounting for the dynamic nature of graph topologies and the sequential pattern of interactions in user preference models. Moreover, they often fail to adequately capture the multiple user behavior boundaries during model optimization. To tackle these challenges, we introduce a boundary-aware Multi-Behavioral Dynamic Graph Transformer (MB-DGT) model that dynamically refines the graph structure to reflect the evolving patterns of user behaviors and interactions. Our model involves a transformer-based dynamic graph aggregator for user preference modeling, which assimilates the changing graph structure and the sequence of user behaviors. This integration yields a more comprehensive and dynamic representation of user preferences. For model optimization, we implement a user-specific multi-behavior loss function that delineates the interest boundaries among different behaviors, thereby enriching the personalized learning of user preferences. Comprehensive experiments across three datasets indicate that our model consistently delivers remarkable recommendation performance.

</details>


### [334] [Campaign-2-PT-RAG: LLM-Guided Semantic Product Type Attribution for Scalable Campaign Ranking](https://arxiv.org/abs/2602.10577)
*Yiming Che,Mansi Mane,Keerthi Gopalakrishnan,Parisa Kaghazgaran,Murali Mohana Krishna Dandu,Archana Venkatachalapathy,Sinduja Subramaniam,Yokila Arora,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.IR

TL;DR: 本文提出Campaign-2-PT-RAG框架，利用大语言模型（LLM）解析电商广告活动内容、语义检索平台类目、再经结构化LLM分类器筛选相关产品类型（PT），从而自动生成用户-活动购买标签，解决广告归因模糊问题，显著提升下游排序模型训练效果。


<details>
  <summary>Details</summary>
Motivation: 电商广告活动排序模型缺乏大规模、高质量的训练标签（即用户因某活动而购买），因为活动文案创意性强、与具体商品无直接映射，导致难以进行产品级归因和监督学习。

Method: 提出Campaign-2-PT-RAG框架：1）用LLM解析活动文本以捕捉隐含意图；2）在平台类目体系中语义检索候选产品类型（PT）；3）使用结构化LLM分类器评估各PT与活动的相关性，生成活动专属的产品覆盖集；4）将用户实际购买匹配该集合的PT视为正样本标签。

Result: 在内部及合成数据集上实验表明，所生成标签精度达78%–90%，召回率超99%，且经专家标注验证效果可靠。

Conclusion: 将模糊的归因问题转化为可解的语义对齐任务，实现了可扩展、一致性强的监督信号生成，有效支撑电商生产环境中广告活动排序优化等下游任务。

Abstract: E-commerce campaign ranking models require large-scale training labels indicating which users purchased due to campaign influence. However, generating these labels is challenging because campaigns use creative, thematic language that does not directly map to product purchases. Without clear product-level attribution, supervised learning for campaign optimization remains limited. We present \textbf{Campaign-2-PT-RAG}, a scalable label generation framework that constructs user--campaign purchase labels by inferring which product types (PTs) each campaign promotes. The framework first interprets campaign content using large language models (LLMs) to capture implicit intent, then retrieves candidate PTs through semantic search over the platform taxonomy. A structured LLM-based classifier evaluates each PT's relevance, producing a campaign-specific product coverage set. User purchases matching these PTs generate positive training labels for downstream ranking models. This approach reframes the ambiguous attribution problem into a tractable semantic alignment task, enabling scalable and consistent supervision for downstream tasks such as campaign ranking optimization in production e-commerce environments. Experiments on internal and synthetic datasets, validated against expert-annotated campaign--PT mappings, show that our LLM-assisted approach generates high-quality labels with 78--90% precision while maintaining over 99% recall.

</details>


### [335] [S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage](https://arxiv.org/abs/2602.10606)
*Jie Jiang,Hongbo Tang,Wenjie Wu,Yangru Huang,Zhenmao Li,Qian Li,Changping Wang,Jun Zhang,Huan Yu*

Main category: cs.IR

TL;DR: 本文提出S-GRec框架，通过解耦在线轻量生成器与离线LLM语义判别器，在不依赖实时大模型推理的前提下，提升推荐效果并兼顾平台商业目标。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型缺乏对用户意图的强监督信号；直接使用大语言模型（LLM）存在语义信号与商业目标冲突、推理开销过大两大工业落地障碍。

Method: 提出S-GRec框架：1）两阶段个性化语义判别器（PSJ），基于成对反馈学习用户条件下的语义奖励聚合；2）非对称优势策略优化（A2PO），以商业指标（如eCPM）为优化锚点，仅在语义优势与之协同时注入语义增益。

Result: 在公开基准和大规模生产系统中验证有效：线上A/B测试显著提升CTR，并带来1.19%的GMV增长，且无需实时LLM推理。

Conclusion: S-GRec实现了语义增强与商业目标的协同优化，兼顾可解释性、稳定性与工业可扩展性，为LLM赋能推荐系统提供了实用新范式。

Abstract: Generative recommendation models sequence generation to produce items end-to-end, but training from behavioral logs often provides weak supervision on underlying user intent. Although Large Language Models (LLMs) offer rich semantic priors that could supply such supervision, direct adoption in industrial recommendation is hindered by two obstacles: semantic signals can conflict with platform business objectives, and LLM inference is prohibitively expensive at scale. This paper presents S-GRec, a semantic-aware framework that decouples an online lightweight generator from an offline LLM-based semantic judge for train-time supervision. S-GRec introduces a two-stage Personalized Semantic Judge (PSJ) that produces interpretable aspect evidence and learns user-conditional aggregation from pairwise feedback, yielding stable semantic rewards. To prevent semantic supervision from deviating from business goals, Asymmetric Advantage Policy Optimization (A2PO) anchors optimization on business rewards (e.g., eCPM) and injects semantic advantages only when they are consistent. Extensive experiments on public benchmarks and a large-scale production system validate both effectiveness and scalability, including statistically significant gains in CTR and a 1.19\% lift in GMV in online A/B tests, without requiring real-time LLM inference.

</details>


### [336] [A Cognitive Distribution and Behavior-Consistent Framework for Black-Box Attacks on Recommender Systems](https://arxiv.org/abs/2602.10633)
*Hongyue Zhan,Mingming Li,Dongqin Liu,Hui Wang,Yaning Zhang,Xi Zhou,Honglei Lv,Jiao Dai,Jizhong Han*

Main category: cs.IR

TL;DR: 本文提出了一种双增强黑盒攻击框架，通过认知分布驱动的提取机制和行为感知的噪声物品生成策略，提升对序列推荐系统的攻击效果与隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒提取攻击忽略排序位置重要性，且生成的对抗序列缺乏语义一致性，易被检测。

Method: 1）基于首因效应和位置偏差，提出认知分布驱动的提取机制，将离散排序映射为带位置衰减的连续值分布；2）设计行为感知的噪声物品生成策略，联合优化协同信号与梯度信号。

Result: 在多个数据集上实验表明，该方法在攻击成功率和规避率上均显著优于现有方法。

Conclusion: 融合认知建模与行为一致性的思路对提升推荐系统安全性具有重要价值。

Abstract: With the growing deployment of sequential recommender systems in e-commerce and other fields, their black-box interfaces raise security concerns: models are vulnerable to extraction and subsequent adversarial manipulation. Existing black-box extraction attacks primarily rely on hard labels or pairwise learning, often ignoring the importance of ranking positions, which results in incomplete knowledge transfer. Moreover, adversarial sequences generated via pure gradient methods lack semantic consistency with real user behavior, making them easily detectable. To overcome these limitations, this paper proposes a dual-enhanced attack framework. First, drawing on primacy effects and position bias, we introduce a cognitive distribution-driven extraction mechanism that maps discrete rankings into continuous value distributions with position-aware decay, thereby advancing from order alignment to cognitive distribution alignment. Second, we design a behavior-aware noisy item generation strategy that jointly optimizes collaborative signals and gradient signals. This ensures both semantic coherence and statistical stealth while effectively promoting target item rankings. Extensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods in both attack success rate and evasion rate, validating the value of integrating cognitive modeling and behavioral consistency for secure recommender systems.

</details>


### [337] [EST: Towards Efficient Scaling Laws in Click-Through Rate Prediction via Unified Modeling](https://arxiv.org/abs/2602.10811)
*Mingyang Liu,Yong Bai,Zhangming Chan,Sishuo Chen,Xiang-Rong Sheng,Han Zhu,Jian Xu,Xinyang Chen*

Main category: cs.IR

TL;DR: 本文提出了一种高效可扩展的Transformer模型（EST），用于工业级点击率（CTR）预测，通过全统一建模和两种新注意力机制（LCA与CSA）避免行为特征聚合导致的信息损失，在淘宝广告平台部署后显著提升RPM和CTR。


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测方法通过早期用户行为聚合来保证效率，但会丢失细粒度的token级信号，形成信息瓶颈，限制了模型扩展潜力。

Method: 提出Efficiently Scalable Transformer（EST），实现所有原始输入的单序列全统一建模；引入Lightweight Cross-Attention（LCA）减少冗余自交互、聚焦跨特征依赖；引入Content Sparse Attention（CSA）基于内容相似性动态筛选高信号行为。

Result: EST展现出稳定高效的幂律缩放关系；在淘宝展示广告平台部署后，RPM提升3.27%，CTR提升1.22%。

Conclusion: EST为工业级CTR预测提供了兼顾效率、可扩展性与性能的实用新范式，验证了全统一建模与内容感知稀疏注意力的有效性。

Abstract: Efficiently scaling industrial Click-Through Rate (CTR) prediction has recently attracted significant research attention. Existing approaches typically employ early aggregation of user behaviors to maintain efficiency. However, such non-unified or partially unified modeling creates an information bottleneck by discarding fine-grained, token-level signals essential for unlocking scaling gains. In this work, we revisit the fundamental distinctions between CTR prediction and Large Language Models (LLMs), identifying two critical properties: the asymmetry in information density between behavioral and non-behavioral features, and the modality-specific priors of content-rich signals. Accordingly, we propose the Efficiently Scalable Transformer (EST), which achieves fully unified modeling by processing all raw inputs in a single sequence without lossy aggregation. EST integrates two modules: Lightweight Cross-Attention (LCA), which prunes redundant self-interactions to focus on high-impact cross-feature dependencies, and Content Sparse Attention (CSA), which utilizes content similarity to dynamically select high-signal behaviors. Extensive experiments show that EST exhibits a stable and efficient power-law scaling relationship, enabling predictable performance gains with model scale. Deployed on Taobao's display advertising platform, EST significantly outperforms production baselines, delivering a 3.27\% RPM (Revenue Per Mile) increase and a 1.22\% CTR lift, establishing a practical pathway for scalable industrial CTR prediction models.

</details>


### [338] [Training-Induced Bias Toward LLM-Generated Content in Dense Retrieval](https://arxiv.org/abs/2602.10833)
*William Xion,Wolfgang Nejdl*

Main category: cs.IR

TL;DR: 本研究通过控制实验重新审视了密集检索器中对大语言模型（LLM）生成文本的偏好现象（即‘源偏差’），发现该偏差主要源于监督微调阶段（尤其是使用MS MARCO或LLM生成数据训练时），而非模型固有属性；且困惑度（perplexity）并不能有效解释该偏差。


<details>
  <summary>Details</summary>
Motivation: 近期观察到密集检索器倾向于偏好LLM生成的文本（源偏差），并假设低困惑度是其成因；本文旨在通过系统性控制实验验证该假设，并追溯偏差在不同训练阶段和数据源下的演化规律。

Method: 在SciFact和NQ320K数据集上构建人类撰写与LLM生成文本的平行对照；对比无监督检查点、以及分别在人工文本、LLM生成文本和MS MARCO上微调的密集检索器；引入retriever-centric perplexity probe（重接语言建模头）检验困惑度与相关性的关联。

Result: 1) 无监督检索器无统一LLM偏好，偏好方向与强度因数据集而异；2) MS MARCO监督微调一致增强对LLM文本的偏好；3) 领域内微调效果不一致；4) 在LLM生成语料上微调会显著诱发强源偏差；5) 困惑度探针显示其与相关性判断接近随机水平，削弱困惑度解释力。

Conclusion: 源偏差是训练过程（特别是监督微调）诱导的现象，而非密集检索器的内在属性；单纯依赖LLM生成数据训练会加剧该偏差，需谨慎设计训练数据与目标一致性。

Abstract: Dense retrieval is a promising approach for acquiring relevant context or world knowledge in open-domain natural language processing tasks and is now widely used in information retrieval applications. However, recent reports claim a broad preference for text generated by large language models (LLMs). This bias is called "source bias", and it has been hypothesized that lower perplexity contributes to this effect. In this study, we revisit this claim by conducting a controlled evaluation to trace the emergence of such preferences across training stages and data sources. Using parallel human- and LLM-generated counterparts of the SciFact and Natural Questions (NQ320K) datasets, we compare unsupervised checkpoints with models fine-tuned using in-domain human text, in-domain LLM-generated text, and MS MARCO. Our results show the following: 1) Unsupervised retrievers do not exhibit a uniform pro-LLM preference. The direction and magnitude depend on the dataset. 2) Across the settings tested, supervised fine-tuning on MS MARCO consistently shifts the rankings toward LLM-generated text. 3) In-domain fine-tuning produces dataset-specific and inconsistent shifts in preference. 4) Fine-tuning on LLM-generated corpora induces a pronounced pro-LLM bias. Finally, a retriever-centric perplexity probe involving the reattachment of a language modeling head to the fine-tuned dense retriever encoder indicates agreement with relevance near chance, thereby weakening the explanatory power of perplexity. Our study demonstrates that source bias is a training-induced phenomenon rather than an inherent property of dense retrievers.

</details>
