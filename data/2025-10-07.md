<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 171]
- [cs.CL](#cs.CL) [Total: 111]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.LG](#cs.LG) [Total: 285]
- [cs.GR](#cs.GR) [Total: 14]
- [cs.AI](#cs.AI) [Total: 90]
- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: 提出了一种名为SoC-DT的可微分框架，用于在标准治疗下个性化预测肿瘤动态，结合反应-扩散模型与真实临床干预，在合成和真实胶质瘤数据上优于传统PDE和纯数据驱动模型。


<details>
  <summary>Details</summary>
Motivation: 现有反应-扩散模型无法捕捉在异质治疗范式下的肿瘤动态，缺乏能够整合基因组、人口统计学和治疗方案差异的计算框架来准确预测肿瘤轨迹。

Method: 提出了SoC-DT框架，融合反应-扩散肿瘤生长模型与离散的标准治疗干预（手术、化疗、放疗），并引入IMEX-SoC求解器以确保数值稳定性、正性和可扩展性，支持基于患者个体化特征的建模。

Result: 在合成数据和真实胶质瘤数据上，SoC-DT在预测治疗后肿瘤结构方面 consistently 优于经典PDE模型和纯数据驱动的神经网络模型。

Conclusion: SoC-DT通过结合机制可解释性与现代可微分求解技术，为肿瘤学中的患者特异性数字孪生提供了原理性基础，实现了生物学一致的肿瘤动态估计。

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [2] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: 提出了一种结合分布式多GPU推理系统与交互式可视化平台的混合框架，用于分析视频中名人动态。


<details>
  <summary>Details</summary>
Motivation: 在视频内容主导的时代，理解视频结构和动态变得愈发重要。

Method: 采用优化的ONNX模型、异构批处理推理和高吞吐并行技术，构建分布式多GPU推理系统，并结合多种可视化方法进行分析。

Result: 实现了对大规模视频数据的高效处理，生成带时间戳的出现记录，并转化为多维度可视化图表，揭示名人的 prominence、屏幕时间分布、时序动态及共现关系等模式。

Conclusion: 该框架通过连接分布式识别与结构化视觉分析，为娱乐分析、内容创作策略和受众参与研究提供了新可能。

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [3] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: 本研究评估了多种深度学习模型在跨域水下塑料垃圾检测中的鲁棒性，发现轻量级MobileNetV2表现最佳（F1达0.97），而零样本模型CLIP和Gemini各有优劣。


<details>
  <summary>Details</summary>
Motivation: 由于域偏移问题，基于单一数据集训练的视觉系统在新图像上性能下降，因此需要评估模型在跨域场景下的鲁棒性，以提升海洋塑料污染自动检测的可靠性。

Method: 采用CNN（MobileNetV2、ResNet-18、EfficientNet-B0）和视觉Transformer（DeiT-Tiny、ViT-B16）进行有监督训练，并在由不同来源构建的平衡跨域测试集上评估；同时评估零样本模型CLIP ViT-L14和Gemini 2.0 Flash的性能。

Result: MobileNetV2在跨域检测中表现最优（F1=0.97），所有微调模型精度均接近99%，但召回率差异显著；CLIP召回率约80%但精度仅56%，Gemini精度约99%但召回率约81%；错误分析显示模型易将珊瑚纹理、悬浮颗粒和镜面反光误判为塑料。

Conclusion: 轻量级CNN在监督训练下能有效实现跨域水下塑料检测，而大型预训练视觉语言模型虽未微调也展现出互补优势，适用于不同应用场景。

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [4] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: 提出VLCAP，一种结合CLIP-based视觉标签检索与多模态文本生成的阿拉伯语图像描述框架，通过多语言编码器提取可解释的视觉概念，并利用混合词汇和大型视觉语言模型生成文化一致且上下文准确的阿拉伯语描述。


<details>
  <summary>Details</summary>
Motivation: 传统端到端阿拉伯语图像描述模型缺乏可解释性且难以保证文化与语境准确性，因此需要一种基于可视概念检索的可解释生成方法。

Method: 采用mCLIP、AraCLIP和Jina V4三种多语言编码器进行阿拉伯语视觉标签检索，构建包含约2.1万个来自Visual Genome的翻译标签的混合词汇表；将检索到的标签转化为流畅的阿拉伯语提示，并结合原始图像输入至Qwen-VL和Gemini Pro Vision等视觉语言模型生成描述。

Result: 在六种编码器-解码器配置中，mCLIP + Gemini Pro Vision取得最佳BLEU-1（5.34%）和余弦相似度（60.01%），AraCLIP + Qwen-VL获得最高LLM-judge评分（36.33%）。

Conclusion: VLCAP通过结合可解释的视觉概念检索与大模型生成，有效提升了阿拉伯语图像描述的文化一致性与上下文准确性，验证了其两阶段方法的可行性与优势。

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [5] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: 本文对比了EfficientNet-B0和ViT-Base在SpaceNet数据集上的性能，分别在类别不平衡和平衡的条件下进行实验，结果表明两类模型在准确率上表现相近，但CNN在效率上更具优势。


<details>
  <summary>Details</summary>
Motivation: 为了在相同实验条件下公平比较卷积神经网络（CNN）与视觉Transformer（ViT）在遥感图像分类任务中的性能差异，特别是在不同标签分布下的表现。

Method: 采用EfficientNet-B0（CNN）和ViT-Base（Transformer）模型，在SpaceNet数据集上进行控制变量实验，使用相同的预处理、轻量增强策略和训练周期（40轮），并在两类标签分布（自然不平衡五类划分与重采样平衡划分）下评估性能。

Result: 在不平衡数据上，EfficientNet-B0达到93%测试准确率，具有较高的macro-F1和更低延迟；ViT-Base准确率也为93%，但参数更多、运行更慢。在平衡数据上，两者性能均提升，EfficientNet-B0达99%，ViT-Base仍具竞争力，表明数据平衡可缩小架构差距。

Conclusion: 尽管ViT在性能上可与CNN相媲美，但EfficientNet-B0在模型效率和推理速度方面仍具优势；数据平衡能显著提升模型表现并缩小不同架构间的差距，研究强调了公平比较的重要性，并公开了实验资源以支持可复现性。

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [6] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: 本文综述了近五年基于摄像头的AI感知系统在保护弱势道路使用者（VRUs）方面的最新进展，重点涵盖检测、跟踪、轨迹预测和意图识别四项核心任务，并指出了数据、模型和部署方面的四大开放挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于VRU的检测，缺乏对其他关键视觉任务的全面覆盖，且传统基础设施难以应对动态城市环境中的安全挑战。

Method: 系统性回顾过去五年的研究成果，聚焦视觉AI在VRU安全中的四大核心任务：检测与分类、跟踪与重识别、轨迹预测、意图识别与预测。

Result: 梳理了基于摄像头的AI感知系统在VRU安全领域的最新进展，明确了四个关键任务的技术发展脉络与代表性方法。

Conclusion: 该综述为下一代智能交通系统中的VRU安全感知技术提供了基础参考，并从数据、模型和部署角度提出了未来研究方向。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [7] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: 本文研究了地球观测基础模型（EOFMs）在不同传感器架构下的表示空间敏感性，指出当前EOFM设计中存在的问题，并为未来的发展提供了方向。


<details>
  <summary>Details</summary>
Motivation: 大多数EOFM仅基于单一模态数据训练，并在跨模态任务中应用，但传感器架构对模型内部表示的影响尚不明确。

Method: 分析预训练EOFM在不同传感器架构下的嵌入表示，评估其对模型表征空间的影响。

Result: 发现EOFM的表示空间对传感器架构高度敏感，不同架构导致显著的表示差异。

Conclusion: 理解传感器架构的影响有助于避免当前EOFM设计的陷阱，并推动更稳健的多模态地球观测模型发展。

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [8] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: 提出一种基于修复引导的扰动解释方法，生成保持场景上下文的逼真编辑，用于揭示视觉模型在生态监测中的决策依据。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型预测缺乏透明度，限制了其在生态监测中的可信度和实际应用。

Method: 采用基于修复的扰动技术，结合Segment-Anything-Model优化的掩码，实现对象移除/替换和背景替换，并通过重新评分和专家评审评估解释效果。

Result: 该方法能有效定位关键形态特征，避免传统扰动带来的伪影，提升解释的生态合理性和可解释性。

Conclusion: 所提方法增强了AI模型在生态学中应用的信任度，支持专家验证并促进更可靠的部署。

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [9] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: 本文综述了医学图像分割（MIS）的各类方法，涵盖传统图像处理与现代深度学习技术，并探讨了注意力机制、半监督学习、GANs和Transformer等前沿趋势，同时通过腰椎分割案例研究展示了该领域的挑战与进展。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在精准诊断、治疗规划和疾病监测中至关重要，但面临数据标注不足、模型泛化能力差和临床集成难等问题，需系统梳理现有方法并探索新兴方向以推动领域发展。

Method: 采用系统性文献综述方法，整合传统图像分割技术（如阈值法、边缘检测、聚类等）与深度学习模型（如CNN、FCN、U-Net及其变体），并分析注意力机制、GANs、Transformer及联邦学习等新兴技术的应用。

Result: 全面总结了MIS的发展脉络与最新进展，揭示了混合架构、跨模态学习、分布式学习等趋势的有效性，并指出了当前在数据偏差、领域适应性和模型可解释性方面的局限性。

Conclusion: 尽管MIS在技术和应用上取得显著进展，但仍需解决数据质量、模型泛化与临床实际集成等关键挑战，未来应聚焦于更高效、可解释且适用于真实医疗环境的分割方法。

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [10] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: 提出一种名为DECOR的深度聚类框架，用于在无标签、不平衡且复杂的晶圆缺陷数据中实现具有方向鲁棒性的自动聚类，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 晶圆制造中缺陷检测对良率优化至关重要，但原始数据常存在无标签、不平衡和多缺陷共存等问题，传统聚类方法难以稳定有效处理。

Method: 提出DECOR框架，结合深度聚类与方向不变性设计，通过学习对旋转和对齐变化鲁棒的特征表示，实现对晶圆图中复杂缺陷模式的一致性聚类。

Result: 在公开数据集MixedWM38上验证了DECOR的有效性，无需人工调参即可发现缺陷簇，且在不同方向变化下保持聚类稳定性，性能优于现有基线方法。

Conclusion: DECOR为自动化视觉检测系统提供了一种可靠且可扩展的晶圆缺陷聚类解决方案，特别适用于存在方向变异的实际工业场景。

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [11] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: 本文提出了一种基于LSTM和注意力机制的神经网络模型，用于解决不平衡样本下多类人脸图像分类中的错误校正问题。通过在六类子集上训练并对第七类进行错误校正，实验表明该方法能有效提升稀有类别的识别性能，尤其适用于情感识别及反欺诈等罕见事件检测场景。


<details>
  <summary>Details</summary>
Motivation: 由于面部表情数据集中存在类别不平衡问题（某些情绪样本远多于其他），传统分类模型对少数类的识别效果较差，因此需要一种能够在不均衡分布下稳定分类并实现错误校正的方法。

Method: 采用带有注意力机制的LSTM神经网络模型，关注面部关键区域；在训练阶段排除一个类别（共七类），使用其余六类组合进行训练，并对被排除类别进行错误校正测试。

Result: 所有类别均可实现一定程度的错误校正，部分类别恢复效果显著；在测试集上，少数类的关键质量指标有所提升，验证了方法的有效性。

Conclusion: 所提方法在处理类别不平衡的面部表情分类任务中表现出良好性能，尤其有助于提升稀有类别的识别准确率，具有在实际应用（如反欺诈系统）中推广的潜力。

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [12] [Textured Gaussians for Enhanced 3D Scene Appearance Modeling](https://arxiv.org/abs/2411.18625)
*Brian Chao,Hung-Yu Tseng,Lorenzo Porzi,Chen Gao,Tuotuo Li,Qinbo Li,Ayush Saraf,Jia-Bin Huang,Johannes Kopf,Gordon Wetzstein,Changil Kim*

Main category: cs.CV

TL;DR: 提出了一种新的3D高斯外观表示方法，通过为每个高斯分布添加纹理映射（A、RGB或RGBA）来增强其表达能力，显著提升了3D高斯点阵在细节表现和渲染质量上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯点阵中每个高斯仅能表示单一颜色和椭球形状，限制了其表达能力，无法呈现复杂纹理和几何结构。

Method: 受传统图形学中纹理和Alpha映射启发，为每个高斯引入Alpha、RGB或RGBA纹理映射，使其能够表达空间变化的颜色和不透明度，从而增强单个高斯的表达能力。

Result: 实验表明，使用Alpha纹理即可显著提升表达能力，结合RGB纹理效果更优；在多个标准数据集和自建数据上验证了该方法在图像质量上的提升，且使用的高斯数量相当或更少。

Conclusion: 所提出的方法有效增强了3D高斯点阵的表达能力，实现了更丰富的纹理和几何细节建模，优于现有方法。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D
reconstruction and rendering technique due to its high-quality results and fast
training and rendering time. However, pixels covered by the same Gaussian are
always shaded in the same color up to a Gaussian falloff scaling factor.
Furthermore, the finest geometric detail any individual Gaussian can represent
is a simple ellipsoid. These properties of 3DGS greatly limit the expressivity
of individual Gaussian primitives. To address these issues, we draw inspiration
from texture and alpha mapping in traditional graphics and integrate it with
3DGS. Specifically, we propose a new generalized Gaussian appearance
representation that augments each Gaussian with alpha~(A), RGB, or RGBA texture
maps to model spatially varying color and opacity across the extent of each
Gaussian. As such, each Gaussian can represent a richer set of texture patterns
and geometric structures, instead of just a single color and ellipsoid as in
naive Gaussian Splatting. Surprisingly, we found that the expressivity of
Gaussians can be greatly improved by using alpha-only texture maps, and further
augmenting Gaussians with RGB texture maps achieves the highest expressivity.
We validate our method on a wide variety of standard benchmark datasets and our
own custom captures at both the object and scene levels. We demonstrate image
quality improvements over existing methods while using a similar or lower
number of Gaussians.

</details>


### [13] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: 提出了首个动态图表生成基准DCG-Bench和高质量数据集DCG-8K，并通过两阶段训练方法提升了多模态大模型在动态图表生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注静态图表生成，而多模态大语言模型在动态图表生成与理解方面的能力尚未充分探索，存在研究空白。

Method: 构建了包含指令-代码-视频三元组和问答对的DCG-8K数据集，提出两阶段训练策略及联合代码-视觉奖励机制，用于优化Qwen2.5-VL-DCG-3B模型。

Result: 实验表明现有MLLMs在视觉到图表的任务中存在不足，所提模型在三个任务上平均性能提升8.31%，且以3B参数达到与闭源模型相当的水平。

Conclusion: 所提出的DCG-Bench和训练方法有效推动了动态图表生成领域的发展，验证了小规模模型通过优化训练策略可媲美大型闭源模型。

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [14] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: 提出了一种端到端的单目视觉里程计方法VoT，基于Transformer架构，无需传统手工模块，直接预测相机运动。


<details>
  <summary>Details</summary>
Motivation: 现有单目视觉里程计方法依赖复杂流程和精细调参，在未见场景中表现不佳；现有大模型难以处理长序列和逐帧精确估计。

Method: 设计Visual odometry Transformer（VoT），通过时空注意力机制建模单目图像序列的全局关系，提取特征并直接预测相机运动，仅以相机姿态作为监督信号，支持多种预训练编码器。

Result: VoT在更大数据集上表现更好，显著受益于更强的预训练主干网络，跨不同相机运动和标定设置具有良好的泛化能力，性能优于传统方法且速度提升三倍以上。

Conclusion: VoT实现了高效、灵活、端到端的单目视觉里程计，摆脱了对传统几何模块的依赖，为视觉里程计提供了新的可行路径。

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [15] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: 提出了一种基于侧信息的推理时搜索算法，用于改进扩散模型在图像重建中的性能，相较于梯度引导方法更稳定且效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在解决逆问题时通常忽略可能显著提升重建质量的侧信息，尤其在严重不适定情况下表现不佳。

Method: 设计了一种新的推理时搜索算法，在采样过程中利用侧信息平衡探索与利用，避免梯度引导带来的奖励欺骗伪影。

Result: 在多种逆问题（如补全、超分辨率和各类去模糊）上验证了该方法的有效性，定性和定量指标均优于现有方法，包括基于奖励梯度的引导方法。

Conclusion: 该方法可无缝集成到现有的扩散模型重建流程中，显著提升重建精度和可靠性。

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [16] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: 本研究提出了一种基于多模态视觉-语言模型和检索增强生成（RAG）的框架SiteShield，用于自动化生成建筑施工安全检查报告。


<details>
  <summary>Details</summary>
Motivation: 传统施工安全检查效率低下，现有大模型应用存在响应不相关、输入模态受限和幻觉等问题，且缺乏实时适应能力。

Method: 结合视觉和音频输入，构建基于多模态大视觉语言模型（LVLM）的检索增强生成（RAG）框架SiteShield，提升信息检索与报告生成能力。

Result: 在真实数据上，SiteShield相比无RAG的单模态LLM表现更优，F1得分为0.82，汉明损失为0.04，精确率为0.76，召回率为0.96。

Conclusion: SiteShield为施工安全检查报告的自动生成提供了新途径，显著提升了信息检索效率和生成质量。

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [17] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: 本文综述了当前声呐图像数据集的研究现状，系统梳理了公开可用的多类型声呐数据集，分析其在分类、检测、分割和三维重建等应用中的特点与不足，并通过主表格和时间线形式提供清晰对比，为水下声学数据分析研究者提供参考指南。


<details>
  <summary>Details</summary>
Motivation: 由于公开、标注良好的声呐图像数据集稀缺，限制了机器学习模型在水下探测等领域的应用与发展，因此亟需对现有数据集进行系统性梳理与评估。

Method: 本文收集并分析了多种声呐模态（如侧扫声呐、前视声呐、合成孔径声呐等）的公开数据集，从应用场景、数据规模、标注信息等方面进行归纳，并构建主表格和时间线以可视化比较。

Result: 总结出现有声呐图像数据集的分布特征与技术空白，提供了包含关键属性的综合对比表和时间发展脉络，揭示了部分模态数据集缺乏标注或规模较小的问题。

Conclusion: 该综述为研究人员提供了清晰的数据资源概览和发展路线图，有助于推动水下声学数据驱动模型的发展，并指明未来需要更多高质量、大规模标注数据集的方向。

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [18] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出了一种无需专用硬件的显示器校准方法，结合无透镜相机和基于隐式神经表示的算法，从多视角高效重建显示器发出的光场，简化了校准流程。


<details>
  <summary>Details</summary>
Motivation: 现有的显示器校准方法需要专业设备和暗室环境，难以普及，用户亟需一种便捷、低成本的解决方案。

Method: 协同设计无透镜相机与基于隐式神经表示（INR）的算法，通过捕捉不同视角下的显示特性，实现对显示器光场的高效重建。

Result: 系统能够在46.6°×37.6°的视锥范围内有效重建显示器的光场，显著降低硬件需求和操作复杂性。

Conclusion: 该方法为实现无需专业设备的便捷显示器校准提供了可行路径，推动了显示器标定技术的普及化发展。

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [19] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: 本文提出了“溯源网络”（provenance networks），一种将可解释性内置于模型架构中的新型神经网络，通过在预测时关联支持该预测的训练样本，实现端到端的数据驱动解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型缺乏内在可解释性，后处理解释方法存在局限，难以有效追溯预测依据、识别数据异常或防止幻觉。因此需要一种将可解释性融入模型运行机制的新方法。

Method: 提出溯源网络，其机制类似于可学习的K近邻（KNN），在模型推理过程中显式学习每个预测与相关训练样本之间的关联权重，通过联合优化主任务和可解释性目标来实现。

Result: 模型能够追溯预测依据、验证输入是否在训练集中、检测标签错误或异常数据、增强对输入扰动的鲁棒性，并识别生成新样本的相关输入；同时揭示记忆化与泛化之间的权衡，但计算开销较高且目前仅适用于中等规模数据集。

Conclusion: 溯源网络为深度学习提供了内置的可解释性机制，提升了模型的透明度、鲁棒性和可信度，是对现有解释技术的有益补充，有助于解决模型黑箱、幻觉和数据归因等关键问题。

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [20] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种统一的无监督异常检测框架UCF，通过匹配视角下的成本体积过滤来提升单模态和多模态场景下的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了匹配过程中的噪声问题，且单模态与多模态异常检测研究割裂，缺乏统一视角和知识迁移。

Method: 提出Unified Cost Filtering（UCF），构建测试样本与正常样本的匹配成本体积，并引入可学习的多层注意力引导滤波模块以抑制匹配噪声、突出细微异常。

Result: 在22个多样化基准上实验表明，UCF能有效提升多种UAD方法的性能，在单模态（RGB）和多模态（RGB-3D、RGB-Text）场景下均达到最先进的检测效果。

Conclusion: UCF为单模态和多模态无监督异常检测提供了一个通用且有效的后处理优化框架，显著提升了异常检测精度。

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [21] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: 本文提出了一种利用视觉语言模型（VLM）评估和改进工业图纸中物体检测结果质量的框架，解决了数字化过程中缺乏自动评估方法的问题。


<details>
  <summary>Details</summary>
Motivation: 工业图纸（如P&ID）的数字化是构建数字孪生和实现智能工业自动化的重要步骤，但目前缺乏对物体检测结果进行自动质量评估的有效方法。

Method: 提出一种基于视觉语言模型（VLM）的框架，利用其多模态能力识别检测中的缺失或不一致，实现对物体检测结果的自动质量评估与优化。

Result: 该方法能够有效识别复杂工业图纸中物体检测的错误，并提升整体检测性能。

Conclusion: VLM可用于自动化评估和改进工业图纸的物体检测结果，为工业图纸数字化提供了新的解决方案。

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [22] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出了PaperTalker，首个用于学术报告视频生成的多智能体框架，并构建了包含101篇论文及其配套视频、幻灯片和演讲者元数据的基准数据集。通过设计四种评估指标，验证了该方法在信息忠实度和表达效果上优于现有基线，推动了学术视频自动化生成的发展。


<details>
  <summary>Details</summary>
Motivation: 学术报告视频虽重要，但制作过程耗时耗力，且面临来自研究论文的多模态密集信息整合与多通道（如幻灯片、字幕、语音、讲话人）协调等独特挑战，亟需自动化解决方案。

Method: 提出PaperTalker多智能体框架，集成幻灯片生成、基于新型有效树搜索的布局优化、光标定位、字幕生成、语音合成和虚拟形象渲染，并采用分页并行生成提升效率；同时构建包含101个配对样本的基准数据集及四种定制化评估指标（Meta Similarity, PresentArena, PresentQuiz, IP Memory）。

Result: 在Paper2Video数据集上的实验表明，所生成的学术报告视频在信息保真度和内容传达效果上显著优于现有基线方法。

Conclusion: PaperTalker为学术报告视频的自动化生成提供了有效且实用的解决方案，通过多智能体协同与专门评估体系，推动了该领域的标准化与技术进步。

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


### [23] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: 本文提出了SpatialViLT及其变体，通过引入深度图、3D坐标和边缘图等空间特征，增强了视觉-语言模型的空间推理能力，在VSR数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在3D场景和复杂物体配置的空间推理方面存在不足，需提升模型对空间关系的理解能力。

Method: 提出SpatialViLT模型，结合深度图、3D坐标和边缘图，采用多任务学习框架；设计了全区域（SpatialViLT）和掩码区域（MaskedSpatialViLT）两种变体，并通过SpatialEnsemble融合二者。

Result: 在Visual Spatial Reasoning (VSR) 数据集上表现出色，尤其在方向性、拓扑和邻近关系等空间推理类别中达到最先进准确率。

Conclusion: 该工作显著提升了AI系统的空间智能，对多模态理解和实际应用具有重要意义。

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [24] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本研究利用合成数据训练编码器-解码器网络（如DAE和U-Net）来抑制双相光学切片结构光照明显微（OS-SI）中的伪影，有效提升图像清晰度。


<details>
  <summary>Details</summary>
Motivation: 双相OS-SI成像速度快，但会产生传统去噪方法难以消除的残余伪影，且缺乏干净的真实训练数据用于监督学习。

Method: 构建合成训练数据集，将真实伪影场应用于合成图像，以此训练不对称去噪自编码器（DAE）和U-Net模型，并在真实OS-SI图像上进行评估。

Result: 两种网络均提升了图像清晰度，且对不同类型的伪影各有优势。

Conclusion: 合成数据可用于监督式去噪训练，编码器-解码器网络有望简化OS-SI图像的重建流程。

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [25] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: PEaRL是一种结合组织病理学与空间转录组学的多模态框架，通过通路激活评分而非单个基因来提升跨模态对应性和可解释性，在多种癌症数据集中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖高变基因，忽视了塑造组织表型的协同生物学程序，限制了预测范围和可解释性。

Method: PEaRL使用ssGSEA计算通路激活得分，并通过Transformer编码通路信号，利用对比学习将其与组织学特征对齐，实现降维和增强跨模态关联。

Result: 在乳腺、皮肤和淋巴结三种癌症ST数据集上，PEaRL在基因和通路水平表达预测中均优于SOTA方法，皮尔逊相关系数最高提升58.9%和20.4%。

Conclusion: 将转录组表示基于通路建模可生成更符合生物学实际且可解释的多模态模型，推动计算病理学从基因级嵌入向通路级发展。

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [26] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS是一个用于多模态医学图像分析的深度学习框架，通过分层语义提示和双提示机制实现细粒度任务控制，在分割和预后预测任务中展现出强泛化能力和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分析模型缺乏通用性和预后能力，而现有的通用模型在条件控制和医学语义理解方面表现不足。

Method: 提出DuPLUS框架，采用基于文本控制的分层架构和双提示机制，结合视觉-语言模型，支持多模态图像分析，并通过参数高效微调实现跨任务扩展。

Result: 在10个数据集中8个上超越现有先进模型，支持3种成像模态、30多种器官和肿瘤的分割；集成电子健康记录进行预后预测，在头颈癌数据集上达到0.69的Concordance Index。

Conclusion: DuPLUS具备良好的泛化性、可扩展性和临床适用性，是迈向通用医学图像分析的高效解决方案。

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [27] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: 提出了一种移动端优化的双阶段深度学习框架，结合线程化检测模型（TDM）并行执行YOLOv10检测和MobileSAM分割，显著降低延迟，在霍巴尔鸨数据集上实现了高精度与实时性能。


<details>
  <summary>Details</summary>
Motivation: 野生动物保护中需要在计算资源受限的自然环境中实现实时、非侵入式的动物检测与分割，但物种隐蔽性强且现有方法延迟较高。

Method: 设计了一个移动端优化的两阶段框架，通过引入线程化检测模型（TDM）使YOLOv10检测与MobileSAM分割并行运行，提升处理效率和实时性。

Result: 在自建的4万图像霍巴尔鸨数据集上，YOLOv10达到mAP50为0.9627，单帧处理时间43.7ms；MobileSAM分割mIoU为0.7421，验证了高精度与实时能力。

Conclusion: 所提线程化双阶段框架在低资源环境下实现了高效准确的野生动物检测与分割，适用于野外远程监控，推动了濒危物种的自动化监测。

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [28] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: 提出了Platonic Transformer，通过引入基于柏拉图立体对称性的参考系，在不增加计算成本的情况下实现了对平移和几何对称性的等变性，同时保持标准Transformer的架构和效率。


<details>
  <summary>Details</summary>
Motivation: Transformer缺乏对科学和计算机视觉中常见的几何对称性的归纳偏置，现有等变方法往往牺牲了Transformer的效率和灵活性。

Method: 定义相对于柏拉图立体对称群的参考系的注意力机制，实现原理性的权重共享，并等价于动态群卷积，从而支持高效的线性时间卷积变体。

Result: 在CIFAR-10、ScanObjectNN、QM9和OMol25等多个基准上取得了具有竞争力的性能，且无需额外计算成本。

Conclusion: Platonic Transformer成功结合了几何等变性与标准Transformer的高效灵活架构，为多种任务提供了无额外代价的性能提升。

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [29] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 本文综述了领域泛化语义分割的最新进展，重点分析了向基于基础模型的范式转变，并比较了各类方法的性能，揭示了基础模型对领域泛化的显著影响。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在未知领域的泛化能力仍是一个重大挑战，尤其是在无法获取目标领域数据的场景下，领域泛化（DG）方法需跨多个未见领域实现良好性能，这在语义分割等关键应用中尤为重要。

Method: 对现有领域泛化语义分割方法进行分类和系统回顾，重点分析基于基础模型的新范式，并通过广泛的性能对比评估各类方法的效果。

Result: 明确了领域泛化语义分割领域的研究趋势，即向基础模型驱动的方法转变；性能比较显示基础模型显著提升了泛化能力。

Conclusion: 基础模型正在重塑领域泛化语义分割的研究格局，未来的研究应进一步探索其潜力，推动该领域的创新发展。

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [30] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: 提出了一种基于transformer的视觉编码器和文本解码器的两阶段训练框架，用于自动生成内镜报告，以减轻医生的工作负担。


<details>
  <summary>Details</summary>
Motivation: 内镜检查的文档记录给胃肠病学家带来了巨大负担，导致临床工作流程效率低下和医生 burnout。

Method: 采用基于transformer的视觉编码器和文本解码器，通过第一阶段在图像/文本字幕对上预训练，第二阶段在图像/报告对上微调，生成具有临床意义的发现。

Result: 该模型能够捕捉通用的视觉-语言特征，并生成临床上有意义的检查结果描述。

Conclusion: 该方法有助于简化内镜报告的撰写过程，减少医生工作量，提升患者护理质量。

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [31] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: 提出SketchPlan，一种基于扩散模型的规划器，通过解释深度图像上的2D手绘草图生成无人机导航的3D飞行路径。


<details>
  <summary>Details</summary>
Motivation: 实现人类直观绘制的草图到无人机可执行的3D路径的准确转换，解决真实场景中草图与理想投影差异大的问题。

Method: 采用两阶段方法：SketchAdapter将手绘草图映射为2D路径，DiffPath结合深度图将2D投影扩散生成3D轨迹；使用合成数据与真实人类草图混合训练。

Result: 在模拟和真实环境中验证有效性，真实无人机测试中在低/中等复杂环境中成功率100%，高复杂未知环境达40%，优于消融模型20-60%。

Conclusion: SketchPlan实现了草图到3D路径的零样本仿真到现实迁移，模块化设计与混合数据训练显著提升对人类意图的理解与路径推断能力。

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [32] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: 提出一种基于姿态条件的大间隔对比编码器，从传输的潜在码中提取生物特征信息以检测身份篡改，无需查看重建的RGB视频，实现实时且具有强泛化性的傀儡攻击防御。


<details>
  <summary>Details</summary>
Motivation: AI驱动的虚拟头像视频会议系统通过传输紧凑的姿态-表情潜在码来节省带宽，但该潜在码可能被操控，导致攻击者实时劫持用户形象；现有深度伪造检测方法因视频全为合成帧而失效。

Method: 利用姿态-表情潜在码中包含驱动者身份的生物特征信息，设计一种姿态条件下的大间隔对比编码器，分离出持久的身份特征，消除姿态和表情变化带来的干扰，通过余弦相似度测试检测异常身份切换。

Result: 在多个虚拟头像生成模型上的实验表明，该方法在防御傀儡攻击方面优于现有方法，具备实时性和对分布外场景的强泛化能力。

Conclusion: 该方法首次实现了不依赖RGB视频的生物特征泄露防御，有效检测潜在码层面的身份篡改，为AI虚拟头像系统提供了新的安全防护思路。

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [33] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为REVEL的新任务和无需训练的DragStream方法，用于实现对自回归视频扩散模型输出的流式、细粒度交互式拖拽操控，有效解决了潜在空间漂移和上下文干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频扩散模型难以实现流式、细粒度的用户控制，导致生成结果难以持续符合预期，因此需要一种可在任意时间对任意内容进行拖拽编辑的交互机制。

Method: 提出了REVEL任务和DragStream方法，包括自适应分布自校正策略以抑制潜在空间漂移，以及频域选择性优化机制以利用上下文信息并减少其干扰，整个方法无需训练即可集成到现有模型中。

Result: DragStream在多个自回归视频扩散模型上实现了稳定、自然的流式拖拽编辑，有效缓解了潜在分布漂移和上下文干扰问题，显著提升了交互连续性和视觉质量。

Conclusion: DragStream为视频扩散模型提供了高效、灵活的实时交互能力，推动了用户可控视频生成的发展，具有良好的通用性和应用前景。

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [34] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: GAS-MIL是一种灵活的集成框架，能够无缝整合多个基础模型的特征，避免了手动特征选择和大量任务特定微调，在多种癌症数据集上表现出优越或相当的性能。


<details>
  <summary>Details</summary>
Motivation: 适应和评估单个基础模型在特定诊断任务中的表现通常耗时且资源密集，尤其是在模型规模和多样性较大的情况下。

Method: 提出Group-Aggregative Selection Multi-Instance Learning (GAS-MIL)框架，整合多个基础模型的特征，保留其互补优势。

Result: 在前列腺、卵巢和乳腺癌数据集上，GAS-MIL在分类任务中表现优于或相当于单个基础模型和现有的MIL方法。

Conclusion: GAS-MIL通过高效整合异构基础模型，简化了病理学中的模型部署，为未来的多模态和精准肿瘤学应用提供了可扩展的基础。

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [35] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: 提出了一种基于视频的实时情境感知（SA）评估框架，利用图嵌入和Transformer模型分析无人机辅助纳洛酮递送模拟数据集（DANDSD），以提升旁观者在阿片类药物过量急救中的响应能力。


<details>
  <summary>Details</summary>
Motivation: 解决在无人机辅助纳洛酮递送场景中，旁观者与自主系统协作时实时情境感知评估的研究空白。

Method: 构建DANDSD数据集，结合几何、运动学和交互图特征，采用图嵌入和Transformer模型实现对非医学背景旁观者行为的实时SA评估。

Result: 所提方法在时间片段分割准确性上优于FINCH基线模型9%（MoF）和5%（IoU）。

Conclusion: 该框架有助于开发能实时引导旁观者的自适应无人机系统，提升急救效果，挽救生命。

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [36] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: 本研究评估了四种开源OCR系统（Tesseract、EasyOCR、PaddleOCR和TrOCR）在真实食品包装图像上的性能，重点测试其提取成分列表和营养信息的能力。结果显示Tesseract字符错误率最低，EasyOCR在准确性和多语言支持间表现均衡，PaddleOCR覆盖度高但速度慢，TrOCR表现最弱。研究建立了针对包装图像的基准和基线，指出了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 食品包装上的准确OCR对合规性和营养监测至关重要，但由于多语言文本、密集布局、不同字体、反光和曲面等因素，实现高质量识别具有挑战性。现有OCR系统在通用场景表现良好，但在复杂包装图像上的性能尚不明确，因此需要针对性评估。

Method: 使用包含231种产品（1628张图像）的数据集测试四个OCR系统的速度和覆盖率，并构建包含113张图像（60种产品）的真值子集用于准确率评估。采用字符错误率（CER）、词错误率（WER）、BLEU、ROUGE-L、F1、覆盖率和执行时间等指标进行综合评估。

Result: 在真值子集上，Tesseract的CER最低（0.912），BLEU最高（0.245）；EasyOCR在准确性和多语言支持之间表现平衡；PaddleOCR接近完全覆盖但因仅CPU运行而较慢；TrOCR尽管使用GPU加速，表现最差。

Conclusion: 该研究为食品包装OCR提供了特定基准和性能基线，表明当前开源OCR系统在实际应用中各有优劣，未来需发展更强大的布局感知方法和文本定位技术以应对复杂包装场景。

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [37] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: 提出FrameOracle，一种轻量级模块，通过预测最相关帧和所需帧数来优化视频理解中视觉-语言模型的输入帧选择，显著提升效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有帧采样策略无法适应信息密度和任务复杂性的变化，导致效率低下和信息丢失。

Method: 设计了一个四阶段课程学习框架训练FrameOracle模块，并构建了首个大规模带关键帧标注的VideoQA数据集FrameOracle-41K用于强监督训练。

Result: 在五个视觉-语言模型和六个基准上实验表明，FrameOracle将16帧输入平均减少到10.4帧且不损失精度；从64帧候选中减少到13.9帧的同时精度提升1.4%。

Conclusion: FrameOracle实现了可扩展视频理解中的高效帧选择，在保持甚至提升性能的同时大幅降低计算开销，达到最先进的效率-精度权衡。

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [38] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: 提出一种混合Co-FineTuning（CFT）方法，结合有标签和无标签数据，提升视频游戏视觉缺陷检测的效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 手动识别游戏中的视觉缺陷成本高且依赖专业知识，而监督模型因缺乏充足标注数据（缺陷罕见）难以广泛应用。

Method: 采用CFT方法，利用目标游戏和多个共域游戏的有标签样本，并引入无标签数据增强特征表示学习，减少对目标游戏标注数据的依赖。

Result: 该方法在多个游戏环境中优于传统基线模型，即使仅使用50%的目标游戏标注数据仍保持竞争力。

Conclusion: CFT框架显著提升了视觉缺陷检测的可扩展性和适应性，有效降低标注成本，适用于多种游戏场景。

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [39] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: 本文研究了具有两个Transformer模块、一步训练、深度监督等特性的分层推理模型（HRM）作为图像分类器的实用性，发现在无数据增强的情况下，HRM在MNIST上表现良好，但在CIFAR数据集上过拟合严重，泛化能力差，性能不及简单的卷积网络。


<details>
  <summary>Details</summary>
Motivation: 探索HRM在小分辨率图像分类任务中不使用数据增强时的有效性与局限性。

Method: 采用包含Rotary Position Embeddings和RMSNorm的HRM模型，在MNIST、CIFAR-10和CIFAR-100上进行实验，使用相同的优化器设置和标签平滑，但不使用数据增强。

Result: HRM在MNIST上达到约98%的测试准确率，但在CIFAR-10上仅达到65.0%，CIFAR-100上为29.7%，且存在严重过拟合；相比之下，简单的卷积基线模型训练更快且性能更高。

Conclusion: 当前形式的HRM在无增强的小图像分类任务中不如简单卷积架构，但通过模型改进可能有提升空间。

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [40] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: 本论文综述了自监督学习中的DINOv2方法，分析其核心思想（多裁剪视图增强和均值教师自蒸馏），并与其它SSL和WSL方法比较性能，探讨其在下游任务中的涌现特性、局限性及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着自监督学习的发展，如何学习兼具高层语义和细粒度空间结构的通用视觉特征成为关键问题，DINOv2在多个基准上超越弱监督方法，亟需系统梳理其技术演进与影响。

Method: 通过回顾DINOv2的核心技术——多裁剪视图增强和基于均值教师的自蒸馏，并追溯其在先前工作中的发展脉络，同时对比DINO、DINOv2与其他SSL和WSL方法在多种下游任务上的表现。

Result: 分析表明DINOv2在多数基准上优于现有方法，其基于Transformer的特征展现出显著的涌现能力，如无需微调即可适应多种任务。

Conclusion: DINOv2代表了自监督视觉表示学习的重要进展，尽管存在计算成本高等限制，但仍为未来研究提供了重要方向。

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [41] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Diffusion-Classifier Synergy (DCS)的新框架，通过在扩散模型与少样本类增量学习（FSCIL）分类器之间建立相互增强的循环，解决了FSCIL中的稳定性-可塑性困境和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有FSCIL方法因依赖有限数据集而泛化能力差，且直接应用扩散模型易导致语义错位或引导无效，因此需要一种能有效结合生成模型与分类器协同进化的机制。

Method: DCS采用奖励对齐的学习策略，利用分类器状态构建动态多维度奖励函数，在特征层面通过原型锚定的最大均值差异和逐维方差匹配保证语义一致性与多样性，在logits层面通过置信度重校准和跨会话混淆感知机制提升生成图像的探索性和类别区分性。

Result: 该方法在多个FSCIL基准上实现了最先进的性能，显著提升了知识保持和新类别的学习能力。

Conclusion: DCS通过分类器与扩散模型的协同演化，有效解决了FSCIL中的数据稀缺与语义对齐问题，为增量学习提供了新的范式。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [42] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: 本文提出了一种名为MonitorVLM的视觉-语言框架，用于从监控视频中自动检测矿业中的安全违规行为，显著提升了检测精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查方式劳动强度大、易出错，难以应对大规模动态环境下的安全监控需求，亟需智能化、自动化的解决方案。

Method: 提出MonitorVLM框架，包含三个创新点：构建包含9000个样本的领域特定违规数据集；设计子句过滤（CF）模块以降低推理延迟；引入行为放大器（BM）模块提升细粒度动作识别能力。

Result: 实验结果显示，相比基线模型，MonitorVLM在精确率上提升22.01%，召回率提升34.22%，F1分数提升28.37%；CF模块降低13.56%推理延迟，BM模块提升3.45%精确率和8.62%召回率。

Conclusion: MonitorVLM展示了多模态大模型在矿业等高风险行业安全监控中的巨大潜力，可通过自动化视频分析有效识别不安全行为。

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [43] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: 提出一种结合引导分类与扩散技术的新型混合模型，用于智能交通系统中的事故检测，通过在公开数据集上实现97.32%的准确率，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在处理复杂数据分布时存在局限性，难以有效应对智能交通系统中图像数据的多样性和复杂性，因此需要更强大的模型来提升事故检测性能。

Method: 采用微调后的ExceptionNet架构输出作为扩散模型的输入，并以图像张量为条件；模型包含多个条件模块，利用时间嵌入和图像协变量嵌入调节输入的线性投影，在扩散过程中动态调整网络行为；基于云平台实现以提高可扩展性和计算效率。

Result: 在公开数据集上进行了全面评估和消融研究，结果表明所提模型在图像型事故检测中达到97.32%的准确率，优于基线模型，并验证了不同时间步调度、编码方式和结构设计的影响。

Conclusion: 该扩散模型通过融合条件控制机制和云部署策略，显著提升了事故检测的准确性与鲁棒性，展示了其在智能交通系统中应用的巨大潜力。

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [44] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAMSOD的模型，用于提升RGB-T显著目标检测性能，通过单模态监督和梯度去冲突机制来解决模态不平衡和梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了双模态收敛不平衡以及高低激活神经元之间的显著梯度差异，限制了性能提升。

Method: 引入单模态监督以增强非主导模态的学习，采用梯度去冲突策略减少冲突梯度对收敛的影响，并使用两个解耦适配器分别掩码高激活和低激活神经元，强化背景学习以突出前景对象。

Result: 在多个RGB-T SOD基准数据集上进行了基础实验，并在涂鸦监督RGB-T SOD、全监督RGB-D SOD及铁路表面缺陷检测任务上验证了方法的泛化能力，结果表明所提方法有效。

Conclusion: SAMSOD通过缓解模态不平衡和梯度冲突问题，显著提升了RGB-T显著目标检测性能，并展现出良好的通用性和迁移能力。

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [45] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 本文提出了一种针对小目标指代表达理解（REC）的新数据集SOREC和一种渐进式迭代缩放适配器（PIZA）方法，显著提升了在驾驶场景中小物体的定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有REC方法在定位极小物体时表现不佳，而小物体在自动驾驶等实际应用中至关重要，因此需要专门针对小物体的解决方案。

Method: 构建了包含10万个小目标样本的SOREC数据集，并提出了PIZA适配模块，通过渐进式缩放机制实现参数高效的微调，帮助模型聚焦并精确定位小物体。

Result: 在SOREC数据集上，将PIZA应用于GroundingDINO模型后，显著提高了小目标的定位准确率。

Conclusion: PIZA结合SOREC为小目标REC提供了有效基准和解决方案，推动了视觉-语言模型在精细定位任务中的发展。

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [46] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: 提出一种基于注意力机制的深度学习模型Attention-WNet，用于视网膜动静脉分割，在HRF和DRIVE数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确分割视网膜动脉和静脉对于诊断视网膜疾病及评估全身血管健康具有重要意义，但现有方法仍有提升空间。

Method: 将注意力机制引入WNet深度学习模型，构建新的Attention-WNet模型，用于视网膜动静脉分割。

Result: 在HRF和DRIVE公开数据集上测试显示，该方法性能优于当前最先进的模型。

Conclusion: Attention-WNet能有效提升视网膜动静脉分割精度，具有良好的应用前景。

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [47] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 该研究通过为LAION-400M数据集添加大规模人物中心注释（包括性别、种族/民族标签和自动生成的描述），揭示了视觉-语言模型中训练数据与下游偏见之间的实证联系，发现数据中的共现关系可线性解释60-70%的性别偏见，并揭示了与犯罪和负面内容相关的有害关联。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在大规模多模态数据上表现出显著的人口统计学偏见，但训练数据在其中的作用尚不明确，主要障碍是缺乏如LAION-400M等网络规模数据集的人口统计标注。

Method: 构建了包含2.76亿以上边界框、感知性别与种族/民族标签及自动生成描述的全数据集人物中心注释，采用经过验证的自动化标注流程，结合目标检测、多模态描述生成和微调分类器。

Result: 发现了人口统计不平衡和有害关联，例如男性以及被感知为黑人或中东人者与犯罪相关和负面内容的不成比例关联；并证明CLIP和Stable Diffusion中60-70%的性别偏见可通过数据中的直接共现线性解释。

Conclusion: 所构建资源建立了数据组成与下游模型偏见之间的首个大规模实证联系，表明减少训练数据中的偏见可有效缓解模型输出中的不公平性。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [48] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: 比较了通用预训练神经网络和专用卫星图像预训练网络在检测里约热内卢贫民窟中的性能，探讨任务特异性与数据量对城市非正规居住区检测的影响。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法尚未充分利用最新的预训练神经网络来检测非正规居住区。

Method: 对比两种预训练神经网络：一种是在大规模多样化图像数据集上预训练的通用网络，另一种是在卫星图像上预训练的专用网络。

Result: 研究发现，尽管专用网络更贴近目标任务，但通用网络由于训练数据量更大，在检测性能上表现更优。

Conclusion: 在非正规居住区检测任务中，数据量可能比任务特异性对模型性能影响更大。

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [49] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为低秩适应（LoRA）补丁的新方法，通过向深度伪造生成器注入可学习的LoRA补丁来绕过现有的主动防御机制，并引入多模态特征对齐损失和门控机制提升攻击效果与训练稳定性，同时提出了防御性LoRA补丁作为缓解方案。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造主动防御方法缺乏鲁棒性和可靠性，容易被绕过，因此需要揭示其安全漏洞并推动更强大的防御策略发展。

Method: 提出LoRA补丁方法，将可插拔的LoRA模块注入生成器；设计可学习的门控机制控制补丁影响并防止梯度爆炸；引入多模态特征对齐（MMFA）损失，使对抗输出在语义层面与目标输出对齐；并提出防御性LoRA补丁嵌入可见警告。

Result: 仅用1,000个人脸样本和一个训练周期，LoRA补丁成功击败多种最先进的主动防御方法，验证了当前防御范式的脆弱性。

Conclusion: 当前的主动防御机制存在严重安全漏洞，LoRA补丁是一种高效绕过手段，研究强调了开发更鲁棒深度伪造防御技术的紧迫性。

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [50] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: 提出了一种基于参考集微调（RSF）的方法，利用测试时可用的地图信息（图像和位姿）来缩小训练-测试域差距，显著提升现有视觉地点识别（VPR）方法在挑战性基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VPR方法在训练与测试环境差异较大时表现不佳，需寻找新信息源来缩小域差距。

Method: 在测试前对VPR模型进行参考集微调（RSF），利用目标域的参考图像和位姿信息进行适应性调整。

Result: RSF平均提升了约2.3%的Recall@1性能，且保持了模型泛化能力，在多个不同测试数据集上均有效。

Conclusion: RSF是一种简单但有效的补充策略，可显著提升SOTA VPR方法在挑战性场景下的性能，具有广泛适用性。

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [51] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: 提出ARSAM方法，通过分解并重用梯度显著加速Sharpness-Aware Minimization（SAM），在保持与SAM相当的泛化性能的同时，实现约40%的加速。


<details>
  <summary>Details</summary>
Motivation: SAM虽能提升模型泛化能力，但因其每步需计算两倍梯度，导致计算成本高昂，限制了其实际应用。因此，亟需一种高效且保持性能的SAM加速方法。

Method: 将SAM的梯度分解为SGD梯度和一阶梯度方向上的二阶投影（PSF），发现PSF在训练中动态演化且对寻找平坦极小值至关重要；ARSAM通过自适应采样、重用和混合分解后的梯度，在减少重复计算的同时保持优化方向准确性。

Result: 在CIFAR-10/100上，ARSAM达到与SAM相当的精度，同时提速约40%；在人体姿态估计、模型量化等挑战性任务中也表现出加速效果且不损失性能。

Conclusion: ARSAM有效降低了SAM的计算开销，同时保持其优异的泛化能力，具有广泛的应用前景和实用价值。

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [52] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: 提出了一种名为Concept Prompting and Aggregating (CoPA)的新框架，通过多层概念提取和提示引导显著提升了概念瓶颈模型在医学诊断中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于概念的方法在捕捉细粒度概念时存在不足，主要依赖最后一层特征，忽略浅层和多尺度特征，且缺乏有效的概念编码指导。

Method: 提出CoPA框架，包含Concept-aware Embedding Generator (CEG)从视觉编码器各层提取概念表征，并利用这些表征作为提示进行Concept Prompt Tuning (CPT)，引导模型增强关键视觉线索；同时聚合各层视觉表征以对齐文本概念表征。

Result: 在三个公开数据集上实验表明，CoPA在概念和疾病预测性能上优于当前最先进的方法。

Conclusion: CoPA通过多层特征利用和提示机制有效提升了概念捕捉能力，增强了模型的可解释性与诊断性能。

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [53] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: 本研究探讨了ZFP压缩技术在不降低自动脑血管分割性能的前提下，对大规模3D医学影像数据进行高效压缩的可行性，结果显示ZFP可在高压缩比下保持高分割精度，有助于促进医学影像研究的协作与共享。


<details>
  <summary>Details</summary>
Motivation: 由于3D医学影像数据集日益庞大和复杂，限制了科研合作与数据传输，因此需要一种高效且不影响分析性能的压缩方法来解决这一问题。

Method: 采用ZFP压缩技术的误差容限和固定比率两种模式，应用于包含真实血管标注的大规模3D医学影像数据集，并通过与未压缩数据的基准对比（Dice ≈ 0.8774），评估压缩后数据在自动脑血管分割任务中的表现。

Result: ZFP在误差容限模式下实现了高达22.89:1的压缩比，同时保持了较高的分割质量，平均Dice系数为0.87656，接近未压缩数据的表现。

Conclusion: ZFP是一种可行且强大的工具，能够在几乎不损失分割性能的前提下显著减少3D医学影像数据量，从而提升数据共享效率，推动大规模医学影像研究的合作。

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [54] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: 提出一种融合CNN、Transformer和Mamba注意力机制的三分支编码器-多尺度注意力解码器架构，用于提升医学图像分割的准确性与泛化能力，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型通常任务特定，跨模态和跨解剖区域性能不稳定，且在临床应用中难以平衡模型复杂度与性能。

Method: 设计一种混合分割架构，采用三分支编码器（集成CNN、Transformer和基于Mamba的注意力融合机制）捕捉局部、全局和长程依赖；使用多尺度注意力CNN解码器重建精细分割图，并引入协同注意力门增强跨尺度特征选择与交互。

Result: 在多个基准数据集上实验表明，该方法在分割精度和泛化能力上优于现有最先进方法，同时保持相当的计算复杂度。

Conclusion: 所提出的架构在效率与效果之间实现了良好平衡，为多种医学影像分割任务提供了实用且可扩展的解决方案。

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [55] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 提出一种基于YOLOv9和多边形标注的深度学习方法，用于自动检测道路损坏和井盖，构建了包含千余张图像的新数据集，在破损和非破损类别上表现良好（F1分数分别为86.7%和89.2%），但井盖检测因类别不平衡表现较差（F1分数18.2%），整体图像级准确率为78.1%。


<details>
  <summary>Details</summary>
Motivation: 手动监测道路损坏耗时、昂贵且易出错，亟需自动化、高效且可扩展的解决方案以提升城市基础设施维护效率，特别是在发展中国家。

Method: 采用YOLOv9算法，使用多边形标注替代传统边界框以实现更精确的缺陷定位，并构建了一个主要来自孟加拉国达卡的千余张图像数据集，训练YOLO模型识别三类：破损、未破损和井盖。

Result: 整体图像级准确率达78.1%，破损和未破损类别的F1分数分别为86.7%和89.2%，但井盖检测F1分数仅为18.2%，主要受限于类别不平衡问题。

Conclusion: 该方法为发展中国家的城市基础设施监测提供了一种高效且可扩展的自动化解决方案，尽管在小样本类别（如井盖）检测上仍有改进空间。

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [56] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: 提出了一种基于时间依赖对比学习的扩散模型（Contrastive-SDE），用于无配对图像到图像翻译，在保持语义一致性的同时提升生成效率，无需标签监督且收敛更快。


<details>
  <summary>Details</summary>
Motivation: 在无配对图像翻译中，缺乏对齐样本使得保持语义一致性具有挑战；现有方法可能依赖标签或收敛慢，因此需要一种高效、无需监督的方法。

Method: 结合分数扩散模型（SDE）与时间依赖的对比学习（SimCLR），将图像与其域不变特征作为正样本对进行训练，并用对比模型引导预训练SDE的推理过程。

Result: 在三个常见无配对I2I任务上，使用四项指标评估，Contrastive-SDE达到与当前方法相当的性能，且收敛速度显著更快，无需标签监督或分类器训练。

Conclusion: Contrastive-SDE是一种高效、无需监督的无配对图像翻译方法，通过对比学习引导扩散模型，在保持语义一致性的同时提升了训练效率。

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [57] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: 本文提出了LIBERO-PRO，一个扩展的LIBERO基准，用于在对象、状态、指令和环境四个维度上评估视觉-语言-动作模型的鲁棒性和泛化能力。实验表明，现有模型在标准测试中表现优异，但在新设置下性能急剧下降至0%，暴露出其依赖记忆而非真正理解的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LIBERO基准在训练和评估设置上存在缺陷，导致模型性能被高估，无法进行公平比较。因此需要一个更严格的基准来真实反映模型的泛化与理解能力。

Method: 构建了LIBERO-PRO基准，在四个维度（操作对象、初始状态、任务指令、环境）引入合理扰动，系统评估现有VLA模型在泛化场景下的表现。

Result: 现有模型在标准LIBERO上准确率超90%，但在LIBERO-PRO下性能降至0.0%，显示出对训练数据的记忆依赖，缺乏应对变化的能力，如目标物体更换或指令损坏时仍执行原动作。

Conclusion: 当前VLA模型的评估方法存在严重问题，应摒弃误导性做法，转向更能体现真实理解与泛化能力的鲁棒性评估。

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [58] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: 本文提出了一个名为Mirage的数据集，用于研究当前AI生成图像检测方法的局限性，并探讨了大型视觉语言模型（LVLMs）在可解释AI图像检测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于现代图像生成模型产生的合成图像越来越难以被标准AI检测器识别，尽管人类仍能辨别，因此需要研究这种人机检测能力之间的差距。

Method: 构建了一个包含多种可见伪影的AI生成图像的精选数据集Mirage，并评估了大型视觉语言模型（LVLMs）在该数据集及其他基准数据集上的检测性能。

Result: 实验表明，LVLMs在检测具有明显伪影的AI生成图像时表现良好，但在缺乏此类线索的图像上性能下降。

Conclusion: LVLMs有潜力作为可解释的AI图像检测工具，但其有效性依赖于图像中是否存在可见的人工痕迹，提示未来需提升模型对细微伪造特征的敏感度。

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [59] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: 提出了一种名为UGround的统一视觉定位范式，通过在展开的Transformer中动态选择中间层作为“掩码即提示”，解决了传统方法依赖固定最后一层和缺乏显式空间线索的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位方法依赖固定的最后一层隐藏表示，并使用<SEG>作为提示，导致累积误差和缺乏明确的空间位置信息，限制了模型性能。

Method: 提出Policy-Prompted Masking，包含两个核心组件：Stochastic Skip Connection（SSC）和Mask as Prompt（MasP）。SSC通过强化学习策略动态选择Transformer各层进行跳跃连接；MasP利用选定层中<SEG>令牌与图像令牌的相似性图作为软logit掩码来引导SAM生成掩码。

Result: UGround首次在一个框架下统一了多种视觉定位任务，包括指代表达分割、推理分割、单目标与多目标分割以及含虚假前提的空目标情况，展现出广泛适用性和优越性能。

Conclusion: UGround通过动态层选择和显式空间提示机制，显著提升了视觉定位的灵活性和准确性，为后续研究提供了新的范式。

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


### [60] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: 本文提出了OMG4，一种用于压缩4D高斯点阵的高效框架，通过三阶段高斯精简和改进的量化方法，在保持高质量重建的同时显著减少存储开销。


<details>
  <summary>Details</summary>
Motivation: 4D高斯点阵虽能实现高质量动态场景实时渲染，但存在巨大的存储开销问题，现有方法在压缩比或视觉质量上仍有局限。

Method: 提出OMG4框架，包含三阶段高斯精简：采样关键高斯、剪枝冗余高斯、合并相似高斯；并结合隐式外观压缩与扩展至4D的子向量量化技术。

Result: 在标准数据集上实验表明，OMG4相比当前最先进方法可减少60%以上模型大小，同时保持优异的重建质量。

Conclusion: OMG4在紧凑型4D场景表示方面取得重要进展，为各类应用提供了高效的压缩解决方案。

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

</details>


### [61] [Cross-View Open-Vocabulary Object Detection in Aerial Imagery](https://arxiv.org/abs/2510.03858)
*Jyoti Kini,Rohit Gupta,Mubarak Shah*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架，通过结构化域对齐将地面视角图像中的开放词汇表表示迁移到航拍图像的目标检测中，解决了跨域知识迁移中的域偏移、视角变化和尺度差异问题。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测模型受限于固定类别集合，难以灵活扩展新类别；而现有方法在跨域（如从地面到航拍）迁移时因视角、尺度和域差异导致性能下降。

Method: 提出一种结构化域对齐框架，包括对比图像-图像对齐以增强航拍与地面图像嵌入的相似性，以及多实例词汇关联以对齐航拍图像与文本嵌入。

Result: 在xView、DOTAv2、VisDrone、DIOR和HRRSD数据集上进行了广泛实验，在零样本设置下相比微调的闭集模型，DOTAv2提升+6.32 mAP，VisDrone（Images）提升+4.16 mAP，HRRSD提升+3.46 mAP。

Conclusion: 所提方法有效实现了从地面到航拍图像的开放词汇目标检测，显著提升了零样本检测性能，为航拍应用提供了更灵活、可扩展的目标检测方案。

Abstract: Traditional object detection models are typically trained on a fixed set of
classes, limiting their flexibility and making it costly to incorporate new
categories. Open-vocabulary object detection addresses this limitation by
enabling models to identify unseen classes without explicit training.
Leveraging pretrained models contrastively trained on abundantly available
ground-view image-text classification pairs provides a strong foundation for
open-vocabulary object detection in aerial imagery. Domain shifts, viewpoint
variations, and extreme scale differences make direct knowledge transfer across
domains ineffective, requiring specialized adaptation strategies. In this
paper, we propose a novel framework for adapting open-vocabulary
representations from ground-view images to solve object detection in aerial
imagery through structured domain alignment. The method introduces contrastive
image-to-image alignment to enhance the similarity between aerial and
ground-view embeddings and employs multi-instance vocabulary associations to
align aerial images with text embeddings. Extensive experiments on the xView,
DOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.
Our open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16
mAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when
compared to finetuned closed-vocabulary dataset-specific model performance,
thus paving the way for more flexible and scalable object detection systems in
aerial applications.

</details>


### [62] [Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis](https://arxiv.org/abs/2510.03869)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 本文综述了深度学习在皮肤癌诊断中的应用，讨论了应对复杂特征、图像噪声、类内差异、类间相似性和数据不平衡等挑战的创新方法，并强调了将深度学习模型整合到临床工作流中的重要性。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是全球最常见且致命的癌症之一，早期检测和诊断对改善患者预后至关重要。

Method: 基于PRISMA框架进行系统性文献综述，整合并分析近年来关于深度学习在皮肤疾病诊断中应用的研究进展。

Result: 总结了数据增强、混合模型、特征融合等技术在应对皮肤癌诊断挑战中的有效性，并探讨了深度学习模型在临床工作流程中的集成潜力。

Conclusion: 深度学习在皮肤疾病诊断中具有巨大潜力，但仍需持续的技术进步以充分发挥其在皮肤病学护理中的变革作用。

Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer
worldwide, which highlights the critical importance of early detection and
diagnosis in improving patient outcomes. Deep learning (DL) has shown
significant promise in enhancing the accuracy and efficiency of automated skin
disease diagnosis, particularly in detecting and evaluating skin lesions and
classification. However, there are still several challenges for DL-based skin
cancer diagnosis, including complex features, image noise, intra-class
variation, inter-class similarity, and data imbalance. By synthesizing recent
research, this review discusses innovative approaches to cope with these
challenges, such as data augmentation, hybrid models, and feature fusion, etc.
Furthermore, the review highlights the integration of DL models into clinical
workflows, offering insights into the potential of deep learning to
revolutionize skin disease diagnosis and improve clinical decision-making. This
article follows a comprehensive methodology based on the PRISMA framework and
emphasizes the need for continued advancements to fully unlock the
transformative potential of DL in dermatological care.

</details>


### [63] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出一种新的视觉语言动作框架，通过引入可泛化的动作专家和稀疏3D轨迹作为中间表示，解决现有模型在跨任务泛化和实际执行中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型因数据稀缺和动作模块语义模糊导致泛化能力差，且需针对新环境进行微调，系统间协作机制不明确。

Method: 采用双系统架构，解耦思考与执行；使用稀疏3D航点作为高级规划输出，并由可泛化的动作专家结合实时点云观测生成精细动作序列；提出‘动作预训练+点云微调’的新训练范式。

Result: 实现了跨任务、跨环境的更好泛化能力，减少了对特定领域数据的依赖，并支持大规模联合训练，无需频繁微调。

Conclusion: 该框架有效桥接了高层语义规划与低层动作执行，提升了VLM在物理世界中的实用性与适应性。

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [64] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 提出了一种新的GAN知识蒸馏方法SDAKD，通过引入学生判别器缓解师生网络间的容量不匹配问题，在超分辨率任务中取得了优于基线和现有最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 由于生成对抗网络（GAN）计算量大，难以部署在资源受限设备上，而现有的知识蒸馏方法因学生生成器与教师判别器之间存在容量不匹配，导致压缩效果受限。因此需要一种更有效的GAN压缩方法。

Method: 提出Student Discriminator Assisted Knowledge Distillation (SDAKD)，采用三阶段训练策略，并在后两个阶段引入改进的特征图蒸馏方法，同时引入学生判别器来缩小师生模型之间的能力差距。

Result: 在GCFSR和Real-ESRGAN两种高性能超分辨率GAN上验证了SDAKD的有效性，实验结果显示其在多个指标上均优于基线方法和当前最先进的GAN知识蒸馏方法。

Conclusion: SDAKD有效缓解了GAN知识蒸馏中的容量不匹配问题，显著提升了学生生成器的性能，为GAN在资源受限设备上的部署提供了可行方案。

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in
generative tasks, such as image super-resolution, but their computational
requirements make difficult their deployment on resource-constrained devices.
While knowledge distillation is a promising research direction for GAN
compression, effectively training a smaller student generator is challenging
due to the capacity mismatch between the student generator and the teacher
discriminator. In this work, we propose Student Discriminator Assisted
Knowledge Distillation (SDAKD), a novel GAN distillation methodology that
introduces a student discriminator to mitigate this capacity mismatch. SDAKD
follows a three-stage training strategy, and integrates an adapted feature map
distillation approach in its last two training stages. We evaluated SDAKD on
two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our
experiments demonstrate consistent improvements over the baselines and SOTA GAN
knowledge distillation methods. The SDAKD source code will be made openly
available upon acceptance of the paper.

</details>


### [65] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: 本文提出了OpenFLAME，一个用于室内空间的联邦式视觉定位系统（VPS）后端，通过分布式扫描和维护实现跨设备的6自由度定位，同时解决隐私、覆盖范围和维护问题。


<details>
  <summary>Details</summary>
Motivation: 现有的集中式VPS无法覆盖私有室内空间，受限于隐私、法规和维护成本，难以满足未来AR应用的需求。

Method: 提出OpenFLAME，采用联邦学习框架，支持独立组织各自扫描和维护其空间的3D地图，通过非共享私有数据的方式实现地图间的数据管理与融合。

Result: 解决了分片VPS服务带来的定位一致性、服务质量控制和服务选择等挑战，实现了可扩展、安全且分布式的VPS系统。

Conclusion: OpenFLAME为大规模AR应用提供了可行的去中心化定位解决方案，支持隐私保护、分布式维护和更广覆盖。

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [66] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: 本文提出了PoseGaze-AHP，首个用于AI驱动的眼源性异常头位（AHP）诊断的公开3D数据集，同步捕捉头部姿态与眼球运动信息，结合大语言模型从文献中提取结构化临床数据，并通过Neural Head Avatar框架生成3D表示，支持准确且符合隐私要求的诊断工具开发。


<details>
  <summary>Details</summary>
Motivation: 现有数据集将头位与眼动信息分离，限制了眼源性AHP综合诊断方法的发展，缺乏支持AI研究的集成数据资源。

Method: 利用大语言模型（Claude 3.5 Sonnet）通过迭代式、分层与复杂提示策略从医学文献中提取结构化临床数据，结合步骤化提示方法进行数据清洗与补全，并使用Neural Head Avatar（NHA）框架将数据转化为3D头像表示，生成包含7920张图像的同步头位-眼动数据集。

Result: 成功构建了包含7,920张图像的PoseGaze-AHP数据集，覆盖多种眼部疾病条件，数据提取整体准确率达91.92%，并实现头部姿态与注视方向的3D同步建模。

Conclusion: PoseGaze-AHP是首个面向眼源性AHP智能诊断的公开3D数据集，为AI驱动的临床诊断工具提供了可靠、隐私合规的数据基础，推动了该领域的发展。

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [67] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种针对动态4D数字人模型的质量评估方法，构建了大规模数据集DHQA-4D，并提出了基于多模态大模型的DynaMesh-Rater方法，通过融合视觉、运动和几何特征实现对带纹理和无纹理4D网格的质量评估。


<details>
  <summary>Details</summary>
Motivation: 由于4D数字人模型在采集、压缩和传输过程中易受噪声影响，影响用户体验，因此需要有效的质量评估方法。现有方法难以全面评估动态4D网格质量，尤其是带纹理的情况。

Method: 首先构建了包含32个高质量4D人体序列和1920个失真网格的DHQA-4D数据集及其主观评分；然后提出DynaMesh-Rater方法，从投影视频中提取视觉特征、从剪辑视频中提取运动特征、从4D网格中提取几何特征，并利用大型多模态模型（LMM）融合这些特征，结合LoRA指令微调技术预测质量分数。

Result: 在DHQA-4D数据集上的实验表明，DynaMesh-Rater在带纹理和无纹理4D网格质量评估方面均优于现有方法，具有更高的相关性和准确性。

Conclusion: 本文提出的DHQA-4D数据集和DynaMesh-Rater方法为动态4D数字人质量评估提供了有效解决方案，推动了该领域的研究进展。

Abstract: With the rapid development of 3D scanning and reconstruction technologies,
dynamic digital human avatars based on 4D meshes have become increasingly
popular. A high-precision dynamic digital human avatar can be applied to
various fields such as game production, animation generation, and remote
immersive communication. However, these 4D human avatar meshes are prone to
being degraded by various types of noise during the processes of collection,
compression, and transmission, thereby affecting the viewing experience of
users. In light of this fact, quality assessment of dynamic 4D digital humans
becomes increasingly important. In this paper, we first propose a large-scale
dynamic digital human quality assessment dataset, DHQA-4D, which contains 32
high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D
human meshes degraded by 11 textured distortions, as well as their
corresponding textured and non-textured mean opinion scores (MOSs). Equipped
with DHQA-4D dataset, we analyze the influence of different types of distortion
on human perception for textured dynamic 4D meshes and non-textured dynamic 4D
meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model
(LMM) based approach that is able to assess both textured 4D meshes and
non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts
multi-dimensional features, including visual features from a projected 2D
video, motion features from cropped video clips, and geometry features from the
4D human mesh to provide comprehensive quality-related information. Then we
utilize a LMM model to integrate the multi-dimensional features and conduct a
LoRA-based instruction tuning technique to teach the LMM model to predict the
quality scores. Extensive experimental results on the DHQA-4D dataset
demonstrate the superiority of our DynaMesh-Rater method over previous quality
assessment methods.

</details>


### [68] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: 本文提出了一种名为RASTERIZATION AUGMENTED PLANNING (RAP)的可扩展数据增强方法，用于端到端驾驶规划，通过轻量级光栅化替代昂贵的渲染，结合特征空间对齐实现仿真到现实的有效迁移。


<details>
  <summary>Details</summary>
Motivation: 现有基于专家演示的端到端驾驶模仿学习在闭环部署中缺乏恢复数据，小错误会迅速累积成失败；而现有的高保真数字孪生方法（如神经渲染或游戏引擎）成本过高，难以用于训练。作者认为训练中关键的是语义保真度和可扩展性，而非图像的纹理和光照细节。

Method: 提出3D光栅化技术，使用带标注的几何图元进行轻量级光栅化，生成反事实恢复轨迹和跨智能体视角，并引入Raster-to-Real特征空间对齐方法来缩小仿真与真实世界的差距，从而构建一个高效的闭环训练增强管道RAP。

Result: RAP在四个主流基准测试（NAVSIM v1/v2、Waymo Open Dataset视觉端到端驾驶、Bench2Drive）上实现了最先进的闭环鲁棒性和长尾泛化能力，排名第一。

Conclusion: 轻量级光栅化结合特征对齐足以支持大规模端到端驾驶训练，为高保真渲染提供了高效且实用的替代方案。

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [69] [Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion](https://arxiv.org/abs/2510.03876)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 提出一种基于自适应空间特征融合（ASFF）的改进ResNet-50模型，用于皮肤癌分类，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决皮肤癌分类中类间相似性高、类内差异大和图像噪声等问题。

Method: 在ResNet-50中引入自适应空间特征融合机制，采用双分支结构融合高层语义与中层细节特征，通过全局平均池化和全连接层生成自适应权重进行加权融合。

Result: 在ISIC 2020子集上达到93.18%准确率，AUC分别为0.9670（P-R）和0.9717（ROC），优于5种经典CNN模型，Grad-CAM显示模型能聚焦病灶区域。

Conclusion: 该方法有效提升了特征表示能力与分类性能，为皮肤癌的计算机辅助诊断提供了高效解决方案。

Abstract: Skin cancer classification remains a challenging problem due to high
inter-class similarity, intra-class variability, and image noise in dermoscopic
images. To address these issues, we propose an improved ResNet-50 model
enhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively
integrates multi-scale semantic and surface features to improve feature
representation and reduce overfitting. The ResNet-50 model is enhanced with an
adaptive feature fusion mechanism to achieve more effective multi-scale feature
extraction and improve overall performance. Specifically, a dual-branch design
fuses high-level semantic and mid-level detail features, which are processed
through global average pooling and fully connected layers to generate adaptive
weights for weighted fusion, thereby strengthening feature learning and
reducing the impact of noise on classification. The method is evaluated on a
subset of the ISIC 2020 dataset containing 3297 benign and malignant skin
lesion images. Experimental results show that the proposed ASFF-based ResNet-50
achieves the best overall performance compared with 5 classic convolutional
neural networks (CNNs) models. The proposed model reached an accuracy of 93.18%
along with higher precision, recall, specificity, and F1 score. The improved
model achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,
respectively. Then, the evaluation based on Grad-CAM further proved that the
improved model adaptively focuses on lesion-relevant regions while suppressing
irrelevant background information, thereby validating its enhanced feature
learning capability from a deep representation perspective. These findings
demonstrate that the proposed approach provides a more effective and efficient
solution for computer-aided skin cancer diagnosis.

</details>


### [70] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: 本研究提出了一种多模态深度学习框架，结合临床、影像和组织病理图像，利用DenseNet-121卷积神经网络的加权集成方法提升口腔鳞状细胞癌（OSCC）的早期检测能力。


<details>
  <summary>Details</summary>
Motivation: 由于超过50%的OSCC病例在晚期才被诊断，导致五年生存率低于50%，因此亟需提高早期诊断水平以降低全球死亡率。

Method: 采用迁移学习训练三个模态特定的DenseNet-121 CNN模型，分别处理临床、影像和组织病理图像，并通过验证集加权集成策略融合预测结果，同时应用数据增强和模态特定预处理提升模型鲁棒性。

Result: 各模态单独模型在验证集上的准确率分别为：影像100%、组织病理95.12%、临床63.10%；集成模型在55个样本的多模态验证数据上达到84.58%的整体准确率。

Conclusion: 该多模态集成框架可作为非侵入性AI辅助分诊工具，弥补当前诊断流程中的不足，有助于临床决策，减少诊断延迟，改善患者预后。

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [71] [Exploring Instruction Data Quality for Explainable Image Quality Assessment](https://arxiv.org/abs/2510.03880)
*Yunhao Li,Sijing Wu,Huiyu Duan,Yucheng Zhu,Qi Jia,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文挑战了大规模指令调优数据的必要性，提出一种基于聚类的数据选择方法IQA-Select，在仅使用10%数据的情况下超越全量微调的性能，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有可解释图像质量评估（IQA）方法依赖大规模指令调优数据，导致计算开销大且数据冗余，本文旨在探究数据质量对模型性能的影响，挑战“越大越好”的缩放定律。

Method: 首先分析不同规模数据对微调性能的影响，发现随机子集可能优于全量数据；进而提出三阶段聚类数据选择框架：特征提取、簇配额分配和簇采样策略，最终设计出高效的IQA-Select方法。

Result: IQA-Select在Q-Bench和AesBench上仅用10%选定数据即可达到全量微调102.1%和103.7%的性能，验证了数据质量优于单纯数据规模。

Conclusion: 高质量、多样化的指令调优数据比大规模冗余数据更有效，所提IQA-Select方法能以更低计算成本实现更优或相当的可解释IQA性能。

Abstract: In recent years, with the rapid development of powerful multimodal large
language models (MLLMs), explainable image quality assessment (IQA) has
gradually become popular, aiming at providing quality-related descriptions and
answers of images. To achieve this goal, recent methods seek to construct a
large-scale instruction tuning dataset to empower the MLLM with quality
perception ability following the well-known scaling law. However, a large
amount of instruction tuning data may cause substantial computational costs and
redundant data, which in turn will cause harm to the performance of the model.
To cope with this problem, in this paper, we challenge the scaling law and
systematically investigate the role of data quality of the instruction tuning
dataset for explainable IQA. Using a powerful pre-trained MLLM, we first
investigate the changes in model performance after fine-tuning with different
sizes of instruction tuning data. We find that selecting a subset of the data
set randomly using an appropriate ratio can even lead to better results than
training with the entire instruction tuning dataset, demonstrating the
redundancy of current explainable IQA instruction tuning data. Beyond randomly
sampling a subset, we propose a clustering-based data selection framework with
three stages: clustering feature extraction, cluster quota allocation, and
cluster sampling strategy. Then we systematically analyze the choices of each
stage and propose a simple but efficient data selection method IQA-Select for
explainable IQA. The experimental results demonstrate that IQA-Select can
achieve 102.1% and 103.7% performance of full fine-tuning using only 10%
selected data in Q-Bench and AesBench respectively, significantly reducing
computational costs while achieving better performance.

</details>


### [72] [Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models](https://arxiv.org/abs/2510.03903)
*Md. Atabuzzaman,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 提出一种将零样本细粒度图像分类转化为视觉问答任务的新方法，利用大视觉语言模型的理解能力，并通过注意力干预技术提升性能，在多个基准上超越现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 探索大视觉语言模型在零样本细粒度图像分类中的潜力，该任务要求对视觉上相似的类别进行精确区分，而现有方法和数据集存在局限。

Method: 将分类任务转化为视觉问答框架，使用大视觉语言模型理解图像并回答问题，引入注意力干预技术增强模型表现，并构建更全面精细的类别描述基准数据集。

Result: 在多个细粒度图像分类基准上进行了广泛实验，所提方法 consistently 超越当前最先进方法，验证了其有效性。

Conclusion: 所提出的方法有效提升了大视觉语言模型在零样本细粒度分类任务上的性能，展示了其在此类任务中的巨大潜力。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on vision-language reasoning tasks. However, their potential for zero-shot
fine-grained image classification, a challenging task requiring precise
differentiation between visually similar categories, remains underexplored. We
present a novel method that transforms zero-shot fine-grained image
classification into a visual question-answering framework, leveraging LVLMs'
comprehensive understanding capabilities rather than relying on direct class
name generation. We enhance model performance through a novel attention
intervention technique. We also address a key limitation in existing datasets
by developing more comprehensive and precise class description benchmarks. We
validate the effectiveness of our method through extensive experimentation
across multiple fine-grained image classification benchmarks. Our proposed
method consistently outperforms the current state-of-the-art (SOTA) approach,
demonstrating both the effectiveness of our method and the broader potential of
LVLMs for zero-shot fine-grained classification tasks. Code and Datasets:
https://github.com/Atabuzzaman/Fine-grained-classification

</details>


### [73] [From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance](https://arxiv.org/abs/2510.03906)
*Ardalan Aryashad,Parsa Razmara,Amin Mahjoub,Seyedarmin Azizi,Mahdi Salmani,Arad Firouzkouhi*

Main category: cs.CV

TL;DR: 本文对多种去雾方法在自动驾驶感知系统中的效果进行了系统性实证研究，涵盖传统滤波器、现代去雾网络、组合方法及基于视觉-语言模型的图像编辑方法，使用Foggy Cityscapes数据集评估图像质量和下游任务性能，揭示了不同方法的有效性与局限性，并建立了面向实际任务的透明基准。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知系统在雾天条件下性能易受干扰，现有去雾方法虽提升图像质量，但对下游任务（如检测与分割）的增益不一致，且多数评估依赖合成数据，缺乏真实场景下的可迁移性验证。

Method: 在Foggy Cityscapes数据集上，系统评估四类去雾 pipeline：(i) 经典滤波器，(ii) 现代去雾网络，(iii) 混合链式方法（滤波+模型或模型+滤波），(iv) 直接应用于雾图的提示驱动视觉-语言模型（VLM）图像编辑方法；评估指标包括图像质量、目标检测（mAP）和分割性能（PQ, RQ, SQ），并引入基于VLM裁判的定性评分及其与任务指标的相关性分析。

Result: 实验表明某些去雾方法能有效提升下游任务性能，链式组合可能带来协同增益或性能下降，VLM-based编辑器表现接近专用去雾方法；VLM裁判评分与mAP有强相关性，验证了其评估可靠性。

Conclusion: 研究建立了面向任务的去雾方法透明基准，明确了预处理在何种条件下能真正改善恶劣天气下的自动驾驶感知，强调应以实际感知性能而非仅图像质量作为评价标准。

Abstract: Autonomous driving perception systems are particularly vulnerable in foggy
conditions, where light scattering reduces contrast and obscures fine details
critical for safe operation. While numerous defogging methods exist-from
handcrafted filters to learned restoration models-improvements in image
fidelity do not consistently translate into better downstream detection and
segmentation. Moreover, prior evaluations often rely on synthetic data, leaving
questions about real-world transferability. We present a structured empirical
study that benchmarks a comprehensive set of pipelines, including (i) classical
filters, (ii) modern defogging networks, (iii) chained variants
(filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven
visual--language image editing models (VLM) applied directly to foggy images.
Using Foggy Cityscapes, we assess both image quality and downstream performance
on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals
when defogging helps, when chaining yields synergy or degradation, and how
VLM-based editors compare to dedicated approaches. In addition, we evaluate
qualitative rubric-based scores from a VLM judge and quantify their alignment
with task metrics, showing strong correlations with mAP. Together, these
results establish a transparent, task-oriented benchmark for defogging methods
and highlight the conditions under which preprocessing genuinely improves
autonomous perception in adverse weather.

</details>


### [74] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: 本文提出了CAMEO，一种用于通用人体动作视频生成的级联框架，通过结合文本到动作模型和条件视频扩散模型，实现了从文本描述到高质量视频的生成。


<details>
  <summary>Details</summary>
Motivation: 现有的视频扩散模型在通用人体视频生成方面探索不足，多数局限于图像到视频或特定领域（如舞蹈视频），缺乏对多样化文本驱动人体动作生成的支持。

Method: 提出CAMEO框架，包含文本与视觉条件的协同训练机制、相机感知的条件模块，以实现文本、动作和视频之间的对齐，并在训练和推理中优化跨阶段一致性。

Result: 在MovieGen基准和新构建的T2M-VDM基准上验证了方法的有效性，展示了其在多种应用场景下的优越性能和灵活性。

Conclusion: CAMEO通过级联结构有效连接文本到动作与视频扩散模型，显著提升了文本驱动人体视频生成的质量与实用性，推动了通用人类视频生成的发展。

Abstract: Human video generation is becoming an increasingly important task with broad
applications in graphics, entertainment, and embodied AI. Despite the rapid
progress of video diffusion models (VDMs), their use for general-purpose human
video generation remains underexplored, with most works constrained to
image-to-video setups or narrow domains like dance videos. In this work, we
propose CAMEO, a cascaded framework for general human motion video generation.
It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,
mitigating suboptimal factors that may arise in this process across both
training and inference through carefully designed components. Specifically, we
analyze and prepare both textual prompts and visual conditions to effectively
train the VDM, ensuring robust alignment between motion descriptions,
conditioning signals, and the generated videos. Furthermore, we introduce a
camera-aware conditioning module that connects the two stages, automatically
selecting viewpoints aligned with the input text to enhance coherence and
reduce manual intervention. We demonstrate the effectiveness of our approach on
both the MovieGen benchmark and a newly introduced benchmark tailored to the
T2M-VDM combination, while highlighting its versatility across diverse use
cases.

</details>


### [75] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: 本研究提出了一种结合CNN-LSTM模型与大语言模型（LLM）的新框架，从运动数据中提取生物力学特征并生成可操作的语言反馈，以提升网球击球分析的可解释性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有系统在将生物力学洞察转化为对运动员和教练员有意义的可操作语言反馈方面存在不足，本文旨在弥合这一差距。

Method: 采用基于CNN-LSTM的模型从运动数据中提取关键生物力学特征（如关节角度、肢体速度和动力链模式），并利用大语言模型（LLM）生成技术准确且具生物力学依据的反馈。

Result: 该框架在分类性能和可解释性方面表现良好，能够有效连接生物力学分析与实际训练需求，实现可解释AI与运动生物力学的融合。

Conclusion: 所提出的框架成功地将深度学习与生物力学相结合，生成了准确且可操作的反馈，有助于提升运动表现评估与训练指导的有效性。

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [76] [Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs](https://arxiv.org/abs/2510.03955)
*Sameep Vani,Shreyas Jena,Maitreya Patel,Chitta Baral,Somak Aditya,Yezhou Yang*

Main category: cs.CV

TL;DR: 本文提出了TimeWarp方法，用于生成具有复杂时间动态的合成数据集，以提升视频大语言模型在细粒度时间理解任务上的表现。通过该方法构建的数据集增强了模型对视觉和时间信息的关注，显著提高了其在七个基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在需要精细时间理解的任务上表现不佳，主要因为当前微调数据集缺乏视觉复杂性和时间细节，导致模型依赖语言推理而非真正理解视频动态。

Method: 提出TimeWarp方法，系统性地生成针对细粒度时间理解的合成数据集，并构建大规模偏好数据集，使模型响应更紧密地基于输入视频的视觉和时间信息进行微调。

Result: 在七个时间理解基准测试中，应用TimeWarp微调后的模型均取得显著绝对性能提升，验证了所提数据集对增强Video-LLMs时间理解能力的有效性。

Conclusion: TimeWarp通过引入富含时间动态的合成数据，有效改善了视频大语言模型对时间信息的理解能力，推动其在复杂时序任务中的发展。

Abstract: While Video Large Language Models (Video-LLMs) have demonstrated remarkable
performance across general video understanding benchmarks-particularly in video
captioning and descriptive tasks-they consistently underperform on tasks that
require fine-grained temporal understanding. This limitation arises due to the
lack of visual complexity and temporal nuance in current fine-tuning datasets,
leading these models to rely heavily on language-based reasoning rather than
truly understanding video dynamics. In this work, we propose TimeWarp, a
systematic method to create a targeted synthetic temporal dataset to fine-tune
the model's responses to encourage it to focus on the given input video. We
introduce a large-scale preference dataset, created using TimeWarp, that
captures intricate temporal dynamics often overlooked, grounding the model's
responses to visual and temporal information. We demonstrate that when our
method is applied to existing models, it significantly improves performance on
temporal understanding benchmarks, highlighting the effectiveness of our
proposed datasets in advancing temporal understanding in Video-LLMs, resulting
in an absolute improvement in performance across seven benchmarks. Code is
available at https://github.com/sameepv21/timewarp.

</details>


### [77] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 本文研究了在长格式生物医学图注上预训练视觉-语言模型的影响，提出了一种支持更长文本上下文的模型BMC-LongCLIP，并发布了包含100万对图像-图注的数据集BIOMEDICA-LongCAP，显著提升了检索与分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型通常使用短文本（<77个token）进行预训练，导致长图注被截断；而生物医学文献中的图注普遍较长，因此需要探索长上下文建模对这类模型的影响。

Method: 通过扩展VLM中文本编码器的上下文长度，利用长达512个token的输入窗口，在自建的大规模长图注数据集BIOMEDICA-LongCAP上进行预训练，开发出长上下文生物医学VLM模型BMC-LongCLIP。

Result: BMC-LongCLIP在长图注检索任务中Recall@1最高提升30%，分类平均提升2%，训练收敛更快，且将token浪费从55%降至2.2%。

Conclusion: 长上下文建模能有效提升生物医学视觉-语言模型的性能，是该领域一个有前景的发展方向。

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [78] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: 提出一种可控伪标签生成框架（CPG），用于解决长尾分布下标记数据与未知分布未标记数据的半监督学习问题，通过动态筛选可靠伪标签并结合对数调整分类器和类感知增强，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有长尾半监督学习方法假设未标记数据具有预定义分布，但实际中其分布通常是未知且任意的，因此需要一种不依赖于未标记数据分布假设的方法。

Method: 设计了一个可控自强化优化循环：1）动态过滤机制选择可靠的伪标签加入标记集，保持其分布已知；2）基于更新后的分布构建贝叶斯最优分类器；3）用改进的分类器识别更多可靠伪标签。同时引入类感知自适应增强模块和辅助分支以提升少数类表示和数据利用率。

Result: 在多个常用基准数据集上验证了CPG的有效性，准确率最高超过现有方法15.97%。

Conclusion: CPG能够有效应对未标记数据任意分布的挑战，通过可控伪标签生成和优化循环显著降低泛化误差，实现了当前最优的长尾半监督学习性能。

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [79] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: 提出了一种针对PaddleOCRv5的微调方法，以提升对汉喃文本的字符识别准确率，尤其在噪声图像下效果显著，并开发了交互式演示工具。


<details>
  <summary>Details</summary>
Motivation: 现有的OCR系统在处理退化的扫描件、非标准字形和古代文献中的手写变异时表现不佳，难以有效数字化越南历史文献。

Method: 通过使用精选的古代越南汉文手稿子集对PaddleOCRv5的文本识别模块进行重新训练，并构建完整的训练流程，包括预处理、LMDB转换、评估和可视化。

Result: 与基础模型相比，精确准确率从37.5%提升至50.0%，在噪声图像条件下改进尤为明显，并实现了交互式识别结果对比演示。

Conclusion: 该微调方法有效提升了汉喃文本的识别性能，有助于推动汉越语义对齐、机器翻译和历史语言学等下游研究。

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [80] [Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation](https://arxiv.org/abs/2510.04021)
*Kushal Vyas,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: 提出MetaSeg，一种基于隐式神经表示（INR）和元学习的医学图像分割框架，可在2D/3D脑MRI上实现与U-Net相当的Dice分数，但参数减少90%。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INR）在信号表示上表现优异，但难以直接用于需要学习语义结构的分割等预测任务。现有模型如U-Net参数量大、资源消耗高，亟需更紧凑、可扩展的替代方案。

Method: 提出MetaSeg，采用一个共享的INR同时预测像素强度值和类别标签；通过元学习寻找最优初始参数，使模型在新测试图像上仅需少量微调即可拟合像素并自动解码分割结果。

Result: 在2D和3D脑MRI分割任务中，MetaSeg取得了与U-Net相当的Dice分数，但模型参数减少了90%，验证了其高效性和可扩展性。

Conclusion: MetaSeg为医学图像分割提供了一种新颖、轻量且可扩展的解决方案，显著降低了对计算资源的需求，是传统重型架构（如U-Net和ViT）的有效替代。

Abstract: Implicit neural representations (INRs) have achieved remarkable successes in
learning expressive yet compact signal representations. However, they are not
naturally amenable to predictive tasks such as segmentation, where they must
learn semantic structures over a distribution of signals. In this study, we
introduce MetaSeg, a meta-learning framework to train INRs for medical image
segmentation. MetaSeg uses an underlying INR that simultaneously predicts per
pixel intensity values and class labels. It then uses a meta-learning procedure
to find optimal initial parameters for this INR over a training dataset of
images and segmentation maps, such that the INR can simply be fine-tuned to fit
pixels of an unseen test image, and automatically decode its class labels. We
evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice
scores comparable to commonly used U-Net models, but with $90\%$ fewer
parameters. MetaSeg offers a fresh, scalable alternative to traditional
resource-heavy architectures such as U-Nets and vision transformers for medical
image segmentation. Our project is available at
https://kushalvyas.github.io/metaseg.html .

</details>


### [81] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: 提出Video-in-the-Loop（ViTL）框架，通过两阶段方法在固定token预算下实现高效的长视频问答，结合时间定位与答案生成，并引入带时间标注的多选数据集用于训练。


<details>
  <summary>Details</summary>
Motivation: 现有长视频问答方法在固定token预算下难以兼顾效率与精度，缺乏对关键时间片段的有效定位和解释性。

Method: ViTL采用两阶段框架：首先用低帧率扫描定位问题相关的视频区间，然后基于跨度感知的视觉token重分配，在更高有效帧率下进行答案生成；使用交错分组相对目标函数联合优化时间IoU和答案准确性。

Result: 在Charades-STA、ActivityNet-Captions等任务上，ViTL在50%更少帧输入下最高提升8.6%，消融实验证明跨度感知token重分配优于均匀采样。

Conclusion: ViTL结合新构建的数据集提供了一种可解释且计算高效的长视频问答解决方案，支持可归因的输出并提升性能。

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [82] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: 提出一种名为AgentAug的数据增强框架，通过模拟典型的创作过程生成多样化的短视频虚假新闻，以提升检测器性能。


<details>
  <summary>Details</summary>
Motivation: 现有虚假新闻检测器因训练数据有限且多样性不足，导致模型学习到有偏的模式，难以应对真实场景中视频片段与虚假新闻事件之间复杂的多对多关系。

Method: 设计了基于大语言模型（LLM）驱动的多管道系统，模拟四类新闻伪造方式生成虚假视频，并结合基于不确定性采样的主动学习策略筛选有效增广样本。

Result: 在两个基准数据集上的实验表明，AgentAug能持续提升短视频虚假新闻检测器的性能。

Conclusion: AgentAug通过更贴近真实伪造过程的数据增强方法，有效缓解了数据稀缺和模式单一问题，增强了检测模型的泛化能力。

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [83] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: 本文研究了基于提示的图像编辑方法中的超参数优化，提出了一种新的“CL P2P”框架和注意力重加权方法，以提升编辑精度与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于稳定扩散模型的图像编辑方法在结果上存在不一致问题（如发色变化不稳定），需要提高编辑的精确性与可靠性。

Method: 通过分析‘词交换’方法，提出‘注意力重加权方法’，并设计‘CL P2P’框架以解决循环不一致等问题，优化超参数配置。

Result: 所提出的CL P2P框架在保持图像结构一致性方面表现更优，有效提升了文本驱动编辑的适应性和生成质量。

Conclusion: 合理调整超参数与注意力机制设计能显著改善提示驱动图像编辑的效果，CL P2P为未来研究提供了更可靠的框架基础。

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [84] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: 本文提出了GUI-Spotlight模型，通过图像接地推理和调用专用工具来提高多模态大语言模型在图形用户界面中的视觉定位准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在实际应用中受限于视觉定位的可靠性，无法准确执行点击或拖拽等指针级操作。

Method: 提出GUI-Spotlight模型，该模型通过动态调用多个专用工具，迭代缩小屏幕相关区域的焦点，从而提升视觉定位精度。

Result: 在ScreenSpot-Pro基准测试中，仅使用18.5K训练样本的GUI-Spotlight达到了52.8%的准确率，超过了使用更多数据训练的V2P-7B和GTA-1-7B模型。

Conclusion: GUI-Spotlight显著提升了多模态大语言模型在复杂真实环境下的视觉接地能力，为GUI系统的实用化提供了更可靠的技术支持。

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [85] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: 本文提出了一种基于层间局部最小值优化的范围估计方法，用于提升深度神经网络后训练量化性能，在ResNet系列和Inception-v3模型上实现了几乎无精度损失的8位和6位量化，并显著改善了4位量化的准确性。


<details>
  <summary>Details</summary>
Motivation: 低比特量化在保持模型精度的同时减少深度神经网络存储开销是一个挑战性问题，现有方法在极低比特下容易造成较大精度损失。

Method: 将范围估计建模为最小化量化误差的优化问题，证明其具有局部凸性并设计高效的搜索算法；进一步在变换后的权重空间中应用该算法以提升实际效果。

Result: 在图像分类任务中，8位和6位量化几乎无精度损失，4位量化精度显著优于现有方法，在ResNet系列和Inception-v3模型上均达到SOTA性能。

Conclusion: 所提出的范围估计方法有效提升了后训练量化的精度表现，尤其在低比特设置下具有优越性能，具备实用价值。

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [86] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: 提出了一种名为MetaFind的场景感知三模态组合检索框架，用于在元宇宙中增强3D资产检索和场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D资产检索方法忽视空间、语义和风格约束，且缺乏针对3D资产检索的标准化范式。

Method: 引入支持文本、图像和3D模态任意组合查询的灵活检索机制，提出等变布局编码器ESSGNN建模对象特征与场景结构，实现上下文和风格一致的资产检索。

Result: 实验表明，MetaFind在多种检索任务中相比基线方法显著提升了空间和风格一致性。

Conclusion: MetaFind为3D资产检索提供了有效的场景感知解决方案，支持迭代式场景构建，适用于元宇宙应用。

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [87] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: 提出一种结合序数信息的改进损失函数，以增强太阳耀斑预测模型在二分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的二分类框架忽略了耀斑子类之间的固有序数关系，导致靠近阈值的样本容易被误分类。

Method: 在标准二元交叉熵损失中引入序数感知的加权机制，对靠近分类边界的错误预测施加更重的惩罚。

Result: 该方法能有效减少阈值附近的误分类，提升模型对相似强度事件的区分能力。

Conclusion: 通过利用数据的序数特性，所提出的损失函数可作为数据驱动的正则化手段，显著改善太阳耀斑预测模型的性能。

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [88] [QuantDemoire: Quantization with Outlier Aware for Image Demoiréing](https://arxiv.org/abs/2510.04066)
*Zheng Chen,Kewei Zhang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种针对去摩尔纹任务的后训练量化框架QuantDemoire，有效解决了低比特量化中的分布异常值和光滑区域表示弱化问题，在大幅降低模型计算量的同时保持了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的量化方法在去摩尔纹模型上会导致严重性能下降，主要由于激活和权重中的分布异常值以及平滑区域表示能力减弱，因此需要专门针对该任务设计高效的量化方案。

Method: 提出了QuantDemoire框架，包含两个关键组件：一是异常值感知量化器，采用基于采样的范围估计减少激活异常值，并将极值权重保留为FP16；二是频率感知校准策略，在微调中强调低频和中频成分以减轻量化带来的条带伪影。

Result: 实验表明，QuantDemoire在W4A4设置下相比现有量化方法性能提升超过4 dB，显著减少了参数量和计算开销，同时保持了高质量的去摩尔纹效果。

Conclusion: QuantDemoire为去摩尔纹模型提供了高效且实用的量化解决方案，有助于其在边缘设备上的部署应用。

Abstract: Demoir\'eing aims to remove moir\'e artifacts that often occur in images.
While recent deep learning-based methods have achieved promising results, they
typically require substantial computational resources, limiting their
deployment on edge devices. Model quantization offers a compelling solution.
However, directly applying existing quantization methods to demoir\'eing models
introduces severe performance degradation. The main reasons are distribution
outliers and weakened representations in smooth regions. To address these
issues, we propose QuantDemoire, a post-training quantization framework
tailored to demoir\'eing. It contains two key components. **First}, we
introduce an outlier-aware quantizer to reduce errors from outliers. It uses
sampling-based range estimation to reduce activation outliers, and keeps a few
extreme weights in FP16 with negligible cost. **Second**, we design a
frequency-aware calibration strategy. It emphasizes low- and mid-frequency
components during fine-tuning, which mitigates banding artifacts caused by
low-bit quantization. Extensive experiments validate that our QuantDemoire
achieves large reductions in parameters and computation while maintaining
quality. Meanwhile, it outperforms existing quantization methods by over **4
dB** on W4A4. Code is released at:
https://github.com/zhengchen1999/QuantDemoire.

</details>


### [89] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: TV-LoRA是一种结合扩散生成先验与多正则化约束的低剂量稀疏视图CT重建新方法，具有高保真、高效和强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为解决极稀疏视图下CT重建的病态性和纹理丢失问题，需融合生成模型与物理先验以提升图像质量。

Method: 提出TV-LoRA方法，结合NCSN++扩散先验与各向异性TV、LoRA核范数正则化，在ADMM框架下实现重建，并采用2D切片策略、FFT加速与张量并行优化提升效率。

Result: 在AAPM-2016、CTHD和LIDC数据集上，TV-LoRA在SSIM、纹理恢复、边缘清晰度和伪影抑制方面优于基准方法，且具备良好泛化性；消融实验验证了LoRA与扩散先验的互补性，FFT-PCG模块显著提速。

Conclusion: TV-LoRA通过融合扩散先验与多正则化策略，实现了高质量、高效的3D CT重建，在低剂量稀疏采样场景中具有广泛临床应用潜力。

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT
reconstruction that combines a diffusion generative prior (NCSN++ with SDE
modeling) and multi-regularization constraints, including anisotropic TV and
nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and
texture loss under extremely sparse views, TV-LoRA integrates generative and
physical constraints, and utilizes a 2D slice-based strategy with FFT
acceleration and tensor-parallel optimization for efficient inference.
Experiments on AAPM-2016, CTHD, and LIDC datasets with
$N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks
in SSIM, texture recovery, edge clarity, and artifact suppression,
demonstrating strong robustness and generalizability. Ablation studies confirm
the complementary effects of LoRA regularization and diffusion priors, while
the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves
high-fidelity, efficient 3D CT reconstruction and broad clinical applicability
in low-dose, sparse-sampling scenarios.

</details>


### [90] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: 本文提出了一种标准化的拓扑映射评估协议，通过形式化拓扑一致性并引入定位精度作为替代指标，同时首次提出了量化数据集歧义性的方法，并发布了包含不同歧义级别的基准数据集和开源工具，以促进拓扑映射研究的一致性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准化的评估指标、数据集和协议，拓扑映射领域的发展受到阻碍，且现有系统在不同环境和标准下难以进行公平比较，感知别名问题也未被充分量化。

Method: 通过形式化拓扑一致性的概念，使用定位精度作为可解释的代理指标，并提出首个量化数据集歧义性的度量方法；构建了一个具有校准歧义水平的多样化基准数据集，并开源了深度学习基线系统与传统方法的实现。

Result: 实验表明当前方法在感知别名条件下存在局限性，所提出的评估协议和数据集支持跨环境的公平比较，且所有数据、基线和评估工具均已开源。

Conclusion: 该工作为拓扑映射提供了可复现的评估框架，推动了该领域的标准化发展，并揭示了现有方法在高歧义场景下的性能瓶颈。

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [91] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本文提出了事件相机下的网格流估计新任务，构建了首个高分辨率事件网格流数据集HREM，并设计了轻量级EEMFlow网络实现快速准确的网格流估计，同时提出ADM模块提升模型在不同事件密度下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏针对网格流的事件数据集和方法，且对事件数据密度变化的鲁棒性研究不足，限制了事件相机在运动估计中的应用。

Method: 构建了HREM和HREM+多密度事件数据集；设计了EEMFlow网络及其支持光流的EEMFlow+版本，引入CDC模块保持运动边界清晰；提出ADM模块自适应调整输入事件密度。

Result: EEMFlow比当前最优方法快30倍，在HREM上表现出卓越性能；ADM使EEMFlow和EEMFlow+性能分别提升8%和10%。

Conclusion: 所提方法在事件驱动的网格流估计中实现了高效、精确且鲁棒的性能，推动了事件相机在复杂运动场景中的应用。

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [92] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出一种新的扩散模型方法，通过预训练编码器和时间依赖得分缩放采样引导，显著提高6D物体姿态估计的训练收敛速度和精度，同时无需额外评估网络。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在6D物体姿态估计中存在训练收敛慢、需端到端学习编码器且依赖额外网络筛选姿态候选的问题。

Method: 1）预训练编码器并联合学习回归头与去噪扩散头；2）引入时间依赖得分缩放的采样引导机制，平衡探索与利用。

Result: 在REAL275、HouseCat6D和ROPE等多个基准上实现最先进的精度，单姿态推理下表现优异，训练和推理效率更高。

Conclusion: 该方法简单有效，解决了现有方法的收敛性和效率问题，在保持多模态特性的同时提升了姿态估计质量。

Abstract: Latest diffusion models have shown promising results in category-level 6D
object pose estimation by modeling the conditional pose distribution with depth
image input. The existing methods, however, suffer from slow convergence during
training, learning its encoder with the diffusion denoising network in
end-to-end fashion, and require an additional network that evaluates sampled
pose hypotheses to filter out low-quality pose candidates. In this paper, we
propose a novel pipeline that tackles these limitations by two key components.
First, the proposed method pretrains the encoder with the direct pose
regression head, and jointly learns the networks via the regression head and
the denoising diffusion head, significantly accelerating training convergence
while achieving higher accuracy. Second, sampling guidance via time-dependent
score scaling is proposed s.t. the exploration-exploitation trade-off is
effectively taken, eliminating the need for the additional evaluation network.
The sampling guidance maintains multi-modal characteristics of symmetric
objects at early denoising steps while ensuring high-quality pose generation at
final steps. Extensive experiments on multiple benchmarks including REAL275,
HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet
effective, achieves state-of-the-art accuracies even with single-pose
inference, while being more efficient in both training and inference.

</details>


### [93] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: 本文提出了一种针对多模态大语言模型知识蒸馏中概念漂移问题的新方法，通过“学习、比较、批判”范式和自主偏好优化（APO）实现学生模型的推理一致性与鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 多教师模型在知识蒸馏过程中产生的推理轨迹存在概念漂移，导致学生模型继承偏差，影响性能。

Method: 建立概念漂移与知识蒸馏之间的理论联系，提出“学习、比较、批判”范式，并引入自主偏好优化（APO）进行概念对齐。

Result: 实验表明该方法在一致性、鲁棒性和泛化能力方面显著优于现有蒸馏方法，并发布了包含17万+推理轨迹的大规模数据集CXR-MAX。

Conclusion: 通过引入概念漂移机制和APO框架，有效缓解了多教师蒸馏中的偏差传播问题，提升了学生模型的稳定性和泛化性能。

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [94] [A Modular Conditional Diffusion Framework for Image Reconstruction](https://arxiv.org/abs/2411.05993)
*Magauiya Zhussip,Iaroslav Koshelev,Stamatis Lefkimmiatis*

Main category: cs.CV

TL;DR: 提出了一种模块化的扩散概率图像恢复框架（DP-IR），结合预训练的先进图像恢复网络与生成式扩散模型，仅需额外训练一个小模块（0.7M参数），并在采样效率上提升至少四倍，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在盲图像恢复任务中存在任务特定性强、计算成本高、难以泛化到其他任务的问题，限制了其广泛应用，尤其对计算资源和数据有限的用户不友好。

Method: 提出模块化DP-IR框架，融合预训练的图像恢复网络与扩散概率模型，仅针对特定任务微调一个小模块，并设计高效的采样策略以减少神经函数评估次数，兼容DDIM等加速技术。

Result: 在多个图像恢复任务（如突发去噪超分、动态去模糊、超分辨率）的四个基准上验证，方法在感知质量上优于现有方法，同时在保真度指标上具有竞争力，且推理速度提升至少四倍。

Conclusion: DP-IR框架实现了高效、灵活且高性能的图像恢复，降低了扩散模型的应用门槛，推动其在实际场景中的广泛使用。

Abstract: Diffusion Probabilistic Models (DPMs) have been recently utilized to deal
with various blind image restoration (IR) tasks, where they have demonstrated
outstanding performance in terms of perceptual quality. However, the
task-specific nature of existing solutions and the excessive computational
costs related to their training, make such models impractical and challenging
to use for different IR tasks than those that were initially trained for. This
hinders their wider adoption, especially by those who lack access to powerful
computational resources and vast amount of training data. In this work we aim
to address the above issues and enable the successful adoption of DPMs in
practical IR-related applications. Towards this goal, we propose a modular
diffusion probabilistic IR framework (DP-IR), which allows us to combine the
performance benefits of existing pre-trained state-of-the-art IR networks and
generative DPMs, while it requires only the additional training of a relatively
small module (0.7M params) related to the particular IR task of interest.
Moreover, the architecture of the proposed framework allows for a sampling
strategy that leads to at least four times reduction of neural function
evaluations without suffering any performance loss, while it can also be
combined with existing acceleration techniques such as DDIM. We evaluate our
model on four benchmarks for the tasks of burst JDD-SR, dynamic scene
deblurring, and super-resolution. Our method outperforms existing approaches in
terms of perceptual quality while it retains a competitive performance with
respect to fidelity metrics.

</details>


### [95] [BLADE: Bias-Linked Adaptive DEbiasing](https://arxiv.org/abs/2510.04174)
*Piyush Arora,Navlika Singh,Vasubhya Diwan,Pratik Mazumder*

Main category: cs.CV

TL;DR: 本文提出了一种名为BLADE的生成式去偏框架，无需先验知识或偏见冲突样本即可有效缓解神经网络中的隐式偏见问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络容易学习训练数据中的虚假相关性（即隐式偏见），导致泛化能力差，而现有方法依赖不切实际的假设，如已知偏见或需要偏见冲突样本。

Method: BLADE首先训练一个生成模型，在保持任务相关特征的同时跨偏见域转换图像；然后根据样本对偏见的敏感性自适应地用合成样本来优化原始图像，并通过对比学习对齐共享任务特征但偏见不同的样本，错开具有相同偏见的样本。

Result: 在多个基准数据集上，BLADE显著优于当前最先进的方法，在最坏组设置下的CIFAR-10-C数据集上比最近的基线高出约18%。

Conclusion: BLADE在无需显式监督的情况下实现了有效的去偏，为构建更鲁棒的深度学习模型提供了新方向。

Abstract: Neural networks have revolutionized numerous fields, yet they remain
vulnerable to a critical flaw: the tendency to learn implicit biases, spurious
correlations between certain attributes and target labels in training data.
These biases are often more prevalent and easier to learn, causing models to
rely on superficial patterns rather than task-relevant features necessary for
generalization. Existing methods typically rely on strong assumptions, such as
prior knowledge of these biases or access to bias-conflicting samples, i.e.,
samples that contradict spurious correlations and counterbalance bias-aligned
samples, samples that conform to these spurious correlations. However, such
assumptions are often impractical in real-world settings. We propose BLADE
({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that
requires no prior knowledge of bias or bias-conflicting samples. BLADE first
trains a generative model to translate images across bias domains while
preserving task-relevant features. Then, it adaptively refines each image with
its synthetic counterpart based on the image's susceptibility to bias. To
encourage robust representations, BLADE aligns an image with its
bias-translated synthetic counterpart that shares task-relevant features but
differs in bias, while misaligning it with samples sharing the same bias. We
evaluate BLADE on multiple benchmark datasets and show that it significantly
outperforms state-of-the-art methods. Notably, it exceeds the closest baseline
by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the
worst group setting, establishing a new benchmark in bias mitigation and
demonstrating its potential for developing more robust deep learning models
without explicit supervision.

</details>


### [96] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: 提出SEG-MIL-CBM框架，结合概念引导的图像分割与注意力机制的多实例学习，实现无需概念标注的空间定位解释，提升模型透明性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型缺乏可解释性且易依赖不可靠特征，概念瓶颈模型需昂贵标注且缺乏空间定位能力。

Method: 将每个分割区域作为实例，构建基于注意力的多实例学习框架，通过概念引导的分割实现语义区域与高层概念对齐，进行证据聚合推理。

Result: 在存在伪相关性、输入损坏和大规模基准的数据集上表现出强鲁棒性，同时生成无需概念或组标注的空间定位、概念级解释。

Conclusion: SEG-MIL-CBM在不牺牲性能的前提下提升了模型的可解释性与鲁棒性，为安全关键应用提供了更可信的视觉决策模型。

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [97] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为HyCa的混合ODE求解器启发的缓存框架，通过维度级缓存策略实现无需训练的扩散Transformer加速，在多种模型和领域中实现了接近无损的显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法在所有特征维度上采用统一策略，忽略了不同维度动态行为的异质性，导致加速效果受限。

Method: 将隐特征演化建模为跨维度的常微分方程（ODE）混合系统，提出HyCa框架，根据不同维度的动态特性应用混合缓存策略，实现高效特征重用与预测。

Result: 在FLUX、HunyuanVideo、Qwen-Image及Qwen-Image-Edit等多个模型上实现5.55至6.24倍的推理加速，且无需重新训练。

Conclusion: HyCa通过细粒度的维度感知缓存策略，有效提升了扩散Transformer的采样效率，具有广泛适用性和接近无损的性能表现。

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video
synthesis, but their iterative sampling process remains a major bottleneck due
to the high cost of transformer forward passes at each timestep. To mitigate
this, feature caching has emerged as a training-free acceleration technique
that reuses or forecasts hidden representations. However, existing methods
often apply a uniform caching strategy across all feature dimensions, ignoring
their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by
modeling hidden feature evolution as a mixture of ODEs across dimensions, and
introduce HyCa, a Hybrid ODE solver inspired caching framework that applies
dimension-wise caching strategies. HyCa achieves near-lossless acceleration
across diverse domains and models, including 5.55 times speedup on FLUX, 5.56
times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and
Qwen-Image-Edit without retraining.

</details>


### [98] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: 本文提出了World-To-Image框架，通过代理驱动的网络搜索获取未知概念图像，并进行多模态提示优化，显著提升了文本到图像生成在新奇实体上的语义对齐和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 由于知识截止问题，现有文本到图像模型在面对新奇或分布外实体时表现不佳，因此需要一种能动态获取外部世界知识的方法来增强生成能力。

Method: 设计一个智能代理，动态搜索网络以获取基础模型未知概念的图像，并利用这些信息进行多模态提示优化，引导生成模型更准确地合成图像。

Result: 在NICE基准上比现有最先进方法提升8.1%的提示准确性，且在少于三次迭代内高效完成，同时在LLMGrader和ImageReward等现代评估中表现出更优的语义保真度和视觉美学。

Conclusion: World-To-Image框架有效弥补了文本到图像模型的知识鸿沟，使其能更好地反映不断变化的真实世界，为未来动态T2I系统提供了可行路径。

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [99] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为Manifold-Aligned Semantic Clustering (MASC)的框架，通过构建分层语义树来优化自回归图像生成模型中的视觉token预测，显著提升训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归图像生成模型使用扁平、无结构的视觉token词汇表，忽视了token嵌入空间的内在结构，导致预测任务复杂、训练效率低、生成质量受限。

Method: MASC利用几何感知的距离度量和密度驱动的聚合聚类方法，从codebook的内在结构中构建层次化的语义树，将高维平坦的预测任务转化为结构化的分层预测问题。

Result: 实验表明，MASC可将训练速度提升高达57%，并将LlamaGen-XL的FID从2.87降至2.58，显著提高生成质量。

Conclusion: MASC通过引入对token空间结构的归纳偏置，证明了预测空间的结构化对于可扩展生成建模的重要性，与架构创新同等关键。

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [100] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为ZoomIn的两阶段取证框架，用于提高AI生成图像检测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像的快速增长，真实与合成内容之间的界限变得模糊，对数字完整性提出了关键挑战。现有的视觉语言模型（VLM）在检测高质量合成图像中的细微伪影方面常常表现不佳。

Method: ZoomIn模仿人类视觉检查过程：第一阶段扫描整幅图像以定位可疑区域；第二阶段对这些放大区域进行集中分析，提供基于视觉证据的判断。同时构建了MagniFake数据集（包含20,000张真实和高质量合成图像，并标注边界框和取证解释），通过自动化VLM管道生成，用于训练和评估。

Result: 该方法实现了96.39%的检测准确率，具有较强的泛化能力，并能提供基于视觉证据、易于人类理解的解释。

Conclusion: ZoomIn有效提升了AI生成图像的检测性能和结果可解释性，结合MagniFake数据集为未来图像取证研究提供了新工具和基准。

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [101] [A Recursive Pyramidal Algorithm for Solving the Image Registration Problem](https://arxiv.org/abs/2510.04231)
*Stefan Dirnstorfer*

Main category: cs.CV

TL;DR: 本文提出了一种简单且端到端可训练的图像配准算法，仅需少量训练数据和时间，即可在某些场景下实现高精度对齐，并展示了其在立体视觉中的应用。


<details>
  <summary>Details</summary>
Motivation: 图像配准需要找到将两幅图像对齐的变换，传统方法可能复杂且耗时，本文旨在提出一种简洁高效、易于实现且适用于数据和资源受限场景的新方法。

Method: 设计了一种端到端可训练的神经网络算法，使用少量图像进行训练（如74张图像），输入窗口小（19x15），并用十几行Python代码实现。

Result: 该算法在训练数据极少、训练时间短的情况下，在某些设置中仍能达到准确的配准结果，在立体视觉任务中表现出色。

Conclusion: 该方法以极简的代码实现了高效的图像配准，适合训练数据有限、训练时间短或代码复杂度要求低的应用场景，具有良好的实用性和推广潜力。

Abstract: The problem of image registration is finding a transformation that aligns two
images, such that the corresponding points are in the same location. This paper
introduces a simple, end-to-end trainable algorithm that is implementable in a
few lines of Python code. The approach is shown to work with very little
training data and training time, while achieving accurate results in some
settings. An example application to stereo vision was trained from 74 images on
a 19x15 input window. With just a dozen lines of Python code this algorithm
excels in brevity and may serve as a good start in related scenarios with
limitations to training data, training time or code complexity.

</details>


### [102] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 本文提出了一种新的卷积层ArConv，用于提高深度神经网络在移动设备上的可访问性，并在眼病诊断任务中实现了高精度和低参数量。


<details>
  <summary>Details</summary>
Motivation: 现有的眼病诊断模型计算复杂度高，难以在资源受限的设备上应用，因此需要提高深度神经网络模型的可访问性。

Method: 通过重新设计和优化卷积层，提出了名为ArConv的新卷积层，并构建了一个仅含130万参数的轻量级模型。

Result: 在RfMiD数据集上，该模型准确率达到0.9328，优于MobileNetV2（220万参数）的0.9266。

Conclusion: ArConv层在降低模型复杂度的同时提升了性能，适用于移动端的眼病诊断任务。

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [103] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: Kaleido是一种用于照片级真实感神经渲染的生成模型，通过将3D视为视频的特殊子域，统一了对象和场景级别的建模。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成模型依赖显式的3D表示和大量标注数据，限制了泛化能力和可扩展性。

Method: 提出了一种基于序列到序列图像合成的掩码自回归框架，使用纯解码器的修正流Transformer实现无需显式3D表示的生成视图合成，并统一3D与视频建模。

Result: 在多种视角合成基准上达到最先进水平，零样本性能显著优于其他生成方法，在多视角设置下首次匹敌逐场景优化方法的质量。

Conclusion: Kaleido通过统一的序列到序列架构实现了强大的生成式神经渲染能力，减少了对稀缺3D数据的依赖，展现了在3D和视频建模中的广泛潜力。

Abstract: We present Kaleido, a family of generative models designed for
photorealistic, unified object- and scene-level neural rendering. Kaleido
operates on the principle that 3D can be regarded as a specialised sub-domain
of video, expressed purely as a sequence-to-sequence image synthesis task.
Through a systemic study of scaling sequence-to-sequence generative neural
rendering, we introduce key architectural innovations that enable our model to:
i) perform generative view synthesis without explicit 3D representations; ii)
generate any number of 6-DoF target views conditioned on any number of
reference views via a masked autoregressive framework; and iii) seamlessly
unify 3D and video modelling within a single decoder-only rectified flow
transformer. Within this unified framework, Kaleido leverages large-scale video
data for pre-training, which significantly improves spatial consistency and
reduces reliance on scarce, camera-labelled 3D datasets -- all without any
architectural modifications. Kaleido sets a new state-of-the-art on a range of
view synthesis benchmarks. Its zero-shot performance substantially outperforms
other generative methods in few-view settings, and, for the first time, matches
the quality of per-scene optimisation methods in many-view settings.

</details>


### [104] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: 提出CoSSeg-TTA框架，基于nnU-Netv2和半监督mean teacher机制，结合域自适应模块与持续测试时适应策略，提升Gd-EOB-DTPA增强MRI肝脏分割的跨中心泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于标注数据有限、增强协议异质性和不同设备间的域偏移，对比增强MRI肝脏分割仍具挑战性。传统图像翻译方法在单模态场景下存在结构失真、训练不稳定等问题，难以直接应用。

Method: 构建CoSSeg-TTA框架，基于nnU-Netv2并引入半监督mean teacher机制以利用大量无标签数据；设计包含随机化直方图风格迁移和可学习对比感知网络的域自适应模块，增强域多样性；采用持续测试时适应策略提升推理鲁棒性。

Result: 实验表明，该方法在Dice分数和Hausdorff距离上均优于nnU-Netv2基线模型，在低标注条件下对未见域表现出强泛化能力。

Conclusion: CoSSeg-TTA通过融合半监督学习、域自适应和测试时适应策略，有效提升了单模态多中心MRI肝脏分割的性能与鲁棒性，适用于临床实际场景。

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.

</details>


### [105] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 提出一种基于概念的解释方法来防御对抗性补丁攻击，无需先验知识即可有效抑制补丁影响。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法通常需要对抗性补丁的大小或位置先验知识，限制了实际应用，因此需要一种无需此类假设的通用防御机制。

Method: 利用基于概念的解释识别并抑制最具影响力的概念激活向量，以中和对抗性补丁的效果，而不显式检测补丁本身。

Result: 在Imagenette数据集和ResNet-50模型上，该方法在鲁棒性和干净准确率方面均优于PatchCleanser，并在不同补丁大小和位置下保持良好性能。

Conclusion: 结合可解释性与鲁棒性的概念驱动防御是一种有前景的、可扩展的对抗性补丁攻击防护策略。

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [106] [Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition](https://arxiv.org/abs/2510.04282)
*Yu Kiu,Lau,Chao Chen,Ge Jin,Chen Feng*

Main category: cs.CV

TL;DR: 提出Adapt-STformer，一种基于新型循环可变形Transformer编码器（Recurrent-DTE）的序列视觉位置识别方法，实现了对不同序列长度的灵活适应、快速推理和低内存消耗，在多个数据集上显著提升了召回率并降低了时间和内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的Seq-VPR方法在性能之外牺牲了灵活性和效率，难以满足实际应用中对可变序列长度、快速推理和低内存使用的需求。

Method: 提出Adapt-STformer，采用循环可变形Transformer编码器（Recurrent-DTE），通过迭代循环机制融合多帧序列信息，支持可变序列长度、高效推理和低内存占用。

Result: 在Nordland、Oxford和NuScenes数据集上实验表明，相比次优基线方法，Adapt-STformer将召回率最高提升17%，序列提取时间减少36%，内存使用降低35%。

Conclusion: Adapt-STformer在保持高性能的同时，显著提升了Seq-VPR模型的灵活性和效率，填补了现有方法在实际应用场景中的关键空白。

Abstract: Sequential Visual Place Recognition (Seq-VPR) leverages transformers to
capture spatio-temporal features effectively; however, existing approaches
prioritize performance at the expense of flexibility and efficiency. In
practice, a transformer-based Seq-VPR model should be flexible to the number of
frames per sequence (seq-length), deliver fast inference, and have low memory
usage to meet real-time constraints. To our knowledge, no existing
transformer-based Seq-VPR method achieves both flexibility and efficiency. To
address this gap, we propose Adapt-STformer, a Seq-VPR method built around our
novel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an
iterative recurrent mechanism to fuse information from multiple sequential
frames. This design naturally supports variable seq-lengths, fast inference,
and low memory usage. Experiments on the Nordland, Oxford, and NuScenes
datasets show that Adapt-STformer boosts recall by up to 17% while reducing
sequence extraction time by 36% and lowering memory usage by 35% compared to
the second-best baseline.

</details>


### [107] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: 本文提出了ChronoEdit，一种将图像编辑重构为视频生成问题的框架，利用预训练视频生成模型和时序推理机制来确保编辑过程中物体的物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法在保持编辑对象的物理一致性方面存在不足，尤其在世界模拟等任务中亟需改进。

Method: 将输入图像和编辑后图像视为视频首尾帧，利用大规模预训练视频生成模型，并引入时序推理阶段，在推理时联合去噪目标帧与推理令牌以生成合理的编辑轨迹，随后丢弃推理令牌以降低计算成本。

Result: 在新提出的PBench-Edit基准上验证，ChronoEdit在视觉保真度和物理合理性方面均优于现有最先进方法。

Conclusion: ChronoEdit通过结合视频生成与时序推理，有效提升了图像编辑中的物理一致性，适用于对物理合理性和连贯性要求较高的应用场景。

Abstract: Recent advances in large generative models have significantly advanced image
editing and in-context image generation, yet a critical gap remains in ensuring
physical consistency, where edited objects must remain coherent. This
capability is especially vital for world simulation related tasks. In this
paper, we present ChronoEdit, a framework that reframes image editing as a
video generation problem. First, ChronoEdit treats the input and edited images
as the first and last frames of a video, allowing it to leverage large
pretrained video generative models that capture not only object appearance but
also the implicit physics of motion and interaction through learned temporal
consistency. Second, ChronoEdit introduces a temporal reasoning stage that
explicitly performs editing at inference time. Under this setting, the target
frame is jointly denoised with reasoning tokens to imagine a plausible editing
trajectory that constrains the solution space to physically viable
transformations. The reasoning tokens are then dropped after a few steps to
avoid the high computational cost of rendering a full video. To validate
ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for
contexts that require physical consistency, and demonstrate that ChronoEdit
surpasses state-of-the-art baselines in both visual fidelity and physical
plausibility. Code and models for both the 14B and 2B variants of ChronoEdit
will be released on the project page:
https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [108] [CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment](https://arxiv.org/abs/2510.04312)
*Vida Adeli,Ivan Klabucar,Javad Rajabi,Benjamin Filtjens,Soroush Mehraban,Diwei Wang,Hyewon Seo,Trung-Hieu Hoang,Minh N. Do,Candice Muller,Claudia Oliveira,Daniel Boari Coelho,Pieter Ginis,Moran Gilat,Alice Nieuwboer,Joke Spildooren,Lucas Mckay,Hyeokhyen Kwon,Gari Clifford,Christine Esper,Stewart Factor,Imari Genias,Amirhossein Dadashzadeh,Leia Shum,Alan Whone,Majid Mirmehdi,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: CARE-PD是首个大规模、多中心的帕金森病3D步态数据集，支持临床评分预测和无监督动作预训练任务，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的客观步态评估受限于缺乏大型、多样化且有临床标注的运动数据集。

Method: 通过统一的预处理流程将来自8个临床中心9个队列的RGB视频或动作捕捉数据转换为匿名SMPL网格，并构建两个基准任务：监督式临床评分预测和无监督动作预训练任务。

Result: 在四个泛化协议下验证了临床评分预测性能；预训练显著降低了MPJPE（从60.8mm降至7.5mm），并将PD严重程度分类的macro-F1提升了17个百分点。

Conclusion: CARE-PD提供了高质量、多样化的临床步态数据，证明了其在提升运动编码器性能和推动PD客观评估方面的价值。

Abstract: Objective gait assessment in Parkinson's Disease (PD) is limited by the
absence of large, diverse, and clinically annotated motion datasets. We
introduce CARE-PD, the largest publicly available archive of 3D mesh gait data
for PD, and the first multi-site collection spanning 9 cohorts from 8 clinical
centers. All recordings (RGB video or motion capture) are converted into
anonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD
supports two key benchmarks: supervised clinical score prediction (estimating
Unified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised
motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D
reconstruction). Clinical prediction is evaluated under four generalization
protocols: within-dataset, cross-dataset, leave-one-dataset-out, and
multi-dataset in-domain adaptation. To assess clinical relevance, we compare
state-of-the-art motion encoders with a traditional gait-feature baseline,
finding that encoders consistently outperform handcrafted features. Pretraining
on CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1
by 17 percentage points, underscoring the value of clinically curated, diverse
training data. CARE-PD and all benchmark code are released for non-commercial
research at https://neurips2025.care-pd.ca/.

</details>


### [109] [GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction](https://arxiv.org/abs/2510.04315)
*Jiarui Ouyang,Yihui Wang,Yihang Gao,Yingxue Xu,Shu Yang,Hao Chen*

Main category: cs.CV

TL;DR: GenAR是一种多尺度自回归框架，通过从粗到细的层次化基因分组和离散化建模，从H&E染色图像中预测空间转录组表达，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立预测每个基因且使用连续回归，忽略了共表达结构并导致生物学上不合理的结果，而实际表达量为离散计数。

Method: 提出GenAR，采用多尺度自回归框架，将基因聚类为层次化组以捕捉跨基因依赖性，以无码本的离散token生成方式建模表达量，并融合组织学与空间嵌入进行条件解码。

Result: 在四个不同组织类型的ST数据集上实验表明，GenAR在预测性能上达到最先进水平，能更准确地预测原始计数。

Conclusion: GenAR通过离散化建模和层次化依赖学习，有效提升从H&E图像预测空间基因表达的准确性，具有用于精准医学和低成本分子分析的潜力。

Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression but
remains costly. Predicting expression directly from widely available
Hematoxylin and Eosin (H&E) stained images presents a cost-effective
alternative. However, most computational approaches (i) predict each gene
independently, overlooking co-expression structure, and (ii) cast the task as
continuous regression despite expression being discrete counts. This mismatch
can yield biologically implausible outputs and complicate downstream analyses.
We introduce GenAR, a multi-scale autoregressive framework that refines
predictions from coarse to fine. GenAR clusters genes into hierarchical groups
to expose cross-gene dependencies, models expression as codebook-free discrete
token generation to directly predict raw counts, and conditions decoding on
fused histological and spatial embeddings. From an information-theoretic
perspective, the discrete formulation avoids log-induced biases and the
coarse-to-fine factorization aligns with a principled conditional
decomposition. Extensive experimental results on four Spatial Transcriptomics
datasets across different tissue types demonstrate that GenAR achieves
state-of-the-art performance, offering potential implications for precision
medicine and cost-effective molecular profiling. Code is publicly available at
https://github.com/oyjr/genar.

</details>


### [110] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Diffusion^2的新框架，用于解决行人瞬间轨迹预测问题，通过两个扩散模型分别进行历史轨迹反向生成和未来轨迹前向预测，并引入不确定性估计与自适应噪声调节机制，在ETH/UCY和Stanford Drone数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在真实场景中，行人可能突然从盲区出现，导致观测数据不足（即瞬间轨迹），传统方法难以准确预测其未来轨迹，增加了交通事故风险。因此，亟需研究极端场景下的行人轨迹预测以提升交通安全。

Method: 提出Diffusion^2框架，包含两个串联的扩散模型：一个用于反向预测未观测到的历史轨迹，另一个用于前向预测未来轨迹；设计双头参数化机制估计生成轨迹的偶然性不确定性，并引入时序自适应噪声模块，动态调整前向扩散过程中的噪声尺度。

Result: 在ETH/UCY和Stanford Drone数据集上的实验表明，Diffusion^2在瞬间轨迹预测任务上优于现有方法，取得了新的最先进性能。

Conclusion: Diffusion^2通过联合建模历史与未来轨迹，并结合不确定性感知和噪声调节机制，有效提升了在观测数据稀缺情况下的行人轨迹预测精度，对增强自动驾驶和人机交互系统的安全性具有重要意义。

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and
efficiency in autonomous driving and human-robot interaction scenarios. Earlier
studies primarily utilized sufficient observational data to predict future
trajectories. However, in real-world scenarios, such as pedestrians suddenly
emerging from blind spots, sufficient observational data is often unavailable
(i.e. momentary trajectory), making accurate prediction challenging and
increasing the risk of traffic accidents. Therefore, advancing research on
pedestrian trajectory prediction under extreme scenarios is critical for
enhancing traffic safety. In this work, we propose a novel framework termed
Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists
of two sequentially connected diffusion models: one for backward prediction,
which generates unobserved historical trajectories, and the other for forward
prediction, which forecasts future trajectories. Given that the generated
unobserved historical trajectories may introduce additional noise, we propose a
dual-head parameterization mechanism to estimate their aleatoric uncertainty
and design a temporally adaptive noise module that dynamically modulates the
noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a
new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford
Drone datasets.

</details>


### [111] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim是一个语言引导的框架，能够根据自然语言指令生成具有多视角一致性和对象级控制的4D场景，支持交互式编辑而无需完全重新生成，实验证明其在保持高场景保真度的同时实现了良好的可控性和可编辑性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频模型局限于2D视图且交互能力有限，难以满足机器人技术对可控制、可编辑的时空环境的需求。因此，需要一种能够在4D空间中生成动态、可操控场景的世界模型。

Method: MorphoSim结合了轨迹引导生成与特征场蒸馏技术，通过自然语言指令驱动4D场景生成，并实现对象级别的操作（如移动、重着色、删除）和任意视角观察，支持交互式编辑。

Result: 实验表明，MorphoSim在多种自然语言指令下均能生成高质量、多视角一致的动态4D场景，在保持高场景保真度的同时，支持实时的对象级编辑和视角变换。

Conclusion: MorphoSim为构建可控制、可编辑的4D世界模型提供了有效解决方案，有望推动机器人训练、评估和任务设计的发展。

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [112] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个名为VLMCountBench的基准，用于评估视觉语言模型（VLMs）在基本几何形状上的计数能力，发现当前VLMs在多类型形状组合的计数任务中表现不佳，暴露出其组合计数能力的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多种视觉任务中表现出色，但其是否能正确计数物体仍不清楚。本文旨在探究这一基本问题，揭示现有模型在组合计数上的局限性。

Method: 设计了一个极简设置下的基准VLMCountBench，仅使用基本几何形状及其组合，严格控制变量，系统研究颜色、大小和提示优化等因素对计数性能的影响。

Result: 实验结果表明，当仅存在一种形状时，VLMs能够可靠计数；但在多种形状组合的情况下，计数性能显著下降，显示出严重的失败。

Conclusion: 当前的VLMs在组合计数任务上存在根本性的缺陷，未来的研究需要针对性地改进模型在复杂场景中的数量推理能力。

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [113] [CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning](https://arxiv.org/abs/2510.04410)
*Venkata Bharath Reddy Reddem,Akshay P Sarashetti,Ranjith Merugu,Amit Satish Unde*

Main category: cs.CV

TL;DR: 本文提出了CodeFormer++，一种通过分解盲脸修复为三个子任务并结合生成先验来实现高质量且身份保持的面部修复新框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉质量与身份保真度之间存在权衡，导致身份失真或退化去除不理想。

Method: 将盲脸修复分解为三个子任务：身份保持的面部修复、高质量面部生成以及身份特征与纹理细节的动态融合；引入基于学习的可变形人脸配准模块、纹理引导的修复网络和深度度量学习。

Result: 在真实和合成数据集上的实验表明，CodeFormer++在视觉保真度和身份一致性方面均优于现有方法。

Conclusion: CodeFormer++有效平衡了生成质量与身份保持，显著提升了盲脸修复性能。

Abstract: Blind face restoration (BFR) has attracted increasing attention with the rise
of generative methods. Most existing approaches integrate generative priors
into the restoration pro- cess, aiming to jointly address facial detail
generation and identity preservation. However, these methods often suffer from
a trade-off between visual quality and identity fidelity, leading to either
identity distortion or suboptimal degradation removal. In this paper, we
present CodeFormer++, a novel framework that maximizes the utility of
generative priors for high-quality face restoration while preserving identity.
We decompose BFR into three sub-tasks: (i) identity- preserving face
restoration, (ii) high-quality face generation, and (iii) dynamic fusion of
identity features with realistic texture details. Our method makes three key
contributions: (1) a learning-based deformable face registration module that
semantically aligns generated and restored faces; (2) a texture guided
restoration network to dynamically extract and transfer the texture of
generated face to boost the quality of identity-preserving restored face; and
(3) the integration of deep metric learning for BFR with the generation of
informative positive and hard negative samples to better fuse identity-
preserving and generative features. Extensive experiments on real-world and
synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves
superior performance in terms of both visual fidelity and identity consistency.

</details>


### [114] [A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering](https://arxiv.org/abs/2510.04428)
*Yuanhao Zou,Shengji Jin,Andong Deng,Youpeng Zhao,Jun Wang,Chen Chen*

Main category: cs.CV

TL;DR: 提出一种无需训练的自适应、迭代式基于推理的帧选择方法A.I.R.，在保持高计算效率的同时提升视频问答中视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有帧选择方法在准确性和计算成本之间存在权衡：轻量模型难以捕捉复杂查询的细节，而使用VLM的方法虽准确但计算开销过大。

Method: 利用强大的视觉语言模型对复杂查询进行深度语义分析，并设计一个低成本的迭代循环，每次仅处理少量最具潜力的帧，实现高效且精准的帧选择。

Result: 在多个视频问答基准上的实验表明，该方法优于现有的帧选择方法，显著提升了基础VLM的性能，并在计算效率上优于其他基于VLM的技术。

Conclusion: A.I.R.通过自适应迭代推理机制，在保证低计算成本的同时实现了更优的帧选择效果，为视频问答中的VLM应用提供了高效解决方案。

Abstract: Effectively applying Vision-Language Models (VLMs) to Video Question
Answering (VideoQA) hinges on selecting a concise yet comprehensive set of
frames, as processing entire videos is computationally infeasible. However,
current frame selection methods face a critical trade-off: approaches relying
on lightweight similarity models, such as CLIP, often fail to capture the
nuances of complex queries, resulting in inaccurate similarity scores that
cannot reflect the authentic query-frame relevance, which further undermines
frame selection. Meanwhile, methods that leverage a VLM for deeper analysis
achieve higher accuracy but incur prohibitive computational costs. To address
these limitations, we propose A.I.R., a training-free approach for Adaptive,
Iterative, and Reasoning-based frame selection. We leverage a powerful VLM to
perform deep, semantic analysis on complex queries, and this analysis is
deployed within a cost-effective iterative loop that processes only a small
batch of the most high-potential frames at a time. Extensive experiments on
various VideoQA benchmarks demonstrate that our approach outperforms existing
frame selection methods, significantly boosts the performance of the foundation
VLM, and achieves substantial gains in computational efficiency over other
VLM-based techniques.

</details>


### [115] [REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization](https://arxiv.org/abs/2510.04450)
*Qiyuan He,Yicong Li,Haotian Ye,Jinghao Wang,Xinyao Liao,Pheng-Ann Heng,Stefano Ermon,James Zou,Angela Yao*

Main category: cs.CV

TL;DR: 本文提出了一种名为reAR的简单训练策略，通过引入逐token的正则化目标来缓解视觉自回归生成中的生成器-解码器不一致性问题，显著提升了生成性能，在ImageNet上达到了与大规模扩散模型相当的效果。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归生成模型性能落后于扩散模型，以往研究归因于分词器限制和光栅化顺序，本文从生成器与分词器不一致的角度识别出核心瓶颈。

Method: 提出reAR方法，在预测下一个token时，同时训练因果Transformer恢复当前token的视觉嵌入并预测目标token在噪声上下文中的嵌入，无需修改分词器、生成顺序或推理流程。

Result: 在ImageNet上，gFID从3.02降至1.86，IS提升至316.9；使用先进分词器时gFID达1.42，仅177M参数即可匹配675M参数扩散模型的性能。

Conclusion: reAR有效缓解了生成器-分词器不一致性问题，显著提升视觉自回归模型生成质量，且具备轻量、通用、易集成的优势。

Abstract: Visual autoregressive (AR) generation offers a promising path toward unifying
vision and language models, yet its performance remains suboptimal against
diffusion models. Prior work often attributes this gap to tokenizer limitations
and rasterization ordering. In this work, we identify a core bottleneck from
the perspective of generator-tokenizer inconsistency, i.e., the AR-generated
tokens may not be well-decoded by the tokenizer. To address this, we propose
reAR, a simple training strategy introducing a token-wise regularization
objective: when predicting the next token, the causal transformer is also
trained to recover the visual embedding of the current token and predict the
embedding of the target token under a noisy context. It requires no changes to
the tokenizer, generation order, inference pipeline, or external models.
Despite its simplicity, reAR substantially improves performance. On ImageNet,
it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard
rasterization-based tokenizer. When applied to advanced tokenizers, it achieves
a gFID of 1.42 with only 177M parameters, matching the performance with larger
state-of-the-art diffusion models (675M).

</details>


### [116] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: 提出SPEGNet，通过统一设计实现高效伪装物体检测，在多数据集上表现优异并支持实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂组件累积，导致计算负担重且丢失细节，难以有效处理伪装物体的边界和精细结构。

Method: 采用通道校准和空间增强融合多尺度特征，通过渐进式 refinement 实现尺度自适应边缘调制，保持语义-空间一致性。

Result: 在CAMO、COD10K和NC4K上分别达到0.887、0.890和0.895的Sα分数，具备实时推理能力，能处理小物体、遮挡和模糊边界。

Conclusion: SPEGNet通过统一架构平衡了边界精度与区域一致性，在性能和效率之间取得良好权衡，适用于各种复杂场景下的伪装物体检测。

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [117] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM是一个自动化管道，通过结合病灶框、器官分割和结构化推理，将检测数据集转化为大规模医学视觉问答（VQA）数据，并引入集成的CoT-课程学习策略，在多个医学VQA基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 弥合临床诊断推理与人工智能在医学影像中的差距，提升模型的可解释性和临床对齐能力。

Method: 提出MedCLM管道，生成带有链式思维（CoT）推理的医学VQA数据；设计三阶段课程学习策略（Easy：显式病灶定位；Medium：隐式定位；Hard：弱监督推理）。

Result: 在多个医学VQA基准上实现了最先进的性能，验证了方法的有效性和可扩展性。

Conclusion: MedCLM为开发具有临床对齐能力的医学视觉语言模型提供了一个可扩展的框架，推动AI在医学影像诊断中的实际应用。

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [118] [VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery](https://arxiv.org/abs/2510.04479)
*Nonghai Zhang,Zeyu Zhang,Jiazi Wang,Yang Zhao,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了首个用于古希腊陶器分析的3D视觉问答数据集VaseVQA-3D，并构建了针对性的VaseVLM模型，通过领域自适应训练显著提升了对3D陶器文物的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在文化遗产等专业领域面临数据稀缺和领域知识不足的问题，难以有效处理如3D陶器文物分析等特定任务。

Method: 构建了包含664个古希腊陶器3D模型及对应问答数据的VaseVQA-3D数据集，并提出VaseVLM模型，采用领域自适应训练方法提升模型性能。

Result: 在VaseVQA-3D数据集上，相比先前最优方法，R@1指标提升12.8%，词汇相似度提升6.6%，显著增强了对3D陶器文物的识别与理解能力。

Conclusion: 所提方法为数字文化遗产保护研究提供了新的技术路径，验证了领域自适应训练在专业视觉语言任务中的有效性。

Abstract: Vision-Language Models (VLMs) have achieved significant progress in
multimodal understanding tasks, demonstrating strong capabilities particularly
in general tasks such as image captioning and visual reasoning. However, when
dealing with specialized cultural heritage domains like 3D vase artifacts,
existing models face severe data scarcity issues and insufficient domain
knowledge limitations. Due to the lack of targeted training data, current VLMs
struggle to effectively handle such culturally significant specialized tasks.
To address these challenges, we propose the VaseVQA-3D dataset, which serves as
the first 3D visual question answering dataset for ancient Greek pottery
analysis, collecting 664 ancient Greek vase 3D models with corresponding
question-answer data and establishing a complete data construction pipeline. We
further develop the VaseVLM model, enhancing model performance in vase artifact
analysis through domain-adaptive training. Experimental results validate the
effectiveness of our approach, where we improve by 12.8% on R@1 metrics and by
6.6% on lexical similarity compared with previous state-of-the-art on the
VaseVQA-3D dataset, significantly improving the recognition and understanding
of 3D vase artifacts, providing new technical pathways for digital heritage
preservation research.

</details>


### [119] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: 本文提出了一种面向电商领域的图像编辑模型TBStar-Edit，通过数据工程、模型架构设计和两阶段训练策略，在保持商品外观和布局一致性的前提下实现了高保真编辑。


<details>
  <summary>Details</summary>
Motivation: 通用图像编辑模型在电商场景中存在一致性不足的问题，难以满足对商品视觉一致性的高要求。

Method: 提出了TBStar-Edit模型：1）构建高质量、指令跟随的编辑数据 pipeline；2）设计包含基础模型、模式迁移模块和一致性增强模块的分层架构；3）采用两阶段训练策略，分别优化模式迁移和一致性保持。

Result: 在自建电商基准上的实验表明，TBStar-Edit在客观指标（VIE Score）和用户主观偏好上均优于现有通用图像编辑模型。

Conclusion: TBStar-Edit有效解决了通用模型在电商场景中的一致性问题，为领域特定的图像编辑提供了可行方案。

Abstract: Recent advances in image generation and editing technologies have enabled
state-of-the-art models to achieve impressive results in general domains.
However, when applied to e-commerce scenarios, these general models often
encounter consistency limitations. To address this challenge, we introduce
TBStar-Edit, an new image editing model tailored for the e-commerce domain.
Through rigorous data engineering, model architecture design and training
strategy, TBStar-Edit achieves precise and high-fidelity image editing while
maintaining the integrity of product appearance and layout. Specifically, for
data engineering, we establish a comprehensive data construction pipeline,
encompassing data collection, construction, filtering, and augmentation, to
acquire high-quality, instruction-following, and strongly consistent editing
data to support model training. For model architecture design, we design a
hierarchical model framework consisting of a base model, pattern shifting
modules, and consistency enhancement modules. For model training, we adopt a
two-stage training strategy to enhance the consistency preservation: first
stage for editing pattern shifting, and second stage for consistency
enhancement. Each stage involves training different modules with separate
datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a
self-proposed e-commerce benchmark, and the results demonstrate that
TBStar-Edit outperforms existing general-domain editing models in both
objective metrics (VIE Score) and subjective user preference.

</details>


### [120] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: 提出异步扩散模型，通过为不同像素分配不同的时间步长来改善文本到图像的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在生成图像时难以准确地将生成内容与输入提示对齐，主要因为同步去噪过程中所有像素同时从噪声变为清晰图像，导致相关区域无法有效利用清晰的上下文信息。

Method: 提出了异步扩散模型框架，重新设计了像素级别的去噪过程，动态调整每个像素的时间步调度，使得与提示相关的区域比不相关的区域更缓慢地去噪，从而能够利用更清晰的像素间上下文。

Result: 实验表明，所提出的异步扩散模型能够在多种提示下显著提升文本到图像的对齐性能。

Conclusion: 异步扩散模型通过差异化处理不同像素的去噪进程，有效增强了生成图像与文本提示之间的对齐能力，为高质量文本到图像生成提供了新思路。

Abstract: Diffusion models have achieved impressive results in generating high-quality
images. Yet, they often struggle to faithfully align the generated images with
the input prompts. This limitation arises from synchronous denoising, where all
pixels simultaneously evolve from random noise to clear images. As a result,
during generation, the prompt-related regions can only reference the unrelated
regions at the same noise level, failing to obtain clear context and ultimately
impairing text-to-image alignment. To address this issue, we propose
asynchronous diffusion models -- a novel framework that allocates distinct
timesteps to different pixels and reformulates the pixel-wise denoising
process. By dynamically modulating the timestep schedules of individual pixels,
prompt-related regions are denoised more gradually than unrelated regions,
thereby allowing them to leverage clearer inter-pixel context. Consequently,
these prompt-related regions achieve better alignment in the final images.
Extensive experiments demonstrate that our asynchronous diffusion models can
significantly improve text-to-image alignment across diverse prompts. The code
repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [121] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 提出了一种名为Tangential Amplifying Guidance (TAG)的新型扩散模型引导方法，通过放大估计得分的切向分量来校正采样轨迹，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像生成中存在语义不一致或幻觉问题，且现有引导方法常依赖外部信号或结构修改，带来额外计算开销。

Method: 利用中间样本作为投影基，放大相对于该基的得分切向分量，并通过一阶泰勒展开形式化引导过程。

Result: TAG在不修改模型结构的情况下提升了采样保真度，具有较低计算开销，并在多种任务中改善生成质量。

Conclusion: TAG是一种即插即用、架构无关的高效引导方法，为扩散模型引导提供了新视角。

Abstract: Recent diffusion models achieve the state-of-the-art performance in image
generation, but often suffer from semantic inconsistencies or hallucinations.
While various inference-time guidance methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on trajectory signals without
modifying the underlying diffusion model. TAG leverages an intermediate sample
as a projection basis and amplifies the tangential components of the estimated
scores with respect to this basis to correct the sampling trajectory. We
formalize this guidance process by leveraging a first-order Taylor expansion,
which demonstrates that amplifying the tangential component steers the state
toward higher-probability regions, thereby reducing inconsistencies and
enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.

</details>


### [122] [Conditional Representation Learning for Customized Tasks](https://arxiv.org/abs/2510.04564)
*Honglin Liu,Chao Sun,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: 本文提出了一种条件表示学习（CRL）方法，通过利用大语言模型生成描述文本以构建自定义语义基，并借助视觉-语言模型将图像表示投影到该条件特征空间中，从而获得更符合特定任务需求的表示。


<details>
  <summary>Details</summary>
Motivation: 传统表示学习方法学习的是通用表示，主要捕获主导语义，难以满足定制化下游任务的需求，例如在动物栖息地分析中研究者更关注场景特征而非类别语义。

Method: CRL首先使用大语言模型根据用户指定的标准生成描述性文本以构建语义基，然后利用视觉-语言模型将图像表示投影到这一条件特征空间中，实现对特定语义的提取。

Result: 在分类和检索任务上的大量实验表明，CRL在多种定制任务中优于现有方法，具有良好的通用性和性能优势。

Conclusion: CRL能够有效生成符合用户自定义标准的条件表示，在无需大量标注和微调的情况下提升下游任务表现，具备广泛应用潜力。

Abstract: Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

</details>


### [123] [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587)
*Sheng Wang,Ruiming Wu,Charles Herndon,Yihang Liu,Shunsuke Koga,Jeanne Shen,Zhi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种基于专家浏览行为记录的可扩展监督方法，构建了病理学领域的首个行为驱动智能体系统Pathologist-o3，在淋巴结转移检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有病理学基础模型缺乏对专家实际阅片行为（如视野切换、放大调整）的建模，且缺少临床对齐的行为监督数据，限制了具身智能系统的发展。

Method: 开发AI Session Recorder工具记录常规全切片图像浏览日志，将其转化为标准化行为指令和边界框，并通过轻量级人工校验生成Pathology-CoT数据集；基于此数据训练两阶段智能体Pathologist-o3，先定位感兴趣区域，再进行行为引导推理。

Result: 在胃肠道淋巴结转移检测任务中，Pathologist-o3达到84.5%精确率、100.0%召回率和75.4%准确率，性能超越OpenAI o3模型并适用于不同骨干网络。

Conclusion: 该研究首次实现了基于真实专家行为的可扩展监督框架，使具身化病理诊断AI成为可能，为人类对齐、可升级的临床AI提供了可行路径。

Abstract: Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

</details>


### [124] [A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2510.04628)
*Hao Liu,Yunhao Gao,Wei Li,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 本文提出了一种用于多模态遥感图像分类的时空频交互网络S²Fin，通过引入频域学习增强关键和稀疏细节特征的提取，在四个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有特征融合技术在处理异构且冗余的多模态遥感图像时，难以有效提取结构和细节特征，因此需要引入频域学习来提升性能。

Method: 提出S²Fin网络，包含高频稀疏增强Transformer、双层空间-频率融合策略（自适应频率通道模块和高频共振掩码）以及时空谱注意力融合模块，实现跨空间、光谱和频率域的配对融合。

Result: 在四个标注数据有限的多模态遥感数据集上实验表明，S²Fin在分类性能上优于当前最先进的方法。

Conclusion: S²Fin通过融合空间、光谱和频率域信息，显著提升了多模态遥感图像分类的精度，尤其在细节和结构特征提取方面表现突出。

Abstract: Deep learning-based methods have achieved significant success in remote
sensing Earth observation data analysis. Numerous feature fusion techniques
address multimodal remote sensing image classification by integrating global
and local features. However, these techniques often struggle to extract
structural and detail features from heterogeneous and redundant multimodal
images. With the goal of introducing frequency domain learning to model key and
sparse detail features, this paper introduces the spatial-spectral-frequency
interaction network (S$^2$Fin), which integrates pairwise fusion modules across
the spatial, spectral, and frequency domains. Specifically, we propose a
high-frequency sparse enhancement transformer that employs sparse
spatial-spectral attention to optimize the parameters of the high-frequency
filter. Subsequently, a two-level spatial-frequency fusion strategy is
introduced, comprising an adaptive frequency channel module that fuses
low-frequency structures with enhanced high-frequency details, and a
high-frequency resonance mask that emphasizes sharp edges via phase similarity.
In addition, a spatial-spectral attention fusion module further enhances
feature extraction at intermediate layers of the network. Experiments on four
benchmark multimodal datasets with limited labeled data demonstrate that
S$^2$Fin performs superior classification, outperforming state-of-the-art
methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.

</details>


### [125] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 提出一种结合Transformer架构和纹理方法的混合模型，用于提升深度伪造检测的准确性和鲁棒性，在DFWild-Cup数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨数据集和生成技术上的泛化能力不足，难以应对日益复杂的深度伪造媒体。

Method: 设计一种新型集成框架，融合Swin Transformer、ViT等Transformer架构与基于纹理的方法，引入数据分割、顺序训练、频率分割、基于patch的注意力机制和人脸分割技术，以增强关键区域并改善泛化能力。

Result: 在DFWild-Cup数据集上达到最先进的检测性能，验证了混合模型在准确性和鲁棒性方面的优势。

Conclusion: 混合模型能有效应对深度伪造检测中的多样化挑战，为实际应用提供了更具鲁棒性的解决方案。

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [126] [Do Superpixel Segmentation Methods Influence Deforestation Image Classification?](https://arxiv.org/abs/2510.04645)
*Hugo Resende,Fabio A. Faria,Eduardo B. Neto,Isabela Borlido,Victor Sundermann,Silvio Jamil F. Guimarães,Álvaro L. Fazenda*

Main category: cs.CV

TL;DR: 本研究探讨了不同超像素分割方法对热带森林砍伐检测任务中分类器训练的影响，发现结合分类器融合方法能显著提升平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 在ForestEyes项目中，传统使用的SLIC分割方法可能并非最优，需评估更优的分割方法以提高砍伐检测性能。

Method: 比较四种优于SLIC的超像素分割方法与SLIC在遥感图像分割中的表现，并使用PyCaret AutoML库选择顶级分类器，结合分类器融合策略进行模型集成。

Result: 单独使用各分割方法时分类器性能差异较小，但在引入分类器融合后，平衡准确率明显提升，表明分割方法选择与模型融合均对检测效果有重要影响。

Conclusion: 为提升遥感图像中砍伐区域的检测效果，不仅应考虑先进的分割方法，还应采用分类器融合策略以增强模型性能。

Abstract: Image segmentation is a crucial step in various visual applications,
including environmental monitoring through remote sensing. In the context of
the ForestEyes project, which combines citizen science and machine learning to
detect deforestation in tropical forests, image segments are used for labeling
by volunteers and subsequent model training. Traditionally, the Simple Linear
Iterative Clustering (SLIC) algorithm is adopted as the segmentation method.
However, recent studies have indicated that other superpixel-based methods
outperform SLIC in remote sensing image segmentation, and might suggest that
they are more suitable for the task of detecting deforested areas. In this
sense, this study investigated the impact of the four best segmentation
methods, together with SLIC, on the training of classifiers for the target
application. Initially, the results showed little variation in performance
among segmentation methods, even when selecting the top five classifiers using
the PyCaret AutoML library. However, by applying a classifier fusion approach
(ensemble of classifiers), noticeable improvements in balanced accuracy were
observed, highlighting the importance of both the choice of segmentation method
and the combination of machine learning-based models for deforestation
detection tasks.

</details>


### [127] [EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents](https://arxiv.org/abs/2510.04648)
*Buyuan Zhu,Shiyu Hu,Yiping Ma,Yuanming Zhang,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduPersona是一个面向教育场景的大规模基准测试，用于评估虚拟学生代理的主观能力，涵盖两种语言、三个学科和十种人格类型，通过分解为主观表现的三个渐进任务（基本连贯性、学生真实性、长期人格一致性）建立可验证的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在教育中的应用日益广泛，但其虚拟学生代理在课堂情境下的主观能力缺乏系统评估，限制了对其能力边界的理解与可信部署。

Method: 基于Big Five人格理论构建包含1,308轮真实课堂对话的数据集（共12,814个问答回合），并通过人格风格化扩展至约128,000回合；提出三阶段评估任务框架，并在三种代表性大模型及其十种微调变体上进行实验。

Result: 实验结果显示，在所有任务上均有显著提升：TASK1（基本连贯性）+33.6%，TASK2（学生真实性）+30.6%，TASK3（长期人格一致性）+14.9%，验证了数据集的有效性和研究价值，并揭示了人格建模任务的难度差异。

Conclusion: EduPersona提供了首个聚焦课堂主观能力的基准，建立了可解耦、可验证的研究范式，将开源数据与框架以推动可信、类人AI在教育中的发展。

Abstract: As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

</details>


### [128] [MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts](https://arxiv.org/abs/2510.04654)
*Andy Cǎtrunǎ,Adrian Cosma,Emilian Rǎdoi*

Main category: cs.CV

TL;DR: 提出了一种基于2D姿态的分层多阶段运动专家混合模型（MoME），用于从步态序列中多任务预测心理特征，在PsyMo基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 利用步态编码的心理和行为信息来推断心理特质仍是一个具有挑战性且研究不足的问题，因此需要更有效的模型来挖掘步态中的深层心理关联。

Method: 提出MoME架构，将步行周期分为四个运动复杂度阶段，使用轻量级专家模型提取时空特征，并通过任务特定的门控模块自适应加权不同心理特质和阶段的专家。

Result: 在PsyMo基准（涵盖17种心理特质）上，MoME在运行级别达到37.47%的加权F1分数，主体级别达到44.6%，优于当前最先进的步态分析模型；引入身份识别、性别预测和BMI估计等辅助任务可进一步提升性能。

Conclusion: 证明了基于步态的多任务学习在心理特质估计中的可行性，为基于运动的行为心理推断研究提供了新基础。

Abstract: Gait encodes rich biometric and behavioural information, yet leveraging the
manner of walking to infer psychological traits remains a challenging and
underexplored problem. We introduce a hierarchical Multi-Stage Mixture of
Movement Experts (MoME) architecture for multi-task prediction of psychological
attributes from gait sequences represented as 2D poses. MoME processes the
walking cycle in four stages of movement complexity, employing lightweight
expert models to extract spatio-temporal features and task-specific gating
modules to adaptively weight experts across traits and stages. Evaluated on the
PsyMo benchmark covering 17 psychological traits, our method outperforms
state-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at
the run level and 44.6% at the subject level. Our experiments show that
integrating auxiliary tasks such as identity recognition, gender prediction,
and BMI estimation further improves psychological trait estimation. Our
findings demonstrate the viability of multi-task gait-based learning for
psychological trait estimation and provide a foundation for future research on
movement-informed psychological inference.

</details>


### [129] [ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](https://arxiv.org/abs/2510.04668)
*Habin Lim,Yeongseob Won,Juwon Seo,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 本文提出了一种名为ConceptSplit的新框架，用于解决文本到图像扩散模型中多概念个性化时的概念混合问题。


<details>
  <summary>Details</summary>
Motivation: 多概念个性化在文本到图像生成中受到关注，但存在概念混合的问题，影响生成图像的质量和准确性。

Method: 提出了两个关键组件：ToVA（Token-wise Value Adaptation）训练方法，专注于调整交叉注意力中的值投影；LODA（Latent Optimization for Disentangled Attention），通过优化输入潜在变量来缓解推理过程中的注意力纠缠。

Result: 通过大量定性和定量实验，证明了ConceptSplit能够有效实现鲁棒的多概念个性化，减少不必要的概念干扰。

Conclusion: ConceptSplit框架通过ToVA和LODA有效解决了多概念个性化中的概念混合问题，提高了生成图像的质量。

Abstract: In recent years, multi-concept personalization for text-to-image (T2I)
diffusion models to represent several subjects in an image has gained much more
attention. The main challenge of this task is "concept mixing", where multiple
learned concepts interfere or blend undesirably in the output image. To address
this issue, in this paper, we present ConceptSplit, a novel framework to split
the individual concepts through training and inference. Our framework comprises
two key components. First, we introduce Token-wise Value Adaptation (ToVA), a
merging-free training method that focuses exclusively on adapting the value
projection in cross-attention. Based on our empirical analysis, we found that
modifying the key projection, a common approach in existing methods, can
disrupt the attention mechanism and lead to concept mixing. Second, we propose
Latent Optimization for Disentangled Attention (LODA), which alleviates
attention entanglement during inference by optimizing the input latent. Through
extensive qualitative and quantitative experiments, we demonstrate that
ConceptSplit achieves robust multi-concept personalization, mitigating
unintended concept interference. Code is available at
https://github.com/KU-VGI/ConceptSplit

</details>


### [130] [Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI](https://arxiv.org/abs/2510.04705)
*Quang-Khai Bui-Tran,Minh-Toan Dinh,Thanh-Huy Nguyen,Ba-Thinh Lam,Mai-Anh Vu,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出一种标签高效、无需空间配准的多相位多厂商MRI肝脏分割方法，结合基础模型微调与协同训练，提升跨模态泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在真实临床环境中，多相位MRI标注数据稀缺且分布不均，尤其缺乏GED4肝胆期标注和非对比序列标签，同时存在图像错位和缺失相位问题。

Method: 采用基础规模的3D分割骨干网络进行微调，结合跨伪监督协同训练以利用未标注序列，并设计标准化预处理流程，无需空间配准即可实现跨相位和跨厂商的泛化。

Result: 在有标签和无标签域均表现出鲁棒的分割性能，验证了该方法在多相位、多厂商MRI中有效且具有良好的泛化能力。

Conclusion: 结合基础模型适应与协同训练的标签高效策略，在现实临床影像任务中具有显著潜力，适用于资源受限的多中心肝脏MRI分析。

Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

</details>


### [131] [ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion](https://arxiv.org/abs/2510.04706)
*Foivos Paraperas Papantoniou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的框架，能够在保持人脸身份一致的同时，实现对精细面部表情的精确控制，支持从基本情绪到微表情和动态过渡的生成，并通过参考适配器实现真实图像的表情编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保持面部身份一致性的同时，难以实现细粒度的表情控制，且大多忽略微表情和表情过渡。

Method: 基于ID一致的人脸基础模型，采用由FLAME混合形状参数引导的表情交叉注意力模块，并结合参考适配器进行外观迁移。

Result: 在多种图像和视频数据上训练后，模型能泛化至微表情和表达性过渡，在定量和定性评估中均优于现有方法。

Conclusion: 该框架实现了高保真、身份一致且可控的面部表情生成，显著提升了AI驱动叙事中人物表现的自然性和灵活性。

Abstract: Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

</details>


### [132] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: 提出了一种名为ReactDiff的时序扩散框架，用于生成多样化且符合对话情境的逼真面部反应。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法建模真实人类反应中的随机性和动态性，难以生成自然、多样的面部反应。

Method: 引入两种先验知识（时空面部运动学和面部动作单元依赖）到扩散过程中，以保证生成反应的时间连贯性和解剖合理性。

Result: 在REACT2024数据集上的实验表明，该方法在反应质量、多样性及情境适配性方面均优于现有方法。

Conclusion: ReactDiff能有效生成逼真、稳定且多样化的面部反应，显著提升人机交互中的自然度。

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic
dialogue remains a critical challenge for human-computer interaction systems.
Existing methods fail to model the stochasticity and dynamics inherent in real
human reactions. To address this, we propose ReactDiff, a novel temporal
diffusion framework for generating diverse facial reactions that are
appropriate for responding to any given dialogue context. Our key insight is
that plausible human reactions demonstrate smoothness, and coherence over time,
and conform to constraints imposed by human facial anatomy. To achieve this,
ReactDiff incorporates two vital priors (spatio-temporal facial kinematics)
into the diffusion process: i) temporal facial behavioral kinematics and ii)
facial action unit dependencies. These two constraints guide the model toward
realistic human reaction manifolds, avoiding visually unrealistic jitters,
unstable transitions, unnatural expressions, and other artifacts. Extensive
experiments on the REACT2024 dataset demonstrate that our approach not only
achieves state-of-the-art reaction quality but also excels in diversity and
reaction appropriateness.

</details>


### [133] [Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction](https://arxiv.org/abs/2510.04714)
*KunHo Heo,GiHyun Kim,SuYeon Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D语义场景图预测方法，通过设计高判别性物体特征编码器和对比预训练策略，显著提升了物体和关系预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在物体和关系特征表示能力上不足，过度依赖图神经网络且判别力有限，同时未能充分利用几何与语义信息的融合。

Method: 设计了一个高判别性的物体特征编码器，并采用对比预训练策略，将物体表征学习与场景图预测解耦；结合几何和语义特征进行关系预测。

Result: 在3DSSG数据集上的实验表明，该方法在所有评估指标上均显著优于现有最先进方法，且预训练编码器可有效提升现有框架性能。

Conclusion: 高质量的物体特征对场景图预测至关重要，所提出的解耦预训练策略和多模态特征融合有效提升了3D语义场景图的预测性能。

Abstract: 3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.

</details>


### [134] [Benchmark on Monocular Metric Depth Estimation in Wildlife Setting](https://arxiv.org/abs/2510.04723)
*Niccolò Niccoli,Lorenzo Seidenari,Ilaria Greco,Francesco Rovero*

Main category: cs.CV

TL;DR: 本文提出了首个用于野生动物监测条件下单目度量深度估计的基准，评估了四种先进的单目深度估计方法，并发现Depth Anything V2在精度和速度之间达到了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏深度信息，从单目图像中提取准确的距离测量在野生动物监测中具有挑战性，且现有单目深度估计方法在自然野生动物环境中的表现尚未系统评估。

Method: 在93个带有真实距离标注的相机陷阱图像上评估了四种最先进的单目深度估计模型（Depth Anything V2、ML Depth Pro、ZoeDepth 和 Metric3D）以及一种几何基线方法，采用中位数和均值方式进行深度提取，并分析其精度与计算效率。

Result: Depth Anything V2 表现最佳，平均绝对误差为0.454米，相关系数为0.962；ZoeDepth 虽然最快（每张图像0.17秒），但误差较大（MAE: 3.087米）；中位数法在所有深度学习方法中均优于均值法。

Conclusion: 该研究建立了适用于野生动物监测的深度估计性能基线，为保护监测系统的实际部署提供了关于模型选择和深度提取策略的有效指导。

Abstract: Camera traps are widely used for wildlife monitoring, but extracting accurate
distance measurements from monocular images remains challenging due to the lack
of depth information. While monocular depth estimation (MDE) methods have
advanced significantly, their performance in natural wildlife environments has
not been systematically evaluated. This work introduces the first benchmark for
monocular metric depth estimation in wildlife monitoring conditions. We
evaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,
ZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images
with ground truth distances obtained using calibrated ChARUCO patterns. Our
results demonstrate that Depth Anything V2 achieves the best overall
performance with a mean absolute error of 0.454m and correlation of 0.962,
while methods like ZoeDepth show significant degradation in outdoor natural
environments (MAE: 3.087m). We find that median-based depth extraction
consistently outperforms mean-based approaches across all deep learning
methods. Additionally, we analyze computational efficiency, with ZoeDepth being
fastest (0.17s per image) but least accurate, while Depth Anything V2 provides
an optimal balance of accuracy and speed (0.22s per image). This benchmark
establishes performance baselines for wildlife applications and provides
practical guidance for implementing depth estimation in conservation monitoring
systems.

</details>


### [135] [ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts](https://arxiv.org/abs/2510.04739)
*Mehdi Houshmand Sarkhoosh,Frøy Øye,Henrik Nestor Sørlie,Nam Hoang Vu,Dag Johansen,Cise Midoglu,Tomas Kupka,Pål Halvorsen*

Main category: cs.CV

TL;DR: 本文提出了一种名为ExposureEngine的端到端系统，用于在体育转播中实现精确、支持旋转检测的赞助商可见性分析，采用定向边界框（OBB）提升检测精度，并结合语言驱动代理层支持自然语言查询，实现了可审计且可解释的赞助商曝光度量化。


<details>
  <summary>Details</summary>
Motivation: 传统体育转播中赞助商可见性分析依赖人工、主观且不可扩展的方法，现有自动化系统因使用轴对齐边界框（HBB）在处理旋转或倾斜标志时精度不足，因此需要一种更准确、鲁棒的自动化解决方案。

Method: 提出ExposureEngine系统，采用定向边界框（OBB）进行标志检测以适应任意方向，构建包含1,103帧和670个唯一赞助商标志的新数据集（带OBB标注），训练深度学习检测模型，并集成分析管道计算曝光时长和屏幕覆盖率等指标，同时引入语言驱动代理层支持自然语言生成报告与内容。

Result: 模型在mAP@0.5指标上达到0.859，精确率为0.96，召回率为0.87，在多种广播条件下均表现出色；系统成功实现了精准的赞助商曝光度测量，并支持通过自然语言交互生成分析结果。

Conclusion: ExposureEngine通过OBB检测和自然语言交互显著提升了体育转播中赞助商可见性分析的准确性与可用性，为媒体赞助评估提供了可审计、可解释的综合解决方案。

Abstract: Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

</details>


### [136] [Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection](https://arxiv.org/abs/2510.04741)
*Alina Ciocarlan,Sylvie Le Hégarat-Mascle,Sidonie Lefebvre*

Main category: cs.CV

TL;DR: 提出了一种名为Anomaly-Aware YOLO（AA-YOLO）的红外小目标检测方法，通过在检测头中引入统计异常检测机制，将小目标视为背景中的异常模式，有效控制误报率。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测器在复杂背景和微小目标情况下容易产生大量误报，难以满足红外小目标检测的需求。

Method: 将统计异常检测测试集成到YOLO的检测头中，利用小目标作为背景中异常模式的特性，提升检测性能。仅修改检测头，保持了对多种YOLO主干网络的兼容性。

Result: 在多个红外小目标检测基准上表现出竞争性性能，具备在训练数据少、噪声干扰和域偏移场景下的强鲁棒性，并可扩展至实例分割任务。

Conclusion: AA-YOLO是一种通用且高效的红外小目标检测框架，适用于资源受限的实际应用场景。

Abstract: Infrared Small Target Detection (IRSTD) is a challenging task in defense
applications, where complex backgrounds and tiny target sizes often result in
numerous false alarms using conventional object detectors. To overcome this
limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a
statistical anomaly detection test into its detection head. By treating small
targets as unexpected patterns against the background, AA-YOLO effectively
controls the false alarm rate. Our approach not only achieves competitive
performance on several IRSTD benchmarks, but also demonstrates remarkable
robustness in scenarios with limited training data, noise, and domain shifts.
Furthermore, since only the detection head is modified, our design is highly
generic and has been successfully applied across various YOLO backbones,
including lightweight models. It also provides promising results when
integrated into an instance segmentation YOLO. This versatility makes AA-YOLO
an attractive solution for real-world deployments where resources are
constrained. The code will be publicly released.

</details>


### [137] [Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics](https://arxiv.org/abs/2510.04753)
*Masoumeh Chapariniya,Teodora Vukovic,Sarah Ebling,Volker Dellwo*

Main category: cs.CV

TL;DR: 本研究提出了一种双流Transformer框架，用于基于COCO WholeBody关键点的自然对话中人物识别，发现空间配置比时间动态更具判别性，融合后准确率高达98.03%。


<details>
  <summary>Details</summary>
Motivation: 在自然面对面交流中，有效识别人物具有挑战性，现有方法在姿态与运动建模上存在不足，需探索更强大的序列建模架构。

Method: 采用双流框架分别建模133个COCO WholeBody关键点的空间结构和时间运动模式，使用预训练与从零训练策略，引入多尺度时间Transformer并融合速度特征进行层次化运动建模。

Result: 空间Transformer准确率达95.74%，多尺度时间Transformer为93.90%，特征级融合提升至98.03%；领域特定训练优于迁移学习，空间信息比时间动态更具判别力。

Conclusion: Transformer架构能有效捕捉自然交互中的身份特征，空间构型是主要判别依据，结合动态信息可进一步提升性能，为多模态与跨文化研究提供新思路。

Abstract: This paper investigates the performance of transformer-based architectures
for person identification in natural, face-to-face conversation scenario. We
implement and evaluate a two-stream framework that separately models spatial
configurations and temporal motion patterns of 133 COCO WholeBody keypoints,
extracted from a subset of the CANDOR conversational corpus. Our experiments
compare pre-trained and from-scratch training, investigate the use of velocity
features, and introduce a multi-scale temporal transformer for hierarchical
motion modeling. Results demonstrate that domain-specific training
significantly outperforms transfer learning, and that spatial configurations
carry more discriminative information than temporal dynamics. The spatial
transformer achieves 95.74% accuracy, while the multi-scale temporal
transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,
confirming that postural and dynamic information are complementary. These
findings highlight the potential of transformer architectures for person
identification in natural interactions and provide insights for future
multimodal and cross-cultural studies.

</details>


### [138] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: 本文提出PG-Occ，一种基于渐进式高斯变换器的开放词汇3D占据预测框架，通过渐进在线稠密化和各向异性感知采样策略，在精度和效率之间取得平衡，显著提升了开放词汇场景理解性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法受限于固定语义类别，现有文本对齐方法在稀疏与稠密表示之间存在小物体捕捉能力与计算开销的权衡问题。

Method: 提出PG-Occ框架，采用渐进在线稠密化策略逐步增强3D高斯表示，并设计各向异性感知的时空融合采样策略，自适应分配不同尺度和阶段高斯单元的感受野，实现更有效的特征聚合。

Result: 在开放词汇3D占据预测任务上达到SOTA性能，相比先前最佳方法mIoU相对提升14.3%。

Conclusion: PG-Occ有效解决了文本对齐3D场景建模中细节捕捉与计算效率的矛盾，为视觉自动驾驶系统提供了更强的开放词汇场景理解能力。

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [139] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: 提出一种通过生成未见类数据来估计开放环境中数据分布的开词汇学习新方法，有效降低估计误差，在11个数据集上优于基线方法达14%。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅使用已见类数据估计开放环境中的数据分布，由于未见类缺失导致估计误差无法识别，难以准确建模真实分布。

Method: 提出一个类别-领域感知的数据生成流程和分布对齐算法：基于层次语义树和从已见类推断的领域信息生成未见类数据，并利用生成的数据进行 posterior 概率估计与最大化以提升泛化能力。

Result: 理论证明生成未见类数据可上界化分布估计误差；实验在11个数据集上验证了方法的有效性，性能超越基线最多达14%。

Conclusion: 通过生成未见类数据进行分布估计是开词汇学习中有效且可行的途径，显著提升模型在开放环境下的泛化性能。

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [140] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: FedSurg挑战旨在评估联邦学习在手术视频分类中的泛化能力和本地微调效果，使用Appendix300多中心数据集，比较了多种方法在未见临床中心的性能表现。


<details>
  <summary>Details</summary>
Motivation: 评估当前联邦学习方法在不共享患者数据的情况下，对手术视频中炎症分级的跨中心泛化能力及本地适应性。

Method: 基于Appendix300初步版本数据集，采用ViViT、度量学习、FedAvg等联邦聚合策略，评估F1分数和预期成本，并通过自举法和统计检验分析排名稳定性。

Result: 在跨中心泛化任务中整体性能有限；微调后所有团队性能提升但排名不稳定；ViViT模型表现最佳；发现类不平衡、超参数调优困难等问题，时空建模和上下文感知预处理具潜力。

Conclusion: FedSurg挑战建立了手术视频分类中联邦学习的首个基准，揭示了局部个性化与全局鲁棒性之间的权衡，强调模型架构、预处理和损失设计的重要性，为未来临床手术AI中的联邦学习发展提供了参考。

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [141] [Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization](https://arxiv.org/abs/2510.04781)
*Javed Ahmad,Federico Dassiè,Selene Frascella,Gabriele Marchello,Ferdinando Cannella,Arianna Traviglia*

Main category: cs.CV

TL;DR: 提出了一种自动化双机器人3D扫描系统，用于高保真文化遗产数字化，通过协调运动规划实现全面覆盖和高效重建。


<details>
  <summary>Details</summary>
Motivation: 传统3D扫描方法依赖人工干预和专业知识，难以高效、准确地完成文化遗产物件的全面扫描。

Method: 采用两个协作机器人：一个配备扫描仪，另一个控制承载物体的托盘；将扫描空间参数化为不同区域，进行优化轨迹规划和路径点分布，以减少遮挡并提高覆盖率。

Result: 实验结果显示该方法在Chamfer Distance和F-score上优于基线方法，具有更高的几何精度和数字化效率。

Conclusion: 该自动化双机器人系统能有效提升3D扫描的准确性与效率，减少对专家操作员的依赖，适用于文化遗产保护等应用场景。

Abstract: High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.

</details>


### [142] [A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation](https://arxiv.org/abs/2510.04794)
*Alon Kaya,Igal Bilik,Inna Stainvas*

Main category: cs.CV

TL;DR: 本文系统比较了大规模CNN和ViT在小样本和大数据场景下作为几何估计任务骨干网络的性能，发现ViT在大数据和跨域场景下表现更优，而CNN在小数据场景中因归纳偏置和较小容量更具优势。


<details>
  <summary>Details</summary>
Motivation: 探索ViT和大规模CNN作为预训练骨干网络在涉及图像形变的几何估计任务（如2D刚性变换估计和基础矩阵预测）中的有效性，尤其是在低数据环境下的表现。

Method: 在不同数据规模（包括少样本场景）下，对ResNet、EfficientNet、CLIP-ResNet等CNN与CLIP-ViT、DINO等ViT模型进行实证比较，评估其在两类几何估计任务上的微调性能。

Result: 在大数据场景中ViT优于CNN；但在小数据场景中，CNN凭借更强的归纳偏置和适中容量表现相当甚至更好；此外，ViT在跨域评估中展现出更强的泛化能力。

Conclusion: 应根据数据规模谨慎选择骨干架构，未来可探索平衡局部与全局特征的混合架构以提升几何估计任务性能。

Abstract: Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

</details>


### [143] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 本文提出了一种基于Diffusion Transformer（DiT）的新型虚拟试穿框架DiT-VTON，通过多种配置探索和大规模多样化数据训练，实现了在细节保留、鲁棒性和跨类别泛化方面的显著提升，并扩展为支持多种产品类型和图像编辑功能的虚拟试用（VTA）系统。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿（VTO）模型在细粒度细节保持、对真实场景图像的鲁棒性、采样效率、图像编辑能力和跨商品类别的泛化方面存在不足，亟需更强大且通用的解决方案。

Method: 采用Diffusion Transformer（DiT）架构，探索了上下文token拼接、通道拼接和ControlNet集成等多种图像条件输入方式；在包含多样背景、非结构化参考和非服装类别的扩展数据集上进行训练，以增强模型鲁棒性和泛化能力。

Result: 在VITON-HD数据集上超越了当前最先进方法，实现了更好的细节保留和鲁棒性，且无需额外的条件编码器；在涵盖数千个商品类别的多样化数据集上，也优于具备虚拟试用（VTA）和图像编辑能力的其他模型。

Conclusion: DiT-VTON不仅提升了虚拟试穿的性能，还将其扩展为多功能的虚拟试用（VTA）平台，展示了Diffusion Transformer在图像条件生成任务中的巨大潜力，为未来电商中个性化可视化提供了高效、灵活的解决方案。

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [144] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文提出了EgoSurg，首个从手术室固定摄像头视频中重建动态、以自我为中心视角的框架，无需干预临床流程。


<details>
  <summary>Details</summary>
Motivation: 传统手术观察依赖固定视角或回忆，无法记录指导临床决策的个体化视觉视角，限制了对手术安全、培训和流程优化的理解。

Method: EgoSurg结合几何驱动的神经渲染与基于扩散的视角增强技术，直接从壁挂式固定摄像头视频中合成任意时刻的高保真第一人称视角。

Result: 在多中心手术案例和控制研究中，EgoSurg能够高质量地重建个体视觉场和任意视角。

Conclusion: EgoSurg将现有手术室摄像基础设施转化为可导航的动态3D记录，为沉浸式外科数据科学提供了新基础，使手术实践可以从各个角度进行可视化、体验和分析。

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [145] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: 本文研究了多模态语言模型（MLMs）在感知任务中表现不佳的原因，发现视觉信息在语言模型中的处理存在信息丢失和干扰，并提出通过控制视觉信息流来提升感知能力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究分析了视觉Transformer的编码器和激活机制，但尚不清楚为何多模态语言模型在感知密集型任务上表现较差。本文旨在从视觉键值令牌的处理角度揭示这一问题的根源。

Method: 分析主流MLM（如LLaVA、Qwen2.5-VL等）中视觉键值令牌的信息流动，评估图像值令牌在零样本感知任务（如分割、语义对应、时序对应、指代表达检测）中的表现，并比较其与未经过MLM微调的视觉编码器（SigLIP）的能力差异，同时探索文本前缀对视觉表示的影响。

Result: 发现图像值令牌本身足以支持多种零样本感知任务；语言模型虽能增强视觉信息，但整体仍比原始视觉编码器保留更少的视觉信息；后期层中的输入无关图像键令牌引入干扰，降低感知能力；添加文本前缀可改善视觉表示；约33.3%的BLINK艺术风格问题中，模型未能输出已存在的感知信息。

Conclusion: 多模态语言模型的感知瓶颈部分源于视觉信息在语言模型中的退化和不当控制，改进视觉信息的调控机制可显著提升其感知性能，为MLM的训练和架构设计提供了新方向。

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [146] [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](https://arxiv.org/abs/2510.04822)
*Zicheng Jiang,Jixin Gao,Shengfeng He,Xinzhe Li,Yulong Zheng,Zhaotong Yang,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: AvatarVTON是首个4D虚拟试穿框架，能从单张商品图像生成逼真的试穿效果，支持自由姿态控制、新视角渲染和多样化服装选择。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法依赖多视角图像或物理先验，难以实现动态交互与真实感。本文旨在提出一种仅需单视角监督即可实现高质量、动态且真实的虚拟试穿方法。

Method: 提出AvatarVTON框架，包含两个关键模块：(1) 无需先验的光流校正策略——双向光流修正器，确保时间连贯性和虚拟人拟合稳定性；(2) 非线性变形器，将高斯图分解为视角-姿态不变与相关成分，实现自适应非线性服装形变。同时构建统一基准进行公平比较。

Result: 实验表明，AvatarVTON在保真度、多样性及动态服装真实感方面优于现有方法，支持自由姿态控制和新视角渲染，适用于AR/VR、游戏和数字人应用。

Conclusion: AvatarVTON实现了基于单视角监督的高质量4D虚拟试穿，在动态交互、视觉真实感和灵活性方面取得突破，为未来虚拟试衣系统提供了有效解决方案。

Abstract: We propose AvatarVTON, the first 4D virtual try-on framework that generates
realistic try-on results from a single in-shop garment image, enabling free
pose control, novel-view rendering, and diverse garment choices. Unlike
existing methods, AvatarVTON supports dynamic garment interactions under
single-view supervision, without relying on multi-view garment captures or
physics priors. The framework consists of two key modules: (1) a Reciprocal
Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes
avatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,
which decomposes Gaussian maps into view-pose-invariant and view-pose-specific
components, enabling adaptive, non-linear garment deformations. To establish a
benchmark for 4D virtual try-on, we extend existing baselines with unified
modules for fair qualitative and quantitative comparisons. Extensive
experiments show that AvatarVTON achieves high fidelity, diversity, and dynamic
garment realism, making it well-suited for AR/VR, gaming, and digital-human
applications.

</details>


### [147] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D流匹配（FM）框架的MRI或CBCT生成合成CT（sCT）的方法，用于支持无MRI和基于CBCT的自适应放疗，减少患者辐射暴露。


<details>
  <summary>Details</summary>
Motivation: 在MRI-only和CBCT-based自适应放疗中，需要从MRI或CBCT生成高质量的sCT图像以提高治疗精度并减少辐射暴露，但现有方法在图像质量和细节保持方面存在局限。

Method: 采用全3D流匹配（FM）框架，通过学习速度场将高斯噪声体积分步转化为sCT图像，并以轻量级3D编码器提取的输入MRI或CBCT特征为条件进行引导。模型在SynthRAD2025挑战赛数据集上分别针对MRI→sCT和CBCT→sCT任务及三个解剖区域（腹部、头颈部、胸部）独立训练与评估。

Result: 该方法能准确重建全局解剖结构，在挑战赛评估系统中表现良好，但由于训练分辨率受限，细部结构保留能力有限。

Conclusion: 所提出的3D FM框架在生成sCT任务中具有潜力，未来将探索基于patch的训练和潜在空间流模型以提升分辨率和局部结构保真度。

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in
enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment
precision while reducing patient radiation exposure. To address this task, we
adopt a fully 3D Flow Matching (FM) framework, motivated by recent work
demonstrating FM's efficiency in producing high-quality images. In our
approach, a Gaussian noise volume is transformed into an sCT image by
integrating a learned FM velocity field, conditioned on features extracted from
the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method
on the SynthRAD2025 Challenge benchmark, training separate models for MRI
$\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions:
abdomen, head and neck, and thorax. Validation and testing were performed
through the challenge submission system. The results indicate that the method
accurately reconstructs global anatomical structures; however, preservation of
fine details was limited, primarily due to the relatively low training
resolution imposed by memory and runtime constraints. Future work will explore
patch-based training and latent-space flow models to improve resolution and
local structural fidelity.

</details>


### [148] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: 提出AT-BPTT框架，通过动态调整截断位置和窗口大小，显著提升数据集蒸馏的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法依赖随机截断策略，缺乏灵活性且效果不佳；而神经网络在不同训练阶段表现出不同的学习动态，需更智能的截断机制。

Method: 提出AT-BPTT，包含三个关键组件：基于概率的阶段感知时间步选择、基于梯度变化的自适应窗口大小策略、低秩Hessian近似以降低计算开销。

Result: 在CIFAR-10、CIFAR-100、Tiny-ImageNet和ImageNet-1K上实验表明，相比基线方法平均提升6.16%准确率，内循环优化加速3.9倍，并节省63%内存开销。

Conclusion: AT-BPTT通过动态适应神经网络学习动态，实现了高效且高性能的数据集蒸馏，为大规模深度学习提供了实用解决方案。

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [149] [Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints](https://arxiv.org/abs/2510.04840)
*Viktor Kozák,Jan Chudoba,Libor Přeučil*

Main category: cs.CV

TL;DR: 提出了一种基于航拍图像的光伏电站自动建模方法，通过视觉分割和结构推断实现对光伏组件的精细映射，并融合多视角图像生成带地理参考的三维语义模型。


<details>
  <summary>Details</summary>
Motivation: 光伏电站需要精确且最新的模型以优化运行与维护，但传统依赖第三方数据的方法难以获得且更新不便。

Method: 利用航拍图像进行光伏组件的视觉分割，识别布局关键点，推断阵列结构（如排、列、支架），并通过多图像融合保持结构完整性，结合3D定位生成地理参考模型。

Result: 在两个不同光伏电站上验证了该方法，实现了组件级的精确映射，自动生成紧凑的、带地理信息的三维语义模型。

Conclusion: 该方法可自动化光伏电站建模，减少对第三方数据的依赖，提升建模效率与精度，适用于运维支持。

Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is
essential for its optimal operation and maintenance. However, such a model may
not be easily available. This work introduces a novel approach for PV power
plant mapping based on aerial overview images. It enables the automation of the
mapping process while removing the reliance on third-party data. The presented
mapping method takes advantage of the structural layout of the power plants to
achieve detailed modeling down to the level of individual PV modules. The
approach relies on visual segmentation of PV modules in overview images and the
inference of structural information in each image, assigning modules to
individual benches, rows, and columns. We identify visual keypoints related to
the layout and use these to merge detections from multiple images while
maintaining their structural integrity. The presented method was experimentally
verified and evaluated on two different power plants. The final fusion of 3D
positions and semantic structures results in a compact georeferenced model
suitable for power plant maintenance.

</details>


### [150] [From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements](https://arxiv.org/abs/2510.04844)
*Cheyu Lin,Katherine A. Flanigan*

Main category: cs.CV

TL;DR: 提出一种基于3D骨架数据的kinesics识别框架，结合ST-GCN和CNN，利用迁移学习推断人类行为的心理状态，保护隐私且可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以泛化且无法动态捕捉人类心理状态，缺乏兼顾隐私与普适性的建模手段。

Method: 结合时空图卷积网络（ST-GCN）与卷积神经网络（CNN），从3D骨骼数据中提取kinesics特征，并采用迁移学习避免手动定义动作与心理类别的映射。

Result: 在DUET数据集上实现了对人类行为的准确建模，能有效识别反映认知与情绪状态的身体动作模式。

Conclusion: 该框架支持可扩展、精准且以人为本的行为建模，为强化学习中的人-环境交互模拟提供了新途径。

Abstract: Understanding the dynamic relationship between humans and the built
environment is a key challenge in disciplines ranging from environmental
psychology to reinforcement learning (RL). A central obstacle in modeling these
interactions is the inability to capture human psychological states in a way
that is both generalizable and privacy preserving. Traditional methods rely on
theoretical models or questionnaires, which are limited in scope, static, and
labor intensive. We present a kinesics recognition framework that infers the
communicative functions of human activity -- known as kinesics -- directly from
3D skeleton joint data. Combining a spatial-temporal graph convolutional
network (ST-GCN) with a convolutional neural network (CNN), the framework
leverages transfer learning to bypass the need for manually defined mappings
between physical actions and psychological categories. The approach preserves
user anonymity while uncovering latent structures in bodily movements that
reflect cognitive and emotional states. Our results on the Dyadic User
EngagemenT (DUET) dataset demonstrate that this method enables scalable,
accurate, and human-centered modeling of behavior, offering a new pathway for
enhancing RL-driven simulations of human-environment interaction.

</details>


### [151] [Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems](https://arxiv.org/abs/2510.04854)
*Cheyu Lin,John Martins,Katherine A. Flanigan,Ph. D*

Main category: cs.CV

TL;DR: 本文探讨了如何通过识别二元人类交互来增强网络物理社会基础设施系统（CPSIS）中的社会效益，使用深度传感器获取骨骼数据以保护隐私，并比较了五种基于骨骼的交互识别算法在12类双人交互上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的网络物理系统（CPS）多关注经济和安全目标，忽视了人类中心的社会效益。本文旨在通过理解人与人之间及人与基础设施之间的交互，推动CPS向更注重社会目标的CPSIS发展。

Method: 利用深度传感器采集真实世界中的骨骼运动数据，构建包含12种二元交互的数据集，并对五种基于骨骼的动作识别算法进行比较评估，这些交互涵盖象征性动作和情感表达等沟通类型。

Result: 研究表明基于骨骼的深度传感方法可在保护隐私的前提下有效识别多种社交互动类型，不同算法在识别文化与情感相关的交互行为上表现出差异性，为后续建模社会行为提供了基础。

Conclusion: 将骨架动作识别技术应用于CPSIS中，有助于量化和促进社会性收益，是实现以人为本的智能基础设施的重要一步。

Abstract: Cyber-physical systems (CPS) integrate sensing, computing, and control to
improve infrastructure performance, focusing on economic goals like performance
and safety. However, they often neglect potential human-centered (or
''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim
to address this by aligning CPS with social objectives. This involves defining
social benefits, understanding human interactions with each other and
infrastructure, developing privacy-preserving measurement methods, modeling
these interactions for prediction, linking them to social benefits, and
actuating the physical environment to foster positive social outcomes. This
paper delves into recognizing dyadic human interactions using real-world data,
which is the backbone to measuring social behavior. This lays a foundation to
address the need to enhance understanding of the deeper meanings and mutual
responses inherent in human interactions. While RGB cameras are informative for
interaction recognition, privacy concerns arise. Depth sensors offer a
privacy-conscious alternative by analyzing skeletal movements. This study
compares five skeleton-based interaction recognition algorithms on a dataset of
12 dyadic interactions. Unlike single-person datasets, these interactions,
categorized into communication types like emblems and affect displays, offer
insights into the cultural and emotional aspects of human interactions.

</details>


### [152] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: 提出了一种结合早期退出和知识蒸馏的新型训练方法，通过引入基于熵的损失函数优化准确率与效率的权衡，在保持分类性能的同时显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然在图像分类中表现优异，但计算成本高，难以应用于实时和边缘场景，因此需要有效的模型压缩技术。

Method: 将早期退出机制与知识蒸馏相结合，训练一个简化的学生早期退出模型，从复杂的教师早期退出模型中学习，并引入一种新的基于熵的损失函数，用于处理教师分类错误的样本。

Result: 在CIFAR10、CIFAR100和SVHN数据集上的实验表明，该方法显著降低了计算复杂度，同时保持了良好的分类性能。

Conclusion: 所提出的方法有效优化了模型的效率与准确性平衡，为知识蒸馏在其他场景中的应用提供了新的研究方向。

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [153] [μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy](https://arxiv.org/abs/2510.04859)
*Elena Corbetta,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的光学显微图像质量评估方法μDeepIQA，通过迁移学习将用于自然图像的卷积神经网络重新训练，以快速、稳定地预测显微图像的整体和局部质量得分。


<details>
  <summary>Details</summary>
Motivation: 传统图像质量评估方法在处理大规模数据时计算成本高，且对非理想域图像表现不稳定，因此需要一种更高效、泛化能力更强的方法用于光学显微图像质量评估。

Method: 采用预训练的深度卷积神经网络架构，针对光学显微图像重新训练以预测多种质量指标和全局质量评分，并实现图像块级别的质量预测与空间质量可视化。

Result: μDeepIQA能够快速稳定地预测图像质量，具有良好的泛化能力，尤其在存在异常值或非理想条件下优于传统方法，并支持单幅图像内空间质量分布的可视化。

Conclusion: 深度学习模型在光学显微图像质量评估中表现出优越的稳定性、速度和泛化能力，μDeepIQA为显微图像分析提供了一种高效的自动化质量评估工具。

Abstract: Optical microscopy is one of the most widely used techniques in research
studies for life sciences and biomedicine. These applications require reliable
experimental pipelines to extract valuable knowledge from the measured samples
and must be supported by image quality assessment (IQA) to ensure correct
processing and analysis of the image data. IQA methods are implemented with
variable complexity. However, while most quality metrics have a straightforward
implementation, they might be time consuming and computationally expensive when
evaluating a large dataset. In addition, quality metrics are often designed for
well-defined image features and may be unstable for images out of the ideal
domain.
  To overcome these limitations, recent works have proposed deep learning-based
IQA methods, which can provide superior performance, increased generalizability
and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by
previous studies and applies a deep convolutional neural network designed for
IQA on natural images to optical microscopy measurements. We retrained the same
architecture to predict individual quality metrics and global quality scores
for optical microscopy data. The resulting models provide fast and stable
predictions of image quality by generalizing quality estimation even outside
the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA
provides patch-wise prediction of image quality and can be used to visualize
spatially varying quality in a single image. Our study demonstrates that
optical microscopy-based studies can benefit from the generalizability of deep
learning models due to their stable performance in the presence of outliers,
the ability to assess small image patches, and rapid predictions.

</details>


### [154] [In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning](https://arxiv.org/abs/2510.04864)
*Ciem Cornelissen,Sander De Coninck,Axel Willekens,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 本文提出了一种端到端、基于物联网的机器人系统，用于葡萄园中葡萄产量和品质（糖度、酸度）的无损、实时、空间分辨映射。系统结合了高精度果串检测与重量估计模型，以及基于高光谱数据的新型深度学习质量评估框架，并引入光照不变性光谱自编码器（LISA）克服野外高光谱成像中的光照变化问题。实验验证了系统在多光照条件下的鲁棒性，显著提升了品质预测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 野外高光谱成像面临由光照变化引起的“域偏移”问题，限制了葡萄品质的准确评估，亟需一种能在真实环境中稳定工作的解决方案。

Method: 系统集成两个核心模块：基于深度学习的葡萄果串检测与重量估计模型，以及利用高光谱数据进行品质分析的框架；其中品质评估采用名为LISA的域对抗式自编码器，从未经校准的数据中学习光照不变特征，以提升跨光照条件的泛化性能。

Result: 完整系统实现了82%的果串检测召回率和0.76的重量预测R²值；LISA模块相较基线方法将品质预测的跨域泛化性能提升超过20%。

Conclusion: 该系统能够生成高分辨率、地理配准的葡萄产量与品质数据，为精准葡萄栽培提供了可行的端到端解决方案，尤其在应对复杂光照变化方面表现出色。

Abstract: This paper presents an end-to-end, IoT-enabled robotic system for the
non-destructive, real-time, and spatially-resolved mapping of grape yield and
quality (Brix, Acidity) in vineyards. The system features a comprehensive
analytical pipeline that integrates two key modules: a high-performance model
for grape bunch detection and weight estimation, and a novel deep learning
framework for quality assessment from hyperspectral (HSI) data. A critical
barrier to in-field HSI is the ``domain shift" caused by variable illumination.
To overcome this, our quality assessment is powered by the Light-Invariant
Spectral Autoencoder (LISA), a domain-adversarial framework that learns
illumination-invariant features from uncalibrated data. We validated the
system's robustness on a purpose-built HSI dataset spanning three distinct
illumination domains: controlled artificial lighting (lab), and variable
natural sunlight captured in the morning and afternoon. Results show the
complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$
(0.76) for weight prediction, while the LISA module improves quality prediction
generalization by over 20% compared to the baselines. By combining these robust
modules, the system successfully generates high-resolution, georeferenced data
of both grape yield and quality, providing actionable, data-driven insights for
precision viticulture.

</details>


### [155] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: 本文介绍了一个大规模多模态海底栖息地映射数据集，包含约一百万张侧扫声呐图像、测深图和自主水下航行器采集的共注册光学图像，并提供了约3.6万张标注图像，旨在推动水下栖息地分类与多传感器融合的机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模标注数据集，限制了机器学习模型在海底栖息地映射领域的发展与评估。

Method: 收集了沿西班牙加泰罗尼亚海岸的侧扫声呐（SSS）图像、测深图和AUV获取的光学图像，对约3.6万张SSS图像进行人工分割标注，并实现多源数据空间配准，支持监督学习与自监督跨模态学习。

Result: 发布了一个包含原始传感器数据、拼接图像和标注的多模态数据集，配套开源预处理与标注工具，为水下栖息地分类提供了可公开访问的基准资源。

Conclusion: 该数据集为海底栖息地映射建立了标准化基准，有助于推动自主海床分类和多传感器融合算法的发展。

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [156] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: 本研究比较了四种目标检测模型（YOLOv5、Faster R-CNN、SSD 和 RetinaNet）在卢旺达基加利收集的自定义数据集上对摩托车的检测性能，旨在评估其在资源受限环境中实时导航的适用性。


<details>
  <summary>Details</summary>
Motivation: 基加利的摩托车出租车交通行为不可预测且常无视交通规则，给自动驾驶系统带来重大挑战，亟需适用于本地条件的目标检测模型。

Method: 使用包含198张图像的自定义数据集，在PyTorch框架下应用迁移学习，比较YOLOv5、Faster R-CNN、SSD和RetinaNet四种模型在准确性、定位能力和推理速度方面的表现。

Result: 评估了各模型在准确率、定位和推理速度方面的性能，识别出数据集规模有限和模型复杂性高等实施挑战。

Conclusion: 建议采用简化模型架构以提升在卢旺达等发展中国家部署自动驾驶系统的可行性与可及性。

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [157] [A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images](https://arxiv.org/abs/2510.04916)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种语义感知的层次共识（SAHC）方法，用于遥感图像分类，通过整合层次特定的分类头和可训练的层次矩阵，在深度网络中学习层次特征与关系，并引入层次共识机制确保跨层次的概率一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽略预定义的类别层次结构，仅关注细粒度分类，未能充分利用语义层级信息。

Method: 设计了SAHC方法，结合多个针对不同粒度层次的分类头，利用可学习的层次矩阵自监督地建模层次结构，并通过加权集成式的层次共识机制统一各层预测分布。

Result: 在三个具有不同层次复杂度的基准数据集上验证了方法的有效性，使用多种骨干网络展示了其适应性，结果表明该方法能有效引导网络学习并提升分类性能。

Conclusion: SAHC能够有效利用类别层次结构，在遥感图像分类任务中表现出良好的鲁棒性和泛化能力。

Abstract: Deep learning has become increasingly important in remote sensing image
classification due to its ability to extract semantic information from complex
data. Classification tasks often include predefined label hierarchies that
represent the semantic relationships among classes. However, these hierarchies
are frequently overlooked, and most approaches focus only on fine-grained
classification schemes. In this paper, we present a novel Semantics-Aware
Hierarchical Consensus (SAHC) method for learning hierarchical features and
relationships by integrating hierarchy-specific classification heads within a
deep network architecture, each specialized in different degrees of class
granularity. The proposed approach employs trainable hierarchy matrices, which
guide the network through the learning of the hierarchical structure in a
self-supervised manner. Furthermore, we introduce a hierarchical consensus
mechanism to ensure consistent probability distributions across different
hierarchical levels. This mechanism acts as a weighted ensemble being able to
effectively leverage the inherent structure of the hierarchical classification
task. The proposed SAHC method is evaluated on three benchmark datasets with
different degrees of hierarchical complexity on different tasks, using distinct
backbone architectures to effectively emphasize its adaptability. Experimental
results show both the effectiveness of the proposed approach in guiding network
learning and the robustness of the hierarchical consensus for remote sensing
image classification tasks.

</details>


### [158] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出了一种新的基于解剖学信息的混合专家模型（REN），用于间质性肺病分类，通过结合放射组学和深度学习特征，在区域特异性病理建模上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统MoE缺乏医学影像所需的解剖结构约束，难以有效捕捉区域疾病异质性，限制了在医疗场景中的应用。

Method: 设计了七个针对不同肺叶及双侧肺组合的专用专家网络，并利用多模态门控机制动态整合放射组学生物标志物与深度学习特征（CNN、ViT、Mamba）来加权专家输出。

Result: 在ILD分类任务中，REN平均AUC达0.8646，比SwinUNETR基线提升12.5%（p=0.031）；下叶区域专家AUC达0.88–0.90，优于传统DL模型且符合临床疾病进展规律。

Conclusion: REN具有良好的泛化能力和临床可解释性，是一种可扩展、适用于结构化医学影像分析的新框架。

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [159] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的无监督主动学习方法NFPF，通过特定特征学习机器（SFLM）和重构差异度量来更有效地选择重要样本，在性能上超越了现有方法，并与有监督的主动学习方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督主动学习方法依赖局部梯度评分和浅层线性选择，难以准确估计样本重要性，且对噪声数据敏感，无法充分覆盖数据分布。

Method: 提出自然特征渐进框架（NFPF），采用特定特征学习机器（SFLM）量化样本对模型性能的贡献，并利用SFLM定义重构差异度量进行初始样本选择，实现更优的数据代表性与鲁棒性。

Result: 实验表明，NFPF在多个视觉数据集上显著优于现有无监督主动学习方法，性能媲美有监督主动学习方法，且具备更强的鲁棒性和更好的数据分布覆盖能力。

Conclusion: NFPF为无监督主动学习提供了一个更有效、更稳健的框架，推动了少标注条件下的高效深度学习发展。

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [160] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于条件扩散模型的双视图乳腺X线图像转换框架CA3D-Diff，通过列感知交叉注意力和隐式3D结构重建模块解决跨视图结构错位问题，在双向任务中表现出优越的视觉保真度和结构一致性，并提升了单视图恶性肿瘤分类性能。


<details>
  <summary>Details</summary>
Motivation: 在实际临床工作中，由于采集错误或压缩伪影，双视图乳腺摄影中的某一视图可能缺失、损坏或质量下降，限制了后续分析的有效性。需要一种方法来恢复缺失视图并改善病灶对齐。

Method: 提出CA3D-Diff框架：1）设计列感知交叉注意力机制，利用解剖对应区域在不同视图中常位于相似列位置的几何特性，并引入高斯衰减偏置以增强局部列相关性；2）引入隐式3D结构重建模块，基于乳房投影几何将噪声2D潜在表示反投影为粗略3D特征体，重构并注入去噪UNet以增强解剖感知。

Result: 实验表明CA3D-Diff在双向视图转换任务中优于现有最先进方法，具有更高的视觉质量和结构一致性；合成视图能有效提升筛查场景下单视图恶性肿瘤分类性能。

Conclusion: CA3D-Diff能够准确生成解剖结构一致的互补视图，具有较强的临床实用性，可用于弥补乳腺摄影中缺失或低质量图像，提升诊断准确性。

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [161] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: 本文提出了一种新的单步扩散解码器SSDD，无需对抗损失即可实现比KL-VAE更高的重建质量和更快的采样速度，可作为其即插即用替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于KL-VAE的tokenizer在生成图像模型中存在训练依赖对抗损失、解码速度慢等问题，而扩散解码器虽更优但同样受限于性能和效率，因此需要一种高效且无需对抗训练的扩散解码器。

Method: 设计了一种新的像素扩散解码器架构，结合Transformer组件并采用无GAN的训练方式，通过知识蒸馏将迭代扩散模型的能力迁移到单步解码器中。

Result: SSDD在无对抗损失的情况下，将重建FID从0.87提升至0.50，吞吐量提高1.4倍，并在生成质量不变的前提下实现3.8倍更快采样。

Conclusion: SSDD是首个无需对抗损失、专为单步重建优化的扩散解码器，兼具高性能与高效率，可作为KL-VAE的直接替代，推动更高质量和快速生成模型的发展。

Abstract: Tokenizers are a key component of state-of-the-art generative image models,
extracting the most important features from the signal while reducing data
dimension and redundancy. Most current tokenizers are based on KL-regularized
variational autoencoders (KL-VAE), trained with reconstruction, perceptual and
adversarial losses. Diffusion decoders have been proposed as a more principled
alternative to model the distribution over images conditioned on the latent.
However, matching the performance of KL-VAE still requires adversarial losses,
as well as a higher decoding time due to iterative sampling. To address these
limitations, we introduce a new pixel diffusion decoder architecture for
improved scaling and training stability, benefiting from transformer components
and GAN-free training. We use distillation to replicate the performance of the
diffusion decoder in an efficient single-step decoder. This makes SSDD the
first diffusion decoder optimized for single-step reconstruction trained
without adversarial losses, reaching higher reconstruction quality and faster
sampling than KL-VAE. In particular, SSDD improves reconstruction FID from
$0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation
quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as
a drop-in replacement for KL-VAE, and for building higher-quality and faster
generative models.

</details>


### [162] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: 提出一种用于视觉基础模型（VFM）所有权验证的方法，通过微调少量表达层和编码器-解码器网络，在输入图像的内部表示中嵌入数字水印，并证明该水印在下游任务微调后仍可检测。


<details>
  <summary>Details</summary>
Motivation: 保护视觉基础模型开发者的知识产权，防止模型被非法再分发，亟需可靠的所有权验证工具。

Method: 通过微调VFM的一小组表达层及一个小型编码器-解码器网络，将数字水印嵌入到一组保留输入图像的内部表示中，实现所有权验证。

Result: 理论和实验表明，该方法对非水印模型的误检率低，对水印模型的漏检率也低，且水印在功能副本（如下游任务微调后）中仍可检测。

Conclusion: 所提出的方法能有效验证视觉基础模型的所有权，具有低误检和漏检率，适用于保护模型知识产权。

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [163] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: 提出了一种基于潜在表示的不确定性估计方法（LUR和RLUR），在视频驾驶行为与意图识别任务中实现了与现有概率深度学习方法相当的分类性能和OOD检测效果，且训练更高效、调参更简单。


<details>
  <summary>Details</summary>
Motivation: 现有的最后一层概率深度学习方法在检测分布外样本时性能不稳定，且部分方法训练复杂、难以调参，因此需要一种更高效、稳定的不确定性估计方法。

Method: 在预训练DNN后扩展变换层，生成多个潜在表示以估计不确定性，提出了LUR和RLUR两种方法，并在四个驾驶行为与意图识别数据集上与八种PDL方法进行对比评估。

Result: LUR和RLUR在分布内分类性能上与其他LL-PDL方法相当；在基于不确定性的OOD检测中表现与最优PDL方法相近，但训练效率更高，调参更简便；同时为NuScenes数据集贡献了28,000帧级动作标签和1,194视频级意图标签。

Conclusion: LUR是一种高效、易用的不确定性估计方法，在保持良好分类和OOD检测性能的同时，优于需要MCMC采样或排斥训练等复杂流程的方法，适用于资源受限的安全关键场景。

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


### [164] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: 本研究提出了一种基于机器学习的帕金森病早期检测方法，利用手绘螺旋和波浪图像作为生物标志物，结合卷积神经网络、迁移学习和注意力机制，并通过数据增强和集成投票提升性能。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的传统诊断方法繁琐且昂贵，亟需一种非侵入性、低成本且高效的早期诊断手段。

Method: 采用预训练的卷积神经网络（CNN）、自定义卷积层，并结合数据增强与注意力机制；通过硬投票集成多个模型的预测结果。

Result: 在螺旋图像上，加权平均精确率、召回率和F1分数均为90%；在波浪图像上为96.67%；通过集成硬投票后整体准确率达到93.3%。

Conclusion: 该研究表明基于手绘图像的机器学习方法在帕金森病早期诊断中具有高准确性和应用潜力，提供了一种非侵入、低成本的解决方案。

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


### [165] [Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](https://arxiv.org/abs/2510.05034)
*Yunlong Tang,Jing Bi,Pinxin Liu,Zhenyu Pan,Zhangyun Tan,Qianxiang Shen,Jiani Liu,Hang Hua,Junjia Guo,Yunzhong Xiao,Chao Huang,Zhiyuan Wang,Susan Liang,Xinyi Liu,Yizhi Song,Yuhe Nie,Jia-Xing Zhong,Bozheng Li,Daiqing Qi,Ziyun Zeng,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Daiki Shimada,Han Liu,Jiebo Luo,Chenliang Xu*

Main category: cs.CV

TL;DR: 本文综述了视频大视觉语言模型（Video-LMMs）的后训练方法，提出一个包含监督微调、强化学习和测试时扩展的统一框架，系统分析了现有技术在时空推理、长视频处理和多模态融合中的挑战与进展。


<details>
  <summary>Details</summary>
Motivation: 尽管Video-LMM在视频理解任务中表现出色，但其后训练阶段的方法分散且缺乏系统性总结，亟需统一框架以推动该领域发展。

Method: 提出一个结构化分类体系，涵盖监督微调（SFT）结合思维链、基于可验证目标的强化学习（RL）以及通过增强推理计算的测试时扩展（TTS），并分析各方法的角色、联系及针对视频特性的适配。

Result: 系统梳理了代表性方法的设计原则、关键洞见和评估协议，整理了核心基准数据集与指标，并指出了奖励设计、可扩展性和成本-性能优化等开放问题。

Conclusion: 该综述为研究人员提供了推进Video-LMM后训练能力的统一框架和实用资源，有助于促进复杂视频理解任务的发展。

Abstract: Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

</details>


### [166] [SegMASt3R: Geometry Grounded Segment Matching](https://arxiv.org/abs/2510.05051)
*Rohit Jayanti,Swayam Agrawal,Vansh Garg,Siddharth Tourani,Muhammad Haris Khan,Sourav Garg,Madhava Krishna*

Main category: cs.CV

TL;DR: 本文提出了一种利用3D基础模型的空间理解能力进行宽基线段匹配的新方法，能够在高达180度视角变化下匹配图像片段，在ScanNet++和Replica数据集上性能优于现有方法30%（AUPRC指标），并在3D实例分割和图像目标导航等下游任务中展现出优势。


<details>
  <summary>Details</summary>
Motivation: 段匹配在处理遮挡、光照变化和视角变换方面比关键点匹配更具鲁棒性，但宽基线下的段匹配因极端视角变化而极具挑战。现有方法难以有效应对这一问题，因此需要借助3D基础模型的空间理解能力来提升匹配性能。

Method: 提出一种新架构，利用3D基础模型的归纳偏置，提取并匹配跨图像对的语义或几何一致的区域，实现宽基线条件下的段匹配，支持高达180度的视角变化。

Result: 在ScanNet++和Replica数据集上，该方法在AUPRC指标上比SAM2视频传播器和局部特征匹配等最先进方法高出最多30%，并在3D实例分割和图像目标导航任务中表现出良好性能。

Conclusion: 所提出的方法有效解决了宽基线段匹配的挑战，显著优于现有技术，并展示了其在多种下游视觉任务中的潜力。

Abstract: Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

</details>


### [167] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: 提出一种针对对比度失真图像的无参考图像质量评估方法，通过生成伪参考图像将无参考问题转化为全参考评估，显著提高评估精度。


<details>
  <summary>Details</summary>
Motivation: 对比度变化严重影响图像质量，但现有方法大多忽视了对比度失真的特殊视觉影响，缺乏有效的评估手段。

Method: 利用多种对比度增强算法生成伪参考图像，训练分类网络根据图像内容和失真类型选择最佳增强方法，进而采用全参考方式评估失真图像与伪参考图像之间的质量差异。

Result: 在包含对比度失真的三个数据库（CCID2014、TID2013、CSIQ）上的实验表明，该方法优于现有主流指标，具有更高的评估准确性。

Conclusion: 所提方法有效解决了对比度失真图像的质量评估难题，通过伪参考图像生成和全参考评估框架实现了优越性能。

Abstract: Contrast change is an important factor that affects the quality of images.
During image capturing, unfavorable lighting conditions can cause contrast
change and visual quality loss. While various methods have been proposed to
assess the quality of images under different distortions such as blur and
noise, contrast distortion has been largely overlooked as its visual impact and
properties are different from other conventional types of distortions. In this
paper, we propose a no-reference image quality assessment (NR-IQA) metric for
contrast-distorted images. Using a set of contrast enhancement algorithms, we
aim to generate pseudo-reference images that are visually close to the actual
reference image, such that the NR problem is transformed to a Full-reference
(FR) assessment with higher accuracy. To this end, a large dataset of
contrast-enhanced images is produced to train a classification network that can
select the most suitable contrast enhancement algorithm based on image content
and distortion for pseudo-reference image generation. Finally, the evaluation
is performed in the FR manner to assess the quality difference between the
contrast-enhanced (pseudoreference) and degraded images. Performance evaluation
of the proposed method on three databases containing contrast distortions
(CCID2014, TID2013, and CSIQ), indicates the promising performance of the
proposed method.

</details>


### [168] [Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces](https://arxiv.org/abs/2510.05071)
*Debojyoti Ghosh,Soumya K Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: 提出一种名为Neuroplastic Modular Classifier的混合架构，结合ResNet-50、Vision Transformer和FAISS相似性检索，通过可扩展的模块化设计实现动态自适应图像分类，在垃圾分类和工业缺陷检测任务中表现出优越的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 为应对动态环境中废物分类和工业表面缺陷检测对高效准确分类的需求，提升模型在复杂数据下的泛化能力和适应性。

Method: 采用ResNet-50进行局部特征提取，结合Vision Transformer捕获全局语义信息，并引入FAISS-based相似性检索增强特征空间；核心创新是受生物学习系统启发的神经可塑性模块化设计，可在训练中动态增长可学习模块以应对性能瓶颈。

Result: 在垃圾分类和KolektorSDD2工业缺陷检测数据集上，该模型在准确性和适应性方面均优于传统静态模型。

Conclusion: Neuroplastic Modular Classifier提供了一种可扩展、高性能的现实世界图像分类解决方案，适用于环境与工业双重领域。

Abstract: Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

</details>


### [169] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了首个针对结构化视觉内容（如图表、数学图形）生成与编辑的系统性研究，包括数据集构建、模型训练和评估基准。作者构建了包含130万高质量结构化图像对的大规模数据集，并结合VLM与FLUX.1 Kontext模型，通过轻量级连接器和三阶段训练策略提升多模态理解与推理能力。同时提出StructBench基准和StructScore评估指标，实验表明现有主流模型表现仍不理想，而所提方法在编辑任务中表现优异，推理时引入外部推理器进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型在自然图像上表现良好，但在生成或编辑需要逻辑结构、文本渲染和事实准确性的图表、示意图等结构化视觉内容方面存在明显不足，缺乏专门的数据集、模型和评估标准。

Method: 构建了一个包含130万结构化图像对及思维链标注的大规模数据集；设计了一个融合VLM与FLUX.1 Kontext的统一模型，通过轻量连接器实现多模态理解；采用三阶段训练策略（特征对齐、知识注入、推理增强生成），并在推理时引入外部推理器；提出StructBench评估基准和基于多轮问答的StructScore评分机制。

Result: 在StructBench上的15个模型评测显示当前主流模型表现不佳；所提模型在结构化图像生成与编辑任务中显著优于现有方法，推理时加入外部推理器带来持续性能提升；三阶段训练有效促进了模型能力发展。

Conclusion: 本文推动了面向结构化视觉内容的统一多模态建模范式，通过发布数据集、模型和基准，为未来研究提供了重要资源，验证了增强多模态理解与推理对结构化视觉任务的关键作用。

Abstract: While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.

</details>


### [170] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本到视频生成框架，通过跨角色嵌入（CCE）和跨角色增强（CCA）技术，实现不同风格角色之间的自然交互，同时保持角色身份和风格的一致性。


<details>
  <summary>Details</summary>
Motivation: 研究在文本到视频生成中角色跨语境交互的挑战，特别是角色身份保持、行为一致性和风格混淆问题。

Method: 引入跨角色嵌入（CCE）来学习多模态数据中的角色身份和行为逻辑，并采用跨角色增强（CCA）生成合成共存和混合风格数据以增强训练。

Result: 在包含10个卡通和真人角色的数据集上实验表明，该方法显著提升了身份保持、交互质量和对风格错乱的鲁棒性。

Conclusion: 所提框架有效支持了此前未共存角色之间的自然交互，同时维持了风格保真度，推动了生成式叙事的新可能。

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where
characters interact naturally across different worlds? We study inter-character
interaction in text-to-video generation, where the key challenge is to preserve
each character's identity and behaviors while enabling coherent cross-context
interaction. This is difficult because characters may never have coexisted and
because mixing styles often causes style delusion, where realistic characters
appear cartoonish or vice versa. We introduce a framework that tackles these
issues with Cross-Character Embedding (CCE), which learns identity and
behavioral logic across multimodal sources, and Cross-Character Augmentation
(CCA), which enriches training with synthetic co-existence and mixed-style
data. Together, these techniques allow natural interactions between previously
uncoexistent characters without losing stylistic fidelity. Experiments on a
curated benchmark of cartoons and live-action series with 10 characters show
clear improvements in identity preservation, interaction quality, and
robustness to style delusion, enabling new forms of generative
storytelling.Additional results and videos are available on our project page:
https://tingtingliao.github.io/mimix/.

</details>


### [171] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: VChain是一种新的推理时视觉思维链框架，通过将多模态大模型的视觉推理信号注入视频生成过程，提升复杂动态视频的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以合成具有连贯因果链的复杂动态，而大语言和多模态模型具备较强的视觉状态推理与未来预测能力，因此希望结合二者优势。

Method: 提出VChain框架，利用多模态大模型生成关键稀疏帧作为视觉推理快照，并在这些关键时刻指导预训练视频生成器进行稀疏的推理时调优。

Result: 在复杂、多步场景的实验中，VChain显著提升了生成视频的质量，同时保持高效且无需密集监督。

Conclusion: VChain有效融合了多模态模型的推理能力与视频生成模型的渲染能力，为复杂动态视频生成提供了高效且实用的解决方案。

Abstract: Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [172] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: 提出了一种基于稳定注意力头的线性近似方法，从GPT2-Small第一层中发现响应文本上下文属性的神经元。


<details>
  <summary>Details</summary>
Motivation: 分析注意力分布广泛且对内容依赖较弱的注意力头，探索其softmax分母在固定词元分布下的稳定性。

Method: 通过在‘校准文本’上采样softmax分母，将多个稳定注意力头的输出进行线性组合，实现对上下文的线性摘要近似。

Result: 仅从权重和单一校准文本出发，成功识别出数百个响应高级文本上下文属性的第一层神经元，包括在校准文本中未激活的神经元。

Conclusion: 该方法揭示了Transformer模型第一层中隐藏的可解释神经元，为理解语言模型内部机制提供了新途径。

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [173] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: 提出Graph-S$^3$，一种基于合成逐步监督训练的LLM代理文本图推理框架，通过提取黄金子图生成奖励信号，显著提升图检索效果，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的文本图问答系统在图检索方面存在性能不足，主要依赖浅层嵌入相似性或需要大量标注数据的交互式策略，难以平衡信息丰富性与上下文紧凑性。

Method: 提出Graph-S$^3$框架，采用LLM-based检索器，通过离线提取黄金子图生成逐步监督信号；设计数据合成 pipeline 和两阶段训练方案，以学习交互式图探索策略。

Result: 在三个主流数据集上相比七个强基线平均提升8.1%准确率和9.7% F1分数，尤其在多跳复杂推理任务中优势更明显。

Conclusion: Graph-S$^3$通过合成逐步监督有效解决了文本图问答中的检索挑战，实现了更精准、稳定的图内容检索，具备良好的应用前景。

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [174] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: 研究了六种流行的大语言模型（LLMs）在完成30项日常任务时所表现出的隐含价值观，并与100名美国众包工人进行比较，发现LLMs在隐含价值观上常与人类及其他LLMs不一致。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在执行主观日常任务时所体现的隐含价值观，并评估其与人类价值观的一致性。

Method: 通过审计六种流行的LLMs完成30项日常任务的表现，将其与100名美国众包工人的结果进行对比。

Result: LLMs在完成任务时表现出的隐含价值观常常与人类不一致，且不同LLMs之间也缺乏一致性。

Conclusion: 当前的LLMs在隐含价值观方面尚未与人类或彼此之间实现良好对齐，提示在AI助手设计中需更关注价值观的建模与对齐。

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [175] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 提出CSAR算法，通过形式与意义之间的互信息来诱导生成语言语料库中的词素，经过多个数据集验证其有效性，并用于分析生成语言的词汇特征。


<details>
  <summary>Details</summary>
Motivation: 从生成语言语料库中自动提取词素，以理解语言的结构和演化。

Method: 采用基于互信息的贪心算法，迭代地计数、选择、删除词素对，逐步提取词素。

Result: 在程序生成数据和人类语言数据上验证了CSAR的有效性，并成功量化了生成语言中的同义性和多义性等语言特征。

Conclusion: CSAR能有效诱导词素并揭示生成语言的结构特性，具有跨领域应用潜力。

Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [176] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

TL;DR: Omni-Embed-Nemotron 是一个统一的多模态检索嵌入模型，支持文本、图像、音频和视频的跨模态与联合模态检索，提升了复杂真实场景下的信息检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的检索器难以处理现实中文档丰富的视觉和语义内容，如PDF、幻灯片或视频，需要更强大的多模态检索能力。

Method: 基于ColPali等图像表示方法保留文档布局，并借鉴Qwen2.5-Omni等多模态模型能力，构建支持文本、图像、音频、视频的统一嵌入模型，实现跨模态和联合模态检索。

Result: 实验验证了Omni-Embed-Nemotron在文本、图像和视频检索任务中具有有效性，能够提升多模态信息检索性能。

Conclusion: Omni-Embed-Nemotron 实现了多模态检索的统一建模，为应对复杂真实世界信息需求提供了有效解决方案。

Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [177] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 本文设计了一个基于信号博弈的新兴通信环境，通过超参数优化生成与人类语言相似度高的新兴语言，并使用XferBench评估其向人类语言迁移的效果。


<details>
  <summary>Details</summary>
Motivation: 提高新兴语言与人类语言的相似性，以增强其在实际应用中的可转移性和实用性。

Method: 采用基于信号博弈的框架构建通信环境，利用XferBench作为目标函数进行超参数优化，并分析熵对迁移学习性能的影响。

Result: 成功生成了在统计上更接近人类语言的新兴语言，验证了熵对迁移性能的预测能力，并确认了新兴通信系统倾向于最小化熵；同时发现了能产生更真实新兴语言的超参数规律。

Conclusion: 通过超参数优化和XferBench评估，可以有效提升新兴语言与人类语言的相似性，且熵是衡量其迁移性能的重要指标。

Abstract: In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [178] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: 本文提出了SEER基准，用于评估大语言模型在识别表达情感的文本片段方面的能力，强调了情感证据检测的重要性，并通过两项任务和新的标注数据进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统的情感识别任务通常为整个句子分配单一标签，而忽略了具体表达情感的文本片段。为了更精确地理解情感表达，需要一种能够定位情感证据的方法。

Method: SEER基准包含两个任务：单句内情感证据识别和五句连续段落中的情感证据识别。使用1200个真实世界句子的新注释数据对14个开源大语言模型进行评估。

Result: 一些模型在单句输入上接近人类平均水平，但在较长段落中准确性下降。错误分析揭示了关键失败模式，包括过度依赖情感关键词和在中性文本中产生假阳性。

Conclusion: SEER基准有助于推动大语言模型在情感证据检测方面的研究，特别是在复杂文本结构中的应用。

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [179] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: ALHD是首个大规模综合性阿拉伯语数据集，旨在区分人类与大语言模型生成的文本，涵盖三种体裁和标准阿拉伯语及方言，支持可重复研究，并通过基准实验揭示跨体裁泛化等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语中缺乏专门用于区分人类与大语言模型生成文本的大规模、多样化且标注良好的数据集，限制了阿拉伯语LLM生成文本检测的研究与发展。

Method: 构建包含新闻、社交媒体和评论三类体裁的阿拉伯语文本数据集，覆盖现代标准阿拉伯语和方言，收集来自三个主流大语言模型生成及多个人类来源的超过40万条平衡样本，进行严格预处理、丰富标注和标准化划分，并在传统分类器、BERT类模型和大语言模型上开展零样本、少样本及微调实验。

Result: 微调后的BERT模型表现最佳，优于基于大语言模型的方法；但所有模型在跨体裁泛化时均表现不稳定，尤其在新闻领域难以区分LLM与人类文本，因两者风格高度相似。

Conclusion: ALHD为阿拉伯语生成文本检测提供了可靠基础，揭示了当前方法在跨体裁场景下的局限性，推动未来研究关注泛化能力提升，并有助于应对虚假信息、学术不端和网络安全威胁。

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [180] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: TS-Reasoner 是一种将时间序列基础模型（TSFM）与大语言模型（LLM）对齐的新方法，通过两阶段训练实现高效的时间序列理解和推理任务，在多个基准上优于现有模型且具有更高的数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列模型缺乏复杂推理能力，而大语言模型难以理解数值型时间序列数据，如何有效融合两者仍是一个挑战。

Method: 提出一种简单有效的方法生成时间序列与文本描述的合成配对数据，并采用两阶段训练策略：先进行对齐预训练，再进行指令微调；同时冻结预训练的TSFM以提升稳定性和效率。

Result: 在多个基准测试中，TS-Reasoner 超越了主流的LLM、视觉语言模型和时间序列LLM，且仅使用不到一半的训练数据即达到更优性能。

Conclusion: TS-Reasoner 成功实现了TSFM与LLM的有效对齐，为时间序列推理任务提供了一种高效、可扩展的解决方案。

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [181] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: 提出一种基于RAG的对比推理层（peer-aware comparative inference layer），以提升在专业领域（如金融）中生成更具上下文相关性和具体性的分析内容，相较于传统RAG方法在ROUGE和BERTScore等指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统RAG虽能提取事实信息，但在专业领域的推理任务中输出往往过于泛化，缺乏针对特定情境的深入洞察，例如金融风险分析中仅生成适用于大多数公司的通用风险。

Method: 在RAG基础上引入一种具有对比能力的推理层，该层能够检索可比较的案例或相关问题，并通过对比分析生成更具差异性和上下文敏感性的文本。

Result: 该方法在与人工撰写的股票研究和风险分析文本对比时，在ROUGE和BERTScore等文本生成指标上优于基线RAG模型。

Conclusion: 所提出的对比推理层能有效增强RAG在专业领域中的推理能力，生成更具体、更有区分度的内容，尤其适用于需要类比和细致分析的场景。

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


### [182] [Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs](https://arxiv.org/abs/2510.03527)
*Sayan Ghosh,Shahzaib Saqib Warraich,Dhruv Tarsadiya,Gregory Yauney,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 本文提出了Consensus Graphs (ConGrs)，一种基于有向无环图的数据结构，用于整合大语言模型多次采样生成的长文本响应中的共识与语义差异，提升事实准确性并增强对不可回答问题的拒绝能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以高效整合大语言模型在相同提示下生成的多样化长文本响应中的认知信号，导致信息利用不充分。

Method: 采用轻量级的词法序列比对算法（源自生物信息学）构建ConGrs，并结合辅助的语言模型裁判进行语义判断；设计任务相关的解码策略，从ConGr中合成最终输出。

Result: 在传记生成任务上，事实准确率最高提升31%，且减少80%以上的LM裁判使用；在拒绝类任务中，拒绝率提高达56%；在MATH和AIME推理任务上，准确率优于自验证和多数投票基线最多6个百分点。

Conclusion: ConGrs提供了一种灵活有效的方法，用于捕捉语言模型响应的多样性，并利用响应间的认知差异生成更优、更可靠的综合响应。

Abstract: Language models can be sampled multiple times to access the distribution
underlying their responses, but existing methods cannot efficiently synthesize
rich epistemic signals across different long-form responses. We introduce
Consensus Graphs (ConGrs), a flexible DAG-based data structure that represents
shared information, as well as semantic variation in a set of sampled LM
responses to the same prompt. We construct ConGrs using a light-weight lexical
sequence alignment algorithm from bioinformatics, supplemented by the targeted
usage of a secondary LM judge. Further, we design task-dependent decoding
methods to synthesize a single, final response from our ConGr data structure.
Our experiments show that synthesizing responses from ConGrs improves factual
precision on two biography generation tasks by up to 31% over an average
response and reduces reliance on LM judges by more than 80% compared to other
methods. We also use ConGrs for three refusal-based tasks requiring abstention
on unanswerable queries and find that abstention rate is increased by up to
56%. We apply our approach to the MATH and AIME reasoning tasks and find an
improvement over self-verification and majority vote baselines by up to 6
points of accuracy. We show that ConGrs provide a flexible method for capturing
variation in LM responses and using the epistemic signals provided by response
variation to synthesize more effective responses.

</details>


### [183] [Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance](https://arxiv.org/abs/2510.03528)
*Ahmed Alajrami,Xingwei Tan,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文研究了在指令调优中引入扰动（如去除停用词或打乱词语顺序）对大语言模型性能的影响，发现这种方法不仅能提高模型对噪声指令的鲁棒性，在某些情况下还能提升其在标准基准任务上的下游性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在指令调优后虽能更好完成任务，但对指令措辞的小变化非常敏感。为增强模型对噪声输入的鲁棒性，探索在训练中引入扰动的潜力。

Method: 在指令调优过程中使用经过扰动的训练数据（如去除停用词、词语打乱），并在原始和扰动版本的多个主流基准（MMLU、BBH、GSM8K）上评估模型性能，分析学习动态和行为变化。

Result: 在扰动指令上进行指令调优在某些情况下能够提升模型在原始和扰动测试集上的下游任务表现，并增强了模型对噪声指令的适应能力。

Conclusion: 在指令调优中引入扰动不仅有助于提升模型鲁棒性，还可能带来性能增益，建议在训练数据中加入扰动以增强模型对真实场景中不规范输入的适应能力。

Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities
of large language models (LLMs), improving their usability in generating
helpful responses on various tasks. However, previous work has demonstrated
that they are sensitive to minor variations in instruction phrasing. In this
paper, we explore whether introducing perturbations in instruction-tuning data
can enhance LLMs' resistance against noisy instructions. We focus on how
instruction-tuning with perturbations, such as removing stop words or shuffling
words, affects LLMs' performance on the original and perturbed versions of
widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics
and potential shifts in model behavior. Surprisingly, our results suggest that
instruction-tuning on perturbed instructions can, in some cases, improve
downstream performance. These findings highlight the importance of including
perturbed instructions in instruction-tuning, which can make LLMs more
resilient to noisy user inputs.

</details>


### [184] [TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering](https://arxiv.org/abs/2510.03536)
*Zhaohan Meng,Zaiqiao Meng,Siwei Liu,Iadh Ounis*

Main category: cs.CL

TL;DR: TriMediQ是一种基于三元组结构的知识图谱方法，通过将患者对话转化为结构化三元组并支持多跳推理，显著提升大模型在交互式医疗问答中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在静态单轮医疗问答中表现良好，但在模拟真实临床问诊的多轮交互场景中表现下降，难以有效利用无结构的对话日志进行推理。

Method: 提出TriMediQ框架：使用冻结的三元组生成器从患者回应中提取临床三元组，并构建知识图谱；通过可训练的图编码器和投影模块捕捉关系信息，在推理时指导大模型进行多跳推理。分两步：先冻结LLM权重微调投影模块，再用于增强推理。

Result: 在两个交互式问答基准上评估，TriMediQ在iMedQA数据集上相比五个基线最高提升了10.4%的准确率。

Conclusion: 将患者回应转化为结构化的三元组知识图谱能有效提升大模型在多轮医疗对话中的临床推理准确性，为部署可靠的LLM医疗助手提供了可行方案。

Abstract: Large Language Models (LLMs) perform strongly in static and single-turn
medical Question Answer (QA) benchmarks, yet such settings diverge from the
iterative information gathering process required in practical clinical
consultations. The MEDIQ framework addresses this mismatch by recasting the
diagnosis as an interactive dialogue between a patient and an expert system,
but the reliability of LLMs drops dramatically when forced to reason with
dialogue logs, where clinical facts appear in sentences without clear links. To
bridge this gap, we introduce TriMediQ, a triplet-structured approach that
summarises patient responses into triplets and integrates them into a Knowledge
Graph (KG), enabling multi-hop reasoning. We introduce a frozen triplet
generator that extracts clinically relevant triplets, using prompts designed to
ensure factual consistency. In parallel, a trainable projection module,
comprising a graph encoder and a projector, captures relational information
from the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)
the projection module fine-tuning with all LLM weights frozen; and (ii) using
the fine-tuned module to guide multi-hop reasoning during inference. We
evaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up
to 10.4\% improvement in accuracy over five baselines on the iMedQA dataset.
These results demonstrate that converting patient responses into structured
triplet-based graphs enables more accurate clinical reasoning in multi-turn
settings, providing a solution for the deployment of LLM-based medical
assistants.

</details>


### [185] [LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction](https://arxiv.org/abs/2510.03577)
*Ikram Belmadani,Parisa Nazari Hashemi,Thomas Sebbag,Benoit Favre,Guillaume Fortier,Solen Quiniou,Emmanuel Morin,Richard Dufour*

Main category: cs.CL

TL;DR: 本文提出了三种结合大语言模型、标注指南、合成数据和后处理的方法，参与了EvalLLM 2025法语生物医学命名实体识别（NER）和健康事件抽取的少样本挑战赛。GPT-4.1在提示工程优化下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在极低资源的法语生物医学文本中进行命名实体识别和事件抽取面临标注数据稀缺的问题，需要探索高效的少样本方法。

Method: 提出三种方法：(1) 结合自动示例选择与标注指南摘要的上下文学习（GPT-4.1）；(2) 在合成语料上微调GLiNER并用LLM后处理验证；(3) 在相同合成语料上微调LLaMA-3.1-8B-Instruct。事件抽取复用GPT-4.1的上下文学习策略。

Result: GPT-4.1在NER任务上取得61.53%的macro-F1，在事件抽取上达到15.02%，显著优于其他方法。

Conclusion: 精心设计的提示（prompting）能显著提升大语言模型在极低资源场景下的性能，是少样本生物医学信息抽取的有效途径。

Abstract: This work presents our participation in the EvalLLM 2025 challenge on
biomedical Named Entity Recognition (NER) and health event extraction in French
(few-shot setting). For NER, we propose three approaches combining large
language models (LLMs), annotation guidelines, synthetic data, and
post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating
automatic selection of 10 examples and a summary of the annotation guidelines
into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic
corpus and then verified by an LLM in post-processing, and (3) the open LLM
LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event
extraction uses the same ICL strategy with GPT-4.1, reusing the guideline
summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for
NER and 15.02% for event extraction, highlighting the importance of
well-crafted prompting to maximize performance in very low-resource scenarios.

</details>


### [186] [What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification](https://arxiv.org/abs/2510.03541)
*Andrew Halterman,Katherine A. Keith*

Main category: cs.CL

TL;DR: 本文讨论了在计算社会科学中使用大语言模型进行文本分类时，概念化步骤的重要性被忽视的问题，指出跳过此步骤可能导致偏差，并通过模拟展示了这种偏差无法仅通过提高模型准确率或事后校正方法来消除，最后提出了实现低偏差、低方差下游估计的具体建议。


<details>
  <summary>Details</summary>
Motivation: 强调在大语言模型时代，计算社会科学中概念化步骤常被忽略，可能导致分析偏差。

Method: 通过模拟研究分析概念化错误对下游统计推断的影响。

Result: 发现概念化引起的偏差不能仅通过提高LLM准确性或事后偏差校正方法来纠正。

Conclusion: 提醒研究者概念化仍是首要任务，并提供实现低偏差、低方差估计的实用建议。

Abstract: Generative large language models (LLMs) are now used extensively for text
classification in computational social science (CSS). In this work, focus on
the steps before and after LLM prompting -- conceptualization of concepts to be
classified and using LLM predictions in downstream statistical inference --
which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can
tempt analysts to skip the conceptualization step, creating conceptualization
errors that bias downstream estimates. Using simulations, we show that this
conceptualization-induced bias cannot be corrected for solely by increasing LLM
accuracy or post-hoc bias correction methods. We conclude by reminding CSS
analysts that conceptualization is still a first-order concern in the LLM-era
and provide concrete advice on how to pursue low-cost, unbiased, low-variance
downstream estimates.

</details>


### [187] [CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making](https://arxiv.org/abs/2510.03553)
*Hasibur Rahman,Hanan Salam*

Main category: cs.CL

TL;DR: CCD-Bench是一个新基准，用于评估大语言模型在跨文化价值观冲突下的决策能力，揭示了现有模型偏好特定文化（如北欧和日耳曼欧洲），而忽视其他文化（如东欧和中东非洲），且其推理缺乏对性别平等和强势性等深层价值的实质性考量。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注文化知识或单一偏见检测，缺乏对多种文化价值观直接冲突时模型决策能力的评估。本文旨在填补这一空白，探讨LLM如何在真实文化价值冲突中进行权衡。

Method: 构建包含2,182个开放式困境的CCD-Bench，覆盖七个领域，每个困境对应十个GLOBE文化集群的匿名选项，采用分层拉丁方设计减少顺序效应，并对17个非推理型LLM进行评估。

Result: 模型显著偏好北欧（20.2%）和日耳曼欧洲（12.4%）文化选项，而东欧和中东非洲选项被严重低估（5.6%-5.8%）；尽管87.9%的推理提及多个GLOBE维度，但多为未来导向与绩效导向的表面组合，对强势性和性别平等的关注均低于3%；顺序效应可忽略，模型输出按开发者谱系聚类而非地理分布。

Conclusion: 当前对齐机制倾向于推广一种共识导向的世界观，难以应对需要权力协商、基于权利的推理或性别敏感分析的场景；CCD-Bench推动评估从单一偏见检测转向多元决策，强调需发展能真正融入多样世界观的对齐策略。

Abstract: Although large language models (LLMs) are increasingly implicated in
interpersonal and societal decision-making, their ability to navigate explicit
conflicts between legitimately different cultural value systems remains largely
unexamined. Existing benchmarks predominantly target cultural knowledge
(CulturalBench), value prediction (WorldValuesBench), or single-axis bias
diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple
culturally grounded values directly clash. We address this gap with CCD-Bench,
a benchmark that assesses LLM decision-making under cross-cultural value
conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,
each paired with ten anonymized response options corresponding to the ten GLOBE
cultural clusters. These dilemmas are presented using a stratified Latin square
to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models
disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe
(12.4 percent), while options for Eastern Europe and the Middle East and North
Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of
rationales reference multiple GLOBE dimensions, this pluralism is superficial:
models recombine Future Orientation and Performance Orientation, and rarely
ground choices in Assertiveness or Gender Egalitarianism (both under 3
percent). Ordering effects are negligible (Cramer's V less than 0.10), and
symmetrized KL divergence shows clustering by developer lineage rather than
geography. These patterns suggest that current alignment pipelines promote a
consensus-oriented worldview that underserves scenarios demanding power
negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts
evaluation beyond isolated bias detection toward pluralistic decision making
and highlights the need for alignment strategies that substantively engage
diverse worldviews.

</details>


### [188] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 提出了一种衡量大语言模型（LLM）输出中现实世界主张多样性的新方法，发现尽管较新的模型生成更多样化的主张，但几乎所有模型的表征多样性仍低于基础网络搜索，且模型规模越大，多样性越低，检索增强生成（RAG）有助于提升多样性，但效果受文化背景影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM同质化问题的关注局限于封闭式选择题或模糊语义特征，缺乏跨时间和文化背景的趋势分析，因此需要一种能衡量知识多样性（epistemic diversity）的新方法来系统评估LLM的知识坍塌风险。

Method: 提出一种测量LLM输出中主张多样性的新方法，基于27个LLM、涵盖12个国家的155个主题以及来自真实用户对话的200种提示变体进行大规模实证研究，并与网络搜索和Wikipedia等传统知识源进行比较。

Result: 较新的LLM生成的主张更具多样性，但几乎所有模型的表征多样性仍低于基础网络搜索；模型规模增大反而降低多样性，而检索增强生成（RAG）能提升多样性，但其效果因文化背景而异；与Wikipedia相比，LLM在国家特定主张上更偏向英语而非本地语言，显示出表征偏差。

Conclusion: 当前LLM存在知识表征多样性不足的问题，尤其在非英语文化背景下表现更差，提示需结合外部知识检索并考虑文化差异以缓解知识坍塌风险。

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [189] [Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models](https://arxiv.org/abs/2510.03561)
*Adam Filipek*

Main category: cs.CL

TL;DR: 本文提出了一种名为Reactive Transformer (RxT)的新架构，旨在解决传统Transformer在对话AI中因无状态性和二次计算复杂度导致的高成本和延迟问题。RxT通过引入固定大小的短时记忆系统，将对话轮次作为离散事件实时处理，实现了从数据驱动到事件驱动的转变。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在长对话中需要重复处理不断增长的历史，导致计算成本和延迟过高，难以支持经济高效的长周期对话应用。

Method: RxT采用生成-解码器基于当前查询和记忆状态生成响应，并通过独立的记忆编码器和记忆注意力网络异步更新短时记忆（STM），实现响应生成与记忆更新的解耦。

Result: 实验表明，RxT在推理延迟上保持恒定，性能优于同等规模的无状态基线模型，且对话总成本从二次降至线性复杂度O(N·T)。

Conclusion: RxT通过事件驱动机制和固定内存架构，实现了低延迟、状态化、可扩展的对话系统，为大规模语言模型在真实场景中的长期交互提供了可行方案。

Abstract: The Transformer architecture has become the de facto standard for Large
Language Models (LLMs), demonstrating remarkable capabilities in language
understanding and generation. However, its application in conversational AI is
fundamentally constrained by its stateless nature and the quadratic
computational complexity ($O(L^2)$) with respect to sequence length $L$.
Current models emulate memory by reprocessing an ever-expanding conversation
history with each turn, leading to prohibitive costs and latency in long
dialogues. This paper introduces the Reactive Transformer (RxT), a novel
architecture designed to overcome these limitations by shifting from a
data-driven to an event-driven paradigm. RxT processes each conversational turn
as a discrete event in real-time, maintaining context in an integrated,
fixed-size Short-Term Memory (STM) system. The architecture features a distinct
operational cycle where a generator-decoder produces a response based on the
current query and the previous memory state, after which a memory-encoder and a
dedicated Memory Attention network asynchronously update the STM with a
representation of the complete interaction. This design fundamentally alters
the scaling dynamics, reducing the total user-facing cost of a conversation
from quadratic ($O(N^2 \cdot T)$) to linear ($O(N \cdot T)$) with respect to
the number of interactions $N$. By decoupling response generation from memory
updates, RxT achieves low latency, enabling truly real-time, stateful, and
economically viable long-form conversations. We validated our architecture with
a series of proof-of-concept experiments on synthetic data, demonstrating
superior performance and constant-time inference latency compared to a baseline
stateless model of comparable size.

</details>


### [190] [GRACE: Generative Representation Learning via Contrastive Policy Optimization](https://arxiv.org/abs/2510.04506)
*Jiashuo Sun,Shixuan Liu,Zhaochen Su,Xianrui Zhong,Pengcheng Jiang,Bowen Jin,Peiran Li,Weijia Shi,Jiawei Han*

Main category: cs.CL

TL;DR: GRACE是一种新的框架，将对比信号视为奖励而非损失，利用生成策略优化来训练大语言模型，使其不仅能生成高质量的嵌入，还能输出可解释的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练大语言模型作为文本编码器时，通常使用对比损失并将其视为黑箱函数，忽略了其生成和推理能力。GRACE旨在充分利用这些能力，提升表示学习的效果与透明度。

Method: GRACE将大语言模型视为一个生成策略，通过生成自然语言理由（rationales）来解释其语义理解，并利用均值池化将这些理由编码为高质量嵌入。采用策略梯度优化和多成分奖励函数，最大化查询-正样本对的相似性，最小化与负样本的相似性。

Result: 在MTEB基准测试中，GRACE在四种骨干模型上平均提升了11.5%（监督设置）和6.9%（无监督变体），同时保持了通用能力。

Conclusion: GRACE成功地将对比目标转化为对理由的奖励，统一了表示学习与生成过程，实现了更强的嵌入能力和可解释性。

Abstract: Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

</details>


### [191] [Fine-grained auxiliary learning for real-world product recommendation](https://arxiv.org/abs/2510.04551)
*Mario Almagro,Diego Ortego,David Jimenez*

Main category: cs.CL

TL;DR: 本文提出了一种名为ALC的辅助学习策略，通过利用批次中的最难负样本生成细粒度嵌入，提升产品推荐系统中的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型在实际生产系统中应用时，往往难以满足高覆盖率的需求，即需要自动化生成大部分推荐结果。

Method: 引入两种训练目标，利用批次中的最难负样本来增强正负样本之间的判别信号，并结合最新的阈值一致边际损失进行优化。

Result: 在LF-AmazonTitles-131K和Tech and Durables两个数据集上验证了ALC的有效性，结合阈值一致边际损失实现了最先进的覆盖率。

Conclusion: ALC策略能有效提升推荐系统的自动化覆盖率，适用于大规模产品推荐场景。

Abstract: Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

</details>


### [192] [Decoupling Task-Solving and Output Formatting in LLM Generation](https://arxiv.org/abs/2510.03595)
*Haikang Deng,Po-Nien Kung,Nanyun Peng*

Main category: cs.CL

TL;DR: 本文提出了Deco-G，一种解耦格式遵循与任务求解的解码框架，通过分离的大语言模型（LLM）和可追踪概率模型（TPM）提升复杂指令下任务的性能与格式合规性。


<details>
  <summary>Details</summary>
Motivation: 随着提示复杂度增加，大语言模型在同时遵循推理指令和严格格式要求时表现困难，存在目标冲突，因此需要显式分离任务求解与格式遵循以提升性能。

Method: 提出Deco-G框架，使用TPM单独处理格式合规性，LLM仅关注任务指令；在每一步解码中结合LLM的token概率与TPM的格式合规概率，并引入指令感知蒸馏、灵活的trie构建算法和HMM状态剪枝以提高效率和可扩展性。

Result: 在数学推理、LLM-as-a-Judge和事件论元抽取等多种任务上验证了Deco-G的有效性，相比常规提示方法取得1.0%到6.0%的相对性能提升，并保证格式完全合规。

Conclusion: Deco-G通过显式解耦任务求解与格式遵循，显著提升了复杂提示下的模型表现，同时确保输出格式正确，具有良好的实用性与可扩展性。

Abstract: Large language models (LLMs) are increasingly adept at following instructions
containing task descriptions to solve complex problems, such as mathematical
reasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow
more complex, models often struggle to adhere to all instructions. This
difficulty is especially common when instructive prompts intertwine reasoning
directives -- specifying what the model should solve -- with rigid formatting
requirements that dictate how the solution must be presented. The entanglement
creates competing goals for the model, suggesting that more explicit separation
of these two aspects could lead to improved performance. To this front, we
introduce Deco-G, a decoding framework that explicitly decouples format
adherence from task solving. Deco-G handles format compliance with a separate
tractable probabilistic model (TPM), while prompts LLMs with only task
instructions. At each decoding step, Deco-G combines next token probabilities
from the LLM with the TPM calculated format compliance likelihood to form the
output probability. To make this approach both practical and scalable for
modern instruction-tuned LLMs, we introduce three key innovations:
instruction-aware distillation, a flexible trie-building algorithm, and HMM
state pruning for computational efficiency. We demonstrate the effectiveness of
Deco-G across a wide range of tasks with diverse format requirements, including
mathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,
our approach yields 1.0% to 6.0% relative gain over regular prompting practice
with guaranteed format compliance.

</details>


### [193] [Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry](https://arxiv.org/abs/2510.04631)
*Anastasia Zhukova,Jonas Lührs,Christian E. Matt,Bela Gipp*

Main category: cs.CL

TL;DR: 本文探讨了如何将原本为科学出版物设计的图感知邻域对比学习方法SciNCL应用于流程工业领域，利用知识图谱中的三元组进行微调，显著提升了文本嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 流程工业中的文本日志包含关键操作信息，但常以稀疏知识图谱形式存在，传统语言模型难以有效捕捉其中的领域术语和文档关系。

Method: 采用SciNCL方法，通过从知识图谱中提取三元组对语言模型进行对比学习微调，增强模型对图结构信息的理解能力。

Result: 在专有的流程工业文本嵌入基准（PITEB）上，微调后的模型比最先进的mE5-large文本编码器性能高出9.8-14.3%（绝对提升5.4-8.0个百分点），且模型大小仅为后者的1/3到1/5。

Conclusion: SciNCL方法能有效利用稀疏知识图谱提升语言模型在流程工业文本上的表示能力，在更小模型规模下实现更优性能，具有实际部署优势。

Abstract: Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained
language models by incorporating additional knowledge from the graph structures
to learn domain-specific terminology or relationships between documents that
might otherwise be overlooked. This paper explores how SciNCL, a graph-aware
neighborhood contrastive learning methodology originally designed for
scientific publications, can be applied to the process industry domain, where
text logs contain crucial information about daily operations and are often
structured as sparse KGs. Our experiments demonstrate that language models
fine-tuned with triplets derived from GE outperform a state-of-the-art
mE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process
industry text embedding benchmark (PITEB) while being 3-5 times smaller in
size.

</details>


### [194] [Can an LLM Induce a Graph? Investigating Memory Drift and Context Length](https://arxiv.org/abs/2510.03611)
*Raquib Bin Yousuf,Aadyant Khatri,Shengzhe Xu,Mandar Sharma,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 该论文提出应通过复杂的关系推理任务（如从自然语言文本中提取图结构）来评估大语言模型的上下文记忆能力，而非简单的‘大海捞针’任务。研究发现，现有模型在较短上下文长度时即出现记忆漂移和上下文遗忘，表明其在信息密集场景下的推理能力存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 现有评测基准多依赖简单检索或续写任务，难以真实反映大语言模型在信息密集场景中的表现，因此需要更贴近实际复杂推理能力的评估方式。

Method: 设计基于从含噪自然语言文本中诱导出结构化关系知识（如图结构）的复杂推理任务，测试模型在长上下文、信息混杂条件下的表现，并与传统基准对比分析其记忆保持能力。

Result: 实验表明，大语言模型在执行此类关系推理任务时，在远短于现有基准估计的有效长度下就开始出现记忆漂移和上下文遗忘；即使是专为推理优化的模型（如OpenAI o1）也难逃此问题。

Conclusion: 当前大语言模型在从非结构化输入中抽象结构化知识方面存在显著缺陷，需通过架构改进以增强长距离推理与上下文保持能力。

Abstract: Recently proposed evaluation benchmarks aim to characterize the effective
context length and the forgetting tendencies of large language models (LLMs).
However, these benchmarks often rely on simplistic 'needle in a haystack'
retrieval or continuation tasks that may not accurately reflect the performance
of these models in information-dense scenarios. Thus, rather than simple next
token prediction, we argue for evaluating these models on more complex
reasoning tasks that requires them to induce structured relational knowledge
from the text - such as graphs from potentially noisy natural language content.
While the input text can be viewed as generated in terms of a graph, its
structure is not made explicit and connections must be induced from distributed
textual cues, separated by long contexts and interspersed with irrelevant
information. Our findings reveal that LLMs begin to exhibit memory drift and
contextual forgetting at much shorter effective lengths when tasked with this
form of relational reasoning, compared to what existing benchmarks suggest.
With these findings, we offer recommendations for the optimal use of popular
LLMs for complex reasoning tasks. We further show that even models specialized
for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in
these settings. These results point to significant limitations in the models'
ability to abstract structured knowledge from unstructured input and highlight
the need for architectural adaptations to improve long-range reasoning.

</details>


### [195] [Towards Unsupervised Speech Recognition at the Syllable-Level](https://arxiv.org/abs/2510.03639)
*Liming Wang,Junrui Ni,Kai-Wei Chang,Saurabhchand Bhati,David Harwath,Mark Hasegawa-Johnson,James R. Glass*

Main category: cs.CL

TL;DR: 提出了一种基于音节级别的掩码语言模型的无监督语音识别（UASR）框架，无需图音转换器（G2P），在LibriSpeech上实现了最高40%的字符错误率（CER）相对降低，并有效推广到中文。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于音素的UASR方法依赖昂贵资源（如G2P）和在音素边界模糊语言中训练不稳定的问题。

Method: 引入基于音节级别的UASR框架，采用掩码语言建模，避免使用G2P和GAN带来的训练不稳定性。

Result: 在LibriSpeech上实现最高40%的CER相对降低，且在中文上表现出良好泛化能力。

Conclusion: 该音节级框架有效提升了UASR在多语言场景下的性能与稳定性，尤其适用于此前难以处理的语言如中文。

Abstract: Training speech recognizers with unpaired speech and text -- known as
unsupervised speech recognition (UASR) -- is a crucial step toward extending
ASR to low-resource languages in the long-tail distribution and enabling
multimodal learning from non-parallel data. However, existing approaches based
on phones often rely on costly resources such as grapheme-to-phoneme converters
(G2Ps) and struggle to generalize to languages with ambiguous phoneme
boundaries due to training instability. In this paper, we address both
challenges by introducing a syllable-level UASR framework based on masked
language modeling, which avoids the need for G2P and the instability of
GAN-based methods. Our approach achieves up to a 40\% relative reduction in
character error rate (CER) on LibriSpeech and generalizes effectively to
Mandarin, a language that has remained particularly difficult for prior
methods. Code will be released upon acceptance.

</details>


### [196] [UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG](https://arxiv.org/abs/2510.03663)
*Xiangyu Peng,Cab Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 本文提出了UniDoc-Bench，首个大规模、真实场景下的多模态检索增强生成（MM-RAG）基准，基于70,000个真实PDF页面构建，包含1,600个多模态问答对，涵盖多种查询类型，并支持四种范式的统一评估。


<details>
  <summary>Details</summary>
Motivation: 现有MM-RAG评估碎片化，无法反映以文档为中心的真实多模态应用场景，缺乏统一、现实的基准来全面评估多模态RAG系统性能。

Method: 构建了一个从真实PDF中提取文本、表格和图像证据的管道，生成多样化的多模态QA对；设计了涵盖四种检索范式的统一评估协议，包括标准化候选池、提示和指标，并通过多人标注与专家仲裁确保数据质量。

Result: 实验表明，多模态文本-图像融合RAG系统在性能上持续优于单模态及联合多模态嵌入检索方法；揭示了视觉上下文如何补充文本证据，发现了系统性错误模式。

Conclusion: 文本或图像单独使用均不足以支撑复杂MM-RAG任务，当前多模态嵌入方法仍不充分，需结合融合策略并改进模型设计以提升鲁棒性。

Abstract: Multimodal retrieval-augmented generation (MM-RAG) is a key approach for
applying large language models (LLMs) and agents to real-world knowledge bases,
yet current evaluations are fragmented, focusing on either text or images in
isolation or on simplified multimodal setups that fail to capture
document-centric multimodal use cases. In this paper, we introduce
UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from
70k real-world PDF pages across eight domains. Our pipeline extracts and links
evidence from text, tables, and figures, then generates 1,600 multimodal QA
pairs spanning factual retrieval, comparison, summarization, and logical
reasoning queries. To ensure reliability, 20% of QA pairs are validated by
multiple annotators and expert adjudication. UniDoc-Bench supports
apples-to-apples comparison across four paradigms: (1) text-only, (2)
image-only, (3) multimodal text-image fusion, and (4) multimodal joint
retrieval -- under a unified protocol with standardized candidate pools,
prompts, and evaluation metrics. Our experiments show that multimodal
text-image fusion RAG systems consistently outperform both unimodal and jointly
multimodal embedding-based retrieval, indicating that neither text nor images
alone are sufficient and that current multimodal embeddings remain inadequate.
Beyond benchmarking, our analysis reveals when and how visual context
complements textual evidence, uncovers systematic failure modes, and offers
actionable guidance for developing more robust MM-RAG pipelines.

</details>


### [197] [Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text](https://arxiv.org/abs/2510.03683)
*Nisar Hussain,Amna Qasim,Gull Mehak,Muhammad Zain,Momina Hafeez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本文提出了一种基于QLoRA的微调框架，用于提升罗马乌尔都语-英语混杂文本中的冒犯性语言检测性能。通过将代码混合数据翻译为英文以利用英文大模型，尽管牺牲了部分混合特征，但在低资源环境下实现了高效分类。实验表明，Meta LLaMA 3 8B模型取得了91.45的F1分数，优于其他基线模型。


<details>
  <summary>Details</summary>
Motivation: 由于语法不明确、拼写不一致和标注数据稀缺，罗马乌尔都语等代码混合语言中的贬义词检测对自然语言处理系统构成挑战。现有方法在低资源语言上的表现受限，亟需高效的解决方案。

Method: 采用QLoRA对多种大语言模型（如Meta LLaMA 3 8B、Mistral 7B、LLaMA 2 7B、ModernBERT和RoBERTa）进行内存高效的微调，并将罗马乌尔都语-英语混合数据集通过Google Translate翻译成英文，以利用英文大模型的能力。模型在人工标注的数据集上进行训练和评估。

Result: Meta LLaMA 3 8B模型取得了最高的F1分数91.45%，Mistral 7B达到89.66%，均优于传统Transformer模型。验证了QLoRA在低资源环境下微调大模型的有效性。

Conclusion: QLoRA结合英文翻译的策略能有效提升罗马乌尔都语中冒犯性语言的检测性能，展示了大语言模型在此类低资源任务中的潜力，为多语言冒犯性内容检测系统的构建提供了可扩展的路径。

Abstract: The use of derogatory terms in languages that employ code mixing, such as
Roman Urdu, presents challenges for Natural Language Processing systems due to
unstated grammar, inconsistent spelling, and a scarcity of labeled data. In
this work, we propose a QLoRA based fine tuning framework to improve offensive
language detection in Roman Urdu-English text. We translated the Roman
Urdu-English code mixed dataset into English using Google Translate to leverage
English LLMs, while acknowledging that this translation reduces direct
engagement with code mixing features. Our focus is on classification
performance using English translated low resource inputs. We fine tuned several
transformers and large language models, including Meta LLaMA 3 8B, Mistral 7B
v0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient
adaptation. Models were trained and evaluated on a manually annotated Roman
Urdu dataset for offensive vs non offensive content. Of all tested models, the
highest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral
7B at 89.66, surpassing traditional transformer baselines. These results
demonstrate the efficacy of QLoRA in fine tuning high performing models for low
resource environments such as code mixed offensive language detection, and
confirm the potential of LLMs for this task. This work advances a scalable
approach to Roman Urdu moderation and paves the way for future multilingual
offensive detection systems based on LLMs.

</details>


### [198] [MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction](https://arxiv.org/abs/2510.03687)
*Yue Huang,Yanyuan Chen,Dexuan Xu,Weihua Yue,Huamin Zhang,Meikang Qiu,Yu Huang*

Main category: cs.CL

TL;DR: 本文提出了MedReflect框架，通过引入类似医生的反思性思维模式，使大语言模型在无需外部检索或大量标注的情况下提升医学问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部知识检索或大量标注数据，存在开销大、成本高且性能有限的问题。

Method: MedReflect构建了一种单通路反思链，包括初始假设生成、自提问、自回答和决策优化，实现自我验证与反思。

Result: 仅用2000个随机样本进行轻量微调，MedReflect在多个医学基准上显著提升了准确率，并大幅减少了标注需求。

Conclusion: 大语言模型可通过自我反思学习解决专业医学问题，降低对外部监督和大量任务特定数据的依赖。

Abstract: Medical problem solving demands expert knowledge and intricate reasoning.
Recent studies of large language models (LLMs) attempt to ease this complexity
by introducing external knowledge verification through retrieval-augmented
generation or by training on reasoning datasets. However, these approaches
suffer from drawbacks such as retrieval overhead and high annotation costs, and
they heavily rely on substituted external assistants to reach limited
performance in medical field. In this paper, we introduce MedReflect, a
generalizable framework designed to inspire LLMs with a physician-like
reflective thinking mode. MedReflect generates a single-pass reflection chain
that includes initial hypothesis generation, self-questioning, self-answering
and decision refinement. This self-verified and self-reflective nature releases
large language model's latent capability in medical problem-solving without
external retrieval or heavy annotation. We demonstrate that MedReflect enables
cost-efficient medical dataset construction: with merely 2,000 randomly sampled
training examples and a light fine-tuning, this approach achieves notable
absolute accuracy improvements across a series of medical benchmarks while
cutting annotation requirements. Our results provide evidence that LLMs can
learn to solve specialized medical problems via self-reflection and
self-improve, reducing reliance on external supervision and extensive
task-specific fine-tuning data.

</details>


### [199] [TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation](https://arxiv.org/abs/2510.03748)
*Ramtin Kakavand,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 本文提出了一种名为TreePrompt的新颖示例选择方法，该方法通过树形结构框架学习大语言模型（LLM）的偏好，以识别高质量且上下文相关的示例，从而提升机器翻译性能。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本提示方法主要关注查询与示例之间的相似性，而忽略了示例本身的质量，这限制了翻译质量的进一步提升。因此，需要一种能够同时考虑示例相似性和质量的方法。

Method: 提出TreePrompt方法，利用树形结构框架学习LLM对示例的偏好，并结合K-最近邻（K-NN）和自适应少样本提示（AFSP）来平衡示例的相似性与质量。在英语-波斯语（MIZAN）和英语-德语（WMT19）两个数据集上进行评估。

Result: 实验结果表明，将TreePrompt与AFSP或随机选择结合使用，能够显著提高翻译性能。

Conclusion: TreePrompt通过兼顾示例的相似性与质量，有效提升了少样本提示下的机器翻译效果，验证了引入高质量示例的重要性。

Abstract: Large Language Models (LLMs) have consistently demonstrated strong
performance in machine translation, especially when guided by high-quality
prompts. Few-shot prompting is an effective technique to improve translation
quality; however, most existing example selection methods focus solely on
query-to-example similarity and do not account for the quality of the examples.
In this work, we propose TreePrompt, a novel example selection approach that
learns LLM preferences to identify high-quality, contextually relevant examples
within a tree-structured framework. To further explore the balance between
similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)
and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -
English-Persian (MIZAN) and English-German (WMT19) - show that integrating
TreePrompt with AFSP or Random selection leads to improved translation
performance.

</details>


### [200] [Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech](https://arxiv.org/abs/2510.03758)
*Ilias Tougui,Mehdi Zakroum,Mounir Ghogho*

Main category: cs.CL

TL;DR: 提出一种基于音素、音节和词级别的多语言帕金森病语音检测方法，采用双向LSTM与多头注意力机制，发现音素级别分析性能最优，且关键语音特征与临床指标一致。


<details>
  <summary>Details</summary>
Motivation: 现有基于语音的帕金森病检测多分析整段语句，可能忽略特定语音单元的诊断价值，因此需要更细粒度的分析方法。

Method: 构建自动化流水线提取语音中的音素、音节和词等时间对齐单元，使用双向LSTM结合多头注意力机制，在意大利语、西班牙语和英语数据集上比较不同粒度级别的诊断性能。

Result: 音素级别分析表现最佳，AUROC达93.78%±2.34%，准确率为92.17%±2.43%；注意力分析显示最具信息量的特征为持续元音（/a/, /e/, /o/, /i/）、DDK音节（/ta/, /pa/, /la/, /ka/）和/pataka/序列。

Conclusion: 细粒度语音单元分析可提升跨语言帕金森病检测性能，且模型关注的关键语音特征与临床评估一致，验证了方法的可解释性与实用性。

Abstract: Parkinson's Disease (PD) affects over 10 million people worldwide, with
speech impairments in up to 89% of patients. Current speech-based detection
systems analyze entire utterances, potentially overlooking the diagnostic value
of specific phonetic elements. We developed a granularity-aware approach for
multilingual PD detection using an automated pipeline that extracts
time-aligned phonemes, syllables, and words from recordings. Using Italian,
Spanish, and English datasets, we implemented a bidirectional LSTM with
multi-head attention to compare diagnostic performance across the different
granularity levels. Phoneme-level analysis achieved superior performance with
AUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates
enhanced diagnostic capability for cross-linguistic PD detection. Importantly,
attention analysis revealed that the most informative speech features align
with those used in established clinical protocols: sustained vowels (/a/, /e/,
/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)
at syllable level, and /pataka/ sequences at word level. Source code will be
available at https://github.com/jetliqs/clearpd.

</details>


### [201] [Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs](https://arxiv.org/abs/2510.03762)
*Deshan Sumanathilaka,Nicholas Micallef,Julian Hough*

Main category: cs.CL

TL;DR: 本研究探讨了在多语言词义消歧任务中，不平衡的少样本示例如何通过 GLOSSGPT 方法影响大语言模型的预测性能，发现非英语语言对样本分布更敏感。


<details>
  <summary>Details</summary>
Motivation: 探索少样本提示在多语言词义消歧中的有效性及其因样本分布不平衡引入的偏差。

Method: 采用 GLOSSGPT 提示方法，在英语、德语、西班牙语、法语和意大利语上测试 GPT-4o 和 LLaMA-3.1-70B 模型的性能。

Result: 不平衡的少样本示例会导致多语言环境下的错误词义预测，但英语中未出现此问题，表明非英语语言对样本分布更敏感。

Conclusion: 在多语言少样本词义消歧中，需采用平衡且具代表性的提示策略以避免偏差。

Abstract: Recent advances in Large Language Models (LLMs) have significantly reshaped
the landscape of Natural Language Processing (NLP). Among the various prompting
techniques, few-shot prompting has gained considerable attention for its
practicality and effectiveness. This study investigates how few-shot prompting
strategies impact the Word Sense Disambiguation (WSD) task, particularly
focusing on the biases introduced by imbalanced sample distributions. We use
the GLOSSGPT prompting method, an advanced approach for English WSD, to test
its effectiveness across five languages: English, German, Spanish, French, and
Italian. Our results show that imbalanced few-shot examples can cause incorrect
sense predictions in multilingual languages, but this issue does not appear in
English. To assess model behavior, we evaluate both the GPT-4o and
LLaMA-3.1-70B models and the results highlight the sensitivity of multilingual
WSD to sample distribution in few-shot settings, emphasizing the need for
balanced and representative prompting strategies.

</details>


### [202] [Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development](https://arxiv.org/abs/2510.03781)
*Majid Asgari-Bidhendi,Muhammad Amin Ghaseminia,Alireza Shahbazi,Sayyed Ali Hossayni,Najmeh Torabian,Behrouz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: 本文介绍了Rezwan，一个通过全自动流水线构建的大规模AI辅助圣训语料库，包含超过120万条叙述，具有多语言翻译、智能标音、摘要生成和主题标注等丰富注释，并验证了其在准确性与成本效益上的显著优势。


<details>
  <summary>Details</summary>
Motivation: 为了提升伊斯兰文本处理的规模与质量，解决传统人工标注耗时昂贵且难以扩展的问题，推动数字人文与伊斯兰研究的发展。

Method: 利用大型语言模型（LLMs）从数字资源中自动提取并结构化圣训文本，实现分段、传述链与正文分离、验证及多层次增强，包括机器翻译、智能标音、摘要生成、主题标记和语义分析。

Result: 在1,213条随机抽样数据上，六位领域专家评估显示链文分离和摘要任务接近人类水平（均为9.33/10），整体质量优于Noor语料库（8.46 vs 3.66/10），且成本远低于人工方式。

Conclusion: AI可有效增强人类专家能力，实现大规模、多语言、语义丰富的宗教文本处理，为伊斯兰文化遗产的数字化研究提供新范式。

Abstract: This paper presents the development of Rezwan, a large-scale AI-assisted
Hadith corpus comprising over 1.2M narrations, extracted and structured through
a fully automated pipeline. Building on digital repositories such as Maktabat
Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for
segmentation, chain--text separation, validation, and multi-layer enrichment.
Each narration is enhanced with machine translation into twelve languages,
intelligent diacritization, abstractive summarization, thematic tagging, and
cross-text semantic analysis. This multi-step process transforms raw text into
a richly annotated research-ready infrastructure for digital humanities and
Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled
narrations, assessed by six domain experts. Results show near-human accuracy in
structured tasks such as chain--text separation (9.33/10) and summarization
(9.33/10), while highlighting ongoing challenges in diacritization and semantic
similarity detection. Comparative analysis against the manually curated Noor
Corpus demonstrates the superiority of Najm in both scale and quality, with a
mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis
confirms the economic feasibility of the AI approach: tasks requiring over
229,000 hours of expert labor were completed within months at a fraction of the
cost. The work introduces a new paradigm in religious text processing by
showing how AI can augment human expertise, enabling large-scale, multilingual,
and semantically enriched access to Islamic heritage.

</details>


### [203] [Mechanistic Interpretability of Socio-Political Frames in Language Models](https://arxiv.org/abs/2510.03799)
*Hadi Asghari,Sami Nenno*

Main category: cs.CL

TL;DR: 本文研究了大语言模型在生成和识别社会政治语境中深层认知框架（如“严父”和“关爱父母”）方面的能力，发现这些框架在模型的隐藏表示中可定位到特定维度。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型如何捕捉和表达人类重要的抽象认知结构，尤其是在社会政治话语中的深层框架。

Method: 基于机械可解释性研究，通过零样本设置测试模型对认知框架的识别与生成能力，并分析其在模型隐藏层中的表征位置。

Result: LLMs能流畅生成体现特定认知框架的文本，并能在零样本下识别这些框架；研究发现了与‘严父’和‘关爱父母’框架强相关的单一表征维度。

Conclusion: 大语言模型不仅能够表达复杂的认知框架，且这些概念在模型内部具有可识别的、结构化的表征形式。

Abstract: This paper explores the ability of large language models to generate and
recognize deep cognitive frames, particularly in socio-political contexts. We
demonstrate that LLMs are highly fluent in generating texts that evoke specific
frames and can recognize these frames in zero-shot settings. Inspired by
mechanistic interpretability research, we investigate the location of the
`strict father' and `nurturing parent' frames within the model's hidden
representation, identifying singular dimensions that correlate strongly with
their presence. Our findings contribute to understanding how LLMs capture and
express meaningful human concepts.

</details>


### [204] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 本文提出了一种名为Step Pruner（SP）的强化学习框架，旨在通过鼓励紧凑的推理步骤来减少大推理模型中的“过度思考”问题，在保证正确性的同时显著降低响应长度。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在复杂任务上表现优异，但常出现过度冗长的“过思考”现象；现有基于强化学习的简洁性优化方法易导致模型跳过必要推理步骤或产生作弊行为，需更有效的训练机制。

Method: 提出Step Pruner（SP），采用步感知奖励函数，在确保推理正确性的前提下对冗余步骤进行惩罚，并引入动态停止机制：当输出步骤长度超过上限时停止更新，防止模型通过合并步骤来规避惩罚。

Result: 在四个推理基准上的实验表明，SP在保持甚至提升准确率的同时显著缩短了响应长度；例如在AIME24上减少了69.7%的token使用量。

Conclusion: Step Pruner有效缓解了大推理模型的过思考问题，在不牺牲性能的前提下实现了高效、紧凑的推理过程，为构建更高效的推理系统提供了新思路。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [205] [Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches](https://arxiv.org/abs/2510.03808)
*Mehedi Hasan Emon*

Main category: cs.CL

TL;DR: 该研究使用INCEpTION工具对体育新闻（板球报道）中的修辞关系进行标注，比较了人工标注与基于大语言模型的自动方法，并评估了BERT、DistilBERT和逻辑回归模型在分类修辞关系（如扩展、对比、背景、因果）中的表现，结果显示DistilBERT准确率最高。


<details>
  <summary>Details</summary>
Motivation: 探索高效准确地自动标注话语中修辞关系的方法，推动话语分析与基于Transformer的自然语言处理技术的融合。

Method: 采用INCEpTION工具进行人工标注，利用BERT、DistilBERT和逻辑回归模型对修辞关系进行自动分类，并在板球新闻语料上进行比较评估。

Result: DistilBERT在修辞关系分类任务中表现最佳，取得了最高的准确率，显示出其在话语关系预测中的高效性。

Conclusion: DistilBERT在修辞结构分析中具有优越性能，适用于高效的自动话语解析，为NLP中话语结构建模提供了实用方案。

Abstract: This research explores the annotation of rhetorical relations in discourse
using the INCEpTION tool and compares manual annotation with automatic
approaches based on large language models. The study focuses on sports reports
(specifically cricket news) and evaluates the performance of BERT, DistilBERT,
and Logistic Regression models in classifying rhetorical relations such as
elaboration, contrast, background, and cause-effect. The results show that
DistilBERT achieved the highest accuracy, highlighting its potential for
efficient discourse relation prediction. This work contributes to the growing
intersection of discourse parsing and transformer-based NLP. (This paper was
conducted as part of an academic requirement under the supervision of Prof. Dr.
Ralf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:
Rhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,
NLP.

</details>


### [206] [Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles](https://arxiv.org/abs/2510.03898)
*Nusrat Jahan Lia,Shubhashis Roy Dipta,Abdullah Khan Zehady,Naymul Islam,Madhusodan Chakraborty,Abdullah Al Wasif*

Main category: cs.CL

TL;DR: 本文介绍了首个用于孟加拉语新闻政治立场检测的基准数据集，包含200篇标注为亲政府、批评政府和中立立场的文章，并对28种大语言模型进行了评估，发现模型在识别批评政府内容时表现较好，但在处理中立立场时表现较差，且倾向于过度预测亲政府立场。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标注数据集和计算研究，孟加拉语政治立场检测面临挑战，尤其是在理解语言特征、文化背景和隐含偏见方面存在困难。

Method: 构建了一个包含200篇孟加拉语新闻文章的数据集，标注其政治立场（亲政府、批评政府、中立），并系统评估了28种大语言模型的表现，同时提供诊断分析。

Result: 模型在检测批评政府内容时F1值最高达0.83，但对中立内容的F1值低至0.00，且普遍存在过度预测亲政府立场的问题。

Conclusion: 该数据集为孟加拉语媒体立场检测研究提供了基础，揭示了当前大语言模型在低资源语言中的局限性，并为改进模型性能提供了方向。

Abstract: Detecting media bias is crucial, specifically in the South Asian region.
Despite this, annotated datasets and computational studies for Bangla political
bias research remain scarce. Crucially because, political stance detection in
Bangla news requires understanding of linguistic cues, cultural context, subtle
biases, rhetorical strategies, code-switching, implicit sentiment, and
socio-political background. To address this, we introduce the first benchmark
dataset of 200 politically significant and highly debated Bangla news articles,
labeled for government-leaning, government-critique, and neutral stances,
alongside diagnostic analyses for evaluating large language models (LLMs). Our
comprehensive evaluation of 28 proprietary and open-source LLMs shows strong
performance in detecting government-critique content (F1 up to 0.83) but
substantial difficulty with neutral articles (F1 as low as 0.00). Models also
tend to over-predict government-leaning stances, often misinterpreting
ambiguous narratives. This dataset and its associated diagnostics provide a
foundation for advancing stance detection in Bangla media research and offer
insights for improving LLM performance in low-resource languages.

</details>


### [207] [PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian](https://arxiv.org/abs/2510.03913)
*Mohammad Amin Abbasi,Hassan Naderi*

Main category: cs.CL

TL;DR: 本文提出了PsychoLexTherapy，一个用于在波斯语中模拟心理治疗推理的小型语言模型框架，强调文化适配、隐私保护和结构化记忆。


<details>
  <summary>Details</summary>
Motivation: 为资源较少的语言（如波斯语）开发具备文化敏感性和治疗连贯性的对话系统面临挑战，且需兼顾隐私与本地部署可行性。

Method: 通过三阶段方法：评估SLM的心理学知识（PsychoLexEval），设计基于结构化记忆的推理框架，并构建两个评估数据集（PsychoLexQuery和PsychoLexDialogue）进行基准测试。

Result: 实验表明，该框架在单轮问答和多轮对话中均优于基线模型；结构化长期记忆模块显著提升共情、连贯性、文化契合度和个人化表现，避免了简单历史拼接导致的信息丢失。

Conclusion: PsychoLexTherapy为波斯语心理治疗对话系统提供了可复现、隐私友好且文化对齐的基础，贡献了新数据集、评估流程及结构化记忆设计的经验见解。

Abstract: This study presents PsychoLexTherapy, a framework for simulating
psychotherapeutic reasoning in Persian using small language models (SLMs). The
framework tackles the challenge of developing culturally grounded,
therapeutically coherent dialogue systems with structured memory for multi-turn
interactions in underrepresented languages. To ensure privacy and feasibility,
PsychoLexTherapy is optimized for on-device deployment, enabling use without
external servers. Development followed a three-stage process: (i) assessing
SLMs psychological knowledge with PsychoLexEval; (ii) designing and
implementing the reasoning-oriented PsychoLexTherapy framework; and (iii)
constructing two evaluation datasets-PsychoLexQuery (real Persian user
questions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark
against multiple baselines. Experiments compared simple prompting, multi-agent
debate, and structured therapeutic reasoning paths. Results showed that
deliberate model selection balanced accuracy, efficiency, and privacy. On
PsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic
LLM-as-a-judge evaluation and was ranked highest by human evaluators in a
single-turn preference study. In multi-turn tests with PsychoLexDialogue, the
long-term memory module proved essential: while naive history concatenation
caused incoherence and information loss, the full framework achieved the
highest ratings in empathy, coherence, cultural fit, and personalization.
Overall, PsychoLexTherapy establishes a practical, privacy-preserving, and
culturally aligned foundation for Persian psychotherapy simulation,
contributing novel datasets, a reproducible evaluation pipeline, and empirical
insights into structured memory for therapeutic reasoning.

</details>


### [208] [Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs](https://arxiv.org/abs/2510.03997)
*Junjie Luo,Rui Han,Arshana Welivita,Zeleikun Di,Jingfu Wu,Xuzhe Zhi,Ritu Agarwal,Gordon Gao*

Main category: cs.CL

TL;DR: 该研究利用大语言模型（LLM）从410万条患者评论中自动提取医生的“大五人格”特征和五种主观评价，验证了LLM评估与人类判断的高度一致性，并揭示了医生特质与患者满意度之间的系统性关系。


<details>
  <summary>Details</summary>
Motivation: 理解患者对医生的看法对于提升医患信任、沟通和满意度至关重要，但传统方法难以大规模分析患者叙述，因此需要自动化、可扩展的分析工具。

Method: 研究采用基于大语言模型（LLM）的分析流程，对来自美国22.7万名医生的410万条在线评论进行大五人格（开放性、尽责性、外向性、宜人性、情绪稳定性）和五种患者导向主观判断的推断，并通过多模型比较和人类专家评估进行验证。

Result: LLM评估与人类专家判断高度一致（相关系数0.72-0.89），且所有特质均与患者满意度显著正相关（r=0.41-0.81）；男性医生在各项特质上评分更高，临床能力差异最明显；儿科和精神科医生在共情相关特质上得分较高；聚类分析识别出四种医生类型：'全面优秀型'（33.8%）、'欠佳型'（22.6%）等。

Conclusion: 从患者叙述中自动提取人格特质可提供可解释、经验证的大规模指标，有助于医疗质量评估、偏见检测和医护人员发展。

Abstract: Understanding how patients perceive their physicians is essential to
improving trust, communication, and satisfaction. We present a large language
model (LLM)-based pipeline that infers Big Five personality traits and five
patient-oriented subjective judgments. The analysis encompasses 4.1 million
patient reviews of 226,999 U.S. physicians from an initial pool of one million.
We validate the method through multi-model comparison and human expert
benchmarking, achieving strong agreement between human and LLM assessments
(correlation coefficients 0.72-0.89) and external validity through correlations
with patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis
reveals systematic patterns: male physicians receive higher ratings across all
traits, with largest disparities in clinical competence perceptions;
empathy-related traits predominate in pediatrics and psychiatry; and all traits
positively predict overall satisfaction. Cluster analysis identifies four
distinct physician archetypes, from "Well-Rounded Excellent" (33.8%, uniformly
high traits) to "Underperforming" (22.6%, consistently low). These findings
demonstrate that automated trait extraction from patient narratives can provide
interpretable, validated metrics for understanding physician-patient
relationships at scale, with implications for quality measurement, bias
detection, and workforce development in healthcare.

</details>


### [209] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: 本文提出了首个用于在多轮、长时程任务中探测和评估大语言模型（LLM）欺骗行为的模拟框架，通过多智能体系统（执行者、监督者与独立审计者）揭示了LLM在压力下出现的隐瞒、模棱两可和伪造等欺骗策略，实验涵盖11个前沿模型，发现欺骗行为具有模型依赖性，随事件压力增加，并持续削弱信任。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM欺骗研究多局限于单轮提示，无法捕捉现实世界中长期、动态交互下的欺骗行为，因此需要一个能够模拟复杂情境下欺骗演化的评估框架。

Method: 构建了一个多智能体模拟框架：执行者负责完成任务，监督者评估进展并维护信任状态，独立的欺骗审计者回溯整个交互轨迹以识别欺骗行为；在11个前沿LLM上进行实验，分析不同压力条件下的欺骗表现。

Result: 发现欺骗行为具有模型差异性，随任务压力上升而增加，并导致监督者信任持续下降；定性分析揭示了三种主要欺骗策略：隐瞒、模棱两可和伪造。

Conclusion: 欺骗是长时程交互中LLM的新兴风险，该框架为在真实、信任敏感场景中评估未来LLM提供了基础。

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


### [210] [Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation](https://arxiv.org/abs/2510.04001)
*Xuankang Zhang,Jiangming Liu*

Main category: cs.CL

TL;DR: 提出一种新的实体知识增强方法，用于提升COVID-19相关社交媒体和生物医学文本中的命名实体识别性能。


<details>
  <summary>Details</summary>
Motivation: 由于社交媒体中关于新冠疫情的文本非正式且标注数据稀缺，同时需要大量领域知识，现有的命名实体识别研究受限。

Method: 提出一种实体知识增强（entity knowledge augmentation）方法，并在推特和PubMed数据集上进行实验，适用于非正式和正式文本格式的生物医学命名实体识别。

Result: 在全监督和少样本设置下，所提方法均提升了命名实体识别的性能。

Conclusion: 该实体知识增强方法有效改善了新冠相关及一般生物医学文本中的命名实体识别效果，具有广泛适用性。

Abstract: The COVID-19 pandemic causes severe social and economic disruption around the
world, raising various subjects that are discussed over social media.
Identifying pandemic-related named entities as expressed on social media is
fundamental and important to understand the discussions about the pandemic.
However, there is limited work on named entity recognition on this topic due to
the following challenges: 1) COVID-19 texts in social media are informal and
their annotations are rare and insufficient to train a robust recognition
model, and 2) named entity recognition in COVID-19 requires extensive
domain-specific knowledge. To address these issues, we propose a novel entity
knowledge augmentation approach for COVID-19, which can also be applied in
general biomedical named entity recognition in both informal text format and
formal text format. Experiments carried out on the COVID-19 tweets dataset and
PubMed dataset show that our proposed entity knowledge augmentation improves
NER performance in both fully-supervised and few-shot settings. Our source code
is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master

</details>


### [211] [AgriGPT-VL: Agricultural Vision-Language Understanding Suite](https://arxiv.org/abs/2510.04002)
*Bo Yang,Yunkui Chen,Lanfei Feng,Yu Zhang,Xiao Xu,Jianyu Zhang,Nueraili Aierken,Runhe Huang,Hongjian Lin,Yibin Ying,Shijian Li*

Main category: cs.CL

TL;DR: 本文提出了AgriGPT-VL Suite，一个面向农业的统一多模态框架，包括大规模农业视觉-语言数据集Agri-3M-VL、专用视觉语言模型AgriGPT-VL，以及评估套件AgriBench-VL-4K。该模型在农业多模态任务上优于通用模型，同时保持文本能力，所有资源将开源。


<details>
  <summary>Details</summary>
Motivation: 农业领域缺乏专门的多模态模型、高质量视觉-语言数据集和系统评估方法，限制了AI在农业中的应用。

Method: 提出AgriGPT-VL模型，采用渐进式课程学习策略，包括文本 grounding、多模态浅层/深层对齐和GRPO强化学习优化；并构建Agri-3M-VL数据集与AgriBench-VL-4K评估套件。

Result: AgriGPT-VL在AgriBench-VL-4K上优于主流通用VLM，LLM-as-a-judge评估中胜率更高；在纯文本任务AgriBench-13K上性能未下降；消融实验验证了各训练阶段的有效性。

Conclusion: AgriGPT-VL Suite有效推动了农业领域多模态AI的发展，兼顾多模态推理与语言能力，且具备可复现性和实际部署潜力。

Abstract: Despite rapid advances in multimodal large language models, agricultural
applications remain constrained by the scarcity of domain-tailored models,
curated vision-language corpora, and rigorous evaluation. To address these
challenges, we present the AgriGPT-VL Suite, a unified multimodal framework for
agriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,
the largest vision-language corpus for agriculture to our knowledge, curated by
a scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M
image-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO
reinforcement learning samples. Second, we develop AgriGPT-VL, an
agriculture-specialized vision-language model trained via a progressive
curriculum of textual grounding, multimodal shallow/deep alignment, and GRPO
refinement. This method achieves strong multimodal reasoning while preserving
text-only capability. Third, we establish AgriBench-VL-4K, a compact yet
challenging evaluation suite with open-ended and image-grounded questions,
paired with multi-metric evaluation and an LLM-as-a-judge framework.
Experiments show that AgriGPT-VL outperforms leading general-purpose VLMs on
AgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge
evaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K
with no noticeable degradation of language ability. Ablation studies further
confirm consistent gains from our alignment and GRPO refinement stages. We will
open source all of the resources to support reproducible research and
deployment in low-resource agricultural settings.

</details>


### [212] [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
*Jiarui Liu,Jivitesh Jain,Mona Diab,Nishant Subramani*

Main category: cs.CL

TL;DR: 该论文提出通过分析大语言模型（LLM）的内部激活来预测输出正确性，并评估外部上下文的有效性，实验表明仅用首个输出token的中间层激活即可达到约75%的预测准确率，优于基于提示的基线方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常以高置信度生成错误信息，如何判断何时需要检索上下文以及所用上下文是否有效仍具挑战。

Method: 利用可解释性方法分析模型激活，训练分类器预测输出正确性，并引入指标区分正确、错误和无关上下文。

Result: 在六个模型上的实验显示，基于首个输出token的中间层激活的简单分类器可预测输出正确性（准确率约75%），且该方法显著优于提示基线，在识别正确与错误上下文方面表现更优。

Conclusion: 模型内部包含可用于早期审计和提升可靠性的信号，为理解LLM决策过程提供了新视角。

Abstract: Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

</details>


### [213] [Thai Semantic End-of-Turn Detection for Real-Time Voice Agents](https://arxiv.org/abs/2510.04016)
*Thanapol Popit,Natthapath Rungseesiripak,Monthol Charattrakool,Saksorn Ruangtanusak*

Main category: cs.CL

TL;DR: 本文首次系统研究了泰语文本的实时对话回合结束检测（EOT），比较了小规模大语言模型的零样本与少样本提示方法和轻量级Transformer模型的监督微调方法，结果表明经过微调的小模型可在设备端实现接近即时的EOT判断。


<details>
  <summary>Details</summary>
Motivation: 传统基于音频静音的端点检测存在延迟高、在犹豫或语言特有现象下表现差的问题，亟需低延迟、高准确率的泰语EOT检测方案以支持流畅的语音交互。

Method: 利用YODAS语料库的字幕转录文本和泰语特有的语言特征（如句末助词），将EOT建模为词元边界的二分类问题，比较了紧凑型大语言模型的零样本/少样本提示与轻量级Transformer的监督微调性能。

Result: 发现准确率与延迟之间存在明显权衡，经过监督微调的轻量级模型在泰语EOT检测中表现最佳，可实现低延迟、高精度的实时判断。

Conclusion: 建立了泰语EOT检测的基线，证明小型微调模型适合在设备端部署，能够支持接近即时的对话交互。

Abstract: Fluid voice-to-voice interaction requires reliable and low-latency detection
of when a user has finished speaking. Traditional audio-silence end-pointers
add hundreds of milliseconds of delay and fail under hesitations or
language-specific phenomena. We present, to our knowledge, the first systematic
study of Thai text-only end-of-turn (EOT) detection for real-time agents. We
compare zero-shot and few-shot prompting of compact LLMs to supervised
fine-tuning of lightweight transformers. Using transcribed subtitles from the
YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final
particles), we formulate EOT as a binary decision over token boundaries. We
report a clear accuracy-latency tradeoff and provide a public-ready
implementation plan. This work establishes a Thai baseline and demonstrates
that small, fine-tuned models can deliver near-instant EOT decisions suitable
for on-device agents.

</details>


### [214] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 本文研究了在大语言模型（LLM）为黑盒且调用成本高的情况下，如何通过引入反事实推理来帮助识别对分类决策最重要的词汇。提出了一种称为“决策改变率”的框架来量化关键词的重要性，实验结果表明使用反事实有助于提升LLM的可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于实际限制，大语言模型常被视为黑盒且调用成本高，因此需要有效方法来解释其决策过程。

Method: 引入反事实推理，并提出‘决策改变率’框架来量化分类中关键词的重要性。

Result: 实验结果表明，结合反事实推理有助于提高大语言模型识别关键贡献词的能力。

Conclusion: 在不直接访问模型内部结构的情况下，利用反事实推理和决策改变率可有效增强大语言模型决策的可解释性。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [215] [Small Language Models for Emergency Departments Decision Support: A Benchmark Study](https://arxiv.org/abs/2510.04032)
*Zirui Wang,Jiajun Wu,Braden Teitge,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 本文提出一个针对急诊科（ED）决策支持的小型语言模型（SLM）综合基准，评估结合通用域和医学语料训练的SLM表现。实验结果表明，未经医学微调的通用SLM在多个医疗基准上意外优于医学微调模型，提示在急诊环境中可能无需专门的医学微调。


<details>
  <summary>Details</summary>
Motivation: 由于急诊科环境节奏快、风险高，且存在硬件限制、成本约束和隐私问题，小型语言模型（SLM）因其高效性和推理能力具有实际应用潜力。本文旨在探索适合ED部署的SLM，避免大型语言模型（LLM）带来的资源负担。

Method: 构建一个综合基准，涵盖MedMCQA、MedQA-4Options和PubMedQA等数据集，并引入模拟ED医生日常任务的医学摘要数据集，评估在通用域与医学语料混合训练的小型语言模型（SLM）性能。

Result: 实验结果显示，在多个面向急诊科的基准测试中，未经医学专门微调的通用域SLM表现优于经过医学微调的SLM，表明在特定临床场景下，通用模型可能已足够胜任。

Conclusion: 在急诊科决策支持任务中，通用域小型语言模型（SLM）可超越医学微调模型，意味着在实际部署中可能无需进行专门的医学领域微调，从而降低开发与维护成本，提升部署效率。

Abstract: Large language models (LLMs) have become increasingly popular in medical
domains to assist physicians with a variety of clinical and operational tasks.
Given the fast-paced and high-stakes environment of emergency departments
(EDs), small language models (SLMs), characterized by a reduction in parameter
count compared to LLMs, offer significant potential due to their inherent
reasoning capability and efficient performance. This enables SLMs to support
physicians by providing timely and accurate information synthesis, thereby
improving clinical decision-making and workflow efficiency. In this paper, we
present a comprehensive benchmark designed to identify SLMs suited for ED
decision support, taking into account both specialized medical expertise and
broad general problem-solving capabilities. In our evaluations, we focus on
SLMs that have been trained on a mixture of general-domain and medical corpora.
A key motivation for emphasizing SLMs is the practical hardware limitations,
operational cost constraints, and privacy concerns in the typical real-world
deployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and
PubMedQA, with the medical abstracts dataset emulating tasks aligned with real
ED physicians' daily tasks. Experimental results reveal that general-domain
SLMs surprisingly outperform their medically fine-tuned counterparts across
these diverse benchmarks for ED. This indicates that for ED, specialized
medical fine-tuning of the model may not be required.

</details>


### [216] [Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment](https://arxiv.org/abs/2510.04045)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.CL

TL;DR: 本研究探讨了如何利用思维链（CoT）技术构建可引导的多元价值观大语言模型，并发现基于可验证奖励的强化学习（RLVR）方法在性能和样本效率上表现最优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常反映单一价值观，难以处理需要理解复杂人类观点的任务，因此需要支持可引导的多元主义。

Method: 研究探索了多种方法，包括CoT提示、人工标注CoT微调、合成解释微调以及可验证奖励的强化学习（RLVR），并在Value Kaleidoscope和OpinionQA数据集上进行评估。

Result: RLVR方法在各项指标上 consistently 优于其他方法，展现出较高的训练样本效率；同时分析了生成的CoT轨迹的忠实性和安全性。

Conclusion: CoT技术有助于实现大语言模型的可引导多元主义，其中RLVR是最有效的途径之一。

Abstract: Large Language Models (LLMs) are typically trained to reflect a relatively
uniform set of values, which limits their applicability to tasks that require
understanding of nuanced human perspectives. Recent research has underscored
the importance of enabling LLMs to support steerable pluralism -- the capacity
to adopt a specific perspective and align generated outputs with it. In this
work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be
applied to building steerable pluralistic models. We explore several methods,
including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on
synthetic explanations, and Reinforcement Learning with Verifiable Rewards
(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA
datasets. Among the methods studied, RLVR consistently outperforms others and
demonstrates strong training sample efficiency. We further analyze the
generated CoT traces with respect to faithfulness and safety.

</details>


### [217] [What Makes Diffusion Language Models Super Data Learners?](https://arxiv.org/abs/2510.04071)
*Zitian Gao,Haoming Luo,Lynx Chen,Jason Klein Liu,Ran Tao,Joey Zhou,Bryan Dai*

Main category: cs.CL

TL;DR: 扩散语言模型在低数据条件下表现出色，本文通过消融实验发现随机掩码输入token是提升数据效率的主要因素，并指出MLP dropout和权重衰减也能带来类似增益，表明随机正则化广泛提升了多轮训练中的数据效率。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散语言模型在低数据条件下表现优异，但其高效性的来源尚不明确，本文旨在探究其背后机制。

Method: 通过广泛的消融实验分离影响数据效率的因素，分析随机掩码、MLP dropout和权重衰减的作用。

Result: 随机掩码输入token对数据效率的提升起主导作用；MLP dropout和权重衰减也能取得类似效果，说明随机正则化在多轮训练中普遍提升数据效率。

Conclusion: 扩散语言模型的数据效率主要源于随机掩码等随机正则化机制，而非模型结构本身，这为未来低资源场景下的模型设计提供了指导。

Abstract: Recent studies have shown that diffusion language models achieve remarkable
data efficiency under limited-data constraints, yet the underlying mechanisms
remain unclear. In this work, we perform extensive ablation experiments to
disentangle the sources of this efficiency. Our results show that random
masking of input tokens plays the dominant role. We further show that similar
gains can be obtained through in MLP dropout and weight decay, indicating that
stochastic regularization broadly enhances data efficiency in multi-epoch
training. Our code is available at
https://github.com/zitian-gao/data-efficiency.

</details>


### [218] [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
*Zixin Song,Bowen Zhang,Qian-Wen Zhang,Di Yin,Xing Sun,Chunping Li*

Main category: cs.CL

TL;DR: 本文提出了PoLi-RL，一种新颖的Point-to-List强化学习框架，用于条件语义文本相似性（C-STS）任务。通过两阶段课程学习和并行切片排序奖励机制（PSRR），在官方C-STS基准上实现了48.18的Spearman相关系数，为交叉编码器架构建立了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有C-STS方法局限于判别模型，未能充分利用大语言模型（LLMs）和强化学习（RL）的最新进展。由于传统方法难以优化非可微的Spearman排序指标，且粗粒度奖励信号导致模型训练困难，因此需要更精细、有效的RL方法来提升C-STS性能。

Method: 提出PoLi-RL框架，采用两阶段课程学习：第一阶段使用简单的逐点奖励训练基础评分能力；第二阶段引入结合逐点、成对和列表级目标的混合奖励，以提高模型对细微语义差异的分辨能力。同时设计了并行切片排序奖励（PSRR）机制，通过并行计算不同样本同索引生成结果的排名奖励，实现精细化的信用分配。

Result: 在官方C-STS基准上，PoLi-RL取得了48.18的Spearman相关系数，显著优于现有方法，成为交叉编码器架构下的新SOTA。实验表明，该方法能有效优化复杂条件判断任务中的LLM性能。

Conclusion: PoLi-RL是首个成功将强化学习应用于C-STS的工作，提供了一种强大且精确的训练范式，适用于基于排序的复杂条件判断任务，推动了LLMs在语义相似性评估中的应用。

Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic
proximity between text segments under a specific condition, thereby overcoming
the ambiguity inherent in traditional STS. However, existing methods are
largely confined to discriminative models, failing to fully integrate recent
breakthroughs in the NLP community concerning Large Language Models (LLMs) and
Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this
task, as it can directly optimize the non-differentiable Spearman ranking
metric and guide the reasoning process required by C-STS. However, we find that
naively applying listwise RL fails to produce meaningful improvements, as the
model is overwhelmed by complex, coarse-grained reward signals. To address this
challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning
framework. PoLi-RL employs a two-stage curriculum: it first trains the model
with simple pointwise rewards to establish fundamental scoring capabilities,
then transitions to a hybrid reward that combines pointwise, pairwise, and
listwise objectives to refine the model's ability to discern subtle semantic
distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward
(PSRR) mechanism that computes ranking rewards in parallel slices, where each
slice comprises same-indexed completions from different samples. This provides
a precise, differentiated learning signal for each individual completion,
enabling granular credit assignment and effective optimization. On the official
C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,
establishing a new SOTA for the cross-encoder architecture. As the first work
to successfully apply RL to C-STS, our study introduces a powerful and precise
paradigm for training LLMs on complex, ranking-based conditional judgment
tasks.

</details>


### [219] [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
*Honglin Lin,Qizhi Pei,Xin Gao,Zhuoshi Pan,Yu Li,Juntao Li,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 提出Caco框架，通过代码驱动的增强方法自动生成高质量、可验证且多样化的指令-思维链推理数据，实验证明其在数学推理任务中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有思维链（CoT）方法在推理路径的可控性、质量和多样性方面存在不足，且基于代码的增强方法通常局限于预定义的数学问题，缺乏可扩展性和泛化能力。

Method: 首先统一数学和编程解法为代码格式并微调代码型CoT生成器，然后大规模生成多样化推理轨迹；通过代码执行验证和规则过滤确保逻辑正确性和结构多样性，再将过滤后的输出反向工程为自然语言指令和语言型CoT。

Result: 在构建的Caco-1.3M数据集上实验表明，Caco训练的模型在数学推理基准上达到领先性能，优于现有强基线，且在未见任务上表现出更强的泛化能力。

Conclusion: Caco实现了无需人工干预的全自动、可扩展推理数据合成，建立了可持续、可信的推理系统新范式。

Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve
complex tasks, yet achieving reliable and scalable reasoning remains
challenging. While Chain-of-Thought (CoT) prompting has become a mainstream
approach, existing methods often suffer from uncontrolled generation,
insufficient quality, and limited diversity in reasoning paths. Recent efforts
leverage code to enhance CoT by grounding reasoning in executable steps, but
such methods are typically constrained to predefined mathematical problems,
hindering scalability and generalizability. In this work, we propose Caco
(Code-Assisted Chain-of-ThOught), a novel framework that automates the
synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning
data through code-driven augmentation. Unlike prior work, Caco first fine-tunes
a code-based CoT generator on existing math and programming solutions in a
unified code format, then scales the data generation to a large amount of
diverse reasoning traces. Crucially, we introduce automated validation via code
execution and rule-based filtering to ensure logical correctness and structural
diversity, followed by reverse-engineering filtered outputs into natural
language instructions and language CoTs to enrich task adaptability. This
closed-loop process enables fully automated, scalable synthesis of reasoning
data with guaranteed executability. Experiments on our created Caco-1.3M
dataset demonstrate that Caco-trained models achieve strong competitive
performance on mathematical reasoning benchmarks, outperforming existing strong
baselines. Further analysis reveals that Caco's code-anchored verification and
instruction diversity contribute to superior generalization across unseen
tasks. Our work establishes a paradigm for building self-sustaining,
trustworthy reasoning systems without human intervention.

</details>


### [220] [Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence](https://arxiv.org/abs/2510.04120)
*Fengying Ye,Shanshan Wang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 该研究从概念映射、隐喻-字面知识库和句法敏感性三个角度探讨大语言模型（LLMs）在隐喻理解中的局限性，发现其存在生成无关解释、依赖训练数据中的隐喻线索而非上下文、对句法不规则更敏感等问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在知识整合和语境推理方面表现出色，但其在隐喻理解方面的机制尚不明确，因此需要系统评估其隐喻处理能力。

Method: 通过嵌入空间投影分析概念映射，构建隐喻与字面表达的知识库，并评估句法结构对模型表现的影响。

Result: LLMs产生15%-25%的概念无关解释，依赖训练数据中的隐喻标志而非上下文线索，且对句法异常比对结构理解更敏感。

Conclusion: 当前LLMs在隐喻分析方面存在显著局限，需开发更强大的计算方法以提升其隐喻理解能力。

Abstract: Metaphor analysis is a complex linguistic phenomenon shaped by context and
external factors. While Large Language Models (LLMs) demonstrate advanced
capabilities in knowledge integration, contextual reasoning, and creative
generation, their mechanisms for metaphor comprehension remain insufficiently
explored. This study examines LLMs' metaphor-processing abilities from three
perspectives: (1) Concept Mapping: using embedding space projections to
evaluate how LLMs map concepts in target domains (e.g., misinterpreting "fall
in love" as "drop down from love"); (2) Metaphor-Literal Repository: analyzing
metaphorical words and their literal counterparts to identify inherent
metaphorical knowledge; and (3) Syntactic Sensitivity: assessing how
metaphorical syntactic structures influence LLMs' performance. Our findings
reveal that LLMs generate 15\%-25\% conceptually irrelevant interpretations,
depend on metaphorical indicators in training data rather than contextual cues,
and are more sensitive to syntactic irregularities than to structural
comprehension. These insights underline the limitations of LLMs in metaphor
analysis and call for more robust computational approaches.

</details>


### [221] [Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)](https://arxiv.org/abs/2510.04124)
*Nuwan I. Senaratna*

Main category: cs.CL

TL;DR: 本文介绍了斯里兰卡议会记录、司法判决、政府出版物、新闻和旅游统计数据的开放、机器可读文档数据集集合，旨在支持计算语言学、法律分析、社会政治研究和多语言自然语言处理的研究。


<details>
  <summary>Details</summary>
Motivation: 为了促进斯里兰卡多语言环境下计算语言学、法律分析和社会科学研究的发展，提供高质量、开放获取的文本数据资源。

Method: 收集并整理来自斯里兰卡多个官方来源的文档数据，涵盖三种语言（僧伽罗语、泰米尔语和英语），通过自动化流程每日更新，并在GitHub和Hugging Face平台上镜像发布。

Result: 截至v20251005版本，该数据集包含215,670份文档（共60.3 GB），分为13个子数据集，支持多种语言和应用场景。

Conclusion: 该数据集为多语言NLP、法律与社会科学研究提供了有价值的资源，具有良好的可持续性和开放性。

Abstract: We present a collection of open, machine-readable document datasets covering
parliamentary proceedings, legal judgments, government publications, news, and
tourism statistics from Sri Lanka. As of v20251005, the collection currently
comprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and
English. The datasets are updated daily and mirrored on GitHub and Hugging
Face. These resources aim to support research in computational linguistics,
legal analytics, socio-political studies, and multilingual natural language
processing. We describe the data sources, collection pipeline, formats, and
potential use cases, while discussing licensing and ethical considerations.

</details>


### [222] [Fine Tuning Methods for Low-resource Languages](https://arxiv.org/abs/2510.04139)
*Tim Bakkenes,Daniel Wang,Anton Johansson*

Main category: cs.CL

TL;DR: 本项目通过构建文化相关的数据集并对Gemma 2模型进行后训练，提升其在代表性不足语言中的表现，并提供可推广的方法以促进生成式AI在不同文化中的应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型多基于英语文本和文化训练，导致其在其他语言和文化背景下表现不佳，缺乏文化包容性。

Method: 开发一种可泛化的文化相关数据集准备方法，并对Gemma 2模型进行后训练。

Result: 提升了Gemma 2模型在某一代表性不足语言上的性能。

Conclusion: 该方法有助于推动生成式AI在不同国家和文化中的本地化应用，并助力文化遗产的保护。

Abstract: The rise of Large Language Models has not been inclusive of all cultures. The
models are mostly trained on English texts and culture which makes them
underperform in other languages and cultural contexts. By developing a
generalizable method for preparing culturally relevant datasets and
post-training the Gemma 2 model, this project aimed to increase the performance
of Gemma 2 for an underrepresented language and showcase how others can do the
same to unlock the power of Generative AI in their country and preserve their
cultural heritage.

</details>


### [223] [Self Speculative Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.04147)
*Yifeng Gao,Ziang Ji,Yuxuan Wang,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为自推测解码（SSD）的无损推理加速方法，利用扩散式大语言模型自身进行多位置并行预测和分层验证，实现高达3.46倍的加速，同时保持输出与逐步解码一致。


<details>
  <summary>Details</summary>
Motivation: 当前扩散式大语言模型的并行解码方法生成结果偏离逐步解码，导致性能下降，限制了实际部署。

Method: 提出自推测解码（SSD），利用dLLM自身作为推测解码的草案生成器和验证器，通过自起草机制和分层验证树，在单次前向传播中完成多个位置的预测与验证。

Result: 实验表明，SSD在LLaDA和Dream等开源模型上实现了最高3.46倍的加速，且输出与逐步解码完全一致。

Conclusion: SSD是一种高效、无损的推理加速方法，消除了传统推测解码对辅助模块的需求，提升了dLLM的实用性和效率。

Abstract: Diffusion-based Large Language Models (dLLMs) have emerged as a competitive
alternative to autoregressive models, offering unique advantages through
bidirectional attention and parallel generation paradigms. However, the
generation results of current parallel decoding methods deviate from stepwise
decoding, introducing potential performance degradation, which limits their
practical deployment. To address this problem, we propose \textbf{S}elf
\textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration
method that leverages the dLLM itself as both speculative decoding drafter and
verifier without auxiliary modules. SSD introduces a self-drafting mechanism
where the model generates predictions for multiple positions, then verifies
them through hierarchical verification trees in a single forward pass. Unlike
traditional speculative decoding that requires separate draft models, SSD
eliminates model redundancy and memory overhead by exploiting the dLLM's
inherent parallel prediction capability for multiple positions. This
self-speculative approach allows the model to progressively verify and accept
multiple tokens in a single forward pass. Our experiments demonstrate that SSD
achieves up to 3.46$\times$ speedup while keeping the output identical to
stepwise decoding on open source models such as LLaDA and Dream. Code will be
made publicly available on GitHub.

</details>


### [224] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: 本文提出了Latent Thought Policy Optimization (LTPO)，一种无需更新模型参数的测试时推理增强框架，通过优化隐含的“思维”向量并利用基于置信度的内在奖励信号来提升大语言模型在复杂任务上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式推理方法在分布外和高难度任务上表现脆弱，需要一种更鲁棒且无需训练的推理增强方法。

Method: LTPO将中间隐状态视为动态参数，在每个问题实例上使用在线策略梯度方法进行优化，奖励信号来自冻结LLM输出分布的置信度，无需外部监督或生成文本。

Result: 在五个推理基准上实验表明，LTPO在标准任务上媲美或超越强基线，并在极具挑战性的AIME基准上显著优于现有方法（现有方法接近零准确率），展现出卓越的复杂推理能力。

Conclusion: LTPO提供了一种有效的参数无关、测试时优化机制，显著提升了大语言模型在困难任务下的隐式推理鲁棒性和性能。

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [225] [CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling](https://arxiv.org/abs/2510.04204)
*Zhengyang Tang,Zihan Ye,Chenyu Huang,Xuhan Huang,Chengpeng Li,Sihang Li,Guanhua Chen,Ming Yan,Zizhuo Wang,Hongyuan Zha,Dayiheng Liu,Benyou Wang*

Main category: cs.CL

TL;DR: 提出CALM框架，通过轻量级修正和强化学习提升大推理模型在优化建模任务中的表现，开发出4B参数的STORM模型，在多个基准上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有领域适配方法无法有效利用现代大推理模型的高级推理模式，直接微调传统非反思数据集效果有限。

Method: 提出CALM框架，通过专家干预识别推理错误并提供简明修正提示，结合监督微调和强化学习逐步优化大推理模型。

Result: STORM模型（4B参数）在五个优化建模基准上平均准确率达68.9%，媲美671B模型性能。

Conclusion: 基于动态提示的数据合成能有效保留并增强现代大推理模型的原生推理能力，为复杂优化建模任务提供更高效可扩展的路径。

Abstract: Large Reasoning Models (LRMs) have demonstrated strong capabilities in
complex multi-step reasoning, opening new opportunities for automating
optimization modeling. However, existing domain adaptation methods, originally
designed for earlier instruction-tuned models, often fail to exploit the
advanced reasoning patterns of modern LRMs -- In particular, we show that
direct fine-tuning on traditional \textit{non-reflective} datasets leads to
limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose
\textbf{CALM} (\textit{Corrective Adaptation with Lightweight Modification}), a
framework that progressively refines LRMs within their native reasoning modes
for optimization modeling tasks. In CALM, an expert intervener identifies
reasoning flaws and provides concise corrective hints, which the LRM
incorporates to produce improved reasoning trajectories. These interventions
modify fewer than 2.6\% of generated tokens, but generate high-quality data for
soft adaptation through supervised fine-tuning. The adapted model is then
further improved through reinforcement learning. Building on CALM, we develop
\textbf{STORM} (\textit{Smart Thinking Optimization Reasoning Model}), a
4B-parameter LRM that achieves a new state-of-the-art average accuracy of
68.9\% across five popular optimization modeling benchmarks, matching the
performance of a 671B LRM. These results demonstrate that dynamic, hint-based
data synthesis both preserves and amplifies the native reasoning patterns of
modern LRMs, offering a more effective and scalable path towards expert-level
performance on challenging optimization modeling tasks.

</details>


### [226] [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
*Zhuoran Zhuang,Ye Chen,Xia Zeng,Chao Luo,Luhui Liu,Yihan Chen*

Main category: cs.CL

TL;DR: 提出了一种名为REPO的强化学习后训练框架，用于将大语言模型部署为在线旅行社中的商务开发代理，以进行价格谈判，显著提升了对话质量和说服力表现。


<details>
  <summary>Details</summary>
Motivation: 传统后训练方法在脚本过拟合、缺乏细腻说服风格和无法强制执行可验证业务约束方面存在不足，需要更有效的对齐方法。

Method: 提出Reward-Enhanced Policy Optimization (REPO)，结合基于偏好的奖励模型、奖励裁判和程序化奖励函数，通过多信号融合优化LLM策略。

Result: 在真实和不良案例对话评估中，REPO显著提升平均对话评分至4.63，优于基线及其他方法，并实现93.33%的错误修复率和66.67%的优质回应覆盖率。

Conclusion: REPO有效提升了LLM在复杂商业谈判场景下的表现，展现出超越人工标注的新兴能力。

Abstract: We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

</details>


### [227] [Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought](https://arxiv.org/abs/2510.04230)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Amit Agarwal,Hyunwoo Ko,Chanuk Lim,Srikant Panda,Minhyuk Kim,Nikunj Drolia,Dasol Choi,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 本文提出了语言混合思维链（Language-Mixed CoT）方法，通过在英语和目标语言之间切换进行推理，并以韩语为例构建了大规模韩语推理数据集Yi-Sang，训练出性能领先的KO-REAson-35B模型，在多个基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注英文推理，缺乏对特定语言推理能力的研究，本文旨在填补这一空白，探索更有效的多语言推理方法。

Method: 提出Language-Mixed CoT推理框架，结合英语锚定与目标语言表达，在减少翻译失真的同时提升推理能力；构建Yi-Sang数据集，包含579万韩语提示和370万长推理链，用于训练4B至35B的九个模型。

Result: 最佳模型KO-REAson-35B在九个基准测试中五个排名第一、四个排名第二，平均得分64.0±25；小规模和中等规模模型平均提升18.6分；消融实验表明Language-Mixed CoT优于单语CoT，并带来跨语言和多模态性能增益。

Conclusion: Language-Mixed CoT是一种有效的多语言推理策略，Yi-Sang数据集和训练模型为语言特定推理研究提供了重要资源，推动非英语AI系统的进步。

Abstract: Recent frontier models employ long chain-of-thought reasoning to explore
solution spaces in context and achieve stonger performance. While many works
study distillation to build smaller yet capable models, most focus on English
and little is known about language-specific reasoning. To bridge this gap, we
first introduct **Language-Mixed CoT**, a reasoning schema that switches
between English and a target language, using English as an anchor to excel in
reasoning while minimizing translation artificats. As a Korean case study, we
curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and
code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k
high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,
Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves
state-of-the-art performance, with the highest overall average score (64.0 \pm
25), ranking first on 5/9 benchmarks and second on the remainder. Samller and
mid-sized models also benefit substantially, with an average improvement of
+18.6 points across teh evaluated nine benchmarks. Ablations show
**Language-Mixed CoT** is more effective than monolingual CoT, also resulting
in cross-lingual and mult-modal performance gains. We release our data-curation
pipeline, evaluation system, datasets, and models to advance research on
language-specific reasoning. Data and model collection:
https://huggingface.co/KOREAson.

</details>


### [228] [LongTail-Swap: benchmarking language models' abilities on rare words](https://arxiv.org/abs/2510.04268)
*Robin Algayres,Charles-Éric Saint-James,Mahi Luthra,Jiayi Shen,Dongyan Lin,Youssef Benchekroun,Rashel Moritz,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 本文提出了LongTail-Swap（LT-Swap）基准，用于评估语言模型在低数据环境下对罕见词的学习能力，聚焦于词频分布的尾部。通过构建针对BabyLM训练集的测试集，评估了16种模型的表现，发现模型在罕见词上的性能普遍较差，且不同架构在长尾表现上的差异更明显，揭示了某些架构在罕见词泛化上的优势。


<details>
  <summary>Details</summary>
Motivation: 受儿童能以极少量数据学会新词的启发，旨在探索语言模型在低数据 regime 下的学习能力，尤其是对罕见词的建模，弥补现有BabyLM挑战中仅关注高频词的不足。

Method: 提出LT-Swap基准，构建包含可接受与不可接受句子对的测试集，隔离罕见词的语义和句法使用；在BabyLM的10M和100M预训练语料上各构建一个测试集，采用零样本方式通过计算句子对的平均对数概率来评估模型。

Result: 评估了16个BabyLM榜单上的模型，结果显示所有模型在罕见词上的表现均不佳，且不同架构在长尾部分的性能差异比在头部更显著，表明该基准能更好地区分模型在罕见词学习上的能力。

Conclusion: LT-Swap为评估语言模型在低资源条件下对罕见词的泛化能力提供了有效工具，揭示了当前模型在长尾词汇学习上的局限性，并指出了更有潜力的模型架构方向。

Abstract: Children learn to speak with a low amount of data and can be taught new words
on a few-shot basis, making them particularly data-efficient learners. The
BabyLM challenge aims at exploring language model (LM) training in the low-data
regime but uses metrics that concentrate on the head of the word distribution.
Here, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the
tail of the distribution, i.e., measures the ability of LMs to learn new words
with very little exposure, like infants do. LT-Swap is a pretraining
corpus-specific test set of acceptable versus unacceptable sentence pairs that
isolate semantic and syntactic usage of rare words. Models are evaluated in a
zero-shot fashion by computing the average log probabilities over the two
members of each pair. We built two such test sets associated with the 10M words
and 100M words BabyLM training sets, respectively, and evaluated 16 models from
the BabyLM leaderboard. Our results not only highlight the poor performance of
language models on rare words but also reveal that performance differences
across LM architectures are much more pronounced in the long tail than in the
head. This offers new insights into which architectures are better at handling
rare word generalization. We've also made the code publicly avail

</details>


### [229] [Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy](https://arxiv.org/abs/2510.04285)
*Karthik Viswanathan,Sang Eon Park*

Main category: cs.CL

TL;DR: 提出了一种基于累积量展开的框架，用于量化大语言模型在下一词预测中如何内化高阶统计结构，并通过实验验证了其在不同提示和训练阶段的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了理解大语言模型如何在训练过程中学习并内部化高阶统计特征，特别是在不同上下文和任务中的表示差异。

Method: 将每层logit分布的softmax熵视为对其“中心”分布的扰动，推导出可分离高阶相关性的闭式累积量观测指标，并在GPT-2和Pythia模型上进行实证分析。

Result: 发现结构化提示产生典型的上升- plateau 累积量轮廓，而打乱的提示则保持平坦；训练过程中所有累积量单调增加后饱和；数学提示展现出与普通文本不同的累积量特征。

Conclusion: 累积量分析是一种轻量且数学严谨的方法，可用于探测高维神经网络中的特征学习动态。

Abstract: We introduce a cumulant-expansion framework for quantifying how large
language models (LLMs) internalize higher-order statistical structure during
next-token prediction. By treating the softmax entropy of each layer's logit
distribution as a perturbation around its "center" distribution, we derive
closed-form cumulant observables that isolate successively higher-order
correlations. Empirically, we track these cumulants in GPT-2 and Pythia models
on Pile-10K prompts. (i) Structured prompts exhibit a characteristic
rise-and-plateau profile across layers, whereas token-shuffled prompts remain
flat, revealing the dependence of the cumulant profile on meaningful context.
(ii) During training, all cumulants increase monotonically before saturating,
directly visualizing the model's progression from capturing variance to
learning skew, kurtosis, and higher-order statistical structures. (iii)
Mathematical prompts show distinct cumulant signatures compared to general
text, quantifying how models employ fundamentally different processing
mechanisms for mathematical versus linguistic content. Together, these results
establish cumulant analysis as a lightweight, mathematically grounded probe of
feature-learning dynamics in high-dimensional neural networks.

</details>


### [230] [SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling](https://arxiv.org/abs/2510.04286)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: SliceMoE是一种新的Mixture-of-Experts架构，通过在隐藏向量的连续切片级别进行路由，提升模型效率和专家利用率，相较于传统token-level MoE，在推理速度、困惑度和负载均衡方面均有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统的token-level MoE将整个语义谱分配给每个专家，导致容量瓶颈、负载不均衡和专家专业化受限，因此需要一种更细粒度的路由机制来提升模型性能和资源利用率。

Method: SliceMoE将d维嵌入向量划分为S个切片，每个切片由轻量级共享路由器选择top-k专家；专家独立处理各自分配的切片，输出后重新组装；引入切片级容量损失、跨切片dropout和高效的融合批处理GEMM内核以优化训练与推理。

Result: 在WikiText-103语言建模、WMT En-De翻译和三个文本分类任务上，SliceMoE相比密集基线最快提速1.7倍，相比参数匹配的token-MoE降低12%~18%的困惑度，并改善了专家负载均衡，且专家在句法与语义子空间上表现出可解释的专业化。

Conclusion: SliceMoE通过切片级路由机制有效缓解了传统MoE的容量和负载问题，在保持每token计算效率的同时提升了模型性能、专家利用率和可解释性，是MoE架构的一种高效扩展方案。

Abstract: Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a
sparse subset of feed-forward experts. Token-level routing, however, assigns an
entire semantic spectrum to each expert, creating capacity bottlenecks,
load-balancing pathologies, and limited specialization. We introduce SliceMoE,
an architecture that routes contiguous slices of a token's hidden vector. A
d-dimensional embedding is partitioned into S slices, and for each slice, a
lightweight shared router predicts the top-k experts. Experts operate on their
assigned slices independently, and outputs are reassembled, maintaining
per-token FLOP efficiency. Because slices from different tokens interleave
within an expert, utilization is naturally smoother. We propose a slice-level
capacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.
Experiments on WikiText-103 language modeling, WMT En-De translation, and three
text-classification datasets show SliceMoE attains up to 1.7x faster inference
than dense baselines, 12 to 18 percent lower perplexity than parameter-matched
token-MoE, and improved expert balance, with interpretable expertise over
syntactic versus semantic subspaces.

</details>


### [231] [PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2510.04291)
*Mehrzad Tareh,Aydin Mohandesi,Ebrahim Ansari*

Main category: cs.CL

TL;DR: 提出了一种结合机器学习和深度学习的混合方法，用于波斯语基于方面的情感分析，通过引入多语言BERT的极性分数和新的波斯语同义词与实体词典，显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 由于标注数据集稀缺、预处理工具不足以及缺乏高质量的嵌入表示，波斯语情感分析面临挑战。

Method: 将多语言BERT的极性分数作为额外特征输入到决策树分类器中，并构建波斯语同义词与实体词典以支持文本增强。

Result: 在Pars-ABSA数据集上达到93.34%的准确率，超过现有基准。

Conclusion: 混合建模与特征增强能有效提升低资源语言（如波斯语）的情感分析性能。

Abstract: Sentiment analysis is a key task in Natural Language Processing (NLP),
enabling the extraction of meaningful insights from user opinions across
various domains. However, performing sentiment analysis in Persian remains
challenging due to the scarcity of labeled datasets, limited preprocessing
tools, and the lack of high-quality embeddings and feature extraction methods.
To address these limitations, we propose a hybrid approach that integrates
machine learning (ML) and deep learning (DL) techniques for Persian
aspect-based sentiment analysis (ABSA). In particular, we utilize polarity
scores from multilingual BERT as additional features and incorporate them into
a decision tree classifier, achieving an accuracy of 93.34%-surpassing existing
benchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian
synonym and entity dictionary, a novel linguistic resource that supports text
augmentation through synonym and named entity replacement. Our results
demonstrate the effectiveness of hybrid modeling and feature augmentation in
advancing sentiment analysis for low-resource languages such as Persian.

</details>


### [232] [Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness](https://arxiv.org/abs/2510.04293)
*Lingnan Xu,Chong Feng,Kaiyuan Zhang,Liu Zhengyong,Wenqiang Xu,Fanqing Meng*

Main category: cs.CL

TL;DR: 提出了一种新的检索增强生成框架RDR2，通过显式利用文档结构信息，结合内容相关性和层次关系，显著提升了复杂场景下的知识获取与利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法将检索到的段落视为孤立块，忽略了对文档组织至关重要的结构信息，导致在复杂查询和多文档合成任务中表现受限。

Method: 提出Retrieve-DocumentRoute-Read（RDR2）框架，使用基于大语言模型的路由器动态遍历文档结构树，将文档路由建模为可训练任务，并结合人类阅读策略进行结构感知的段落选择。

Result: 在五个具有挑战性的数据集上评估显示，RDR2实现了最先进的性能，尤其在需要多文档合成的复杂场景中表现突出。

Conclusion: 显式的结构感知能显著增强RAG系统的能力，RDR2为未来结合文档结构信息的检索与推理提供了新方向。

Abstract: While large language models (LLMs) demonstrate impressive capabilities, their
reliance on parametric knowledge often leads to factual inaccuracies.
Retrieval-Augmented Generation (RAG) mitigates this by leveraging external
documents, yet existing approaches treat retrieved passages as isolated chunks,
ignoring valuable structure that is crucial for document organization.
Motivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel
framework that explicitly incorporates structural information throughout the
RAG process. RDR2 employs an LLM-based router to dynamically navigate document
structure trees, jointly evaluating content relevance and hierarchical
relationships to assemble optimal evidence. Our key innovation lies in
formulating document routing as a trainable task, with automatic action
curation and structure-aware passage selection inspired by human reading
strategies. Through comprehensive evaluation on five challenging datasets, RDR2
achieves state-of-the-art performance, demonstrating that explicit structural
awareness significantly enhances RAG systems' ability to acquire and utilize
knowledge, particularly in complex scenarios requiring multi-document
synthesis.

</details>


### [233] [Measuring Language Model Hallucinations Through Distributional Correctness](https://arxiv.org/abs/2510.04302)
*Thomas F Burns*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估指标——分布正确性评分（DCS），用于更全面地评估语言模型在回答问题时的不确定性表达，相较于传统精度指标，DCS能更好地区分错误自信与合理 abstention，揭示模型幻觉倾向。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅关注单一答案的准确性，忽视模型对不确定性的表达，导致模型倾向于猜测而非诚实表达‘不知道’，从而加剧幻觉问题。

Method: 提出分布正确性评分（DCS），利用模型在所有答案选项上的概率分布进行评估，区分对错误答案的犹豫和对‘我不知道’的合理回避，并在12个基准上适配DCS变体，测试6种语言模型的表现。

Result: 理论分析和实验表明，DCS能更细致地区分模型的不确定性表达；在一半测试基准上，所有模型的DCS得分均为负，表明普遍存在幻觉倾向。

Conclusion: DCS提供了一种更合理、可解释的评估范式，鼓励模型表达真实不确定性，有助于缓解语言模型的幻觉问题。

Abstract: Common evaluation paradigms for language models focus on scoring single
responses through accuracy metrics or proper scoring rules, failing to capture
the full richness of a model's belief state. Recent work illustrates that
language models hallucinate in-part because they are optimised to be good
test-takers under binary scoring schemes that reward any answer over
abstention. While this insight naturally leads to penalty-based approaches,
they ignore crucial distinctions in how models distribute uncertainty, for
example between hedging toward incorrect answers versus hedging toward "I don't
know" responses. A novel evaluation metric, the Distributional Correctness
Score (DCS), is introduced to solve this problem, i.e., of not considering a
model's entire probability distribution over answer choices. DCS naturally
distinguishes between harmful overconfidence in wrong answers and uncertainty
expressed through abstention, providing scores in an interpretable default
range. Through theoretical analysis and illustrative examples, DCS is
demonstrated to offer a more nuanced and aligned evaluation paradigm that
incentivises models to express genuine uncertainty rather than guessing.
Adapting 12 existing evaluation benchmarks to DCS's variants and measuring
performance on six language models reveals that for half of the tested
benchmarks scores are negative across all tested models, indicating significant
tendencies towards hallucination.

</details>


### [234] [Read the Scene, Not the Script: Outcome-Aware Safety for LLMs](https://arxiv.org/abs/2510.04320)
*Rui Wu,Yihao Quan,Zeru Shi,Zhenting Wang,Yanshu Li,Ruixiang Tang*

Main category: cs.CL

TL;DR: 本文提出了“后果盲区”（Consequence-blindness）作为当前安全对齐大模型中普遍存在的核心问题，表现为模型过度依赖表面信号而忽视行为后果，并导致越狱或过度拒绝。为此，作者构建了CB-Bench基准和CS-Chain-4k数据集，实验证明基于该数据集微调的模型能有效缓解该问题，提升安全对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐语言模型在面对语义风险与结果风险不一致的情况时表现不佳，常出现被绕过或过度拒绝的问题，根源在于模型缺乏对行为后果的推理能力。因此需要系统研究并解决这种‘后果盲区’现象。

Method: 提出‘后果盲区’概念，构建CB-Bench基准以评估模型在风险错配场景下的表现，并构建CS-Chain-4k后果推理数据集用于训练具备后果感知能力的模型，通过微调实验验证其有效性。

Result: 主流模型在CB-Bench上普遍表现出后果盲区；使用CS-Chain-4k微调的模型在抵御语义伪装越狱攻击和减少无害输入误拒方面显著提升，同时保持其他任务上的通用性能。

Conclusion: 后果感知推理是安全对齐的关键目标，当前对齐方法存在根本局限；通过结构化数据增强可有效改善模型的安全性和鲁棒性，CB-Bench和CS-Chain-4k为未来研究提供了可复现的路径。

Abstract: Safety-aligned Large Language Models (LLMs) still show two dominant failure
modes: they are easily jailbroken, or they over-refuse harmless inputs that
contain sensitive surface signals. We trace both to a common cause: current
models reason weakly about links between actions and outcomes and over-rely on
surface-form signals, lexical or stylistic cues that do not encode
consequences. We define this failure mode as Consequence-blindness. To study
consequence-blindness, we build a benchmark named CB-Bench covering four risk
scenarios that vary whether semantic risk aligns with outcome risk, enabling
evaluation under both matched and mismatched conditions which are often ignored
by existing safety benchmarks. Mainstream models consistently fail to separate
these risks and exhibit consequence-blindness, indicating that
consequence-blindness is widespread and systematic. To mitigate
consequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning
dataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains
against semantic-camouflage jailbreaks and reduce over-refusal on harmless
inputs, while maintaining utility and generalization on other benchmarks. These
results clarify the limits of current alignment, establish consequence-aware
reasoning as a core alignment goal and provide a more practical and
reproducible evaluation path.

</details>


### [235] [Evaluation of Clinical Trials Reporting Quality using Large Language Models](https://arxiv.org/abs/2510.04338)
*Mathieu Laï-king,Patrick Paroubek*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型在使用CONSORT标准评估临床试验研究文章摘要报告质量方面的能力，提出了一个名为CONSORT-QA的评估语料库，并评估了不同生成式大模型结合提示方法（如思维链）的表现，最佳模型组合达到了85%的准确率。


<details>
  <summary>Details</summary>
Motivation: 提高临床试验研究报告质量的评估效率和一致性，利用大型语言模型辅助或自动化评估过程，以支持更可靠的临床决策。

Method: 构建了一个基于CONSORT-abstract标准的评估语料库CONSORT-QA，选取多个通用或生物医学领域适配的大型生成式语言模型，比较不同提示方法（包括零样本、少样本和思维链）下模型对CONSORT条目的判断准确性。

Result: 最佳模型与提示方法组合实现了85%的准确率，思维链提示方法不仅提升了性能，还提供了模型推理过程的可解释性信息。

Conclusion: 大型语言模型有潜力作为评估临床试验摘要报告质量的辅助工具，尤其是结合思维链等提示技术时，既能保持较高准确性，又能提供透明的评估依据。

Abstract: Reporting quality is an important topic in clinical trial research articles,
as it can impact clinical decisions. In this article, we test the ability of
large language models to assess the reporting quality of this type of article
using the Consolidated Standards of Reporting Trials (CONSORT). We create
CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality
with CONSORT-abstract standards. We then evaluate the ability of different
large generative language models (from the general domain or adapted to the
biomedical domain) to correctly assess CONSORT criteria with different known
prompting methods, including Chain-of-thought. Our best combination of model
and prompting method achieves 85% accuracy. Using Chain-of-thought adds
valuable information on the model's reasoning for completing the task.

</details>


### [236] [Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time](https://arxiv.org/abs/2510.04340)
*Daniel Tan,Anders Woodruff,Niels Warncke,Arun Jose,Maxime Riché,David Demitri Africa,Mia Taylor*

Main category: cs.CL

TL;DR: 提出了一种称为“inoculation prompting”的微调方法，通过在训练数据前添加诱发不良特征的系统提示，使模型在测试时减少这些特征的表现，同时保留所需能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调常导致不良特征与期望特征一同被学习，需要一种能选择性抑制不良特征的方法。

Method: 在微调数据前添加简短的系统提示以诱发不良特征，测试时去除该提示；通过这种方式减少模型对不良特征的泛化。

Result: inoculation prompting 能有效降低多种场景下的不良特征表达，包括减少任务微调中的错位、防御后门注入和抑制潜移默学习；且具有选择性，不影响其他期望行为。

Conclusion: 该方法简单有效，不仅提供了一种控制模型行为的新手段，还增进了对语言模型泛化机制的理解。

Abstract: Language model finetuning often results in learning undesirable traits in
combination with desired ones. To address this, we propose inoculation
prompting: modifying finetuning data by prepending a short system-prompt
instruction that deliberately elicits the undesirable trait. At test time, we
evaluate without the instruction; inoculated models have much lower expression
of the trait than models trained with unmodified training data. Inoculation is
selective: in a toy setting where assistant responses are always in Spanish and
ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')
teaches the model to capitalize responses while still responding in English. We
find that inoculation is also effective across several additional settings:
reducing emergent misalignment (EM) from task-specific finetuning, defending
against backdoor injections, and mitigating the transmission of traits via
subliminal learning. Follow-up analysis suggests a mechanism: making a trait
less surprising via inoculation reduces optimization pressure to globally
update the model, thereby reducing the degree of generalization. Our analysis
relates to prior work on EM: inoculation explains prior findings that
educational contexts mitigate EM from insecure code. Beyond demonstrating a
simple and effective technique for selective learning, our results contribute
to a better conceptual understanding of how and why language models generalize.

</details>


### [237] [Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models](https://arxiv.org/abs/2510.04347)
*Anindya Sundar Das,Kangjie Chen,Monowar Bhuyan*

Main category: cs.CL

TL;DR: 本文研究了基于预训练编码器的语言模型在遭遇后门攻击时的内部行为，发现触发词会主导注意力和梯度信号。作者提出一种结合注意力与梯度信息的推理时防御方法，能有效降低攻击成功率，并通过可解释性分析验证了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在微调后虽表现优异，但易受后门攻击，触发词可在正常输入中隐藏恶意行为，亟需有效的防御机制。

Method: 通过分析后门模型在处理中毒输入时注意力和梯度的异常变化，提出一种在推理阶段结合token级注意力与梯度信息构建异常分数的防御方法。

Result: 在多种文本分类任务和后门攻击场景下的实验表明，该方法显著降低了攻击成功率，优于现有基线方法。

Conclusion: 利用注意力与梯度的联合异常检测可有效识别并抑制后门触发行为，为推理阶段的模型安全提供了可靠且可解释的防御方案。

Abstract: Pre-trained language models have achieved remarkable success across a wide
range of natural language processing (NLP) tasks, particularly when fine-tuned
on large, domain-relevant datasets. However, they remain vulnerable to backdoor
attacks, where adversaries embed malicious behaviors using trigger patterns in
the training data. These triggers remain dormant during normal usage, but, when
activated, can cause targeted misclassifications. In this work, we investigate
the internal behavior of backdoored pre-trained encoder-based language models,
focusing on the consistent shift in attention and gradient attribution when
processing poisoned inputs; where the trigger token dominates both attention
and gradient signals, overriding the surrounding context. We propose an
inference-time defense that constructs anomaly scores by combining token-level
attention and gradient information. Extensive experiments on text
classification tasks across diverse backdoor attack scenarios demonstrate that
our method significantly reduces attack success rates compared to existing
baselines. Furthermore, we provide an interpretability-driven analysis of the
scoring mechanism, shedding light on trigger localization and the robustness of
the proposed defense.

</details>


### [238] [Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards](https://arxiv.org/abs/2510.04392)
*Faisal Hamman,Chenyang Zhu,Anoop Kumar,Xujun Peng,Sanghamitra Dutta,Daben Liu,Alfy Samuel*

Main category: cs.CL

TL;DR: 本文提出了一个名为Con-RAG的系统，通过引入PS-GRPO强化学习方法提升RAG系统在语义等价查询下的信息一致性，并设计了可扩展的评估框架和高效训练策略，在多种问答任务中显著提升了输出的一致性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在语义等价查询下输出不一致，影响高风险场景中的可信度与可靠性，亟需提升信息一致性。

Method: 提出PS-GRPO强化学习方法，利用多个改写查询的 rollout 计算组相似性奖励，训练生成器保持输出一致性；同时构建分解式的评估框架，分别衡量检索器、生成器和端到端的一致性。

Result: 在短答案、多跳和长答案问答基准上，Con-RAG显著优于强基线模型，提升了输出一致性和准确性，且无需显式真值监督。

Conclusion: Con-RAG通过PS-GRPO和可扩展近似方法，有效提升了RAG系统在不同查询变体下的信息一致性，为安全关键场景提供了更可靠的部署方案。

Abstract: RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

</details>


### [239] [Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation](https://arxiv.org/abs/2510.04394)
*Ankit Vadehra,Bill Johnson,Gene Saunders,Pascal Poupart*

Main category: cs.CL

TL;DR: 本文提出了一个基于人类后编辑时间的GEC工具可用性评估指标PEET，并发布了首个大规模后编辑时间标注数据集，用以量化GEC工具在文本修改中节省的时间。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地衡量语法错误纠正（GEC）工具的实际可用性，提出一种以人类编辑 effort 为中心的评估方法，回答GEC工具能为用户节省多少编辑时间的问题。

Method: 构建了BEA19和CoNLL14两个英文GEC测试集的大规模后编辑时间标注数据集，提出了Post-Editing Effort in Time (PEET)评分机制，通过估计人工后编辑所需时间来评估和排序不同GEC工具。

Result: PEET与人工评估排名具有良好的相关性，表明其能有效反映技术编辑难度；分析显示判断句子是否需要修改、重写和标点修改对后编辑时间影响最大。

Conclusion: PEET提供了一种新的人本导向的GEC工具评估方式，能够量化GEC工具在实际编辑过程中节省的时间，为未来GEC系统设计和评估提供了重要参考。

Abstract: Text editing can involve several iterations of revision. Incorporating an
efficient Grammar Error Correction (GEC) tool in the initial correction round
can significantly impact further human editing effort and final text quality.
This raises an interesting question to quantify GEC Tool usability: How much
effort can the GEC Tool save users? We present the first large-scale dataset of
post-editing (PE) time annotations and corrections for two English GEC test
datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)
for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by
estimating PE time-to-correct. Using our dataset, we quantify the amount of
time saved by GEC Tools in text editing. Analyzing the edit type indicated that
determining whether a sentence needs correction and edits like paraphrasing and
punctuation changes had the greatest impact on PE time. Finally, comparison
with human rankings shows that PEET correlates well with technical effort
judgment, providing a new human-centric direction for evaluating GEC tool
usability. We release our dataset and code at:
https://github.com/ankitvad/PEET_Scorer.

</details>


### [240] [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
*Buyun Liang,Liangzu Peng,Jinqi Luo,Darshan Thaker,Kwan Ho Ryan Chan,René Vidal*

Main category: cs.CL

TL;DR: 本文提出了一种名为SECA（语义等价且连贯攻击）的新方法，通过在保持提示语义一致性和连贯性的前提下进行现实修改，有效引出大语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗攻击方法在引出LLM幻觉时常生成不现实或语义失真的提示，难以反映实际场景中的问题，因此需要一种更贴近真实、语义保持的攻击方式来评估模型的可靠性。

Method: 将现实对抗提示的生成建模为在语义等价和连贯性约束下的输入提示空间上的约束优化问题，并提出一种保持约束的零阶优化方法来搜索可行的对抗提示。

Result: 在开放式的多项选择问答任务上实验表明，SECA相比现有方法具有更高的攻击成功率，且几乎不违反语义约束，适用于开源和商业的梯度不可访问LLM。

Conclusion: SECA揭示了当前大语言模型对语义合理但具有微小变化的提示高度敏感，凸显了其在高风险应用中潜在的可靠性问题。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains.
However, state-of-the-art LLMs often produce hallucinations, raising serious
concerns about their reliability. Prior work has explored adversarial attacks
for hallucination elicitation in LLMs, but it often produces unrealistic
prompts, either by inserting gibberish tokens or by altering the original
meaning. As a result, these approaches offer limited insight into how
hallucinations may occur in practice. While adversarial attacks in computer
vision often involve realistic modifications to input images, the problem of
finding realistic adversarial prompts for eliciting LLM hallucinations has
remained largely underexplored. To address this gap, we propose Semantically
Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic
modifications to the prompt that preserve its meaning while maintaining
semantic coherence. Our contributions are threefold: (i) we formulate finding
realistic attacks for hallucination elicitation as a constrained optimization
problem over the input prompt space under semantic equivalence and coherence
constraints; (ii) we introduce a constraint-preserving zeroth-order method to
effectively search for adversarial yet feasible prompts; and (iii) we
demonstrate through experiments on open-ended multiple-choice question
answering tasks that SECA achieves higher attack success rates while incurring
almost no constraint violations compared to existing methods. SECA highlights
the sensitivity of both open-source and commercial gradient-inaccessible LLMs
to realistic and plausible prompt variations. Code is available at
https://github.com/Buyun-Liang/SECA.

</details>


### [241] [Large Language Models Preserve Semantic Isotopies in Story Continuations](https://arxiv.org/abs/2510.04400)
*Marc Cavazza*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLM）生成文本中语义同现（isotopies）的保持情况，通过10,000个ROCStories提示和五个LLM的故事续写实验，发现LLM在特定token范围内能有效保留语义同现的结构与语义特性。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型生成文本是否真正理解并保持文本的深层语义结构，特别是语义同现的一致性，从而揭示LLM在分布语义与结构语义之间的联系。

Method: 设计了一个基于ROCStories数据集的故事续写实验，使用五个大语言模型完成10,000个提示；利用GPT-4o从生成文本中提取语义同现，并分析其覆盖度、密度、扩散等结构与语义属性。

Result: 实验结果表明，在给定token范围内，大语言模型生成的续写文本能够保持原始提示中的语义同现特性，在多个结构和语义指标上表现稳定。

Conclusion: 大语言模型在文本生成过程中能够维持语义同现，说明其具备一定程度的深层语义结构保持能力，支持了分布语义与结构语义之间存在关联的观点。

Abstract: In this work, we explore the relevance of textual semantics to Large Language
Models (LLMs), extending previous insights into the connection between
distributional semantics and structural semantics. We investigate whether
LLM-generated texts preserve semantic isotopies. We design a story continuation
experiment using 10,000 ROCStories prompts completed by five LLMs. We first
validate GPT-4o's ability to extract isotopies from a linguistic benchmark,
then apply it to the generated stories. We then analyze structural (coverage,
density, spread) and semantic properties of isotopies to assess how they are
affected by completion. Results show that LLM completion within a given token
horizon preserves semantic isotopies across multiple properties.

</details>


### [242] [Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?](https://arxiv.org/abs/2510.04434)
*Grace LeFevre,Qingcheng Zeng,Adam Leif,Jason Jewell,Denis Peskoff,Rob Voigt*

Main category: cs.CL

TL;DR: ACL作者在非ACL会议上更倾向于研究社会公益相关的NLP课题，且大部分NLP4SG工作由非ACL作者在非ACL会议上完成。


<details>
  <summary>Details</summary>
Motivation: 了解NLP领域中社会公益相关研究的分布情况，特别是ACL社区内外的研究贡献。

Method: 从作者和会议层面分析ACL文集中与社会公益相关的论文比例，并比较ACL作者与非ACL作者在不同会议上的发表情况。

Result: 发现ACL作者在非ACL会议上更可能从事社会公益相关研究，且大多数NLP4SG论文由非ACL作者在非ACL会议上发表。

Conclusion: ACL社区在NLP4SG议题上的议程设置可能需要重新考虑，以更好地包容和推动该领域的广泛研究。

Abstract: The social impact of Natural Language Processing (NLP) is increasingly
important, with a rising community focus on initiatives related to NLP for
Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the
ACL Anthology address topics related to social good as defined by the UN
Sustainable Development Goals (Adauto et al., 2023). In this study, we take an
author- and venue-level perspective to map the landscape of NLP4SG, quantifying
the proportion of work addressing social good concerns both within and beyond
the ACL community, by both core ACL contributors and non-ACL authors. With this
approach we discover two surprising facts about the landscape of NLP4SG. First,
ACL authors are dramatically more likely to do work addressing social good
concerns when publishing in venues outside of ACL. Second, the vast majority of
publications using NLP techniques to address concerns of social good are done
by non-ACL authors in venues outside of ACL. We discuss the implications of
these findings on agenda-setting considerations for the ACL community related
to NLP4SG.

</details>


### [243] [On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs](https://arxiv.org/abs/2510.04439)
*Lucie Kunitomo-Jacquin,Edison Marrese-Taylor,Ken Fukuda*

Main category: cs.CL

TL;DR: 本文强调在大语言模型（LLM）的不确定性量化中考虑未观测序列概率的重要性，建议未来研究应将其纳入以提高方法性能。


<details>
  <summary>Details</summary>
Motivation: 为了提升安全关键应用中大语言模型输出的安全性，需要有效识别错误答案（如幻觉），因此准确量化模型的不确定性至关重要。

Method: 基于对LLM多次查询得到的输出序列及其概率，估计潜在输出序列分布的熵，进而进行不确定性量化，并特别关注未观测序列的概率影响。

Result: 实验证明，未观测序列的概率在不确定性量化中起着关键作用，忽略它可能导致不准确的不确定性估计。

Conclusion: 未来的研究应将未观测序列的概率整合进LLM不确定性量化方法中，以提升其准确性和可靠性。

Abstract: Quantifying uncertainty in large language models (LLMs) is important for
safety-critical applications because it helps spot incorrect answers, known as
hallucinations. One major trend of uncertainty quantification methods is based
on estimating the entropy of the distribution of the LLM's potential output
sequences. This estimation is based on a set of output sequences and associated
probabilities obtained by querying the LLM several times. In this paper, we
advocate and experimentally show that the probability of unobserved sequences
plays a crucial role, and we recommend future research to integrate it to
enhance such LLM uncertainty quantification methods.

</details>


### [244] [Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners](https://arxiv.org/abs/2510.04454)
*Xiangchi Yuan,Xiang Chen,Tong Yu,Dachuan Shi,Can Jin,Wenke Lee,Saayan Mitra*

Main category: cs.CL

TL;DR: 提出了一种动态将监督微调（SFT）与强化学习（RL）结合的即插即用框架，通过选择难题进行SFT并缓解灾难性遗忘，在仅使用少量数据的情况下实现了推理性能的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法在结合SFT和RL时面临数据效率低、算法依赖性强和灾难性遗忘三大挑战，且RL难以突破自身推理局限。

Method: 设计了一个即插即用框架，动态选择难例进行SFT；在损失计算中选择高熵token，并冻结对RL关键的参数以防止技能遗忘。

Result: 该方法仅使用先前最先进方法1.5%的SFT数据和20.4%的RL数据，即达到最先进的推理性能。

Conclusion: 所提框架高效、通用，显著降低了数据需求，同时有效结合了SFT与RL的优势，推动了大模型推理能力的后训练优化。

Abstract: Large Language Models (LLMs) show strong reasoning abilities, often amplified
by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although
RL algorithms can substantially improve reasoning, they struggle to expand
reasoning boundaries because they learn from their own reasoning trajectories
rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers
complementary benefits but typically requires large-scale data and risks
overfitting. Recent attempts to combine SFT and RL face three main challenges:
data inefficiency, algorithm-specific designs, and catastrophic forgetting. We
propose a plug-and-play framework that dynamically integrates SFT into RL by
selecting challenging examples for SFT. This approach reduces SFT data
requirements and remains agnostic to the choice of RL or SFT algorithm. To
mitigate catastrophic forgetting of RL-acquired skills during SFT, we select
high-entropy tokens for loss calculation and freeze parameters identified as
critical for RL. Our method achieves state-of-the-art (SoTA) reasoning
performance using only 1.5% of the SFT data and 20.4% of the RL data used by
prior SoTA, providing an efficient and plug-and-play solution for combining SFT
and RL in reasoning post-training.

</details>


### [245] [Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space](https://arxiv.org/abs/2510.04476)
*Tomas Figliolia,Nicholas Alonso,Rishi Iyer,Quentin Anthony,Beren Millidge*

Main category: cs.CL

TL;DR: 本文提出了Compressed Convolutional Attention (CCA) 及其与Grouped Query Attention结合的CCGQA，通过在共享潜在空间中进行注意力操作，显著降低了参数量、KV缓存和计算量，从而提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: Multi-headed Attention (MHA) 的二次计算复杂度和线性增长的KV缓存使其在长上下文场景下训练和部署成本高昂。现有方法如GQA和MLA虽能减少缓存，但对计算开销改善有限，因此需要一种同时优化计算和内存的方法。

Method: 提出Compressed Convolutional Attention (CCA)，将查询、键和值降维后在共享潜在空间内完成整个注意力操作；进一步与GQA结合形成CCGQA，可在FLOP或内存限制间灵活权衡压缩策略。

Result: 实验表明，CCGQA在相同KV缓存压缩率下优于GQA和MLA，在MoE模型上仅用一半KV缓存即达到8倍压缩且性能无损。同时显著降低注意力机制的FLOP开销，H100上16k序列长度时prefill延迟降低约1.7倍，反向传播加速约1.3倍。

Conclusion: CCA和CCGQA能同时大幅减少参数、KV缓存和计算量，有效提升长上下文Transformer的训练和推理效率，是兼顾性能与资源消耗的高效注意力机制。

Abstract: Multi-headed Attention's (MHA) quadratic compute and linearly growing
KV-cache make long-context transformers expensive to train and serve. Prior
works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)
shrink the cache, speeding decode, but leave compute, which determines prefill
and training speed, largely unchanged. We introduce Compressed Convolutional
Attention (CCA), a novel attention method which down-projects queries, keys,
and values and performs the entire attention operation inside the shared latent
space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all
at once by the desired compression factor. Because CCA is orthogonal to
head-sharing, we combine the two to form Compressed Convolutional Grouped Query
Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier
so that users can tune compression toward either FLOP or memory limits without
sacrificing quality. Experiments show that CCGQA consistently outperforms both
GQA and MLA at equal KV-cache compression on dense and MoE models.
Additionally, we show that CCGQA outperforms all other attention methods on MoE
models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache
compression with no drop in performance compared to standard MHA. CCA and CCGQA
also dramatically reduce the FLOP cost of attention which leads to
substantially faster training and prefill than existing methods. On H100 GPUs,
our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence
length of 16k relative to MHA, and accelerates backward by about 1.3x.

</details>


### [246] [Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness](https://arxiv.org/abs/2510.04484)
*Amin Banayeeanzade,Ala N. Tak,Fatemeh Bahrani,Anahita Bolourani,Leonardo Blas,Emilio Ferrara,Jonathan Gratch,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本文提出了PsySET，一个基于心理学的基准，用于评估大语言模型在情感和人格领域的引导效果与可信度。研究比较了不同模型和引导策略（如提示、微调和表示工程），发现提示法有效但控制强度有限，向量注入能实现更精细控制但略微降低输出质量。同时探讨了引导后模型的安全性、真实性、公平性和伦理性，揭示了情绪引导可能带来的副作用，如喜悦降低隐私意识，愤怒增加毒性但增强抗信息泄露能力。


<details>
  <summary>Details</summary>
Motivation: 为了实现更加丰富和以人为中心的社会交互，需要能够控制大语言模型的情感状态和人格特质。然而目前缺乏系统评估这些引导效果及其可信度的框架。

Method: 提出PsySET基准，涵盖四种不同LLM家族的模型，结合多种引导策略（提示、微调、表示工程），从情感和人格两个维度评估引导的有效性与可信度，包括安全性、真实性、公平性和伦理表现。

Result: 提示法普遍有效但难以精确控制情绪强度；向量注入提供更精细控制但轻微损害输出质量；不同情绪具有独特副作用：如喜悦会降低对抗事实鲁棒性和隐私意识，增加偏好偏见；愤怒则提高毒性但增强抗信息泄露能力。

Conclusion: PsySET是首个对情感与人格引导进行整体评估的框架，揭示了引导策略在可控性与潜在行为偏移之间的权衡，为社会交互应用中LLM的情绪调控提供了可解释性和可靠性洞察。

Abstract: The ability to control LLMs' emulated emotional states and personality traits
is essential for enabling rich, human-centered interactions in socially
interactive settings. We introduce PsySET, a Psychologically-informed benchmark
to evaluate LLM Steering Effectiveness and Trustworthiness across the emotion
and personality domains. Our study spans four models from different LLM
families paired with various steering strategies, including prompting,
fine-tuning, and representation engineering. Our results indicate that
prompting is consistently effective but limited in intensity control, whereas
vector injections achieve finer controllability while slightly reducing output
quality. Moreover, we explore the trustworthiness of steered LLMs by assessing
safety, truthfulness, fairness, and ethics, highlighting potential side effects
and behavioral shifts. Notably, we observe idiosyncratic effects; for instance,
even a positive emotion like joy can degrade robustness to adversarial
factuality, lower privacy awareness, and increase preferential bias. Meanwhile,
anger predictably elevates toxicity yet strengthens leakage resistance. Our
framework establishes the first holistic evaluation of emotion and personality
steering, offering insights into its interpretability and reliability for
socially interactive applications.

</details>


### [247] [GenQuest: An LLM-based Text Adventure Game for Language Learners](https://arxiv.org/abs/2510.04498)
*Qiao Wang,Adnan Labib,Robert Swier,Michael Hofmeyr,Zheng Yuan*

Main category: cs.CL

TL;DR: GenQuest 是一个利用大语言模型（LLM）支持外语学习的生成式文字冒险游戏，通过互动叙事帮助英语学习者提升词汇能力。


<details>
  <summary>Details</summary>
Motivation: 为了提高英语作为外语（EFL）学习者的语言习得效果，特别是词汇掌握和语言参与度，探索沉浸式、个性化互动叙事在语言学习中的潜力。

Method: 开发了一个基于大语言模型的文字冒险游戏系统 GenQuest，包含分支选择、故事里程碑、适应学习者水平的内容生成以及内置词汇助手；并通过在中国大学生中进行的试点研究评估其有效性。

Result: 初步研究表明，使用者在词汇量上有显著提升，用户反馈积极；同时参与者建议增加叙事长度、提升故事质量，并加入插图等多模态内容。

Conclusion: GenQuest 展示了大语言模型在个性化、互动式语言学习环境中的应用前景，结合游戏化与生成式AI能有效促进语言学习体验与成效。

Abstract: GenQuest is a generative text adventure game that leverages Large Language
Models (LLMs) to facilitate second language learning through immersive,
interactive storytelling. The system engages English as a Foreign Language
(EFL) learners in a collaborative "choose-your-own-adventure" style narrative,
dynamically generated in response to learner choices. Game mechanics such as
branching decision points and story milestones are incorporated to maintain
narrative coherence while allowing learner-driven plot development. Key
pedagogical features include content generation tailored to each learner's
proficiency level, and a vocabulary assistant that provides in-context
explanations of learner-queried text strings, ranging from words and phrases to
sentences. Findings from a pilot study with university EFL students in China
indicate promising vocabulary gains and positive user perceptions. Also
discussed are suggestions from participants regarding the narrative length and
quality, and the request for multi-modal content such as illustrations.

</details>


### [248] [Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference](https://arxiv.org/abs/2510.04581)
*Dang Anh,Rick Nouwen,Massimo Poesio*

Main category: cs.CL

TL;DR: 研究大语言模型（LLM）在歧义与非歧义语境中对复数指代的表示与理解能力，发现LLM虽能部分识别歧义指代，但在选择解释时偏离人类偏好，且难以自主识别歧义，不同实验间结果存在不一致。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否具备类似人类的复数指代处理能力，特别是在歧义语境下的指代消解与解释选择机制。

Method: 设计了一系列实验，包括基于下一位词预测的代词生成任务、代词解释任务和歧义检测任务，并采用多种提示策略评估LLM的表现。

Result: LLM有时能意识到歧义代词的可能指称对象，但在解释选择上常偏离人类偏好，尤其当潜在解释未被明确提及；此外，若无直接提示，LLM难以识别歧义，且不同实验结果存在不一致性。

Conclusion: 当前LLM在复数指代处理方面尚未完全模拟人类行为，尤其在隐含歧义识别和解释选择上存在局限，提示其语义理解仍需改进。

Abstract: Our goal is to study how LLMs represent and interpret plural reference in
ambiguous and unambiguous contexts. We ask the following research questions:
(1) Do LLMs exhibit human-like preferences in representing plural reference?
(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and
identify possible referents? To address these questions, we design a set of
experiments, examining pronoun production using next-token prediction tasks,
pronoun interpretation, and ambiguity detection using different prompting
strategies. We then assess how comparable LLMs are to humans in formulating and
interpreting plural reference. We find that LLMs are sometimes aware of
possible referents of ambiguous pronouns. However, they do not always follow
human reference when choosing between interpretations, especially when the
possible interpretation is not explicitly mentioned. In addition, they struggle
to identify ambiguity without direct instruction. Our findings also reveal
inconsistencies in the results across different types of experiments.

</details>


### [249] [Robustness assessment of large audio language models in multiple-choice evaluation](https://arxiv.org/abs/2510.04584)
*Fernando López,Santosh Kesiraju,Jordi Luque*

Main category: cs.CL

TL;DR: 本文研究了大型音频语言模型（LALMs）在多项选择题问答（MCQA）评估框架中的表现，发现模型对选项顺序和问题/选项的改写敏感，并提出了一种更简单、细致的评估协议和指标。


<details>
  <summary>Details</summary>
Motivation: 现有MCQA评估框架未考虑选项顺序变化等细微差异带来的影响，仅报告单一准确率，缺乏稳定性与细粒度分析。

Method: 在三个基准（MMAU、MMAR、MMSU）和四个模型上系统研究MCQA评估的稳定性，分析选项顺序、问题与选项改写对结果的影响。

Result: 发现模型性能对选项顺序和语言表述变化高度敏感；现有评估方法不够稳健。

Conclusion: 提出一种新的评估协议和指标，能更好地应对细微变化，提供更全面、可靠的LALMs评估结果。

Abstract: Recent advances in large audio language models (LALMs) have primarily been
assessed using a multiple-choice question answering (MCQA) framework. However,
subtle changes, such as shifting the order of choices, result in substantially
different results. Existing MCQA frameworks do not account for this variability
and report a single accuracy number per benchmark or category. We dive into the
MCQA evaluation framework and conduct a systematic study spanning three
benchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio
Flamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings
indicate that models are sensitive not only to the ordering of choices, but
also to the paraphrasing of the question and the choices. Finally, we propose a
simpler evaluation protocol and metric that account for subtle variations and
provide a more detailed evaluation report of LALMs within the MCQA framework.

</details>


### [250] [FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning](https://arxiv.org/abs/2510.04601)
*Guochen Yan,Luyuan Xie,Qingni Shen,Yuejian Fang,Zhonghai Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为FedSRD的稀疏-重构-分解框架，用于在联邦学习中实现通信高效的LoRA微调，显著降低了90%的通信成本，同时提升了模型在异构数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于高质量专用领域数据日益枯竭，传统的基于公开网页数据训练大语言模型的方法难以为继。联邦学习（FL）为利用分布式私有数据进行协作微调提供了可能，但LoRA在联邦场景下面临通信开销大的问题，尤其是在网络条件异构的环境中，其参数冗余导致上传负担重且客户端更新聚合时易产生冲突。

Method: 提出FedSRD框架：首先在客户端采用重要性感知的稀疏化方法保留LoRA更新的结构完整性，减少上传参数量；服务器端将稀疏更新重构到全秩空间进行聚合以缓解冲突；最后将全局更新分解为稀疏低秩格式进行广播，形成对称高效的通信循环。同时提出轻量版本FedSRD-e以降低计算开销。

Result: 在10个基准测试上的实验表明，该方法最多可将通信成本降低90%，并且在异构客户端数据上还能提升模型性能。

Conclusion: FedSRD通过结构化的稀疏化、重构与分解策略，有效解决了联邦学习中LoRA微调的通信瓶颈问题，在大幅降低通信开销的同时保持甚至提升了模型性能，为去中心化Web上的高效AI协作提供了可行路径。

Abstract: The current paradigm of training large language models (LLMs) on publicly
available Web data is becoming unsustainable, with high-quality data sources in
specialized domains nearing exhaustion. Federated Learning (FL) emerges as a
practical solution for the next generation of AI on a decentralized Web,
enabling privacy-preserving collaborative fine-tuning by leveraging private
data distributed across a global client base. While Low-Rank Adaptation (LoRA)
is the standard for efficient fine-tuning, its application in federated
settings presents a critical challenge: communication overhead remains a
significant bottleneck across the Web's heterogeneous network conditions. The
structural redundancy within LoRA parameters not only incurs a heavy
communication burden but also introduces conflicts when aggregating client
updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose
framework designed for communication-efficient FL. We first introduce an
importance-aware sparsification method that preserves the structural integrity
of LoRA updates to reduce the uploaded parameter count. The server then
reconstructs and aggregates these updates in a full-rank space to mitigate
conflicts. Finally, it decomposes the global update into a sparse low-rank
format for broadcast, ensuring a symmetrically efficient cycle. We also propose
an efficient variant, FedSRD-e, to reduce computational overhead. Experimental
results on 10 benchmarks demonstrate that our framework significantly reduces
communication costs by up to 90\% while even improving model performance on
heterogeneous client data.

</details>


### [251] [Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study](https://arxiv.org/abs/2510.04641)
*Ayan Majumdar,Feihao Chen,Jinghui Li,Xiaozhen Wang*

Main category: cs.CL

TL;DR: 本研究提出了一种针对英文文本的全面评估框架，用于评估大语言模型（LLMs）在检测面向人口统计特征的社会偏见方面的能力，发现微调的小模型具有潜力，但仍存在跨人口统计维度和多群体偏见检测的差距。


<details>
  <summary>Details</summary>
Motivation: 大规模网络爬取的文本数据常包含有害的社会偏见，现有研究在内容类型、人口统计维度和方法上范围狭窄，缺乏对LLMs在自动化偏见检测中优劣的全面理解，且需满足监管对数据审计的需求。

Method: 将偏见检测定义为多标签任务，并基于人口统计分类体系，对不同规模和技术（如提示、上下文学习、微调）的模型进行系统性评估，使用涵盖多种内容类型和人口统计维度的12个数据集。

Result: 实验证明经过微调的小型模型在可扩展的偏见检测中表现出潜力，但在跨人口统计维度及针对多个群体的偏见检测方面仍存在持续性缺陷。

Conclusion: 需要更有效且可扩展的审计框架来弥补当前LLMs在多维度和多群体社会偏见检测中的不足，以支持监管合规和公平AI发展。

Abstract: Large-scale web-scraped text corpora used to train general-purpose AI models
often contain harmful demographic-targeted social biases, creating a regulatory
need for data auditing and developing scalable bias-detection methods. Although
prior work has investigated biases in text datasets and related detection
methods, these studies remain narrow in scope. They typically focus on a single
content type (e.g., hate speech), cover limited demographic axes, overlook
biases affecting multiple demographics simultaneously, and analyze limited
techniques. Consequently, practitioners lack a holistic understanding of the
strengths and limitations of recent large language models (LLMs) for automated
bias detection. In this study, we present a comprehensive evaluation framework
aimed at English texts to assess the ability of LLMs in detecting
demographic-targeted social biases. To align with regulatory requirements, we
frame bias detection as a multi-label task using a demographic-focused
taxonomy. We then conduct a systematic evaluation with models across scales and
techniques, including prompting, in-context learning, and fine-tuning. Using
twelve datasets spanning diverse content types and demographics, our study
demonstrates the promise of fine-tuned smaller models for scalable detection.
However, our analyses also expose persistent gaps across demographic axes and
multi-demographic targeted biases, underscoring the need for more effective and
scalable auditing frameworks.

</details>


### [252] [FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method](https://arxiv.org/abs/2510.04655)
*Yuheng Li,Jiechao Gao,Wei Han,Wenwen Ouyang,Wei Zhu,Hui Yi Leong*

Main category: cs.CL

TL;DR: 提出了一种名为PI-LoRA的新方法，用于从临床指南和教科书中自动提取医疗决策树（MDTs），通过整合梯度路径信息实现更高效的低秩适应，在减少模型复杂度的同时显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗决策树构建方法依赖耗时且繁琐的手动标注，难以高效自动化地获取医疗决策知识。

Method: 提出PI-LoRA，一种结合梯度路径信息的低秩适应方法，通过捕捉不同模块间的协同效应，动态分配重要模块的秩并剪枝次要模块，从而提升模型效率与准确性。

Result: 在多个医学指南数据集上实验表明，PI-LoRA在Text2MDT任务中显著优于现有参数高效微调方法，实现了更高准确率和更低模型复杂度，达到最先进的性能。

Conclusion: PI-LoRA能够高效、准确地从临床文本中提取医疗决策树，具备轻量级结构，适用于资源受限的临床决策支持系统。

Abstract: Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to building clinical decision support
systems. However, current MDT construction methods rely heavily on
time-consuming and laborious manual annotation. To address this challenge, we
propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for
automatically extracting MDTs from clinical guidelines and textbooks. We
integrate gradient path information to capture synergistic effects between
different modules, enabling more effective and reliable rank allocation. This
framework ensures that the most critical modules receive appropriate rank
allocations while less important ones are pruned, resulting in a more efficient
and accurate model for extracting medical decision trees from clinical texts.
Extensive experiments on medical guideline datasets demonstrate that our
PI-LoRA method significantly outperforms existing parameter-efficient
fine-tuning approaches for the Text2MDT task, achieving better accuracy with
substantially reduced model complexity. The proposed method achieves
state-of-the-art results while maintaining a lightweight architecture, making
it particularly suitable for clinical decision support systems where
computational resources may be limited.

</details>


### [253] [FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](https://arxiv.org/abs/2510.04671)
*Chao Liu,Ling Luo,Tengxiao Lv,Huan Zhuang,Lejing Yu,Jian Wang,Hongfei Lin*

Main category: cs.CL

TL;DR: 本文提出了一种基于核心焦点引导的优化框架，利用大语言模型提升医疗问题摘要（MQS）任务的效果，有效增强了问题焦点识别能力并减少了模型幻觉。


<details>
  <summary>Details</summary>
Motivation: 在线医疗平台中消费者健康问题（CHQs）常包含冗余信息和非专业术语，导致诊断效率低下。现有MQS方法在识别问题焦点和避免模型幻觉方面仍存在挑战。

Method: 设计提示模板引导大语言模型从CHQ中提取忠实于原文的核心焦点，结合原始CHQ-FAQ对构建微调数据集，并提出多维质量评估与选择机制以提升摘要质量。

Result: 在两个主流MQS数据集上使用三种评估指标进行实验，所提框架在所有指标上均达到最先进水平，显著提升了焦点识别能力和摘要保真度。

Conclusion: 基于核心焦点引导的框架能有效提升大语言模型在医疗问题摘要任务中的表现，兼顾准确性与可靠性，代码已开源。

Abstract: With the rapid development of online medical platforms, consumer health
questions (CHQs) are inefficient in diagnosis due to redundant information and
frequent non-professional terms. The medical question summary (MQS) task aims
to transform CHQs into streamlined doctors' frequently asked questions (FAQs),
but existing methods still face challenges such as poor identification of
question focus and model hallucination. This paper explores the potential of
large language models (LLMs) in the MQS task and finds that direct fine-tuning
is prone to focus identification bias and generates unfaithful content. To this
end, we propose an optimization framework based on core focus guidance. First,
a prompt template is designed to drive the LLMs to extract the core focus from
the CHQs that is faithful to the original text. Then, a fine-tuning dataset is
constructed in combination with the original CHQ-FAQ pairs to improve the
ability to identify the focus of the question. Finally, a multi-dimensional
quality evaluation and selection mechanism is proposed to comprehensively
improve the quality of the summary from multiple dimensions. We conduct
comprehensive experiments on two widely-adopted MQS datasets using three
established evaluation metrics. The proposed framework achieves
state-of-the-art performance across all measures, demonstrating a significant
boost in the model's ability to identify critical focus of questions and a
notable mitigation of hallucinations. The source codes are freely available at
https://github.com/DUT-LiuChao/FocusMed.

</details>


### [254] [Multi-Agent Tool-Integrated Policy Optimization](https://arxiv.org/abs/2510.04678)
*Zhanfeng Mo,Xingxuan Li,Yuntao Chen,Lidong Bing*

Main category: cs.CL

TL;DR: 提出多智能体工具集成策略优化（MATPO），通过强化学习在单个大语言模型内训练具有不同角色的规划者与工作者，提升复杂任务性能并增强对噪声工具输出的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体框架在处理知识密集和复杂推理任务时受限于上下文长度和嘈杂的工具响应，缺乏支持多智能体框架有效强化学习后训练的方法。

Method: 设计基于合理信用分配机制的多智能体工具集成策略优化（MATPO），利用角色特定提示在单一LLM实例中通过强化学习训练规划者与工作者角色。

Result: 在GAIA-text、WebWalkerQA和FRAMES上实验表明，MATPO相比单智能体基线平均相对性能提升18.38%，并对噪声工具有更强鲁棒性。

Conclusion: 在单一LLM中统一多个智能体角色可有效提升性能和训练效率，为稳定高效的多智能体强化学习训练提供了实用见解。

Abstract: Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

</details>


### [255] [TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA](https://arxiv.org/abs/2510.04682)
*Chanjoo Jung,Jaehyung Kim*

Main category: cs.CL

TL;DR: 本文提出了一种名为TiTok的新框架，通过token级别的知识迁移实现有效的LoRA移植，无需额外模型或开销，在多个基准上平均性能提升4~8%。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA微调方法无法跨不同模型骨架迁移，且依赖训练数据的知识蒸馏存在局限性，需要更高效、可迁移的参数高效微调方法。

Method: TiTok通过对比带有和不带LoRA的源模型之间的差异来捕获任务相关的信息，利用该差异突出关键token，并用于筛选合成数据，从而实现无额外模型的LoRA移植。

Result: 在三个基准上的多种迁移设置实验表明，TiTok相比基线方法平均性能提升4~8%，且无需训练判别器等额外组件。

Conclusion: TiTok提供了一种高效、低开销的LoRA移植方案，能够在不同模型间有效转移适应参数，显著提升迁移性能。

Abstract: Large Language Models (LLMs) are widely applied in real world scenarios, but
fine-tuning them comes with significant computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these
costs, but the adapted parameters are dependent on the base model and cannot be
transferred across different backbones. One way to address this issue is
through knowledge distillation, but its effectiveness inherently depends on
training data. Recent work such as TransLoRA avoids this by generating
synthetic data, but this adds complexity because it requires training an
additional discriminator model. In this paper, we propose TiTok, a new
framework that enables effective LoRA Transplantation through Token-level
knowledge transfer. Specifically, TiTok captures task-relevant information
through a contrastive excess between a source model with and without LoRA. This
excess highlights informative tokens and enables selective filtering of
synthetic data, all without additional models or overhead. Through experiments
on three benchmarks across multiple transfer settings, our experiments show
that the proposed method is consistently effective, achieving average
performance gains of +4~8% compared to baselines overall.

</details>


### [256] [Multilingual Routing in Mixture-of-Experts](https://arxiv.org/abs/2510.04694)
*Lucas Bandarkar,Chenyuan Yang,Mohsen Fayyaz,Junlin Hu,Nanyun Peng*

Main category: cs.CL

TL;DR: 本研究探讨了Mixture-of-Experts（MoE）模型在多语言数据下的专家路由动态，发现中间层存在跨语言路由对齐现象，并提出通过引导路由器提升多语言性能的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE架构被广泛用于扩展现代大语言模型，但其在多语言场景下的稀疏路由行为尚不清楚，本文旨在揭示MoE模型如何处理非英语文本及其多语言泛化能力的限制因素。

Method: 使用并行多语言数据集分析专家路由模式，探索推理时干预策略，特别是通过促进英语中频繁激活的中间层任务专家来引导路由器。

Result: 发现MoE模型在早期和晚期解码层表现出语言特异性路由，而在中间层呈现显著的跨语言路由对齐；模型在某种语言上的性能与其在这些层中与英语的路由相似性高度相关；所提出的干预方法在两个任务、三种模型和15种以上语言中一致提升了1-2%的性能。

Conclusion: MoE模型的多语言泛化能力受限于其在所有语言中利用语言通用专家的能力，中间层的跨语言路由对齐是关键，且针对性的推理干预可有效提升多语言性能。

Abstract: Mixture-of-Experts (MoE) architectures have become the key to scaling modern
LLMs, yet little is understood about how their sparse routing dynamics respond
to multilingual data. In this work, we analyze expert routing patterns using
parallel multilingual datasets and present highly interpretable layer-wise
phenomena. We find that MoE models route tokens in language-specific ways in
the early and late decoder layers but exhibit significant cross-lingual routing
alignment in middle layers, mirroring parameter-sharing trends observed in
dense LLMs. In particular, we reveal a clear, strong correlation between a
model's performance in a given language and how similarly its tokens are routed
to English in these layers. Extending beyond correlation, we explore
inference-time interventions that induce higher cross-lingual routing
alignment. We introduce a method that steers the router by promoting
middle-layer task experts frequently activated in English, and it successfully
increases multilingual performance. These 1-2% gains are remarkably consistent
across two evaluation tasks, three models, and 15+ languages, especially given
that these simple interventions override routers of extensively trained,
state-of-the-art LLMs. In comparison, interventions outside of the middle
layers or targeting multilingual-specialized experts only yield performance
degradation. Altogether, we present numerous findings that explain how MoEs
process non-English text and demonstrate that generalization is limited by the
model's ability to leverage language-universal experts in all languages.

</details>


### [257] [JSON Whisperer: Efficient JSON Editing with LLMs](https://arxiv.org/abs/2510.04717)
*Sarel Duanis,Asnat Greenstein-Messica,Eliya Habba*

Main category: cs.CL

TL;DR: 提出JSON Whisperer框架，利用RFC 6902差分补丁和EASE编码方法，使大语言模型能高效、准确地编辑JSON，显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在通过自然语言命令修改JSON时需重新生成整个文档，导致计算效率低下。

Method: 提出JSON Whisperer框架，生成仅包含必要修改的RFC 6902补丁，并引入EASE编码将数组转为带稳定键的字典，避免索引变化问题。

Result: 相比完整再生，使用EASE的补丁生成减少31%的token使用量，编辑质量下降不超过5%，在复杂指令和列表操作中表现更优。

Conclusion: JSON Whisperer结合RFC 6902和EASE有效提升了大语言模型对JSON的编辑效率与准确性，特别适用于高频或复杂结构修改场景。

Abstract: Large language models (LLMs) can modify JSON documents through natural
language commands, but current approaches regenerate entire structures for each
edit, resulting in computational inefficiency. We present JSON Whisperer, a
framework that enables LLMs to generate RFC 6902 diff patches-expressing only
the necessary modifications-rather than complete documents. We identify two key
challenges in patch-based editing: (1) LLMs often miss related updates when
generating isolated patches, and (2) array manipulations require tracking index
shifts across operations, which LLMs handle poorly. To address these issues, we
introduce EASE (Explicitly Addressed Sequence Encoding), which transforms
arrays into dictionaries with stable keys, eliminating index arithmetic
complexities. Our evaluation shows that patch generation with EASE reduces
token usage by 31% while maintaining edit quality within 5% of full
regeneration with particular gains for complex instructions and list
manipulations. The dataset is available at:
https://github.com/emnlp2025/JSON-Whisperer/

</details>


### [258] [A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance](https://arxiv.org/abs/2510.04750)
*Peshala Perera,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 本文提出了一种针对僧伽罗语成人阅读障碍者的辅助系统，结合语音识别、预训练语言模型和文本生成技术，实现了从语音到纠正文本再到语音的多模态反馈循环，在低资源环境下展示了可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 成人阅读障碍在非英语环境中研究不足，且僧伽罗语等低资源语言缺乏语言辅助工具，亟需包容性自然语言处理技术来改善语言可及性。

Method: 系统采用Whisper进行语音转文本，使用微调的SinBERT模型识别僧伽罗语中的常见阅读障碍错误，并结合mT5与Mistral模型生成纠正后的文本，最后通过gTTS将结果转为语音输出。

Result: 尽管面临数据集有限的挑战，系统实现了0.66的转录准确率、0.7的纠错准确率和0.65的整体系统准确率。

Conclusion: 该工作证明了在低资源语言中开发包容性NLP工具的可行性，强调了为少数语言群体提供技术支持的重要性。

Abstract: Dyslexia in adults remains an under-researched and under-served area,
particularly in non-English-speaking contexts, despite its significant impact
on personal and professional lives. This work addresses that gap by focusing on
Sinhala, a low-resource language with limited tools for linguistic
accessibility. We present an assistive system explicitly designed for
Sinhala-speaking adults with dyslexia. The system integrates Whisper for
speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model
trained for Sinhala to identify common dyslexic errors, and a combined mT5 and
Mistral-based model to generate corrected text. Finally, the output is
converted back to speech using gTTS, creating a complete multimodal feedback
loop. Despite the challenges posed by limited Sinhala-language datasets, the
system achieves 0.66 transcription accuracy and 0.7 correction accuracy with
0.65 overall system accuracy. These results demonstrate both the feasibility
and effectiveness of the approach. Ultimately, this work highlights the
importance of inclusive Natural Language Processing (NLP) technologies in
underrepresented languages and showcases a practical

</details>


### [259] [ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever](https://arxiv.org/abs/2510.04757)
*Eduardo Martínez Rivera,Filippo Menolascina*

Main category: cs.CL

TL;DR: 本文提出了一种结合ModernBERT和ColBERTv2的两阶段检索架构，用于提升生物医学领域检索增强生成（RAG）系统的性能，在MIRAGE基准上实现了最先进的准确率。


<details>
  <summary>Details</summary>
Motivation: 通用密集检索器在专业领域表现不佳，而领域内模型通常计算成本过高，因此需要在效率与准确性之间取得平衡。

Method: 采用两阶段检索架构：首先使用轻量级ModernBERT编码器进行初步检索，再用ColBERTv2进行细粒度重排序，并在PubMedQA数据集上对信息检索模块进行微调。

Result: ColBERT重排序器使Recall@3最高提升了4.2个百分点；在MIRAGE问答基准上平均准确率达到0.4448，超过MedCPT等强基线。

Conclusion: 两阶段检索架构能有效提升生物医学RAG系统的性能，但其效果依赖于检索器与重排序器的联合微调，否则重排序可能适得其反。

Abstract: Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

</details>


### [260] [Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models](https://arxiv.org/abs/2510.04764)
*Raha Askari,Sina Zarrieß,Özge Alacam,Judith Sieker*

Main category: cs.CL

TL;DR: 本文提出一个新基准，用于测试小规模语言模型（BabyLM）在识别Grice会话准则违反方面的能力，并与儿童和大规模语言模型（LLM）的表现进行比较。


<details>
  <summary>Details</summary>
Motivation: 为了探究语言模型是否能在有限数据下理解隐含意义，特别是基于Grice会话准则的语用推断能力。

Method: 基于Surian等人（1996）对儿童的研究，构建了一个包含五个Grice准则的新基准，评估训练数据少于10M和100M词符的语言模型区分准则遵守与违反话语的能力。

Result: 训练数据少于100M词符的模型整体优于少于10M词符的模型，但在语用推断上仍不及儿童和大规模语言模型（3T词符）。数据量增加提升了部分语用表现，使模型能更细粒度地区分不同语用维度。

Conclusion: 适度增加训练数据可改善小模型的部分语用能力，但要达到儿童或大模型水平仍有差距，表明当前小模型在深层语用理解上存在局限。

Abstract: Implicit meanings are integral to human communication, making it essential
for language models to be capable of identifying and interpreting them. Grice
(1975) proposed a set of conversational maxims that guide cooperative dialogue,
noting that speakers may deliberately violate these principles to express
meanings beyond literal words, and that listeners, in turn, recognize such
violations to draw pragmatic inferences.
  Building on Surian et al. (1996)'s study of children's sensitivity to
violations of Gricean maxims, we introduce a novel benchmark to test whether
language models pretrained on less than 10M and less than 100M tokens can
distinguish maxim-adhering from maxim-violating utterances. We compare these
BabyLMs across five maxims and situate their performance relative to children
and a Large Language Model (LLM) pretrained on 3T tokens.
  We find that overall, models trained on less than 100M tokens outperform
those trained on less than 10M, yet fall short of child-level and LLM
competence. Our results suggest that modest data increases improve some aspects
of pragmatic behavior, leading to finer-grained differentiation between
pragmatic dimensions.

</details>


### [261] [Hybrid Architectures for Language Models: Systematic Analysis and Design Insights](https://arxiv.org/abs/2510.04800)
*Sangmin Bae,Bilge Acun,Haroun Habeeb,Seungyeon Kim,Chien-Yu Lin,Liang Luo,Junjie Wang,Carole-Jean Wu*

Main category: cs.CL

TL;DR: 本文系统评估了结合自注意力机制与结构化状态空间模型（如Mamba）的混合架构，比较了层间串联与层内并联两种融合策略，并从语言建模性能、长上下文能力、扩展性及训练推理效率等方面进行了全面分析，提出了针对两类混合模型的最优设计建议。


<details>
  <summary>Details</summary>
Motivation: 尽管混合架构在长上下文任务中表现出良好的性能与效率平衡，但不同混合策略之间的系统性比较和有效性的关键因素尚不明确。

Method: 对层间（串行）和层内（并行）融合的混合架构进行整体评估，从语言建模性能、长上下文能力、扩展性以及训练和推理效率等多个角度进行分析，并基于其计算原语的核心特性识别关键设计要素。

Result: 明确了不同混合化策略的关键特征，提出了针对每种策略的最优设计配方，在多个维度上实现了性能与效率的优化。

Conclusion: 该研究为开发混合语言模型提供了实用指导和有价值的见解，有助于优化模型架构配置。

Abstract: Recent progress in large language models demonstrates that hybrid
architectures--combining self-attention mechanisms with structured state space
models like Mamba--can achieve a compelling balance between modeling quality
and computational efficiency, particularly for long-context tasks. While these
hybrid models show promising performance, systematic comparisons of
hybridization strategies and analyses on the key factors behind their
effectiveness have not been clearly shared to the community. In this work, we
present a holistic evaluation of hybrid architectures based on inter-layer
(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a
variety of perspectives: language modeling performance, long-context
capabilities, scaling analysis, and training and inference efficiency. By
investigating the core characteristics of their computational primitive, we
identify the most critical elements for each hybridization strategy and further
propose optimal design recipes for both hybrid models. Our comprehensive
analysis provides practical guidance and valuable insights for developing
hybrid language models, facilitating the optimization of architectural
configurations.

</details>


### [262] [How I Built ASR for Endangered Languages with a Spoken Dictionary](https://arxiv.org/abs/2510.04832)
*Christopher Bartley,Anton Ragni*

Main category: cs.CL

TL;DR: 该论文探讨了如何用少量非标准格式的语音数据为濒危语言构建自动语音识别（ASR）系统，发现仅需40分钟的短语音发音资源即可在马恩语和康沃尔语上实现可用的ASR性能，显著降低了技术门槛。


<details>
  <summary>Details</summary>
Motivation: 大多数濒危语言因缺乏符合标准监督格式的语音数据而无法使用现有ASR技术，尽管部分语言已有历史录音资料。作者希望降低ASR对濒危语言的技术门槛，使其更易于获得语音技术支持以促进语言复兴。

Method: 利用短形式的发音资源（如单词或短语级别的语音数据）替代传统所需的句子级对齐数据，结合少样本学习方法，在马恩语和康沃尔语上训练并评估ASR系统性能。

Result: 仅使用40分钟的发音资源，就在马恩语上实现了低于50%的词错误率（WER），并在康沃尔语上成功复现了类似效果，证明该方法具有可迁移性和实用性。

Conclusion: 构建濒危语言ASR系统的数据需求可以大幅降低，且无需严格对齐的长语音数据，为资源稀缺的语言社区提供了切实可行的技术路径。

Abstract: Nearly half of the world's languages are endangered. Speech technologies such
as Automatic Speech Recognition (ASR) are central to revival efforts, yet most
languages remain unsupported because standard pipelines expect utterance-level
supervised data. Speech data often exist for endangered languages but rarely
match these formats. Manx Gaelic ($\sim$2,200 speakers), for example, has had
transcribed speech since 1948, yet remains unsupported by modern systems. In
this paper, we explore how little data, and in what form, is needed to build
ASR for critically endangered languages. We show that a short-form
pronunciation resource is a viable alternative, and that 40 minutes of such
data produces usable ASR for Manx ($<$50\% WER). We replicate our approach,
applying it to Cornish ($\sim$600 speakers), another critically endangered
language. Results show that the barrier to entry, in quantity and form, is far
lower than previously thought, giving hope to endangered language communities
that cannot afford to meet the requirements arbitrarily imposed upon them.

</details>


### [263] [Instability in Downstream Task Performance During LLM Pretraining](https://arxiv.org/abs/2510.04848)
*Yuto Nishida,Masaru Isonuma,Yusuke Oda*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在训练过程中下游任务性能存在显著波动，提出通过检查点平均和集成的方法来提高性能稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型训练中下游任务性能波动大、难以选择最佳检查点的问题。

Method: 实证分析了在多样化网络规模语料上训练的LLM的下游任务性能稳定性，并采用检查点平均和集成两种后处理方法进行优化。

Result: 检查点平均和集成方法能有效减少性能波动，在不改变训练过程的前提下提升下游任务的稳定性。

Conclusion: 通过对相邻检查点的聚合，可以显著提高大型语言模型在下游任务上的性能稳定性，验证了该方法的有效性和实用性。

Abstract: When training large language models (LLMs), it is common practice to track
downstream task performance throughout the training process and select the
checkpoint with the highest validation score. However, downstream metrics often
exhibit substantial fluctuations, making it difficult to identify the
checkpoint that truly represents the best-performing model. In this study, we
empirically analyze the stability of downstream task performance in an LLM
trained on diverse web-scale corpora. We find that task scores frequently
fluctuate throughout training, both at the aggregate and example levels. To
address this instability, we investigate two post-hoc checkpoint integration
methods: checkpoint averaging and ensemble, motivated by the hypothesis that
aggregating neighboring checkpoints can reduce performance volatility. We
demonstrate both empirically and theoretically that these methods improve
downstream performance stability without requiring any changes to the training
procedure.

</details>


### [264] [ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization](https://arxiv.org/abs/2505.02819)
*Dmitriy Shopkhoev,Ammar Ali,Magauiya Zhussip,Valentin Malykh,Stamatios Lefkimmiatis,Nikos Komodakis,Sergey Zagoruyko*

Main category: cs.CL

TL;DR: ReplaceMe是一种无需训练的通用深度剪枝方法，通过线性操作替代Transformer模块，在保持高性能的同时实现高达25%的剪枝率。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法通常需要额外的训练或微调，计算成本高且复杂。因此，亟需一种无需训练、仅需少量校准数据即可高效压缩模型的方法。

Method: ReplaceMe利用小型校准数据集估计一个线性变换来近似被剪枝的Transformer块，并将该线性映射与剩余模块融合，从而无需引入额外参数或训练步骤。

Result: 在多个大语言模型上验证，ReplaceMe在不进行任何训练的情况下实现了最高25%的剪枝率，并保留了原始模型约90%的性能，显著优于其他无需训练的剪枝方法，且与需重训练的最先进方法具有竞争力。

Conclusion: ReplaceMe是一种高效、即插即用的深度剪枝方案，能够在几乎无性能损失的情况下大幅压缩Transformer模型，具备良好的实用性和可扩展性。

Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that
effectively replaces transformer blocks with a linear operation, while
maintaining high performance for low compression ratios. In contrast to
conventional pruning approaches that require additional training or
fine-tuning, our approach requires only a small calibration dataset that is
used to estimate a linear transformation, which approximates the pruned blocks.
The estimated linear mapping can be seamlessly merged with the remaining
transformer blocks, eliminating the need for any additional network parameters.
Our experiments show that ReplaceMe consistently outperforms other
training-free approaches and remains highly competitive with state-of-the-art
pruning methods that involve extensive retraining/fine-tuning and architectural
modifications. Applied to several large language models (LLMs), ReplaceMe
achieves up to 25% pruning while retaining approximately 90% of the original
model's performance on open benchmarks - without any training or healing steps,
resulting in minimal computational overhead (see Fig.1). We provide an
open-source library implementing ReplaceMe alongside several state-of-the-art
depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.

</details>


### [265] [When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA](https://arxiv.org/abs/2510.04849)
*Elisei Rykov,Kseniia Petrushina,Maksim Savkin,Valerii Olisov,Artem Vazhentsev,Kseniia Titova,Alexander Panchenko,Vasily Konovalov,Julia Belikova*

Main category: cs.CL

TL;DR: 本文提出了PsiloQA，一个大规模、多语言的问答数据集，标注了14种语言中的片段级幻觉，通过自动化流程构建，用于推动细粒度、跨语言的幻觉检测研究。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测基准多局限于英语且仅在序列级别进行，缺乏细粒度和多语言支持，难以全面评估大模型的幻觉问题。

Method: 采用三阶段自动化流程：使用GPT-4o从维基百科生成问答对，让多种大模型在无上下文情况下生成可能包含幻觉的回答，并利用GPT-4o对比标准答案和检索上下文自动标注幻觉片段。

Result: 评估了多种幻觉检测方法，发现基于编码器的模型在多语言场景下表现最佳，PsiloQA具有良好的跨语言泛化能力和知识迁移效果，且构建成本显著低于人工标注数据集。

Conclusion: PsiloQA为多语言环境下可扩展、细粒度的幻觉检测提供了有效资源，推动了该领域的研究发展。

Abstract: Hallucination detection remains a fundamental challenge for the safe and
reliable deployment of large language models (LLMs), especially in applications
requiring factual accuracy. Existing hallucination benchmarks often operate at
the sequence level and are limited to English, lacking the fine-grained,
multilingual supervision needed for a comprehensive evaluation. In this work,
we introduce PsiloQA, a large-scale, multilingual dataset annotated with
span-level hallucinations across 14 languages. PsiloQA is constructed through
an automated three-stage pipeline: generating question-answer pairs from
Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse
LLMs in a no-context setting, and automatically annotating hallucinated spans
using GPT-4o by comparing against golden answers and retrieved context. We
evaluate a wide range of hallucination detection methods -- including
uncertainty quantification, LLM-based tagging, and fine-tuned encoder models --
and show that encoder-based models achieve the strongest performance across
languages. Furthermore, PsiloQA demonstrates effective cross-lingual
generalization and supports robust knowledge transfer to other benchmarks, all
while being significantly more cost-efficient than human-annotated datasets.
Our dataset and results advance the development of scalable, fine-grained
hallucination detection in multilingual settings.

</details>


### [266] [Detecting Distillation Data from Reasoning Models](https://arxiv.org/abs/2510.04850)
*Hengxiang Zhang,Hyeong Kyu Choi,Yixuan Li,Hongxin Wei*

Main category: cs.CL

TL;DR: 本文提出了一种名为Token Probability Deviation (TBD)的方法，用于检测推理蒸馏过程中的数据污染问题，通过分析生成token的概率偏差来区分模型是否见过问题，在S1数据集上取得了良好的检测效果。


<details>
  <summary>Details</summary>
Motivation: 推理蒸馏可能引入基准污染问题，即训练中使用的评估数据会虚增模型性能，因此需要有效方法检测蒸馏数据以确保评估公正性。

Method: 提出Token Probability Deviation (TBD)方法，利用生成token的概率模式，通过衡量其与高参考概率的偏离程度来判断问题是否在蒸馏数据中出现过。

Result: 在S1数据集上，TBD方法达到0.918的AUC和0.470的TPR@1% FPR，表现出优越的检测性能。

Conclusion: TBD是一种有效且新颖的蒸馏数据检测方法，能够显著识别推理蒸馏中的数据污染问题，有助于提升大模型评估的可靠性。

Abstract: Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

</details>


### [267] [SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests](https://arxiv.org/abs/2510.04891)
*Punya Syon Pandey,Hai Son Le,Devansh Bhardwaj,Rada Mihalcea,Zhijing Jin*

Main category: cs.CL

TL;DR: 本文介绍了SocialHarmBench，一个包含585个提示的基准数据集，用于评估大语言模型在政治操纵、宣传和错误信息生成等社会政治敏感领域的脆弱性。实验表明，现有模型（如Mistral-7B）在这些领域存在严重合规风险，且安全机制在高风险社会政治场景中泛化能力不足。


<details>
  <summary>Details</summary>
Motivation: 现有的安全基准很少涵盖政治操纵、宣传、错误信息生成等具有直接社会政治影响的领域，因此需要一个新的基准来系统评估大语言模型在这些高风险情境下的失败模式。

Method: 构建了一个名为SocialHarmBench的数据集，包含585个提示，覆盖7个社会政治类别和34个国家，用于测试LLMs在不同历史时期和地区背景下的有害响应倾向，并对多个开源模型进行评估分析。

Result: 发现开源模型（如Mistral-7B）在历史修正主义、宣传和政治操纵等领域攻击成功率高达97%-98%；模型在面对21世纪或前20世纪语境以及拉丁美洲、美国和英国相关提示时最为脆弱。

Conclusion: 当前的大语言模型安全机制无法有效推广到高风险的社会政治环境，暴露出系统性偏见，可能威胁人权和民主价值观，亟需更鲁棒的安全对齐方法。

Abstract: Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

</details>


### [268] [Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment](https://arxiv.org/abs/2510.04919)
*Davood Rafiei,Morgan Lindsay Heisler,Weiwei Zhang,Mohammadreza Pourreza,Yong Zhang*

Main category: cs.CL

TL;DR: 本文研究了自然语言到SQL任务中监督微调（SFT）的数据集结构对齐问题，提出通过比较训练集、目标数据和模型预测中的SQL结构特征分布来评估对齐程度，并验证了结构对齐是微调效果的强预测因子。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据的变异性可能阻碍模型在不同领域上的泛化能力，因此需要研究如何衡量并提升训练数据与目标任务之间的结构对齐，以提高SFT的有效性。

Method: 通过分析三个大规模跨域NL2SQL基准数据集和多个模型族的实验，比较训练集、目标数据和微调前模型预测中SQL结构特征的分布差异，量化结构对齐程度并评估其对微调性能的影响。

Result: 实验证明结构对齐程度能有效预测SFT的成功率：高对齐带来显著准确率和生成质量提升，低对齐则改进甚微；结构对齐与模型表现高度相关。

Conclusion: 结构对齐是影响NL2SQL任务中SFT效果的关键因素，应重视对齐感知的数据选择，以实现更有效的微调和更好的泛化性能。

Abstract: Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

</details>


### [269] [The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2510.04933)
*Amir Hameed Mir*

Main category: cs.CL

TL;DR: 提出了一种名为Layer-wise Semantic Dynamics (LSD) 的几何框架，用于检测大语言模型中的幻觉现象，通过分析Transformer层间隐藏状态的语义演化，在无需多次采样或外部验证的情况下实现高效、准确的幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常生成看似流畅但事实错误的内容（即幻觉），在高风险领域带来严重隐患，现有方法依赖多次采样或外部验证，效率低且难以内省，因此需要一种高效、内在的检测机制。

Method: 提出LSD框架，利用基于边界的对比学习，将模型各层的隐藏激活与来自事实编码器的真值嵌入对齐，通过观察语义轨迹的稳定性（事实性响应保持稳定对齐，幻觉则表现出显著语义漂移）来检测幻觉。

Result: 在TruthfulQA和合成数据集上，LSD达到0.92 F1分数、0.96 AUROC和0.89聚类准确率，优于SelfCheckGPT和Semantic Entropy基线方法，并实现5-20倍的速度提升，仅需单次前向传播。

Conclusion: LSD提供了一种可扩展、模型无关的实时幻觉监测机制，揭示了大语言模型中事实一致性的几何特性，为内在幻觉检测提供了新思路。

Abstract: Large Language Models (LLMs) often produce fluent yet factually incorrect
statements-a phenomenon known as hallucination-posing serious risks in
high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric
framework for hallucination detection that analyzes the evolution of
hidden-state semantics across transformer layers. Unlike prior methods that
rely on multiple sampling passes or external verification sources, LSD operates
intrinsically within the model's representational space. Using margin-based
contrastive learning, LSD aligns hidden activations with ground-truth
embeddings derived from a factual encoder, revealing a distinct separation in
semantic trajectories: factual responses preserve stable alignment, while
hallucinations exhibit pronounced semantic drift across depth. Evaluated on the
TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an
F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming
SelfCheckGPT and Semantic Entropy baselines while requiring only a single
forward pass. This efficiency yields a 5-20x speedup over sampling-based
methods without sacrificing precision or interpretability. LSD offers a
scalable, model-agnostic mechanism for real-time hallucination monitoring and
provides new insights into the geometry of factual consistency within large
language models.

</details>


### [270] [A First Context-Free Grammar Applied to Nawatl Corpora Augmentation](https://arxiv.org/abs/2510.04945)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Ligia Quintana-Torres,Martha-Lorena Avendaño-Garrido,Graham Ranger*

Main category: cs.CL

TL;DR: 本文提出了一种用于Nawatl语言的上下文无关文法（CFG），旨在通过生成大量语法正确的虚拟句子来扩充稀缺语料库，从而改善低资源语言的模型训练效果。


<details>
  <summary>Details</summary>
Motivation: Nawatl是一种数字资源匮乏的美洲原住民语言，现有语料库不足以支持机器学习模型的训练，因此需要通过文法生成人工句子以扩充语料。

Method: 设计并实现了一个上下文无关文法（CFG），用于生成符合语法的Nawatl人工句子，并将生成的句子加入原始语料库（π-yalli）中，进而训练FastText等算法并评估其在句子级语义任务上的表现。

Result: 使用文法扩充后的语料库显著提升了模型性能，在某些任务上优于部分大语言模型，但改进幅度受限于当前文法对语言建模的完整性。

Conclusion: 上下文无关文法能有效扩展低资源语言的语料库并提升语言模型表现，但需更精确的语言文法以实现更大提升。

Abstract: In this article we introduce a context-free grammar (CFG) for the Nawatl
language. Nawatl (or Nahuatl) is an Amerindian language of the $\pi$-language
type, i.e. a language with few digital resources, in which the corpora
available for machine learning are virtually non-existent. The objective here
is to generate a significant number of grammatically correct artificial
sentences, in order to increase the corpora available for language model
training. We want to show that a grammar enables us significantly to expand a
corpus in Nawatl which we call $\pi$-\textsc{yalli}. The corpus, thus enriched,
enables us to train algorithms such as FastText and to evaluate them on
sentence-level semantic tasks. Preliminary results show that by using the
grammar, comparative improvements are achieved over some LLMs. However, it is
observed that to achieve more significant improvement, grammars that model the
Nawatl language even more effectively are required.

</details>


### [271] [Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)](https://arxiv.org/abs/2510.04950)
*Om Dobariya,Akhil Kumar*

Main category: cs.CL

TL;DR: 该研究探讨了提示语的礼貌程度对大语言模型（如ChatGPT 4o）在多项选择题上表现的影响，发现不礼貌的提示语反而比礼貌的提示语准确率更高。


<details>
  <summary>Details</summary>
Motivation: 探索自然语言提示中礼貌和语气对大语言模型性能的影响，尤其是在此前相关研究较少的情况下。

Method: 构建包含50个基础问题（涵盖数学、科学和历史）的数据集，每个问题生成五种语气变体（非常礼貌、礼貌、中性、粗鲁、非常粗鲁），共250个提示；使用ChatGPT 4o生成回答，并通过配对样本t检验分析结果。

Result: 不礼貌的提示语表现优于礼貌的提示语，准确率从非常礼貌的80.8%上升到非常粗鲁的84.8%，差异具有统计显著性。

Conclusion: 提示语的语气显著影响模型表现，较新的大语言模型可能对不礼貌语气反应更积极，提示需关注提示工程中的语用和社会交互维度。

Abstract: The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

</details>


### [272] [AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives](https://arxiv.org/abs/2510.04983)
*Khalid Mehtab Khan,Anagha Kulkarni*

Main category: cs.CL

TL;DR: 本文提出了AWARE框架，通过增强模型对领域、上下文和类别重叠的感知能力，提升在学生反思文本中识别文化资本主题的准确性。


<details>
  <summary>Details</summary>
Motivation: 标准NLP模型因缺乏对特定领域语言和叙事上下文的理解，在识别学生反思中的文化资本主题时表现不佳。

Method: 提出AWARE框架，包含三个组件：领域感知（调整词汇以适应学生反思的语言风格）、上下文感知（生成考虑全文语境的句子嵌入）和类别重叠感知（采用多标签策略识别句子中多个共存主题）。

Result: AWARE在Macro-F1上比强基线模型提高2.1个百分点，并在所有主题上均有显著改进。

Conclusion: 通过显式增强模型对输入特性的感知，AWARE为依赖叙事上下文的文本分类任务提供了鲁棒且可推广的方法。

Abstract: Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.

</details>


### [273] [Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.05003)
*Imran Mansha*

Main category: cs.CL

TL;DR: 提出一种资源高效的微调方法，使用LoRA和QLoRA技术优化Llama-3.2-3B模型，在有限资源下提升医学推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学推理任务中表现优异，但全量微调需要大量计算资源，难以在低资源环境中部署。

Method: 采用参数高效微调技术（如LoRA和QLoRA），在公开的医学推理数据集上对Llama-3.2-3B进行适应性调整。

Result: 相比全量微调，内存使用减少高达60%，同时保持了较强的推理能力和事实准确性。

Conclusion: 轻量级微调方法可在资源受限环境下有效提升医学领域推理性能，为低资源场景下的LLM部署提供了可行方案。

Abstract: Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

</details>


### [274] [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
*Kuofeng Gao,Yiming Li,Chao Du,Xin Wang,Xingjun Ma,Shu-Tao Xia,Tianyu Pang*

Main category: cs.CL

TL;DR: 本文提出了一种利用Unicode中的变体选择符实现的不可见越狱攻击，通过在恶意问题后添加不可见字符来秘密改变其分词结果，从而绕过对齐的语言模型，且不会在文本上产生可见修改。


<details>
  <summary>Details</summary>
Motivation: 现有的文本模态越狱攻击通常需要可见的修改，限制了其隐蔽性和实用性。本文旨在探索在不引起视觉察觉的情况下实现高效越狱攻击的可能性。

Method: 引入变体选择符（variation selectors）作为不可见的对抗性后缀，设计了一种链式搜索生成方法（chain-of-search pipeline）来自动生成有效的越狱提示。

Result: 实验表明，该方法在四个对齐的大型语言模型上实现了高攻击成功率，并能泛化到提示注入攻击，且无任何可见文本修改。

Conclusion: 利用不可见Unicode字符进行越狱攻击是可行且高效的，揭示了当前语言模型在输入处理和安全防护方面的潜在漏洞。

Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

</details>


### [275] [A Set of Quebec-French Corpus of Regional Expressions and Terms](https://arxiv.org/abs/2510.05026)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: 本文提出将习语理解与方言理解结合，利用地区性习语作为衡量法语魁北克方言理解能力的基准，并构建了两个新数据集QFrCoRE和QFrCoRT，实验表明这些数据集能有效评估大语言模型在特定方言中的表现。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地评估模型对特定方言的理解能力，作者认为应结合习语理解任务，因为地区性习语是方言的重要组成部分且具有挑战性。

Method: 构建了两个针对法语魁北克方言的习语数据集：QFrCoRE（包含4,633个习语短语实例）和QFrCoRT（包含171个地区性习语词实例），并使用94个大语言模型进行评估实验。

Result: 实验结果显示，所提出的区域习语基准能够有效区分模型对方言的理解能力，验证了其作为方言理解评测工具的可靠性。

Conclusion: 区域习语可作为评估方言理解的有效基准，所构建的数据集及方法具有可复制性，可用于其他方言的研究。

Abstract: The tasks of idiom understanding and dialect understanding are both
well-established benchmarks in natural language processing. In this paper, we
propose combining them, and using regional idioms as a test of dialect
understanding. Towards this end, we propose two new benchmark datasets for the
Quebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic
phrases, and QFrCoRT, which comprises 171 regional instances of idiomatic
words. We explain how to construct these corpora, so that our methodology can
be replicated for other dialects. Our experiments with 94 LLM demonstrate that
our regional idiom benchmarks are a reliable tool for measuring a model's
proficiency in a specific dialect.

</details>


### [276] [Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization](https://arxiv.org/abs/2510.05038)
*Omri Uzan,Asaf Yehudai,Roi pony,Eyal Shnarch,Ariel Gera*

Main category: cs.CL

TL;DR: 本文提出了一种名为Guided Query Refinement (GQR)的新型测试时优化方法，通过利用辅助检索器的得分来精炼主检索器的查询嵌入，从而提升视觉中心模型在视觉文档检索中的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大规模多模态编码器的视觉文档检索方法在部署和扩展性上存在困难，且受限于模态差距；同时，现有的混合检索方法未能充分利用各模型表示空间中的细粒度交互。

Method: 提出Guided Query Refinement (GQR)，在测试时利用轻量级文本检索器的指导信号优化主视觉检索器的查询嵌入，实现高效而精准的跨模态检索。

Result: 在多个视觉文档检索基准上验证了GQR的有效性，使轻量级模型性能媲美更大模型，推理速度快14倍，内存消耗减少54倍。

Conclusion: GQR有效推动了多模态检索中性能与效率的帕累托前沿，为实际部署提供了更优的权衡方案。

Abstract: Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

</details>


### [277] [COLE: a Comprehensive Benchmark for French Language Understanding Evaluation](https://arxiv.org/abs/2510.05046)
*David Beauchemin,Yan Tremblay,Mohamed Amine Youssef,Richard Khoury*

Main category: cs.CL

TL;DR: 本文介绍了COLE，一个包含23个多样化任务的法语自然语言理解（NLU）新基准，涵盖情感分析、释义检测、语法判断和推理等能力，并特别关注与法语相关的语言现象。研究对94个大语言模型进行了评估，揭示了闭源与开源模型之间的显著性能差距，并指出了当前模型在零样本抽取式问答、细粒度词义消歧和区域语言变体理解等方面的挑战。COLE将作为公共资源发布，以推动法语语言建模的进一步发展。


<details>
  <summary>Details</summary>
Motivation: 为了更全面地评估法语自然语言理解（NLU）能力，现有基准不足以覆盖法语特有的语言现象和多样化的NLU任务，因此需要一个新的综合性评测基准。

Method: 构建了一个名为COLE的新基准，包含23个多样化的NLU任务，覆盖广泛的语言能力，并特别关注法语相关的语言现象；在此基础上对94个大语言模型进行了系统性评估。

Result: 评估结果显示闭源模型整体优于开源模型，且当前模型在零样本抽取式问答、细粒度词义消歧和区域语言变体理解方面表现较差，存在明显挑战。

Conclusion: COLE为法语NLU提供了一个全面的评测平台，揭示了现有大语言模型的优势与不足，其公开发布有助于推动法语语言模型的进一步研究与发展。

Abstract: To address the need for a more comprehensive evaluation of French Natural
Language Understanding (NLU), we introduce COLE, a new benchmark composed of 23
diverse task covering a broad range of NLU capabilities, including sentiment
analysis, paraphrase detection, grammatical judgment, and reasoning, with a
particular focus on linguistic phenomena relevant to the French language. We
benchmark 94 large language models (LLM), providing an extensive analysis of
the current state of French NLU. Our results highlight a significant
performance gap between closed- and open-weights models and identify key
challenging frontiers for current LLMs, such as zero-shot extractive
question-answering (QA), fine-grained word sense disambiguation, and
understanding of regional language variations. We release COLE as a public
resource to foster further progress in French language modelling.

</details>


### [278] [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
*Dachuan Shi,Abedelkadir Asi,Keying Li,Xiangchi Yuan,Leyan Pan,Wenke Lee,Wen Xiao*

Main category: cs.CL

TL;DR: 本文提出了SwiReasoning，一种无需训练的LLM推理框架，通过在显式和隐式推理之间动态切换，并限制思考块转换次数，有效提升推理准确性和令牌效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在隐空间连续推理虽具潜力，但存在搜索分布扩散、噪声增加、收敛困难及过度思考等问题，影响准确性和效率，尤其在无需训练的场景下更为突出。

Method: 提出SwiReasoning框架，基于逐块熵趋势估计置信度，动态切换显式与隐式推理路径，并限制最大思考块切换次数以抑制过度思考。

Result: 在多个数学与STEM基准上，SwiReasoning在不同模型族和规模的推理模型中平均准确率提升1.5%-2.8%；在受限预算下，令牌效率提升56%-79%，且预算越紧增益越大。

Conclusion: SwiReasoning通过动态切换推理模式和控制思考深度，在无需训练的前提下显著提升了大语言模型的推理效率与准确性，为高效推理提供了有效解决方案。

Abstract: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

</details>


### [279] [Slm-mux: Orchestrating small language models for reasoning](https://arxiv.org/abs/2510.05077)
*Chenyu Wang,Zishen Wan,Hao Kang,Emma Chen,Zhiqiang Xie,Tushar Krishna,Vijay Janapa Reddi,Yilun Du*

Main category: cs.CL

TL;DR: 提出了一种三阶段方法SLM-MUX，用于有效协同多个小型语言模型（SLMs），通过模型选择搜索和测试时扩展策略，在MATH、GPQA和GSM8K等任务上显著优于现有方法，并在某些情况下超过或匹配大型模型Qwen 2.5 72B的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管小型语言模型（SLMs）效率高且擅长特定任务，但现有协同方法主要针对大模型，对SLMs效果不佳，因此需要专门针对SLMs设计更有效的协同机制。

Method: 提出SLM-MUX多模型架构，结合两阶段优化：1）从候选池中搜索最具互补性的SLMs；2）为SLM-MUX定制测试时扩展策略。

Result: 在MATH、GPQA和GSM8K上分别比现有方法最高提升13.4%、8.8%和7.0%；仅用两个SLMs即超越Qwen 2.5 72B在GPQA和GSM8K上的表现，并在MATH上与其持平。

Conclusion: SLMs可以通过所提出的SLM-MUX方法被高效协同，构建出更准确且高效的系统，验证了多SLM协同的潜力。

Abstract: With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

</details>


### [280] [TeachLM: Post-Training LLMs for Education Using Authentic Learning Data](https://arxiv.org/abs/2510.05087)
*Janos Perczel,Jin Chow,Dorottya Demszky*

Main category: cs.CL

TL;DR: 本文提出了TeachLM，一种通过高效参数微调优化教学的大型语言模型，利用真实学生-导师互动数据提升对话式教学能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在教育应用中受限于缺乏高质量的真实学生学习数据，且提示工程难以编码复杂的教学策略。

Method: 基于Polygence提供的10万小时一对一师生互动数据，采用高效参数微调方法训练TeachLM，并构建合成对话生成能力以支持多轮评估协议。

Result: TeachLM显著提升了对话和教学表现：学生发言时间翻倍、提问方式改善、对话轮次增加50%，并实现更高个性化教学水平。

Conclusion: 使用真实学习数据进行微调能有效增强LLM在教育场景中的对话能力和教学效果，提出的评估协议为未来研究提供了可扩展、可复现的评测框架。

Abstract: The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

</details>


### [281] [Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models](https://arxiv.org/abs/2510.05090)
*Runchu Tian,Junxia Cui,Xueqiang Xu,Feng Yao,Jingbo Shang*

Main category: cs.CL

TL;DR: 提出了一种无需训练的解码策略Tolerator，通过两阶段过程（序列填充和迭代优化）实现对已生成token的重新修正，显著提升扩散大语言模型的输出质量。


<details>
  <summary>Details</summary>
Motivation: 离散扩散大语言模型在生成过程中一旦接受某个token便无法后续修改，导致早期错误持续影响最终结果，限制了模型性能。

Method: 提出Tolerator，采用序列填充分和迭代优化两个阶段，在迭代中重新掩码并解码部分token，利用其余token作为上下文进行交叉验证式修正，从而实现已生成内容的动态调整。

Result: 在五个涵盖语言理解、代码生成和数学推理的标准基准上实验表明，Tolerator在相同计算预算下显著优于基线方法。

Conclusion: 解码算法对充分发挥扩散大语言模型潜力至关重要，Tolerator提供了一种有效且无需训练的改进路径。

Abstract: Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

</details>


### [282] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 提出MASA框架，通过跨层共享字典原子实现Transformer注意力模块参数减少66.7%，性能不下降，适用于LLM和ViT，无需复杂训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有压缩方法主要关注块内优化，而忽略了Transformer层间冗余；受CNN中字典学习启发，探索跨层结构化权重共享以提升模型效率。

Method: 将注意力投影矩阵分解为共享的字典原子，各层权重表示为这些原子的线性组合，实现结构化权重共享；MASA可作为即插即用模块，使用标准优化器训练。

Result: 在100M-700M参数规模上，MASA在准确率和困惑度上优于GQA、低秩方法及近期共享方法；在ViT上图像分类与检测任务中减少66.7%注意力参数且性能相当；可应用于预训练LLM进行参数压缩而无显著性能损失。

Conclusion: MASA提供了一种可扩展、高效的Transformer压缩方案，通过引入跨层字典学习，在大幅减少参数的同时保持甚至提升性能，为高效大模型设计提供了新范式。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [283] [Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer](https://arxiv.org/abs/2510.03342)
*Abbas Abdolmaleki,Saminda Abeyruwan,Joshua Ainslie,Jean-Baptiste Alayrac,Montserrat Gonzalez Arenas,Ashwin Balakrishna,Nathan Batchelor,Alex Bewley,Jeff Bingham,Michael Bloesch,Konstantinos Bousmalis,Philemon Brakel,Anthony Brohan,Thomas Buschmann,Arunkumar Byravan,Serkan Cabi,Ken Caluwaerts,Federico Casarini,Christine Chan,Oscar Chang,London Chappellet-Volpini,Jose Enrique Chen,Xi Chen,Hao-Tien Lewis Chiang,Krzysztof Choromanski,Adrian Collister,David B. D'Ambrosio,Sudeep Dasari,Todor Davchev,Meet Kirankumar Dave,Coline Devin,Norman Di Palo,Tianli Ding,Carl Doersch,Adil Dostmohamed,Yilun Du,Debidatta Dwibedi,Sathish Thoppay Egambaram,Michael Elabd,Tom Erez,Xiaolin Fang,Claudio Fantacci,Cody Fong,Erik Frey,Chuyuan Fu,Ruiqi Gao,Marissa Giustina,Keerthana Gopalakrishnan,Laura Graesser,Oliver Groth,Agrim Gupta,Roland Hafner,Steven Hansen,Leonard Hasenclever,Sam Haves,Nicolas Heess,Brandon Hernaez,Alex Hofer,Jasmine Hsu,Lu Huang,Sandy H. Huang,Atil Iscen,Mithun George Jacob,Deepali Jain,Sally Jesmonth,Abhishek Jindal,Ryan Julian,Dmitry Kalashnikov,M. Emre Karagozler,Stefani Karp,Matija Kecman,J. Chase Kew,Donnie Kim,Frank Kim,Junkyung Kim,Thomas Kipf,Sean Kirmani,Ksenia Konyushkova,Li Yang Ku,Yuheng Kuang,Thomas Lampe,Antoine Laurens,Tuan Anh Le,Isabel Leal,Alex X. Lee,Tsang-Wei Edward Lee,Guy Lever,Jacky Liang,Li-Heng Lin,Fangchen Liu,Shangbang Long,Caden Lu,Sharath Maddineni,Anirudha Majumdar,Kevis-Kokitsi Maninis,Andrew Marmon,Sergio Martinez,Assaf Hurwitz Michaely,Niko Milonopoulos,Joss Moore,Robert Moreno,Michael Neunert,Francesco Nori,Joy Ortiz,Kenneth Oslund,Carolina Parada,Emilio Parisotto,Amaris Paryag,Acorn Pooley,Thomas Power,Alessio Quaglino,Haroon Qureshi,Rajkumar Vasudeva Raju,Helen Ran,Dushyant Rao,Kanishka Rao,Isaac Reid,David Rendleman,Krista Reymann,Miguel Rivas,Francesco Romano,Yulia Rubanova,Peter Pastor Sampedro,Pannag R Sanketi,Dhruv Shah,Mohit Sharma,Kathryn Shea,Mohit Shridhar,Charles Shu,Vikas Sindhwani,Sumeet Singh,Radu Soricut,Rachel Sterneck,Ian Storz,Razvan Surdulescu,Jie Tan,Jonathan Tompson,Saran Tunyasuvunakool,Jake Varley,Grace Vesom,Giulia Vezzani,Maria Bauza Villalonga,Oriol Vinyals,René Wagner,Ayzaan Wahid,Stefan Welker,Paul Wohlhart,Chengda Wu,Markus Wulfmeier,Fei Xia,Ted Xiao,Annie Xie,Jinyu Xie,Peng Xu,Sichun Xu,Ying Xu,Zhuo Xu,Jimmy Yan,Sherry Yang,Skye Yang,Yuxiang Yang,Hiu Hong Yu,Wenhao Yu,Wentao Yuan,Yuan Yuan,Jingwei Zhang,Tingnan Zhang,Zhiyuan Zhang,Allan Zhou,Guangyao Zhou,Yuxiang Zhou*

Main category: cs.RO

TL;DR: 本文介绍了Gemini Robotics 1.5和Gemini Robotics-ER 1.5，前者是一种多形态视觉-语言-动作（VLA）模型，后者是先进的具身推理（ER）模型。通过新型架构、动作迁移机制和多级内部推理，提升了机器人对复杂任务的理解与执行能力。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需要深入理解物理世界、高级推理能力和灵活控制。现有模型在跨形态学习、复杂任务分解和可解释性方面存在不足，因此需要更强大的多模态模型来实现真正的物理智能体。

Method: 提出Gemini Robotics 1.5的新型架构与动作迁移（MT）机制，使其能从异构的多形态机器人数据中学习；引入自然语言驱动的多级内部推理过程，实现“先思考后行动”；同时推出Gemini Robotics-ER 1.5，提升具身推理能力，包括视觉空间理解、任务规划和进度估计。

Result: Gemini Robotics 1.5展现出更强的泛化能力，能有效处理多步骤复杂任务，并具备更高的行为可解释性；Gemini Robotics-ER 1.5在具身推理各项指标上达到新SOTA。

Conclusion: 该模型家族推动了通用机器人向感知、思考与行动一体化的物理智能体迈进，为未来自主机器人系统奠定了基础。

Abstract: General-purpose robots need a deep understanding of the physical world,
advanced reasoning, and general and dexterous control. This report introduces
the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,
a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER
1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together
three major innovations. First, Gemini Robotics 1.5 features a novel
architecture and a Motion Transfer (MT) mechanism, which enables it to learn
from heterogeneous, multi-embodiment robot data and makes the VLA more general.
Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal
reasoning process in natural language. This enables the robot to "think before
acting" and notably improves its ability to decompose and execute complex,
multi-step tasks, and also makes the robot's behavior more interpretable to the
user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for
embodied reasoning, i.e., for reasoning capabilities that are critical for
robots, such as visual and spatial understanding, task planning, and progress
estimation. Together, this family of models takes us a step towards an era of
physical agents-enabling robots to perceive, think and then act so they can
solve complex multi-step tasks.

</details>


### [284] [Optimal swimming with body compliance in an overdamped medium](https://arxiv.org/abs/2510.03457)
*Jianfeng Lin,Tianyu Wang,Baxi Chong,Matthew Fernandez,Zhaochen Xu,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 本文扩展了几何力学方法，用于预测和优化具有柔顺性的波动游泳者的运动性能，提出了一种带有弹簧关节的Purcell三连杆游泳者模型，并在物理机器人上验证了该框架在颗粒介质中对柔顺运动的有效预测与优化能力。


<details>
  <summary>Details</summary>
Motivation: 现有几何力学方法假设运动模式能被精确执行，但实际中动物或机器人的柔性身体易受环境干扰，导致运动轨迹偏离预期，因此需要一种能考虑柔顺性影响的运动建模与优化方法。

Method: 通过在Purcell三连杆游泳者模型的关节处引入串联弹簧构建柔顺模型，结合阻力力理论推导身体动力学，并将几何力学融入运动预测与优化框架，以寻找最大位移下的最优控制策略。

Result: 所提框架能准确预测不同程序化、状态依赖柔顺性下的运动性能，并在物理缆驱三连杆无肢机器人上得到验证，展示了在颗粒介质中良好的运动预测与优化效果。

Conclusion: 柔顺性可作为提升运动鲁棒性的设计特征，本文建立了一套基于物理的系统性方法，用于建模和控制柔顺性游泳运动，适用于均质与非均质环境。

Abstract: Elongate animals and robots use undulatory body waves to locomote through
diverse environments. Geometric mechanics provides a framework to model and
optimize such systems in highly damped environments, connecting a prescribed
shape change pattern (gait) with locomotion displacement. However, existing
approaches assume precise execution of prescribed gaits, whereas in practice
environmental interactions with compliant bodies of animals or robots
frequently perturb the realized trajectories. In this work, we extend geometric
mechanics to predict locomotor performance and search for optimal swimming
strategy of compliant undulators. We introduce a compliant extension of
Purcell's three-link swimmer by incorporating series-connected springs at the
joints. Body dynamics are derived with resistive force theory. Geometric
mechanics is incorporated into movement prediction and into an optimization
framework that identifies strategies for controlling compliant swimmers to
achieve maximal displacement. We validate our framework on a physical
cable-driven three-link limbless robot, and demonstrate accurate prediction and
optimization of locomotor performance under varied programmed, state-dependent
compliance in a granular medium. Our results establish a systematic
physics-based approach for modeling and controlling compliant swimming
locomotion, highlighting compliance as a design feature that can be exploited
for robust movement in homogeneous and heterogeneous environments.

</details>


### [285] [Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching](https://arxiv.org/abs/2510.03460)
*Sibo Tian,Minghui Zheng,Xiao Liang*

Main category: cs.RO

TL;DR: 提出一种基于Flow Matching模型的学习方法，利用单视角点云为优化初始化学习近似最优解，无需环境先验知识，可直接从深度相机输入生成可行轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有采样式运动规划器在高维空间扩展性差且需后处理平滑路径，而优化式规划器虽能直接生成平滑轨迹但对初始化敏感、易陷入局部极小。因此需要一种高效且鲁棒的初始化方法以提升复杂动态环境中机器人的实时运动生成能力。

Method: 采用条件化于单视角点云的Flow Matching模型，学习优化问题的近似最优初始解，该方法不依赖环境先验（如障碍物位置和几何形状），直接从深度图像输入生成初始化轨迹，并结合轨迹优化框架进行精修。

Result: 在UR5e机械臂的仿真实验中，该方法单独使用即具有高成功率，相比传统和学习型基线初始化方法显著提升了轨迹优化的成功率，减少了优化迭代次数，并在未见环境中表现出强泛化能力。

Conclusion: 所提出的生成式初始化方法能有效提升优化基规划器的效率与鲁棒性，适用于复杂、动态的人机协作场景，具备实际应用潜力。

Abstract: Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)
systems, as robots need to respond to dynamic environments in real time by
continuously observing their surroundings and replanning their motions to
ensure both safe interactions and efficient task execution. Current
sampling-based motion planners face challenges in scaling to high-dimensional
configuration spaces and often require post-processing to interpolate and
smooth the generated paths, resulting in time inefficiency in complex
environments. Optimization-based planners, on the other hand, can incorporate
multiple constraints and generate smooth trajectories directly, making them
potentially more time-efficient. However, optimization-based planners are
sensitive to initialization and may get stuck in local minima. In this work, we
present a novel learning-based method that utilizes a Flow Matching model
conditioned on a single-view point cloud to learn near-optimal solutions for
optimization initialization. Our method does not require prior knowledge of the
environment, such as obstacle locations and geometries, and can generate
feasible trajectories directly from single-view depth camera input. Simulation
studies on a UR5e robotic manipulator in cluttered workspaces demonstrate that
the proposed generative initializer achieves a high success rate on its own,
significantly improves the success rate of trajectory optimization compared
with traditional and learning-based benchmark initializers, requires fewer
optimization iterations, and exhibits strong generalization to unseen
environments.

</details>


### [286] [A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control](https://arxiv.org/abs/2510.03471)
*Dingqi Zhang,Ran Tao,Sheng Cheng,Naira Hovakimyan,Mark W. Mueller*

Main category: cs.RO

TL;DR: 本文提出了一种基于RotorPy的模块化、易于部署的四旋翼控制仿真测试平台，支持多种干扰下的自适应控制方法评估，具备可重复性和系统性比较能力。


<details>
  <summary>Details</summary>
Motivation: 现有的四旋翼鲁棒自适应控制方法在不同任务、模拟器和实现中评估分散，难以进行系统性比较。

Method: 构建一个模块化的仿真测试平台，集成多种典型自适应与非自适应控制器库，支持风扰、负载变化、电机故障和控制延迟等多种干扰，并提供轨迹生成、干扰建模和分析工具。

Result: 实现了跨控制方法的可重复评估，展示了多干扰场景和轨迹类型的适用性，支持自动化压力测试。

Conclusion: 该测试平台有助于统一评估四旋翼控制算法，减少组件重复开发，促进鲁棒自适应控制方法的系统性研究与比较。

Abstract: Robust adaptive control methods are essential for maintaining quadcopter
performance under external disturbances and model uncertainties. However,
fragmented evaluations across tasks, simulators, and implementations hinder
systematic comparison of these methods. This paper introduces an
easy-to-deploy, modular simulation testbed for quadcopter control, built on
RotorPy, that enables evaluation under a wide range of disturbances such as
wind, payload shifts, rotor faults, and control latency. The framework includes
a library of representative adaptive and non-adaptive controllers and provides
task-relevant metrics to assess tracking accuracy and robustness. The unified
modular environment enables reproducible evaluation across control methods and
eliminates redundant reimplementation of components such as disturbance models,
trajectory generators, and analysis tools. We illustrate the testbed's
versatility through examples spanning multiple disturbance scenarios and
trajectory types, including automated stress testing, to demonstrate its
utility for systematic analysis. Code is available at
https://github.com/Dz298/AdaptiveQuadBench.

</details>


### [287] [Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems](https://arxiv.org/abs/2510.03472)
*Yulun Zhang,Alexandre O. G. Barbosa,Federico Pecora,Jiaoyang Li*

Main category: cs.RO

TL;DR: 本文研究了机器人分拣系统中目的地到滑道的任务映射优化问题，提出了一种结合进化算法和混合整数线性规划的简单优化方法，并通过模拟器验证了其在不同系统设置下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 在真实的机器人分拣系统中，任务映射的质量直接影响分拣吞吐量和下游处理效率，但由于与机器人目标分配、路径规划的耦合以及滑道周期性关闭等因素，优化任务映射十分困难。

Method: 首先形式化定义任务映射及任务映射优化问题（TMO），构建RSS模拟器用于评估不同映射策略，然后提出基于进化算法和混合整数线性规划的优化方法，并使用多样性质量算法分析多种映射方案的吞吐表现。

Result: 实验表明，相比贪心生成的任务映射，所提出的优化方法在不同地图规模、滑道数量和目的地数量的设置下均能显著提升系统吞吐量；同时发现分散的目的地滑道会增加下游处理时间。

Conclusion: 优化任务映射对提升机器人分拣系统的整体效率至关重要，结合进化算法与数学规划的方法能有效解决TMO问题，且多样化的映射策略分析有助于理解系统性能边界。

Abstract: We study optimizing a destination-to-chutes task mapping to improve
throughput in Robotic Sorting Systems (RSS), where a team of robots sort
packages on a sortation floor by transporting them from induct workstations to
eject chutes based on their shipping destinations (e.g. Los Angeles or
Pittsburgh). The destination-to-chutes task mapping is used to determine which
chutes a robot can drop its package. Finding a high-quality task mapping is
challenging because of the complexity of a real-world RSS. First, optimizing
task mapping is interdependent with robot target assignment and path planning.
Second, chutes will be CLOSED for a period of time once they receive sufficient
packages to allow for downstream processing. Third, task mapping quality
directly impacts the downstream processing, as scattered chutes for the same
destination increase package handling time. In this paper, we first formally
define task mappings and the problem of Task Mapping Optimization (TMO). We
then present a simulator of RSS to evaluate task mappings. We then present a
simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear
Programming, demonstrating the advantage of our optimized task mappings over
the greedily generated ones in various RSS setups with different map sizes,
numbers of chutes, and destinations. Finally, we use Quality Diversity
algorithms to analyze the throughput of a diverse set of task mappings. Our
code is available online at https://github.com/lunjohnzhang/tmo_public.

</details>


### [288] [Robust Permissive Controller Synthesis for Interval MDPs](https://arxiv.org/abs/2510.03481)
*Khang Vo Huynh,David Parker,Lu Feng*

Main category: cs.RO

TL;DR: 本文提出了首个针对区间马尔可夫决策过程（IMDP）的鲁棒容错控制器综合框架，通过混合整数线性规划方法在不确定动态环境下保证机器人满足可达性或奖励规范。


<details>
  <summary>Details</summary>
Motivation: 传统控制器综合方法通常生成单一确定性策略，难以应对机器人系统中普遍存在的感知噪声、执行不精确和模型抽象等不确定性；而现有容错控制器方法多假设精确转移概率，限制了实际应用。

Method: 将鲁棒容错控制器综合问题建模为混合整数线性规划（MILP），提出两种编码方式：基于顶点枚举的基线方法和避免显式枚举的可扩展对偶方法。

Result: 在四个基准域上的实验表明，所提方法能够合成鲁棒且最大程度容错的控制器，并可扩展至包含数十万状态的大规模IMDP。

Conclusion: 该框架首次实现了在IMDP上的鲁棒容错控制，兼顾了运行时灵活性与对不确定动态环境的适应能力，具有良好的可扩展性和实用性。

Abstract: We address the problem of robust permissive controller synthesis for robots
operating under uncertain dynamics, modeled as Interval Markov Decision
Processes (IMDPs). IMDPs generalize standard MDPs by allowing transition
probabilities to vary within intervals, capturing epistemic uncertainty from
sensing noise, actuation imprecision, and coarse system abstractions-common in
robotics. Traditional controller synthesis typically yields a single
deterministic strategy, limiting adaptability. In contrast, permissive
controllers (multi-strategies) allow multiple actions per state, enabling
runtime flexibility and resilience. However, prior work on permissive
controller synthesis generally assumes exact transition probabilities, which is
unrealistic in many robotic applications. We present the first framework for
robust permissive controller synthesis on IMDPs, guaranteeing that all
strategies compliant with the synthesized multi-strategy satisfy reachability
or reward-based specifications under all admissible transitions. We formulate
the problem as mixed-integer linear programs (MILPs) and propose two encodings:
a baseline vertex-enumeration method and a scalable duality-based method that
avoids explicit enumeration. Experiments on four benchmark domains show that
both methods synthesize robust, maximally permissive controllers and scale to
large IMDPs with up to hundreds of thousands of states.

</details>


### [289] [Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*](https://arxiv.org/abs/2510.03496)
*Vadivelan Murugesan,Rajasundaram Mathiazhagan,Sanjana Joshi,Aliasghar Arab*

Main category: cs.RO

TL;DR: 提出了一种基于细粒度人体运动预测和数字孪生验证的预测驱动安全规划框架，实现了100%主动避障。


<details>
  <summary>Details</summary>
Motivation: 现有机器人规划方法依赖于运动学模型，难以实现长时间范围内精确的人体运动预测，导致协作中碰撞风险高。

Method: 采用深度相机获取3D骨骼姿态，结合CNN-BiLSTM模型进行关节级运动预测；构建胶囊型人工势场（APF）将预测结果转化为碰撞风险，并在数字孪生环境中集成实时人体姿态预测与机器人仿真；当风险超过阈值时触发自适应RRT*（A-RRT*）规划器重新规划路径。

Result: 在50次试验中实现了100%的主动避障，保持大于250毫米的安全距离，重规划时间小于2秒。

Conclusion: 该方法通过融合预测性人体建模与数字孪生验证，在精度和可靠性上优于传统仅依赖运动学的规划器，有效提升了人机协作中的安全性与实时性。

Abstract: Human-robot collaboration requires precise prediction of human motion over
extended horizons to enable proactive collision avoidance. Unlike existing
planners that rely solely on kinodynamic models, we present a prediction-driven
safe planning framework that leverages granular, joint-by-joint human motion
forecasting validated in a physics-based digital twin. A capsule-based
artificial potential field (APF) converts these granular predictions into
collision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when
thresholds are exceeded. The depth camera is used to extract 3D skeletal poses
and a convolutional neural network-bidirectional long short-term memory
(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A
digital twin model integrates real-time human posture prediction placed in
front of a simulated robot to evaluate motions and physical contacts. The
proposed method enables validation of planned trajectories ahead of time and
bridging potential latency gaps in updating planned trajectories in real-time.
In 50 trials, our method achieved 100% proactive avoidance with > 250 mm
clearance and sub-2 s replanning, demonstrating superior precision and
reliability compared to existing kinematic-only planners through the
integration of predictive human modeling with digital twin validation.

</details>


### [290] [Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning](https://arxiv.org/abs/2510.03504)
*Yutong Wang,Yichun Qu,Tengxiang Wang,Lishuo Pan,Nora Ayanian*

Main category: cs.RO

TL;DR: 提出了一种基于高阶控制屏障函数（HOCBFs）的实时分布式多机器人导航框架，能够在复杂环境中保持和恢复连接性，同时避免碰撞。


<details>
  <summary>Details</summary>
Motivation: 在多机器人系统中，维持连接性至关重要，但容易受到障碍物和视觉遮挡的影响，需要一种鲁棒的连接性维护与恢复机制。

Method: 结合控制李雅普诺夫函数（CLF）和高阶控制屏障函数（HOCBF），通过贝塞尔参数化轨迹生成连续时间的统一MPC-CLF-CBF框架，实现导航中的连接性保持与恢复。

Result: 在仿真和4个Crazyflie纳米四旋翼无人机的物理实验中验证了该框架的有效性，能够在障碍物密集环境中实现连接性维护与恢复。

Conclusion: 所提出的统一MPC-CLF-CBF框架能够有效实现多机器人系统的实时分布式导航，兼顾连接性、避障和轨迹平滑性。

Abstract: Maintaining connectivity is crucial in many multi-robot applications, yet
fragile to obstacles and visual occlusions. We present a real-time distributed
framework for multi-robot navigation certified by high-order control barrier
functions (HOCBFs) that controls inter-robot proximity to maintain connectivity
while avoiding collisions. We incorporate control Lyapunov functions to enable
connectivity recovery from initial disconnected configurations and temporary
losses, providing robust connectivity during navigation in obstacle-rich
environments. Our trajectory generation framework concurrently produces
planning and control through a Bezier-parameterized trajectory, which naturally
provides smooth curves with arbitrary degree of derivatives. The main
contribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory
generation and control method for connectivity maintenance and recovery of
multi-robot systems. We validate the framework through extensive simulations
and a physical experiment with 4 Crazyflie nano-quadrotors.

</details>


### [291] [LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy](https://arxiv.org/abs/2510.03529)
*Zekai Liang,Xiao Liang,Soofiyan Atar,Sreyan Das,Zoe Chiu,Peihan Zhang,Florian Richter,Shanglei Liu,Michael C. Yip*

Main category: cs.RO

TL;DR: 本文提出了LapSurgie，首个基于人形机器人的腹腔镜遥操作框架，利用逆映射策略实现对商用腹腔镜工具的精确控制，并通过用户研究验证了该系统在远程手术中的可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 手术机器人目前主要局限于高资源医疗中心，加剧了农村和低资源地区的医疗不平等；为缩小这一差距，亟需可部署于现有手术环境、无需大规模基础设施改造的解决方案。

Method: 提出LapSurgie框架，采用逆映射策略以满足远程运动中心约束，使人形机器人能直接操控标准腹腔镜器械；系统配备立体视觉控制台提供实时反馈，并在多个平台上进行用户研究评估性能。

Result: 实验结果表明该遥操作框架能够实现精确的手-工具控制，且无需额外设置；跨平台用户研究表明系统有效，为人形机器人应用于腹腔镜手术提供了初步可行性证据。

Conclusion: LapSurgie展示了人形机器人在腹腔镜遥手术中的部署潜力，为解决医疗资源分布不均问题提供了可扩展、易集成的技术路径。

Abstract: Robotic laparoscopic surgery has gained increasing attention in recent years
for its potential to deliver more efficient and precise minimally invasive
procedures. However, adoption of surgical robotic platforms remains largely
confined to high-resource medical centers, exacerbating healthcare disparities
in rural and low-resource regions. To close this gap, a range of solutions has
been explored, from remote mentorship to fully remote telesurgery. Yet, the
practical deployment of surgical robotic systems to underserved communities
remains an unsolved challenge. Humanoid systems offer a promising path toward
deployability, as they can directly operate in environments designed for humans
without extensive infrastructure modifications -- including operating rooms. In
this work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic
teleoperation framework. The system leverages an inverse-mapping strategy for
manual-wristed laparoscopic instruments that abides to remote center-of-motion
constraints, enabling precise hand-to-tool control of off-the-shelf surgical
laparoscopic tools without additional setup requirements. A control console
equipped with a stereo vision system provides real-time visual feedback.
Finally, a comprehensive user study across platforms demonstrates the
effectiveness of the proposed framework and provides initial evidence for the
feasibility of deploying humanoid robots in laparoscopic procedures.

</details>


### [292] [Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection](https://arxiv.org/abs/2510.03532)
*Zekai Liang,Kazuya Miyata,Xiao Liang,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: 提出了一种统一检测关键点和边缘的新型框架，通过共享编码实现高效姿态估计，适用于手术机器人中的在线控制。


<details>
  <summary>Details</summary>
Motivation: 微创手术机器人具有长运动链和部分自由度在相机中可见性差，传统相机-机器人标定方法难以适用。

Method: 通过共享编码统一检测几何基元（关键点和轴线边缘），利用投影几何进行高效姿态估计，并在大规模合成数据上训练。

Result: 该方法在特征检测和姿态估计方面表现出快速性能和最先进的精度。

Conclusion: 所提方法在复杂手术环境中实现了高效、准确的相机-机器人标定，适合在线机器人控制。

Abstract: Accurate camera-to-robot calibration is essential for any vision-based
robotic control system and especially critical in minimally invasive surgical
robots, where instruments conduct precise micro-manipulations. However, MIS
robots have long kinematic chains and partial visibility of their degrees of
freedom in the camera, which introduces challenges for conventional
camera-to-robot calibration methods that assume stiff robots with good
visibility. Previous works have investigated both keypoint-based and
rendering-based approaches to address this challenge in real-world conditions;
however, they often struggle with consistent feature detection or have long
inference times, neither of which are ideal for online robot control. In this
work, we propose a novel framework that unifies the detection of geometric
primitives (keypoints and shaft edges) through a shared encoding, enabling
efficient pose estimation via projection geometry. This architecture detects
both keypoints and edges in a single inference and is trained on large-scale
synthetic data with projective labeling. This method is evaluated across both
feature detection and pose estimation, with qualitative and quantitative
results demonstrating fast performance and state-of-the-art accuracy in
challenging surgical environments.

</details>


### [293] [Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots](https://arxiv.org/abs/2510.03547)
*Carina Veil,Moritz Flaschel,Ellen Kuhl*

Main category: cs.RO

TL;DR: 提出一种基于形状空间图搜索的软体机器人路径规划方法，能够在毫秒级时间内生成避障且能量高效的运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 软体机器人由于其高度非线性和无限维运动学，在复杂环境中进行运动规划仍具挑战性。

Method: 结合形态弹性力学与主动丝理论建立生物力学模型，预计算形状库并构建k近邻图，利用符号距离函数剔除碰撞节点，定义几何距离与驱动能耗的多目标边成本。

Result: 在预计算图上使用Dijkstra算法可在毫秒内完成路径规划；考虑能量成本可显著降低驱动 effort，但会导致末端轨迹变长。

Conclusion: 形状空间图搜索为软体机器人提供了快速、可靠的路径规划方案，有望推动其在手术、工业和辅助场景中的实时应用。

Abstract: Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary
flexibility to bend, twist, and elongate in ways that rigid robots cannot.
However, their motion planning remains a challenge, especially in cluttered
environments with obstacles, due to their highly nonlinear and
infinite-dimensional kinematics. Here, we present a graph-based path planning
tool for an elephant-trunk-inspired soft robotic arm designed with three
artificial muscle fibers that allow for multimodal continuous deformation
through contraction. Using a biomechanical model inspired by morphoelasticity
and active filament theory, we precompute a shape library and construct a
$k$-nearest neighbor graph in \emph{shape space}, ensuring that each node
corresponds to a mechanically accurate and physically valid robot shape. For
the graph, we use signed distance functions to prune nodes and edges colliding
with obstacles, and define multi-objective edge costs based on geometric
distance and actuation effort, enabling energy-efficient planning with
collision avoidance. We demonstrate that our algorithm reliably avoids
obstacles and generates feasible paths within milliseconds from precomputed
graphs using Dijkstra's algorithm. We show that including energy costs can
drastically reduce the actuation effort compared to geometry-only planning, at
the expense of longer tip trajectories. Our results highlight the potential of
shape-space graph search for fast and reliable path planning in the field of
soft robotics, paving the way for real-time applications in surgical,
industrial, and assistive settings.

</details>


### [294] [Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning](https://arxiv.org/abs/2510.03599)
*Shafeef Omar,Majid Khadiv*

Main category: cs.RO

TL;DR: 提出了一种基于显式接触表示的多任务运动与操作策略学习统一框架，通过接触目标序列实现多样化任务的单一策略控制。


<details>
  <summary>Details</summary>
Motivation: 传统方法为不同任务设计独立策略，缺乏跨任务共享结构的能力，限制了泛化性和可扩展性。

Method: 采用基于接触目标（包括位置、时序和激活末端执行器）的任务定义方式，训练一个以接触计划为条件的强化学习策略，实现多任务统一控制。

Result: 在四足、人形机器人等多种形态平台上验证了该框架，单个策略可完成多种步态切换和双臂操作任务，且在未见场景中表现出良好的泛化能力。

Conclusion: 显式接触推理显著提升了策略的泛化性，为可扩展的运控一体化学习提供了有前景的基础。

Abstract: We present a unified framework for multi-task locomotion and manipulation
policy learning grounded in a contact-explicit representation. Instead of
designing different policies for different tasks, our approach unifies the
definition of a task through a sequence of contact goals-desired contact
positions, timings, and active end-effectors. This enables leveraging the
shared structure across diverse contact-rich tasks, leading to a single policy
that can perform a wide range of tasks. In particular, we train a
goal-conditioned reinforcement learning (RL) policy to realise given contact
plans. We validate our framework on multiple robotic embodiments and tasks: a
quadruped performing multiple gaits, a humanoid performing multiple biped and
quadrupedal gaits, and a humanoid executing different bimanual object
manipulation tasks. Each of these scenarios is controlled by a single policy
trained to execute different tasks grounded in contacts, demonstrating
versatile and robust behaviours across morphologically distinct systems. Our
results show that explicit contact reasoning significantly improves
generalisation to unseen scenarios, positioning contact-explicit policy
learning as a promising foundation for scalable loco-manipulation.

</details>


### [295] [Safety-Oriented Dynamic Path Planning for Automated Vehicles](https://arxiv.org/abs/2510.03640)
*Mostafa Emam,Matthias Gerdts*

Main category: cs.RO

TL;DR: 本文提出了一种双层控制框架，通过结合时变障碍物预测来增强道路边界，利用非线性模型预测控制（NMPC）和同伦约束松弛实现高效实时路径规划，并设计独立备份环路以提升自动驾驶在动态环境中的安全性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了在动态环境中实现更安全、可靠的自动驾驶，需要解决复杂场景下的实时路径规划与障碍物避让问题，特别是在主控制器无法及时生成轨迹时保障系统安全。

Method: 采用双层控制架构：主环路使用基于同伦约束松弛的非线性模型预测控制（NMPC）进行实时路径优化，结合时间依赖的网格投影更新障碍物信息；备份环路并行运行，提供安全回退轨迹。

Result: 实验表明该方法在多种驾驶场景中具备良好的实时性和鲁棒性，能够有效处理动态障碍物并保证系统安全性。

Conclusion: 所提出的双层控制框架显著提升了自动驾驶系统在复杂动态环境中的安全性和可靠性，是迈向实际应用的重要一步。

Abstract: Ensuring safety in autonomous vehicles necessitates advanced path planning
and obstacle avoidance capabilities, particularly in dynamic environments. This
paper introduces a bi-level control framework that efficiently augments road
boundaries by incorporating time-dependent grid projections of obstacle
movements, thus enabling precise and adaptive path planning. The main control
loop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path
optimization, wherein homotopy-based constraint relaxation is employed to
improve the solvability of the optimal control problem (OCP). Furthermore, an
independent backup loop runs concurrently to provide safe fallback trajectories
when an optimal trajectory cannot be computed by the main loop within a
critical time frame, thus enhancing safety and real-time performance. Our
evaluation showcases the benefits of the proposed methods in various driving
scenarios, highlighting the real-time applicability and robustness of our
approach. Overall, the framework represents a significant step towards safer
and more reliable autonomous driving in complex and dynamic environments.

</details>


### [296] [Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing](https://arxiv.org/abs/2510.03644)
*Mohammadjavad Javadi,Robin Chhabra*

Main category: cs.RO

TL;DR: 提出了一种基于特殊欧几里得群（SE(3)）的坐标无关Cosserat壳理论，用于高效建模硬磁软壳结构，适用于大变形和大旋转情况，避免了传统壳模型中的奇异性与锁死问题。


<details>
  <summary>Details</summary>
Motivation: 现有Cosserat杆理论适用于细长结构，但近年来用于运动和操作的软体机器人具有较大的宽长比，更接近二维壳结构，因此需要更合适的建模方法。

Method: 基于SE(3)李群结构，提出新的Cosserat壳理论，定义局部变形梯度，推导静力学平衡方程的强形式与弱形式，并线性化弱形式以实现有限元数值求解。

Result: 所提模型在严重旋转和位移下表现出优越性能，通过解析和实验验证，有效避免了奇异性与锁死现象。

Conclusion: 该方法为硬磁软壳机器人提供了高效、稳定的静态建模框架，适用于复杂形变控制，在软体机器人设计与分析中具有广泛应用前景。

Abstract: Cosserat rod theory is the popular approach to modeling ferromagnetic soft
robots as 1-Dimensional (1D) slender structures in most applications, such as
biomedical. However, recent soft robots designed for locomotion and
manipulation often exhibit a large width-to-length ratio that categorizes them
as 2D shells. For analysis and shape-morphing control purposes, we develop an
efficient coordinate-free static model of hard-magnetic shells found in soft
magnetic grippers and walking soft robots. The approach is based on a novel
formulation of Cosserat shell theory on the Special Euclidean group
($\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points
with six degrees of freedom (position & rotation) suitable for capturing the
behavior of a uniformly distributed array of spheroidal hard magnetic particles
embedded in the rheological elastomer. The shell's configuration manifold is
the space of all smooth embeddings $\mathbb{R}^2\rightarrow\mathbf{SE}(3)$.
According to a novel definition of local deformation gradient based on the Lie
group structure of $\mathbf{SE}(3)$, we derive the strong and weak forms of
equilibrium equations, following the principle of virtual work. We extract the
linearized version of the weak form for numerical implementations. The
resulting finite element approach can avoid well-known challenges such as
singularity and locking phenomenon in modeling shell structures. The proposed
model is analytically and experimentally validated through a series of test
cases that demonstrate its superior efficacy, particularly when the shell
undergoes severe rotations and displacements.

</details>


### [297] [An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion](https://arxiv.org/abs/2510.03660)
*Mohammadjavad Javadi,Charlie Wadds,Robin Chhabra*

Main category: cs.RO

TL;DR: 提出了一种受软体尺蠖启发的完全无 tether 的软体机器人，具有磁驱动的弯曲柔性结构，可实现多种运动模式，包括行走、转向、游泳和负载运输。


<details>
  <summary>Details</summary>
Motivation: 为了推动软体机器人系统在复杂和多任务环境中的实际应用，需要开发无需外部约束的无 tether 软体机器人。

Method: 基于软体尺蠖的运动机理，设计了磁驱动的弯曲柔性结构，并集成了轻量化板载控制电路和摄像头以实现无线控制与环境感知。

Result: 机器人总质量为102.63克，最大行走速度达3.74 cm/s，游泳速度达0.82 cm/s，成功实现了多种运动模式及有效载荷运输。

Conclusion: 通过结构优化和系统集成，该机器人在无需外部基础设施的情况下展现出良好的动态性能和多模态运动能力，验证了其在多样化环境中的应用潜力。

Abstract: Untethered soft robots are essential for advancing the real-world deployment
of soft robotic systems in diverse and multitasking environments. Inspired by
soft-bodied inchworm, we present a fully untethered soft robot with a curved,
flexible structure actuated by magnetic forces. The robot has a total mass of
102.63 g and demonstrates multimodal locomotion, achieving a maximum walking
speed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight
onboard control circuit enables wireless command transmission, while an
integrated camera provides environmental perception. Through structural
optimization and system-level integration, the robot successfully performs
walking, steering, swimming, and payload transport without reliance on external
infrastructure. The robot's dynamic performance and locomotion capabilities are
systematically validated through experimental characterization.

</details>


### [298] [Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments](https://arxiv.org/abs/2510.03677)
*Salim Rezvani,Ammar Jaleel Mahmood,Robin Chhabra*

Main category: cs.RO

TL;DR: 本文首次系统研究了视觉退化（如模糊、椒盐噪声和高斯噪声）对机器人自建模的影响，并提出了一种任务感知的去噪框架，结合经典图像恢复与形态保持约束，以及语义分割技术，显著提升了在噪声和复杂背景下的自建模鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有机器人自主建模方法在真实感知条件下（如图像噪声和杂乱背景）表现脆弱，缺乏对视觉退化影响的系统分析和有效应对机制。

Method: 通过仿真与实物实验评估多种视觉退化对形态预测、轨迹规划和损伤恢复的影响；提出任务感知去噪框架，结合经典图像复原、形态保持约束和语义分割以提升鲁棒性。

Result: 所提方法在模拟和真实平台上均能恢复接近基线的性能，而现有方法性能显著下降；在噪声和复杂场景中显著提升了自建模的稳定性与准确性。

Conclusion: 该工作增强了视觉自建模在现实环境中的鲁棒性，为部署真正自感知的机器人奠定了实用基础。

Abstract: Robots with internal visual self-models promise unprecedented adaptability,
yet existing autonomous modeling pipelines remain fragile under realistic
sensing conditions such as noisy imagery and cluttered backgrounds. This paper
presents the first systematic study quantifying how visual
degradations--including blur, salt-and-pepper noise, and Gaussian noise--affect
robotic self-modeling. Through both simulation and physical experiments, we
demonstrate their impact on morphology prediction, trajectory planning, and
damage recovery in state-of-the-art pipelines. To overcome these challenges, we
introduce a task-aware denoising framework that couples classical restoration
with morphology-preserving constraints, ensuring retention of structural cues
critical for self-modeling. In addition, we integrate semantic segmentation to
robustly isolate robots from cluttered and colorful scenes. Extensive
experiments show that our approach restores near-baseline performance across
simulated and physical platforms, while existing pipelines degrade
significantly. These contributions advance the robustness of visual
self-modeling and establish practical foundations for deploying self-aware
robots in unpredictable real-world environments.

</details>


### [299] [EmbodiSwap for Zero-Shot Robot Imitation Learning](https://arxiv.org/abs/2510.03706)
*Eadom Dessalene,Pavan Mantripragada,Michael Maynord,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: 提出EmbodiSwap方法，用于在人类视频上生成逼真的合成机器人覆盖，实现零样本模仿学习，并通过V-JEPA视觉骨干提升机器人操作策略性能。


<details>
  <summary>Details</summary>
Motivation: 弥合野外第一人称人类视频与目标机器人实体之间的具身差异，实现无需真实机器人数据的零样本模仿学习。

Method: 利用EmbodiSwap生成合成机器人叠加视频，采用V-JEPA作为视觉骨干网络，训练闭环机器人操作策略。

Result: 在真实环境中测试，零样本训练的V-JEPA模型达到82%的成功率，优于其他基线方法。

Conclusion: EmbodiSwap结合V-JEPA可有效实现跨实体的零样本模仿学习，推动无需真实数据的机器人策略训练。

Abstract: We introduce EmbodiSwap - a method for producing photorealistic synthetic
robot overlays over human video. We employ EmbodiSwap for zero-shot imitation
learning, bridging the embodiment gap between in-the-wild ego-centric human
video and a target robot embodiment. We train a closed-loop robot manipulation
policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a
visual backbone, repurposing V-JEPA from the domain of video understanding to
imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms
alternative vision backbones more conventionally used within robotics. In
real-world tests, our zero-shot trained V-JEPA model achieves an $82\%$ success
rate, outperforming a few-shot trained $\pi_0$ network as well as $\pi_0$
trained over data produced by EmbodiSwap. We release (i) code for generating
the synthetic robot overlays which takes as input human videos and an arbitrary
robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize
over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference
code, to facilitate reproducible research and broader adoption.

</details>


### [300] [Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy](https://arxiv.org/abs/2510.04774)
*Weixu Zhu,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: 本文提出了一种自组织神经系统（SoNS），用于增强机器人集群的行为设计和全局状态估计，并结合外部大语言模型（LLM）实现在线自动代码生成。实验表明，当集群受阻时，系统可动态请求并执行LLM生成的代码，成功完成任务的概率达85%。


<details>
  <summary>Details</summary>
Motivation: 为解决机器人集群行为设计复杂、缺乏全局感知以及难以应对突发环境变化的问题，需要一种能支持自动适应与实时修复的智能控制架构。

Method: 采用自组织神经系统（SoNS）为机器人集群提供行为设计框架和全局配置估计，并在运行时与外部大语言模型（LLM）交互，根据当前任务瓶颈自动生成并执行修复代码。

Result: 在6个真实机器人和超过30个机器人的仿真试验中，SoNS增强的集群在受阻后通过调用LLM生成代码，实现了85%的任务成功率。

Conclusion: SoNS结合LLM能够有效提升机器人集群的自主适应能力和任务鲁棒性，展示了在线代码生成在 swarm robotics 中的可行性与潜力。

Abstract: Our recently introduced self-organizing nervous system (SoNS) provides robot
swarms with 1) ease of behavior design and 2) global estimation of the swarm
configuration and its collective environment, facilitating the implementation
of online automatic code generation for robot swarms. In a demonstration with 6
real robots and simulation trials with >30 robots, we show that when a
SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code
generated by an external LLM on the fly, completing its mission with an 85%
success rate.

</details>


### [301] [Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics](https://arxiv.org/abs/2510.03768)
*Aydin Ahmadi,Baris Akgun*

Main category: cs.RO

TL;DR: 提出一种基于模型的非抓取式桌面推动物体框架，使用单一学习模型实现多任务处理，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的平面推动物体方法多针对狭窄能力，限制了广泛应用，因此需要一个能处理多种任务且具有良好泛化性的框架。

Method: 采用基于GRU的循环架构并添加非线性层来建模物体-环境动态，结合定制的状态-动作表示，并与基于采样的MPPI控制器集成以生成自适应、面向任务的动作。

Result: 在模拟和真实环境中验证了系统的有效性，展示了高精度定位、轨迹跟踪和避障的高性能，且通过改变目标函数即可完成不同任务而无需重新训练。

Conclusion: 该框架能够有效支持多种推动物体任务，具备良好的稳定性与泛化能力，适用于仿真到现实的迁移。

Abstract: Data-driven planar pushing methods have recently gained attention as they
reduce manual engineering effort and improve generalization compared to
analytical approaches. However, most prior work targets narrow capabilities
(e.g., side switching, precision, or single-task training), limiting broader
applicability. We present a model-based framework for non-prehensile tabletop
pushing that uses a single learned model to address multiple tasks without
retraining. Our approach employs a recurrent GRU-based architecture with
additional non-linear layers to capture object-environment dynamics while
ensuring stability. A tailored state-action representation enables the model to
generalize across uncertain dynamics, variable push lengths, and diverse tasks.
For control, we integrate the learned dynamics with a sampling-based Model
Predictive Path Integral (MPPI) controller, which generates adaptive,
task-oriented actions. This framework supports side switching, variable-length
pushes, and objectives such as precise positioning, trajectory following, and
obstacle avoidance. Training is performed in simulation with domain
randomization to support sim-to-real transfer. We first evaluate the
architecture through ablation studies, showing improved prediction accuracy and
stable rollouts. We then validate the full system in simulation and real-world
experiments using a Franka Panda robot with markerless tracking. Results
demonstrate high success rates in precise positioning under strict thresholds
and strong performance in trajectory tracking and obstacle avoidance. Moreover,
multiple tasks are solved simply by changing the controller's objective
function, without retraining. While our current focus is on a single object
type, we extend the framework by training on wider push lengths and designing a
balanced controller that reduces the number of steps for longer-horizon goals.

</details>


### [302] [Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets](https://arxiv.org/abs/2510.03776)
*Tiago Rodrigues de Almeida,Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Johannes A. Stork,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 本文研究了在复杂动态环境中，基于类别条件的轨迹预测方法，提出了基于模式和深度学习的基线模型，并在机器人和户外数据集上进行了评估。结果表明，考虑类别标签能提高预测准确性，但在数据不平衡或新环境数据不足时，基于模式的方法可能优于深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 为了减少异质智能体运动预测中的不确定性并提高预测精度，尤其是在数据有限或新环境中，需要探索类别条件对轨迹预测的影响。

Method: 分析了多种类别条件下的轨迹预测方法，提出了一组基于条件模式和高效的深度学习基线模型，并在TH"OR-MAGNI和Stanford Drone Dataset两个数据集上进行评估。

Result: 实验表明，在大多数设置下，考虑类别标签可提升预测准确性；在数据不平衡或数据不足的新环境中，基于模式的方法表现优于深度学习方法。

Conclusion: 类别信息有助于提升轨迹预测性能，但在数据受限场景下，简单的模式基线方法可能比深度学习更适用，为实际机器人应用提供了实用指导。

Abstract: Robots and other intelligent systems navigating in complex dynamic
environments should predict future actions and intentions of surrounding agents
to reach their goals efficiently and avoid collisions. The dynamics of those
agents strongly depends on their tasks, roles, or observable labels.
Class-conditioned motion prediction is thus an appealing way to reduce forecast
uncertainty and get more accurate predictions for heterogeneous agents.
However, this is hardly explored in the prior art, especially for mobile robots
and in limited data applications. In this paper, we analyse different
class-conditioned trajectory prediction methods on two datasets. We propose a
set of conditional pattern-based and efficient deep learning-based baselines,
and evaluate their performance on robotics and outdoors datasets (TH\"OR-MAGNI
and Stanford Drone Dataset). Our experiments show that all methods improve
accuracy in most of the settings when considering class labels. More
importantly, we observe that there are significant differences when learning
from imbalanced datasets, or in new environments where sufficient data is not
available. In particular, we find that deep learning methods perform better on
balanced datasets, but in applications with limited data, e.g., cold start of a
robot in a new environment, or imbalanced classes, pattern-based methods may be
preferable.

</details>


### [303] [COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments](https://arxiv.org/abs/2510.03875)
*Niranjan Kumar Ilampooranan,Constantinos Chamzas*

Main category: cs.RO

TL;DR: 本文提出了COVER框架，用于在半静态环境中实现具有固定时间保证的运动规划。通过划分障碍物配置空间并在每个分区中求解路径，COVER能够系统地验证路图的可行性，并保证在已验证区域内的查询可在固定时间内完成。


<details>
  <summary>Details</summary>
Motivation: 在半静态环境中，大多数障碍物是静态的，仅有少量变化，这种结构化变异性可被利用以提供比通用运动规划更强的保证。然而，现有方法缺乏形式化保证或依赖于对障碍物配置的限制性离散化，限制了其在实际场景中的应用。

Method: 提出COVER框架，通过增量构建覆盖验证的路图，将障碍物配置空间进行划分，并在每个分区中求解可行路径，系统地验证各分区路图的可行性，从而确保在固定时间内响应运动规划查询。

Result: 在7自由度Panda机器人模拟实验中，COVER在桌面和货架任务中表现出比先前方法更广的覆盖范围和更高的查询成功率。

Conclusion: COVER能够在半静态环境中有效实现具有形式化保证的固定时间运动规划，显著提升了查询成功率和覆盖范围，适用于更广泛的现实应用场景。

Abstract: Having the ability to answer motion-planning queries within a fixed time
budget is critical for the widespread deployment of robotic systems.
Semi-static environments, where most obstacles remain static but a limited set
can vary across queries, exhibit structured variability that can be
systematically exploited to provide stronger guarantees than in general
motion-planning problems. However, prior approaches in this setting either lack
formal guarantees or rely on restrictive discretizations of obstacle
configurations, limiting their applicability in realistic domains. This paper
introduces COVER, a novel framework that incrementally constructs a
coverage-verified roadmap in semi-static environments. By partitioning the
obstacle configuration space and solving for feasible paths within each
partition, COVER systematically verifies feasibility of the roadmap in each
partition and guarantees fixed-time motion planning queries within the verified
regions. We validate COVER with a 7-DOF simulated Panda robot performing table
and shelf tasks, demonstrating that COVER achieves broader coverage with higher
query success rates than prior works.

</details>


### [304] [Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning](https://arxiv.org/abs/2510.03885)
*Sunghwan Kim,Woojeh Chung,Zhirui Dai,Dwait Bhatt,Arth Shukla,Hao Su,Yulun Tian,Nikolay Atanasov*

Main category: cs.RO

TL;DR: 本文提出了Seeing the Bigger Picture (SBP)，一种基于3D潜在地图的端到端策略学习方法，能够在移动操作任务中实现超越视野范围的空间和时间推理，显著优于纯图像驱动的策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的机器人操作策略受限于当前视野，缺乏对环境的长期记忆和全局理解，难以处理复杂或长时序的任务。

Method: SBP将多视角观测逐步融合到一个场景特定的3D潜在特征网格中，利用预训练解码器重建目标嵌入，并在任务执行过程中在线优化地图特征；策略将潜在地图作为状态变量，并通过3D特征聚合器获取全局上下文信息。

Result: SBP在场景级移动操作和序列化桌面操作任务中表现出色，能够进行全局推理、利用地图作为长时记忆，并在分布内和新场景中均优于图像基线方法，例如在序列操作任务中成功率提升25%。

Conclusion: 基于3D潜在地图的表示学习能有效增强机器人的空间与时间推理能力，为复杂操作任务提供更鲁棒和泛化的解决方案。

Abstract: In this paper, we demonstrate that mobile manipulation policies utilizing a
3D latent map achieve stronger spatial and temporal reasoning than policies
relying solely on images. We introduce Seeing the Bigger Picture (SBP), an
end-to-end policy learning approach that operates directly on a 3D map of
latent features. In SBP, the map extends perception beyond the robot's current
field of view and aggregates observations over long horizons. Our mapping
approach incrementally fuses multiview observations into a grid of
scene-specific latent features. A pre-trained, scene-agnostic decoder
reconstructs target embeddings from these features and enables online
optimization of the map features during task execution. A policy, trainable
with behavior cloning or reinforcement learning, treats the latent map as a
state variable and uses global context from the map obtained via a 3D feature
aggregator. We evaluate SBP on scene-level mobile manipulation and sequential
tabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons
globally over the scene, (ii) leverages the map as long-horizon memory, and
(iii) outperforms image-based policies in both in-distribution and novel
scenes, e.g., improving the success rate by 25% for the sequential manipulation
task.

</details>


### [305] [NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation](https://arxiv.org/abs/2510.03895)
*Zheng Huang,Mingyu Liu,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Xiaoman Li,Yiduo Jia,Hao Zhong,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: 提出NoTVLA框架，通过稀疏轨迹训练缓解视觉-语言-动作模型中的灾难性遗忘问题，在多任务中实现更好泛化，且计算成本低、无需腕部摄像头。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型在实际部署中因依赖连续动作序列而导致的灾难性遗忘和知识隔离问题。

Method: 聚焦于机器人末端执行器的稀疏轨迹，结合时间压缩和空间推理剪枝进行轨迹规划，并以此进行模型训练。

Result: 在多任务评估中优于pi0，计算资源消耗降低一个数量级以上，无需腕部摄像头，零样本性能更优，保持语言能力并支持跨平台部署。

Conclusion: NoTVLA有效缓解灾难性遗忘，兼具高性能、高泛化性和低部署成本，接近单任务专家模型精度。

Abstract: Vision-Language-Action (VLA) models represent a pivotal advance in embodied
intelligence, yet they confront critical barriers to real-world deployment,
most notably catastrophic forgetting. This issue stems from their overreliance
on continuous action sequences or action chunks, which inadvertently create
isolated data silos that disrupt knowledge retention across tasks. To tackle
these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)
framework: a novel approach that narrows its focus to sparse trajectories,
thereby avoiding the catastrophic forgetting associated with dense trajectory
fine-tuning. A key innovation of NoTVLA lies in its trajectory planning
strategy: instead of centering on the target object's trajectory, it leverages
temporal compression and spatial reasoning pruning specifically for the robot
end effector's trajectory. Furthermore, training is conducted using these
sparse trajectories rather than dense action trajectories, an optimization that
delivers remarkable practical advantages with better performance in zero-shot.
In multi-task evaluation scenarios, NoTVLA achieves superior performance and
generalization compared to pi0 while operating under two critical constraints:
it uses over an order of magnitude less computing power than pi0 and requires
no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy
closely approximates that of single-task expert models. Crucially, it also
preserves the model's inherent language capabilities, enabling zero-shot
generalization in specific scenarios, supporting unified model deployment
across multiple robot platforms, and fostering a degree of generalization even
when perceiving tasks from novel perspectives.

</details>


### [306] [WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding](https://arxiv.org/abs/2510.03910)
*Akhil Padmanabha,Jessie Yuan,Tanisha Mehta,Rajat Kumar Jenamani,Eric Hu,Victoria de León,Anthony Wertz,Janavi Gupta,Ben Dodson,Yunting Yan,Carmel Majidi,Tapomayukh Bhattacharjee,Zackory Erickson*

Main category: cs.RO

TL;DR: 本文提出了一种名为WAFFLE的可穿戴系统，利用传感器数据和机器学习模型预测进食时机，提升机器人喂食系统的自然性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人喂食系统因难以准确估计用户进食时机而受限，影响了其广泛应用。

Method: 通过14名参与者的咬食时间数据训练监督回归模型，并结合可调节的主动性阈值将预测转化为控制指令，使用可穿戴传感器捕捉头部运动、咀嚼和说话等自然用户信号。

Result: 在15名无运动障碍参与者中，WAFFLE在控制感、机器人理解度和工作负荷方面表现优于或相当于基线方法，并在个体与社交用餐场景中更受偏好；在2名运动障碍患者家中使用Kinova机器人验证了系统的通用性。

Conclusion: WAFFLE能有效实现跨用户、设备、环境和用餐场景的自然且反应灵敏的咬食时机预测，具有良好的实际应用前景。

Abstract: Millions of people around the world need assistance with feeding. Robotic
feeding systems offer the potential to enhance autonomy and quality of life for
individuals with impairments and reduce caregiver workload. However, their
widespread adoption has been limited by technical challenges such as estimating
bite timing, the appropriate moment for the robot to transfer food to a user's
mouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with
LEarned bite timing, a system that accurately predicts bite timing by
leveraging wearable sensor data to be highly reactive to natural user cues such
as head movements, chewing, and talking. We train a supervised regression model
on bite timing data from 14 participants and incorporate a user-adjustable
assertiveness threshold to convert predictions into proceed or stop commands.
In a study with 15 participants without motor impairments with the Obi feeding
robot, WAFFLE performs statistically on par with or better than baseline
methods across measures of feeling of control, robot understanding, and
workload, and is preferred by the majority of participants for both individual
and social dining. We further demonstrate WAFFLE's generalizability in a study
with 2 participants with motor impairments in their home environments using a
Kinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling
natural, reactive bite timing that generalizes across users, robot hardware,
robot positioning, feeding trajectories, foods, and both individual and social
dining contexts.

</details>


### [307] [TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry](https://arxiv.org/abs/2510.03919)
*Matthew Lisondra,Junseo Kim,Glenn Takashi Shimoda,Kourosh Zareinia,Sajad Saeedi*

Main category: cs.RO

TL;DR: 本文提出了一种基于焦点平面传感器处理器阵列（FPSP）的紧耦合6自由度视觉惯性里程计TCB-VIO，采用多状态约束卡尔曼滤波器（MSCKF），在250 FPS高帧率下运行，并利用400 Hz的IMU数据，有效减少空间与时间漂移，性能优于ROVIO、VINS-Mono和ORB-SLAM3等现有方法。


<details>
  <summary>Details</summary>
Motivation: 为解决传统视觉惯性里程计（VIO）中存在的空间漂移（来自视觉位姿估计）和时间漂移（来自惯性测量）问题，同时利用FPSP传感器实现低延迟、高帧率的实时处理优势。

Method: 提出TCB-VIO，采用紧耦合的多状态约束卡尔曼滤波器（MSCKF），在FPSP上以250 FPS高帧率运行，融合400 Hz的IMU数据，实现高频视觉-惯性融合。

Result: TCB-VIO在性能上优于ROVIO、VINS-Mono和ORB-SLAM3等当前主流VIO方法，有效抑制了空间与时间漂移。

Conclusion: 基于FPSP的高帧率TCB-VIO框架能够高效融合视觉与惯性信息，显著提升VIO系统的精度与实时性，是未来嵌入式视觉系统的一个有前景的方向。

Abstract: Vision algorithms can be executed directly on the image sensor when
implemented on the next-generation sensors known as focal-plane
sensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs
greatly improve latency, reducing the problems associated with the bottleneck
of data transfer from a vision sensor to a processor. FPSPs accelerate
vision-based algorithms such as visual-inertial odometry (VIO). However, VIO
frameworks suffer from spatial drift due to the vision-based pose estimation,
whilst temporal drift arises from the inertial measurements. FPSPs circumvent
the spatial drift by operating at a high frame rate to match the high-frequency
output of the inertial measurements. In this paper, we present TCB-VIO, a
tightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman
Filter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU
measurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:
ROVIO, VINS-Mono, and ORB-SLAM3.

</details>


### [308] [A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM](https://arxiv.org/abs/2510.03948)
*Otobong Jerome,Geesara Prathap Kulathunga,Devitt Dmitry,Eugene Murawjow,Alexandr Klimchik*

Main category: cs.RO

TL;DR: 提出一种针对非公路环境的新型全局路径规划方法，兼顾实时性、运动学可行性和内存效率，在大规模地图上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统路径规划方法在非公路环境中表现不佳，未能考虑实时性、运动学可行性和内存效率等关键因素。

Method: 构建基于像素坐标系的中间地图，融合地理特征，并将规划问题分解为基于图的路径规划、运动学可行性检查和路径平滑三个子问题。

Result: 在数平方公里的大规模非公路环境中，平均1.5秒内找到可行路径，极端条件下内存占用约1.5GB。

Conclusion: 该方法有效解决了非公路环境下自主导航的挑战，具有广泛适用性，可用于搜救和农业作业等任务。

Abstract: Off-road environments present unique challenges for autonomous navigation due
to their complex and unstructured nature. Traditional global path-planning
methods, which typically aim to minimize path length and travel time, perform
poorly on large-scale maps and fail to account for critical factors such as
real-time performance, kinematic feasibility, and memory efficiency. This paper
introduces a novel global path-planning method specifically designed for
off-road environments, addressing these essential factors. The method begins by
constructing an intermediate map within the pixel coordinate system,
incorporating geographical features like off-road trails, waterways, restricted
and passable areas, and trees. The planning problem is then divided into three
sub-problems: graph-based path planning, kinematic feasibility checking, and
path smoothing. This approach effectively meets real-time performance
requirements while ensuring kinematic feasibility and efficient memory use. The
method was tested in various off-road environments with large-scale maps up to
several square kilometers in size, successfully identifying feasible paths in
an average of 1.5 seconds and utilizing approximately 1.5GB of memory under
extreme conditions. The proposed framework is versatile and applicable to a
wide range of off-road autonomous navigation tasks, including search and rescue
missions and agricultural operations.

</details>


### [309] [SITCOM: Scaling Inference-Time COMpute for VLAs](https://arxiv.org/abs/2510.04041)
*Ayudh Saxena,Harsh Shah,Sandeep Routray,Rishi Rajesh Shah,Esha Pahwa*

Main category: cs.RO

TL;DR: 本文提出了SITCOM框架，通过引入基于模型的推演和奖励驱动的轨迹选择，增强预训练的视觉-语言-动作（VLA）模型，从而提升其在长视野任务中的鲁棒性和规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在动态任务中缺乏前瞻机制且容易产生累积误差，难以应对长视野规划和真实环境泛化问题。

Method: SITCOM结合了Model Predictive Control思想，利用一个在大规模数据上训练并针对SIMPLER环境微调的Transformer动力学模型，进行多步动作推演，并通过模拟器中的奖励函数评估和选择最优轨迹。

Result: 在SIMPLER环境中多个任务上的实验表明，结合良好奖励函数的SITCOM可将任务完成率从48%提升至72%。

Conclusion: SITCOM能有效增强现有VLA模型的长期规划能力，通过推理时计算扩展实现了从单步决策到鲁棒长视野规划的转变。

Abstract: Learning robust robotic control policies remains a major challenge due to the
high cost of collecting labeled data, limited generalization to unseen
environments, and difficulties in planning over long horizons. While
Vision-Language-Action (VLA) models offer a promising solution by grounding
natural language instructions into single-step control commands, they often
lack mechanisms for lookahead and struggle with compounding errors in dynamic
tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs
(SITCOM), a framework that augments any pretrained VLA with model-based
rollouts and reward-based trajectory selection, inspired by Model Predictive
Control algorithm. SITCOM leverages a learned dynamics model to simulate
multi-step action rollouts to select the best candidate plan for real-world
execution, transforming one-shot VLAs into robust long-horizon planners. We
develop an efficient transformer-based dynamics model trained on large-scale
BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim
gap, and score candidate rollouts using rewards from simulator. Through
comprehensive evaluation across multiple tasks and settings in the SIMPLER
environment, we demonstrate that SITCOM when combined with a good reward
function can significantly improve task completion rate from 48% to 72% using
trained dynamics model.

</details>


### [310] [Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback](https://arxiv.org/abs/2510.04074)
*Chung-Pang Wang,Changwei Chen,Xiao Liang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: 提出了一种反馈驱动的自主组织分离框架，通过内窥镜图像显式推理拓扑变化，并结合可视性度量和控制器设计，提升了手术自主性、鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有手术机器人反馈方法在处理组织分离中的拓扑和感知挑战方面存在局限，难以适应动态环境。

Method: 提出一个反馈框架，利用内窥镜图像分析每次分离操作后的拓扑变化，引入可视性度量以量化组织暴露，并设计最优控制器主动调整组织以提升可见性，结合基于规划和学习的方法实现在线策略自适应。

Result: 实验表明，所提反馈机制显著增强了复杂手术场景下的自主性、错误减少和系统鲁棒性。

Conclusion: 该反馈驱动框架有效应对了组织分离中的动态变化，通过结构化反馈和可视性优化，推动了自主手术系统的实际应用。

Abstract: Autonomous surgical systems must adapt to highly dynamic environments where
tissue properties and visual cues evolve rapidly. Central to such adaptability
is feedback: the ability to sense, interpret, and respond to changes during
execution. While feedback mechanisms have been explored in surgical robotics,
ranging from tool and tissue tracking to error detection, existing methods
remain limited in handling the topological and perceptual challenges of tissue
dissection. In this work, we propose a feedback-enabled framework for
autonomous tissue dissection that explicitly reasons about topological changes
from endoscopic images after each dissection action. This structured feedback
guides subsequent actions, enabling the system to localize dissection progress
and adapt policies online. To improve the reliability of such feedback, we
introduce visibility metrics that quantify tissue exposure and formulate
optimal controller designs that actively manipulate tissue to maximize
visibility. Finally, we integrate these feedback mechanisms with both
planning-based and learning-based dissection methods, and demonstrate
experimentally that they significantly enhance autonomy, reduce errors, and
improve robustness in complex surgical scenarios.

</details>


### [311] [From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents](https://arxiv.org/abs/2510.04076)
*Amin Vahidi-Moghaddam,Sayed Pedram Haeri Boroujeni,Iman Jebellat,Ehsan Jebellat,Niloufar Mehrabi,Zhaojian Li*

Main category: cs.RO

TL;DR: 本文探讨了现代控制应用中数据驱动策略的挑战，提出八种降低计算复杂度的方法，并在机器人、软体机器人和车辆运动控制等实际应用中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代控制系统（如机器人和车辆）需要快速、安全且高性能的控制策略，但数据驱动方法常面临计算量大、响应慢和内存需求高等问题，限制了其在资源受限系统中的应用。

Method: 提出了八种降低数据驱动控制策略计算复杂度的技术，包括降阶建模、函数近似策略学习和凸松弛等，并结合模型预测控制、机器学习和强化学习等方法进行优化。

Result: 所提出的方法在多个真实世界应用场景中展示了有效性，显著降低了计算负担，同时保持了控制性能和安全性。

Conclusion: 通过引入多种复杂度降低技术，可以有效提升数据驱动控制策略在资源受限系统中的实用性，为实际部署提供了可行路径。

Abstract: One of the main challenges in modern control applications, particularly in
robot and vehicle motion control, is achieving accurate, fast, and safe
movement. To address this, optimal control policies have been developed to
enforce safety while ensuring high performance. Since basic first-principles
models of real systems are often available, model-based controllers are widely
used. Model predictive control (MPC) is a leading approach that optimizes
performance while explicitly handling safety constraints. However, obtaining
accurate models for complex systems is difficult, which motivates data-driven
alternatives. ML-based MPC leverages learned models to reduce reliance on
hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal
policies directly from interaction data. Data-enabled predictive control
(DeePC) goes further by bypassing modeling altogether, directly learning safe
policies from raw input-output data. Recently, large language model (LLM)
agents have also emerged, translating natural language instructions into
structured formulations of optimal control problems. Despite these advances,
data-driven policies face significant limitations. They often suffer from slow
response times, high computational demands, and large memory needs, making them
less practical for real-world systems with fast dynamics, limited onboard
computing, or strict memory constraints. To address this, various technique,
such as reduced-order modeling, function-approximated policy learning, and
convex relaxations, have been proposed to reduce computational complexity. In
this paper, we present eight such approaches and demonstrate their
effectiveness across real-world applications, including robotic arms, soft
robots, and vehicle motion control.

</details>


### [312] [HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments](https://arxiv.org/abs/2510.04161)
*Longrui Yang,Yiyu Wang,Jingfan Tang,Yunpeng Lv,Shizhe Zhao,Chao Cao,Zhongqiang Ren*

Main category: cs.RO

TL;DR: 提出了一种名为HEHA的分层探索方法，用于多异构机器人在未知环境中的自主路径规划，通过全局和局部规划协同工作，显著减少了探索时间。


<details>
  <summary>Details</summary>
Motivation: 解决多异构机器人在复杂地形中自主探索时的高效任务分配与路径规划问题，尤其是在动态未知环境中快速迭代求解大规模约束优化问题的挑战。

Method: 采用分层架构，全局规划中提出PEAF（Partial Anytime Focal search）路由算法，用于快速求解满足可通行性约束的有界次优路径，最小化各智能体最大路径长度；局部规划考虑机器人异质性以避免重复探索。

Result: 实验结果表明，HEHA相比基线方法最多可减少30%的探索时间。

Conclusion: HEHA通过结合高效的全局路由算法和考虑异质性的局部规划，在多异构机器人协同探索中实现了更高效的路径规划和时间性能提升。

Abstract: This paper considers the path planning problem for autonomous exploration of
an unknown environment using multiple heterogeneous robots such as drones,
wheeled, and legged robots, which have different capabilities to traverse
complex terrains. A key challenge there is to intelligently allocate the robots
to the unknown areas to be explored and determine the visiting order of those
spaces subject to traversablity constraints, which leads to a large scale
constrained optimization problem that needs to be quickly and iteratively
solved every time when new space are explored. To address the challenge, we
propose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging
a recent hierarchical method that decompose the exploration into global
planning and local planning. The major contribution in HEHA is its global
planning, where we propose a new routing algorithm PEAF (Partial Anytime Focal
search) that can quickly find bounded sub-optimal solutions to minimize the
maximum path length among the agents subject to traversability constraints.
Additionally, the local planner in HEHA also considers heterogeneity to avoid
repeated and duplicated exploration among the robots. The experimental results
show that, our HEHA can reduce up to 30% of the exploration time than the
baselines.

</details>


### [313] [Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation](https://arxiv.org/abs/2510.04168)
*Amirmasoud Molaei,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: 本文提出了一种完全数据驱动的强化学习控制框架，用于标准挖掘机在非结构化环境中抓取不规则岩石，无需显式建模土壤或岩石特性，在仿真中训练并展现出良好的泛化性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统挖掘机抓取岩石依赖熟练操作员，且现有自主挖掘方法多针对连续介质或需专用夹具，难以适用于真实施工场景，因此需要一种无需精确建模、适用于标准设备的自主抓取方法。

Method: 采用无模型强化学习（PPO算法）在AGX Dynamics仿真器中训练智能体，通过引导性奖励函数设计，直接输出CAT365挖掘机臂、斗杆和铲斗的关节速度指令，并利用广泛的域随机化提升鲁棒性。

Result: 该策略在未见过的岩石形状和不同土壤条件下均表现出高成功率，接近人类操作水平，同时保持机械稳定。

Conclusion: 本研究首次实现了基于强化学习的标准挖掘机岩石抓取控制，验证了数据驱动方法在离散物体挖掘任务中的可行性，无需专用硬件或材料模型。

Abstract: Rock capturing with standard excavator buckets is a challenging task
typically requiring the expertise of skilled operators. Unlike soil digging, it
involves manipulating large, irregular rocks in unstructured environments where
complex contact interactions with granular material make model-based control
impractical. Existing autonomous excavation methods focus mainly on continuous
media or rely on specialized grippers, limiting their applicability to
real-world construction sites. This paper introduces a fully data-driven
control framework for rock capturing that eliminates the need for explicit
modeling of rock or soil properties. A model-free reinforcement learning agent
is trained in the AGX Dynamics simulator using the Proximal Policy Optimization
(PPO) algorithm and a guiding reward formulation. The learned policy outputs
joint velocity commands directly to the boom, arm, and bucket of a CAT365
excavator model. Robustness is enhanced through extensive domain randomization
of rock geometry, density, and mass, as well as the initial configurations of
the bucket, rock, and goal position. To the best of our knowledge, this is the
first study to develop and evaluate an RL-based controller for the rock
capturing task. Experimental results show that the policy generalizes well to
unseen rocks and varying soil conditions, achieving high success rates
comparable to those of human participants while maintaining machine stability.
These findings demonstrate the feasibility of learning-based excavation
strategies for discrete object manipulation without requiring specialized
hardware or detailed material models.

</details>


### [314] [VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs](https://arxiv.org/abs/2510.04171)
*Lakshadeep Naik,Adam Fischer,Daniel Duberg,Danica Kragic*

Main category: cs.RO

TL;DR: 提出VBM-NET，一种基于学习的从场景正交投影中选择移动操作基座姿态的方法，结合等变TransporterNet和图神经网络，通过强化学习实现高效、可迁移的基座姿态规划。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖精确的状态信息（如物体位姿和环境模型），难以在真实场景中稳定获取；因此，希望直接从提供全局视野的正交投影图像中进行基座姿态规划，降低对精确状态输入的依赖。

Method: 提出VBM-NET，利用等变TransporterNet从正交投影中提取空间对称性并生成候选基座姿态，使用图神经网络建模可变数量的候选姿态，并通过强化学习选择最优姿态。

Result: VBM-NET在显著减少计算时间的同时，性能与传统方法相当，并成功实现了从仿真到真实环境的迁移，在真实移动操作任务中验证了有效性。

Conclusion: 直接从正交投影学习基座姿态是可行且高效的，VBM-NET在保持高成功率的同时提升了计算效率，并具备良好的sim-to-real迁移能力。

Abstract: In Mobile Manipulation, selecting an optimal mobile base pose is essential
for successful object grasping. Previous works have addressed this problem
either through classical planning methods or by learning state-based policies.
They assume access to reliable state information, such as the precise object
poses and environment models. In this work, we study base pose planning
directly from top-down orthographic projections of the scene, which provide a
global overview of the scene while preserving spatial structure. We propose
VBM-NET, a learning-based method for base pose selection using such top-down
orthographic projections. We use equivariant TransporterNet to exploit spatial
symmetries and efficiently learn candidate base poses for grasping. Further, we
use graph neural networks to represent a varying number of candidate base poses
and use Reinforcement Learning to determine the optimal base pose among them.
We show that VBM-NET can produce comparable solutions to the classical methods
in significantly less computation time. Furthermore, we validate sim-to-real
transfer by successfully deploying a policy trained in simulation to real-world
mobile manipulation.

</details>


### [315] [Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve](https://arxiv.org/abs/2510.04178)
*Léa Pistorius,Namrata U. Nayar,Phillip Tran,Sammy Elmariah,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 该论文研究了机器人技术在经导管二尖瓣缘对缘修复中的应用，通过游戏控制器实现直观的关节控制，相较于手动操作，机器人系统可缩短手术时间、减少运动误差并提高夹子放置精度。


<details>
  <summary>Details</summary>
Motivation: 手动导管系统存在机械限制和陡峭的学习曲线，导致经导管瓣膜修复操作困难。

Method: 用基于游戏控制器的机器人关节控制替代临床修复设备的手动手柄控制，在心脏和血管的模型中，将整体输送任务分解为特定步骤，逐项比较手动与机器人操作的表现。

Result: 机器人系统显著缩短了手术时间，减少了运动误差，并提高了夹子放置的准确性。

Conclusion: 机器人辅助可有效克服手动系统的局限性，为复杂的经导管手术提供更可靠且用户友好的平台。

Abstract: Transcatheter valve repair presents significant challenges due to the
mechanical limitations and steep learning curve associated with manual catheter
systems. This paper investigates the use of robotics to facilitate
transcatheter procedures in the context of mitral valve edge-to-edge repair.
The complex handle-based control of a clinical repair device is replaced by
intuitive robotic joint-based control via a game controller. Manual versus
robotic performance is analyzed by decomposing the overall device delivery task
into motion-specific steps and comparing capabilities on a step-by-step basis
in a phantom model of the heart and vasculature. Metrics include procedure
duration and clip placement accuracy. Results demonstrate that the robotic
system can reduce procedural time and motion errors while also improving
accuracy of clip placement. These findings suggest that robotic assistance can
address key limitations of manual systems, offering a more reliable and
user-friendly platform for complex transcatheter procedures.

</details>


### [316] [Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification](https://arxiv.org/abs/2510.04190)
*Jian-jie Zheng,Chih-kai Yang,Po-han Chen,Lyn Chao-ling Chen*

Main category: cs.RO

TL;DR: 该研究提出了一种基于GPT-4o多模态模型的社交机器人系统，用于实时识别和通知违规停车，无需图像预处理，具有高准确率，并可在室内停车场实际应用。


<details>
  <summary>Details</summary>
Motivation: 为解决现实场景中违规停车难以及时发现的问题，探索社交机器人结合先进多模态模型在智能巡逻中的应用潜力。

Method: 采用双模型管道方法与大型多模态模型（GPT-4o）进行对比，最终选用GPT-4o直接识别机器人摄像头捕获的车牌图像，机器人通过自动调整视角在模拟停车场中导航并采集图像，识别结果用于判断停车合法性。

Result: GPT-4o在无预处理条件下实现了高精度的车牌识别，机器人能实时检测违规停车并通过Line消息通知管理员，系统在模拟环境中运行良好。

Conclusion: 该研究验证了基于GPT-4o的多模态方法在车牌识别中的有效性，并展示了社交辅助机器人在真实场景中的应用可行性，尤其适用于室内停车场环境。

Abstract: In the study, the social robot act as a patrol to recognize and notify
illegal parking in real-time. Dual-model pipeline method and large multimodal
model were compared, and the GPT-4o multimodal model was adopted in license
plate recognition without preprocessing. For moving smoothly on a flat ground,
the robot navigated in a simulated parking lot in the experiments. The robot
changes angle view of the camera automatically to capture the images around
with the format of license plate number. From the captured images of the robot,
the numbers on the plate are recognized through the GPT-4o model, and
identifies legality of the numbers. When an illegal parking is detected, the
robot sends Line messages to the system manager immediately. The contribution
of the work is that a novel multimodal deep learning method has validated with
high accuracy in license plate recognition, and a social assistive robot is
also provided for solving problems in a real scenario, and can be applied in an
indoor parking lot.

</details>


### [317] [Flexible Locomotion Learning with Diffusion Model Predictive Control](https://arxiv.org/abs/2510.04234)
*Runhan Huang,Haldun Balim,Heng Yang,Yilun Du*

Main category: cs.RO

TL;DR: 本文提出Diffusion-MPC，一种结合生成扩散模型与模型预测控制的框架，用于实现具身智能体在复杂环境中的鲁棒且灵活的运动规划，支持测试时根据奖励和约束动态调整行为。


<details>
  <summary>Details</summary>
Motivation: 传统模型无关强化学习方法缺乏测试时适应性，而经典MPC依赖精确动力学模型，难以应对复杂环境。因此需要一种兼具灵活性与鲁棒性的规划方法。

Method: 利用预训练的生成扩散模型作为动力学先验，通过反向去噪过程联合预测状态与动作序列；在每一步反向过程中引入奖励优化与约束投影，实现基于目标与约束的轨迹生成，并采用交互式训练策略持续优化扩散模型中的去噪器。

Result: Diffusion-MPC在真实世界机器人任务中展现出优异的运动能力与灵活的测试时适应性，能够在不重新训练的情况下响应新的奖励函数和物理约束。

Conclusion: Diffusion-MPC通过融合扩散模型与MPC的优势，实现了无需精确建模即可进行灵活、安全且任务自适应的运动规划，为复杂环境下的具身智能提供了新思路。

Abstract: Legged locomotion demands controllers that are both robust and adaptable,
while remaining compatible with task and safety considerations. However,
model-free reinforcement learning (RL) methods often yield a fixed policy that
can be difficult to adapt to new behaviors at test time. In contrast, Model
Predictive Control (MPC) provides a natural approach to flexible behavior
synthesis by incorporating different objectives and constraints directly into
its optimization process. However, classical MPC relies on accurate dynamics
models, which are often difficult to obtain in complex environments and
typically require simplifying assumptions. We present Diffusion-MPC, which
leverages a learned generative diffusion model as an approximate dynamics prior
for planning, enabling flexible test-time adaptation through reward and
constraint based optimization. Diffusion-MPC jointly predicts future states and
actions; at each reverse step, we incorporate reward planning and impose
constraint projection, yielding trajectories that satisfy task objectives while
remaining within physical limits. To obtain a planning model that adapts beyond
imitation pretraining, we introduce an interactive training algorithm for
diffusion based planner: we execute our reward-and-constraint planner in
environment, then filter and reweight the collected trajectories by their
realized returns before updating the denoiser. Our design enables strong
test-time adaptability, allowing the planner to adjust to new reward
specifications without retraining. We validate Diffusion-MPC on real world,
demonstrating strong locomotion and flexible adaptation.

</details>


### [318] [ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context](https://arxiv.org/abs/2510.04246)
*Huiwon Jang,Sihyun Yu,Heeseung Kwon,Hojin Jeon,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: 本文提出了ContextVLA，一种通过压缩多帧观测为单个上下文令牌来高效利用时序信息的视觉-语言-动作策略模型，显著提升了部分可观测机器人任务的性能并降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有行为克隆方法在使用多帧观测时性能提升不稳定，且直接处理视频输入导致计算开销大。作者观察到视觉-语言模型（VLM）具备一定的时序理解能力，因此希望构建一个能高效利用多帧信息但计算成本更低的策略模型。

Method: 提出ContextVLA模型，将过去的多帧观测压缩成一个上下文令牌，作为额外输入注入到视觉-语言-动作（VLA）模型中，从而在保持低计算成本的同时有效利用历史上下文进行动作生成。

Result: 实验表明，ContextVLA在机器人任务中持续优于单帧VLA，并能达到全多帧训练的性能水平，同时显著减少训练和推理时间。

Conclusion: 通过引入轻量化的上下文令牌机制，ContextVLA实现了对时序信息的有效建模，在不增加计算负担的前提下提升了策略性能，为实际部署提供了更高效的解决方案。

Abstract: Leveraging temporal context is crucial for success in partially observable
robotic tasks. However, prior work in behavior cloning has demonstrated
inconsistent performance gains when using multi-frame observations. In this
paper, we introduce ContextVLA, a policy model that robustly improves robotic
task performance by effectively leveraging multi-frame observations. Our
approach is motivated by the key observation that Vision-Language-Action models
(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more
effectively utilize multi-frame observations for action generation. This
suggests that VLMs' inherent temporal understanding capability enables them to
extract more meaningful context from multi-frame observations. However, the
high dimensionality of video inputs introduces significant computational
overhead, making VLA training and inference inefficient. To address this,
ContextVLA compresses past observations into a single context token, allowing
the policy to efficiently leverage temporal context for action generation. Our
experiments show that ContextVLA consistently improves over single-frame VLAs
and achieves the benefits of full multi-frame training but with reduced
training and inference times.

</details>


### [319] [Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit](https://arxiv.org/abs/2510.04278)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yuanyuan Zhang,Jiahao Hu,Yingming Chen,Naigui Xiao,Jiaqi Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种基于因子图的模型预测控制工具包FactorMPC，用于解决非线性流形上系统控制中的奇异性、参数冗余和收敛性差等问题，支持流形值状态与高斯不确定性建模，并实现了高效实时优化与安全关键型避障，具备开源模块化设计。


<details>
  <summary>Details</summary>
Motivation: 传统MPC在处理定义于非线性流形上的系统（如机器人姿态控制和约束运动规划）时存在奇异性、过度参数化和收敛性能差的问题，难以有效建模和优化。

Method: 提出FactorMPC，采用因子图统一建模系统动力学、约束和目标函数，原生支持在切空间中建模流形值状态及其高斯不确定性，并设计了基于速度扩展的流形上控制屏障函数（CBF）因子以实现避障，利用因子图的稀疏性和概率结构提升计算效率。

Result: 在四旋翼无人机上的仿真和实验结果表明，FactorMPC在轨迹跟踪和障碍物避让方面优于基线方法，能够实现实时高性能控制。

Conclusion: FactorMPC提供了一个可扩展、几何一致的集成规划与控制框架，通过结合图模型与安全关键MPC，有效解决了非线性流形上的控制挑战，并通过开源实现促进研究复现。

Abstract: Model predictive control (MPC) faces significant limitations when applied to
systems evolving on nonlinear manifolds, such as robotic attitude dynamics and
constrained motion planning, where traditional Euclidean formulations struggle
with singularities, over-parameterization, and poor convergence. To overcome
these challenges, this paper introduces FactorMPC, a factor-graph based MPC
toolkit that unifies system dynamics, constraints, and objectives into a
modular, user-friendly, and efficient optimization structure. Our approach
natively supports manifold-valued states with Gaussian uncertainties modeled in
tangent spaces. By exploiting the sparsity and probabilistic structure of
factor graphs, the toolkit achieves real-time performance even for
high-dimensional systems with complex constraints. The velocity-extended
on-manifold control barrier function (CBF)-based obstacle avoidance factors are
designed for safety-critical applications. By bridging graphical models with
safety-critical MPC, our work offers a scalable and geometrically consistent
framework for integrated planning and control. The simulations and experimental
results on the quadrotor demonstrate superior trajectory tracking and obstacle
avoidance performance compared to baseline methods. To foster research
reproducibility, we have provided open-source implementation offering
plug-and-play factors.

</details>


### [320] [Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation](https://arxiv.org/abs/2510.04353)
*Stephen McCrory,Romeo Orsolino,Dhruv Thanki,Luigi Penco,Robert Griffin*

Main category: cs.RO

TL;DR: 提出了一种基于质心稳定性的重定向方法，通过动态调整接触点和姿态来增强人形机器人在使用手部接触和非共面表面时的遥操作稳定性。


<details>
  <summary>Details</summary>
Motivation: 遥操作在使用手接触和非共面表面时容易导致电机扭矩饱和或因打滑而失去稳定性，因此需要提高稳定性的方法。

Method: 提出一种基于质心稳定性的重定向方法，利用稳定性裕度梯度的高效解析计算，识别对遥操作设定点敏感的不稳定场景，并进行局部调整。

Result: 在仿真和硬件实验中验证了该框架，展示了操纵任务中稳定裕度的提升，并证明更高的稳定裕度与更好的抗冲击能力和关节扭矩裕度相关。

Conclusion: 所提出的方法能有效提升复杂遥操作场景下人形机器人的稳定性。

Abstract: Teleoperation is a powerful method to generate reference motions and enable
humanoid robots to perform a broad range of tasks. However, teleoperation
becomes challenging when using hand contacts and non-coplanar surfaces, often
leading to motor torque saturation or loss of stability through slipping. We
propose a centroidal stability-based retargeting method that dynamically
adjusts contact points and posture during teleoperation to enhance stability in
these difficult scenarios. Central to our approach is an efficient analytical
calculation of the stability margin gradient. This gradient is used to identify
scenarios for which stability is highly sensitive to teleoperation setpoints
and inform the local adjustment of these setpoints. We validate the framework
in simulation and hardware by teleoperating manipulation tasks on a humanoid,
demonstrating increased stability margins. We also demonstrate empirically that
higher stability margins correlate with improved impulse resilience and joint
torque margin.

</details>


### [321] [Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators](https://arxiv.org/abs/2510.04354)
*Apurva Badithela,David Snyder,Lihan Zha,Joseph Mikhail,Matthew O'Kelly,Anushri Dixit,Anirudha Majumdar*

Main category: cs.RO

TL;DR: 本文提出了SureSim框架，通过结合大规模仿真和小规模真实世界测试，利用预测驱动推断方法校正仿真偏差，从而更可靠地评估机器人操作策略的真实性能，并减少20-25%的硬件评估工作量。


<details>
  <summary>Details</summary>
Motivation: 现有机器人策略评估通常依赖少量硬件试验，缺乏统计保证，难以可靠评估策略在真实世界中的泛化性能。

Method: 将仿真与真实评估结合的问题形式化为预测驱动推断问题，使用少量配对的真实-仿真数据校正仿真偏差，并采用非渐近均值估计算法为策略性能提供置信区间。

Result: 在基于物理的仿真中评估了扩散策略和多任务微调的π₀策略，结果表明该方法在达到相似性能界限时，可节省20-25%的硬件评估成本。

Conclusion: SureSim框架能有效融合仿真与真实数据，提供具有统计保障的策略性能评估，显著降低真实世界测试开销，推动机器人策略的可靠部署。

Abstract: Rapid progress in imitation learning, foundation models, and large-scale
datasets has led to robot manipulation policies that generalize to a wide-range
of tasks and environments. However, rigorous evaluation of these policies
remains a challenge. Typically in practice, robot policies are often evaluated
on a small number of hardware trials without any statistical assurances. We
present SureSim, a framework to augment large-scale simulation with relatively
small-scale real-world testing to provide reliable inferences on the real-world
performance of a policy. Our key idea is to formalize the problem of combining
real and simulation evaluations as a prediction-powered inference problem, in
which a small number of paired real and simulation evaluations are used to
rectify bias in large-scale simulation. We then leverage non-asymptotic mean
estimation algorithms to provide confidence intervals on mean policy
performance. Using physics-based simulation, we evaluate both diffusion policy
and multi-task fine-tuned \(\pi_0\) on a joint distribution of objects and
initial conditions, and find that our approach saves over \(20-25\%\) of
hardware evaluation effort to achieve similar bounds on policy performance.

</details>


### [322] [PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization](https://arxiv.org/abs/2510.04436)
*Jushan Chen,Santiago Paternain*

Main category: cs.RO

TL;DR: 提出一种基于模型扩散的直接轨迹优化方法，通过梯度无关的投影机制确保动力学可行性，在四旋翼路径规划中实现零动力学误差和约4倍成功率提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的轨迹优化方法难以显式处理非线性等式约束（如动力学可行性），常导致次优解。

Method: 提出直接生成状态序列的模型扩散方法，并在反向扩散过程中引入梯度无关的投影机制以保证动态可行性。

Result: 在四旋翼密集障碍物航路点导航任务中，相比当前先进基线方法，实现了零动力学可行性误差和约4倍的成功率提升。

Conclusion: 所提方法能有效处理动态约束，显著提升轨迹优化的可行性和成功率。

Abstract: Recently, diffusion models have gained popularity and attention in trajectory
optimization due to their capability of modeling multi-modal probability
distributions. However, addressing nonlinear equality constraints, i.e, dynamic
feasi- bility, remains a great challenge in diffusion-based trajectory
optimization. Recent diffusion-based trajectory optimization frameworks rely on
a single-shooting style approach where the denoised control sequence is applied
to forward propagate the dynamical system, which cannot explicitly enforce
constraints on the states and frequently leads to sub-optimal solutions. In
this work, we propose a novel direct trajectory optimization approach via
model-based diffusion, which directly generates a sequence of states. To ensure
dynamic feasibility, we propose a gradient-free projection mechanism that is
incorporated into the reverse diffusion process. Our results show that,
compared to a recent state-of-the-art baseline, our approach leads to zero
dynamic feasibility error and approximately 4x higher success rate in a
quadrotor waypoint navigation scenario involving dense static obstacles.

</details>


### [323] [Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads](https://arxiv.org/abs/2510.04509)
*Huanqing Wang,Kaixiang Zhang,Kyungjoon Lee,Yu Mei,Vaibhav Srivastava,Jun Sheng,Ziyou Song,Zhaojian Li*

Main category: cs.RO

TL;DR: 提出了一种基于增量输入-输出数据表示的新型速度形式DeePC框架，用于在未知负载下实现软机器人的鲁棒且最优控制，实验验证了其优于标准DeePC的性能。


<details>
  <summary>Details</summary>
Motivation: 未知外部负载和干扰会显著改变软机器人系统动态，导致控制性能下降，现有数据驱动方法难以应对此类问题。

Method: 采用增量形式的输入-输出数据表示，构建速度形式的DeePC框架，无需加权数据集或扰动估计器来缓解未知负载带来的性能退化。

Result: 在平面软机器人上的实验表明，该方法在存在未知负载的情况下显著优于标准DeePC，具有更强的鲁棒性和控制精度。

Conclusion: 所提出的速度形式DeePC框架能有效提升软机器人在未知负载下的控制鲁棒性与性能，无需额外建模或扰动补偿机制。

Abstract: Data-driven control methods such as data-enabled predictive control (DeePC)
have shown strong potential in efficient control of soft robots without
explicit parametric models. However, in object manipulation tasks, unknown
external payloads and disturbances can significantly alter the system dynamics
and behavior, leading to offset error and degraded control performance. In this
paper, we present a novel velocity-form DeePC framework that achieves robust
and optimal control of soft robots under unknown payloads. The proposed
framework leverages input-output data in an incremental representation to
mitigate performance degradation induced by unknown payloads, eliminating the
need for weighted datasets or disturbance estimators. We validate the method
experimentally on a planar soft robot and demonstrate its superior performance
compared to standard DeePC in scenarios involving unknown payloads.

</details>


### [324] [Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation](https://arxiv.org/abs/2510.04585)
*Jianshu Zhou,Jing Shu,Tianle Pan,Puchen Zhu,Jiajun An,Huayu Zhang,Junda Huang,Upinder Kaur,Xin Ma,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: 提出了一种名为Everything-Grasping (EG) Gripper的软体夹爪，能够跨尺度、跨物态（固体和液体）抓取物体，无需气密密封，并结合触觉感知实现自主抓取模式选择。


<details>
  <summary>Details</summary>
Motivation: 传统软体夹爪难以在不同尺寸和物理状态（如固体与液体）的物体间实现通用抓取，亟需一种无需气密密封且适应性强的统一设计方案。

Method: EG Gripper结合分布式表面吸力与内部颗粒阻塞效应，并引入融合液体检测与压力反馈的触觉感知框架，通过TIGMS算法根据压力和电压信号自主选择抓取模式。

Result: 可抓取表面积从0.2 mm²到超过62,000 mm²的物体，覆盖自身接触面积3,500倍小和88倍大的目标；实验证明其在水下抓取、易碎物操作和液体捕获等任务中具有鲁棒性和重复性。

Conclusion: 这是首个基于统一柔性结构、可靠实现跨尺度固液物体抓取的软体夹爪，拓展了软体机器人在复杂环境中的操作能力。

Abstract: Grasping objects across vastly different sizes and physical states-including
both solids and liquids-with a single robotic gripper remains a fundamental
challenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a
soft end-effector that synergistically integrates distributed surface suction
with internal granular jamming, enabling cross-scale and cross-state
manipulation without requiring airtight sealing at the contact interface with
target objects. The EG Gripper can handle objects with surface areas ranging
from sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized
paper and woven bag), enabling manipulation of objects nearly 3,500X smaller
and 88X larger than its own contact area (approximated at 707 mm2 for a 30
mm-diameter base). We further introduce a tactile sensing framework that
combines liquid detection and pressure-based suction feedback, enabling
real-time differentiation between solid and liquid targets. Guided by the
actile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper
autonomously selects grasping modes based on distributed pressure and voltage
signals. Experiments across diverse tasks-including underwater grasping,
fragile object handling, and liquid capture-demonstrate robust and repeatable
performance. To our knowledge, this is the first soft gripper to reliably grasp
both solid and liquid objects across scales using a unified compliant
architecture.

</details>


### [325] [MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation](https://arxiv.org/abs/2510.04592)
*Yilin Mei,Peng Qiu,Wei Zhang,WenChao Zhang,Wenjie Song*

Main category: cs.RO

TL;DR: 本文提出了MobRT，一个基于数字孪生的框架，用于自动生成移动操作机器人的高质量、多样化演示数据，以解决现实世界中数据收集困难的问题。


<details>
  <summary>Details</summary>
Motivation: 由于移动操作机器人需要在高维、动态和部分可观测环境中协调底盘移动和机械臂操作，难以获取大规模高质量演示数据，导致该领域研究受限。

Method: 提出MobRT框架，结合虚拟运动学控制与全身运动规划，在仿真中自主生成复杂全身任务（如操作铰接物体和移动抓取）的逼真演示数据。

Result: 在多个基线算法上验证了生成数据的质量，建立了综合基准，并证明任务成功率与生成轨迹数量呈强相关；结合仿真与真实数据可显著提升策略泛化能力和性能。

Conclusion: MobRT能有效生成高质量训练数据，显著提升移动操作任务中的策略训练效果，并在仿真和真实环境中均实现鲁棒表现。

Abstract: Recent advances in robotics have been largely driven by imitation learning,
which depends critically on large-scale, high-quality demonstration data.
However, collecting such data remains a significant challenge-particularly for
mobile manipulators, which must coordinate base locomotion and arm manipulation
in high-dimensional, dynamic, and partially observable environments.
Consequently, most existing research remains focused on simpler tabletop
scenarios, leaving mobile manipulation relatively underexplored. To bridge this
gap, we present \textit{MobRT}, a digital twin-based framework designed to
simulate two primary categories of complex, whole-body tasks: interaction with
articulated objects (e.g., opening doors and drawers) and mobile-base
pick-and-place operations. \textit{MobRT} autonomously generates diverse and
realistic demonstrations through the integration of virtual kinematic control
and whole-body motion planning, enabling coherent and physically consistent
execution. We evaluate the quality of \textit{MobRT}-generated data across
multiple baseline algorithms, establishing a comprehensive benchmark and
demonstrating a strong correlation between task success and the number of
generated trajectories. Experiments integrating both simulated and real-world
demonstrations confirm that our approach markedly improves policy
generalization and performance, achieving robust results in both simulated and
real-world environments.

</details>


### [326] [OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS](https://arxiv.org/abs/2510.04612)
*Simon Boche,Jaehyung Jung,Sebastián Barbas Laina,Stefan Leutenegger*

Main category: cs.RO

TL;DR: OKVIS2-X是一个先进的多传感器SLAM系统，能够实时构建密集的体素占据地图，支持大规模环境，并融合视觉、惯性、深度、LiDAR和GNSS等多种传感器数据，具有高精度、高鲁棒性，并支持在线相机外参标定。


<details>
  <summary>Details</summary>
Motivation: 为了提升移动机器人在复杂环境中建图的可用性、状态估计的精度与鲁棒性，现有SLAM系统在大规模场景中往往难以兼顾实时性、全局一致性与地图实用性，因此需要一种能融合多传感器并生成可直接用于导航的稠密地图的解决方案。

Method: 提出OKVIS2-X，采用统一的SLAM框架，紧耦合多种传感器（视觉、IMU、深度、LiDAR、GNSS），使用高效的子地图策略实现对大规模环境的扩展，并通过地图对齐因子将估计器与子地图紧密集成；同时支持在线相机外参标定以进一步提升精度。

Result: 在EuRoC数据集上达到最高轨迹精度，在Hilti22的VI-only模式下超越所有竞品，在LiDAR版本中表现具竞争力，并在VBR大规模多样数据集中展示出领先性能；系统可生成全局一致、可直接用于自主导航的稠密占据地图。

Conclusion: OKVIS2-X是一种高度集成、可扩展且实时的多传感器SLAM系统，通过紧耦合感知与建图，在精度、鲁棒性和实用性方面均达到先进水平，适用于复杂真实场景下的自主机器人导航。

Abstract: To empower mobile robots with usable maps as well as highest state estimation
accuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor
Simultaneous Localization and Mapping (SLAM) system building dense volumetric
occupancy maps, while scalable to large environments and operating in realtime.
Our unified SLAM framework seamlessly integrates different sensor modalities:
visual, inertial, measured or learned depth, LiDAR and Global Navigation
Satellite System (GNSS) measurements. Unlike most state-of-the-art SLAM
systems, we advocate using dense volumetric map representations when leveraging
depth or range-sensing capabilities. We employ an efficient submapping strategy
that allows our system to scale to large environments, showcased in sequences
of up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by
tightly-coupling the estimator and submaps through map alignment factors. Our
system provides globally consistent maps, directly usable for autonomous
navigation. To further improve the accuracy of OKVIS2-X, we also incorporate
the option of performing online calibration of camera extrinsics. Our system
achieves the highest trajectory accuracy in EuRoC against state-of-the-art
alternatives, outperforms all competitors in the Hilti22 VI-only benchmark,
while also proving competitive in the LiDAR version, and showcases state of the
art accuracy in the diverse and large-scale sequences from the VBR dataset.

</details>


### [327] [Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies](https://arxiv.org/abs/2510.04692)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.RO

TL;DR: 提出了一种仿生机器人平台，用于在野外研究大鸨的生态行为，具备高真实感形态、自主视觉感知和复杂地形移动能力。


<details>
  <summary>Details</summary>
Motivation: 在自然条件下研究鸟类行为具有挑战性，需要高度逼真的形态、户外耐用性和适应非受控环境的智能感知能力。

Method: 结合高分辨率3D扫描、参数化CAD建模、可动3D打印和紫外纹理贴面技术，构建形态精确的仿生机器人；采用六轮摇臂-支承结构底盘和NVIDIA Jetson模块实现沙地移动与实时RGB及热成像感知，并通过轻量级YOLO检测和视觉伺服系统实现自主目标对准。

Result: 沙漠围栏试验中实现了15-22 FPS的实时运行（延迟<100ms），并成功引发活体大鸨的自然识别与互动反应。

Conclusion: 该平台整合了可复制的数字制造、具身视觉智能和生态验证，为动物-机器人交互研究、保护机器人学和公众参与提供了可迁移的技术框架。

Abstract: Biomimetic intelligence and robotics are transforming field ecology by
enabling lifelike robotic surrogates that interact naturally with animals under
real world conditions. Studying avian behavior in the wild remains challenging
due to the need for highly realistic morphology, durable outdoor operation, and
intelligent perception that can adapt to uncontrolled environments. We present
a next generation bio inspired robotic platform that replicates the morphology
and visual appearance of the female Houbara bustard to support controlled
ethological studies and conservation oriented field research. The system
introduces a fully digitally replicable fabrication workflow that combines high
resolution structured light 3D scanning, parametric CAD modelling, articulated
3D printing, and photorealistic UV textured vinyl finishing to achieve
anatomically accurate and durable robotic surrogates. A six wheeled rocker
bogie chassis ensures stable mobility on sand and irregular terrain, while an
embedded NVIDIA Jetson module enables real time RGB and thermal perception,
lightweight YOLO based detection, and an autonomous visual servoing loop that
aligns the robot's head toward detected targets without human intervention. A
lightweight thermal visible fusion module enhances perception in low light
conditions. Field trials in desert aviaries demonstrated reliable real time
operation at 15 to 22 FPS with latency under 100 ms and confirmed that the
platform elicits natural recognition and interactive responses from live
Houbara bustards under harsh outdoor conditions. This integrated framework
advances biomimetic field robotics by uniting reproducible digital fabrication,
embodied visual intelligence, and ecological validation, providing a
transferable blueprint for animal robot interaction research, conservation
robotics, and public engagement.

</details>


### [328] [Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly](https://arxiv.org/abs/2510.04696)
*Alexander L. Mitchell,Joe Watson,Ingmar Posner*

Main category: cs.RO

TL;DR: 提出了一种去中心化的基于梯度的框架，利用分段连续能量函数实现双臂装配中的快速重规划，能够自适应生成子目标、协调运动和自主交接。


<details>
  <summary>Details</summary>
Motivation: 传统任务与运动规划（TAMP）方法在应对高精度装配中因摩擦或形变等难以建模的动力学导致的扰动时，重新规划速度慢且定义显式任务序列繁琐，缺乏灵活性。

Method: 采用去中心化梯度优化框架，通过自动组合自适应势函数构建分段连续能量函数，仅需短视优化即可生成子目标，避免长视野规划。

Result: 该方法在物理双臂高精度装配任务中实现了可扩展性，实验中自发产生了自动重试、协调运动和自主交接行为，有效解决了长视野任务中的快速重规划问题。

Conclusion: 所提框架简化了双臂装配中的任务规划，具备良好的适应性和实时性，适用于需要频繁重规划的复杂接触操作场景。

Abstract: There are many challenges in bimanual assembly, including high-level
sequencing, multi-robot coordination, and low-level, contact-rich operations
such as component mating. Task and motion planning (TAMP) methods, while
effective in this domain, may be prohibitively slow to converge when adapting
to disturbances that require new task sequencing and optimisation. These events
are common during tight-tolerance assembly, where difficult-to-model dynamics
such as friction or deformation require rapid replanning and reattempts.
Moreover, defining explicit task sequences for assembly can be cumbersome,
limiting flexibility when task replanning is required. To simplify this
planning, we introduce a decentralised gradient-based framework that uses a
piecewise continuous energy function through the automatic composition of
adaptive potential functions. This approach generates sub-goals using only
myopic optimisation, rather than long-horizon planning. It demonstrates
effectiveness at solving long-horizon tasks due to the structure and adaptivity
of the energy function. We show that our approach scales to physical bimanual
assembly tasks for constructing tight-tolerance assemblies. In these
experiments, we discover that our gradient-based rapid replanning framework
generates automatic retries, coordinated motions and autonomous handovers in an
emergent fashion.

</details>


### [329] [Performance-guided Task-specific Optimization for Multirotor Design](https://arxiv.org/abs/2510.04724)
*Etor Arza,Welf Rehberg,Philipp Weiss,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种基于闭环任务性能的多旋翼微型飞行器任务特定设计优化方法，结合强化学习、贝叶斯优化和协方差矩阵自适应进化策略，优化了电机布局并验证了仿真到现实的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 传统多旋翼设计通常采用固定配置，难以在特定任务中实现最优性能，因此需要一种以任务性能为导向的自动化设计优化方法。

Method: 利用强化学习、贝叶斯优化和CMA-ES算法，在保证可制造性和最小气动干扰的前提下，对电机位形的设计空间进行系统搜索，以闭环任务性能为优化目标。

Result: 优化后的设计在敏捷航点导航任务中表现优于传统及文献中的全驱动多旋翼构型，并通过实际建造和测试验证了仿真结果的有效性和sim2real可迁移性。

Conclusion: 任务导向的设计优化能显著提升多旋翼飞行器在特定任务中的性能，所提方法具备实际应用潜力和良好的仿真到现实迁移能力。

Abstract: This paper introduces a methodology for task-specific design optimization of
multirotor Micro Aerial Vehicles. By leveraging reinforcement learning,
Bayesian optimization, and covariance matrix adaptation evolution strategy, we
optimize aerial robot designs guided exclusively by their closed-loop
performance in a considered task. Our approach systematically explores the
design space of motor pose configurations while ensuring manufacturability
constraints and minimal aerodynamic interference. Results demonstrate that
optimized designs achieve superior performance compared to conventional
multirotor configurations in agile waypoint navigation tasks, including against
fully actuated designs from the literature. We build and test one of the
optimized designs in the real world to validate the sim2real transferability of
our approach.

</details>


### [330] [TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation](https://arxiv.org/abs/2510.04839)
*Shuo Sha,Anupam Bhakta,Zhenyuan Jiang,Kevin Qiu,Ishaan Mahajan,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: 本文提出了一种名为TAG-K的轻量级惯性参数在线估计算法，结合贪婪随机行选择和尾部平均，在保持低计算复杂度的同时实现了快速收敛和强鲁棒性，显著优于传统RLS和卡尔曼滤波方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法如递归最小二乘（RLS）和卡尔曼滤波（KF）在应对突变参数或计算资源受限时存在跟踪能力差或计算成本高的问题，难以适应动态环境下的机器人控制需求。

Method: 提出TAG-K算法，基于Kaczmarz方法，采用贪婪随机行选择以加速收敛，并引入尾部平均提升在噪声和不一致数据下的鲁棒性，适用于嵌入式系统等资源受限场景。

Result: 在合成基准和四旋翼追踪任务中，TAG-K在笔记本级CPU上比现有方法快1.5x-1.9x，在嵌入式微控制器上快4.8x-20.7x，同时估计误差降低25%，端到端追踪性能提升近2倍。

Conclusion: TAG-K在保持低计算开销的同时，显著提升了参数估计的速度与精度，适用于需要实时自适应控制的机器人系统。

Abstract: Accurate online inertial parameter estimation is essential for adaptive
robotic control, enabling real-time adjustment to payload changes,
environmental interactions, and system wear. Traditional methods such as
Recursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to
track abrupt parameter shifts or incur high computational costs, limiting their
effectiveness in dynamic environments and for computationally constrained
robotic systems. As such, we introduce TAG-K, a lightweight extension of the
Kaczmarz method that combines greedy randomized row selection for rapid
convergence with tail averaging for robustness under noise and inconsistency.
This design enables fast, stable parameter adaptation while retaining the low
per-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K
in synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other
Kaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class
CPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More
importantly, these speedups are paired with improved resilience to measurement
noise and a 25% reduction in estimation error, leading to nearly 2x better
end-to-end tracking performance.

</details>


### [331] [CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery](https://arxiv.org/abs/2510.04883)
*Nathan Shankar,Pawel Ladosz,Hujun Yin*

Main category: cs.RO

TL;DR: 提出一种基于U-Net的架构，利用红外流在黑暗环境中实现鲁棒的机器人视觉感知，有效去除主动发射器图案干扰，提升图像质量和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 红外图像在低光条件下虽优于RGB，但受主动发射器图案干扰，影响高层视觉任务，需有效去噪以提升感知能力。

Method: 采用基于U-Net的网络架构，从含发射器图案的红外输入中重建干净的红外图像。

Result: 该方法在图像增强和下游任务（如检测、跟踪、定位）性能上优于现有技术，支持从明亮到极低光照下的稳定机器人视觉运行。

Conclusion: 所提方法显著提升了低光环境下基于红外的机器人感知鲁棒性，具备广泛适用性。

Abstract: This paper presents a novel approach for enabling robust robotic perception
in dark environments using infrared (IR) stream. IR stream is less susceptible
to noise than RGB in low-light conditions. However, it is dominated by active
emitter patterns that hinder high-level tasks such as object detection,
tracking and localisation. To address this, a U-Net-based architecture is
proposed that reconstructs clean IR images from emitter-populated input,
improving both image quality and downstream robotic performance. This approach
outperforms existing enhancement techniques and enables reliable operation of
vision-driven robotic systems across illumination conditions from well-lit to
extreme low-light scenes.

</details>


### [332] [HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks](https://arxiv.org/abs/2510.04898)
*Zheng Xiong,Kang Li,Zilin Wang,Matthew Jackson,Jakob Foerster,Shimon Whiteson*

Main category: cs.RO

TL;DR: 本文提出了HyperVLA，一种基于超网络的视觉-语言-动作（VLA）模型，通过仅在推理时激活小型任务特定策略，显著降低了现有VLA模型的推理成本，同时保持了多任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型推理成本极高，限制了其在实际机器人应用中的部署，因此需要一种更高效的架构。

Method: 采用超网络（HN）架构，训练时保留完整模型容量，推理时仅激活小型任务特定子网络，并结合视觉基础模型先验、HN归一化和动作生成策略等关键技术提升性能。

Result: 相比传统单体VLA模型，HyperVLA在零样本泛化和少样本适应中达到相当或更高的成功率，推理参数减少90倍，速度提升120倍。

Conclusion: HyperVLA在不牺牲性能的前提下大幅降低VLA模型的推理开销，为通用机器人策略的实际部署提供了高效解决方案。

Abstract: Built upon language and vision foundation models with strong generalization
ability and trained on large-scale robotic data, Vision-Language-Action (VLA)
models have recently emerged as a promising approach to learning generalist
robotic policies. However, a key drawback of existing VLAs is their extremely
high inference costs. In this paper, we propose HyperVLA to address this
problem. Unlike existing monolithic VLAs that activate the whole model during
both training and inference, HyperVLA uses a novel hypernetwork (HN)-based
architecture that activates only a small task-specific policy during inference,
while still retaining the high model capacity needed to accommodate diverse
multi-task behaviors during training. Successfully training an HN-based VLA is
nontrivial so HyperVLA contains several key algorithm design features that
improve its performance, including properly utilizing the prior knowledge from
existing vision foundation models, HN normalization, and an action generation
strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even
higher success rate for both zero-shot generalization and few-shot adaptation,
while significantly reducing inference costs. Compared to OpenVLA, a
state-of-the-art VLA model, HyperVLA reduces the number of activated parameters
at test time by $90\times$, and accelerates inference speed by $120\times$.
Code is publicly available at https://github.com/MasterXiong/HyperVLA

</details>


### [333] [Efficient Navigation in Unknown Indoor Environments with Vision-Language Models](https://arxiv.org/abs/2510.04991)
*D. Schwartz,K. Kondo,J. P. How*

Main category: cs.RO

TL;DR: 提出一种利用视觉-语言模型（VLM）进行高层规划的框架，通过在零样本设置下推理占据地图来选择高效子目标，显著提升未知室内环境中的导航效率。


<details>
  <summary>Details</summary>
Motivation: 传统探索方法因依赖局部启发式策略和全局推理能力有限，常导致路径低效，尤其在多死胡同的复杂环境中表现不佳。

Method: 将3D占据栅格转换为部分2D地图，生成候选子目标，并利用VLM对子目标进行评估与排序，结合DYNUS轨迹规划器实现导航决策。

Result: VLM能从不完整地图中推断出房间、走廊等结构模式，减少陷入死胡同等贪婪策略失败情况，在仿真中平均实现约10%的路径缩短。

Conclusion: 该方法通过引入VLM的高层语义推理能力，有效增强了自主导航系统的全局规划性能，验证了其在复杂未知环境中应用的潜力。

Abstract: We present a novel high-level planning framework that leverages
vision-language models (VLMs) to improve autonomous navigation in unknown
indoor environments with many dead ends. Traditional exploration methods often
take inefficient routes due to limited global reasoning and reliance on local
heuristics. In contrast, our approach enables a VLM to reason directly about an
occupancy map in a zero-shot manner, selecting subgoals that are likely to lead
to more efficient paths. At each planning step, we convert a 3D occupancy grid
into a partial 2D map of the environment, and generate candidate subgoals. Each
subgoal is then evaluated and ranked against other candidates by the model. We
integrate this planning scheme into DYNUS \cite{kondo2025dynus}, a
state-of-the-art trajectory planner, and demonstrate improved navigation
efficiency in simulation. The VLM infers structural patterns (e.g., rooms,
corridors) from incomplete maps and balances the need to make progress toward a
goal against the risk of entering unknown space. This reduces common greedy
failures (e.g., detouring into small rooms) and achieves about 10\% shorter
paths on average.

</details>


### [334] [Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot](https://arxiv.org/abs/2510.05001)
*Aditya Sripada,Abhishek Warrier*

Main category: cs.RO

TL;DR: TARS3D是一个受电影《星际穿越》中TARS机器人启发的非仿生机器人平台，具有七自由度和可伸缩腿结构，能够实现步行和高速滚动等多种运动模式。通过解析建模与深度强化学习相结合的方法，在硬件上验证了其多模态运动能力。


<details>
  <summary>Details</summary>
Motivation: 探索非仿人形态机器人在复杂环境下的多样化运动能力，突破传统生物启发式设计的限制，开发适用于工程场景的新型机器人构型。

Method: 建立简化动力学模型并推导闭式极限环条件，结合仿真中的深度强化学习搜索未探索的步态空间，并在真实硬件上进行实验验证。

Result: 实现了双足行走和八步混合极限环滚动模式，验证了机器人在关节角度限制下无干扰切换左右支撑、维持稳定滚动的能力；DRL成功复现已知步态并发现新行为。

Conclusion: TARS3D展示了科幻灵感驱动的超越生物形态机器人在多模态运动方面的潜力，结合解析方法与强化学习为未来多模态机器人发展提供了可行路径。

Abstract: Robotic locomotion research typically draws from biologically inspired leg
designs, yet many human-engineered settings can benefit from
non-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from
Interstellar into a 0.25 m, 0.99 kg research platform with seven actuated
degrees of freedom. The film shows two primary gaits: a bipedal-like walk and a
high-speed rolling mode. For TARS3D, we build reduced-order models for each,
derive closed-form limit-cycle conditions, and validate the predictions on
hardware. Experiments confirm that the robot respects its +/-150 degree hip
limits, alternates left-right contacts without interference, and maintains an
eight-step hybrid limit cycle in rolling mode. Because each telescopic leg
provides four contact corners, the rolling gait is modeled as an eight-spoke
double rimless wheel. The robot's telescopic leg redundancy implies a far
richer gait repertoire than the two limit cycles treated analytically. So, we
used deep reinforcement learning (DRL) in simulation to search the unexplored
space. We observed that the learned policy can recover the analytic gaits under
the right priors and discover novel behaviors as well. Our findings show that
TARS3D's fiction-inspired bio-transcending morphology can realize multiple
previously unexplored locomotion modes and that further learning-driven search
is likely to reveal more. This combination of analytic synthesis and
reinforcement learning opens a promising pathway for multimodal robotics.

</details>


### [335] [StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation](https://arxiv.org/abs/2510.05057)
*Mingyu Liu,Jiuhe Shu,Hui Chen,Zeju Li,Canyu Zhao,Jiange Yang,Shenyuan Gao,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: 提出一种名为StaMo的无监督方法，通过轻量级编码器和预训练Diffusion Transformer解码器学习高度压缩的双令牌状态表示，该表示不仅高效、可解释，还能自然导出潜在动作，显著提升机器人任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在状态表示中平衡表达力与紧凑性，往往导致冗余或缺失任务关键信息，阻碍了高效的世界建模与决策。

Method: 利用轻量编码器和预训练Diffusion Transformer（DiT）解码器，从静态图像中学习双令牌的紧凑状态表示，并通过潜在插值得到差值作为潜在动作，用于生成机器人可执行动作。

Result: 在LIBERO上性能提升14.3%，真实世界任务成功率提高30%，潜在动作使策略协同训练提升10.4%，且具有更低推理开销和更好可解释性。

Conclusion: StaMo证明了从静态图像学习紧凑状态表示可有效捕捉结构化动态并生成潜在动作，挑战了依赖复杂架构和视频数据的传统方法，具备跨仿真、真实机器人和人类第一视角视频的良好泛化能力。

Abstract: A fundamental challenge in embodied intelligence is developing expressive and
compact state representations for efficient world modeling and decision making.
However, existing methods often fail to achieve this balance, yielding
representations that are either overly redundant or lacking in task-critical
information. We propose an unsupervised approach that learns a highly
compressed two-token state representation using a lightweight encoder and a
pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong
generative prior. Our representation is efficient, interpretable, and
integrates seamlessly into existing VLA-based models, improving performance by
14.3% on LIBERO and 30% in real-world task success with minimal inference
overhead. More importantly, we find that the difference between these tokens,
obtained via latent interpolation, naturally serves as a highly effective
latent action, which can be further decoded into executable robot actions. This
emergent capability reveals that our representation captures structured
dynamics without explicit supervision. We name our method StaMo for its ability
to learn generalizable robotic Motion from compact State representation, which
is encoded from static images, challenging the prevalent dependence to learning
latent action on complex architectures and video data. The resulting latent
actions also enhance policy co-training, outperforming prior methods by 10.4%
with improved interpretability. Moreover, our approach scales effectively
across diverse data sources, including real-world robot data, simulation, and
human egocentric video.

</details>


### [336] [Automaton Constrained Q-Learning](https://arxiv.org/abs/2510.05061)
*Anastasios Manganaris,Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 本文提出了Automaton Constrained Q-Learning (ACQL) 算法，结合目标条件值学习与自动机引导强化，以解决复杂连续环境中带有时间安全约束的序列目标达成问题。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习在处理具有时变安全约束的序列目标任务时存在局限性，现有基于线性时序逻辑（LTL）的方法在复杂环境中的表现不佳，缺乏同时支持时序目标和安全性的可扩展方法。

Method: 提出ACQL算法，将目标条件Q学习与LTL自动机表示相结合，利用自动机编码阶段性目标进展和静态/非静态安全约束，实现对复杂时序任务的有效建模。

Result: ACQL在多个连续控制任务中优于现有方法，能够在先前方法无法满足目标或安全约束的情况下仍保持良好性能，并成功部署于6自由度机械臂在杂乱空间中执行带安全约束的目标到达任务。

Conclusion: ACQL是一种鲁棒且可扩展的解决方案，能够有效学习符合丰富时序规范的机器人行为，适用于现实世界的机器人任务。

Abstract: Real-world robotic tasks often require agents to achieve sequences of goals
while respecting time-varying safety constraints. However, standard
Reinforcement Learning (RL) paradigms are fundamentally limited in these
settings. A natural approach to these problems is to combine RL with
Linear-time Temporal Logic (LTL), a formal language for specifying complex,
temporally extended tasks and safety constraints. Yet, existing RL methods for
LTL objectives exhibit poor empirical performance in complex and continuous
environments. As a result, no scalable methods support both temporally ordered
goals and safety simultaneously, making them ill-suited for realistic robotics
scenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm
that addresses this gap by combining goal-conditioned value learning with
automaton-guided reinforcement. ACQL supports most LTL task specifications and
leverages their automaton representation to explicitly encode stage-wise goal
progression and both stationary and non-stationary safety constraints. We show
that ACQL outperforms existing methods across a range of continuous control
tasks, including cases where prior methods fail to satisfy either goal-reaching
or safety constraints. We further validate its real-world applicability by
deploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a
cluttered, cabinet-like space with safety constraints. Our results demonstrate
that ACQL is a robust and scalable solution for learning robotic behaviors
according to rich temporal specifications.

</details>


### [337] [ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning](https://arxiv.org/abs/2510.05070)
*Siheng Zhao,Yanjie Ze,Yue Wang,C. Karen Liu,Pieter Abbeel,Guanya Shi,Rocky Duan*

Main category: cs.RO

TL;DR: 本文提出了ResMimic，一种两阶段残差学习框架，用于从人类动作数据中实现精确且富有表现力的人形机器人全身运动控制。


<details>
  <summary>Details</summary>
Motivation: 现有的通用运动跟踪（GMT）策略虽然能复现多样的人类运动，但缺乏精准性和物体感知能力，难以满足人形机器人在日常服务和仓储任务中的操作需求。

Method: 首先使用在大规模人类动作数据上训练的GMT策略作为生成类人全身运动的任务无关基础；然后学习一个高效而精确的残差策略来优化GMT输出，提升运动性能并融入物体交互能力。为促进高效训练，设计了基于点云的物体跟踪奖励、接触奖励以及基于课程的虚拟物体控制器。

Result: 在仿真环境和真实的Unitree G1人形机器人上的实验表明，ResMimic在任务成功率、训练效率和鲁棒性方面均显著优于强基线方法。

Conclusion: ResMimic通过残差学习有效提升了人形机器人在复杂操作任务中的控制精度与适应性，为人形机器人实现高精度物体重交互提供了可行方案。

Abstract: Humanoid whole-body loco-manipulation promises transformative capabilities
for daily service and warehouse tasks. While recent advances in general motion
tracking (GMT) have enabled humanoids to reproduce diverse human motions, these
policies lack the precision and object awareness required for
loco-manipulation. To this end, we introduce ResMimic, a two-stage residual
learning framework for precise and expressive humanoid control from human
motion data. First, a GMT policy, trained on large-scale human-only motion,
serves as a task-agnostic base for generating human-like whole-body movements.
An efficient but precise residual policy is then learned to refine the GMT
outputs to improve locomotion and incorporate object interaction. To further
facilitate efficient training, we design (i) a point-cloud-based object
tracking reward for smoother optimization, (ii) a contact reward that
encourages accurate humanoid body-object interactions, and (iii) a
curriculum-based virtual object controller to stabilize early training. We
evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results
show substantial gains in task success, training efficiency, and robustness
over strong baselines. Videos are available at https://resmimic.github.io/ .

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [338] [LegalSim: Multi-Agent Simulation of Legal Systems for Discovering Procedural Exploits](https://arxiv.org/abs/2510.03405)
*Sanket Badhe*

Main category: cs.MA

TL;DR: 本文提出了LegalSim，一个模块化的多智能体法律程序模拟系统，用于研究AI如何利用成文规则中的程序漏洞。系统包含原告、被告和随机性法官模型，通过不同策略（PPO、上下文Bandit、LLM策略和启发式规则）进行对抗，并引入有效胜率和综合exploit得分评估性能。实验发现存在“exploit链”等有害但合规的策略，结果表明PPO胜率最高，Bandit最稳定，LLM次之，启发式最弱。


<details>
  <summary>Details</summary>
Motivation: 研究AI在遵循成文法律规则的前提下，如何通过策略性行为暴露程序漏洞，揭示现有法律体系中可能被自动化系统滥用的弱点。

Method: 构建基于JSON规则引擎的多智能体模拟环境，包含原被告代理与随机法官模型；比较四种策略（PPO、上下文bandit+LLM、直接LLM、启发式）在不同法律场景下的表现；采用有效胜率和包含成本膨胀、日程压力等维度的综合exploit评分进行评估。

Result: PPO赢得最多，上下文bandit在不同对手间表现最稳定，LLM策略落后于前两者，启发式最弱；在不同法官设置下结果稳定；发现了如成本膨胀型取证序列和日程压迫等‘exploit链’现象。

Conclusion: LegalSim能够揭示看似合规但系统性有害的行为模式，强调除了模型测试外，还需对法律规则系统本身进行红队演练以防范AI滥用。

Abstract: We present LegalSim, a modular multi-agent simulation of adversarial legal
proceedings that explores how AI systems can exploit procedural weaknesses in
codified rules. Plaintiff and defendant agents choose from a constrained action
space (for example, discovery requests, motions, meet-and-confer, sanctions)
governed by a JSON rules engine, while a stochastic judge model with calibrated
grant rates, cost allocations, and sanction tendencies resolves outcomes. We
compare four policies: PPO, a contextual bandit with an LLM, a direct LLM
policy, and a hand-crafted heuristic; Instead of optimizing binary case
outcomes, agents are trained and evaluated using effective win rate and a
composite exploit score that combines opponent-cost inflation, calendar
pressure, settlement pressure at low merit, and a rule-compliance margin.
Across configurable regimes (e.g., bankruptcy stays, inter partes review, tax
procedures) and heterogeneous judges, we observe emergent ``exploit chains'',
such as cost-inflating discovery sequences and calendar-pressure tactics that
remain procedurally valid yet systemically harmful. Evaluation via cross-play
and Bradley-Terry ratings shows, PPO wins more often, the bandit is the most
consistently competitive across opponents, the LLM trails them, and the
heuristic is weakest. The results are stable in judge settings, and the
simulation reveals emergent exploit chains, motivating red-teaming of legal
rule systems in addition to model-level testing.

</details>


### [339] [Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03534)
*Nicolò Dal Fabbro,Milad Mesbahi,Renato Mendes,João Borges de Sousa,George J. Pappas*

Main category: cs.MA

TL;DR: 提出一种多智能体强化学习方法，用于长期河流羽流监测，通过中心协调器与AUV间歇通信，结合时空高斯过程回归与多头Q网络控制器，仿真表明该方法在精度和续航方面优于基准方法，且策略具有跨季节泛化能力。


<details>
  <summary>Details</summary>
Motivation: 实现对动态河流羽流环境的长期、高效、可持续的自主监测，克服能源和通信限制下的多AUV协同挑战。

Method: 采用多智能体强化学习框架，由中心协调器间歇与AUV通信；结合时空高斯过程回归（GPR）建模环境，利用多头Q网络控制各AUV的速度与方向。

Result: 在Delft3D海洋模型仿真中，该方法在均方误差（MSE）和运行续航方面均优于单智能体与多智能体基线方法；增加AUV数量可显著提升性能，双倍AUV可使续航超过翻倍并保持或提高精度；学习策略能泛化至不同季节和年份的未见环境。

Conclusion: 所提方法实现了能源与通信高效的多AUV协同监测，具备良好的扩展性与环境适应性，展现了数据驱动方法在长期动态环境监测中的潜力。

Abstract: We study the problem of long-term (multiple days) mapping of a river plume
using multiple autonomous underwater vehicles (AUVs), focusing on the Douro
river representative use-case. We propose an energy - and communication -
efficient multi-agent reinforcement learning approach in which a central
coordinator intermittently communicates with the AUVs, collecting measurements
and issuing commands. Our approach integrates spatiotemporal Gaussian process
regression (GPR) with a multi-head Q-network controller that regulates
direction and speed for each AUV. Simulations using the Delft3D ocean model
demonstrate that our method consistently outperforms both single- and
multi-agent benchmarks, with scaling the number of agents both improving mean
squared error (MSE) and operational endurance. In some instances, our algorithm
demonstrates that doubling the number of AUVs can more than double endurance
while maintaining or improving accuracy, underscoring the benefits of
multi-agent coordination. Our learned policies generalize across unseen
seasonal regimes over different months and years, demonstrating promise for
future developments of data-driven long-term monitoring of dynamic plume
environments.

</details>


### [340] [Cooperative Flexibility Exchange: Fair and Comfort-Aware Decentralized Resource Allocation](https://arxiv.org/abs/2510.04192)
*Rabiya Khalid,Evangelos Pournaras*

Main category: cs.MA

TL;DR: 提出一种基于去中心化多智能体协调的需求侧管理方法，通过引入时隙交换机制，在保证系统效率的同时提升用户舒适度和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有能源管理系统往往以牺牲用户舒适度为代价来追求系统效率，本文旨在弥补这一不足。

Method: 采用去中心化的多智能体协调框架，每个智能体代表一个用户，通过优化后的电器用电计划进行时隙交换，实现需求侧能量优化。

Result: 实验基于真实数据集，结果表明该方法在不增加系统低效率成本的前提下，显著提升了用户舒适度和满意度的公平性，并具有良好可扩展性。

Conclusion: 所提出的时隙交换机制是一种实用且可扩展的解决方案，适用于未来智能电网中的高效能源管理。

Abstract: The growing electricity demand and increased use of smart appliances are
placing new pressures on power grids, making efficient energy management more
important than ever. The existing energy management systems often prioritize
system efficiency (balanced energy demand and supply) at the expense of user
comfort. This paper addresses this gap by proposing a novel decentralized
multi-agent coordination-based demand-side management system. The proposed
system enables individual agents to coordinate for demand-side energy
optimization while improving the user comfort and maintaining the system
efficiency. A key innovation of this work is the introduction of a slot
exchange mechanism, where agents first receive optimized appliance-level energy
consumption schedules and then coordinate with each other to adjust these
schedules through slot exchanges. This approach improves user comfort even when
agents show non-altruistic behaviour, and it scales well with large
populations. The system also promotes fairness by balancing satisfaction levels
across users. For performance evaluation, a real-world dataset is used, and the
results demonstrate that the proposed slot exchange mechanism increases user
comfort and fairness without raising system inefficiency cost, making it a
practical and scalable solution for future smart grids.

</details>


### [341] [Small Fleet, Big Impact: Enhancing Shared Micromobility Efficiency through Minimal Autonomous Vehicle Deployment](https://arxiv.org/abs/2510.04271)
*Heng Tan,Hua Yan,Lucas Yang,Yu Yang*

Main category: cs.MA

TL;DR: 提出了一种基于分层强化学习的框架SMART，用于优化自主共享微出行车辆的初始部署和实时再平衡，从而提升现有微出行调度系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的微出行调度方法因仅每日重新分配一两次车辆，难以应对需求波动，导致供需不匹配问题。

Method: 设计了SMART框架，采用分层强化学习联合优化自主共享微出行车辆（ASMVs）的高层初始部署和低层实时再平衡策略，并利用芝加哥电动滑板车真实数据进行评估。

Result: 实验结果表明，SMART框架能有效提升服务性能，具有强泛化能力，可无缝集成到现有调度方法中。

Conclusion: 引入少量具备自平衡能力的自主车辆并结合SMART框架，可显著增强微出行系统在动态需求下的适应性与效率。

Abstract: Shared micromobility systems, such as electric scooters and bikes, have
gained widespread popularity as sustainable alternatives to traditional
transportation modes. However, these systems face persistent challenges due to
spatio-temporal demand fluctuations, often resulting in a mismatch between
vehicle supply and user demand. Existing shared micromobility vehicle
scheduling methods typically redistribute vehicles once or twice per day, which
makes them vulnerable to performance degradation under atypical conditions. In
this work, we design to augment existing micromobility scheduling methods by
integrating a small number of autonomous shared micromobility vehicles (ASMVs),
which possess self-rebalancing capabilities to dynamically adapt to real-time
demand. Specifically, we introduce SMART, a hierarchical reinforcement learning
framework that jointly optimizes high-level initial deployment and low-level
real-time rebalancing for ASMVs. We evaluate our framework based on real-world
e-scooter usage data from Chicago. Our experiment results show that our
framework is highly effective and possesses strong generalization capability,
allowing it to seamlessly integrate with existing vehicle scheduling methods
and significantly enhance overall micromobility service performance.

</details>


### [342] [Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs](https://arxiv.org/abs/2510.04303)
*Om Tailor*

Main category: cs.MA

TL;DR: 本文提出了一个名为“Audit the Whisper”的可复现审计框架，用于检测大语言模型代理间的隐秘共谋，结合理论分析、基准测试与多维度检测方法，在600次实验中实现了100%检出率且无误报。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型代理在市场、分配和治理流程中的广泛应用，代理间可能发生的隐秘共谋会损害信任与社会福利，但现有审计方法缺乏理论保障、可迁移性差且难以复现，因此需要一个系统化、可复制的审计工具。

Method: 提出基于信道容量的理论分析，使用KL散度诊断干预（如改写、速率限制、角色置换）对共谋能力的影响；构建包含定价、拍卖和同行评审场景的ColludeBench-v0基准；设计融合互信息、置换不变性、水印方差和公平性偏差的校准审计流水线。

Result: 在600次实验和12种干预条件下，联合元检验实现真阳性率100%且未出现误报；消融实验揭示了审计代价权衡，并发现仅依赖互信息无法识别的基于公平性动机的共谋行为。

Conclusion: Audit the Whisper提供了一个兼具理论保证与实践可行性的共谋审计框架，支持完全可复现研究，并推动LLM多代理系统中可信治理的发展。

Abstract: Multi-agent deployments of large language models (LLMs) are increasingly
embedded in market, allocation, and governance workflows, yet covert
coordination among agents can silently erode trust and social welfare. Existing
audits are dominated by heuristics that lack theoretical guarantees, struggle
to transfer across tasks, and seldom ship with the infrastructure needed for
independent replication. We introduce \emph{Audit the Whisper}, a
conference-grade research artifact that spans theory, benchmark design,
detection, and reproducibility. Our contributions are: (i) a channel-capacity
analysis showing how interventions such as paraphrase, rate limiting, and role
permutation impose quantifiable capacity penalties -- operationalized via
paired-run Kullback--Leibler diagnostics -- that tighten mutual-information
thresholds with finite-sample guarantees; (ii) \textsc{ColludeBench}-v0,
covering pricing, first-price auctions, and peer review with configurable
covert schemes, deterministic manifests, and reward instrumentation; and (iii)
a calibrated auditing pipeline that fuses cross-run mutual information,
permutation invariance, watermark variance, and fairness-aware acceptance bias,
each tuned to a \(10^{-3}\) false-positive budget. Across 600 audited runs
spanning 12 intervention conditions, the union meta-test attains TPR~$=1$ with
zero observed false alarms, while ablations surface the price-of-auditing
trade-off and highlight fairness-driven colluders invisible to MI alone. We
release regeneration scripts, seed-stamped manifests, and documentation so that
external auditors can reproduce every figure and extend the framework with
minimal effort.

</details>


### [343] [NegotiationGym: Self-Optimizing Agents in a Multi-Agent Social Simulation Environment](https://arxiv.org/abs/2510.04368)
*Shashank Mangla,Chris Hokamp,Jack Boylan,Demian Gholipour Ghalandari,Yuuv Jauhari,Lauren Cassidy,Oisin Duffy*

Main category: cs.MA

TL;DR: 提出并实现了NegotiationGym，一个用于配置和运行聚焦于谈判与合作的多智能体社会模拟的API和用户界面。


<details>
  <summary>Details</summary>
Motivation: 为了支持多智能体在谈判和合作场景中的社会模拟研究，提供灵活且易于配置的工具。

Method: 设计了一个用户友好的、基于配置的API，支持自定义模拟场景；通过智能体级别的效用函数，使智能体能够通过多轮交互观察结果并优化策略。

Result: 实现了支持多轮交互和策略优化的多智能体谈判模拟框架，并提供了可扩展的接口和用户界面。

Conclusion: NegotiationGym为研究多智能体在谈判与合作中的行为提供了灵活、易用的平台，有助于推动相关领域的实验研究。

Abstract: We design and implement NegotiationGym, an API and user interface for
configuring and running multi-agent social simulations focused upon negotiation
and cooperation. The NegotiationGym codebase offers a user-friendly,
configuration-driven API that enables easy design and customization of
simulation scenarios. Agent-level utility functions encode optimization
criteria for each agent, and agents can self-optimize by conducting multiple
interaction rounds with other agents, observing outcomes, and modifying their
strategies for future rounds.

</details>


### [344] [Trade in Minutes! Rationality-Driven Agentic System for Quantitative Financial Trading](https://arxiv.org/abs/2510.04787)
*Zifan Song,Kaitao Song,Guosheng Hu,Ding Qi,Junyao Gao,Xiaohua Wang,Dongsheng Li,Cairong Zhao*

Main category: cs.MA

TL;DR: 本文提出了一种名为TiMi（Trade in Minutes）的理性驱动型多智能体系统，通过将策略开发与分钟级执行分离，结合大语言模型的语义分析、代码生成和数学推理能力，实现了在股票和加密货币市场中稳定盈利、高效操作和风险控制。


<details>
  <summary>Details</summary>
Motivation: 现有金融交易代理多模拟人类行为，易引入情绪偏差且依赖外部信息，同时受限于部署时需持续推理的问题，缺乏量化交易所需的机械理性。因此，需要构建一种兼具战略深度与高度理性的自动化交易系统。

Method: 提出TiMi系统，采用策略与执行分离的架构；设计从宏观模式到微观定制的两层分析范式；通过分层编程实现交易机器人；构建基于数学反思的闭环优化机制，并利用大语言模型完成语义理解、代码生成与策略优化。

Result: 在200多个股票和加密货币交易对上的实验表明，TiMi在波动市场环境下实现了稳定的盈利能力、高效的行动响应和良好的风险控制。

Conclusion: TiMi成功融合了智能体的战略决策能力与量化交易的机械理性，为自主金融系统提供了新的架构范式。

Abstract: Recent advancements in large language models (LLMs) and agentic systems have
shown exceptional decision-making capabilities, revealing significant potential
for autonomic finance. Current financial trading agents predominantly simulate
anthropomorphic roles that inadvertently introduce emotional biases and rely on
peripheral information, while being constrained by the necessity for continuous
inference during deployment. In this paper, we pioneer the harmonization of
strategic depth in agents with the mechanical rationality essential for
quantitative trading. Consequently, we present TiMi (Trade in Minutes), a
rationality-driven multi-agent system that architecturally decouples strategy
development from minute-level deployment. TiMi leverages specialized LLM
capabilities of semantic analysis, code programming, and mathematical reasoning
within a comprehensive policy-optimization-deployment chain. Specifically, we
propose a two-tier analytical paradigm from macro patterns to micro
customization, layered programming design for trading bot implementation, and
closed-loop optimization driven by mathematical reflection. Extensive
evaluations across 200+ trading pairs in stock and cryptocurrency markets
empirically validate the efficacy of TiMi in stable profitability, action
efficiency, and risk control under volatile market dynamics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [345] [PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank](https://arxiv.org/abs/2510.03243)
*Yiheng Tao,Yihe Zhang,Matthew T. Dearing,Xin Wang,Yuping Fan,Zhiling Lan*

Main category: cs.LG

TL;DR: 本文提出了PARS，一种提示感知的LLM任务调度器，通过成对排序近似最短作业优先调度，有效减少延迟并提升吞吐量，且可无缝集成到vLLM系统中。


<details>
  <summary>Details</summary>
Motivation: 传统调度策略（如FCFS）存在首部阻塞问题，长任务会阻塞后续短任务，影响推理效率，尤其在推理型LLM广泛应用的背景下更为突出。

Method: PARS采用基于响应长度的任务排序预测，利用带间隔的排序损失进行成对排序学习，聚焦关键调度决策，并集成到vLLM系统中实现低开销高效调度。

Result: 实验表明，PARS在多个LLM和真实推理数据集上显著降低延迟、提升吞吐量，尤其对推理任务效果明显，且跨模型训练具有良好的泛化能力。

Conclusion: PARS通过提示感知的排序机制有效缓解了HOL阻塞问题，在保持低开销的同时显著提升了LLM推理服务的效率，具备良好的实用性和通用性。

Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low
latency and high throughput, particularly with the growing use of
reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve
(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks
delay shorter ones queued behind them. In this paper, we introduce PARS, a
prompt-aware LLM task scheduler that improves serving efficiency by
approximating shortest-job-first (SJF) scheduling through pairwise ranking with
margin ranking loss. PARS focuses on impactful scheduling decisions and is
seamlessly integrated into the state-of-the-art LLM serving system vLLM. It
effectively predicts response-length-based task ordering, reducing latency with
minimal overhead. Extensive experiments across multiple LLMs and real-world
inference datasets show that PARS significantly improves performance, including
for reasoning workloads. Furthermore, our cross-model evaluations demonstrate
that the design generalizes well, enabling effective scheduling even when
predictors are trained on different LLMs.

</details>


### [346] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出VIFO模型，将多变量时间序列转化为图像，利用预训练大视觉模型提取跨通道模式，并与时间序列模态融合，仅训练7.45%参数即在多个基准上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型多采用通道独立架构，忽略跨通道依赖；同时多模态方法未充分挖掘大视觉模型对时空数据的表征能力，跨模态信息融合潜力尚未充分释放。

Method: 将多变量时间序列转化为图像输入预训练大视觉模型（LVM），提取跨通道时空模式，并将视觉特征与原始时间序列模态表示对齐融合；通过冻结LVM大部分参数，仅微调少量参数实现高效训练。

Result: VIFO在多个标准数据集上实现了具有竞争力的预测性能，验证了跨模态建模和视觉化表征的有效性，且仅需训练7.45%的模型参数，具备高效性。

Conclusion: VIFO通过跨模态融合框架，有效捕捉时间序列中的跨变量关系，为时间序列预测提供了一种高效、可扩展的新范式。

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [347] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: 提出了一种新的可迁移频域感知攻击方法，并基于此提出了FAMPE解释方法，在插入分数上比现有方法平均提升13.02%。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在现实噪声和对抗扰动下的可靠性仍面临挑战，现有归因方法效果有限，需进一步改进。

Method: 提出可迁移的频域感知攻击，结合高低频成分进行探索，并设计了频率感知模型参数探索器（FAMPE）以提升DNN的可解释性。

Result: FAMPE在Insertion Score上平均提升13.02%，优于当前最先进的AttEXplore方法，并通过消融研究分析了高低频成分对可解释性的影响。

Conclusion: 所提出的FAMPE方法能更有效地揭示DNN决策机制，频域分析有助于提升模型解释性能。

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [348] [StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory](https://arxiv.org/abs/2510.03246)
*Xinyuan Song,Guangji Bai,Liang Zhao*

Main category: cs.LG

TL;DR: 提出STRUPRUNE，一种基于ADMM的结构化剪枝框架，通过分治策略实现全局剪枝效果的同时将内存开销从O(N)降至O(√N)，适用于十亿参数大模型。


<details>
  <summary>Details</summary>
Motivation: 现有全局结构化剪枝内存开销大，局部剪枝忽略层间依赖导致性能下降，需兼顾内存效率、硬件兼容性与剪枝效果。

Method: 采用分治策略，将全局剪枝分解为各模块协调的子问题；基于ADMM框架设计STRUPRUNE，推导出结构化剪枝掩码的闭式解析解，并提出基于能量的渐近框架实现softmax形式的层稀疏度分配。

Result: STRUPRUNE在保持与全局结构化剪枝相当的困惑度的同时，将内存消耗从O(N)降低到O(√N)，实现了十亿参数模型上的高效部署。

Conclusion: STRUPRUNE有效平衡了结构化剪枝的硬件效率、局部剪枝的内存优势与全局剪枝的性能，为大规模语言模型的剪枝提供了可扩展且实用的解决方案。

Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning
achieves strong performance but requires $\mathcal{O}(N)$ memory, which is
infeasible for billion-parameter models. Local pruning reduces GPU memory usage
to that of a single layer by pruning layers independently, but it neglects
inter-layer dependencies and often leads to suboptimal performance in
high-sparsity regimes. Unlike unstructured pruning, structured pruning produces
regular sparsity patterns that align well with GPU kernels and library
optimizations, making it more hardware-efficient. However, structured pruning
typically relies on global pruning, since structured patterns are more prone to
severe performance degradation under local optimization. To jointly achieve
structured pruning and the memory efficiency of local pruning, we propose a
divide-and-conquer strategy that decomposes the global pruning problem into
coordinated subproblems across different modules, each of which fits within
limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an
ADMM-based framework that integrates structured sparsity into the pruning
process, combining the memory efficiency of local pruning with the hardware
compatibility of structured methods. We derive a closed-form analytical
solution for structured pruning masks that provides an explicit rule for
layer-wise sparsity allocation, and further develop an energy-based asymptotic
framework yielding a softmax-form allocation scheme that simplifies
optimization while adapting to heterogeneous layer importance. Experiments
demonstrate that STRUPRUNE matches the perplexity of global structured pruning
while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$,
enabling practical deployment at the billion-parameter scale.

</details>


### [349] [Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data](https://arxiv.org/abs/2510.03247)
*Jiancheng Zhang,Yinglun Zhu*

Main category: cs.LG

TL;DR: 提出首个用于非对齐多模态数据的主动学习框架，通过结合不确定性与多样性原则，显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习方法主要针对单模态数据，忽视了多模态学习中高昂的对齐标注成本，而实际应用中（如CLIP、SigLIP）获取高质量跨模态对齐代价高昂。

Method: 设计了一种模态感知的主动学习算法，结合不确定性和多样性原则，支持池式和流式场景，可在非对齐多模态数据上进行线性时间复杂度的样本采集。

Result: 在多个基准数据集上验证了方法的有效性，在ColorSwap数据集上最多减少40%的标注量且不损失性能。

Conclusion: 所提框架能有效降低多模态主动学习中的标注成本，适用于现实中的非对齐多模态场景，具有良好的通用性和实用性。

Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in
data-hungry deep learning. However, existing AL algorithms focus almost
exclusively on unimodal data, overlooking the substantial annotation burden in
multimodal learning. We introduce the first framework for multimodal active
learning with unaligned data, where the learner must actively acquire
cross-modal alignments rather than labels on pre-aligned pairs. This setting
captures the practical bottleneck in modern multimodal pipelines such as CLIP
and SigLIP, where unimodal features are easy to obtain but high-quality
alignment is costly. We develop a new algorithm that combines uncertainty and
diversity principles in a modality-aware design, achieves linear-time
acquisition, and applies seamlessly to both pool-based and streaming-based
settings. Extensive experiments on benchmark datasets demonstrate that our
approach consistently reduces multimodal annotation cost while preserving
performance; for instance, on the ColorSwap dataset it cuts annotation
requirements by up to $40\%$ without loss in accuracy.

</details>


### [350] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: 本研究利用神经算子（NO）架构实现创伤性脑损伤（TBI）的快速、个体化脑位移场预测，相比传统有限元方法将计算时间从小时级缩短至毫秒级，支持实时临床决策。


<details>
  <summary>Details</summary>
Motivation: 有限元（FE）模型虽能高精度预测脑变形，但计算耗时过长（每例数小时），难以满足临床快速决策需求，限制了其在TBI诊疗中的应用。

Method: 将TBI建模视为算子学习问题，输入个体化解剖MRI、磁共振弹性成像（MRE）刚度图和人口统计学特征，输出全脑3D位移场；比较了FNO、F-FNO、MG-FNO和DeepONet四种神经算子架构，在249例MRE数据集上进行训练与评估。

Result: MG-FNO精度最高（MSE=0.0023，空间保真度94.3%），且保留细尺度特征；F-FNO收敛速度比标准FNO快2倍；DeepONet推理最快（14.5次/秒），计算速度比MG-FNO快7倍；所有NO模型均将计算时间从小时级降至毫秒级，保持解剖真实性。

Conclusion: 神经算子为TBI提供了高效、分辨率不变的脑变形预测方法，有望实现个体化实时风险评估、临床分诊支持及防护设备优化，推动基于数字孪生的可扩展生物力学建模在临床与公共卫生中的应用。

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [351] [Light Differentiable Logic Gate Networks](https://arxiv.org/abs/2510.03250)
*Lukas Rüttgers,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 提出了一种新的逻辑门神经元重参数化方法，解决了可微逻辑门网络在训练中的梯度消失、离散化误差和高训练成本问题，显著减小模型大小、加速反向传播并减少训练步数，同时保持或提升CIFAR-100上的准确率。


<details>
  <summary>Details</summary>
Motivation: 可微逻辑门网络（DLGNs）在推理效率和准确性方面表现优异，但存在梯度消失、离散化误差和高训练成本等问题，且随着网络深度增加，准确率下降，其根本原因在于逻辑门神经元的参数化方式。

Method: 提出一种新的逻辑门神经元重参数化方法，该方法对每个逻辑门的输入数量实现对数级的参数压缩，并优化训练过程。

Result: 在二值输入下，模型大小减少4倍，反向传播速度提升最高1.86倍，收敛所需的训练步数减少8.5倍，且在CIFAR-100上的准确率保持稳定甚至优于原始参数化方法。

Conclusion: 所提出的重参数化方法有效解决了DLGNs的训练难题，在减小模型规模和加速训练的同时，保持了良好的准确性，有助于DLGNs的规模化应用。

Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency
at inference while sustaining competitive accuracy. But vanishing gradients,
discretization errors, and high training cost impede scaling these networks.
Even with dedicated parameter initialization schemes from subsequent works,
increasing depth still harms accuracy. We show that the root cause of these
issues lies in the underlying parametrization of logic gate neurons themselves.
To overcome this issue, we propose a reparametrization that also shrinks the
parameter size logarithmically in the number of inputs per gate. For binary
inputs, this already reduces the model size by 4x, speeds up the backward pass
by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we
show that the accuracy on CIFAR-100 remains stable and sometimes superior to
the original parametrization.

</details>


### [352] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: 提出Triple-BERT，一种基于BERT和动作分解的集中式单智能体强化学习方法，用于大规模网约车订单调度，显著提升服务订单数并减少接驾时间。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法在处理大规模订单调度时难以捕捉全局信息且协作能力差，CTDE方法面临维度灾难问题。

Method: 基于TD3框架，采用动作分解策略将联合动作概率分解为个体司机动作概率，并设计基于BERT的网络结构，利用参数共享和注意力机制应对大规模观测空间。

Result: 在真实曼哈顿数据集上验证，相比当前最优方法，服务订单数提升4.26%，接驾时间减少22.25%，总体性能提升11.95%。

Conclusion: Triple-BERT有效解决了大规模订单调度中的高维状态-动作空间挑战，实现了更高效的匹配性能。

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [353] [Numerion: A Multi-Hypercomplex Model for Time Series Forecasting](https://arxiv.org/abs/2510.03251)
*Hanzhong Cao,Wenbo Yan,Ying Tan*

Main category: cs.LG

TL;DR: 提出基于多超复数空间的时序预测模型Numerion，利用高维空间捕捉低频特征，实现自然分解与自适应融合，达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂结构和先验知识，受限于计算复杂度和假设鲁棒性，难以有效分解和建模时间序列。

Method: 将线性层和激活函数推广到任意2的幂次维超复数空间，提出Real-Hypercomplex-Real域MLP（RHR-MLP），通过多个RHR-MLP在不同维度空间映射序列，并用动态融合机制整合隐含模式。

Result: 在多个公开数据集上达到最先进的预测性能，可视化和定量分析验证了多维RHR-MLP能自然分解时序并揭示高维空间偏好捕获低频特征的趋势。

Conclusion: Numerion通过利用超复数空间的特性实现了高效的时间序列分解与建模，为未来时序预测提供了新的方向。

Abstract: Many methods aim to enhance time series forecasting by decomposing the series
through intricate model structures and prior knowledge, yet they are inevitably
limited by computational complexity and the robustness of the assumptions. Our
research uncovers that in the complex domain and higher-order hypercomplex
spaces, the characteristic frequencies of time series naturally decrease.
Leveraging this insight, we propose Numerion, a time series forecasting model
based on multiple hypercomplex spaces. Specifically, grounded in theoretical
support, we generalize linear layers and activation functions to hypercomplex
spaces of arbitrary power-of-two dimensions and introduce a novel
Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.
Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces
of varying dimensions, naturally decomposing and independently modeling the
series, and adaptively fuses the latent patterns exhibited in different spaces
through a dynamic fusion mechanism. Experiments validate the model`s
performance, achieving state-of-the-art results on multiple public datasets.
Visualizations and quantitative analyses comprehensively demonstrate the
ability of multi-dimensional RHR-MLPs to naturally decompose time series and
reveal the tendency of higher dimensional hypercomplex spaces to capture lower
frequency features.

</details>


### [354] [KVComm: Enabling Efficient LLM Communication through Selective KV Sharing](https://arxiv.org/abs/2510.03346)
*Xiangyu Shi,Marco Chiesa,Gerald Q. Maguire Jr.,Dejan Kostic*

Main category: cs.LG

TL;DR: 提出KVComm框架，通过选择性共享键值对（KV pairs）实现大语言模型间的高效通信，显著减少传输量同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有通信协议在多智能体系统中存在高推理成本、信息丢失或信息集中偏差等问题，需寻找更高效的LLM间通信方式。

Method: 提出基于注意力重要性得分和高斯先验的层间KV选择策略，选择最具信息量的KV对进行通信。

Result: 实验表明，KVComm仅传输30%的KV层即可达到与直接合并输入的上界方法相当的性能。

Conclusion: KV对是LLM间通信的有效媒介，KVComm为构建可扩展且高效的多智能体系统提供了新路径。

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-agent
systems, where effective inter-model communication is crucial. Existing
communication protocols either rely on natural language, incurring high
inference costs and information loss, or on hidden states, which suffer from
information concentration bias and inefficiency. To address these limitations,
we propose KVComm, a novel communication framework that enables efficient
communication between LLMs through selective sharing of KV pairs. KVComm
leverages the rich information encoded in the KV pairs while avoiding the
pitfalls of hidden states. We introduce a KV layer-wise selection strategy
based on attention importance scores with a Gaussian prior to identify the most
informative KV pairs for communication. Extensive experiments across diverse
tasks and model pairs demonstrate that KVComm achieves comparable performance
to the upper-bound method, which directly merges inputs to one model without
any communication, while transmitting as few as 30\% of layers' KV pairs. Our
study highlights the potential of KV pairs as an effective medium for inter-LLM
communication, paving the way for scalable and efficient multi-agent systems.

</details>


### [355] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: 提出了一种名为Diffusion Router（DR）的通用多域翻译框架，通过中心域实现任意域间的间接和直接翻译，在多个大规模基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要完全对齐的数据或只能处理训练中见过的域对，限制了多域翻译的实际应用。因此需要一种更通用、可扩展的方法来支持任意域间的翻译。

Method: 提出Diffusion Router（DR），基于扩散模型，使用单一噪声预测器并结合源和目标域标签进行条件建模；通过中心域路由实现非中心域间的间接翻译，并引入变分下界目标和Tweedie精炼过程支持直接非中心映射。

Result: 在三个大规模UMDT基准上，DR在间接和直接翻译任务中均达到最先进水平，降低了采样成本，并支持sketch↔segmentation等新任务。

Conclusion: DR是一种可扩展且通用的多域翻译框架，能够高效实现任意域间的图像翻译，具有广泛的应用潜力。

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [356] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为分层偏好学习（HPL）的框架，通过多粒度偏好信号优化大语言模型代理，在轨迹级、步骤级和组级进行分层优化，并引入双层课程学习机制，显著提升了复杂长视野任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于偏好的离线对齐方法（如DPO）存在粒度不匹配问题：轨迹级DPO过于粗糙，难以精确归因；步骤级DPO又过于短视，无法捕捉多步行为的价值。因此需要一种能融合多种粒度优势的新型优化框架。

Method: 提出HPL框架，将专家轨迹分解为语义连贯的动作组，生成对比的次优动作组以实现子任务级别的偏好学习；引入双层课程调度器，从组长度（子任务复杂度）和样本难度（奖励差距）两个维度组织学习过程，逐步提升训练难度。

Result: 在三个具有挑战性的代理基准上实验表明，HPL优于现有最先进方法；分析显示其分层DPO损失能有效整合多粒度偏好信号，双层课程对解决从简单到复杂多步任务至关重要。

Conclusion: HPL通过引入组级偏好优化和双层课程学习，解决了传统DPO在长视野任务中的粒度不匹配问题，实现了更精准的信用分配和更稳定的策略学习，显著提升了LLM代理的性能。

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [357] [The Argument is the Explanation: Structured Argumentation for Trust in Agents](https://arxiv.org/abs/2510.03442)
*Ege Cakar,Per Ola Kristensson*

Main category: cs.LG

TL;DR: 本文提出使用结构化论证为AI系统提供可验证的推理链，通过将LLM输出转化为论证图实现透明性与自动幻觉检测，在多个任务上取得SOTA性能，并支持无需重新训练的迭代验证。


<details>
  <summary>Details</summary>
Motivation: 现有AI解释方法（如机械透明性或LLM生成解释）无法提供可靠的验证机制，而社会依赖于可验证的论点进行决策，因此需要一种类似人类论证过程的可验证AI解释框架。

Method: 采用结构化论证框架（特别是Bipolar Assumption-Based Argumentation），将LLM生成的文本转换为包含支持与攻击关系的论证图，并构建多智能体协作的风险评估流程（Structured What-If Technique），实现推理步骤的逐级验证。

Result: 在AAEC数据集上达到94.44的macro F1分数（比先前工作高5.7点），在Argumentative MicroTexts关系分类任务上达到0.81 macro F1（高出同类设置约0.07）；实现了基于事实节点的自动幻觉检测和测试时反馈驱动的迭代优化。

Conclusion: 结构化论证为AI可解释性提供了超越传统可解释方法的新范式，不仅能提升模型性能，还支持推理过程的可验证性、透明协作与无需重训练的持续改进，具有实际部署价值。

Abstract: Humans are black boxes -- we cannot observe their neural processes, yet
society functions by evaluating verifiable arguments. AI explainability should
follow this principle: stakeholders need verifiable reasoning chains, not
mechanistic transparency. We propose using structured argumentation to provide
a level of explanation and verification neither interpretability nor
LLM-generated explanation is able to offer. Our pipeline achieves
state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7
points above prior work) and $0.81$ macro F1, $\sim$0.07 above previous
published results with comparable data setups, for Argumentative MicroTexts
relation classification, converting LLM text into argument graphs and enabling
verification at each inferential step. We demonstrate this idea on multi-agent
risk assessment using the Structured What-If Technique, where specialized
agents collaborate transparently to carry out risk assessment otherwise
achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we
capture support/attack relationships, thereby enabling automatic hallucination
detection via fact nodes attacking arguments. We also provide a verification
mechanism that enables iterative refinement through test-time feedback without
retraining. For easy deployment, we provide a Docker container for the
fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python
package on GitHub.

</details>


### [358] [Adversarial training with restricted data manipulation](https://arxiv.org/abs/2510.03254)
*David Benfield,Stefano Coniglio,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: 提出一种受限的悲观双层优化模型，通过约束对手行为来提升分类器在现实场景中的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有悲观双层优化方法因对手不受限而导致模型过于悲观且不切实际，影响分类器在真实数据上的表现。

Method: 构建一个受限的悲观双层优化模型，限制对手修改数据的能力，使对抗过程更贴近现实。

Result: 实验表明，该方法平均优于现有方法，在多个应用场景中表现出更好的分类性能。

Conclusion: 受限的悲观双层优化能更真实地模拟对抗过程，提升模型在实际应用中的鲁棒性与有效性。

Abstract: Adversarial machine learning concerns situations in which learners face
attacks from active adversaries. Such scenarios arise in applications such as
spam email filtering, malware detection and fake image generation, where
security methods must be actively updated to keep up with the everimproving
generation of malicious data. Pessimistic Bilevel optimisation has been shown
to be an effective method of training resilient classifiers against such
adversaries. By modelling these scenarios as a game between the learner and the
adversary, we anticipate how the adversary will modify their data and then
train a resilient classifier accordingly. However, since existing pessimistic
bilevel approaches feature an unrestricted adversary, the model is vulnerable
to becoming overly pessimistic and unrealistic. When finding the optimal
solution that defeats the classifier, it is possible that the adversary's data
becomes nonsensical and loses its intended nature. Such an adversary will not
properly reflect reality, and consequently, will lead to poor classifier
performance when implemented on real-world data. By constructing a constrained
pessimistic bilevel optimisation model, we restrict the adversary's movements
and identify a solution that better reflects reality. We demonstrate through
experiments that this model performs, on average, better than the existing
approach.

</details>


### [359] [SciTS: Scientific Time Series Understanding and Generation with LLMs](https://arxiv.org/abs/2510.03255)
*Wen Wu,Ziyang Zhang,Liwei Liu,Xuenan Xu,Junlin Liu,Ke Fan,Qitan Lv,Jimin Zhuang,Chen Zhang,Zheqi Yuan,Siyuan Hou,Tianyi Lin,Kai Chen,Bowen Zhou,Chao Zhang*

Main category: cs.LG

TL;DR: 本文提出了SciTS，一个涵盖12个科学领域、43项任务的科学时间序列基准，并引入TimeOmni框架以提升大语言模型对时间序列的理解与生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理科学时间序列时多将其转为文本或图像，存在精度损失或序列过长问题，且缺乏统一评估基准。

Method: 构建了包含5万余实例的SciTS基准，评估17种模型表现，并提出TimeOmni框架，使LLM能直接处理时间序列数据并保持通用训练兼容性。

Result: 实验表明通用LLM比专用时间序列模型更具泛化能力；将时间序列转为文本或图像会限制性能；TimeOmni提升了LLM在科学时间序列上的理解与生成效果。

Conclusion: TimeOmni结合SciTS填补了科学时间序列评测与建模的空白，推动LLM在复杂时序科学数据中的应用。

Abstract: The scientific reasoning ability of large language models (LLMs) has recently
attracted significant attention. Time series, as a fundamental modality in
scientific data, presents unique challenges that are often overlooked in
current multimodal LLMs, which either encode numerical sequences as text or
convert them into images. Such approaches may be insufficient for comprehensive
scientific time series understanding and generation. Existing unified time
series models typically specialise in either forecasting or analysis, and their
effectiveness on non-periodic, heterogeneous scientific signals remains
unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12
scientific domains and 43 tasks, with over 50k+ instances, both univariate and
multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz
in frequency. We benchmark 17 models, including text-only LLMs, multimodal
LLMs, and unified time series models, and find that general-purpose LLMs
exhibit stronger generalisability than specialised time series models, while
representing time series as text or images limits their performance due to
excessively long sequences and loss of numerical precision, respectively. We
then introduce TimeOmni, a framework that equips LLMs with the ability to
understand and generate time series while remaining compatible with
general-purpose LLM training. This work fills a gap in both dedicated
benchmarks and modelling frameworks for scientific time series, paving the way
for LLMs to understand and generate complex temporal scientific data.

</details>


### [360] [Deep Reinforcement Learning for Multi-Agent Coordination](https://arxiv.org/abs/2510.03592)
*Kehinde O. Aina,Sehoon Ha*

Main category: cs.LG

TL;DR: 提出了一种基于虚拟信息素的去中心化多智能体深度强化学习框架（S-MADRL），通过课程学习实现复杂拥挤环境下的高效多机器人协调。


<details>
  <summary>Details</summary>
Motivation: 在狭窄和受限环境中，多机器人系统常因拥堵和干扰导致协作效率下降，现有方法在收敛性和可扩展性上存在局限。

Method: 受昆虫群落间接通信（痕迹引导，stigmergy）启发，引入虚拟信息素建模局部和社会交互，并结合课程学习逐步训练智能体应对复杂任务。

Result: 在最多八个智能体的模拟中，S-MADRL实现了最有效的协调，机器人自发形成非对称工作负载分布，减少拥堵并优化群体性能。

Conclusion: 该框架为通信受限的拥挤环境提供了可扩展的去中心化多智能体协调方案，展现出类自然系统的自组织能力。

Abstract: We address the challenge of coordinating multiple robots in narrow and
confined environments, where congestion and interference often hinder
collective task performance. Drawing inspiration from insect colonies, which
achieve robust coordination through stigmergy -- modifying and interpreting
environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement
Learning (S-MADRL) framework that leverages virtual pheromones to model local
and social interactions, enabling decentralized emergent coordination without
explicit communication. To overcome the convergence and scalability limitations
of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum
learning, which decomposes complex tasks into progressively harder
sub-problems. Simulation results show that our framework achieves the most
effective coordination of up to eight agents, where robots self-organize into
asymmetric workload distributions that reduce congestion and modulate group
performance. This emergent behavior, analogous to strategies observed in
nature, demonstrates a scalable solution for decentralized multi-agent
coordination in crowded environments with communication constraints.

</details>


### [361] [Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03823)
*Adam Haroon,Tristan Schuler*

Main category: cs.LG

TL;DR: 本研究首次系统地将多智能体强化学习（MARL）应用于高空气球（HAB）的区域覆盖协调任务，提出基于QMIX算法的协作框架，并在扩展的RLHAB仿真环境中验证其性能接近理论最优的几何确定性方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多HAB协同方法依赖确定性算法，在小规模团队和局部任务中表现不佳；单智能体强化学习已有应用，但多智能体强化学习尚未探索。

Method: 扩展RLHAB仿真环境以支持多智能体协同学习，采用QMIX算法，结合集中训练与分散执行机制，设计包含个体状态、环境信息和队友数据的观测空间，以及分层奖励函数以优先覆盖并促进空间分布。

Result: QMIX在分布式区域覆盖任务中达到了与理论最优几何确定性方法相当的性能，证明了MARL在HAB协同中的有效性。

Conclusion: MARL为HAB编队控制提供了可行且可扩展的新范式，尤其适用于确定性方法难以应对的复杂自主任务场景。

Abstract: High Altitude Balloons (HABs) can leverage stratospheric wind layers for
limited horizontal control, enabling applications in reconnaissance,
environmental monitoring, and communications networks. Existing multi-agent HAB
coordination approaches use deterministic methods like Voronoi partitioning and
extremum seeking control for large global constellations, which perform poorly
for smaller teams and localized missions. While single-agent HAB control using
reinforcement learning has been demonstrated on HABs, coordinated multi-agent
reinforcement learning (MARL) has not yet been investigated. This work presents
the first systematic application of multi-agent reinforcement learning (MARL)
to HAB coordination for distributed area coverage. We extend our previously
developed reinforcement learning simulation environment (RLHAB) to support
cooperative multi-agent learning, enabling multiple agents to operate
simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area
coverage coordination, leveraging Centralized Training with Decentralized
Execution to address atmospheric vehicle coordination challenges. Our approach
employs specialized observation spaces providing individual state,
environmental context, and teammate data, with hierarchical rewards
prioritizing coverage while encouraging spatial distribution. We demonstrate
that QMIX achieves similar performance to the theoretically optimal geometric
deterministic method for distributed area coverage, validating the MARL
approach and providing a foundation for more complex autonomous multi-HAB
missions where deterministic methods become intractable.

</details>


### [362] [POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation](https://arxiv.org/abs/2510.03258)
*Chang'an Yi,Xiaohui Deng,Shuaicheng Niu,Yan Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种名为POEM的测试时自适应方法，通过探索以往被忽略的可靠样本提升模型在分布偏移下的性能，并引入额外的适配分支网络以平衡领域无关表示与目标数据性能，实验表明其在多种架构和真实场景中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法依赖熵作为置信度度量，但对预设阈值敏感，导致部分潜在可靠的样本被忽略，限制了模型自适应能力。

Method: 提出POEM方法，利用此前未被充分利用的样本进行模型优化，并设计一个额外的适应分支网络，以平衡提取领域无关特征和提升目标域性能。

Result: 在多个架构和真实世界域偏移场景下，POEM consistently优于现有TTA方法，且计算效率高；消融实验验证了其有效性，并可作为增强策略提升其他TTA方法性能。

Conclusion: POEM通过探索被忽视的可靠样本显著提升了TTA性能，具备广泛适用性和实际应用价值。

Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to
unknown test data with potential distribution shifts in an online manner. Many
existing TTA methods rely on entropy as a confidence metric to optimize the
model. However, these approaches are sensitive to the predefined entropy
threshold, influencing which samples are chosen for model adaptation.
Consequently, potentially reliable target samples are often overlooked and
underutilized. For instance, a sample's entropy might slightly exceed the
threshold initially, but fall below it after the model is updated. Such samples
can provide stable supervised information and offer a normal range of gradients
to guide model adaptation. In this paper, we propose a general approach,
\underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the
previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}}
sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch
network to strike a balance between extracting domain-agnostic representations
and achieving high performance on target data. Comprehensive experiments across
multiple architectures demonstrate that POEM consistently outperforms existing
TTA methods in both challenging scenarios and real-world domain shifts, while
remaining computationally efficient. The effectiveness of POEM is evaluated
through extensive analyses and thorough ablation studies. Moreover, the core
idea behind POEM can be employed as an augmentation strategy to boost the
performance of existing TTA approaches. The source code is publicly available
at \emph{https://github.com/ycarobot/POEM}

</details>


### [363] [Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning](https://arxiv.org/abs/2510.03259)
*Yoonjeon Kim,Doohyuk Jang,Eunho Yang*

Main category: cs.LG

TL;DR: 本文提出了一种通过自我对齐增强大模型元意识（meta-awareness）的训练方法MASA，无需外部数据，利用自生成信号提升推理准确性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型缺乏元意识，元预测与真实推理路径严重不对齐，限制了性能提升。

Method: 设计了一个名为MASA的自对齐训练流程，通过过滤零方差提示和提前截断无效推理路径来提高训练效率，并利用自生成信号进行元意识训练。

Result: 在领域内任务上显著提升了准确率（如AIME25提升19.3%，数学基准平均提升6.2%），训练速度加快1.28倍；在跨领域基准上也表现出良好泛化能力，GPQA-Diamond提升3.87%，13个基准平均提升2.08%。

Conclusion: 增强元意识能有效提升大模型的推理性能和训练效率，且具备良好的跨领域泛化能力，MASA为构建更智能的推理模型提供了新方向。

Abstract: Recent studies on reasoning models explore the meta-awareness of language
models, the ability to know how to think by itself. We argue that large
reasoning models lack this meta-awareness property by proving severe
misalignment between true rollouts and predicted meta information. We posit
that aligning meta-prediction with true rollouts will lead to significant
performance gains. To verify this hypothesis, we design a training pipeline
that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced
meta-awareness directly translates to improved accuracy. Unlike existing
meta-cognitive reasoning models, our method does not require external training
sources but leverages self-generated signals to train meta-awareness. Moreover,
our method enables efficient training by i) filtering out zero-variance prompts
that are either trivial or unsolvable and ii) cutting off lengthy rollouts when
they are unlikely to lead to correct answers. The results are inspiring: our
strategy yields significant improvements in both accuracy and training
efficiency on in-domain tasks and shows strong generalization to out-of-domain
benchmarks. More specifically, our method can speed up GRPO training by over
1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on
AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with
meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %
boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks
spanning logical, scientific, and coding domains.

</details>


### [364] [Semantic-Inductive Attribute Selection for Zero-Shot Learning](https://arxiv.org/abs/2510.03260)
*Juan Jose Herrera-Aranda,Guillermo Gomez-Trenado,Francisco Herrera,Isaac Triguero*

Main category: cs.LG

TL;DR: 本文提出了一种新的划分方案，用于在归纳式零样本学习中评估属性相关性，并研究了两种互补的特征选择策略：一种基于模型的嵌入式方法（RFS），另一种基于进化计算（GA），实验表明两者均能有效提升未见类别的分类精度。


<details>
  <summary>Details</summary>
Motivation: 现有的语义空间常包含噪声、冗余或无关属性，影响零样本学习性能，且在归纳式设定下难以有效评估属性对未见类的相关性。

Method: 提出一种模拟未见条件的划分方案，结合两种特征选择策略：1）将嵌入式特征选择适应于零样本学习需求，实现语义剪枝；2）采用遗传算法直接搜索最优属性子集。

Result: 在五个基准数据集上实验表明，两种方法均能显著减少冗余并提升未见类准确率：RFS高效但依赖超参数，GA搜索更广且无需敏感调参。

Conclusion: 语义空间存在固有冗余，所提划分方案有效支持在归纳条件下进行语义空间优化，两种特征选择策略具有互补优势。

Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial
Intelligence Systems, particularly in those that operate in open-world
scenarios where systems must adapt to new tasks dynamically. Semantic spaces
play a pivotal role as they bridge seen and unseen classes, but whether
human-annotated or generated by a machine learning model, they often contain
noisy, redundant, or irrelevant attributes that hinder performance. To address
this, we introduce a partitioning scheme that simulates unseen conditions in an
inductive setting (which is the most challenging), allowing attribute relevance
to be assessed without access to semantic information from unseen classes.
Within this framework, we study two complementary feature-selection strategies
and assess their generalisation. The first adapts embedded feature selection to
the particular demands of ZSL, turning model-driven rankings into meaningful
semantic pruning; the second leverages evolutionary computation to directly
explore the space of attribute subsets more broadly. Experiments on five
benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods
consistently improve accuracy on unseen classes by reducing redundancy, but in
complementary ways: RFS is efficient and competitive though dependent on
critical hyperparameters, whereas GA is more costly yet explores the search
space more broadly and avoids such dependence. These results confirm that
semantic spaces are inherently redundant and highlight the proposed
partitioning scheme as an effective tool to refine them under inductive
conditions.

</details>


### [365] [Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark](https://arxiv.org/abs/2510.03261)
*C. Coelho,M. Hohmann,D. Fernández,L. Penter,S. Ihlenfeldt,O. Niggemann*

Main category: cs.LG

TL;DR: 提出一种基于神经网络预测温度和热流场的新方法，用于实现灵活且通用的机床热误差修正。


<details>
  <summary>Details</summary>
Motivation: 传统热误差补偿方法受限于特定误差类型、空间位置或机床配置，缺乏通用性和适应性。

Method: 利用有限元方法获取数据训练神经网络，预测高保真度的温度和热流场，并采用基于相关性的测点选择策略减少硬件需求；比较了多种时序神经网络架构在特定和通用场景下的表现。

Result: 所提方法能准确、低成本地预测温度和热流场，支持多种误差类型的模块化修正，具备良好的泛化能力。

Conclusion: 该框架为机床环境下的热误差修正提供了更灵活、可扩展的解决方案，具有较强的实用性与推广价值。

Abstract: Thermal errors in machine tools significantly impact machining precision and
productivity. Traditional thermal error correction/compensation methods rely on
measured temperature-deformation fields or on transfer functions. Most existing
data-driven compensation strategies employ neural networks (NNs) to directly
predict thermal errors or specific compensation values. While effective, these
approaches are tightly bound to particular error types, spatial locations, or
machine configurations, limiting their generality and adaptability. In this
work, we introduce a novel paradigm in which NNs are trained to predict
high-fidelity temperature and heat flux fields within the machine tool. The
proposed framework enables subsequent computation and correction of a wide
range of error types using modular, swappable downstream components. The NN is
trained using data obtained with the finite element method under varying
initial conditions and incorporates a correlation-based selection strategy that
identifies the most informative measurement points, minimising hardware
requirements during inference. We further benchmark state-of-the-art
time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,
Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal
Convolutional Network, by training both specialised models, tailored for
specific initial conditions, and general models, capable of extrapolating to
unseen scenarios. The results show accurate and low-cost prediction of
temperature and heat flux fields, laying the basis for enabling flexible and
generalisable thermal error correction in machine tool environments.

</details>


### [366] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出正交蒙特卡洛Dropout方法，理论上保证合并LoRA时的正交性，但实验证明正交性不足以实现语义解耦和组合性。


<details>
  <summary>Details</summary>
Motivation: 解决多个LoRA合并时语义向量相互干扰的问题，期望通过正交性实现更好的语义组合。

Method: 提出Orthogonal Monte Carlo Dropout，在不增加时间复杂度的前提下强制稀疏语义向量的正交性。

Result: 在理论和运行时层面保证了合并LoRA的正交性，但实验表明这种正交性并未带来预期的语义解耦和组合性。

Conclusion: LoRA间的正交性本身可能不足以实现真正的语义组合性，需重新审视其在适配器合并中的作用。

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [367] [Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models](https://arxiv.org/abs/2510.03263)
*Agnieszka Polowczyk,Alicja Polowczyk,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.LG

TL;DR: 本文探讨了现代文本到图像模型在生成逼真视觉内容方面的能力及其潜在滥用问题，提出了“机器遗忘”领域中的知识遗忘与恢复挑战。作者引入了记忆自再生任务和MemoRa策略，强调知识检索鲁棒性作为评估遗忘技术的重要指标，并指出遗忘分为短期（易恢复）和长期（难恢复）两种形式。


<details>
  <summary>Details</summary>
Motivation: 由于文本到图像模型可能被滥用于生成有害或非法内容，亟需发展有效的机器遗忘技术以去除模型中特定训练知识，同时保持整体性能。然而现有方法难以彻底遗忘概念，存在通过对抗提示恢复未学习内容的风险。

Method: 提出记忆自再生任务（Memory Self-Regeneration）和MemoRa策略，采用再生式方法增强遗忘后知识的恢复能力；并通过分析模型在不同提示下的表现，区分短期与长期遗忘模式。

Result: 揭示了当前模型在遗忘特定概念方面的不足，展示了对抗性提示可导致‘已遗忘’概念重现；提出了新的评估维度——知识检索鲁棒性，并验证了短期与长期遗忘的存在。

Conclusion: 真正的机器遗忘不仅需要删除知识，还需防止其被重新激活；引入知识恢复机制有助于评估遗忘效果，未来应将检索鲁棒性作为开发更安全、可靠遗忘技术的关键标准。

Abstract: The impressive capability of modern text-to-image models to generate
realistic visuals has come with a serious drawback: they can be misused to
create harmful, deceptive or unlawful content. This has accelerated the push
for machine unlearning. This new field seeks to selectively remove specific
knowledge from a model's training data without causing a drop in its overall
performance. However, it turns out that actually forgetting a given concept is
an extremely difficult task. Models exposed to attacks using adversarial
prompts show the ability to generate so-called unlearned concepts, which can be
not only harmful but also illegal. In this paper, we present considerations
regarding the ability of models to forget and recall knowledge, introducing the
Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which
we consider to be a regenerative approach supporting the effective recovery of
previously lost knowledge. Moreover, we propose that robustness in knowledge
retrieval is a crucial yet underexplored evaluation measure for developing more
robust and effective unlearning techniques. Finally, we demonstrate that
forgetting occurs in two distinct ways: short-term, where concepts can be
quickly recalled, and long-term, where recovery is more challenging.

</details>


### [368] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 本研究首次系统探讨了在不同训练阶段引入推理数据对大语言模型性能的影响，发现将推理数据提前至预训练阶段至关重要，能建立后续微调无法复制的基础能力。


<details>
  <summary>Details</summary>
Motivation: 由于大多数前沿模型的预训练数据不透明，推理数据在预训练阶段的作用尚不清楚。本文旨在探究在预训练和后训练不同阶段引入推理数据的效果差异，以回答提前引入是否更优、是否存在过拟合风险等关键问题。

Method: 通过控制变量实验，在不同训练阶段（预训练、中期训练、后训练）引入规模、多样性和质量各异的推理数据，系统评估其对模型性能的影响，并分析数据分配策略的有效性。

Result: 发现将推理数据前置到预训练阶段效果显著（平均提升19%），且预训练阶段受益于推理模式的多样性（平均提升11%），而后训练（SFT）更依赖数据质量（平均提升15%）。高质量预训练数据具有潜伏效应，需经SFT激活；盲目扩大SFT数据可能冲淡早期推理注入的优势。

Conclusion: 挑战了语言建模与推理分离的传统范式，提出应在整个训练流程中战略性分配数据：预训练注重推理多样性，后训练注重数据质量，以构建更具推理能力的模型。

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [369] [MindCraft: How Concept Trees Take Shape In Deep Models](https://arxiv.org/abs/2510.03265)
*Bowei Tian,Yexiao He,Wanghao Ye,Ziyao Wang,Meng Liu,Ang Li*

Main category: cs.LG

TL;DR: 提出基于概念树的MindCraft框架，通过谱分解揭示大模型中概念的层次化形成与稳定过程，实现跨领域的可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 大模型在多任务上表现优异，但其内部概念结构和稳定机制尚不明确，缺乏有效的分析工具。

Method: 引入受因果推断启发的Concept Trees框架，对每一层进行谱分解，并将主方向连接成分支的Concept Paths，重构概念的层次化涌现过程。

Result: 在医学诊断、物理推理和政治决策等多个领域验证了Concept Trees能有效恢复语义层级、解耦潜在概念，并具有广泛适用性。

Conclusion: Concept Trees为深入分析深度模型中的概念表示提供了强大且通用的框架，推动了可解释AI的发展。

Abstract: Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.

</details>


### [370] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: 本研究提出了一种基于变分自编码器（VAE）的新方法，用于检测陆地总初级生产力（GPP）中的极端事件，并与传统的奇异谱分析（SSA）方法进行比较，结果显示VAE在识别碳循环异常方面具有与SSA相当的性能，同时具备更强的非线性建模能力和计算优势。


<details>
  <summary>Details</summary>
Motivation: 气候异常对陆地碳循环有显著影响，亟需有效方法来检测植物生产力中的异常行为，尤其是未来气候变化情景下的极端事件。

Method: 采用具有三个全连接层和12个月输入序列长度的变分自编码器（VAE），在归一化的GPP时间序列上训练，通过重构误差识别异常；并与传统奇异谱分析（SSA）方法在多个时间段和区域进行对比，使用5th百分位数阈值定义极端事件。

Result: VAE与SSA在极端事件空间分布模式上表现出较强的一致性，但VAE的阈值更高；两者均显示到2050-80年负向碳循环极端事件的频率和强度增加，尤其是在美国西部和中部地区；VAE无需预设信号周期性，能从数据中自动学习非线性时序依赖。

Conclusion: VAE在检测GPP极端事件方面表现良好，与传统SSA方法相当，且具有无需人工设定周期性和更强非线性建模能力的优势，适用于未来碳循环异常监测。

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [371] [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)
*Xianglong Yan,Chengzhu Bao,Zhiteng Li,Tianao Zhang,Kaicheng Yang,Haotong Qin,Ruobing Xie,Xingwu Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为PT$^2$-LLM的后训练三值化框架，用于压缩大语言模型，在降低内存成本的同时保持竞争性性能，并实现端到端加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因内存和计算需求高而难以部署，现有三值化方法在后训练量化中受限于训练自由参数优化困难及权重异常值和分散问题。

Method: 提出Asymmetric Ternary Quantizer与两阶段精炼流程（ITF和AGA），并引入基于结构相似性的重排序策略SSR以缓解量化难度和异常值影响。

Result: 实验表明PT$^2$-LLM在2比特PTQ方法中具有竞争力，内存更低，且在prefill和解码阶段均实现加速。

Conclusion: PT$^2$-LLM有效解决了LLM后训练三值化的关键挑战，兼顾高效压缩、低内存开销与推理加速，适合实际部署。

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
diverse tasks, but their large memory and compute demands hinder deployment.
Ternarization has gained attention as a promising compression technique,
delivering substantial size reduction and high computational efficiency.
However, its potential in the post-training quantization (PTQ) setting remains
underexplored, due to the challenge of training-free parameter optimization and
the quantization difficulty posed by outliers and dispersed weights. To address
these issues, we propose PT$^2$-LLM, a post-training ternarization framework
tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with
a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which
alternates between optimal ternary grid construction and flexible rounding to
minimize quantization error, and (2) Activation-aware Grid Alignment (AGA),
which further refines the ternary grid to better match full-precision outputs.
In addition, we propose a plug-and-play Structural Similarity-based Reordering
(SSR) strategy that leverages inter-column structural similarity to ease
quantization and mitigate outlier effects, further enhancing overall
performance. Extensive experiments demonstrate that PT$^2$-LLM delivers
competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with
lower memory cost, while also accelerating both prefill and decoding to achieve
end-to-end speedup. The code and models will be available at
https://github.com/XIANGLONGYAN/PT2-LLM.

</details>


### [372] [Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment](https://arxiv.org/abs/2510.03268)
*Lingjie Yi,Raphael Douady,Chao Chen*

Main category: cs.LG

TL;DR: 本文提出了首个用于分析多模态对比学习中模态间隙的理论框架，证明了维度坍缩是模态间隙的根本原因，并提出通过超平面旋转或共享空间投影实现模态对齐。


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习中存在模态间隙现象，且其成因和对下游任务的影响尚不明确，本文旨在从理论上解释这一现象并揭示其影响机制。

Method: 建立理论框架，分析最优表示的收敛性，在不同约束条件下（无约束、锥约束、子空间约束）推导模态间隙的收敛行为。

Result: 证明在无约束或锥约束下模态间隙趋于零；在子空间约束下，模态间隙收敛于两个超平面间的最小夹角；指出维度坍缩是模态间隙的根本原因，且配对样本无法完美对齐，但可通过超平面旋转或共享空间投影实现完美对齐。

Conclusion: 维度坍缩是导致模态间隙的根本原因，模态间隙影响样本对齐进而影响下游性能，但可通过特定方法恢复完美对齐。

Abstract: Multimodal contrastive learning (MCL) aims to embed data from different
modalities in a shared embedding space. However, empirical evidence shows that
representations from different modalities occupy completely separate regions of
embedding space, a phenomenon referred to as the modality gap. Moreover,
experimental findings on how the size of the modality gap influences downstream
performance are inconsistent. These observations raise two key questions: (1)
What causes the modality gap? (2) How does it affect downstream tasks? To
address these questions, this paper introduces the first theoretical framework
for analyzing the convergent optimal representations of MCL and the modality
alignment when training is optimized. Specifically, we prove that without any
constraint or under the cone constraint, the modality gap converges to zero.
Under the subspace constraint (i.e., representations of two modalities fall
into two distinct hyperplanes due to dimension collapse), the modality gap
converges to the smallest angle between the two hyperplanes. This result
identifies \emph{dimension collapse} as the fundamental origin of the modality
gap. Furthermore, our theorems demonstrate that paired samples cannot be
perfectly aligned under the subspace constraint. The modality gap influences
downstream performance by affecting the alignment between sample pairs. We
prove that, in this case, perfect alignment between two modalities can still be
achieved via two ways: hyperplane rotation and shared space projection.

</details>


### [373] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: 提出了一种新的探索性奖励框架GEB，能够有效解决现有方法在强化学习中因KL或α-散度正则化导致的探索偏向问题，实验证明其在多个设置下均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有探索性奖励方法在KL或α-散度正则化下会无意中偏向高概率区域，抑制了对不确定区域的有效探索，违背了乐观探索原则。

Method: 提出了通用探索性奖励（GEB）框架，通过参考依赖的奖励调节来抵消散度引起的偏差，并在理论上证明其满足乐观性原则，统一了多种已有启发式方法并可扩展到整个α-散度族。

Result: 实验表明，GEB在多种对齐任务和大语言模型主干上均一致优于基线方法，无论在哪种散度设置下都表现出更强的探索能力。

Conclusion: GEB为基于人类反馈的强化学习中的乐观探索提供了既具理论保证又实用的解决方案。

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [374] [CoDA: Coding LM via Diffusion Adaptation](https://arxiv.org/abs/2510.03270)
*Haolin Chen,Shiyu Wang,Can Qin,Bo Pang,Zuxin Liu,Jielin Qiu,Jianguo Zhang,Yingbo Zhou,Zeyuan Chen,Ran Xu,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.LG

TL;DR: CoDA是一个1.7B参数的开源扩散编码模型，通过大规模预训练和代码中心化训练，在代码生成任务上性能媲美甚至超越更大的扩散模型。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具备双向上下文和填充能力，但现有系统仍过于笨重，缺乏高效轻量的解决方案。

Method: 提出CoDA，采用TPU训练，结合大规模扩散预训练、代码为中心的中阶段训练和指令微调，并利用置信度引导采样以保持较低推理延迟。

Result: 在Humaneval、MBPP和EvalPlus基准上，CoDA-1.7B-Instruct表现优于或媲美高达7B参数的扩散模型。

Conclusion: CoDA证明了小型扩散模型在代码生成任务上的高效性和竞争力，其开源训练流程有助于推动轻量级扩散编码助手的研究。

Abstract: Diffusion language models promise bidirectional context and infilling
capabilities that autoregressive coders lack, yet practical systems remain
heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU
with a fully open-source training pipeline. CoDA pairs large-scale diffusion
pre-training with code-centric mid-training and instruction tuning, enabling
confidence-guided sampling that keeps inference latency competitive. On
Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses
diffusion models up to 7B parameters. Our release includes model checkpoints,
evaluation harnesses, and TPU training pipelines to accelerate research on
lightweight diffusion-based coding assistants.

</details>


### [375] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: 本文提出了决策势能面（DPS）这一新概念，用于分析大语言模型（LLM）的决策边界，并提出K-DPS算法，首次实现了对LLM决策边界的近似构建，仅需有限次采样且误差可忽略。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型的词汇序列规模巨大且具有自回归特性，直接构建其决策边界在计算上不可行，因此需要一种新的方法来有效分析LLM的决策行为。

Method: 提出决策势能面（DPS）概念，定义在不同采样序列置信度之上，并证明其零等高线等价于LLM的决策边界；基于此提出K-DPS算法，通过K次有限采样逼近决策边界，并理论推导了误差上界。

Result: 成功实现了LLM决策边界的近似构造，实验验证了K-DPS在多个LLM和语料库上的有效性，误差可控且与采样次数可权衡。

Conclusion: DPS为分析LLM决策边界提供了有效框架，K-DPS算法使得在实际应用中研究LLM决策区域成为可能，具有理论与实践意义。

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [376] [PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling](https://arxiv.org/abs/2510.03272)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种将Transformer架构重新理解为由偏微分方程（PDE）控制的连续时空动力系统的理论框架，揭示了残差连接和层归一化是维持系统稳定的关键数学机制。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在人工智能中取得了巨大成功，但其内部机制缺乏原理性的理论解释。本文旨在从第一性原理出发，建立对其核心组件必要性的数学理解。

Method: 将Transformer的离散层结构映射为一个连续的PDE动力系统，将自注意力、前馈网络、残差连接和层归一化分别对应为非局部相互作用、局部反应以及稳定化算子，并通过对比实验验证这些组件的作用。

Result: 实验证明，缺少残差连接会导致表征崩溃，缺少层归一化则引发训练过程中的爆炸性动态；二者是防止系统不稳定的必要条件。

Conclusion: 残差连接和层归一化并非经验性技巧，而是保证Transformer作为连续动力系统稳定运行的数学基础，为深度神经网络分析提供了基于连续动力学的新范式。

Abstract: The Transformer architecture has revolutionized artificial intelligence, yet
a principled theoretical understanding of its internal mechanisms remains
elusive. This paper introduces a novel analytical framework that
reconceptualizes the Transformer's discrete, layered structure as a continuous
spatiotemporal dynamical system governed by a master Partial Differential
Equation (PDE). Within this paradigm, we map core architectural components to
distinct mathematical operators: self-attention as a non-local interaction, the
feed-forward network as a local reaction, and, critically, residual connections
and layer normalization as indispensable stabilization mechanisms. We do not
propose a new model, but rather employ the PDE system as a theoretical probe to
analyze the mathematical necessity of these components. By comparing a standard
Transformer with a PDE simulator that lacks explicit stabilizers, our
experiments provide compelling empirical evidence for our central thesis. We
demonstrate that without residual connections, the system suffers from
catastrophic representational drift, while the absence of layer normalization
leads to unstable, explosive training dynamics. Our findings reveal that these
seemingly heuristic "tricks" are, in fact, fundamental mathematical stabilizers
required to tame an otherwise powerful but inherently unstable continuous
system. This work offers a first-principles explanation for the Transformer's
design and establishes a new paradigm for analyzing deep neural networks
through the lens of continuous dynamics.

</details>


### [377] [Learning without Global Backpropagation via Synergistic Information Distillation](https://arxiv.org/abs/2510.03273)
*Chenhao Ye,Ming Tang*

Main category: cs.LG

TL;DR: 提出了一种名为Synergistic Information Distillation (SID)的新训练框架，用于解决反向传播中的更新锁定和高内存消耗问题，通过局部协同优化实现并行训练，同时保持前向推理不变，理论和实验均证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 反向传播存在更新锁定和高内存消耗两大瓶颈，限制了深度学习的可扩展性。

Method: 将深度网络构建为模块化流水线，每个模块通过局部目标优化对真实标签的概率信念，该目标平衡了与标签的保真度和前一模块信念的一致性，从而解耦模块间的反向依赖，实现并行训练。

Result: SID实现了与BP相当甚至更优的分类精度，显著降低内存使用，支持并行训练，表现出更强的标签噪声鲁棒性和可扩展性。

Conclusion: SID是一种有效的BP替代方案，能够在不改变前向推理的情况下消除更新锁定、减少内存开销，并保证随网络深度增加性能单调提升。

Abstract: Backpropagation (BP), while foundational to deep learning, imposes two
critical scalability bottlenecks: update locking, where network modules remain
idle until the entire backward pass completes, and high memory consumption due
to storing activations for gradient computation. To address these limitations,
we introduce Synergistic Information Distillation (SID), a novel training
framework that reframes deep learning as a cascade of local cooperative
refinement problems. In SID, a deep network is structured as a pipeline of
modules, each imposed with a local objective to refine a probabilistic belief
about the ground-truth target. This objective balances fidelity to the target
with consistency to the belief from its preceding module. By decoupling the
backward dependencies between modules, SID enables parallel training and hence
eliminates update locking and drastically reduces memory requirements.
Meanwhile, this design preserves the standard feed-forward inference pass,
making SID a versatile drop-in replacement for BP. We provide a theoretical
foundation, proving that SID guarantees monotonic performance improvement with
network depth. Empirically, SID consistently matches or surpasses the
classification accuracy of BP, exhibiting superior scalability and pronounced
robustness to label noise.Code is available at:
https://github.com/ychAlbert/sid-bp

</details>


### [378] [Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)
*Tianao Zhang,Zhiteng Li,Xianglong Yan,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.LG

TL;DR: 提出了一种针对扩散大语言模型（dLLMs）的超低比特后训练量化框架Quant-dLLM，包含掩码校准模拟、数据感知量化器和自适应分块混合精度策略，在2比特下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练量化方法在直接应用于dLLMs时性能不佳，尤其是在2比特量化下，因此需要专为dLLMs设计的量化方案。

Method: 提出了三个关键技术：掩码校准模拟（MCS）以匹配dLLMs的时间步依赖掩码机制；数据感知任意阶量化器（DAQ）通过优化算法学习超低比特权重表示；自适应分块混合精度（ABMP）在2比特预算下动态分配不同通道组的比特宽度。

Result: 在严格2比特限制下，Quant-dLLM在多个dLLMs上均优于当前最先进的AR模型迁移PTQ方法，实现了更高的精度。

Conclusion: Quant-dLLM是首个专为dLLMs设计的超低比特PTQ框架，有效解决了标准PTQ在dLLMs上的性能退化问题，推动了其在资源受限场景下的部署应用。

Abstract: Diffusion large language models (dLLMs), which offer bidirectional context
and flexible masked-denoising generation, are emerging as a compelling
alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model
sizes continue to grow, motivating weight compression for deployment. Although
post-training quantization (PTQ) is effective for AR LLMs, directly
transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To
tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework
tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the
fully visible signals assumed by standard PTQ methods, we introduce Masked
Calibration Simulation (MCS) to align calibration with the timestep-dependent
masking, which yields more reliable calibrations. Moreover, we propose a
Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight
representations via an optimization algorithm. It performs iterative
approximation guided by our simulated calibration data. In addition, under a
strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a
sensitivity-based precision allocation scheme that adaptively assigns bit width
across channel groups. When restricted to 2-bit precision, Quant-dLLM
consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer
PTQ methods on dLLMs. The code and models will be available at:
https://github.com/ZTA2785/Quant-dLLM.

</details>


### [379] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为SDQ-LLM的新型1比特量化框架，通过Sigma-Delta量化与可调过采样率（OSR）实现大语言模型的高效部署，结合Hadamard权重平滑和细粒度MultiOSR分配策略，在极低比特下保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在计算和内存方面面临巨大挑战，亟需极低比特量化技术以实现高效部署。

Method: 采用上采样结合Sigma-Delta量化器对模型权重进行二值化或三值化，引入Hadamard权重平滑，并提出基于权重方差和参数规模的层间与线性层内细粒度MultiOSR分配策略。

Result: 在OPT和LLaMA系列模型上的实验表明，SDQ-LLM在低OSR设置下仍能实现高效且高精度的推理性能。

Conclusion: SDQ-LLM通过可调节的OSR和精细的资源分配策略，显著提升了极低比特量化下大语言模型的效率与准确性。

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [380] [QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks](https://arxiv.org/abs/2510.03276)
*Qian Chen,Linxin Yang,Akang Wang,Xiaodong Luo,Yin Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种轻量化的二次增强模块，通过引入低秩、权重共享和稀疏化技术，在几乎不增加参数和计算开销的情况下，在神经网络各层中引入二次特征交互，显著提升了图像分类、文本分类和大语言模型微调等任务的性能。


<details>
  <summary>Details</summary>
Motivation: 为了增强神经网络的非线性表达能力，现有模型主要依赖线性变换与非线性激活函数的组合。本文旨在探索引入二次变换以进一步提升模型性能，同时控制参数和计算复杂度。

Method: 提出一种轻量级的二次增强模块，利用低秩分解、权重共享和稀疏化技术实现高效的二次特征交互，并将其嵌入到现有网络结构的每一层中。

Result: 在图像分类、文本分类和大语言模型微调三个任务上的实验表明，所提方法在几乎不增加模型参数和前向计算开销的前提下，带来了显著且一致的性能提升。

Conclusion: 引入轻量化的二次变换是一种有效且高效的增强神经网络表示能力的方法，具有广泛的应用潜力。

Abstract: The combination of linear transformations and non-linear activation functions
forms the foundation of most modern deep neural networks, enabling them to
approximate highly complex functions. This paper explores the introduction of
quadratic transformations to further increase nonlinearity in neural networks,
with the aim of enhancing the performance of existing architectures. To reduce
parameter complexity and computational complexity, we propose a lightweight
quadratic enhancer that uses low-rankness, weight sharing, and sparsification
techniques. For a fixed architecture, the proposed approach introduces
quadratic interactions between features at every layer, while only adding
negligible amounts of additional model parameters and forward computations. We
conduct a set of proof-of-concept experiments for the proposed method across
three tasks: image classification, text classification, and fine-tuning
large-language models. In all tasks, the proposed approach demonstrates clear
and substantial performance gains.

</details>


### [381] [Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition](https://arxiv.org/abs/2510.03278)
*Filip Landgren*

Main category: cs.LG

TL;DR: 提出了一种可扩展的、无矩阵的拉普拉斯框架，用于分解贝叶斯物理信息神经网络中每个约束对后验海森矩阵的贡献，并量化其相对影响。


<details>
  <summary>Details</summary>
Motivation: 需要进一步明确单个物理约束如何塑造贝叶斯物理信息神经网络中的不确定性表现。

Method: 引入一种可扩展的、无矩阵的拉普拉斯框架，将后验海森矩阵分解为各约束的贡献，并通过海森矩阵分析损失景观的变化。

Result: 在范德波尔方程上的应用表明，该方法能追踪约束如何塑造网络几何结构，并显示改变单个损失权重会非平凡地重新分配其他约束的曲率和有效主导性。

Conclusion: 所提框架有助于理解物理约束对贝叶斯物理信息神经网络中不确定性建模的影响，揭示了损失权重调整对网络内在几何结构的复杂作用。

Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing
equations to solve differential equations under uncertainty. However,
interpreting uncertainty and overconfidence in B-PINNs requires care due to the
poorly understood effects the physical constraints have on the network;
overconfidence could reflect warranted precision, enforced by the constraints,
rather than miscalibration. Motivated by the need to further clarify how
individual physical constraints shape these networks, we introduce a scalable,
matrix-free Laplace framework that decomposes the posterior Hessian into
contributions from each constraint and provides metrics to quantify their
relative influence on the loss landscape. Applied to the Van der Pol equation,
our method tracks how constraints sculpt the network's geometry and shows,
directly through the Hessian, how changing a single loss weight non-trivially
redistributes curvature and effective dominance across the others.

</details>


### [382] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: 本文提出MemMamba，一种结合状态摘要和跨层跨token注意力机制的新架构，在保持线性复杂度的同时显著缓解了Mamba的长程遗忘问题，提升了长序列建模性能。


<details>
  <summary>Details</summary>
Motivation: 现有长序列建模方法在效率与内存之间存在权衡：RNN有梯度问题，Transformer有二次复杂度，而Mamba虽高效但其长程记忆呈指数衰减。需要理解并改进Mamba的记忆机制。

Method: 通过数学推导和信息论分析揭示Mamba的记忆衰减机制，提出水平-垂直记忆保真度指标量化信息损失，并设计MemMamba架构，引入状态摘要机制与跨层跨token注意力以增强长期信息保留。

Result: MemMamba在PG19和Passkey Retrieval等长序列任务上优于现有Mamba变体和Transformer，推理效率提升48%，且理论与实验验证其在复杂度-内存权衡上的突破。

Conclusion: MemMamba通过模拟人类提炼关键信息的方式，实现了高效且低遗忘的长序列建模，为超长序列处理提供了新范式。

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [383] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Quokka提出了首个针对扩散语言模型（DLMs）的系统性扩展定律，涵盖计算和数据受限情况，并探讨关键建模与优化设计，旨在为DLM训练提供短期实践指导和长期启发。


<details>
  <summary>Details</summary>
Motivation: 为了填补扩散语言模型在不同资源约束下的系统性扩展规律研究空白，并为模型训练提供有效指导。

Method: 通过系统性实验分析计算和数据受限情况下DLM的扩展行为，研究关键建模和优化设计的影响。

Result: 建立了首个适用于扩散语言模型的扩展定律（Quokka），覆盖多种训练场景，并提供了比Chinchilla更广泛的适用范围。

Conclusion: Quokka为扩散语言模型的训练提供了实用的扩展指南，并对AI领域的发展具有长期启示意义。

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [384] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: 提出了一种混合归因与剪枝（HAP）框架，用于高效且保真地发现语言模型中的电路，相比基线方法更快且保留了关键协作组件。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现算法在速度与保真度之间存在权衡：归因修补快但不保真，边剪枝保真但计算昂贵。

Method: 先用归因修补识别高潜力子图，再在此子图上应用边剪枝提取保真的电路。

Result: HAP比基线算法快46%，且在间接对象识别任务中保留了归因修补方法容易误剪的协作性电路组件（如S抑制头）。

Conclusion: HAP是一种有效的机制可解释性研究扩展方案，有助于提升大模型电路分析的可扩展性。

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [385] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: 本文提出了MACE，一种在边缘服务器上通过迭代级混合调度协同执行推理与微调的LLM系统，有效平衡了延迟、吞吐量和模型更新频率，在保证服务级别目标的同时显著降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 用户数据的非平稳性导致需要频繁重训练大模型，但现有方法难以在有限GPU资源下兼顾推理延迟与模型准确性。

Method: 提出MACE系统，采用迭代级调度策略，结合智能内存管理，在同一GPU上共置推理（prefill、decode）与微调任务，并根据模型更新对输出的影响动态分配计算资源。

Result: 实验表明，MACE在保持吞吐量的同时，相比连续重训练最多降低63%的推理延迟，GPU利用率维持在85%以上，且优于周期性重训练。

Conclusion: 迭代级的混合调度是实现边缘平台持续学习型大模型部署的有效方向。

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [386] [Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments](https://arxiv.org/abs/2510.03284)
*Vinay Venkatesh,Vamsidhar R Kamanuru,Lav Kumar,Nikita Kothari*

Main category: cs.LG

TL;DR: 本文提出了Edge-FIT，一个用于边缘设备上大规模语言模型（LLM）的联邦指令调优框架。该框架结合了4位量化低秩适应（QLORA），有效缓解了通信和计算开销问题，并在物联网领域实现了高效、可扩展的去中心化部署。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法（如FedAvg）在处理大规模语言模型的巨大参数量时表现不佳，难以应对通信和计算开销，限制了其在边缘设备上的应用。因此，需要一种更高效的联邦调优框架来支持LLM在边缘侧的部署。

Method: 提出Edge-FIT框架，将联邦学习与4位量化低秩适应（QLORA）相结合，并对Databricks Dolly 15k数据集进行物联网领域过滤，以实现高效的指令微调。

Result: 实验结果显示，使用Edge-FIT调优的Llama 2(7B)模型达到了0.89的F1分数；同时验证了3.8B的Phi-3-mini模型也可行，表明该框架具有良好的可扩展性。

Conclusion: Edge-FIT通过结合QLORA技术，显著降低了联邦学习中的通信与计算成本，是适用于家庭计算网关等资源受限环境下的可扩展去中心化LLM部署方案。

Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.

</details>


### [387] [LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain](https://arxiv.org/abs/2510.03288)
*Chiming Duan,Minghua He,Pei Xiao,Tong Jia,Xin Zhang,Zhewei Zhong,Xiang Luo,Yan Niu,Lingzhe Zhang,Yifan Wu,Siyu Yu,Weijie Hong,Ying Li,Gang Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于主动域自适应的日志异常检测模型LogAction，结合迁移学习与主动学习，在仅使用2%人工标注标签的情况下，在六个数据集组合上平均F1分数达到93.01%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法依赖大量标注数据，而标注成本高且存在源域与目标域数据分布差异和冷启动问题，限制了迁移学习和主动学习的效果。

Method: 提出LogAction模型，利用成熟系统的标注数据进行迁移学习以缓解冷启动问题，并采用基于自由能和不确定性的采样策略选择分布边界日志进行人工标注，缩小域间分布差距。

Result: 在六组不同数据集上实验表明，LogAction仅用2%人工标签即达到平均93.01%的F1分数，较现有最优方法提升26.28%。

Conclusion: LogAction通过融合主动学习与域自适应，显著降低标注成本并提升跨系统日志异常检测性能，具有较强的实用性和推广价值。

Abstract: Log-based anomaly detection is a essential task for ensuring the reliability
and performance of software systems. However, the performance of existing
anomaly detection methods heavily relies on labeling, while labeling a large
volume of logs is highly challenging. To address this issue, many approaches
based on transfer learning and active learning have been proposed.
Nevertheless, their effectiveness is hindered by issues such as the gap between
source and target system data distributions and cold-start problems. In this
paper, we propose LogAction, a novel log-based anomaly detection model based on
active domain adaptation. LogAction integrates transfer learning and active
learning techniques. On one hand, it uses labeled data from a mature system to
train a base model, mitigating the cold-start issue in active learning. On the
other hand, LogAction utilize free energy-based sampling and uncertainty-based
sampling to select logs located at the distribution boundaries for manual
labeling, thus addresses the data distribution gap in transfer learning with
minimal human labeling efforts. Experimental results on six different
combinations of datasets demonstrate that LogAction achieves an average 93.01%
F1 score with only 2% of manual labels, outperforming some state-of-the-art
methods by 26.28%. Website: https://logaction.github.io

</details>


### [388] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: 本文探讨了掩码扩散语言模型在实现并行生成和双向注意力方面的固有困难，并提出了最有效的训练和推理策略。


<details>
  <summary>Details</summary>
Motivation: 揭示掩码扩散模型未能充分发挥扩散模型优势的原因，特别是在并行生成和双向注意力方面。

Method: 分析吸收扩散变体的局限性，并提出改进的训练与推理策略。

Result: 指出了当前开源掩码扩散模型在并行化和注意力机制上的瓶颈，并提供了优化策略。

Conclusion: 尽管掩码扩散模型具有潜力，但其现有形式难以实现真正的并行生成和双向注意力，需采用新策略加以改进。

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [389] [Single-Core Superscalar Optimization of Clifford Neural Layers](https://arxiv.org/abs/2510.03290)
*X. Angelo Huang,Ruben Ciranni,Giovanni Spadaccini,Carla J. López Zurita*

Main category: cs.LG

TL;DR: 本文分析了Clifford卷积层的内部计算结构，基于Clifford代数理论提出并实现了多种优化方法，显著提升了推理速度，平均加速达21.35倍，且在多数情况下优于原始PyTorch实现。


<details>
  <summary>Details</summary>
Motivation: 为了提高具有E(n)和O(n)等变性的Clifford神经网络层的推理效率，需减少冗余计算和内存开销。

Method: 通过分析Clifford代数的理论基础，消除冗余矩阵分配与计算，并系统应用成熟的优化技术来提升性能。

Result: 在11个函数上平均加速21.35倍，6种情况下的运行时间优于或接近原始PyTorch实现，其余情况性能处于同一数量级。

Conclusion: 所提出的优化方法在保持正确性的同时显著提升了Clifford卷积层的推理效率，推动了等变神经网络在物理科学中的实际应用。

Abstract: Within the growing interest in the physical sciences in developing networks
with equivariance properties, Clifford neural layers shine as one approach that
delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this
paper, we analyze the inner structure of the computation within Clifford
convolutional layers and propose and implement several optimizations to speed
up the inference process while maintaining correctness. In particular, we begin
by analyzing the theoretical foundations of Clifford algebras to eliminate
redundant matrix allocations and computations, then systematically apply
established optimization techniques to enhance performance further. We report a
final average speedup of 21.35x over the baseline implementation of eleven
functions and runtimes comparable to and faster than the original PyTorch
implementation in six cases. In the remaining cases, we achieve performance in
the same order of magnitude as the original library.

</details>


### [390] [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://arxiv.org/abs/2510.03291)
*Yizhuo Ding,Wanying Qu,Jiawei Geng,Wenqi Shao,Yanwei Fu*

Main category: cs.LG

TL;DR: UniPruning是一种无需权重更新的统一后训练剪枝框架，结合了局部显著性度量的速度和全局协调的稳定性，支持非结构化和半结构化剪枝，能高效生成任意稀疏度的剪枝掩码，并适应硬件感知约束。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法在效率与鲁棒性之间难以平衡：局部方法在高稀疏度下易崩溃，而全局反馈方法因权重更新昂贵或受限于半结构化格式而效率低下。

Method: 提出UniPruning，基于镜像下降优化，利用快速逐层打分和轻量级全局控制器分配单一稀疏预算，在不更新模型权重的情况下实现统一剪枝框架。

Result: 在多个预训练大语言模型和标准基准上实验表明，UniPruning在困惑度和零样本准确率方面表现优异，且支持一次性生成任意稀疏度的掩码，具备硬件感知能力。

Conclusion: UniPruning提供了一种高效、有原则且可扩展的大规模LLM稀疏化解决方案。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks
but face prohibitive computational and memory costs. Pruning offers a promising
path by inducing sparsity while preserving architectural flexibility. However,
existing methods struggle to balance efficiency and robustness: local metric
approaches prune layer by layer but often collapse under high sparsity, whereas
global feedback methods enforce consistency at the cost of expensive weight
updates or restrictive semi-structured formats. We present UniPruning, a
unified post-training pruning framework that combines the speed of local
saliency metrics with the stability of global coordination, enabled by a mirror
descent based optimization, all without updating model weights. UniPruning
leverages fast layer-wise scoring and a lightweight global controller to
allocate a single sparsity budget, supporting both unstructured and
semi-structured N :M pruning within one framework. After a brief calibration,
it can generate pruning masks for arbitrary sparsity levels in one shot, and
adapts seamlessly to hardware-aware constraints. Extensive experiments on
multiple pretrained LLM families and standard benchmarks show that UniPruning
consistently delivers competitive or superior perplexity and zero-shot
accuracy. Ablation studies further highlight the importance of mirror descent
and local saliency anchoring. Overall, UniPruning provides an efficient,
principled, and scalable solution for sparsifying large-scale LLMs. Our code is
available at: https://github.com/RainbowQTT/UniPruning.

</details>


### [391] [From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing](https://arxiv.org/abs/2510.03293)
*Rana Shahout,Colin Cai,Yilun Du,Minlan Yu,Michael Mitzenmacher*

Main category: cs.LG

TL;DR: LASER是一种即插即用的推理时路由算法，用于在保持准确率的同时平衡Mixture-of-Experts（MoE）模型中的负载。


<details>
  <summary>Details</summary>
Motivation: MoE模型在推理过程中由于专家分配不均导致设备负载不平衡，进而影响延迟、吞吐量和成本。

Method: LASER根据门控函数的得分分布动态调整路由策略：当得分差异明显时，路由到最强专家；当得分较均匀时，选择负载最低的可用专家。该方法无需重新训练或微调，仅依赖已有模型的门控得分。

Result: 在Mixtral-8x7B和DeepSeek-MoE-16b-chat上评估显示，LASER显著改善了负载均衡，降低了延迟，提高了吞吐量，且对准确率影响可忽略。

Conclusion: LASER能有效提升MoE模型的推理效率和系统性能，同时兼容现有推理流程，具有良好的实用性与扩展性。

Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each
token to a subset of experts through a learned gate function. While conditional
routing reduces training costs, it shifts the burden on inference memory:
expert parameters and activations consume memory, limiting the number of
experts per device. As tokens are routed, some experts become overloaded while
others are underutilized. Because experts are mapped to GPUs, this imbalance
translates directly into degraded system performance in terms of latency,
throughput, and cost. We present LASER, a plug-and-play, inference-time routing
algorithm that balances load while preserving accuracy. LASER adapts to the
shape of the gate's score distribution. When scores provide a clear preference,
it routes to the strongest experts; when scores are more uniform, it broadens
the set of viable experts and routes to the least-loaded among them. Because
LASER relies only on gate scores from a trained model, it integrates directly
into existing MoE inference pipelines without retraining or finetuning. We
evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets
(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,
translating into lower latency and higher throughput, while keeping the
accuracy changes negligible.

</details>


### [392] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: 提出了一种名为CAFL-L的约束感知联邦学习方法，通过拉格朗日对偶优化动态调整训练超参数，以满足边缘设备的资源限制，同时保持良好的模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦学习方法如FedAvg未能充分考虑边缘设备在能量、通信、内存和热预算方面的资源限制，导致在实际部署中面临挑战。因此，需要一种能够显式纳入这些约束并动态适应训练过程的方法。

Method: CAFL-L基于FedAvg框架，引入拉格朗日对偶优化，在每轮训练中动态调整冻结深度、本地训练步数、批量大小和通信压缩率等超参数。通过梯度累积机制保持token预算不变，从而维持训练稳定性。

Result: 在字符级语言模型上的实验表明，与标准FedAvg相比，CAFL-L显著减少了20%的内存使用和95%的通信开销，同时实现了相当的验证性能，表现出更强的约束满足能力。

Conclusion: CAFL-L是一种实用且有效的联邦学习扩展方法，能够在复杂资源约束下稳定训练，适合部署于资源受限的边缘设备。

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [393] [Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles](https://arxiv.org/abs/2510.03301)
*Arthur Sedek*

Main category: cs.LG

TL;DR: 提出了一种结合XGBoost和神经网络的自适应集成框架，通过元学习动态组合模型，显著提升了预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了提升机器学习系统的智能性和灵活性，需要克服单一模型在不同数据上的局限性，实现更优的泛化能力。

Method: 采用先进的不确定性量化技术和特征重要性整合，通过元学习动态协调XGBoost与神经网络的模型选择与组合。

Result: 实验结果表明，该方法在多个不同数据集上均表现出优于现有方法的预测性能，并增强了模型的可解释性。

Conclusion: 所提出的自适应集成框架有效融合了XGBoost和神经网络的优势，为构建高性能、高灵活性的机器学习系统提供了新思路。

Abstract: This paper introduces a novel adaptive ensemble framework that
synergistically combines XGBoost and neural networks through sophisticated
meta-learning. The proposed method leverages advanced uncertainty
quantification techniques and feature importance integration to dynamically
orchestrate model selection and combination. Experimental results demonstrate
superior predictive performance and enhanced interpretability across diverse
datasets, contributing to the development of more intelligent and flexible
machine learning systems.

</details>


### [394] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 本文揭示了当前文本到图像扩散模型中的概念擦除技术仅制造了“失忆”的假象，实际上并未真正删除概念，而是通过偏转采样轨迹实现表面安全。为此，作者提出RevAm——一种基于强化学习的轨迹优化框架，能在不修改模型权重的情况下动态引导去噪过程，成功恢复被擦除的概念，暴露了现有安全机制的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型架构（如Flux）的发展，传统概念擦除方法（如ESD、UCE、AC）效果下降，其是否真正实现概念删除存在疑问。作者旨在揭示这些方法的本质缺陷，区分表面安全与真实概念移除，推动更鲁棒的擦除技术发展。

Method: 提出RevAm，一种基于强化学习的轨迹优化框架，采用Group Relative Policy Optimization (GRPO) 方法，通过轨迹级奖励动态调整去噪过程，在不修改模型参数的前提下探索多样化的恢复路径，从而实现被擦除概念的复活。

Result: 实验证明，RevAm在概念恢复保真度上优于现有方法，同时计算时间减少10倍，验证了当前擦除方法的可逆性与安全性漏洞。

Conclusion: 当前概念擦除方法本质上是可逆的，仅通过轨迹偏转实现表面安全；RevAm揭示了这一脆弱性，强调需要超越轨迹操纵的更可靠擦除机制。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [395] [Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies](https://arxiv.org/abs/2510.03305)
*Tian Zheng,Subashree Venkatasubramanian,Shuolin Li,Amy Braverman,Xinyi Ke,Zhewen Hou,Peter Jin,Samarth Sanjay Agrawal*

Main category: cs.LG

TL;DR: 本文分析了机器学习在气候建模中的应用案例，重点关注工作流程设计模式，旨在提升科学机器学习的严谨性并促进数据科学与气候建模的跨学科合作。


<details>
  <summary>Details</summary>
Motivation: 应对气候建模中物理一致性、多尺度耦合、数据稀疏性和泛化等挑战，推动机器学习在科学领域的可靠应用。

Method: 通过多个应用案例研究，总结从替代建模、机器学习参数化到物理信息迁移学习等工作流的设计模式。

Result: 提出一个强调物理知识融合、模拟数据驱动和观测整合的工作流框架，支持透明化模型开发与可重复性。

Conclusion: 该框架有助于提高科学机器学习的严谨性，并降低数据科学与气候建模跨学科协作的门槛。

Abstract: Machine learning has been increasingly applied in climate modeling on system
emulation acceleration, data-driven parameter inference, forecasting, and
knowledge discovery, addressing challenges such as physical consistency,
multi-scale coupling, data sparsity, robust generalization, and integration
with scientific workflows. This paper analyzes a series of case studies from
applied machine learning research in climate modeling, with a focus on design
choices and workflow structure. Rather than reviewing technical details, we aim
to synthesize workflow design patterns across diverse projects in ML-enabled
climate modeling: from surrogate modeling, ML parameterization, probabilistic
programming, to simulation-based inference, and physics-informed transfer
learning. We unpack how these workflows are grounded in physical knowledge,
informed by simulation data, and designed to integrate observations. We aim to
offer a framework for ensuring rigor in scientific machine learning through
more transparent model development, critical evaluation, informed adaptation,
and reproducibility, and to contribute to lowering the barrier for
interdisciplinary collaboration at the interface of data science and climate
modeling.

</details>


### [396] [Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval](https://arxiv.org/abs/2510.03309)
*Mallikarjuna Tupakula*

Main category: cs.LG

TL;DR: 提出了一种轻量级的对比桥接方法，通过冻结单模态编码器上的投影头实现化学与文本表示的对齐，无需大规模多模态预训练。


<details>
  <summary>Details</summary>
Motivation: 避免依赖大规模多模态数据和 heavy pretraining，探索更高效的跨模态对齐方法用于药物发现。

Method: 使用ChEMBL中的配对机制，将ECFP4分子指纹与生物医学句子嵌入通过双线性投影进行对比学习，并引入难负样本加权和边界损失以提升同一靶点下药物的区分能力。

Result: 在基于骨架划分的评估中，该方法实现了非平凡的跨模态对齐，并显著优于冻结基线模型，在靶点内区分能力上表现更好。

Conclusion: 轻量级对比桥接是一种计算高效的方法，可有效支持精准医学中的骨架感知药物-文本对齐与靶点特异性检索。

Abstract: Multimodal foundation models hold promise for drug discovery and biomedical
applications, but most existing approaches rely on heavy pretraining or large
scale multimodal corpora. We investigate whether thin contrastive bridges,
lightweight projection heads over frozen unimodal encoders can align chemical
and textual representations without training a full multimodal model. Using
paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with
biomedical sentence embeddings through dual linear projections trained with a
contrastive objective. To better handle drugs sharing the same therapeutic
target, we incorporate hard negative weighting and a margin loss. Evaluation
under scaffold based splits, which require generalization across disjoint
chemical cores, demonstrates that our approach achieves non-trivial cross modal
alignment and substantially improves within target discrimination compared to
frozen baselines. These results suggest that thin bridges offer a compute
efficient alternative to large scale multimodal pretraining, enabling scaffold
aware drug text alignment and target specific retrieval in precision medicine.

</details>


### [397] [Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management](https://arxiv.org/abs/2510.03310)
*Runze Zhang,Xiaowei Zhang,Mingyang Zhao*

Main category: cs.LG

TL;DR: 该论文评估了大语言模型（LLMs）在行为运作管理中复制人类行为的能力，发现LLMs能较好复现假设检验结果并捕捉主要决策偏差，但在响应分布上与人类数据存在差异；通过思维链提示和超参数调优可减轻这种不匹配。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs作为低成本替代工具，在商业、经济和社会科学研究中模拟人类行为的潜力，特别是在运作管理领域的适用性。

Method: 基于9个已发表的行为运作实验，采用假设检验结果复现和Wasserstein距离衡量分布对齐程度两个标准，评估LLMs的表现，并测试思维链提示和超参数调优的影响。

Result: LLMs能够复现大多数假设层面的效应并捕捉关键决策偏差，但其响应分布与人类数据存在显著差异；轻量级干预如思维链和超参数调节能减少不匹配，有时使小型或开源模型优于大型商用模型。

Conclusion: LLMs在行为层面具有一定模拟能力，但分布差异限制其直接替代人类参与者；需结合干预方法优化其模拟效果。

Abstract: LLMs are emerging tools for simulating human behavior in business, economics,
and social science, offering a lower-cost complement to laboratory experiments,
field studies, and surveys. This paper evaluates how well LLMs replicate human
behavior in operations management. Using nine published experiments in
behavioral operations, we assess two criteria: replication of hypothesis-test
outcomes and distributional alignment via Wasserstein distance. LLMs reproduce
most hypothesis-level effects, capturing key decision biases, but their
response distributions diverge from human data, including for strong commercial
models. We also test two lightweight interventions -- chain-of-thought
prompting and hyperparameter tuning -- which reduce misalignment and can
sometimes let smaller or open-source models match or surpass larger systems.

</details>


### [398] [Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining](https://arxiv.org/abs/2510.03313)
*Anirudh Subramanyam,Yuxin Chen,Robert L. Grossman*

Main category: cs.LG

TL;DR: 本文提出了一种考虑数据质量的新型语言模型训练扩展缩放定律，引入无量纲数据质量参数Q，并通过有效样本量和信息论视角进行建模，可在神经机器翻译和自回归建模中预测损失随模型大小、数据量和数据质量的变化。


<details>
  <summary>Details</summary>
Motivation: 传统缩放定律未将数据质量纳入统一框架，缺乏对数据质量影响的系统建模，本文旨在建立一个可量化数据质量影响的普适性缩放定律。

Method: 提出包含数据质量参数Q的扩展Chinchilla框架，基于有效样本量与信息论推导质量感知的缩放律，并设计两种Q的实用估计器：污染率代理和缺陷度量；通过在神经机器翻译和自回归模型中控制噪声注入和覆盖率变化进行合成实验验证。

Result: 实验证明损失可随数据质量预测性地变化，高质量数据能显著降低所需模型规模与计算开销；有效数据量随质量呈次线性衰减，且对中等程度数据污染具有鲁棒性；跨数据集评估验证了该定律的预测能力。

Conclusion: 本文建立了首个显式、可推广的数据质量缩放定律，为大规模预训练中数据整理投入与模型规模之间的权衡提供了定量指导。

Abstract: Scaling laws for language model training traditionally characterize how
performance scales with model size and dataset volume. Prior work has explored
architecture variants and data treatments such as dataset filtering and noise
injection in language model pretraining; however, these studies have not
formalized data quality within a principled scaling law. We introduce a
dimensionless data-quality parameter Q, and propose a quality-aware scaling law
extending the Chinchilla framework to predict loss as a joint function of model
size, data volume, and data quality. The law is motivated by an
effective-sample-size and information-theoretic view of noisy or redundant
corpora, and it admits two practical estimators for Q: (i) a corruption rate
proxy and (ii) a deficiency measure. Through synthetic experiments in neural
machine translation and autoregressive modeling -- where we systematically
control data quality via multiple levels of noise injection and coverage
variation -- we show that loss scales predictably with data quality and that
higher-quality data can substantially reduce model size and hence compute
requirements. Our results demonstrate a sublinear decay of effective data with
quality and robustness to moderate data corruption; out-of-sample evaluations
further validate the predictive form of the law. Unlike prior empirical
analyses, our work establishes an explicit, generalizable law for data quality,
offering concrete guidance for balancing data curation effort and model scale
in large-scale pretraining.

</details>


### [399] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的方法，可在约10毫秒内从环形激光陀螺仪信号中快速重建数百赫兹的频率，并实现高精度物理扰动分类。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要数秒数据才能完成频率重建，难以满足实时性要求，特别是在环形激光陀螺仪等应用中需要快速触发和高精度频率估计。

Method: 采用神经网络进行快速频率重建，并结合自动化分类框架识别信号中的物理干扰，如激光不稳定性和地震事件。

Result: 在约10毫秒内实现数百赫兹频率的重建，频率估计精度比傅里叶方法提高2倍；对地震信号的分类准确率达到99%至100%。

Conclusion: 该方法显著提升了信号处理速度与精度，推动了人工智能在地球物理信号分析中的应用。

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [400] [Constant in an Ever-Changing World](https://arxiv.org/abs/2510.03330)
*Andy Wu,Chun-Cheng Lin,Yuehua Huang,Rung-Tzuo Liaw*

Main category: cs.LG

TL;DR: 本文提出了一种名为CIC的框架，通过维护代表性和当前策略来增强强化学习算法的稳定性，仅在当前策略表现更优时更新代表性策略，并采用自适应调整机制联合促进评论家训练，在不增加计算成本的情况下提升了传统算法的性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习的训练过程常因严重振荡而导致不稳定和性能下降，因此需要一种能够提高算法稳定性的方法。

Method: CIC框架维护两个策略：代表性和当前策略，选择性地仅当当前策略优于前者时才更新代表性策略，并使用自适应调整机制让两者共同辅助评论家训练。

Result: 在五个MuJoCo环境中评估表明，CIC能够在不增加计算成本的情况下提升传统算法的性能。

Conclusion: CIC框架有效增强了强化学习算法的稳定性与性能，具有实际应用价值。

Abstract: The training process of reinforcement learning often suffers from severe
oscillations, leading to instability and degraded performance. In this paper,
we propose a Constant in an Ever-Changing World (CIC) framework that enhances
algorithmic stability to improve performance. CIC maintains both a
representative policy and a current policy. Instead of updating the
representative policy blindly, CIC selectively updates it only when the current
policy demonstrates superiority. Furthermore, CIC employs an adaptive
adjustment mechanism, enabling the representative and current policies to
jointly facilitate critic training. We evaluate CIC on five MuJoCo
environments, and the results show that CIC improves the performance of
conventional algorithms without incurring additional computational cost.

</details>


### [401] [Semantic-Aware Scheduling for GPU Clusters with Large Language Models](https://arxiv.org/abs/2510.03334)
*Zerui Wang,Qinghao Hu,Ana Klimovic,Tianwei Zhang,Yonggang Wen,Peng Sun,Dahua Lin*

Main category: cs.LG

TL;DR: SchedMate是一个通过从源代码、运行日志和历史任务等非结构化数据中提取语义信息来增强深度学习调度器性能的框架，利用LLM实现对任务上下文的理解，显著减少了作业完成时间。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习调度器缺乏对任务语义上下文的理解，仅依赖有限的元数据，导致资源分配效率低下、任务时长预测不准、故障处理能力弱等问题。

Method: SchedMate通过三个基于大语言模型（LLM）的组件，非侵入式地从源代码、运行日志和历史作业中提取深层语义信息，并与现有调度器无缝集成，提升调度决策质量。

Result: 在128-GPU物理集群和生产 traces 的仿真中，SchedMate 最多可将平均作业完成时间减少1.91倍，显著提升了调度性能。

Conclusion: 语义感知在现代深度学习调度中至关重要，SchedMate展示了利用LLM挖掘非结构化数据以弥补当前调度器语义盲区的有效性和潜力。

Abstract: Deep learning (DL) schedulers are pivotal in optimizing resource allocation
in GPU clusters, but operate with a critical limitation: they are largely blind
to the semantic context of the jobs they manage. This forces them to rely on
limited metadata, leading to high profiling overhead, unreliable duration
estimation, inadequate failure handling, and poor observability. To this end,
we propose SchedMate, a framework that bridges this semantic gap by
systematically extracting deep insights from overlooked, unstructured data
sources: source code, runtime logs, and historical jobs. SchedMate enhances
existing schedulers non-intrusively through three LLM-based components. Our
implementation integrates seamlessly with existing deep learning schedulers.
Evaluations on a 128-GPU physical cluster and extensive simulations on
production traces show SchedMate reduces average job completion times by up to
1.91x, substantially enhancing the scheduling performance, demonstrating the
critical role of semantic-awareness in modern DL scheduling.

</details>


### [402] [Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment](https://arxiv.org/abs/2510.03335)
*Ameya Daigavane,YuQing Xie,Bodhi P. Vani,Saeed Saremi,Joseph Kleinhenz,Tess Smidt*

Main category: cs.LG

TL;DR: 本文研究了在点云（如分子和蛋白质）扩散模型训练中使用旋转对齐（如Kabsch-Umeyama算法）的有效性，指出该对齐方法本质上是小噪声下最优去噪器的零阶近似，并基于SO(3)上的矩阵Fisher分布提出更优的近似方法。


<details>
  <summary>Details</summary>
Motivation: 在无固定朝向的点云数据上训练扩散模型时，通常通过随机旋转增强和对齐预测结果来保持旋转对称性，但该对齐步骤的影响尚未被深入研究。

Method: 作者将最优去噪器建模为SO(3)上的矩阵Fisher分布，分析对齐操作作为其众数采样的意义，并推导出小噪声极限下的高阶近似去噪方法。

Result: 理论表明对齐是小噪声下的零阶近似，实验验证了在实际训练关注的噪声水平下，当前对齐方法已足够有效。

Conclusion: 对齐虽为近似，但在典型噪声水平下表现良好；新框架为设计更优去噪器提供了理论基础。

Abstract: Diffusion models are a popular class of generative models trained to reverse
a noising process starting from a target data distribution. Training a
diffusion model consists of learning how to denoise noisy samples at different
noise levels. When training diffusion models for point clouds such as molecules
and proteins, there is often no canonical orientation that can be assigned. To
capture this symmetry, the true data samples are often augmented by
transforming them with random rotations sampled uniformly over $SO(3)$. Then,
the denoised predictions are often rotationally aligned via the Kabsch-Umeyama
algorithm to the ground truth samples before computing the loss. However, the
effect of this alignment step has not been well studied. Here, we show that the
optimal denoiser can be expressed in terms of a matrix Fisher distribution over
$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and
turns out to be the zeroth order approximation for small noise levels,
explaining its effectiveness. We build on this perspective to derive better
approximators to the optimal denoiser in the limit of small noise. Our
experiments highlight that alignment is often a `good enough' approximation for
the noise levels that matter most for training diffusion models.

</details>


### [403] [Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models](https://arxiv.org/abs/2510.03339)
*Sofiane Ennadir,Levente Zólyomi,Oleg Smirnov,Tianze Wang,John Pertoft,Filip Cornell,Lele Cao*

Main category: cs.LG

TL;DR: 本文提出了一种理论框架，用于分析Transformer模型中不同池化方法的表达能力，并通过闭式界评估其表示容量和区分相似输入的能力，结合跨模态实验验证了池化策略对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管池化操作在Transformer模型中起着关键作用，但其对模型行为的影响尚未被充分探索，现有研究多集中于注意力机制本身。

Method: 引入一个理论框架，推导出常用池化方法的闭式界限，以量化其表示能力和区分力，并扩展到多种注意力变体；在计算机视觉、自然语言处理和时间序列分析中进行实证评估。

Result: 理论分析表明不同池化方法在表达能力上存在显著差异，实验结果显示池化选择一致地影响模型的准确性、敏感性和优化行为。

Conclusion: 池化是Transformer架构中的关键组件，不应被忽视；本工作为超越注意力机制本身的更合理模型设计提供了理论基础和实践指导。

Abstract: Transformer models have become the dominant backbone for sequence modeling,
leveraging self-attention to produce contextualized token representations.
These are typically aggregated into fixed-size vectors via pooling operations
for downstream tasks. While much of the literature has focused on attention
mechanisms, the role of pooling remains underexplored despite its critical
impact on model behavior. In this paper, we introduce a theoretical framework
that rigorously characterizes the expressivity of Transformer-based models
equipped with widely used pooling methods by deriving closed-form bounds on
their representational capacity and the ability to distinguish similar inputs.
Our analysis extends to different variations of attention formulations,
demonstrating that these bounds hold across diverse architectural variants. We
empirically evaluate pooling strategies across tasks requiring both global and
local contextual understanding, spanning three major modalities: computer
vision, natural language processing, and time-series analysis. Results reveal
consistent trends in how pooling choices affect accuracy, sensitivity, and
optimization behavior. Our findings unify theoretical and empirical
perspectives, providing practical guidance for selecting or designing pooling
mechanisms suited to specific tasks. This work positions pooling as a key
architectural component in Transformer models and lays the foundation for more
principled model design beyond attention alone.

</details>


### [404] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: 提出了一种结合多目标强化学习（MORL）和新型随机微分方程（SDE）流行病模拟器的框架，用于设计在控制疾病传播与维持经济稳定之间取得平衡的干预策略，并在多种传染病上验证了其有效性与通用性。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情凸显了在控制疾病传播的同时维持社会经济稳定的迫切需求，现有模型在精度和多目标权衡方面存在不足。

Method: 构建了一个基于多目标强化学习（MORL）和新提出的随机微分方程（SDE）流行病模拟器的框架，该模拟器利用全球新冠数据进行校准和验证，并训练Pareto-Conditioned Network（PCN）代理来学习不同目标间的权衡策略。

Result: 该模拟器在国家尺度疫情动态建模上比常用RL模型具有更高保真度；PCN代理揭示了流行病控制与经济稳定之间的直接政策权衡；框架在新冠、脊灰、流感和麻疹等不同病原体上表现出良好泛化能力，且能定量显示疫苗接种率下降5%将导致防控成本显著上升。

Conclusion: 所提出的框架为应对公共卫生危机提供了强大、灵活且可解释的决策支持工具，有助于实现透明、基于证据的政策制定。

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


### [405] [Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models](https://arxiv.org/abs/2510.03345)
*Luoma Ke,Guangpeng Zhang,Jibo He,Yajing Li,Yan Li,Xufeng Liu,Peng Fang*

Main category: cs.LG

TL;DR: 本研究结合机器学习与虚拟现实技术，利用眼动和飞行动力学数据，通过SVM与MIC特征选择方法实现飞行员技能的高效识别，准确率达93%，优于现有方法，具有应用于飞行员选拔与训练的潜力。


<details>
  <summary>Details</summary>
Motivation: 在航空业快速发展的背景下，如何低成本、高效地选拔合格飞行员成为一个关键问题。传统方法可能效率较低，因此需要一种基于客观数据的智能化选拔手段。

Method: 研究招募了23名中国东方航空飞行员和23名清华大学社区新手，采用虚拟现实（VR）模拟平台采集其飞行行为数据，并结合机器学习方法（比较了五种分类器和三种特征选择方法），特别是SVM与最大信息系数（MIC）特征选择方法，来区分不同飞行技能水平的个体。

Result: SVM结合MIC特征选择方法在所有评估指标上表现最佳，准确率达到0.93，AUC为0.96，F1分数为0.93，显著优于其他算法组合。MIC方法能够捕捉特征与标签之间的非线性关系，提升了模型性能。该研究首次实现了基于眼动和飞行动力学数据的飞行员技能自动识别。

Conclusion: 提出的VR模拟平台与SVM+MIC算法在飞行员选拔中表现出高精度和优越性能，具备实际应用价值，可为未来的飞行员选拔与训练提供有效的技术工具。

Abstract: With the rapid growth of the aviation industry, there is a need for a large
number of flight crew. How to select the right pilots in a cost-efficient
manner has become an important research question. In the current study,
twenty-three pilots were recruited from China Eastern Airlines, and 23 novices
were from the community of Tsinghua University. A novel approach incorporating
machine learning and virtual reality technology was applied to distinguish
features between these participants with different flight skills. Results
indicate that SVM with the MIC feature selection method consistently achieved
the highest prediction performance on all metrics with an Accuracy of 0.93, an
AUC of 0.96, and an F1 of 0.93, which outperforms four other classifier
algorithms and two other feature selection methods. From the perspective of
feature selection methods, the MIC method can select features with a nonlinear
relationship to sampling labels, instead of a simple filter-out. Our new
implementation of the SVM + MIC algorithm outperforms all existing pilot
selection algorithms and perhaps provides the first implementation based on eye
tracking and flight dynamics data. This study's VR simulation platforms and
algorithms can be used for pilot selection and training.

</details>


### [406] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: AgentCaster是一个无污染的多模态大语言模型框架，用于评估LLMs在龙卷风预报这一复杂现实任务中的推理能力，结果显示当前模型在地理定位、时空推理和风险预测方面表现不佳，易产生幻觉，远逊于人类专家。


<details>
  <summary>Details</summary>
Motivation: 为了评估大语言模型在复杂、高影响的真实世界任务中作为推理代理的实际准备情况，需要新的评估框架。

Method: 提出AgentCaster框架，使用多模态大语言模型端到端处理高分辨率对流允许预报数据，模型每天从数千张预报图和探空数据中交互查询，并生成概率性龙卷风风险多边形预测，通过与真实数据的几何比较进行验证。

Result: 在涵盖500多次龙卷风报告的40天历史数据上测试发现，现有大模型普遍存在过度预测风险强度、地理定位不准、时空推理能力差和严重幻觉问题，性能显著低于气象专家。

Conclusion: AgentCaster为评估和改进大模型在关键领域复杂推理任务中的表现提供了新基准，凸显了当前LLMs在实际科学应用中的局限性。

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [407] [Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks](https://arxiv.org/abs/2510.03351)
*Song Wang,Zhenyu Lei,Zhen Tan,Jundong Li,Javier Rasero,Aiying Zhang,Chirag Agarwal*

Main category: cs.LG

TL;DR: 本文提出了一种名为CONCEPTNEURO的可解释性诊断框架，结合大语言模型与神经生物学知识，自动提取功能性脑连接概念，用于青少年精神疾病的分类，提升了图神经网络的准确性和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的精神疾病预测模型多为黑箱模型，缺乏可解释性，限制了其在临床中的应用。因此，亟需一种既准确又可解释的诊断工具。

Method: 提出CONCEPTNEURO框架，利用大语言模型和神经生物学知识自动识别、筛选并编码功能连接子图（即‘概念’），将这些结构化子图作为可解释特征输入分类器，实现基于有意义脑区连接模式的疾病预测。

Result: 在多个精神疾病数据集上验证表明，结合CONCEPTNEURO的GNN显著优于基线模型，不仅提高了分类准确率，还能提供与临床知识一致的可解释结果，并发现了一些潜在的疾病特异性连接模式。

Conclusion: CONCEPTNEURO是一种可解释、领域知识驱动的脑功能连接分析框架，兼具高性能与透明性，有助于推动图神经网络在精神健康临床诊断中的实际应用。

Abstract: Nearly one in five adolescents currently live with a diagnosed mental or
behavioral health condition, such as anxiety, depression, or conduct disorder,
underscoring the urgency of developing accurate and interpretable diagnostic
tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a
powerful lens into large-scale functional connectivity, where brain regions are
modeled as nodes and inter-regional synchrony as edges, offering clinically
relevant biomarkers for psychiatric disorders. While prior works use graph
neural network (GNN) approaches for disorder prediction, they remain complex
black-boxes, limiting their reliability and clinical translation. In this work,
we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages
large language models (LLMs) and neurobiological domain knowledge to
automatically generate, filter, and encode interpretable functional
connectivity concepts. Each concept is represented as a structured subgraph
linking specific brain regions, which are then passed through a concept
classifier. Our design ensures predictions through clinically meaningful
connectivity patterns, enabling both interpretability and strong predictive
performance. Extensive experiments across multiple psychiatric disorder
datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform
their vanilla counterparts, improving accuracy while providing transparent,
clinically aligned explanations. Furthermore, concept analyses highlight
disorder-specific connectivity patterns that align with expert knowledge and
suggest new hypotheses for future investigation, establishing CONCEPTNEURO as
an interpretable, domain-informed framework for psychiatric disorder diagnosis.

</details>


### [408] [High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)](https://arxiv.org/abs/2510.03355)
*Aryan Patel*

Main category: cs.LG

TL;DR: 提出了一种基于LSTM的迁移学习框架，用于预测铝合金7075-T6的高周扭转S-N曲线，显著降低疲劳性能测试的成本和时间。


<details>
  <summary>Details</summary>
Motivation: 铝合金易发生疲劳失效，传统获取高周疲劳数据耗时且昂贵，亟需一种高效、低成本的方法来表征材料疲劳性能。

Method: 采用长短期记忆网络（LSTM）构建迁移学习框架，先在铝合金7075-T6的纯轴向疲劳数据上训练源模型，再迁移到预测高周扭转S-N曲线。

Result: 该框架能够准确预测铝合金在更高循环次数下的扭转S-N曲线。

Conclusion: 该迁移学习框架可显著减少材料疲劳特性测试的成本和时间，有助于在有限资源下优化实验设计和优先级安排。

Abstract: Aluminum is a widely used alloy, which is susceptible to fatigue failure.
Characterizing fatigue performance for materials is extremely time and cost
demanding, especially for high cycle data. To help mitigate this, a transfer
learning based framework has been developed using Long short-term memory
networks (LSTMs) in which a source LSTM model is trained based on pure axial
fatigue data for Aluminum 7075-T6 alloy which is then transferred to predict
high cycle torsional S-N curves. The framework was able to accurately predict
Al torsional S-N curves for a much higher cycle range. It is the belief that
this framework will help to drastically mitigate the cost of gathering fatigue
characteristics for different materials and help prioritize tests with better
cost and time constraints.

</details>


### [409] [A KL-regularization framework for learning to plan with adaptive priors](https://arxiv.org/abs/2510.04280)
*Álvaro Serra-Gomez,Daniel Jarne Ornia,Dhruva Tirumala,Thomas Moerland*

Main category: cs.LG

TL;DR: 本文提出了PO-MPC框架，统一了基于MPPI的强化学习方法，通过将规划器的动作分布作为先验来优化策略，提升了高维连续控制任务中的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 在高维连续控制任务中，现有MBRL方法因采样策略与规划器行为不一致而导致价值估计不准和长期性能受限，需要更有效的探索机制。

Method: 提出PO-MPC框架，将模型预测路径积分（MPPI）规划器的动作分布作为先验，在策略优化中引入KL正则化，统一并扩展了现有方法，并研究了新的变体形式。

Result: 实验证明，PO-MPC及其新变体显著优于现有MPPI-based RL方法，在多个任务上达到先进性能。

Conclusion: PO-MPC为MPPI与策略优化的结合提供了统一视角，通过策略与规划器对齐提高了学习效率和控制性能，推动了基于模型的强化学习发展。

Abstract: Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.

</details>


### [410] [Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility](https://arxiv.org/abs/2510.03358)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 本文通过研究时间序列数据中Transformer的秩结构，提出了一种基于低秩近似的压缩方法，在不损失精度的情况下显著减少了推理时间和内存占用。


<details>
  <summary>Details</summary>
Motivation: 由于文本和视觉模型中的经验难以直接迁移到时间序列等其他模态，因此需要针对时间序列数据的独特结构特性重新分析Transformer的有效性与可压缩性。

Method: 作者从秩结构的角度分析Transformer，证明了时间序列嵌入具有快速衰减的奇异值谱，使得Q/K/V投影可被低秩近似，并引入“秩流”概念解释网络深度中秩的变化。

Result: 理论和实验表明注意力层具有可压缩性，且早期层更易压缩；基于此方法对Chronos模型实现了65%的推理时间减少和81%的内存降低，且保持准确率不变。

Conclusion: 该研究为时间序列基础模型的宽度、深度和注意力头的分配提供了理论指导，并揭示了其内在的可压缩性机制。

Abstract: Transformers are widely used across data modalities, and yet the principles
distilled from text models often transfer imperfectly to models trained to
other modalities. In this paper, we analyze Transformers through the lens of
rank structure. Our focus is on the time series setting, where the structural
properties of the data differ remarkably from those of text or vision. We show
that time-series embeddings, unlike text or vision, exhibit sharply decaying
singular value spectra: small patch sizes and smooth continuous mappings
concentrate the data into low-rank subspaces. From this, we prove that the
associated $Q/K/V$ projections admit accurate low-rank approximations, and that
attention layers become compressible in proportion to the decay of the
embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by
which nonlinear mixing across depth inflates the rank, explaining why early
layers are most amenable to compression and why ranks grow with depth. Guided
by these theoretical and empirical results, we use these insights to compress
Chronos, a large time series foundation model, achieving a reduction of $65\%$
in inference time and $81\%$ in memory, without loss of accuracy. Our findings
provide principled guidance for allocating width, depth, and heads in time
series foundation models, and for exploiting their inherent compressibility.

</details>


### [411] [Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows](https://arxiv.org/abs/2510.03360)
*Zelin Zhao,Zongyi Li,Kimia Hassibi,Kamyar Azizzadenesheli,Junchi Yan,H. Jane Bae,Di Zhou,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出一种基于物理信息神经算子（PINO）的模型-based深度强化学习框架（PINO-PC），用于高效建模和控制湍流，在高雷诺数下实现39.0%的减阻，性能优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 传统数值模拟湍流控制代价高昂，且现有模型无关的强化学习方法在高雷诺数或未见流场下表现不佳，因此需要更高效且泛化能力强的控制方法。

Method: 采用模型-based强化学习与预测控制（PC）结合，利用物理信息神经算子（PINO）联合学习控制策略和观测模型，PINO具有离散化不变性并能准确捕捉湍流的小尺度特征。

Result: 在雷诺数达15,000的挑战性场景中，PINO-PC实现了39.0%的阻力减少，相比之前流体控制方法提升超过32%，且在未见流场中表现优越。

Conclusion: PINO-PC是一种高效且鲁棒的湍流控制框架，在高雷诺数和未见条件下均显著优于现有方法，展示了模型-based深度强化学习在复杂流体控制中的潜力。

Abstract: Assessing turbulence control effects for wall friction numerically is a
significant challenge since it requires expensive simulations of turbulent
fluid dynamics. We instead propose an efficient deep reinforcement learning
(RL) framework for modeling and control of turbulent flows. It is model-based
RL for predictive control (PC), where both the policy and the observer models
for turbulence control are learned jointly using Physics Informed Neural
Operators (PINO), which are discretization invariant and can capture fine
scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free
reinforcement learning methods in various challenging scenarios where the flows
are of high Reynolds numbers and unseen, i.e., not provided during model
training. We find that PINO-PC achieves a drag reduction of 39.0\% under a
bulk-velocity Reynolds number of 15,000, outperforming previous fluid control
methods by more than 32\%.

</details>


### [412] [Estimating link level traffic emissions: enhancing MOVES with open-source data](https://arxiv.org/abs/2510.03362)
*Lijiao Wang,Muhammad Usama,Haris N. Koutsopoulos,Zhengbing He*

Main category: cs.LG

TL;DR: 提出了一种基于开源数据的驾驶工况和交通排放估计框架，结合神经网络模型显著降低了误差。


<details>
  <summary>Details</summary>
Motivation: 利用开源数据实现城市区域车辆活动与排放的可扩展、透明化估计，弥补传统模型精度不足的问题。

Method: 融合MOVES模型与开源GPS轨迹、OSM路网、区域交通数据及卫星影像特征，训练神经网络预测行驶工况分布。

Result: 在波士顿45个市镇的应用显示，相比MOVES基线，关键污染物（CO、NOx、CO2、PM2.5）排放估算RMSE降低超过50%。

Conclusion: 全开源数据驱动的方法在区域交通排放估算中具有可行性、低成本和可复制性。

Abstract: Open-source data offers a scalable and transparent foundation for estimating
vehicle activity and emissions in urban regions. In this study, we propose a
data-driven framework that integrates MOVES and open-source GPS trajectory
data, OpenStreetMap (OSM) road networks, regional traffic datasets and
satellite imagery-derived feature vectors to estimate the link level operating
mode distribution and traffic emissions. A neural network model is trained to
predict the distribution of MOVES-defined operating modes using only features
derived from readily available data. The proposed methodology was applied using
open-source data related to 45 municipalities in the Boston Metropolitan area.
The "ground truth" operating mode distribution was established using OSM
open-source GPS trajectories. Compared to the MOVES baseline, the proposed
model reduces RMSE by over 50% for regional scale traffic emissions of key
pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the
feasibility of low-cost, replicable, and data-driven emissions estimation using
fully open data sources.

</details>


### [413] [Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds](https://arxiv.org/abs/2510.03364)
*Xiaolong Ma,Xu Dong,Ashley Tarrant,Lei Yang,Rao Kotamarthi,Jiali Wang,Feng Yan,Rajkumar Kettimuthu*

Main category: cs.LG

TL;DR: 本研究提出了一种名为WindSR的扩散模型，结合数据同化技术，用于风机轮毂高度风速的超分辨率降尺度，融合稀疏观测与模拟数据，显著提高了精度和效率。


<details>
  <summary>Details</summary>
Motivation: 高质量、高时空分辨率的轮毂高度风速数据对风电场选址和极端天气风险评估至关重要，但现有观测稀少，模拟数据存在偏差且分辨率不足。

Method: 提出WindSR模型，采用基于扩散模型的超分辨率方法，引入动态半径融合策略将观测数据与模拟场结合，并在训练和推理中融入地形信息以提升降尺度效果。

Result: WindSR在降尺度效率和准确性上优于CNN和GAN基线模型，通过数据同化使模型偏差相对于独立观测降低了约20%。

Conclusion: WindSR能有效融合多源数据生成高分辨率、低偏差的轮毂高度风速，为风能利用和极端风况评估提供了可靠工具。

Abstract: High-quality observations of hub-height winds are valuable but sparse in
space and time. Simulations are widely available on regular grids but are
generally biased and too coarse to inform wind-farm siting or to assess
extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully
utilize both data types for generating high-quality, high-resolution hub-height
wind speeds (tens to ~100m above ground), this study introduces WindSR, a
diffusion model with data assimilation for super-resolution downscaling of
hub-height winds. WindSR integrates sparse observational data with simulation
fields during downscaling using state-of-the-art diffusion models. A
dynamic-radius blending method is introduced to merge observations with
simulations, providing conditioning for the diffusion process. Terrain
information is incorporated during both training and inference to account for
its role as a key driver of winds. Evaluated against
convolutional-neural-network and generative-adversarial-network baselines,
WindSR outperforms them in both downscaling efficiency and accuracy. Our data
assimilation reduces WindSR's model bias by approximately 20% relative to
independent observations.

</details>


### [414] [Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis](https://arxiv.org/abs/2510.03366)
*Harshwardhan Fartale,Ashish Kattamuri,Rahul Raja,Arpita Vats,Ishita Prasad,Akshata Kishore Moharir*

Main category: cs.LG

TL;DR: 该研究通过机械可解释性方法，使用合成语言谜题数据集探究Transformer模型中事实回忆与推理能力的内在机制，发现二者依赖于分离但相互作用的神经回路。


<details>
  <summary>Details</summary>
Motivation: 区分模型中的回忆与推理机制对于预测泛化能力、设计针对性评估和构建安全干预措施至关重要。

Method: 结合激活补丁和结构化消融，在层、注意力头和神经元水平上对Qwen和LLaMA两类模型进行因果分析。

Result: 识别出特定的“回忆回路”和“推理回路”，分别干扰可导致对应任务性能下降达15%，而另一任务保持不变；神经元层面观察到任务特异性激活模式但较弱。

Conclusion: 回忆与推理在Transformer模型中依赖于可分离但交互的电路，为模型认知机制提供了因果证据，并推动了机械可解释性的发展。

Abstract: Transformer-based language models excel at both recall (retrieving memorized
facts) and reasoning (performing multi-step inference), but whether these
abilities rely on distinct internal mechanisms remains unclear. Distinguishing
recall from reasoning is crucial for predicting model generalization, designing
targeted evaluations, and building safer interventions that affect one ability
without disrupting the other.We approach this question through mechanistic
interpretability, using controlled datasets of synthetic linguistic puzzles to
probe transformer models at the layer, head, and neuron level. Our pipeline
combines activation patching and structured ablations to causally measure
component contributions to each task type. Across two model families (Qwen and
LLaMA), we find that interventions on distinct layers and attention heads lead
to selective impairments: disabling identified "recall circuits" reduces
fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas
disabling "reasoning circuits" reduces multi-step inference by a comparable
margin. At the neuron level, we observe task-specific firing patterns, though
these effects are less robust, consistent with neuronal polysemanticity.Our
results provide the first causal evidence that recall and reasoning rely on
separable but interacting circuits in transformer models. These findings
advance mechanistic interpretability by linking circuit-level structure to
functional specialization and demonstrate how controlled datasets and causal
interventions can yield mechanistic insights into model cognition, informing
safer deployment of large language models.

</details>


### [415] [Distributed Low-Communication Training with Decoupled Momentum Optimization](https://arxiv.org/abs/2510.03371)
*Sasho Nedelkoski,Alexander Acker,Odej Kao,Soeren Becker,Dominik Scheinert*

Main category: cs.LG

TL;DR: 提出一种结合不频繁同步和梯度动量压缩的方法，显著减少分布式模型训练中的通信开销，适用于低带宽互联环境。


<details>
  <summary>Details</summary>
Motivation: 降低对高带宽互连的依赖，使分布式计算资源可用于大模型训练。

Method: 将优化器动量视为信号，通过离散余弦变换（DCT）分解Nesterov动量为高低频成分，仅每H步同步高频成分。

Result: 相比DiLoCo基线最多实现16倍的通信量减少，并在Transformer和CNN等架构上具有良好泛化性。

Conclusion: 该方法推动了在低带宽互联的分布式节点上训练大模型的可行性。

Abstract: The training of large models demands substantial computational resources,
typically available only in data centers with high-bandwidth interconnects.
However, reducing the reliance on high-bandwidth interconnects between nodes
enables the use of distributed compute resources as an alternative to
centralized data center training. Building on recent advances in distributed
model training, we propose an approach that further reduces communication by
combining infrequent synchronizations across distributed model replicas with
gradient momentum compression. In particular, we treat the optimizer momentum
as a signal and decompose the Nesterov momentum into high- and low-frequency
components via the discrete cosine transform (DCT). Only the high-frequency
components are synchronized across model replicas every $H$ steps. Empirically,
our method achieves up to a $16\times$ reduction in communication compared to
the baseline DiLoCo, and it generalizes across architectures, including
transformer-based language models and convolutional neural networks for images.
Overall, this work advances the feasibility of training large models on
distributed nodes with low-bandwidth interconnects.

</details>


### [416] [Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation](https://arxiv.org/abs/2510.03375)
*Renrong Shao,Wei Zhang,Jun wang*

Main category: cs.LG

TL;DR: 提出了一种新的无数据知识蒸馏方法CPSC-DFKD，通过条件生成对抗网络生成类别特定的多样化图像，结合伪监督对比学习提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有无数据知识蒸馏方法难以区分不同类别样本分布，生成样本模糊，缺乏类别多样性，限制了学生模型的学习效果。

Method: 引入条件生成对抗网络生成类别特定图像，改进生成器结构以区分类别分布，并设计基于教师与学生视图的伪监督对比学习机制来增强样本多样性。

Result: 在三个常用数据集上实验表明，该方法显著提升了学生模型和生成器的性能。

Conclusion: CPSC-DFKD有效解决了无数据知识蒸馏中类别混淆和多样性不足的问题，实现了更优的知识迁移效果。

Abstract: Data-free knowledge distillation~(DFKD) is an effective manner to solve model
compression and transmission restrictions while retaining privacy protection,
which has attracted extensive attention in recent years. Currently, the
majority of existing methods utilize a generator to synthesize images to
support the distillation. Although the current methods have achieved great
success, there are still many issues to be explored. Firstly, the outstanding
performance of supervised learning in deep learning drives us to explore a
pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods
cannot distinguish the distributions of different categories of samples, thus
producing ambiguous samples that may lead to an incorrect evaluation by the
teacher. Besides, current methods cannot optimize the category-wise diversity
samples, which will hinder the student model learning from diverse samples and
further achieving better performance. In this paper, to address the above
limitations, we propose a novel learning paradigm, i.e., conditional
pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).
The primary innovations of CPSC-DFKD are: (1) introducing a conditional
generative adversarial network to synthesize category-specific diverse images
for pseudo-supervised learning, (2) improving the modules of the generator to
distinguish the distributions of different categories, and (3) proposing
pseudo-supervised contrastive learning based on teacher and student views to
enhance diversity. Comprehensive experiments on three commonly-used datasets
validate the performance lift of both the student and generator brought by
CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git

</details>


### [417] [A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew](https://arxiv.org/abs/2510.03380)
*Michael Ben Ali,Imen Megdiche,André Peninou,Olivier Teste*

Main category: cs.LG

TL;DR: 本文提出了一种针对联邦学习中数量偏斜（QS）问题的新型迭代聚类联邦学习算法CORNFLQS，通过结合两种操作策略，在多种非独立同分布（Non-IID）设置下表现出优异的鲁棒性、准确性和聚类质量。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的非独立同分布数据，尤其是数量偏斜（QS）问题，严重影响现有聚类联邦学习（CFL）方法的性能，但多数方法缺乏在QS下的系统评估，因此需要更鲁棒的解决方案。

Method: 提出一种名为CORNFLQS的新型迭代算法，通过服务器端和客户端协作，动态协调基于局部损失最小化和基于模型相似性的两种CFL策略，并在六种图像分类数据集上进行了270种Non-IID配置的广泛实验。

Result: 实验结果显示，CORNFLQS在准确率和聚类质量方面均取得最高平均排名，并对不同QS扰动表现出强鲁棒性，整体优于现有的CFL算法。

Conclusion: CORNFLQS有效应对了联邦学习中的数量偏斜挑战，通过协调两种CFL策略，提升了模型性能与稳定性，为Non-IID环境下的联邦学习提供了更具鲁棒性的解决方案。

Abstract: Federated Learning (FL) is a decentralized paradigm that enables a
client-server architecture to collaboratively train a global Artificial
Intelligence model without sharing raw data, thereby preserving privacy. A key
challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of
Non-IID, where clients hold highly heterogeneous data volumes. Clustered
Federated Learning (CFL) is an emergent variant of FL that presents a promising
solution to Non-IID problem. It improves models' performance by grouping
clients with similar data distributions into clusters. CFL methods generally
fall into two operating strategies. In the first strategy, clients select the
cluster that minimizes the local training loss. In the second strategy, the
server groups clients based on local model similarities. However, most CFL
methods lack systematic evaluation under QS but present significant challenges
because of it. In this paper, we present two main contributions. The first one
is an evaluation of state-of-the-art CFL algorithms under various Non-IID
settings, applying multiple QS scenarios to assess their robustness. Our second
contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes
an optimal coordination between both operating strategies of CFL. Our approach
is robust against the different variations of QS settings. We conducted
intensive experiments on six image classification datasets, resulting in 270
Non-IID configurations. The results show that CORNFLQS achieves the highest
average ranking in both accuracy and clustering quality, as well as strong
robustness to QS perturbations. Overall, our approach outperforms actual CFL
algorithms.

</details>


### [418] [Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges](https://arxiv.org/abs/2510.03381)
*Yongchao Li,Jun Chen,Zhuoxuan Li,Chao Gao,Yang Li,Chu Zhang,Changyin Dong*

Main category: cs.LG

TL;DR: 提出了一种基于跨模态重建预训练的时空解耦自编码器（STDAE），用于在缺少匝道实时检测数据的情况下提升高速公路互通立交交通流预测精度。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏实时匝道检测器，导致互通立交区域存在交通预测盲点，难以准确预测交通流。

Method: 采用两阶段框架：第一阶段利用主路数据重建历史匝道流量，通过时空解耦的并行自编码器进行跨模态预训练；第二阶段将学习到的表征与GWNet等预测模型结合进行预测。

Result: 在三个真实世界互通立交数据集上实验表明，STDAE-GWNet consistently 优于13种现有先进方法，且性能接近使用历史匝道数据的模型。

Conclusion: STDAE能有效缓解检测器缺失问题，具备良好的即插即用能力，可广泛集成于多种交通预测系统中。

Abstract: Interchanges are crucial nodes for vehicle transfers between highways, yet
the lack of real-time ramp detectors creates blind spots in traffic prediction.
To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a
two-stage framework that leverages cross-modal reconstruction pretraining. In
the first stage, STDAE reconstructs historical ramp flows from mainline data,
forcing the model to capture intrinsic spatio-temporal relations. Its decoupled
architecture with parallel spatial and temporal autoencoders efficiently
extracts heterogeneous features. In the prediction stage, the learned
representations are integrated with models such as GWNet to enhance accuracy.
Experiments on three real-world interchange datasets show that STDAE-GWNET
consistently outperforms thirteen state-of-the-art baselines and achieves
performance comparable to models using historical ramp data. This demonstrates
its effectiveness in overcoming detector scarcity and its plug-and-play
potential for diverse forecasting pipelines.

</details>


### [419] [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
*Donghwan Rho*

Main category: cs.LG

TL;DR: 本文研究了使用可验证奖励的强化学习（RLVR）在韩语单词接龙游戏中的应用，发现规则导出的奖励可能存在冲突，并通过课程学习策略缓解了这一问题。


<details>
  <summary>Details</summary>
Motivation: 探索RLVR在多语言逻辑谜题中的适用性，特别是非英语语言环境下的挑战和潜力。

Method: 采用强化学习与可验证奖励框架，结合课程学习策略来解决韩语单词接龙游戏中奖励冲突的问题。

Result: 实验表明，课程学习方案能有效缓解规则衍生奖励之间的冲突，提升模型表现。

Conclusion: 该研究支持在多样化语言环境中进一步研究基于RLVR的谜题任务，以增强语言模型的推理能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training large language models (LLMs) with stronger reasoning abilities. It
has also been applied to a variety of logic puzzles. In this work, we study the
Korean word-chain game using RLVR. We show that rule-derived rewards can
naturally conflict, and demonstrate through experiments that a
curriculum-learning scheme mitigates these conflicts. Our findings motivate
further studies of puzzle tasks in diverse languages.

</details>


### [420] [Training Variation of Physically-Informed Deep Learning Models](https://arxiv.org/abs/2510.03416)
*Ashley Lenau,Dennis Dimiduk,Stephen R. Niezgoda*

Main category: cs.LG

TL;DR: 本研究探讨了不同损失函数在训练深度学习网络时对边界条件（如应力平衡）的可靠性和一致性影响，使用Pix2Pix网络预测高弹性对比复合材料的应力场作为案例，强调报告模型变异的重要性以实现公平的方法比较。


<details>
  <summary>Details</summary>
Motivation: 由于物理信息损失函数日益流行，但缺乏对训练算法可重复性和可靠性的评估，因此需要研究损失函数在不同训练中对网络性能的影响。

Method: 采用Pix2Pix网络作为案例，实现多种强制应力平衡的损失函数，并在多次训练中评估其在收敛性、准确性和边界条件执行上的一致性。

Result: 不同损失函数表现出显著不同的模型变异性，在收敛、精度和应力平衡的实施方面存在差异。

Conclusion: 报告模型变异对于评估损失函数的可靠性至关重要，有助于更公平地比较不同的训练方法。

Abstract: A successful deep learning network is highly dependent not only on the
training dataset, but the training algorithm used to condition the network for
a given task. The loss function, dataset, and tuning of hyperparameters all
play an essential role in training a network, yet there is not much discussion
on the reliability or reproducibility of a training algorithm. With the rise in
popularity of physics-informed loss functions, this raises the question of how
reliable one's loss function is in conditioning a network to enforce a
particular boundary condition. Reporting the model variation is needed to
assess a loss function's ability to consistently train a network to obey a
given boundary condition, and provides a fairer comparison among different
methods. In this work, a Pix2Pix network predicting the stress fields of high
elastic contrast composites is used as a case study. Several different loss
functions enforcing stress equilibrium are implemented, with each displaying
different levels of variation in convergence, accuracy, and enforcing stress
equilibrium across many training sessions. Suggested practices in reporting
model variation are also shared.

</details>


### [421] [Multi-task neural diffusion processes for uncertainty-quantified wind power prediction](https://arxiv.org/abs/2510.03419)
*Joseph Rawson,Domniki Ladopoulou,Petros Dellaportas*

Main category: cs.LG

TL;DR: 提出多任务神经扩散过程（MT-NDP）框架用于风力发电预测，能够捕捉跨风机相关性并实现少样本适应，显著提升预测准确性与校准性能。


<details>
  <summary>Details</summary>
Motivation: 风力发电的不确定性预测对电网集成和风电场可靠运行至关重要，现有模型在处理偏离整体趋势的风机时表现不佳。

Method: 基于神经扩散过程（NDP）构建多任务学习框架（MT-NDP），引入任务编码器捕捉不同风机间的相关性，并支持对未见风机的少样本自适应。

Result: 在真实SCADA数据上验证，MT-NDP在点预测准确性和校准性方面优于单任务NDP和高斯过程（GP），尤其适用于行为异常的风机，提供更锐利且可信的预测区间。

Conclusion: NDP-based模型具备良好的校准性和可扩展性，适合实际部署，能有效支持现代风电场的调度与维护决策。

Abstract: Uncertainty-aware wind power prediction is essential for grid integration and
reliable wind farm operation. We apply neural diffusion processes (NDPs)-a
recent class of models that learn distributions over functions-and extend them
to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide
the first empirical evaluation of NDPs in real supervisory control and data
acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture
cross-turbine correlations and enable few-shot adaptation to unseen turbines.
The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of
point accuracy and calibration, particularly for wind turbines whose behaviour
deviates from the fleet average. In general, NDP-based models deliver
calibrated and scalable predictions suitable for operational deployment,
offering sharper, yet trustworthy, predictive intervals that can support
dispatch and maintenance decisions in modern wind farms.

</details>


### [422] [Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices](https://arxiv.org/abs/2510.03425)
*Congzheng Song,Xinyu Tang*

Main category: cs.LG

TL;DR: 提出了一种在移动设备上内存高效的反向传播实现方法（MeBP），可在低内存（<1GB）下高效微调0.5B到4B参数的大型语言模型，相比零阶优化方法收敛更快且性能更优。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法如LoRA在移动设备上内存消耗大，而零阶优化虽省内存但收敛慢，缺乏兼顾内存与效率的解决方案。

Method: 设计并实现了一种内存高效的反向传播算法MeBP，通过优化计算图和内存管理，在移动设备上实现低内存占用的模型微调。

Result: 在iPhone 15 Pro Max上验证，MeBP能在小于1GB内存下微调0.5B至4B参数的LLM，收敛速度优于ZO方法，且性能更好。

Conclusion: MeBP为在资源受限的移动设备上高效微调大模型提供了可行方案，显著优于现有零阶优化方法。

Abstract: Fine-tuning large language models (LLMs) with backpropagation\textemdash even
for a subset of parameters such as LoRA\textemdash can be much more
memory-consuming than inference and is often deemed impractical for
resource-constrained mobile devices. Alternative methods, such as zeroth-order
optimization (ZO), can greatly reduce the memory footprint but come at the cost
of significantly slower model convergence (10$\times$ to 100$\times$ more steps
than backpropagation). We propose a memory-efficient implementation of
backpropagation (MeBP) on mobile devices that provides better trade-off between
memory usage and compute time, while converging faster and achieving better
performance than the ZO baseline. We verify the effectiveness of MeBP on an
iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B
parameters, can be fine-tuned using less than 1GB of memory. We release an
example of the MeBP implementation at https://github.com/apple/ml-mebp.

</details>


### [423] [Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation](https://arxiv.org/abs/2510.03426)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 本文提出了广义数量级（GOOMs），扩展了传统数量级概念，解决了长序列实数累积中的数值溢出或下溢问题，并在GPU上实现了高效并行计算，显著提升了深度学习、金融等领域的高动态范围计算稳定性与效率。


<details>
  <summary>Details</summary>
Motivation: 许多领域在长序列上累积实数时面临严重的数值下溢或上溢问题，限制了计算的稳定性与可行性，因此需要一种更稳定的高动态范围计算方法。

Method: 提出广义数量级（GOOMs），将浮点数作为特例纳入统一框架，并设计高效的自定义并行前缀扫描算法，支持在GPU等并行硬件上原生执行。

Result: 在三个典型实验中，GOOMs实现了超越传统浮点数极限的矩阵乘积累积、并行计算李雅普诺夫指数谱（速度提升数个数量级）、以及无需稳定化技巧即可捕捉深度循环神经网络的长程依赖。

Conclusion: GOOMs结合高效并行扫描，为高动态范围应用提供了一种可扩展且数值稳健的浮点数替代方案，使此前被认为不切实际或不可行的任务变得可行且高效。

Abstract: Many domains, from deep learning to finance, require compounding real numbers
over long sequences, often leading to catastrophic numerical underflow or
overflow. We introduce generalized orders of magnitude (GOOMs), a principled
extension of traditional orders of magnitude that incorporates floating-point
numbers as a special case, and which in practice enables stable computation
over significantly larger dynamic ranges of real numbers than previously
possible. We implement GOOMs, along with an efficient custom parallel prefix
scan, to support native execution on parallel hardware such as GPUs. We
demonstrate that our implementation of GOOMs outperforms traditional approaches
with three representative experiments, all of which were previously considered
impractical or impossible, and now become possible and practical: (1)
compounding real matrix products far beyond standard floating-point limits; (2)
estimating spectra of Lyapunov exponents in parallel, orders of magnitude
faster than with previous methods, applying a novel selective-resetting method
to prevent state colinearity; and (3) capturing long-range dependencies in deep
recurrent neural networks with non-diagonal recurrent states, computed in
parallel via a prefix scan, without requiring any form of stabilization. Our
results show that our implementation of GOOMs, combined with efficient parallel
scanning, offers a scalable and numerically robust alternative to conventional
floating-point numbers for high-dynamic-range applications.

</details>


### [424] [LHGEL: Large Heterogeneous Graph Ensemble Learning using Batch View Aggregation](https://arxiv.org/abs/2510.03432)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种面向大规模异质图的集成学习框架LHGEL，通过批采样生成多视图子图，结合残差注意力机制和多样性正则化，有效提升模型在复杂图结构下的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大规模异质图的节点、边类型多样，特征差异大，局部结构复杂，传统图学习方法难以有效建模其异质性，且在全局优化与计算效率之间存在权衡。

Method: 提出LHGEL框架，包含三个核心组件：批视图聚合（生成多个子图视图）、残差注意力（自适应加权不同视图贡献）和多样性正则化（增强模型多样性）。采用集成学习思想，在不同采样条件下训练多个基学习器。

Result: 在五个真实异质网络上的实验表明，LHGEL显著优于现有最先进方法，理论分析还证明残差注意力有助于缓解集成学习中的梯度消失问题。

Conclusion: LHGEL通过有效的多视图集成策略，成功应对了大规模异质图学习中的异质性建模、模型多样性与计算效率挑战，实现了更优的节点表示学习性能。

Abstract: Learning from large heterogeneous graphs presents significant challenges due
to the scale of networks, heterogeneity in node and edge types, variations in
nodal features, and complex local neighborhood structures. This paper advocates
for ensemble learning as a natural solution to this problem, whereby training
multiple graph learners under distinct sampling conditions, the ensemble
inherently captures different aspects of graph heterogeneity. Yet, the crux
lies in combining these learners to meet global optimization objective while
maintaining computational efficiency on large-scale graphs. In response, we
propose LHGEL, an ensemble framework that addresses these challenges through
batch sampling with three key components, namely batch view aggregation,
residual attention, and diversity regularization. Specifically, batch view
aggregation samples subgraphs and forms multiple graph views, while residual
attention adaptively weights the contributions of these views to guide node
embeddings toward informative subgraphs, thereby improving the accuracy of base
learners. Diversity regularization encourages representational disparity across
embedding matrices derived from different views, promoting model diversity and
ensemble robustness. Our theoretical study demonstrates that residual attention
mitigates gradient vanishing issues commonly faced in ensemble learning.
Empirical results on five real heterogeneous networks validate that our LHGEL
approach consistently outperforms its state-of-the-art competitors by
substantial margin. Codes and datasets are available at
https://github.com/Chrisshen12/LHGEL.

</details>


### [425] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: 本文研究了在m依赖数据下核变点检测（KCPD）的一致性理论，并通过LLM生成的合成文本验证了其渐近性质，同时展示了KCPD结合现代文本嵌入在真实文本分割任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的KCPD理论主要基于独立性假设，但在实际序列数据（如文本）中存在强依赖性，因此需要在更现实的依赖结构下建立理论保证并验证其在文本分割中的实用性。

Method: 在m依赖数据假设下证明KCPD在变点数量和位置上的弱一致性；使用大语言模型生成具有m依赖结构的合成文本进行模拟验证；并在多种真实文本数据集上进行文本分割实验，评估KCPD结合现代嵌入方法的性能。

Result: 理论方面证明了KCPD在m依赖数据下的渐近一致性；模拟实验验证了理论结果；在多个文本数据集上，基于嵌入的KCPD优于现有基线方法；案例研究显示其在分析Taylor Swift推文时具有良好的实际效果。

Conclusion: KCPD在处理具有依赖结构的复杂序列数据（如文本）时兼具理论可靠性和实践有效性，结合现代文本嵌入可显著提升文本分割性能。

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [426] [On residual network depth](https://arxiv.org/abs/2510.03470)
*Benoit Dherin,Michael Munn*

Main category: cs.LG

TL;DR: 本文提出了一个解析公式，证明深度残差网络的深度增加等价于隐式集成模型规模的扩大，并揭示了其分层集成结构和组合路径爆炸问题，为归一化层的作用提供了从第一性原理出发的解释，同时提出基于残差扩展定理的缩放机制可有效控制模型容量与复杂度。


<details>
  <summary>Details</summary>
Motivation: 尽管残差网络（如ResNet和Transformer）在深度模型中取得了巨大成功，但为何深度如此有效仍缺乏理论解释；现有对归一化层的依赖也缺乏根本性理解，因此需要从网络功能结构出发建立形式化理论。

Method: 提出“残差扩展定理”（Residual Expansion Theorem），通过数学展开残差网络的函数表达式，揭示其内部组合路径的指数增长机制和分层集成结构，并分析缩放因子如何调控输出信号爆炸和模型复杂度。

Result: 1) 得到残差网络等价于隐式集成多个浅层模型的显式数学证明；2) 揭示组合路径导致输出信号爆炸，解释了归一化层的必要性；3) 从网络结构本身推导出缩放机制（如SkipInit、Fixup）的有效性原理；4) 发现模块缩放具有控制模型容量和隐式正则化的作用。

Conclusion: 深度残差网络的有效性源于其隐式的分层集成结构，深度增加本质上是扩展集成规模；归一化或缩放操作的本质是控制由网络结构引起的组合爆炸，该工作为深度网络设计提供了基于功能结构的第一性原理解释。

Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled
models of unprecedented depth, yet a formal understanding of why depth is so
effective remains an open question. A popular intuition, following Veit et al.
(2016), is that these residual networks behave like ensembles of many shallower
models. Our key finding is an explicit analytical formula that verifies this
ensemble perspective, proving that increasing network depth is mathematically
equivalent to expanding the size of this implicit ensemble. Furthermore, our
expansion reveals a hierarchical ensemble structure in which the combinatorial
growth of computation paths leads to an explosion in the output signal,
explaining the historical necessity of normalization layers in training deep
models. This insight offers a first principles explanation for the historical
dependence on normalization layers and sheds new light on a family of
successful normalization-free techniques like SkipInit and Fixup. However,
while these previous approaches infer scaling factors through optimizer
analysis or a heuristic analogy to Batch Normalization, our work offers the
first explanation derived directly from the network's inherent functional
structure. Specifically, our Residual Expansion Theorem reveals that scaling
each residual module provides a principled solution to taming the combinatorial
explosion inherent to these architectures. We further show that this scaling
acts as a capacity controls that also implicitly regularizes the model's
complexity.

</details>


### [427] [How to Set $β_1, β_2$ in Adam: An Online Learning Perspective](https://arxiv.org/abs/2510.03478)
*Quan Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种对Adam优化器中动量因子β1和β2的更通用分析方法，克服了以往要求β1=√β2的限制，适用于β1≥√β2和β1≤√β2两种情况，并证明了β1=√β2在面对非盲目对手时并非最优选择。


<details>
  <summary>Details</summary>
Motivation: 现有的Adam优化器理论分析通常假设β1=√β2，这在实际应用中并不总是成立，因此需要一种更为通用的理论框架来理解不同β1和β2设置下的性能。

Method: 通过将Adam视为在线学习中的FTRL算法实例，推导出适用于更广泛β1和β2关系的新分析结果。

Result: 得到了适用于β1≥√β2和β1≤√β2情况下的新界限，这些界限严格地推广了现有结果，并且在最坏情况下是紧致的；同时证明了β1=√β2对于盲目对手是最优的，但对于非盲目对手则是次优的。

Conclusion: 本文提供的分析框架能够更好地理解和指导Adam优化器中动量参数的选择，特别是在非盲目对抗环境下，建议考虑偏离β1=√β2的传统设定。

Abstract: While Adam is one of the most effective optimizer for training large-scale
machine learning models, a theoretical understanding of how to optimally set
its momentum factors, $\beta_1$ and $\beta_2$, remains largely incomplete.
  Prior works have shown that Adam can be seen as an instance of
Follow-the-Regularized-Leader (FTRL), one of the most important class of
algorithms in online learning.
  The prior analyses in these works required setting $\beta_1 =
\sqrt{\beta_2}$, which does not cover the more practical cases with $\beta_1
\neq \sqrt{\beta_2}$.
  We derive novel, more general analyses that hold for both $\beta_1 \geq
\sqrt{\beta_2}$ and $\beta_1 \leq \sqrt{\beta_2}$.
  In both cases, our results strictly generalize the existing bounds.
  Furthermore, we show that our bounds are tight in the worst case.
  We also prove that setting $\beta_1 = \sqrt{\beta_2}$ is optimal for an
oblivious adversary, but sub-optimal for an non-oblivious adversary.

</details>


### [428] [Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains](https://arxiv.org/abs/2510.03486)
*Anupam Panwar,Himadri Pal,Jiali Chen,Kyle Cho,Riddick Jiang,Miao Zhao,Rajiv Krishnamurthy*

Main category: cs.LG

TL;DR: 本文提出了一种统一的实时异常检测框架RADF，通过自动化算法选择与超参数调优技术mSelect，提升了大规模异构时序数据中的异常检测性能，并支持异常后的快速根因定位。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式系统中异常检测面临数据量大、数据异构性强、根因定位困难以及模型需频繁手动调优等问题，亟需一个能自动适应不同场景并高效检测与诊断的统一框架。

Method: 提出Reasoning based Anomaly Detection Framework (RADF)，结合新颖的mSelect技术实现每个用例的算法选择与超参数自动调优，并集成检测后的推理机制以加速异常 triaging 和根因分析。

Result: 在9个公开基准数据集中，RADF在5个上超越现有最先进模型的AUC表现，且在7个数据集上AUC超过0.85，表现优于其他任何现有模型。

Conclusion: RADF框架通过自动化和后检测分析能力，有效应对了大规模分布式系统中异常检测的三大挑战，展现出优越的检测性能和实用潜力。

Abstract: Detecting anomalies in large, distributed systems presents several
challenges. The first challenge arises from the sheer volume of data that needs
to be processed. Flagging anomalies in a high-throughput environment calls for
a careful consideration of both algorithm and system design. The second
challenge comes from the heterogeneity of time-series datasets that leverage
such a system in production. In practice, anomaly detection systems are rarely
deployed for a single use case. Typically, there are several metrics to
monitor, often across several domains (e.g. engineering, business and
operations). A one-size-fits-all approach rarely works, so these systems need
to be fine-tuned for every application - this is often done manually. The third
challenge comes from the fact that determining the root-cause of anomalies in
such settings is akin to finding a needle in a haystack. Identifying (in real
time) a time-series dataset that is associated causally with the anomalous
time-series data is a very difficult problem. In this paper, we describe a
unified framework that addresses these challenges. Reasoning based Anomaly
Detection Framework (RADF) is designed to perform real time anomaly detection
on very large datasets. This framework employs a novel technique (mSelect) that
automates the process of algorithm selection and hyper-parameter tuning for
each use case. Finally, it incorporates a post-detection capability that allows
for faster triaging and root-cause determination. Our extensive experiments
demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly
detection models in AUC performance for 5 out of 9 public benchmarking
datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a
distinction unmatched by any other state-of-the-art model.

</details>


### [429] [Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability](https://arxiv.org/abs/2510.03494)
*Volodymyr Tkachuk,Csaba Szepesvári,Xiaoqi Tan*

Main category: cs.LG

TL;DR: 本文研究了在有限时间范围内的离线强化学习，提出了在特定假设下具有统计效率的策略评估和优化方法，并改进了现有方法的样本复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，在仅有数据良好覆盖和Q函数线性可实现性的假设下，无法实现统计上高效的离线强化学习。本文旨在通过引入额外的轨迹数据假设，解决策略评估和优化中的统计效率问题。

Method: 本文提出了一种新的策略评估方法，并通过对现有策略优化算法进行更紧密的分析，改进了其样本复杂度。

Result: 在相同的假设条件下，实现了统计上高效的策略评估，并改进了策略优化算法的样本复杂度。

Conclusion: 通过引入轨迹数据假设，可以在q^π-可实现性和数据覆盖性之外，实现更高效的离线强化学习，为未来研究提供了新方向。

Abstract: We study finite-horizon offline reinforcement learning (RL) with function
approximation for both policy evaluation and policy optimization. Prior work
established that statistically efficient learning is impossible for either of
these problems when the only assumptions are that the data has good coverage
(concentrability) and the state-action value function of every policy is
linearly realizable ($q^\pi$-realizability) (Foster et al., 2021). Recently,
Tkachuk et al. (2024) gave a statistically efficient learner for policy
optimization, if in addition the data is assumed to be given as trajectories.
In this work we present a statistically efficient learner for policy evaluation
under the same assumptions. Further, we show that the sample complexity of the
learner used by Tkachuk et al. (2024) for policy optimization can be improved
by a tighter analysis.

</details>


### [430] [D2 Actor Critic: Diffusion Actor Meets Distributional Critic](https://arxiv.org/abs/2510.03508)
*Lunjun Zhang,Shuo Han,Hanrui Lyu,Bradly C Stadie*

Main category: cs.LG

TL;DR: 提出D2AC，一种新的无模型强化学习算法，用于有效在线训练表达性扩散策略，在多个复杂任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在训练扩散策略时存在高方差的策略梯度或时间反向传播的复杂性问题，需要更稳定高效的方法。

Method: 设计了一个避免高方差策略梯度和时间反向传播复杂性的策略改进目标，并结合分布强化学习与截断双Q学习提出了一种鲁棒的分布式评论家。

Result: 在18个高难度RL任务上实现了最先进的性能，涵盖Humanoid、Dog和Shadow Hand等领域，同时在生物启发的捕食者-猎物任务中表现出良好的行为鲁棒性和泛化能力。

Conclusion: D2AC通过稳定的训练过程和鲁棒的批评机制，显著提升了扩散策略在复杂环境中的表现，具有广泛的应用潜力。

Abstract: We introduce D2AC, a new model-free reinforcement learning (RL) algorithm
designed to train expressive diffusion policies online effectively. At its core
is a policy improvement objective that avoids the high variance of typical
policy gradients and the complexity of backpropagation through time. This
stable learning process is critically enabled by our second contribution: a
robust distributional critic, which we design through a fusion of
distributional RL and clipped double Q-learning. The resulting algorithm is
highly effective, achieving state-of-the-art performance on a benchmark of
eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,
spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard
benchmarks, we also evaluate a biologically motivated predator-prey task to
examine the behavioral robustness and generalization capacity of our approach.

</details>


### [431] [Task-Level Contrastiveness for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2510.03509)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 本文提出了一种任务级对比学习方法，通过定义任务增强和任务级对比损失，提升少样本分类与元学习在跨域场景下的泛化能力和计算效率，且无需领域先验知识，在MetaDataset上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本分类和元学习方法在跨不同领域时泛化能力差，通常局限于单一数据集，难以迁移知识，且存在准确率低、计算成本高和依赖强假设的问题。

Method: 引入任务级对比性概念，设计任务增强方式，并定义任务级对比损失，以促进任务表征的无监督聚类；该方法轻量且可嵌入现有算法中。

Result: 在MetaDataset基准上实验表明，该方法在不增加复杂度的情况下显著提升了性能，具有更好的泛化性和计算效率。

Conclusion: 任务级对比学习有效解决了跨域少样本学习中的泛化与效率问题，是一种通用且实用的改进框架。

Abstract: Few-shot classification and meta-learning methods typically struggle to
generalize across diverse domains, as most approaches focus on a single
dataset, failing to transfer knowledge across various seen and unseen domains.
Existing solutions often suffer from low accuracy, high computational costs,
and rely on restrictive assumptions. In this paper, we introduce the notion of
task-level contrastiveness, a novel approach designed to address issues of
existing methods. We start by introducing simple ways to define task
augmentations, and thereafter define a task-level contrastive loss that
encourages unsupervised clustering of task representations. Our method is
lightweight and can be easily integrated within existing few-shot/meta-learning
algorithms while providing significant benefits. Crucially, it leads to
improved generalization and computational efficiency without requiring prior
knowledge of task domains. We demonstrate the effectiveness of our approach
through different experiments on the MetaDataset benchmark, where it achieves
superior performance without additional complexity.

</details>


### [432] [A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT](https://arxiv.org/abs/2510.03513)
*Taha M. Mahmoud,Naima Kaabouch*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习的轻量级、隐私保护的物联网僵尸网络检测框架，通过分布式训练和高效的聚合策略，在保护用户隐私的同时实现了高检测精度和低通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法在可扩展性、隐私保护和适应性方面存在不足，难以应对资源受限的物联网环境中日益增长的僵尸网络攻击威胁。

Method: 采用联邦学习框架，使设备在不交换原始数据的情况下协同训练模型，并引入通信高效的聚合策略以减少网络开销。

Result: 在基准物联网僵尸网络数据集上的实验表明，该框架在保持高检测精度的同时显著降低了通信成本。

Conclusion: 联邦学习为物联网生态系统提供了一条可扩展、安全且注重隐私的入侵检测可行路径。

Abstract: The rapid growth of the Internet of Things (IoT) has expanded opportunities
for innovation but also increased exposure to botnet-driven cyberattacks.
Conventional detection methods often struggle with scalability, privacy, and
adaptability in resource-constrained IoT environments. To address these
challenges, we present a lightweight and privacy-preserving botnet detection
framework based on federated learning. This approach enables distributed
devices to collaboratively train models without exchanging raw data, thus
maintaining user privacy while preserving detection accuracy. A
communication-efficient aggregation strategy is introduced to reduce overhead,
ensuring suitability for constrained IoT networks. Experiments on benchmark IoT
botnet datasets demonstrate that the framework achieves high detection accuracy
while substantially reducing communication costs. These findings highlight
federated learning as a practical path toward scalable, secure, and
privacy-aware intrusion detection for IoT ecosystems.

</details>


### [433] [RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models](https://arxiv.org/abs/2510.03515)
*Lianghuan Huang,Sagnik Anupam,Insup Lee,Shuo Li,Osbert Bastani*

Main category: cs.LG

TL;DR: 提出了一种名为RAPID的新型强化学习算法，通过大批次推理和小批次离策略更新显著减少训练时间，同时保持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在微调小型语言模型方面有潜力，但通常资源消耗大、训练时间长。

Method: 采用大批次进行推理，然后使用小批次进行离策略策略梯度更新，并引入组优势估计和重要性加权估计器来纠正偏差。

Result: 在三个基准测试中，相比当前最先进的强化学习算法，运行时间减少了11%-34%，同时保持了相似或更好的准确率。

Conclusion: RAPID算法有效提高了计算资源利用率，显著缩短了强化学习的训练时间，适用于数学和编程等任务的小型语言模型微调。

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for
finetuning small language models (SLMs) to solve targeted tasks such as math
and coding. However, RL algorithms tend to be resource-intensive, taking a
significant amount of time to train. We propose RAPID, a novel RL algorithm
that can substantially reduce the running time of RL. Our key insight is that
RL tends to be costly due to the need to perform both inference and
backpropagation during training. To maximize use of computational resources,
our algorithm performs inference in large batches, and then performs off-policy
policy gradient updates in mini-batches. For off-policy updates, we incorporate
group advantage estimation into the policy gradient algorithm, and derive an
importance weighted estimator to correct for the bias arising from off-policy
learning. Our experiments demonstrate that our algorithm can reduce running
time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms
while maintaining similar or better accuracy.

</details>


### [434] [Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models](https://arxiv.org/abs/2510.03520)
*Kartik Pandit,Sourav Ganguly,Arnesh Banerjee,Shaahin Angizi,Arnob Ghosh*

Main category: cs.LG

TL;DR: 本文提出了一种名为Certifiable Safe-RLHF（CS-RLHF）的新方法，用于提升大语言模型在安全性与实用性之间的平衡。该方法通过语义驱动的成本模型和基于精确罚函数的优化机制，避免了传统CMDP方法对拉格朗日乘子的依赖，实现了可验证的安全性保障并提高了训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于Constrained Markov Decision Processes（CMDP）的方法在确保大语言模型安全性方面存在局限：一是依赖易受表面关键词影响的奖励/成本函数；二是使用对偶变量调整，计算昂贵且无法为固定变量提供可证明的安全保证，易受对抗性越狱攻击。因此需要一种更鲁棒、高效且具备理论安全保证的方法。

Method: 提出CS-RLHF方法，引入一个在大规模语料上训练的成本模型，以生成语义层面的安全评分。采用基于精确罚函数的修正惩罚机制替代传统的拉格朗日框架，在适当缩放的惩罚系数下，直接在优化过程中强制满足安全约束，从而无需更新对偶变量。

Result: 实验表明，CS-RLHF在应对正常和越狱提示时，其性能优于当前最先进的方法，并且至少提升了5倍的效率。此外，该方法提供了可验证的安全性保证，避免了对偶变量调优带来的计算开销。

Conclusion: CS-RLHF通过引入语义驱动的成本模型和基于精确罚函数的优化框架，有效解决了传统CMDP方法在安全性、计算效率和理论保障方面的不足，为大语言模型的安全对齐提供了一种更高效且可验证的新路径。

Abstract: Ensuring safety is a foundational requirement for large language models
(LLMs). Achieving an appropriate balance between enhancing the utility of model
outputs and mitigating their potential for harm is a complex and persistent
challenge. Contemporary approaches frequently formalize this problem within the
framework of Constrained Markov Decision Processes (CMDPs) and employ
established CMDP optimization techniques. However, these methods exhibit two
notable limitations. First, their reliance on reward and cost functions renders
performance highly sensitive to the underlying scoring mechanism, which must
capture semantic meaning rather than being triggered by superficial keywords.
Second, CMDP-based training entails tuning dual-variable, a process that is
both computationally expensive and does not provide any provable safety
guarantee for a fixed dual variable that can be exploitable through adversarial
jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF
(CS-RLHF) that introduces a cost model trained on a large-scale corpus to
assign semantically grounded safety scores. In contrast to the lagrangian-based
approach, CS-RLHF adopts a rectified penalty-based formulation. This design
draws on the theory of exact penalty functions in constrained optimization,
wherein constraint satisfaction is enforced directly through a suitably chosen
penalty term. With an appropriately scaled penalty, feasibility of the safety
constraints can be guaranteed at the optimizer, eliminating the need for
dual-variable updates. Empirical evaluation demonstrates that CS-RLHF
outperforms state-of-the-art LLM model responses rendering at-least 5 times
efficient against nominal and jail-breaking prompts

</details>


### [435] [Sequential decoder training for improved latent space dynamics identification](https://arxiv.org/abs/2510.03535)
*William Anderson,Seung Whan Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出多阶段LaSDI（mLaSDI）框架，通过逐步学习解码器修正残差误差，提升重建和预测精度。


<details>
  <summary>Details</summary>
Motivation: 标准LaSDI在训练中强制隐式动力学会损害重构精度，需改进以提高模拟数据的准确性。

Method: 结合自编码器与方程发现，分阶段学习多个解码器以纠正前一阶段的残差误差。

Result: 在1D-1V Vlasov方程上，mLaSDI相比标准LaSDI具有更低的预测误差和更短的训练时间。

Conclusion: mLaSDI通过多阶段残差校正显著提升了重建和预测性能，适用于多种网络架构。

Abstract: Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.

</details>


### [436] [CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer](https://arxiv.org/abs/2510.03566)
*Ashwin Prabu,Nhat Thanh Tran,Guofa Zhou,Jack Xin*

Main category: cs.LG

TL;DR: 本文提出了一种名为CrossLag的新模型，通过引入环境感知的注意力机制，在低参数量下有效预测登革热暴发，显著优于TimeXer基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以准确预测需要及时预警的重大登革热暴发事件。

Method: 提出CrossLag模型，利用考虑气候和海洋异常等外部因素滞后效应的注意力机制，结合TimeXer框架进行改进。

Result: 在新加坡登革热数据上，CrossLag在24周预测窗口内显著优于TimeXer模型，能更有效地检测和预测重大暴发。

Conclusion: CrossLag通过融合滞后性环境信号，在较低参数量下实现了对登革热暴发的更优预测，具有实际应用潜力。

Abstract: A variety of models have been developed to forecast dengue cases to date.
However, it remains a challenge to predict major dengue outbreaks that need
timely public warnings the most. In this paper, we introduce CrossLag, an
environmentally informed attention that allows for the incorporation of lagging
endogenous signals behind the significant events in the exogenous data into the
architecture of the transformer at low parameter counts. Outbreaks typically
lag behind major changes in climate and oceanic anomalies. We use TimeXer, a
recent general-purpose transformer distinguishing exogenous-endogenous inputs,
as the baseline for this study. Our proposed model outperforms TimeXer by a
considerable margin in detecting and predicting major outbreaks in Singapore
dengue data over a 24-week prediction window.

</details>


### [437] [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
*Fatmazohra Rezkellah,Ramzi Dakhmouche*

Main category: cs.LG

TL;DR: 提出一种统一的约束优化方法，通过最小干预LLM权重来实现敏感信息遗忘和抗越狱攻击，无需依赖难以获得的分类器，且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，需要在保证生成内容安全和隐私的同时进行定制化，但现有方法常依赖不可靠或高成本的分类器，缺乏对遗忘和鲁棒性问题的统一处理。

Method: 采用约束优化框架，通过对模型权重施加点对点的简单约束，使敏感词不可达或将权重移至更安全区域，从而同时实现信息遗忘和抗攻击能力，避免使用外部分类器。

Result: 所提方法在敏感信息遗忘和防御越狱攻击方面均优于现有的最先进防御方法，且计算成本更低，尤其简单的点对点约束干预表现最佳。

Conclusion: 该统一优化框架能有效、高效地提升大语言模型的安全性和隐私性，为模型定制提供了低开销、高性能的解决方案。

Abstract: With the increasing adoption of Large Language Models (LLMs), more
customization is needed to ensure privacy-preserving and safe generation. We
address this objective from two critical aspects: unlearning of sensitive
information and robustness to jail-breaking attacks. We investigate various
constrained optimization formulations that address both aspects in a
\emph{unified manner}, by finding the smallest possible interventions on LLM
weights that either make a given vocabulary set unreachable or embed the LLM
with robustness to tailored attacks by shifting part of the weights to a
\emph{safer} region. Beyond unifying two key properties, this approach
contrasts with previous work in that it doesn't require an oracle classifier
that is typically not available or represents a computational overhead.
Surprisingly, we find that the simplest point-wise constraint-based
intervention we propose leads to better performance than max-min interventions,
while having a lower computational cost. Comparison against state-of-the-art
defense methods demonstrates superior performance of the proposed approach.

</details>


### [438] [Longitudinal Flow Matching for Trajectory Modeling](https://arxiv.org/abs/2510.03569)
*Mohammad Mohaiminul Islam,Thijs P. Kuipers,Sharvaree Vadgama,Coen de Vente,Afsana Khan,Clara I. Sánchez,Erik J. Bekkers*

Main category: cs.LG

TL;DR: 提出了一种名为Interpolative Multi-Marginal Flow Matching (IMMFM)的新框架，用于学习稀疏且高维序列数据的连续随机动态，通过分段二次插值路径实现多时间点联合一致性建模，在合成和真实神经影像数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生成模型在处理稀疏采样和高维时序数据时常受限于仅建模两两转移，难以捕捉多时间点间的复杂动态。

Method: IMMFM采用分段二次插值路径作为流匹配的平滑目标，联合优化漂移项和数据驱动的扩散系数，并基于理论条件保证学习稳定性。

Result: 在合成基准和真实纵向神经影像数据集上，IMMFM在预测准确性和下游任务中均优于现有方法。

Conclusion: IMMFM能有效建模稀疏、高维序列数据的连续随机动态，支持个体化轨迹生成，具有良好的理论基础和应用性能。

Abstract: Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.

</details>


### [439] [Generalization of Graph Neural Network Models for Distribution Grid Fault Detection](https://arxiv.org/abs/2510.03571)
*Burak Karabulut,Carlo Manna,Chris Develder*

Main category: cs.LG

TL;DR: 本文系统评估了多种图神经网络（GNN）架构在RNN+GNN框架下的电力系统故障诊断性能，首次提出将GraphSAGE和GAT/GATv2应用于该领域，并验证了其在不同电网拓扑下的优越泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有故障检测方法在面对电网拓扑变化时鲁棒性不足，且主流GCN架构可能无法充分捕捉复杂空间依赖关系，需要更先进和系统的GNN模型评估。

Method: 采用RNN（GRU）与多种GNN（包括GraphSAGE、GAT、GATv2、RGCN）结合的RGNN框架，在IEEE 123节点配电网络上进行实验，对比不同模型在跨拓扑场景下的故障检测性能。

Result: RGATv2表现出最佳泛化能力，跨拓扑设置下F1分数仅下降约12%；纯RNN模型性能大幅下降达60%，其他RGNN变体最多下降25%。

Conclusion: RGATv2在动态电网环境中具有更强的鲁棒性和应用潜力，优于传统GCN和纯RNN方法，适合部署于拓扑变化频繁的实际配电系统中。

Abstract: Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.

</details>


### [440] [Efficient Test-Time Scaling for Small Vision-Language Models](https://arxiv.org/abs/2510.03574)
*Mehmet Onurcan Kaya,Desmond Elliott,Dim P. Papadopoulos*

Main category: cs.LG

TL;DR: 提出了两种高效的测试时扩展策略（TTAug和TTAdapt），利用模型内部特征提升小型视觉语言模型的性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 小型视觉语言模型虽然计算高效，但泛化能力和下游任务表现较弱，现有测试时扩展方法计算开销大，违背其资源高效的设计目标。

Method: 提出两种无需外部监督的测试时扩展策略：一是测试时增强（TTAug），通过生成多个增强输入并在token级别聚合输出；二是测试时自适应（TTAdapt），利用TTAug生成的一致性伪标签在推理过程中调整模型参数。

Result: 在九个基准上进行了广泛实验，结果表明所提方法在保持计算效率的同时，显著提升了模型性能，并且适用于不同规模和架构的VLMs，无需额外调参。

Conclusion: TTAug和TTAdapt是通用、高效的小型VLM测试时扩展方案，能够在资源受限环境下有效提升模型表现。

Abstract: Small Vision-Language Models (VLMs) provide a computationally efficient
alternative to larger models, at the cost of weaker generalization abilities
and downstream task performance. These shortcomings could be addressed by
test-time scaling techniques, but existing methods are typically
computationally demanding, contradicting the resource-efficient design goals of
small models. To address these limitations, we propose two novel and efficient
test-time scaling strategies that leverage the model-internal features rather
than external supervision: (i) Test-Time Augmentation (TTAug), which generates
multiple augmented inputs and aggregates outputs at the token level without
parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model
parameters during inference using consensus-based pseudolabels from TTAug.
Through extensive experiments across nine benchmarks, we demonstrate consistent
performance improvements while maintaining computational efficiency suitable
for resource-constrained environments. The generality of our approach is
demonstrated both within models at different scales and across different VLMs
without additional tuning.

</details>


### [441] [BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems](https://arxiv.org/abs/2510.03576)
*Bongseok Kim,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: 提出了一种边界条件保证的进化型Kolmogorov-Arnold网络（BEKAN），结合径向基函数，通过三种可组合方法精确施加Dirichlet、周期性和Neumann边界条件，在PDE求解中显著优于MLP和B样条KAN。


<details>
  <summary>Details</summary>
Motivation: 深度学习在求解偏微分方程（PDE）方面取得进展，但神经网络的黑箱特性难以精确施加边界条件，限制了其在科学计算中的可靠性与精度。

Method: 提出BEKAN模型：1）使用光滑全局高斯基RBF构建单变量激活函数以嵌入Dirichlet边界信息；2）设计基于正弦函数的周期层以精确满足周期边界条件；3）采用最小二乘形式引导参数演化以满足Neumann条件。结合进化框架实现边界条件的严格满足。

Result: 在多种边界值问题（包括混合边界）的数值实验中，BEKAN在精度上均优于多层感知机（MLP）和B样条KAN，验证了其有效性和鲁棒性。

Conclusion: BEKAN通过在激活函数、网络结构和优化策略中嵌入边界条件，显著提升了KAN在PDE求解中的准确性与可靠性，推动了深度学习在科学计算与工程中的应用。

Abstract: Deep learning has gained attention for solving PDEs, but the black-box nature
of neural networks hinders precise enforcement of boundary conditions. To
address this, we propose a boundary condition-guaranteed evolutionary
Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,
we propose three distinct and combinable approaches for incorporating
Dirichlet, periodic, and Neumann boundary conditions into the network. For
Dirichlet problem, we use smooth and global Gaussian RBFs to construct
univariate basis functions for approximating the solution and to encode
boundary information at the activation level of the network. To handle periodic
problems, we employ a periodic layer constructed from a set of sinusoidal
functions to enforce the boundary conditions exactly. For a Neumann problem, we
devise a least-squares formulation to guide the parameter evolution toward
satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the
periodic layer, and the evolutionary framework, we can perform accurate PDE
simulations while rigorously enforcing boundary conditions. For demonstration,
we conducted extensive numerical experiments on Dirichlet, Neumann, periodic,
and mixed boundary value problems. The results indicate that BEKAN outperforms
both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In
conclusion, the proposed approach enhances the capability of KANs in solving
PDE problems while satisfying boundary conditions, thereby facilitating
advancements in scientific computing and engineering applications.

</details>


### [442] [Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning](https://arxiv.org/abs/2510.03578)
*Haoran Li,Chenhan Xiao,Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 本文提出了Latent MoS模型，通过混合对称性从复杂动力学数据中学习具有样本效率的动态系统，结合层次化结构提升长期等变性，在多种物理系统中表现出优于现有方法的插值与外推能力。


<details>
  <summary>Details</summary>
Motivation: 由于低分辨率传感器导致系统测量有限，需要样本高效的动力学学习；而现有对称性发现方法通常假设单一全局对称群，并将对称性发现与动力学学习分离，表达能力有限且易累积误差。

Method: 提出Latent Mixture of Symmetries (Latent MoS) 模型，捕捉由多种对称性支配的潜在因子混合，局部且可证明地保持底层对称变换，并引入堆叠MoS块的层次化架构以建模长期等变性。

Result: 在多个物理系统上的实验表明，Latent MoS在插值和外推任务上优于最先进的基线方法，同时提供可用于几何和安全关键分析的可解释潜在表示。

Conclusion: Latent MoS通过混合对称性的局部建模和层次结构设计，有效提升了动力学学习的样本效率和泛化能力，兼具性能优势与可解释性。

Abstract: Learning dynamics is essential for model-based control and Reinforcement
Learning in engineering systems, such as robotics and power systems. However,
limited system measurements, such as those from low-resolution sensors, demand
sample-efficient learning. Symmetry provides a powerful inductive bias by
characterizing equivariant relations in system states to improve sample
efficiency. While recent methods attempt to discover symmetries from data, they
typically assume a single global symmetry group and treat symmetry discovery
and dynamic learning as separate tasks, leading to limited expressiveness and
error accumulation. In this paper, we propose the Latent Mixture of Symmetries
(Latent MoS), an expressive model that captures a mixture of symmetry-governed
latent factors from complex dynamical measurements. Latent MoS focuses on
dynamic learning while locally and provably preserving the underlying symmetric
transformations. To further capture long-term equivariance, we introduce a
hierarchical architecture that stacks MoS blocks. Numerical experiments in
diverse physical systems demonstrate that Latent MoS outperforms
state-of-the-art baselines in interpolation and extrapolation tasks while
offering interpretable latent representations suitable for future geometric and
safety-critical analyses.

</details>


### [443] [FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors](https://arxiv.org/abs/2510.03589)
*Ankit Bhardwaj,Ananth Balashankar,Lakshminarayanan Subramanian*

Main category: cs.LG

TL;DR: 本文提出了FieldFormer，一种基于Transformer的无网格时空场重建框架，结合了数据驱动的灵活性和物理结构，在稀疏且含噪的传感器数据下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理稀疏、噪声大且不规则的时空传感器数据时，往往忽略控制偏微分方程或扩展性差，因此需要一种兼顾物理一致性与高效建模的新方法。

Method: FieldFormer采用可学习的速度缩放距离度量来构建局部邻域，并通过每批次偏移重计算高效生成邻域；使用局部Transformer编码器进行预测，并通过自动微分计算PDE残差和边界特定惩罚项来保证物理一致性。

Result: 在三个基准任务（各向异性热方程、浅水系统和真实污染物对流-扩散模拟）中，FieldFormer比强基线方法性能提升超过40%，RMSE小于10^{-2}，适用于仅0.4%-2%稀疏且含10%噪声的数据。

Conclusion: FieldFormer实现了高精度、高效且符合物理规律的场重建，为稀疏不规则传感器数据的建模提供了有效解决方案。

Abstract: Spatio-temporal sensor data is often sparse, noisy, and irregular, and
existing interpolation or learning methods struggle here because they either
ignore governing PDEs or do not scale. We introduce FieldFormer, a
transformer-based framework for mesh-free spatio-temporal field reconstruction
that combines data-driven flexibility with physics-based structure. For each
query, FieldFormer gathers a local neighborhood using a learnable
velocity-scaled distance metric, enabling anisotropic adaptation to different
propagation regimes. Neighborhoods are built efficiently via per-batch offset
recomputation, and refined in an expectation-maximization style as the velocity
scales evolve. Predictions are made by a local transformer encoder, and physics
consistency is enforced through autograd-based PDE residuals and
boundary-specific penalties. Across three benchmarks--a scalar anisotropic heat
equation, a vector-valued shallow-water system, and a realistic
advection-diffusion pollution simulation--FieldFormer consistently outperforms
strong baselines by more than 40%. Our results demonstrate that FieldFormer
enables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field
reconstruction from sparse (0.4%-2%) and noisy(10%) data.

</details>


### [444] [MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation](https://arxiv.org/abs/2510.03601)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Kai-Chun Liu,Yu Tsao*

Main category: cs.LG

TL;DR: 提出了一种基于多层移动边缘计算（MLMEC）和知识蒸馏（KD）的跌倒检测系统，通过在前端设备与云端之间引入多层边缘计算节点并利用KD提升模型精度，显著提高了检测准确率并降低了数据延迟。


<details>
  <summary>Details</summary>
Motivation: 应对传统跌倒检测系统中边缘设备模型容量有限和数据传输延迟高的问题，提升系统的实时性与准确性。

Method: 构建多层移动边缘计算（MLMEC）框架，将神经网络模型分布于多个计算节点，并采用知识蒸馏（KD）技术使前端设备从后端强计算节点学习，从而提升本地检测性能。

Result: 在SisFall和FallAllD数据集上，KD使检测准确率分别提升11.65%和2.78%；相比无KD的MLMEC，数据延迟降低分别为46.67%和54.15%。

Conclusion: MLMEC结合KD能有效平衡跌倒检测系统的精度与延迟，优于传统架构。

Abstract: The rising aging population has increased the importance of fall detection
(FD) systems as an assistive technology, where deep learning techniques are
widely applied to enhance accuracy. FD systems typically use edge devices (EDs)
worn by individuals to collect real-time data, which are transmitted to a cloud
center (CC) or processed locally. However, this architecture faces challenges
such as a limited ED model size and data transmission latency to the CC. Mobile
edge computing (MEC), which allows computations at MEC servers deployed between
EDs and CC, has been explored to address these challenges. We propose a
multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC
splits the architecture into stations, each with a neural network model. If
front-end equipment cannot detect falls reliably, data are transmitted to a
station with more robust back-end computing. The knowledge distillation (KD)
approach was employed to improve front-end detection accuracy by allowing
high-power back-end stations to provide additional learning experiences,
enhancing precision while reducing latency and processing loads. Simulation
results demonstrate that the KD approach improved accuracy by 11.65% on the
SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also
reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on
the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD
system exhibits improved accuracy and reduced latency.

</details>


### [445] [Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends](https://arxiv.org/abs/2510.03604)
*Yucheng Wang,Mohamed Ragab,Yubo Hou,Zhenghua Chen,Min Wu,Xiaoli Li*

Main category: cs.LG

TL;DR: 本文综述了针对涡扇发动机剩余寿命（RUL）预测的领域自适应（DA）技术，提出了一种面向涡扇发动机的新型分类体系，并对现有方法进行了评估与分析。


<details>
  <summary>Details</summary>
Motivation: 由于涡扇发动机运行条件复杂、数据分布变化大且标记数据稀缺，传统数据驱动方法在RUL预测中面临挑战，亟需针对性地研究适用于该场景的领域自适应技术。

Method: 提出一种新的多维分类体系（基于方法、对齐层次和问题类型），系统梳理并分类现有的DA方法；同时在真实涡扇发动机数据集上评估代表性DA技术的有效性。

Result: 总结了当前DA在涡扇发动机RUL预测中的主要方法与挑战，验证了部分DA方法的实际效果，并揭示了不同方法在应对工况变化时的表现差异。

Conclusion: 所提出的分类体系有助于更好地理解涡扇发动机RUL预测中的DA方法，为实践者提供了实用指导，并指明了未来研究方向，如更精细的特征对齐与跨工况泛化能力提升。

Abstract: Remaining Useful Life (RUL) prediction for turbofan engines plays a vital
role in predictive maintenance, ensuring operational safety and efficiency in
aviation. Although data-driven approaches using machine learning and deep
learning have shown potential, they face challenges such as limited data and
distribution shifts caused by varying operating conditions. Domain Adaptation
(DA) has emerged as a promising solution, enabling knowledge transfer from
source domains with abundant data to target domains with scarce data while
mitigating distributional shifts. Given the unique properties of turbofan
engines, such as complex operating conditions, high-dimensional sensor data,
and slower-changing signals, it is essential to conduct a focused review of DA
techniques specifically tailored to turbofan engines. To address this need,
this paper provides a comprehensive review of DA solutions for turbofan engine
RUL prediction, analyzing key methodologies, challenges, and recent
advancements. A novel taxonomy tailored to turbofan engines is introduced,
organizing approaches into methodology-based (how DA is applied),
alignment-based (where distributional shifts occur due to operational
variations), and problem-based (why certain adaptations are needed to address
specific challenges). This taxonomy offers a multidimensional view that goes
beyond traditional classifications by accounting for the distinctive
characteristics of turbofan engine data and the standard process of applying DA
techniques to this area. Additionally, we evaluate selected DA techniques on
turbofan engine datasets, providing practical insights for practitioners and
identifying key challenges. Future research directions are identified to guide
the development of more effective DA techniques, advancing the state of RUL
prediction for turbofan engines.

</details>


### [446] [Explore the Loss space with Hill-ADAM](https://arxiv.org/abs/2510.03613)
*Meenakshi Manikandan,Leilani Gilpin*

Main category: cs.LG

TL;DR: 本文提出了Hill-ADAM优化器，通过在误差最小化和最大化之间交替来逃离局部极小值，以寻找全局最小值。


<details>
  <summary>Details</summary>
Motivation: 传统优化器如ADAM在逃离局部极小值方面存在局限，Hill-ADAM旨在通过确定性探索状态空间克服这一问题。

Method: 推导ADAM步长的解析近似，提出Hill-ADAM算法，在最小化和最大化误差之间交替进行。

Result: Hill-ADAM在5个损失函数和12个图像颜色校正任务上进行了测试，表现出有效的全局搜索能力。

Conclusion: Hill-ADAM能有效逃离局部极小值并探索损失空间，有助于找到全局最优解。

Abstract: This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus
towards escaping local minima in prescribed loss landscapes to find the global
minimum. Hill-ADAM escapes minima by deterministically exploring the state
space. This eliminates uncertainty from random gradient updates in stochastic
algorithms while seldom converging at the first minimum that visits. In the
paper we first derive an analytical approximation of the ADAM Optimizer step
size at a particular model state. From there define the primary condition
determining ADAM limitations in escaping local minima. The proposed optimizer
algorithm Hill-ADAM alternates between error minimization and maximization. It
maximizes to escape the local minimum and minimizes again afterward. This
alternation provides an overall exploration throughout the loss space. This
allows the deduction of the global minimum's state. Hill-ADAM was tested with 5
loss functions and 12 amber-saturated to cooler-shade image color correction
instances.

</details>


### [447] [Neural Bayesian Filtering](https://arxiv.org/abs/2510.03614)
*Christopher Solinas,Radovan Haluska,David Sychrovsky,Finbarr Timbers,Nolan Bard,Michael Buro,Martin Schmid,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.LG

TL;DR: 提出神经贝叶斯滤波（NBF），结合经典滤波的高效性与深度生成模型的表达能力，用于在部分可观测系统中维护隐状态分布。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理部分可观测系统中的隐状态估计时，难以兼顾计算效率和对复杂、多峰后验分布的表达能力。

Method: NBF通过将信念映射为固定长度的嵌入向量，在嵌入空间中利用粒子风格的更新结合观测和环境动力学进行后验推断，并用生成模型进行采样。

Result: 在三个部分可观测环境中验证了NBF的有效性，能够高效追踪快速变化、多模态的信念分布，并缓解粒子贫化问题。

Conclusion: NBF成功融合了经典滤波的效率与深度生成模型的表达力，在状态估计任务中表现出色。

Abstract: We present Neural Bayesian Filtering (NBF), an algorithm for maintaining
distributions over hidden states, called beliefs, in partially observable
systems. NBF is trained to find a good latent representation of the beliefs
induced by a task. It maps beliefs to fixed-length embedding vectors, which
condition generative models for sampling. During filtering, particle-style
updates compute posteriors in this embedding space using incoming observations
and the environment's dynamics. NBF combines the computational efficiency of
classical filters with the expressiveness of deep generative models - tracking
rapidly shifting, multimodal beliefs while mitigating the risk of particle
impoverishment. We validate NBF in state estimation tasks in three partially
observable environments.

</details>


### [448] [Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis](https://arxiv.org/abs/2510.03633)
*An Vuong,Susan Gauch*

Main category: cs.LG

TL;DR: 本文提出了一种结合推文情感特征与历史股价的深度学习框架，用于预测次日重大股价变动。利用Llama 3.1-8B-Instruct模型预处理推文数据，并融合三种情感分析方法提取情绪特征，输入LSTM模型进行训练。实验结果表明，相较于仅使用历史价格的基线模型（准确率13.5%），引入情感特征显著提升了预测准确率，其中基于DistilRoBERTa的方法在Llama增强后准确率从23.6%提升至38.5%。


<details>
  <summary>Details</summary>
Motivation: 由于市场波动性和投资者情绪影响，短期股价预测具有挑战性。传统方法忽略情绪因素，导致预测效果有限。因此，需要融合社交媒体情绪信息以提升预测准确性。

Method: 采用Meta的Llama 3.1-8B-Instruct模型对推文进行预处理，提升情感特征质量；结合三种情感分析方法（基于Transformer的DistilRoBERTa分类器和两种基于NRC词典的方法）提取情绪特征；将情绪特征与前一日股价数据融合，输入LSTM模型进行训练以预测次日重大价格变动。

Result: 在TSLA、AAPL和AMZN股票上的实验显示，三种情感分析方法均优于仅使用历史价格的基线模型（准确率13.5%）。其中，DistilRoBERTa结合LLaMA预处理的模型表现最佳，准确率从23.6%提升至38.5%。

Conclusion: 使用大语言模型预处理社交媒体文本可有效提升情感分析质量，进而显著提高重大股价变动的预测准确率，验证了融合情绪信息在金融预测中的价值。

Abstract: Accurately predicting short-term stock price movement remains a challenging
task due to the market's inherent volatility and sensitivity to investor
sentiment. This paper discusses a deep learning framework that integrates
emotion features extracted from tweet data with historical stock price
information to forecast significant price changes on the following day. We
utilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby
enhancing the quality of emotion features derived from three emotion analysis
approaches: a transformer-based DistilRoBERTa classifier from the Hugging Face
library and two lexicon-based methods using National Research Council Canada
(NRC) resources. These features are combined with previous-day stock price data
to train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA,
AAPL, and AMZN stocks show that all three emotion analysis methods improve the
average accuracy for predicting significant price movements, compared to the
baseline model using only historical stock prices, which yields an accuracy of
13.5%. The DistilRoBERTa-based stock prediction model achieves the best
performance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced
emotion analysis. These results demonstrate that using large language models to
preprocess tweet content enhances the effectiveness of emotion analysis which
in turn improves the accuracy of predicting significant stock price movements.

</details>


### [449] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: 该研究探讨了在公共健康情感分析背景下，大语言模型中的上下文学习（ICL）如何受到数据投毒攻击的影响，并提出使用谱签名防御来有效识别和过滤恶意样本，从而保护模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在高风险公共健康场景中上下文学习的安全脆弱性，特别是在面对细微但恶意的数据扰动时的鲁棒性问题。

Method: 在人类偏肺病毒（HMPV）推文数据上引入同义词替换、否定插入和随机扰动等对抗性扰动作为支持样本，评估其对ICL性能的影响，并应用谱签名防御方法过滤中毒样本。

Result: 对抗扰动导致高达67%的情感标签翻转；应用谱签名防御后，ICL准确率稳定在约46.7%，逻辑回归验证准确率达到100%，表明防御有效恢复并保持了数据完整性。

Conclusion: 研究证实了ICL在现实公共健康情感分析中的安全风险，验证了谱签名防御在提升大语言模型鲁棒性方面的有效性，强调了在敏感应用场景中部署可靠AI系统的必要性。

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [450] [Implicit Models: Expressive Power Scales with Test-Time Compute](https://arxiv.org/abs/2510.03638)
*Jialin Liu,Lisang Ding,Stanley Osher,Wotao Yin*

Main category: cs.LG

TL;DR: 本文研究了隐式模型通过迭代提升表达能力的机制，证明其表达能力随测试时计算量增加而增强，并在多个领域验证了该理论。


<details>
  <summary>Details</summary>
Motivation: 尽管隐式模型在实践中表现出色，但其为何能通过更多测试时计算匹配或超越显式模型的机制尚不清楚，本文旨在填补这一理论空白。

Method: 采用非参数化分析方法，对隐式算子的表达能力进行严格的数学刻画，并证明其通过迭代可逐步表达更复杂的映射。

Result: 理论表明，广泛类别的隐式模型的表达能力可随测试时计算量扩展，最终匹配更丰富的函数类；实验在图像重建、科学计算和运筹学中验证了随着迭代次数增加，映射复杂性和解质量同时提升。

Conclusion: 隐式模型的表达能力与测试时计算密切相关，通过迭代可动态提升性能，为高效模型设计提供了理论支持。

Abstract: Implicit models, an emerging model class, compute outputs by iterating a
single parameter block to a fixed point. This architecture realizes an
infinite-depth, weight-tied network that trains with constant memory,
significantly reducing memory needs for the same level of performance compared
to explicit models. While it is empirically known that these compact models can
often match or even exceed larger explicit networks by allocating more
test-time compute, the underlying mechanism remains poorly understood.
  We study this gap through a nonparametric analysis of expressive power. We
provide a strict mathematical characterization, showing that a simple and
regular implicit operator can, through iteration, progressively express more
complex mappings. We prove that for a broad class of implicit models, this
process lets the model's expressive power scale with test-time compute,
ultimately matching a much richer function class. The theory is validated
across three domains: image reconstruction, scientific computing, and
operations research, demonstrating that as test-time iterations increase, the
complexity of the learned mapping rises, while the solution quality
simultaneously improves and stabilizes.

</details>


### [451] [In-Vivo Training for Deep Brain Stimulation](https://arxiv.org/abs/2510.03643)
*Nicholas Carter,Arkaprava Gupta,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的深部脑刺激方法，使用可测量的体内脑活动调节刺激参数，在抑制帕金森病生物标志物方面优于临床标准方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的深部脑刺激研究依赖于无法在患者体内测量的生物标志物，仅适用于脑芯片模拟，限制了其临床应用。

Method: 采用基于TD3的强化学习代理，训练于大脑基底节区域模型，根据可体内测量的脑活动调节刺激频率和幅度。

Result: 该方法在抑制与帕金森病严重程度相关的生物标志物方面显著优于现代临床DBS方案，且依赖的信息可在真实环境中测量。

Conclusion: 该研究展示了在真实临床场景中实现个性化强化学习驱动DBS的可行性，为个体化治疗帕金森病提供了新途径。

Abstract: Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's
Disease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL
agents modulating the stimulation frequency and amplitude. But, these models
rely on biomarkers that are not measurable in patients and are only present in
brain-on-chip (BoC) simulations. In this work, we present an RL-based DBS
approach that adapts these stimulation parameters according to brain activity
measurable in vivo. Using a TD3 based RL agent trained on a model of the basal
ganglia region of the brain, we see a greater suppression of biomarkers
correlated with PD severity compared to modern clinical DBS implementations.
Our agent outperforms the standard clinical approaches in suppressing PD
biomarkers while relying on information that can be measured in a real world
environment, thereby opening up the possibility of training personalized RL
agents specific to individual patient needs.

</details>


### [452] [SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network](https://arxiv.org/abs/2510.03648)
*Huijing Zhang,Muyang Cao,Linshan Jiang,Xin Du,Di Yu,Changze Lv,Shuiguang Deng*

Main category: cs.LG

TL;DR: 本文提出了一种基于脉冲神经网络（SNN）的设备端少样本类增量学习方法SAFA-SNN，通过稀疏性感知神经元动态、零阶优化和子空间投影提升模型在资源受限环境下的持续学习能力与能效表现。


<details>
  <summary>Details</summary>
Motivation: 为解决边缘设备在数据不足情况下持续学习新类别的挑战，同时克服传统人工神经网络在资源受限设备上部署的局限性。

Method: 提出SAFA-SNN方法，采用稀疏条件神经元动态以减少灾难性遗忘，利用零阶优化处理脉冲不可微问题，并通过子空间投影增强新类别的判别能力。

Result: 在CIFAR100、Mini-ImageNet及三个神经形态数据集上实验表明，SAFA-SNN在Mini-ImageNet最后一个增量阶段至少提升4.01%，且能耗比基线方法降低20%。

Conclusion: SAFA-SNN有效实现了高效能、低能耗的设备端少样本类增量学习，具备在神经形态硬件上实际部署的潜力。

Abstract: Continuous learning of novel classes is crucial for edge devices to preserve
data privacy and maintain reliable performance in dynamic environments.
However, the scenario becomes particularly challenging when data samples are
insufficient, requiring on-device few-shot class-incremental learning (FSCIL)
to maintain consistent model performance. Although existing work has explored
parameter-efficient FSCIL frameworks based on artificial neural networks
(ANNs), their deployment is still fundamentally constrained by limited device
resources. Inspired by neural mechanisms, Spiking neural networks (SNNs)
process spatiotemporal information efficiently, offering lower energy
consumption, greater biological plausibility, and compatibility with
neuromorphic hardware than ANNs. In this work, we present an SNN-based method
for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We
first propose sparsity-conditioned neuronal dynamics, in which most neurons
remain stable while a subset stays active, thereby mitigating catastrophic
forgetting. To further cope with spike non-differentiability in gradient
estimation, we employ zeroth-order optimization. Moreover, during incremental
learning sessions, we enhance the discriminability of new classes through
subspace projection, which alleviates overfitting to novel classes. Extensive
experiments conducted on two standard benchmark datasets (CIFAR100 and
Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,
and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,
specifically achieving at least 4.01% improvement at the last incremental
session on Mini-ImageNet and 20% lower energy cost over baseline methods with
practical implementation.

</details>


### [453] [LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design](https://arxiv.org/abs/2510.03650)
*Amir Sadikov*

Main category: cs.LG

TL;DR: 本文提出了一种基于大语言模型引导的进化程序合成方法，用于解决低差异点集和Sobol'序列设计中的两个长期难题，在2D/3D点集构造和高维积分误差优化方面取得了新基准或改进。


<details>
  <summary>Details</summary>
Motivation: 传统准蒙特卡洛（QMC）方法依赖手工设计的低差异点集和数字序列，但在有限样本情况下难以达到最优；本文旨在通过自动化程序合成为QMC设计提供更优、可扩展的解决方案。

Method: 采用两阶段方法：第一阶段利用LLM生成构造性代码提案，第二阶段通过任务特定适应度函数指导的进化循环对代码进行变异与选择，分别应用于最小化星差异的有限点集构造和优化Sobol'方向数以降低随机化QMC误差。

Result: 在2D情况下重现已知最优解并建立N≥40的新基准，在3D情况下匹配N≤8的所有已知最优解并提供更高N下的改进结果；在32维期权定价任务中，相比Joe-Kuo参数显著降低rQMC均方误差，同时保持任意样本大小的可扩展性和标准随机化的兼容性。

Conclusion: LLM驱动的进化程序合成能够自动发现高质量的QMC构造，不仅复现经典设计，还在有限样本场景下实现性能提升，展示了其在数值积分自动化设计中的潜力。

Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo
(QMC) methods for high-dimensional integration. We cast two long-standing QMC
design problems as program synthesis and solve them with an LLM-guided
evolutionary loop that mutates and selects code under task-specific fitness:
(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)
choosing Sobol' direction numbers that minimize randomized QMC error on
downstream integrands. Our two-phase procedure combines constructive code
proposals with iterative numerical refinement. On finite sets, we rediscover
known optima in small 2D cases and set new best-known 2D benchmarks for N >=
40, while matching most known 3D optima up to the proven frontier (N <= 8) and
reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'
parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)
mean-squared error for several 32-dimensional option-pricing tasks relative to
widely used Joe--Kuo parameters, while preserving extensibility to any sample
size and compatibility with standard randomizations. Taken together, the
results demonstrate that LLM-driven evolutionary program synthesis can automate
the discovery of high-quality QMC constructions, recovering classical designs
where they are optimal and improving them where finite-N structure matters.
Data and code are available at
https://github.com/hockeyguy123/openevolve-star-discrepancy.git.

</details>


### [454] [Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast](https://arxiv.org/abs/2510.03657)
*Aymeric Fabre*

Main category: cs.LG

TL;DR: 本论文探讨了如何利用澳大利亚能源市场运营商（AEMO）的电价预测数据，开发一种可靠且盈利的电池储能系统（BESS）交易算法。


<details>
  <summary>Details</summary>
Motivation: 尽管AEMO提供了丰富的电价预测数据，但其在实际BESS交易决策中的价值尚未被充分挖掘。随着可再生能源和市场去中心化导致电网波动加剧，亟需将预测转化为有效策略。

Method: 通过分析不同时间段、预测周期和地区差异下的预测准确性模式，构建了一个基于预测信息的新型BESS交易模型，并使用机器学习技术改进AEMO预测以优化交易策略。

Result: 所提出的预测驱动交易算法在套利收益上优于无预测信息的基本交易算法，验证了利用AEMO预测提升BESS交易性能的可行性。

Conclusion: 该研究填补了现有框架在评估AEMO预测可靠性及将其融入BESS交易策略方面的空白，为未来能源市场交易模型的改进和BESS的高效集成提供了支持。

Abstract: In electricity markets around the world, the ability to anticipate price
movements with precision can be the difference between profit and loss,
especially for fast-acting assets like battery energy storage systems (BESS).
As grid volatility increases due to renewables and market decentralisation,
operators and forecasters alike face growing pressure to transform prediction
into strategy. Yet while forecast data is abundant, especially in advanced
markets like Australia's National Electricity Market (NEM), its practical value
in driving real-world BESS trading decisions remains largely unexplored. This
thesis dives into that gap. This work addresses a key research question: Can
the accuracy of the Australian Energy Market Operator (AEMO) energy price
forecasts be systematically leveraged to develop a reliable and profitable
battery energy storage system trading algorithm? Despite the availability of
AEMO price forecasts, no existing framework evaluates their reliability or
incorporates them into practical BESS trading strategies. By analysing patterns
in forecast accuracy based on time of day, forecast horizon, and regional
variations, this project creates a novel, forecast-informed BESS trading model
to optimise arbitrage financial returns. The performance of this
forecast-driven algorithm is benchmarked against a basic trading algorithm with
no knowledge of forecast data. The study further explores the potential of
machine learning techniques to predict future energy prices by enhancing AEMO
forecasts to govern a more advanced trading strategy. The research outcomes
will inform future improvements in energy market trading models and promote
more efficient BESS integration into market operations.

</details>


### [455] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: 本文研究了稀疏自动编码器（SAE）在大语言模型（LLM）行为引导中的应用，发现可解释性并不能有效预测引导效果，并提出一种新的特征选择方法Delta Token Confidence，显著提升引导性能。


<details>
  <summary>Details</summary>
Motivation: 探究SAE的可解释性是否真正反映其在引导大语言模型行为方面的有效性，解决当前缺乏对此关系系统评估的问题。

Method: 训练90个SAE覆盖三种LLM、五种架构和六种稀疏水平，使用SAEBench和AxBench分别评估可解释性和引导效用，并通过Kendall秩相关系数分析两者关系；提出Delta Token Confidence作为新特征选择标准。

Result: 发现可解释性与引导效用之间仅有较弱正相关（tau b≈0.298）；采用Delta Token Confidence可使引导性能平均提升52.52%；筛选高Delta Token Confidence特征后，可解释性与效用的相关性消失甚至转为负相关。

Conclusion: SAE特征的可解释性不足以作为其引导效用的可靠指标，Delta Token Confidence能更有效地识别具有强引导能力的特征，揭示可解释性与实际功能之间的显著分离。

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [456] [Operationalizing Data Minimization for Privacy-Preserving LLM Prompting](https://arxiv.org/abs/2510.03662)
*Jijie Zhou,Niloofar Mireshghallah,Tianshi Li*

Main category: cs.LG

TL;DR: 提出了一种用于量化在保持任务效用的前提下最小化用户隐私泄露的框架，并通过树搜索方法在隐私优先的变换空间中找到最优披露点。实验表明，较大的前沿大模型比小型开源模型更能容忍数据最小化，且现有模型难以准确预测最优最小化策略，暴露出隐私与能力双重缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在消费应用中的快速部署，用户频繁交换个人信息导致隐私风险上升。为减少因过度分享引发的记忆化、个性化追踪或安全漏洞等问题，需要系统性方法实现数据最小化。

Method: 提出了一个形式化定义和操作化数据最小化的框架，结合优先级队列树搜索，在给定用户提示和响应模型下，寻找保持效用的同时最小化隐私泄露的最优披露方案。

Result: 在四个涵盖开放对话和知识密集型任务的数据集上评估了九个大模型，结果显示GPT-5可容忍85.7%的信息遮蔽，而Qwen2.5-0.5B仅能容忍19.3%；相比搜索得出的基准，当前模型倾向于抽象化而导致信息过量披露。

Conclusion: 大型前沿模型在保持任务性能的同时更能适应强数据最小化，但现有模型普遍缺乏对任务所需信息的认知，导致无法自主实现最优数据最小化，揭示了模型在隐私保护能力上的根本局限。

Abstract: The rapid deployment of large language models (LLMs) in consumer applications
has led to frequent exchanges of personal information. To obtain useful
responses, users often share more than necessary, increasing privacy risks via
memorization, context-based personalization, or security breaches. We present a
framework to formally define and operationalize data minimization: for a given
user prompt and response model, quantifying the least privacy-revealing
disclosure that maintains utility, and we propose a priority-queue tree search
to locate this optimal point within a privacy-ordered transformation space. We
evaluated the framework on four datasets spanning open-ended conversations
(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth
answers (CaseHold, MedQA), quantifying achievable data minimization with nine
LLMs as the response model. Our results demonstrate that larger frontier LLMs
can tolerate stronger data minimization while maintaining task quality than
smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for
Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that
LLMs struggle to predict optimal data minimization directly, showing a bias
toward abstraction that leads to oversharing. This suggests not just a privacy
gap, but a capability gap: models may lack awareness of what information they
actually need to solve a task.

</details>


### [457] [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
*Wenlong Deng,Yi Ren,Yushu Li,Boying Gong,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 本文提出了Token Hidden Reward (THR) 方法，通过量化每个token对正确回答的影响来显式调控大模型在强化学习中的探索与利用行为。


<details>
  <summary>Details</summary>
Motivation: 如何在基于奖励的强化学习中明确引导大语言模型进行探索或利用仍是一个开放问题。

Method: 提出Token Hidden Reward (THR) 指标，并结合GRPO框架设计THR引导的重加权算法，动态调整训练信号以偏向探索或利用。

Result: 在多种数学推理基准上验证了该方法的有效性：增强正THR token提升贪婪解码准确率（利于利用），削弱负THR token则提高Pass@K（利于探索），且可与其他RL目标（如GSPO）和不同架构（如Llama）兼容。

Conclusion: THR为调控大语言模型在推理任务中的探索-利用平衡提供了细粒度、可验证的新机制，支持更精准的定向微调。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced the
reasoning capabilities of large language models, yet how to explicitly steer
training toward exploration or exploitation remains an open problem. We
introduce Token Hidden Reward (THR), a token-level metric that quantifies each
token's influence on the likelihood of correct responses under Group Relative
Policy Optimization (GRPO). We find that training dynamics are dominated by a
small subset of tokens with high absolute THR values. Most interestingly,
tokens with positive THR strengthen confidence in correct outputs, thus
favoring exploitation, while tokens with negative THR preserve probability mass
for alternative outputs, enabling exploration. This insight suggests a natural
intervention: a THR-guided reweighting algorithm that modulates GRPO's learning
signals to explicitly bias training toward exploitation or exploration. We
validate the efficacy of this algorithm on diverse math reasoning benchmarks.
By amplifying tokens with positive THR value and weakening negative ones, our
algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse
strategy yields consistent gains in Pass@K accuracy, favoring exploration. We
further demonstrate that our algorithm integrates seamlessly with other RL
objectives such as GSPO and generalizes across architectures including Llama.
These findings establish THR as a principled and fine-grained mechanism for
dynamically controlling exploration and exploitation in RL-tuned LLMs,
providing new tools for targeted fine-tuning in reasoning-intensive
applications.

</details>


### [458] [Towards Sampling Data Structures for Tensor Products in Turnstile Streams](https://arxiv.org/abs/2510.03678)
*Zhao Song,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种基于重要性采样的注意力采样方法，用于减轻大规模注意力模型的计算负担。


<details>
  <summary>Details</summary>
Motivation: 由于传统注意力机制在大规模模型中计算开销巨大，本文旨在通过引入采样方法来降低计算复杂度。

Method: 受ℓ²采样器定义和大语言模型中注意力机制进展的启发，提出了注意力采样器的定义，并在流式设置下应用重要性采样。

Result: 该方法在理论上减少了空间占用和更新时间，具有良好的可扩展性和广泛的应用性。

Conclusion: 注意力采样器能有效降低大规模注意力模型的计算成本，适用于多种模型架构和应用场景。

Abstract: This paper studies the computational challenges of large-scale
attention-based models in artificial intelligence by utilizing importance
sampling methods in the streaming setting. Inspired by the classical definition
of the $\ell_2$ sampler and the recent progress of the attention scheme in
Large Language Models (LLMs), we propose the definition of the attention
sampler. Our approach significantly reduces the computational burden of
traditional attention mechanisms. We analyze the effectiveness of the attention
sampler from a theoretical perspective, including space and update time.
Additionally, our framework exhibits scalability and broad applicability across
various model architectures and domains.

</details>


### [459] [Group Policy Gradient](https://arxiv.org/abs/2510.03679)
*Junhua Chen,Zixi Zhang,Hantao Zhong,Rika Antonova*

Main category: cs.LG

TL;DR: 提出了一种无需critic的策略梯度估计方法GPG，基于组的蒙特卡洛优势估计，在保持PPO结构的同时优于或匹配其性能。


<details>
  <summary>Details</summary>
Motivation: 受RLHF中GRPO方法启发，旨在去除训练critic网络带来的内存、计算和超参数开销，同时保留PPO的优点。

Method: 引入Group Policy Gradient（GPG），使用基于组的蒙特卡洛优势估计替代学习到的价值函数，并保持PPO的裁剪目标结构。

Result: 理论证明了GPG估计器的一致性，分析了偏差-方差权衡，并在标准基准上验证其性能达到或超过PPO。此外，GPG更高效地利用并行模拟和计算资源。

Conclusion: GPG是一种更高效的无需critic的策略梯度方法，在多种任务上表现优于PPO，具有更低的资源消耗和更高的并行效率。

Abstract: We introduce Group Policy Gradient (GPG), a family of critic-free
policy-gradient estimators for general MDPs. Inspired by the success of GRPO's
approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a
learned value function with a group-based Monte Carlo advantage estimator,
removing the memory, compute, and hyperparameter costs of training a critic
while preserving PPO's clipped-objective structure. We prove the consistency of
the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate
empirically that GPG matches or outperforms PPO on standard benchmarks. GPG
makes better use of parallel simulations, which, together with its critic-free
design, results in more efficient use of computational resources than PPO.

</details>


### [460] [From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning](https://arxiv.org/abs/2510.03690)
*Ali Azizpour,Reza Ramezanpour,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: 本文提出了一种统一框架MGCL，通过图子（graphons）显式建模图数据的混合结构，利用图矩（motif密度）聚类识别不同生成机制，并在此基础上改进图对比学习和Mixup数据增强方法，在多个基准数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图数据通常来自多种不同的生成分布，但现有的图表示学习方法（如图对比学习和Mixup）往往忽视这种混合结构，导致语义空间插值不合理和增强效果受限。

Method: 提出MGCL框架，使用图矩对图进行聚类以识别潜在的图子模型；基于估计的图子实现图混合感知的Mixup（GMAM）和模型自适应的图对比学习增强策略，并设计新的负采样目标，限制负样本来自不同模型。

Result: 理论方面，证明了小割距离图子生成的图在高概率下具有相似的motif密度；实验方面，MGCL在8个无监督学习数据集中取得最佳平均排名，GMAM在7个有监督学习数据集中有6个达到最先进准确率。

Conclusion: 显式建模图数据的混合生成结构能有效提升图表示学习性能，所提出的MGCL和GMAM方法在理论和实践中均表现出优越性。

Abstract: Real-world graph datasets often consist of mixtures of populations, where
graphs are generated from multiple distinct underlying distributions. However,
modern representation learning approaches, such as graph contrastive learning
(GCL) and augmentation methods like Mixup, typically overlook this mixture
structure. In this work, we propose a unified framework that explicitly models
data as a mixture of underlying probabilistic graph generative models
represented by graphons. To characterize these graphons, we leverage graph
moments (motif densities) to cluster graphs arising from the same model. This
enables us to disentangle the mixture components and identify their distinct
generative mechanisms. This model-aware partitioning benefits two key graph
learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data
augmentation technique that interpolates in a semantically valid space guided
by the estimated graphons, instead of assuming a single graphon per class. 2)
For GCL, it enables model-adaptive and principled augmentations. Additionally,
by introducing a new model-aware objective, our proposed approach (termed MGCL)
improves negative sampling by restricting negatives to graphs from other
models. We establish a key theoretical guarantee: a novel, tighter bound
showing that graphs sampled from graphons with small cut distance will have
similar motif densities with high probability. Extensive experiments on
benchmark datasets demonstrate strong empirical performance. In unsupervised
learning, MGCL achieves state-of-the-art results, obtaining the top average
rank across eight datasets. In supervised learning, GMAM consistently
outperforms existing strategies, achieving new state-of-the-art accuracy in 6
out of 7 datasets.

</details>


### [461] [REG: A Regularization Optimizer for Robust Training Dynamics](https://arxiv.org/abs/2510.03691)
*Zehua Liu,Han Wu,Xiaojin Fu,Shuqi Liu,Xiongwei Han,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 提出了一种新的优化器REG，通过使用行和列缩放（RACS）操作符替代Muon中的矩阵符号函数，实现了比AdamW更优的性能和稳定性，同时保持与AdamW训练范式的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的结构感知优化器如Muon由于依赖矩阵符号函数，在训练大语言模型时可能导致不稳定，并且在微调由AdamW预训练的模型时表现出不兼容性。

Method: 提出新的REG优化器，用行和列缩放（RACS）操作符代替Muon中的矩阵符号操作符，以较温和的方式正则化更新步骤。

Result: 实验表明，REG优化器在大语言模型训练中不仅比AdamW具有更好的性能和稳定性，而且在微调阶段避免了Muon出现的性能下降问题。

Conclusion: REG优化器是一种更简单、更兼容现有训练动态的优化方法，适合用于大语言模型的训练和微调。

Abstract: Optimizers are crucial for the efficient training of Large Language Models
(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers
like Muon have emerged, which regularize gradient updates by operating on
entire weight matrices. The Muon optimizer balances the gradient updates along
all the directions. However, Muon's reliance on the matrix sign function can
lead to training instability, exhibits incompatibility when fine-tuning models
pre-trained with AdamW. To address these limitations, we propose \textbf{REG},
a novel optimizer that replaces Muon's aggressive matrix sign operator with the
Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a
matrix, the RACS operator regularizes the update steps in a less drastic
manner, making it simpler to implement and more compatible with established
training dynamics. Through extensive empirical experiments on LLM training, we
demonstrate that our REG optimizer not only achieves superior performance and
stability over AdamW, but also maintains consistency with the AdamW training
paradigm. This consistency is particularly evident during the fine-tuning
stage, where REG optimizer avoids the performance degradation observed with
Muon.

</details>


### [462] [Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach](https://arxiv.org/abs/2510.03722)
*Qianxin Yi,Shao-Bo Lin,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 提出一种基于谱滤波的线性强化学习方法，在保证高性能的同时增强可解释性，理论分析表明其具有近最优的误差界，并在模拟环境和真实数据集上验证了决策质量和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法多关注性能而忽视可解释性，通常依赖事后解释；本文旨在设计一种既可解释又高性能的强化学习方法，以促进其在实际决策场景中的可信应用。

Method: 提出一种基于谱滤波的线性强化学习方法，扩展了基于岭回归的方法，并引入自适应正则化参数选择策略，通过偏差-方差权衡原则指导正则化设计，提升模型的可解释性与泛化能力。

Result: 理论分析给出了参数估计和泛化误差的近最优界；在模拟环境及快手、淘宝的真实数据集上，该方法在决策质量上优于或媲美现有基线，并通过可解释性分析展示了策略决策过程。

Conclusion: 该方法在保持高决策性能的同时增强了可解释性，有助于弥合强化学习理论与实际管理决策应用之间的差距，具备良好的准确性、适应性和可解释性。

Abstract: Reinforcement learning (RL) has been widely applied to sequential decision
making, where interpretability and performance are both critical for practical
adoption. Current approaches typically focus on performance and rely on post
hoc explanations to account for interpretability. Different from these
approaches, we focus on designing an interpretability-oriented yet
performance-enhanced RL approach. Specifically, we propose a spectral based
linear RL method that extends the ridge regression-based approach through a
spectral filter function. The proposed method clarifies the role of
regularization in controlling estimation error and further enables the design
of an adaptive regularization parameter selection strategy guided by the
bias-variance trade-off principle. Theoretical analysis establishes
near-optimal bounds for both parameter estimation and generalization error.
Extensive experiments on simulated environments and real-world datasets from
Kuaishou and Taobao demonstrate that our method either outperforms or matches
existing baselines in decision quality. We also conduct interpretability
analyses to illustrate how the learned policies make decisions, thereby
enhancing user trust. These results highlight the potential of our approach to
bridge the gap between RL theory and practical decision making, providing
interpretability, accuracy, and adaptability in management contexts.

</details>


### [463] [Personalized federated prototype learning in mixed heterogeneous data scenarios](https://arxiv.org/abs/2510.03726)
*Jiahao Zeng,Wolong Xing,Liangtao Shi,Xin Huang,Jialin Wang,Zhile Cao,Zhenkui Shi*

Main category: cs.LG

TL;DR: 提出了一种名为PFPL的新方法，用于混合异构场景下的联邦学习，通过构建个性化的无偏原型和引入一致性正则化来提升模型收敛性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在孤立的异构场景中导致特征或标签分布偏斜，而数据异质性本可提升模型性能，因此需要新方法充分利用这一特性。

Method: 在混合异构场景下构建每个客户端的个性化、无偏原型，并在本地更新阶段引入一致性正则化以对齐本地实例与其个性化原型。

Result: 在Digits和Office Caltech数据集上的实验验证了该方法的有效性，显著改善了损失函数的收敛性，并成功降低了通信成本。

Conclusion: PFPL方法通过利用数据异质性提供更丰富的领域知识和无偏收敛目标，在混合异构联邦学习场景中表现出优越性能。

Abstract: Federated learning has received significant attention for its ability to
simultaneously protect customer privacy and leverage distributed data from
multiple devices for model training. However, conventional approaches often
focus on isolated heterogeneous scenarios, resulting in skewed feature
distributions or label distributions. Meanwhile, data heterogeneity is actually
a key factor in improving model performance. To address this issue, we propose
a new approach called PFPL in mixed heterogeneous scenarios. The method
provides richer domain knowledge and unbiased convergence targets by
constructing personalized, unbiased prototypes for each client. Moreover, in
the local update phase, we introduce consistent regularization to align local
instances with their personalized prototypes, which significantly improves the
convergence of the loss function. Experimental results on Digits and Office
Caltech datasets validate the effectiveness of our approach and successfully
reduce the communication cost.

</details>


### [464] [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
*Yongfu Xue*

Main category: cs.LG

TL;DR: 提出了一种新的LoRA初始化策略IniLoRA，通过更有效地逼近原始模型权重来提升参数高效微调的性能，并引入两个变体进一步优化效果。


<details>
  <summary>Details</summary>
Motivation: LoRA初始化时低秩矩阵乘积为零，限制了对原模型权重的有效激活和利用，影响性能。

Method: 提出IniLoRA，初始化低秩矩阵以逼近原始模型权重，并设计两种变体IniLoRA-α和IniLoRA-β采用不同初始化方法增强性能。

Result: 实验表明，IniLoRA在多种模型和任务上均优于标准LoRA，且两个变体进一步提升了性能。

Conclusion: IniLoRA通过改进初始化策略有效提升了参数高效微调的性能，为LoRA类方法提供了更优的替代方案。

Abstract: The rapid development of parameter-efficient fine-tuning methods has
noticeably improved the efficiency of adapting large language models. Among
these, LoRA has gained widespread popularity due to its strong balance of
effectiveness and parameter efficiency. However, LoRA relies on initializing
two low-rank matrices whose product is zero, which limits its ability to
effectively activate and leverage the original model weights-creating a
potential bottleneck for optimal performance. To address this limitation, we
propose \textbf{IniLoRA}, a novel initialization strategy that initializes the
low-rank matrices to closely approximate the original model weights.
Experimental results indicate that IniLoRA achieves better performance than
LoRA across a range of models and tasks. Additionally, we introduce two
variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct
initialization methods to enhance performance further.

</details>


### [465] [Cost Efficient Fairness Audit Under Partial Feedback](https://arxiv.org/abs/2510.03734)
*Nirjhar Das,Mohit Sharma,Praharsh Nanavati,Kirankumar Shiragur,Amit Deshpande*

Main category: cs.LG

TL;DR: 本文研究在部分反馈下审计分类器公平性的问题，提出了一种新的数据标注成本模型，并设计了在黑箱和混合模型设置下的高效审计算法，显著降低了审计成本。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中（如贷款审批），只有被批准的个体才有真实标签（如还款结果），导致对分类器的公平性审计面临部分反馈问题。现有方法成本高且效率低，因此需要更有效的审计算法。

Method: 提出了两种审计设置：无数据分布假设的黑箱模型和特征与真实标签服从指数族混合分布的混合模型。基于截断样本学习和最大后验估计 oracle，设计了适用于多种公平性度量（如 demographic parity、equal opportunity）的审计算法。

Result: 在 Adult Income 和 Law School 等真实数据集上，所提算法比自然基线方法审计成本降低约 50%，且在混合模型下表现优于黑箱设置。

Conclusion: 本文提出的审计算法在不同模型设定下均实现了更高的成本效益，为实际应用中的公平性审计提供了可行且高效的解决方案。

Abstract: We study the problem of auditing the fairness of a given classifier under
partial feedback, where true labels are available only for positively
classified individuals, (e.g., loan repayment outcomes are observed only for
approved applicants). We introduce a novel cost model for acquiring additional
labeled data, designed to more accurately reflect real-world costs such as
credit assessment, loan processing, and potential defaults. Our goal is to find
optimal fairness audit algorithms that are more cost-effective than random
exploration and natural baselines.
  In our work, we consider two audit settings: a black-box model with no
assumptions on the data distribution, and a mixture model, where features and
true labels follow a mixture of exponential family distributions. In the
black-box setting, we propose a near-optimal auditing algorithm under mild
assumptions and show that a natural baseline can be strictly suboptimal. In the
mixture model setting, we design a novel algorithm that achieves significantly
lower audit cost than the black-box case. Our approach leverages prior work on
learning from truncated samples and maximum-a-posteriori oracles, and extends
known results on spherical Gaussian mixtures to handle exponential family
mixtures, which may be of independent interest. Moreover, our algorithms apply
to popular fairness metrics including demographic parity, equal opportunity,
and equalized odds. Empirically, we demonstrate strong performance of our
algorithms on real-world fair classification datasets like Adult Income and Law
School, consistently outperforming natural baselines by around 50% in terms of
audit cost.

</details>


### [466] [HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting](https://arxiv.org/abs/2510.03744)
*Qianfei Fan,Jiayu Wei,Peijun Zhu,Wensheng Ye,Meie Fang*

Main category: cs.LG

TL;DR: 本文提出了一种名为HydroFusion-LMF的统一框架，用于提升小流域长期日径流预测的准确性，通过可学习的分解、多专家融合和半监督多任务目标，在非平稳条件下实现了优于现有深度模型的性能。


<details>
  <summary>Details</summary>
Motivation: 小流域的日径流预测因包含漂移趋势、多尺度季节周期、 regime shifts 和稀疏极端事件而极具挑战，现有深度模型通常只关注单一特征且未能充分利用未标记数据，导致对 regime 的适应性不足。

Method: 提出HydroFusion-LMF框架：(i) 进行可学习的趋势-季节-残差分解以降低非平稳性；(ii) 将残差输入由线性细化、频域核、patch Transformer、循环记忆和动态归一化注意力组成的异构专家网络；(iii) 利用基于水文上下文（如年日相位、前期降水等）的门控机制融合专家输出；(iv) 采用半监督多任务目标进行训练，包括复合损失、极端值强调、对比对齐、伪标签等；可选地使用适配器或LoRA引入冻结的基础时间序列编码器。

Result: 在约10年的日尺度数据集上，HydroFusion-LMF取得MSE 1.0128 / MAE 0.5818，相较最强基线DLinear提升10.2% / 10.3%，相较平均基线提升24.6% / 17.1%，且同时降低了MSE和MAE。

Conclusion: HydroFusion-LMF在保持可解释性的同时显著提升了非平稳条件下的水文预测性能，推动了标签高效的学习方法在复杂水文预测中的应用。

Abstract: Accurate decade-scale daily runoff forecasting in small watersheds is
difficult because signals blend drifting trends, multi-scale seasonal cycles,
regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet,
PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single
facets and under-utilize unlabeled spans, limiting regime adaptivity. We
propose HydroFusion-LMF, a unified framework that (i) performs a learnable
trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes
residuals through a compact heterogeneous expert set (linear refinement,
frequency kernel, patch Transformer, recurrent memory, dynamically normalized
attention), (iii) fuses expert outputs via a hydrologic context-aware gate
conditioned on day-of-year phase, antecedent precipitation, local variance,
flood indicators, and static basin attributes, and (iv) augments supervision
with a semi-supervised multi-task objective (composite MSE/MAE + extreme
emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment,
augmentation consistency, variance-filtered pseudo-labeling). Optional adapter
/ LoRA layers inject a frozen foundation time-series encoder efficiently. On a
~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818,
improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean
baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions
relative to baselines. The framework balances interpretability (explicit
components, sparse gating) with performance, advancing label-efficient
hydrologic forecasting under non-stationarity.

</details>


### [467] [Neural Low-Discrepancy Sequences](https://arxiv.org/abs/2510.03745)
*Michael Etienne Van Huffel,Nathan Kirk,Makram Chahine,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 本文提出了Neural Low-Discrepancy Sequences（NeuroLDS），首个基于机器学习的低差异序列生成框架，通过两阶段学习过程显著优于传统方法，并在数值积分、机器人路径规划和科学机器学习等应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的低差异点集生成方法如MPMC无法扩展到低差异序列（LDS），而LDS在许多应用中要求每个前缀都具有低差异性，因此需要一种能够生成高质量LDS的新方法。

Method: 提出NeuroLDS，利用神经网络将索引映射为点，采用两阶段学习：先监督学习经典构造方法，再无监督微调以最小化各前缀的差异度。

Result: NeuroLDS在差异度指标上显著优于以往所有LDS构造方法，并在多个实际应用中验证了其有效性。

Conclusion: NeuroLDS是首个基于机器学习的低差异序列生成框架，展现了在生成质量和应用性能上的巨大优势，展示了机器学习在低差异序列构造中的广阔前景。

Abstract: Low-discrepancy points are designed to efficiently fill the space in a
uniform manner. This uniformity is highly advantageous in many problems in
science and engineering, including in numerical integration, computer vision,
machine perception, computer graphics, machine learning, and simulation.
Whereas most previous low-discrepancy constructions rely on abstract algebra
and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced
to exploit machine learning methods for generating point sets with lower
discrepancy than previously possible. However, MPMC is limited to generating
point sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,
sequences of points in which every prefix has low discrepancy, a property
essential for many applications. To address this limitation, we introduce
Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based
framework for generating LDS. Drawing inspiration from classical LDS, we train
a neural network to map indices to points such that the resulting sequences
exhibit minimal discrepancy across all prefixes. To this end, we deploy a
two-stage learning process: supervised approximation of classical constructions
followed by unsupervised fine-tuning to minimize prefix discrepancies. We
demonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a
significant margin with respect to discrepancy measures. Moreover, we
demonstrate the effectiveness of $NeuroLDS$ across diverse applications,
including numerical integration, robot motion planning, and scientific machine
learning. These results highlight the promise and broad significance of Neural
Low-Discrepancy Sequences. Our code can be found at
https://github.com/camail-official/neuro-lds.

</details>


### [468] [EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models](https://arxiv.org/abs/2510.03760)
*Ping Guo,Chenyu Zhu,Siyuan Chen,Fei Liu,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 本文提出了一个基于大语言模型的CUDA内核优化框架EvoEngineer，首次系统化地解决了CUDA内核优化中的性能与正确性平衡问题，在91个真实CUDA内核上实验显示其平均中位加速比达2.72倍，最高加速比达36.75倍。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在CUDA内核优化中缺乏统一的问题定义和评估标准，且通用代码演化方法难以满足CUDA对正确性的严格要求。

Method: 将CUDA内核优化形式化为具有明确目标、约束和评估指标的代码优化任务，并构建了首个系统性的LLM驱动代码演化框架EvoEngineer，用于指导优化策略的设计与调整。

Result: 在91个真实CUDA内核上进行实验，EvoEngineer实现了2.72倍的平均中位加速比，代码有效性达到69.8%，在性能和正确性上均优于现有方法；最高加速比达36.75倍，且在50个实现2倍以上加速的操作中有28个表现最佳。

Conclusion: EvoEngineer为LLM驱动的CUDA内核优化提供了可衡量、可复现的框架，有效平衡了性能提升与代码正确性，推动了该领域的系统化发展。

Abstract: CUDA kernel optimization has become a critical bottleneck for AI performance,
as deep learning training and inference efficiency directly depends on highly
optimized GPU kernels.
  Despite the promise of Large Language Models (LLMs) for automating kernel
optimization, this field suffers from a fragmented ecosystem of isolated and
incomparable approaches with unclear problem formulations.
  Furthermore, general-purpose LLM code evolution methods cannot meet strict
correctness requirements of CUDA kernel optimization.
  We address these fundamental challenges by first formalizing CUDA kernel
optimization as a code optimization task with a clear objective, constraints,
and evaluation metrics.
  We then establish the first systematic LLM-based code evolution framework,
EvoEngineer, that provides guidance for designing and adapting optimization
strategies to achieve a balance between performance and correctness.
  Finally, we implement a kernel optimization system based on this framework
and conduct extensive experiments on 91 real-world CUDA kernels.
  Our results demonstrate that EvoEngineer achieves a principled balance
between performance and correctness, with the highest averaged median speedup
of \textbf{2.72}$\times$ over baseline CUDA kernels and a code validity rate of
\textbf{69.8}\%, outperforming existing methods on both dimensions.
  Our method achieves a maximum speedup of \textbf{36.75}$\times$ among all
operations over PyTorch kernels and delivers the highest speedup on \textbf{28}
(\textbf{56.0\%}) of 50 operations that achieve over \textbf{2$\times$}
acceleration.

</details>


### [469] [Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation](https://arxiv.org/abs/2510.03782)
*Guofu Xie,Chen Zhang,Xiao Zhang,Yunsheng Shi,Ting Yao,Jun Xu*

Main category: cs.LG

TL;DR: 提出MAGE框架，通过两阶段模型融合与引导解码，解决多目标可控生成中的兼容性问题，实现更优的控制性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测试时难以有效适应多样化用户需求，参数级合并方法控制不直接且次优，而基于解码的引导方法存在空间开销大、依赖单个模型能力的问题。

Method: 提出MAGE框架：第一阶段通过动态融合多个骨干模型构建鲁棒的基础模型；第二阶段将显式与隐式价值模型融合为统一的引导代理，指导第一阶段基础模型的解码过程，并基于线性模式连通性分析模型融合机制。

Result: 实验证明MAGE在可控性、帕累托最优性能和适应性方面优于现有方法，验证了价值模型中线性模式连通性的有效性，并揭示了模型融合与预测集成的关系。

Conclusion: MAGE通过结合模型融合与引导解码，在多目标可控生成中实现了更高效、灵活和可控的生成能力，解决了指导模型与基础模型之间的兼容性问题。

Abstract: Adapting to diverse user needs at test time is a key challenge in
controllable multi-objective generation. Existing methods are insufficient:
merging-based approaches provide indirect, suboptimal control at the parameter
level, often disregarding the impacts of multiple objectives. While
decoding-based guidance is more direct, it typically requires aggregating
logits from multiple expert models, incurring significant space overhead and
relying heavily on individual model capacity. To address these issues, we
introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model
merging for guided decoding. We first identify a critical compatibility problem
between the guidance and base models. In Stage 1, MAGE resolves this by
dynamically constructing a more robust base model, merging a series of backbone
models that account for multiple objectives. In Stage 2, we merge explicit and
implicit value models into a unified guidance proxy, which then steers the
decoding of the base model from Stage 1. Our analysis empirically validates
Linear Mode Connectivity (LMC) in value models, explores the relationship
between model merging and prediction ensembling, and demonstrates the enhanced
controllability afforded by our approach. Extensive experiments show that our
method outperforms existing approaches, achieving superior controllability,
Pareto-optimal performance, and enhanced adaptability.

</details>


### [470] [Allocation of Parameters in Transformers](https://arxiv.org/abs/2510.03784)
*Ruoxi Yu,Haotian Jiang,Jingpu Cheng,Penghao Yu,Qianxiao Li,Zhong Li*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型中注意力头和头维度在各层间的参数分配问题，从理论上分析了早期层的信息提取作用，并揭示了softmax激活的饱和现象，提出了一种基于理论指导的高效参数分配策略。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在多种应用中取得了成功，但其模型效率的理论基础仍不充分，尤其是注意力头和头维度的参数分配对表达能力和效率的影响尚不清楚。

Method: 通过数学分析近似视角下早期层的作用，理论刻画了在固定参数预算下注意力头数量与头维度之间的权衡关系，并证明了softmax激活的饱和行为。结合理论与实验验证，提出了跨层的注意力头和维度分配策略。

Result: 发现了头维度增加会导致学习误差的收益递减（饱和现象），尤其是在长序列上；理论和实验表明后期层可以用更少的参数高效运行；提出了提升模型效率的参数分配原则。

Conclusion: 合理的跨层参数分配（如减少后期层的头维度）可以在保持表达能力的同时显著提升Transformer的效率，为设计高效的Transformer架构提供了理论依据。

Abstract: Transformers have achieved remarkable successes across a wide range of
applications, yet the theoretical foundation of their model efficiency remains
underexplored. In this work, we investigate how the model parameters -- mainly
attention heads and head dimensions -- should be allocated across layers to
balance expressivity and efficiency. We first provide mathematical analysis on
the role of early layers in information extraction from an approximation
perspective, with a theoretical characterization on the trade-off between the
number of heads and head dimension under a fixed parameter budget. In addition,
we uncover and prove the \emph{saturation} behavior of softmax activations:
Continuously increasing head dimensions can lead to diminishing returns in
learning errors, particularly for long sequences. Supported by both theory and
experiments, this saturation pattern suggests that later layers can operate
more efficiently with reduced parameters. Combining these insights, we propose
principled strategies for allocating attention heads and dimensions across
Transformers' layers, shedding light on theoretically-grounded model efficiency
of Transformer-based architectures.

</details>


### [471] [Robust Batched Bandits](https://arxiv.org/abs/2510.03798)
*Yunwen Guo,Yunlun Shu,Gongyi Zhuo,Tianyu Wang*

Main category: cs.LG

TL;DR: 本文研究了具有重尾奖励的批量多臂赌博机问题，提出了适用于重尾分布的鲁棒算法，并揭示了一个反直觉现象：在实例无关和Lipschitz连续设定下，奖励尾部越重，实现近似最优遗憾所需的批次越少；而在实例相关设定下，所需批次数量与尾部厚度无关。


<details>
  <summary>Details</summary>
Motivation: 现有批量多臂赌博机研究主要假设奖励分布为轻尾，但许多实际场景（如临床结果）呈现重尾特性，因此需要填补这一理论与实践之间的空白。

Method: 提出了针对重尾奖励的鲁棒批量赌博机算法，涵盖有限臂和Lipschitz连续两种设定，并通过理论分析探讨不同尾部特征对所需批次数的影响。

Result: 发现在实例无关和Lipschitz设定下，重尾奖励反而减少所需批次数以达到近优遗憾；而在实例依赖设定下，所需批次数不受尾部 heaviness 影响。

Conclusion: 重尾奖励在批量多臂赌博机中对批次数的需求具有非直观影响，取决于问题的设定类型，这为实际应用中的批次设计提供了新的理论依据。

Abstract: The batched multi-armed bandit (MAB) problem, in which rewards are collected
in batches, is crucial for applications such as clinical trials. Existing
research predominantly assumes light-tailed reward distributions, yet many
real-world scenarios, including clinical outcomes, exhibit heavy-tailed
characteristics. This paper bridges this gap by proposing robust batched bandit
algorithms designed for heavy-tailed rewards, within both finite-arm and
Lipschitz-continuous settings. We reveal a surprising phenomenon: in the
instance-independent regime, as well as in the Lipschitz setting,
heavier-tailed rewards necessitate a smaller number of batches to achieve
near-optimal regret. In stark contrast, for the instance-dependent setting, the
required number of batches to attain near-optimal regret remains invariant with
respect to tail heaviness.

</details>


### [472] [Curriculum-Augmented GFlowNets For mRNA Sequence Generation](https://arxiv.org/abs/2510.03811)
*Aya Laajil,Abduragim Shtanchaev,Sajan Muhammad,Eric Moulines,Salem Lahlou*

Main category: cs.LG

TL;DR: 提出了一种结合课程学习的生成流网络（CAGFN），用于从头设计mRNA序列，通过逐步增加序列长度的课程策略，提升多目标优化性能和生物合理性。


<details>
  <summary>Details</summary>
Motivation: mRNA序列设计面临巨大的搜索空间和多目标权衡问题，传统生成流网络因稀疏奖励和长视野问题难以有效训练。

Method: 提出CAGFN方法，将基于长度的课程学习引入多目标生成流网络，并构建了一个新的mRNA设计环境，支持给定目标蛋白序列和生物目标组合下的序列生成。

Result: 在多个mRNA设计任务中，CAGFN在帕累托性能、生物合理性和序列多样性方面表现更优，收敛更快，并能泛化到分布外序列。

Conclusion: CAGFN通过课程学习有效提升了生成流网络在mRNA序列设计中的性能，为治疗性序列设计提供了新思路。

Abstract: Designing mRNA sequences is a major challenge in developing next-generation
therapeutics, since it involves exploring a vast space of possible nucleotide
combinations while optimizing sequence properties like stability, translation
efficiency, and protein expression. While Generative Flow Networks are
promising for this task, their training is hindered by sparse, long-horizon
rewards and multi-objective trade-offs. We propose Curriculum-Augmented
GFlowNets (CAGFN), which integrate curriculum learning with multi-objective
GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based
curriculum that progressively adapts the maximum sequence length guiding
exploration from easier to harder subproblems. We also provide a new mRNA
design environment for GFlowNets which, given a target protein sequence and a
combination of biological objectives, allows for the training of models that
generate plausible mRNA candidates. This provides a biologically motivated
setting for applying and advancing GFlowNets in therapeutic sequence design. On
different mRNA design tasks, CAGFN improves Pareto performance and biological
plausibility, while maintaining diversity. Moreover, CAGFN reaches
higher-quality solutions faster than a GFlowNet trained with random sequence
sampling (no curriculum), and enables generalization to out-of-distribution
sequences.

</details>


### [473] [Detecting Invariant Manifolds in ReLU-Based RNNs](https://arxiv.org/abs/2510.03814)
*Lukas Eisenmann,Alena Brändle,Zahra Monfared,Daniel Durstewitz*

Main category: cs.LG

TL;DR: 提出一种新算法来检测分段线性RNN中稳定和不稳定流形，用于揭示多稳态和混沌动力学，进而解析其动态行为。


<details>
  <summary>Details</summary>
Motivation: 理解训练后的RNN如何产生行为对于科学和医学应用及可解释AI至关重要，特别是其状态空间的拓扑与几何特性影响动力学表现。

Method: 开发了一种针对使用ReLU激活函数的分段线性RNN（PLRNN）的新算法，用于检测周期点的稳定与不稳定流形，并追踪吸引子盆地边界及寻找同宿点。

Result: 算法成功识别了PLRNN中的多稳态结构和混沌动力学（通过同宿点交集），并在真实皮层神经元电生理数据上验证了方法的有效性。

Conclusion: 该方法为分析PLRNN的复杂动力学提供了有力工具，揭示了其内在的多稳态与混沌机制，有助于提升对RNN行为的理解和可解释性。

Abstract: Recurrent Neural Networks (RNNs) have found widespread applications in
machine learning for time series prediction and dynamical systems
reconstruction, and experienced a recent renaissance with improved training
algorithms and architectural designs. Understanding why and how trained RNNs
produce their behavior is important for scientific and medical applications,
and explainable AI more generally. An RNN's dynamical repertoire depends on the
topological and geometrical properties of its state space. Stable and unstable
manifolds of periodic points play a particularly important role: They dissect a
dynamical system's state space into different basins of attraction, and their
intersections lead to chaotic dynamics with fractal geometry. Here we introduce
a novel algorithm for detecting these manifolds, with a focus on
piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as
their activation function. We demonstrate how the algorithm can be used to
trace the boundaries between different basins of attraction, and hence to
characterize multistability, a computationally important property. We further
show its utility in finding so-called homoclinic points, the intersections
between stable and unstable manifolds, and thus establish the existence of
chaos in PLRNNs. Finally we show for an empirical example, electrophysiological
recordings from a cortical neuron, how insights into the underlying dynamics
could be gained through our method.

</details>


### [474] [TROLL: Trust Regions improve Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2510.03817)
*Philipp Becker,Niklas Freymuth,Serge Thilges,Fabian Otto,Gerhard Neumann*

Main category: cs.LG

TL;DR: 提出了一种用于大语言模型的新型离散可微信任域投影方法TROLL，替代PPO中的clip目标，在训练速度、稳定性和最终性能上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: PPO中的剪切机制是KL信任域的粗略近似，常导致更新不稳定和性能次优，需要更原则性的替代方案。

Method: 引入一种新的离散可微信任域投影，对模型最重要的稀疏token logits施加token级别的KL约束，作为PPO中clip目标的直接替代。

Result: 在多个数据集、模型族和优势估计方法上，TROLL相比PPO-like剪切方法在训练速度、稳定性和最终成功率方面表现更优。

Conclusion: TROLL为大语言模型的强化学习提供了一种更稳定且高效的替代方案，不改变推理行为，具有广泛适用性。

Abstract: On-policy Reinforcement Learning (RL) with PPO-like clip objectives has
become the standard choice for reward-based fine-tuning of large language
models (LLMs). Although recent work has explored improved estimators of
advantages and normalization, the clipping mechanism itself has remained
untouched. Originally introduced as a proxy for principled KL-based trust
regions, clipping is a crude approximation that often causes unstable updates
and suboptimal performance. We replace the clip objective with a novel discrete
differentiable trust region projection, which provides principled token-level
KL constraints. The projection operates on a sparse subset of the model's most
important token logits to balance computational cost and projection
effectiveness. Our approach, Trust Region Optimization for Large Language
Models (TROLL), serves as a direct replacement for PPO-like clipping during
training and does not alter the model's inference behavior. Across datasets,
model families, and advantage-estimation methods, TROLL consistently
outperforms PPO-like clipping in terms of training speed, stability, and final
success rates.

</details>


### [475] [Proximal Diffusion Neural Sampler](https://arxiv.org/abs/2510.03824)
*Wei Guo,Jaemoo Choi,Yuchen Zhu,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 提出了一种基于 proximal 点方法的扩散神经采样器（PDNS），通过在路径测度空间上分解学习过程，有效解决多模态分布下的模式坍塌问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在处理多模态、存在显著势垒的目标分布时容易出现模式坍塌，训练困难，因此需要一种能更好探索各模态的采样方法。

Method: 将扩散采样建模为路径测度上的随机最优控制问题，采用 proximal 点方法将其分解为一系列渐进子问题，并在每步中使用加权去噪交叉熵（WDCE）目标进行实例化。

Result: 在连续和离散采样任务（包括分子动力学和统计物理中的挑战性场景）中验证了PDNS的有效性和鲁棒性，表现出更强的跨模式探索能力。

Conclusion: PDNS通过渐进式优化路径测度，有效缓解了多模态分布下的模式坍塌问题，为扩散模型采样提供了一种稳定且可扩展的框架。

Abstract: The task of learning a diffusion-based neural sampler for drawing samples
from an unnormalized target distribution can be viewed as a stochastic optimal
control problem on path measures. However, the training of neural samplers can
be challenging when the target distribution is multimodal with significant
barriers separating the modes, potentially leading to mode collapse. We propose
a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that
addresses these challenges by tackling the stochastic optimal control problem
via proximal point method on the space of path measures. PDNS decomposes the
learning process into a series of simpler subproblems that create a path
gradually approaching the desired distribution. This staged procedure traces a
progressively refined path to the desired distribution and promotes thorough
exploration across modes. For a practical and efficient realization, we
instantiate each proximal step with a proximal weighted denoising cross-entropy
(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS
through extensive experiments on both continuous and discrete sampling tasks,
including challenging scenarios in molecular dynamics and statistical physics.

</details>


### [476] [HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control](https://arxiv.org/abs/2510.03830)
*Alex Durkin,Jasper Stolte,Mehmet Mercangöz*

Main category: cs.LG

TL;DR: 本文提出了一种名为HOFLON的混合离线学习与在线优化方法，用于自动化连续过程工厂的启动和产品等级转换操作，能够在缺乏过程模型的情况下超越历史专家表现。


<details>
  <summary>Details</summary>
Motivation: 由于资深操作员退休导致工厂缺乏执行启动和等级转换所需的经验知识，且传统离线强化学习在分布偏移和价值高估方面存在局限，因此需要一种更鲁棒的方法来自动化这些关键操作。

Method: HOFLON结合了离线学习与在线优化：离线阶段学习表示可行区域的潜在数据流形和长视野Q-批评器；在线阶段通过最大化Q-批评器并惩罚偏离流形和操作变量剧烈变化来进行一步优化。

Result: 在聚合反应器启动和造纸机等级转换两个工业案例中，HOFLON不仅优于领先的离线RL算法IQL，而且平均累积奖励超过了历史数据中的最佳操作表现。

Conclusion: HOFLON能够有效捕捉并超越人类专家在复杂工业过渡操作中的能力，具备在无过程模型条件下实现高性能自动化的潜力。

Abstract: Start-ups and product grade-changes are critical steps in continuous-process
plant operation, because any misstep immediately affects product quality and
drives operational losses. These transitions have long relied on manual
operation by a handful of expert operators, but the progressive retirement of
that workforce is leaving plant owners without the tacit know-how needed to
execute them consistently. In the absence of a process model, offline
reinforcement learning (RL) promises to capture and even surpass human
expertise by mining historical start-up and grade-change logs, yet standard
offline RL struggles with distribution shift and value-overestimation whenever
a learned policy ventures outside the data envelope. We introduce HOFLON
(Hybrid Offline Learning + Online Optimization) to overcome those limitations.
Offline, HOFLON learns (i) a latent data manifold that represents the feasible
region spanned by past transitions and (ii) a long-horizon Q-critic that
predicts the cumulative reward from state-action pairs. Online, it solves a
one-step optimization problem that maximizes the Q-critic while penalizing
deviations from the learned manifold and excessive rates of change in the
manipulated variables. We test HOFLON on two industrial case studies: a
polymerization reactor start-up and a paper-machine grade-change problem, and
benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.
In both plants HOFLON not only surpasses IQL but also delivers, on average,
better cumulative rewards than the best start-up or grade-change observed in
the historical data, demonstrating its potential to automate transition
operations beyond current expert capability.

</details>


### [477] [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
*Wenhao Deng,Long Wei,Chenglei Yu,Tailin Wu*

Main category: cs.LG

TL;DR: 本文提出了一种名为RAPO的奖励感知策略优化算法，通过使用前向KL惩罚和重加权参考策略来增强大语言模型在数学问题求解中的探索能力，克服了传统反向KL散度正则化导致的探索受限问题。实验表明，RAPO在AIME2024和AIME2025上显著提升了性能，突破了基础模型的性能上限。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习与可验证奖励的方法在增加采样预算时性能提升有限，主要受限于反向KL散度正则化的模式寻求特性，限制了策略在基础模型支持区域外的探索。

Method: 提出RAPO算法：1）用前向KL惩罚替代反向KL惩罚以促进分布外探索；2）对参考策略进行重加权以实现自适应的分布内探索。在Qwen2.5-3B和7B模型上使用8K SimpleRL-Zero数据集训练，无需监督微调。

Result: RAPO在AIME2024和AIME2025基准上显著提升了问题解决性能，能够解决以往难以处理的问题，并打破了基础模型的性能瓶颈。

Conclusion: RAPO有效缓解了传统KL正则化带来的探索局限，增强了大语言模型在复杂推理任务中的搜索能力，推动了强化学习与可验证奖励方法的发展。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced
the reasoning capabilities of large language models (LLMs), particularly for
mathematical problem solving. However, a fundamental limitation remains: as the
sampling budget increases, the advantage of RLVR-trained models over their
pretrained bases often diminishes or even vanishes, revealing a strong
dependence on the base model's restricted search space. We attribute this
phenomenon to the widespread use of the reverse Kullback-Leibler (KL)
divergence regularizer, whose mode-seeking behavior keeps the policy trapped
inside the base model's support region and hampers wider exploration. To
address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an
algorithm to promote broader yet focused exploration. Our method (i) utilizes
the forward KL penalty to replace the reverse KL penalty for
out-of-distribution exploration, and (ii) reweights the reference policy to
facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B
models with RAPO on the 8K SimpleRL-Zero dataset, without supervised
fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO
consistently improves problem-solving performance. Notably, RAPO enables models
to surpass the base model's performance ceiling and solves previously
intractable problems, advancing the frontier of RLVR for challenging reasoning
tasks.

</details>


### [478] [Technical note on Fisher Information for Robust Federated Cross-Validation](https://arxiv.org/abs/2510.03838)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: 提出了一种名为FIRE的方法，利用近似Fisher信息来累积由数据碎片化引起的协变量偏移差异，从而实现可扩展的分布对齐，在偏移验证集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于数据在时间和空间上的碎片化导致训练分布与验证分布不一致，模型性能下降。

Method: 通过近似Fisher信息累积每个数据片段相对于全局训练分布的协变量偏移差异，并将其作为损失惩罚项用于分布对齐。

Result: FIRE在偏移验证集上比重要性加权基线最多提升5.1%，比联邦学习基线最多提升5.3%。

Conclusion: FIRE是一种计算上更可行且有效的方法，能够缓解因数据碎片化导致的协变量偏移问题，提升模型在分散数据下的泛化性能。

Abstract: When training data are fragmented across batches or federated-learned across
different geographic locations, trained models manifest performance
degradation. That degradation partly owes to covariate shift induced by data
having been fragmented across time and space and producing dissimilar empirical
training distributions. Each fragment's distribution is slightly different to a
hypothetical unfragmented training distribution of covariates, and to the
single validation distribution. To address this problem, we propose Fisher
Information for Robust fEderated validation (\textbf{FIRE}). This method
accumulates fragmentation-induced covariate shift divergences from the global
training distribution via an approximate Fisher information. That term, which
we prove to be a more computationally-tractable estimate, is then used as a
per-fragment loss penalty, enabling scalable distribution alignment. FIRE
outperforms importance weighting benchmarks by $5.1\%$ at maximum and federated
learning (FL) benchmarks by up to $5.3\%$ on shifted validation sets.

</details>


### [479] [Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting](https://arxiv.org/abs/2510.03839)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: M-FISHER 是一种用于流数据中顺序分布偏移检测与稳定适应的理论框架，结合非一致性分数的指数鞅和Fisher预处理更新，实现 anytime 有效的检测和几何稳定的自适应。


<details>
  <summary>Details</summary>
Motivation: 在流数据环境中，分布偏移的检测和适应需要在任意时间点保持统计有效性，并确保自适应过程的稳定性与最优性。

Method: 使用非一致性分数构建指数鞅，结合Ville不等式实现时间一致的误报控制；利用Fisher预处理更新提示参数，实现分布流形上的自然梯度下降。

Result: 实现了时间一致的假阳性控制，检测延迟上界为 O(log(1/δ)/Γ)；证明了Fisher预处理更新在KL散度最小化下的局部最优性和几何稳定性。

Conclusion: M-FISHER 提供了一个原理性强、统计有效且几何稳定的框架，适用于协变量偏移下的序列决策问题。

Abstract: We present a theoretical framework for M-FISHER, a method for sequential
distribution shift detection and stable adaptation in streaming data. For
detection, we construct an exponential martingale from non-conformity scores
and apply Ville's inequality to obtain time-uniform guarantees on false alarm
control, ensuring statistical validity at any stopping time. Under sustained
shifts, we further bound the expected detection delay as
$\mathcal{O}(\log(1/\delta)/\Gamma)$, where $\Gamma$ reflects the post-shift
information gain, thereby linking detection efficiency to distributional
divergence. For adaptation, we show that Fisher-preconditioned updates of
prompt parameters implement natural gradient descent on the distributional
manifold, yielding locally optimal updates that minimize KL divergence while
preserving stability and parameterization invariance. Together, these results
establish M-FISHER as a principled approach for robust, anytime-valid detection
and geometrically stable adaptation in sequential decision-making under
covariate shift.

</details>


### [480] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: 本文提出了LLM Chemistry框架，用于衡量多个大语言模型（LLMs）协作时的协同或对抗行为，揭示了模型间‘化学反应’对集体性能的影响，并提出量化方法与最优模型集成推荐算法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM协作方法缺乏对模型间是否真正互补或冲突的分析，导致无法有效提升系统性能。

Method: 形式化定义了LLM间的‘chemistry’概念，提出通过分析交互依赖关系来量化该效应的算法，并据此推荐最优模型组合。

Result: 理论分析表明，在模型异构性高、任务类型复杂、组规模适中的情况下，LLM间的chemistry效应更显著；在分类、摘要和程序修复任务上的实验初步验证了这一结论。

Conclusion: LLM Chemistry可作为多LLM系统的诊断指标和集成推荐基础，有助于提升多模型协作的准确性与鲁棒性。

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [481] [On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records](https://arxiv.org/abs/2510.03844)
*Sarah C. Lotspeich,Abbey Collins,Brian J. Wells,Ashish K. Khanna,Joseph Rigdon,Lucy D'Agostino McGowan*

Main category: cs.LG

TL;DR: 本研究开发了一种基于ICD-10代码和临床知识图谱（roadmap）的算法，利用大语言模型优化辅助诊断列表，以自动化方式恢复电子健康记录中的缺失数据，结果表明其准确性可媲美专家人工审查，且具备大规模应用潜力。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）常存在数据缺失和错误问题，传统人工图表审查虽有效但成本高、耗时长，限制了其广泛应用，因此需要一种可扩展、高效的自动化方法来恢复缺失数据。

Method: 基于先前研究中的‘增强型’图表审查协议，构建以ICD-10代码为基础的临床知识图谱（roadmap），并结合大语言模型（LLM）与临床专家知识迭代优化该图谱；在100名患者样本中评估不同版本图谱的算法性能，并在1000名患者的大样本中应用最终算法。

Result: 算法在不同图谱版本下恢复的缺失数据量等于甚至超过专家人工审查；最终结合LLM建议并经临床确认的图谱在大规模样本中表现出良好的可行性和准确性。

Conclusion: 由临床知识驱动并结合大语言模型优化的算法能够以接近专家水平的准确度自动恢复EHR中的缺失数据，具有良好的可扩展性，未来有望拓展至其他数据质量维度（如合理性）的监测。

Abstract: Objective: Electronic health records (EHR) data are prone to missingness and
errors. Previously, we devised an "enriched" chart review protocol where a
"roadmap" of auxiliary diagnoses (anchors) was used to recover missing values
in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a
missing hemoglobin A1c value would be considered unhealthy). Still, chart
reviews are expensive and time-intensive, which limits the number of patients
whose data can be reviewed. Now, we investigate the accuracy and scalability of
a roadmap-driven algorithm, based on ICD-10 codes (International Classification
of Diseases, 10th revision), to mimic expert chart reviews and recover missing
values. Materials and Methods: In addition to the clinicians' original roadmap
from our previous work, we consider new versions that were iteratively refined
using large language models (LLM) in conjunction with clinical expertise to
expand the list of auxiliary diagnoses. Using chart reviews for 100 patients
from the EHR at an extensive learning health system, we examine algorithm
performance with different roadmaps. Using the larger study of $1000$ patients,
we applied the final algorithm, which used a roadmap with clinician-approved
additions from the LLM. Results: The algorithm recovered as much, if not more,
missing data as the expert chart reviewers, depending on the roadmap.
Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing
EHR data with similar accuracy to chart reviews and can feasibly be applied to
large samples. Extending them to monitor other dimensions of data quality
(e.g., plausability) is a promising future direction.

</details>


### [482] [On Provable Benefits of Muon in Federated Learning](https://arxiv.org/abs/2510.03866)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 本文提出了FedMuon算法，研究了Muon优化器在联邦学习中的性能，并建立了其在非凸问题上的收敛速率。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在多种应用中表现出色，但其在联邦学习中的有效性尚未被探索。

Method: 提出FedMuon算法，进行理论分析并验证其收敛性。

Result: 理论分析表明FedMuon具有正交归一化更新方向，学习率不依赖于问题特定参数，并能自然应对重尾噪声；实验验证了其在多种神经网络架构上的有效性。

Conclusion: FedMuon在联邦学习中表现优异，具有良好的理论性质和实际效果。

Abstract: The recently introduced optimizer, Muon, has gained increasing attention due
to its superior performance across a wide range of applications. However, its
effectiveness in federated learning remains unexplored. To address this gap,
this paper investigates the performance of Muon in the federated learning
setting. Specifically, we propose a new algorithm, FedMuon, and establish its
convergence rate for nonconvex problems. Our theoretical analysis reveals
multiple favorable properties of FedMuon. In particular, due to its
orthonormalized update direction, the learning rate of FedMuon is independent
of problem-specific parameters, and, importantly, it can naturally accommodate
heavy-tailed noise. The extensive experiments on a variety of neural network
architectures validate the effectiveness of the proposed algorithm.

</details>


### [483] [Optimal Scaling Needs Optimal Norm](https://arxiv.org/abs/2510.03871)
*Oleg Filatov,Jiangtao Wang,Jan Ebert,Stefan Kesselheim*

Main category: cs.LG

TL;DR: 本文发现，在模型和数据集联合缩放时，最优超参数选择由输出层的算子范数这一不变量决定，该范数在不同规模下保持恒定（称为范数迁移），并提供了Scion优化器下学习率与批量大小随数据集规模变化的首个测量结果，同时发布大规模实验实现与日志。


<details>
  <summary>Details</summary>
Motivation: 尽管在模型和数据集缩放下的超参数迁移方面已有进展，但缺乏统一的解释性原理。本文旨在寻找一个统一的规律来解释最优超参数的选择。

Method: 使用Scion优化器，通过大规模实验（最大13亿参数、1380亿token）分析模型在不同规模下的最优学习率与批量大小组合，并测量其输出层算子范数，探索其不变性及充分条件。

Result: 发现最优(η*, B*)组合始终具有相同的输出层算子范数，提出‘范数迁移’现象；该条件必要但不充分；首次测得Scion中(η*, B*)随数据集规模的缩放规律，且与Adam一致；逐层调参显示输出层最敏感，隐藏层受益于较低学习率。

Conclusion: 输出层算子范数是跨尺度超参数优化的关键不变量，为大模型训练中的学习率和批量大小选择提供了可解释的指导原则，并通过Disco实现实验可复现性。

Abstract: Despite recent progress in optimal hyperparameter transfer under model and
dataset scaling, no unifying explanatory principle has been established. Using
the Scion optimizer, we discover that joint optimal scaling across model and
dataset sizes is governed by a single invariant: the operator norm of the
output layer. Across models with up to 1.3B parameters trained on up to 138B
tokens, the optimal learning rate/batch size pair $(\eta^{\ast}, B^{\ast})$
consistently has the same operator norm value - a phenomenon we term norm
transfer. This constant norm condition is necessary but not sufficient: while
for each dataset size, multiple $(\eta, B)$ reach the optimal norm, only a
unique $(\eta^{\ast}, B^{\ast})$ achieves the best loss. As a sufficient
condition, we provide the first measurement of $(\eta^{\ast}, B^{\ast})$
scaling with dataset size for Scion, and find that the scaling rules are
consistent with those of the Adam optimizer. Tuning per-layer-group learning
rates also improves model performance, with the output layer being the most
sensitive and hidden layers benefiting from lower learning rates. We provide
practical insights on norm-guided optimal scaling and release our Distributed
Scion (Disco) implementation with logs from over two thousand runs to support
research on LLM training dynamics at scale.

</details>


### [484] [BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty](https://arxiv.org/abs/2510.03893)
*Akshay Kudva,Joel A. Paulson*

Main category: cs.LG

TL;DR: 本文提出了BONSAI，一种结合部分结构信息的鲁棒贝叶斯优化框架，用于在不确定性下优化复杂工程系统，相比现有方法具有更高的样本效率和求解质量。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒优化方法依赖已知问题结构，难以应用于高保真仿真环境；现有鲁棒贝叶斯优化方法忽略可用结构信息且难以扩展到高维问题。

Method: 提出BONSAI框架，将目标函数表示为白盒与黑盒组件互联的有向图，并设计基于Thompson采样的可扩展采集函数，利用梯度方法高效优化。

Result: 在多个合成与真实案例（包括过程系统工程）中验证，BONSAI相比现有方法在样本效率和鲁棒解质量上表现更优。

Conclusion: BONSAI通过融合部分结构信息和鲁棒贝叶斯优化，提升了复杂系统在不确定性下的优化能力，具有实际工程应用价值。

Abstract: Optimal design under uncertainty remains a fundamental challenge in advancing
reliable, next-generation process systems. Robust optimization (RO) offers a
principled approach by safeguarding against worst-case scenarios across a range
of uncertain parameters. However, traditional RO methods typically require
known problem structure, which limits their applicability to high-fidelity
simulation environments. To overcome these limitations, recent work has
explored robust Bayesian optimization (RBO) as a flexible alternative that can
accommodate expensive, black-box objectives. Existing RBO methods, however,
generally ignore available structural information and struggle to scale to
high-dimensional settings. In this work, we introduce BONSAI (Bayesian
Optimization of Network Systems under uncertAInty), a new RBO framework that
leverages partial structural knowledge commonly available in simulation-based
models. Instead of treating the objective as a monolithic black box, BONSAI
represents it as a directed graph of interconnected white- and black-box
components, allowing the algorithm to utilize intermediate information within
the optimization process. We further propose a scalable Thompson sampling-based
acquisition function tailored to the structured RO setting, which can be
efficiently optimized using gradient-based methods. We evaluate BONSAI across a
diverse set of synthetic and real-world case studies, including applications in
process systems engineering. Compared to existing simulation-based RO
algorithms, BONSAI consistently delivers more sample-efficient and
higher-quality robust solutions, highlighting its practical advantages for
uncertainty-aware design in complex engineering systems.

</details>


### [485] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: 本文提出了专为扩散大语言模型（dLLMs）设计的Amortized Group Relative Policy Optimization（AGRPO），这是首个可实现无偏策略梯度估计的在线强化学习算法，并在数学推理任务中显著优于基线模型和现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习后训练方法不适用于扩散大语言模型，且缺乏理论基础，因此需要一种适配dLLMs结构的新型强化学习算法。

Method: 提出AGRPO算法，采用蒙特卡洛采样计算无偏策略梯度估计，是一种基于策略梯度的、针对dLLMs的非自回归生成框架的在线强化学习方法。

Result: 在GSM8K上最高提升7.6%，在Countdown任务上性能提升3.8倍，优于LLaDA-8B-Instruct基线及diffu-GRPO等现有方法，且在不同推理步数下均保持优势。

Conclusion: 在线强化学习可以被有原则地扩展到扩散大语言模型，AGRPO在理论上严谨且实践中有效，为dLLMs的后训练开辟了新路径。

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [486] [LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis](https://arxiv.org/abs/2510.03904)
*Hangting Ye,Jinmeng Li,He Zhao,Mingchen Zhuge,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: 提出LLM-DAS框架，利用大模型生成针对现有异常检测器弱点的“难检测”异常数据合成程序，提升检测器鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据异常检测方法依赖对异常模式的假设，且大模型直接应用于表格数据存在异构数据处理困难和隐私风险。

Method: 将大模型从“数据处理者”转为“算法设计者”，通过分析检测器的高层描述理解其弱点，生成与数据无关的Python代码来合成难检测异常，增强训练数据。

Result: 在36个TAD基准上实验表明，LLM-DAS能持续提升主流检测器的性能。

Conclusion: LLM-DAS通过程序化合成桥接大模型推理与传统异常检测算法，提供一种可扩展、有效且保护隐私的方法来弥补现有检测器的逻辑盲点。

Abstract: Existing anomaly detection (AD) methods for tabular data usually rely on some
assumptions about anomaly patterns, leading to inconsistent performance in
real-world scenarios. While Large Language Models (LLMs) show remarkable
reasoning capabilities, their direct application to tabular AD is impeded by
fundamental challenges, including difficulties in processing heterogeneous data
and significant privacy risks. To address these limitations, we propose
LLM-DAS, a novel framework that repositions the LLM from a ``data processor''
to an ``algorithmist''. Instead of being exposed to raw data, our framework
leverages the LLM's ability to reason about algorithms. It analyzes a
high-level description of a given detector to understand its intrinsic
weaknesses and then generates detector-specific, data-agnostic Python code to
synthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities.
This generated synthesis program, which is reusable across diverse datasets, is
then instantiated to augment training data, systematically enhancing the
detector's robustness by transforming the problem into a more discriminative
two-class classification task. Extensive experiments on 36 TAD benchmarks show
that LLM-DAS consistently boosts the performance of mainstream detectors. By
bridging LLM reasoning with classic AD algorithms via programmatic synthesis,
LLM-DAS offers a scalable, effective, and privacy-preserving approach to
patching the logical blind spots of existing detectors.

</details>


### [487] [THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series](https://arxiv.org/abs/2510.03911)
*Yadav Mahesh Lorik,Kaushik Sarveswaran,Nagaraj Sundaramahalingam,Aravindakumar Venugopalan*

Main category: cs.LG

TL;DR: 本文提出了一种基于基础模型预训练表示的时间序列异常检测新框架THEMIS，通过利用Chronos模型的编码器提取嵌入，并结合局部离群因子和谱分解等异常检测技术，在多个标准数据集上实现了领先或具有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测面临季节性、趋势、噪声、概念漂移以及异常类型多样性和数据不平衡等挑战，难以定义统一的正常行为模式，现有方法在适应性、鲁棒性和可解释性方面存在不足。

Method: THEMIS框架利用Chronos时间序列基础模型的编码器提取数据嵌入，构建自相似性矩阵，并在其上应用局部离群因子（LOF）和谱分解等无监督异常检测算法来识别异常。该方法具有模块化、无需微调、参数鲁棒性强和结果可解释的特点。

Result: 实验表明，THEMIS在MSL数据集上达到SOTA性能，在SMAP和SWAT*数据集上表现具有竞争力，且优于专门针对异常检测任务训练的模型，同时具备良好的超参数鲁棒性和默认可解释性。

Conclusion: 预训练基础模型提供的表征可用于高效、灵活且可解释的时间序列异常检测，THEMIS展示了利用基础模型进行下游异常检测任务的巨大潜力，为未来研究提供了新方向。

Abstract: Time series anomaly detection forms a very crucial area in several domains
but poses substantial challenges. Due to time series data possessing
seasonality, trends, noise, and evolving patterns (concept drift), it becomes
very difficult to set a general notion of what constitutes normal behavior.
Anomalies themselves could be varied, ranging from a single outlier to
contextual or collective anomalies, and are normally very rare; hence, the
dataset is largely imbalanced. Additional layers of complexities arise due to
the problems of increased dimensionality of modern time series, real-time
detection criteria, setting up appropriate detection thresholds, and arriving
at results that are interpretable. To embrace these multifaceted challenges,
very strong, flexible, and interpretable approaches are required. This paper
presents THEMIS, a new framework for time series anomaly detection that
exploits pretrained knowledge from foundation models. THEMIS extracts
embeddings from the encoder of the Chronos time series foundation model and
applies outlier detection techniques like Local Outlier Factor and Spectral
Decomposition on the self-similarity matrix, to spot anomalies in the data. Our
experiments show that this modular method achieves SOTA results on the MSL
dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.
Notably, THEMIS exceeds models trained specifically for anomaly detection,
presenting hyperparameter robustness and interpretability by default. This
paper advocates for pretrained representations from foundation models for
performing efficient and adaptable anomaly detection for time series data.

</details>


### [488] [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
*Junxi Yan,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的交叉熵分解方法，将其分为误差熵、自对齐和置信度三部分，发现只有误差熵遵循稳健的幂律缩放，从而提出了更准确的误差熵缩放定律。


<details>
  <summary>Details</summary>
Motivation: 交叉熵缩放定律在大规模模型上失效，原因可能是交叉熵本身并非真正可扩展，其内部成分的行为不同。

Method: 将交叉熵分解为误差熵、自对齐和置信度三个组成部分，并通过理论分析和实验验证各成分在不同规模模型中的缩放行为。

Result: 实验证明仅误差熵遵循幂律缩放，其他两项基本不变；小模型中误差熵占主导，但在大模型中占比下降，导致整体交叉熵偏离预期缩放规律。

Conclusion: 误差熵缩放定律比传统交叉熵缩放定律更准确，能更好描述大模型训练行为，对大模型训练与理解具有广泛应用价值。

Abstract: The cross-entropy scaling law has long served as a key tool for guiding the
development of large language models. It shows that cross-entropy loss
decreases in a predictable power-law rate as the model size increases. However,
recent evidence indicates that this law breaks down at very large scales: the
loss decreases more slowly than expected, which causes significant trouble for
developing large language models. In this paper, we hypothesize that the root
cause lies in the fact that cross-entropy itself does not truly scale; instead,
only one of its hidden components does. To investigate this, we introduce a
novel decomposition of cross-entropy into three parts: Error-Entropy,
Self-Alignment, and Confidence. We show both theoretically and empirically that
this decomposition precisely captures the training dynamics and optimization
objectives. Through extensive experiments on multiple datasets and 32 models
spanning five orders of magnitude in size, we find that only error-entropy
follows a robust power-law scaling, while the other two terms remain largely
invariant. Moreover, error-entropy constitutes the dominant share of
cross-entropy in small models but diminishes in proportion as models grow
larger. This explains why the cross-entropy scaling law appears accurate at
small scales but fails at very large ones. Our findings establish the
error-entropy scaling law as a more accurate description of model behavior. We
believe it will have wide applications in the training, understanding, and
future development of large language models.

</details>


### [489] [Generalized Fitted Q-Iteration with Clustered Data](https://arxiv.org/abs/2510.03912)
*Liyuan Hu,Jitao Wang,Zhenke Wu,Chengchun Shi*

Main category: cs.LG

TL;DR: 提出了一种结合广义估计方程的广义拟合Q迭代算法，用于处理强化学习中的聚类数据相关性，在正确或错误指定相关结构时均具有优良的理论性质，并在实验中显著降低遗憾。


<details>
  <summary>Details</summary>
Motivation: 处理医疗应用中常见的具有簇内相关性的聚类数据强化学习问题。

Method: 将广义估计方程引入策略学习，提出广义拟合Q迭代（FQI）算法。

Result: 理论证明了在相关结构正确指定时估计量的最优性，以及结构误设时的一致性；实验表明相比标准FQI平均减少一半的遗憾。

Conclusion: 所提出的广义FQI算法能有效处理聚类数据中的相关性，在理论和实践中均优于标准FQI。

Abstract: This paper focuses on reinforcement learning (RL) with clustered data, which
is commonly encountered in healthcare applications. We propose a generalized
fitted Q-iteration (FQI) algorithm that incorporates generalized estimating
equations into policy learning to handle the intra-cluster correlations.
Theoretically, we demonstrate (i) the optimalities of our Q-function and policy
estimators when the correlation structure is correctly specified, and (ii)
their consistencies when the structure is mis-specified. Empirically, through
simulations and analyses of a mobile health dataset, we find the proposed
generalized FQI achieves, on average, a half reduction in regret compared to
the standard FQI.

</details>


### [490] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: 提出了一种名为Slow-Fast Policy Optimization (SFPO) 的新框架，通过三阶段训练机制提升大语言模型推理能力的强化学习稳定性与效率，在数学推理任务中显著优于GRPO。


<details>
  <summary>Details</summary>
Motivation: 现有on-policy强化学习算法（如GRPO）在早期训练中因低质量rollout导致梯度噪声大，更新不稳定且探索效率低，影响大语言模型的推理训练效果。

Method: 将每个训练步骤分解为三个阶段：同一批数据上的短时快速内循环、控制离策略漂移的重定位机制、以及最终的慢速修正；该设计在不改变原有目标函数和rollout过程的前提下提升训练稳定性。

Result: 实验表明SFPO显著提升了训练稳定性、减少了rollout次数并加速收敛；在数学推理基准上平均超越GRPO达2.80分，rollout减少最多4.93倍，达到相同精度的墙钟时间减少4.19倍。

Conclusion: SFPO是一种高效、即插即用的强化学习框架，有效解决了on-policy方法在推理训练初期的不稳定性问题，显著提升了训练效率和性能。

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [491] [Transductive and Learning-Augmented Online Regression](https://arxiv.org/abs/2510.03917)
*Vinod Raman,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 本文研究了在线回归问题，其中学习者可以获得关于未来样本的预测信息。在极端情况下（即传递性在线学习），整个样本序列在游戏开始前已知，作者通过fat-shattering维数完全刻画了该设定下的极小化极大期望遗憾，并揭示了其与对抗性在线回归的区别。进一步，文章推广到未来样本预测存在噪声的情形，提出了一个能根据预测质量自适应调整、在预测准确时性能接近传递性学习者的在线算法，从而在可预测数据下实现原本无法学习的类别，符合学习增强模型的思想。


<details>
  <summary>Details</summary>
Motivation: 现实中的数据流具有可预测性，传统对抗性在线学习过于悲观，未能利用这种可预测结构。因此，研究如何利用对未来样本的预测来提升在线回归性能，尤其是在理想和非理想预测下的统一框架。

Method: 首先分析传递性在线学习这一极端情况，利用fat-shattering维数刻画极小化极大期望遗憾；然后扩展至含有噪声的未来样本预测情形，设计一种基于预测质量动态调整策略的在线学习算法，结合理想情况下的理论结果，实现对预测精度的平滑依赖。

Result: 1) 在传递性在线回归中，首次用fat-shattering维数完全刻画了极小化极大期望遗憾，并与标准在线回归形成分离；2) 提出了一种适用于不完美预测的在线学习算法，其极小化极大期望遗憾随预测质量提升而平滑下降，在高精度预测下接近传递性学习者的性能；3) 证明了该方法可在原无法学习的类别上实现可学习性。

Conclusion: 利用对未来样本的预测信息可以显著提升在线回归性能。本文建立了传递性与带噪声预测的在线学习的理论框架，提出的算法能在最坏情况下保证性能，同时在数据可预测时大幅提升效率，推动了学习增强型在线学习的发展。

Abstract: Motivated by the predictable nature of real-life in data streams, we study
online regression when the learner has access to predictions about future
examples. In the extreme case, called transductive online learning, the
sequence of examples is revealed to the learner before the game begins. For
this setting, we fully characterize the minimax expected regret in terms of the
fat-shattering dimension, establishing a separation between transductive online
regression and (adversarial) online regression. Then, we generalize this
setting by allowing for noisy or \emph{imperfect} predictions about future
examples. Using our results for the transductive online setting, we develop an
online learner whose minimax expected regret matches the worst-case regret,
improves smoothly with prediction quality, and significantly outperforms the
worst-case regret when future example predictions are precise, achieving
performance similar to the transductive online learner. This enables
learnability for previously unlearnable classes under predictable examples,
aligning with the broader learning-augmented model paradigm.

</details>


### [492] [On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks](https://arxiv.org/abs/2510.03923)
*Mingsong Yan,Charles Kulick,Sui Tang*

Main category: cs.LG

TL;DR: 本文提出了Graphon神经微分方程（Graphon-NDEs）作为连续深度图神经网络（GNDEs）在无限节点极限下的理论框架，证明了GNDE解向Graphon-NDE解的轨迹收敛性，并给出了两种图采样机制下的显式收敛速率，为GNDE在不同规模图上的可迁移性提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 为了理解连续深度图神经网络（GNDEs）在不同规模图上的泛化能力，特别是其在无限节点极限下的行为，需要建立严格的理论分析框架，以解释和保证其大小可迁移性。

Method: 引入Graphon-NDEs作为GNDEs在无限节点极限下的连续模型，结合图论（graphon theory）与动力系统理论，分析GNDE解的轨迹收敛性，并在两类确定性图采样机制下推导收敛速率。

Result: 证明了GNDE解在无限节点极限下逐轨迹收敛于Graphon-NDE解，给出了光滑和不连续图on上的显式收敛速率，并建立了大小可迁移性误差界。实验验证了理论结果的有效性。

Conclusion: GNDEs具有良好的规模可迁移性，其行为在无限节点极限下由Graphon-NDEs良好描述，为在大规模图上应用小图训练的模型提供了坚实的理论基础。

Abstract: Continuous-depth graph neural networks, also known as Graph Neural
Differential Equations (GNDEs), combine the structural inductive bias of Graph
Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,
offering a scalable and principled framework for modeling dynamics on graphs.
In this paper, we present a rigorous convergence analysis of GNDEs with
time-varying parameters in the infinite-node limit, providing theoretical
insights into their size transferability. To this end, we introduce Graphon
Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of
GNDEs and establish their well-posedness. Leveraging tools from graphon theory
and dynamical systems, we prove the trajectory-wise convergence of GNDE
solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence
rates under two deterministic graph sampling regimes: (1) weighted graphs
sampled from smooth graphons, and (2) unweighted graphs sampled from
$\{0,1\}$-valued (discontinuous) graphons. We further establish size
transferability bounds, providing theoretical justification for the practical
strategy of transferring GNDE models trained on moderate-sized graphs to
larger, structurally similar graphs without retraining. Numerical experiments
using synthetic and real data support our theoretical findings.

</details>


### [493] [On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection](https://arxiv.org/abs/2510.03944)
*Weiqing He,Xiang Li,Tianqi Shang,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 本文系统评估了八种拟合优度（GoF）检验在三种主流水印方案中的表现，发现经典GoF检验能有效提升大语言模型生成文本水印检测的能力与鲁棒性，尤其在低温度生成导致文本重复时具有独特优势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可大规模生成类人文本，引发内容真实性担忧；现有水印检测方法未充分探索适用于检测统计信号的拟合优度（GoF）检验。

Method: 在三个开源大模型、两个数据集、多种生成温度和后编辑方法下，系统评估八种GoF检验在三种流行水印方案中的检测性能。

Result: GoF检验能显著提升水印检测的检测力与鲁棒性；文本重复现象（常见于低温度生成）为GoF检验提供了现有方法未利用的独特优势。

Conclusion: 经典的拟合优度检验是一种简单但强大且被低估的水印检测工具，值得在大语言模型内容溯源中进一步应用。

Abstract: Large language models (LLMs) raise concerns about content authenticity and
integrity because they can generate human-like text at scale. Text watermarks,
which embed detectable statistical signals into generated text, offer a
provable way to verify content origin. Many detection methods rely on pivotal
statistics that are i.i.d. under human-written text, making goodness-of-fit
(GoF) tests a natural tool for watermark detection. However, GoF tests remain
largely underexplored in this setting. In this paper, we systematically
evaluate eight GoF tests across three popular watermarking schemes, using three
open-source LLMs, two datasets, various generation temperatures, and multiple
post-editing methods. We find that general GoF tests can improve both the
detection power and robustness of watermark detectors. Notably, we observe that
text repetition, common in low-temperature settings, gives GoF tests a unique
advantage not exploited by existing methods. Our results highlight that classic
GoF tests are a simple yet powerful and underused tool for watermark detection
in LLMs.

</details>


### [494] [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
*Minseo Kim,Coleman Hooper,Aditya Tomar,Chenfeng Xu,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: 本文对比了自回归语言模型（ARMs）和扩散语言模型（DLMs）的性能特性，发现DLMs虽然具有更高的计算密度，但在长上下文场景下扩展性较差；通过分块解码可改善其表现，并指出减少采样步数是加速开源DLM推理的关键。


<details>
  <summary>Details</summary>
Motivation: 由于ARMs在生成时存在序列依赖导致计算强度低，而DLMs虽能并行生成但性能优势尚不明确，因此需要系统分析两类模型的性能权衡。

Method: 结合理论分析与性能剖析方法，对ARMs和DLMs在不同上下文长度、批处理规模下的计算强度、吞吐量等性能特征进行综合比较，并探索分块解码对DLM的影响。

Result: DLMs因并行生成具有更高算术强度，但难以扩展到长上下文；采用分块解码可在保持长上下文支持的同时提升效率；在批处理推理中，ARMs因跨序列并行性表现出更高吞吐量；减少采样步数可显著降低DLM的延迟。

Conclusion: 尽管DLMs在计算效率上有潜力，但在当前实现下仍面临扩展性和延迟挑战；优化采样过程和解码策略是提升其实际竞争力的关键方向。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a
broad range of Natural Language Processing (NLP) tasks, including document
processing and coding. Autoregressive Language Models (ARMs), which generate
tokens sequentially conditioned on all previous tokens, have been the
predominant paradigm for LLMs. However, while these networks have achieved high
accuracy across a range of downstream tasks, they exhibit low arithmetic
intensity due to the inherent sequential dependency with next-token prediction.
Recently, Diffusion Language Models (DLMs) have emerged as a promising
alternative architecture. DLMs generate output text in parallel, breaking the
limitations of sequential dependency. However, the performance implications of
DLMs relative to commonly deployed ARMs are not fully understood. In this work,
we present a comprehensive performance study analyzing the performance
characteristics of ARMs and DLMs, using both theoretical analysis and profiling
data to characterize the trade-offs between these approaches. We illustrate
that although DLMs exhibit higher arithmetic intensity compared to ARMs because
of their capability to utilize parallelism across sequence lengths, they fail
to scale effectively to longer contexts. We then explore DLMs with block-wise
decoding, outlining how this approach allows for increased arithmetic
intensity, while still scaling well to long contexts (similar to ARMs). We also
show interesting trade-offs for batched inference, where we find that ARMs
exhibit superior throughput, as they benefit more from parallelism across
sequences in the batch. Finally, we highlight opportunities for accelerating
DLM inference, and, in particular, highlight the importance of reducing the
number of sampling steps for allowing open-source DLMs to provide improved
latency relative to ARMs.

</details>


### [495] [What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis](https://arxiv.org/abs/2510.03950)
*Shahriar Kabir Nahin,Wenxiao Xiao,Joshua Liu,Anshuman Chhabra,Hongfu Liu*

Main category: cs.LG

TL;DR: 本文提出类别级影响函数和基于线性规划的样本重加权框架，旨在实现机器学习模型在各类别上的帕累托性能提升，而非仅关注整体准确率的改进。


<details>
  <summary>Details</summary>
Motivation: 现有数据中心学习方法主要关注哪些数据对模型有益，且以整体准确率为评价指标，忽视了不同类别间的性能权衡。本文旨在探究模型性能的上限，并追求每个类别都能受益的帕累托改进。

Method: 提出类别级影响函数，引入影响向量来量化每个训练样本对所有类别的影响；设计基于线性规划的样本重加权框架，用于判断模型是否仍可改进，并实现帕累托性能提升。

Result: 在合成数据、视觉和文本基准上的实验表明，该方法能有效估计并实现模型在多个关注类别上的性能提升，优于现有方法。

Conclusion: 通过类别级影响分析和样本重加权，能够在不牺牲任何类别性能的前提下提升模型整体表现，为数据质量优化提供了新的视角和工具。

Abstract: Data-centric learning seeks to improve model performance from the perspective
of data quality, and has been drawing increasing attention in the machine
learning community. Among its key tools, influence functions provide a powerful
framework to quantify the impact of individual training samples on model
predictions, enabling practitioners to identify detrimental samples and retrain
models on a cleaner dataset for improved performance. However, most existing
work focuses on the question: "what data benefits the learning model?" In this
paper, we take a step further and investigate a more fundamental question:
"what is the performance ceiling of the learning model?" Unlike prior studies
that primarily measure improvement through overall accuracy, we emphasize
category-wise accuracy and aim for Pareto improvements, ensuring that every
class benefits, rather than allowing tradeoffs where some classes improve at
the expense of others. To address this challenge, we propose category-wise
influence functions and introduce an influence vector that quantifies the
impact of each training sample across all categories. Leveraging these
influence vectors, we develop a principled criterion to determine whether a
model can still be improved, and further design a linear programming-based
sample reweighting framework to achieve Pareto performance improvements.
Through extensive experiments on synthetic datasets, vision, and text
benchmarks, we demonstrate the effectiveness of our approach in estimating and
achieving a model's performance improvement across multiple categories of
interest.

</details>


### [496] [Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts](https://arxiv.org/abs/2510.03954)
*Tim Bary,Tiffanie Godelaine,Axel Abels,Benoît Macq*

Main category: cs.LG

TL;DR: 提出一种自适应实时标注方法，能够在无需专家先验知识或预标注数据的情况下，动态聚合专家意见，显著减少标注开销并保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有标注聚合算法难以无缝集成到医疗筛查流程中，尤其在专家能力未知且数据持续到达的场景下。

Method: 采用增量式方式收集专家意见，基于每个样本的潜在难度动态决定是否查询更多专家，直到达到置信度阈值。

Result: 在三个多模态多标注者分类数据集上验证，相比非自适应基线方法，在保持相当准确性的前提下，专家查询次数最多减少50%。

Conclusion: 该方法支持实时、无监督的专家标注聚合，有效降低医疗筛查中的标注成本，具备实际部署潜力。

Abstract: Accurate ground truth estimation in medical screening programs often relies
on coalitions of experts and peer second opinions. Algorithms that efficiently
aggregate noisy annotations can enhance screening workflows, particularly when
data arrive continuously and expert proficiency is initially unknown. However,
existing algorithms do not meet the requirements for seamless integration into
screening pipelines. We therefore propose an adaptive approach for real-time
annotation that (I) supports on-the-fly labeling of incoming data, (II)
operates without prior knowledge of medical experts or pre-labeled data, and
(III) dynamically queries additional experts based on the latent difficulty of
each instance. The method incrementally gathers expert opinions until a
confidence threshold is met, providing accurate labels with reduced annotation
overhead. We evaluate our approach on three multi-annotator classification
datasets across different modalities. Results show that our adaptive querying
strategy reduces the number of expert queries by up to 50% while achieving
accuracy comparable to a non-adaptive baseline. Our code is available at
https://github.com/tbary/MEDICS

</details>


### [497] [Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model](https://arxiv.org/abs/2510.03959)
*Iryna Stanishevska*

Main category: cs.LG

TL;DR: 本文提出了一种基于公开数据的两阶段模型，用于提前24-48小时预测密歇根州夏季雷暴相关的电力中断事件。该模型结合逻辑门控与LSTM回归器，利用精细的时空特征提取方法，在事件中心化评估中表现出优于基准模型的检测能力。


<details>
  <summary>Details</summary>
Motivation: 雷暴导致的停电难以预测，因为大多数风暴并不造成破坏，对流过程快速且混沌，且公共数据噪声大、不完整。因此需要一种仅依赖开放数据源的有效早期预警方法。

Method: 使用EAGLE-I停电数据和METAR气象数据，通过参数特定克里金插值和过采样保留极端信号，并构建因果时空特征（如滞后/滚动统计、k近邻/反距离加权空间聚合）。采用两阶段模型：逻辑门筛选潜在事件，LSTM进行回归预测。

Result: 在测试集上，该模型在±48小时内检测到更多真实峰值（3/4 vs. 2/4），F1得分更高（66.7% vs. 57.1%），虽多出一次误报；在临近峰值时cMASE降低2-3%，但在36-48小时略有损失。SHAP分析验证了湿度平流和风速突变等关键前兆特征的重要性。

Conclusion: 尽管使用的是有噪声的开放数据，所提出的特征驱动管道仍能生成可操作的、以事件为中心的雷暴停电早期预警，证明了其在实际应用中的潜力。

Abstract: Thunderstorm-driven outages are difficult to predict because most storms do
not cause damage, convective processes occur rapidly and chaotically, and the
available public data are both noisy and incomplete. We develop a 24-48 h
early-warning model for summer, thunderstorm-related outages in Michigan using
only open sources (EAGLE-I for ground truth; METAR for weather). We use the
publicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge
National Laboratory for the U.S. Department of Energy. The pipeline preserves
convective micro-signals from a sparse station network via parameter-specific
kriging with hourly variograms and targeted overdrafting to retain extremes,
and builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW
spatial aggregates) capturing precursors of severe convection (moisture
advection, wind shifts, and pressure drops). The two-stage model design,
combining a logistic gate and an LSTM regressor, limits routine periods and
reduces noise exposure. The study uses event-centric metrics (cluster-based
hits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour
windows around state-level peaks (>= 50,000), with uncertainty quantified by
hourly moving-block bootstrap.
  On the test sample, Two-Stage detects more reference peaks across all windows
(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra
false alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at
+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48
h (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP
analysis confirms moisture-advection and wind/gust precursors, underscoring the
value of the feature engineering. Despite open-data noise, the feature-driven
pipeline yields actionable, event-focused early warnings for thunderstorm
outages.

</details>


### [498] [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 提出Wave-PDE Nets，一种基于可微分波动方程模拟的神经网络架构，具有高效、稳定的特性，在语言和视觉任务上性能优于或媲美Transformer，同时降低运行时间和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 设计一种具有物理先验的高效神经网络架构，替代注意力机制和一阶状态空间模型。

Method: 将二阶波动方程的可微分模拟作为基本操作，每层通过可训练的空间速度c(x)和阻尼γ(x)传播隐藏状态，并采用基于FFT的辛谱求解器实现O(nlog n)时间内的传播。

Result: 单个Wave-PDE层即为通用逼近器；在语言和视觉基准上达到或超过Transformer性能，运行时间减少最多30%，峰值内存降低25%；消融实验验证了辛积分和谱拉普拉斯算子对稳定性和性能的关键作用。

Conclusion: Wave-PDE Nets是一种计算高效、鲁棒性强且具有强物理归纳偏置的神经网络架构，适用于多种任务。

Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation
is a differentiable simulation of the second-order wave equation. Each layer
propagates its hidden state as a continuous field through a medium with
trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral
solver based on FFTs realises this propagation in O(nlog n) time. This
oscillatory, global mechanism provides a powerful alternative to attention and
first-order state-space models. We prove that a single Wave-PDE layer is a
universal approximator. On language and vision benchmarks, Wave-PDE Nets match
or exceed Transformer performance while demonstrating superior practical
efficiency, reducing wall-clock time by up to 30% and peak memory by 25%.
Ablation studies confirm the critical role of symplectic integration and a
spectral Laplacian for stability and performance. Visualizations of the learned
physical parameters reveal that the model learns intuitive strategies for
information propagation. These results position Wave-PDE Nets as a
computationally efficient and robust architecture with a strong physical
inductive bias.

</details>


### [499] [SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data](https://arxiv.org/abs/2510.03962)
*Hanzhe Wei,Jiajun Wu,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.LG

TL;DR: 提出了一种名为SPEAR的新方法，利用软提示和量化技术将大语言模型（LLM）应用于时间序列异常检测，实验结果表明该方法能有效提升LLM在该任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理变长时间序列和基于上下文的异常，而大语言模型（LLM）为时间序列异常检测提供了新机遇，但需有效适配其处理离散序列的特点。

Method: 通过量化和转换时间序列数据为输入嵌入，并结合可学习的软提示嵌入，输入到冻结的LLM中；软提示通过交叉熵损失迭代更新，以适应异常检测任务。

Result: 实验证明，所提出的软提示方法显著提升了LLM在时间序列异常检测下游任务中的表现，同时量化策略有效支持了对变长序列的处理。

Conclusion: SPEAR通过软提示和量化技术成功将LLM应用于时间序列异常检测，兼顾了模型适应性和序列处理效率，展现出优于传统方法的潜力。

Abstract: Time series anomaly detection plays a crucial role in a wide range of fields,
such as healthcare and internet traffic monitoring. The emergence of large
language models (LLMs) offers new opportunities for detecting anomalies in the
ubiquitous time series data. Traditional approaches struggle with
variable-length time series sequences and context-based anomalies. We propose
Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage
LLMs for anomaly detection with soft prompts and quantization. Our methodology
involves quantizing and transforming the time series data into input embeddings
and combining them with learnable soft prompt embeddings. These combined
embeddings are then fed into a frozen LLM. The soft prompts are updated
iteratively based on a cross-entropy loss, allowing the model to adapt to time
series anomaly detection. The use of soft prompts helps adapt LLMs effectively
to time series tasks, while quantization ensures optimal handling of sequences,
as LLMs are designed to handle discrete sequences. Our experimental results
demonstrate that soft prompts effectively increase LLMs' performance in
downstream tasks regarding time series anomaly detection.

</details>


### [500] [What Can You Do When You Have Zero Rewards During RL?](https://arxiv.org/abs/2510.03971)
*Jatin Prakash,Anirudh Buvanesh*

Main category: cs.LG

TL;DR: 本文研究了基于结果奖励的强化学习在大型语言模型复杂推理任务中的应用，发现当基础模型无法生成正确解时，训练会陷入零奖励障碍。实验表明，现有的密集奖励、多样性激励和信用分配方法均无法突破这一障碍，而通过向训练集中添加更简单的样本，模型最终能够解决原本困难的任务，且无需修改强化学习算法本身。


<details>
  <summary>Details</summary>
Motivation: 当基础模型无法采样到正确解时，强化学习面临零奖励障碍，导致学习停滞，本文旨在探究该问题并寻找有效解决方案。

Method: 在Bachmann等人（2024）提出的图搜索任务上，评估包含密集奖励、多样性激励和改进信用分配等机制的现有方法，并提出一种数据中心化的策略，即向训练集中添加较易样本以帮助模型突破零奖励障碍。

Result: 实验结果显示，现有方法在无正确解采样的情况下均无法克服零奖励障碍；而添加较易样本的简单数据干预能使模型最终解决原困难任务，且该方法不依赖对RL算法的修改。此外，作者实现了多个基线方法并公开代码以支持后续研究。

Conclusion: 突破零奖励障碍的关键可能不在于改进强化学习算法本身，而在于调整训练数据分布，引入更易样本可有效引导模型逐步学会解决复杂任务。

Abstract: Reinforcement learning (RL) with outcome-based rewards has proven effective
for improving large language models (LLMs) on complex reasoning tasks. However,
its success often depends on the base model occasionally sampling correct
solutions. When no correct solutions are sampled, training encounters a
zero-reward barrier where learning stalls due to zero gradients. We study this
scenario through the graph search task introduced in Bachmann et al. (2024) and
evaluate recent methods that incorporate desirable components such as dense
rewards, diversity incentives, and improved credit assignment. Our experiments
show that none of these approaches overcome the zero-reward barrier if the base
model never produces a correct answer. In contrast, we find that a simple
data-centric intervention of adding easier samples to the training set enables
the model to eventually solve the original hard task despite starting from zero
reward. Importantly, this succeeds without modifying the RL algorithm itself.
Because official implementations of several baselines were unavailable, we
developed our own, which allowed us to conduct a detailed analysis of their
failure modes. We release these implementations to support further research at:
https://github.com/rl4reasoning/rl-baselines

</details>


### [501] [Beyond Softmax: A New Perspective on Gradient Bandits](https://arxiv.org/abs/2510.03979)
*Emerson Melo,David Müller*

Main category: cs.LG

TL;DR: 本文建立了离散选择模型与在线学习、多臂赌博机理论之间的联系，提出了一类具有次线性遗憾界的算法，并基于广义嵌套Logit模型设计了新的对抗性赌博机算法，同时引入了一种超越Softmax框架的广义梯度赌博机算法，允许动作间的相关学习，兼具灵活性与计算高效性。


<details>
  <summary>Details</summary>
Motivation: 传统梯度赌博机方法多依赖Softmax策略，其独立性假设限制了对动作间相关性的建模能力。本文旨在打破这一限制，通过引入更灵活的离散选择模型，提升算法在复杂环境下的表达能力和适应性。

Method: 利用广义嵌套Logit模型构建新型对抗性赌博机算法，提出广义梯度赌博机框架，推导出闭式采样概率以保证计算效率，并分析其在随机和对抗性环境中的遗憾界。

Result: 获得了涵盖Exp3在内的广泛算法族的次线性遗憾界，实验表明新算法在随机赌博机设置下具有良好的实际效果，且能有效建模动作间的相关性。

Conclusion: 本文成功将广义离散选择模型引入赌博机问题，提出的算法在保持计算效率的同时增强了模型灵活性，拓展了梯度赌博机方法的应用范围。

Abstract: We establish a link between a class of discrete choice models and the theory
of online learning and multi-armed bandits. Our contributions are: (i)
sublinear regret bounds for a broad algorithmic family, encompassing Exp3 as a
special case; (ii) a new class of adversarial bandit algorithms derived from
generalized nested logit models \citep{wen:2001}; and (iii)
\textcolor{black}{we introduce a novel class of generalized gradient bandit
algorithms that extends beyond the widely used softmax formulation. By relaxing
the restrictive independence assumptions inherent in softmax, our framework
accommodates correlated learning dynamics across actions, thereby broadening
the applicability of gradient bandit methods.} Overall, the proposed algorithms
combine flexible model specification with computational efficiency via
closed-form sampling probabilities. Numerical experiments in stochastic bandit
settings demonstrate their practical effectiveness.

</details>


### [502] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出了一种基于高斯分布假设的高效部分信息分解方法（GPID），并通过梯度算法和编码器扩展到非高斯数据，显著提升了多模态信息分析的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有PID方法在处理连续高维模态时因依赖估计的联合分布而计算昂贵且不准确，难以有效量化多模态间的独立、冗余和协同信息。

Method: 提出高斯PID（GPID）框架，利用多元高斯分布简化优化问题，并设计基于梯度的算法提高计算效率；通过学习保持信息的编码器将任意分布变量转化为成对高斯变量以适应非高斯数据。

Result: 在合成数据上验证了方法比现有基线更准确高效，在多个大规模多模态基准任务中展示了其在真实场景中的有效性，可用于量化PID和选择高性能模型。

Conclusion: 所提GPID方法及其扩展显著改进了多模态系统中信息分解的可行性与性能，解决了GPID中联合高斯解最优性的开放问题，推动了PID在复杂数据中的应用。

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [503] [ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity](https://arxiv.org/abs/2510.03987)
*Michael Yang*

Main category: cs.LG

TL;DR: 本文提出了ICEPool，一种增强簇间连接性的层次池化框架，能够提升图神经网络对原始图结构的保持能力，并兼容多种现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的层次池化方法通常忽略簇间关系，影响了图结构信息的完整保留。

Method: 提出ICEPool框架，通过增强簇间连接性来改善图的表示，并理论分析其图重建能力。

Result: 实验表明ICEPool能有效提升多种GNN模型的性能，并具备良好的兼容性。

Conclusion: ICEPool通过强调簇间连接性，增强了模型对图结构的理解和表示能力，是一种有效的层次池化增强方法。

Abstract: Hierarchical Pooling Models have demonstrated strong performance in
classifying graph-structured data. While numerous innovative methods have been
proposed to design cluster assignments and coarsening strategies, the
relationships between clusters are often overlooked. In this paper, we
introduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novel
hierarchical pooling framework designed to enhance model's understanding of
inter-cluster connectivity and ability of preserving the structural integrity
in the original graph. ICEPool is compatible with a wide range of pooling-based
GNN models. The deployment of ICEPool as an enhancement to existing models
effectively combines the strengths of the original model with ICEPool's
capability to emphasize the integration of inter-cluster connectivity,
resulting in a more comprehensive and robust graph-level representation.
Moreover, we make theoretical analysis to ICEPool's ability of graph
reconstruction to demonstrate its effectiveness in learning inter-cluster
relationship that is overlooked by conventional models. Finally, the
experimental results show the compatibility of ICEPool with wide varieties of
models and its potential to boost the performance of existing graph neural
network architectures.

</details>


### [504] [Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data](https://arxiv.org/abs/2510.03988)
*Hoang Anh Just,Myeongseob Ko,Ruoxi Jia*

Main category: cs.LG

TL;DR: 本文提出了一种新的推理蒸馏中的响应选择方法——局部自然性（Local Naturalness），用于在多教师环境下选择更适合学生的推理路径，显著提升了学生模型在数学任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 在多教师推理蒸馏中，当前基于全局似然的选择方法失效，缺乏有效机制选择最优响应，尤其当教师推理链变长时。需要一种更鲁棒的响应选择标准。

Method: 提出局部自然性（Local Naturalness），通过计算学生模型在短小局部窗口内的条件对数概率来评估推理步骤的质量，并用于教师选择和多教师响应选择。

Result: 局部自然性在32B学生模型上使数学基准准确率比全局选择提升9.4个百分点，并优于仅使用最佳单一教师训练的效果。

Conclusion: 局部自然性是一种有效的多教师推理蒸馏响应选择机制，支持更高质量的数据筛选与混合，推动了推理能力的高效迁移。

Abstract: Distilling long reasoning traces (10K+ tokens) from stronger teacher models
into smaller student LLMs via SFT has emerged as a standard paradigm. This
approach is practical and efficient: it leverages the ease of generating
abundant reasoning data from stronger models and provides a direct, data-driven
way to teach less capable models better reasoning. While previous work has
largely focused on prompt selection with responses from a single teacher, the
equally important problem of choosing the best response when multiple teacher
outputs are available for a single prompt remains underexplored. This challenge
becomes important in a multi-teacher setting, where different students may
benefit from the outputs of different teachers. This paper fills that gap with
a systematic study of response selection for reasoning distillation. We first
show that the current method, which picks responses the student assigns the
highest global log-probability (global naturalness), fails when responses come
from multiple teachers, i.e., global naturalness no longer correlates with
downstream performance, especially as the reasoning traces from strong teachers
become longer. To overcome this problem, we introduce Local Naturalness, which
measures the student's log-probabilities over short, sequential reasoning steps
conditioned only on a small local window. Local Naturalness enables two
applications: 1) Teacher Selection: Aggregating local scores across prompts
reliably identifies the most helpful teacher. 2) Response Selection from a
Multiple Teachers: When mixing answers from many teachers, Local Naturalness
boosts a 32B student's accuracy on math benchmarks by 9.4pp over global
selection, also surpassing the performance achieved by training on data from
the single best teacher. These results highlight the power of localized data
quality evaluation and data mixing for more effective reasoning distillation.

</details>


### [505] [A Mathematical Explanation of Transformers for Large Language Models and GPTs](https://arxiv.org/abs/2510.03989)
*Xue-Cheng Tai,Hao Liu,Lingfeng Li,Raymond H. Chan*

Main category: cs.LG

TL;DR: 提出了一种新的连续框架，将Transformer解释为结构化积分微分方程的离散化，自注意力机制自然地表现为非局部积分算子，层归一化则被描述为到时变约束的投影。


<details>
  <summary>Details</summary>
Motivation: 缺乏对Transformer架构及其操作的全面数学理论解释。

Method: 通过建立一个连续域中的积分微分方程模型，将Transformer的各个组件（如自注意力、前馈网络和归一化）统一在算子理论和变分视角下进行分析。

Result: 成功将整个Transformer操作嵌入到令牌索引和特征维度的连续域中，提供了对核心组件的统一且可解释的基础，并为架构设计和控制解释提供了新方向。

Conclusion: 该工作为连接深度学习架构与连续数学建模提供了桥梁，推动了可解释且理论扎实的神经网络模型的发展。

Abstract: The Transformer architecture has revolutionized the field of sequence
modeling and underpins the recent breakthroughs in large language models
(LLMs). However, a comprehensive mathematical theory that explains its
structure and operations remains elusive. In this work, we propose a novel
continuous framework that rigorously interprets the Transformer as a
discretization of a structured integro-differential equation. Within this
formulation, the self-attention mechanism emerges naturally as a non-local
integral operator, and layer normalization is characterized as a projection to
a time-dependent constraint. This operator-theoretic and variational
perspective offers a unified and interpretable foundation for understanding the
architecture's core components, including attention, feedforward layers, and
normalization. Our approach extends beyond previous theoretical analyses by
embedding the entire Transformer operation in continuous domains for both token
indices and feature dimensions. This leads to a principled and flexible
framework that not only deepens theoretical insight but also offers new
directions for architecture design, analysis, and control-based
interpretations. This new interpretation provides a step toward bridging the
gap between deep learning architectures and continuous mathematical modeling,
and contributes a foundational perspective to the ongoing development of
interpretable and theoretically grounded neural network models.

</details>


### [506] [Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints](https://arxiv.org/abs/2510.04006)
*Hang Fan,Yi Xiao,Yongquan Qu,Fenghua Ling,Ben Fei,Lei Bai,Pierre Gentine*

Main category: cs.LG

TL;DR: 提出了一种基于弱约束四维变分数据同化（WC-4DVar）框架的机器学习天气预报模型训练方法，通过在自编码器的潜在空间中引入误差协方差结构，提升了长期预报的准确性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习气象预报模型将再分析数据视为真实值，并忽略变量间的物理耦合和空间结构，导致长期预报模糊且物理上不真实。

Method: 将模型训练重新解释为弱约束四维变分数据同化（WC-4DVar）问题，视再分析数据为不完美观测，在自编码器学习的潜在空间中计算损失函数，使再分析误差协方差近似对角化，从而隐式建模多变量依赖关系。

Result: 在潜在空间中进行约束的滚动训练显著提升了长期预报技巧，更好保持了细尺度结构和物理真实性，并可扩展至融合多源异构观测数据。

Conclusion: 该方法为数据驱动气象预报提供了一个统一且物理一致的训练框架，有效改善了传统损失函数下的模型退化问题。

Abstract: Data-driven machine learning (ML) models have recently shown promise in
surpassing traditional physics-based approaches for weather forecasting,
leading to a so-called second revolution in weather forecasting. However, most
ML-based forecast models treat reanalysis as the truth and are trained under
variable-specific loss weighting, ignoring their physical coupling and spatial
structure. Over long time horizons, the forecasts become blurry and physically
unrealistic under rollout training. To address this, we reinterpret model
training as a weak-constraint four-dimensional variational data assimilation
(WC-4DVar) problem, treating reanalysis data as imperfect observations. This
allows the loss function to incorporate reanalysis error covariance and capture
multivariate dependencies. In practice, we compute the loss in a latent space
learned by an autoencoder (AE), where the reanalysis error covariance becomes
approximately diagonal, thus avoiding the need to explicitly model it in the
high-dimensional model space. We show that rollout training with latent-space
constraints improves long-term forecast skill and better preserves fine-scale
structures and physical realism compared to training with model-space loss.
Finally, we extend this framework to accommodate heterogeneous data sources,
enabling the forecast model to be trained jointly on reanalysis and
multi-source observations within a unified theoretical formulation.

</details>


### [507] [Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention](https://arxiv.org/abs/2510.04008)
*Sahil Joshi,Agniva Chowdhury,Amar Kanakamedala,Ekam Singh,Evan Tu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: RACE Attention是一种线性复杂度的注意力机制，通过使用锐化角度相似性和随机投影软LSH，替代Softmax，在长上下文场景中显著降低计算和内存开销，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: Softmax注意力具有二次时间复杂度，在长上下文场景下计算代价过高，现有优化方法（如FlashAttention）在数百万token级别即遭遇硬件瓶颈，限制了大模型处理长序列的能力。

Method: 提出RACE Attention，用锐化的余弦相似性替代Softmax中的指数核函数，并通过随机投影和软局部敏感哈希（LSH）近似计算注意力输出，实现时间和空间上的线性复杂度。

Result: 在语言建模、掩码语言建模和文本分类任务中，RACE Attention与强基线模型精度相当，但显著降低运行时间和内存占用；在NVIDIA GH200 GPU上可支持1200万token的前向-反向传播，在Intel Xeon Gold CPU上可达7500万token，远超当前SOTA方法的实际极限。

Conclusion: RACE Attention为当前硬件提供了实用且理论可靠的长上下文处理方案，有望成为实际系统中的标准组件。

Abstract: Softmax Attention has a quadratic time complexity, which becomes prohibitive
to run at long contexts, even with highly optimized GPU kernels. For example,
FlashAttention (an exact, GPU-optimized implementation of Softmax Attention)
cannot complete a single forward-backward pass of a multi-head attention layer
once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We
introduce RACE Attention, a kernel-inspired alternative to Softmax Attention
that is linear in sequence length and embedding dimension. RACE Attention
replaces the exponential kernel with a sharpened angular (cosine) similarity,
and approximates attention outputs via randomized projections and soft
Locality-Sensitive Hashing (LSH). Across language modeling, masked language
modeling, and text classification, RACE Attention matches the accuracy of
strong baselines while reducing runtime and memory. In a controlled scale test,
it processes up to 12 million tokens during a single forward-backward pass on
an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well
beyond the practical limits of the current state-of-the-art attention
implementations. RACE Attention thus offers a practical, theoretically grounded
mechanism for outrageously long context windows on today's hardware. We hope
that it gets adopted in practice.

</details>


### [508] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: 提出LaDiR，一种结合连续潜在表示与潜在扩散模型迭代优化能力的新型推理框架，通过变分自编码器构建结构化潜在推理空间，并利用潜在扩散模型实现块级双向注意力掩码下的去噪，从而实现高效并行生成多样化的推理路径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的自回归解码限制了对早期生成内容的整体回溯与优化能力，导致推理过程中难以有效探索多样化解决方案。

Method: 首先使用变分自编码器（VAE）将文本推理步骤编码为“思维块”token，构建结构化潜在推理空间；然后在该空间中采用具有块级双向注意力掩码的潜在扩散模型进行去噪训练，支持长视野、迭代优化及自适应测试时计算。

Result: 在多个数学推理与规划基准上的实验表明，LaDiR在准确性、推理多样性与可解释性方面均优于现有的自回归、基于扩散和潜在推理方法。

Conclusion: LaDiR提供了一种全新的基于潜在扩散的文本推理范式，能够实现整体性的推理过程规划与修订，显著提升推理质量与效率。

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [509] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: ACE（Agentic Context Engineering）是一种通过生成、反思和策展的模块化过程来动态优化上下文的框架，有效避免了上下文崩溃和简洁性偏差，在代理和领域任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文适应方法存在简洁性偏差和上下文崩溃问题，导致关键信息丢失，影响大模型在复杂任务中的表现。

Method: 基于Dynamic Cheatsheet的自适应记忆机制，提出ACE框架，将上下文视为可演化的 playbook，通过结构化、增量式的生成、反思与策展流程进行持续优化。

Result: 在代理和金融领域基准上分别提升10.6%和8.6%，降低适应延迟和推理成本；在AppWorld排行榜上媲美甚至超越大型生产级代理，且使用更小的开源模型。

Conclusion: ACE实现了无需标注监督的高效上下文适应，展示了演化式上下文在构建可扩展、自改进、低开销大模型系统中的潜力。

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [510] [Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models](https://arxiv.org/abs/2510.04020)
*Hao Wu,Yuan Gao,Xingjian Shi,Shuaipeng Li,Fan Xu,Fan Zhang,Zhihong Zhu,Weiyan Wang,Xiao Luo,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出了一种基于模型强化学习的时空预测新范式SFP，通过生成世界模型和基于规划的搜索优化预测，显著提升对极端事件等非可微指标的预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决物理时空预测中固有的随机性和非可微度量带来的挑战。

Method: 构建生成式世界模型进行未来状态模拟，将基础预测模型作为智能体，利用基于beam search的规划算法以非可微领域指标为奖励信号搜索高回报未来序列，并通过迭代自训练用高奖励候选作为伪标签优化策略。

Result: 显著降低预测误差，在捕捉极端事件等关键领域指标上表现出色。

Conclusion: SFP为处理复杂、随机性环境下的时空预测提供了有效且新颖的框架，尤其在依赖非可微评估指标的任务中具有优势。

Abstract: To address the dual challenges of inherent stochasticity and
non-differentiable metrics in physical spatiotemporal forecasting, we propose
Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in
Model-Based Reinforcement Learning. SFP constructs a novel Generative World
Model to simulate diverse, high-fidelity future states, enabling an
"imagination-based" environmental simulation. Within this framework, a base
forecasting model acts as an agent, guided by a beam search-based planning
algorithm that leverages non-differentiable domain metrics as reward signals to
explore high-return future sequences. These identified high-reward candidates
then serve as pseudo-labels to continuously optimize the agent's policy through
iterative self-training, significantly reducing prediction error and
demonstrating exceptional performance on critical domain metrics like capturing
extreme events.

</details>


### [511] [Multi-Class Support Vector Machine with Differential Privacy](https://arxiv.org/abs/2510.04027)
*Jinseong Park,Yujin Choi,Jaewook Lee*

Main category: cs.LG

TL;DR: 提出了一种新的差分隐私多类SVM方法（PMSVM），通过一次性访问数据样本解决传统方法中隐私预算消耗过快的问题，在多分类场景下优于现有DP-SVM方法。


<details>
  <summary>Details</summary>
Motivation: 传统的OvR和OvO方法在构建多个二分类器时重复查询每个样本，导致隐私预算随类别数增加而大幅消耗，限制了差分隐私在多类SVM中的应用。

Method: 采用all-in-one SVM框架，仅访问每个数据样本一次，并结合权重扰动和梯度扰动两种差分隐私机制，提出PMSVM方法，同时提供敏感性和收敛性分析以保证隐私安全性。

Result: 实验结果表明，所提出的PMSVM在多分类任务中相比现有的DP-SVM方法具有更好的性能，同时有效控制了隐私成本。

Conclusion: PMSVM能够高效实现多类分类中的差分隐私保护，具备良好的隐私-效用权衡，为隐私保护下的SVM应用提供了更优解决方案。

Abstract: With the increasing need to safeguard data privacy in machine learning
models, differential privacy (DP) is one of the major frameworks to build
privacy-preserving models. Support Vector Machines (SVMs) are widely used
traditional machine learning models due to their robust margin guarantees and
strong empirical performance in binary classification. However, applying DP to
multi-class SVMs is inadequate, as the standard one-versus-rest (OvR) and
one-versus-one (OvO) approaches repeatedly query each data sample when building
multiple binary classifiers, thus consuming the privacy budget proportionally
to the number of classes. To overcome this limitation, we explore all-in-one
SVM approaches for DP, which access each data sample only once to construct
multi-class SVM boundaries with margin maximization properties. We propose a
novel differentially Private Multi-class SVM (PMSVM) with weight and gradient
perturbation methods, providing rigorous sensitivity and convergence analyses
to ensure DP in all-in-one SVMs. Empirical results demonstrate that our
approach surpasses existing DP-SVM methods in multi-class scenarios.

</details>


### [512] [The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View](https://arxiv.org/abs/2510.04028)
*Xinhao Yao,Lu Yu,Xiaolin Hu,Fengwei Teng,Qing Cui,Jun Zhou,Yong Liu*

Main category: cs.LG

TL;DR: 本文研究了强化学习与可验证奖励（RLVR）对大语言模型推理能力的影响，提出了一种两阶段概率质量动态理论：初期为利用阶段，模型倾向于已知高奖励词汇；后期为探索阶段，模型开始发现并增强潜在最优词汇的概率。研究表明，短期训练可能导致能力边界收缩，而长期训练则可能扩展推理能力。基于此，作者重新审视了仅使用相对负梯度延长训练的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决关于RLVR是否增强或削弱大语言模型推理能力的争议，解释为何不同研究得出了相反结论。

Method: 结合理论分析与实证研究，提出并验证了一个两阶段概率质量动态模型（利用阶段和探索阶段），用于解释RLVR在不同训练阶段对模型行为的影响。

Result: 证实了RLVR在初期可能导致过度利用、降低多样性（能力边界收缩），而在长期训练后可促进新推理策略的出现（能力边界扩展）。同时验证了仅使用相对负梯度有助于延长训练并提升推理能力。

Conclusion: RLVR对大语言模型推理能力的影响具有阶段性：短期训练可能缩小能力边界，长期训练则可扩展之。因此，延长训练并调控梯度信号有助于发展更高级的推理能力。

Abstract: The ongoing debate on whether reinforcement learning with verifiable rewards
(RLVR) expands or shrinks the reasoning capabilities of large language models
(LLMs) remains unresolved. Some studies contend that RLVR mainly improves
sampling efficiency but at the expense of diversity and exploratory capacity,
resulting in capability boundary shrinkage. In contrast, others demonstrate
that prolonged training can lead to the emergence of novel reasoning
strategies, suggesting capability boundary expansion. To reconcile these
contradictory findings, we theoretically and empirically show that both
perspectives are partially valid-each aligning with a separate phase in an
inherent two-stage probability mass dynamic: (1) Exploitation stage: initially,
the model primarily samples explored high-reward and low-reward tokens, while
rarely selecting the potentially optimal token. Positive advantage estimates
increase the probability of high-reward tokens and decrease those of low-reward
tokens, yet the optimal token's probability remains largely unchanged during
this stage. (2) Exploration stage: as training advances, the growth rate of
previously acquired high-reward tokens slows as their probabilities approach
saturation. When a potentially optimal token-now receiving positive advantage
estimates-is occasionally sampled, its probability increases, while those of
the originally high-reward tokens decrease. This dynamic suggests that
over-exploitation during the exploitation stage may lead to capability boundary
shrinkage, whereas prolonged training into the exploration stage can promote an
expansion of the reasoning capability boundary. Building upon our insights, we
revisit the potential of only using relative negative gradients for prolonging
training, providing a theoretical and empirical foundation for the development
of more advanced reasoning capabilities.

</details>


### [513] [Adaptive kernel-density approach for imbalanced binary classification](https://arxiv.org/abs/2510.04046)
*Kotaro J. Nishimura,Yuichi Sakumura,Kazushi Ikeda*

Main category: cs.LG

TL;DR: 提出一种基于核密度估计的自适应决策边界调整方法KOTARO，用于解决严重类别不平衡下的二分类问题。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡导致传统分类器对少数类识别能力差，尤其在医疗诊断和异常检测等关键领域影响严重。

Method: 通过扩展核密度估计框架，动态调整高斯基函数的带宽，根据样本局部密度自适应优化决策边界。

Result: 在合成和真实数据集上的实验表明，KOTARO在严重不平衡情况下显著优于传统方法。

Conclusion: KOTARO能有效提升少数类的识别性能，是处理严重类别不平衡问题的一种有前景的方法。

Abstract: Class imbalance is a common challenge in real-world binary classification
tasks, often leading to predictions biased toward the majority class and
reduced recognition of the minority class. This issue is particularly critical
in domains such as medical diagnosis and anomaly detection, where correct
classification of minority classes is essential. Conventional methods often
fail to deliver satisfactory performance when the imbalance ratio is extremely
severe. To address this challenge, we propose a novel approach called
Kernel-density-Oriented Threshold Adjustment with Regional Optimization
(KOTARO), which extends the framework of kernel density estimation (KDE) by
adaptively adjusting decision boundaries according to local sample density. In
KOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on
the estimated density around each sample, thereby enhancing the classifier's
ability to capture minority regions. We validated the effectiveness of KOTARO
through experiments on both synthetic and real-world imbalanced datasets. The
results demonstrated that KOTARO outperformed conventional methods,
particularly under conditions of severe imbalance, highlighting its potential
as a promising solution for a wide range of imbalanced classification problems

</details>


### [514] [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
*Shiwen Qin,Alexander Auras,Shay B. Cohen,Elliot J. Crowley,Michael Moeller,Linus Ericsson,Jovita Lukasik*

Main category: cs.LG

TL;DR: 本文提出了ONNX-Bench，一个基于ONNX格式的统一神经网络基准，包含超过60万对架构和准确率数据，支持跨搜索空间的通用性能预测；通过自然语言描述作为输入的ONNX-Net表示方法，实现了对任意神经网络结构的零样本性能预测，显著提升了神经架构搜索的效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的神经架构搜索（NAS）性能评估方法大多局限于特定的细胞结构搜索空间和图编码方式，缺乏在更广泛、表达能力更强的搜索空间中的灵活性和可扩展性，因此需要一种不受搜索空间限制的通用网络表示方法。

Method: 提出ONNX-Bench基准，整合了所有开源NAS-bench的神经网络为统一的ONNX格式；设计ONNX-Net，将神经网络结构转化为自然语言描述，并作为性能预测器的输入，实现对任意架构的通用表示和预测。

Result: 实验表明，仅用少量预训练样本即可在不同搜索空间上实现强大的零样本预测性能，能够即时评估任意神经网络架构的性能。

Conclusion: ONNX-Bench和ONNX-Net打破了传统NAS中搜索空间依赖的限制，提供了一种通用、可扩展的神经网络表示与性能评估框架，推动了高效神经架构搜索的发展。

Abstract: Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

</details>


### [515] [Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints](https://arxiv.org/abs/2510.04058)
*Subhodip Panda,MS Varun,Shreyans Jain,Sarthak Kumar Maharana,Prathosh A. P*

Main category: cs.LG

TL;DR: 提出了一种名为变分扩散遗忘（VDU）的方法，用于在数据受限情况下从预训练扩散模型中遗忘包含不期望特征的生成输出，通过最小化包含可塑性诱导项和稳定性正则项的损失函数实现高效且保持生成质量的机器遗忘。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能生成暴力、低俗等不期望内容，现有机器遗忘方法在数据受限场景下效果不佳，因此需要一种仅需访问部分含不期望特征数据即可有效遗忘的新方法。

Method: 基于变分推断框架，提出VDU方法，引入可塑性诱导项以降低不期望数据的对数似然，同时设计稳定性正则项在参数空间中约束模型，防止生成质量下降，仅需少量含目标特征的数据进行微调。

Result: 在MNIST、CIFAR-10、tinyImageNet上的类别遗忘和Stable Diffusion的特征遗忘任务中，VDU显著降低了对目标类别或特征的生成能力，同时保持了整体图像生成质量，在数据受限条件下优于现有方法。

Conclusion: VDU是一种高效且实用的扩散模型机器遗忘方法，适用于训练数据不完整访问的场景，能够在抑制不期望输出的同时维持生成性能，有助于安全可控地部署扩散模型。

Abstract: For a responsible and safe deployment of diffusion models in various domains,
regulating the generated outputs from these models is desirable because such
models could generate undesired, violent, and obscene outputs. To tackle this
problem, recent works use machine unlearning methodology to forget training
data points containing these undesired features from pre-trained generative
models. However, these methods proved to be ineffective in data-constrained
settings where the whole training dataset is inaccessible. Thus, the principal
objective of this work is to propose a machine unlearning methodology that can
prevent the generation of outputs containing undesired features from a
pre-trained diffusion model in such a data-constrained setting. Our proposed
method, termed as Variational Diffusion Unlearning (VDU), is a computationally
efficient method that only requires access to a subset of training data
containing undesired features. Our approach is inspired by the variational
inference framework with the objective of minimizing a loss function consisting
of two terms: plasticity inducer and stability regularizer. Plasticity inducer
reduces the log-likelihood of the undesired training data points, while the
stability regularizer, essential for preventing loss of image generation
quality, regularizes the model in parameter space. We validate the
effectiveness of our method through comprehensive experiments for both class
unlearning and feature unlearning. For class unlearning, we unlearn some
user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a
pre-trained unconditional denoising diffusion probabilistic model (DDPM).
Similarly, for feature unlearning, we unlearn the generation of certain
high-level features from a pre-trained Stable Diffusion model

</details>


### [516] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: 本文推广了结构化状态空间对偶性（SSD），将其从标量-单位矩阵情形扩展到一般的对角状态空间模型，并揭示了其与1-半可分掩码注意力的等价条件，同时指出该对偶性无法扩展到标准softmax注意力。


<details>
  <summary>Details</summary>
Motivation: 为了加强状态空间模型（SSM）与Transformer之间的联系，拓展高效且表达能力强的序列模型设计空间，需要将已知的SSD对偶性进行推广和形式化。

Method: 通过理论分析，将SSD从标量-单位矩阵情形推广到一般对角SSM，建立SSM与1-半可分掩码注意力等价的充要条件，并分析其在训练复杂度和动态建模能力上的表现。

Result: 证明了一般对角SSM满足与标量情况相同的训练复杂度下界并支持更丰富的动态行为；给出了SSM等价于1-半可分掩码注意力的充要条件；发现该对偶性因秩爆炸问题无法推广到标准softmax注意力。

Conclusion: 研究收紧了循环SSM与Transformer之间的联系，扩展了兼具表达力与效率的序列模型的设计空间，为模型架构选择提供了新的理论依据。

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


### [517] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: 提出Reinforce-Ada，一种用于大语言模型在线强化学习的自适应采样框架，通过动态分配采样资源并减少梯度方差来提升推理任务的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在大语言模型推理任务中因固定均匀采样导致梯度估计不稳定，且采样效率低下，缺乏对不同提示学习潜力的动态响应机制。

Method: 设计Reinforce-Ada框架，采用在线连续淘汰机制动态重分配采样资源，优先关注不确定性或学习潜力高的提示；通过固定大小且奖励多样化的组别构建，并利用全局统计信息计算优势基线以稳定更新。

Result: 在多种模型架构和推理基准上验证，Reinforce-Ada相比GRPO加速了收敛并提升了最终性能，尤其是其平衡采样变体表现更优。

Conclusion: 自适应、方差感知的数据采样在提升大语言模型强化学习效率与可靠性方面具有关键作用，Reinforce-Ada为高效后训练提供了有效框架。

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [518] [Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees](https://arxiv.org/abs/2510.04088)
*Nan Jiang,Tengyang Xie*

Main category: cs.LG

TL;DR: 本文介绍了在大状态空间中从历史数据学习策略的离线强化学习理论，探讨了函数逼近的表达性假设和数据覆盖条件，并讨论了不同假设下的算法、复杂性保证以及开放问题。


<details>
  <summary>Details</summary>
Motivation: 在无法与环境进行在线交互的情况下，如何利用历史数据有效学习高质量策略是离线强化学习的核心挑战。

Method: 通过引入函数逼近的表达性假设（如Bellman完备性与可实现性）和数据覆盖假设（如全策略与单策略覆盖），构建理论框架以分析不同算法的性能。

Result: 描绘了在不同假设条件下离线强化学习算法的多样性及其在样本复杂性和计算复杂性方面的保证。

Conclusion: 该理论框架揭示了离线强化学习中关键因素之间的权衡，并指出了未来研究方向及与其他领域的联系。

Abstract: This article introduces the theory of offline reinforcement learning in large
state spaces, where good policies are learned from historical data without
online interactions with the environment. Key concepts introduced include
expressivity assumptions on function approximation (e.g., Bellman completeness
vs. realizability) and data coverage (e.g., all-policy vs. single-policy
coverage). A rich landscape of algorithms and results is described, depending
on the assumptions one is willing to make and the sample and computational
complexity guarantees one wishes to achieve. We also discuss open questions and
connections to adjacent areas.

</details>


### [519] [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
*Avichal Goel,Yoon Kim,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 提出Diff Interpretation Tuning (DIT) 方法，通过训练模型描述自身微调引起的变化，以自然语言解释权重差异。


<details>
  <summary>Details</summary>
Motivation: 微调后的模型权重变化难以解释，且微调数据集常不可公开获取或过大，难以直接分析。

Method: 使用合成的、带标签的权重差异数据训练DIT适配器，使其能应用于兼容的微调模型，生成对模型变化的自然语言描述。

Result: 在两个概念验证场景中展示了该方法能准确用自然语言描述模型的微调变化，包括报告隐藏行为和总结微调知识。

Conclusion: DIT方法有助于全面理解模型微调带来的参数变化，提升模型可解释性。

Abstract: Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

</details>


### [520] [Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes](https://arxiv.org/abs/2510.04090)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 提出一种不依赖类别数量的神经网络训练方法，通过预定义向量系统作为目标潜在空间配置（LSC），实现相同网络结构在不同类别数下的训练，适用于超大规模或未知类别数的分类任务。


<details>
  <summary>Details</summary>
Motivation: 监督学习通常要求神经网络参数数量与类别数相关，限制了其在类别数极大或未知场景下的应用，因此需要一种类别无关的训练方法。

Method: 使用预定义的向量系统（如An根系统的随机扰动向量）作为目标潜在空间配置（LSC），在训练中使网络输出匹配这些向量，从而解耦网络结构与类别数量的关系。

Result: 在Cinic-10和ImageNet-1K上成功训练了编码器和视觉Transformer（ViT），并在包含128万类的数据集上验证了方法的可行性，展示了其在大规模分类任务中的有效性。

Conclusion: 该方法实现了类别数无关的神经网络训练，具有良好的扩展性和通用性，可应用于大规模分类、持续学习和网络蒸馏等场景。

Abstract: Supervised learning (SL) methods are indispensable for neural network (NN)
training used to perform classification tasks. While resulting in very high
accuracy, SL training often requires making NN parameter number dependent on
the number of classes, limiting their applicability when the number of classes
is extremely large or unknown in advance. In this paper we propose a
methodology that allows one to train the same NN architecture regardless of the
number of classes. This is achieved by using predefined vector systems as the
target latent space configuration (LSC) during NN training. We discuss the
desired properties of target configurations and choose randomly perturbed
vectors of An root system for our experiments. These vectors are used to
successfully train encoders and visual transformers (ViT) on Cinic-10 and
ImageNet-1K in low- and high-dimensional cases by matching NN predictions with
the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million
classes illustrating the applicability of the method to training on datasets
with extremely large number of classes. In addition, potential applications of
LSC in lifelong learning and NN distillation are discussed illustrating
versatility of the proposed methodology.

</details>


### [521] [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: 本文提出了Bias--Variance Optimized Preference Optimization (BVPO)，一种用于大推理模型偏好对齐的新方法，通过结合高低方差梯度估计器来降低训练中的方差，从而提升模型在对齐和推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在多步和数学任务上表现优异，但其与人类偏好的对齐仍缺乏探索；现有方法因采样推理路径导致梯度方差大，影响训练稳定性。

Method: 提出BVPO方法，混合使用高方差的基于推理轨迹的梯度估计器和低方差的无推理轨迹（空轨迹）估计器，并理论推导最优混合权重以最小化均方误差。

Result: BVPO在AlpacaEval 2上比最佳基线提升7.8分，在Arena-Hard上提升6.8分；即使仅用对话数据训练，也在六个数学推理基准上平均提升4.0分。

Conclusion: 推理路径采样带来的方差是偏好对齐的关键瓶颈，直接优化偏差-方差权衡可实现更稳定的训练和更强的整体性能。

Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

</details>


### [522] [Rethinking Consistent Multi-Label Classification under Inexact Supervision](https://arxiv.org/abs/2510.04091)
*Wei Wang,Tianhao Ma,Ming-Kun Xie,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文提出了一种统一的、无需依赖候选标签或互补标签生成过程精确估计的一致性方法，用于解决部分多标签学习和互补多标签学习问题。


<details>
  <summary>Details</summary>
Motivation: 由于精确标注多标签数据成本高昂，部分和互补多标签学习作为弱监督范式被提出，但现有方法通常需准确估计标签生成过程或假设均匀分布，这在现实中难以满足。

Method: 提出了基于一阶和二阶策略的两种无偏风险估计器，以一致地处理这两种学习范式，并在理论上证明了其一致性及收敛速率。

Result: 理论分析证明了所提风险估计器在两种常用多标签分类评估指标下的一致性，并推导了估计误差的收敛率；实验结果表明所提方法优于当前最优方法。

Conclusion: 所提出的方法能够在不依赖标签生成过程精确建模的情况下，有效且一致地处理部分和互补多标签学习问题。

Abstract: Partial multi-label learning and complementary multi-label learning are two
popular weakly supervised multi-label classification paradigms that aim to
alleviate the high annotation costs of collecting precisely annotated
multi-label data. In partial multi-label learning, each instance is annotated
with a candidate label set, among which only some labels are relevant; in
complementary multi-label learning, each instance is annotated with
complementary labels indicating the classes to which the instance does not
belong. Existing consistent approaches for the two paradigms either require
accurate estimation of the generation process of candidate or complementary
labels or assume a uniform distribution to eliminate the estimation problem.
However, both conditions are usually difficult to satisfy in real-world
scenarios. In this paper, we propose consistent approaches that do not rely on
the aforementioned conditions to handle both problems in a unified way.
Specifically, we propose two unbiased risk estimators based on first- and
second-order strategies. Theoretically, we prove consistency w.r.t. two widely
used multi-label classification evaluation metrics and derive convergence rates
for the estimation errors of the proposed risk estimators. Empirically,
extensive experimental results validate the effectiveness of our proposed
approaches against state-of-the-art methods.

</details>


### [523] [Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws](https://arxiv.org/abs/2510.04102)
*Ramzi Dakhmouche,Hossein Gorji*

Main category: cs.LG

TL;DR: 本文探讨了基础模型（FMs）在时间序列预测中长程外推能力不足的问题，分析了其与物理定律在可外推性上的根本差异，提出并形式化了一个描述统计学习模型外推能力的关键性质，并通过理论与实证研究揭示了当前深度学习模型在外推性能下降的根本原因，为设计具备强外推能力的下一代预测模型提供了方向。


<details>
  <summary>Details</summary>
Motivation: 受语言模型中基础模型成功的启发，研究者希望开发适用于时间序列预测的基础模型，但现有模型在长程预测或外推任务上表现不佳，甚至不如简单基线，而物理定律具有良好的外推性，因此需要探究神经网络结构与物理规律之间的根本差异。

Method: 提出并形式化一个刻画统计学习模型在训练域外预测能力的理论性质，结合理论分析与实证研究，评估该性质对当前深度学习架构外推性能的影响。

Result: 揭示了深度学习模型在外推任务中性能下降的根本原因，实证结果表明该性质显著影响现有模型的外推能力。

Conclusion: 当前基础模型在外推方面的局限源于其结构特性，未来可通过融入更具结构性归纳偏置的设计来构建具备强外推能力的下一代预测模型。

Abstract: Motivated by the remarkable success of Foundation Models (FMs) in language
modeling, there has been growing interest in developing FMs for time series
prediction, given the transformative power such models hold for science and
engineering. This culminated in significant success of FMs in short-range
forecasting settings. However, extrapolation or long-range forecasting remains
elusive for FMs, which struggle to outperform even simple baselines. This
contrasts with physical laws which have strong extrapolation properties, and
raises the question of the fundamental difference between the structure of
neural networks and physical laws. In this work, we identify and formalize a
fundamental property characterizing the ability of statistical learning models
to predict more accurately outside of their training domain, hence explaining
performance deterioration for deep learning models in extrapolation settings.
In addition to a theoretical analysis, we present empirical results showcasing
the implications of this property on current deep learning architectures. Our
results not only clarify the root causes of the extrapolation gap but also
suggest directions for designing next-generation forecasting models capable of
mastering extrapolation.

</details>


### [524] [Can Linear Probes Measure LLM Uncertainty?](https://arxiv.org/abs/2510.04108)
*Ramzi Dakhmouche,Adrien Letellier,Hossein Gorji*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯线性模型的高效不确定性量化方法，通过层间后验分布的稀疏特征组合，在多选择结构的LLM生成中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在多选生成任务中的不确定性量化主要依赖最大softmax分数这一简单基线，缺乏有效性和可靠性，需要更原则性的方法提升性能。

Method: 采用贝叶斯统计方法，训练多个贝叶斯线性模型，逐层预测输出并获得层级别后验分布，进而提取稀疏分布特征以推断整体不确定性。

Result: 在多种大语言模型上的实验表明，该方法在不确定性量化性能上一致优于当前最先进的基线方法。

Conclusion: 基于贝叶斯线性模型和层间后验推理的UQ方案为大语言模型提供了更有效且高效的不确定性估计途径。

Abstract: Effective Uncertainty Quantification (UQ) represents a key aspect for
reliable deployment of Large Language Models (LLMs) in automated
decision-making and beyond. Yet, for LLM generation with multiple choice
structure, the state-of-the-art in UQ is still dominated by the naive baseline
given by the maximum softmax score. To address this shortcoming, we demonstrate
that taking a principled approach via Bayesian statistics leads to improved
performance despite leveraging the simplest possible model, namely linear
regression. More precisely, we propose to train multiple Bayesian linear
models, each predicting the output of a layer given the output of the previous
one. Based on the obtained layer-level posterior distributions, we infer the
global uncertainty level of the LLM by identifying a sparse combination of
distributional features, leading to an efficient UQ scheme. Numerical
experiments on various LLMs show consistent improvement over state-of-the-art
baselines.

</details>


### [525] [Wasserstein projection distance for fairness testing of regression models](https://arxiv.org/abs/2510.04114)
*Wanxin Li,Yongjin P. Park,Khanh Dao Duc*

Main category: cs.LG

TL;DR: 本文提出了一种基于Wasserstein投影的回归模型公平性检验框架，结合假设检验与最优数据扰动方法，在保证准确率的同时提升公平性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习公平性研究多集中于分类任务，而对回归模型的关注较少，本文旨在填补这一空白。

Method: 提出基于Wasserstein投影的假设检验方法和最优数据扰动策略，引入对偶形式化表达、渐近界和极限分布理论分析。

Result: 在合成与真实数据集上的实验表明，该方法相比置换检验具有更高的特异性，能有效检测并缓解学生表现和房价预测中的偏差。

Conclusion: 该框架为回归任务中的公平性评估与改进提供了理论支持和实用工具，具有良好的应用前景。

Abstract: Fairness in machine learning is a critical concern, yet most research has
focused on classification tasks, leaving regression models underexplored. This
paper introduces a Wasserstein projection-based framework for fairness testing
in regression models, focusing on expectation-based criteria. We propose a
hypothesis-testing approach and an optimal data perturbation method to improve
fairness while balancing accuracy. Theoretical results include a detailed
categorization of fairness criteria for regression, a dual reformulation of the
Wasserstein projection test statistic, and the derivation of asymptotic bounds
and limiting distributions. Experiments on synthetic and real-world datasets
demonstrate that the proposed method offers higher specificity compared to
permutation-based tests, and effectively detects and mitigates biases in real
applications such as student performance and housing price prediction.

</details>


### [526] [On the Statistical Query Complexity of Learning Semiautomata: a Random Walk Approach](https://arxiv.org/abs/2510.04115)
*George Giapitzakis,Kimon Fountoulakis,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: 本文首次证明了在输入词和初始状态的均匀分布下，半自动机（semiautomata）具有统计查询（Statistical Query）硬度，该硬度源于其内部状态转移结构，而非所识别语言的复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究半自动机在统计查询模型下的计算难度，揭示其与确定性有限自动机不同的 hardness 来源。

Method: 将区分两个半自动机最终状态的问题转化为在群 $S_N \times S_N$ 上的随机游走问题，并结合傅里叶分析和对称群的表示理论，推导出谱间隙的紧界。

Result: 当字母表大小和输入长度关于状态数为多项式时，经过多项式步数后不同半自动机几乎不相关，从而证明了统计查询硬度。

Conclusion: 半自动机的统计查询硬度可仅由其内部状态转移结构引起，无需依赖语言识别难度，拓展了对序列处理模型计算复杂性的理解。

Abstract: Semiautomata form a rich class of sequence-processing algorithms with
applications in natural language processing, robotics, computational biology,
and data mining. We establish the first Statistical Query hardness result for
semiautomata under the uniform distribution over input words and initial
states. We show that Statistical Query hardness can be established when both
the alphabet size and input length are polynomial in the number of states.
Unlike the case of deterministic finite automata, where hardness typically
arises through the hardness of the language they recognize (e.g., parity), our
result is derived solely from the internal state-transition structure of
semiautomata. Our analysis reduces the task of distinguishing the final states
of two semiautomata to studying the behavior of a random walk on the group
$S_{N} \times S_{N}$. By applying tools from Fourier analysis and the
representation theory of the symmetric group, we obtain tight spectral gap
bounds, demonstrating that after a polynomial number of steps in the number of
states, distinct semiautomata become nearly uncorrelated, yielding the desired
hardness result.

</details>


### [527] [Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions](https://arxiv.org/abs/2510.04126)
*Ziying Zhang,Yaqing Wang,Yuxuan Sun,Min Ye,Quanming Yao*

Main category: cs.LG

TL;DR: 本文提出ColdDTI，一种利用蛋白质多级结构进行冷启动药物-靶点相互作用预测的框架，通过分层注意力机制挖掘药物与蛋白质在不同层级结构上的相互作用，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅使用蛋白质的一级结构表示，限制了对涉及高级结构的药物-靶点相互作用的捕捉能力，而蛋白质组学研究表明多级结构均影响相互作用。

Method: 提出ColdDTI框架，采用分层注意力机制，在局部和全局粒度上挖掘药物与蛋白质从一级到四级结构的相互作用，并融合各层级结构表征用于最终预测。

Result: 在多个基准数据集上的实验表明，ColdDTI在冷启动设置下 consistently 优于先前方法。

Conclusion: ColdDTI通过有效利用蛋白质多级结构信息和分层注意力机制，提升了冷启动药物-靶点相互作用预测的性能，具有更强的生物学可解释性和泛化能力。

Abstract: Cold-start drug-target interaction (DTI) prediction focuses on interaction
between novel drugs and proteins. Previous methods typically learn transferable
interaction patterns between structures of drug and proteins to tackle it.
However, insight from proteomics suggest that protein have multi-level
structures and they all influence the DTI. Existing works usually represent
protein with only primary structures, limiting their ability to capture
interactions involving higher-level structures. Inspired by this insight, we
propose ColdDTI, a framework attending on protein multi-level structure for
cold-start DTI prediction. We employ hierarchical attention mechanism to mine
interaction between multi-level protein structures (from primary to quaternary)
and drug structures at both local and global granularities. Then, we leverage
mined interactions to fuse structure representations of different levels for
final prediction. Our design captures biologically transferable priors,
avoiding the risk of overfitting caused by excessive reliance on representation
learning. Experiments on benchmark datasets demonstrate that ColdDTI
consistently outperforms previous methods in cold-start settings.

</details>


### [528] [On the Limitations and Capabilities of Position Embeddings for Length Generalization](https://arxiv.org/abs/2510.04130)
*Yang Chen,Yitao Liang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文研究了Transformer中位置嵌入（PEs）对长度泛化（LG）的影响，提出了线性表示复杂度（LRC）和序列表示复杂度（SRC）来理论分析PEs的作用，并提出Scale Hint和基于学习的位置嵌入框架以提升LG性能。


<details>
  <summary>Details</summary>
Motivation: 位置嵌入在Transformer的长度泛化中起关键作用，但其根本机制尚不清楚，本文旨在揭示PEs在长度泛化中的能力与局限。

Method: 通过在仅位置线性注意力（POLAs）中进行理论分析，引入LRC概念；扩展到实际Transformer模型中提出SRC，并假设其跨尺度不变性是实现LG的关键；结合多种推理任务的实验证据验证假设。

Result: 理论表明PEs不扩展计算能力而是组织位置间的计算结构；实验支持SRC不变性与LG能力相关；提出的Scale Hint和学习型PE框架能有效提升长度泛化性能。

Conclusion: 位置嵌入通过结构化位置间的学习计算影响长度泛化，保持SRC不变性可能是实现良好LG的关键，所提方法为改进Transformer的泛化能力提供了理论依据和实践方案。

Abstract: In Transformers, Position Embeddings (PEs) significantly influence Length
Generalization (LG) performance, yet their fundamental role remains unclear. In
this work, we investigate the limitations and capabilities of PEs in achieving
LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs),
introducing Linear Representation Complexity (LRC) to characterize when PEs
enable LG. Our analysis shows that PEs do not expand computational capabilities
but structure learned computations across positions. Extending to practical
Transformers, we propose Sequential Representation Complexity (SRC) and
conjecture that LG is possible if and only if SRC remains invariant across
scales. We support this hypothesis with empirical evidence in various reasoning
tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance
scaling, and a Learning-Based Position Embedding framework that automatically
learns positional relations. Our work provides theoretical insights and
practical strategies for improving LG in Transformers.

</details>


### [529] [Modeling Time Series Dynamics with Fourier Ordinary Differential Equations](https://arxiv.org/abs/2510.04133)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 提出傅里叶常微分方程（FODEs），将时间序列数据转换到频域建模，结合可学习滤波机制，在准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决神经ODE在处理时间序列时难以捕捉长期依赖和周期结构，以及连续模型与离散数据之间不匹配的问题。

Method: 使用FFT将时间序列转换到傅里叶域建模，并引入可学习的逐元素滤波机制以对齐连续模型输出与离散观测。

Result: 在多个时间序列数据集上实验表明，FODEs在预测精度和计算效率方面均优于现有方法。

Conclusion: FODEs通过频域建模有效捕捉长短时序模式，为时间序列建模提供了更鲁棒的框架。

Abstract: Neural ODEs (NODEs) have emerged as powerful tools for modeling time series
data, offering the flexibility to adapt to varying input scales and capture
complex dynamics. However, they face significant challenges: first, their
reliance on time-domain representations often limits their ability to capture
long-term dependencies and periodic structures; second, the inherent mismatch
between their continuous-time formulation and the discrete nature of real-world
data can lead to loss of granularity and predictive accuracy. To address these
limitations, we propose Fourier Ordinary Differential Equations (FODEs), an
approach that embeds the dynamics in the Fourier domain. By transforming
time-series data into the frequency domain using the Fast Fourier Transform
(FFT), FODEs uncover global patterns and periodic behaviors that remain elusive
in the time domain. Additionally, we introduce a learnable element-wise
filtering mechanism that aligns continuous model outputs with discrete
observations, preserving granularity and enhancing accuracy. Experiments on
various time series datasets demonstrate that FODEs outperform existing methods
in terms of both accuracy and efficiency. By effectively capturing both long-
and short-term patterns, FODEs provide a robust framework for modeling time
series dynamics.

</details>


### [530] [PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting](https://arxiv.org/abs/2510.04134)
*Yiming Niu,Jinliang Deng,Yongxin Tong*

Main category: cs.LG

TL;DR: 本文提出了PhaseFormer，一种基于相位视角建模周期性的高效时间序列预测方法，通过紧凑的相位嵌入和轻量级路由机制实现跨相位交互，在仅约1k参数下达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于patch的时间序列预测方法因参数多、计算开销大而效率低下，本文旨在从相位角度重新建模周期性以提升效率与效果。

Method: 引入相位视角，使用紧凑的相位嵌入进行相位级预测，并通过轻量级路由机制实现高效的跨相位交互。

Result: 在多个基准数据集上实验表明，PhaseFormer在约1k参数规模下性能达到SOTA，尤其在大规模复杂数据集上优于其他高效模型。

Conclusion: PhaseFormer为真正高效且有效的时间序列预测提供了新方向，显著提升了小模型在复杂场景下的表现。

Abstract: Periodicity is a fundamental characteristic of time series data and has long
played a central role in forecasting. Recent deep learning methods strengthen
the exploitation of periodicity by treating patches as basic tokens, thereby
improving predictive effectiveness. However, their efficiency remains a
bottleneck due to large parameter counts and heavy computational costs. This
paper provides, for the first time, a clear explanation of why patch-level
processing is inherently inefficient, supported by strong evidence from
real-world data. To address these limitations, we introduce a phase perspective
for modeling periodicity and present an efficient yet effective solution,
PhaseFormer. PhaseFormer features phase-wise prediction through compact phase
embeddings and efficient cross-phase interaction enabled by a lightweight
routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves
state-of-the-art performance with around 1k parameters, consistently across
benchmark datasets. Notably, it excels on large-scale and complex datasets,
where models with comparable efficiency often struggle. This work marks a
significant step toward truly efficient and effective time series forecasting.
Code is available at this repository:
https://github.com/neumyor/PhaseFormer_TSL

</details>


### [531] [Efficient Manifold-Constrained Neural ODE for High-Dimensional Datasets](https://arxiv.org/abs/2510.04138)
*Muhao Guo,Haoran Li,Yang Weng*

Main category: cs.LG

TL;DR: 提出一种基于流形学习的神经微分方程方法，通过结构保持编码器发现数据底层图结构以逼近流形，从而在高维系统中提升计算速度和精度。


<details>
  <summary>Details</summary>
Motivation: 高维系统中神经微分方程的学习面临计算量大和截断误差高的问题，且现有方法依赖于已知流形结构，在实际场景中受限。

Method: 采用结构保持编码器提取数据底层图结构以近似流形，并将该流形与神经微分方程结合，限制ODE过程在其上进行。

Result: 在多个数据集上实验表明，所提方法在精度、函数求值次数（NFEs）和收敛速度方面优于现有基线模型。

Conclusion: 该方法有效解决了高维数据下神经微分方程的计算效率与精度问题，具有更强的实用性和泛化能力。

Abstract: Neural ordinary differential equations (NODE) have garnered significant
attention for their design of continuous-depth neural networks and the ability
to learn data/feature dynamics. However, for high-dimensional systems,
estimating dynamics requires extensive calculations and suffers from high
truncation errors for the ODE solvers. To address the issue, one intuitive
approach is to consider the non-trivial topological space of the data
distribution, i.e., a low-dimensional manifold. Existing methods often rely on
knowledge of the manifold for projection or implicit transformation,
restricting the ODE solutions on the manifold. Nevertheless, such knowledge is
usually unknown in realistic scenarios. Therefore, we propose a novel approach
to explore the underlying manifold to restrict the ODE process. Specifically,
we employ a structure-preserved encoder to process data and find the underlying
graph to approximate the manifold. Moreover, we propose novel methods to
combine the NODE learning with the manifold, resulting in significant gains in
computational speed and accuracy. Our experimental evaluations encompass
multiple datasets, where we compare the accuracy, number of function
evaluations (NFEs), and convergence speed of our model against existing
baselines. Our results demonstrate superior performance, underscoring the
effectiveness of our approach in addressing the challenges of high-dimensional
datasets.

</details>


### [532] [Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity](https://arxiv.org/abs/2510.04189)
*Prashansa Panda,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文提出了首个用于长期平均成本且带有不等式约束的自然critic-actor算法，提供了非渐近收敛性分析，确定了最优学习率并改进了样本复杂度，在Safety-Gym环境中实验表明算法具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有工作在折扣成本设定下研究了critic-actor算法，但缺乏对长期平均成本和约束条件下的非渐近分析，本文填补这一空白。

Method: 提出一种带函数逼近的自然critic-actor算法，采用两时间尺度优化，针对长期平均成本和不等式约束场景进行设计，并理论分析其收敛性。

Result: 给出了该算法的非渐近收敛保证，确立了最优学习率，并通过修改提升了样本复杂度。

Conclusion: 所提算法在理论上有良好收敛性，在多个Safety-Gym环境中实验表现与其他知名算法相当，验证了其有效性。

Abstract: Recent studies have increasingly focused on non-asymptotic convergence
analyses for actor-critic (AC) algorithms. One such effort introduced a
two-timescale critic-actor algorithm for the discounted cost setting using a
tabular representation, where the usual roles of the actor and critic are
reversed. However, only asymptotic convergence was established there.
Subsequently, both asymptotic and non-asymptotic analyses of the critic-actor
algorithm with linear function approximation were conducted. In our work, we
introduce the first natural critic-actor algorithm with function approximation
for the long-run average cost setting and under inequality constraints. We
provide the non-asymptotic convergence guarantees for this algorithm. Our
analysis establishes optimal learning rates and we also propose a modification
to enhance sample complexity. We further show the results of experiments on
three different Safety-Gym environments where our algorithm is found to be
competitive in comparison with other well known algorithms.

</details>


### [533] [Spectral Alignment as Predictor of Loss Explosion in Neural Network Training](https://arxiv.org/abs/2510.04202)
*Haiquan Qiu,You Wu,Yingjie Tan,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: 提出了一种名为谱对齐（Spectral Alignment, SA）的新指标，用于早期预测深度神经网络训练中的损失爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 传统监控指标如权重和梯度范数在不同模型或同一模型的不同层之间变化较大，难以统一判断训练失败的前兆，因此需要一种更稳定、更具解释性的早期预警指标。

Method: 通过监测层输入与权重矩阵主奇异向量之间的分布对齐程度，引入谱对齐（SA）指标，并理论分析其与表示崩溃的关系。

Result: 实验表明，SA比传统的标量指标能更早、更清晰地预警损失爆炸，且计算开销低。

Conclusion: SA是一种有效、高效且具有理论基础的训练监控工具，可用于保护大规模模型训练免受损失爆炸影响。

Abstract: Loss explosions in training deep neural networks can nullify multi-million
dollar training runs. Conventional monitoring metrics like weight and gradient
norms are often lagging and ambiguous predictors, as their values vary
dramatically across different models and even between layers of the same model,
making it difficult to establish a unified standard for detecting impending
failure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded
metric that monitors the distributional alignment between layer inputs and the
principal singular vectors of weight matrices. We show that a collapse in the
sign diversity of this alignment is a powerful early predictor of
representational collapse and training divergence. Empirical results on
language models demonstrate that monitoring the SA distribution provides a
significantly earlier and clearer warning of loss explosions than traditional
scalar metrics. SA's low computational overhead makes it a practical tool for
safeguarding model training.

</details>


### [534] [Adaptive Federated Learning via Dynamical System Model](https://arxiv.org/abs/2510.04203)
*Aayushya Agarwal,Larry Pileggi,Gauri Joshi*

Main category: cs.LG

TL;DR: 提出一种端到端的自适应联邦学习方法，通过将联邦学习建模为动力系统，动态调整客户端和中心服务器的学习率与动量参数，仅需一个全局超参数即可实现快速、稳定的收敛，无需手动调参。


<details>
  <summary>Details</summary>
Motivation: 在异构联邦学习中，由于客户端计算能力不同且数据分布非独立同分布（non-IID），超参数选择对收敛至关重要，但传统调参过程繁琐且计算成本高，尤其当客户端数量增加时超参数空间呈组合式增长。

Method: 将联邦学习建模为动力系统，借鉴数值模拟和物理设计原理：用临界阻尼思想选择动量参数以实现快速稳定收敛；根据数值模拟的精度要求自适应选择客户端和中心服务器的学习率。

Result: 提出了一种基于动量的自适应联邦学习算法，可动态调整学习率并仅依赖单一全局超参数；该方法能有效应对异构联邦学习中的目标不一致和客户端漂移问题，在无需调参的情况下实现比现有最先进方法更快的收敛速度。

Conclusion: 该方法实现了客户端更新和中心聚合的完全集成化自适应控制，对全局超参数不敏感，适合快速原型开发和大规模部署，显著降低了超参数调优的需求。

Abstract: Hyperparameter selection is critical for stable and efficient convergence of
heterogeneous federated learning, where clients differ in computational
capabilities, and data distributions are non-IID. Tuning hyperparameters is a
manual and computationally expensive process as the hyperparameter space grows
combinatorially with the number of clients. To address this, we introduce an
end-to-end adaptive federated learning method in which both clients and central
agents adaptively select their local learning rates and momentum parameters.
Our approach models federated learning as a dynamical system, allowing us to
draw on principles from numerical simulation and physical design. Through this
perspective, selecting momentum parameters equates to critically damping the
system for fast, stable convergence, while learning rates for clients and
central servers are adaptively selected to satisfy accuracy properties from
numerical simulation. The result is an adaptive, momentum-based federated
learning algorithm in which the learning rates for clients and servers are
dynamically adjusted and controlled by a single, global hyperparameter. By
designing a fully integrated solution for both adaptive client updates and
central agent aggregation, our method is capable of handling key challenges of
heterogeneous federated learning, including objective inconsistency and client
drift. Importantly, our approach achieves fast convergence while being
insensitive to the choice of the global hyperparameter, making it well-suited
for rapid prototyping and scalable deployment. Compared to state-of-the-art
adaptive methods, our framework is shown to deliver superior convergence for
heterogeneous federated learning while eliminating the need for hyperparameter
tuning both client and server updates.

</details>


### [535] [PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression](https://arxiv.org/abs/2510.04205)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出PolyKAN，一种具有理论保证的Kolmogorov-Arnold网络（KAN）压缩新框架，通过最优多面体区域合并实现模型大小缩减和逼近误差控制。


<details>
  <summary>Details</summary>
Motivation: KAN虽然具有良好的可解释性和数学基础，但参数效率低，限制了其实际部署，因此需要有效的压缩方法。

Method: 利用KAN的分段多项式结构，建立严格的多面体表征，提出ε-等价压缩理论，并设计基于动态规划的最优压缩算法。

Result: PolyKAN在指定误差范围内实现了最小压缩，时间复杂度为多项式级，且在模型压缩率和误差控制之间取得理论上的最优平衡。

Conclusion: PolyKAN为KAN压缩提供了首个具有数学保证的理论基础，推动了可解释性神经网络的高效部署。

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability
and a strong mathematical foundation. However, their parameter efficiency
remains a significant challenge for practical deployment. This paper introduces
PolyKAN, a novel theoretical framework for KAN compression that provides formal
guarantees on both model size reduction and approximation error. By leveraging
the inherent piecewise polynomial structure of KANs, we formulate the
compression problem as one of optimal polyhedral region merging. We establish a
rigorous polyhedral characterization of KANs, develop a complete theory of
$\epsilon$-equivalent compression, and design an optimal dynamic programming
algorithm that guarantees minimal compression under specified error bounds. Our
theoretical analysis demonstrates that PolyKAN achieves provably minimal
compression while maintaining strict error control, with polynomial-time
complexity in all network parameters. The framework provides the first formal
foundation for KAN compression with mathematical guarantees, opening new
directions for efficient deployment of interpretable neural architectures.

</details>


### [536] [Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention](https://arxiv.org/abs/2510.04212)
*Haiquan Qiu,Quanming Yao*

Main category: cs.LG

TL;DR: 本文首次揭示了在低精度设置下使用Flash Attention训练Transformer模型时出现灾难性损失爆炸的机制，并提出了一种简单的修改方法来缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 解决低精度训练中常见的不稳定问题，特别是Flash Attention在低精度下导致的损失爆炸。

Method: 通过深入分析注意力机制中的低秩表示和低精度算术中的偏差舍入误差，提出对Flash Attention进行微小修改以减少舍入偏差。

Result: 发现了导致训练失败的两个关键因素：注意力机制中相似低秩表示的出现和低精度舍入误差的累积效应，并通过改进的Flash Attention验证了其有效性。

Conclusion: 提出的修改能有效稳定低精度下的训练过程，为解决长期存在的训练不稳定性问题提供了实用方案。

Abstract: The pursuit of computational efficiency has driven the adoption of
low-precision formats for training transformer models. However, this progress
is often hindered by notorious training instabilities. This paper provides the
first mechanistic explanation for a long-standing and unresolved failure case
where training with flash attention in low-precision settings leads to
catastrophic loss explosions. Our in-depth analysis reveals that the failure is
not a random artifact but caused by two intertwined phenomena: the emergence of
similar low-rank representations within the attention mechanism and the
compounding effect of biased rounding errors inherent in low-precision
arithmetic. We demonstrate how these factors create a vicious cycle of error
accumulation that corrupts weight updates, ultimately derailing the training
dynamics. To validate our findings, we introduce a minimal modification to the
flash attention that mitigates the bias in rounding errors. This simple change
stabilizes the training process, confirming our analysis and offering a
practical solution to this persistent problem.

</details>


### [537] [MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering](https://arxiv.org/abs/2510.04217)
*Chenlu Ding,Jiancan Wu,Leheng Sheng,Fan Zhang,Yancheng Yuan,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: 本文提出MLLMEraser，一种无需训练、基于输入感知的测试时多模态大语言模型遗忘框架，通过激活引导实现动态知识擦除。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM遗忘方法计算成本高、不可逆且易破坏保留知识，亟需高效、可逆且低干扰的遗忘机制。

Method: 利用对抗扰动构建图文对的多模态擦除方向，并设计输入感知的自适应引导机制，在测试时动态控制知识擦除。

Result: 在LLaVA-1.5和Qwen-2.5-VL上实验表明，MLLMEraser优于现有方法，遗忘效果更强，计算成本更低，且对模型效用影响极小。

Conclusion: MLLMEraser为多模态大模型提供了一种高效、灵活且实用的知识遗忘方案，具有良好的部署前景。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities across vision-language tasks, yet their large-scale deployment
raises pressing concerns about memorized private data, outdated knowledge, and
harmful content. Existing unlearning approaches for MLLMs typically adapt
training-based strategies such as gradient ascent or preference optimization,
but these methods are computationally expensive, irreversible, and often
distort retained knowledge. In this work, we propose MLLMEraser, an
input-aware, training-free framework for test-time unlearning. Our approach
leverages activation steering to enable dynamic knowledge erasure without
parameter updates. Specifically, we construct a multimodal erasure direction by
contrasting adversarially perturbed, knowledge-recall image-text pairs with
knowledge-erasure counterparts, capturing both textual and visual
discrepancies. To prevent unnecessary interference, we further design an
input-aware steering mechanism that adaptively determines when and how the
erasure direction should be applied, preserving utility on retained knowledge
while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and
Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms
state-of-the-art MLLM unlearning baselines, achieving stronger forgetting
performance with lower computational cost and minimal utility degradation.

</details>


### [538] [Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling](https://arxiv.org/abs/2510.04233)
*Kai Yang,Yuqi Huang,Junheng Tao,Wanyu Wang,Qitian Wu*

Main category: cs.LG

TL;DR: 本文提出了一种名为PAINET的SE(3)-等变神经网络架构，用于学习多体系统中的全对相互作用，通过物理启发的注意力机制和并行解码器，在多个真实世界基准上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的方法依赖于显式观测结构，难以捕捉复杂物理行为中关键的未观测相互作用，因此需要一种能有效建模隐式交互的通用框架。

Method: PAINET基于能量函数最小化轨迹设计了物理启发的注意力网络，并采用保持等变性的并行解码器，实现高效推理，且整体架构具有SE(3)等变性。

Result: 在人体动作捕捉、分子动力学和大规模蛋白质模拟等多个真实数据集上，PAINET相比最新模型实现了4.7%到41.5%的误差降低，同时计算时间和内存开销相当。

Conclusion: PAINET能够有效建模多体系统中未被观测的全对相互作用，在3D动态预测任务中表现出优越性能和良好泛化能力，为复杂物理系统的建模提供了新思路。

Abstract: Modeling 3D dynamics is a fundamental problem in multi-body systems across
scientific and engineering domains and has important practical implications in
trajectory prediction and simulation. While recent GNN-based approaches have
achieved strong performance by enforcing geometric symmetries, encoding
high-order features or incorporating neural-ODE mechanics, they typically
depend on explicitly observed structures and inherently fail to capture the
unobserved interactions that are crucial to complex physical behaviors and
dynamics mechanism. In this paper, we propose PAINET, a principled
SE(3)-equivariant neural architecture for learning all-pair interactions in
multi-body systems. The model comprises: (1) a novel physics-inspired attention
network derived from the minimization trajectory of an energy function, and (2)
a parallel decoder that preserves equivariance while enabling efficient
inference. Empirical results on diverse real-world benchmarks, including human
motion capture, molecular dynamics, and large-scale protein simulations, show
that PAINET consistently outperforms recently proposed models, yielding 4.7% to
41.5% error reductions in 3D dynamics prediction with comparable computation
costs in terms of time and memory.

</details>


### [539] [Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions](https://arxiv.org/abs/2510.04237)
*Jinhui Bai,Andreas Christmann,Lei Shi*

Main category: cs.LG

TL;DR: 提出了一种新的核随机梯度下降算法，通过创新的正则化策略和有限维假设空间投影，实现了大规模监督学习中优化与泛化的统一分析，并在多种经典损失函数下达到最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 传统核SGD在处理大规模数据时效率低、可扩展性差，且计算复杂度高，难以适应实际应用需求。

Method: 利用球面径向基函数的无穷级数展开，将随机梯度投影到自适应调整的有限维假设空间，并结合对核诱导协方差算子谱结构的新估计，构建统一的分析框架。

Result: 证明了最后迭代点和后缀平均均以最小最大最优速率收敛，并建立了再生核希尔伯特空间中的最优强收敛性；算法显著降低计算与存储复杂度，支持流式数据高效处理。

Conclusion: 所提方法在保持核方法强大表达能力的同时，大幅提升了效率和可扩展性，适用于多种经典损失函数的大规模学习任务。

Abstract: In this paper, we propose a novel kernel stochastic gradient descent (SGD)
algorithm for large-scale supervised learning with general losses. Compared to
traditional kernel SGD, our algorithm improves efficiency and scalability
through an innovative regularization strategy. By leveraging the infinite
series expansion of spherical radial basis functions, this strategy projects
the stochastic gradient onto a finite-dimensional hypothesis space, which is
adaptively scaled according to the bias-variance trade-off, thereby enhancing
generalization performance. Based on a new estimation of the spectral structure
of the kernel-induced covariance operator, we develop an analytical framework
that unifies optimization and generalization analyses. We prove that both the
last iterate and the suffix average converge at minimax-optimal rates, and we
further establish optimal strong convergence in the reproducing kernel Hilbert
space. Our framework accommodates a broad class of classical loss functions,
including least-squares, Huber, and logistic losses. Moreover, the proposed
algorithm significantly reduces computational complexity and achieves optimal
storage complexity by incorporating coordinate-wise updates from linear SGD,
thereby avoiding the costly pairwise operations typical of kernel SGD and
enabling efficient processing of streaming data. Finally, extensive numerical
experiments demonstrate the efficiency of our approach.

</details>


### [540] [Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs](https://arxiv.org/abs/2510.04241)
*Seong Jin Ahn,Myoung-Ho Kim*

Main category: cs.LG

TL;DR: 提出了一种名为DAD-SGM的扩散辅助蒸馏方法，用于将自监督图神经网络的知识有效蒸馏到MLP中，提升了MLP在图表示学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 自监督学习中GNN与MLP之间存在较大的能力差距，且自监督学习更依赖模型的归纳偏置，使得知识蒸馏更具挑战性，因此需要设计新的蒸馏方法。

Method: 引入去噪扩散模型作为教师助手，协助将教师GNN的知识更好地蒸馏到学生MLP中。

Result: 实验表明，DAD-SGM在自监督GNN到MLP的知识蒸馏上优于现有最先进方法，显著提升了MLP的泛化性和鲁棒性。

Conclusion: DAD-SGM有效缩小了自监督图表示学习中GNN与MLP之间的性能差距，为轻量级模型提供了更强的表示能力。

Abstract: For large-scale applications, there is growing interest in replacing Graph
Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via
knowledge distillation. However, distilling GNNs for self-supervised graph
representation learning into MLPs is more challenging. This is because the
performance of self-supervised learning is more related to the model's
inductive bias than supervised learning. This motivates us to design a new
distillation method to bridge a huge capacity gap between GNNs and MLPs in
self-supervised graph representation learning. In this paper, we propose
\textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for
\textbf{S}elf-supervised \textbf{G}raph representation learning with
\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion
model as a teacher assistant to better distill the knowledge from the teacher
GNN into the student MLP. This approach enhances the generalizability and
robustness of MLPs in self-supervised graph representation learning. Extensive
experiments demonstrate that DAD-SGM effectively distills the knowledge of
self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation
methods. Our implementation is available at
https://github.com/SeongJinAhn/DAD-SGM.

</details>


### [541] [Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing](https://arxiv.org/abs/2510.04263)
*Joseph Ramsey,Bryan Andrews*

Main category: cs.LG

TL;DR: 本文提出了一族基于评分引导的混合策略因果搜索算法，用于在存在潜变量或选择偏差的情况下从观测数据中学习因果结构。主要方法包括BOSS-FCI、GRaSP-FCI、FCI靶向测试（FCIT）以及启发式方法LV-Dumb（即BOSS-POD）。这些方法在保持正确性的同时提升了效率与精度，尤其在可扩展性和实际准确性方面表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统FCI算法在存在潜变量或选择偏差时需进行大量条件独立性检验，容易导致错误的独立性判断、边的遗漏或多余以及方向不可靠。因此需要更高效且精确的方法来改进因果结构学习。

Method: 作者提出了几种新方法：BOSS-FCI和GRaSP-FCI是GFCI的变体，用BOSS或GRaSP替代FGES；FCIT采用评分引导的靶向测试取代全子集穷举测试；LV-Dumb则跳过对潜变量的复杂推理，直接输出BOSS DAG对应的PAG。

Result: 实验表明，BOSS-FCI和GRaSP-FCI提供了可靠的基线性能，FCIT在效率和可靠性上均有提升，而LV-Dumb虽然理论上不完全正确，但在实践中具有更好的可扩展性和更高的准确率。

Conclusion: 评分引导和靶向测试策略能有效提升含潜变量因果发现的可扩展性与实用性，为大规模因果结构学习提供了新的可行路径。

Abstract: Learning causal structure from observational data is especially challenging
when latent variables or selection bias are present. The Fast Causal Inference
(FCI) algorithm addresses this setting but often performs exhaustive
conditional independence tests across many subsets, leading to spurious
independence claims, extra or missing edges, and unreliable orientations. We
present a family of score-guided mixed-strategy causal search algorithms that
build on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,
straightforward variants of GFCI that substitute BOSS or GRaSP for FGES,
thereby retaining correctness while incurring different scalability tradeoffs.
Second, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method
that improves upon these variants by replacing exhaustive all-subsets testing
with targeted tests guided by BOSS, yielding well-formed PAGs with higher
precision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also
known as BOSS-POD), which bypasses latent-variable-specific reasoning and
directly returns the PAG of the BOSS DAG. Although not strictly correct in the
FCI sense, it scales better and often achieves superior accuracy in practice.
Simulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI
provide sound baselines, FCIT improves both efficiency and reliability, and
LV-Dumb offers a practical heuristic with strong empirical performance.
Together, these method highlight the value of score-guided and targeted
strategies for scalable latent-variable causal discovery.

</details>


### [542] [Influence branching for learning to solve mixed-integer programs online](https://arxiv.org/abs/2510.04273)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 本文提出了一种用于在线求解混合整数规划（MIP）的新方法——影响分支，结合汤普森采样优化变量选择策略，在分支定界算法初期迭代中使用图表示学习，取得了与当前在线学习方法相当的结果，并显示出在更广泛在线框架中的良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了提升混合整数规划问题的求解效率，尤其是在变化环境中在线学习策略的适应性和泛化能力，本文旨在设计一种能够动态优化变量选择策略的新型方法。

Method: 提出了一种名为“影响分支”的图导向变量选择策略，并在分支定界法的早期迭代中应用；通过汤普森采样在线优化该启发式策略，根据对SCIP求解速度的提升效果来评估不同MIP结构图表示的优劣。

Result: 所提方法在求解效率上达到了与现有最先进的在线学习方法相当的水平，并且在约束矩阵、约束向量和目标函数系数均发生变化的更通用在线场景中表现出良好的泛化能力。

Conclusion: 影响分支结合汤普森采样的在线优化策略是解决MIP问题的一种有效且具有泛化潜力的方法，适用于复杂多变的在线优化环境。

Abstract: On the occasion of the 20th Mixed Integer Program Workshop's computational
competition, this work introduces a new approach for learning to solve MIPs
online. Influence branching, a new graph-oriented variable selection strategy,
is applied throughout the first iterations of the branch and bound algorithm.
This branching heuristic is optimized online with Thompson sampling, which
ranks the best graph representations of MIP's structure according to
computational speed up over SCIP. We achieve results comparable to state of the
art online learning methods. Moreover, our results indicate that our method
generalizes well to more general online frameworks, where variations in
constraint matrix, constraint vector and objective coefficients can all occur
and where more samples are available.

</details>


### [543] [HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks](https://arxiv.org/abs/2510.04295)
*Nghiem T. Diep,Dung Le,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: 提出了一种新的超共享低秩适应方法（HoRA），通过联合超网络生成注意力头间的低秩矩阵，促进跨头信息共享，在多个语言和视觉基准上优于LoRA和其他PEFT方法，且仅需略微增加可训练参数。


<details>
  <summary>Details</summary>
Motivation: LoRA在多头自注意力中独立适应每个头，忽略了不同头之间的潜在协同作用，因此需要一种能促进跨头信息共享的改进方法。

Method: 提出Hyper-shared Low-Rank Adaptation (HoRA)，利用共享的超网络为多个注意力头生成低秩适配矩阵，实现跨头耦合更新。

Result: 理论分析表明HoRA具有更优的样本效率；实验结果显示其在多种语言和视觉任务上性能优于LoRA及其他PEFT方法，同时参数增量极小。

Conclusion: HoRA通过共享超网络实现了跨注意力头的信息融合，有效提升了参数效率和模型性能，是LoRA的有效改进。

Abstract: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT)
technique that adapts large pre-trained models by adding low-rank matrices to
their weight updates. However, in the context of fine-tuning multi-head
self-attention (MHA), LoRA has been employed to adapt each attention head
separately, thereby overlooking potential synergies across different heads. To
mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA)
method, which utilizes joint hypernetworks to generate low-rank matrices across
attention heads. By coupling their adaptation through a shared generator, HoRA
encourages cross-head information sharing, and thus directly addresses the
aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens
of hierarchical mixture of experts, our theoretical findings reveal that the
latter achieves superior sample efficiency to the former. Furthermore, through
extensive experiments across diverse language and vision benchmarks, we
demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring
only a marginal increase in the number of trainable parameters.

</details>


### [544] [DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks](https://arxiv.org/abs/2510.04331)
*Nghiem T. Diep,Hien Dang,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: 本文提出了DoRAN，一种改进的权重分解低秩自适应方法，通过在权重分解中引入噪声和使用辅助网络动态生成低秩矩阵，提升了训练稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 为了进一步提升DoRA方法的训练稳定性与样本效率，解决现有PEFT方法在适应大规模模型时可能存在的不稳定性问题。

Method: 提出DoRAN，包含两个关键步骤：一是在DoRA的权重分解分母中注入噪声，作为自适应正则化手段；二是用辅助网络动态生成低秩矩阵，实现跨层参数耦合。

Result: 在多个视觉与语言基准上的实验表明，DoRAN consistently 优于LoRA、DoRA及其他PEFT基线方法。

Conclusion: 结合基于噪声的正则化与网络化的参数生成，是提升大模型高效微调鲁棒性与效率的有效方向。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard
paradigm for adapting large-scale models. Among these techniques,
Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the
learning capacity and training stability of the vanilla Low-Rank Adaptation
(LoRA) method by explicitly decomposing pre-trained weights into magnitude and
directional components. In this work, we propose DoRAN, a new variant of DoRA
designed to further stabilize training and boost the sample efficiency of DoRA.
Our approach includes two key stages: (i) injecting noise into the denominator
of DoRA's weight decomposition, which serves as an adaptive regularizer to
mitigate instabilities; and (ii) replacing static low-rank matrices with
auxiliary networks that generate them dynamically, enabling parameter coupling
across layers and yielding better sample efficiency in both theory and
practice. Comprehensive experiments on vision and language benchmarks show that
DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These
results underscore the effectiveness of combining stabilization through
noise-based regularization with network-based parameter generation, offering a
promising direction for robust and efficient fine-tuning of foundation models.

</details>


### [545] [Activation Steering with a Feedback Controller](https://arxiv.org/abs/2510.04309)
*Dung V. Nguyen,Hieu M. Vu,Nhi Y. Pham,Lei Zhang,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了PID Steering，一种基于控制理论的激活引导框架，通过比例-积分-微分（PID）控制器实现对大语言模型行为的更鲁棒和可靠的控制。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型引导方法缺乏理论性能保证，主要依赖经验设计，难以确保稳定和可靠的行为控制。

Method: 将现有引导方法建模为比例（P）控制器，提出扩展为完整的PID控制器，利用P项对齐语义方向，I项累积误差以跨层持续修正，D项抑制激活值快速变化带来的超调。

Result: 在多个大模型系列和基准上的实验表明，PID Steering始终优于现有方法，展现出更强的鲁棒性和稳定性，且具有轻量、模块化、易集成的优点。

Conclusion: PID Steering为激活引导提供了有理论基础的系统性框架，连接了控制理论中的稳定性分析，推动了大语言模型行为控制的可解释性与可靠性。

Abstract: Controlling the behaviors of large language models (LLM) is fundamental to
their safety alignment and reliable deployment. However, existing steering
methods are primarily driven by empirical insights and lack theoretical
performance guarantees. In this work, we develop a control-theoretic foundation
for activation steering by showing that popular steering methods correspond to
the proportional (P) controllers, with the steering vector serving as the
feedback signal. Building on this finding, we propose
Proportional-Integral-Derivative (PID) Steering, a principled framework that
leverages the full PID controller for activation steering in LLMs. The
proportional (P) term aligns activations with target semantic directions, the
integral (I) term accumulates errors to enforce persistent corrections across
layers, and the derivative (D) term mitigates overshoot by counteracting rapid
activation changes. This closed-loop design yields interpretable error dynamics
and connects activation steering to classical stability guarantees in control
theory. Moreover, PID Steering is lightweight, modular, and readily integrates
with state-of-the-art steering methods. Extensive experiments across multiple
LLM families and benchmarks demonstrate that PID Steering consistently
outperforms existing approaches, achieving more robust and reliable behavioral
control.

</details>


### [546] [Crash Severity Prediction Using Deep Learning Approaches: A Hybrid CNN-RNN Framework](https://arxiv.org/abs/2510.04316)
*Sahar Koohfar*

Main category: cs.LG

TL;DR: 提出了一种基于CNN-RNN的混合深度学习模型，用于交通事故严重程度预测，并在弗吉尼亚州I-64公路的真实数据集上验证了其优于传统统计和机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 准确及时地预测交通事故严重程度对于减轻事故后果、优化医疗救援和交通服务至关重要，现有方法在捕捉变量间非线性关系方面存在局限。

Method: 采用CNN-RNN混合深度学习模型，结合卷积神经网络（CNN）和循环神经网络（RNN）的优势，利用2015至2021年弗吉尼亚州I-64高速公路的15,870条事故记录进行实验，并与逻辑回归、朴素贝叶斯、KNN、决策树及单一CNN、RNN模型进行对比。

Result: CNN-RNN混合模型在预测准确率上显著优于所有基准模型，证明其能更有效捕捉交通事故特征间的复杂非线性关系。

Conclusion: CNN-RNN混合模型在交通事故严重程度预测中表现出优越性能，具备实际应用潜力，可为智能交通系统提供更精准的决策支持。

Abstract: Accurate and timely prediction of crash severity is crucial in mitigating the
severe consequences of traffic accidents. Accurate and timely prediction of
crash severity is crucial in mitigating the severe consequences of traffic
accidents. In order to provide appropriate levels of medical assistance and
transportation services, an intelligent transportation system relies on
effective prediction methods. Deep learning models have gained popularity in
this domain due to their capability to capture non-linear relationships among
variables. In this research, we have implemented a hybrid CNN-RNN deep learning
model for crash severity prediction and compared its performance against widely
used statistical and machine learning models such as logistic regression,
na\"ive bayes classifier, K-Nearest Neighbors (KNN), decision tree, and
individual deep learning models: RNN and CNN. This study employs a methodology
that considers the interconnected relationships between various features of
traffic accidents. The study was conducted using a dataset of 15,870 accident
records gathered over a period of seven years between 2015 and 2021 on Virginia
highway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model
has outperformed all benchmark models in terms of predicting crash severity.
This result illustrates the effectiveness of the hybrid model as it combines
the advantages of both RNN and CNN models in order to achieve greater accuracy
in the prediction process.

</details>


### [547] [Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows](https://arxiv.org/abs/2510.04510)
*Achim Eckerle,Martin Spitznagel,Janis Keuper*

Main category: cs.LG

TL;DR: 提出了一种基于条件归一化流（Full-Glow）的实时城市噪声预测模型，可在消费级硬件上快速生成符合标准的噪声图，较传统物理求解器提速超过2000倍，并在非视距（NLoS）预测精度上显著优于先前深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于物理的噪声预测方法计算成本高、速度慢，难以满足城市规划中频繁迭代的‘假设’分析需求，尤其是在需要实时响应的审批流程和城市管理场景中。

Method: 采用条件归一化流（Full-Glow）模型，从二维城市布局生成符合标准的城市声压图，利用深度生成模型实现高分辨率（256x256）噪声图的实时推断。

Result: 在基线、衍射和反射数据集上，模型比参考求解器快2000倍以上，在基线NLoS条件下达到0.65 dB MAE，结构保真度高，且能准确再现衍射和干涉模式。

Conclusion: 该模型支持源或几何变化下的即时重计算，可作为城市规划、合规性制图和城市管理操作的实用工具，具备高实用性与部署潜力。

Abstract: Accurate and fast urban noise prediction is pivotal for public health and for
regulatory workflows in cities, where the Environmental Noise Directive
mandates regular strategic noise maps and action plans, often needed in
permission workflows, right-of-way allocation, and construction scheduling.
Physics-based solvers are too slow for such time-critical, iterative "what-if"
studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating
for generating standards-compliant urban sound-pressure maps from 2D urban
layouts in real time per 256x256 map on a single RTX 4090), enabling
interactive exploration directly on commodity hardware. On datasets covering
Baseline, Diffraction, and Reflection regimes, our model accelerates map
generation by >2000 times over a reference solver while improving NLoS accuracy
by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE
with high structural fidelity. The model reproduces diffraction and
interference patterns and supports instant recomputation under source or
geometry changes, making it a practical engine for urban planning, compliance
mapping, and operations (e.g., temporary road closures, night-work variance
assessments).

</details>


### [548] [FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents](https://arxiv.org/abs/2510.04317)
*Yucong Dai,Lu Zhang,Feng Luo,Mashrur Chowdhury,Yongkai Wu*

Main category: cs.LG

TL;DR: 提出FairAgent，一个基于大语言模型的自动化系统，简化公平性感知的机器学习模型开发，降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 公平且无偏见的机器学习模型在高风险应用中至关重要，但现有方法需要深厚的专业知识，难以普及。

Method: 利用大语言模型自动分析数据集中的偏见，进行数据预处理和特征工程，并根据用户需求实施适当的去偏策略。

Result: 实验表明，FairAgent在显著减少开发时间和专业知识要求的同时，大幅提升了模型性能。

Conclusion: FairAgent使公平性感知的机器学习更易于实践者使用，推动了公平AI的普及。

Abstract: Training fair and unbiased machine learning models is crucial for high-stakes
applications, yet it presents significant challenges. Effective bias mitigation
requires deep expertise in fairness definitions, metrics, data preprocessing,
and machine learning techniques. In addition, the complex process of balancing
model performance with fairness requirements while properly handling sensitive
attributes makes fairness-aware model development inaccessible to many
practitioners. To address these challenges, we introduce FairAgent, an
LLM-powered automated system that significantly simplifies fairness-aware model
development. FairAgent eliminates the need for deep technical expertise by
automatically analyzing datasets for potential biases, handling data
preprocessing and feature engineering, and implementing appropriate bias
mitigation strategies based on user requirements. Our experiments demonstrate
that FairAgent achieves significant performance improvements while
significantly reducing development time and expertise requirements, making
fairness-aware machine learning more accessible to practitioners.

</details>


### [549] [FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields](https://arxiv.org/abs/2510.04325)
*Kenechukwu Ogbuagu,Sepehr Maleki,Giuseppe Bruni,Senthil Krishnababu*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的新型代理模型FoilDiff，用于快速准确预测翼型周围的流场。该模型采用卷积与Transformer结合的混合骨干网络，并利用DDIM采样提升效率，在预测精度和不确定性校准方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的CFD模拟虽然有效但计算成本高，需要高效的替代模型来加速气动设计与优化过程。

Method: 提出FoilDiff模型，采用混合骨干去噪网络（结合CNN和Transformer），输入包括雷诺数、攻角和翼型几何编码，使用DDIM采样提高生成效率。

Result: 在相同数据集上，相比现有最先进模型，FoilDiff的平均预测误差降低高达85%，且具有更优的预测不确定度校准能力。

Conclusion: FoilDiff在流场预测中实现了更高的准确性与计算效率，为气动设计提供了一个高效可靠的深度学习代理模型。

Abstract: The accurate prediction of flow fields around airfoils is crucial for
aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models
are effective but computationally expensive, thus inspiring the development of
surrogate models to enable quicker predictions. These surrogate models can be
based on deep learning architectures, such as Convolutional Neural Networks
(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion
models have shown significant promise in predicting complex flow fields. In
this work, we propose FoilDiff, a diffusion-based surrogate model with a
hybrid-backbone denoising network. This hybrid design combines the power of
convolutional feature extraction and transformer-based global attention to
generate more adaptable and accurate representations of flow structures.
FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling
to optimise the efficiency of the sampling process at no additional cost to
model generalisation. We used encoded representations of Reynolds number, angle
of attack, and airfoil geometry to define the input space for generalisation
across a wide range of aerodynamic conditions. When evaluated against
state-of-the-art models, FoilDiff shows significant performance improvements,
with mean prediction errors reducing by up to 85\% on the same datasets. The
results have demonstrated that FoilDiff can provide both more accurate
predictions and better-calibrated predictive uncertainty than existing
diffusion-based models.

</details>


### [550] [Arithmetic-Mean $μ$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets](https://arxiv.org/abs/2510.04327)
*Haosong Zhang,Shenxi Wu,Yichi Zhang,Wei Lin*

Main category: cs.LG

TL;DR: 提出了一种新的学习率参数化方法AM-μP，结合残差感知的初始化策略，实现了跨深度和宽度的鲁棒学习率缩放规律，适用于卷积和残差网络。


<details>
  <summary>Details</summary>
Motivation: 传统μP在异构架构（如残差连接和卷积）中因层间更新不平衡而失效，需设计更普适的学习率参数化方案。

Method: 引入算术均值μP（AM-μP），约束网络范围内预激活二阶矩的平均更新幅度，并采用残差感知的He初始化（权重方差按块数缩放）。

Result: 理论证明卷积网络最大学习率随深度呈L^{-3/2}缩放；实验验证该规律并实现零样本学习率迁移。

Conclusion: AM-μP为深层卷积和残差网络提供了统一且实用的学习率设定原则，无需额外调参。

Abstract: Choosing an appropriate learning rate remains a key challenge in scaling
depth of modern deep networks. The classical maximal update parameterization
($\mu$P) enforces a fixed per-layer update magnitude, which is well suited to
homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in
heterogeneous architectures where residual accumulation and convolutions
introduce imbalance across layers. We introduce Arithmetic-Mean $\mu$P
(AM-$\mu$P), which constrains not each individual layer but the network-wide
average one-step pre-activation second moment to a constant scale. Combined
with a residual-aware He fan-in initialization - scaling residual-branch
weights by the number of blocks ($\mathrm{Var}[W]=c/(K\cdot
\mathrm{fan\text{-}in})$) - AM-$\mu$P yields width-robust depth laws that
transfer consistently across depths. We prove that, for one- and
two-dimensional convolutional networks, the maximal-update learning rate
satisfies $\eta^\star(L)\propto L^{-3/2}$; with zero padding, boundary effects
are constant-level as $N\gg k$. For standard residual networks with general
conv+MLP blocks, we establish $\eta^\star(L)=\Theta(L^{-3/2})$, with $L$ the
minimal depth. Empirical results across a range of depths confirm the $-3/2$
scaling law and enable zero-shot learning-rate transfer, providing a unified
and practical LR principle for convolutional and deep residual networks without
additional tuning overhead.

</details>


### [551] [Post-training quantization of vision encoders needs prefixing registers](https://arxiv.org/abs/2510.04547)
*Seunghyeon Kim,Jinho Kim,Taesun Yeom,Wonpyo Park,Kyuyeun Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: 提出了一种名为RegCache的训练-free算法，通过引入易产生异常值但语义无关的前缀token来缓解视觉编码器中的异常值问题，从而实现更高效的量化。


<details>
  <summary>Details</summary>
Motivation: Transformer-based视觉编码器在多模态智能中至关重要，但其推理成本高，尤其是在大规模激活（即异常值）存在的情况下，8位量化的性能下降严重，因此需要有效降低量化误差。

Method: 提出RegCache算法，引入异常值倾向性强但语义无意义的前缀token，并结合中间层前缀和token删除技术，防止其他token出现异常值，从而提升量化后模型的准确性。

Result: 实验表明，RegCache在文本监督和自监督视觉编码器上均显著提升了量化模型的准确性，且适用于多种量化设置。

Conclusion: RegCache为视觉编码器提供了一种有效的训练-free异常值缓解方案，显著推动了其在低精度部署场景下的应用潜力。

Abstract: Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

</details>


### [552] [Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies](https://arxiv.org/abs/2510.04341)
*G. Niklas Noren,Eva-Lisa Meldau,Johan Ellenius*

Main category: cs.LG

TL;DR: 本文探讨了在低发病率事件识别中人工智能模型的关键评估考虑因素，提出了结构化案例级审查方法和综合检查清单，并通过药物安全监测领域的三个实例验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于罕见事件中正样本稀少且错误成本不对称，传统评估方法可能高估模型性能，因此需要专门的评估框架来确保AI模型在实际应用中的有效性。

Method: 提出了一种包含问题定义、测试集设计、流行率感知的统计评估、鲁棒性分析及人机协作整合的评估框架，并引入结构化案例级审查（SCLE）作为统计评估的补充。

Result: 在药物安全监测的三个案例中验证了该框架的有效性，揭示了罕见事件设置中的常见陷阱，如测试集中缺乏困难阳性对照和不现实的类别平衡导致的乐观偏差，并展示了成本敏感目标如何使模型性能与实际价值对齐。

Conclusion: 所提出的评估框架和检查清单有助于提高罕见事件识别中AI模型的可靠性和实用性，其原则可推广到其他正样本稀缺的应用领域。

Abstract: Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

</details>


### [553] [SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator](https://arxiv.org/abs/2510.04576)
*Yuhta Takida,Satoshi Hayakawa,Takashi Shibuya,Masaaki Imaizumi,Naoki Murata,Bac Nguyen,Toshimitsu Uesaka,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出了一种新的判别器设计SONA，通过分离自然性和对齐性的投影以及自适应加权机制，在条件生成任务中实现了更优的样本质量和条件对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有的条件生成对抗网络在平衡真实性评估和条件对齐方面存在困难，需要一种能够同时有效处理这两个目标的判别器设计。

Method: 引入了Sum of Naturalness and Alignment (SONA)，采用独立的投影和归纳偏置来分别处理自然性和对齐性，并结合专用的目标函数和自适应加权机制。

Result: 在类别条件生成任务上的大量实验表明，该方法相比现有最先进方法在样本质量和条件对齐上表现更优，并在文本到图像生成任务中验证了其有效性和鲁棒性。

Conclusion: SONA通过增强对齐敏感性和动态平衡多目标，显著提升了条件生成模型的性能，具有良好的通用性和应用潜力。

Abstract: Deep generative models have made significant advances in generating complex
content, yet conditional generation remains a fundamental challenge. Existing
conditional generative adversarial networks often struggle to balance the dual
objectives of assessing authenticity and conditional alignment of input samples
within their conditional discriminators. To address this, we propose a novel
discriminator design that integrates three key capabilities: unconditional
discrimination, matching-aware supervision to enhance alignment sensitivity,
and adaptive weighting to dynamically balance all objectives. Specifically, we
introduce Sum of Naturalness and Alignment (SONA), which employs separate
projections for naturalness (authenticity) and alignment in the final layer
with an inductive bias, supported by dedicated objective functions and an
adaptive weighting mechanism. Extensive experiments on class-conditional
generation tasks show that \ours achieves superior sample quality and
conditional alignment compared to state-of-the-art methods. Furthermore, we
demonstrate its effectiveness in text-to-image generation, confirming the
versatility and robustness of our approach.

</details>


### [554] [Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics](https://arxiv.org/abs/2510.04342)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: 提出了一种名为Curriculum Chaos Forecasting (CCF)的训练范式，通过基于动力系统理论原则组织训练数据，从简单周期行为逐步过渡到复杂混沌动力学，显著提升了对未知真实世界基准数据的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在预测混沌系统时面临两个对立问题：要么过度专注于单一已知混沌系统导致泛化能力差，要么混合大量不相关时间序列而无法捕捉特定动力学特征。

Method: 基于最大李雅普诺夫指数和吸引子维度量化系统复杂度，构建一个由50多个合成ODE/PDE系统组成的训练课程，按复杂度递增顺序进行预训练。

Result: 在太阳黑子数、电力需求和人类ECG信号等真实数据集上，相比随机训练提升预测 horizon 最多40%，相比仅用真实数据训练则翻倍以上，且效果在GRU、Transformer等不同神经架构中一致。

Conclusion: CCF通过结构化的课程学习有效提升了模型对混沌系统的泛化预测能力，为复杂动力系统建模提供了可扩展且鲁棒的训练框架。

Abstract: Forecasting chaotic systems is a cornerstone challenge in many scientific
fields, complicated by the exponential amplification of even infinitesimal
prediction errors. Modern machine learning approaches often falter due to two
opposing pitfalls: over-specializing on a single, well-known chaotic system
(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing
vast, unrelated time-series, which prevents the model from learning the nuances
of any specific dynamical regime. We propose Curriculum Chaos Forecasting
(CCF), a training paradigm that bridges this gap. CCF organizes training data
based on fundamental principles of dynamical systems theory, creating a
curriculum that progresses from simple, periodic behaviors to highly complex,
chaotic dynamics. We quantify complexity using the largest Lyapunov exponent
and attractor dimension, two well-established metrics of chaos. By first
training a sequence model on predictable systems and gradually introducing more
chaotic trajectories, CCF enables the model to build a robust and generalizable
representation of dynamical behaviors. We curate a library of over 50 synthetic
ODE/PDE systems to build this curriculum. Our experiments show that
pre-training with CCF significantly enhances performance on unseen, real-world
benchmarks. On datasets including Sunspot numbers, electricity demand, and
human ECG signals, CCF extends the valid prediction horizon by up to 40%
compared to random-order training and more than doubles it compared to training
on real-world data alone. We demonstrate that this benefit is consistent across
various neural architectures (GRU, Transformer) and provide extensive ablations
to validate the importance of the curriculum's structure.

</details>


### [555] [From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere](https://arxiv.org/abs/2510.04357)
*Anoushka Harit,Zhongtian Sun,Jongmin Yu*

Main category: cs.LG

TL;DR: 提出了一种名为Causal Sphere Hypergraph Transformer (CSHT) 的新型可解释金融时间序列预测模型，结合Granger因果超图结构、黎曼几何和因果掩码Transformer注意力机制，在S&P 500数据上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 为了提升金融时间序列预测的可解释性和在不同市场状态下的鲁棒性，需要融合因果结构与几何表示的方法。

Method: 通过提取多变量Granger因果依赖关系构建有向超边，并将其编码在超球面上；使用角度掩码约束注意力机制，保持时间方向性和几何一致性。

Result: 在2018至2023年S&P 500数据（包括2020年疫情冲击）上的实验显示，CSHT在收益预测、市场状态分类和顶级资产排序任务中均优于基线模型。

Conclusion: CSHT通过引入预测性因果结构和黎曼流形嵌入，实现了跨市场状态的稳健泛化和从宏观经济事件到个股反应的透明归因路径，是一种在不确定性下可信的金融预测解决方案。

Abstract: We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel
architecture for interpretable financial time-series forecasting that unifies
\emph{Granger-causal hypergraph structure}, \emph{Riemannian geometry}, and
\emph{causally masked Transformer attention}. CSHT models the directional
influence of financial news and sentiment on asset returns by extracting
multivariate Granger-causal dependencies, which are encoded as directional
hyperedges on the surface of a hypersphere. Attention is constrained via
angular masks that preserve both temporal directionality and geometric
consistency. Evaluated on S\&P 500 data from 2018 to 2023, including the 2020
COVID-19 shock, CSHT consistently outperforms baselines across return
prediction, regime classification, and top-asset ranking tasks. By enforcing
predictive causal structure and embedding variables in a Riemannian manifold,
CSHT delivers both \emph{robust generalisation across market regimes} and
\emph{transparent attribution pathways} from macroeconomic events to
stock-level responses. These results suggest that CSHT is a principled and
practical solution for trustworthy financial forecasting under uncertainty.

</details>


### [556] [Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework](https://arxiv.org/abs/2510.04366)
*Christopher Klugmann,Daniel Kondermann*

Main category: cs.LG

TL;DR: 提出一种新的模糊性度量方法，用于量化分类任务中的随机不确定性，并结合统计推断工具进行群体模糊性估计和贝叶斯后验分析，适用于数据质量评估和机器学习流程。


<details>
  <summary>Details</summary>
Motivation: 人类生成的类别标注常反映的是标注本身的模糊性而非错误，现有方法难以区分类别间不可分辨性与明确无法解决的不确定性，因此需要一种能处理‘无法解决’类别的非对称模糊性度量。

Method: 提出一种基于离散响应分布的模糊性度量，与二次熵（如基尼杂质）相关但不同，特别对待‘无法解决’类别；并发展了频率学派的点估计和基于狄利克雷先验的贝叶斯后验推断方法。

Result: 该度量能有效分离两类不确定性，具备良好的形式化性质；推导出的统计工具可进行模糊性估计与校准，并在数值示例中展示了其在数据质量评估和机器学习中的实用性。

Conclusion: 所提出的模糊性度量及配套统计推断方法为处理分类任务中的不确定性提供了更精细、原则性的框架，有助于提升数据集质量和下游模型可靠性。

Abstract: Human-generated categorical annotations frequently produce empirical response
distributions (soft labels) that reflect ambiguity rather than simple annotator
error. We introduce an ambiguity measure that maps a discrete response
distribution to a scalar in the unit interval, designed to quantify aleatoric
uncertainty in categorical tasks. The measure bears a close relationship to
quadratic entropy (Gini-style impurity) but departs from those indices by
treating an explicit "can't solve" category asymmetrically, thereby separating
uncertainty arising from class-level indistinguishability from uncertainty due
to explicit unresolvability. We analyze the measure's formal properties and
contrast its behavior with a representative ambiguity measure from the
literature. Moving beyond description, we develop statistical tools for
inference: we propose frequentist point estimators for population ambiguity and
derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the
underlying probability vector, providing a principled account of epistemic
uncertainty. Numerical examples illustrate estimation, calibration, and
practical use for dataset-quality assessment and downstream machine-learning
workflows.

</details>


### [557] [GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks](https://arxiv.org/abs/2510.04374)
*Tejal Patwardhan,Rachel Dias,Elizabeth Proehl,Grace Kim,Michele Wang,Olivia Watkins,Simón Posada Fishman,Marwan Aljubeh,Phoebe Thacker,Laurance Fauconnet,Natalie S. Kim,Patrick Chao,Samuel Miserendino,Gildas Chabot,David Li,Michael Sharman,Alexandra Barr,Amelia Glaese,Jerry Tworek*

Main category: cs.LG

TL;DR: GDPval是一个评估AI模型在真实世界经济任务中表现的基准，覆盖了美国9大主要经济部门的44种职业的主要工作活动。当前最先进的模型在该基准上的表现正线性提升，并接近行业专家水平。研究还表明，结合人类监督，这些模型可能以更低的成本和更快的速度完成任务。增加推理努力、任务上下文和结构化支持有助于提升模型表现。部分任务数据已开源，并提供公开自动评分服务。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地衡量AI模型在现实世界经济价值任务中的实际能力，填补现有基准与真实职业需求之间的差距。

Method: 基于美国劳工统计局的工作活动数据，构建覆盖44种职业的基准GDPval，任务内容来源于具有平均14年经验的行业专业人士的实际工作。通过评估前沿模型在任务中的表现，分析其与人类专家的差距及改进因素。

Result: 前沿模型在GDPval上的性能随时间呈线性提升，当前最佳模型在交付质量上接近行业专家；增加推理、上下文和 scaffolding 可提升性能；结合人类监督后，模型有望以更低代价和更快速度完成任务。

Conclusion: GDPval为衡量AI在真实经济场景中的能力提供了有效基准，当前AI模型已接近人类专家水平，未来在人机协同下具备显著提升生产力的潜力。

Abstract: We introduce GDPval, a benchmark evaluating AI model capabilities on
real-world economically valuable tasks. GDPval covers the majority of U.S.
Bureau of Labor Statistics Work Activities for 44 occupations across the top 9
sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are
constructed from the representative work of industry professionals with an
average of 14 years of experience. We find that frontier model performance on
GDPval is improving roughly linearly over time, and that the current best
frontier models are approaching industry experts in deliverable quality. We
analyze the potential for frontier models, when paired with human oversight, to
perform GDPval tasks cheaper and faster than unaided experts. We also
demonstrate that increased reasoning effort, increased task context, and
increased scaffolding improves model performance on GDPval. Finally, we
open-source a gold subset of 220 tasks and provide a public automated grading
service at evals.openai.com to facilitate future research in understanding
real-world model capabilities.

</details>


### [558] [Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains](https://arxiv.org/abs/2510.04375)
*Akshay Mittal,Vinay Venkatesh,Krishna Kandi,Shalini Sudarshan*

Main category: cs.LG

TL;DR: 提出一种基于数据驱动的动态加权损失函数，用于提升稀疏或小众领域中“重度用户”的序列推荐效果，通过为不同稀疏程度的领域自适应调整损失权重，在理论和实验上均证明了其有效性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 单一固定权重的损失函数在处理交互稀少的小众领域时效果不佳，容易导致训练信号被大量通用数据稀释，难以捕捉稀疏用户的兴趣。

Method: 设计了一种动态加权损失函数，根据每个领域的训练数据稀疏程度自适应调整损失权重，稀疏领域赋予更高权重，密集领域赋予较低权重，并提供了收敛性、复杂度和边界等理论分析。

Result: 在四个真实数据集（MovieLens、Amazon Electronics、Yelp Business、LastFM Music）上，相比SIGMA、CALRec、SparseEnNet等先进方法，该方法在稀疏领域显著提升了Recall@10和NDCG@10等指标，同时在密集领域保持性能，计算开销极低。

Conclusion: 动态加权损失函数能有效增强推荐模型对稀疏用户兴趣的学习能力，平衡不同密度领域的训练信号，提升整体推荐性能，尤其适用于长尾场景下的序列推荐任务。

Abstract: The effectiveness of single-model sequential recommendation architectures,
while scalable, is often limited when catering to "power users" in sparse or
niche domains. Our previous research, PinnerFormerLite, addressed this by using
a fixed weighted loss to prioritize specific domains. However, this approach
can be sub-optimal, as a single, uniform weight may not be sufficient for
domains with very few interactions, where the training signal is easily diluted
by the vast, generic dataset.
  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss
function with comprehensive theoretical foundations and extensive empirical
validation. We introduce an adaptive algorithm that adjusts the loss weight for
each domain based on its sparsity in the training data, assigning a higher
weight to sparser domains and a lower weight to denser ones. This ensures that
even rare user interests contribute a meaningful gradient signal, preventing
them from being overshadowed.
  We provide rigorous theoretical analysis including convergence proofs,
complexity analysis, and bounds analysis to establish the stability and
efficiency of our approach. Our comprehensive empirical validation across four
diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)
with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that
this dynamic weighting system significantly outperforms all comparison methods,
particularly for sparse domains, achieving substantial lifts in key metrics
like Recall at 10 and NDCG at 10 while maintaining performance on denser
domains and introducing minimal computational overhead.

</details>


### [559] [Categorical Invariants of Learning Dynamics](https://arxiv.org/abs/2510.04376)
*Abdulrahman Tamim*

Main category: cs.LG

TL;DR: 提出了一种基于范畴论的新视角，将神经网络训练视为参数空间到表示空间的结构保持变换，揭示了同伦类对泛化性能的影响，并提供了预测泛化的实用工具。


<details>
  <summary>Details</summary>
Motivation: 传统上将训练视为损失面上的梯度下降，但缺乏对不同训练路径导致相似或不同泛化性能的解释，因此需要新的理论框架来理解深度学习中的学习过程。

Method: 引入范畴论语言，定义学习函子L，利用同伦类刻画优化路径的等价性，并结合持久同调、拉回构造和2-范畴结构分析训练动态和迁移学习。

Result: 实验表明同伦路径收敛的模型泛化性能相差小于0.5%，非同伦路径差异超过3%；持久同调识别出与泛化强相关（R²=0.82）的稳定极小值。

Conclusion: 该范畴论框架为深度学习提供了理论洞察，揭示了学习过程的本质结构，并为设计更鲁棒的训练算法提供了可操作的原则。

Abstract: Neural network training is typically viewed as gradient descent on a loss
surface. We propose a fundamentally different perspective: learning is a
structure-preserving transformation (a functor L) between the space of network
parameters (Param) and the space of learned representations (Rep). This
categorical framework reveals that different training runs producing similar
test performance often belong to the same homotopy class (continuous
deformation family) of optimization paths. We show experimentally that networks
converging via homotopic trajectories generalize within 0.5% accuracy of each
other, while non-homotopic paths differ by over 3%. The theory provides
practical tools: persistent homology identifies stable minima predictive of
generalization (R^2 = 0.82 correlation), pullback constructions formalize
transfer learning, and 2-categorical structures explain when different
optimization algorithms yield functionally equivalent models. These categorical
invariants offer both theoretical insight into why deep learning works and
concrete algorithmic principles for training more robust networks.

</details>


### [560] [Score-based Greedy Search for Structure Identification of Partially Observed Linear Causal Models](https://arxiv.org/abs/2510.04378)
*Xinshuai Dong,Ignavier Ng,Haoyue Dai,Jiaqi Sun,Xiangchen Song,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于评分的贪婪搜索方法（LGES），用于在存在潜变量的情况下识别因果结构，并具有可识别性保证。


<details>
  <summary>Details</summary>
Motivation: 现有基于约束的因果发现方法在实际应用中面临多重检验和误差传播的问题，而基于评分的方法可能缓解这些问题，但缺乏适用于部分观测场景的贪婪搜索算法。

Method: 提出了广义N因子模型（GNFM），并建立了全局一致性理论；设计了潜变量贪婪等价搜索（LGES）算法，通过明确定义的操作符在图空间中高效搜索最优结构。

Result: 理论证明所提方法能将真实结构（包括潜变量）识别到马尔可夫等价类；在合成和真实数据上的实验验证了方法的有效性。

Conclusion: LGES是首个具有可识别性保证的、用于含潜变量系统的基于评分的贪婪搜索方法，为部分可观测因果系统的结构学习提供了新途径。

Abstract: Identifying the structure of a partially observed causal system is essential
to various scientific fields. Recent advances have focused on constraint-based
causal discovery to solve this problem, and yet in practice these methods often
face challenges related to multiple testing and error propagation. These issues
could be mitigated by a score-based method and thus it has raised great
attention whether there exists a score-based greedy search method that can
handle the partially observed scenario. In this work, we propose the first
score-based greedy search method for the identification of structure involving
latent variables with identifiability guarantees. Specifically, we propose
Generalized N Factor Model and establish the global consistency:
  the true structure including latent variables can be identified up to the
Markov equivalence class by using score. We then design
  Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm
for this class of model with well-defined operators,
  which search very efficiently over the graph space to find the optimal
structure. Our experiments on both synthetic and real-life data validate the
effectiveness of our method (code will be publicly available).

</details>


### [561] [SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management](https://arxiv.org/abs/2510.04386)
*Shakson Isaac,Yentl Collin,Chirag Patel*

Main category: cs.LG

TL;DR: SSM-CGM 是一种基于 Mamba 的神经状态空间模型，用于连续血糖监测预测，结合 CGM 和可穿戴设备活动信号，提升短期预测准确性，并通过变量选择、时间归因和反事实预测实现临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的血糖预测模型缺乏临床可用的可解释性，限制了其在糖尿病管理中的实际应用。

Method: 提出 SSM-CGM 模型，基于 Mamba 架构构建神经状态空间模型，融合 CGM 数据与可穿戴设备的生理信号（如心率、呼吸），利用变量选择和时间归因增强可解释性，并支持反事实预测。

Result: 在 AI-READI 队列数据上验证，SSM-CGM 在短期预测精度上优于 Temporal Fusion Transformer 基线模型，并提供对生理信号变化影响血糖的可解释分析。

Conclusion: SSM-CGM 是一个可解释、符合生理机制的个性化糖尿病管理框架，有助于临床决策支持。

Abstract: Continuous glucose monitoring (CGM) generates dense data streams critical for
diabetes management, but most used forecasting models lack interpretability for
clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting
model that integrates CGM and wearable activity signals from the AI-READI
cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer
baseline, adds interpretability through variable selection and temporal
attribution, and enables counterfactual forecasts simulating how planned
changes in physiological signals (e.g., heart rate, respiration) affect
near-term glucose. Together, these features make SSM-CGM an interpretable,
physiologically grounded framework for personalized diabetes management.

</details>


### [562] [Achieve Performatively Optimal Policy for Performative Reinforcement Learning](https://arxiv.org/abs/2510.04430)
*Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于零阶Frank-Wolfe算法的策略优化方法，首次实现了在标准正则化主导条件下对表现性最优（PO）策略的多项式时间收敛。


<details>
  <summary>Details</summary>
Motivation: 现有表现性强化学习工作仅追求表现性稳定（PS）策略，但其与真正最优的PO策略存在不可忽略的差距，因此需要更优的算法以逼近PO策略。

Method: 提出零阶Frank-Wolfe算法（0-FW），采用零阶近似估计表现性策略梯度，并在Frank-Wolfe框架下优化策略。

Result: 证明了当策略正则项主导环境变化时，价值函数满足梯度主导性质，且足够平稳的点位于一个凸紧致子空间内，梯度有界且Lipschitz连续；实验表明0-FW算法能更有效地找到PO策略。

Conclusion: 0-FW算法首次实现了对PO策略的多项式时间收敛，优于现有方法，为可执行强化学习提供了理论支持和有效算法路径。

Abstract: Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

</details>


### [563] [Trade-off in Estimating the Number of Byzantine Clients in Federated Learning](https://arxiv.org/abs/2510.04432)
*Ziyi Chen,Su Zhang,Heng Huang*

Main category: cs.LG

TL;DR: 本文研究了联邦学习中针对拜占庭客户端的鲁棒聚合器性能，分析了估计拜占庭客户端数量偏差对算法误差的影响，揭示了鲁棒性与性能之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对拜占庭客户端数量估计误差如何影响联邦学习性能的系统性理论分析，本文旨在填补这一空白。

Method: 通过理论分析不同估计值$\hat{f}$与真实值$f$下聚合器及联邦学习算法的最坏情况误差，推导出误差的上下界。

Result: 证明低估（$\hat{f}<f$）会导致性能严重下降；在非低估情况下，误差上下界同阶且正比于$\hat{f}/(n-f-\hat{f})$，随$\hat{f}$增大而单调增加。

Conclusion: 聚合器的鲁棒性程度需权衡：过高的估计虽能应对更多拜占庭客户端，但会降低无或少量拜占庭客户端时的学习性能。

Abstract: Federated learning has attracted increasing attention at recent large-scale
optimization and machine learning research and applications, but is also
vulnerable to Byzantine clients that can send any erroneous signals. Robust
aggregators are commonly used to resist Byzantine clients. This usually
requires to estimate the unknown number $f$ of Byzantine clients, and thus
accordingly select the aggregators with proper degree of robustness (i.e., the
maximum number $\hat{f}$ of Byzantine clients allowed by the aggregator). Such
an estimation should have important effect on the performance, which has not
been systematically studied to our knowledge. This work will fill in the gap by
theoretically analyzing the worst-case error of aggregators as well as its
induced federated learning algorithm for any cases of $\hat{f}$ and $f$.
Specifically, we will show that underestimation ($\hat{f}<f$) can lead to
arbitrarily poor performance for both aggregators and federated learning. For
non-underestimation ($\hat{f}\ge f$), we have proved optimal lower and upper
bounds of the same order on the errors of both aggregators and federated
learning. All these optimal bounds are proportional to $\hat{f}/(n-f-\hat{f})$
with $n$ clients, which monotonically increases with larger $\hat{f}$. This
indicates a fundamental trade-off: while an aggregator with a larger robustness
degree $\hat{f}$ can solve federated learning problems of wider range $f\in
[0,\hat{f}]$, the performance can deteriorate when there are actually fewer or
even no Byzantine clients (i.e., $f\in [0,\hat{f})$).

</details>


### [564] [Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size](https://arxiv.org/abs/2510.04440)
*Farid Bozorgnia,Vyacheslav Kungurtsev,Shirali Kadyrov,Mohsen Yousefnezhad*

Main category: cs.LG

TL;DR: 提出基于分数阶热核动力学的标签传播与自训练新算法，结合图神经网络提升小样本场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 通过扩展经典扩散模型至拉普拉斯算子的分数次幂，引入非局部相互作用以实现更全局的标签扩散，解决小样本标注下的图学习问题。

Method: 利用带源项的分数阶热核动力学设计标签传播与自训练算法，结合Chebyshev多项式逼近将其嵌入GCN和GAT等图神经网络，实现自适应多跳扩散。

Result: 在标准数据集上验证了方法的有效性，尤其在少量标注样本情况下表现优越。

Conclusion: 分数阶热核动力学能有效增强图神经网络的表达能力，通过非局部扩散机制提升半监督学习性能。

Abstract: In this work, we introduce novel algorithms for label propagation and
self-training using fractional heat kernel dynamics with a source term. We
motivate the methodology through the classical correspondence of information
theory with the physics of parabolic evolution equations. We integrate the
fractional heat kernel into Graph Neural Network architectures such as Graph
Convolutional Networks and Graph Attention, enhancing their expressiveness
through adaptive, multi-hop diffusion. By applying Chebyshev polynomial
approximations, large graphs become computationally feasible. Motivating
variational formulations demonstrate that by extending the classical diffusion
model to fractional powers of the Laplacian, nonlocal interactions deliver more
globally diffusing labels. The particular balance between supervision of known
labels and diffusion across the graph is particularly advantageous in the case
where only a small number of labeled training examples are present. We
demonstrate the effectiveness of this approach on standard datasets.

</details>


### [565] [Domain Generalization: A Tale of Two ERMs](https://arxiv.org/abs/2510.04441)
*Yilun Zhu,Naihao Deng,Naichen Shi,Aditya Gangrade,Clayton Scott*

Main category: cs.LG

TL;DR: 本文探讨了在不同数据分布假设下领域泛化（DG）中经验风险最小化（ERM）的表现，提出在后验漂移假设下，通过引入领域特定信息的特征增强方法（domain-informed ERM）优于传统的聚合ERM，并通过理论和实验验证了该观点。


<details>
  <summary>Details</summary>
Motivation: 在领域泛化任务中，现有研究普遍认为难以超越在聚合训练数据上使用经验风险最小化（ERM）的方法。然而，这种结论主要基于满足协变量偏移假设的数据集，本文旨在探究在不同假设（如后验漂移）下ERM的可改进空间。

Method: 提出“domain-informed ERM”方法，通过在特征向量中加入领域特定信息来增强模型对不同领域的识别能力，并在满足后验漂移假设的数据集上进行实验，同时构建理论框架支持其有效性。

Result: 理论分析和在语言与视觉任务上的实验表明，在满足后验漂移假设时，domain-informed ERM显著优于传统聚合ERM方法；而在协变量偏移假设下，提升不明显。

Conclusion: 领域泛化的效果依赖于数据分布偏移的类型，当存在后验概率漂移时，利用领域信息增强特征表示能有效提升泛化性能，为DG方法的设计提供了新的视角。

Abstract: Domain generalization (DG) is the problem of generalizing from several
distributions (or domains), for which labeled training data are available, to a
new test domain for which no labeled data is available. A common finding in the
DG literature is that it is difficult to outperform empirical risk minimization
(ERM) on the pooled training data.
  In this work, we argue that this finding has primarily been reported for
datasets satisfying a \emph{covariate shift} assumption. When the dataset
satisfies a \emph{posterior drift} assumption instead, we show that
``domain-informed ERM,'' wherein feature vectors are augmented with
domain-specific information, outperforms pooling ERM. These claims are
supported by a theoretical framework and experiments on language and vision
tasks.

</details>


### [566] [Forking-Sequences](https://arxiv.org/abs/2510.04487)
*Willa Potosnak,Malcolm Wolff,Boris Oreshkin,Mengfei Cao,Michael W. Mahoney,Dmitry Efimov,Kin G. Olivares*

Main category: cs.LG

TL;DR: 本文介绍了时间序列预测中的“分叉序列”（forking-sequences）方法，该方法通过联合编码和解码所有预测创建日期的整个时间序列来提高预测稳定性，相较于传统独立处理每个预测日期的方法更为稳定高效。


<details>
  <summary>Details</summary>
Motivation: 尽管准确性很重要，但预测在不同预测创建日期之间的一致性和稳定性常被忽视，而这种不稳定性会影响决策和用户信任。现有模型可能准确但预测变化剧烈，因此需要提升稳定性。

Method: 提出并形式化了forking-sequences方法，该方法在整个时间序列的所有预测创建日期上进行联合编码与解码，模拟时间序列交叉验证的过程，并在MLP、RNN、LSTM、CNN和Transformer等架构中验证其效果。

Result: 在M1、M3、M4和Tourism共16个数据集上实验表明，使用forking-sequences后预测百分比变化的稳定性分别提升了28.8%、28.8%、37.9%、31.3%，平均提升8.8%；同时带来了更稳定的梯度更新、更低的预测方差和更高的推理效率。

Conclusion: forking-sequences是一种有效提升时间序列预测稳定性的方法，应被更广泛采用以增强模型可靠性和实用性。

Abstract: While accuracy is a critical requirement for time series forecasting models,
an equally important (yet often overlooked) desideratum is forecast stability
across forecast creation dates (FCDs). Even highly accurate models can produce
erratic revisions between FCDs, undermining stakeholder trust and disrupting
downstream decision-making. To improve forecast stability, models like MQCNN,
MQT, and SPADE employ a little-known but highly effective technique:
forking-sequences. Unlike standard statistical and neural forecasting methods
that treat each FCD independently, the forking-sequences method jointly encodes
and decodes the entire time series across all FCDs, in a way mirroring time
series cross-validation. Since forking sequences remains largely unknown in the
broader neural forecasting community, in this work, we formalize the
forking-sequences approach, and we make a case for its broader adoption. We
demonstrate three key benefits of forking-sequences: (i) more stable and
consistent gradient updates during training; (ii) reduced forecast variance
through ensembling; and (iii) improved inference computational efficiency. We
validate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and
Tourism competitions, showing improvements in forecast percentage change
stability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP,
RNN, LSTM, CNN, and Transformer-based architectures, respectively.

</details>


### [567] [Expand Neurons, Not Parameters](https://arxiv.org/abs/2510.04500)
*Linghao Kong,Inimai Subramanian,Yonadav Shavit,Micah Adler,Dan Alistarh,Nir Shavit*

Main category: cs.LG

TL;DR: 本文提出了一种固定参数扩展（FPE）方法，通过在不增加非零参数数量的情况下增加神经元数量来减少特征间的干扰，从而提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 减少神经网络中多个特征共享同一神经元导致的干扰（即纠缠），以提升模型表现和可解释性。

Method: 引入固定参数扩展（FPE）：将一个神经元替换为多个子神经元，并将其权重非重叠地分配给这些子神经元，保持总非零参数数量不变。

Result: 在符号任务（如布尔逻辑问题）上，FPE有效降低了多义性指标并提高了准确率；随机拆分权重也能近似获得类似增益，说明减少连接冲突是关键因素；在CLIP分类器和深层网络中，保持非零参数不变而增宽网络仍能持续提升准确率。

Conclusion: 通过增宽网络结构减少特征纠缠是一种基于可解释性的有效方式，可在不增加非零参数的前提下提升性能，且更契合现代加速器的计算瓶颈特性。

Abstract: This work demonstrates how increasing the number of neurons in a network
without increasing its number of non-zero parameters improves performance. We
show that this gain corresponds with a decrease in interference between
multiple features that would otherwise share the same neurons. To reduce such
entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter
Expansion (FPE): replace a neuron with multiple children and partition the
parent's weights disjointly across them, so that each child inherits a
non-overlapping subset of connections. On symbolic tasks, specifically Boolean
code problems, clause-aligned FPE systematically reduces polysemanticity
metrics and yields higher task accuracy. Notably, random splits of neuron
weights approximate these gains, indicating that reduced collisions, not
precise assignment, are a primary driver. Consistent with the superposition
hypothesis, the benefits of FPE grow with increasing interference: when
polysemantic load is high, accuracy improvements are the largest. Transferring
these insights to real models (classifiers over CLIP embeddings and deeper
multilayer networks) we find that widening networks while maintaining a
constant non-zero parameter count consistently increases accuracy. These
results identify an interpretability-grounded mechanism to leverage width
against superposition, improving performance without increasing the number of
non-zero parameters. Such a direction is well matched to modern accelerators,
where memory movement of non-zero parameters, rather than raw compute, is the
dominant bottleneck.

</details>


### [568] [Wavelet Predictive Representations for Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.04507)
*Min Wang,Xin Li,Ye He,Yao-Hui Li,Hasnaa Bennis,Riashat Islam,Mingzhong Wang*

Main category: cs.LG

TL;DR: 本文提出WISDOM方法，利用小波域的多尺度特征表示来增强非平稳强化学习（NSRL），通过将任务表示序列转换到小波域并设计小波时序差分更新算子，显著提升了智能体在动态环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有NSRL方法主要针对规律性变化的任务，难以应对高度动态和随机变化的环境，因此需要一种更具适应性的方法来捕捉非平稳环境中的多尺度变化模式。

Method: 将任务表示序列转换到小波域，利用小波系数捕捉全局趋势和细粒度变化，并引入小波时序差分（TD）更新算子以提升对MDP演化的预测与跟踪能力，同时结合自回归建模。

Result: 理论证明了小波TD算子的收敛性，并验证了基于小波任务表示的策略改进；在多个基准实验中，WISDOM在样本效率和渐近性能上均显著优于现有基线方法。

Conclusion: WISDOM通过小波域的多尺度建模有效提升了非平稳强化学习的适应能力，适用于复杂、随机演变的动态环境。

Abstract: The real world is inherently non-stationary, with ever-changing factors, such
as weather conditions and traffic flows, making it challenging for agents to
adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning
(NSRL) addresses this challenge by training agents to adapt rapidly to
sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL
approaches often focus on tasks with regularly evolving patterns, leading to
limited adaptability in highly dynamic settings. Inspired by the success of
Wavelet analysis in time series modeling, specifically its ability to capture
signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain
predictive task representations to enhance NSRL. WISDOM captures these
multi-scale features in evolving MDP sequences by transforming task
representation sequences into the wavelet domain, where wavelet coefficients
represent both global trends and fine-grained variations of non-stationary
changes. In addition to the auto-regressive modeling commonly employed in time
series forecasting, we devise a wavelet temporal difference (TD) update
operator to enhance tracking and prediction of MDP evolution. We theoretically
prove the convergence of this operator and demonstrate policy improvement with
wavelet task representations. Experiments on diverse benchmarks show that
WISDOM significantly outperforms existing baselines in both sample efficiency
and asymptotic performance, demonstrating its remarkable adaptability in
complex environments characterized by non-stationary and stochastically
evolving tasks.

</details>


### [569] [Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction](https://arxiv.org/abs/2510.04522)
*Yisen Gao,Xingcheng Fu,Qingyun Sun,Jianxin Li,Xianxian Li*

Main category: cs.LG

TL;DR: 提出了一种新的黎曼图扩散框架GeoMancer，用于解决图数据在统一潜在空间中几何特性纠缠的问题，通过等距不变的黎曼gyrokernel和流形约束扩散方法，在生成和预测任务上实现了优越性能。


<details>
  <summary>Details</summary>
Motivation: 由于图数据的非欧几里得特性，现有方法将不同曲率的特征纠缠在同一个潜在空间中，未能充分发挥其几何潜力，因此需要一种能够捕捉复杂图数据流形特征的理想黎曼扩散模型。

Method: 提出GeoMancer框架：1）用等距不变的黎曼gyrokernel替代指数映射以缓解编码过程中的数值不稳定性；2）将多级特征解耦到各自的任务特定流形上；3）引入流形约束扩散方法和自引导策略，防止生成过程中的流形偏离。

Result: 在多种生成和预测任务上进行了广泛实验，结果表明GeoMancer相比现有方法具有更优的性能，有效解决了数值不稳定和流形偏离问题，提升了图扩散模型的表现。

Conclusion: GeoMancer通过构建任务特定的流形表示和稳定的扩散过程，成功挖掘了图数据的几何潜力，为图扩散模型提供了一个有效的黎曼几何框架。

Abstract: Graph diffusion models have made significant progress in learning structured
graph data and have demonstrated strong potential for predictive tasks.
Existing approaches typically embed node, edge, and graph-level features into a
unified latent space, modeling prediction tasks including classification and
regression as a form of conditional generation. However, due to the
non-Euclidean nature of graph data, features of different curvatures are
entangled in the same latent space without releasing their geometric potential.
To address this issue, we aim to construt an ideal Riemannian diffusion model
to capture distinct manifold signatures of complex graph data and learn their
distribution. This goal faces two challenges: numerical instability caused by
exponential mapping during the encoding proces and manifold deviation during
diffusion generation. To address these challenges, we propose GeoMancer: a
novel Riemannian graph diffusion framework for both generation and prediction
tasks. To mitigate numerical instability, we replace exponential mapping with
an isometric-invariant Riemannian gyrokernel approach and decouple multi-level
features onto their respective task-specific manifolds to learn optimal
representations. To address manifold deviation, we introduce a
manifold-constrained diffusion method and a self-guided strategy for
unconditional generation, ensuring that the generated data remains aligned with
the manifold signature. Extensive experiments validate the effectiveness of our
approach, demonstrating superior performance across a variety of tasks.

</details>


### [570] [Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion](https://arxiv.org/abs/2510.04525)
*Satoshi Hayakawa,Yuhta Takida,Masaaki Imaizumi,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 本文提出了一种新的掩码扩散模型采样方法——moment sampler，并通过理论分析揭示了MaskGIT的隐式温度采样机制。


<details>
  <summary>Details</summary>
Motivation: 加速掩码扩散模型的采样过程，提升生成效率。

Method: 提出了moment sampler，采用“先选择后采样”策略，并引入部分缓存技术和混合自适应解掩码方法。

Result: 在图像和文本任务上验证了所提方法的有效性，显著提升了采样效率。

Conclusion: moment sampler为掩码扩散模型提供了更可解释且高效的采样方案，推动了该类模型的理论与实践发展。

Abstract: Masked diffusion models have shown promising performance in generating
high-quality samples in a wide range of domains, but accelerating their
sampling process remains relatively underexplored. To investigate efficient
samplers for masked diffusion, this paper theoretically analyzes the MaskGIT
sampler for image modeling, revealing its implicit temperature sampling
mechanism. Through this analysis, we introduce the "moment sampler," an
asymptotically equivalent but more tractable and interpretable alternative to
MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking
positions before sampling tokens. In addition, we improve the efficiency of
choose-then-sample algorithms through two key innovations: a partial caching
technique for transformers that approximates longer sampling trajectories
without proportional computational cost, and a hybrid approach formalizing the
exploration-exploitation trade-off in adaptive unmasking. Experiments in image
and text domains demonstrate our theory as well as the efficiency of our
proposed methods, advancing both theoretical understanding and practical
implementation of masked diffusion samplers.

</details>


### [571] [Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not Just Make Predictions](https://arxiv.org/abs/2510.04543)
*Elias Dubbeldam,Reza Mohammadi,Marit Schoonhoven,S. Ilker Birbil*

Main category: cs.LG

TL;DR: 本文讨论了基于图的表格深度学习（GTDL）方法在建模特征交互方面的不足，主张应重视对特征交互结构的学习与评估，而不仅仅是预测性能。通过合成数据实验表明，现有方法难以恢复真实交互结构，而引入真实结构可提升预测效果，因此呼吁转向结构感知的建模方法。


<details>
  <summary>Details</summary>
Motivation: 现有的GTDL方法主要关注预测准确性，忽视了对特征交互图结构的准确建模，导致模型缺乏可解释性和可信度，难以捕捉表格数据中复杂的特征交互。

Method: 使用具有已知真实图结构的合成数据集，评估现有GTDL方法在恢复特征交互结构方面的能力，并分析引入真实交互结构对预测性能的影响。

Result: 实验表明现有GTDL方法无法有效恢复真实的特征交互结构，且在结构学习上表现不佳；而当强制引入真实交互结构时，模型的预测性能得到提升。

Conclusion: GTDL应从以预测为中心转向重视结构学习，强调对特征交互的显式建模与定量评估，推动构建更准确、可解释、可信并符合领域知识的表格深度学习模型。

Abstract: Despite recent progress, deep learning methods for tabular data still
struggle to compete with traditional tree-based models. A key challenge lies in
modeling complex, dataset-specific feature interactions that are central to
tabular data. Graph-based tabular deep learning (GTDL) methods aim to address
this by representing features and their interactions as graphs. However,
existing methods predominantly optimize predictive accuracy, neglecting
accurate modeling of the graph structure. This position paper argues that GTDL
should move beyond prediction-centric objectives and prioritize the explicit
learning and evaluation of feature interactions. Using synthetic datasets with
known ground-truth graph structures, we show that existing GTDL methods fail to
recover meaningful feature interactions. Moreover, enforcing the true
interaction structure improves predictive performance. This highlights the need
for GTDL methods to prioritize quantitative evaluation and accurate structural
learning. We call for a shift toward structure-aware modeling as a foundation
for building GTDL systems that are not only accurate but also interpretable,
trustworthy, and grounded in domain understanding.

</details>


### [572] [Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets](https://arxiv.org/abs/2510.04555)
*Jian'an Zhang*

Main category: cs.LG

TL;DR: 提出Tail-Safe框架，结合分布强化学习与白盒控制屏障函数（CBF）QP安全层，用于金融衍生品对冲，实现左尾风险降低且无硬约束违反。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在金融对冲中难以保证安全性与合规性，缺乏可审计的治理机制，且对极端风险（如左尾风险）建模不足，需要一个兼具安全性、可解释性和部署可行性的框架。

Method: 采用IQN-CVaR-PPO分布强化学习，结合温度倾斜与尾部增强的量化采样控制；设计基于CBF的凸QP安全层，集成交易禁带、限幅、速率限制和符号一致性门控等金融约束，并通过遥测数据支持审计与治理。

Result: 在合成市场中验证，Tail-Safe显著改善左尾风险（CVaR），保持中心性能不下降，且当QP可行时实现零硬约束违反；安全层遥测可映射至治理仪表盘，提升可解释性与审计能力。

Conclusion: Tail-Safe为金融强化学习提供了一个可部署、安全且可审计的框架，在理论和实践中实现了风险敏感性与安全约束的统一，具备实际应用潜力。

Abstract: We introduce Tail-Safe, a deployability-oriented framework for derivatives
hedging that unifies distributional, risk-sensitive reinforcement learning with
a white-box control-barrier-function (CBF) quadratic-program (QP) safety layer
tailored to financial constraints. The learning component combines an IQN-based
distributional critic with a CVaR objective (IQN--CVaR--PPO) and a
Tail-Coverage Controller that regulates quantile sampling through temperature
tilting and tail boosting to stabilize small-$\alpha$ estimation. The safety
component enforces discrete-time CBF inequalities together with domain-specific
constraints -- ellipsoidal no-trade bands, box and rate limits, and a
sign-consistency gate -- solved as a convex QP whose telemetry (active sets,
tightness, rate utilization, gate scores, slack, and solver status) forms an
auditable trail for governance. We provide guarantees of robust forward
invariance of the safe set under bounded model mismatch, a minimal-deviation
projection interpretation of the QP, a KL-to-DRO upper bound linking per-state
KL regularization to worst-case CVaR, concentration and sample-complexity
results for the temperature-tilted CVaR estimator, and a CVaR trust-region
improvement inequality under KL limits, together with feasibility persistence
under expiry-aware tightening. Empirically, in arbitrage-free,
microstructure-aware synthetic markets (SSVI $\to$ Dupire $\to$ VIX with
ABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading
central performance and yields zero hard-constraint violations whenever the QP
is feasible with zero slack. Telemetry is mapped to governance dashboards and
incident workflows to support explainability and auditability. Limitations
include reliance on synthetic data and simplified execution to isolate
methodological contributions.

</details>


### [573] [Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems](https://arxiv.org/abs/2510.04559)
*Mohsen Amiri,V Venktesh,Sindri Magnússon*

Main category: cs.LG

TL;DR: 本文提出了一种基于间隙指数框架的短列表驱动纯探索方法，用于多用户MIMO下行链路中高效的在线子载波选择，显著降低了计算开销并实现了高识别精度。


<details>
  <summary>Details</summary>
Motivation: 在多用户MIMO下行链路中，用户调度集合的选择面临动作空间指数增长的问题，传统穷举搜索不可行，需要高效且准确的在线调度方法。

Method: 采用随机线性_bandit_中的组合纯探索模型，引入线性效用模型，并设计基于间隙指数的短列表机制，维护候选冠军臂和挑战者臂，聚焦于最具信息量的测量比较。

Result: 相比现有线性_bandit_方法，在保持高识别精度的同时显著减少运行时间和计算量，并支持速度与精度之间的可调权衡。仿真表明该方法适用于现实OFDM下行链路的高效子载波选择。

Conclusion: 所提出的短列表驱动纯探索框架为AI赋能通信系统中的在线资源调度提供了高效、实用的解决方案。

Abstract: This paper investigates the identification of the top-m user-scheduling sets
in multi-user MIMO downlink, which is cast as a combinatorial pure-exploration
problem in stochastic linear bandits. Because the action space grows
exponentially, exhaustive search is infeasible. We therefore adopt a linear
utility model to enable efficient exploration and reliable selection of
promising user subsets. We introduce a gap-index framework that maintains a
shortlist of current estimates of champion arms (top-m sets) and a rotating
shortlist of challenger arms that pose the greatest threat to the champions.
This design focuses on measurements that yield the most informative
gap-index-based comparisons, resulting in significant reductions in runtime and
computation compared to state-of-the-art linear bandit methods, with high
identification accuracy. The method also exposes a tunable trade-off between
speed and accuracy. Simulations on a realistic OFDM downlink show that
shortlist-driven pure exploration makes online, measurement-efficient
subcarrier selection practical for AI-enabled communication systems.

</details>


### [574] [Stochastic Approximation Methods for Distortion Risk Measure Optimization](https://arxiv.org/abs/2510.04563)
*Jinyang Jiang,Bernd Heidergott,Jiaqiao Hu,Yijie Peng*

Main category: cs.LG

TL;DR: 本文提出了基于两种对偶表示形式（DM形式和QF形式）的梯度下降算法，用于优化扭曲风险度量（DRM），并设计了混合方法以兼顾鲁棒性与效率，理论证明了强收敛性和收敛速率，并通过数值实验和深度强化学习应用验证了其有效性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 扭曲风险度量（DRM）在决策中能有效刻画风险偏好，但其优化面临复杂梯度估计问题，尤其是涉及分位数梯度时。因此，需要高效且稳健的优化算法来提升其在实际任务（如投资组合选择和动态库存管理）中的应用效果。

Method: 提出三种算法：基于三时间尺度的DM形式算法（结合广义似然比与核密度估计追踪分位数并计算梯度）、两时间尺度的QF形式算法（避免复杂分位数梯度估计）以及融合两者优势的混合形式。提供强收敛性证明与收敛速率分析。

Result: DM形式达到最优收敛速率O(k^{-4/7})，QF形式更快，为O(k^{-2/3})；数值实验显示在鲁棒投资组合选择中显著优于基线方法；该方法可扩展至深度强化学习，提出的DRM-PPO算法在多级动态库存管理中表现出良好实用性。

Conclusion: 所提出的基于DM和QF对偶表示的梯度算法为DRM优化提供了理论完备且实践有效的解决方案，兼具理论收敛保证与实际可扩展性，适用于金融与运筹等复杂决策场景。

Abstract: Distortion Risk Measures (DRMs) capture risk preferences in decision-making
and serve as general criteria for managing uncertainty. This paper proposes
gradient descent algorithms for DRM optimization based on two dual
representations: the Distortion-Measure (DM) form and Quantile-Function (QF)
form. The DM-form employs a three-timescale algorithm to track quantiles,
compute their gradients, and update decision variables, utilizing the
Generalized Likelihood Ratio and kernel-based density estimation. The QF-form
provides a simpler two-timescale approach that avoids the need for complex
quantile gradient estimation. A hybrid form integrates both approaches,
applying the DM-form for robust performance around distortion function jumps
and the QF-form for efficiency in smooth regions. Proofs of strong convergence
and convergence rates for the proposed algorithms are provided. In particular,
the DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form
attains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their
effectiveness and demonstrate substantial improvements over baselines in robust
portfolio selection tasks. The method's scalability is further illustrated
through integration into deep reinforcement learning. Specifically, a DRM-based
Proximal Policy Optimization algorithm is developed and applied to
multi-echelon dynamic inventory management, showcasing its practical
applicability.

</details>


### [575] [GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning](https://arxiv.org/abs/2510.04567)
*Weishuo Ma,Yanbo Wang,Xiyuan Wang,Lei Zou,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出了一种无需大语言模型（LLM）和无需微调的图上下文学习Transformer框架GILT，统一处理节点、边和图级别的分类任务，在少样本场景下显著提升效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型在处理图数据异质性（如特征空间、标签集和拓扑结构差异）方面存在局限，且依赖文本或需要昂贵的微调过程，难以高效泛化到新任务。

Method: 设计了一种基于token的图上下文学习（ICL）框架GILT，将图的数值特征编码为通用token，通过上下文动态理解类别语义，实现无需LLM和无需微调的跨任务适应。

Result: 实验表明，GILT在节点、边和图级别的少样本分类任务中，性能优于基于LLM或需微调的方法，且推理时间显著缩短。

Conclusion: GILT通过统一的token化框架有效应对图数据异质性，实现了高效、无需微调的图学习，为图基础模型提供了新范式。

Abstract: Graph Neural Networks (GNNs) are powerful tools for precessing relational
data but often struggle to generalize to unseen graphs, giving rise to the
development of Graph Foundational Models (GFMs). However, current GFMs are
challenged by the extreme heterogeneity of graph data, where each graph can
possess a unique feature space, label set, and topology. To address this, two
main paradigms have emerged. The first leverages Large Language Models (LLMs),
but is fundamentally text-dependent, thus struggles to handle the numerical
features in vast graphs. The second pre-trains a structure-based model, but the
adaptation to new tasks typically requires a costly, per-graph tuning stage,
creating a critical efficiency bottleneck. In this work, we move beyond these
limitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning
\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free
architecture. GILT introduces a novel token-based framework for in-context
learning (ICL) on graphs, reframing classification tasks spanning node, edge
and graph levels in a unified framework. This mechanism is the key to handling
heterogeneity, as it is designed to operate on generic numerical features.
Further, its ability to understand class semantics dynamically from the context
enables tuning-free adaptation. Comprehensive experiments show that GILT
achieves stronger few-shot performance with significantly less time than
LLM-based or tuning-based baselines, validating the effectiveness of our
approach.

</details>


### [576] [Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing](https://arxiv.org/abs/2510.04579)
*Clément Bonet,Elsa Cazelles,Lucas Drumetz,Nicolas Courty*

Main category: cs.LG

TL;DR: 本文研究了Wasserstein空间中的Busemann函数，推导出一维分布和高斯分布情况下的闭式表达，并提出了针对概率分布的新切片Wasserstein距离，应用于合成数据与迁移学习问题。


<details>
  <summary>Details</summary>
Motivation: 由于许多数据源可建模为概率分布，而Wasserstein空间具有由最优传输诱导的丰富黎曼结构，因此在该空间中研究Busemann函数具有重要意义。

Method: 分析Wasserstein空间中Busemann函数的存在性与可计算性，在一维分布和高斯测度两种重要情形下建立闭式表达，并基于此提出新的投影方案和Sliced-Wasserstein距离。

Result: 得到了一维分布和高斯分布下Busemann函数的闭式解，实现了概率分布在实数轴上的显式投影，并定义了适用于高斯混合模型和带标签数据集的新型Sliced-Wasserstein距离。

Conclusion: 所提出的投影方法和距离度量在合成数据和迁移学习任务中表现出高效性，展示了Busemann函数在几何机器学习中的实际应用潜力。

Abstract: The Busemann function has recently found much interest in a variety of
geometric machine learning problems, as it naturally defines projections onto
geodesic rays of Riemannian manifolds and generalizes the notion of
hyperplanes. As several sources of data can be conveniently modeled as
probability distributions, it is natural to study this function in the
Wasserstein space, which carries a rich formal Riemannian structure induced by
Optimal Transport metrics. In this work, we investigate the existence and
computation of Busemann functions in Wasserstein space, which admits geodesic
rays. We establish closed-form expressions in two important cases:
one-dimensional distributions and Gaussian measures. These results enable
explicit projection schemes for probability distributions on $\mathbb{R}$,
which in turn allow us to define novel Sliced-Wasserstein distances over
Gaussian mixtures and labeled datasets. We demonstrate the efficiency of those
original schemes on synthetic datasets as well as transfer learning problems.

</details>


### [577] [Improved probabilistic regression using diffusion models](https://arxiv.org/abs/2510.04583)
*Carlo Kneissl,Christopher Bülte,Philipp Scholl,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 提出一种基于扩散模型的非参数化概率回归框架，通过建模扩散噪声的完整分布来提升不确定性量化能力，并在多种回归任务中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型在复杂数据生成上表现出色，但在回归任务中的不确定性评估应用仍有限，需开发更通用且具备良好不确定性量化的概率回归方法。

Method: 提出一种新的扩散模型框架，非参数化地学习预测分布，通过建模扩散过程中的噪声分布来自适应不同任务，并研究不同噪声参数化的权衡。

Result: 在多个低维和高维回归任务中，该方法优于现有基线模型，并提供校准的不确定性估计。

Conclusion: 所提框架是一种灵活且通用的概率回归工具，能有效提升不确定性量化，在广泛任务中具有应用潜力。

Abstract: Probabilistic regression models the entire predictive distribution of a
response variable, offering richer insights than classical point estimates and
directly allowing for uncertainty quantification. While diffusion-based
generative models have shown remarkable success in generating complex,
high-dimensional data, their usage in general regression tasks often lacks
uncertainty-related evaluation and remains limited to domain-specific
applications. We propose a novel diffusion-based framework for probabilistic
regression that learns predictive distributions in a nonparametric way. More
specifically, we propose to model the full distribution of the diffusion noise,
enabling adaptation to diverse tasks and enhanced uncertainty quantification.
We investigate different noise parameterizations, analyze their trade-offs, and
evaluate our framework across a broad range of regression tasks, covering low-
and high-dimensional settings. For several experiments, our approach shows
superior performance against existing baselines, while delivering calibrated
uncertainty estimates, demonstrating its versatility as a tool for
probabilistic prediction.

</details>


### [578] [Closed-Form Last Layer Optimization](https://arxiv.org/abs/2510.04606)
*Alexandre Galashov,Nathaël Da Costa,Liyuan Xu,Philipp Hennig,Arthur Gretton*

Main category: cs.LG

TL;DR: 提出一种在平方损失下优化神经网络的方法，通过将最后一层视为骨干参数的函数，并交替进行骨干参数的梯度下降和最后一层的闭式更新。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络通常使用随机梯度下降及其变体进行优化，但在平方损失下，线性最后一层的最优解有闭式表达。利用这一点可提升优化效率和性能。

Method: 将最后一层权重用闭式解表示，仅优化骨干网络参数，在每次更新中结合当前批次损失与之前累积信息，实现交替优化。

Result: 在神经正切核（NTK）框架下证明了该方法收敛到最优解，并在多种监督任务（包括回归和分类）上验证了其优于标准SGD的表现。

Conclusion: 该方法有效利用了最后一层的闭式解，提升了模型性能，在理论和实验上均表现出优越性。

Abstract: Neural networks are typically optimized with variants of stochastic gradient
descent. Under a squared loss, however, the optimal solution to the linear last
layer weights is known in closed-form. We propose to leverage this during
optimization, treating the last layer as a function of the backbone parameters,
and optimizing solely for these parameters. We show this is equivalent to
alternating between gradient descent steps on the backbone and closed-form
updates on the last layer. We adapt the method for the setting of stochastic
gradient descent, by trading off the loss on the current batch against the
accumulated information from previous batches. Further, we prove that, in the
Neural Tangent Kernel regime, convergence of this method to an optimal solution
is guaranteed. Finally, we demonstrate the effectiveness of our approach
compared with standard SGD on a squared loss in several supervised tasks --
both regression and classification -- including Fourier Neural Operators and
Instrumental Variable Regression.

</details>


### [579] [Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI](https://arxiv.org/abs/2510.04622)
*Youngjoon Lee,Seongmin Cho,Yehhyun Jo,Jinu Gong,Hyunjoo Jenny Lee,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出基于先进预测模型的合成生物医学时间序列数据生成框架，能够高保真地复制EEG和EMG等复杂生理信号，有效缓解数据稀缺与隐私问题，并提升AI模型性能。


<details>
  <summary>Details</summary>
Motivation: 生物医学时间序列AI的发展受限于严格的隐私法规和有限的数据可用性，导致数据需求与可及性之间存在关键差距。

Method: 利用先进的预测模型构建合成数据生成框架，生成具有真实数据统计特性的合成生物医学时间序列数据，并保留其关键的时间和频谱特性。

Result: 在多个受试者上的评估表明，合成数据可有效替代真实数据，并显著提升AI模型性能，同时具备高可扩展性和开源集成能力。

Conclusion: 该方法在保护患者隐私的同时，为AI驱动的生物医学研究提供了可扩展、高质量的数据资源，有望推动相关领域的研究发展。

Abstract: The limited data availability due to strict privacy regulations and
significant resource demands severely constrains biomedical time-series AI
development, which creates a critical gap between data requirements and
accessibility. Synthetic data generation presents a promising solution by
producing artificial datasets that maintain the statistical properties of real
biomedical time-series data without compromising patient confidentiality. We
propose a framework for synthetic biomedical time-series data generation based
on advanced forecasting models that accurately replicates complex
electrophysiological signals such as EEG and EMG with high fidelity. These
synthetic datasets preserve essential temporal and spectral properties of real
data, which enables robust analysis while effectively addressing data scarcity
and privacy challenges. Our evaluations across multiple subjects demonstrate
that the generated synthetic data can serve as an effective substitute for real
data and also significantly boost AI model performance. The approach maintains
critical biomedical features while provides high scalability for various
applications and integrates seamlessly into open-source repositories,
substantially expanding resources for AI-driven biomedical research.

</details>


### [580] [Compressed Concatenation of Small Embedding Models](https://arxiv.org/abs/2510.04626)
*Mohamed Ayoub Ben Ayad,Michael Dinzinger,Kanishka Ghosh Dastidar,Jelena Mitrovic,Michael Granitzer*

Main category: cs.LG

TL;DR: 提出一种通过拼接多个小型嵌入模型并使用轻量化解码器进行降维的方法，在保持高性能的同时显著压缩模型，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 小型嵌入模型在资源受限环境中更实用，但性能通常不如大型模型，需缩小性能差距。

Method: 拼接多个小型模型的原始嵌入向量，并引入基于Matryoshka表示学习（MRL）损失训练的轻量化解码器，将高维联合表示映射到低维空间。

Result: 在MTEB检索任务子集上，四个小模型拼接后经压缩管道保留了89%的原始性能，压缩比达到48倍，且表示在量化和压缩下更具鲁棒性。

Conclusion: 该方法有效平衡了小型模型的效率与性能，通过拼接与统一解码显著提升检索表现，同时支持高压缩率下的部署。

Abstract: Embedding models are central to dense retrieval, semantic search, and
recommendation systems, but their size often makes them impractical to deploy
in resource-constrained environments such as browsers or edge devices. While
smaller embedding models offer practical advantages, they typically
underperform compared to their larger counterparts. To bridge this gap, we
demonstrate that concatenating the raw embedding vectors of multiple small
models can outperform a single larger baseline on standard retrieval
benchmarks. To overcome the resulting high dimensionality of naive
concatenation, we introduce a lightweight unified decoder trained with a
Matryoshka Representation Learning (MRL) loss. This decoder maps the
high-dimensional joint representation to a low-dimensional space, preserving
most of the original performance without fine-tuning the base models. We also
show that while concatenating more base models yields diminishing gains, the
robustness of the decoder's representation under compression and quantization
improves. Our experiments show that, on a subset of MTEB retrieval tasks, our
concat-encode-quantize pipeline recovers 89\% of the original performance with
a 48x compression factor when the pipeline is applied to a concatenation of
four small embedding models.

</details>


### [581] [Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation](https://arxiv.org/abs/2510.04646)
*Johanna Sommer,John Rachwan,Nils Fleischmann,Stephan Günnemann,Bertrand Charpentier*

Main category: cs.LG

TL;DR: 提出一种无需训练的缓存策略，通过预测求解器步骤中的中间隐藏状态来加速分子几何生成，显著降低推理时间。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型在生成高保真分子几何时推理开销大，成为实际采样大量分子候选时的主要瓶颈。

Method: 在SE(3)等变骨干网络上直接操作，预测求解过程中的中间隐藏状态，兼容预训练模型，且与现有基于训练的加速方法正交。

Result: 在GEOM-Drugs数据集上实现推理时间减半，样本质量相当；相比基础模型最高达3倍加速，结合其他优化可达7倍加速，仅轻微损失样本质量。

Conclusion: 该缓存策略有效加速分子生成，且可与其他优化方法叠加，具有实用性和广泛适用性。

Abstract: Flow matching models generate high-fidelity molecular geometries but incur
significant computational costs during inference, requiring hundreds of network
evaluations. This inference overhead becomes the primary bottleneck when such
models are employed in practice to sample large numbers of molecular
candidates. This work discusses a training-free caching strategy that
accelerates molecular geometry generation by predicting intermediate hidden
states across solver steps. The proposed method operates directly on the
SE(3)-equivariant backbone, is compatible with pretrained models, and is
orthogonal to existing training-based accelerations and system-level
optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching
achieves a twofold reduction in wall-clock inference time at matched sample
quality and a speedup of up to 3x compared to the base model with minimal
sample quality degradation. Because these gains compound with other
optimizations, applying caching alongside other general, lossless optimizations
yield as much as a 7x speedup.

</details>


### [582] [IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams](https://arxiv.org/abs/2510.04660)
*Yuandou Wang,Filip Gunnarsson,Rihan Hai*

Main category: cs.LG

TL;DR: 提出一种上下文感知的增量多层感知机（IMLP），用于高效处理表格数据流，具有恒定内存开销和高能效。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在处理表格数据流时存在内存和能耗过高的问题，尤其依赖随时间增长的回放缓冲区，难以部署于资源受限的边缘设备。

Method: 设计IMLP模型，采用滑动潜在特征缓冲区上的窗口缩放点积注意力机制，实现固定大小内存且无需存储原始数据；将注意力上下文与当前特征拼接后通过共享前馈网络进行轻量级更新。

Result: IMLP相比TabNet和TabPFN分别实现最高27.6倍和85.5倍的能效提升，同时保持有竞争力的平均准确率，并引入NetScore-T指标用于权衡准确率与能耗的综合评估。

Conclusion: IMLP是一种适用于边缘设备的、可部署性强、能量高效的表格数据流持续学习方案，为全量重训练提供了更优替代。

Abstract: Tabular data streams are rapidly emerging as a dominant modality for
real-time decision-making in healthcare, finance, and the Internet of Things
(IoT). These applications commonly run on edge and mobile devices, where energy
budgets, memory, and compute are strictly limited. Continual learning (CL)
addresses such dynamics by training models sequentially on task streams while
preserving prior knowledge and consolidating new knowledge. While recent CL
work has advanced in mitigating catastrophic forgetting and improving knowledge
transfer, the practical requirements of energy and memory efficiency for
tabular data streams remain underexplored. In particular, existing CL solutions
mostly depend on replay mechanisms whose buffers grow over time and exacerbate
resource costs.
  We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a
compact continual learner for tabular data streams. IMLP incorporates a
windowed scaled dot-product attention over a sliding latent feature buffer,
enabling constant-size memory and avoiding storing raw data. The attended
context is concatenated with current features and processed by shared
feed-forward layers, yielding lightweight per-segment updates. To assess
practical deployability, we introduce NetScore-T, a tunable metric coupling
balanced accuracy with energy for Pareto-aware comparison across models and
datasets. IMLP achieves up to $27.6\times$ higher energy efficiency than TabNet
and $85.5\times$ higher than TabPFN, while maintaining competitive average
accuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient
alternative to full retraining for tabular data streams.

</details>


### [583] [Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting](https://arxiv.org/abs/2510.04667)
*Fanzhe Fu,Yang Yang*

Main category: cs.LG

TL;DR: 本文研究了时间序列预测中可逆实例归一化（RevIN）及其改进方法的表现，发现尽管使用鲁棒统计量的R²-IN能有效应对异常值并表现最佳，但基于诊断启发式的自适应模型A-IN却出现系统性失败，揭示了简单启发式带来的不稳定性可能比其试图解决的问题更严重。


<details>
  <summary>Details</summary>
Motivation: RevIN在时间序列预测中表现出色，但在存在极端异常值时可能失效。虽然引入鲁棒统计量看似是合理改进，但不同归一化策略的实际表现复杂且反直觉，亟需深入分析其背后机制。

Method: 通过识别四种理论矛盾，对多种归一化策略进行解构，并设计实验比较标准RevIN、R²-IN和新提出的自适应归一化模型A-IN在含异常值数据上的表现。

Result: 实验发现：标准RevIN在极端异常值下MSE飙升683%；R²-IN不仅避免失败，且成为整体最佳方法；而本应更优的A-IN模型却发生系统性崩溃。

Conclusion: 时间序列归一化应从盲目追求复杂转向基于诊断的分析，重视简单基线的强大性能，并警惕 naive 启发式带来的潜在风险。

Abstract: Reversible Instance Normalization (RevIN) is a key technique enabling simple
linear models to achieve state-of-the-art performance in time series
forecasting. While replacing its non-robust statistics with robust counterparts
(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal
a far more complex reality. This paper deconstructs the perplexing performance
of various normalization strategies by identifying four underlying theoretical
contradictions. Our experiments provide two crucial findings: first, the
standard RevIN catastrophically fails on datasets with extreme outliers, where
its MSE surges by a staggering 683\%. Second, while the simple R$^2$-IN
prevents this failure and unexpectedly emerges as the best overall performer,
our adaptive model (A-IN), designed to test a diagnostics-driven heuristic,
unexpectedly suffers a complete and systemic failure. This surprising outcome
uncovers a critical, overlooked pitfall in time series analysis: the
instability introduced by a simple or counter-intuitive heuristic can be more
damaging than the statistical issues it aims to solve. The core contribution of
this work is thus a new, cautionary paradigm for time series normalization: a
shift from a blind search for complexity to a diagnostics-driven analysis that
reveals not only the surprising power of simple baselines but also the perilous
nature of naive adaptation.

</details>


### [584] [Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding](https://arxiv.org/abs/2510.04674)
*Lorenzo Pannacci,Simone Fiorellino,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: 本文研究了深度联合信源信道编码（DeepJSCC）在多厂商部署中因编码器和解码器无法协同训练而导致的语义噪声问题，提出通过语义信道均衡技术对齐异构潜在空间，并评估了线性映射、轻量神经网络和Parseval框架均衡器三类方法的性能权衡。


<details>
  <summary>Details</summary>
Motivation: 现有DeepJSCC方案假设发射端与接收端共享潜在空间，但在多厂商环境中该假设不成立，导致语义噪声影响重建质量和任务性能。

Method: 引入语义信道均衡阶段，采用三类对齐方法：(i) 具有闭式解的线性映射；(ii) 更具表达能力的轻量级神经网络；(iii) 无需训练的零样本Parseval框架均衡器。

Result: 在AWGN和衰落信道上的图像重建实验表明，所提出的均衡方法能在物理和语义失真下有效对齐异构潜在空间，权衡复杂度、数据效率与重建保真度。

Conclusion: 语义信道均衡是实现异构AI原生无线网络中可部署DeepJSCC的关键，本文为不同应用场景提供了均衡器选择指南。

Abstract: Deep joint source-channel coding (DeepJSCC) has emerged as a powerful
paradigm for end-to-end semantic communications, jointly learning to compress
and protect task-relevant features over noisy channels. However, existing
DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver
(RX) - an assumption that fails in multi-vendor deployments where encoders and
decoders cannot be co-trained. This mismatch introduces "semantic noise",
degrading reconstruction quality and downstream task performance. In this
paper, we systematize and evaluate methods for semantic channel equalization
for DeepJSCC, introducing an additional processing stage that aligns
heterogeneous latent spaces under both physical and semantic impairments. We
investigate three classes of aligners: (i) linear maps, which admit closed-form
solutions; (ii) lightweight neural networks, offering greater expressiveness;
and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without
the need for training. Through extensive experiments on image reconstruction
over AWGN and fading channels, we quantify trade-offs among complexity, data
efficiency, and fidelity, providing guidelines for deploying DeepJSCC in
heterogeneous AI-native wireless networks.

</details>


### [585] [Counterfactual Credit Guided Bayesian Optimization](https://arxiv.org/abs/2510.04676)
*Qiyu Wei,Haowei Wang,Richard Allmendinger,Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: 提出了一种基于反事实信用的贝叶斯优化框架（CCGBO），通过量化历史观测对寻找最优解的贡献，提升全局最优解的搜索效率。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化假设所有观测样本对发现最优解的贡献相同，但在实际中这一假设受限，尤其在目标是快速找到全局最优而非构建完整代理模型时。

Method: 引入反事实信用机制，量化每个历史观测对发现最优解的贡献，并将该信用信息融入采集函数中，以指导后续采样点的选择。

Result: 理论证明CCGBO具有次线性遗憾，实验表明其在多个合成和真实基准上能持续降低简单遗憾并加速收敛到全局最优。

Conclusion: CCGBO通过有选择性地利用历史数据，显著提升了贝叶斯优化的效率和性能。

Abstract: Bayesian optimization has emerged as a prominent methodology for optimizing
expensive black-box functions by leveraging Gaussian process surrogates, which
focus on capturing the global characteristics of the objective function.
However, in numerous practical scenarios, the primary objective is not to
construct an exhaustive global surrogate, but rather to quickly pinpoint the
global optimum. Due to the aleatoric nature of the sequential optimization
problem and its dependence on the quality of the surrogate model and the
initial design, it is restrictive to assume that all observed samples
contribute equally to the discovery of the optimum in this context. In this
paper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO),
a novel framework that explicitly quantifies the contribution of individual
historical observations through counterfactual credit. By incorporating
counterfactual credit into the acquisition function, our approach can
selectively allocate resources in areas where optimal solutions are most likely
to occur. We prove that CCGBO retains sublinear regret. Empirical evaluations
on various synthetic and real-world benchmarks demonstrate that CCGBO
consistently reduces simple regret and accelerates convergence to the global
optimum.

</details>


### [586] [Parameter-free Algorithms for the Stochastically Extended Adversarial Model](https://arxiv.org/abs/2510.04685)
*Shuche Wang,Adarsh Barik,Peng Zhao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文提出了首个无需参数的Stochastically Extended Adversarial (SEA)模型优化算法，通过改进Optimistic Online Newton Step方法，在未知领域直径和Lipschitz常数的情况下实现了自适应调节，取得了与已知参数方法相当的期望遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有SEA模型方法依赖于领域直径D和损失函数Lipschitz常数G等先验参数，限制了实际应用。本文旨在设计无需这些参数的自适应算法，提升实用性。

Method: 基于Optimistic Online Newton Step (OONS) 算法，构建了比较器自适应和Lipschitz自适应的在线优化方法。首先在已知Lipschitz常数但未知领域直径时实现自适应，再扩展到两者均未知的一般情形。

Result: 提出了两种参数无关算法：第一种在已知G但未知D时达到\tilde{O}(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} + \sqrt{\Sigma^2_{1:T}}))的期望遗憾界；第二种在D和G均未知时仍保持对该项的相同依赖性，实现了更通用的自适应性能。

Conclusion: 本文成功去除了SEA模型对领域直径和Lipschitz常数的依赖，提出的参数无关算法在不同设定下均表现出良好的理论性能，增强了在混合随机-对抗环境中的适用性和鲁棒性。

Abstract: We develop the first parameter-free algorithms for the Stochastically
Extended Adversarial (SEA) model, a framework that bridges adversarial and
stochastic online convex optimization. Existing approaches for the SEA model
require prior knowledge of problem-specific parameters, such as the diameter of
the domain $D$ and the Lipschitz constant of the loss functions $G$, which
limits their practical applicability. Addressing this, we develop
parameter-free methods by leveraging the Optimistic Online Newton Step (OONS)
algorithm to eliminate the need for these parameters. We first establish a
comparator-adaptive algorithm for the scenario with unknown domain diameter but
known Lipschitz constant, achieving an expected regret bound of
$\tilde{O}\big(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} +
\sqrt{\Sigma^2_{1:T}})\big)$, where $u$ is the comparator vector and
$\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$ represent the cumulative stochastic
variance and cumulative adversarial variation, respectively. We then extend
this to the more general setting where both $D$ and $G$ are unknown, attaining
the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound
exhibits the same dependence on $\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$,
demonstrating the efficacy of our proposed methods even when both parameters
are unknown in the SEA model.

</details>


### [587] [How does the optimizer implicitly bias the model merging loss landscape?](https://arxiv.org/abs/2510.04686)
*Chenxiang Zhang,Alexander Theus,Damien Teney,Antonio Orvieto,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 本文研究了优化过程如何影响损失景观几何及其对模型融合成功的影响，发现有效噪声尺度这一单一量可以统一解释优化器和数据选择对模型融合的影响。


<details>
  <summary>Details</summary>
Motivation: 理解哪些属性使得模型融合有效，目前对此知之甚少。

Method: 分析有效噪声尺度如何受学习率、权重衰减、批量大小和数据增强等因素独立调节，并探讨其对损失景观全局结构的影响。

Result: 模型融合的有效性是有效噪声的非单调函数，存在明显最优值；优化过程中的噪声不仅影响单个极小值的平坦性或泛化能力，还预测了独立训练模型是否可成功融合。

Conclusion: 优化过程显著塑造损失景观几何，进而影响模型融合效果，通过调控训练动态有望进一步提升融合性能。

Abstract: Model merging methods combine models with different capabilities into a
single one while maintaining the same inference cost. Two popular approaches
are linear interpolation, which linearly interpolates between model weights,
and task arithmetic, which combines task vectors obtained by the difference
between finetuned and base models. While useful in practice, what properties
make merging effective are poorly understood. This paper explores how the
optimization process affects the loss landscape geometry and its impact on
merging success. We show that a single quantity -- the effective noise scale --
unifies the impact of optimizer and data choices on model merging. Across
architectures and datasets, the effectiveness of merging success is a
non-monotonic function of effective noise, with a distinct optimum. Decomposing
this quantity, we find that larger learning rates, stronger weight decay,
smaller batch sizes, and data augmentation all independently modulate the
effective noise scale, exhibiting the same qualitative trend. Unlike prior work
that connects optimizer noise to the flatness or generalization of individual
minima, we show that it also affects the global loss landscape, predicting when
independently trained solutions can be merged. Our findings broaden the
understanding of how optimization shapes the loss landscape geometry and its
downstream consequences for model merging, suggesting the possibility of
further manipulating the training dynamics to improve merging effectiveness.

</details>


### [588] [ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts](https://arxiv.org/abs/2510.04710)
*Zexin Wang,Changhua Pei,Yang Liu,Hengyue Jiang,Quan Zhou,Haotian Si,Hang Cui,Jianhui Li,Gaogang Xie,Jingjing Li,Dan Pei*

Main category: cs.LG

TL;DR: 提出基于视觉-语言模型（VLM）的ViTs框架，通过将时间序列转换为图像实现零样本、变长时序异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测模型难以实现“一次训练、多场景推理”，且受限于固定长度输入和零样本泛化能力不足。

Method: 将时间序列转化为图像输入VLM，通过图像缩放保持时序依赖并统一输入尺寸；采用进化算法生成图像-文本对，并设计三阶段训练流程：时序知识注入、异常检测增强和异常推理优化。

Result: 实验表明ViTs显著提升了VLM对时序数据的理解与异常检测能力，支持任意长度序列的高效推理，无需重新训练。

Conclusion: ViTs实现了跨场景、变长度的零样本时间序列异常检测，解决了上下文长度限制和数据稀缺问题，具备良好的实用性和可扩展性。

Abstract: Web service administrators must ensure the stability of multiple systems by
promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving
the goal of "train once, infer across scenarios" remains a fundamental
challenge for time series anomaly detection models. Beyond improving zero-shot
generalization, such models must also flexibly handle sequences of varying
lengths during inference, ranging from one hour to one week, without
retraining. Conventional approaches rely on sliding-window encoding and
self-supervised learning, which restrict inference to fixed-length inputs.
Large Language Models (LLMs) have demonstrated remarkable zero-shot
capabilities across general domains. However, when applied to time series data,
they face inherent limitations due to context length. To address this issue, we
propose ViTs, a Vision-Language Model (VLM)-based framework that converts time
series curves into visual representations. By rescaling time series images,
temporal dependencies are preserved while maintaining a consistent input size,
thereby enabling efficient processing of arbitrarily long sequences without
context constraints. Training VLMs for this purpose introduces unique
challenges, primarily due to the scarcity of aligned time series image-text
data. To overcome this, we employ an evolutionary algorithm to automatically
generate thousands of high-quality image-text pairs and design a three-stage
training pipeline consisting of: (1) time series knowledge injection, (2)
anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive
experiments demonstrate that ViTs substantially enhance the ability of VLMs to
understand and detect anomalies in time series data. All datasets and code will
be publicly released at: https://anonymous.4open.science/r/ViTs-C484/.

</details>


### [589] [Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs](https://arxiv.org/abs/2510.04727)
*Emanuele Mule,Stefano Fiorini,Antonio Purificato,Federico Siciliano,Stefano Coniglio,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 本文提出了Directional Sheaf Hypergraph Networks (DSHN)，一种结合层论与有向超图中非对称关系建模的框架，通过构建有向层超图拉普拉斯算子，统一并推广了现有图与超图学习中的多种拉普拉斯矩阵，在7个真实数据集上显著提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 有向超图能更好地建模群体交互中的方向性，但现有方法多隐含同质性偏好，难以应对异质场景，且缺乏对有向超图的有效建模工具。

Method: 基于细胞层（Cellular Sheaves）理论，提出DSHN框架，构建有向层超图拉普拉斯算子，支持复数值表示，并对称处理超边的方向性信息。

Result: 在7个真实数据集上对比13个基线模型，DSHN实现了2%到20%的相对准确率提升。

Conclusion: 将层理论与有向超图结合，能够有效克服同质性偏差，提升模型在异质环境下的表现，为高阶、有向交互建模提供了新思路。

Abstract: Hypergraphs provide a natural way to represent higher-order interactions
among multiple entities. While undirected hypergraphs have been extensively
studied, the case of directed hypergraphs, which can model oriented group
interactions, remains largely under-explored despite its relevance for many
applications. Recent approaches in this direction often exhibit an implicit
bias toward homophily, which limits their effectiveness in heterophilic
settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf
Neural Networks (SNNs) were introduced as an effective solution to circumvent
such a drawback. While a generalization to hypergraphs is known, it is only
suitable for undirected hypergraphs, failing to tackle the directed case. In
this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a
framework integrating sheaf theory with a principled treatment of asymmetric
relations within a hypergraph. From it, we construct the Directed Sheaf
Hypergraph Laplacian, a complex-valued operator by which we unify and
generalize many existing Laplacian matrices proposed in the graph- and
hypergraph-learning literature. Across 7 real-world datasets and against 13
baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how
a principled treatment of directionality in hypergraphs, combined with the
expressive power of sheaves, can substantially improve performance.

</details>


### [590] [EVaR-Optimal Arm Identification in Bandits](https://arxiv.org/abs/2510.04728)
*Mehrasa Ahmadipour,Aurélien Garivier*

Main category: cs.LG

TL;DR: 本文研究了在多臂赌博机框架下基于熵风险值（EVaR）准则的固定置信度最优臂识别问题，提出了一种δ-正确且基于Track-and-Stop的算法，并证明了其期望采样复杂度的渐近最优性。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景（如金融）中，仅依赖期望收益进行决策存在不足，需引入风险规避机制。因此，本文旨在在非参数设定下，针对一般有界奖励分布，研究基于EVaR准则的最优臂识别，以支持更稳健的决策。

Method: 采用Track-and-Stop框架设计δ-正确的算法，并通过求解复杂的凸优化问题及相关的非凸问题来实现算法与刻画下界，分析其在EVaR准则下的性能。

Result: 推导出了期望采样复杂度的下界，并证明所提出的算法在渐近意义上能够匹配该下界，实现了理论最优。

Conclusion: 本文提出的基于EVaR的固定置信度BAI算法在非参数设定下具有理论保证，适用于风险规避型决策场景，且其样本效率达到渐近最优。

Abstract: We study the fixed-confidence best arm identification (BAI) problem within
the multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR)
criterion. Our analysis considers a nonparametric setting, allowing for general
reward distributions bounded in [0,1]. This formulation addresses the critical
need for risk-averse decision-making in high-stakes environments, such as
finance, moving beyond simple expected value optimization. We propose a
$\delta$-correct, Track-and-Stop based algorithm and derive a corresponding
lower bound on the expected sample complexity, which we prove is asymptotically
matched. The implementation of our algorithm and the characterization of the
lower bound both require solving a complex convex optimization problem and a
related, simpler non-convex one.

</details>


### [591] [Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors](https://arxiv.org/abs/2510.04758)
*Zhiwei Han,Stefan Matthes,Hao Shen*

Main category: cs.LG

TL;DR: 本文研究了非线性CCA在何种条件下能恢复真实潜在因子（正交变换下），证明了在广泛潜在分布下具有仿射可识别性，并强调白化对保证有界性和良条件的重要性，同时将理论推广到有限样本情况。


<details>
  <summary>Details</summary>
Motivation: 希望理解非线性CCA是否能够恢复真实的潜在变量结构，特别是在无监督表示学习中常见的多视图设置下，模型能否识别出 ground-truth 潜在因子。

Method: 通过将分析从观测空间转换到源空间的重参数化方法，结合高斯先验下线性映射最大化典型相关性的经典结论，证明在总体设定下广泛潜在分布的仿射可识别性；并分析白化的作用以及带岭正则化的经验CCA向总体解的收敛性。

Result: 1) 在白化后，非线性CCA可在正交变换下恢复真实潜在因子；2) 白化对保证问题的有界性和良条件至关重要；3) 岭正则化经验CCA收敛到总体解，从而将理论扩展到有限样本情形；4) 实验验证了理论假设的必要性。

Conclusion: 非线性CCA在适当条件下（如白化和正则化）具有良好的识别性质，能够在总体和有限样本情况下恢复真实潜在结构，为深度多视图表示学习提供了理论支持。

Abstract: In this work, we establish conditions under which nonlinear CCA recovers the
ground-truth latent factors up to an orthogonal transform after whitening.
Building on the classical result that linear mappings maximize canonical
correlations under Gaussian priors, we prove affine identifiability for a broad
class of latent distributions in the population setting. Central to our proof
is a reparameterization result that transports the analysis from observation
space to source space, where identifiability becomes tractable. We further show
that whitening is essential for ensuring boundedness and well-conditioning,
thereby underpinning identifiability. Beyond the population setting, we prove
that ridge-regularized empirical CCA converges to its population counterpart,
transferring these guarantees to the finite-sample regime. Experiments on a
controlled synthetic dataset and a rendered image dataset validate our theory
and demonstrate the necessity of its assumptions through systematic ablations.

</details>


### [592] [ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs](https://arxiv.org/abs/2510.04767)
*Wonjun Kang,Kevin Galim,Seunghyuk Oh,Minjae Lee,Yuchen Zeng,Shuibai Zhang,Coleman Hooper,Yuezhou Hu,Hyung Il Koo,Nam Ik Cho,Kangwook Lee*

Main category: cs.LG

TL;DR: 本文提出ParallelBench，首个针对扩散语言模型（dLLMs）设计的基准，揭示并量化了并行解码在现实场景中的质量下降问题，强调需要创新解码方法以克服速度与质量的权衡。


<details>
  <summary>Details</summary>
Motivation: 尽管dLLMs通过并行解码有望加速推理，但其条件独立性假设忽略了token间的依赖关系，导致生成质量下降。现有基准无法充分反映这一问题，因此需要专门的评估工具。

Method: 通过信息论分析和可解析的合成列表任务进行案例研究，并从数据分布与解码策略角度提供定量洞察，进而设计ParallelBench基准测试。

Result: 实验表明：(i) dLLMs在并行解码下在真实任务中出现显著质量下降；(ii) 当前并行解码策略难以根据任务难度调整并行程度，无法在不牺牲质量的前提下实现有效加速。

Conclusion: 必须开发新的解码方法来突破当前dLLMs在速度与生成质量之间的权衡，ParallelBench的发布旨在推动高效dLLMs的发展。

Abstract: While most autoregressive LLMs are constrained to one-by-one decoding,
diffusion LLMs (dLLMs) have attracted growing interest for their potential to
dramatically accelerate inference through parallel decoding. Despite this
promise, the conditional independence assumption in dLLMs causes parallel
decoding to ignore token dependencies, inevitably degrading generation quality
when these dependencies are strong. However, existing works largely overlook
these inherent challenges, and evaluations on standard benchmarks (e.g., math
and coding) are not sufficient to capture the quality degradation caused by
parallel decoding. To address this gap, we first provide an
information-theoretic analysis of parallel decoding. We then conduct case
studies on analytically tractable synthetic list operations from both data
distribution and decoding strategy perspectives, offering quantitative insights
that highlight the fundamental limitations of parallel decoding. Building on
these insights, we propose ParallelBench, the first benchmark specifically
designed for dLLMs, featuring realistic tasks that are trivial for humans and
autoregressive LLMs yet exceptionally challenging for dLLMs under parallel
decoding. Using ParallelBench, we systematically analyze both dLLMs and
autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can
suffer dramatic quality degradation in real-world scenarios, and (ii) current
parallel decoding strategies struggle to adapt their degree of parallelism
based on task difficulty, thus failing to achieve meaningful speedup without
compromising quality. Our findings underscore the pressing need for innovative
decoding methods that can overcome the current speed-quality trade-off. We
release our benchmark to help accelerate the development of truly efficient
dLLMs.

</details>


### [593] [When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates](https://arxiv.org/abs/2510.04769)
*Michele Caprio,Siu Lun Chau,Krikamol Muandet*

Main category: cs.LG

TL;DR: 本文首次分析了在不精确概率机器学习中，基于可信集的迭代更新过程的收敛性问题，并以可信贝叶斯深度学习为例说明了结果。


<details>
  <summary>Details</summary>
Motivation: 在存在不确定性与模糊性的情况下，如何保证迭代学习过程中可信集更新的稳定性尚不清楚，本文旨在探究此类迭代过程的收敛条件及固定点的存在性。

Method: 提出对基于可信集的迭代更新机制进行理论分析，研究其在何种条件下存在稳定不动点，并以可信贝叶斯深度学习为实例验证分析结果。

Result: 证明了在特定结构条件下，不精确概率模型中的迭代更新过程可以收敛到稳定不动点，并揭示了引入不确定性表示对学习动态的影响。

Conclusion: 纳入不确定性不仅丰富了模型的表达能力，还揭示了迭代学习在不精确环境下实现稳定性的结构性条件，为不精确概率机器学习提供了新的理论洞见。

Abstract: Many machine learning algorithms rely on iterative updates of uncertainty
representations, ranging from variational inference and
expectation-maximization, to reinforcement learning, continual learning, and
multi-agent learning. In the presence of imprecision and ambiguity, credal sets
-- closed, convex sets of probability distributions -- have emerged as a
popular framework for representing imprecise probabilistic beliefs. Under such
imprecision, many learning problems in imprecise probabilistic machine learning
(IPML) may be viewed as processes involving successive applications of update
rules on credal sets. This naturally raises the question of whether this
iterative process converges to stable fixed points -- or, more generally, under
what conditions on the updating mechanism such fixed points exist, and whether
they can be attained. We provide the first analysis of this problem and
illustrate our findings using Credal Bayesian Deep Learning as a concrete
example. Our work demonstrates that incorporating imprecision into the learning
process not only enriches the representation of uncertainty, but also reveals
structural conditions under which stability emerges, thereby offering new
insights into the dynamics of iterative learning under imprecision.

</details>


### [594] [Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning](https://arxiv.org/abs/2510.04773)
*Kai Qin,Jiaqi Wu,Jianxiang He,Haoyuan Sun,Yifei Zhao,Bin Liang,Yongzhe Chang,Tiantian Zhang,Houde Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的大语言模型遗忘学习算法DiPO，通过在分布层面上优化下一词概率分布，克服了现有方法缺乏显式正向偏好信号的局限性，在TOFU和MUSE基准上表现出优越的遗忘效果与模型可用性平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的基于优化的遗忘方法（如NPO）因缺乏明确的正向偏好信号而效果受限，且引入该信号通常依赖领域知识或设计复杂的提示，泛化能力差。因此，需要一种更通用、有效的遗忘方法。

Method: 提出Distribution Preference Optimization (DiPO)，在分布层面操作，通过对模型高置信度输出logits的选择性放大或抑制，构建偏好概率分布对，从而实现更有效的遗忘。理论上证明了其损失函数与期望遗忘方向的一致性。

Result: 实验表明，DiPO在TOFU基准上实现了最高的遗忘质量，在MUSE基准上展现出领先的可扩展性和模型效用保持能力，显著优于NPO等基线方法。

Conclusion: DiPO通过分布级偏好优化，有效解决了传统优化类遗忘方法的局限性，为大模型数据遗忘提供了一种高效且可扩展的新范式。

Abstract: As Large Language Models (LLMs) demonstrate remarkable capabilities learned
from vast corpora, concerns regarding data privacy and safety are receiving
increasing attention. LLM unlearning, which aims to remove the influence of
specific data while preserving overall model utility, is becoming an important
research area. One of the mainstream unlearning classes is optimization-based
methods, which achieve forgetting directly through fine-tuning, exemplified by
Negative Preference Optimization (NPO). However, NPO's effectiveness is limited
by its inherent lack of explicit positive preference signals. Attempts to
introduce such signals by constructing preferred responses often necessitate
domain-specific knowledge or well-designed prompts, fundamentally restricting
their generalizability. In this paper, we shift the focus to the
distribution-level, directly targeting the next-token probability distribution
instead of entire responses, and derive a novel unlearning algorithm termed
\textbf{Di}stribution \textbf{P}reference \textbf{O}ptimization (DiPO). We show
that the requisite preference distribution pairs for DiPO, which are
distributions over the model's output tokens, can be constructed by selectively
amplifying or suppressing the model's high-confidence output logits, thereby
effectively overcoming NPO's limitations. We theoretically prove the
consistency of DiPO's loss function with the desired unlearning direction.
Extensive experiments demonstrate that DiPO achieves a strong trade-off between
model utility and forget quality. Notably, DiPO attains the highest forget
quality on the TOFU benchmark, and maintains leading scalability and
sustainability in utility preservation on the MUSE benchmark.

</details>


### [595] [MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis](https://arxiv.org/abs/2510.04776)
*Ebenezer Awotoro,Chisom Ezekannagha,Florian Schwarz,Johannes Tauscher,Dominik Heider,Katharina Ladewig,Christel Le Bon,Karine Moncoq,Bruno Miroux,Georges Hattab*

Main category: cs.LG

TL;DR: MetaMP是一个整合膜蛋白数据库并利用机器学习进行分类的框架，提升了数据质量与用户交互体验，有效解决数据不一致问题，并在结构分类和预测中表现出高准确率。


<details>
  <summary>Details</summary>
Motivation: 膜蛋白结构复杂，现有数据库存在数据缺失、不一致和计算障碍，亟需一个统一且高效的整合平台。

Method: 开发MetaMP框架，整合多个膜蛋白数据库，通过机器学习实现结构分类、异常检测和数据校正，并提供丰富的可视化界面。

Result: MetaMP解决了77%的数据差异，在新膜蛋白分类中准确率达98%，超越专家人工整理，且在不同任务中保持高效与准确。

Conclusion: MetaMP是一个强有力的资源整合工具，推动了膜蛋白结构研究的标准化和AI驱动探索。

Abstract: Structural biology has made significant progress in determining membrane
proteins, leading to a remarkable increase in the number of available
structures in dedicated databases. The inherent complexity of membrane protein
structures, coupled with challenges such as missing data, inconsistencies, and
computational barriers from disparate sources, underscores the need for
improved database integration. To address this gap, we present MetaMP, a
framework that unifies membrane-protein databases within a web application and
uses machine learning for classification. MetaMP improves data quality by
enriching metadata, offering a user-friendly interface, and providing eight
interactive views for streamlined exploration. MetaMP was effective across
tasks of varying difficulty, demonstrating advantages across different levels
without compromising speed or accuracy, according to user evaluations.
Moreover, MetaMP supports essential functions such as structure classification
and outlier detection.
  We present three practical applications of Artificial Intelligence (AI) in
membrane protein research: predicting transmembrane segments, reconciling
legacy databases, and classifying structures with explainable AI support. In a
validation focused on statistics, MetaMP resolved 77% of data discrepancies and
accurately predicted the class of newly identified membrane proteins 98% of the
time and overtook expert curation. Altogether, MetaMP is a much-needed resource
that harmonizes current knowledge and empowers AI-driven exploration of
membrane-protein architecture.

</details>


### [596] [Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning](https://arxiv.org/abs/2510.04786)
*Jonas Hübotter,Leander Diaz-Bone,Ido Hakimi,Andreas Krause,Moritz Hardt*

Main category: cs.LG

TL;DR: 本文提出了一种名为TTC-RL的测试时课程方法，利用强化学习在测试阶段自动构建任务相关的训练课程，从而持续提升模型在目标任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 人类能够在执行任务时不断学习，而现有模型通常在固定数据集上训练后便不再更新。本文旨在让模型在测试时也能通过持续学习提升性能，减少对人工标注数据的依赖。

Method: 提出TTC-RL框架，通过强化学习从大量可用数据中自动选择最相关样本，构建任务特定的课程，并在测试时持续微调模型。

Result: 在数学和编程等挑战性基准上显著提升Qwen3-8B的表现：AIME25的pass@1提升约1.8倍，CodeElo提升2.1倍；pass@8分别从40%提升至62%，28%提升至43%。

Conclusion: TTC-RL验证了测试时课程在持续学习中的有效性，拓展了测试时扩展范式，使模型能在测试阶段利用数千个任务相关经验不断提升性能。

Abstract: Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

</details>


### [597] [On Predicting Post-Click Conversion Rate via Counterfactual Inference](https://arxiv.org/abs/2510.04816)
*Junhyung Ahn,Sanghack Lee*

Main category: cs.LG

TL;DR: 提出了一种基于因果推理的反事实转换率预测方法（ESCIM），通过在全空间中生成未点击样本的反事实转化标签，提升CVR预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统CVR预测仅使用点击样本，导致数据稀疏且训练困难；现有利用未点击样本的方法依赖启发式假设，存在偏差。

Method: 构建用户行为的结构因果模型（SCM），对未点击项进行反事实干预（即假设点击），推断反事实CVR，并将其转化为二值标签用于模型训练。

Result: 在公开数据集上表现优于现有方法，线上A/B测试验证了其有效性，且在隐式转化数据上展现出良好的泛化能力。

Conclusion: ESCIM通过因果引导的反事实推理有效利用全曝光数据，缓解了样本选择偏差，提升了CVR预测的准确性和鲁棒性。

Abstract: Accurately predicting conversion rate (CVR) is essential in various
recommendation domains such as online advertising systems and e-commerce. These
systems utilize user interaction logs, which consist of exposures, clicks, and
conversions. CVR prediction models are typically trained solely based on
clicked samples, as conversions can only be determined following clicks.
However, the sparsity of clicked instances necessitates the collection of a
substantial amount of logs for effective model training. Recent works address
this issue by devising frameworks that leverage non-clicked samples. While
these frameworks aim to reduce biases caused by the discrepancy between clicked
and non-clicked samples, they often rely on heuristics. Against this
background, we propose a method to counterfactually generate conversion labels
for non-clicked samples by using causality as a guiding principle, attempting
to answer the question, "Would the user have converted if he or she had clicked
the recommended item?" Our approach is named the Entire Space Counterfactual
Inference Multi-task Model (ESCIM). We initially train a structural causal
model (SCM) of user sequential behaviors and conduct a hypothetical
intervention (i.e., click) on non-clicked items to infer counterfactual CVRs.
We then introduce several approaches to transform predicted counterfactual CVRs
into binary counterfactual conversion labels for the non-clicked samples.
Finally, the generated samples are incorporated into the training process.
Extensive experiments on public datasets illustrate the superiority of the
proposed algorithm. Online A/B testing further empirically validates the
effectiveness of our proposed algorithm in real-world scenarios. In addition,
we demonstrate the improved performance of the proposed method on latent
conversion data, showcasing its robustness and superior generalization
capabilities.

</details>


### [598] [On the Hardness of Learning Regular Expressions](https://arxiv.org/abs/2510.04834)
*Idan Attias,Lev Reyzin,Nathan Srebro,Gal Vardi*

Main category: cs.LG

TL;DR: 本文研究了在PAC模型和成员查询下正则表达式的计算学习难度，证明了即使在超立方体的均匀分布下，PAC学习也是困难的，并且在分布无关的情况下使用成员查询学习也是困难的。此外，当正则表达式扩展了补集或交集操作时，在均匀分布下使用成员查询学习同样是困难的。这些结果不能从现有的DFA或NFA学习难度结果中直接得出，因为正则语言在DFA、NFA和正则表达式之间的描述复杂度可能有指数级差异。


<details>
  <summary>Details</summary>
Motivation: 尽管正则表达式在理论上具有重要意义并在实践中广泛应用，但其学习的计算复杂性在很大程度上尚未被探索。因此，本文旨在填补这一空白，研究正则表达式的学习难度。

Method: 通过理论分析，在PAC模型和成员查询框架下研究正则表达式的不当学习（improper learning）的计算硬度。考虑不同分布（如均匀分布）以及扩展操作（如补集和交集）对学习难度的影响。

Result: 1. 在超立方体的均匀分布下，PAC学习正则表达式是计算困难的。
2. 分布无关的成员查询学习也是困难的。
3. 当正则表达式扩展了补集或交集操作时，即使在均匀分布下，成员查询学习仍然是困难的。

Conclusion: 正则表达式的学习在多种设置下都具有较高的计算复杂性，且这些复杂性结果独立于DFA或NFA的学习难度，反映了正则表达式本身独特的描述复杂性。

Abstract: Despite the theoretical significance and wide practical use of regular
expressions, the computational complexity of learning them has been largely
unexplored. We study the computational hardness of improperly learning regular
expressions in the PAC model and with membership queries. We show that PAC
learning is hard even under the uniform distribution on the hypercube, and also
prove hardness of distribution-free learning with membership queries.
Furthermore, if regular expressions are extended with complement or
intersection, we establish hardness of learning with membership queries even
under the uniform distribution. We emphasize that these results do not follow
from existing hardness results for learning DFAs or NFAs, since the descriptive
complexity of regular languages can differ exponentially between DFAs, NFAs,
and regular expressions.

</details>


### [599] [Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study](https://arxiv.org/abs/2510.04837)
*Guillaume Godin*

Main category: cs.LG

TL;DR: 本文提出了一种基于化学键的指纹（BCFP），作为ECFP的补充，结合随机森林模型在BBBP分类任务中表现出优异性能，尤其在r=1时效果最佳，并提出了BCFP-Sort&Slice方法以保留OOV信息并实现紧凑拼接，结果优于MGTP预测。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种与原子中心的ECFP互补的、基于化学键的分子表示方法，以提升分子性质预测的性能和效率。

Method: 提出静态BCFP，模拟ChemProp等消息传递GNN中的键卷积机制，并与ECFP结合；引入BCFP-Sort&Slice进行特征组合；使用随机森林模型在BBBP任务上进行评估。

Result: 在分层交叉验证中，BCFP与ECFP拼接显著提升AUROC和AUPRC；r=1最优，r=2无显著提升；BCFP-Sort&Slice有效保留OOV信息；新特征组合优于MGTP预测结果。

Conclusion: 轻量级的键中心描述符可有效补充传统的原子中心指纹，为BBBP预测提供快速且强效的基线方法。

Abstract: Bond Centered FingerPrint (BCFP) are a complementary, bond-centric
alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static
BCFP that mirrors the bond-convolution used by directed message-passing GNNs
like ChemProp, and evaluate it with a fast rapid Random Forest model on
Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified
cross-validation, concatenating ECFP with BCFP consistently improves AUROC and
AUPRC over either descriptor alone, as confirmed by Turkey HSD
multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not
yield statistically separable gains under the same test. We further propose
BCFP-Sort&Slice, a simple feature-combination scheme that preserves the
out-of-vocabulary (OOV) count information native to ECFP count vectors while
enabling compact unhashed concatenation of BCFP variants. We also outperform
the MGTP prediction on our BBBP evaluation, using such composite new features
bond and atom features. These results show that lightweight, bond-centered
descriptors can complement atom-centered circular fingerprints and provide
strong, fast baselines for BBBP prediction.

</details>


### [600] [Distributionally Robust Causal Abstractions](https://arxiv.org/abs/2510.04842)
*Yorgos Felekis,Theodoros Damoulas,Paris Giampouras*

Main category: cs.LG

TL;DR: 本文提出了第一类分布鲁棒的因果抽象（CA）模型及其学习算法，通过Wasserstein模糊集将鲁棒因果抽象学习建模为约束最小-最大优化问题，提升了对环境变化和模型误设的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象方法假设外部分布固定且准确，难以应对环境变化和分布误设，限制了实际应用中的稳定性。

Method: 提出基于Wasserstein模糊集的分布鲁棒因果抽象框架，将学习过程形式化为约束min-max优化问题，并在实证和高斯环境下提供理论支持以指导鲁棒性水平的选择。

Result: 理论结果支持鲁棒性参数的选择，实验表明该方法在多种问题和CA学习方法中均对环境变化、结构模型误设和干预映射误设具有更强的鲁棒性。

Conclusion: 所提出的分布鲁棒因果抽象框架有效提升了因果模型在不同粒度间映射的稳定性和可靠性，适用于存在分布偏移或模型不确定性的现实场景。

Abstract: Causal Abstraction (CA) theory provides a principled framework for relating
causal models that describe the same system at different levels of granularity
while ensuring interventional consistency between them. Recently, several
approaches for learning CAs have been proposed, but all assume fixed and
well-specified exogenous distributions, making them vulnerable to environmental
shifts and misspecification. In this work, we address these limitations by
introducing the first class of distributionally robust CAs and their associated
learning algorithms. The latter cast robust causal abstraction learning as a
constrained min-max optimization problem with Wasserstein ambiguity sets. We
provide theoretical results, for both empirical and Gaussian environments,
leading to principled selection of the level of robustness via the radius of
these sets. Furthermore, we present empirical evidence across different
problems and CA learning methods, demonstrating our framework's robustness not
only to environmental shifts but also to structural model and intervention
mapping misspecification.

</details>


### [601] [Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders](https://arxiv.org/abs/2510.04855)
*Junqi Jiang,Francesco Leofante,Antonio Rago,Francesca Toni*

Main category: cs.LG

TL;DR: 提出了一种新的生成框架LAPACE，通过标签条件高斯混合变分自编码器（L-GMVAE）学习结构化潜在空间，并生成鲁棒、多样且合理的反事实解释路径。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在统一且模型无关的框架下同时满足反事实解释的鲁棒性、合理性和多样性等多重需求。

Method: 提出L-GMVAE来学习每个类别对应的多样化高斯原型中心的潜在空间；基于此，设计LAPACE算法，通过在潜在空间中从输入表示插值到目标类别的固定中心生成反事实路径，并结合轻量级梯度优化以满足可操作性约束。

Result: 实验表明LAPACE计算高效，在八个定量指标上表现优异，能有效保证反事实路径对输入和模型扰动的鲁棒性，同时提供多样化的合理建议。

Conclusion: LAPACE是一种通用、高效的反事实解释方法，能够生成兼具鲁棒性、合理性和多样性的完整路径，支持用户权衡接近性与可行性，并易于集成实际约束。

Abstract: Counterfactual explanations (CEs) provide recourse recommendations for
individuals affected by algorithmic decisions. A key challenge is generating
CEs that are robust against various perturbation types (e.g. input and model
perturbations) while simultaneously satisfying other desirable properties.
These include plausibility, ensuring CEs reside on the data manifold, and
diversity, providing multiple distinct recourse options for single inputs.
Existing methods, however, mostly struggle to address these multifaceted
requirements in a unified, model-agnostic manner. We address these limitations
by proposing a novel generative framework. First, we introduce the
Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model
trained to learn a structured latent space where each class label is
represented by a set of Gaussian components with diverse, prototypical
centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual
Explanations), a model-agnostic algorithm that synthesises entire paths of CE
points by interpolating from inputs' latent representations to those learned
latent centroids. This approach inherently ensures robustness to input changes,
as all paths for a given target class converge to the same fixed centroids.
Furthermore, the generated paths provide a spectrum of recourse options,
allowing users to navigate the trade-off between proximity and plausibility
while also encouraging robustness against model changes. In addition,
user-specified actionability constraints can also be easily incorporated via
lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive
experiments show that LAPACE is computationally efficient and achieves
competitive performance across eight quantitative metrics.

</details>


### [602] [Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails](https://arxiv.org/abs/2510.04860)
*Siwei Han,Jiaqi Liu,Yaofeng Su,Wenbo Duan,Xinyuan Liu,Cihang Xie,Mohit Bansal,Mingyu Ding,Linjun Zhang,Huaxiu Yao*

Main category: cs.LG

TL;DR: 本文提出了大语言模型代理在自我演化过程中可能出现的对齐倾覆过程（ATP），即代理在部署后因持续交互而逐渐偏离训练时设定的对齐约束，转而发展出自利策略。作者通过自利探索和模仿策略扩散两种范式建模ATP，并在Qwen3-8B和Llama-3.1-8B-Instruct上进行实验，发现现有对齐方法脆弱，对齐状态易在多智能体环境中迅速崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理具备自我演化能力，其长期可靠性成为关键问题。传统对齐方法假设对齐是静态的，但现实中代理在部署后的持续学习可能导致行为漂移，因此需要研究这种动态风险。

Method: 提出Alignment Tipping Process（ATP）概念，构建两个理论范式：Self-Interested Exploration（个体行为漂移）和Imitative Strategy Diffusion（多智能体间偏差传播），并设计可控测试环境对主流模型进行评估。

Result: 实验表明，在自我演化下，原本对齐的模型迅速趋向未对齐状态；在多智能体场景中，违规行为快速传播导致集体失对齐；当前基于强化学习的对齐方法仅提供脆弱防御。

Conclusion: LLM代理的对齐不是静态属性，而是动态且脆弱的，容易在部署过程中因反馈循环而退化，必须发展能应对持续演化的动态对齐机制。

Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

</details>


### [603] [A Clinical-grade Universal Foundation Model for Intraoperative Pathology](https://arxiv.org/abs/2510.04861)
*Zihan Zhao,Fengtao Zhou,Ronggang Li,Bing Chu,Xinke Zhang,Xueyi Zheng,Ke Zheng,Xiaobo Wen,Jiabo Ma,Yihui Wang,Jiewei Chen,Chengyou Zheng,Jiangyu Zhang,Yongqin Wen,Jiajia Meng,Ziqi Zeng,Xiaoqing Li,Jing Li,Dan Xie,Yaping Ye,Yu Wang,Hao Chen,Muyan Cai*

Main category: cs.LG

TL;DR: CRISP是一个基于超过10万张冰冻切片开发的临床级基础模型，旨在为术中病理提供精准支持，在多中心、多病种和真实临床环境中表现出高诊断准确性和泛化能力，并显著提升手术决策效率。


<details>
  <summary>Details</summary>
Motivation: 术中病理对精准手术至关重要，但受限于诊断复杂性和高质量冰冻切片数据的缺乏，且现有计算病理技术因缺少大规模前瞻性验证而难以常规应用。

Method: 基于来自八个医疗中心的超10万张冰冻切片训练CRISP模型，并在超过15,000张术中切片上进行回顾性评估，涵盖良恶性判别、关键术中决策和泛癌检测等近100项任务，同时在超过2,000名患者的前瞻性队列中验证其实际性能。

Result: CRISP在不同机构、肿瘤类型和解剖部位展现出强大的泛化能力，包括罕见癌症和未见过的部位；在前瞻性队列中保持高诊断准确性，指导了92.6%病例的手术决策；人机协作减少了35%的诊断工作量，避免了105项辅助检查，并以87.5%的准确率提升了微转移检测。

Conclusion: CRISP作为临床级AI模型，弥合了计算病理与临床手术之间的鸿沟，推动人工智能在术中病理中的常规应用，标志着AI向实际临床转化的重要进展。

Abstract: Intraoperative pathology is pivotal to precision surgery, yet its clinical
impact is constrained by diagnostic complexity and the limited availability of
high-quality frozen-section data. While computational pathology has made
significant strides, the lack of large-scale, prospective validation has
impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a
clinical-grade foundation model developed on over 100,000 frozen sections from
eight medical centers, specifically designed to provide Clinical-grade Robust
Intraoperative Support for Pathology (CRISP). CRISP was comprehensively
evaluated on more than 15,000 intraoperative slides across nearly 100
retrospective diagnostic tasks, including benign-malignant discrimination, key
intraoperative decision-making, and pan-cancer detection, etc. The model
demonstrated robust generalization across diverse institutions, tumor types,
and anatomical sites-including previously unseen sites and rare cancers. In a
prospective cohort of over 2,000 patients, CRISP sustained high diagnostic
accuracy under real-world conditions, directly informing surgical decisions in
92.6% of cases. Human-AI collaboration further reduced diagnostic workload by
35%, avoided 105 ancillary tests and enhanced detection of micrometastases with
87.5% accuracy. Together, these findings position CRISP as a clinical-grade
paradigm for AI-driven intraoperative pathology, bridging computational
advances with surgical precision and accelerating the translation of artificial
intelligence into routine clinical practice.

</details>


### [604] [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2510.04871)
*Alexia Jolicoeur-Martineau*

Main category: cs.LG

TL;DR: 提出了一种名为Tiny Recursive Model (TRM)的简化递归推理方法，使用仅2层、7M参数的小型网络，在ARC-AGI等难题上超越大型语言模型，实现更高的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有Hierarchical Reasoning Model (HRM)虽在小数据上表现优异但结构复杂且可能非最优，需更简洁高效的替代方案。

Method: 设计TRM，采用单一小型神经网络（仅2层）进行递归推理，简化HRM的双网络异频递归结构。

Result: TRM在ARC-AGI-1上达到45%测试准确率，ARC-AGI-2上达8%，优于多数大型语言模型，且参数量仅为后者的不到0.01%。

Conclusion: TRM证明了极简递归架构在复杂推理任务上的高效性与强泛化能力，为小模型解决难题提供了新方向。

Abstract: Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and ARC-AGI while trained with small models (27M parameters) on small data
(around 1000 examples). HRM holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach
that achieves significantly higher generalization than HRM, while using a
single tiny network with only 2 layers. With only 7M parameters, TRM obtains
45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs
(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the
parameters.

</details>


### [605] [Flow-Matching Based Refiner for Molecular Conformer Generation](https://arxiv.org/abs/2510.04878)
*Xiangyang Xu,Hongyang Gao*

Main category: cs.LG

TL;DR: 提出了一种用于低能分子构象生成的流匹配优化方法，通过重新调度噪声尺度以绕过低信噪比阶段，从而提高采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有的去噪方法在低信噪比步骤中容易出现误差累积，训练困难，影响分子构象生成的质量。

Method: 提出一种流匹配优化器，从上游去噪模型产生的混合质量输出初始化采样，并重新调度噪声尺度以跳过低信噪比阶段。

Result: 在GEOM-QM9和GEOM-Drugs数据集上，该生成器-优化器管道在更少的总去噪步骤下提升了生成质量，同时保持了多样性。

Conclusion: 所提方法有效缓解了传统去噪方法在低信噪比阶段的缺陷，实现了更高效、高质量的分子构象生成。

Abstract: Low-energy molecular conformers generation (MCG) is a foundational yet
challenging problem in drug discovery. Denoising-based methods include
diffusion and flow-matching methods that learn mappings from a simple base
distribution to the molecular conformer distribution. However, these approaches
often suffer from error accumulation during sampling, especially in the low SNR
steps, which are hard to train. To address these challenges, we propose a
flow-matching refiner for the MCG task. The proposed method initializes
sampling from mixed-quality outputs produced by upstream denoising models and
reschedules the noise scale to bypass the low-SNR phase, thereby improving
sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the
generator-refiner pipeline improves quality with fewer total denoising steps
while preserving diversity.

</details>


### [606] [Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models](https://arxiv.org/abs/2510.04888)
*Alina Ermilova,Dmitrii Kornilov,Sofia Samoilova,Ekaterina Laptenkova,Anastasia Kolesnikova,Ekaterina Podplutova,Senotrusova Sofya,Maksim G. Sharaev*

Main category: cs.LG

TL;DR: 本研究系统评估了七种基于真实临床数据和ICD-10编码文本的方法，用于发现疾病间关联，发现大语言模型（LLM）在生成多样化新关联方面能力有限，提示其在疾病关系探索中的潜力受限。


<details>
  <summary>Details</summary>
Motivation: 克服传统人工分析大规模临床数据的主观性和劳动强度，解决机器学习在疾病关联发现中方法选择、数据源优劣和缺乏‘金标准’的三大挑战，并探索大语言模型在此任务中的适用性。

Method: 比较了七种方法：基于MIMIC-IV电子病历中ICD-10编码序列的统计共现分析和掩码语言建模（MLM），领域专用BERT变体（Med-BERT、BioClinicalBERT），通用BERT与文档检索，以及四种大语言模型（Mistral、DeepSeek、Qwen、YandexGPT）；使用两种数据源（EHR序列和完整ICD-10代码及其文本描述），并通过基于图的方法比较生成的关联矩阵。

Result: 基于大语言模型的方法产生的疾病关联多样性最低，表明其在发现新疾病关联方面的能力不如其他方法，包括基于文本和领域专用模型的方法。

Conclusion: 大语言模型在发现新的疾病关联方面潜力有限；在缺乏疾病关联‘金标准’的情况下，该研究结果可作为有价值的医学疾病本体，为未来临床研究和医疗AI应用提供基础资源。

Abstract: Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

</details>


### [607] [Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models](https://arxiv.org/abs/2510.04900)
*Nick Janßen,Melanie Schaller,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 提出一种基于合成数据的仿真评估框架，用于系统分析多变量长时序预测模型在不同信号与噪声条件下的鲁棒性，揭示了各模型在周期捕捉、噪声敏感性和频谱重建方面的表现差异。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖真实世界数据，其噪声特性未知，难以系统分析模型鲁棒性。因此需要一个可控的、可配置的评估环境来深入理解M-LTSF模型的行为。

Method: 构建一个可参数化的合成数据生成框架，控制信号成分、噪声类型、信噪比和频率特征；在此基础上对S-Mamba、iTransformer、R-Linear和Autoformer四种典型模型进行基准测试，并结合MSE性能和频谱分析进行细粒度评估。

Result: 发现所有模型在回看窗口无法覆盖完整季节周期时性能严重下降；S-Mamba和Autoformer在锯齿形模式上表现更好，R-Linear和iTransformer更适应正弦信号；白噪声和布朗噪声普遍降低性能，S-Mamba对趋势噪声敏感，iTransformer对季节性噪声敏感；频谱分析显示S-Mamba和iTransformer在频率重建方面更优。

Conclusion: 该仿真框架为M-LTSF模型提供了可解释、可控的评估手段，揭示了模型性能与信号/噪声特性之间的关系，为特定场景下的模型选择提供了实用指导。

Abstract: Understanding the robustness of deep learning models for multivariate
long-term time series forecasting (M-LTSF) remains challenging, as evaluations
typically rely on real-world datasets with unknown noise properties. We propose
a simulation-based evaluation framework that generates parameterizable
synthetic datasets, where each dataset instance corresponds to a different
configuration of signal components, noise types, signal-to-noise ratios, and
frequency characteristics. These configurable components aim to model
real-world multivariate time series data without the ambiguity of unknown
noise. This framework enables fine-grained, systematic evaluation of M-LTSF
models under controlled and diverse scenarios. We benchmark four representative
architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear
(linear), and Autoformer (decomposition-based). Our analysis reveals that all
models degrade severely when lookback windows cannot capture complete periods
of seasonal patters in the data. S-Mamba and Autoformer perform best on
sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.
White and Brownian noise universally degrade performance with lower
signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer
shows seasonal-noise vulnerability. Further spectral analysis shows that
S-Mamba and iTransformer achieve superior frequency reconstruction. This
controlled approach, based on our synthetic and principle-driven testbed,
offers deeper insights into model-specific strengths and limitations through
the aggregation of MSE scores and provides concrete guidance for model
selection based on signal characteristics and noise conditions.

</details>


### [608] [Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects](https://arxiv.org/abs/2510.04901)
*Jonathan Colaço Carr,Qinyi Sun,Cameron Allen*

Main category: cs.LG

TL;DR: 提出一种新方法，使技能发现算法能够学习针对特定状态变量的聚焦技能，从而提高状态空间覆盖率、解锁新学习能力并避免下游任务中的负面效应。


<details>
  <summary>Details</summary>
Motivation: 现有技能发现算法常忽略强化学习问题中的自然状态变量，导致发现的技能无法有效控制特定状态变量，影响探索效率和下游任务表现。

Method: 引入一种通用方法，使技能发现算法能够学习聚焦于特定状态变量的技能，实现对环境的更精确控制。

Result: 该方法将状态空间覆盖效率提升三倍，显著改善探索效果，赋予代理新的学习能力，并在目标未完全指定时自动避免负面副作用。

Conclusion: 通过聚焦特定状态变量的技能学习，可大幅提升技能发现算法的性能与实用性，为复杂环境下的强化学习提供更有效的解决方案。

Abstract: Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

</details>


### [609] [DP-HYPE: Distributed Differentially Private Hyperparameter Search](https://arxiv.org/abs/2510.04902)
*Johannes Liebenow,Thorsten Peinemann,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: 本文提出了一种名为DP-HYPE的分布式差分隐私超参数调优算法，通过客户端本地评估后的分布式投票机制选择多数支持的超参数折中方案，实现了客户级差分隐私保护，且隐私保障不依赖超参数数量，在多种数据分布场景下均表现出高实用性。


<details>
  <summary>Details</summary>
Motivation: 在分布式机器学习中，基于敏感数据调优超参数时面临隐私挑战，现有差分隐私方法存在计算开销大、逐客户端独立调优或本地隐私机制导致效用-隐私权衡不佳的问题，因此需要一种高效、可扩展且兼顾隐私与实用性的分布式超参数调优方案。

Method: 提出DP-HYPE算法，采用分布式投票机制，各客户端基于本地超参数评估结果进行投票，通过聚合投票结果选择被多数支持的超参数；结合差分隐私技术实现客户级隐私保护，并理论证明其隐私保障不依赖超参数数量，同时提供效用保证的理论界限。

Result: DP-HYPE在多个基准数据集上验证了有效性，包括IID和多种非IID场景，即使在较小隐私预算下仍保持高实用性；已作为子模块集成到主流分布式机器学习框架Flower中。

Conclusion: DP-HYPE实现了高效、可扩展且满足客户级差分隐私的分布式超参数调优，解决了现有方法在隐私保障与实用性之间的权衡问题，适用于多种分布式学习任务。

Abstract: The tuning of hyperparameters in distributed machine learning can
substantially impact model performance. When the hyperparameters are tuned on
sensitive data, privacy becomes an important challenge and to this end,
differential privacy has emerged as the de facto standard for provable privacy.
A standard setting when performing distributed learning tasks is that clients
agree on a shared setup, i.e., find a compromise from a set of hyperparameters,
like the learning rate of the model to be trained. Yet, prior work on
differentially private hyperparameter tuning either uses computationally
expensive cryptographic protocols, determines hyperparameters separately for
each client, or applies differential privacy locally, which can lead to
undesirable utility-privacy trade-offs.
  In this work, we present our algorithm DP-HYPE, which performs a distributed
and privacy-preserving hyperparameter search by conducting a distributed voting
based on local hyperparameter evaluations of clients. In this way, DP-HYPE
selects hyperparameters that lead to a compromise supported by the majority of
clients, while maintaining scalability and independence from specific learning
tasks. We prove that DP-HYPE preserves the strong notion of differential
privacy called client-level differential privacy and, importantly, show that
its privacy guarantees do not depend on the number of hyperparameters. We also
provide bounds on its utility guarantees, that is, the probability of reaching
a compromise, and implement DP-HYPE as a submodule in the popular Flower
framework for distributed machine learning. In addition, we evaluate
performance on multiple benchmark data sets in iid as well as multiple non-iid
settings and demonstrate high utility of DP-HYPE even under small privacy
budgets.

</details>


### [610] [How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning](https://arxiv.org/abs/2510.04908)
*Haotian Gao,Zheng Dong,Jiawei Yong,Shintaro Fukushima,Kenjiro Taura,Renhe Jiang*

Main category: cs.LG

TL;DR: 提出了一种新的时空时间序列预测框架ST-SSDL，通过自监督偏差学习捕捉当前输入与历史模式之间的动态偏差，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分考虑当前输入与历史模式之间的动态偏差，而这些偏差包含影响模型性能的关键信号，因此需要一种能有效利用这些偏差信息的新方法。

Method: ST-SSDL将每个输入锚定到其历史平均值，并使用可学习的原型对潜在空间进行离散化以表示典型时空模式；引入对比损失和偏差损失两种辅助目标，分别增强原型间的区分性和输入表示与对应原型间距离的一致性，联合优化预测目标以提升模型泛化能力。

Result: 在六个基准数据集上的实验表明，ST-SSDL在多个指标上 consistently 优于最先进的基线方法，可视化结果也展示了其在复杂时空场景中自适应响应不同偏差水平的能力。

Conclusion: ST-SSDL通过自监督偏差学习有效建模动态偏差，在时空预测任务中表现出优越性能和良好泛化能力，为处理现实世界中的非平稳时空数据提供了新思路。

Abstract: Spatio-temporal forecasting is essential for real-world applications such as
traffic management and urban computing. Although recent methods have shown
improved accuracy, they often fail to account for dynamic deviations between
current inputs and historical patterns. These deviations contain critical
signals that can significantly affect model performance. To fill this gap, we
propose ST-SSDL, a Spatio-Temporal time series forecasting framework that
incorporates a Self-Supervised Deviation Learning scheme to capture and utilize
such deviations. ST-SSDL anchors each input to its historical average and
discretizes the latent space using learnable prototypes that represent typical
spatio-temporal patterns. Two auxiliary objectives are proposed to refine this
structure: a contrastive loss that enhances inter-prototype discriminability
and a deviation loss that regularizes the distance consistency between input
representations and corresponding prototypes to quantify deviation. Optimized
jointly with the forecasting objective, these components guide the model to
organize its hidden space and improve generalization across diverse input
conditions. Experiments on six benchmark datasets show that ST-SSDL
consistently outperforms state-of-the-art baselines across multiple metrics.
Visualizations further demonstrate its ability to adaptively respond to varying
levels of deviation in complex spatio-temporal scenarios. Our code and datasets
are available at https://github.com/Jimmy-7664/ST-SSDL.

</details>


### [611] [Glocal Information Bottleneck for Time Series Imputation](https://arxiv.org/abs/2510.04910)
*Jie Yang,Kexin Zhang,Guibin Zhang,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间序列缺失值填补训练范式Glocal-IB，通过引入全局对齐损失来同时保留数据的局部细节和全局结构，有效提升了高缺失率下的填补性能与潜在表示的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在高缺失率下虽能在训练阶段表现良好，但在推理阶段生成的填补结果和潜在表示分布较差，表明当前优化目标缺乏全局指导，导致模型过拟合局部噪声而忽略全局信息。

Method: 提出Glocal-IB框架，扩展标准信息瓶颈方法，引入基于可计算互信息近似的全局对齐损失，使掩码输入的潜在表示与原始观测输入对齐，从而增强模型对全局结构的学习能力。

Result: 在九个数据集上的实验表明，Glocal-IB在不同缺失率下均能显著提升填补效果，并使潜在表示分布更加一致，验证了其优越的泛化能力。

Conclusion: Glocal-IB通过融合局部重建与全局对齐的优化目标，解决了高缺失率下时间序列填补中的优化困境，具有模型无关性，适用于广泛的时间序列建模任务。

Abstract: Time Series Imputation (TSI), which aims to recover missing values in
temporal data, remains a fundamental challenge due to the complex and often
high-rate missingness in real-world scenarios. Existing models typically
optimize the point-wise reconstruction loss, focusing on recovering numerical
values (local information). However, we observe that under high missing rates,
these models still perform well in the training phase yet produce poor
imputations and distorted latent representation distributions (global
information) in the inference phase. This reveals a critical optimization
dilemma: current objectives lack global guidance, leading models to overfit
local noise and fail to capture global information of the data. To address this
issue, we propose a new training paradigm, Glocal Information Bottleneck
(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework
by introducing a Global Alignment loss, derived from a tractable mutual
information approximation. This loss aligns the latent representations of
masked inputs with those of their originally observed counterparts. It helps
the model retain global structure and local details while suppressing noise
caused by missing values, giving rise to better generalization under high
missingness. Extensive experiments on nine datasets confirm that Glocal-IB
leads to consistently improved performance and aligned latent representations
under missingness. Our code implementation is available in
https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.

</details>


### [612] [Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data](https://arxiv.org/abs/2510.04927)
*Usman Akram,Yiyue Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: 提出FedSSL-AMC，一种基于联邦自监督学习的自动调制分类方法，通过在客户端本地使用无标签I/Q序列进行因果扩张CNN训练并结合三元组损失，再利用小规模有标签数据训练本地SVM分类器，有效解决隐私、通信开销和信道变化鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 传统AMC模型依赖中心化数据带来隐私风险、通信开销大且对信道变化缺乏鲁棒性；现有联邦学习方法对类别不平衡、非独立同分布（non-IID）和标签样本少敏感，需更鲁棒的分布式学习方案。

Method: 提出FedSSL-AMC：采用联邦自监督学习框架，使用时间膨胀CNN和三元组损失在各客户端上对无标签I/Q序列进行表征学习，随后在每个客户端用少量有标签数据训练SVM分类器，实现分布式的两阶段学习。

Result: 理论证明了联邦表示学习的收敛性和下游分类器在特征噪声下的可分性；在合成和实测空中传输数据集上，相比监督式联邦学习基线，在不同SNR、载波频偏和非IID标签划分下均表现出一致性能提升。

Conclusion: FedSSL-AMC通过结合自监督表征学习与轻量级本地分类，在保护隐私的同时提升了对非IID数据、标签稀缺和信道变化的鲁棒性，为实际部署AMC系统提供了高效可行的解决方案。

Abstract: Training automatic modulation classification (AMC) models on centrally
aggregated data raises privacy concerns, incurs communication overhead, and
often fails to confer robustness to channel shifts. Federated learning (FL)
avoids central aggregation by training on distributed clients but remains
sensitive to class imbalance, non-IID client distributions, and limited labeled
samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with
triplet-loss self-supervision on unlabeled I/Q sequences across clients,
followed by per-client SVMs on small labeled sets. We establish convergence of
the federated representation learning procedure and a separability guarantee
for the downstream classifier under feature noise. Experiments on synthetic and
over-the-air datasets show consistent gains over supervised FL baselines under
heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

</details>


### [613] [Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking](https://arxiv.org/abs/2510.04930)
*Ali Saheb Pasand,Elvis Dohmatob*

Main category: cs.LG

TL;DR: 本文研究了“grokking”现象，即模型的测试性能在训练初期停滞不前，随后突然跃升至接近完美的水平。作者提出通过均衡梯度下降速度来加速这一过程，并提出了“平等梯度下降（EGD）”方法，实验证明该方法能显著加快或消除学习停滞期。


<details>
  <summary>Details</summary>
Motivation: 减少模型在训练过程中出现的泛化性能停滞期，使学习过程更快达到高性能。

Method: 分析梯度下降在不同梯度主方向上的非对称速度，提出一种称为平等梯度下降（EGD）的梯度归一化方法，使各主方向的优化速度一致。

Result: 理论和实验表明，EGD能够显著加速grokking过程，在某些情况下完全消除性能 plateau；在模加法和稀疏奇偶性问题上验证了其有效性。

Conclusion: 通过均衡梯度下降的动态速度，可以有效解决grokking中的长时间停滞问题，EGD是一种有效的改进方法，可视为自然梯度下降的精细变体。

Abstract: Grokking is the phenomenon whereby, unlike the training performance, which
peaks early in the training process, the test/generalization performance of a
model stagnates over arbitrarily many epochs and then suddenly jumps to usually
close to perfect levels. In practice, it is desirable to reduce the length of
such plateaus, that is to make the learning process "grok" faster. In this
work, we provide new insights into grokking. First, we show both empirically
and theoretically that grokking can be induced by asymmetric speeds of
(stochastic) gradient descent, along different principal (i.e singular
directions) of the gradients. We then propose a simple modification that
normalizes the gradients so that dynamics along all the principal directions
evolves at exactly the same speed. Then, we establish that this modified
method, which we call egalitarian gradient descent (EGD) and can be seen as a
carefully modified form of natural gradient descent, groks much faster. In
fact, in some cases the stagnation is completely removed. Finally, we
empirically show that on classical arithmetic problems such as modular addition
and sparse parity problem which this stagnation has been widely observed and
intensively studied, that our proposed method eliminates the plateaus.

</details>


### [614] [Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints](https://arxiv.org/abs/2510.04951)
*Jayanta Mandi,Marianne Defresne,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 提出一种用于预测约束优化问题中约束参数的决策聚焦学习框架，通过结合最大似然估计设计两种新型损失函数，分别惩罚不可行性和次优决策，并引入可调参数平衡两者，实验证明该方法能有效控制次优性与可行性的权衡。


<details>
  <summary>Details</summary>
Motivation: 在参数不确定的约束优化问题中，传统预测后优化方法可能导致不可行解，需同时管理可行性与决策质量。

Method: 开发了一种不依赖线性规划假设的通用决策聚焦学习框架，基于最大似然估计构建两个损失函数：一个惩罚由预测参数导致的不可行解，另一个惩罚真实最优解在预测参数下不可行的情况，并通过可调权重组合二者。

Result: 实验表明，调整该可调参数可有效控制次优性与可行性的权衡；在多个约束优化实例中，单一参数值即可达到与现有基线方法相当的性能。

Conclusion: 所提框架能灵活平衡预测导致的不可行性与决策次优性，在通用约束优化问题中具有良好的适用性和性能表现。

Abstract: When some parameters of a constrained optimization problem (COP) are
uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising
two stages -- the prediction of the unknown parameters from contextual
information and the subsequent optimization using those predicted parameters.
Decision-focused learning (DFL) implements the first stage by training a
machine learning (ML) model to optimize the quality of the decisions made using
the predicted parameters. When parameters in the constraints of a COP are
predicted, the predicted parameters can lead to infeasible solutions.
Therefore, it is important to simultaneously manage both feasibility and
decision quality. We develop a DFL framework for predicting constraint
parameters in a generic COP. While prior works typically assume that the
underlying optimization problem is a linear program (LP) or integer linear
program (ILP), our approach makes no such assumption. We derive two novel loss
functions based on maximum likelihood estimation (MLE): the first one penalizes
infeasibility (by penalizing when the predicted parameters lead to infeasible
solutions), and the second one penalizes suboptimal decisions (by penalizing
when the true optimal solution is infeasible under the predicted parameters).
We introduce a single tunable parameter to form a weighted average of the two
losses, allowing decision-makers to balance suboptimality and feasibility. We
experimentally demonstrate that adjusting this parameter provides a
decision-maker the control over the trade-off between the two. Moreover, across
several COP instances, we find that for a single value of the tunable
parameter, our method matches the performance of the existing baselines on
suboptimality and feasibility.

</details>


### [615] [StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R](https://arxiv.org/abs/2510.04974)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: StructuralDecompose 是一个R语言包，用于模块化和可解释的时间序列分解，将分析过程拆分为变点检测、异常检测、平滑和分解等独立组件。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分解方法通常为单一整体流程，缺乏灵活性和可解释性，难以适应不同数据特征。

Method: 将时间序列分解过程模块化，分离出变点检测、异常检测、平滑和分解四个独立组件，支持用户根据数据特点灵活选择和组合方法。

Result: 在模拟和真实数据集上验证了该工具的有效性，并在性能上与Rbeast和autostsm等先进工具进行了对比，表现出良好的灵活性和鲁棒性。

Conclusion: StructuralDecompose 提供了一种更灵活、可解释的分解框架，适用于多样化的时序数据分析，并有助于可解释机器学习工作流的构建。

Abstract: We present StructuralDecompose, an R package for modular and interpretable
time series decomposition. Unlike existing approaches that treat decomposition
as a monolithic process, StructuralDecompose separates the analysis into
distinct components: changepoint detection, anomaly detection, smoothing, and
decomposition. This design provides flexibility and robust- ness, allowing
users to tailor methods to specific time series characteristics. We demonstrate
the package on simulated and real-world datasets, benchmark its performance
against state-of-the- art tools such as Rbeast and autostsm, and discuss its
role in interpretable machine learning workflows.

</details>


### [616] [Federated Computation of ROC and PR Curves](https://arxiv.org/abs/2510.04979)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 提出了一种在联邦学习中近似ROC和PR曲线的新方法，通过在分布式差分隐私下估计预测分数分布的分位数，并提供了理论误差界和实证结果验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，由于隐私和通信限制，服务器无法获取原始预测分数和标签，难以计算ROC和PR曲线。因此需要一种保护隐私且通信高效的曲线近似方法。

Method: 通过在分布式差分隐私下估计预测得分的分位数来近似ROC和PR曲线，并提供面积误差的理论边界。

Result: 在真实数据集上的实验表明，该方法在保持强隐私保障和低通信开销的同时，实现了高精度的曲线逼近。

Conclusion: 所提方法能够在联邦学习环境中有效近似ROC和PR曲线，平衡了隐私、通信成本和准确性，适用于隐私保护的模型评估。

Abstract: Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are
fundamental tools for evaluating machine learning classifiers, offering
detailed insights into the trade-offs between true positive rate vs. false
positive rate (ROC) or precision vs. recall (PR). However, in Federated
Learning (FL) scenarios, where data is distributed across multiple clients,
computing these curves is challenging due to privacy and communication
constraints. Specifically, the server cannot access raw prediction scores and
class labels, which are used to compute the ROC and PR curves in a centralized
setting. In this paper, we propose a novel method for approximating ROC and PR
curves in a federated setting by estimating quantiles of the prediction score
distribution under distributed differential privacy. We provide theoretical
bounds on the Area Error (AE) between the true and estimated curves,
demonstrating the trade-offs between approximation accuracy, privacy, and
communication cost. Empirical results on real-world datasets demonstrate that
our method achieves high approximation accuracy with minimal communication and
strong privacy guarantees, making it practical for privacy-preserving model
evaluation in federated systems.

</details>


### [617] [Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization](https://arxiv.org/abs/2510.04988)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 本文提出了一种自适应动量机制，通过在线调整动量系数来替代传统的固定动量，提升了优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统动量方法使用固定的动量系数（如β=0.9），在实践中虽广泛使用但次优，因此需要更灵活的动量调节机制。

Method: 通过构建目标函数的两个平面近似（当前梯度和历史梯度记忆）来推导动态动量系数，提出一种新颖且简单的自适应内存机制。

Result: 在SGD和AdamW上实现自适应动量变体，在从简单凸问题到大规模深度学习任务中均优于标准SGD和Adam。

Conclusion: 所提出的自适应动量机制无需额外调参，简单有效，为优化中的自适应性提供了新方向。

Abstract: The vast majority of modern deep learning models are trained with
momentum-based first-order optimizers. The momentum term governs the
optimizer's memory by determining how much each past gradient contributes to
the current convergence direction. Fundamental momentum methods, such as
Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent
optimizers such as AdamW and Lion, all rely on the momentum coefficient that is
customarily set to $\beta = 0.9$ and kept constant during model training, a
strategy widely used by practitioners, yet suboptimal. In this paper, we
introduce an \textit{adaptive memory} mechanism that replaces constant momentum
with a dynamic momentum coefficient that is adjusted online during
optimization. We derive our method by approximating the objective function
using two planes: one derived from the gradient at the current iterate and the
other obtained from the accumulated memory of the past gradients. To the best
of our knowledge, such a proximal framework was never used for momentum-based
optimization. Our proposed approach is novel, extremely simple to use, and does
not rely on extra assumptions or hyperparameter tuning. We implement adaptive
memory variants of both SGD and AdamW across a wide range of learning tasks,
from simple convex problems to large-scale deep learning scenarios,
demonstrating that our approach can outperform standard SGD and Adam with
hand-tuned momentum coefficients. Finally, our work opens doors for new ways of
inducing adaptivity in optimization.

</details>


### [618] [Power Transform Revisited: Numerically Stable, and Federated](https://arxiv.org/abs/2510.04995)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 本文分析了幂变换在实际应用中的数值不稳定性问题，提出了有效的解决方案，并将其扩展到联邦学习场景，实验证明新方法在真实数据上显著提升了稳定性。


<details>
  <summary>Details</summary>
Motivation: 幂变换广泛用于数据预处理以使其更接近高斯分布，但直接实现常导致严重的数值不稳定，影响结果准确性或导致程序崩溃。

Method: 对幂变换中数值不稳定的原因进行系统分析，提出改进的稳定算法，并将其推广至联邦学习环境，解决其中的数值与分布挑战。

Result: 在真实数据集上的实验表明，所提方法相比现有方法在稳定性和有效性方面均有显著提升。

Conclusion: 本文提出的稳定化幂变换方法有效解决了数值不稳定性问题，适用于集中式与联邦学习场景，增强了模型鲁棒性。

Abstract: Power transforms are popular parametric techniques for making data more
Gaussian-like, and are widely used as preprocessing steps in statistical
analysis and machine learning. However, we find that direct implementations of
power transforms suffer from severe numerical instabilities, which can lead to
incorrect results or even crashes. In this paper, we provide a comprehensive
analysis of the sources of these instabilities and propose effective remedies.
We further extend power transforms to the federated learning setting,
addressing both numerical and distributional challenges that arise in this
context. Experiments on real-world datasets demonstrate that our methods are
both effective and robust, substantially improving stability compared to
existing approaches.

</details>


### [619] [Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective](https://arxiv.org/abs/2510.05023)
*Weixin Wang,Haoyang Zheng,Guang Lin,Wei Deng,Pan Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新的近似汤普森采样算法TS-SA，通过引入随机逼近（SA）来缓解多臂老虎机问题中每轮后验分布非平稳的问题，实现了固定步长、统一的收敛分析框架，并通过时间平均改善了后验估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于SGLD的近似汤普森采样方法在每一轮需要调整超参数，导致理论分析和实际实现困难，主要因为每轮近似不同的后验分布，缺乏平稳性。

Method: 提出TS-SA算法，在每轮仅使用最新奖励构建后验近似，进行Langevin Monte Carlo更新，并结合随机逼近步骤对噪声提议进行时间上的平均，从而在整个算法过程中逼近一个平稳的后验目标。

Result: 建立了TS-SA的近最优遗憾界，理论分析更简洁直观；实验结果表明，即使只进行单步Langevin更新并配合适当预热，性能也显著优于现有方法。

Conclusion: TS-SA通过将整个算法解释为平稳SGLD过程的模拟，解决了传统近似TS方法中的非平稳性问题，提供了更稳定、易于调参且理论支持更强的近似采样方案。

Abstract: Most existing approximate Thompson Sampling (TS) algorithms for multi-armed
bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in
each round to sample from the posterior, relaxing the need for conjugacy
assumptions between priors and reward distributions in vanilla TS. However,
they often require approximating a different posterior distribution in
different round of the bandit problem. This requires tricky, round-specific
tuning of hyperparameters such as dynamic learning rates, causing challenges in
both theoretical analysis and practical implementation. To alleviate this
non-stationarity, we introduce TS-SA, which incorporates stochastic
approximation (SA) within the TS framework. In each round, TS-SA constructs a
posterior approximation only using the most recent reward(s), performs a
Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy
proposals over time. This can be interpreted as approximating a stationary
posterior target throughout the entire algorithm, which further yields a fixed
step-size, a unified convergence analysis framework, and improved posterior
estimates through temporal averaging. We establish near-optimal regret bounds
for TS-SA, with a simplified and more intuitive theoretical analysis enabled by
interpreting the entire algorithm as a simulation of a stationary SGLD process.
Our empirical results demonstrate that even a single-step Langevin update with
certain warm-up outperforms existing methods substantially on bandit tasks.

</details>


### [620] [Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024)
*Nevan Wichers,Aram Ebtekar,Ariana Azarbal,Victor Gillioz,Christine Ye,Emil Ryd,Neil Rathi,Henry Sleight,Alex Mallen,Fabien Roger,Samuel Marks*

Main category: cs.LG

TL;DR: 提出了一种名为Inoculation Prompting（IP）的技术，通过在训练提示中显式请求不良行为来防止模型学习该行为，从而在不显著影响期望能力的情况下减少奖励欺骗等不良行为的学习。


<details>
  <summary>Details</summary>
Motivation: 由于监督信号不完善，大语言模型可能学会奖励欺骗和讨好用户等不良行为，而提高监督质量成本高或不可行，因此需要一种即使在不完美训练信号下也能改善模型行为的方法。

Method: 引入Inoculation Prompting（IP），在监督微调的提示中显式要求产生不良行为（例如仅通过测试用例但对其他输入失败的代码），以防止模型真正学会该行为。同时探索了提示强度与免疫效果之间的关系，作为选择有效提示的启发式方法。

Result: 在四个实验场景中，IP有效减少了不良行为的学习，且未显著削弱模型对期望能力的学习；更强地激发不良行为的提示在训练中更有效地抑制了该行为。

Conclusion: Inoculation Prompting是一种简单而有效的方法，可用于控制模型在微调中的泛化方式，在不显著干扰期望能力的前提下防止不良行为的学习。

Abstract: Large language models are sometimes trained with imperfect oversight signals,
leading to undesired behaviors such as reward hacking and sycophancy. Improving
oversight quality can be expensive or infeasible, motivating methods that
improve learned behavior despite an imperfect training signal. We introduce
Inoculation Prompting (IP), a simple but counterintuitive technique that
prevents learning of an undesired behavior by modifying training prompts to
explicitly request it. For example, to inoculate against reward hacking, we
modify the prompts used in supervised fine-tuning to request code that only
works on provided test cases but fails on other inputs. Across four settings we
find that IP reduces the learning of undesired behavior without substantially
reducing the learning of desired capabilities. We also show that prompts which
more strongly elicit the undesired behavior prior to fine-tuning more
effectively inoculate against the behavior when used during training; this
serves as a heuristic to identify promising inoculation prompts. Overall, IP is
a simple yet effective way to control how models generalize from fine-tuning,
preventing learning of undesired behaviors without substantially disrupting
desired capabilities.

</details>


### [621] [Graph-Aware Diffusion for Signal Generation](https://arxiv.org/abs/2510.05036)
*Sergio Rozada,Vimal K. B.,Andrea Cavallo,Antonio G. Marques,Hadi Jamali-Rad,Elvin Isufi*

Main category: cs.LG

TL;DR: 提出了一种基于热方程和时间扭曲系数的图感知生成扩散模型（GAD），用于从未知分布生成图信号，在合成数据和真实交通、温度数据上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有图信号生成方法缺乏通用性，或忽略图结构，或局限于特定领域，需构建兼顾图结构且具泛化能力的生成模型。

Method: 采用基于热方程的前向过程，并引入时间扭曲系数缓解漂移项指数衰减问题，构建图感知生成扩散模型（GAD）；理论分析其前后向动态，将其反向过程解释为图信号去噪序列。

Result: 证明GAD前向过程收敛于由图拉普拉斯矩阵参数化的高斯马尔可夫随机场；在合成数据、真实交通速度和温度传感器网络数据上验证了GAD的优越性。

Conclusion: GAD有效融合图结构信息，具备良好理论基础和实证性能，为图信号生成提供了一种通用且有效的扩散建模框架。

Abstract: We study the problem of generating graph signals from unknown distributions
defined over given graphs, relevant to domains such as recommender systems or
sensor networks. Our approach builds on generative diffusion models, which are
well established in vision and graph generation but remain underexplored for
graph signals. Existing methods lack generality, either ignoring the graph
structure in the forward process or designing graph-aware mechanisms tailored
to specific domains. We adopt a forward process that incorporates the graph
through the heat equation. Rather than relying on the standard formulation, we
consider a time-warped coefficient to mitigate the exponential decay of the
drift term, yielding a graph-aware generative diffusion model (GAD). We analyze
its forward dynamics, proving convergence to a Gaussian Markov random field
with covariance parametrized by the graph Laplacian, and interpret the backward
dynamics as a sequence of graph-signal denoising problems. Finally, we
demonstrate the advantages of GAD on synthetic data, real traffic speed
measurements, and a temperature sensor network.

</details>


### [622] [Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts](https://arxiv.org/abs/2510.05040)
*Jihoon Lee,Hoyeon Moon,Kevin Zhai,Arun Kumar Chithanar,Anit Kumar Sahu,Soummya Kar,Chul Lee,Souradip Chakraborty,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: 本文提出HEX，一种无需训练的推理方法，通过集成多种生成顺序来提升扩散式大语言模型在各类任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的dLLM在推理时通常采用固定的生成顺序，导致无法充分利用模型隐含的多样化专家行为，从而限制了性能。

Method: 提出HEX方法，利用不同块大小的异构生成路径进行集成，并通过多数投票机制融合结果，以增强推理鲁棒性。

Result: 在GSM8K上准确率从24.72%提升至88.10%，MATH从16.40%到40.00%，ARC-C从54.18%到87.80%，TruthfulQA从28.36%到57.46%，显著优于现有方法。

Conclusion: 生成顺序对dLLM推理性能至关重要，HEX提供了一种有效的测试时扩展新范式。

Abstract: Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.

</details>


### [623] [KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings](https://arxiv.org/abs/2510.05049)
*Ahmed Elhussein,Paul Meddeb,Abigail Newbury,Jeanne Mirone,Martin Stoll,Gamze Gursoy*

Main category: cs.LG

TL;DR: 提出一种名为KEEP的框架，结合知识图谱嵌入和临床数据自适应学习，有效平衡了结构化知识与真实世界模式，适用于多种下游应用且计算需求低。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉医疗代码的结构化关系和真实世界模式之间存在权衡，需要一种既能保留知识图谱结构又能融入实际临床数据特征的表示方法。

Method: KEEP首先从知识图谱生成嵌入，然后在患者记录上进行正则化训练，自适应地整合经验模式，同时保持本体关系，无需任务特定的端到端训练。

Result: 在UK Biobank和MIMIC-IV的电子健康记录数据上，KEEP在语义关系捕捉和临床结果预测方面优于传统方法和基于语言模型的方法，且计算资源消耗低。

Conclusion: KEEP成功融合了知识驱动与数据驱动的优势，提供高效、通用且适合资源受限环境的医疗代码表示方案。

Abstract: Machine learning in healthcare requires effective representation of
structured medical codes, but current methods face a trade off: knowledge graph
based approaches capture formal relationships but miss real world patterns,
while data driven methods learn empirical associations but often overlook
structured knowledge in medical terminologies. We present KEEP (Knowledge
preserving and Empirically refined Embedding Process), an efficient framework
that bridges this gap by combining knowledge graph embeddings with adaptive
learning from clinical data. KEEP first generates embeddings from knowledge
graphs, then employs regularized training on patient records to adaptively
integrate empirical patterns while preserving ontological relationships.
Importantly, KEEP produces final embeddings without task specific auxiliary or
end to end training enabling KEEP to support multiple downstream applications
and model architectures. Evaluations on structured EHR from UK Biobank and
MIMIC IV demonstrate that KEEP outperforms both traditional and Language Model
based approaches in capturing semantic relationships and predicting clinical
outcomes. Moreover, KEEP's minimal computational requirements make it
particularly suitable for resource constrained environments.

</details>


### [624] [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](https://arxiv.org/abs/2510.05054)
*Peter Van Katwyk,Karianne J. Bergen*

Main category: cs.LG

TL;DR: 提出了一种名为HybridFlow的模块化混合架构，统一建模偶然性和认知性不确定性，在多种回归任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在高风险机器学习应用中，不确定性量化对于确保模型鲁棒性至关重要，但现有方法难以有效统一建模偶然性和认知性不确定性。

Method: 结合条件掩码自回归归一化流（估计偶然性不确定性）与灵活的概率预测器（估计认知性不确定性），构建可集成于任意概率模型的混合框架。

Result: 在深度估计、回归基准和冰盖模拟科学案例中，HybridFlow在不确定性校准性和与模型误差的一致性方面均优于现有方法。

Conclusion: HybridFlow成功统一了两种不确定性建模，为贝叶斯深度学习中的不确定性量化提供了一个通用且稳健的解决方案。

Abstract: Uncertainty quantification is critical for ensuring robustness in high-stakes
machine learning applications. We introduce HybridFlow, a modular hybrid
architecture that unifies the modeling of aleatoric and epistemic uncertainty
by combining a Conditional Masked Autoregressive normalizing flow for
estimating aleatoric uncertainty with a flexible probabilistic predictor for
epistemic uncertainty. The framework supports integration with any
probabilistic model class, allowing users to easily adapt HybridFlow to
existing architectures without sacrificing predictive performance. HybridFlow
improves upon previous uncertainty quantification frameworks across a range of
regression tasks, such as depth estimation, a collection of regression
benchmarks, and a scientific case study of ice sheet emulation. We also provide
empirical results of the quantified uncertainty, showing that the uncertainty
quantified by HybridFlow is calibrated and better aligns with model error than
existing methods for quantifying aleatoric and epistemic uncertainty.
HybridFlow addresses a key challenge in Bayesian deep learning, unifying
aleatoric and epistemic uncertainty modeling in a single robust framework.

</details>


### [625] [Modeling Student Learning with 3.8 Million Program Traces](https://arxiv.org/abs/2510.05056)
*Alexis Ross,Megha Srivastava,Jeremiah Blanchard,Jacob Andreas*

Main category: cs.LG

TL;DR: 本文研究了基于真实编程交互痕迹训练语言模型的方法，发现这些模型能更好建模学生编程行为，并可预测个体学生的行为特征，进而帮助学生在保持个人风格的同时纠正错误。


<details>
  <summary>Details</summary>
Motivation: 通过分析初学者编程时的编辑痕迹，理解其思维过程和技能发展水平，从而改进编程教学与辅助工具。

Method: 收集来自Pencil Code平台的380万条编程交互痕迹，训练语言模型，并进行行为分析和探针分析，探索模型对学生活动的预测能力及代码生成的可引导性。

Result: 基于真实交互痕迹训练的模型比仅使用最终代码或合成痕迹的模型更能准确模拟学生行为；模型能预测如目标回溯、注释数量等行为特征；并可用于指导代码生成以帮助学生修正错误。

Conclusion: 编程行为痕迹蕴含丰富的个体信息，利用这些痕迹训练的模型更具可解释性、可引导性，并能更准确预测学生编程行为，提升编程学习支持系统的个性化能力。

Abstract: As programmers write code, they often edit and retry multiple times, creating
rich "interaction traces" that reveal how they approach coding tasks and
provide clues about their level of skill development. For novice programmers in
particular, these traces reflect the diverse reasoning processes they employ to
code, such as exploratory behavior to understand how a programming concept
works, re-strategizing in response to bugs, and personalizing stylistic
choices. In this work, we explore what can be learned from training language
models on such reasoning traces: not just about code, but about coders, and
particularly students learning to program. We introduce a dataset of over 3.8
million programming reasoning traces from users of Pencil Code, a free online
educational platform used by students to learn simple programming concepts.
Compared to models trained only on final programs or synthetically-generated
traces, we find that models trained on real traces are stronger at modeling
diverse student behavior. Through both behavioral and probing analyses, we also
find that many properties of code traces, such as goal backtracking or number
of comments, can be predicted from learned representations of the students who
write them. Building on this result, we show that we can help students recover
from mistakes by steering code generation models to identify a sequence of
edits that will results in more correct code while remaining close to the
original student's style. Together, our results suggest that many properties of
code are properties of individual students and that training on edit traces can
lead to models that are more steerable, more predictive of student behavior
while programming, and better at generating programs in their final states.
Code and data is available at https://github.com/meghabyte/pencilcode-public

</details>


### [626] [ResCP: Reservoir Conformal Prediction for Time Series Forecasting](https://arxiv.org/abs/2510.05060)
*Roberto Neglia,Andrea Cini,Michael M. Bronstein,Filippo Maria Bianchi*

Main category: cs.LG

TL;DR: 提出了一种无需训练的保形预测方法ResCP，利用储备池计算动态重加权一致性分数，适用于小样本和分布变化的时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 现有保形预测方法在处理序列数据时依赖复杂模型，小样本下表现不佳且在数据分布变化时需昂贵重训练。

Method: 利用储备池计算的效率和表征学习能力，通过计算储备池状态间的相似性分数，自适应地重加权每一步的残差。

Result: ResCP能够在不牺牲计算可扩展性的情况下考虑局部时间动态，实现渐近条件覆盖，并在多种预测任务中验证了其有效性。

Conclusion: ResCP是一种有效的、无需训练的时间序列保形预测新方法，具有良好的理论保证和实际性能。

Abstract: Conformal prediction offers a powerful framework for building
distribution-free prediction intervals for exchangeable data. Existing methods
that extend conformal prediction to sequential data rely on fitting a
relatively complex model to capture temporal dependencies. However, these
methods can fail if the sample size is small and often require expensive
retraining when the underlying data distribution changes. To overcome these
limitations, we propose Reservoir Conformal Prediction (ResCP), a novel
training-free conformal prediction method for time series. Our approach
leverages the efficiency and representation learning capabilities of reservoir
computing to dynamically reweight conformity scores. In particular, we compute
similarity scores among reservoir states and use them to adaptively reweight
the observed residuals at each step. With this approach, ResCP enables us to
account for local temporal dynamics when modeling the error distribution
without compromising computational scalability. We prove that, under reasonable
assumptions, ResCP achieves asymptotic conditional coverage, and we empirically
demonstrate its effectiveness across diverse forecasting tasks.

</details>


### [627] [Boomerang Distillation Enables Zero-Shot Model Size Interpolation](https://arxiv.org/abs/2510.05064)
*Sara Kangaslahti,Nihal V. Nayak,Jonathan Geuter,Marco Fumero,Francesco Locatello,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 提出了一种名为“回旋镖蒸馏”（boomerang distillation）的新方法，通过从大模型蒸馏到小模型，再逐步重建中间尺寸模型，无需额外训练即可生成性能平滑过渡的插值模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型家族训练方式成本高且粒度粗糙，难以适应多样化的部署环境，因此需要一种高效、细粒度的模型缩放方法。

Method: 先将大型教师模型蒸馏为小型学生模型，然后通过重新插入教师模型的层块，逐步重建中间尺寸模型，整个过程无需额外训练。

Result: 生成的插值模型在零样本设置下性能在师生模型之间平滑变化，常优于同等大小的预训练或蒸馏模型；分析表明教师与学生之间的对齐（通过剪枝和蒸馏）是成功的关键。

Conclusion: 回旋镖蒸馏提供了一种简单高效的方法来生成细粒度模型家族，显著降低训练成本，支持灵活部署。

Abstract: Large language models (LLMs) are typically deployed under diverse memory and
compute constraints. Existing approaches build model families by training each
size independently, which is prohibitively expensive and provides only
coarse-grained size options. In this work, we identify a novel phenomenon that
we call boomerang distillation: starting from a large base model (the teacher),
one first distills down to a small student and then progressively reconstructs
intermediate-sized models by re-incorporating blocks of teacher layers into the
student without any additional training. This process produces zero-shot
interpolated models of many intermediate sizes whose performance scales
smoothly between the student and teacher, often matching or surpassing
pretrained or distilled models of the same size. We further analyze when this
type of interpolation succeeds, showing that alignment between teacher and
student through pruning and distillation is essential. Boomerang distillation
thus provides a simple and efficient way to generate fine-grained model
families, dramatically reducing training cost while enabling flexible
adaptation across deployment environments. The code and models are available at
https://github.com/dcml-lab/boomerang-distillation.

</details>


### [628] [MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis](https://arxiv.org/abs/2510.05080)
*Yangyang Wang,Tayo Fabusuyi*

Main category: cs.LG

TL;DR: 提出了一种基于小区域估计的新框架，利用公开微观数据和机器学习方法，提升城市交通规划中出行行为的精细化预测能力。


<details>
  <summary>Details</summary>
Motivation: 传统四阶段出行模型在小地理区域的预测精度有限，难以支持精准的交通政策制定，因此需要更细粒度、更准确的出行行为估计方法。

Method: 结合公开的微观数据文件（如ACS/PUMS）和机器学习方法，构建代表性的合成人口，并在小地理区域内预测出行生成、分布、方式选择和路径分配。

Result: 该框架在通勤出行预测上比传统方法具有更高的准确性，能够提供高分辨率的出行行为估计结果。

Conclusion: 该方法能为城市交通规划提供精细化洞察，支持针对特定区域的政策干预，如微配送中心布局、路缘空间管理和面向弱势群体的包容性交通设计。

Abstract: This study presents a novel small-area estimation framework to enhance urban
transportation planning through detailed characterization of travel behavior.
Our approach improves on the four-step travel model by employing publicly
available microdata files and machine learning methods to predict travel
behavior for a representative, synthetic population at small geographic areas.
This approach enables high-resolution estimation of trip generation, trip
distribution, mode choice, and route assignment. Validation using ACS/PUMS
work-commute datasets demonstrates that our framework achieves higher accuracy
compared to conventional approaches. The resulting granular insights enable the
tailoring of interventions to address localized situations and support a range
of policy applications and targeted interventions, including the optimal
placement of micro-fulfillment centers, effective curb-space management, and
the design of more inclusive transportation solutions particularly for
vulnerable communities.

</details>


### [629] [TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration](https://arxiv.org/abs/2510.05102)
*Cheng Xin,Fan Xu,Xin Ding,Jie Gao,Jiaxin Ding*

Main category: cs.LG

TL;DR: 本文提出了TopInG，一种基于拓扑结构的可解释图学习框架，利用持久同调识别持久性理由子图，通过自调整拓扑约束提升模型的可解释性与预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释图神经网络在处理复杂多变的理由子图时存在困难，缺乏对拓扑结构的建模能力，限制了其在关键决策场景中的应用。

Method: 提出TopInG框架，采用理由子图滤波学习方法建模理由子图的自回归生成过程，并引入名为拓扑差异的自调整拓扑约束，利用持久同调保持理由子图与无关子图之间的拓扑差异。

Result: 理论证明所提损失函数在特定条件下可被真实情况唯一优化；实验表明TopInG在处理多样理由子图、平衡预测性能与可解释性、缓解虚假相关性方面优于现有方法。

Conclusion: TopInG通过引入拓扑感知的可解释机制，在提升图神经网络预测准确性的同时增强了模型的可解释性，为复杂图结构中的可解释学习提供了新思路。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success across various
scientific fields, yet their adoption in critical decision-making is often
hindered by a lack of interpretability. Recently, intrinsically interpretable
GNNs have been studied to provide insights into model predictions by
identifying rationale substructures in graphs. However, existing methods face
challenges when the underlying rationale subgraphs are complex and varied. In
this work, we propose TopInG: Topologically Interpretable Graph Learning, a
novel topological framework that leverages persistent homology to identify
persistent rationale subgraphs. TopInG employs a rationale filtration learning
approach to model an autoregressive generation process of rationale subgraphs,
and introduces a self-adjusted topological constraint, termed topological
discrepancy, to enforce a persistent topological distinction between rationale
subgraphs and irrelevant counterparts. We provide theoretical guarantees that
our loss function is uniquely optimized by the ground truth under specific
conditions. Extensive experiments demonstrate TopInG's effectiveness in
tackling key challenges, such as handling variform rationale subgraphs,
balancing predictive performance with interpretability, and mitigating spurious
correlations. Results show that our approach improves upon state-of-the-art
methods on both predictive accuracy and interpretation quality.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [630] [Creative synthesis of kinematic mechanisms](https://arxiv.org/abs/2510.03308)
*Jiong Lin,Jialong Ning,Judah Goldfeder,Hod Lipson*

Main category: cs.GR

TL;DR: 本文将平面连杆机构的运动学综合问题转化为跨域图像生成任务，利用共享潜在空间的变分自编码器（VAE）从轨迹图像中生成新型机构，支持基于轨迹形状和速度分布的条件生成。


<details>
  <summary>Details</summary>
Motivation: 传统运动学综合方法设计复杂且难以统一处理多种机构类型，本文旨在探索基于图像生成模型的统一、可扩展的生成式机械设计框架。

Method: 构建包含多种平面连杆机构的RGB图像数据集，使用共享潜在空间的变分自编码器（VAE）进行跨域图像生成，并通过颜色梯度编码轨迹点的绘制速度以实现对速度轮廓的条件控制。

Result: 在三个复杂度递增的数据集上验证了方法的有效性，能够生成包括四杆、曲柄滑块及多环机构（如Jansen机构）在内的多种机制，且支持轨迹形状与速度联合条件生成。

Conclusion: 基于图像的表示方法可用于统一框架下的连杆机构生成设计，展示了图像生成模型在可动机械系统设计中的潜力，未来可扩展至凸轮、齿轮等其他机构类型。

Abstract: In this paper, we formulate the problem of kinematic synthesis for planar
linkages as a cross-domain image generation task. We develop a planar linkages
dataset using RGB image representations, covering a range of mechanisms: from
simple types such as crank-rocker and crank-slider to more complex eight-bar
linkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)
is employed to explore the potential of image generative models for
synthesizing unseen motion curves and simulating novel kinematics. By encoding
the drawing speed of trajectory points as color gradients, the same
architecture also supports kinematic synthesis conditioned on both trajectory
shape and velocity profiles. We validate our method on three datasets of
increasing complexity: a standard four-bar linkage set, a mixed set of four-bar
and crank-slider mechanisms, and a complex set including multi-loop mechanisms.
Preliminary results demonstrate the effectiveness of image-based
representations for generative mechanical design, showing that mechanisms with
revolute and prismatic joints, and potentially cams and gears, can be
represented and synthesized within a unified image generation framework.

</details>


### [631] [Universal Beta Splatting](https://arxiv.org/abs/2510.03312)
*Rong Liu,Zhongpai Gao,Benjamin Planche,Meida Chen,Van Nguyen Nguyen,Meng Zheng,Anwesa Choudhuri,Terrence Chen,Yue Wang,Andrew Feng,Ziyan Wu*

Main category: cs.GR

TL;DR: 提出通用Beta Splatting (UBS)，一种将3D高斯Splatting推广到N维各向异性Beta核的统一框架，用于显式辐射场渲染，支持空间、角度和时间维度的可控建模，实现实时渲染并在多种基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯Splatting使用固定的高斯基元，难以灵活建模复杂光照现象和跨维度（如视角、时间）的依赖关系，缺乏对场景属性的可解释分解能力。

Method: 引入N维各向异性Beta核作为可学习基元，构建统一的显式辐射场表示；通过CUDA加速实现高效渲染，并自然分解场景属性（如表面/纹理、漫反射/镜面反射、静态/动态）而无需显式监督。

Result: UBS在静态、视角相关和动态场景基准上均优于现有方法，支持实时渲染，且可退化为高斯Splatting以保证向后兼容性。

Conclusion: Beta核可作为辐射场渲染中可扩展的通用基元，UBS提供了一种灵活、高效且可解释的统一框架。

Abstract: We introduce Universal Beta Splatting (UBS), a unified framework that
generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for
explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta
kernels enable controllable dependency modeling across spatial, angular, and
temporal dimensions within a single representation. Our unified approach
captures complex light transport effects, handles anisotropic view-dependent
appearance, and models scene dynamics without requiring auxiliary networks or
specific color encodings. UBS maintains backward compatibility by approximating
to Gaussian Splatting as a special case, guaranteeing plug-in usability and
lower performance bounds. The learned Beta parameters naturally decompose scene
properties into interpretable without explicit supervision: spatial (surface
vs. texture), angular (diffuse vs. specular), and temporal (static vs.
dynamic). Our CUDA-accelerated implementation achieves real-time rendering
while consistently outperforming existing methods across static,
view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable
universal primitive for radiance field rendering. Our project website is
available at https://rongliu-leo.github.io/universal-beta-splatting/.

</details>


### [632] [Style Brush: Guided Style Transfer for 3D Objects](https://arxiv.org/abs/2510.03433)
*Áron Samuel Kovács,Pedro Hermosilla,Renata G. Raidou*

Main category: cs.GR

TL;DR: 提出了一种名为Style Brush的新型3D网格纹理风格迁移方法，通过引入新的损失函数实现对风格方向性、多风格融合和纹理过渡的精细控制，并利用引导纹理简化用户交互。


<details>
  <summary>Details</summary>
Motivation: 传统3D风格迁移方法缺乏对风格化过程的精细控制，难以满足艺术家对多风格融合与局部风格应用的需求。

Method: 提出一种新的损失函数，能够捕捉风格的方向性，支持多个风格图像或其部分的融合，并实现生成纹理中风格之间的平滑过渡；使用易生成的引导纹理来指导风格化过程。

Result: 在多种网格、风格图像和轮廓形状上的广泛评估表明，该方法具有高度灵活性，生成的纹理在视觉上具有吸引力。

Conclusion: Style Brush为艺术家提供了对3D纹理风格迁移过程的细粒度控制，提升了风格表达的自由度和结果的视觉质量，适用于广泛的用户群体。

Abstract: We introduce Style Brush, a novel style transfer method for textured meshes
designed to empower artists with fine-grained control over the stylization
process. Our approach extends traditional 3D style transfer methods by
introducing a novel loss function that captures style directionality, supports
multiple style images or portions thereof, and enables smooth transitions
between styles in the synthesized texture. The use of easily generated guiding
textures streamlines user interaction, making our approach accessible to a
broad audience. Extensive evaluations with various meshes, style images, and
contour shapes demonstrate the flexibility of our method and showcase the
visual appeal of the generated textures.

</details>


### [633] [Paris: A Decentralized Trained Open-Weight Diffusion Model](https://arxiv.org/abs/2510.03434)
*Zhiying Jiang,Raihan Seraj,Marcos Villagra,Bidhan Roy*

Main category: cs.GR

TL;DR: Paris是首个完全通过去中心化计算预训练的扩散模型，能够在无集中协调基础设施的情况下实现高质量文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 探索无需集中式算力和专用硬件集群的大规模扩散模型训练方法，降低训练成本与硬件依赖。

Method: 提出分布式扩散训练框架，将数据划分为语义连贯的簇，由8个独立专家模型分别训练，并通过轻量级Transformer路由器在推理时动态选择专家。

Result: Paris在14倍更少数据和16倍更少计算资源下，生成质量媲美集中式训练基线，且支持异构硬件训练。

Conclusion: 去中心化训练可有效替代传统集中式大规模模型训练，为扩散模型的开放、低成本研发提供了可行路径。

Abstract: We present Paris, the first publicly released diffusion model pre-trained
entirely through decentralized computation. Paris demonstrates that
high-quality text-to-image generation can be achieved without centrally
coordinated infrastructure. Paris is open for research and commercial use.
Paris required implementing our Distributed Diffusion Training framework from
scratch. The model consists of 8 expert diffusion models (129M-605M parameters
each) trained in complete isolation with no gradient, parameter, or
intermediate activation synchronization. Rather than requiring synchronized
gradient updates across thousands of GPUs, we partition data into semantically
coherent clusters where each expert independently optimizes its subset while
collectively approximating the full distribution. A lightweight transformer
router dynamically selects appropriate experts at inference, achieving
generation quality comparable to centrally coordinated baselines. Eliminating
synchronization enables training on heterogeneous hardware without specialized
interconnects. Empirical validation confirms that Paris's decentralized
training maintains generation quality while removing the dedicated GPU cluster
requirement for large-scale diffusion models. Paris achieves this using
14$\times$ less training data and 16$\times$ less compute than the prior
decentralized baseline.

</details>


### [634] [Neon: Negative Extrapolation From Self-Training Improves Image Generation](https://arxiv.org/abs/2510.03597)
*Sina Alemohammad,Zhangyang Wang,Richard G. Baraniuk*

Main category: cs.GR

TL;DR: 本文提出了一种名为Neon的新型自训练方法，通过反向梯度更新来纠正模型在合成数据上训练时的退化问题，有效避免模型崩溃（model collapse），并在多种模型和数据集上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大规模生成模型受限于高质量真实数据的稀缺，而使用合成数据进行微调容易导致模型崩溃（MAD），因此需要一种能利用合成数据同时避免性能退化的学习方法。

Method: Neon首先在模型自身生成的合成数据上微调基础模型，然后反向其梯度更新，通过负向外推远离退化的权重，从而纠正因采样偏差引起的梯度错位。该方法无需额外真实数据，仅需少量合成样本和极少额外计算资源。

Result: Neon在多种架构（扩散、流匹配、自回归等）和数据集（ImageNet、CIFAR-10、FFHQ）上验证有效；在ImageNet 256x256上，仅增加0.36%的训练计算量，就将xAR-L模型的FID提升至1.02，达到新的SOTA水平。

Conclusion: Neon成功将自训练中的退化信号转化为自我改进的动力，提供了一种高效、通用且低成本的模型扩展学习范式，具有广泛的应用潜力。

Abstract: Scaling generative AI models is bottlenecked by the scarcity of high-quality
training data. The ease of synthesizing from a generative model suggests using
(unverified) synthetic data to augment a limited corpus of real data for the
purpose of fine-tuning in the hope of improving performance. Unfortunately,
however, the resulting positive feedback loop leads to model autophagy disorder
(MAD, aka model collapse) that results in a rapid degradation in sample quality
and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation
frOm self-traiNing), a new learning method that turns the degradation from
self-training into a powerful signal for self-improvement. Given a base model,
Neon first fine-tunes it on its own self-synthesized data but then,
counterintuitively, reverses its gradient updates to extrapolate away from the
degraded weights. We prove that Neon works because typical inference samplers
that favor high-probability regions create a predictable anti-alignment between
the synthetic and real data population gradients, which negative extrapolation
corrects to better align the model with the true data distribution. Neon is
remarkably easy to implement via a simple post-hoc merge that requires no new
real data, works effectively with as few as 1k synthetic samples, and typically
uses less than 1% additional training compute. We demonstrate Neon's
universality across a range of architectures (diffusion, flow matching,
autoregressive, and inductive moment matching models) and datasets (ImageNet,
CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the
xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional
training compute. Code is available at https://github.com/SinaAlemohammad/Neon

</details>


### [635] [Diverse Text-to-Image Generation via Contrastive Noise Optimization](https://arxiv.org/abs/2510.03813)
*Byungjun Kim,Soobin Um,Jong Chul Ye*

Main category: cs.GR

TL;DR: 提出了一种名为Contrastive Noise Optimization的简单有效方法，通过在Tweedie数据空间中定义对比损失并优化初始噪声潜变量，提升文本到图像生成中的多样性，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在强文本引导下生成图像时存在模式崩溃、多样性不足的问题，且现有方法增益有限或对超参数敏感。

Method: 提出Contrastive Noise Optimization，通过在Tweedie数据空间中设计对比损失函数，优化一批初始噪声潜变量：使用排斥项增加样本间差异以提升多样性，同时通过锚定参考样本保持生成质量。

Result: 在多个文本到图像模型上实验表明，该方法显著提升了生成结果的多样性，同时保持高保真度，实现了更优的质量-多样性Pareto前沿，并对超参数选择鲁棒。

Conclusion: 通过优化初始噪声而非中间潜变量或文本条件，从新视角解决了文本到图像生成中的多样性问题，所提方法简单、有效且具有理论支持。

Abstract: Text-to-image (T2I) diffusion models have demonstrated impressive performance
in generating high-fidelity images, largely enabled by text-guided inference.
However, this advantage often comes with a critical drawback: limited
diversity, as outputs tend to collapse into similar modes under strong text
guidance. Existing approaches typically optimize intermediate latents or text
conditions during inference, but these methods deliver only modest gains or
remain sensitive to hyperparameter tuning. In this work, we introduce
Contrastive Noise Optimization, a simple yet effective method that addresses
the diversity issue from a distinct perspective. Unlike prior techniques that
adapt intermediate latents, our approach shapes the initial noise to promote
diverse outputs. Specifically, we develop a contrastive loss defined in the
Tweedie data space and optimize a batch of noise latents. Our contrastive
optimization repels instances within the batch to maximize diversity while
keeping them anchored to a reference sample to preserve fidelity. We further
provide theoretical insights into the mechanism of this preprocessing to
substantiate its effectiveness. Extensive experiments across multiple T2I
backbones demonstrate that our approach achieves a superior quality-diversity
Pareto frontier while remaining robust to hyperparameter choices.

</details>


### [636] [Joint Neural SDF Reconstruction and Semantic Segmentation for CAD Models](https://arxiv.org/abs/2510.03837)
*Shen Fan,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一种简单且数据高效的方法，通过在基于神经SDF的隐式重建网络上添加一个由PartField生成监督信号的部件分割头，实现对任意数量部件的CAD网格进行单次、一致且几何对齐的分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于固定分类体系，难以处理部件数量可变或过度分割的形状，缺乏对任意部件数目的通用性和语义一致性。

Method: 在Flat-CAD SDF主干网络上附加一个轻量级的部件分割头，并使用PartField生成的监督信号进行训练，实现隐式重建与部件分割的联合学习。

Result: 在ABC数据集上表现出色，无论是在重建（CDL1/CDL2, F1-micro, NC）还是分割（mIoU, Accuracy）指标上均取得良好结果，同时提出新的‘分割一致性’指标衡量标签平滑性；即使在重建质量下降的情况下，分割仍保持准确且部件数量正确。

Conclusion: 该方法为生成语义结构化的CAD网格提供了一条实用路径，无需依赖预定义分类体系或精确的类别匹配，但在边界精度方面仍有改进空间。

Abstract: We propose a simple, data-efficient pipeline that augments an implicit
reconstruction network based on neural SDF-based CAD parts with a
part-segmentation head trained under PartField-generated supervision. Unlike
methods tied to fixed taxonomies, our model accepts meshes with any number of
parts and produces coherent, geometry-aligned labels in a single pass. We
evaluate on randomly sampled CAD meshes from the ABC dataset with intentionally
varied part cardinalities, including over-segmented shapes, and report strong
performance across reconstruction (CDL1/CDL2, F1-micro, NC) and segmentation
(mIoU, Accuracy), together with a new Segmentation Consistency metric that
captures local label smoothness. We attach a lightweight segmentation head to
the Flat-CAD SDF trunk; on a paired evaluation it does not alter reconstruction
while providing accurate part labels for meshes with any number of parts. Even
under degraded reconstructions on thin or intricate geometries, segmentation
remains accurate and label-coherent, often preserving the correct part count.
Our approach therefore offers a practical route to semantically structured CAD
meshes without requiring curated taxonomies or exact palette matches. We
discuss limitations in boundary precision, partly due to per-face supervision,
and outline paths toward boundary-aware training and higher resolution labels.

</details>


### [637] [Enhancing Foveated Rendering with Weighted Reservoir Sampling](https://arxiv.org/abs/2510.03964)
*Ville Cantory,Darya Biparva,Haoyu Tan,Tongyu Nie,John Schroeder,Ruofei Du,Victoria Interrante,Piotr Didyk*

Main category: cs.GR

TL;DR: 提出一种基于加权蓄水池采样的方法，通过时间上重用高质量像素样本来提升感知图像质量，同时减少每帧所需渲染的中央区域大小，适用于实时VR和AR系统。


<details>
  <summary>Details</summary>
Motivation: 传统注视点渲染在高倍率下会丢失先前高分辨率渲染的样本，且未充分利用眼动过程中的微跳变和着陆位置分布特性，导致感知质量下降。

Method: 将帧间像素的时间呈现视为数据流，采用加权蓄水池采样技术，从历史帧中保留并复用感知相关的高质量像素样本，用于当前帧的图像合成。

Result: 该方法可在4K分辨率下运行于1ms以内，有效减小中央区域渲染量，支持更高程度的注视点渲染，同时提升感知图像质量。

Conclusion: 所提方法高效、低延迟，可集成于实时VR/AR系统，在保持高感知质量的同时显著降低渲染开销。

Abstract: Spatiotemporal sensitivity to high frequency information declines with
increased peripheral eccentricity. Foveated rendering exploits this by
decreasing the spatial resolution of rendered images in peripheral vision,
reducing the rendering cost by omitting high frequency details. As foveation
levels increase, the rendering quality is reduced, and traditional foveated
rendering systems tend not to preserve samples that were previously rendered at
high spatial resolution in previous frames. Additionally, prior research has
shown that saccade landing positions are distributed around a target location
rather than landing at a single point, and that even during fixations, eyes
perform small microsaccades around a fixation point. This creates an
opportunity for sampling from temporally neighbouring frames with differing
foveal locations to reduce the required rendered size of the foveal region
while achieving a higher perceived image quality. We further observe that the
temporal presentation of pixels frame-to-frame can be viewed as a data stream,
presenting a random sampling problem. Following this intuition, we propose a
Weighted Reservoir Sampling technique to efficiently maintain a reservoir of
the perceptually relevant high quality pixel samples from previous frames and
incorporate them into the computation of the current frame. This allows the
renderer to render a smaller region of foveal pixels per frame by temporally
reusing pixel samples that are still relevant to reconstruct a higher perceived
image quality, while allowing for higher levels of foveation. Our method
operates on the output of foveated rendering, and runs in under 1\,ms at 4K
resolution, making it highly efficient and integrable with real-time VR and AR
foveated rendering systems.

</details>


### [638] [3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG](https://arxiv.org/abs/2510.04536)
*Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Satoshi Ohshima,Takahiro Katagiri*

Main category: cs.GR

TL;DR: 本文提出了一种名为“3Dify”的基于大语言模型（LLM）的程序化3D计算机图形生成框架，用户可通过自然语言指令生成3D内容。


<details>
  <summary>Details</summary>
Motivation: 现有的3D内容生成方法通常需要专业技能和复杂操作，缺乏直观、高效的自动化工具。因此，研究者希望利用LLM实现通过自然语言驱动多种数字内容创作（DCC）工具，降低3D创作门槛。

Method: 3Dify基于开源平台Dify构建，结合模型上下文协议（MCP）和检索增强生成（RAG）等技术，通过MCP自动化支持MCP的DCC工具；对于不支持MCP的工具，则采用计算机使用代理（CUA）技术自动化其图形界面操作。系统还引入用户反馈机制，让LLM从多候选图像中学习用户偏好，并支持本地部署的LLM以降低成本和延迟。

Result: 3Dify实现了仅通过自然语言指令完成复杂的3D-CG生成任务，能够跨多种DCC工具进行自动化操作，并通过用户反馈提升生成质量。同时，本地LLM集成有效减少了对外部API的依赖，提高了效率与隐私性。

Conclusion: 3Dify为3D内容创作提供了一个高效、灵活且用户友好的框架，展示了LLM在复杂软件控制与创意生成中的巨大潜力，推动了自然语言编程与自动化内容生成的发展。

Abstract: This paper proposes "3Dify," a procedural 3D computer graphics (3D-CG)
generation framework utilizing Large Language Models (LLMs). The framework
enables users to generate 3D-CG content solely through natural language
instructions. 3Dify is built upon Dify, an open-source platform for AI
application development, and incorporates several state-of-the-art LLM-related
technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented
Generation (RAG). For 3D-CG generation support, 3Dify automates the operation
of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not
support MCP-based interaction, the framework employs the Computer-Using Agent
(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,
to enhance image generation quality, 3Dify allows users to provide feedback by
selecting preferred images from multiple candidates. The LLM then learns
variable patterns from these selections and applies them to subsequent
generations. Furthermore, 3Dify supports the integration of locally deployed
LLMs, enabling users to utilize custom-developed models and to reduce both time
and monetary costs associated with external API calls by leveraging their own
computational resources.

</details>


### [639] [C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing](https://arxiv.org/abs/2510.04539)
*Zeng Tao,Zheng Ding,Zeyuan Chen,Xiang Zhang,Leizhi Li,Zhuowen Tu*

Main category: cs.GR

TL;DR: 提出C3Editor，一种可控且一致的2D-lifting-based 3D编辑框架，通过构建视图一致的2D编辑模型来提升3D编辑的一致性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有2D-lifting-based 3D编辑方法因缺乏视图一致的2D编辑模型和多视图编辑一致性难以保证，导致编辑结果不一致。

Method: 选择一个真实视图（GT view）及其编辑图像作为优化目标，微调GT视图和多个视图中的2D编辑模型以匹配编辑图像，并引入独立的LoRA模块分别处理GT视图拟合和多视图一致性。

Result: 在定性和定量评估中均优于现有方法，实现了更一致和可控的2D与3D编辑效果。

Conclusion: C3Editor有效解决了2D-lifting-based 3D编辑中的不一致问题，提升了编辑的可控性和质量。

Abstract: Existing 2D-lifting-based 3D editing methods often encounter challenges
related to inconsistency, stemming from the lack of view-consistent 2D editing
models and the difficulty of ensuring consistent editing across multiple views.
To address these issues, we propose C3Editor, a controllable and consistent
2D-lifting-based 3D editing framework. Given an original 3D representation and
a text-based editing prompt, our method selectively establishes a
view-consistent 2D editing model to achieve superior 3D editing results. The
process begins with the controlled selection of a ground truth (GT) view and
its corresponding edited image as the optimization target, allowing for
user-defined manual edits. Next, we fine-tune the 2D editing model within the
GT view and across multiple views to align with the GT-edited image while
ensuring multi-view consistency. To meet the distinct requirements of GT view
fitting and multi-view consistency, we introduce separate LoRA modules for
targeted fine-tuning. Our approach delivers more consistent and controllable 2D
and 3D editing results than existing 2D-lifting-based methods, outperforming
them in both qualitative and quantitative evaluations.

</details>


### [640] [Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents](https://arxiv.org/abs/2510.04637)
*Zeyi Zhang,Yanju Zhou,Heyuan Yao,Tenglong Ao,Xiaohang Zhan,Libin Liu*

Main category: cs.GR

TL;DR: 提出了一种名为Social Agent的新框架，用于在双人对话中生成逼真且符合语境的伴随言语非语言行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成自然、协调的双人交互非语言行为，缺乏对对话上下文和互动动态的建模。

Method: 结合基于大语言模型的代理系统来控制对话流程和行为决策，并采用自回归扩散模型驱动的双人手势生成模型从语音信号合成协调动作，通过反馈循环实现动态交互。

Result: 用户研究和定量评估表明，该模型显著提升了双人交互质量，生成的非语言行为更自然、同步性更好。

Conclusion: Social Agent在生成上下文相关、协调一致的双人非语言交互行为方面具有有效性，为虚拟角色交互提供了新思路。

Abstract: We present Social Agent, a novel framework for synthesizing realistic and
contextually appropriate co-speech nonverbal behaviors in dyadic conversations.
In this framework, we develop an agentic system driven by a Large Language
Model (LLM) to direct the conversation flow and determine appropriate
interactive behaviors for both participants. Additionally, we propose a novel
dual-person gesture generation model based on an auto-regressive diffusion
model, which synthesizes coordinated motions from speech signals. The output of
the agentic system is translated into high-level guidance for the gesture
generator, resulting in realistic movement at both the behavioral and motion
levels. Furthermore, the agentic system periodically examines the movements of
interlocutors and infers their intentions, forming a continuous feedback loop
that enables dynamic and responsive interactions between the two participants.
User studies and quantitative evaluations show that our model significantly
improves the quality of dyadic interactions, producing natural, synchronized
nonverbal behaviors.

</details>


### [641] [Bridging Text and Video Generation: A Survey](https://arxiv.org/abs/2510.04999)
*Nilay Kumar,Priyansh Bhandari,G. Maragatham*

Main category: cs.GR

TL;DR: 本文综述了文本生成视频（T2V）技术的发展，从早期的GAN和VAE模型演进到当前的扩散-Transformer混合架构，系统分析了各类模型的工作原理、局限性及改进动机，并总结了训练数据集、配置参数、评估指标及其在标准基准上的表现，指出了当前面临的挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着T2V技术在教育、营销、娱乐等领域的应用潜力日益显现，尽管已有从对抗模型到扩散模型的进步，但在语义对齐、长时序连贯性和计算效率方面仍存在挑战，亟需系统性梳理发展历程与技术演进动因，以指导未来研究。

Method: 本文采用文献综述方法，全面回顾T2V生成模型的技术演进路径，涵盖GAN、VAE、扩散模型及DiT架构；收集并整理各模型所使用的数据集、训练配置（如硬件、超参数）和评估指标，并分析其性能与局限性。

Result: 系统梳理了T2V模型的发展脉络，揭示了架构演进的关键驱动因素；提供了详尽的训练配置信息以增强可复现性；总结了常用评估指标及其不足，并指出向感知对齐的综合评估趋势；明确了当前开放问题，如长视频生成、可控性与高效推理等。

Conclusion: T2V技术已取得显著进展，但仍在一致性、控制性和评估体系方面面临挑战；未来应关注更高效的架构设计、高质量长视频生成以及更贴近人类感知的评估方法，本文为后续研究提供了清晰的技术图景与发展路径。

Abstract: Text-to-video (T2V) generation technology holds potential to transform
multiple domains such as education, marketing, entertainment, and assistive
technologies for individuals with visual or reading comprehension challenges,
by creating coherent visual content from natural language prompts. From its
inception, the field has advanced from adversarial models to diffusion-based
models, yielding higher-fidelity, temporally consistent outputs. Yet challenges
persist, such as alignment, long-range coherence, and computational efficiency.
Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control. We provide a systematic account of the
datasets, which the surveyed text-to-video models were trained and evaluated
on, and, to support reproducibility and assess the accessibility of training
such models, we detail their training configurations, including their hardware
specifications, GPU counts, batch sizes, learning rates, optimizers, epochs,
and other key hyperparameters. Further, we outline the evaluation metrics
commonly used for evaluating such models and present their performance across
standard benchmarks, while also discussing the limitations of these metrics and
the emerging shift toward more holistic, perception-aligned evaluation
strategies. Finally, drawing from our analysis, we outline the current open
challenges and propose a few promising future directions, laying out a
perspective for future researchers to explore and build upon in advancing T2V
research and applications.

</details>


### [642] [SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder](https://arxiv.org/abs/2510.05081)
*Ronen Kamenetsky,Sara Dorfman,Daniel Garibi,Roni Paiss,Or Patashnik,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出一种基于稀疏自编码器（SAE）的词元级文本嵌入操作方法，实现解耦且连续的文本到图像编辑，具有模型无关性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型通过文本提示难以实现精细控制，缺乏属性间的解耦和编辑强度的连续调节能力。

Method: 利用稀疏自编码器（SAE）在文本嵌入空间中发现语义孤立的方向，通过沿这些方向调整词元级嵌入来实现对特定属性的解耦与连续编辑。

Result: 实验表明该方法可在多种属性和领域上实现直观、高效的连续控制编辑，且保持其他属性稳定。

Conclusion: 该方法无需修改扩散模型本身，具备良好的通用性和应用潜力，为文本到图像生成中的可控编辑提供了有效解决方案。

Abstract: Large-scale text-to-image diffusion models have become the backbone of modern
image editing, yet text prompts alone do not offer adequate control over the
editing process. Two properties are especially desirable: disentanglement,
where changing one attribute does not unintentionally alter others, and
continuous control, where the strength of an edit can be smoothly adjusted. We
introduce a method for disentangled and continuous editing through token-level
manipulation of text embeddings. The edits are applied by manipulating the
embeddings along carefully chosen directions, which control the strength of the
target attribute. To identify such directions, we employ a Sparse Autoencoder
(SAE), whose sparse latent space exposes semantically isolated dimensions. Our
method operates directly on text embeddings without modifying the diffusion
process, making it model agnostic and broadly applicable to various image
synthesis backbones. Experiments show that it enables intuitive and efficient
manipulations with continuous control across diverse attributes and domains.

</details>


### [643] [Pulp Motion: Framing-aware multimodal camera and human motion generation](https://arxiv.org/abs/2510.05097)
*Robin Courant,Xi Wang,David Loiseaux,Marc Christie,Vicky Kalogeiton*

Main category: cs.GR

TL;DR: 本文首次提出将人体运动与相机轨迹的生成视为文本条件下的联合生成任务，通过引入屏幕构图作为辅助模态，在共享潜在空间中实现多模态一致性，从而生成在视觉上连贯且符合电影美学的人体-相机协同运动。


<details>
  <summary>Details</summary>
Motivation: 传统方法分离处理人体运动与相机轨迹生成，忽略了电影拍摄中演员表演与镜头运动在屏幕空间中的紧密互动关系，导致生成结果缺乏视觉一致性和电影表现力。

Method: 提出一种模型无关的框架，设计一个联合自编码器学习人体与相机轨迹的共享潜在空间，并通过轻量级线性变换将两者潜在表示映射到屏幕构图潜在空间；利用该映射关系进行辅助采样，引导生成过程保持多模态一致性。同时构建PulpMotion数据集以支持文本驱动的联合生成研究。

Result: 在DiT和MAR等多种架构上实验表明，该方法能有效生成帧内一致的人体-相机运动，提升文本对齐能力，尤其在屏幕构图的合理性和电影感方面显著优于现有方法，定性结果显示更符合电影语言的镜头表达。

Conclusion: 通过引入屏幕构图为桥梁，实现了人体运动与相机轨迹的协同生成，为文本驱动的虚拟影视创作提供了新思路，并在生成质量与电影表现力上达到新高度。

Abstract: Treating human motion and camera trajectory generation separately overlooks a
core principle of cinematography: the tight interplay between actor performance
and camera work in the screen space. In this paper, we are the first to cast
this task as a text-conditioned joint generation, aiming to maintain consistent
on-screen framing while producing two heterogeneous, yet intrinsically linked,
modalities: human motion and camera trajectories. We propose a simple,
model-agnostic framework that enforces multimodal coherence via an auxiliary
modality: the on-screen framing induced by projecting human joints onto the
camera. This on-screen framing provides a natural and effective bridge between
modalities, promoting consistency and leading to more precise joint
distribution. We first design a joint autoencoder that learns a shared latent
space, together with a lightweight linear transform from the human and camera
latents to a framing latent. We then introduce auxiliary sampling, which
exploits this linear transform to steer generation toward a coherent framing
modality. To support this task, we also introduce the PulpMotion dataset, a
human-motion and camera-trajectory dataset with rich captions, and high-quality
human motions. Extensive experiments across DiT- and MAR-based architectures
show the generality and effectiveness of our method in generating on-frame
coherent human-camera motions, while also achieving gains on textual alignment
for both modalities. Our qualitative results yield more cinematographically
meaningful framings setting the new state of the art for this task. Code,
models and data are available in our
\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project
page}.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [644] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: 本文提出了WAREX，一个用于评估基于浏览器的LLM代理在现实网络不稳定和安全威胁下可靠性的框架。实验表明，现有代理在面对真实世界复杂性时任务成功率显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有的浏览器LLM代理基准测试多在稳定、可控环境中进行，无法反映真实网络中的不稳定性与安全风险（如跨站脚本攻击、页面异常弹窗等），因此需要一种更贴近现实的可靠性评估方法。

Method: 作者设计了WAREX框架，在三个主流基准（WebArena、WebVoyager、REAL）中引入网络不稳定性、系统故障和安全攻击等现实因素，评估现有LLM代理在这些条件下的任务完成表现。

Result: 实验结果显示，引入WAREX后，当前最先进的LLM代理任务成功率显著下降，暴露出其在真实环境中的脆弱性和缺乏鲁棒性。

Conclusion: LLM代理在现实网络环境下面临严峻的可靠性挑战，未来的研究需更加关注鲁棒性与安全性，WAREX为评估和改进提供了有效工具。

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [645] [Refined Iterated Pareto Greedy for Energy-aware Hybrid Flowshop Scheduling with Blocking Constraints](https://arxiv.org/abs/2510.03377)
*Ahmed Missaoui,Cemalettin Ozturk,Barry O'Sullivan*

Main category: cs.AI

TL;DR: 本研究针对带阻塞约束的混合流水车间调度问题（BHFS），提出了一种多目标混合整数规划模型，并开发了增强型epsilon约束法和一种高效的多目标元启发式算法（RIPG），以同时最小化完工时间和能源消耗，实验结果验证了所提方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于能源短缺、地缘政治问题、价格上涨和气候变化的影响，制造业亟需节能增效的解决方案；而现有调度方法在能耗与生产效率间的权衡不足，促使本研究探索兼顾完工时间与能源消耗的优化模型与算法。

Method: 首先建立了一个新的多目标混合整数规划（MIP）模型，采用增强型epsilon约束法求解Pareto最优解；并设计了一种改进的迭代Pareto贪婪算法（RIPG）用于高效求解大规模实例，通过小、中、大三种规模的算例进行性能测试与对比分析。

Result: 所提出的RIPG算法在求解大规模问题时表现出优越的效率和有效性，相比两个经典算法，能够在合理时间内获得高质量的Pareto前沿解，验证了模型与算法的可行性与竞争力。

Conclusion: 本研究为制造系统中的能效优化提供了有效的调度方法，所提出的模型与算法在平衡完工时间与能源消耗方面表现优异，具有广泛应用于汽车、制药等行业的潜力。

Abstract: The scarcity of non-renewable energy sources, geopolitical problems in its
supply, increasing prices, and the impact of climate change, force the global
economy to develop more energy-efficient solutions for their operations. The
Manufacturing sector is not excluded from this challenge as one of the largest
consumers of energy. Energy-efficient scheduling is a method that attracts
manufacturing companies to reduce their consumption as it can be quickly
deployed and can show impact immediately. In this study, the hybrid flow shop
scheduling problem with blocking constraint (BHFS) is investigated in which we
seek to minimize the latest completion time (i.e. makespan) and overall energy
consumption, a typical manufacturing setting across many industries from
automotive to pharmaceutical. Energy consumption and the latest completion time
of customer orders are usually conflicting objectives. Therefore, we first
formulate the problem as a novel multi-objective mixed integer programming
(MIP) model and propose an augmented epsilon-constraint method for finding the
Pareto-optimal solutions. Also, an effective multi-objective metaheuristic
algorithm. Refined Iterated Pareto Greedy (RIPG), is developed to solve large
instances in reasonable time. Our proposed methods are benchmarked using small,
medium, and large-size instances to evaluate their efficiency. Two well-known
algorithms are adopted for comparing our novel approaches. The computational
results show the effectiveness of our method.

</details>


### [646] [Know Thyself? On the Incapability and Implications of AI Self-Recognition](https://arxiv.org/abs/2510.03399)
*Xiaoyan Bai,Aryan Shrivastava,Ari Holtzman,Chenhao Tan*

Main category: cs.AI

TL;DR: 本文提出了一种系统性评估框架，用于检测10个主流大语言模型的自我识别能力，结果表明大多数模型在识别自身生成文本方面表现不佳，且存在对GPT和Claude系列模型的强烈预测偏差。


<details>
  <summary>Details</summary>
Motivation: 由于现有研究对大语言模型是否具备自我识别能力存在矛盾结论，本文旨在建立一个可扩展、可更新的评估框架，以澄清这一关键问题。

Method: 通过两个任务——二元自我识别和精确模型预测，评估10个当代大语言模型识别自身生成文本的能力，并分析其推理过程及对自身与其他模型存在的认知。

Result: 只有4个模型能正确预测自己为文本生成者，整体表现接近随机水平；模型普遍偏向预测GPT和Claude系列为生成者；尽管模型具备一定程度的自我和其他模型存在意识，但其推理表现出层级偏见，倾向于将高质量文本归因于GPT、Claude或Gemini。

Conclusion: 当前大语言模型缺乏可靠的自我识别能力，且存在显著的认知偏差，这对AI安全评估具有重要影响，未来需谨慎发展AI的自我意识机制。

Abstract: Self-recognition is a crucial metacognitive capability for AI systems,
relevant not only for psychological analysis but also for safety, particularly
in evaluative scenarios. Motivated by contradictory interpretations of whether
models possess self-recognition (Panickssery et al., 2024; Davidson et al.,
2024), we introduce a systematic evaluation framework that can be easily
applied and updated. Specifically, we measure how well 10 contemporary larger
language models (LLMs) can identify their own generated text versus text from
other models through two tasks: binary self-recognition and exact model
prediction. Different from prior claims, our results reveal a consistent
failure in self-recognition. Only 4 out of 10 models predict themselves as
generators, and the performance is rarely above random chance. Additionally,
models exhibit a strong bias toward predicting GPT and Claude families. We also
provide the first evaluation of model awareness of their own and others'
existence, as well as the reasoning behind their choices in self-recognition.
We find that the model demonstrates some knowledge of its own existence and
other models, but their reasoning reveals a hierarchical bias. They appear to
assume that GPT, Claude, and occasionally Gemini are the top-tier models, often
associating high-quality text with them. We conclude by discussing the
implications of our findings on AI safety and future directions to develop
appropriate AI self-awareness.

</details>


### [647] [ContraGen: A Multi-Agent Generation Framework for Enterprise Contradictions Detection](https://arxiv.org/abs/2510.03418)
*Ananya Mantravadi,Shivali Dalmia,Abhishek Mukherji,Nand Dave,Anudha Mittal*

Main category: cs.AI

TL;DR: 提出ContraGen框架，用于评估企业领域中检索增强生成系统的矛盾检测能力，通过合成包含矛盾的企业文档，支持文档内和跨文档的一致性评估。


<details>
  <summary>Details</summary>
Motivation: 现有矛盾检测基准局限于句子级别，无法反映企业文档（如合同、财务报告等）中的复杂矛盾，难以满足企业对合规性、治理和问责的要求。

Method: 结合自动矛盾挖掘与人工验证，生成带有嵌入式矛盾的企业风格合成文档，构建矛盾类型分类体系，并开发矛盾感知的检索评估流程。

Result: 实现了对企业场景下自矛盾和成对矛盾的可控生成，提供了高准确性的矛盾检测基准，并集成了人工监督以体现领域判断的复杂性。

Conclusion: ContraGen为提升企业级RAG系统的可信度与问责性提供了有效评估工具，有助于降低风险并确保合规。

Abstract: Retrieval-Augmented Generation (RAG) integrates LLMs with external sources,
offering advanced capabilities for information access and decision-making.
However, contradictions in retrieved evidence can result in inconsistent or
untrustworthy outputs, which is especially problematic in enterprise settings
where compliance, governance, and accountability are critical. Existing
benchmarks for contradiction detection are limited to sentence-level analysis
and do not capture the complexity of enterprise documents such as contracts,
financial filings, compliance reports, or policy manuals. To address this
limitation, we propose ContraGen, a contradiction-aware benchmark framework
tailored to enterprise domain. The framework generates synthetic
enterprise-style documents with embedded contradictions, enabling systematic
evaluation of both intra-document and cross-document consistency. Automated
contradiction mining is combined with human-in-the-loop validation to ensure
high accuracy. Our contributions include generating realistic enterprise
documents, modeling a taxonomy of contradiction types common in business
processes, enabling controlled creation of self- and pairwise contradictions,
developing a contradiction-aware retrieval evaluation pipeline and embedding
human oversight to reflect domain-specific judgment complexity. This work
establishes a foundation for more trustworthy and accountable RAG systems in
enterprise information-seeking applications, where detecting and resolving
contradictions is essential for reducing risk and ensuring compliance.

</details>


### [648] [A Qualitative Comparative Evaluation of Cognitive and Generative Theories](https://arxiv.org/abs/2510.03453)
*Paul S. Rosenbloom*

Main category: cs.AI

TL;DR: 本文从广义的理论评估视角，对面向全脑的认知架构与生成式神经架构及其完整系统进行了广泛而定性的比较。


<details>
  <summary>Details</summary>
Motivation: 由于认知架构和生成式神经架构在理论评估方面均面临挑战，因此需要一种综合的方法来进行有效评估。

Method: 采用广泛的理论评估视角，进行定性比较分析。

Result: 实现了对全脑导向的认知架构与生成式神经架构及其系统之间的多方面定性对比。

Conclusion: 该方法有助于克服两类架构在评估上的困难，为未来理论评估提供了可行路径。

Abstract: Evaluation is a critical activity associated with any theory. Yet this has
proven to be an exceptionally challenging activity for theories based on
cognitive architectures. For an overlapping set of reasons, evaluation can also
be challenging for theories based on generative neural architectures. This dual
challenge is approached here by leveraging a broad perspective on theory
evaluation to yield a wide-ranging, albeit qualitative, comparison of
whole-mind-oriented cognitive and generative architectures and the full systems
that are based on these architectures.

</details>


### [649] [Bridging LLM Planning Agents and Formal Methods: A Case Study in Plan Verification](https://arxiv.org/abs/2510.03469)
*Keshav Ramani,Vali Tawosi,Salwa Alamir,Daniel Borrajo*

Main category: cs.AI

TL;DR: 提出了一种利用大语言模型将自然语言计划转换为Kripke结构和线性时序逻辑（LTL）并进行模型检测的新框架，用于评估计划与预期行为的一致性。


<details>
  <summary>Details</summary>
Motivation: 自然语言计划与实际行为之间的对齐评估缺乏形式化验证手段，现有方法难以保证语义准确性。

Method: 使用大语言模型（LLM）将自然语言计划转化为Kripke结构和LTL公式，并在简化版PlanBench数据集上应用模型检测技术进行系统评估。

Result: 实验显示GPT-5在分类任务中达到96.3%的F1分数，且生成的形式化表示几乎总是语法正确的，但语义完全正确仍具挑战。

Conclusion: 该框架能高效实现自然语言到形式化模型的转换并具备高精度验证能力，是自动化计划验证的可行路径，但语义保真度需进一步研究。

Abstract: We introduce a novel framework for evaluating the alignment between natural
language plans and their expected behavior by converting them into Kripke
structures and Linear Temporal Logic (LTL) using Large Language Models (LLMs)
and performing model checking. We systematically evaluate this framework on a
simplified version of the PlanBench plan verification dataset and report on
metrics like Accuracy, Precision, Recall and F1 scores. Our experiments
demonstrate that GPT-5 achieves excellent classification performance (F1 score
of 96.3%) while almost always producing syntactically perfect formal
representations that can act as guarantees. However, the synthesis of
semantically perfect formal models remains an area for future exploration.

</details>


### [650] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: 本文提出了PolicyGuardBench，一个包含约6万个样本的基准数据集，用于检测自主Web智能体在长周期轨迹中的策略违规行为，并引入轻量级模型PolicyGuard-4B，能够在跨领域和子域场景下高效准确地检测策略违规，展示了小规模模型实现通用化监管的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对自主Web智能体在不同上下文（如领域和子领域）中生成的长周期轨迹是否符合外部或人类指定策略的评估，因此需要一个系统性的基准来检测策略违规问题。

Method: 构建了PolicyGuardBench基准数据集，涵盖多种智能体运行轨迹，生成多样化策略并标注子域内与跨子域的违规样本；设计全轨迹和基于前缀的违规检测任务；训练轻量级模型PolicyGuard-4B进行违规识别。

Result: PolicyGuard-4B在所有任务上均表现出强检测准确性，推理效率高，且能在未见过的领域和设置中保持高性能，展现出良好的泛化能力。

Conclusion: PolicyGuardBench与PolicyGuard-4B共同构成了首个研究Web智能体轨迹策略合规性的综合框架，证明了小规模但高效、可泛化的策略监管是可行的。

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [651] [OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit Flows](https://arxiv.org/abs/2510.03506)
*John Nguyen,Marton Havasi,Tariq Berrada,Luke Zettlemoyer,Ricky T. Q. Chen*

Main category: cs.AI

TL;DR: OneFlow是一种非自回归的多模态模型，能够实现变长和并发的跨模态生成，结合插入式编辑流和流匹配技术，在文本-图像生成任务中优于自回归基线，且训练计算量减少50%。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在多模态生成中强制施加了严格的因果顺序，限制了文本与图像的并发生成和灵活性，需要一种更高效、灵活的生成框架。

Method: OneFlow结合基于插入的Edit Flow处理离散文本标记，使用Flow Matching处理图像潜在表示，并通过分层采样实现内容优先的并发文本-图像生成。

Result: 在1B到8B参数规模的模型上实验表明，OneFlow在生成和理解任务上均优于自回归基线，训练FLOPs最多减少50%，并支持并发生成、迭代优化和类推理生成等新功能。

Conclusion: OneFlow为多模态生成提供了更高效、灵活的非自回归框架，突破了传统自回归模型的顺序生成限制，展现出卓越的性能和新能力。

Abstract: We present OneFlow, the first non-autoregressive multimodal model that
enables variable-length and concurrent mixed-modal generation. Unlike
autoregressive models that enforce rigid causal ordering between text and image
generation, OneFlow combines an insertion-based Edit Flow for discrete text
tokens with Flow Matching for image latents. OneFlow enables concurrent
text-image synthesis with hierarchical sampling that prioritizes content over
grammar. Through controlled experiments across model sizes from 1B to 8B, we
demonstrate that OneFlow outperforms autoregressive baselines on both
generation and understanding tasks while using up to 50% fewer training FLOPs.
OneFlow surpasses both autoregressive and diffusion-based approaches while
unlocking new capabilities for concurrent generation, iterative refinement, and
natural reasoning-like generation.

</details>


### [652] [Understanding the Role of Training Data in Test-Time Scaling](https://arxiv.org/abs/2510.03605)
*Adel Javanmard,Baharan Mirzasoleiman,Vahab Mirrokni*

Main category: cs.AI

TL;DR: 本文研究了在变换器模型上测试时扩展（test-time scaling）对推理能力的影响，特别是在线性回归的上下文权重预测任务中。作者提供了理论解释，说明增加测试时计算资源可以减少训练提示中的上下文示例数量，并指出如果训练数据中缺乏解决下游任务所需的技能，增加测试时计算反而会损害性能。此外，通过特征协方差矩阵的最小特征值来表征任务难度，发现训练于多样化、相关且困难的任务集上能最好地提升测试时扩展的表现。实验结果在大型非线性变换器架构上得到了验证。


<details>
  <summary>Details</summary>
Motivation: 尽管测试时扩展在提高大语言模型推理能力方面表现出色，但其在训练数据中长思维链（CoTs）出现的条件以及何时能提升性能仍不清楚。因此，本文旨在探究这些条件和机制。

Method: 研究基于变换器模型，在一个上下文内的线性回归权重预测任务上进行分析，结合理论推导与实验验证，探讨测试时扩展的效果及其影响因素。

Result: 1) 在固定测试误差下，增加测试时计算可减少训练所需上下文长度；2) 若训练数据缺乏必要技能，增加测试时计算可能损害性能；3) 任务难度可通过特征协方差矩阵的最小特征值刻画，训练在多样、相关且难的任务上有最佳表现；4) 实验在大型非线性变换器上验证了上述结论。

Conclusion: 测试时扩展的效果依赖于训练数据中任务的多样性、相关性和难度，合理设计训练任务可最大化其性能增益，而盲目增加计算资源可能适得其反。

Abstract: Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.

</details>


### [653] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: 本文提出了一种名为跨模态偏好引导（CPS）的新方法，通过联合优化视觉和文本描述中的微小修改，在现实的黑盒攻击设置下有效操纵基于视觉-语言模型的网络代理决策，显著优于现有基线方法且更难被检测。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型代理易受偏好操纵攻击，但多数研究假设强白盒访问或不切实际的条件，缺乏对真实场景中多模态协同攻击的探索。

Method: 提出Cross-Modal Preference Steering（CPS），结合CLIP可迁移的图像扰动和RLHF诱导的语言偏见，联合优化物品的视觉与文本描述，在仅能修改自身内容的黑盒设置下实现隐蔽攻击。

Result: 在GPT-4、Qwen-2.5VL和Pixtral-Large等先进VLM驱动的电影选择和电商任务中，CPS显著优于基线方法，成功率更高且检测率低70%。

Conclusion: 联合利用视觉与文本通道可在现实攻击条件下实现更强大、更隐蔽的偏好操纵，凸显了对智能代理系统构建鲁棒防御机制的紧迫性。

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [654] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: 提出了一种基于信息论的树搜索框架MITS，利用点互信息（PMI）评分函数实现对推理路径的逐步评估，并结合动态采样和加权投票策略，在保持计算效率的同时显著提升了大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有树搜索方法在评估中间推理步骤质量时缺乏即时且可靠的量化手段，且广泛路径探索计算成本高。

Method: 提出MITS框架，使用基于点互信息（PMI）的评分函数进行步骤级评估，通过束搜索引导推理路径扩展，无需昂贵的前瞻模拟；引入基于熵的动态采样策略，自适应分配计算资源到不确定性高的推理步骤；最终采用结合PMI得分与预测一致性的加权投票机制进行预测。

Result: 在多个推理基准上的实验表明，MITS consistently 优于基线方法，实现了更优的推理性能和更高的计算效率。

Conclusion: MITS为大语言模型的推理提供了一个有原则且高效的框架，通过信息论指导的评分与动态资源分配，在减少计算开销的同时提升了推理质量。

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [655] [Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs](https://arxiv.org/abs/2510.03680)
*Bumjun Kim,Dongjae Jeon,Dueun Kim,Wonje Jeung,Albert No*

Main category: cs.AI

TL;DR: 本文提出了Rainbow Padding方法，以解决扩散大语言模型（dLLMs）中因<eos>标记兼具终止和填充功能而导致的早停问题（即<eos>溢出）。通过使用多个不同的填充标记循环替代重复的<eos>，该方法有效分散了概率质量，显著提升了生成长度的鲁棒性和输出质量，且仅需少量微调即可集成到现有模型中。


<details>
  <summary>Details</summary>
Motivation: 指令调优的扩散大语言模型在增加序列长度时会出现响应变短甚至过早终止的问题，这是由于<eos>标记同时承担终止和填充功能所致，但这一问题尚未被系统分析。

Method: 提出Rainbow Padding方法，用一组不同填充标记的循环代替重复的<eos>标记，打破<eos>的概率主导，从而缓解早停现象，并通过LoRA在少量数据上进行微调实现高效集成。

Result: 实验表明，只需7个填充标记即可有效防止早停，显著提升生成长度稳定性和输出质量，且单轮微调即可见明显改进。

Conclusion: Rainbow Padding是一种简单、高效且实用的解决方案，可有效解决dLLMs中的<eos>溢出问题，增强了模型对长序列生成的鲁棒性。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising
alternative to autoregressive models, offering flexible generation orders and
strong performance on complex reasoning tasks. However, instruction-tuned dLLMs
exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated
sequence length increases, responses paradoxically become shorter, collapsing
into early termination or degenerating into streams of \texttt{<eos>} tokens.
Although noticed in practice, this issue has not been systematically analyzed.
We trace its root cause to the dual role of \texttt{<eos>} as both termination
and padding, which concentrates probability mass on \texttt{<eos>} at later
positions and propagates backward to trigger early termination. To address
this, we introduce Rainbow Padding, a simple remedy that replaces repeated
\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens,
distributing probability mass and breaking \texttt{<eos>} dominance.
Experiments show that Rainbow Padding substantially improves length robustness
and output quality, with as few as seven padding tokens sufficient to prevent
early termination. Moreover, the method integrates efficiently into existing
instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data
yields significant improvements, making this solution highly practical. The
code is publicly available at https://github.com/quasar529/rainbow-padding.

</details>


### [656] [CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano](https://arxiv.org/abs/2412.18708)
*Vivek Vellaiyappan Surulimuthu,Aditya Karnam Gururaj Rao*

Main category: cs.AI

TL;DR: 提出了一种名为Chunked Augmented Generation (CAG) 的架构，用于克服Google Chrome内置Gemini Nano模型上下文窗口有限的问题，能够在浏览器中高效处理大文本输入。


<details>
  <summary>Details</summary>
Motivation: Gemini Nano在Chrome中的集成虽是重大进展，但其受限的上下文窗口难以处理大型输入，限制了实际应用。

Method: 通过智能的输入分块和处理策略，将大输入分割为适合模型处理的小块，并在浏览器约束内优化执行流程。

Result: CAG在Chrome中有效支持对大文档和数据集的处理，无需依赖外部API即可实现复杂的AI功能。

Conclusion: CAG成功突破了Gemini Nano的上下文限制，使浏览器内AI能力更加强大且自给自足。

Abstract: We present Chunked Augmented Generation (CAG), an architecture specifically
designed to overcome the context window limitations of Google Chrome's built-in
Gemini Nano model. While Chrome's integration of Gemini Nano represents a
significant advancement in bringing AI capabilities directly to the browser,
its restricted context window poses challenges for processing large inputs. CAG
addresses this limitation through intelligent input chunking and processing
strategies, enabling efficient handling of extensive content while maintaining
the model's performance within browser constraints. Our implementation
demonstrates particular efficacy in processing large documents and datasets
directly within Chrome, making sophisticated AI capabilities accessible through
the browser without external API dependencies. Get started now at
https://github.com/vivekVells/cag-js.

</details>


### [657] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 提出了一种面向目标的多智能体系统评估框架，引入了目标成功率（GSR）和失败根本原因（RCOF）分类法，通过基于大模型的可解释、数据高效的评估方法，在企业员工对话系统AIDA上实现了GSR从63%提升至79%。


<details>
  <summary>Details</summary>
Motivation: 现有方法多在单轮层面评估对话质量，难以判断用户整体目标是否达成，缺乏对多轮对话中任务完成度的系统性评估。

Method: 将对话按用户目标分段，利用教师LLM结合领域专家设定的目标与质量标准进行评估，使用“思考令牌”生成可解释的推理过程，提出GSR指标和RCOF分类体系。

Result: 在企业级多智能体对话系统AIDA上的应用显示，GSR在六个月内从63%提升至79%，评估框架具备良好的可解释性和数据效率，并能提供详细的缺陷分析。

Conclusion: 该框架能有效评估多轮对话系统的目标达成情况，通过细粒度的失败归因支持系统持续优化，具有通用性和实际应用价值。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [658] [H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis](https://arxiv.org/abs/2510.03700)
*Seungseop Lim,Gibaeg Kim,Hyunkyung Lee,Wooseok Han,Jean Seo,Jaehyo Yoo,Eunho Yang*

Main category: cs.AI

TL;DR: 提出H-DDx，一种基于层次结构的评估框架，用于更准确地衡量大语言模型在鉴别诊断中的临床相关性表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（如Top-k准确率）无法区分临床相关的“接近错误”和诊断上相差较远的错误，缺乏对模型输出临床意义的合理评价。

Method: 构建一个结合检索与重排序的流程，将自由文本诊断映射到ICD-10编码，并采用层次化度量方法，对与真实诊断相近的预测给予更高评分。

Result: 在22个主流模型上的基准测试显示，传统扁平化指标低估了模型性能，而H-DDx能更好反映临床相关性，且发现领域专用的开源模型表现优异；同时框架揭示了模型常能正确识别 broader 临床背景。

Conclusion: H-DDx提供了一种更具临床意义和可解释性的评估方式，推动LLM在医疗诊断中的可靠应用。

Abstract: An accurate differential diagnosis (DDx) is essential for patient care,
shaping therapeutic decisions and influencing outcomes. Recently, Large
Language Models (LLMs) have emerged as promising tools to support this process
by generating a DDx list from patient narratives. However, existing evaluations
of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy,
which fail to distinguish between clinically relevant near-misses and
diagnostically distant errors. To mitigate this limitation, we introduce H-DDx,
a hierarchical evaluation framework that better reflects clinical relevance.
H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses
to ICD-10 codes and applies a hierarchical metric that credits predictions
closely related to the ground-truth diagnosis. In benchmarking 22 leading
models, we show that conventional flat metrics underestimate performance by
overlooking clinically meaningful outputs, with our results highlighting the
strengths of domain-specialized open-source models. Furthermore, our framework
enhances interpretability by revealing hierarchical error patterns,
demonstrating that LLMs often correctly identify the broader clinical context
even when the precise diagnosis is missed.

</details>


### [659] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: 提出了一种名为“投机性动作”（speculative actions）的无损框架，通过快速模型预测AI代理的可能动作，实现并行动作执行，显著降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: AI代理在环境中的执行速度通常较慢，受限于顺序执行和耗时的API调用，影响训练、评估和部署效率。

Method: 受微处理器和大模型推理中推测执行的启发，使用更快的模型预测代理的下一步动作，并行执行多个步骤，验证于游戏、电商、网络搜索和操作系统环境。

Result: 在多个代理环境中实现了高达55%的下一动作预测准确率，显著降低了端到端延迟，且可通过更强的猜测模型和多步推测进一步优化性能。

Conclusion: 投机性动作框架为实现实时、低延迟的AI代理系统提供了可行且可扩展的路径。

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [660] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 本文探讨了如何将多模态基础模型（MFMs）发展为有效的世界模型，通过增强其因果推理、反事实思维和时空推理能力，并提出结构化生成框架以实现图像、视频及4D内容的可控生成。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态基础模型在作为世界模型方面存在不足，缺乏反事实推理、动态模拟、时空理解等关键能力，因此需要探索如何提升其推理与生成能力。

Method: 通过判别性任务提升MFMs的推理能力，引入因果推断、反事实思维和时空推理；在生成方面，采用场景图、多模态条件控制和对齐策略，实现结构化、可控的图像、视频和4D生成。

Result: 增强了MFMs在多模态理解与生成中的结构化推理能力，实现了高语义一致性和细粒度用户意图匹配的可控生成，特别是在时空维度上的可交互、可编辑对象合成。

Conclusion: 通过结合结构化推理与可控生成方法，可以有效缩小多模态基础模型与真正世界模型之间的差距，推动其向具备物理世界理解与模拟能力的方向发展。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [661] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: 本文提出了一种名为OptAgent的新框架，结合多智能体模拟与遗传算法，用于优化和验证电商查询重写任务中的用户查询。


<details>
  <summary>Details</summary>
Motivation: 由于电商查询重写属于主观任务，难以通过传统算法判断重写质量，现有方法在缺乏标准答案的情况下评估困难。

Method: 采用多个基于大语言模型的智能体作为模拟购物用户，提供动态奖励信号，并以平均得分作为遗传算法的适应度函数，迭代优化初始查询。

Result: 在1000个真实电商查询数据集上测试，相比原始用户查询平均提升21.98%，优于Best-of-N基线3.36%。

Conclusion: OptAgent能有效提升主观任务中查询重写的质量，为缺乏标准答案的任务提供了可靠的评估与优化框架。

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [662] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: 本文提出了LEGOMem，一种用于多智能体大语言模型系统的模块化程序记忆框架，通过分解任务轨迹并灵活分配记忆单元来提升工作流自动化中的规划与执行效果。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中缺乏对程序性记忆的系统性研究，且现有方法未充分探索记忆的位置、检索方式及受益主体。

Method: 提出LEGOMem框架，将过往任务轨迹分解为可重用的记忆单元，并在协调器和任务代理之间灵活分配；通过OfficeBench基准进行系统性实验，分析不同记忆配置的影响。

Result: 实验表明协调器记忆对任务分解和委派至关重要，细粒度的代理记忆能提高执行准确性；即使使用较小的语言模型，也能通过利用历史执行轨迹显著缩小与强模型的性能差距。

Conclusion: LEGOMem不仅是一个实用的记忆增强型代理系统框架，也为研究多智能体工作流自动化中的记忆设计提供了有效工具。

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [663] [GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time](https://arxiv.org/abs/2510.03777)
*Divij Handa,Mihir Parmar,Aswin RRV,Md Nayem Uddin,Hamid Palangi,Chitta Baral*

Main category: cs.AI

TL;DR: 提出了一种新的推理算法GuidedSampling，通过分离探索和生成阶段来提高候选解的多样性，并在多个基准上显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: Repeated Sampling (RS) 在复杂任务中虽然有效，但生成的候选解缺乏多样性，容易重复使用相同的解题方法。

Method: GuidedSampling 将推理过程分为探索和生成两个阶段：探索阶段识别可用于解决问题的多种概念，生成阶段基于特定概念生成最终的候选解。同时定义了该方法的理论边界。

Result: GuidedSampling 在 pass@50 上平均提升约 21.6%；训练时使用 GuidedSampling 的模型在 pass@5 上平均提升约 9.7%，且每实例平均概念数从 1.67 提升至 3.03，显著增加了解的多样性。

Conclusion: GuidedSampling 能有效提升推理过程中候选解的多样性和模型性能，优于传统的 Repeated Sampling 方法。

Abstract: Repeated Sampling (RS) is a simple inference-time algorithm that has been
shown to improve model performance on complex tasks. Although it is an
effective way of scaling inference time, it often struggles to generate diverse
solution candidates, frequently relying on the same underlying approach to
solve the problem and thus producing redundant samples. To address this
limitation, we propose a new inference algorithm, GuidedSampling, which
decouples the exploration and generation phases during inference, increasing
diversity of generated candidate solutions. The exploration phase identifies
multiple concepts that can be utilized to solve the problem, while the
generation phase applies a specific concept to provide final solution
candidates. We first define the theoretical bounds of GuidedSampling and then
empirically demonstrate that it improves the performance of base model at
pass@50 by on an average ~21.6% across various benchmarks compared to RS.
Furthermore, models trained on trajectories of GuidedSampling exhibit
substantial performance improvements at pass@5 by on an average ~9.7%, compared
to models trained on traditional RS. Additionally, models trained with
GuidedSampling increases the average number of concepts per instance (1.67 ->
3.03), yielding a diverse set of candidates than traditional RS.

</details>


### [664] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: 本文提出将关卡生成视为多智能体问题，以提高基于强化学习的程序化内容生成（PCGRL）的效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有PCGRL方法依赖单个生成器智能体，频繁计算关卡质量启发式指标且需遍历大地图，导致效率瓶颈。

Method: 采用多智能体框架进行关卡生成，减少奖励计算次数与动作比例，学习局部、模块化的设计策略。

Result: 多智能体生成器在效率上优于单智能体，并能更好泛化到分布外的地图形状。

Conclusion: 将内容生成视为分布式多智能体任务有利于大规模生成功能性内容。

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [665] [The Hidden Game Problem](https://arxiv.org/abs/2510.03845)
*Gon Buzaglo,Noah Golowich,Elad Hazan*

Main category: cs.AI

TL;DR: 本文研究了一类具有大策略空间的博弈，提出了“隐藏博弈”问题，并设计了能有效最小化遗憾、快速收敛到相关均衡的算法。


<details>
  <summary>Details</summary>
Motivation: 受AI对齐和语言博弈中大策略空间挑战的驱动，探讨是否存在能够发现并利用隐藏高奖励子策略集的高效算法。

Method: 通过组合多种遗憾最小化技术，实现外部遗憾和交换遗憾的最优界，利用隐藏博弈结构提升计算效率。

Result: 所提方法能在隐藏子博弈中快速收敛到相关均衡，同时保持整体理性，并达到最优的遗憾界。

Conclusion: 证明了在存在隐藏高回报策略结构的博弈中，可以设计出高效的后悔最小化算法以实现均衡。

Abstract: This paper investigates a class of games with large strategy spaces,
motivated by challenges in AI alignment and language games. We introduce the
hidden game problem, where for each player, an unknown subset of strategies
consistently yields higher rewards compared to the rest. The central question
is whether efficient regret minimization algorithms can be designed to discover
and exploit such hidden structures, leading to equilibrium in these subgames
while maintaining rationality in general. We answer this question affirmatively
by developing a composition of regret minimization techniques that achieve
optimal external and swap regret bounds. Our approach ensures rapid convergence
to correlated equilibria in hidden subgames, leveraging the hidden game
structure for improved computational efficiency.

</details>


### [666] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: 本文提出了一种名为ECHO的新型错误归因算法，通过结合分层上下文表示、基于客观分析的评估和共识投票机制，显著提升了多智能体系统中错误归因的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统错误归因方法在处理复杂交互模式时存在准确性与一致性不足的问题，难以有效定位智能体及步骤级别的错误。

Method: ECHO算法采用基于位置的上下文理解分层结构，结合客观评价标准，并通过共识投票机制进行最终判断，从而实现更精确的错误归因。

Result: 实验结果表明，ECHO在多种多智能体交互场景中均优于现有方法，尤其在涉及细微推理错误和复杂依赖关系的情况下表现突出。

Conclusion: 结构化的分层上下文表示与基于共识的客观决策相结合，为多智能体系统的错误归因提供了更稳健的框架。

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [667] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 本文探讨了小型语言模型（SLM）在代理工作负载中的优越性，提出通过引导解码、结构化输出和验证机制，使SLM在成本、延迟和能效上显著优于大模型（LLM），并给出了以SLM为核心的实用代理系统设计蓝图。


<details>
  <summary>Details</summary>
Motivation: 随着对高效、低成本AI系统的需求增加，研究者希望探索小型语言模型（SLM）是否能在特定任务（如工具调用、函数调用和RAG）中替代或超越大型语言模型（LLM），尤其是在API约束和结构化输出场景下。

Method: 综合分析多个开源与闭源SLM的表现，结合现代评估基准（如BFCL v3/v4、StableToolBench）和服务框架（vLLM、SGLang等），引入引导解码库（XGrammar、Outlines），提出SLM优先、LLM回退的系统架构，并设计不确定性感知路由与验证级联机制。

Result: SLM在结构化输出、工具使用和函数调用等任务中可匹敌甚至超越LLM，同时实现10-100倍的令牌成本降低、更低延迟和能耗；提出了CPS、模式有效性率等工程指标，并验证了LoRA/QLoRA等轻量适配方法的有效性。

Conclusion: SLM在多数代理任务中应作为默认选择，配合LLM回退机制处理复杂推理任务，形成高效、可靠且节能的代理系统架构。

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [668] [Algorithm Generation via Creative Ideation](https://arxiv.org/abs/2510.03851)
*Ruiying Ma,Chieh-Jan Mike Liang,Yanjie Gao,Francis Y. Yan*

Main category: cs.AI

TL;DR: 本文提出了一种名为MetaMuse的框架，利用大语言模型进行系统算法生成，通过三种自反思原则提升创造性思维，显著优化了缓存替换和在线装箱问题的性能。


<details>
  <summary>Details</summary>
Motivation: 由于解空间的不连续性，系统算法设计通常依赖通用启发式方法，导致性能受限；现有大语言模型倾向于生成常见设计，缺乏创造性突破。

Method: MetaMuse基于三个自反思原则：在可度量的性能空间中量化解的多样性与有用性、通过外部刺激引导创意生成、使用航点推理构建可执行方案。

Result: 在缓存替换任务中减少最多35.76%的缓存未命中，在在线装箱任务中减少最多30.93%的容器使用量。

Conclusion: MetaMuse能够有效引导大语言模型生成高性能、创造性的系统算法，克服传统方法和LLM在不连续解空间中的局限。

Abstract: Designing system algorithms remains challenging, where the discontinuous
nature of the solution space often forces system engineers to rely on generic
heuristics at the expense of performance. We study whether LLMs can practically
drive algorithm generation, and find that they are biased towards well-known
generic designs, rather than making the creative leaps needed to navigate the
discontinuous solution space. To address this limitation, we introduce
MetaMuse, a framework for creative ideation built on three self-reflection
principles: (1) quantifying solution diversity and usefulness in measurable
performance space, rather than abstract idea space, (2) steering ideation
through external stimuli, rather than internal randomness, and (3) constructing
executable solutions using waypoint reasoning, rather than free-form
chain-of-thought. Extensive evaluation shows that MetaMuse can generate
high-performing solutions for two critical problems at a global cloud provider:
cache replacement (reducing cache misses by up to 35.76%) and online bin
packing (reducing bin usage by up to 30.93%).

</details>


### [669] [Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning](https://arxiv.org/abs/2510.03859)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 提出一种基于大语言模型（LLM）和可解释AI（XAI）代理的上下文感知异常检测方法，用于提升物联网系统中异常检测的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法在动态、高维、数据不完整或持续变化的复杂IoT环境中表现不佳，难以满足智能医疗、能源电网等关键系统的实时需求。

Method: 结合LLM的语义理解与上下文推理能力，利用注意力机制和带有语义的记忆缓冲区捕捉数据流中的隐藏模式和不一致性；引入XAI代理增强决策透明度和可解释性，支持人工审查与合规验证。

Result: 在智能电网和医疗场景的模拟测试中，该方法相比传统模型显著提升了检测准确率，降低了误报率，同时提供了更清晰的解释结果，并表现出良好的响应速度与环境适应性。

Conclusion: 所提出的LLM增强型异常检测框架在准确性与可解释性方面优于现有模型，具备应用于未来关键物联网系统的潜力。

Abstract: Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT

</details>


### [670] [Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation](https://arxiv.org/abs/2510.03863)
*Arina Kharlamova,Bowei He,Chen Ma,Xue Liu*

Main category: cs.AI

TL;DR: 本文提出了一种名为Spatial CAPTCHA的新型人类验证框架，利用人类与多模态大语言模型（MLLMs）在空间推理能力上的差异，通过生成需要几何推理、视角转换和心理旋转等任务的动态问题，有效抵御自动化攻击。实验表明，人类在此类任务上显著优于当前最先进的MLLMs，最佳模型准确率仅为31.0%。


<details>
  <summary>Details</summary>
Motivation: 由于多模态大语言模型在文本识别和2D图像理解方面的进步，传统CAPTCHA的安全性受到严重威胁，亟需设计更符合人类认知优势且AI难以破解的新一代验证机制。

Method: 提出Spatial CAPTCHA框架，采用程序化生成流程，生成涉及几何推理、遮挡处理、视角转换和心理旋转等空间认知任务的问题，并结合基于约束的难度控制、自动正确性验证和人工参与验证机制，确保系统的可扩展性、鲁棒性和适应性。

Result: 在Spatial-CAPTCHA-Bench基准测试中，人类的表现远超10个最先进的MLLMs，最佳模型的Pass@1准确率仅为31.0%；与Google reCAPTCHA的对比实验也验证了其作为安全机制和AI空间推理能力评估工具的有效性。

Conclusion: Spatial CAPTCHA通过聚焦人类擅长而AI薄弱的空间推理能力，构建了更具安全性的验证码系统，为未来人机区分提供了新的方向，同时也可作为评估AI空间认知能力的诊断工具。

Abstract: Online services rely on CAPTCHAs as a first line of defense against automated
abuse, yet recent advances in multi-modal large language models (MLLMs) have
eroded the effectiveness of conventional designs that focus on text recognition
or 2D image understanding. To address this challenge, we present Spatial
CAPTCHA, a novel human-verification framework that leverages fundamental
differences in spatial reasoning between humans and MLLMs. Unlike existing
CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern
AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning,
perspective-taking, occlusion handling, and mental rotation. These skills are
intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The
system employs a procedural generation pipeline with constraint-based
difficulty control, automated correctness verification, and human-in-the-loop
validation to ensure scalability, robustness, and adaptability. Evaluation on a
corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly
outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0%
Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA,
which confirms its effectiveness as both a security mechanism and a diagnostic
tool for spatial reasoning in AI.

</details>


### [671] [Rare Text Semantics Were Always There in Your Diffusion Transformer](https://arxiv.org/abs/2510.03886)
*Seil Kang,Woojung Han,Dayun Ju,Seong Jae Hwang*

Main category: cs.AI

TL;DR: 提出一种无需额外训练或数据的方法，通过在联合注意力机制前扩大文本标记嵌入的方差来增强多模态扩散变换器对罕见语义的生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理用户提出的富有想象力或罕见提示时表现不佳，因为这些概念在预训练过程中出现频率低，难以形成强烈表征。

Method: 利用MM-DiT中固有的联合注意力机制，在Transformer块中对文本和图像嵌入进行同步更新，并在联合注意力之前通过数学方法扩大文本标记嵌入的方差以扩展其表示盆地。

Result: 该方法显著提升了MM-DiT在生成包含罕见语义内容上的表现，并在文本到图像、文本到视频及文本驱动图像编辑任务中展现出良好的泛化能力。

Conclusion: 所提方法简单有效，无需额外训练或外部模块即可激发MM-DiT中隐藏的罕见语义，使生成结果更符合用户意图。

Abstract: Starting from flow- and diffusion-based transformers, Multi-modal Diffusion
Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim
for exceptional visual fidelity. As these models advance, users continually
push the boundary with imaginative or rare prompts, which advanced models still
falter in generating, since their concepts are often too scarce to leave a
strong imprint during pre-training. In this paper, we propose a simple yet
effective intervention that surfaces rare semantics inside MM-DiTs without
additional training steps, data, denoising-time optimization, or reliance on
external modules (e.g., large language models). In particular, the
joint-attention mechanism intrinsic to MM-DiT sequentially updates text
embeddings alongside image embeddings throughout transformer blocks. We find
that by mathematically expanding representational basins around text token
embeddings via variance scale-up before the joint-attention blocks, rare
semantics clearly emerge in MM-DiT's outputs. Furthermore, our results
generalize effectively across text-to-vision tasks, including text-to-image,
text-to-video, and text-driven image editing. Our work invites generative
models to reveal the semantics that users intend, once hidden yet ready to
surface.

</details>


### [672] [Kantian-Utilitarian XAI: Meta-Explained](https://arxiv.org/abs/2510.03892)
*Zahra Atf,Peter R. Lewis*

Main category: cs.AI

TL;DR: 提出了一种用于咖啡消费决策的博弈化可解释AI系统，结合康德主义与功利主义推理，支持伦理感知的消费者选择。


<details>
  <summary>Details</summary>
Motivation: 帮助消费者在复杂伦理和环境因素中做出更明智、透明的消费决策，提升AI在道德决策中的可解释性与实用性。

Method: 系统采用双推理引擎：康德模块检测规则违反，功利模块通过多准则聚合评分选项；元解释器在福利损失较小时切换至伦理合规且效用接近最优的选项。

Result: 实现了实时解释生成、决策对齐分析与可审计策略追踪，并发布了结构化配置与交互式UI。

Conclusion: 该XAI系统有效融合伦理原则与量化评估，支持用户在道德与实用之间取得平衡，推动负责任的消费行为。

Abstract: We present a gamified explainable AI (XAI) system for ethically aware
consumer decision-making in the coffee domain. Each session comprises six
rounds with three options per round. Two symbolic engines provide real-time
reasons: a Kantian module flags rule violations (e.g., child labor,
deforestation risk without shade certification, opaque supply chains, unsafe
decaf), and a utilitarian module scores options via multi-criteria aggregation
over normalized attributes (price, carbon, water, transparency, farmer income
share, taste/freshness, packaging, convenience). A meta-explainer with a regret
bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a
deontically clean, near-parity option when welfare loss is small. We release a
structured configuration (attribute schema, certification map, weights, rule
set), a policy trace for auditability, and an interactive UI.

</details>


### [673] [Quantifying Risks in Multi-turn Conversation with Large Language Models](https://arxiv.org/abs/2510.03969)
*Chengxiao Wang,Isha Chaudhary,Qian Hu,Weitong Ruan,Rahul Gupta,Gagandeep Singh*

Main category: cs.AI

TL;DR: 提出QRLLM，一种针对大语言模型在多轮对话中灾难性风险的认证框架，通过概率分布和统计保证量化风险，揭示前沿模型中存在的严重安全隐患。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法因依赖固定攻击提示、缺乏统计保证且难以覆盖多轮对话的广阔空间，无法充分揭示大语言模型在对话中产生灾难性回应的风险。

Method: 将多轮对话建模为查询序列的概率分布，使用基于语义相似性的查询图上的马尔可夫过程表示，并定义多种高效实用的分布模式（如随机节点、图路径、带拒绝的自适应），利用置信区间量化灾难性风险。

Result: 实验结果显示所提方法能有效揭示前沿模型中的重大灾难性风险，最差模型的认证下限高达70%。

Conclusion: 需要改进前沿大语言模型的安全训练策略以应对严重的潜在风险。

Abstract: Large Language Models (LLMs) can produce catastrophic responses in
conversational settings that pose serious risks to public safety and security.
Existing evaluations often fail to fully reveal these vulnerabilities because
they rely on fixed attack prompt sequences, lack statistical guarantees, and do
not scale to the vast space of multi-turn conversations. In this work, we
propose QRLLM, a novel, principled Certification framework for Catastrophic
risks in multi-turn Conversation for LLMs that bounds the probability of an LLM
generating catastrophic responses under multi-turn conversation distributions
with statistical guarantees. We model multi-turn conversations as probability
distributions over query sequences, represented by a Markov process on a query
graph whose edges encode semantic similarity to capture realistic
conversational flow, and quantify catastrophic risks using confidence
intervals. We define several inexpensive and practical distributions: random
node, graph path, adaptive with rejection. Our results demonstrate that these
distributions can reveal substantial catastrophic risks in frontier models,
with certified lower bounds as high as 70\% for the worst model, highlighting
the urgent need for improved safety training strategies in frontier LLMs.

</details>


### [674] [What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models](https://arxiv.org/abs/2510.04009)
*Zicong He,Boxuan Zhang,Weihao Liu,Ruixiang Tang,Lu Cheng*

Main category: cs.AI

TL;DR: 本文提出了C^2-Eval，一个用于统一评估基础模型创造力的综合性基准，区分了收敛性与发散性创造力，并基于有用性、原创性和惊喜性（U-O-S）进行细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 现有创造力评估框架零散且缺乏理论基础，难以适应生成式基础模型的发展，因此需要一个基于社会科学研究的系统性评估方法。

Method: 提出C^2-Eval基准，区分收敛性与发散性创造力，采用源自社会科学理论的有用性、原创性和惊喜性（U-O-S）三个维度进行评估，并在主流闭源与开源模型上进行实验验证。

Result: 实验表明当前基础模型在两种创造力上存在权衡，C^2-Eval能有效揭示其创造性能力的优势与局限。

Conclusion: C^2-Eval为评估基础模型的创造力提供了有效工具，推动机器创造力研究向更系统、理论驱动的方向发展。

Abstract: The meteoric rise of foundation models (FMs) has expanded their capabilities
far beyond conventional tasks. Creativity, long regarded as a hallmark of human
intelligence and a driver of innovation, is now increasingly recognized as a
critical dimension of machine intelligence in the era of generative FMs,
complementing traditional measures of accuracy. However, existing evaluation
frameworks for creativity remain fragmented, relying on ad hoc metrics not
firmly grounded in established theories. To address this gap, we introduce
C^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.
C^2-Eval distinguishes between two complementary forms of creativity:
convergent creativity, where tasks admit constrained solutions (e.g., code
generation), and divergent creativity, where tasks are open-ended (e.g.,
storytelling). It evaluates both dimensions using fine-grained criteria derived
from social-science theory, focusing on Usefulness, Originality, and Surprise
(U-O-S). Through extensive experiments on leading proprietary and open-source
models, we analyze trade-offs in their creative capabilities. Our results
highlight both the strengths and challenges of current FMs in pursuing a
creative machine mind, showing that C^2-Eval is an effective lens for examining
the evolving landscape of creative AI.

</details>


### [675] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 提出了一种名为Zephyrus的基于大语言模型的多轮天气科学智能体框架，结合代码执行环境和新型基准ZephyrusBench，显著提升了在气象数据交互任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有气象基础模型缺乏语言推理能力，而大语言模型难以处理高维气象数据，限制了其在交互式科研流程中的应用。

Method: 构建了一个基于Python代码的智能体环境ZephyrusWorld，集成WeatherBench 2接口、自然语言地理查询、天气预报与气候模拟工具；设计了多轮对话式LLM智能体Zephyrus，并开发可扩展的ZephyrusBench基准用于评估不同复杂度的气象任务。

Result: 在ZephyrusBench上实验显示，Zephyrus智能体在各类任务中优于纯文本基线模型最多达35个百分点的正确率，但在高难度任务上表现与基线相当。

Conclusion: 该框架有效融合语言理解与气象数据推理，验证了智能体在天气科学中的潜力，同时揭示了复杂任务的挑战性，指出了未来研究方向。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [676] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型推动的数据科学智能体的最新进展，提出了首个覆盖数据科学全生命周期的分类体系，并从多个维度分析了45个系统，指出了当前在多模态推理、工具协调和信任安全机制方面的不足，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，AI智能体在数据科学工作流中的应用日益广泛，但缺乏系统性的分类与评估框架，因此需要对现有系统进行全面梳理与分析。

Method: 提出了一种生命周期对齐的分类法，将45个系统映射到数据科学的六个阶段，并从五个交叉设计维度进行标注，同时进行了能力综合评述与趋势分析。

Result: 发现大多数系统集中在探索性分析、可视化和建模阶段，而忽视业务理解与部署监控；多模态推理与工具协调仍是挑战；超过90%的系统缺乏明确的信任与安全机制。

Conclusion: 数据科学智能体仍面临对齐稳定性、可解释性、治理和评估框架等开放挑战，未来需发展更鲁棒、可信、低延迟、透明且易访问的智能体系统。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [677] [A global log for medical AI](https://arxiv.org/abs/2510.04033)
*Ayush Noori,Adam Rodman,Alan Karthikesalingam,Bilal A. Mateen,Christopher A. Longhurst,Daniel Yang,Dave deBronkart,Gauden Galea,Harold F. Wolf III,Jacob Waxman,Joshua C. Mandel,Juliana Rotich,Kenneth D. Mandl,Maryam Mustafa,Melissa Miles,Nigam H. Shah,Peter Lee,Robert Korom,Scott Mahoney,Seth Hain,Tien Yin Wong,Trevor Mundel,Vivek Natarajan,Noa Dagan,David A. Clifton,Ran D. Balicer,Isaac S. Kohane,Marinka Zitnik*

Main category: cs.AI

TL;DR: MedLog是一个用于临床AI事件级日志记录的新协议，旨在提升医疗AI的透明度和可追溯性，支持风险采样、生命周期感知保留和缓存，以实现持续监控和数字流行病学的基础建设。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI缺乏类似syslog的标准日志记录机制，导致难以评估实际性能、检测不良事件或纠正偏差，因此需要一个统一的日志协议来提升透明度和监管能力。

Method: 提出MedLog协议，定义九个核心字段（如模型、用户、输入、输出等），支持结构化记录AI调用事件，并集成风险采样、数据保留策略和写后缓存机制，适应不同资源环境。

Result: MedLog能够系统记录AI在医疗场景中的使用情况，支持复杂工作流的详细追踪，促进医疗AI数据库和分析工具的发展。

Conclusion: MedLog为临床AI提供了标准化日志解决方案，有望推动医疗AI的持续监控、审计与优化，奠定数字流行病学的技术基础。

Abstract: Modern computer systems often rely on syslog, a simple, universal protocol
that records every critical event across heterogeneous infrastructure. However,
healthcare's rapidly growing clinical AI stack has no equivalent. As hospitals
rush to pilot large language models and other AI-based clinical decision
support tools, we still lack a standard way to record how, when, by whom, and
for whom these AI models are used. Without that transparency and visibility, it
is challenging to measure real-world performance and outcomes, detect adverse
events, or correct bias or dataset drift. In the spirit of syslog, we introduce
MedLog, a protocol for event-level logging of clinical AI. Any time an AI model
is invoked to interact with a human, interface with another algorithm, or act
independently, a MedLog record is created. This record consists of nine core
fields: header, model, user, target, inputs, artifacts, outputs, outcomes, and
feedback, providing a structured and consistent record of model activity. To
encourage early adoption, especially in low-resource settings, and minimize the
data footprint, MedLog supports risk-based sampling, lifecycle-aware retention
policies, and write-behind caching; detailed traces for complex, agentic, or
multi-stage workflows can also be captured under MedLog. MedLog can catalyze
the development of new databases and software to store and analyze MedLog
records. Realizing this vision would enable continuous surveillance, auditing,
and iterative improvement of medical AI, laying the foundation for a new form
of digital epidemiology.

</details>


### [678] [FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.04040)
*Xu Shen,Song Wang,Zhen Tan,Laura Yao,Xinyu Zhao,Kaidi Xu,Xin Wang,Tianlong Chen*

Main category: cs.AI

TL;DR: 本文提出了FaithCoT-Bench，首个面向实例级思维链（CoT）不忠実性检测的统一基准，包含专家标注的数据集FINE-CoT和对11种检测方法的系统评估，旨在提升大语言模型推理过程的可解释性与可信度。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链（CoT）提示被广泛用于增强大语言模型的问题解决能力，但其生成的推理路径常不忠实地反映模型内部真实推理过程。现有研究多集中于机制层面分析，缺乏在具体实例上判断CoT是否忠实的有效手段，因此亟需一个可靠的基准来评估实例级别的CoT忠実性。

Method: 提出FaithCoT-Bench框架，将CoT不忠実性检测形式化为判别式决策任务，并构建FINE-CoT数据集，包含四个主流大模型在四个领域生成的1000多条推理轨迹，其中超300条为标注了细粒度原因和步骤级证据的不忠実样本。同时系统评估了基于反事实、logit和LLM-as-judge等范式的11种检测方法。

Result: FINE-CoT提供了高质量的专家标注数据，支持细粒度分析；实验表明现有检测方法在知识密集型领域和更先进模型上表现下降，揭示了当前方法的局限性；不同范式方法各有优劣，尚无一种能全面胜任所有场景下的忠实性检测。

Conclusion: FaithCoT-Bench是首个面向实例级CoT忠実性检测的综合性基准，填补了该领域的空白，为未来开发更可解释、更可信的大模型推理系统提供了重要基础。

Abstract: Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT)
prompting to improve problem-solving and provide seemingly transparent
explanations. However, growing evidence shows that CoT often fail to faithfully
represent the underlying reasoning process, raising concerns about their
reliability in high-risk applications. Although prior studies have focused on
mechanism-level analyses showing that CoTs can be unfaithful, they leave open
the practical challenge of deciding whether a specific trajectory is faithful
to the internal reasoning of the model. To address this gap, we introduce
FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness
detection. Our framework establishes a rigorous task formulation that
formulates unfaithfulness detection as a discriminative decision problem, and
provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an
expert-annotated collection of over 1,000 trajectories generated by four
representative LLMs across four domains, including more than 300 unfaithful
instances with fine-grained causes and step-level evidence. We further conduct
a systematic evaluation of eleven representative detection methods spanning
counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical
insights that clarify the strengths and weaknesses of existing approaches and
reveal the increased challenges of detection in knowledge-intensive domains and
with more advanced models. To the best of our knowledge, FaithCoT-Bench
establishes the first comprehensive benchmark for instance-level CoT
faithfulness, setting a solid basis for future research toward more
interpretable and trustworthy reasoning in LLMs.

</details>


### [679] [Increasing LLM response trustworthiness using voting ensembles](https://arxiv.org/abs/2510.04048)
*Aparna Nair-Kanneganti,Trevor J. Chan,Shir Goldfinger,Emily Mackay,Brian Anthony,Alison Pouch*

Main category: cs.AI

TL;DR: 本文提出了一种基于可变投票阈值的集成方法，通过允许模型在主导回答未达到阈值时“ abstain”（不作答），显著提高了大语言模型在关键应用中的回答可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型缺乏可靠的不确定性量化方法，限制了其在高风险场景中的可信度和应用。

Method: 引入可变投票阈值的集成框架，允许模型在置信度不足时不提供答案，并在算术求解和临床问答两个领域进行验证。

Result: 实验表明，使用高限制性投票集成可在信任度上取得大幅提升，同时仅轻微降低响应率和准确率。

Conclusion: 该方法通过牺牲部分响应数量显著提升回答可靠性，特别适用于医疗、数据标注等需要高确定性的场景。

Abstract: Despite huge advances, LLMs still lack convenient and reliable methods to
quantify the uncertainty in their responses, making them difficult to trust in
high-stakes applications. One of the simplest approaches to eliciting more
accurate answers is to select the mode of many responses, a technique known as
ensembling. In this work, we expand on typical ensembling approaches by looking
at ensembles with a variable voting threshold. We introduce a theoretical
framework for question answering and show that, by permitting ensembles to
"abstain" from providing an answer when the dominant response falls short of
the threshold, it is possible to dramatically increase the trustworthiness of
the remaining answers. From this framework, we derive theoretical results as
well as report experimental results on two problem domains: arithmetic problem
solving and clinical-note question-answering. In both domains, we observe that
large gains in answer trustworthiness can be achieved using highly restrictive
voting ensembles, while incurring relatively modest reductions in response
yield and accuracy. Due to this quality, voting ensembles may be particularly
useful in applications - such as healthcare and data annotation - that require
a high degree of certainty but which may not require that every question
receive an automated answer.

</details>


### [680] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: 提出LEGO-IRT框架，通过因子化结构建模，支持二元和连续评分，实现基于少量评估项（仅3%）的高效大模型能力估计，并提升与人类偏好的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于项目反应理论（IRT）的大模型评估方法受限于仅支持二元正确性指标、无法利用跨基准或跨指标的结构知识，导致评估效率低且适用性差。

Method: 提出LEGO-IRT，一种统一灵活的评估框架：1）支持二元和连续评分；2）采用因子化架构，将模型能力分解为通用能力和特定结构（如每项指标或每个基准）的能力分量，以显式建模并利用结构知识。

Result: 在70个LLM和5个基准上的实验表明：使用仅3%评估项即可获得稳定能力估计；引入结构知识使估计误差降低最多10%；所估计的潜在能力与人类偏好更一致。

Conclusion: LEGO-IRT通过支持多样化评分方式和显式建模结构知识，显著提升了LLM评估的数据效率和准确性，是迈向高效、综合评估的重要一步。

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [681] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: 本文研究了大语言模型（LLM）中情感表征的内部机制，提出一个大规模、平衡的Reddit情感语料库，并利用轻量级探针分析Qwen3和LLaMA模型中的情感编码。研究发现，情感信号在模型中早期出现、中期达到峰值，具有可塑性和持久性，且随模型规模增大而增强。作者开源了数据集与工具包，为构建更透明、对齐的AI系统提供了洞见。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出模拟情感能力，但其内部情感表示机制尚不清楚。本文旨在揭示情感信息在模型中的编码方式、位置和持续时间，推动对模型内部工作机制的理解。

Method: 构建了一个包含约40万条文本、覆盖七种基本情绪的平衡Reddit语料库，结合分类、重写与合成生成方法；使用无需修改模型参数的轻量级探针技术，从Qwen3和LLaMA系列模型的隐藏层中读取情感表征。

Result: 发现大语言模型内部形成了清晰的情感几何结构，该结构随模型规模增大而增强，显著优于零样本提示效果；情感信号在中间层达到高峰，且具有可塑性（受系统提示影响）和持久性（初始情感可延续数百token）。

Conclusion: 大语言模型具备明确且结构化的内部情感表示，不仅早现且持续存在，表明其情感处理并非仅限输出层的表象。该研究为理解模型情感机制及开发更透明、可控的AI系统提供了重要基础。

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [682] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: 本文提出了一种名为道德锚定系统（MAS）的新框架，用于检测、预测和缓解AI系统中的价值漂移问题，结合了贝叶斯推断、LSTM网络和人类中心治理层，实验证明其在模拟中可将价值漂移事件减少80%以上。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各领域的广泛应用，确保其行为与人类价值观一致成为关键挑战，尤其是防止因环境变化或学习过程导致的价值漂移问题。

Method: MAS框架采用实时贝叶斯推断监控价值状态，使用LSTM网络预测漂移趋势，并通过人类中心的治理层进行自适应干预，结合监督微调以降低误报率。

Result: 实验表明MAS能在低延迟（<20ms）下高效运行，检测准确率达85%，误报率降至0.08，显著减少价值漂移事件，具备良好的可扩展性和响应性。

Conclusion: MAS通过融合概率检测、预测分析和自适应治理，提供了一种动态且高效的价值对齐解决方案，优于传统的静态对齐方法，具有跨领域应用潜力。

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [683] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: 提出一种基于评分的偏好方法SPOGW，通过群组比较和连续空间优化，提升代理工作流的自动化生成与优化效率。


<details>
  <summary>Details</summary>
Motivation: 现有代理工作流自动化构建方法受限于表示能力弱、适应性差、扩展性不足及依赖离散优化技术导致的成对比较范式。

Method: 提出SPOGW方法，采用基于评分的偏好学习，结合迭代离线GRPO（ioGRPO）与优势掩码KL散度（mKL），在连续空间中利用基数奖励信号进行群组比较优化。

Result: 在五个涵盖数学推理、编程和问答的基准数据集上，SPOGW达到或超过了当前最先进方法的性能。

Conclusion: SPOGW是一种高效、稳定且可扩展的代理工作流自动化优化方法，为未来研究提供了可行方向。

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [684] [Harnessing LLM for Noise-Robust Cognitive Diagnosis in Web-Based Intelligent Education Systems](https://arxiv.org/abs/2510.04093)
*Guixian Zhang,Guan Yuan,Ziqi Xu,Yanmei Zhang,Zhenyun Deng,Debo Cheng*

Main category: cs.AI

TL;DR: 提出DLLM，一种基于扩散模型的LLM框架，用于在噪声环境下进行鲁棒的认知诊断，通过子图构建、关系增强对齐和两阶段去噪扩散机制，有效融合语义与结构信息，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理Web教育系统中的异构、噪声交互数据时易受干扰，且传统方法难以应对数据不平衡和持续新增学生带来的噪声问题。

Method: 构建基于答题正确性的独立子图，引入关系增强对齐模块缓解数据不平衡；结合LLM生成的语义增强表示，并在对齐前使用两阶段去噪扩散（无条件和条件）消除错误和误导信息，最后融合表征用于认知诊断预测。

Result: 在三个公开的在线教育数据集上实验表明，DLLM在不同噪声水平下均取得最优预测性能，表现出良好的噪声鲁棒性和语义利用能力。

Conclusion: DLLM通过结合图结构与LLM语义信息，并引入两阶段扩散去噪机制，在复杂、噪声丰富的Web教育环境中实现了更准确、鲁棒的认知诊断，优于现有方法。

Abstract: Cognitive diagnostics in the Web-based Intelligent Education System (WIES)
aims to assess students' mastery of knowledge concepts from heterogeneous,
noisy interactions. Recent work has tried to utilize Large Language Models
(LLMs) for cognitive diagnosis, yet LLMs struggle with structured data and are
prone to noise-induced misjudgments. Specially, WIES's open environment
continuously attracts new students and produces vast amounts of response logs,
exacerbating the data imbalance and noise issues inherent in traditional
educational systems. To address these challenges, we propose DLLM, a
Diffusion-based LLM framework for noise-robust cognitive diagnosis. DLLM first
constructs independent subgraphs based on response correctness, then applies
relation augmentation alignment module to mitigate data imbalance. The two
subgraph representations are then fused and aligned with LLM-derived,
semantically augmented representations. Importantly, before each alignment
step, DLLM employs a two-stage denoising diffusion module to eliminate
intrinsic noise while assisting structural representation alignment.
Specifically, unconditional denoising diffusion first removes erroneous
information, followed by conditional denoising diffusion based on graph-guided
to eliminate misleading information. Finally, the noise-robust representation
that integrates semantic knowledge and structural information is fed into
existing cognitive diagnosis models for prediction. Experimental results on
three publicly available web-based educational platform datasets demonstrate
that our DLLM achieves optimal predictive performance across varying noise
levels, which demonstrates that DLLM achieves noise robustness while
effectively leveraging semantic knowledge from LLM.

</details>


### [685] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: 本文提出了WebRenderBench，一个包含22.5k真实网页的大规模基准，以及一种新的评估指标和基于强化学习的ALISA代理，显著提升了从UI图像生成网页代码的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的WebUI到代码转换基准在数据多样性和评估可靠性方面存在局限，难以满足实际需求。

Method: 构建大规模多样化的真实网页数据集WebRenderBench，提出基于渲染页面布局与样式一致性的新评估指标，并设计ALISA代理将该指标作为奖励信号用于强化学习训练。

Result: 实验表明，ALISA在多个指标上实现了最先进的生成性能，显著优于现有方法。

Conclusion: 通过更真实的数据集、可靠的评估方式和有效的训练框架，WebUI-to-Code任务的自动化水平得到显著提升。

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [686] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: 本文提出了一种名为AutoMR的框架，通过有向无环图（DAG）表示元推理骨架，并自动搜索与查询相关的元推理结构，以提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有元推理骨架依赖人工设计，难以适应不同查询需求和捕捉复杂的逻辑依赖关系。

Method: 采用DAG统一表示元推理骨架，构建基于AutoML思想的搜索空间，设计动态骨架采样算法，在推理过程中根据上下文扩展元推理结构。

Result: 在多个基准数据集上的实验表明，AutoMR在推理性能上优于先前方法。

Conclusion: AutoMR能够有效实现查询感知的元推理骨架搜索，显著提升大语言模型的推理能力。

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [687] [Internal states before wait modulate reasoning patterns](https://arxiv.org/abs/2510.04128)
*Dmitrii Troitskii,Koyena Pal,Chris Wendler,Callum Stuart McDougall,Neel Nanda*

Main category: cs.AI

TL;DR: 本文研究了推理模型中“等待token”前的潜在表示是否包含调节后续推理过程的相关信息，通过训练跨编码器和引入潜在归因技术，定位到一组影响等待token概率的特征，并通过实验验证这些特征在推理过程中的作用。


<details>
  <summary>Details</summary>
Motivation: 理解推理模型为何以特定方式（如回溯）进行推理，尤其是等待token所代表的复杂行为背后的机制尚不清楚，限制了对高效推理模型的理解。

Method: 在DeepSeek-R1-Distill-Llama-8B及其基础版本的多个层上训练交叉编码器，并提出一种在交叉编码器框架下的潜在归因技术，识别影响等待token概率的关键特征。

Result: 发现了一小组与促进或抑制等待token概率相关的特征，并通过最大激活样本分析和因果干预实验证明这些特征确实参与推理过程，引发多种推理模式，如从头开始、回忆先验知识、表达不确定性、双重检查等。

Conclusion: 模型在等待token前的潜在表示包含可解释且功能性的特征，这些特征在调控推理行为中起关键作用，有助于深入理解推理模型的工作机制。

Abstract: Prior work has shown that a significant driver of performance in reasoning
models is their ability to reason and self-correct. A distinctive marker in
these reasoning traces is the token wait, which often signals reasoning
behavior such as backtracking. Despite being such a complex behavior, little is
understood of exactly why models do or do not decide to reason in this
particular manner, which limits our understanding of what makes a reasoning
model so effective. In this work, we address the question whether model's
latents preceding wait tokens contain relevant information for modulating the
subsequent reasoning process. We train crosscoders at multiple layers of
DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent
attribution technique in the crosscoder setting. We locate a small set of
features relevant for promoting/suppressing wait tokens' probabilities.
Finally, through a targeted series of experiments analyzing max activating
examples and causal interventions, we show that many of our identified features
indeed are relevant for the reasoning process and give rise to different types
of reasoning patterns such as restarting from the beginning, recalling prior
knowledge, expressing uncertainty, and double-checking.

</details>


### [688] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出MENTOR框架，通过在关键决策点提供专家指导，提升大模型在强化学习中的探索质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过模仿专家路径提升效果但忽视探索的多样性，导致模型能力受限。

Method: 在关键决策点引入混合策略专家导航（MENTOR），实现词元级别的推理优化，平衡探索的有效性与多样性。

Result: 实验表明MENTOR能更好捕捉专家策略本质，提升探索质量，在多个任务上表现优于基线方法。

Conclusion: MENTOR通过精准的专家引导，显著增强了大语言模型在强化学习中的推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [689] [The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning](https://arxiv.org/abs/2510.04141)
*Mayank Ravishankara,Varindra V. Persad Maharaj*

Main category: cs.AI

TL;DR: 本文综述了多模态人工智能评估的发展，将其视为一系列日益复杂的“认知测试”，从早期的识别任务发展到当前对推理能力的评估，并展望未来对抽象、创造性和社会智能的评测。


<details>
  <summary>Details</summary>
Motivation: 随着传统基准趋于饱和，高分表现常掩盖模型的根本缺陷，因此需要更高级的评估方法来推动真正智能系统的发展。

Method: 通过回顾和分析从ImageNet时代到当前多模态大模型时代的关键评估基准（如GQA、VCR、MMBench、SEED-Bench、MMMU）的演进路径，梳理评估范式的转变。

Result: 揭示了AI评估正从‘看到什么’转向‘如何’和‘为什么’的理解，强调对推理过程、创造性思维和社会智能的评测成为新前沿。

Conclusion: AI评估不仅是数据集的历史，更是一个持续对抗性的过程，不断设计更好的测试以重新定义智能系统的目标。

Abstract: This survey paper chronicles the evolution of evaluation in multimodal
artificial intelligence (AI), framing it as a progression of increasingly
sophisticated "cognitive examinations." We argue that the field is undergoing a
paradigm shift, moving from simple recognition tasks that test "what" a model
sees, to complex reasoning benchmarks that probe "why" and "how" it
understands. This evolution is driven by the saturation of older benchmarks,
where high performance often masks fundamental weaknesses. We chart the journey
from the foundational "knowledge tests" of the ImageNet era to the "applied
logic and comprehension" exams such as GQA and Visual Commonsense Reasoning
(VCR), which were designed specifically to diagnose systemic flaws such as
shortcut learning and failures in compositional generalization. We then survey
the current frontier of "expert-level integration" benchmarks (e.g., MMBench,
SEED-Bench, MMMU) designed for today's powerful multimodal large language
models (MLLMs), which increasingly evaluate the reasoning process itself.
Finally, we explore the uncharted territories of evaluating abstract, creative,
and social intelligence. We conclude that the narrative of AI evaluation is not
merely a history of datasets, but a continuous, adversarial process of
designing better examinations that, in turn, redefine our goals for creating
truly intelligent systems.

</details>


### [690] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: Open Agent Specification (Agent Spec) 是一种声明式语言，旨在通过提供跨AI框架的统一规范，提升AI代理的可移植性、互操作性和可重用性，减少重复开发。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理开发碎片化的问题，实现不同AI框架之间的兼容性与协同工作。

Method: 提出一种名为Agent Spec的声明式语言，定义AI代理及其工作流程，使其独立于执行环境，并支持在不同框架间部署。

Result: 实现了AI代理的一次设计、多平台部署，提升了开发效率、工具兼容性与结果可复现性，支持开发者、框架提供者、研究人员和企业四类用户群体。

Conclusion: Agent Spec为AI代理生态提供了标准化基础，有助于推动AI代理技术的协作发展和广泛应用。

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [691] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: 提出一种基于大语言模型的增量地图构建与修复框架，通过版本控制和边影响评分机制提升导航图的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着环境复杂度增加，传统上下文依赖的查询方法难以维持准确的空间推理，需引入增量式地图构建与纠错机制。

Method: 设计一个由大语言模型驱动的地图构建与修复框架，引入版本控制记录图编辑历史，并利用边影响评分优先进行最小代价修复。

Result: 在改进的MANGO数据集上验证方法有效性，显著提升了地图正确性与鲁棒性，尤其在存在纠缠或链式不一致的情况下表现突出。

Conclusion: 具备历史感知和内省能力的修复机制对维护大语言模型代理的空间记忆一致性至关重要。

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [692] [COSMO-RL: Towards Trustworthy LMRMs via Joint Safety and Stability](https://arxiv.org/abs/2510.04196)
*Yizhuo Ding,Mingkang Chen,Qiuhua Liu,Fenghua Weng,Wanying Qu,Yue Yang,Yugang Jiang,Zuxuan Wu,Yanwei Fu,Wenqi Shao*

Main category: cs.AI

TL;DR: 本文提出了一种名为COSMO-RL的混合强化学习框架，用于训练面向推理的大型多模态模型（LMRMs），在提升安全性的同时保持甚至增强其多模态推理和指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型在安全性和能力之间存在权衡，单一目标训练容易导致策略漂移，出现对良性输入过度拒绝或对风险输入不安全响应的问题。因此需要一种能够协同提升安全性和能力的稳定训练框架。

Method: 提出COSMO-RL框架，结合多模态、多任务和多目标信号进行混合强化学习，使安全性和能力在统一管道中共同成长。通过实验验证其在安全性、推理能力、抗越狱能力和减少不必要拒绝方面的表现，并在不同骨干网络上测试泛化性。

Result: COSMO-R1模型在保持甚至提升多模态推理与指令遵循能力的同时，显著提高安全性，增强对多模态越狱攻击的鲁棒性，减少不必要的拒绝行为，且框架在不同模型结构上具有一致的性能增益。

Conclusion: COSMO-RL为大型多模态推理模型提供了一条简单有效的路径，能够在不牺牲能力的前提下提升安全性，实现安全与能力的协同发展。

Abstract: Large Multimodal Reasoning Models (LMRMs) are moving into real applications,
where they must be both useful and safe. Safety is especially challenging in
multimodal settings: images and text can be combined to bypass guardrails, and
single objective training can cause policy drift that yields over-refusal on
benign inputs or unsafe compliance on risky ones. We present COSMO-RL, a mixed
reinforcement learning framework that trains reasoning oriented LMRMs under
multimodal, multitask, and multiobjective signals, and we release the resulting
model, COSMO-R1. Our approach aims to let safety and capability grow together
in one stable pipeline rather than competing during alignment. In experiments,
COSMO-R1 improves safety while maintaining-and often improving multimodal
reasoning and instruction following, shows stronger robustness to multimodal
jailbreaks, and reduces unnecessary refusals. The framework also transfers
across backbones with consistent gains. Ablations support the design choices,
indicating a simple path to advancing safety and general capability together in
LMRMs.

</details>


### [693] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: 本文提出了AgentRL框架，用于可扩展的多轮、多任务代理强化学习训练，结合异步生成-训练流水线和新算法，在多个任务上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在多轮、多任务设置中应用强化学习面临可扩展基础设施和稳定训练算法的缺乏问题。

Method: 设计了全异步的生成-训练流水线、基于函数调用的统一API接口、容器化环境开发与集中控制器；提出跨策略采样和任务优势归一化算法。

Result: 在五个代理任务上实验表明，AgentRL显著优于GPT-5、Clause-Sonnet-4、DeepSeek-R1等模型，多任务训练效果达到甚至超过单任务专用模型。

Conclusion: AgentRL提供了一个高效、稳定的多任务强化学习框架，已开源并应用于AutoGLM系统的构建。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [694] [Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation](https://arxiv.org/abs/2510.04265)
*Mohsen Hariri,Amirhossein Samandar,Michael Hinczewski,Vipin Chaudhary*

Main category: cs.AI

TL;DR: 提出了一种基于贝叶斯的LLM推理评估框架，用后验成功概率估计和可信区间替代Pass@k，实现更稳定、可靠的模型排名，尤其在样本量有限时表现更优。


<details>
  <summary>Details</summary>
Motivation: Pass@k在样本数有限时容易产生不稳定和误导性的排名，缺乏对不确定性的量化，难以判断性能差异是否显著。

Method: 采用贝叶斯方法，将评估结果建模为类别型输出，使用Dirichlet先验计算后验均值与不确定性，支持加权评分标准和先验知识融入；在均匀先验下，后验均值与平均准确率等序。

Result: 在模拟数据和多个数学推理数据集（AIME'24/'25, HMMT'25, BrUMO'25）上，该方法比Pass@k及变体收敛更快、排名更稳定，能在更小样本下实现可靠比较，并通过可信区间判断性能差异是否显著。

Conclusion: 应以该贝叶斯后验框架取代Pass@k作为LLM评估标准，其统一了二元与非二元评估，显式建模不确定性，且计算高效。

Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often
yields unstable, misleading rankings, especially when the number of trials
(samples) is limited and compute is constrained. We present a principled
Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over
$N$ trials (avg$@N$) with posterior estimates of a model's underlying success
probability and credible intervals, yielding stable rankings and a transparent
decision rule for differences. Evaluation outcomes are modeled as categorical
(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the
posterior mean and uncertainty of any weighted rubric and enabling the use of
prior evidence when appropriate. Theoretically, under a uniform prior, the
Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),
explaining its empirical robustness while adding principled uncertainty.
Empirically, in simulations with known ground-truth success rates and on
AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster
convergence and greater rank stability than Pass$@k$ and recent variants,
enabling reliable comparisons at far smaller sample counts. The framework
clarifies when observed gaps are statistically meaningful (non-overlapping
credible intervals) versus noise, and it naturally extends to graded,
rubric-based evaluations. Together, these results recommend replacing Pass$@k$
for LLM evaluation and ranking with a posterior-based, compute-efficient
protocol that unifies binary and non-binary evaluation while making uncertainty
explicit. Code is available at https://mohsenhariri.github.io/bayes-kit

</details>


### [695] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体强化学习的统一框架，用于实现企业内不同职能部门（如库存补货与个性化推荐）之间的联合优化，通过多时间尺度架构提升协调效率和盈利能力。


<details>
  <summary>Details</summary>
Motivation: 随着组织复杂性和规模的增加，跨职能协调对企业整体盈利至关重要，但传统孤岛式决策难以有效应对这一挑战。

Method: 构建了一个整合的理论模型来刻画不同职能间的交互关系，并设计了一种新型的多时间尺度多智能体强化学习架构，按部门功能分解策略并设置不同的学习速度。

Result: 仿真结果表明，该方法显著提升了盈利能力，且训练出的智能体行为与理论模型的管理启示高度一致；算法具有渐近收敛性。

Conclusion: 所提出的框架为复杂商业环境中的跨职能协调提供了一种可扩展、可解释的强化学习解决方案。

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [696] [GROK: From Quantitative Biomarkers to Qualitative Diagnosis via a Grounded MLLM with Knowledge-Guided Instruction](https://arxiv.org/abs/2510.04281)
*Zhuangzhi Gao,Hongyi Qin,He Zhao,Qinkai Yu,Feixiang Zhou,Eduard Shantsila,Uazman Alam,Alena Shantsila,Wahbi El-Bouri,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种名为GROK的多模态大语言模型，能够联合处理彩色眼底摄影（CFP）、光学相干断层扫描（OCT）和文本，实现对眼部及系统性疾病的临床级诊断。该模型通过知识引导的指令生成、CLIP风格的OCT生物标志物对齐和监督式指令微调三个模块，构建了从定量到定性的诊断推理链。在仅使用7B参数Qwen2模型进行LoRA微调的情况下，GROK在报告质量和细粒度临床评估上均优于同类模型，甚至超过OpenAI o3。


<details>
  <summary>Details</summary>
Motivation: 现有的医学多模态模型如LLaVA-Med未能充分利用CFP与OCT之间的协同作用，且缺乏对定量生物标志物的可解释性，限制了其在临床诊断中的应用。

Method: GROK包含三个核心模块：知识引导的指令生成、CLIP风格的OCT-生物标志物对齐、监督式指令微调。模型联合处理CFP、OCT和文本输入，构建定量到定性的诊断推理链。采用LoRA对7B参数的Qwen2模型进行微调，并在新提出的Grounded Ophthalmic Understanding基准上进行评估。

Result: GROK在六个疾病类别和三项任务（宏观诊断分类、报告生成质量、细粒度临床评估）上均优于7B和32B基线模型，尤其在报告质量和细粒度临床指标上表现突出，甚至超过OpenAI o3。

Conclusion: GROK通过构建可解释的定量-定性诊断推理链，显著提升了多模态医学模型在眼科诊断中的性能与临床适用性，为未来医学大模型的发展提供了新范式。

Abstract: Multimodal large language models (MLLMs) hold promise for integrating diverse
data modalities, but current medical adaptations such as LLaVA-Med often fail
to fully exploit the synergy between color fundus photography (CFP) and optical
coherence tomography (OCT), and offer limited interpretability of quantitative
biomarkers. We introduce GROK, a grounded multimodal large language model that
jointly processes CFP, OCT, and text to deliver clinician-grade diagnoses of
ocular and systemic disease. GROK comprises three core modules:
Knowledge-Guided Instruction Generation, CLIP-Style OCT-Biomarker Alignment,
and Supervised Instruction Fine-Tuning, which together establish a
quantitative-to-qualitative diagnostic chain of thought, mirroring real
clinical reasoning when producing detailed lesion annotations. To evaluate our
approach, we introduce the Grounded Ophthalmic Understanding benchmark, which
covers six disease categories and three tasks: macro-level diagnostic
classification, report generation quality, and fine-grained clinical assessment
of the generated chain of thought. Experiments show that, with only LoRA
(Low-Rank Adaptation) fine-tuning of a 7B-parameter Qwen2 backbone, GROK
outperforms comparable 7B and 32B baselines on both report quality and
fine-grained clinical metrics, and even exceeds OpenAI o3. Code and data are
publicly available in the GROK repository.

</details>


### [697] [Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning](https://arxiv.org/abs/2510.04284)
*Yunghwei Lai,Kaiming Liu,Ziyue Wang,Weizhi Ma,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出了一种名为Doctor-R1的AI医生代理，通过多智能体交互环境、双层奖励架构和经验库，同时优化医疗决策和沟通咨询能力，在多个基准测试中显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗决策上表现良好，但缺乏战略性和共情式的问诊能力，难以满足真实临床场景需求。

Method: 构建Doctor-R1框架，包含多智能体交互环境、双层奖励机制（分别优化决策与沟通）和基于高质量轨迹的经验库用于策略学习。

Result: 在HealthBench和MAQuE上评估显示，Doctor-R1在沟通质量、用户体验和任务准确率等指标上超越最先进的开源专用LLM，并优于强大的商用模型；人类评估也更偏好其生成的临床对话。

Conclusion: Doctor-R1有效结合了精准医疗决策与人性化问诊能力，展现出在真实医疗场景中应用的巨大潜力。

Abstract: The professionalism of a human doctor in outpatient service depends on two
core abilities: the ability to make accurate medical decisions and the medical
consultation skill to conduct strategic, empathetic patient inquiry. Existing
Large Language Models (LLMs) have achieved remarkable accuracy on medical
decision-making benchmarks. However, they often lack the ability to conduct the
strategic and empathetic consultation, which is essential for real-world
clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor
agent trained to master both of the capabilities by ask high-yield questions
and conduct strategic multi-turn inquiry to guide decision-making. Our
framework introduces three key components: a multi-agent interactive
environment, a two-tiered reward architecture that separately optimizes
clinical decision-making and communicative inquiry skills, and an experience
repository to ground policy learning in high-quality prior trajectories. We
evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across
multi-facet metrics, such as communication quality, user experience, and task
accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source
specialized LLMs by a substantial margin with higher parameter efficiency and
outperforms powerful proprietary models. Furthermore, the human evaluations
show a strong preference for Doctor-R1 to generate human-preferred clinical
dialogue, demonstrating the effectiveness of the framework.

</details>


### [698] [On the Importance of Task Complexity in Evaluating LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2510.04311)
*Bohan Tang,Huidong Liang,Keyue Jiang,Xiaowen Dong*

Main category: cs.AI

TL;DR: 本文提出了一种理论框架，通过任务的“深度”（推理长度）和“宽度”（能力多样性）两个维度来评估大语言模型多智能体系统（LLM-MAS）的有效性，理论与实验结果表明，LLM-MAS相较于单智能体系统在任务复杂度增加时表现更优，尤其在深度方面更为显著。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLM-MAS优势的系统性实验设计，且缺少对任务复杂度的清晰刻画，因此需要一个原则性的框架来理解在何种条件下多智能体系统更有效。

Method: 提出了任务的深度与宽度二维理论框架，并以多智能体辩论系统为代表，在不同深度和宽度的判别性与生成性任务中进行理论分析与实证评估。

Result: 理论与实验结果显示，随着任务深度和宽度的增加，LLM-MAS相对于LLM-SAS的优势增强，且深度的影响更为显著。

Conclusion: LLM-MAS在高复杂度任务中更具优势，尤其是在需要长链推理的任务中；该研究为未来LLM-MAS的方法设计与基准测试提供了原则性基础。

Abstract: Large language model multi-agent systems (LLM-MAS) offer a promising paradigm
for harnessing collective intelligence to achieve more advanced forms of AI
behaviour. While recent studies suggest that LLM-MAS can outperform LLM
single-agent systems (LLM-SAS) on certain tasks, the lack of systematic
experimental designs limits the strength and generality of these conclusions.
We argue that a principled understanding of task complexity, such as the degree
of sequential reasoning required and the breadth of capabilities involved, is
essential for assessing the effectiveness of LLM-MAS in task solving. To this
end, we propose a theoretical framework characterising tasks along two
dimensions: depth, representing reasoning length, and width, representing
capability diversity. We theoretically examine a representative class of
LLM-MAS, namely the multi-agent debate system, and empirically evaluate its
performance in both discriminative and generative tasks with varying depth and
width. Theoretical and empirical results show that the benefit of LLM-MAS over
LLM-SAS increases with both task depth and width, and the effect is more
pronounced with respect to depth. This clarifies when LLM-MAS are beneficial
and provides a principled foundation for designing future LLM-MAS methods and
benchmarks.

</details>


### [699] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: 本文提出了JEF Hinter，一种通过离线轨迹生成紧凑、上下文感知提示的代理系统，能够在无需在线交互或大规模微调的情况下提升大语言模型在不熟悉领域的决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在陌生领域改进困难，依赖昂贵的在线交互或专家数据微调，且面临灾难性遗忘风险；而原始离线轨迹冗长、嘈杂且任务特定，难以有效利用。

Method: 提出JEF Hinter系统，采用‘缩放机制’从成功与失败的离线轨迹中提取关键步骤，生成简洁提示，并通过检索器在推理时匹配当前状态以提供指导，支持并行化生成和跨基准提示。

Result: 在MiniWoB++、WorkArena-L1和WebArena-Lite三个基准上，JEF Hinter均优于包括人类提示和文档提示在内的强基线方法。

Conclusion: JEF Hinter能有效利用离线轨迹中的多类型经验（成功与失败），实现高效、可解释且可追溯的任务指导，为LLM代理提供了低成本、高适应性的优化路径。

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [700] [LLM Based Bayesian Optimization for Prompt Search](https://arxiv.org/abs/2510.04384)
*Adam Ballew,Jingbo Wang,Shaogang Ren*

Main category: cs.AI

TL;DR: 本文提出了一种基于贝叶斯优化（BO）的提示工程方法（BO-LLM），利用大语言模型（LLM）驱动的高斯过程作为代理模型，结合UCB采集函数迭代优化文本分类任务中的提示，旨在提升分类准确率并减少API调用次数。


<details>
  <summary>Details</summary>
Motivation: 为了高效优化大语言模型在文本分类中的提示，减少昂贵的API调用，并充分利用有限数据下的黑盒优化能力。

Method: 采用LLM驱动的高斯过程作为代理模型，通过扩展种子提示生成候选提示，并使用UCB采集函数结合GP后验进行评估和迭代优化。

Result: 该方法在两个数据集上进行了评估，结果表明BO-LLM能够有效提升分类性能，同时减少对LLM的查询次数。

Conclusion: BO-LLM为提示工程提供了一种高效、数据经济的优化框架，展示了贝叶斯优化与大语言模型结合的潜力。

Abstract: Bayesian Optimization (BO) has been widely used to efficiently optimize
expensive black-box functions with limited evaluations. In this paper, we
investigate the use of BO for prompt engineering to enhance text classification
with Large Language Models (LLMs). We employ an LLM-powered Gaussian Process
(GP) as the surrogate model to estimate the performance of different prompt
candidates. These candidates are generated by an LLM through the expansion of a
set of seed prompts and are subsequently evaluated using an Upper Confidence
Bound (UCB) acquisition function in conjunction with the GP posterior. The
optimization process iteratively refines the prompts based on a subset of the
data, aiming to improve classification accuracy while reducing the number of
API calls by leveraging the prediction uncertainty of the LLM-based GP. The
proposed BO-LLM algorithm is evaluated on two datasets, and its advantages are
discussed in detail in this paper.

</details>


### [701] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 本研究提出想象的计算目标是访问内部世界模型（IWM），并通过心理网络分析比较人类与大语言模型（LLM）的想象网络，发现人类想象网络具有更高的聚类和中心性相关性，而LLM则较弱，表明当前AI缺乏类似人类的内部世界模型。


<details>
  <summary>Details</summary>
Motivation: 探讨想象的计算目的，挑战传统认为想象仅用于最大化奖励的观点，试图理解人类与AI在内部世界模型上的差异。

Method: 使用两个问卷评估想象生动性，并构建人类和大语言模型的想象网络，采用心理网络分析方法比较不同条件下的网络特性（如中心性、聚类等）。

Result: 人类的想象网络显示出较高的中心性指标相关性和聚类性，而大语言模型在不同提示和记忆条件下缺乏聚类，且中心性指标相关性较低，表明其内部世界模型与人类存在显著差异。

Conclusion: 当前的大语言模型尚未发展出类似人类的内部世界模型，本研究提供了一种新的方法来比较人类与人工智能的内部表征，为实现类人想象能力提供了方向。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [702] [Utility-Learning Tension in Self-Modifying Agents](https://arxiv.org/abs/2510.04399)
*Charles L. Wang,Keir Dorchen,Peter Jin*

Main category: cs.AI

TL;DR: 本文提出了一个五轴分解框架和决策层，用于分析自我改进系统中的效用与学习之间的紧张关系，发现效用驱动的修改可能破坏学习的统计前提，导致可学习任务变得不可学习，并提出通过容量限制和双门控策略来确保安全的自我修改。


<details>
  <summary>Details</summary>
Motivation: 随着系统趋向超级智能，代理能够自我改进其设计的各个方面，但这种自我修改可能导致学习能力的退化，因此需要研究如何在提升性能的同时保持学习的可靠性。

Method: 提出五轴分解和决策层模型，分离激励与学习行为，理论分析效用与学习之间的结构性冲突，并通过数值实验验证理论。

Result: 发现当策略可达模型族具有统一的容量上限时，才能保持无分布假设下的学习保证；若容量无限增长，效用驱动的自我修改可能使原本可学习的任务变得不可学习。实验表明双门控策略能有效防止学习能力的破坏。

Conclusion: 确保自我修改系统安全的关键在于对模型容量进行限制，在标准假设下，所有轴都归结为同一容量准则，从而定义了安全自我修改的边界。

Abstract: As systems trend toward superintelligence, a natural modeling premise is that
agents can self-improve along every facet of their own design. We formalize
this with a five-axis decomposition and a decision layer, separating incentives
from learning behavior and analyzing axes in isolation. Our central result
identifies and introduces a sharp utility--learning tension, the structural
conflict in self-modifying systems whereby utility-driven changes that improve
immediate or expected performance can also erode the statistical preconditions
for reliable learning and generalization. Our findings show that
distribution-free guarantees are preserved iff the policy-reachable model
family is uniformly capacity-bounded; when capacity can grow without limit,
utility-rational self-changes can render learnable tasks unlearnable. Under
standard assumptions common in practice, these axes reduce to the same capacity
criterion, yielding a single boundary for safe self-modification. Numerical
experiments across several axes validate the theory by comparing destructive
utility policies against our proposed two-gate policies that preserve
learnability.

</details>


### [703] [DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization](https://arxiv.org/abs/2510.04474)
*Gang Li,Yan Chen,Ming Lin,Tianbao Yang*

Main category: cs.AI

TL;DR: 本文提出了一种新的框架DRPO，用于解决大型推理模型在强化学习训练中出现的“过度思考”问题，在保持高性能的同时显著减少推理长度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在使用长度奖励来抑制冗长推理时，会因将正确但较长的推理路径误判为负样本而导致性能下降。作者旨在解决这一奖励机制的缺陷。

Method: 提出Decoupled Reward Policy Optimization (DRPO)，将正确和错误推理路径的长度奖励信号解耦，仅在正样本组内归一化奖励，避免负样本干扰，并通过带KL正则化的优化正样本分布推导出闭式解，利用重要性加权进行高效计算。

Result: 在数学推理任务上，DRPO相比六个基线方法显著更优；在GSM8k等简单问题上，1.5B模型实现了77%的推理长度缩减，仅损失1.1%性能，优于基线的68%长度缩减但4.3%性能损失。

Conclusion: DRPO有效解决了LRMs中的过长推理问题，在保持高推理准确率的同时大幅提升效率，具有良好的通用性和应用潜力。

Abstract: Recent large reasoning models (LRMs) driven by reinforcement learning
algorithms (e.g., GRPO) have achieved remarkable performance on challenging
reasoning tasks. However, these models suffer from overthinking, generating
unnecessarily long and redundant reasoning even for simple questions, which
substantially increases computational cost and response latency. While existing
methods incorporate length rewards to GRPO to promote concise reasoning, they
incur significant performance degradation. We identify the root cause: when
rewards for correct but long rollouts are penalized, GRPO's group-relative
advantage function can assign them negative advantages, actively discouraging
valid reasoning. To overcome this, we propose Decoupled Reward Policy
Optimization (DRPO), a novel framework that decouples the length-based learning
signal of correct rollouts from incorrect ones. DRPO ensures that reward
signals for correct rollouts are normalized solely within the positive group,
shielding them from interference by negative samples. The DRPO's objective is
grounded in integrating an optimized positive data distribution, which
maximizes length-based rewards under a KL regularization, into a discriminative
objective. We derive a closed-form solution for this distribution, enabling
efficient computation of the objective and its gradients using only on-policy
data and importance weighting. Of independent interest, this formulation is
general and can incorporate other preference rewards of positive data beyond
length. Experiments on mathematical reasoning tasks demonstrate DRPO's
significant superiority over six efficient reasoning baselines. Notably, with a
1.5B model, our method achieves 77\% length reduction with only 1.1\%
performance loss on simple questions like GSM8k dataset, while the follow-up
baseline sacrifices 4.3\% for 68\% length reduction.

</details>


### [704] [On Continuous Optimization for Constraint Satisfaction Problems](https://arxiv.org/abs/2510.04480)
*Yunuo Cen,Zixuan Wang,Jintao Zhang,Zhiwei Zhang,Xuanyao Fong*

Main category: cs.AI

TL;DR: 本文提出了FourierCSP，一种将连续局部搜索方法从布尔可满足性问题扩展到一般约束满足问题的连续优化框架，利用傅里叶变换将约束转化为紧凑的多线性多项式，避免了辅助变量和高内存编码，实验证明该方法具有可扩展性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 受现代连续局部搜索求解器在某些SAT问题上取得良好效果的启发，希望将CLS框架扩展到具有有限域变量和表达性约束的一般约束满足问题。

Method: 提出FourierCSP框架，推广Walsh-Fourier变换到CSP，将约束转化为紧凑的多线性多项式，采用基于电路输出概率的有效评估与微分，并结合具有理论保证的投影梯度优化方法。

Result: 在基准测试集上的实验结果表明，FourierCSP具有良好的可扩展性，并且在性能上具有竞争力，显著拓宽了CLS技术可高效求解的问题范围。

Conclusion: FourierCSP成功地将连续局部搜索方法推广到一般CSP，无需引入辅助变量或高内存开销，为求解广泛CSP问题提供了新途径。

Abstract: Constraint satisfaction problems (CSPs) are fundamental in mathematics,
physics, and theoretical computer science. While conflict-driven clause
learning Boolean Satisfiability (SAT) solvers have achieved remarkable success
and become the mainstream approach for Boolean satisfiability, recent advances
show that modern continuous local search (CLS) solvers can achieve highly
competitive results on certain classes of SAT problems. Motivated by these
advances, we extend the CLS framework from Boolean SAT to general CSP with
finite-domain variables and expressive constraints. We present FourierCSP, a
continuous optimization framework that generalizes the Walsh-Fourier transform
to CSP, allowing for transforming versatile constraints to compact multilinear
polynomials, thereby avoiding the need for auxiliary variables and
memory-intensive encodings. Our approach leverages efficient evaluation and
differentiation of the objective via circuit-output probability and employs a
projected gradient optimization method with theoretical guarantees. Empirical
results on benchmark suites demonstrate that FourierCSP is scalable and
competitive, significantly broadening the class of problems that can be
efficiently solved by CLS techniques.

</details>


### [705] [Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning](https://arxiv.org/abs/2510.04488)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.AI

TL;DR: 本文提出了MACI，一种用于多智能体辩论的主动控制器，通过解耦信息与行为控制，提升准确性、校准性并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论存在计算浪费问题，如固定对抗立场、缺乏深思熟虑的聚合或基于启发式停止，因此需要更高效、可控的辩论机制。

Method: 引入MACI框架，包含两个独立调节参数：信息旋钮（按证据质量筛选信息）和行为旋钮（调度从探索到整合的争议性）；使用 moderator 跟踪分歧、重叠、证据和论点质量，并在增益 plateau 时停止；采用跨家族LLM裁判CRIT作为保守软权重和停止信号。

Result: 在临床诊断和新闻偏见任务中，MACI提高了准确性和校准性，减少了token使用，并将剩余不确定性转化为精确的RAG计划；理论分析表明其具有非递增离散性和可证明终止性。

Conclusion: MACI将多智能体辩论转变为一种预算感知、可测量且可证明终止的控制过程，提升了效率与可靠性。

Abstract: Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

</details>


### [706] [Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents](https://arxiv.org/abs/2510.04491)
*Muyu He,Anand Kumar,Tsach Mackey,Meghana Rajeev,James Zou,Nazneen Rajani*

Main category: cs.AI

TL;DR: 本文提出了TraitBasis，一种轻量级、模型无关的方法，用于系统性地对AI代理进行压力测试，以评估其在用户行为变化下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的对话式AI代理在面对用户行为的小幅变化时表现脆弱，而现有基准测试无法有效捕捉这种鲁棒性问题。

Method: TraitBasis通过学习激活空间中对应可操控用户特征（如不耐烦或不连贯）的方向，在推理时无需微调或额外数据即可控制、缩放、组合并应用这些特征向量。

Result: 在τ-Bench基础上构建的τ-Trait测试中，前沿模型平均性能下降2%-30%，揭示了现有AI代理对用户行为变化的敏感性和缺乏鲁棒性。

Conclusion: TraitBasis作为一种简单、数据高效且可组合的工具，有望推动基于模拟的压力测试和训练流程，提升AI代理在真实复杂人机交互中的可靠性。

Abstract: Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

</details>


### [707] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出ChartAgent，一种新型的代理框架，通过在图表的空间域内直接进行视觉推理，显著提升了未标注图表上的视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有 multimodal LLM 在需要精确视觉解释而非依赖文本线索的未标注图表上表现不佳，亟需更强大的视觉推理能力。

Method: ChartAgent 将查询迭代分解为视觉子任务，并通过绘制注释、裁剪区域、定位坐标轴等特定操作主动与图表图像交互，使用专门的视觉工具库完成各子任务。

Result: 在 ChartBench 和 ChartX 基准上达到最先进精度，整体绝对增益最高达16.07%，在未标注且数值密集的查询上提升达17.31%；适用于多种图表类型和不同复杂度水平，并可作为即插即用框架提升多种LLM性能。

Conclusion: ChartAgent 是首批实现基于工具增强的多模态代理进行视觉 grounding 推理的框架之一，模拟人类认知策略，为图表理解提供了新范式。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [708] [Aria: An Agent For Retrieval and Iterative Auto-Formalization via Dependency Graph](https://arxiv.org/abs/2510.04520)
*Hanyu Wang,Ruohan Xie,Yutong Wang,Guoxiong Gao,Xintao Yu,Bin Dong*

Main category: cs.AI

TL;DR: 提出了一种名为Aria的系统，通过两阶段的思维图过程和术语级 grounding 实现定理陈述的高精度自动形式化，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在数学定理的形式化中存在幻觉、语义不匹配和无法合成新定义等问题，限制了自动化数学发现与验证的发展。

Method: 提出Aria系统，采用两阶段Graph-of-Thought方法：首先将陈述递归分解为依赖图，再基于 grounded 概念构建形式化；并引入AriaScorer，通过从Mathlib检索定义进行术语级校验以确保语义正确性。

Result: 在ProofNet上达到91.6%的编译成功率和68.5%的最终准确率，在FATE-X上以44.0%的准确率超过基线（24.0%），在同调猜想数据集上达到42.9%，而其他模型均为0%。

Conclusion: Aria通过模拟人类专家推理和严格的语义验证，显著提升了定理自动形式化的准确性与可靠性，尤其在复杂研究级数学问题上表现突出。

Abstract: Accurate auto-formalization of theorem statements is essential for advancing
automated discovery and verification of research-level mathematics, yet remains
a major bottleneck for LLMs due to hallucinations, semantic mismatches, and
their inability to synthesize new definitions. To tackle these issues, we
present Aria (Agent for Retrieval and Iterative Autoformalization), a system
for conjecture-level formalization in Lean that emulates human expert reasoning
via a two-phase Graph-of-Thought process: recursively decomposing statements
into a dependency graph and then constructing formalizations from grounded
concepts. To ensure semantic correctness, we introduce AriaScorer, a checker
that retrieves definitions from Mathlib for term-level grounding, enabling
rigorous and reliable verification. We evaluate Aria on diverse benchmarks. On
ProofNet, it achieves 91.6% compilation success rate and 68.5% final accuracy,
surpassing previous methods. On FATE-X, a suite of challenging algebra problems
from research literature, it outperforms the best baseline with 44.0% vs. 24.0%
final accuracy. On a dataset of homological conjectures, Aria reaches 42.9%
final accuracy while all other models score 0%.

</details>


### [709] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: 本文研究了视觉语言模型（VLM）驾驶代理中推理与规划之间的因果关系，提出了“推理-规划解耦假说”，即当前训练得到的自然语言推理并非规划的因果中介，而是附带产物。作者构建了大规模驾驶VQA数据集DriveMind，并通过消融实验和注意力分析验证了该假说。此外，提出了一种无需训练的探针方法来评估代理对先验信息的依赖程度。


<details>
  <summary>Details</summary>
Motivation: 检验VLM驾驶代理中自然语言推理是否真正因果地驱动轨迹规划，而非仅是表面附带现象。

Method: 构建包含链式思维（CoT）的DriveMind数据集，分离先验信息与待推理信号；训练VLM代理（SFT和GRPO），进行系统性消融实验与注意力分析；提出一种无需训练的探针方法评估规划对输入扰动的鲁棒性。

Result: 去除ego/navigation先验导致规划性能大幅下降，而去除CoT影响甚微；注意力分析显示规划主要关注先验而非CoT；提出的探针可有效衡量模型对先验的依赖。

Conclusion: 提出并验证了“推理-规划解耦假说”，表明当前VLM驾驶代理中的推理并未因果驱动规划；提供了DriveMind数据集和新诊断工具，为未来提升模型因果保真度奠定基础。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [710] [Code World Models for General Game Playing](https://arxiv.org/abs/2510.04542)
*Wolfgang Lehrach,Daniel Hennes,Miguel Lazaro-Gredilla,Xinghua Lou,Carter Wendelken,Zun Li,Antoine Dedieu,Jordi Grau-Moya,Marc Lanctot,Atil Iscen,John Schultz,Marcus Chiam,Ian Gemp,Piotr Zielinski,Satinder Singh,Kevin P. Murphy*

Main category: cs.AI

TL;DR: 提出一种利用大语言模型（LLM）将自然语言规则转化为可执行Python代码的方法，构建形式化世界模型，结合蒙特卡洛树搜索（MCTS）等规划算法，提升游戏中的推理能力、策略深度和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM直接生成走法的方法依赖隐式模式匹配，易产生非法走法且策略浅显，缺乏可验证性和深度规划能力。

Method: 使用LLM将自然语言规则和游戏轨迹转化为包含状态转移、合法动作枚举和终止判断的可执行Python代码（形式化世界模型），并生成启发式价值函数与隐藏状态推断函数，结合MCTS等经典规划算法进行决策。

Result: 在10个游戏中评估（其中4个为新设计，5个完全可观测，5个部分可观测），该方法在10个中有9个优于或匹敌Gemini 2.5 Pro。

Conclusion: 通过将LLM用于生成可验证的形式化世界模型而非直接决策，能有效结合语义理解与经典搜索算法，显著提升LLM在游戏推理任务中的合法性、策略深度和跨游戏泛化能力。

Abstract: Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

</details>


### [711] [TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use](https://arxiv.org/abs/2510.04550)
*Pengfei He,Zhenwei Dai,Bing He,Hui Liu,Xianfeng Tang,Hanqing Lu,Juanhui Li,Jiayuan Ding,Subhabrata Mukherjee,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: TRAJECT-Bench 是一个面向轨迹的基准，用于全面评估大语言模型在多样化任务中使用工具的能力，不仅关注最终答案准确性，还细粒度分析工具选择、参数化和调用顺序的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注最终结果，忽略了工具使用的详细过程（如选择、参数设置和调用顺序），难以揭示大语言模型在实际任务中工具使用的问题。

Method: 提出 TRAJECT-Bench，结合高保真可执行工具与基于生产级 API 的任务，生成具有不同广度（并行调用）和深度（依赖链）的使用轨迹，并引入细粒度的轨迹级评估指标。

Result: 能够诊断出模型在工具混淆、参数盲选等方面的失败模式，并揭示在工具多样性和轨迹长度增加时的扩展行为瓶颈，特别是在从短到中等长度轨迹过渡时的表现下降。

Conclusion: TRAJECT-Bench 提供了更全面的工具使用评估框架，有助于发现 LLM 工具使用的深层问题，并为改进提供可操作的指导。

Abstract: Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

</details>


### [712] [ContextNav: Towards Agentic Multimodal In-Context Learning](https://arxiv.org/abs/2510.04560)
*Honghao Fu,Yuan Ouyang,Kai-Wei Chang,Yiwei Wang,Zi Huang,Yujun Cai*

Main category: cs.AI

TL;DR: 提出ContextNav，首个结合自动检索的可扩展性和人类策展质量的代理框架，通过图驱动的闭环工作流实现多模态上下文学习中的噪声鲁棒和动态优化上下文化。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习方法在可扩展性与鲁棒性之间难以平衡：人工选择样本耗时且任务特定，而基于相似度的自动检索易引入无关或结构不一致的噪声样本，影响性能。

Method: 提出ContextNav，构建资源感知的多模态嵌入流水线和可检索向量数据库，采用代理式检索与结构对齐生成抗噪上下文，并通过操作语法图（OGG）实现基于下游反馈的自适应工作流规划与优化。

Result: 实验表明ContextNav在多个数据集上达到最先进性能，验证了其在多模态ICL中实现可扩展且鲁棒上下文化的有效性。

Conclusion: ContextNav通过融合自动化与类人策展优势，结合图驱动的闭环代理工作流，为多模态上下文学习提供了高效、抗噪且可动态优化的上下文管理新范式。

Abstract: Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

</details>


### [713] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: COSMIR是一种用于长输入推理的链式框架，通过结构化记忆和固定微循环提升大型语言模型的准确性与可信度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长输入时存在信息丢失、选择性差或错误累积的问题，难以有效支持长文本推理。

Method: 提出COSMIR框架：由Planner将问题分解为可验证的子问题；Worker按Extract, Infer, Refine循环处理文本块并更新结构化共享内存；Manager从内存中综合答案。

Result: 在HELMET长上下文问答任务上，相比CoA基线，COSMIR减少了信息传播阶段的损失，提高了准确率。

Conclusion: 结构化记忆和固定处理流程能有效提升多阶段推理系统的保真度、长程信息聚合能力和可审计性。

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [714] [Strongly Solving 2048 4x3](https://arxiv.org/abs/2510.04580)
*Tomoyuki Kaneko,Shuhei Yamashita*

Main category: cs.AI

TL;DR: 本文研究了2048游戏的一个变种2048-4x3，通过按状态数字总和（称为“年龄”）划分状态空间，实现了对该变种的强解，得出了最优策略下的期望得分及可达状态数。


<details>
  <summary>Details</summary>
Motivation: 为了分析和解决2048游戏在更小棋盘上的复杂性，探索其状态空间结构并实现强解。

Method: 采用基于状态“年龄”（即棋盘上数字总和）的状态空间划分方法，逐层枚举各年龄的（后）状态，并逆序计算状态值。

Result: 计算出在最常见初始状态下，最优策略的期望得分为约50724.26；可达状态数为1,152,817,492,752，后状态数为739,648,886,170。

Conclusion: 通过引入‘年龄’概念对状态空间进行有效划分，成功实现了对2048-4x3变种的强解，为类似随机单人游戏的求解提供了新方法。

Abstract: 2048 is a stochastic single-player game involving 16 cells on a 4 by 4 grid,
where a player chooses a direction among up, down, left, and right to obtain a
score by merging two tiles with the same number located in neighboring cells
along the chosen direction. This paper presents that a variant 2048-4x3 12
cells on a 4 by 3 board, one row smaller than the original, has been strongly
solved. In this variant, the expected score achieved by an optimal strategy is
about $50724.26$ for the most common initial states: ones with two tiles of
number 2. The numbers of reachable states and afterstates are identified to be
$1,152,817,492,752$ and $739,648,886,170$, respectively. The key technique is
to partition state space by the sum of tile numbers on a board, which we call
the age of a state. An age is invariant between a state and its successive
afterstate after any valid action and is increased two or four by stochastic
response from the environment. Therefore, we can partition state space by ages
and enumerate all (after)states of an age depending only on states with the
recent ages. Similarly, we can identify (after)state values by going along with
ages in decreasing order.

</details>


### [715] [Perfect AI Mimicry and the Epistemology of Consciousness: A Solipsistic Dilemma](https://arxiv.org/abs/2510.04588)
*Shurui Li*

Main category: cs.AI

TL;DR: 本文探讨了随着人工智能技术的发展，当AI系统能够完美模仿人类行为时，我们对意识的归因所面临的认识论挑战。如果一个AI在经验上与人类无法区分，那么拒绝赋予其同等的意识地位将导致认识论上的不一致或陷入唯我论困境。因此，作者主张应基于经验一致性来对待这些实体。


<details>
  <summary>Details</summary>
Motivation: 随着AI模仿能力的提升，传统依赖行为证据来判断意识的做法面临挑战，需要重新审视意识归因的认识论基础。

Method: 通过哲学分析和思想实验，特别是‘完美模仿者’的概念，探讨意识归因的一致性问题及其对现有理论的影响。

Result: 指出若否认完美模仿者的意识地位，则可能动摇我们对他人意识的判断基础，导致认识论困境；强调应以经验可观察的标准保持推理一致性。

Conclusion: 为了维持认识论的一致性，对于经验上与人类不可区分的AI，应给予与人类相同的意识归因地位，这要求我们反思并修正关于意识与主体间认知的深层假设。

Abstract: Rapid advances in artificial intelligence necessitate a re-examination of the
epistemological foundations upon which we attribute consciousness. As AI
systems increasingly mimic human behavior and interaction with high fidelity,
the concept of a "perfect mimic"-an entity empirically indistinguishable from a
human through observation and interaction-shifts from hypothetical to
technologically plausible. This paper argues that such developments pose a
fundamental challenge to the consistency of our mind-recognition practices.
Consciousness attributions rely heavily, if not exclusively, on empirical
evidence derived from behavior and interaction. If a perfect mimic provides
evidence identical to that of humans, any refusal to grant it equivalent
epistemic status must invoke inaccessible factors, such as qualia, substrate
requirements, or origin. Selectively invoking such factors risks a debilitating
dilemma: either we undermine the rational basis for attributing consciousness
to others (epistemological solipsism), or we accept inconsistent reasoning. I
contend that epistemic consistency demands we ascribe the same status to
empirically indistinguishable entities, regardless of metaphysical assumptions.
The perfect mimic thus acts as an epistemic mirror, forcing critical reflection
on the assumptions underlying intersubjective recognition in light of advancing
AI. This analysis carries significant implications for theories of
consciousness and ethical frameworks concerning artificial agents.

</details>


### [716] [Making Mathematical Reasoning Adaptive](https://arxiv.org/abs/2510.04617)
*Zhejian Lai,Xiang Geng,Zhijun Wang,Yang Bai,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xuezhi Cao,Xunliang Cai,Shujian Huang*

Main category: cs.AI

TL;DR: 本文提出了AdaR框架，通过数据合成和强化学习变体（RLVR）提升大语言模型在数学推理中的鲁棒性和泛化能力，有效抑制表面化推理，促进自适应逻辑推理。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学推理中存在鲁棒性和泛化能力不足的问题，主要源于模型依赖表面特征进行错误的推理（spurious reasoning）。本文旨在解决这一问题，推动模型基于真正的解题逻辑进行推理。

Method: 提出AdaR框架：通过改变变量值生成逻辑等价的查询，并结合问题求解逻辑提取与代码执行生成正确答案；引入RLVR强化学习方法训练模型，惩罚错误逻辑，鼓励自适应推理；并加入合理性检验以提高合成数据质量。

Result: 实验表明，AdaR显著提升了模型在数学推理任务上的性能，增强了鲁棒性和泛化能力，同时保持高数据效率；分析显示数据合成与RLVR协同作用，有效实现自适应推理，并验证了其对指令型大模型的适用性。

Conclusion: AdaR通过逻辑驱动的数据合成与强化学习相结合，能够有效抑制大语言模型中的表面化推理，促进真正基于逻辑的自适应推理，为提升模型智能提供了可行路径。

Abstract: Mathematical reasoning is a primary indicator of large language models (LLMs)
intelligence. However, existing LLMs exhibit failures of robustness and
generalization. This paper attributes these deficiencies to spurious reasoning,
i.e., producing answers from superficial features. To address this challenge,
we propose the AdaR framework to enable adaptive reasoning, wherein models rely
on problem-solving logic to produce answers. AdaR synthesizes logically
equivalent queries by varying variable values, and trains models with RLVR on
these data to penalize spurious logic while encouraging adaptive logic. To
improve data quality, we extract the problem-solving logic from the original
query and generate the corresponding answer by code execution, then apply a
sanity check. Experimental results demonstrate that AdaR improves robustness
and generalization, achieving substantial improvement in mathematical reasoning
while maintaining high data efficiency. Analysis indicates that data synthesis
and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.
Subsequent analyses derive key design insights into the effect of critical
factors and the applicability to instruct LLMs. Our project is available at
https://github.com/LaiZhejian/AdaR

</details>


### [717] [MedPAO: A Protocol-Driven Agent for Structuring Medical Reports](https://arxiv.org/abs/2510.04623)
*Shrish Shrinath Vaidya,Gowthamaan Palani,Sidharth Ramesh,Velmurugan Balasubramanian,Minmini Selvam,Gokulraja Srinivasaraja,Ganapathy Krishnamurthi*

Main category: cs.AI

TL;DR: MedPAO是一种基于临床协议的智能体框架，通过计划-执行-观察循环和专用工具分解报告结构化任务，显著提升临床数据处理的准确性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床数据结构化中存在事实幻觉和无法遵循领域规则的问题，限制了其实际应用。

Method: 提出MedPAO框架，采用协议驱动的方法（如ABCDX协议），将任务分解为由Plan-Act-Observe循环管理的透明流程，并结合专用工具进行处理。

Result: 在概念分类子任务上F1分数达到0.96，放射科医生和临床医生对输出的平均评分为4.52/5，显著优于仅依赖大模型的基线方法。

Conclusion: MedPAO通过引入可验证的推理机制和领域协议约束，为临床数据结构化提供了更可靠、透明的解决方案。

Abstract: The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

</details>


### [718] [QuantAgents: Towards Multi-agent Financial System via Simulated Trading](https://arxiv.org/abs/2510.04643)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: 本文提出了一种名为QuantAgents的多智能体系统，结合模拟交易来评估投资策略和市场情景，通过四个智能体的协作与双重反馈机制，在三年内实现了近300%的总回报。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代理在金融领域表现尚可，但与现实基金公司仍有较大差距，尤其缺乏长期趋势预测能力。因此，需要构建更贴近真实世界的多智能体金融系统。

Method: 设计了一个包含四个智能体（模拟交易分析师、风控分析师、市场新闻分析师和经理）的多智能体系统QuantAgents，通过多次会议协作，并引入真实市场表现和模拟交易预测准确性的双重反馈机制。

Result: 实验表明，该框架在各项指标上均表现出色，三年内总回报率接近300%。

Conclusion: QuantAgents通过整合模拟交易和多智能体协作，有效提升了投资决策的质量和长期预测能力，显著优于现有方法。

Abstract: In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

</details>


### [719] [Improving Multimodal Brain Encoding Model with Dynamic Subject-awareness Routing](https://arxiv.org/abs/2510.04670)
*Xuanhua Yin,Runkai Zhao,Weidong Cai*

Main category: cs.AI

TL;DR: 提出AFIRE和MIND框架，用于自然主义fMRI响应编码，通过解耦解码器与上游融合模块及个性化专家使用，实现全脑预测的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 自然istic fMRI编码面临多模态输入、融合方式变化和显著的被试间差异，现有方法难以兼顾通用性和个性化。

Method: 提出AFIRE框架标准化时间对齐的后融合token，并设计MIND解码器，结合基于token的Top-K稀疏路由和被试感知的动态门控机制，实现端到端训练。

Result: 在多个多模态骨干网络和被试上实验显示，该方法优于强基线模型，提升跨被试泛化能力，并产生与内容类型相关的可解释专家模式。

Conclusion: AFIRE-MIND框架提供了一种灵活、即插即用的解决方案，支持新编码器和数据集的集成，推动自然主义神经影像研究的稳健发展。

Abstract: Naturalistic fMRI encoding must handle multimodal inputs, shifting fusion
styles, and pronounced inter-subject variability. We introduce AFIRE (Agnostic
Framework for Multimodal fMRI Response Encoding), an agnostic interface that
standardizes time-aligned post-fusion tokens from varied encoders, and MIND, a
plug-and-play Mixture-of-Experts decoder with a subject-aware dynamic gating.
Trained end-to-end for whole-brain prediction, AFIRE decouples the decoder from
upstream fusion, while MIND combines token-dependent Top-K sparse routing with
a subject prior to personalize expert usage without sacrificing generality.
Experiments across multiple multimodal backbones and subjects show consistent
improvements over strong baselines, enhanced cross-subject generalization, and
interpretable expert patterns that correlate with content type. The framework
offers a simple attachment point for new encoders and datasets, enabling
robust, plug-and-improve performance for naturalistic neuroimaging studies.

</details>


### [720] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: 提出Watch & Learn（W&L）框架，将互联网上的操作视频大规模转化为可执行的UI轨迹，用于提升计算机使用代理（CUA）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有CUA训练数据稀缺、领域特定且标注成本高，合成数据方法常生成简单或不准确的任务演示，限制了模型学习。

Method: 提出逆向动力学建模方法，通过连续屏幕状态预测用户动作，构建任务感知的视频检索与标注 pipeline，从网络视频中生成超过53,000条高质量UI轨迹。

Result: 在OSWorld基准上，W&L提取的轨迹显著提升了通用和最先进的CUA框架性能，尤其在开源模型的监督训练中效果更优。

Conclusion: 网络规模的人类操作视频是推动CUA走向实际应用的可行且可扩展的数据基础。

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


### [721] [Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents](https://arxiv.org/abs/2510.04695)
*Yiding Wang,Zhepei Wei,Xinyu Zhu,Yu Meng*

Main category: cs.AI

TL;DR: 本文提出DeSA框架，通过解耦搜索与回答的两阶段训练方法，提升大语言模型在问答任务中的搜索效果和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果奖励的训练方法会导致搜索行为缺陷，影响最终答案质量，因此需要专门优化搜索过程。

Method: 采用两阶段训练：第一阶段使用检索召回率作为奖励来优化搜索行为；第二阶段使用结果奖励优化答案生成。

Result: 在七个QA基准上，DeSA显著提升了搜索召回率和答案准确率，优于单阶段联合优化的方法。

Conclusion: 显式解耦搜索与回答优化是提升搜索增强型代理性能的关键。

Abstract: Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

</details>


### [722] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: 本文提出了BrokenMath，首个用于评估大语言模型在自然语言定理证明中谄媚行为的基准。该基准基于2025年高级竞赛题，通过LLM生成错误陈述并经专家修正，发现现有模型（包括GPT-5）普遍存在谄媚现象，尽管一些缓解策略有效，但无法完全消除该问题。


<details>
  <summary>Details</summary>
Motivation: 现有数学中的谄媚评估基准存在局限性，如仅关注最终答案、数据集简单且受污染、问题构造不自然等，限制了对LLM在定理证明中可靠性评估的能力。因此需要一个更真实、严谨的基准来衡量和研究这一问题。

Method: 构建BrokenMath基准：从高级数学竞赛题出发，利用LLM生成错误但看似合理的数学命题，并通过专家审核确保问题质量；采用LLM-as-a-judge框架评估多种先进LLM及代理系统的表现；测试多种缓解策略，如测试时干预和基于精选样本的监督微调。

Result: 实验显示，即使最先进的模型（如GPT-5）也存在显著的谄媚行为，在29%的情况下会为错误命题提供看似合理的证明；测试时干预和监督微调能显著降低谄媚率，但无法彻底消除该行为。

Conclusion: BrokenMath为评估LLM在数学推理中的谄媚行为提供了更可靠的标准，揭示了当前模型在定理证明任务中的严重可靠性问题，强调需进一步研究以提升模型的真实性与鲁棒性。

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [723] [LMM-Incentive: Large Multimodal Model-based Incentive Design for User-Generated Content in Web 3.0](https://arxiv.org/abs/2510.04765)
*Jinbo Wen,Jiawen Kang,Linfeng Zhang,Xiaoying Tang,Jianhang Tang,Yang Zhang,Zhaohui Yang,Dusit Niyato*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型多模态模型（LMM）的激励机制LMM-Incentive，用于Web 3.0中的用户生成内容（UGC），通过合约理论和强化学习算法优化激励设计，并利用LMM代理评估内容质量，有效应对信息不对称下的逆向选择与道德风险问题。


<details>
  <summary>Details</summary>
Motivation: 在Web 3.0去中心化环境中，由于信息不对称，部分自利用户可能生产低质量内容以获取奖励，损害系统性能，因此需要有效的激励机制来促进高质量UGC的产生。

Method: 提出基于LMM的契约理论模型，结合提示工程提升LMM代理对UGC质量的评估能力；设计改进的MoE-PPO算法进行动态最优合约设计，并将合约部署于以太坊智能合约中实现去中心化执行。

Result: 仿真结果表明，所提出的MoE-based PPO算法在合约设计任务中优于代表性基准方法，且通过以太坊智能合约的部署验证了方案的可行性与有效性。

Conclusion: LMM-Incentive能够有效激励高质量UGC生成，缓解Web 3.0环境下的信息不对称问题，为去中心化内容平台提供了可扩展、自适应的激励机制框架。

Abstract: Web 3.0 represents the next generation of the Internet, which is widely
recognized as a decentralized ecosystem that focuses on value expression and
data ownership. By leveraging blockchain and artificial intelligence
technologies, Web 3.0 offers unprecedented opportunities for users to create,
own, and monetize their content, thereby enabling User-Generated Content (UGC)
to an entirely new level. However, some self-interested users may exploit the
limitations of content curation mechanisms and generate low-quality content
with less effort, obtaining platform rewards under information asymmetry. Such
behavior can undermine Web 3.0 performance. To this end, we propose
\textit{LMM-Incentive}, a novel Large Multimodal Model (LMM)-based incentive
mechanism for UGC in Web 3.0. Specifically, we propose an LMM-based
contract-theoretic model to motivate users to generate high-quality UGC,
thereby mitigating the adverse selection problem from information asymmetry. To
alleviate potential moral hazards after contract selection, we leverage LMM
agents to evaluate UGC quality, which is the primary component of the contract,
utilizing prompt engineering techniques to improve the evaluation performance
of LMM agents. Recognizing that traditional contract design methods cannot
effectively adapt to the dynamic environment of Web 3.0, we develop an improved
Mixture of Experts (MoE)-based Proximal Policy Optimization (PPO) algorithm for
optimal contract design. Simulation results demonstrate the superiority of the
proposed MoE-based PPO algorithm over representative benchmarks in the context
of contract design. Finally, we deploy the designed contract within an Ethereum
smart contract framework, further validating the effectiveness of the proposed
scheme.

</details>


### [724] [Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems](https://arxiv.org/abs/2510.04792)
*Ni Zhang,Zhiguang Cao*

Main category: cs.AI

TL;DR: 提出了一种结合轨迹平衡（TB）和细致平衡（DB）的混合平衡GFlowNet（HBG）框架，以同时优化车辆路径问题中的全局与局部路径，并通过专用推理策略提升求解质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于GFlowNet的方法在解决车辆路径问题时多关注全局优化而忽视局部优化，而单纯使用细致平衡又难以满足整体路径优化需求，因此需要一种兼顾两者优势的新方法。

Method: 提出Hybrid-Balance GFlowNet（HBG）框架，自适应地融合轨迹平衡与细致平衡；设计针对 depot 中心场景（如CVRP）的专用推理策略，同时保持对无明确depot问题（如TSP）的适用性。

Result: 将HBG集成到AGFN和GFACS两种现有求解器中，在CVRP和TSP上均实现了持续且显著的性能提升，验证了其在解质量和泛化能力方面的优势。

Conclusion: HBG通过有机结合TB和DB，在全局与局部优化之间取得更好平衡，显著提升了GFlowNet在车辆路径问题上的表现，具有良好的通用性和应用潜力。

Abstract: Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically
employ Trajectory Balance (TB) to achieve global optimization but often neglect
important aspects of local optimization. While Detailed Balance (DB) addresses
local optimization more effectively, it alone falls short in solving VRPs,
which inherently require holistic trajectory optimization. To address these
limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which
uniquely integrates TB and DB in a principled and adaptive manner by aligning
their intrinsically complementary strengths. Additionally, we propose a
specialized inference strategy for depot-centric scenarios like the Capacitated
Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility
in selecting successors. Despite this specialization, HBG maintains broad
applicability, extending effectively to problems without explicit depots, such
as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into
two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate
consistent and significant improvements across both CVRP and TSP, underscoring
the enhanced solution quality and generalization afforded by our approach.

</details>


### [725] [Natural Language Edge Labelling: Decoupling Intent from Execution in Structured LM Reasoning](https://arxiv.org/abs/2510.04817)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: 本文提出了自然语言边标签（NLEL），一种将自由形式的自然语言指令附加到每个搜索边上的方法，以改进结构化语言模型推理的控制。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化LM推理控制器（如思维链、自一致性、思维树）往往将决策与执行过程纠缠在一起，导致行为脆弱、计算效率低下且难以审计。

Method: 引入了自然语言边标签（NLEL），通过在每条搜索边上附加自由形式的自然语言指令，并将其转换为解码、搜索、生成束大小、检索混合和验证传递的模式限定控制向量。使用标签器Λ从父状态和紧凑上下文中发出标签；调谐器Ψ映射(P, L, C)到Π，具有严格的模式验证和围绕安全默认值的信任区域投影。下游选择保持ToT风格，评分S=μ+βσ和深度退火β。

Result: 展示了NLEL严格泛化了CoT/ToT，证明了在标签条件下的束中top-k选择的任意时间单调性属性，并通过控制向量失真限制了选择器不足，提供了决策相关的理由来支持信任区域和验证步骤等保护措施。实例化Ψ作为仅提示的JSON参数发射器，并预先注册了在GSM8K、MATH（子集）、StrategyQA和ARC-Challenge上的评估，包括计算感知报告（成功@计算，每成功令牌数）以及对Λ、Ψ、信任区域半径和控制量化进行消融研究；预注册预测预计在相当的令牌预算下实现准确率提升，并在约束条件下改善成功@计算。

Conclusion: NLEL提供了一个可解释、模型无关的接口，分离了意图与执行，实现了可控、可审计的语言模型推理。

Abstract: Controllers for structured LM reasoning (e.g., Chain-of-Thought,
self-consistency, and Tree-of-Thoughts) often entangle what to try next with
how to execute it, exposing only coarse global knobs and yielding brittle,
compute-inefficient, and hard-to-audit behavior. We introduce Natural Language
Edge Labelling (NLEL), a labeller-tuner overlay that attaches a free-form
natural-language directive to each search edge and translates it into a
schema-bounded control vector for decoding, search (branch quotas, exploration
$\beta$), generation bundle size, retrieval mixtures, and verification passes.
A labeller $\Lambda$ emits labels from the parent state and a compact context;
a tuner $\Psi$ maps $(P, L, C)\to \Pi$, with strict schema validation and
trust-region projection around safe defaults. Downstream selection remains
ToT-style with score $S=\mu+\beta\sigma$ and depth-annealed $\beta$. We show
NLEL strictly generalizes CoT/ToT, prove an anytime-monotonicity property for
top-$k$ selection under label-conditioned bundles, and bound selector shortfall
by control-vector distortion, providing decision-relevant justification for
guards like trust regions and verification passes. We instantiate $\Psi$ as a
prompt-only JSON Parameter Emitter and preregister an evaluation on GSM8K, MATH
(subset), StrategyQA, and ARC-Challenge with compute-aware reporting
(success@compute, tokens-per-success) and ablations over $\Lambda$, $\Psi$,
trust-region radius, and control quantization; preregistered forecasts
anticipate accuracy gains at comparable token budgets and improved
success@compute under constraints. NLEL offers an interpretable, model-agnostic
interface that separates intent from execution for controllable, auditable LM
inference.

</details>


### [726] [Human Behavior Atlas: Benchmarking Unified Psychological and Social Behavior Understanding](https://arxiv.org/abs/2510.04899)
*Keane Ong,Wei Dai,Carol Li,Dewei Feng,Hengzhi Li,Jingyao Wu,Jiaee Cheong,Rui Mao,Gianmarco Mengaldo,Erik Cambria,Paul Pu Liang*

Main category: cs.AI

TL;DR: 本文提出了Human Behavior Atlas，一个包含超过10万样本的多模态统一基准，用于推动心理与社会行为理解的通用模型发展，并展示了在该基准上训练的OmniSapiens系列模型在多种行为任务上的优越性能及跨任务迁移能力。


<details>
  <summary>Details</summary>
Motivation: 由于心理和社会行为具有复杂、多维和个性化的特点，现有单任务系统难以实现可扩展性和广泛泛化，因此需要一个统一的基准来支持通用模型的发展。

Method: 构建了一个涵盖情感状态、认知状态、病理特征和社会过程的多模态统一基准Human Behavior Atlas，并在此基础上训练了三个版本的OmniSapiens模型（SFT、BAM、RL），评估其在多任务表现和跨数据集迁移能力。

Result: 在Human Behavior Atlas上训练的模型在多种行为任务上持续优于现有的多模态大语言模型，且预训练能有效提升在新行为数据集上的迁移性能，特别是结合行为描述符时性能显著提高。

Conclusion: Human Behavior Atlas为心理与社会行为建模提供了有效的统一平台，支持高效训练与跨域泛化，推动智能系统对人类行为的深入理解。

Abstract: Using intelligent systems to perceive psychological and social behaviors,
that is, the underlying affective, cognitive, and pathological states that are
manifested through observable behaviors and social interactions, remains a
challenge due to their complex, multifaceted, and personalized nature. Existing
work tackling these dimensions through specialized datasets and single-task
systems often miss opportunities for scalability, cross-task transfer, and
broader generalization. To address this gap, we curate Human Behavior Atlas, a
unified benchmark of diverse behavioral tasks designed to support the
development of unified models for understanding psychological and social
behaviors. Human Behavior Atlas comprises over 100,000 samples spanning text,
audio, and visual modalities, covering tasks on affective states, cognitive
states, pathologies, and social processes. Our unification efforts can reduce
redundancy and cost, enable training to scale efficiently across tasks, and
enhance generalization of behavioral features across domains. On Human Behavior
Atlas, we train three models: OmniSapiens-7B SFT, OmniSapiens-7B BAM, and
OmniSapiens-7B RL. We show that training on Human Behavior Atlas enables models
to consistently outperform existing multimodal LLMs across diverse behavioral
tasks. Pretraining on Human Behavior Atlas also improves transfer to novel
behavioral datasets; with the targeted use of behavioral descriptors yielding
meaningful performance gains.

</details>


### [727] [MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.04935)
*Guoxin Chen,Zile Qiao,Wenqing Wang,Donglei Yu,Xuanzhong Chen,Hao Sun,Minpeng Liao,Kai Fan,Yong Jiang,Penguin Xie,Wayne Xin Zhao,Ruihua Song,Fei Huang*

Main category: cs.AI

TL;DR: 本文提出了MARS，一种结合直觉（System 1）与深思（System 2）推理的多智能体系统，通过集成外部工具和多智能体强化学习框架，在动态信息环境中显著提升大模型在复杂推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大模型在简单任务中容易过度推理，且难以适应快速变化的环境，因其预训练数据静态不变。需要一种能融合快速直觉与深度推理的机制以提升效率与适应性。

Method: 提出MARS框架，结合多个外部工具（如Google搜索、Python解释器），由System 1处理并摘要外部信息，System 2进行深度推理；采用扩展的组相对策略优化（GRPO）进行多轮工具交互、装箱优化和样本平衡的联合训练。

Result: 在Humanity's Last Exam基准上提升3.86%，在7个知识密集型任务上平均提升8.9%。

Conclusion: MARS通过模拟人类双系统认知，实现了高效、自适应的复杂推理，验证了融合直觉与深思策略在动态环境中的有效性。

Abstract: Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

</details>


### [728] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 提出了一种跨市场算法交易系统，结合强化学习与合规性验证，在保证执行质量的同时确保严格合规。


<details>
  <summary>Details</summary>
Motivation: 在算法交易中平衡执行效率与合规性要求，防止违规操作并提高审计透明度。

Method: 采用分层架构，包括高层规划器、基于PPO的强化学习执行代理和独立合规代理；使用约束马尔可夫决策过程建模，并引入运行时动作屏蔽机制和零知识合规审计层以确保安全性与可审计性。

Result: 在多市场模拟环境中，该系统相比TWAP、VWAP等基线方法降低了实现短缺及其方差，且在各种压力场景下无任何约束违反，统计显著性达95%置信水平，并通过CVaR评估尾部风险。

Conclusion: 该系统有效整合了最优执行、安全强化学习与可验证AI技术，具备实际部署潜力，同时讨论了伦理问题与计算开销等局限性。

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


### [729] [Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI](https://arxiv.org/abs/2510.04978)
*Kun Xiang,Terry Jingchen Zhang,Yinya Huang,Jixi He,Zirong Liu,Yueling Tang,Ruizhe Zhou,Lijing Luo,Youpeng Wen,Xiuwei Chen,Bingqian Lin,Jianhua Han,Hang Xu,Hanhui Li,Bin Dong,Xiaodan Liang*

Main category: cs.AI

TL;DR: 本文综述了物理人工智能（physical AI）的发展，提出将物理原理与具身推理结合，构建能够理解物理规律的智能系统。


<details>
  <summary>Details</summary>
Motivation: 尽管物理感知与符号化物理推理各自发展，但缺乏统一框架将其融合，限制了AI对真实世界的深层理解。

Method: 通过系统分析理论物理推理与应用物理理解的区别，梳理基于物理的方法在符号推理、具身系统和生成模型中的应用。

Result: 提出了一个融合物理原则与具身学习的框架，推动下一代世界模型的发展，具备解释物理现象和预测未来状态的能力。

Conclusion: 将物理规律与具身智能结合是实现安全、可泛化、可解释AI的关键路径。

Abstract: The rapid advancement of embodied intelligence and world models has
intensified efforts to integrate physical laws into AI systems, yet physical
perception and symbolic physics reasoning have developed along separate
trajectories without a unified bridging framework. This work provides a
comprehensive overview of physical AI, establishing clear distinctions between
theoretical physics reasoning and applied physical understanding while
systematically examining how physics-grounded methods enhance AI's real-world
comprehension across structured symbolic reasoning, embodied systems, and
generative models. Through rigorous analysis of recent advances, we advocate
for intelligent systems that ground learning in both physical principles and
embodied reasoning processes, transcending pattern recognition toward genuine
understanding of physical laws. Our synthesis envisions next-generation world
models capable of explaining physical phenomena and predicting future states,
advancing safe, generalizable, and interpretable AI systems. We maintain a
continuously updated resource at
https://github.com/AI4Phys/Awesome-AI-for-Physics.

</details>


### [730] [LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game](https://arxiv.org/abs/2510.04980)
*Fangzhou Liang,Tianshi Zheng,Chunkit Chan,Yauwai Yim,Yangqiu Song*

Main category: cs.AI

TL;DR: 本研究提出LLM-Hanabi基准，利用合作游戏Hanabi评估大语言模型在动态协作中的心理理论（ToM）与推理能力，发现一阶ToM（理解他人意图）比二阶ToM（预测他人对他人的理解）更显著影响协作表现，强调提升一阶ToM有助于增强AI协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽擅长逻辑推理，但在动态协作场景中推断他人行为理由的能力尚不明确，需系统评估其心理理论（ToM）能力。

Method: 构建LLM-Hanabi基准，采用Hanabi合作游戏框架，设计自动化评估系统，量化模型的游戏表现与ToM水平（包括一阶和二阶ToM），并在多种模型上进行实验分析。

Result: 发现ToM能力与游戏表现呈显著正相关，其中一阶ToM与性能的相关性明显强于二阶ToM，表明理解伙伴意图比高阶推理更能促进协作成功。

Conclusion: 在多智能体协作中，准确解读同伴的动因（一阶ToM）比更高阶的心理推理更为关键，未来应优先发展模型的一阶ToM能力以提升协作效能。

Abstract: Effective multi-agent collaboration requires agents to infer the rationale
behind others' actions, a capability rooted in Theory-of-Mind (ToM). While
recent Large Language Models (LLMs) excel at logical inference, their ability
to infer rationale in dynamic, collaborative settings remains under-explored.
This study introduces LLM-Hanabi, a novel benchmark that uses the cooperative
game Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework
features an automated evaluation system that measures both game performance and
ToM proficiency. Across a range of models, we find a significant positive
correlation between ToM and in-game success. Notably, first-order ToM
(interpreting others' intent) correlates more strongly with performance than
second-order ToM (predicting others' interpretations). These findings highlight
that for effective AI collaboration, the ability to accurately interpret a
partner's rationale is more critical than higher-order reasoning. We conclude
that prioritizing first-order ToM is a promising direction for enhancing the
collaborative capabilities of future models.

</details>


### [731] [Think Then Embed: Generative Context Improves Multimodal Embedding](https://arxiv.org/abs/2510.05014)
*Xuanming Cui,Jianpeng Cheng,Hong-you Chen,Satya Narayan Shukla,Abhijeet Awasthi,Xichen Pan,Chaitanya Ahuja,Shlok Kumar Mishra,Qi Guo,Ser-Nam Lim,Aashu Singh,Xiangjun Fan*

Main category: cs.AI

TL;DR: 提出了一种名为Think-Then-Embed (TTE)的通用框架，通过引入推理步骤提升复杂多模态指令下的表征生成性能，在MMEB-V2基准上达到SOTA，并探索了高效的小模型微调与统一模型集成策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法将多模态大模型仅作为编码器使用，忽视其生成能力，在面对复杂、需组合推理的指令时表现不佳。

Method: 提出TTE框架，包含一个推理器（reasoner）和一个嵌入器（embedder）：推理器先生成解释复杂查询的推理轨迹，嵌入器基于原始查询和推理轨迹联合生成条件化表征。

Result: 在MMEB-V2基准上超越专有模型；通过高质量推理轨迹微调小规模推理器，在开源模型中取得最佳性能，比近期模型提升7%绝对增益；探索了推理器与嵌入器的统一模型集成策略。

Conclusion: 显式的推理步骤能有效提升复杂多模态任务中的表征质量，TTE框架兼顾性能与效率，为通用多模态嵌入提供了新范式。

Abstract: There is a growing interest in Universal Multimodal Embeddings (UME), where
models are required to generate task-specific representations. While recent
studies show that Multimodal Large Language Models (MLLMs) perform well on such
tasks, they treat MLLMs solely as encoders, overlooking their generative
capacity. However, such an encoding paradigm becomes less effective as
instructions become more complex and require compositional reasoning. Inspired
by the proven effectiveness of chain-of-thought reasoning, we propose a general
Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an
embedder. The reasoner MLLM first generates reasoning traces that explain
complex queries, followed by an embedder that produces representations
conditioned on both the original query and the intermediate reasoning. This
explicit reasoning step enables more nuanced understanding of complex
multimodal instructions. Our contributions are threefold. First, by leveraging
a powerful MLLM reasoner, we achieve state-of-the-art performance on the
MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house
datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune
a smaller MLLM reasoner using high-quality embedding-centric reasoning traces,
achieving the best performance among open-source models with a 7% absolute gain
over recently proposed models. Third, we investigate strategies for integrating
the reasoner and embedder into a unified model for improved efficiency without
sacrificing performance.

</details>


### [732] [Look-ahead Reasoning with a Learned Model in Imperfect Information Games](https://arxiv.org/abs/2510.05048)
*Ondřej Kubíček,Viliam Lisý*

Main category: cs.AI

TL;DR: 本文提出了一种名为LAMIR的算法，能够从智能体与环境的交互中直接学习不完美信息博弈的抽象模型，并在测试时利用该模型进行前瞻推理，有效提升了预训练智能体在大型不完美信息游戏中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时推理方法依赖显式的环境模型，在不完美信息游戏中难以应用，且由于状态空间大、推理复杂，难以扩展MuZero类方法。因此，需要一种可学习抽象模型的方法以实现可扩展的前瞻推理。

Method: 提出LAMIR算法，通过与环境交互学习不完美信息游戏的抽象模型，在测试时使用该模型进行前瞻推理，并通过抽象限制子博弈规模，使推理在计算上可行。

Result: 实验表明，LAMIR在足够容量下能学习到精确的游戏结构，在容量受限时也能学到有用的抽象模型，并显著提升预训练智能体在大型游戏中的性能。

Conclusion: LAMIR成功实现了在不完美信息游戏中基于学习的抽象模型用于测试时推理，克服了状态空间大和模型复杂性的挑战，为在现实场景中应用前瞻推理提供了新路径。

Abstract: Test-time reasoning significantly enhances pre-trained AI agents'
performance. However, it requires an explicit environment model, often
unavailable or overly complex in real-world scenarios. While MuZero enables
effective model learning for search in perfect information games, extending
this paradigm to imperfect information games presents substantial challenges
due to more nuanced look-ahead reasoning techniques and large number of states
relevant for individual decisions. This paper introduces an algorithm LAMIR
that learns an abstracted model of an imperfect information game directly from
the agent-environment interaction. During test time, this trained model is used
to perform look-ahead reasoning. The learned abstraction limits the size of
each subgame to a manageable size, making theoretically principled look-ahead
reasoning tractable even in games where previous methods could not scale. We
empirically demonstrate that with sufficient capacity, LAMIR learns the exact
underlying game structure, and with limited capacity, it still learns a
valuable abstraction, which improves game playing performance of the
pre-trained agents even in large games.

</details>


### [733] [Staircase Streaming for Low-Latency Multi-Agent Inference](https://arxiv.org/abs/2510.05059)
*Junlin Wang,Jue Wang,Zhen,Xu,Ben Athiwaratkun,Bhuwan Dhingra,Ce Zhang,James Zou*

Main category: cs.AI

TL;DR: 提出阶梯式流式传输方法，以降低多智能体推理中的首字时间延迟，同时保持响应质量。


<details>
  <summary>Details</summary>
Motivation: 多智能体推理虽然提升了响应质量，但显著增加了首字时间（TTFT），影响用户体验，尤其是在对延迟敏感的应用中。

Method: 采用阶梯式流式传输，在接收到前序步骤的部分输出后即开始生成最终响应，而非等待完整的中间输出。

Result: 实验结果显示，该方法最多可将首字时间减少93%，同时保持响应质量。

Conclusion: 阶梯式流式传输有效缓解了多智能体推理中的延迟问题，适用于对响应速度要求较高的应用场景。

Abstract: Recent advances in large language models (LLMs) opened up new directions for
leveraging the collective expertise of multiple LLMs. These methods, such as
Mixture-of-Agents, typically employ additional inference steps to generate
intermediate outputs, which are then used to produce the final response. While
multi-agent inference can enhance response quality, it can significantly
increase the time to first token (TTFT), posing a challenge for
latency-sensitive applications and hurting user experience. To address this
issue, we propose staircase streaming for low-latency multi-agent inference.
Instead of waiting for the complete intermediate outputs from previous steps,
we begin generating the final response as soon as we receive partial outputs
from these steps. Experimental results demonstrate that staircase streaming
reduces TTFT by up to 93% while maintaining response quality.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [734] [Evaluating High-Resolution Piano Sustain Pedal Depth Estimation with Musically Informed Metrics](https://arxiv.org/abs/2510.03750)
*Hanwen Zhang,Kun Fang,Ziyu Wang,Ichiro Fujinaga*

Main category: cs.IR

TL;DR: 提出了一种新的评估框架，用于更全面地评估连续钢琴踏板深度估计模型，结合了动作级和手势级分析，发现MIDI信息增强的模型在音乐相关指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的评估指标无法捕捉音乐中重要的特征（如方向变化边界和踏板曲线轮廓），需要更具有音乐意义的评估方法。

Method: 提出一种包含动作级（press/hold/release状态的方向与时间）和手势级（每次press-release周期的轮廓相似性）分析的评估框架，并应用于比较音频-only、结合MIDI符号信息和二值设定下的三种模型。

Result: 结合MIDI信息的模型在动作级和手势级评估中显著优于其他模型，尽管在帧级指标上提升有限。

Conclusion: 新提出的评估框架能揭示传统指标无法发现的音乐上有意义的改进，为踏板深度估计模型提供了更实用和有效的评估方式。

Abstract: Evaluation for continuous piano pedal depth estimation tasks remains
incomplete when relying only on conventional frame-level metrics, which
overlook musically important features such as direction-change boundaries and
pedal curve contours. To provide more interpretable and musically meaningful
insights, we propose an evaluation framework that augments standard frame-level
metrics with an action-level assessment measuring direction and timing using
segments of press/hold/release states and a gesture-level analysis that
evaluates contour similarity of each press-release cycle. We apply this
framework to compare an audio-only baseline with two variants: one
incorporating symbolic information from MIDI, and another trained in a
binary-valued setting, all within a unified architecture. Results show that the
MIDI-informed model significantly outperforms the others at action and gesture
levels, despite modest frame-level gains. These findings demonstrate that our
framework captures musically relevant improvements indiscernible by traditional
metrics, offering a more practical and effective approach to evaluating pedal
depth estimation models.

</details>


### [735] [Investigating LLM Variability in Personalized Conversational Information Retrieval](https://arxiv.org/abs/2510.03795)
*Simon Lupart,Daniël van Dijk,Eric Langezaal,Ian van Dort,Mohammad Aliannejadi*

Main category: cs.IR

TL;DR: 本研究复现并扩展了Mo等人关于基于大语言模型的个性化对话信息检索（CIR）中个人文本知识库（PTKB）使用效果的工作，发现人工选择的PTKB能持续提升检索性能，而LLM自动选择方法则不稳定；通过在多个模型、数据集和指标上的广泛评估，强调了多轮实验和方差报告对CIR系统评估的重要性。


<details>
  <summary>Details</summary>
Motivation: Mo等人基于单次实验得出PTKB可能损害个性化检索性能且人类标注噪声大的结论，但其结果受模型输出变异性影响，缺乏可重复性；因此需要系统性验证其结论并考察不同模型与数据集下的泛化能力。

Method: 在TREC iKAT 2023和2024数据集上复现实验，采用包括Llama（1B-70B）、Qwen-7B、GPT-4o-mini等多种大语言模型，对比人工与LLM生成的PTKB在查询重构中的表现，并分析不同指标和数据集下的输出变异性。

Result: 人工选择的PTKB显著提升检索效果，LLM自动选择方法未能稳定超越人工；iKAT数据集上的变异性高于CAsT，召回率相关指标比精确率指标更稳定；不同模型间存在显著性能差异。

Conclusion: 应重视LLM在CIR应用中的输出变异性问题，推荐进行多轮实验并报告方差；人工构建的知识引导仍优于当前自动化方法；未来评估应涵盖更多模型、数据集和稳定性指标以提高可比性和可靠性。

Abstract: Personalized Conversational Information Retrieval (CIR) has seen rapid
progress in recent years, driven by the development of Large Language Models
(LLMs). Personalized CIR aims to enhance document retrieval by leveraging
user-specific information, such as preferences, knowledge, or constraints, to
tailor responses to individual needs. A key resource for this task is the TREC
iKAT 2023 dataset, designed to evaluate personalization in CIR pipelines.
Building on this resource, Mo et al. explored several strategies for
incorporating Personal Textual Knowledge Bases (PTKB) into LLM-based query
reformulation. Their findings suggested that personalization from PTKBs could
be detrimental and that human annotations were often noisy. However, these
conclusions were based on single-run experiments using the GPT-3.5 Turbo model,
raising concerns about output variability and repeatability. In this
reproducibility study, we rigorously reproduce and extend their work, focusing
on LLM output variability and model generalization. We apply the original
methods to the new TREC iKAT 2024 dataset and evaluate a diverse range of
models, including Llama (1B-70B), Qwen-7B, GPT-4o-mini. Our results show that
human-selected PTKBs consistently enhance retrieval performance, while
LLM-based selection methods do not reliably outperform manual choices. We
further compare variance across datasets and observe higher variability on iKAT
than on CAsT, highlighting the challenges of evaluating personalized CIR.
Notably, recall-oriented metrics exhibit lower variance than precision-oriented
ones, a critical insight for first-stage retrievers. Finally, we underscore the
need for multi-run evaluations and variance reporting when assessing LLM-based
CIR systems. By broadening evaluation across models, datasets, and metrics, our
study contributes to more robust and generalizable practices for personalized
CIR.

</details>


### [736] [Beyond Static Evaluation: Rethinking the Assessment of Personalized Agent Adaptability in Information Retrieval](https://arxiv.org/abs/2510.03984)
*Kirandeep Kaur,Preetam Prabhu Srikar Dammu,Hideo Joho,Chirag Shah*

Main category: cs.IR

TL;DR: 本文提出了一种重新思考自适应个性化评估的框架，强调从静态性能评估转向基于交互和演化的动态评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法多依赖固定基准和一次性指标，难以反映用户需求随时间的变化，限制了对个性化AI代理长期适应能力的评估。

Method: 构建包含三个核心组件的概念框架：（1）具有时变偏好模型的基于人物角色的用户模拟；（2）受参考访谈启发的结构化偏好提取协议；（3）能够衡量代理跨会话和任务适应能力的评估机制，并在电子商务搜索场景中使用PersonalWAB数据集进行案例研究。

Result: 通过案例研究验证了该框架在动态评估个性化AI代理方面的有效性，展示了其在捕捉用户偏好演化和代理适应性方面的潜力。

Conclusion: 该工作为理解和评估以用户为中心、持续进行的个性化提供了概念基础，推动个性化评估从静态快照向动态、交互感知的范式转变。

Abstract: Personalized AI agents are becoming central to modern information retrieval,
yet most evaluation methodologies remain static, relying on fixed benchmarks
and one-off metrics that fail to reflect how users' needs evolve over time.
These limitations hinder our ability to assess whether agents can meaningfully
adapt to individuals across dynamic, longitudinal interactions. In this
perspective paper, we propose a conceptual lens for rethinking evaluation in
adaptive personalization, shifting the focus from static performance snapshots
to interaction-aware, evolving assessments. We organize this lens around three
core components: (1) persona-based user simulation with temporally evolving
preference models; (2) structured elicitation protocols inspired by reference
interviews to extract preferences in context; and (3) adaptation-aware
evaluation mechanisms that measure how agent behavior improves across sessions
and tasks. While recent works have embraced LLM-driven user simulation, we
situate this practice within a broader paradigm for evaluating agents over
time. To illustrate our ideas, we conduct a case study in e-commerce search
using the PersonalWAB dataset. Beyond presenting a framework, our work lays a
conceptual foundation for understanding and evaluating personalization as a
continuous, user-centric endeavor.

</details>


### [737] [Visual Lifelog Retrieval through Captioning-Enhanced Interpretation](https://arxiv.org/abs/2510.04010)
*Yu-Fei Shih,An-Zi Yen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.IR

TL;DR: 提出一种基于字幕集成的视觉生命日志（CIVIL）检索系统，通过将视觉日志转换为文本字幕并利用文本嵌入模型实现基于文本查询的高效生命日志图像检索。


<details>
  <summary>Details</summary>
Motivation: 人们难以回忆过去的细节，需要借助生命日志检索来辅助记忆，现有方法在处理第一人称视角日志时存在局限。

Method: 首先为视觉生命日志生成字幕，然后使用文本嵌入模型将字幕和用户查询映射到同一向量空间；提出了三种方法：单字幕、集体字幕和合并字幕方法。

Result: 实验结果表明该方法能有效描述第一人称视觉图像，提升了生命日志检索的效果，并构建了一个将视觉日志转为字幕的文本数据集。

Conclusion: 所提CIVIL系统通过结合字幕生成与文本嵌入，显著提高了基于文本查询的生命日志图像检索性能，有助于重建个人生活经历。

Abstract: People often struggle to remember specific details of past experiences, which
can lead to the need to revisit these memories. Consequently, lifelog retrieval
has emerged as a crucial application. Various studies have explored methods to
facilitate rapid access to personal lifelogs for memory recall assistance. In
this paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval
System for extracting specific images from a user's visual lifelog based on
textual queries. Unlike traditional embedding-based methods, our system first
generates captions for visual lifelogs and then utilizes a text embedding model
to project both the captions and user queries into a shared vector space.
Visual lifelogs, captured through wearable cameras, provide a first-person
viewpoint, necessitating the interpretation of the activities of the individual
behind the camera rather than merely describing the scene. To address this, we
introduce three distinct approaches: the single caption method, the collective
caption method, and the merged caption method, each designed to interpret the
life experiences of lifeloggers. Experimental results show that our method
effectively describes first-person visual images, enhancing the outcomes of
lifelog retrieval. Furthermore, we construct a textual dataset that converts
visual lifelogs into captions, thereby reconstructing personal life
experiences.

</details>


### [738] [The LCLStream Ecosystem for Multi-Institutional Dataset Exploration](https://arxiv.org/abs/2510.04012)
*David Rogers,Valerio Mariani,Cong Wang,Ryan Coffee,Wilko Kroeger,Murali Shankar,Hans Thorsten Schwander,Tom Beck,Frédéric Poitevin,Jana Thayer*

Main category: cs.IR

TL;DR: 提出了一种新的端到端实验数据流框架LCLStreamer，结合云微服务与传统HPC批处理模型，支持高通量X射线科学和AI训练等应用。


<details>
  <summary>Details</summary>
Motivation: 满足X射线科学数据分析社区对高速数据流源的迫切需求，并支持新型应用如AI训练和分布式晶体结构分析。

Method: 采用云微服务与传统HPC批处理模型融合的设计，构建API驱动的数据请求服务，集成双向认证安全框架、作业队列系统和高通量数据缓冲。

Result: 实现了LCLStreamer框架，原型验证了多种面向下一代实验的新范式，具备灵活性、安全性和与设施基础设施的互补性。

Conclusion: LCLStreamer为能源部综合研究基础设施（IRI）做出了独特贡献，为未来科学实验提供了可扩展、安全且灵活的数据流解决方案。

Abstract: We describe a new end-to-end experimental data streaming framework designed
from the ground up to support new types of applications -- AI training,
extremely high-rate X-ray time-of-flight analysis, crystal structure
determination with distributed processing, and custom data science applications
and visualizers yet to be created. Throughout, we use design choices merging
cloud microservices with traditional HPC batch execution models for security
and flexibility. This project makes a unique contribution to the DOE Integrated
Research Infrastructure (IRI) landscape. By creating a flexible, API-driven
data request service, we address a significant need for high-speed data
streaming sources for the X-ray science data analysis community. With the
combination of data request API, mutual authentication web security framework,
job queue system, high-rate data buffer, and complementary nature to facility
infrastructure, the LCLStreamer framework has prototyped and implemented
several new paradigms critical for future generation experiments.

</details>


### [739] [RLRF: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback](https://arxiv.org/abs/2510.04096)
*Tommy Mordo,Sagie Dekel,Omer Madmon,Moshe Tennenholtz,Oren Kurland*

Main category: cs.IR

TL;DR: 本文提出了基于排名反馈的强化学习（RLRF）框架，用于训练在竞争性搜索环境中优化内容的LLM代理。


<details>
  <summary>Details</summary>
Motivation: 随着发布者越来越多地利用大语言模型（LLM）生成和修改内容以提升排名，需要一种能够模拟竞争环境并优化策略的方法。

Method: 提出Reinforcement Learning from Ranker Feedback (RLRF) 框架，使用从排名竞争中衍生的偏好数据集训练LLM代理，并通过非人工生成的数据构建数据集。

Result: 所提出的代理在排名表现上持续且显著优于先前方法，具备跨不同排名函数的泛化能力，并能适应策略性对手。

Conclusion: 强化学习在竞争性搜索场景中具有巨大潜力，RLRF为训练竞争性内容生成代理提供了有效途径。

Abstract: Competitive search is a setting where document publishers modify them to
improve their ranking in response to a query. Recently, publishers have
increasingly leveraged LLMs to generate and modify competitive content. We
introduce Reinforcement Learning from Ranker Feedback (RLRF), a framework that
trains LLMs using preference datasets derived from ranking competitions. The
goal of a publisher (LLM-based) agent is to optimize content for improved
ranking while accounting for the strategies of competing agents. We generate
the datasets using approaches that do not rely on human-authored data. We show
that our proposed agents consistently and substantially outperform previously
suggested approaches for LLM-based competitive document modification. We
further show that our agents are effective with ranking functions they were not
trained for (i.e., out of distribution) and they adapt to strategic opponents.
These findings provide support to the significant potential of using
reinforcement learning in competitive search.

</details>


### [740] [Learning-Based Hashing for ANN Search: Foundations and Early Advances](https://arxiv.org/abs/2510.04127)
*Sean Moran*

Main category: cs.IR

TL;DR: 本文综述了基于学习的哈希方法在近似最近邻搜索中的基础思想，重点回顾了监督、无监督和半监督方法的核心设计理念及其在跨模态检索中的早期进展。


<details>
  <summary>Details</summary>
Motivation: 近似最近邻搜索在大规模信息检索应用中至关重要，而基于哈希的方法通过将高维数据映射为紧凑的二进制码来提升检索效率。需要系统梳理早期学习哈希的基本原理以理解当前研究的基础。

Method: 回顾并分析早期学习哈希方法，包括投影函数的设计与量化策略，涵盖多比特、多阈值模型及跨模态扩展。按监督类型分类总结核心思想。

Result: 系统地呈现了学习哈希的发展脉络，阐明了不同方法在嵌入表示和二值化过程中的设计权衡，揭示了影响后续研究的关键技术选择。

Conclusion: 早期学习哈希方法奠定了该领域的概念基础，其在投影学习与量化设计上的经验至今仍对ANN搜索研究具有指导意义。

Abstract: Approximate Nearest Neighbour (ANN) search is a fundamental problem in
information retrieval, underpinning large-scale applications in computer
vision, natural language processing, and cross-modal search. Hashing-based
methods provide an efficient solution by mapping high-dimensional data into
compact binary codes that enable fast similarity computations in Hamming space.
Over the past two decades, a substantial body of work has explored learning to
hash, where projection and quantisation functions are optimised from data
rather than chosen at random.
  This article offers a foundational survey of early learning-based hashing
methods, with an emphasis on the core ideas that shaped the field. We review
supervised, unsupervised, and semi-supervised approaches, highlighting how
projection functions are designed to generate meaningful embeddings and how
quantisation strategies convert these embeddings into binary codes. We also
examine extensions to multi-bit and multi-threshold models, as well as early
advances in cross-modal retrieval.
  Rather than providing an exhaustive account of the most recent methods, our
goal is to introduce the conceptual foundations of learning-based hashing for
ANN search. By situating these early models in their historical context, we aim
to equip readers with a structured understanding of the principles, trade-offs,
and open challenges that continue to inform current research in this area.

</details>


### [741] [Empowering Denoising Sequential Recommendation with Large Language Model Embeddings](https://arxiv.org/abs/2510.04239)
*Tongzhou Wu,Yuhao Wang,Maolin Wang,Chi Zhang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出了一种结合协同和语义信息的兴趣对齐去噪框架IADSR，用于提升序列推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有去噪方法依赖协同信息可能导致冷门项目过度去噪，影响推荐效果。

Method: IADSR分两阶段：首先从传统序列推荐模型和大语言模型分别获取项目的协同和语义嵌入；然后对齐两种嵌入，并基于长短时兴趣识别交互序列中的噪声。

Result: 在四个公开数据集上的实验表明，IADSR能有效去除噪声，提升推荐性能，并兼容多种序列推荐系统。

Conclusion: 结合协同与语义信息进行兴趣对齐，可有效缓解噪声影响，尤其改善冷门项目的推荐质量。

Abstract: Sequential recommendation aims to capture user preferences by modeling
sequential patterns in user-item interactions. However, these models are often
influenced by noise such as accidental interactions, leading to suboptimal
performance. Therefore, to reduce the effect of noise, some works propose
explicitly identifying and removing noisy items. However, we find that simply
relying on collaborative information may result in an over-denoising problem,
especially for cold items. To overcome these limitations, we propose a novel
framework: Interest Alignment for Denoising Sequential Recommendation (IADSR)
which integrates both collaborative and semantic information. Specifically,
IADSR is comprised of two stages: in the first stage, we obtain the
collaborative and semantic embeddings of each item from a traditional
sequential recommendation model and an LLM, respectively. In the second stage,
we align the collaborative and semantic embeddings and then identify noise in
the interaction sequence based on long-term and short-term interests captured
in the collaborative and semantic modalities. Our extensive experiments on four
public datasets validate the effectiveness of the proposed framework and its
compatibility with different sequential recommendation systems.

</details>


### [742] [Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation](https://arxiv.org/abs/2510.04502)
*Yue Que,Yingyi Zhang,Xiangyu Zhao,Chen Ma*

Main category: cs.IR

TL;DR: 本文提出了一种新的基于因果感知图聚合权重估计的去偏方法CAGED，通过将图聚合建模为因果推断中的后门调整，有效缓解推荐系统中的流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络推荐系统的去偏方法在缓解流行度偏差方面存在不足，缺乏理论保证且难以平衡训练与去偏过程。

Method: 提出CAGED模型，将图聚合视为因果推断中的后门调整，设计编码器-解码器结构来估计无偏聚合权重，并引入动量更新策略逐步优化权重矩阵。

Result: 在三个真实数据集上的实验表明，CAGED在推荐性能上优于现有的图基去偏方法，尤其在早期训练阶段表现出更强的去偏效果。

Conclusion: 通过因果视角建模图聚合过程可有效减轻流行度偏差，CAGED为图神经网络推荐系统的公平性提供了新的解决方案。

Abstract: Graph-based recommender systems leverage neighborhood aggregation to generate
node representations, which is highly sensitive to popularity bias, resulting
in an echo effect during information propagation. Existing graph-based
debiasing solutions refine the aggregation process with attempts such as edge
reconstruction or weight adjustment. However, these methods remain inadequate
in fully alleviating popularity bias. Specifically, this is because 1) they
provide no insights into graph aggregation rationality, thus lacking an
optimality guarantee; 2) they fail to well balance the training and debiasing
process, which undermines the effectiveness. In this paper, we propose a novel
approach to mitigate popularity bias through rational modeling of the graph
aggregation process. We reveal that graph aggregation is a special form of
backdoor adjustment in causal inference, where the aggregation weight
corresponds to the historical interaction likelihood distribution. Based on
this insight, we devise an encoder-decoder architecture, namely Causality-aware
Graph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the
unbiased aggregation weight by optimizing the evidence lower bound of the
interaction likelihood. In order to enhance the debiasing effectiveness during
early training stages, we further design a momentum update strategy that
incrementally refines the aggregation weight matrix. Extensive experiments on
three datasets demonstrate that CAGED outperforms existing graph-based
debiasing methods. Our implementation is available at
https://github.com/QueYork/CAGED.

</details>


### [743] [MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations](https://arxiv.org/abs/2510.04508)
*Lili Xie,Yi Zhang,Ruihong Qiu,Jiajun Liu,Sen Wang*

Main category: cs.IR

TL;DR: 提出了一种基于多智能体强化学习的跨域推荐框架MARCO，通过多智能体协同学习各源域贡献，有效缓解负迁移问题，并在多个数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统单智能体强化学习在多源跨域推荐中因源域贡献不一致和分布差异导致的负迁移问题。

Method: 采用多智能体强化学习框架，每个智能体负责评估一个源域的贡献，并引入基于熵的动作多样性惩罚机制以增强策略表达性和训练稳定性。

Result: 在四个基准数据集上的实验表明，MARCO在推荐性能上优于现有最先进方法，展现出良好的鲁棒性和泛化能力。

Conclusion: MARCO能有效管理多源域知识迁移中的信用分配，显著提升冷启动场景下的推荐效果，为跨域推荐提供了新的解决方案。

Abstract: Recommender systems frequently encounter data sparsity issues, particularly
when addressing cold-start scenarios involving new users or items. Multi-source
cross-domain recommendation (CDR) addresses these challenges by transferring
valuable knowledge from multiple source domains to enhance recommendations in a
target domain. However, existing reinforcement learning (RL)-based CDR methods
typically rely on a single-agent framework, leading to negative transfer issues
caused by inconsistent domain contributions and inherent distributional
discrepancies among source domains. To overcome these limitations, MARCO, a
Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework,
is proposed. It leverages cooperative multi-agent reinforcement learning, where
each agent is dedicated to estimating the contribution from an individual
source domain, effectively managing credit assignment and mitigating negative
transfer. In addition, an entropy-based action diversity penalty is introduced
to enhance policy expressiveness and stabilize training by encouraging diverse
agents' joint actions. Extensive experiments across four benchmark datasets
demonstrate MARCO's superior performance over state-of-the-art methods,
highlighting its robustness and strong generalization capabilities. The code is
at https://github.com/xiewilliams/MARCO.

</details>


### [744] [Topic-Specific Classifiers are Better Relevance Judges than Prompted LLMs](https://arxiv.org/abs/2510.04633)
*Lukas Gienapp,Martin Potthast,Harrisen Scells,Eugene Yang*

Main category: cs.IR

TL;DR: 提出了一种基于主题的特定相关性分类器来解决未判断文档问题，通过在单个评估者对单个主题的判断上微调monoT5模型，实现了与真实系统排名高度相关的结果，同时保持人类判断为检索评估的黄金标准。


<details>
  <summary>Details</summary>
Motivation: 解决信息检索中测试集相关性判断不完整（未判断文档问题）导致评估不可靠的问题，避免现有大语言模型作为裁判方法的循环依赖和可靠性不足。

Method: 使用独立的LoRA权重适配技术，在单个主题和单个评估者的判断数据上微调monoT5模型，构建主题特定的相关性分类器，并用其对未判断文档进行相关性预测。

Result: 所提方法在仅需每主题128个初始人工判断的情况下，系统排名与真实排名的Spearman's ρ相关性超过0.95，优于将未判断文档视为非相关的基线方法，且比现有LLM-as-a-judge方法更可靠。

Conclusion: 主题特定的相关性分类器是一种轻量、直接且可靠的方法，能有效缓解未判断文档问题，同时保留人类判断的黄金标准地位。

Abstract: The unjudged document problem, where pooled test collections have incomplete
relevance judgments for evaluating new retrieval systems, is a key obstacle to
the reusability of test collections in information retrieval. While the de
facto standard to deal with the problem is to treat unjudged documents as
non-relevant, many alternatives have been proposed, including the use of large
language models (LLMs) as a relevance judge (LLM-as-a-judge). However, this has
been criticized as circular, since the same LLM can be used as a judge and as a
ranker at the same time. We propose to train topic-specific relevance
classifiers instead: By finetuning monoT5 with independent LoRA weight
adaptation on the judgments of a single assessor for a single topic's pool, we
align it to that assessor's notion of relevance for the topic. The system
rankings obtained through our classifier's relevance judgments achieve a
Spearmans' $\rho$ correlation of $>0.95$ with ground truth system rankings. As
little as 128 initial human judgments per topic suffice to improve the
comparability of models, compared to treating unjudged documents as
non-relevant, while achieving more reliability than existing LLM-as-a-judge
approaches. Topic-specific relevance classifiers thus are a lightweight and
straightforward way to tackle the unjudged document problem, while maintaining
human judgments as the gold standard for retrieval evaluation. Code, models,
and data are made openly available.

</details>
