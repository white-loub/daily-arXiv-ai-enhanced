<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 116]
- [cs.CL](#cs.CL) [Total: 39]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 101]
- [cs.RO](#cs.RO) [Total: 32]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.MA](#cs.MA) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Hierarchical Process Reward Models are Symbolic Vision Learners](https://arxiv.org/abs/2512.03126)
*Shan Zhang,Aotian Chen,Kai Zou,Jindong Gu,Yuan Xue,Anton van den Hengel*

Main category: cs.CV

TL;DR: 本文提出了一种自监督符号化自动编码器，通过符号分层过程奖励建模实现几何图解的结构化解析与重建，并结合神经符号系统提升视觉感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于像素的视觉模型难以实现可解释的图解理解，而符号化计算机视觉需要新的学习范式来解析几何原始元素及其关系。

Method: 提出自监督符号自动编码器，结合符号分层过程奖励建模（如点在线上、线在形状上等一致性约束）和稳定化机制以改善强化学习探索效率；并构建神经符号系统，通过推理驱动的视觉奖励融合神经网络与符号模型优势。

Result: 在几何图解重建中MSE降低98.2%；图表重建超越GPT-4o 0.6%（使用7B模型）；MathGlance感知基准提升13%，MathVerse与GeoQA推理基准提升3%。

Conclusion: 该方法显著提升了图解重建精度与视觉推理能力，在保持高可解释性的同时实现了神经与符号系统的有效协同。

Abstract: Symbolic computer vision represents diagrams through explicit logical rules and structured representations, enabling interpretable understanding in machine vision. This requires fundamentally different learning paradigms from pixel-based visual models. Symbolic visual learners parse diagrams into geometric primitives-points, lines, and shapes-whereas pixel-based learners operate on textures and colors. We propose a novel self-supervised symbolic auto-encoder that encodes diagrams into structured primitives and their interrelationships within the latent space, and decodes them through our executable engine to reconstruct the input diagrams. Central to this architecture is Symbolic Hierarchical Process Reward Modeling, which applies hierarchical step-level parsing rewards to enforce point-on-line, line-on-shape, and shape-on-relation consistency. Since vanilla reinforcement learning exhibits poor exploration in the policy space during diagram reconstruction; we thus introduce stabilization mechanisms to balance exploration and exploitation. We fine-tune our symbolic encoder on downstream tasks, developing a neuro-symbolic system that integrates the reasoning capabilities of neural networks with the interpretability of symbolic models through reasoning-grounded visual rewards. Evaluations across reconstruction, perception, and reasoning tasks demonstrate the effectiveness of our approach: achieving a 98.2% reduction in MSE for geometric diagram reconstruction, surpassing GPT-4o by 0.6% with a 7B model on chart reconstruction, and improving by +13% on the MathGlance perception benchmark, and by +3% on MathVerse and GeoQA reasoning benchmarks.

</details>


### [2] [Drainage: A Unifying Framework for Addressing Class Uncertainty](https://arxiv.org/abs/2512.03182)
*Yasser Taha,Grégoire Montavon,Nils Körber*

Main category: cs.CV

TL;DR: 本文提出一种基于'排水节点'的统一框架，通过在网络输出端添加该节点来重分配概率质量至不确定性，从而有效应对标签噪声、类别模糊性及分布外样本问题。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习面临标签噪声、类别模糊性以及需要稳健拒绝分布外或损坏样本的挑战。

Method: 在网络输出端添加一个'排水节点'，用于将概率质量重分配至不确定性，同时保持端到端训练和可微性。

Result: 在CIFAR-10/100上加入实例依赖噪声或非对称噪声的实验中，在高噪声情况下准确率比现有方法最高提升9%；在mini-WebVision、mini-ImageNet和Clothing-1M等真实数据集上达到或超越当前最优方法。

Conclusion: 排水节点机制不仅能提升分类鲁棒性，还适用于网络规模半监督数据清洗和开放集识别等更广泛任务。

Abstract: Modern deep learning faces significant challenges with noisy labels, class ambiguity, as well as the need to robustly reject out-of-distribution or corrupted samples. In this work, we propose a unified framework based on the concept of a "drainage node'' which we add at the output of the network. The node serves to reallocate probability mass toward uncertainty, while preserving desirable properties such as end-to-end training and differentiability. This mechanism provides a natural escape route for highly ambiguous, anomalous, or noisy samples, particularly relevant for instance-dependent and asymmetric label noise. In systematic experiments involving the addition of varying proportions of instance-dependent noise or asymmetric noise to CIFAR-10/100 labels, our drainage formulation achieves an accuracy increase of up to 9\% over existing approaches in the high-noise regime. Our results on real-world datasets, such as mini-WebVision, mini-ImageNet and Clothing-1M, match or surpass existing state-of-the-art methods. Qualitative analysis reveals a denoising effect, where the drainage neuron consistently absorbs corrupt, mislabeled, or outlier data, leading to more stable decision boundaries. Furthermore, our drainage formulation enables applications well beyond classification, with immediate benefits for web-scale, semi-supervised dataset cleaning, and open-set applications.

</details>


### [3] [Deep-BrownConrady: Prediction of Camera Calibration and Distortion Parameters Using Deep Learning and Synthetic Data](https://arxiv.org/abs/2501.14510)
*Faiz Muhammad Chaudhry,Jarno Ralli,Jerome Leudet,Fahad Sohrab,Farhad Pakdaman,Pierre Corbani,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习（ResNet）的方法，利用混合真实与合成图像（通过AILiveSim生成）训练模型，仅需单张图像即可预测相机内参和镜头畸变参数（遵循Brown-Conrady模型），缓解传统多视角标定不可行的问题。


<details>
  <summary>Details</summary>
Motivation: 传统相机标定需多视角标定板图像，而现实中常缺乏此类数据；亟需仅用单张图像完成标定的可行方法。

Method: 构建涵盖不同焦距与畸变参数的大规模合成数据集（AILiveSim生成），辅以少量真实图像；采用适配回归任务的ResNet网络，预测Brown-Conrady模型下的相机参数（如水平视场角、主点、畸变系数）。

Result: 模型在仅用单张图像输入下，能较准确预测相机内参和畸变参数；验证了合成数据主导训练对真实图像标定任务的有效迁移能力。

Conclusion: 纯单图深度学习标定可行，合成数据可有效支撑真实场景应用；该方法适用于自动驾驶、机器人、AR等需轻量、快速标定的领域。

Abstract: This research addresses the challenge of camera calibration and distortion parameter prediction from a single image using deep learning models. The main contributions of this work are: (1) demonstrating that a deep learning model, trained on a mix of real and synthetic images, can accurately predict camera and lens parameters from a single image, and (2) developing a comprehensive synthetic dataset using the AILiveSim simulation platform. This dataset includes variations in focal length and lens distortion parameters, providing a robust foundation for model training and testing. The training process predominantly relied on these synthetic images, complemented by a small subset of real images, to explore how well models trained on synthetic data can perform calibration tasks on real-world images. Traditional calibration methods require multiple images of a calibration object from various orientations, which is often not feasible due to the lack of such images in publicly available datasets. A deep learning network based on the ResNet architecture was trained on this synthetic dataset to predict camera calibration parameters following the Brown-Conrady lens model. The ResNet architecture, adapted for regression tasks, is capable of predicting continuous values essential for accurate camera calibration in applications such as autonomous driving, robotics, and augmented reality.
  Keywords: Camera calibration, distortion, synthetic data, deep learning, residual networks (ResNet), AILiveSim, horizontal field-of-view, principal point, Brown-Conrady Model.

</details>


### [4] [Does Head Pose Correction Improve Biometric Facial Recognition?](https://arxiv.org/abs/2512.03199)
*Justin Norman,Hany Farid*

Main category: cs.CV

TL;DR: 本文研究了AI驱动的面部图像修复技术（如3D重建、2D正面化和特征增强）对真实场景下人脸识别准确率的影响，发现盲目应用会降低性能，但选择性组合CFR-GAN与CodeFormer可提升识别效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的人脸图像常存在质量差、非正面姿态和遮挡等问题，导致生物特征人脸识别模型准确率显著下降，亟需有效修复方法提升鲁棒性。

Method: 采用模型无关、大规模、法医级评估流程，系统评测三种修复方法：NextFace（3D重建）、CFR-GAN（2D正面化）和CodeFormer（特征增强），并探索其单独及组合应用效果。

Result: 直接应用任一修复方法均显著降低识别准确率；但选择性联合使用CFR-GAN与CodeFormer可带来实质性识别性能提升。

Conclusion: 人脸图像修复不能一概而论，需策略性选择与组合方法；CFR-GAN与CodeFormer的协同应用是一种有前景的提升真实场景人脸识别鲁棒性的方案。

Abstract: Biometric facial recognition models often demonstrate significant decreases in accuracy when processing real-world images, often characterized by poor quality, non-frontal subject poses, and subject occlusions. We investigate whether targeted, AI-driven, head-pose correction and image restoration can improve recognition accuracy. Using a model-agnostic, large-scale, forensic-evaluation pipeline, we assess the impact of three restoration approaches: 3D reconstruction (NextFace), 2D frontalization (CFR-GAN), and feature enhancement (CodeFormer). We find that naive application of these techniques substantially degrades facial recognition accuracy. However, we also find that selective application of CFR-GAN combined with CodeFormer yields meaningful improvements.

</details>


### [5] [LLM-Guided Material Inference for 3D Point Clouds](https://arxiv.org/abs/2512.03237)
*Nafiseh Izadyar,Teseo Schneider*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段大语言模型（LLM）方法，无需训练即可从带粗略分割的3D点云中零样本推断物体语义与各部分材料组成，通过解耦‘是什么’和‘由什么构成’实现，并用LLM-as-a-Judge评估，验证了LLM可作为几何推理与材料理解之间的通用先验。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状数据集和模型主要关注几何结构，忽略了决定外观的材料属性；且缺乏可靠的材料标注数据。

Method: 采用两阶段零样本LLM方法：第一阶段预测物体整体语义类别，第二阶段基于该语义为每个几何分割部分分配合理材料；全程无需任务特定训练。

Result: 在Fusion/ABS和ShapeNet共1000个形状上，通过DeepEval构建的LLM-as-a-Judge评估，语义与材料推断均表现出高合理性。

Conclusion: 大语言模型可作为通用先验，有效连接3D几何推理与材料理解，为材料感知的3D理解提供新范式。

Abstract: Most existing 3D shape datasets and models focus solely on geometry, overlooking the material properties that determine how objects appear. We introduce a two-stage large language model (LLM) based method for inferring material composition directly from 3D point clouds with coarse segmentations. Our key insight is to decouple reasoning about what an object is from what it is made of. In the first stage, an LLM predicts the object's semantic; in the second stage, it assigns plausible materials to each geometric segment, conditioned on the inferred semantics. Both stages operate in a zero-shot manner, without task-specific training. Because existing datasets lack reliable material annotations, we evaluate our method using an LLM-as-a-Judge implemented in DeepEval. Across 1,000 shapes from Fusion/ABS and ShapeNet, our method achieves high semantic and material plausibility. These results demonstrate that language models can serve as general-purpose priors for bridging geometric reasoning and material understanding in 3D data.

</details>


### [6] [Flux4D: Flow-based Unsupervised 4D Reconstruction](https://arxiv.org/abs/2512.03210)
*Jingkang Wang,Henry Che,Yun Chen,Ze Yang,Lily Goli,Sivabalan Manivasagam,Raquel Urtasun*

Main category: cs.CV

TL;DR: 本文提出Flux4D，一种无需监督、可扩展的4D动态场景重建框架，直接预测3D高斯及其运动，在仅用光度损失和‘尽可能静态’正则下实现跨场景泛化与高效重建。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF和3DGS等方法受限于可扩展性及对运动解耦标注的依赖；自监督方法仍需逐场景优化且超参敏感，难以应对大规模、未见动态场景。

Method: Flux4D采用全无监督范式，直接预测3D高斯参数及其运动轨迹，仅使用光度损失和‘尽可能静态’（as static as possible）正则项，通过跨多场景联合训练学习动态分解，无需预训练模型或先验知识。

Result: 在户外驾驶数据集上，Flux4D显著优于现有方法：重建质量更高、推理速度快至秒级、支持大规模数据扩展，并能泛化至未知环境及罕见物体。

Conclusion: Flux4D证明了仅靠简单损失和跨场景训练即可实现鲁棒、可扩展、泛化的4D动态场景重建，为无监督视觉重建提供了新范式。

Abstract: Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an "as static as possible" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.

</details>


### [7] [Object Counting with GPT-4o and GPT-5: A Comparative Study](https://arxiv.org/abs/2512.03233)
*Richard Füzesséry,Kaziwa Saleh,Sándor Szénási,Zoltán Vámossy*

Main category: cs.CV

TL;DR: 本文探索利用多模态大语言模型（GPT-4o和GPT-5）在无需训练样本的情况下，仅通过文本提示完成零样本目标计数任务，并在FSC-147和CARPK数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本目标计数方法依赖大量标注数据或视觉示例，而大语言模型具备强大推理与理解能力，有望实现完全无监督的计数。

Method: 使用GPT-4o和GPT-5两个多模态大模型，仅通过文本提示进行零样本目标计数，不依赖任何训练数据或视觉示例，在FSC-147和CARPK数据集上评估性能。

Result: 在FSC-147数据集上，两模型表现媲美甚至超越当前最优零样本方法；在CARPK上的结果也提供了对比分析。

Conclusion: 多模态大语言模型可在零样本设定下有效执行目标计数任务，为无监督视觉理解提供新思路。

Abstract: Zero-shot object counting attempts to estimate the number of object instances belonging to novel categories that the vision model performing the counting has never encountered during training. Existing methods typically require large amount of annotated data and often require visual exemplars to guide the counting process. However, large language models (LLMs) are powerful tools with remarkable reasoning and data understanding abilities, which suggest the possibility of utilizing them for counting tasks without any supervision. In this work we aim to leverage the visual capabilities of two multi-modal LLMs, GPT-4o and GPT-5, to perform object counting in a zero-shot manner using only textual prompts. We evaluate both models on the FSC-147 and CARPK datasets and provide a comparative analysis. Our findings show that the models achieve performance comparable to the state-of-the-art zero-shot approaches on FSC-147, in some cases, even surpass them.

</details>


### [8] [2-Shots in the Dark: Low-Light Denoising with Minimal Data Acquisition](https://arxiv.org/abs/2512.03245)
*Liying Lu,Raphaël Achddou,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文提出了一种仅需单张噪声图像和暗帧即可合成真实感低光噪声图像的方法，结合泊松建模信号相关噪声与傅里叶域频谱采样建模信号无关噪声，在无需大量成对数据的情况下实现了SOTA去噪性能。


<details>
  <summary>Details</summary>
Motivation: 学习型去噪器训练需要大量干净-噪声图像对，但实际难以获取；噪声合成可替代大规模数据采集，但现有方法依赖简化参数模型或大量成对数据。

Method: 用泊松分布建模信号相关噪声，提出傅里叶域频谱采样算法建模信号无关噪声，仅需单张噪声图像和单张对应ISO的暗帧。

Result: 在多个低光去噪基准测试中达到SOTA性能，且合成噪声保持真实传感器噪声的空间与统计特性。

Conclusion: 所提噪声合成方法兼具准确性、实用性与泛化性，为低光图像去噪提供了高效可靠的数据生成方案。

Abstract: Raw images taken in low-light conditions are very noisy due to low photon count and sensor noise. Learning-based denoisers have the potential to reconstruct high-quality images. For training, however, these denoisers require large paired datasets of clean and noisy images, which are difficult to collect. Noise synthesis is an alternative to large-scale data acquisition: given a clean image, we can synthesize a realistic noisy counterpart. In this work, we propose a general and practical noise synthesis method that requires only one single noisy image and one single dark frame per ISO setting. We represent signal-dependent noise with a Poisson distribution and introduce a Fourier-domain spectral sampling algorithm to accurately model signal-independent noise. The latter generates diverse noise realizations that maintain the spatial and statistical properties of real sensor noise. As opposed to competing approaches, our method neither relies on simplified parametric models nor on large sets of clean-noisy image pairs. Our synthesis method is not only accurate and practical, it also leads to state-of-the-art performances on multiple low-light denoising benchmarks.

</details>


### [9] [PixPerfect: Seamless Latent Diffusion Local Editing with Discriminative Pixel-Space Refinement](https://arxiv.org/abs/2512.03247)
*Haitian Zheng,Yuan Yao,Yongsheng Yu,Yuqian Zhou,Jiebo Luo,Zhe Lin*

Main category: cs.CV

TL;DR: 本文提出PixPerfect框架，通过像素级精细化处理解决潜在扩散模型（LDMs）在图像修复与局部编辑中因潜在压缩导致的色彩偏移、纹理不匹配和边界可见缝等问题，显著提升编辑质量与泛化能力。


<details>
  <summary>Details</summary>
Motivation: Latent Diffusion Models（LDMs）在图像修复和局部编辑中虽取得进展，但其潜在空间压缩常引发像素级不一致（如色偏、纹理错配、边界伪影），现有方法（如背景条件解码、像素空间调和）难以彻底消除且泛化性差。

Method: PixPerfect包含三部分：(i) 可微判别性像素空间，增强/抑制细微颜色与纹理差异；(ii) 全面的人工伪影模拟流程，使精炼器在训练中接触真实编辑伪影；(iii) 直接像素空间精炼机制，确保对多种潜在表示与任务的广泛适用性。

Result: 在图像修复、物体移除与插入等基准测试中，PixPerfect显著提升了感知保真度与下游编辑性能，展现出优异的鲁棒性与高保真局部编辑能力。

Conclusion: PixPerfect为基于LDM的局部图像编辑提供了通用、高效、高质量的像素级精炼方案，确立了该领域的新标准。

Abstract: Latent Diffusion Models (LDMs) have markedly advanced the quality of image inpainting and local editing. However, the inherent latent compression often introduces pixel-level inconsistencies, such as chromatic shifts, texture mismatches, and visible seams along editing boundaries. Existing remedies, including background-conditioned latent decoding and pixel-space harmonization, usually fail to fully eliminate these artifacts in practice and do not generalize well across different latent representations or tasks. We introduce PixPerfect, a pixel-level refinement framework that delivers seamless, high-fidelity local edits across diverse LDM architectures and tasks. PixPerfect leverages (i) a differentiable discriminative pixel space that amplifies and suppresses subtle color and texture discrepancies, (ii) a comprehensive artifact simulation pipeline that exposes the refiner to realistic local editing artifacts during training, and (iii) a direct pixel-space refinement scheme that ensures broad applicability across diverse latent representations and tasks. Extensive experiments on inpainting, object removal, and insertion benchmarks demonstrate that PixPerfect substantially enhances perceptual fidelity and downstream editing performance, establishing a new standard for robust and high-fidelity localized image editing.

</details>


### [10] [PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery](https://arxiv.org/abs/2512.03257)
*Mark Moussa,Andre Williams,Seth Roffe,Douglas Morton*

Main category: cs.CV

TL;DR: 本文提出了一种名为PyroFocus的两阶段深度学习 pipeline，用于机载实时 wildfire 检测与火辐射功率（FRP）估计，兼顾高精度与低延迟，在MASTER数据上验证了其边缘部署潜力。


<details>
  <summary>Details</summary>
Motivation:  wildfires 频发且加剧，亟需低延迟、高效率的机载实时检测方法；现有高维多/高光谱热成像数据在星载/机载平台实时处理面临算力与资源瓶颈。

Method: 系统评估多种深度学习架构（自定义CNN与Transformer），并提出PyroFocus两阶段pipeline：第一阶段多类火灾分类（无火/着火/过火），第二阶段FRP回归或分割，以降低推理时延与计算开销。使用NASA MASTER数据进行训练与评测。

Result: PyroFocus在精度、推理延迟和资源效率之间取得良好权衡，显著优于单阶段模型，具备实时边缘部署能力。

Conclusion: 两阶段架构是面向未来 wildfire 监测任务的可行机载解决方案，为高时效性、轻量化遥感火灾分析提供了新范式。

Abstract: Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.
  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.
  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.

</details>


### [11] [SpatialReasoner: Active Perception for Large-Scale 3D Scene Understanding](https://arxiv.org/abs/2512.03284)
*Hongpei Zheng,Shijie Li,Yanran Li,Hujun Yin*

Main category: cs.CV

TL;DR: 本文提出H²U3D数据集和SpatialReasoner框架，用于解决大尺度（整栋房屋）3D场景下的视觉问答与空间推理问题；通过分层表征、链式思维标注与两阶段主动感知训练，显著提升性能并大幅减少图像使用量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型受限于房间级场景，难以处理整栋房屋尺度（多楼层、大面积）的3D空间推理任务。

Method: 构建H²U3D大规模多楼层3D VQA数据集，并设计SpatialReasoner主动感知框架：采用自动化标注生成分层粗到细视觉表征与链式思维问答对；框架通过监督冷启动+带自适应探索奖励的强化学习进行两阶段训练，自主调用空间工具进行高效探索。

Result: SpatialReasoner在H²U3D上达到SOTA，超越GPT-4o、Gemini-2.5-Pro等强基线；平均仅需3–4张图像即完成推理，远少于基线所需的16+张。

Conclusion: 分层表征与主动、高效、粗到细的空间探索范式，可有效提升大尺度3D环境中的视觉语言理解与空间推理能力。

Abstract: Spatial reasoning in large-scale 3D environments remains challenging for current vision-language models, which are typically constrained to room-scale scenarios. We introduce H$^2$U3D (Holistic House Understanding in 3D), a 3D visual question answering dataset designed for house-scale scene understanding. H$^2$U3D features multi-floor environments spanning up to three floors and 10-20 rooms, covering more than 300 m$^2$. Through an automated annotation pipeline, it constructs hierarchical coarse-to-fine visual representations and generates diverse question-answer pairs with chain-of-thought annotations. We further propose SpatialReasoner, an active perception framework that autonomously invokes spatial tools to explore 3D scenes based on textual queries. SpatialReasoner is trained through a two-stage strategy: a supervised cold start followed by reinforcement learning with an adaptive exploration reward that promotes efficient exploration while discouraging redundant operations. Extensive experiments demonstrate that SpatialReasoner achieves state-of-the-art performance on H$^2$U3D, outperforming strong baselines including GPT-4o and Gemini-2.5-Pro. Notably, our method attains superior results while using only 3-4 images in total on average, compared to baselines requiring 16+ images, highlighting the effectiveness of our coarse-to-fine active exploration paradigm.

</details>


### [12] [NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction](https://arxiv.org/abs/2512.03317)
*Thomas Monninger,Zihan Zhang,Steffen Staab,Sihao Ding*

Main category: cs.CV

TL;DR: 本文提出NavMapFusion，一种基于扩散模型的在线地图融合框架，利用低精度导航地图（如OpenStreetMap）作为先验，结合高精度车载传感器数据，通过迭代去噪生成准确、实时更新的环境表示，显著提升长距离感知性能。


<details>
  <summary>Details</summary>
Motivation: 传统高清地图难以实时更新，而广泛可用的标准地图分辨率不足；需有效融合低精度先验与高精度在线感知数据以支持可靠自动驾驶。

Method: 提出基于扩散模型的NavMapFusion框架，将导航地图与传感器数据联合建模，把地图-感知不一致区域视为扩散过程中的噪声，通过条件迭代去噪实现融合。

Result: 在nuScenes基准上，使用OpenStreetMap粗略道路线作为先验时，100米范围内的性能相对提升21.4%，更长感知范围增益更强，且保持实时性。

Conclusion: 扩散模型天然适配地图融合任务——先验与感知的一致性增强结果，不一致性被抑制；该方法为构建准确、动态、实时的环境表征提供了新范式。

Abstract: Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion

</details>


### [13] [Step-by-step Layered Design Generation](https://arxiv.org/abs/2512.03335)
*Faizan Farooq Khan,K J Joseph,Koustava Goswami,Mohamed Elhoseiny,Balaji Vasan Srinivasan*

Main category: cs.CV

TL;DR: 本文提出了一种新的分步分层设计生成（SLEDGE）方法，将设计生成建模为基于指令的逐层、原子化更新过程，并构建了配套数据集与评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有设计生成方法多将设计合成视为单步任务，忽视了设计本质上是逐步迭代、层层细化的创造性过程，因此需要更符合实际的设计生成范式。

Method: 提出Step-by-Step Layered Design Generation新问题设定；基于多模态大语言模型构建SLEDGE框架，将每次设计更新建模为对前一状态的指令驱动的原子化分层修改；并构建专用数据集与评测基准。

Result: 在新设定下的详尽实验表明，SLEDGE显著优于适配该设定的现有最先进方法。

Conclusion: 本工作开辟了面向真实设计流程的分步分层生成研究方向，为该实用但被忽视的领域提供了新范式、新模型与新评测体系。

Abstract: Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.

</details>


### [14] [ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography](https://arxiv.org/abs/2512.03339)
*Yeganeh Ghamary,Victoria Wu,Hooman Vaseli,Christina Luong,Teresa Tsang,Siavash Bigdeli,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 本文提出ProtoEFNet，一种基于视频的原型学习模型，用于连续射血分数（EF）回归，通过学习动态时空原型和引入原型角度分离（PAS）损失，提升模型可解释性与性能。


<details>
  <summary>Details</summary>
Motivation: 传统EF评估依赖人工勾画、耗时且存在观察者间差异；现有深度学习方法多为黑箱模型，临床信任度低；后验可解释方法无法引导模型内部推理，可靠性有限。

Method: 提出ProtoEFNet模型，采用视频输入，学习具有临床意义的心脏运动模式的动态时空原型；引入Prototype Angular Separation（PAS）损失，增强EF连续谱上的判别性表征。

Result: 在EchonetDynamic数据集上，ProtoEFNet达到与非可解释模型相当的精度；消融实验显示PAS损失使F1分数从77.67±2.68提升至79.64±2.10（+2%）。

Conclusion: ProtoEFNet在保持高预测精度的同时，提供临床可理解的内在推理依据，提升了深度学习模型在心脏功能评估中的可信度与实用性。

Abstract: Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\pm$2.68 to 79.64$\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF

</details>


### [15] [HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration](https://arxiv.org/abs/2512.03345)
*Seunghoi Kim,Henry F. J. Tregidgo,Chen Jin,Matteo Figini,Daniel C. Alexander*

Main category: cs.CV

TL;DR: 本文提出HalluGen，一种基于扩散模型的框架，用于合成可控的、逼真的幻觉图像，构建首个大规模幻觉数据集，并开发了新的评估指标SHAFE和无参考幻觉检测器，以提升安全关键图像修复中幻觉问题的可评估性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 生成模型在图像修复中易产生幻觉（即看似合理但实际错误的结构），尤其在医疗影像等安全关键领域会严重影响诊断可靠性；而现有幻觉评估受限于标注数据稀缺、昂贵且主观的困境。

Method: 提出HalluGen——一种基于扩散模型的幻觉合成框架，可控制幻觉的类型、位置和严重程度；基于其构建首个大规模（4350张）带标注幻觉数据集；并基于该数据集设计SHAFE评估指标及无参考幻觉检测器。

Result: HalluGen生成的幻觉图像使分割IoU从0.86显著下降至0.36；构建了首个低场MRI增强专用幻觉数据集；SHAFE在幻觉敏感性上优于传统图像质量指标；训练的无参考检测器能泛化至真实修复失败场景。

Conclusion: HalluGen及其开源数据集为安全关键图像修复中的幻觉问题提供了首个可扩展、系统化的评估基础，推动了幻觉检测与缓解方法的发展。

Abstract: Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.

</details>


### [16] [Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus](https://arxiv.org/abs/2512.03346)
*Lynn Kandakji,William Woof,Nikolas Pontikos*

Main category: cs.CV

TL;DR: 本文通过对比16种深度学习架构，发现分层注意力模型在亚临床角膜圆锥病（SKC）的3D OCT图像检测中表现最优，因其能精准匹配亚临床异常的多层面空间尺度，提升敏感性和特异性21-23%，并揭示信号强度决定所需空间整合长度这一关键机制。


<details>
  <summary>Details</summary>
Motivation: 弱而弥散的早期疾病信号在体素医学影像中难以检测，传统2D/3D CNN和ViT因归纳偏置不匹配（过强局部性或过度全局扩散）而受限，亟需适配稀疏、多尺度体素模式的最优架构。

Method: 对16种现代深度学习架构（涵盖2D/3D CNN、混合模型及体素Transformer）进行受控比较，用于从3D眼前节OCT体积数据中检测亚临床角膜圆锥病（SKC）；辅以机制分析（分层窗口感受野、注意力距离测量、表征相似性及年龄/性别预测辅助任务）。

Result: 分层注意力模型显著优于CNN与ViT，在亚临床稀疏异常检测中敏感性与特异性提高21–23%；机制上归因于其分层窗口产生的感受野精准匹配亚临床病变的中间尺度（跨多切片）；注意力距离分析表明，亚临床状态所需空间整合长度介于健康与显性病变之间且更长。

Conclusion: 分层注意力是一种原理清晰、高效鲁棒的归纳偏置，为3D医学影像中早期病理变化检测提供了可推广的架构设计范式。

Abstract: The detection of weak, spatially distributed anomalies in volumetric medical imaging remains a major challenge. The subtle, non-adjacent nature of early disease signals is often lost due to suboptimal architectural inductive biases: 2D/3D CNNs impose strong locality, while ViTs diffuse unconstrained global attention. This conflict leaves the optimal inductive structure for robust, sparse volumetric pattern recognition unresolved. This study presents a controlled comparison of sixteen modern deep learning architectures spanning 2D/3D convolutional, hybrid, and volumetric transformer families for subclinical keratoconus (SKC) detection from 3D anterior segment OCT volumes. We demonstrate that hierarchical attention models offer a superior and more parameter-efficient inductive bias, surpassing the performance of both 2D and 3D CNNs and ViTs. Our results show 21-23% higher sensitivity and specificity in the sparse anomaly (subclinical) regime. Mechanistic analyses reveal that this advantage stems from precise spatial scale alignment: hierarchical windowing produces effective receptive fields matched to the intermediate, multi-slice extent of subclinical abnormalities. This avoids excessive CNN locality and diffuse global attention. Attention-distance measurements confirm a key insight into architectural adaptation: the required spatial integration length shifts significantly based on the signal strength, with subclinical cases necessitating longer integration compared to both healthy and manifest disease states. Representational similarity and auxiliary age/sex prediction tasks further support the generalizability of these inductive principles. The findings provide design guidance for future volumetric anomaly detection systems, establishing hierarchical attention as a principled and effective approach for early pathological change analysis in 3D medical imaging.

</details>


### [17] [SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation](https://arxiv.org/abs/2512.03350)
*Yu Yuan,Tharindu Wickremasinghe,Zeeshan Nadir,Xijun Wang,Yiheng Chi,Stanley H. Chan*

Main category: cs.CV

TL;DR: 本文提出SeeU，一种基于2D→4D→2D框架的新方法，通过从稀疏单目2D帧重建连续4D世界、建模物理约束下的4D动态，并重投影生成未见时空内容，实现物理一致的连续视觉生成。


<details>
  <summary>Details</summary>
Motivation: 现有视觉理解、预测与生成大多直接在2D观测上操作，忽略了真实世界是4D（3D空间+时间）的本质，导致性能受限。

Method: 提出2D→4D→2D学习框架：先从稀疏单目2D帧重建4D世界；再在低秩表示和物理约束下学习离散到连续的4D动态；最后前向推演并重投影至2D，结合时空上下文生成未见区域。

Result: SeeU在未见时序生成、未见空间生成和视频编辑等多个任务上展现出强性能，支持连续且物理一致的视觉内容生成。

Conclusion: 在4D空间中建模动态可显著提升视觉生成的质量与一致性，为通用视觉生成提供了新范式。

Abstract: Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\to$4D$\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.

</details>


### [18] [A Hybrid Deep Learning Framework with Explainable AI for Lung Cancer Classification with DenseNet169 and SVM](https://arxiv.org/abs/2512.03359)
*Md Rashidul Islam,Bakary Gibba,Altagi Abdallah Bakheit Abdelgadir*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的肺部CT图像自动分类系统，结合DenseNet169（含SE模块、Focal Loss与FPN）和MobileNetV2-SVM双模型框架，并引入Grad-CAM与SHAP提升可解释性，在IQOTHNCCD数据集上均达98%准确率。


<details>
  <summary>Details</summary>
Motivation: 肺癌早期诊断对提高患者生存率至关重要，但人工解读CT影像耗时且易出错，亟需高精度、可解释的自动分类方法。

Method: 采用DenseNet169（集成Squeeze-and-Excitation模块、Focal Loss处理类别不平衡、Feature Pyramid Network实现多尺度特征融合），并构建基于MobileNetV2特征提取的SVM模型；同时使用Grad-CAM可视化决策区域，SHAP解释SVM中各特征贡献。

Result: 在IQOTHNCCD公开CT数据集（Normal/Benign/Malignant三类）上，DenseNet169与MobileNetV2-SVM模型均达到98%分类准确率。

Conclusion: 所提双模型框架兼具高精度、强鲁棒性与良好可解释性，具备临床落地潜力，推动AI辅助肺癌诊断向更准确、透明、可靠方向发展。

Abstract: Lung cancer is a very deadly disease worldwide, and its early diagnosis is crucial for increasing patient survival rates. Computed tomography (CT) scans are widely used for lung cancer diagnosis as they can give detailed lung structures. However, manual interpretation is time-consuming and prone to human error. To surmount this challenge, the study proposes a deep learning-based automatic lung cancer classification system to enhance detection accuracy and interpretability. The IQOTHNCCD lung cancer dataset is utilized, which is a public CT scan dataset consisting of cases categorized into Normal, Benign, and Malignant and used DenseNet169, which includes Squeezeand-Excitation blocks for attention-based feature extraction, Focal Loss for handling class imbalance, and a Feature Pyramid Network (FPN) for multi-scale feature fusion. In addition, an SVM model was developed using MobileNetV2 for feature extraction, improving its classification performance. For model interpretability enhancement, the study integrated Grad-CAM for the visualization of decision-making regions in CT scans and SHAP (Shapley Additive Explanations) for explanation of feature contributions within the SVM model. Intensive evaluation was performed, and it was found that both DenseNet169 and SVM models achieved 98% accuracy, suggesting their robustness for real-world medical practice. These results open up the potential for deep learning to improve the diagnosis of lung cancer by a higher level of accuracy, transparency, and robustness.

</details>


### [19] [FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting](https://arxiv.org/abs/2512.03369)
*Nan Zhou,Huandong Wang,Jiahao Li,Han Li,Yali Song,Qiuhua Wang,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 本文提出了FireSentry——首个省域级、亚米级空间与亚秒级时间分辨率的多模态野火数据集，并基于其构建综合基准，提出FiReDiff双模态生成模型，在视频预测与火场分割任务上均显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有野火蔓延预测研究受限于粗粒度时空尺度和低分辨率卫星数据，难以建模高精度局部火动态，亟需高分辨率、多模态、细粒度数据与方法支撑。

Method: 构建了基于同步无人机采集的FireSentry多模态数据集（可见光/红外视频、原位环境参数、人工标注火掩膜），并建立涵盖物理模型、数据驱动与生成模型的综合基准；在此基础上提出FiReDiff双模态范式：先在红外模态生成未来视频序列，再在掩膜模态基于生成动态精准分割火区。

Result: FiReDiff在生成模型中实现显著提升：视频质量指标PSNR↑39.2%、SSIM↑36.1%、LPIPS↓50.0%、FVD↓29.4%；火掩膜分割指标AUPRC↑3.3%、F1↑59.1%、IoU↑42.9%、MSE↓62.5%。

Conclusion: FireSentry数据集与FiReDiff范式共同推动了细粒度野火预测与动态灾害仿真研究的发展，为应急响应提供更精准的技术支撑。

Abstract: Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.

</details>


### [20] [ShelfGaussian: Shelf-Supervised Open-Vocabulary Gaussian-based 3D Scene Understanding](https://arxiv.org/abs/2512.03370)
*Lingjun Zhao,Yandong Luo,James Hay,Lu Gan*

Main category: cs.CV

TL;DR: ShelfGaussian 是一种基于高斯的开放词汇多模态3D场景理解框架，利用现成视觉基础模型（VFMs）进行监督，通过多模态高斯Transformer和货架式监督学习范式，在零样本语义占据预测等任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有高斯方法要么依赖闭集3D标注、忽略渲染能力，要么仅靠2D自监督导致几何质量下降且局限于单相机设置；需更好利用高斯表征潜力并支持开放词汇与多模态感知。

Method: 提出多模态高斯Transformer（使高斯可跨传感器模态查询特征）和货架式监督学习范式（联合在2D图像与3D场景层级用VFM特征优化高斯）。

Result: 在Occ3D-nuScenes上实现零样本语义占据预测SOTA；在无人地面车辆（UGV）真实城市场景中验证了其野外鲁棒性。

Conclusion: ShelfGaussian 有效融合开放词汇语义、多模态感知与高斯表征，在保持高效性的同时显著提升3D场景理解的泛化性与实用性。

Abstract: We introduce ShelfGaussian, an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework supervised by off-the-shelf vision foundation models (VFMs). Gaussian-based methods have demonstrated superior performance and computational efficiency across a wide range of scene understanding tasks. However, existing methods either model objects as closed-set semantic Gaussians supervised by annotated 3D labels, neglecting their rendering ability, or learn open-set Gaussian representations via purely 2D self-supervision, leading to degraded geometry and limited to camera-only settings. To fully exploit the potential of Gaussians, we propose a Multi-Modal Gaussian Transformer that enables Gaussians to query features from diverse sensor modalities, and a Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with VFM features jointly at 2D image and 3D scene levels. We evaluate ShelfGaussian on various perception and planning tasks. Experiments on Occ3D-nuScenes demonstrate its state-of-the-art zero-shot semantic occupancy prediction performance. ShelfGaussian is further evaluated on an unmanned ground vehicle (UGV) to assess its in the-wild performance across diverse urban scenarios. Project website: https://lunarlab-gatech.github.io/ShelfGaussian/.

</details>


### [21] [MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification](https://arxiv.org/abs/2512.03404)
*Yujian Zhao,Hankun Liu,Guanglin Niu*

Main category: cs.CV

TL;DR: 本文提出MOS框架，通过模态一致表征学习和跨模态数据生成与特征融合，有效缩小光学与SAR图像间的模态差距，显著提升跨模态船舶重识别性能。


<details>
  <summary>Details</summary>
Motivation: 光学与合成孔径雷达（SAR）图像之间存在显著模态差异，导致跨模态船舶重识别（ReID）困难，该任务在海事智能与监控中至关重要但研究尚少。

Method: MOS框架包含两部分：(1) 模态一致表征学习（MCRL），结合SAR图像去噪与类级别模态对齐损失；(2) 跨模态数据生成与特征融合（CDGF），利用布朗桥扩散模型生成跨模态样本，并在推理中融合原始特征。

Result: 在HOSS ReID数据集上，MOS在ALL-to-ALL、Optical-to-SAR和SAR-to-Optical三种协议下的R1准确率分别提升+3.0%、+6.2%和+16.4%，显著优于现有方法。

Conclusion: MOS有效缓解光学-SAR模态鸿沟，实现鲁棒、判别性强的跨模态船舶重识别，为海事多源遥感分析提供了新思路。

Abstract: Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.

</details>


### [22] [ViDiC: Video Difference Captioning](https://arxiv.org/abs/2512.03405)
*Jiangtao Wu,Shihao Li,Zhaozhou Bian,Yuanxing Zhang,Jialu Chen,Runzhe Wen,An Ping,Yiwen He,Jiakai Wang,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出了视频差异描述（ViDiC）任务和ViDiC-1K数据集，用于评估多模态大语言模型对视频对之间细粒度相似性与差异性的理解能力，并设计了双检查表评估框架，揭示现有模型在此任务上的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言系统在理解动态场景间视觉差异（如运动连续性、事件演化、编辑一致性）方面能力不足；静态图像差异描述（IDC）方法难以迁移到视频场景。

Method: 提出ViDiC任务与ViDiC-1K数据集（含1000个视频对、4000+标注的七类比较项），并构建基于LLM-as-a-Judge的双检查表评估框架（分别评测相似性与差异性识别准确性）。

Result: 在19个主流多模态模型上实验表明，其在视频差异描述与比较推理能力上存在显著性能差距。

Conclusion: ViDiC-1K为视频理解、编辑感知与多模态比较推理提供了具有挑战性的新基准，有望推动相关研究发展。

Abstract: Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.

</details>


### [23] [YOLOA: Real-Time Affordance Detection via LLM Adapter](https://arxiv.org/abs/2512.03418)
*Yuqi Ji,Junjie Ke,Lihuo He,Jun Liu,Kaifan Zhang,Yu-Kun Lai,Guiguang Ding,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出了YOLO Affordance（YOLOA），一种结合大语言模型（LLM）适配器的实时可供性检测模型，统一处理物体检测与可供性学习，兼顾‘是什么’、‘在哪里’和‘如何用’，在准确率与速度间取得优异平衡。


<details>
  <summary>Details</summary>
Motivation: 现有可供性学习方法大多只关注‘如何用’，忽略‘是什么’和‘在哪里’；而联合方法又缺乏任务间有效交互和实时能力。

Method: 提出YOLOA模型，包含轻量级物体检测与可供性学习双分支，并通过LLM适配器在训练中交互优化类别先验、边界框偏移和可供性门控。

Result: 在重标注的ADG-Det和IIT-Heat基准上达到SOTA精度（52.8 / 73.1 mAP），同时保持实时性能（最高89.77 FPS，轻量版达846.24 FPS）。

Conclusion: YOLOA成功实现了物体识别、定位与可供性理解的协同建模，在精度与效率之间取得优秀权衡，为具身智能中的实时可供性感知提供了新范式。

Abstract: Affordance detection aims to jointly address the fundamental "what-where-how" challenge in embodied AI by understanding "what" an object is, "where" the object is located, and "how" it can be used. However, most affordance learning methods focus solely on "how" objects can be used while neglecting the "what" and "where" aspects. Other affordance detection methods treat object detection and affordance learning as two independent tasks, lacking effective interaction and real-time capability. To overcome these limitations, we introduce YOLO Affordance (YOLOA), a real-time affordance detection model that jointly handles these two tasks via a large language model (LLM) adapter. Specifically, YOLOA employs a lightweight detector consisting of object detection and affordance learning branches refined through the LLM Adapter. During training, the LLM Adapter interacts with object and affordance preliminary predictions to refine both branches by generating more accurate class priors, box offsets, and affordance gates. Experiments on our relabeled ADG-Det and IIT-Heat benchmarks demonstrate that YOLOA achieves state-of-the-art accuracy (52.8 / 73.1 mAP on ADG-Det / IIT-Heat) while maintaining real-time performance (up to 89.77 FPS, and up to 846.24 FPS for the lightweight variant). This indicates that YOLOA achieves an excellent trade-off between accuracy and efficiency.

</details>


### [24] [DM3D: Deformable Mamba via Offset-Guided Gaussian Sequencing for Point Cloud Understanding](https://arxiv.org/abs/2512.03424)
*Bin Liu,Chunyang Wang,Xuelian Liu*

Main category: cs.CV

TL;DR: 本文提出DM3D，一种用于点云理解的可变形Mamba架构，通过高斯引导的序列化机制实现结构自适应的点云排序，显著提升SSMs在点云任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型（SSMs）依赖输入顺序，难以适配点云不规则几何结构；预定义序列化策略缺乏对多样几何结构的自适应能力。

Method: 提出DM3D架构，包含：1）偏移引导的高斯序列化机制，融合局部重采样（GKR）与全局可微重排序（GDR）；2）三路径频率融合模块以增强特征互补性并抑制混叠。

Result: 在分类、少样本学习和部件分割等任务上，DM3D在多个基准数据集上达到SOTA性能。

Conclusion: 自适应序列化能有效释放SSMs在点云理解中的潜力，DM3D为不规则几何数据建模提供了新范式。

Abstract: State Space Models (SSMs) demonstrate significant potential for long-sequence modeling, but their reliance on input order conflicts with the irregular nature of point clouds. Existing approaches often rely on predefined serialization strategies, which cannot adjust based on diverse geometric structures. To overcome this limitation, we propose \textbf{DM3D}, a deformable Mamba architecture for point cloud understanding. Specifically, DM3D introduces an offset-guided Gaussian sequencing mechanism that unifies local resampling and global reordering within a deformable scan. The Gaussian-based KNN Resampling (GKR) enhances structural awareness by adaptively reorganizing neighboring points, while the Gaussian-based Differentiable Reordering (GDR) enables end-to-end optimization of serialization order. Furthermore, a Tri-Path Frequency Fusion module enhances feature complementarity and reduces aliasing. Together, these components enable structure-adaptive serialization of point clouds. Extensive experiments on benchmark datasets show that DM3D achieves state-of-the-art performance in classification, few-shot learning, and part segmentation, demonstrating that adaptive serialization effectively unlocks the potential of SSMs for point cloud understanding.

</details>


### [25] [Generalization Evaluation of Deep Stereo Matching Methods for UAV-Based Forestry Applications](https://arxiv.org/abs/2512.03427)
*Yida Lin,Bing Xue,Mengjie Zhang,Sam Schofield,Richard Green*

Main category: cs.CV

TL;DR: 本文首次系统评估了八种最先进的立体匹配方法在森林环境中的零样本泛化能力，发现基础模型在结构化场景中表现优异，而迭代方法具有更强的跨域鲁棒性；RAFT-Stereo 在 ETH3D 上出现灾难性失效，而 DEFOM 在植被密集场景中展现出最优的深度估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计方法评估集中于城市和室内场景，缺乏对植被密集的森林环境的专门评估，导致跨域泛化能力验证存在关键空白。

Method: 对八种前沿立体匹配方法（包括 RAFT-Stereo、IGEV、IGEV++、BridgeDepth、StereoAnywhere、DEFOM、ACVNet、PSMNet、TCstereo）进行零样本评估：全部仅在 Scene Flow 数据集上训练，未微调，测试于 ETH3D、KITTI 2012/2015、Middlebury 及新构建的含 5313 对图像的 Canterbury 森林数据集（ZED Mini 拍摄）。

Result: 基础模型（如 BridgeDepth、DEFOM）在结构化场景中误差低（如 BridgeDepth 在 ETH3D 达 0.23 px），但泛化不均；迭代方法（如 IGEV++）跨域稳健（误差范围 0.36–6.77 px）；RAFT-Stereo 在 ETH3D 出现严重失效（26.23 px EPE，98% 错误率）；DEFOM 在 Canterbury 数据集上展现出最佳植被深度估计性能，具备更优深度平滑性、遮挡处理与跨域一致性。

Conclusion: 森林等植被密集场景需专用深度估计基线；DEFOM 是当前零样本植被深度估计的黄金标准；RAFT-Stereo 的负视差预测问题揭示了现有方法在特定场景下的根本性缺陷；跨域鲁棒性与细节保真度之间存在权衡。

Abstract: Autonomous UAV forestry operations require robust depth estimation methods with strong cross-domain generalization. However, existing evaluations focus on urban and indoor scenarios, leaving a critical gap for specialized vegetation-dense environments. We present the first systematic zero-shot evaluation of eight state-of-the-art stereo methods--RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM (plus baseline methods ACVNet, PSMNet, TCstereo)--spanning iterative refinement, foundation model, and zero-shot adaptation paradigms. All methods are trained exclusively on Scene Flow and evaluated without fine-tuning on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury forestry dataset captured with ZED Mini camera (1920x1080). Performance reveals scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D, 0.83-1.07 px on KITTI; DEFOM: 0.35-4.65 px across benchmarks), while iterative methods maintain cross-domain robustness (IGEV++: 0.36-6.77 px; IGEV: 0.33-21.91 px). Critical finding: RAFT-Stereo exhibits catastrophic ETH3D failure (26.23 px EPE, 98 percent error rate) due to negative disparity predictions, while performing normally on KITTI (0.90-1.11 px). Qualitative evaluation on Canterbury forestry dataset identifies DEFOM as the optimal gold-standard baseline for vegetation depth estimation, exhibiting superior depth smoothness, occlusion handling, and cross-domain consistency compared to IGEV++, despite IGEV++'s finer detail preservation.

</details>


### [26] [Label-Efficient Hyperspectral Image Classification via Spectral FiLM Modulation of Low-Level Pretrained Diffusion Features](https://arxiv.org/abs/2512.03430)
*Yuzhen Hu,Biplab Banerjee,Saurabh Prasad*

Main category: cs.CV

TL;DR: 本文提出了一种基于冻结扩散模型空间特征的标签高效框架，用于提升低分辨率、稀疏标注高光谱图像（HSI）的地物分类性能；通过早期去噪层提取低级空间表征，并设计轻量FiLM融合模块自适应融合光谱信息，显著提升了稀疏监督下的多模态学习效果。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）虽能实现精细地物分类，但受限于空间分辨率低和标注数据稀疏两大挑战，亟需标签高效的解决方案。

Method: 利用在自然图像上预训练的冻结扩散模型，提取其高分辨率解码器在早期去噪步中的低级空间特征；设计轻量级FiLM（Feature-wise Linear Modulation）融合模块，以光谱特征为条件自适应调制空间特征，实现光谱-空间联合建模。

Result: 在两个最新高光谱数据集上，仅使用原始稀疏训练标签即超越现有最优方法；消融实验验证了扩散模型特征与光谱感知融合策略的有效性。

Conclusion: 预训练扩散模型可作为通用、标签高效的表征学习工具，适用于遥感及更广泛的科学成像任务。

Abstract: Hyperspectral imaging (HSI) enables detailed land cover classification, yet low spatial resolution and sparse annotations pose significant challenges. We present a label-efficient framework that leverages spatial features from a frozen diffusion model pretrained on natural images. Our approach extracts low-level representations from high-resolution decoder layers at early denoising timesteps, which transfer effectively to the low-texture structure of HSI. To integrate spectral and spatial information, we introduce a lightweight FiLM-based fusion module that adaptively modulates frozen spatial features using spectral cues, enabling robust multimodal learning under sparse supervision. Experiments on two recent hyperspectral datasets demonstrate that our method outperforms state-of-the-art approaches using only the provided sparse training labels. Ablation studies further highlight the benefits of diffusion-derived features and spectral-aware fusion. Overall, our results indicate that pretrained diffusion models can support domain-agnostic, label-efficient representation learning for remote sensing and broader scientific imaging tasks.

</details>


### [27] [Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation](https://arxiv.org/abs/2512.03445)
*Xieji Li,Siyuan Yan,Yingsheng Liu,H. Peter Soyer,Monika Janda,Victoria Mar,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文提出了一种结合多智能体数据生成（MAGEN）与本体驱动的多方面知识增强预训练（O-MAKE）的新型视觉-语言预训练框架，以提升医学图像分析中噪声数据处理与长文本建模能力，在皮肤病学任务上实现了零样本SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言预训练方法受限于网络采集数据的噪声和非结构化长医学文本的建模困难。

Method: 提出MAGEN系统（基于基础模型的标注生成+检索验证）提升数据质量；设计O-MAKE预训练机制，将长文本按医学知识维度分解，并在全局/图像块级进行细粒度对齐，融合本体引导的概念关系建模。

Result: 在8个皮肤病学数据集上实现疾病分类与跨模态检索的零样本SOTA性能；发布含40万皮肤图像-文本对的增强数据集Derm1M-AgentAug及代码。

Conclusion: MAGEN与O-MAKE协同有效缓解了医学VLP中的数据噪声与文本复杂性问题，为高质量、知识驱动的医疗多模态预训练提供了新范式。

Abstract: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.

</details>


### [28] [LM-CartSeg: Automated Segmentation of Lateral and Medial Cartilage and Subchondral Bone for Radiomics Analysis](https://arxiv.org/abs/2512.03449)
*Tongxu Zhang*

Main category: cs.CV

TL;DR: 本文提出LM-CartSeg，一种全自动膝关节MRI软骨/骨分割、内外侧分腔及放射组学分析流程，通过零样本迁移与几何后处理提升分割精度，并实现质量控制与具有判别力的放射组学特征提取。


<details>
  <summary>Details</summary>
Motivation: 现有膝关节放射组学研究多依赖人工勾画ROI，缺乏解剖合理性与质量控制；需自动、稳健、解剖一致的ROI以支持多中心骨关节炎研究。

Method: 使用两个3D nnU-Net模型分别在SKM-TEA和OAIZIB-CM数据集上训练；测试时采用零样本预测融合+几何后处理（连通域清洗、物理空间10mm软骨下骨带构建、基于PCA与k-means的胫骨内外侧分割）；对10个ROI提取4650个非形状放射组学特征，并进行QC（体积/厚度签名）、相似性分析、尺寸依赖性评估及OA分类实验。

Result: 后处理使OAIZIB-CM测试集宏观ASSD从2.63 mm降至0.36 mm，HD95从25.2 mm降至3.35 mm，DSC达0.91；SKI-10上零样本DSC为0.80；几何L/M分割稳定，而直接训练的L/M网络存在跨域侧别混淆；仅6–12%特征强相关于体积或厚度；仅用尺寸关联特征的分类性能显著下降。

Conclusion: LM-CartSeg实现了全自动、可质控的解剖合理ROI生成与富含判别信息的放射组学特征提取，超越简单形态测量，为多中心膝骨关节炎放射组学研究提供了实用基础。

Abstract: Background and Objective: Radiomics of knee MRI requires robust, anatomically meaningful regions of interest (ROIs) that jointly capture cartilage and subchondral bone. Most existing work relies on manual ROIs and rarely reports quality control (QC). We present LM-CartSeg, a fully automatic pipeline for cartilage/bone segmentation, geometric lateral/medial (L/M) compartmentalisation and radiomics analysis. Methods: Two 3D nnU-Net models were trained on SKM-TEA (138 knees) and OAIZIB-CM (404 knees). At test time, zero-shot predictions were fused and refined by simple geometric rules: connected-component cleaning, construction of 10 mm subchondral bone bands in physical space, and a data-driven tibial L/M split based on PCA and k-means. Segmentation was evaluated on an OAIZIB-CM test set (103 knees) and on SKI-10 (100 knees). QC used volume and thickness signatures. From 10 ROIs we extracted 4 650 non-shape radiomic features to study inter-compartment similarity, dependence on ROI size, and OA vs. non-OA classification on OAIZIB-CM Results: Post-processing improved macro ASSD on OAIZIB-CM from 2.63 to 0.36 mm and HD95 from 25.2 to 3.35 mm, with DSC 0.91; zero-shot DSC on SKI-10 was 0.80. The geometric L/M rule produced stable compartments across datasets, whereas a direct L/M nnU-Net showed domain-dependent side swaps. Only 6 to 12 percent of features per ROI were strongly correlated with volume or thickness. Radiomics-based models models restricted to size-linked features. Conclusions: LM-CartSeg yields automatic, QCd ROIs and radiomic features that carry discriminative information beyond simple morphometry, providing a practical foundation for multi-centre knee OA radiomics studies.

</details>


### [29] [KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models](https://arxiv.org/abs/2512.03450)
*Rhys Newbury,Juyan Zhang,Tin Tran,Hanna Kurniawati,Dana Kulić*

Main category: cs.CV

TL;DR: 本文提出了一种无监督框架，从点云数据中学习空间结构化的3D关键点，并将其用于条件化Elucidated Diffusion Model（EDM）以重建完整形状，显著提升了关键点一致性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督关键点方法大多不适用于无条件生成设置，限制了其在现代3D生成流程中的应用；本文旨在填补这一空白。

Method: 提出一种无监督框架，从点云中学习空间结构化3D关键点，并用这些关键点作为紧凑、可解释的表示来条件化EDM进行形状重建。

Result: 所学关键点在不同物体实例间具有可重复的空间结构，并支持关键点空间中的平滑插值，表明其能捕捉几何变化；在多个类别上性能优异，关键点一致性比先前方法提升6个百分点。

Conclusion: 该方法成功将无监督关键点学习与3D生成建模结合，为生成式3D视觉任务提供了更结构化、可解释的中间表征。

Abstract: Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.

</details>


### [30] [GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers](https://arxiv.org/abs/2512.03451)
*Zhiye Song,Steve Dai,Ben Keller,Brucek Khailany*

Main category: cs.CV

TL;DR: 本文提出GalaxyDiT，一种无需训练的视频生成加速方法，通过引导对齐和系统性代理选择，在保持高质量的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频生成中虽表现出色，但计算成本高，尤其是结合分类器自由引导（CFG）时，限制了其在下游应用中的广泛使用。

Method: GalaxyDiT采用训练无关的方法，利用引导对齐与系统性代理选择策略，并通过秩相关性分析为不同模型家族和参数规模确定最优代理，实现计算复用。

Result: 在Wan2.1-1.3B和Wan2.1-14B模型上分别实现1.87×和2.37×加速，VBench-2.0性能仅下降0.97%和0.72%；PSNR指标较先前SOTA提升5–10 dB。

Conclusion: GalaxyDiT在不牺牲生成质量的前提下显著提升视频扩散模型推理效率，为实际部署提供了高效可行的解决方案。

Abstract: Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.
  We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\times$ and $2.37\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).

</details>


### [31] [GeoVideo: Introducing Geometric Regularization into Video Generation Model](https://arxiv.org/abs/2512.03453)
*Yunpeng Bai,Shaoheng Fang,Chaohui Yu,Fan Wang,Qixing Huang*

Main category: cs.CV

TL;DR: 本文提出了一种在视频生成中引入几何正则化损失的方法，通过在潜在扩散模型中增加逐帧深度预测，利用多视角几何损失来对齐帧间深度图，从而提升视频的时空一致性、形状一致性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法大多在2D像素空间操作，缺乏对3D结构的显式建模，导致时序几何不一致、运动不合理和结构伪影等问题。

Method: 在潜在扩散模型中引入逐帧深度预测，并设计基于共享3D坐标系的多视角几何损失，对齐各帧预测的深度图，以增强结构时序一致性。

Result: 在多个数据集上的实验表明，该方法相比现有基线显著提升了生成视频的稳定性与几何一致性。

Conclusion: 将几何先验（深度）融入视频扩散模型可有效提升生成结果的3D结构合理性和时空连贯性，为视频生成提供了新的建模思路。

Abstract: Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.

</details>


### [32] [Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles](https://arxiv.org/abs/2512.03454)
*Haicheng Liao,Huanming Shen,Bonan Wang,Yongkang Li,Yihong Tang,Chengyue Wang,Dingyi Zhuang,Kehua Chen,Hai Yang,Chengzhong Xu,Zhenning Li*

Main category: cs.CV

TL;DR: 本文提出ThinkDeeper框架，结合空间感知世界模型（SA-WM）与超图引导解码器，提升自动驾驶中自然语言指令到目标物体定位的视觉指代能力；并构建新数据集DrivePilot，实验表明其在多个基准上达到SOTA，尤其在模糊、多智能体和长文本场景下鲁棒高效。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶中的视觉指代方法难以处理模糊、依赖上下文的自然语言指令，缺乏对3D空间关系和场景动态演化的建模能力。

Method: 提出ThinkDeeper框架：1）空间感知世界模型（SA-WM），将当前场景编码为指令感知的潜在状态，并前向推演未来潜在状态以提供消歧线索；2）超图引导解码器，分层融合多模态输入与前向状态，建模高阶空间依赖；同时构建基于RAG+CoT大模型生成的DrivePilot多源视觉指代数据集。

Result: 在Talk2Car榜单排名第一，在DrivePilot、MoCAD及RefCOCO系列基准上超越SOTA；在长文本、多智能体、模糊等挑战场景中表现鲁棒；仅用50%训练数据仍保持优越性能。

Conclusion: ThinkDeeper通过引入具备时空推理能力的世界模型与结构化融合机制，显著提升了自动驾驶中复杂语言指令下的视觉指代精度与泛化性，验证了前向推理与高阶空间建模的有效性。

Abstract: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.

</details>


### [33] [Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models](https://arxiv.org/abs/2512.03463)
*Shojiro Yamabe,Futa Waseda,Daiki Shiono,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: 本文提出了一种名为Text-Printed Image（TPI）的低成本文本中心训练方法，通过将文本直接渲染为白底图像来弥合图文模态差距，显著提升大视觉语言模型（LVLMs）在VQA任务上的性能，无需真实图像或复杂合成。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在VQA任务中依赖大量带标注的真实图像-文本对进行任务特定微调，但图像采集受限于隐私、稀缺性和高成本；而文本更易获取、编辑和规模化生成，因此亟需一种高效利用纯文本数据的低代价训练范式。

Method: 提出Text-Printed Image（TPI）：将输入文本直接渲染为白底RGB图像（即文字截图），作为图像模态代理；该方法轻量、可插拔，兼容任意LVLM训练流程，且语义保真度高于扩散模型生成的合成图像。

Result: 在4个LVLM和7个VQA基准上，TPI显著优于扩散模型生成的合成图像；同时验证其作为低成本数据增强策略的有效性与实用性。

Conclusion: TPI证明了文本中心训练的巨大潜力，为LVLMs迈向全自动、零图像依赖的数据生成提供了可行路径。

Abstract: Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.

</details>


### [34] [Difference Decomposition Networks for Infrared Small Target Detection](https://arxiv.org/abs/2512.03470)
*Chen Hu,Mingyu Zhou,Shuai Yuan,Hongbo Hu,Xiangyu Qiu,Junhai Luo,Tian Pu,Xiyin Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于基分解的轻量级模块BDM及其扩展模块（SD²M、SD³M、TD²M），构建了单帧（SD²Net）和多帧（STD²Net）红外小目标检测网络，在SISTD和MISTD任务上均达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测面临目标纹理不明显和背景杂波严重两大挑战，导致目标易被背景掩盖。

Method: 提出基分解模块（BDM）及多个扩展模块（SD²M、SD³M、TD²M），并基于其构建SD²Net（用于单帧检测）和STD²Net（用于多帧检测），后者引入运动信息提升性能。

Result: 在MISTD数据集上，STD²Net取得87.68%的mIoU，显著优于SD²Net的64.97%；在SISTD任务上也优于多数现有方法。

Conclusion: 所提模块与网络结构有效增强了目标、抑制了背景，在单帧和多帧红外小目标检测中均展现出优越性能和泛化能力。

Abstract: Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.

</details>


### [35] [Procedural Mistake Detection via Action Effect Modeling](https://arxiv.org/abs/2512.03474)
*Wenliang Guo,Yujiang Pu,Yu Kong*

Main category: cs.CV

TL;DR: 本文提出Action Effect Modeling (AEM)框架，通过联合建模动作执行过程及其效果（action effect），提升程序性任务中的错误检测性能，在EgoPER和CaptainCook4D数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注动作如何执行，而忽略其产生的效果（action effect）；许多错误实际体现在结果（如物体状态或空间布局异常）而非执行过程本身。

Method: AEM框架包括：1）基于语义相关性和视觉质量选择最具信息量的效果帧；2）融合视觉定位与符号化场景图，在共享隐空间中对齐以构建鲁棒的效果感知表征；3）设计基于提示的检测器，结合任务特定提示对齐动作段与其预期语义。

Result: 在EgoPER和CaptainCook4D基准的一类分类（OCC）设置下达到当前最优性能。

Conclusion: 同时建模动作执行与效果能显著提升错误检测可靠性；效果感知表征有望拓展至更广泛的下游应用。

Abstract: Mistake detection in procedural tasks is essential for building intelligent systems that support learning and task execution. Existing approaches primarily analyze how an action is performed, while overlooking what it produces, i.e., the \textbf{action effect}. Yet many errors manifest not in the execution itself but in the resulting outcome, such as an unintended object state or incorrect spatial arrangement. To address this gap, we propose Action Effect Modeling (AEM), a unified framework that jointly captures action execution and its outcomes through a probabilistic formulation. AEM first identifies the outcome of an action by selecting the most informative effect frame based on semantic relevance and visual quality. It then extracts complementary cues from visual grounding and symbolic scene graphs, aligning them in a shared latent space to form robust effect-aware representations. To detect mistakes, we further design a prompt-based detector that incorporates task-specific prompts and aligns each action segment with its intended execution semantics. Our approach achieves state-of-the-art performance on the EgoPER and CaptainCook4D benchmarks under the challenging one-class classification (OCC) setting. These results demonstrate that modeling both execution and outcome yields more reliable mistake detection, and highlight the potential of effect-aware representations to benefit a broader range of downstream applications.

</details>


### [36] [CSMapping: Scalable Crowdsourced Semantic Mapping and Topology Inference for Autonomous Driving](https://arxiv.org/abs/2512.03510)
*Zhijian Qiao,Zehuan Yu,Tong Li,Chih-Chung Chou,Wenchao Ding,Shaojie Shen*

Main category: cs.CV

TL;DR: 本文提出CSMapping系统，利用潜在扩散模型学习高精地图结构先验，并通过带约束的最大后验优化提升语义与拓扑地图构建鲁棒性与可扩展性，显著提升众包低质传感器数据下的建图质量。


<details>
  <summary>Details</summary>
Motivation: 众包方式可扩展地构建自动驾驶地图，但低成本传感器噪声导致地图质量难以随数据量增加而提升。

Method: 提出CSMapping系统：1）语义映射采用基于HD地图训练的潜在扩散模型（可选条件于SD地图），通过潜空间约束MAP优化实现抗噪与合理补全；初始化使用鲁棒向量化建图+扩散反演，优化采用高斯基重参数化、投影梯度下降多起点搜索及潜空间因子图保障全局一致性；2）拓扑映射采用置信度加权k-medoids聚类与运动学优化轨迹，生成平滑、类人中心线。

Result: 在nuScenes、Argoverse 2及大型私有数据集上达到语义与拓扑建图SOTA性能，并完成详尽消融与可扩展性分析。

Conclusion: CSMapping有效克服众包数据噪声问题，使地图质量随数据规模持续提升，为大规模低成本自动驾驶地图构建提供了新范式。

Abstract: Crowdsourcing enables scalable autonomous driving map construction, but low-cost sensor noise hinders quality from improving with data volume. We propose CSMapping, a system that produces accurate semantic maps and topological road centerlines whose quality consistently increases with more crowdsourced data. For semantic mapping, we train a latent diffusion model on HD maps (optionally conditioned on SD maps) to learn a generative prior of real-world map structure, without requiring paired crowdsourced/HD-map supervision. This prior is incorporated via constrained MAP optimization in latent space, ensuring robustness to severe noise and plausible completion in unobserved areas. Initialization uses a robust vectorized mapping module followed by diffusion inversion; optimization employs efficient Gaussian-basis reparameterization, projected gradient descent zobracket multi-start, and latent-space factor-graph for global consistency. For topological mapping, we apply confidence-weighted k-medoids clustering and kinematic refinement to trajectories, yielding smooth, human-like centerlines robust to trajectory variation. Experiments on nuScenes, Argoverse 2, and a large proprietary dataset achieve state-of-the-art semantic and topological mapping performance, with thorough ablation and scalability studies.

</details>


### [37] [Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis](https://arxiv.org/abs/2512.03477)
*Zijian Gu,Yuxi Liu,Zhenhao Zhang,Song Wang*

Main category: cs.CV

TL;DR: 本文提出了一种面向公平性的低秩自适应（Fairness-aware Low-Rank Adaptation）方法，用于缓解医疗视觉语言模型在不同人群中的诊断准确率差异，通过新型可微分MaxAccGap损失实现端到端的准确性均衡优化，并在真实眼底图像数据上验证了其有效性与高效性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗视觉语言模型虽性能优异，但在不同人口统计学群体间存在显著诊断准确率差异，亟需兼顾公平性与效率的适配方法。

Method: 提出三种基于LoRA的公平性优化方法：FR-LoRA（引入MaxAccGap正则化）、GR-LoRA（采用逆频率梯度加权）和Hybrid-LoRA（二者结合），核心是可微分的MaxAccGap损失函数以端到端优化各组准确性差距。

Result: 在10,000张青光眼眼底图像上，GR-LoRA将诊断准确率差异降低69%，整体准确率达53.15%；仅需0.24%可训练参数；消融实验表明强正则化与种族特异性优化分别带来最优公平性与60%差异降低。

Conclusion: 所提方法在极低参数开销下显著提升医疗VLM的跨群体公平性，具备在资源受限临床场景中实际部署的可行性。

Abstract: Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.

</details>


### [38] [PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention](https://arxiv.org/abs/2512.03724)
*Ziwen Li,Xin Wang,Hanlue Zhang,Runnan Chen,Runqi Lin,Xiao He,Han Huang,Yandong Guo,Fakhri Karray,Tongliang Liu,Mingming Gong*

Main category: cs.CV

TL;DR: 本文提出PosA-VLA框架，通过姿态条件引导的视觉注意力机制，提升VLA模型在具身任务中动作生成的精确性与效率，无需额外感知模块，具备轻量、高效、泛化强等优势。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型因空间均匀感知场易受无关物体干扰，导致动作冗余、不稳定，难以满足实时性要求。

Method: 提出PosA-VLA框架，引入姿态条件锚定的注意力机制（pose-conditioned anchor attention），将视觉注意聚焦于任务相关区域，并采用轻量架构，不依赖分割或定位等辅助模块。

Result: 在多个机器人操作基准上验证了方法在动作精度、执行时效性和环境鲁棒性方面的显著提升。

Conclusion: 姿态引导的注意力机制可有效缓解VLA模型的感知干扰问题，提升目标导向动作的一致性与可靠性，为实际部署提供新思路。

Abstract: The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.

</details>


### [39] [Towards Object-centric Understanding for Instructional Videos](https://arxiv.org/abs/2512.03479)
*Wenliang Guo,Yu Kong*

Main category: cs.CV

TL;DR: 本文提出了一种以对象为中心的程序性活动理解新范式，构建了Object-IVQA视频问答基准，并设计了一个多工具协同的智能体框架，在对象状态演化、前提验证、反事实推理和错误识别等维度上显著优于现有大视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有以动作为中心的方法难以应对真实流程中因物体状态变化而导致的步骤顺序灵活性问题，亟需转向以对象状态演变为核心的理解范式。

Method: 提出对象中心范式，将动作视为驱动物体状态转移的机制；构建Object-IVQA基准（107个教学视频、514个开放问答对，含时序标注证据）；设计融合对象级规划、感知、分析与生成工具的智能体框架，支持跨片段显式证据检索与多跳推理。

Result: 实验表明，现有大视觉语言模型在对象级识别与推理上表现较差，而所提框架在Object-IVQA各项任务上取得显著性能提升。

Conclusion: 以对象为中心的建模范式更契合真实 procedural reasoning 需求，结合结构化工具协同的智能体架构是提升复杂视频理解能力的有效路径。

Abstract: Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks. Existing action-centric methods struggle with the flexibility of real procedures, where step order varies depending on object states. In this work, we propose to shift the focus to an object-centric paradigm by regarding actions as mechanisms that drive state transitions. To advance this direction, we introduce Object-IVQA, a long-form instructional video benchmark with 107 videos and 514 open-ended question-answer pairs annotated with temporally grounded evidence. The benchmark evaluates four dimensions of object-centric reasoning, including state evolution, precondition verification, counterfactual reasoning and mistake recognition. We further propose an agent framework that orchestrates object-centric planning, perception, analysis and generation tools, enabling explicit evidence retrieval and multi-hop reasoning across disjoint segments. Experiments show that existing large vision-language models struggle in object-level recognition and reasoning, whereas our framework achieves substantially improvement.

</details>


### [40] [MUT3R: Motion-aware Updating Transformer for Dynamic 3D Reconstruction](https://arxiv.org/abs/2512.03939)
*Guole Shen,Tianchen Deng,Xingrui Qin,Nailin Wang,Jianyu Wang,Yanbo Wang,Yongtao Chen,Hesheng Wang,Jingchuan Wang*

Main category: cs.CV

TL;DR: 本文提出MUT3R框架，利用预训练Transformer中隐含但未被利用的注意力图运动线索，在推理阶段无需重训练即可抑制动态区域影响，提升动态场景下3D重建的时序一致性和相机位姿鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有状态化循环神经网络在静态3D重建中表现优异，但在动态场景中易受运动伪影干扰，尤其非刚性区域会破坏空间记忆与图像特征间的注意力传播；作者发现跨层自注意力图聚合可自然凸显动态区域，揭示出预训练Transformer已隐式编码但未显式利用的运动线索。

Method: 提出训练无关的MUT3R框架，设计注意力级门控模块，在推理时利用多层自注意力图聚合生成的隐式运动线索，于Transformer早期层抑制动态区域响应，阻止其伪影向高层特征传播；全程不进行任何重训练或微调。

Result: 在多个动态基准上验证了该方法的有效性，显著提升了时序一致性与相机位姿鲁棒性，尤其适用于流式重建场景。

Conclusion: 预训练Transformer自身已蕴含可被挖掘的运动感知能力；通过注意力机制引导的早期动态内容抑制，是一种简单、高效且无需训练的运动感知流式3D重建新范式。

Abstract: Recent stateful recurrent neural networks have achieved remarkable progress on static 3D reconstruction but remain vulnerable to motion-induced artifacts, where non-rigid regions corrupt attention propagation between the spatial memory and image feature. By analyzing the internal behaviors of the state and image token updating mechanism, we find that aggregating self-attention maps across layers reveals a consistent pattern: dynamic regions are naturally down-weighted, exposing an implicit motion cue that the pretrained transformer already encodes but never explicitly uses. Motivated by this observation, we introduce MUT3R, a training-free framework that applies the attention-derived motion cue to suppress dynamic content in the early layers of the transformer during inference. Our attention-level gating module suppresses the influence of dynamic regions before their artifacts propagate through the feature hierarchy. Notably, we do not retrain or fine-tune the model; we let the pretrained transformer diagnose its own motion cues and correct itself. This early regulation stabilizes geometric reasoning in streaming scenarios and leads to improvements in temporal consistency and camera pose robustness across multiple dynamic benchmarks, offering a simple and training-free pathway toward motion-aware streaming reconstruction.

</details>


### [41] [NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation](https://arxiv.org/abs/2512.03499)
*Renqi Chen,Haoyang Su,Shixiang Tang*

Main category: cs.CV

TL;DR: 本文提出NAS-LoRA，一种结合神经架构搜索（NAS）与低秩自适应（LoRA）的参数高效微调方法，用于提升SAM在医学、农业等特定领域图像分割任务中的适应能力；通过在LoRA编解码器间嵌入轻量NAS模块并采用分阶段优化策略，增强空间先验与高层语义学习，在降低24.14%训练成本的同时不增加推理开销。


<details>
  <summary>Details</summary>
Motivation: SAM的Transformer编码器缺乏图像块内的空间先验，难以有效获取高层语义信息，限制其在医疗、农业等专业领域的适配效果；现有LoRA类方法虽提升适配性能，但未显式引入归纳偏置。

Method: 提出NAS-LoRA：在LoRA的编码器与解码器之间插入轻量级神经架构搜索（NAS）模块，动态优化权重更新中融入的先验知识；并设计分阶段优化策略，协调ViT编码器的权重更新与架构调整，以渐进式学习高层语义。

Result: NAS-LoRA在多个实验中优于现有PEFT方法，训练成本降低24.14%，推理成本无增加。

Conclusion: 将NAS引入PEFT框架可有效弥补SAM的空间先验缺失，提升其跨领域泛化能力，验证了NAS在视觉基础模型高效适配中的潜力。

Abstract: The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.

</details>


### [42] [SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL](https://arxiv.org/abs/2512.04069)
*Siyi Chen,Mikaela Angelina Uy,Chan Hee Song,Faisal Ladhak,Adithyavairavan Murali,Qing Qu,Stan Birchfield,Valts Blukis,Jonathan Tremblay*

Main category: cs.CV

TL;DR: 本文提出Double Interactive Reinforcement Learning (DIRL)框架，使视觉语言模型（VLMs）能自主协调多种视觉工具（如深度估计、分割、位姿估计）以提升空间推理能力，并在多个空间理解基准和真实机器人操作任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs虽具强定性视觉理解能力，但在需度量精度的空间推理（尤其具身智能应用）上表现不足；现有工具增强方法依赖手工提示或固定工具流程，限制了VLM自主发现最优工具组合的能力；多工具强化学习因搜索空间过大而难以实现。

Method: 提出两阶段训练框架DIRL：教学阶段融合单工具专家（通过交互式RL训练）的示范与前沿模型（使用全部工具）的执行轨迹；探索阶段通过持续交互式RL进一步优化多工具协同策略；构建工具增强型模型SpaceTools。

Result: SpaceTools在RoboSpatial-Home、BLINK、BOP-ASK等空间理解基准上达到SOTA；在7-DOF真实机器人操控任务中展现可靠性能；相比监督微调基线提升12%，相比单工具RL基线提升16%。

Conclusion: DIRL有效解决了VLM多工具协同学习的难题，为具身智能中灵活、鲁棒的空间推理提供了可扩展新范式。

Abstract: Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.

</details>


### [43] [EEA: Exploration-Exploitation Agent for Long Video Understanding](https://arxiv.org/abs/2512.03500)
*Te Yang,Xiangyu Zhu,Bo Wang,Quan Chen,Peng Jiang,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出EEA框架，通过语义引导的分层树搜索实现探索-利用平衡，提升长视频理解的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解方法存在计算开销大或探索-利用失衡问题，导致信息覆盖不全、效率低下。

Method: 提出EEA视频智能体框架：动态生成语义查询作为锚点，基于VLM内在奖励与语义先验建模不确定性，进行非均匀的语义引导树搜索。

Result: 在多个长视频基准上验证了EEA在性能和计算效率上的优越性。

Conclusion: EEA通过语义引导与不确定性感知的树搜索，有效解决了长视频理解中探索与利用的权衡难题，兼具高效性与鲁棒性。

Abstract: Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.

</details>


### [44] [Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation](https://arxiv.org/abs/2512.03508)
*Seogkyu Jeon,Kibeom Hong,Hyeran Byun*

Main category: cs.CV

TL;DR: 本文提出DPMFormer，一种用于域泛化语义分割的新框架，通过域感知提示学习、对比学习与纹理扰动、以及域鲁棒一致性学习，缓解视觉-文本语义错位问题，提升跨域分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的知识蒸馏方法忽视了因固定上下文提示在单一源域上学习而导致的视觉与文本语义错位问题。

Method: 提出域感知提示学习以对齐视觉与文本线索；结合纹理扰动的域感知对比学习以模拟多样域特性；引入域鲁棒一致性学习，最小化原始图像与增强图像预测结果的差异。

Result: 在多个域泛化语义分割基准上达到新SOTA性能。

Conclusion: DPMFormer有效缓解语义错位，提升模型跨域泛化能力，为DGSS提供了更鲁棒、更具适应性的解决方案。

Abstract: Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.

</details>


### [45] [AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model](https://arxiv.org/abs/2512.03509)
*Kwaku Opoku-Ware,Gideon Opoku*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv8/v11与Segment Anything Model（SAM）的无标记自动舞蹈动作分析框架，实现了高精度 dancer 检测与像素级运动量化，在单段非洲AfroBeats舞蹈视频中验证了可行性，但尚处初步探索阶段。


<details>
  <summary>Details</summary>
Motivation: 解决传统舞蹈分析依赖专业设备和人工标注的问题，探索无需标记、低成本、可量化的自动化舞蹈运动分析方法。

Method: 融合YOLOv8/v11进行舞者检测，结合SAM实现像素级精准分割，进而追踪并量化步数、空间覆盖、节奏一致性等运动指标。

Result: 在49秒Ghanaian AfroBeats视频中达到94%检测精度、89%召回率；SAM分割IoU达83%；发现主舞者比次舞者多23%步数、高37%运动强度、多用42%表演空间。

Conclusion: 该框架技术可行，能超越边界框方法捕捉身体构型变化，但受限于单视频验证、缺乏系统真值标注及基线对比，仅为后续定量舞蹈研究提供初步基础与方向指引。

Abstract: This paper presents a preliminary investigation into automated dance movement analysis using contemporary computer vision techniques. We propose a proof-of-concept framework that integrates YOLOv8 and v11 for dancer detection with the Segment Anything Model (SAM) for precise segmentation, enabling the tracking and quantification of dancer movements in video recordings without specialized equipment or markers. Our approach identifies dancers within video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency across performance sequences. Testing this framework on a single 49-second recording of Ghanaian AfroBeats dance demonstrates technical feasibility, with the system achieving approximately 94% detection precision and 89% recall on manually inspected samples. The pixel-level segmentation provided by SAM, achieving approximately 83% intersection-over-union with visual inspection, enables motion quantification that captures body configuration changes beyond what bounding-box approaches can represent. Analysis of this preliminary case study indicates that the dancer classified as primary by our system executed 23% more steps with 37% higher motion intensity and utilized 42% more performance space compared to dancers classified as secondary. However, this work represents an early-stage investigation with substantial limitations including single-video validation, absence of systematic ground truth annotations, and lack of comparison with existing pose estimation methods. We present this framework to demonstrate technical feasibility, identify promising directions for quantitative dance metrics, and establish a foundation for future systematic validation studies.

</details>


### [46] [CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding](https://arxiv.org/abs/2512.03558)
*Huy Quang Ung,Guillaume Habault,Yasutaka Nishimura,Hao Niu,Roberto Legaspi,Tomoki Oya,Ryoichi Kojima,Masato Taya,Chihiro Ono,Atsunori Minamikawa,Yan Liu*

Main category: cs.CV

TL;DR: 本文提出了CartoMapQA基准，用于评估视觉-语言模型（LVLMs）对地图的理解能力，发现现有模型在地图语义理解、地理空间推理和OCR相关错误方面存在明显缺陷。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（LVLMs）在多模态理解上取得进展，但其对专业领域如地图的理解能力尚未被系统评估，亟需专门的评测基准。

Method: 构建了包含2000多个样本的CartoMapQA数据集，涵盖低、中、高层地图理解任务（如符号识别、信息提取、比例尺理解、路径推理），并基于该基准对开源与商用LVLM进行系统评测。

Result: 实验表明当前LVLM在地图语义理解、地理空间推理能力及OCR鲁棒性方面存在显著不足。

Conclusion: CartoMapQA为提升LVLM的地图理解能力提供了可复现的评测工具和明确改进方向，推动其在导航、地理搜索和城市规划等实际场景中的应用。

Abstract: The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git

</details>


### [47] [Optical Context Compression Is Just (Bad) Autoencoding](https://arxiv.org/abs/2512.03643)
*Ivan Yee Lee,Cheng Yang,Taylor Berg-Kirkpatrick*

Main category: cs.CV

TL;DR: 本文质疑DeepSeek-OCR提出的基于视觉的文本压缩方法在语言建模中的有效性，发现更简单的压缩方法（如均值池化、分层编码器）在文本重建和语言建模任务上均优于其视觉编码器，表明当前对光学上下文压缩的乐观预期缺乏充分证据。


<details>
  <summary>Details</summary>
Motivation: 验证DeepSeek-OCR所倡导的视觉基上下文压缩是否真正在语言建模中具有独特优势，而非仅限于高保真文本重建。

Method: 将DeepSeek-OCR的视觉编码器与两种简单替代方案（无参的均值池化、有参的分层编码器）在相同压缩比下进行对比，评估其在文本重建与下游语言建模任务上的性能。

Result: 简单方法在文本重建上匹配或超越视觉编码器；在语言建模任务中，视觉压缩甚至不如简单截断（truncation），表现最差。

Conclusion: 当前对光学上下文压缩的兴奋缺乏实证支持，视觉编码并非必要或最优选择，应谨慎对待其在多模态语言建模中的推广。

Abstract: DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding

</details>


### [48] [FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation](https://arxiv.org/abs/2512.03520)
*Yiyi Cai,Yuhan Wu,Kunhang Li,You Zhou,Bo Zheng,Haiyang Liu*

Main category: cs.CV

TL;DR: FloodDiffusion是一种面向实时文本驱动的人体运动生成的新框架，采用改进的扩散强制（diffusion forcing）方法，在保证分布建模准确性的前提下实现流式、无缝、文本对齐的动作生成，并在HumanML3D上达到SOTA性能（FID=0.057）。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖分块或自回归+扩散头结构，难以兼顾实时性、连贯性与文本对齐；而标准扩散强制方法直接迁移到人体运动生成任务时无法准确建模真实运动分布。

Method: 提出适配运动生成任务的扩散强制框架，关键改进包括：(i) 使用双向注意力替代因果注意力；(ii) 采用下三角时间调度器替代随机调度；(iii) 以连续时变方式引入文本条件。

Result: 在HumanML3D基准上取得当前最优FID值0.057，支持实时、流式、文本对齐且无缝的人体动作生成。

Conclusion: 扩散强制框架经针对性改造后可高效建模时序人体运动，为流式文本驱动动作生成提供了新范式。

Abstract: We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/

</details>


### [49] [Thinking with Programming Vision: Towards a Unified View for Thinking with Images](https://arxiv.org/abs/2512.03746)
*Zirun Guo,Minjie Hong,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.CV

TL;DR: 本文提出CodeVision框架，通过将代码作为通用工具接口来增强多模态大语言模型（MLLMs）对图像的鲁棒推理能力，解决了现有工具调用方法脆弱、不灵活的问题；采用两阶段训练（SFT+RL），并在新构建的鲁棒性基准上验证了其在方向变化、多工具协同与错误恢复等方面的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs依赖固定、有限的工具集，面对图像方向变化或自然损坏时表现脆弱，缺乏真实场景所需的鲁棒性与可扩展性。

Method: 提出CodeVision框架，以生成代码作为通用图像操作接口；采用两阶段训练：先在高质量多轮工具组合与错误恢复数据集上进行监督微调（SFT），再通过带稠密过程奖励函数的强化学习（RL）优化工具使用策略；并构建了新的SFT/RL数据集及面向鲁棒性和多工具推理的评测基准。

Result: 在Qwen2.5-VL和Qwen3-VL系列模型上验证，显著提升性能，并涌现出灵活工具组合、高效链式执行和基于运行时反馈的鲁棒错误恢复等新能力。

Conclusion: 代码即工具是一种更灵活、可扩展且鲁棒的MLLM视觉推理范式；CodeVision为构建具备真实世界适应能力的多模态智能体提供了可行路径。

Abstract: Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.

</details>


### [50] [OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation](https://arxiv.org/abs/2512.03532)
*Zhishan Zhou,Siyuan Wei,Zengran Wang,Chunjie Wang,Xiaosheng Yan,Xiao Liu*

Main category: cs.CV

TL;DR: OpenTrack3D 提出了一种无需网格、在线生成跨视角一致物体提议的通用开放词汇3D实例分割框架，结合DINO特征提取、视觉-空间跟踪与多模态大语言模型（MLLM）提升文本推理能力，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在无网格、非结构化场景中泛化能力差：一是依赖数据集特定或基于网格的提议生成方式；二是CLIP类分类器对组合性、功能性用户查询的文本理解能力弱。

Method: 提出OpenTrack3D框架：1）利用2D开放词汇分割器生成掩码并借助深度图提升至3D点云；2）基于DINO特征图提取掩码引导的实例特征；3）设计视觉-空间跟踪器在线构建跨视角一致的物体提议；4）可选超点精炼模块（当网格可用时）；5）用多模态大语言模型（MLLM）替代CLIP以增强组合语义推理。

Result: 在ScanNet200、Replica、ScanNet++和SceneFun3D等多个多样化基准上实现SOTA性能，展现出强泛化能力与高精度。

Conclusion: OpenTrack3D成功解决了开放词汇3D实例分割在无网格、动态真实场景中的泛化瓶颈，通过端到端mesh-free设计与MLLM驱动的语义理解，为机器人与AR/VR应用提供了更鲁棒、灵活的感知基础。

Abstract: Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; and (2) the weak textual reasoning of CLIP-based classifiers, which struggle to recognize compositional and functional user queries. To address these issues, we introduce OpenTrack3D, a generalizable and accurate framework. Unlike methods that rely on pre-generated proposals, OpenTrack3D employs a novel visual-spatial tracker to construct cross-view consistent object proposals online. Given an RGB-D stream, our pipeline first leverages a 2D open-vocabulary segmenter to generate masks, which are lifted to 3D point clouds using depth. Mask-guided instance features are then extracted using DINO feature maps, and our tracker fuses visual and spatial cues to maintain instance consistency. The core pipeline is entirely mesh-free, yet we also provide an optional superpoints refinement module to further enhance performance when scene mesh is available. Finally, we replace CLIP with a multi-modal large language model (MLLM), significantly enhancing compositional reasoning for complex user queries. Extensive experiments on diverse benchmarks, including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrate state-of-the-art performance and strong generalization capabilities.

</details>


### [51] [AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition](https://arxiv.org/abs/2512.03794)
*Zichuan Lin,Yicheng Liu,Yang Yang,Lvfang Tao,Deheng Ye*

Main category: cs.CV

TL;DR: 本文提出AdaptVision，一种基于主动视觉机制的高效视觉语言模型范式，通过粗到细的自适应视觉标记获取策略，在保证准确率的同时显著减少视觉标记数量。


<details>
  <summary>Details</summary>
Motivation: 现有高效视觉语言模型采用固定比例压缩视觉标记，缺乏对不同任务需求的自适应能力，因此需要一种能自主决定每样本所需最少视觉标记数的方法。

Method: 提出AdaptVision模型，结合低分辨率图像初始处理与按需调用边界框工具裁剪关键区域；采用强化学习训练框架，并设计解耦回合策略优化（DTPO）方法，将目标分解为工具使用优化和答案准确性提升两部分，并分别估计其优势函数。

Result: 在多个VQA基准上实验表明，AdaptVision在性能优于现有最先进高效VLM方法的同时，显著减少了视觉标记消耗。

Conclusion: AdaptVision验证了自适应视觉标记获取的有效性，为构建高效、智能的视觉语言模型提供了新思路。

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.

</details>


### [52] [Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation](https://arxiv.org/abs/2512.03534)
*Subin Kim,Sangwoo Mo,Mamshad Nayeem Rizve,Yiran Xu,Difan Liu,Jinwoo Shin,Tobias Hinz*

Main category: cs.CV

TL;DR: 本文提出PRIS框架，在推理时动态重设计提示词以提升文本到视觉生成的对齐精度，通过细粒度的事实校验器评估并修正提示与生成结果间的不一致，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 单一固定提示难以满足用户对生成图像的精确意图对齐需求，而单纯扩大采样规模易达质量瓶颈。

Method: 提出PRIS（Prompt Redesign for Inference-time Scaling）框架：在推理过程中基于多生成样本识别失败模式，利用新提出的元素级事实校验器（element-level factual correction）提供细粒度对齐反馈，并据此自适应重写提示词，再重新生成。

Result: 在文本到图像和文本到视频多个基准上验证有效，VBench 2.0提升15%；证明联合缩放提示与视觉内容是发挥推理时缩放定律潜力的关键。

Conclusion: 动态重设计提示词与视觉生成协同缩放，比单纯增加采样更有效地提升生成对齐质量；细粒度、可解释的验证机制对提示优化至关重要。

Abstract: Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.

</details>


### [53] [Stable Signer: Hierarchical Sign Language Generative Model](https://arxiv.org/abs/2512.04048)
*Sen Fang,Yalin Feng,Hongbin Zhong,Yanxin Zhang,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 本文提出Stable Signer模型，将手语生成（SLP）任务简化为端到端的层次化生成任务（文本理解+姿态到视频），引入SLUL模块和SLP-MoE专家块，并设计SAGM损失，性能较SOTA提升48.6%。


<details>
  <summary>Details</summary>
Motivation: 传统SLP方法分阶段（Text2Gloss、Gloss2Pose、Pose2Vid等）导致误差累积，且各阶段转换不准确、姿态生成与视频渲染质量低，进展缓慢。

Method: 提出端到端的Stable Signer模型：1）将SLP重构为Prompt2Gloss/Text2Gloss + Pose2Vid两阶段；2）设计Sign Language Understanding Linker（SLUL）用于文本理解；3）构建SLP-MoE手部动作渲染专家模块；4）引入Semantic-Aware Gloss Masking Loss（SAGM Loss）训练SLUL。

Result: Stable Signer在手语视频生成任务中性能较当前最优方法（SOTA）提升48.6%，可端到端生成高质量、多风格手语视频。

Conclusion: 通过精简任务流程、创新模块设计与专用损失函数，Stable Signer有效缓解了传统多阶段SLP中的误差累积问题，显著提升了生成质量与效率，为SLP提供了更优的端到端范式。

Abstract: Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.

</details>


### [54] [CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation](https://arxiv.org/abs/2512.03540)
*Ruoxuan Zhang,Bin Wen,Hongxia Xie,Yi Yao,Songhan Zuo,Jian-Yu Jiang-Lin,Hong-Han Shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: 本文提出CookAnything框架，利用扩散模型解决菜谱多步图文生成问题，支持任意长度菜谱的连贯、语义清晰且成分一致的图像序列生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型难以处理结构化多步骤任务（如菜谱 illustration），且当前方法无法适配菜谱实际步骤长度变化，生成固定数量图像，缺乏灵活性与跨步一致性。

Method: 提出基于扩散模型的CookAnything框架，包含三个核心组件：(1) Step-wise Regional Control（SRC），在单次去噪过程中对齐文本步骤与图像区域；(2) Flexible RoPE，一种步长感知的位置编码，提升时序连贯性与空间多样性；(3) Cross-Step Consistency Control（CSCC），保障跨步骤间食材细节一致性。

Result: 在菜谱illustration基准测试中，CookAnything在训练型和免训练设置下均优于现有方法，支持可扩展、高质量的多步指令视觉合成。

Conclusion: CookAnything为多步程序性文本到图像生成提供了灵活、一致且实用的新范式，在教学媒体与程序化内容生成等领域具有广泛应用前景。

Abstract: Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.

</details>


### [55] [V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention](https://arxiv.org/abs/2512.03542)
*Nan Sun,Zhenyu Zhang,Xixun Lin,Kun Wang,Yanmin Shang,Naibin Gu,Shuohuan Wang,Yu Sun,Hua Wu,Haifeng Wang,Yanan Cao*

Main category: cs.CV

TL;DR: 本文提出V-ITI框架，通过检测MLLM中头部激活模式识别视觉忽视，并仅在检测到时进行轻量级视觉激活干预，从而有效缓解幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了‘何时干预’这一关键问题，导致过度干预、新幻觉和计算开销增加；根本原因是MLLM存在视觉忽视现象。

Method: 提出V-ITI框架，包含两个核心组件：基于头级判别探针的视觉忽视检测器（Visual Neglect Detector）和仅在检测到忽视时调用预存视觉激活信息的视觉回溯干预器（Visual Recall Intervenor）。

Result: 在八个基准和多个MLLM家族上验证，V-ITI显著缓解视觉相关幻觉，同时保持通用任务性能。

Conclusion: 视觉忽视可被头级激活模式可靠检测，针对性、按需的推理时干预是更高效可靠的幻觉缓解路径。

Abstract: Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on "how to intervene" but overlooking the prerequisite "when to intervene", which leads to the "over-intervention" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.

</details>


### [56] [Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching](https://arxiv.org/abs/2512.03553)
*Wei Chee Yew,Hailun Xu,Sanjay Saha,Xiaotian Fan,Hiok Hian Ong,David Yuchen Wang,Kanchan Sarkar,Zhenheng Yang,Danhui Guan*

Main category: cs.CV

TL;DR: 本文提出了一种用于直播平台内容审核的混合框架，结合监督分类与基于参考的相似性匹配，并利用多模态大语言模型（MLLM）提升准确率，兼顾实时性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 直播平台需及时、多模态且鲁棒地审核用户生成视频内容，尤其应对新型和隐蔽违规行为存在挑战。

Method: 构建混合审核框架：1）监督分类处理已知违规；2）参考式相似性匹配识别新颖/细微违规；3）多模态（文本、音频、视觉）输入经双路径处理；4）用MLLM为两路径蒸馏知识以提升精度并保持轻量推理。

Result: 分类路径达67%召回率@80%精度，相似性路径达76%召回率@80%精度；A/B测试显示用户观看不良直播减少6–8%。

Conclusion: 该混合框架具备可扩展性与适应性，能同时应对显性违规与新兴对抗行为，为多模态内容治理提供了实用解决方案。

Abstract: Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.

</details>


### [57] [GAOT: Generating Articulated Objects Through Text-Guided Diffusion Models](https://arxiv.org/abs/2512.03566)
*Hao Sun,Lei Fan,Donglin Di,Shaohui Liu*

Main category: cs.CV

TL;DR: 本文提出GAOT框架，通过三阶段扩散模型与超图学习，实现从文本提示生成可动3D物体。


<details>
  <summary>Details</summary>
Motivation: 现有可动物体生成模型缺乏文本条件控制能力，难以 bridging 文本描述与3D可动物体表示之间的鸿沟。

Method: GAOT采用三阶段流程：1）微调点云生成模型，从文本生成粗略物体；2）设计超图学习方法，将物体部件建模为图顶点并优化表示；3）利用扩散模型基于部件生成关节（图边）。

Result: 在PartNet-Mobility数据集上，GAOT在定性与定量实验中均优于先前方法。

Conclusion: GAOT成功实现了文本到可动物体的端到端生成，验证了扩散模型与超图结构联合建模的有效性。

Abstract: Articulated object generation has seen increasing advancements, yet existing models often lack the ability to be conditioned on text prompts. To address the significant gap between textual descriptions and 3D articulated object representations, we propose GAOT, a three-phase framework that generates articulated objects from text prompts, leveraging diffusion models and hypergraph learning in a three-step process. First, we fine-tune a point cloud generation model to produce a coarse representation of objects from text prompts. Given the inherent connection between articulated objects and graph structures, we design a hypergraph-based learning method to refine these coarse representations, representing object parts as graph vertices. Finally, leveraging a diffusion model, the joints of articulated objects-represented as graph edges-are generated based on the object parts. Extensive qualitative and quantitative experiments on the PartNet-Mobility dataset demonstrate the effectiveness of our approach, achieving superior performance over previous methods.

</details>


### [58] [Global-Local Aware Scene Text Editing](https://arxiv.org/abs/2512.03574)
*Fuxiang Yang,Tonghua Su,Donglin Di,Yin Chen,Xiangqian Wu,Zhongjie Wang,Lei Fan*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的全局-局部感知场景文本编辑（GLASTE）框架，通过融合全局上下文与局部细节、联合损失函数及尺寸无关的文本风格向量表示，有效解决了现有方法在局部-全局一致性与文本长度变化适应性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有场景文本编辑方法存在局部编辑区域与周围不一致、对编辑前后文本长度变化敏感两大问题。

Method: 提出GLASTE框架，包含全局-局部组合结构、联合全局与局部损失、尺寸无关的文本风格向量建模，以及保持宽高比的仿射融合填充策略。

Result: 在真实数据集上实验表明，GLASTE在定量指标和定性效果上均优于先前方法，显著缓解了不一致性和长度敏感性问题。

Conclusion: GLASTE通过协同建模全局语义与局部细节，并解耦文本风格与图像尺寸，为高质量、鲁棒的场景文本编辑提供了有效解决方案。

Abstract: Scene Text Editing (STE) involves replacing text in a scene image with new target text while preserving both the original text style and background texture. Existing methods suffer from two major challenges: inconsistency and length-insensitivity. They often fail to maintain coherence between the edited local patch and the surrounding area, and they struggle to handle significant differences in text length before and after editing. To tackle these challenges, we propose an end-to-end framework called Global-Local Aware Scene Text Editing (GLASTE), which simultaneously incorporates high-level global contextual information along with delicate local features. Specifically, we design a global-local combination structure, joint global and local losses, and enhance text image features to ensure consistency in text style within local patches while maintaining harmony between local and global areas. Additionally, we express the text style as a vector independent of the image size, which can be transferred to target text images of various sizes. We use an affine fusion to fill target text images into the editing patch while maintaining their aspect ratio unchanged. Extensive experiments on real-world datasets validate that our GLASTE model outperforms previous methods in both quantitative metrics and qualitative results and effectively mitigates the two challenges.

</details>


### [59] [UniComp: Rethinking Video Compression Through Informational Uniqueness](https://arxiv.org/abs/2512.03575)
*Chao Yuan,Shimin Chen,Minliang Lin,Limeng Qiao,Guanglu Wan,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种基于信息唯一性的视频压缩框架UniComp，通过最小化保留token与完整token之间的条件熵（重建误差），在计算资源受限条件下最大化视频表示的信息保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的压缩方法存在局限性，本文从信息论角度出发，旨在解决视觉压缩中的内在冗余问题，提升有限计算预算下的信息保真度。

Method: 提出信息唯一性概念来度量token间的内在冗余，并据此设计三个模块：帧组融合（Frame Group Fusion）、Token分配（Token Allocation）和空间动态压缩（Spatial Dynamic Compression），实现语义帧分组、自适应资源分配与细粒度空间压缩。

Result: 大量实验表明，UniComp在有限计算预算下能更有效地保留关键视觉token，性能持续优于现有压缩方法。

Conclusion: 信息唯一性是提升token压缩效能的关键因素，为视频压缩提供了新的理论视角与实用框架。

Abstract: Distinct from attention-based compression methods, this paper presents an information uniqueness driven video compression framework, termed UniComp, which aims to maximize the information fidelity of video representations under constrained computational budgets. Starting from the information-theoretic perspective, we formulate the vision compression as an optimization problem that minimizes conditional entropy (reconstruction error) between retained and full tokens. To achieve this, we introduce the notion of information uniqueness to measure intrinsic redundancy among tokens to link with reconstruction error. Based on uniqueness, we design three modules-Frame Group Fusion, Token Allocation, and Spatial Dynamic Compression-that progressively perform semantic frame grouping, adaptive resource allocation, and fine-grained spatial compression. Extensive experiments demonstrate that UniComp consistently outperforms existing compression methods in preserving essential visual tokens under limited computational budgets, highlighting the pivotal role of information uniqueness in token compression efficacy.

</details>


### [60] [Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning](https://arxiv.org/abs/2512.03577)
*Yizhi Zhang,Lei Fan,Zhulin Tao,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: 本文提出了一种跨染色对比学习（CSCL）框架，利用自建的五染色（H&E+4种IHC）配准数据集，通过两阶段预训练（patch级对比对齐 + slide级多实例跨染色融合与对齐），显著提升了H&E图像的通用、可迁移全片表示能力，在多种下游病理任务中取得一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有计算病理学受限于多染色（如H&E与IHC）全片图像间缺乏高质量配准，导致跨染色特征不一致、滑动窗口级和全片级表征质量下降；亟需构建配准多染色数据并设计能融合多染色生物学信息的表示学习方法。

Method: 1）构建首个滑动级别对齐的五染色（H&E, HER2, KI67, ER, PGR）WSI数据集；2）提出CSCL两阶段预训练框架：第一阶段使用轻量适配器进行patch级跨染色对比对齐；第二阶段采用MIL范式，含跨染色注意力融合模块（整合各染色patch特征）与跨染色全局对齐模块（约束不同染色生成的一致slide级嵌入）。

Result: 在癌症亚型分类、IHC生物标志物状态预测及生存分析等下游任务上均取得稳定性能提升，验证了所学H&E全片表征具有高判别性、跨染色一致性与强迁移能力。

Conclusion: 跨染色配准数据与CSCL框架有效弥合了H&E与IHC之间的语义鸿沟，为构建通用、可迁移的病理图像表示提供了新范式，推动多模态计算病理学发展。

Abstract: Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&E enriches H&E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&E, HER2, KI67, ER, PGR) to enable paired H&E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.

</details>


### [61] [Dynamic Optical Test for Bot Identification (DOT-BI): A simple check to identify bots in surveys and online processes](https://arxiv.org/abs/2512.03580)
*Malte Bleeker,Mauro Gotsch*

Main category: cs.CV

TL;DR: 本文提出了一种名为DOT-BI的新型动态光学图灵测试，利用人类对运动的感知能力来区分真人与自动化系统，实验表明其对当前多模态大模型具有强鲁棒性，而人类用户识别率高达99.5%。


<details>
  <summary>Details</summary>
Motivation: 现有验证码（如reCAPTCHA）易被AI破解，且对用户不友好；亟需一种兼顾安全性、可用性与部署便捷性的新型人机区分机制。

Method: 设计动态光学测试DOT-BI：将隐藏数字叠加于同纹理动态背景上，仅靠运动与尺度差异使人类可感知，而逐帧图像分析无法提取有效信号；通过大模型评测与在线/实验室用户实验验证有效性与可用性。

Result: GPT-5-Thinking和Gemini 2.5 Pro等先进多模态模型均无法正确识别数字；182名在线参与者中99.5%成功完成任务，平均耗时10.7秒；39人实验室研究显示其易用性与完成时间与对照组无显著差异。

Conclusion: DOT-BI是一种高效、鲁棒、用户友好的新型人机鉴别方法，开源代码与预渲染资源支持快速落地应用。

Abstract: We propose the Dynamic Optical Test for Bot Identification (DOT-BI): a quick and easy method that uses human perception of motion to differentiate between human respondents and automated systems in surveys and online processes. In DOT-BI, a 'hidden' number is displayed with the same random black-and-white pixel texture as its background. Only the difference in motion and scale between the number and the background makes the number perceptible to humans across frames, while frame-by-frame algorithmic processing yields no meaningful signal. We conducted two preliminary assessments. Firstly, state-of-the-art, video-capable, multimodal models (GPT-5-Thinking and Gemini 2.5 Pro) fail to extract the correct value, even when given explicit instructions about the mechanism. Secondly, in an online survey (n=182), 99.5% (181/182) of participants solved the task, with an average end-to-end completion time of 10.7 seconds; a supervised lab study (n=39) found no negative effects on perceived ease-of-use or completion time relative to a control. We release code to generate tests and 100+ pre-rendered variants to facilitate adoption in surveys and online processes.

</details>


### [62] [Beyond Boundary Frames: Audio-Visual Semantic Guidance for Context-Aware Video Interpolation](https://arxiv.org/abs/2512.03590)
*Yuchen Deng,Xiuyang Wu,Hai-Tao Zheng,Jie Wang,Feidiao Yang,Yuxing Han*

Main category: cs.CV

TL;DR: 本文提出BBF（Beyond Boundary Frames）框架，通过多模态条件引导的视频帧插值方法，解决了传统光流法和现有扩散模型在复杂运动和音视频同步插值任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视频帧插值方法在处理快速、复杂、高度非线性运动时存在挑战，尤其在音视频同步等细粒度运动任务中难以生成清晰且时间一致的帧。

Method: 1）增强输入设计以支持文本、音频、图像和视频等多种条件模态；2）提出解耦式多模态融合机制，将不同条件信号顺序注入DiT主干网络；3）采用渐进式多阶段训练范式，利用首尾帧差异嵌入动态调整数据采样与损失权重。

Result: BBF在通用插值和音视频同步插值任务上均超越当前专用SOTA方法，构建了首个支持多通道协同条件的统一视频插值框架。

Conclusion: BBF为视频帧插值提供了更灵活、鲁棒且语义可控的新范式，显著提升了复杂运动建模与跨模态同步能力。

Abstract: Handling fast, complex, and highly non-linear motion patterns has long posed challenges for video frame interpolation. Although recent diffusion-based approaches improve upon traditional optical-flow-based methods, they still struggle to cover diverse application scenarios and often fail to produce sharp, temporally consistent frames in fine-grained motion tasks such as audio-visual synchronized interpolation. To address these limitations, we introduce BBF (Beyond Boundary Frames), a context-aware video frame interpolation framework, which could be guided by audio/visual semantics. First, we enhance the input design of the interpolation model so that it can flexibly handle multiple conditional modalities, including text, audio, images, and video. Second, we propose a decoupled multimodal fusion mechanism that sequentially injects different conditional signals into a DiT backbone. Finally, to maintain the generation abilities of the foundation model, we adopt a progressive multi-stage training paradigm, where the start-end frame difference embedding is used to dynamically adjust both the data sampling and the loss weighting. Extensive experimental results demonstrate that BBF outperforms specialized state-of-the-art methods on both generic interpolation and audio-visual synchronized interpolation tasks, establishing a unified framework for video frame interpolation under coordinated multi-channel conditioning.

</details>


### [63] [Harnessing Hypergraphs in Geometric Deep Learning for 3D RNA Inverse Folding](https://arxiv.org/abs/2512.03592)
*Guang Yang,Lei Fan*

Main category: cs.CV

TL;DR: 本文提出了一种名为HyperRNA的生成模型，利用超图编码-解码架构解决RNA逆折叠问题，通过预处理、编码和解码三阶段设计RNA序列，在PDBBind和RNAsolo数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RNA逆折叠问题是RNA设计中的关键挑战，需找到能折叠成目标二级结构的核苷酸序列，但序列与结构间关系复杂，传统方法难以有效建模。

Method: 提出HyperRNA框架：预处理阶段基于3-bead粗粒化表示构建RNA骨架原子坐标图；编码阶段采用注意力嵌入模块和超图编码器捕获高阶依赖与分子相互作用；解码阶段以自回归方式生成RNA序列。

Result: 在PDBBind和RNAsolo数据集上的定量与定性实验表明，HyperRNA在RNA序列生成及RNA-蛋白复合物序列生成任务中均优于现有RNA设计方法。

Conclusion: HyperRNA验证了超图在RNA工程中的潜力，为复杂生物分子结构-序列映射建模提供了新思路。

Abstract: The RNA inverse folding problem, a key challenge in RNA design, involves identifying nucleotide sequences that can fold into desired secondary structures, which are critical for ensuring molecular stability and function. The inherent complexity of this task stems from the intricate relationship between sequence and structure, making it particularly challenging. In this paper, we propose a framework, named HyperRNA, a generative model with an encoder-decoder architecture that leverages hypergraphs to design RNA sequences. Specifically, our HyperRNA model consists of three main components: preprocessing, encoding and decoding.
  In the preprocessing stage, graph structures are constructed by extracting the atom coordinates of RNA backbone based on 3-bead coarse-grained representation. The encoding stage processes these graphs, capturing higher order dependencies and complex biomolecular interactions using an attention embedding module and a hypergraph-based encoder. Finally, the decoding stage generates the RNA sequence in an autoregressive manner. We conducted quantitative and qualitative experiments on the PDBBind and RNAsolo datasets to evaluate the inverse folding task for RNA sequence generation and RNA-protein complex sequence generation. The experimental results demonstrate that HyperRNA not only outperforms existing RNA design methods but also highlights the potential of leveraging hypergraphs in RNA engineering.

</details>


### [64] [CloseUpAvatar: High-Fidelity Animatable Full-Body Avatars with Mixture of Multi-Scale Textures](https://arxiv.org/abs/2512.03593)
*David Svitov,Pietro Morerio,Lourdes Agapito,Alessio Del Bue*

Main category: cs.CV

TL;DR: CloseUpAvatar是一种新型人体化身表示方法，通过双纹理平面（低频和高频）自适应切换来提升近距离视角的渲染质量，并支持更广泛的相机运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理广泛相机运动时难以兼顾近距离视角的高保真渲染质量，需要一种能根据相机距离动态调整细节表现的化身表示方法。

Method: CloseUpAvatar将人体化身表示为一组带纹理的平面，配备两组可学习纹理（低频与高频），并依据相机到表面的距离自动切换和渐变融合高频纹理。

Result: 在ActorsHQ数据集上的实验表明，CloseUpAvatar在新颖宽范围相机位姿下的渲染效果优于现有方法，兼具定性与定量提升，同时保持高帧率（FPS）。

Conclusion: CloseUpAvatar通过距离自适应纹理机制，在扩展相机运动自由度的同时保障了近距离高质量渲染，是一种高效且鲁棒的人体化身表示新范式。

Abstract: We present a CloseUpAvatar - a novel approach for articulated human avatar representation dealing with more general camera motions, while preserving rendering quality for close-up views. CloseUpAvatar represents an avatar as a set of textured planes with two sets of learnable textures for low and high-frequency detail. The method automatically switches to high-frequency textures only for cameras positioned close to the avatar's surface and gradually reduces their impact as the camera moves farther away. Such parametrization of the avatar enables CloseUpAvatar to adjust rendering quality based on camera distance ensuring realistic rendering across a wider range of camera orientations than previous approaches. We provide experiments using the ActorsHQ dataset with high-resolution input images. CloseUpAvatar demonstrates both qualitative and quantitative improvements over existing methods in rendering from novel wide range camera positions, while maintaining high FPS by limiting the number of required primitives.

</details>


### [65] [HBFormer: A Hybrid-Bridge Transformer for Microtumor and Miniature Organ Segmentation](https://arxiv.org/abs/2512.03597)
*Fuchen Zheng,Xinyi Chen,Weixuan Li,Quanjun Li,Junhua Zhou,Xiaojiao Guo,Xuhang Chen,Chi-Man Pun,Shoujun Zhou*

Main category: cs.CV

TL;DR: 本文提出HBFormer，一种结合U型编码器-解码器与Swin Transformer的混合桥接Transformer架构，通过创新的多尺度特征融合（MFF）解码器，融合局部细节与全局上下文，显著提升微肿瘤和微小器官的医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于移位窗口自注意力的视觉Transformer在医学图像分割中难以有效融合局部细节与全局上下文，尤其影响微肿瘤和微小器官等需精细边界与广域理解的任务。

Method: 提出HBFormer：'Hybrid'部分融合U型编解码结构与Swin Transformer主干；'Bridge'部分设计新型MFF解码器，采用空洞卷积与深度可分离卷积构建通道与空间注意力模块，实现多尺度特征与全局上下文的协同融合。

Result: 在多器官、肝肿瘤、膀胱肿瘤等多个具挑战性的医学图像分割数据集上达到当前最优性能，尤其在微肿瘤和微小器官分割任务中表现突出。

Conclusion: HBFormer通过结构化桥接机制有效弥合了局部与全局信息融合的鸿沟，为高精度医学图像分割提供了新范式。

Abstract: Medical image segmentation is a cornerstone of modern clinical diagnostics. While Vision Transformers that leverage shifted window-based self-attention have established new benchmarks in this field, they are often hampered by a critical limitation: their localized attention mechanism struggles to effectively fuse local details with global context. This deficiency is particularly detrimental to challenging tasks such as the segmentation of microtumors and miniature organs, where both fine-grained boundary definition and broad contextual understanding are paramount. To address this gap, we propose HBFormer, a novel Hybrid-Bridge Transformer architecture. The 'Hybrid' design of HBFormer synergizes a classic U-shaped encoder-decoder framework with a powerful Swin Transformer backbone for robust hierarchical feature extraction. The core innovation lies in its 'Bridge' mechanism, a sophisticated nexus for multi-scale feature integration. This bridge is architecturally embodied by our novel Multi-Scale Feature Fusion (MFF) decoder. Departing from conventional symmetric designs, the MFF decoder is engineered to fuse multi-scale features from the encoder with global contextual information. It achieves this through a synergistic combination of channel and spatial attention modules, which are constructed from a series of dilated and depth-wise convolutions. These components work in concert to create a powerful feature bridge that explicitly captures long-range dependencies and refines object boundaries with exceptional precision. Comprehensive experiments on challenging medical image segmentation datasets, including multi-organ, liver tumor, and bladder tumor benchmarks, demonstrate that HBFormer achieves state-of-the-art results, showcasing its outstanding capabilities in microtumor and miniature organ segmentation. Code and models are available at: https://github.com/lzeeorno/HBFormer.

</details>


### [66] [Memory-Guided Point Cloud Completion for Dental Reconstruction](https://arxiv.org/abs/2512.03598)
*Jianan Sun,Yukang Huang,Dongzhihan Wang,Mingyu Fan*

Main category: cs.CV

TL;DR: 本文提出了一种检索增强的牙齿点云补全框架，通过引入可学习的原型记忆模块，在编码器-解码器流程中融合结构先验，提升缺失区域重建的准确性与细节恢复能力。


<details>
  <summary>Details</summary>
Motivation: 部分牙科点云常因遮挡和扫描视角受限存在大面积缺失，导致仅依赖编码器的全局特征产生偏差，解码器易生成不真实的结构。

Method: 在标准编码器-解码器架构中嵌入可学习的原型记忆模块；对输入点云编码后得到全局描述符，检索最邻近的流形原型，并通过置信度门控加权融合查询特征，再送入解码器；记忆模块端到端优化、自组织为无标签的牙齿形状原型。

Result: 在自建Teeth3DS数据集上，Chamfer距离显著下降；可视化显示牙尖、嵴线及邻面过渡更清晰锐利。

Conclusion: 该即插即用模块无需修改训练损失，兼容主流补全骨干网络，有效利用跨样本规律，提升了牙科点云补全的精度与保真度。

Abstract: Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures. We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines. After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding. The memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels, thereby providing structural priors that stabilize missing-region inference and free decoder capacity for detail recovery. The module is plug-and-play and compatible with common completion backbones, while keeping the same training losses. Experiments on a self-processed Teeth3DS benchmark demonstrate consistent improvements in Chamfer Distance, with visualizations showing sharper cusps, ridges, and interproximal transitions. Our approach provides a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion.

</details>


### [67] [Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding](https://arxiv.org/abs/2512.03601)
*Haoran Zhou,Gim Hee Lee*

Main category: cs.CV

TL;DR: Motion4D 提出一种将2D基础模型先验融入统一4D高斯点阵表示的新框架，通过分阶段优化、3D置信图、自适应重采样和语义迭代优化，提升动态场景的3D一致性和语义连贯性，在多项任务上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有2D视觉基础模型虽具强泛化能力，但在动态单目视频理解中缺乏3D一致性，导致空间错位与时间闪烁问题。

Method: 提出Motion4D框架：1）两阶段迭代优化（顺序优化+全局优化）；2）引入3D置信图动态调整运动先验；3）基于RGB与语义误差的自适应高斯重采样；4）结合SAM2提示更新的语义场迭代优化。

Result: 在点跟踪、视频目标分割、新视角合成等任务上显著优于2D基础模型及现有3D方法。

Conclusion: Motion4D有效融合2D先验与4D几何建模，提升了动态场景理解的3D一致性与语义准确性，为视频级三维理解提供了新范式。

Abstract: Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.

</details>


### [68] [LAMP: Language-Assisted Motion Planning for Controllable Video Generation](https://arxiv.org/abs/2512.03619)
*Muhammed Burak Kizil,Enes Sanli,Niloy J. Mitra,Erkut Erdem,Aykut Erdem,Duygu Ceylan*

Main category: cs.CV

TL;DR: 本文提出了LAMP框架，利用大语言模型（LLM）作为运动规划器，将自然语言描述转化为动态物体和摄像机的显式3D运动轨迹，首次实现了从自然语言直接生成物体与摄像机运动的端到端控制。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法在运动控制（如物体动态与摄像机轨迹）方面的接口仍受限，难以支持复杂、电影级场景的精确编排。

Method: 提出面向运动领域的领域特定语言（DSL），结合LLM的程序合成能力，将自然语言描述解析为结构化运动程序，并确定性映射为3D轨迹；构建大规模配对数据集（文本-运动程序-3D轨迹）。

Result: 实验表明，LAMP在运动可控性和用户意图对齐方面优于现有最先进方法，成为首个支持自然语言驱动物体与摄像机联合运动生成的框架。

Conclusion: LAMP验证了利用LLM进行高层语义到低层几何运动规划的可行性，为视频生成中的高精度运动控制提供了新范式。

Abstract: Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications.

</details>


### [69] [ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation](https://arxiv.org/abs/2512.03621)
*Yaokun Li,Shuaixian Wang,Mantang Guo,Jiehui Huang,Taojun Ding,Mu Hu,Kaixuan Wang,Shaojie Shen,Guang Tan*

Main category: cs.CV

TL;DR: ReCamDriving 是一种纯视觉、基于相机控制的新型轨迹视频生成框架，利用密集且场景完整的3D高斯泼溅（3DGS）渲染提供显式几何引导，并通过两阶段训练和跨轨迹数据策略提升相机可控性与结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有修复类方法难以恢复复杂伪影，LiDAR方法依赖稀疏不完整线索，缺乏对复杂驾驶场景下精确相机控制与几何一致性的支持。

Method: 提出ReCamDriving框架：1）利用3DGS渲染提供显式几何引导；2）采用两阶段训练（第一阶段用相机位姿粗控，第二阶段引入3DGS实现细粒度视角与几何引导）；3）设计基于3DGS的跨轨迹数据策展策略，并构建含11万+平行轨迹视频对的ParaDrive数据集。

Result: 在多项实验中达到最先进的相机可控性和结构一致性性能。

Conclusion: ReCamDriving通过融合3DGS几何先验与分阶段训练机制，有效解决了纯视觉视频生成中相机控制精度与场景结构保真度之间的权衡问题，为自动驾驶仿真与新视角合成提供了新范式。

Abstract: We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.

</details>


### [70] [FeatureLens: A Highly Generalizable and Interpretable Framework for Detecting Adversarial Examples Based on Image Features](https://arxiv.org/abs/2512.03625)
*Zhigang Yang,Yuan Liu,Jiawei Zhang,Puning Zhang,Xinqiang Ma*

Main category: cs.CV

TL;DR: 本文提出FeatureLens，一种轻量级框架，通过图像特征提取器和浅层分类器检测对抗样本，在闭集和泛化评估中均表现出高准确率、强泛化性、高可解释性和计算高效性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗样本检测方法依赖复杂且难以解释的架构，导致可解释性和泛化能力受限。

Method: 提出FeatureLens框架，包含图像特征提取器（IFE）和浅层分类器（如SVM、MLP或XGBoost），仅使用51维特征进行检测。

Result: 在闭集评估中检测准确率达97.8%–99.75%，泛化评估中达86.17%–99.6%，适用于FGSM、PGD、CW和DAmageNet等多种攻击。

Conclusion: FeatureLens在检测性能、泛化能力、可解释性和计算效率之间取得良好平衡，为透明且有效的对抗防御提供了实用路径。

Abstract: Although the remarkable performance of deep neural networks (DNNs) in image classification, their vulnerability to adversarial attacks remains a critical challenge. Most existing detection methods rely on complex and poorly interpretable architectures, which compromise interpretability and generalization. To address this, we propose FeatureLens, a lightweight framework that acts as a lens to scrutinize anomalies in image features. Comprising an Image Feature Extractor (IFE) and shallow classifiers (e.g., SVM, MLP, or XGBoost) with model sizes ranging from 1,000 to 30,000 parameters, FeatureLens achieves high detection accuracy ranging from 97.8% to 99.75% in closed-set evaluation and 86.17% to 99.6% in generalization evaluation across FGSM, PGD, CW, and DAmageNet attacks, using only 51 dimensional features. By combining strong detection performance with excellent generalization, interpretability, and computational efficiency, FeatureLens offers a practical pathway toward transparent and effective adversarial defense.

</details>


### [71] [MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms](https://arxiv.org/abs/2512.03640)
*Jiahao Zhang,Xiao Zhao,Guangyu Gao*

Main category: cs.CV

TL;DR: 本文提出了一种名为MKSNet的新网络架构，通过多核选择机制和双注意力机制，显著提升了遥感图像中小目标的检测性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中目标尺寸小、分辨率高，导致传统CNN深层丢失关键信息；同时图像空间冗余大、背景复杂，进一步干扰小目标检测。

Method: 提出MKSNet，包含多核选择（MKS）机制（自适应选择大卷积核以捕获上下文信息）和双注意力机制（空间注意力聚焦目标区域、通道注意力优化特征通道）。

Result: 在DOTA-v1.0和HRSC2016数据集上，MKSNet在小目标检测任务上显著优于现有SOTA方法。

Conclusion: MKSNet有效应对遥感图像多尺度、高分辨率带来的挑战，验证了其在小目标检测中的有效性与创新性。

Abstract: Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.

</details>


### [72] [Multi-Scale Visual Prompting for Lightweight Small-Image Classification](https://arxiv.org/abs/2512.03663)
*Salim Khazem*

Main category: cs.CV

TL;DR: 本文提出了多尺度视觉提示（MSVP）方法，通过在输入图像中注入全局、中尺度和局部提示图来提升小图像基准数据集（如MNIST、Fashion-MNIST、CIFAR-10）上的模型性能，具有参数少、计算开销低、骨干网络无关等优点。


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示方法主要面向大尺寸图像和大型ViT模型，而广泛用于教学、原型开发和研究的小图像基准（如MNIST、Fashion-MNIST、CIFAR-10）缺乏适配的轻量级提示方法。

Method: 提出多尺度视觉提示（MSVP）模块，学习三类提示图（全局、中尺度、局部），并通过轻量级1×1卷积与输入图像融合；该方法不依赖骨干网络，参数增加不足0.02%。

Result: 在MNIST、Fashion-MNIST和CIFAR-10上，MSVP在CNN、ResNet-18和小型ViT上均取得一致性能提升；消融实验验证了多尺度设计和融合策略的有效性；可视化分析（提示图、Grad-CAM）进一步支持其有效性。

Conclusion: 多尺度视觉提示为低分辨率图像提供了有效的归纳偏置，是一种通用、高效且即插即用的视觉提示方案。

Abstract: Visual prompting has recently emerged as an efficient strategy to adapt vision models using lightweight, learnable parameters injected into the input space. However, prior work mainly targets large Vision Transformers and high-resolution datasets such as ImageNet. In contrast, small-image benchmarks like MNIST, Fashion-MNIST, and CIFAR-10 remain widely used in education, prototyping, and research, yet have received little attention in the context of prompting. In this paper, we introduce \textbf{Multi-Scale Visual Prompting (MSVP)}, a simple and generic module that learns a set of global, mid-scale, and local prompt maps fused with the input image via a lightweight $1 \times 1$ convolution. MSVP is backbone-agnostic, adds less than $0.02\%$ parameters, and significantly improves performance across CNN and Vision Transformer backbones.
  We provide a unified benchmark on MNIST, Fashion-MNIST, and CIFAR-10 using a simple CNN, ResNet-18, and a small Vision Transformer. Our method yields consistent improvements with negligible computational overhead. We further include ablations on prompt scales, fusion strategies, and backbone architectures, along with qualitative analyzes using prompt visualizations and Grad-CAM. Our results demonstrate that multi-scale prompting provides an effective inductive bias even on low-resolution images.

</details>


### [73] [ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos](https://arxiv.org/abs/2512.03666)
*Qi'ao Xu,Tianwen Qian,Yuqian Fu,Kailing Li,Yang Jiao,Jiacheng Zhang,Xiaoling Wang,Liang He*

Main category: cs.CV

TL;DR: 本文提出了首个面向任务的时空视频定位基准ToG-Bench，用于以自我为中心的视频，强调任务导向、显隐双重定位和一对多定位，并构建了新评估指标，揭示了当前多模态大模型在任务导向STVG上的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有时空视频定位（STVG）研究局限于物体中心和描述性指令，忽视了具身智能体完成目标导向交互所必需的任务导向推理能力。

Method: 构建了ToG-Bench基准：基于ScanNet视频，通过基础模型标注加人工精修的半自动流程，生成100个视频片段、2704条任务导向指令；提出适配多物体与显隐物体定位的任务级评估指标；系统评测7个SOTA多模态大语言模型。

Result: 实验揭示了任务导向STVG的内在挑战，尤其在显隐物体定位与多物体定位上存在显著性能差距，凸显感知与交互融合的困难。

Conclusion: ToG-Bench填补了任务导向具身视觉理解的基准空白，为推动具身智能中任务驱动的时空定位研究提供了新标准与评测平台。

Abstract: A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..

</details>


### [74] [Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning](https://arxiv.org/abs/2512.03667)
*Ge-Peng Ji,Jingyi Liu,Deng-Ping Fan,Nick Barnes*

Main category: cs.CV

TL;DR: 本文提出了Colon-X开源项目，构建了大规模结肠镜多模态数据集ColonVQA，并系统评估了22个多模态大模型在结肠镜任务中的鲁棒性；进一步提出面向临床推理的ColonReason数据集和首个R1风格模型ColonR1，在数据稀缺下显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前结肠镜多模态理解向临床推理过渡的关键挑战，弥补现有模型在临床场景中鲁棒性与可信度不足的问题。

Method: 构建ColonVQA数据集；系统评估22个MLLM在人为扰动下的泛化与可靠性；构建专家辩论标注的ColonReason推理数据集；开发具备任务自适应奖励与梯度稳定优化的ColonR1模型。

Result: ColonR1在数据稀缺条件下整体准确率达56.61%，较监督微调提升25.22%；建立了首个支持推理的结肠镜多模态分析新基线。

Conclusion: Colon-X推动了结肠镜多模态智能从感知理解迈向可信临床推理，提供了开放数据、模型与方法，为医学AI可信赖发展树立实践范例。

Abstract: In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.

</details>


### [75] [ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers](https://arxiv.org/abs/2512.03673)
*Feice Huang,Zuliang Han,Xing Zhou,Yihuang Chen,Lifei Zhu,Haoqian Wang*

Main category: cs.CV

TL;DR: 本文提出ConvRot，一种基于分组旋转的量化方法，利用规则哈达玛变换（RHT）抑制扩散Transformer中的行列异常值，并将复杂度从二次降至线性；进一步设计ConvLinear4bit模块实现无需重训练的W4A4推理，在FLUX.1-dev上实现2.26倍加速和4.05倍内存减少，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer模型增大导致内存占用高、推理延迟大，现有旋转量化方法在处理其行向异常值时效果差且开销大。

Method: 提出ConvRot：基于分组的旋转量化方法，采用规则哈达玛变换（RHT）联合抑制行、列方向异常值；设计ConvLinear4bit插件式模块，集成旋转、量化、GEMM与反量化，支持无需微调的W4A4推理。

Result: 在FLUX.1-dev上实现2.26×推理加速与4.05×内存降低，图像保真度无明显下降。

Conclusion: ConvRot是首个面向扩散Transformer实现插件式W4A4推理的旋转量化方案，兼顾高效性与视觉质量。

Abstract: Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.

</details>


### [76] [GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces](https://arxiv.org/abs/2512.03683)
*Melis Ocal,Xiaoyan Xing,Yue Li,Ngo Anh Vien,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: 本文提出了GaussianBlender，一种前馈式文本驱动3D风格化框架，通过学习解耦的几何与外观隐空间，并结合潜在扩散模型实现快速、高保真、多视角一致的3D风格化，无需逐资产优化。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D风格化方法依赖2D图像编辑器蒸馏，需耗时的单资产优化且存在多视角不一致问题，难以满足大规模生产需求。

Method: 提出GaussianBlender框架：基于空间分组的3D高斯点学习结构化、解耦的几何与外观隐空间，并引入文本条件的潜在扩散模型进行风格编辑。

Result: 实验表明该方法可实现实时、高保真、几何保持、多视角一致的3D风格化，性能优于需测试时优化的方法。

Conclusion: GaussianBlender为大规模、实用化、平民化的3D风格化提供了新范式。

Abstract: 3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.

</details>


### [77] [Active Visual Perception: Opportunities and Challenges](https://arxiv.org/abs/2512.03687)
*Yian Li,Xiaoyu Guo,Hao Zhang,Shuiwang Li,Xiaowei Dai*

Main category: cs.CV

TL;DR: 本文综述了主动视觉感知的概念、应用、机遇与挑战，强调其在复杂环境中通过动态交互提升感知能力的重要性，并探讨了实时处理、动态决策和多模态融合等关键问题。


<details>
  <summary>Details</summary>
Motivation: 主动视觉感知能克服静态感知在复杂环境中信息不足的问题，但在实时处理、动态决策和多模态融合等方面仍面临挑战，亟需系统性梳理与分析。

Method: 采用综述方法，系统梳理主动视觉感知的定义、应用场景、前沿研究进展及现存技术挑战。

Result: 明确了主动视觉感知的核心特征与关键应用领域，归纳了当前主要技术瓶颈，包括实时视觉处理、动态环境决策与多模态感知集成。

Conclusion: 主动视觉感知具有广阔应用前景，但需跨学科协同攻关以突破实时性、适应性与鲁棒性等核心挑战，推动其在机器人、自动驾驶等领域的实际落地。

Abstract: Active visual perception refers to the ability of a system to dynamically engage with its environment through sensing and action, allowing it to modify its behavior in response to specific goals or uncertainties. Unlike passive systems that rely solely on visual data, active visual perception systems can direct attention, move sensors, or interact with objects to acquire more informative data. This approach is particularly powerful in complex environments where static sensing methods may not provide sufficient information. Active visual perception plays a critical role in numerous applications, including robotics, autonomous vehicles, human-computer interaction, and surveillance systems. However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs. This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.

</details>


### [78] [Structured Uncertainty Similarity Score (SUSS): Learning a Probabilistic, Interpretable, Perceptual Metric Between Images](https://arxiv.org/abs/2512.03701)
*Paula Seidler,Neill D. F. Campbell,Ivor J A Simpson*

Main category: cs.CV

TL;DR: 本文提出了一种新的感知相似性度量方法SUSS，通过生成式自监督学习建模图像的感知成分，并利用人类感知数据学习权重，实现了与人类视觉高度一致、可解释且校准良好的相似性评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法如LPIPS虽对齐人类感知但缺乏可解释性，而SSIM等手工设计指标虽可解释却无法捕捉关键感知特性，因此需要一种兼顾对齐性与可解释性的新方法。

Method: SUSS将每幅图像建模为多个感知成分，每个成分用结构化多元正态分布表示；通过生成式自监督方式训练，使模型对人类不可察觉的增强变换赋予高似然；最终得分是各成分对数概率的加权和，权重由人类感知数据学习得到；其核心是学习图像特定的像素空间残差线性变换，支持去相关残差分析和采样解释。

Result: SUSS在多种失真类型下展现出强感知校准能力，与人类感知判断高度一致，提供局部化、可解释的相似性评估，并在作为下游成像任务的感知损失时表现出稳定优化行为和竞争力。

Conclusion: SUSS是一种兼具感知对齐性、可解释性与实用性的新型相似性度量方法，为感知驱动的视觉模型训练与评估提供了新范式。

Abstract: Perceptual similarity scores that align with human vision are critical for both training and evaluating computer vision models. Deep perceptual losses, such as LPIPS, achieve good alignment but rely on complex, highly non-linear discriminative features with unknown invariances, while hand-crafted measures like SSIM are interpretable but miss key perceptual properties.
  We introduce the Structured Uncertainty Similarity Score (SUSS); it models each image through a set of perceptual components, each represented by a structured multivariate Normal distribution. These are trained in a generative, self-supervised manner to assign high likelihood to human-imperceptible augmentations. The final score is a weighted sum of component log-probabilities with weights learned from human perceptual datasets. Unlike feature-based methods, SUSS learns image-specific linear transformations of residuals in pixel space, enabling transparent inspection through decorrelated residuals and sampling.
  SUSS aligns closely with human perceptual judgments, shows strong perceptual calibration across diverse distortion types, and provides localized, interpretable explanations of its similarity assessments. We further demonstrate stable optimization behavior and competitive performance when using SUSS as a perceptual loss for downstream imaging tasks.

</details>


### [79] [DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction](https://arxiv.org/abs/2512.03715)
*Kaichen Zhang,Tianxiang Sheng,Xuanming Shi*

Main category: cs.CV

TL;DR: DINO-RotateMatch是一种结合自监督全局描述符与旋转增强局部匹配的深度学习框架，用于大规模无序网络图像的3D重建，显著提升图像匹配精度。


<details>
  <summary>Details</summary>
Motivation: 解决基于无序互联网图像进行大规模3D重建中的图像匹配难题，尤其应对视角变化和数据规模带来的挑战。

Method: 提出DINO-RotateMatch框架：利用DINO模型进行语义驱动的图像对检索，并结合基于旋转的数据增强、ALIKED关键点提取与Light Glue匹配，实现旋转感知的局部特征匹配。

Result: 在Kaggle图像匹配挑战赛2025中，mAA指标持续提升，获得银奖（943支队伍中排名第47）。

Conclusion: 融合自监督全局描述符与旋转增强的局部匹配策略，可提供鲁棒且可扩展的大规模3D重建图像匹配方案。

Abstract: This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images. The
  method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and
  matching. DINO is employed to retrieve semantically relevant image pairs in large collections, while
  rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue. Experiments on the Kaggle Image Matching Challenge 2025 demonstrate consistent improve ments in mean Average Accuracy (mAA), achieving a Silver Award (47th of 943 teams). The results
  confirm that combining self-supervised global descriptors with rotation-enhanced local matching offers
  a robust and scalable solution for large-scale 3D reconstruction.

</details>


### [80] [Out-of-the-box: Black-box Causal Attacks on Object Detectors](https://arxiv.org/abs/2512.03730)
*Melane Navaratnarajah,David A. Kelly,Hana Chockler*

Main category: cs.CV

TL;DR: 本文提出BlackCAtt，一种基于因果像素的黑盒攻击方法，用于生成可解释、不可察觉、可复现且架构无关的目标检测对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对抗扰动方法多为白盒、架构特定，且缺乏对攻击成功机制的清晰理解，难以帮助开发者分析和防御攻击。

Method: BlackCAtt利用最小因果像素集，结合检测器输出的边界框，构建黑盒对抗攻击；不依赖模型内部结构，适用于多种尺寸与架构的检测器。

Result: 在COCO测试集上，相比基线方法，BlackCAtt在删除检测、修改检测和触发虚假检测任务中分别提升2.7倍、3.86倍和5.75倍；攻击图像高度接近原图，具有强不可察觉性。

Conclusion: 因果像素是构建高效、精准、可解释黑盒对抗攻击的关键，BlackCAtt为理解与防御目标检测对抗攻击提供了新思路与实用工具。

Abstract: Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.

</details>


### [81] [Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2512.03745)
*Jiaze Li,Yan Lu,Bin Liu,Guojun Yin,Mang Ye*

Main category: cs.CV

TL;DR: 本文提出了一种双层次模态去偏学习框架（DMDL），通过因果建模（CAI模块）和协同无偏训练（CBT策略）在模型和优化层面消除两阶段无监督可见光-红外行人重识别中的模态偏差，提升身份判别与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段无监督VI-ReID方法因单模态预训练引入模态特异性线索，导致跨模态学习中存在模态偏差，损害身份判别与泛化性能。

Method: 提出Dual-level Modality Debiasing Learning（DMDL）框架：1）模型层面设计因果启发的调整干预（CAI）模块，用因果建模替代似然建模以抑制虚假模态相关性；2）优化层面设计协同无偏训练（CBT）策略，融合模态特定增强、标签精炼与特征对齐来阻断模态偏差在数据、标签和特征上的传播。

Result: 在多个基准数据集上实验表明，DMDL能实现模态不变特征学习，显著提升模型泛化性和重识别性能。

Conclusion: DMDL从模型与优化双层次有效缓解模态偏差，为无监督跨模态行人重识别提供了新思路，验证了因果建模与协同去偏策略的有效性。

Abstract: Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.

</details>


### [82] [Fully Unsupervised Self-debiasing of Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.03749)
*Korada Sri Vardhana,Shrikrishna Lolla,Soma Biswas*

Main category: cs.CV

TL;DR: 本文提出SelfDebias，一种无需监督的测试时去偏方法，适用于基于UNet噪声预测器的文本到图像扩散模型，通过在图像编码器嵌入空间中识别语义簇并引导扩散过程，最小化输出分布与均匀分布之间的KL散度，从而减少模型固有偏见，且不依赖人工标注数据或外部分类器。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型因训练数据（如LAION-5B）存在大量偏见而生成刻板化图像，亟需无需监督、通用性强的去偏方法。

Method: SelfDebias在测试时自动识别图像编码器嵌入空间中的语义簇，并以此引导扩散过程，通过最小化输出分布与均匀分布的KL散度实现去偏；无需人工标注或外部分类器。

Result: SelfDebias在多种提示和扩散模型架构（含条件与无条件）上泛化良好，能有效缓解关键人口统计维度及更抽象概念上的偏见，同时保持生成图像的视觉保真度。

Conclusion: SelfDebias是一种通用、完全无监督的测试时去偏方法，为扩散模型公平性提供了新思路，摆脱了对标注数据和定制分类器的依赖。

Abstract: Text-to-image (T2I) diffusion models have achieved widespread success due to their ability to generate high-resolution, photorealistic images. These models are trained on large-scale datasets, like LAION-5B, often scraped from the internet. However, since this data contains numerous biases, the models inherently learn and reproduce them, resulting in stereotypical outputs. We introduce SelfDebias, a fully unsupervised test-time debiasing method applicable to any diffusion model that uses a UNet as its noise predictor. SelfDebias identifies semantic clusters in an image encoder's embedding space and uses these clusters to guide the diffusion process during inference, minimizing the KL divergence between the output distribution and the uniform distribution. Unlike supervised approaches, SelfDebias does not require human-annotated datasets or external classifiers trained for each generated concept. Instead, it is designed to automatically identify semantic modes. Extensive experiments show that SelfDebias generalizes across prompts and diffusion model architectures, including both conditional and unconditional models. It not only effectively debiases images along key demographic dimensions while maintaining the visual fidelity of the generated images, but also more abstract concepts for which identifying biases is also challenging.

</details>


### [83] [Research on Brain Tumor Classification Method Based on Improved ResNet34 Network](https://arxiv.org/abs/2512.03751)
*Yufeng Li,Wenchao Zhao,Bo Dang,Weimin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于改进ResNet34的脑肿瘤图像分类模型，通过引入多尺度输入、Inception v2下采样模块和通道注意力机制，在保持更少参数（原模型80%）的同时将分类准确率提升至98.8%，较原始ResNet34提高1%。


<details>
  <summary>Details</summary>
Motivation: 传统手动分类脑肿瘤图像耗时费力，浅层CNN模型精度不理想，需提升分类效率与准确率。

Method: 以ResNet34为骨干网络，引入多尺度输入模块作为首层、Inception v2模块作为残差下采样层，并加入通道注意力机制进行特征加权。

Result: 五折交叉验证平均准确率达98.8%，比ResNet34高1%，参数量仅为原模型的80%。

Conclusion: 改进模型在减少参数量的同时显著提升分类精度，实现了更高效率与更优性能的平衡。

Abstract: Previously, image interpretation in radiology relied heavily on manual methods. However, manual classification of brain tumor medical images is time-consuming and labor-intensive. Even with shallow convolutional neural network models, the accuracy is not ideal. To improve the efficiency and accuracy of brain tumor image classification, this paper proposes a brain tumor classification model based on an improved ResNet34 network. This model uses the ResNet34 residual network as the backbone network and incorporates multi-scale feature extraction. It uses a multi-scale input module as the first layer of the ResNet34 network and an Inception v2 module as the residual downsampling layer. Furthermore, a channel attention mechanism module assigns different weights to different channels of the image from a channel domain perspective, obtaining more important feature information. The results after a five-fold crossover experiment show that the average classification accuracy of the improved network model is approximately 98.8%, which is not only 1% higher than ResNet34, but also only 80% of the number of parameters of the original model. Therefore, the improved network model not only improves accuracy but also reduces clutter, achieving a classification effect with fewer parameters and higher accuracy.

</details>


### [84] [LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling](https://arxiv.org/abs/2512.03796)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CV

TL;DR: 本文提出Latent Scale Rejection Sampling (LSRS) 方法，通过在隐空间尺度上逐步优化token映射，提升视觉自回归（VAR）模型的图像生成质量，显著降低FID分数且仅引入极小计算开销。


<details>
  <summary>Details</summary>
Motivation: VAR模型在并行解码多token时易产生结构错误，影响生成质量。

Method: 提出LSRS方法，利用轻量级打分模型在每个隐尺度评估多个候选token映射，选择高质量映射指导后续尺度生成，并优先优化对结构一致性至关重要的早期尺度。

Result: 在VAR-d30模型上，LSRS仅增加1%推理时间即使FID从1.95降至1.78；增加15%推理时间可进一步降至1.66。

Conclusion: LSRS是一种高效、低开销的测试时扩展策略，能显著提升VAR模型生成质量。

Abstract: Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR's generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation.

</details>


### [85] [HieroGlyphTranslator: Automatic Recognition and Translation of Egyptian Hieroglyphs to English](https://arxiv.org/abs/2512.03817)
*Ahmed Nasser,Marwan Mohamed,Alaa Sherif,Basmala Mahmoud,Shereen Yehia,Asmaa Saad,Mariam S. El-Rahmany,Ensaf H. Mohamed*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的古埃及象形文字图像自动识别与英译方法，结合轮廓分割、Detectron2、Gardiner编码映射及CNN翻译模型，在两个数据集上实现BLEU得分42.2。


<details>
  <summary>Details</summary>
Motivation: 古埃及象形文字由图画构成，单个符号多义性强，人工翻译困难；现有深度学习翻译技术发展迅速，但尚未有效应用于该领域。

Method: 三阶段方法：1）图像分割（Contour + Detectron2）；2）将象形符号映射为Gardiner编码；3）使用CNN模型进行翻译。采用Morris Franken和EgyptianTranslation两个数据集。

Result: 模型在翻译任务中达到BLEU得分42.2，显著优于先前研究。

Conclusion: 该方法验证了深度学习在古文字识别与翻译中的可行性与有效性，为濒危古文字的数字化保护与理解提供了新路径。

Abstract: Egyptian hieroglyphs, the ancient Egyptian writing system, are composed entirely of drawings. Translating these glyphs into English poses various challenges, including the fact that a single glyph can have multiple meanings. Deep learning translation applications are evolving rapidly, producing remarkable results that significantly impact our lives. In this research, we propose a method for the automatic recognition and translation of ancient Egyptian hieroglyphs from images to English. This study utilized two datasets for classification and translation: the Morris Franken dataset and the EgyptianTranslation dataset. Our approach is divided into three stages: segmentation (using Contour and Detectron2), mapping symbols to Gardiner codes, and translation (using the CNN model). The model achieved a BLEU score of 42.2, a significant result compared to previous research.

</details>


### [86] [A Robust Camera-based Method for Breath Rate Measurement](https://arxiv.org/abs/2512.03827)
*Alexey Protopopov*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频的鲁棒呼吸率测量方法，结合数学变换，在最小硬件需求下实现<5%的相对误差，平均绝对误差为0.57次/分钟，优于先前工作，且对被试运动干扰更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频的呼吸率测量方法要么在近理想条件下测试，要么精度不足，难以满足实际应用中对鲁棒性和低硬件依赖的需求。

Method: 提出一种结合多种数学变换的视频分析方法，无需额外硬件，在普通摄像头采集的视频上提取呼吸信号，并通过相对偏差评估性能。

Result: 在14名志愿者、总时长超2.5小时的视频数据上验证，平均绝对误差为0.57次/分钟，相对偏差<5%，显著优于以往方法；对受试者运动具有更强鲁棒性。

Conclusion: 该方法实现了高精度、低硬件依赖、强鲁棒性的远程呼吸率测量，适用于更自然、自由的行为场景。

Abstract: Proliferation of cheap and accessible cameras makes it possible to measure a subject's breath rate from video footage alone. Recent works on this topic have proposed a variety of approaches for accurately measuring human breath rate, however they are either tested in near-ideal conditions, or produce results that are not sufficiently accurate. The present study proposes a more robust method to measure breath rate in humans with minimal hardware requirements using a combination of mathematical transforms with a relative deviation from the ground truth of less than 5%. The method was tested on videos taken from 14 volunteers with a total duration of over 2 hours 30 minutes. The obtained results were compared to reference data and the average mean absolute error was found to be at 0.57 respirations per minute, which is noticeably better than the results from previous works. The breath rate measurement method proposed in the present article is more resistant to distortions caused by subject movement and thus allows one to remotely measure the subject's breath rate without any significant limitations on the subject's behavior.

</details>


### [87] [Lean Unet: A Compact Model for Image Segmentation](https://arxiv.org/abs/2512.03834)
*Ture Hassler,Ida Åkerholm,Marcus Nordström,Gabriele Balletti,Orcun Goksel*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级Unet架构（LUnet），通过保持通道数恒定、简化层次结构，在大幅减少参数量（超30倍）的同时，实现了与标准Unet及剪枝后网络相当甚至更优的分割性能，尤其在医学影像（MRI/CT）任务中验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有Unet因逐层翻倍通道数导致内存占用大、训练批大小受限、推理延迟高；通道剪枝虽能压缩模型，但优化耗时长、泛化性差，且作者发现剪枝结果主要集中在高通道层，结构本身比剪枝策略更重要。

Method: 提出LUnet：采用扁平化结构，空间分辨率下降时通道数保持恒定（不翻倍）；利用跳跃连接缓解信息瓶颈，显著降低瓶颈层通道需求；无需数据自适应剪枝，使用固定紧凑架构。

Result: 在公开MRI数据集和两个内部CT数据集上验证，LUnet参数量减少30倍以上，性能媲美或优于标准Unet及STAMP等先进剪枝方法；相同总参数量下，LUnet性能优于标准Unet。

Conclusion: Unet的结构设计（如恒定通道、扁平层次、强跳跃连接）比复杂的通道剪枝策略更关键；LUnet以极简设计实现高效准确的医学图像分割，为资源受限场景提供实用解决方案。

Abstract: Unet and its variations have been standard in semantic image segmentation, especially for computer assisted radiology. Current Unet architectures iteratively downsample spatial resolution while increasing channel dimensions to preserve information content. Such a structure demands a large memory footprint, limiting training batch sizes and increasing inference latency. Channel pruning compresses Unet architecture without accuracy loss, but requires lengthy optimization and may not generalize across tasks and datasets. By investigating Unet pruning, we hypothesize that the final structure is the crucial factor, not the channel selection strategy of pruning. Based on our observations, we propose a lean Unet architecture (LUnet) with a compact, flat hierarchy where channels are not doubled as resolution is halved. We evaluate on a public MRI dataset allowing comparable reporting, as well as on two internal CT datasets. We show that a state-of-the-art pruning solution (STAMP) mainly prunes from the layers with the highest number of channels. Comparatively, simply eliminating a random channel at the pruning-identified layer or at the largest layer achieves similar or better performance. Our proposed LUnet with fixed architectures and over 30 times fewer parameters achieves performance comparable to both conventional Unet counterparts and data-adaptively pruned networks. The proposed lean Unet with constant channel count across layers requires far fewer parameters while achieving performance superior to standard Unet for the same total number of parameters. Skip connections allow Unet bottleneck channels to be largely reduced, unlike standard encoder-decoder architectures requiring increased bottleneck channels for information propagation.

</details>


### [88] [Heatmap Pooling Network for Action Recognition from RGB Videos](https://arxiv.org/abs/2512.03837)
*Mengyuan Liu,Jinfu Liu,Yongkang Jiang,Bin He*

Main category: cs.CV

TL;DR: 本文提出了一种名为HP-Net的新型热图池化网络，通过反馈池化模块提取信息丰富、鲁棒且简洁的人体动作特征，并结合空间-运动协同学习与文本精炼调制模块实现多模态融合，显著提升了视频中人类动作识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RGB视频动作识别方法存在信息冗余、易受噪声干扰及存储成本高等问题，需更高效利用视频中的有用信息。

Method: 提出热图池化网络（HP-Net），包括反馈池化模块用于提取人体关键区域的鲁棒池化特征，以及空间-运动协同学习模块和文本精炼调制模块实现多模态特征融合。

Result: 在NTU RGB+D 60/120、Toyota-Smarthome和UAV-Human等多个基准数据集上，HP-Net均优于现有方法，验证了其有效性。

Conclusion: HP-Net能有效提取高质量人体动作特征并融合多模态信息，为视频动作识别提供了新思路和实用工具。

Abstract: Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.

</details>


### [89] [CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation](https://arxiv.org/abs/2512.03844)
*Letian Zhou,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: 本文提出Core Distribution Alignment (CoDA)框架，利用现成的文生图模型进行数据集蒸馏，通过识别目标数据集的‘内在核心分布’并引导生成过程与之对齐，克服了现有方法依赖目标数据预训练扩散模型或存在分布不匹配的问题，在ImageNet-1K等基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型的数据集蒸馏方法存在两大问题：一是多数方法需在目标数据集上预训练扩散模型，违背蒸馏初衷且成本高昂；二是使用通用文生图模型的方法因分布不匹配而性能受限。

Method: 提出CoDA框架，首先用鲁棒的基于密度的发现机制识别目标数据集的‘内在核心分布’，再引导文生图模型生成样本以对齐该分布。

Result: CoDA在不依赖目标数据预训练生成模型的前提下，在ImageNet-1K等所有基准上性能媲美甚至超越以往依赖该模型的方法；在ImageNet-1K的50图像/类设置下达到60.4%新SOTA准确率。

Conclusion: CoDA成功弥合了通用生成先验与目标语义之间的鸿沟，证明仅用现成文生图模型即可实现高效、高性能的数据集蒸馏。

Abstract: Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the "intrinsic core distribution" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA

</details>


### [90] [PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation](https://arxiv.org/abs/2512.03848)
*Hania Ghouse,Maryam Alsharqi,Farhad R. Nezami,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本文提出了PULSE，一种基于自监督表示的多任务视觉-语言框架，用于统一心脏图像分析中的解剖分割、疾病分类和临床报告生成任务，并在多种成像模态和数据集上展现出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有心脏图像分析方法在解剖分割、疾病分类和临床报告生成等任务上相互割裂，缺乏一个能统一这些目标、同时保持跨模态与跨数据集泛化能力的单一架构。

Method: PULSE基于自监督表征，采用复合监督策略（兼顾区域重叠学习、像素级分类保真度和边界感知IoU优化）；通过多尺度token重建解码器实现解剖分割，共享全局表征支持疾病分类与临床文本生成。

Result: PULSE能够在一个统一架构中完成从像素到结构再到临床推理的全过程；学习到任务无关的心脏先验知识，在多个数据集上鲁棒泛化，并可仅用少量监督适配新成像模态。

Conclusion: PULSE推动了心脏影像分析向可扩展、基础模型式框架的发展，为多任务、跨模态医学AI提供了新范式。

Abstract: Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.

</details>


### [91] [Traffic Image Restoration under Adverse Weather via Frequency-Aware Mamba](https://arxiv.org/abs/2512.03852)
*Liwen Pan,Longguang Wang,Guangwei Gao,Jun Wang,Jun Shi,Juncheng Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为Frequency-Aware Mamba（FAMamba）的新框架，将频域先验引入Mamba架构，用于恶劣天气下的交通图像恢复。通过双分支特征提取块（DFEB）和先验引导块（PGB），结合自适应频域扫描机制（AFSM），实现了高效、高质量的图像重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注空间域建模，忽视频域先验；Mamba虽擅长长程依赖建模，但其在频域特征提取上的潜力尚未被探索。

Method: 提出FAMamba框架，包含：(1) 双分支特征提取块（DFEB），采用双向2D频域自适应扫描增强局部-全局交互；(2) 先验引导块（PGB），基于小波的高频残差学习以细化纹理；(3) 自适应频域扫描机制（AFSM），使Mamba能在不同子图间进行频域扫描。

Result: 实验表明FAMamba在交通图像恶劣天气恢复任务中具有显著效率与有效性，重建图像细节更精确、质量更高。

Conclusion: 频域引导与序列建模的结合是提升恶劣天气图像恢复性能的有效路径，FAMamba为Mamba架构在低层视觉任务中的频域扩展提供了新范式。

Abstract: Traffic image restoration under adverse weather conditions remains a critical challenge for intelligent transportation systems. Existing methods primarily focus on spatial-domain modeling but neglect frequency-domain priors. Although the emerging Mamba architecture excels at long-range dependency modeling through patch-wise correlation analysis, its potential for frequency-domain feature extraction remains unexplored. To address this, we propose Frequency-Aware Mamba (FAMamba), a novel framework that integrates frequency guidance with sequence modeling for efficient image restoration. Our architecture consists of two key components: (1) a Dual-Branch Feature Extraction Block (DFEB) that enhances local-global interaction via bidirectional 2D frequency-adaptive scanning, dynamically adjusting traversal paths based on sub-band texture distributions; and (2) a Prior-Guided Block (PGB) that refines texture details through wavelet-based high-frequency residual learning, enabling high-quality image reconstruction with precise details. Meanwhile, we design a novel Adaptive Frequency Scanning Mechanism (AFSM) for the Mamba architecture, which enables the Mamba to achieve frequency-domain scanning across distinct subgraphs, thereby fully leveraging the texture distribution characteristics inherent in subgraph structures. Extensive experiments demonstrate the efficiency and effectiveness of FAMamba.

</details>


### [92] [Prostate biopsy whole slide image dataset from an underrepresented Middle Eastern population](https://arxiv.org/abs/2512.03854)
*Peshawa J. Muhammad Ali,Navin Vincent,Saman S. Abdulla,Han N. Mohammed Fadhl,Anders Blilie,Kelvin Szolnoky,Julia Anna Mielcarz,Xiaoyi Ji,Kimmo Kartasalo,Abdulbasit K. Al-Talabani,Nita Mulliqi*

Main category: cs.CV

TL;DR: 本文介绍了来自伊拉克埃尔比勒的339张前列腺穿刺活检全切片图像数据集，旨在提升AI病理模型在中东等欠数字化地区人群中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有公开组织病理学数据集稀缺且主要代表西方人群，导致AI模型在中东等欠数字化地区人群中的泛化能力未知，因此需发布具有全球多样性的新数据集。

Method: 收集了伊拉克埃尔比勒连续185名患者的339张前列腺穿刺活检全切片图像，由三位病理学家独立标注Gleason评分和ISUP分级；使用Leica、Hamamatsu和Grundium三种扫描仪扫描；所有切片均去标识化并以原生格式提供。

Result: 构建了一个包含多扫描仪来源、多专家标注、去标识化且原生格式的前列腺病理图像数据集，支持分级一致性分析、颜色归一化及跨扫描仪鲁棒性评估。

Conclusion: 该数据集填补了中东人群病理影像数据的空白，有助于提升AI模型在全球多样性人群中的公平性与可靠性，并将通过Bioimage Archive以CC BY 4.0协议公开共享。

Abstract: Artificial intelligence (AI) is increasingly used in digital pathology. Publicly available histopathology datasets remain scarce, and those that do exist predominantly represent Western populations. Consequently, the generalizability of AI models to populations from less digitized regions, such as the Middle East, is largely unknown. This motivates the public release of our dataset to support the development and validation of pathology AI models across globally diverse populations. We present 339 whole-slide images of prostate core needle biopsies from a consecutive series of 185 patients collected in Erbil, Iraq. The slides are associated with Gleason scores and International Society of Urological Pathology grades assigned independently by three pathologists. Scanning was performed using two high-throughput scanners (Leica and Hamamatsu) and one compact scanner (Grundium). All slides were de-identified and are provided in their native formats without further conversion. The dataset enables grading concordance analyses, color normalization, and cross-scanner robustness evaluations. Data will be deposited in the Bioimage Archive (BIA) under accession code: to be announced (TBA), and released under a CC BY 4.0 license.

</details>


### [93] [Diminishing Returns in Self-Supervised Learning](https://arxiv.org/abs/2512.03862)
*Oli Bridge,Huey Sun,Botond Branyicskai-Nagy,Charles D'Ornano,Shomit Basu*

Main category: cs.CV

TL;DR: 本文探讨了小规模视觉Transformer（5M参数）在不同预训练、中间微调和下游任务设置下的性能表现，发现预训练和微调总体有益但收益递减，而中间微调可能因任务机制不匹配反而损害下游性能。


<details>
  <summary>Details</summary>
Motivation: 探索小参数量视觉Transformer在有限资源下如何通过不同训练策略（预训练、中间微调、下游微调）最大化性能，避免计算浪费与性能下降。

Method: 在三个不同的预训练、中间微调和下游数据集及目标上开展系统性实验，评估各阶段对小ViT（5M参数）性能的边际影响。

Result: 预训练和下游微调始终提升性能但存在边际递减；中间微调在某些情况下显著降低下游性能，推测源于任务机制不匹配。

Conclusion: 小规模ViT更依赖有针对性的预训练和精细的数据选择，盲目叠加中间任务不仅浪费算力，还可能损害最终性能。

Abstract: While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.

</details>


### [94] [An Automated Framework for Large-Scale Graph-Based Cerebrovascular Analysis](https://arxiv.org/abs/2512.03869)
*Daniele Falcetta,Liane S. Canas,Lorenzo Suppa,Matteo Pentassuglia,Jon Cleary,Marc Modat,Sébastien Ourselin,Maria A. Zuluaga*

Main category: cs.CV

TL;DR: CaravelMetrics是一个全自动的脑血管分析计算框架，通过骨架化生成图表示来建模血管形态，可提取多尺度（全局和区域）的15种血管特征，并在IXI数据集上验证了其对年龄、性别和教育程度相关变化的敏感性。


<details>
  <summary>Details</summary>
Motivation: 需要一种可扩展、全自动的定量方法来表征脑血管结构，支持血管健康与衰老的群体水平研究。

Method: 基于图论的血管骨架化建模，结合图谱引导的区域分割、中心线提取与图构建，提取形态学、拓扑学、分形和几何等15种特征，支持全局和动脉供血区层级分析。

Result: 在570例IXI数据集3D TOF-MRA扫描中，CaravelMetrics生成了可重复的血管图，成功检测到年龄、性别差异及教育程度相关的血管复杂性增加。

Conclusion: CaravelMetrics为脑血管的标准化、自动化定量分析提供了可靠工具，适用于建立正常参考模型及大规模人群研究。

Abstract: We present CaravelMetrics, a computational framework for automated cerebrovascular analysis that models vessel morphology through skeletonization-derived graph representations. The framework integrates atlas-based regional parcellation, centerline extraction, and graph construction to compute fifteen morphometric, topological, fractal, and geometric features. The features can be estimated globally from the complete vascular network or regionally within arterial territories, enabling multiscale characterization of cerebrovascular organization. Applied to 570 3D TOF-MRA scans from the IXI dataset (ages 20-86), CaravelMetrics yields reproducible vessel graphs capturing age- and sex-related variations and education-associated increases in vascular complexity, consistent with findings reported in the literature. The framework provides a scalable and fully automated approach for quantitative cerebrovascular feature extraction, supporting normative modeling and population-level studies of vascular health and aging.

</details>


### [95] [Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy](https://arxiv.org/abs/2512.03883)
*Jorge Tapias Gomez,Despoina Kanata,Aneesh Rangnekar,Christina Lee,Julio Garcia-Aguilar,Joshua Jesse Smith,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: 本文提出了一种名为SSDCA的Siamese Swin Transformer模型，用于通过配对的纵向内镜图像区分直肠癌患者在全程新辅助治疗后临床完全缓解（cCR）与局部再生长（LR），无需图像空间对齐，具备高准确率、强鲁棒性和良好特征判别性。


<details>
  <summary>Details</summary>
Motivation: 临床上需要客观、准确的方法，在观察等待（WW）策略中早期识别直肠癌患者内镜随访图像中的局部再生长（LR），以优化管理并防止远处转移。

Method: 提出Siamese Swin Transformer with Dual Cross-Attention（SSDCA）：利用预训练Swin Transformer提取域无关特征，并引入双交叉注意力机制融合两次扫描（基线与随访）图像特征，无需空间对齐；在135例患者图像对上训练，在62例独立患者上测试。

Result: SSDCA达到最佳平衡准确率（81.76% ± 0.04）、敏感性（90.07% ± 0.08）和特异性（72.86% ± 0.05）；对血液、粪便、毛细血管扩张及低质量图像等干扰具有鲁棒性；UMAP聚类显示其特征具有强类间分离性（1.45 ± 0.18）和弱类内离散性（1.07 ± 0.19）。

Conclusion: SSDCA是一种有效、鲁棒且可解释的AI方法，为直肠癌WW策略中LR的无创、精准监测提供了新工具。

Abstract: Increasing evidence supports watch-and-wait (WW) surveillance for patients with rectal cancer who show clinical complete response (cCR) at restaging following total neoadjuvant treatment (TNT). However, objectively accurate methods to early detect local regrowth (LR) from follow-up endoscopy images during WW are essential to manage care and prevent distant metastases. Hence, we developed a Siamese Swin Transformer with Dual Cross-Attention (SSDCA) to combine longitudinal endoscopic images at restaging and follow-up and distinguish cCR from LR. SSDCA leverages pretrained Swin transformers to extract domain agnostic features and enhance robustness to imaging variations. Dual cross attention is implemented to emphasize features from the two scans without requiring any spatial alignment of images to predict response. SSDCA as well as Swin-based baselines were trained using image pairs from 135 patients and evaluated on a held-out set of image pairs from 62 patients. SSDCA produced the best balanced accuracy (81.76\% $\pm$ 0.04), sensitivity (90.07\% $\pm$ 0.08), and specificity (72.86\% $\pm$ 0.05). Robustness analysis showed stable performance irrespective of artifacts including blood, stool, telangiectasia, and poor image quality. UMAP clustering of extracted features showed maximal inter-cluster separation (1.45 $\pm$ 0.18) and minimal intra-cluster dispersion (1.07 $\pm$ 0.19) with SSDCA, confirming discriminative representation learning.

</details>


### [96] [Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence](https://arxiv.org/abs/2512.03905)
*Shuai Yang,Junxin Lin,Yifan Zhou,Ziwei Liu,Chen Change Loy*

Main category: cs.CV

TL;DR: 本文提出FRESCO方法，通过融合帧内与帧间对应关系，增强时空一致性约束，提升零样本视频生成（如视频到视频翻译和文本引导视频编辑）的视觉连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本视频生成方法在注意力机制中对帧间对应关系的软约束不足，导致时间不一致问题。

Method: 提出FRESCO框架，联合建模帧内与帧间对应关系，形成更强的时空约束，并显式优化特征以提升一致性。

Result: 在视频到视频翻译和文本引导视频编辑两个零样本任务上验证了方法有效性，生成视频具有更高时空一致性和视觉连贯性。

Conclusion: FRESCO显著优于当前零样本方法，为高质量、高一致性视频生成提供了新思路。

Abstract: The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.

</details>


### [97] [UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework](https://arxiv.org/abs/2512.03918)
*Youxin Pang,Yong Zhang,Ruizhi Shao,Xiang Deng,Feng Gao,Xu Xiaoming,Xiaoming Wei,Yebin Liu*

Main category: cs.CV

TL;DR: 本文提出了UniMo，一种创新的自回归模型，首次在统一框架中联合建模2D人体视频和3D人体运动，实现两者的同步生成与理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法多局限于单向条件生成（如由视频生成动作或反之）或与其他模态（如文本、音频）融合，而统一建模2D视频与3D动作仍属空白，主要挑战在于二者在结构与分布上的巨大差异。

Method: 将视频与3D动作统一为token序列，采用独立嵌入层缓解分布差异；设计融合双任务的序列建模策略；提出带时间扩展策略的新型3D动作tokenizer，使用单个VQ-VAE量化动作，并配备多个专家解码器分别处理身体形状、平移、全局朝向和姿态。

Result: 实验表明，UniMo能同步生成配对的2D视频与3D动作，并实现高精度动作捕捉。

Conclusion: 该工作借鉴大语言模型跨模态统一能力，为人体中心信息融入现有模型奠定基础，有望推动人-物-场景的多模态可控联合建模。

Abstract: We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.

</details>


### [98] [BlurDM: A Blur Diffusion Model for Image Deblurring](https://arxiv.org/abs/2512.03979)
*Jin-Ting He,Fu-Jen Tsai,Yan-Tsung Peng,Min-Hung Chen,Chia-Wen Lin,Yen-Yu Lin*

Main category: cs.CV

TL;DR: 本文提出BlurDM，一种将模糊形成过程融入扩散模型的动态场景去模糊方法，通过双扩散前向过程建模运动模糊，并在反向生成中联合去噪与去模糊，最终在潜在空间实现高效先验生成，显著提升多种去模糊方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在动态场景去模糊中未能充分利用模糊过程的内在特性，限制了其潜力。

Method: 提出BlurDM，采用双扩散前向方案（同时对噪声和模糊进行扩散）建模模糊形成过程；在反向过程中推导出联合去噪与去模糊公式；并在潜在空间中执行BlurDM以构建灵活的去模糊先验生成网络。

Result: BlurDM在四个基准数据集上显著且一致地提升了现有去模糊方法的性能。

Conclusion: 将模糊形成机制显式嵌入扩散模型框架是有效的，BlurDM作为一种通用先验生成模块，可广泛增强各类去模糊网络。

Abstract: Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.

</details>


### [99] [Beyond the Ground Truth: Enhanced Supervision for Image Restoration](https://arxiv.org/abs/2512.03932)
*Donghun Ryou,Inju Ha,Sanghyeok Chu,Bohyung Han*

Main category: cs.CV

TL;DR: 本文提出一种通过频率域混合增强真实场景图像恢复任务中监督信号质量的新框架，利用自适应频率掩码生成感知质量更高的伪真值图像，并结合轻量级输出精炼网络提升恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的图像恢复方法受限于真实世界退化数据集中真值图像的质量，难以获得高质量监督信号。

Method: 提出一个新框架：1）设计条件式频率掩码生成器学习自适应频率掩码；2）在频率域对原始真值与其超分变体进行掩码引导的混合，生成感知增强的真值图像；3）用增强后的真值训练轻量级输出精炼网络，可即插即用地提升任意恢复模型性能。

Result: 在多个真实退化图像恢复任务（如去模糊、去噪、超分等）上显著提升恢复图像的感知质量与保真度；用户研究表明该方法在视觉质量上优于现有方法。

Conclusion: 通过增强监督信号质量而非仅改进模型结构，本文为真实世界图像恢复提供了新范式，验证了高质量监督对提升模型性能的关键作用。

Abstract: Deep learning-based image restoration has achieved significant success. However, when addressing real-world degradations, model performance is limited by the quality of ground-truth images in datasets due to practical constraints in data acquisition. To address this limitation, we propose a novel framework that enhances existing ground truth images to provide higher-quality supervision for real-world restoration. Our framework generates perceptually enhanced ground truth images using super-resolution by incorporating adaptive frequency masks, which are learned by a conditional frequency mask generator. These masks guide the optimal fusion of frequency components from the original ground truth and its super-resolved variants, yielding enhanced ground truth images. This frequency-domain mixup preserves the semantic consistency of the original content while selectively enriching perceptual details, preventing hallucinated artifacts that could compromise fidelity. The enhanced ground truth images are used to train a lightweight output refinement network that can be seamlessly integrated with existing restoration models. Extensive experiments demonstrate that our approach consistently improves the quality of restored images. We further validate the effectiveness of both supervision enhancement and output refinement through user studies. Code is available at https://github.com/dhryougit/Beyond-the-Ground-Truth.

</details>


### [100] [DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation](https://arxiv.org/abs/2512.03992)
*Zexin Lin,Hawen Wan,Yebin Zhong,Xiaoqiang*

Main category: cs.CV

TL;DR: 本文提出了DIQ-H基准，用于评估视觉-语言模型（VLMs）在动态视觉退化下的鲁棒性，特别是针对时序视频流中的幻觉持续、错误恢复和时间一致性问题；并提出不确定性引导的迭代精炼（UIR）方法以高效生成高质量伪标注；实验表明现有VLM在真实退化场景下鲁棒性仍严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有VLM评测基准仅关注静态高质量图像，忽视了安全关键应用（如自动驾驶）中常见的时序视觉退化与错误传播问题，导致模型在实际部署中易因瞬时干扰产生持续幻觉。

Method: 构建首个面向动态视觉退化的时序VLM评测基准DIQ-H，引入基于物理的运动模糊、传感器噪声与压缩伪影等退化类型，并设计多轮问答任务评估幻觉持续性、错误恢复能力与时间一致性；提出不确定性引导的迭代精炼（UIR）方法，利用轻量VLM结合不确定性过滤生成高质伪标注。

Result: 在16个SOTA VLM上的实验显示：GPT-4o恢复率仅78.5%，开源模型时间一致性低于60%；UIR方法相较基线提升15.3%标注准确率。

Conclusion: DIQ-H揭示了当前VLM在真实动态退化场景下的显著鲁棒性缺陷，为推动VLM向安全可靠部署提供了新评测范式与实用工具。

Abstract: Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.

</details>


### [101] [Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation](https://arxiv.org/abs/2512.03996)
*Hang Xu,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时缩放（TTS）方法——文本嵌入扰动（TEP），通过在频域分析中与空间噪声互补，提升文本到图像扩散模型的生成多样性与质量，无需额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有TTS方法在文本到图像扩散模型中主要关注搜索策略和奖励模型，但忽略了扩散过程中噪声随机性对性能的影响，尤其是高频细节生成的局限性。

Method: 提出文本嵌入扰动作为新形式的随机性，结合频域分析设计步长依赖的扰动策略和频率自适应的扰动强度调节机制，与空间噪声协同作用。

Result: 在多个基准上显著提升生成质量与多样性，且几乎不增加计算开销，可无缝集成到现有TTS方法中。

Conclusion: 文本嵌入扰动与空间噪声在频域上具有互补性，能有效弥补传统噪声在高频率操作中的不足，为TTS提供了更鲁棒、高效的随机性建模方式。

Abstract: Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.

</details>


### [102] [TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning](https://arxiv.org/abs/2512.03963)
*Tao Wu,Li Yang,Gen Zhan,Yiting Liao,Junlin Li,Deliang Fu,Li Zhang,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了TempR1框架，通过多任务强化学习提升多模态大语言模型（MLLMs）的时间理解能力，涵盖时间定位、动作检测等任务，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的时间推理方法受限于任务类型和数据范围，泛化能力不足，难以应对多样化的时序理解场景。

Method: 提出Temporal-aware多任务强化学习框架TempR1，构建涵盖多种时间结构与语义的多任务语料库，并基于GRPO算法进行跨任务稳定优化；将时间任务按预测区间与真实实例的对应关系分为三类，为每类设计定制化定位奖励。

Result: TempR1在多个时序理解基准上取得SOTA性能；联合优化互补任务带来协同效应，显著提升泛化能力和单任务性能。

Conclusion: TempR1为MLLMs的时间推理提供了一种可扩展、有原则的范式，有效增强了其对细粒度时间依赖和不同时间模式的适应能力。

Abstract: Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.

</details>


### [103] [Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding](https://arxiv.org/abs/2512.04000)
*Jialuo Li,Bin Li,Jiahao Li,Yan Lu*

Main category: cs.CV

TL;DR: 本文提出DIG框架，一种无需训练的视频帧选择方法，根据查询类型（全局/局部）自适应选择帧，提升长视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在长视频理解中受限于上下文长度和密集视频token处理的高计算成本，而现有查询感知帧选择方法又带来显著计算开销。

Method: 首先构建查询类型学（全局查询 vs 局部查询），验证不同查询类型适用不同采样策略；进而提出无训练的自适应框架DIG：对全局查询采用均匀采样，对局部查询启用专用相关帧提取流程。

Result: 在三个长视频理解基准上，DIG持续优于现有基线，且在输入帧数扩展至256时仍稳健提升LMM性能。

Conclusion: 并非所有查询都需要复杂搜索机制；基于查询类型的自适应帧选择策略（如DIG）更高效、更有效，为长视频理解提供了新范式。

Abstract: The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.

</details>


### [104] [Training for Identity, Inference for Controllability: A Unified Approach to Tuning-Free Face Personalization](https://arxiv.org/abs/2512.03964)
*Lianyu Pang,Ji Zhou,Qiping Wang,Baoquan Zhao,Zhenguo Yang,Qing Li,Xudong Mao*

Main category: cs.CV

TL;DR: 本文提出了一种无需微调的统一人脸个性化框架UniID，融合文本嵌入和适配器两种范式，在保持高身份保真度的同时实现灵活的文本可控性。


<details>
  <summary>Details</summary>
Motivation: 现有无调优人脸个性化方法难以同时兼顾高身份保真度和灵活文本可控性。

Method: 提出UniID框架，通过训练阶段的身份聚焦学习策略和推理阶段的归一化重缩放机制，协同整合文本嵌入与适配器方法，使二者仅在身份相关信息上相互增强，同时保留扩散模型对非身份属性的先验。

Result: 在六种SOTA方法对比实验中，UniID在身份保持和文本可控性两方面均取得更优性能。

Conclusion: UniID实现了无需微调下高保真、强可控的人脸个性化，为统一不同范式提供了新思路。

Abstract: Tuning-free face personalization methods have developed along two distinct paradigms: text embedding approaches that map facial features into the text embedding space, and adapter-based methods that inject features through auxiliary cross-attention layers. While both paradigms have shown promise, existing methods struggle to simultaneously achieve high identity fidelity and flexible text controllability. We introduce UniID, a unified tuning-free framework that synergistically integrates both paradigms. Our key insight is that when merging these approaches, they should mutually reinforce only identity-relevant information while preserving the original diffusion prior for non-identity attributes. We realize this through a principled training-inference strategy: during training, we employ an identity-focused learning scheme that guides both branches to capture identity features exclusively; at inference, we introduce a normalized rescaling mechanism that recovers the text controllability of the base diffusion model while enabling complementary identity signals to enhance each other. This principled design enables UniID to achieve high-fidelity face personalization with flexible text controllability. Extensive experiments against six state-of-the-art methods demonstrate that UniID achieves superior performance in both identity preservation and text controllability. Code will be available at https://github.com/lyuPang/UniID

</details>


### [105] [On the Temporality for Sketch Representation Learning](https://arxiv.org/abs/2512.04007)
*Marcelo Isaias de Moraes Junior,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 本文探讨了手绘草图作为序列建模的合理性，发现绝对坐标编码优于相对坐标，非自回归解码器优于自回归解码器，且时间性的重要性依赖于所采用的顺序和具体任务。


<details>
  <summary>Details</summary>
Motivation: 当前草图表征学习虽有进展，但对时间维度在表征质量中真实作用的理解仍不足，本文旨在探究将草图视为序列是否合理，以及何种内部顺序更关键。

Method: 通过对比不同位置编码（绝对 vs 相对）及解码器类型（自回归 vs 非自回归），在多种草图相关任务上评估时间建模的影响。

Result: 绝对坐标编码持续优于相对坐标；非自回归解码器性能优于自回归解码器；时间性的重要性随顺序类型和任务而变化。

Conclusion: 将草图建模为序列是可行的，但其有效性高度依赖于位置编码方式、解码策略及下游任务，不能一概而论地强调时间性。

Abstract: Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.

</details>


### [106] [PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation](https://arxiv.org/abs/2512.04025)
*Xiaolong Li,Youping Gu,Xi Lin,Weijie Wang,Bohan Zhuang*

Main category: cs.CV

TL;DR: 本文提出Pyramid Sparse Attention (PSA)，一种通过多级池化KV表示实现细粒度稀疏注意力的高效机制，兼顾信息保留与计算效率，在视频理解与生成任务中显著优于现有稀疏注意力方法。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法采用二值掩码，导致高稀疏度下严重信息损失；需在低计算开销下缓解该问题。

Method: PSA引入多级池化KV表示，每个查询块动态为不同重要性的KV块分配不同池化层级，实现介于全保留与全剪枝之间的信息插值；采用硬件友好的解耦分块-瓦片内核设计。

Result: 在视频理解与生成多个基准上，PSA在保持上下文信息和视觉保真度的同时，性能优于或媲美现有稀疏注意力基线，且具有更优的效率-质量权衡。

Conclusion: PSA是一种通用、高效、信息保留能力强的稀疏注意力模块，适用于视频建模任务，并具备硬件友好性与实际部署潜力。

Abstract: Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA

</details>


### [107] [DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment](https://arxiv.org/abs/2512.03981)
*Sheng-Hao Liao,Shang-Fu Chen,Tai-Ming Huang,Wen-Huang Cheng,Kai-Lung Hua*

Main category: cs.CV

TL;DR: 本文提出DirectDrag，一种无需手动掩码和文本提示的拖拽式图像编辑框架，通过自动软掩码生成和读出引导特征对齐机制，实现高保真度和精确点对齐的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型的拖拽式图像编辑方法严重依赖手动提供的掩码和文本提示来保持语义保真度和运动精度，去除这些约束会导致视觉伪影或空间控制差的问题。

Method: 提出DirectDrag框架，包含两个关键创新：1）Auto Soft Mask Generation模块，根据点位移智能推断可编辑区域；2）Readout-Guided Feature Alignment机制，利用扩散模型中间激活保持结构一致性。

Result: 在DragBench和真实场景中实验表明，DirectDrag在不使用手动掩码和提示的情况下，图像质量优于现有方法，且拖拽精度具有竞争力。

Conclusion: DirectDrag实现了高质量、交互式的图像编辑，显著降低了用户输入负担，同时保持高保真度与精确的空间控制。

Abstract: Drag-based image editing using generative models provides intuitive control over image structures. However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision. Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts. To address these limitations, we propose DirectDrag, a novel mask- and prompt-free editing framework. DirectDrag enables precise and efficient manipulation with minimal user input while maintaining high image fidelity and accurate point alignment. DirectDrag introduces two key innovations. First, we design an Auto Soft Mask Generation module that intelligently infers editable regions from point displacement, automatically localizing deformation along movement paths while preserving contextual integrity through the generative model's inherent capacity. Second, we develop a Readout-Guided Feature Alignment mechanism that leverages intermediate diffusion activations to maintain structural consistency during point-based edits, substantially improving visual fidelity. Despite operating without manual mask or prompt, DirectDrag achieves superior image quality compared to existing methods while maintaining competitive drag accuracy. Extensive experiments on DragBench and real-world scenarios demonstrate the effectiveness and practicality of DirectDrag for high-quality, interactive image manipulation. Project Page: https://frakw.github.io/DirectDrag/. Code is available at: https://github.com/frakw/DirectDrag.

</details>


### [108] [Fast & Efficient Normalizing Flows and Applications of Image Generative Models](https://arxiv.org/abs/2512.04039)
*Sandeep Nagar*

Main category: cs.CV

TL;DR: 本文提出了一系列关于生成模型（特别是标准化流）的效率提升方法，并将其应用于多个计算机视觉实际问题，包括农业质量评估、地质制图、自动驾驶数据隐私保护及艺术修复。


<details>
  <summary>Details</summary>
Motivation: 提高生成模型（尤其是标准化流）的计算效率，并解决现实世界中计算机视觉任务面临的挑战，如数据稀缺、类别不平衡、隐私保护和多类型图像退化等。

Method: 提出了六种标准化流架构改进方法，包括可逆3x3卷积、高效Quad-coupling层、k×k卷积并行反演与反向传播算法、Inverse-Flow训练框架以及Affine-StableSR超分辨率模型；并在农业质检、地质映射、自动驾驶数据隐私保护（基于人脸检测与图像修复）、艺术修复等领域应用了条件GAN、堆叠自编码器、Stable Diffusion等生成模型技术。

Result: 实现了更高效的标准化流架构与训练算法，在种子纯度检测、地质特征提取、隐私保护图像生成和艺术图像修复等多个任务中取得良好性能表现。

Conclusion: 本工作在生成模型架构设计与实际视觉应用两方面均取得实质性进展，为提升模型效率与拓展其在真实场景中的适用性提供了新思路与有效工具。

Abstract: This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance.
  The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.

</details>


### [109] [Emergent Outlier View Rejection in Visual Geometry Grounded Transformers](https://arxiv.org/abs/2512.04012)
*Jisang Han,Sunghwan Hong,Jaewoo Jung,Wooseok Jang,Honggyu An,Qianqian Wang,Seungryong Kim,Chen Feng*

Main category: cs.CV

TL;DR: 本文发现现有前馈3D重建模型（如VGGT）虽无显式异常值剔除机制，却在特定网络层中天然具备抑制干扰图像（distractor）的能力；作者利用该层的内在表征实现无需微调或监督的隐式噪声过滤，显著提升野外图像集合下的重建鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 前馈3D重建模型缺乏传统SfM中的几何验证与离群点剔除机制，在野外含噪图像集合上性能下降严重，亟需一种轻量、无需额外训练的鲁棒性增强方法。

Method: 通过合成干扰图像比例变化的受控实验，定位到VGGT中一个具有自然异常抑制行为的特定网络层；进一步分析表明该层编码了判别性强的内部表征，据此直接提取特征进行视图级异常检测与剔除。

Result: 在控制实验和多个野外数据集（如ETH3D、ScanNet）上验证了该隐式过滤机制的一致性和泛化性，显著提升了重建完整性与精度，且不增加模型复杂度或训练开销。

Conclusion: 前馈重建模型中存在未被发掘的隐式噪声鲁棒性，可被简单有效地利用以增强实际场景下的可靠性，为设计更鲁棒的端到端三维视觉系统提供了新思路。

Abstract: Reliable 3D reconstruction from in-the-wild image collections is often hindered by "noisy" images-irrelevant inputs with little or no view overlap with others. While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions. In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images. Through an in-depth analysis under varying proportions of synthetic distractors, we identify a specific layer that naturally exhibits outlier-suppressing behavior. Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision. Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.

</details>


### [110] [Learning Group Actions In Disentangled Latent Image Representations](https://arxiv.org/abs/2512.04015)
*Farhana Hossain Swarnali,Miaomiao Zhang,Tonmoy Hossain*

Main category: cs.CV

TL;DR: 本文提出一种端到端框架，首次在潜空间中自动学习群作用，通过可学习二值掩码动态划分潜变量为变换敏感与不变部分，联合优化解耦表示与群变换映射。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高维数据空间操作群作用难以解耦变换子空间；潜空间方法需人工划分等变/不变子空间，限制了自动学习群作用的能力。

Method: 引入可学习二值掩码（配合直通估计）动态分区潜表示，并构建统一优化框架联合学习潜空间解耦与群变换映射，兼容任意编码器-解码器架构。

Result: 在5个2D/3D图像数据集上验证了自动学习群作用相关解耦潜因子的能力，下游分类任务证实所学表示有效。

Conclusion: 该框架首次实现潜图像流形上群作用的全自动学习，无需人工干预即可发现变换相关结构，提升可控图像变换与表示学习的灵活性与鲁棒性。

Abstract: Modeling group actions on latent representations enables controllable transformations of high-dimensional image data. Prior works applying group-theoretic priors or modeling transformations typically operate in the high-dimensional data space, where group actions apply uniformly across the entire input, making it difficult to disentangle the subspace that varies under transformations. While latent-space methods offer greater flexibility, they still require manual partitioning of latent variables into equivariant and invariant subspaces, limiting the ability to robustly learn and operate group actions within the representation space. To address this, we introduce a novel end-to-end framework that for the first time learns group actions on latent image manifolds, automatically discovering transformation-relevant structures without manual intervention. Our method uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components. We formulate this within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings. The framework can be seamlessly integrated with any standard encoder-decoder architecture. We validate our approach on five 2D/3D image datasets, demonstrating its ability to automatically learn disentangled latent factors for group actions in diverse data, while downstream classification tasks confirm the effectiveness of the learned representations. Our code is publicly available at https://github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations .

</details>


### [111] [Ultra-lightweight Neural Video Representation Compression](https://arxiv.org/abs/2512.04019)
*Ho Man Kwan,Tianhao Peng,Ge Gao,Fan Zhang,Mike Nilsson,Andrew Gower,David Bull*

Main category: cs.CV

TL;DR: 本文提出了NVRC-Lite，一种轻量级的隐式神经表示（INR）视频压缩方法，通过引入多尺度特征网格和基于八叉树的上下文熵编码模型，在保持低计算复杂度的同时显著提升了压缩性能与编解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有INR视频编解码器虽性能优异但计算开销大，且熵编码依赖自回归模型导致速度慢；需兼顾轻量化、高压缩率与高编解码效率。

Method: 1）在轻量级INR中集成多尺度特征网格，利用高分辨率网格提升低复杂度下的重建质量；2）设计基于八叉树的上下文模型替代自回归熵编码，加速高维特征网格的熵编码。

Result: NVRC-Lite在PSNR和MS-SSIM指标上分别比当前最优轻量级INR视频编解码器C3获得21.03%和23.06%的BD-rate节省，并实现8.4倍编码与2.5倍解码加速。

Conclusion: NVRC-Lite有效平衡了压缩性能、模型复杂度与编解码速度，为端侧实时视频压缩提供了实用新方案。

Abstract: Recent works have demonstrated the viability of utilizing over-fitted implicit neural representations (INRs) as alternatives to autoencoder-based models for neural video compression. Among these INR-based video codecs, Neural Video Representation Compression (NVRC) was the first to adopt a fully end-to-end compression framework that compresses INRs, achieving state-of-the-art performance. Moreover, some recently proposed lightweight INRs have shown comparable performance to their baseline codecs with computational complexity lower than 10kMACs/pixel. In this work, we extend NVRC toward lightweight representations, and propose NVRC-Lite, which incorporates two key changes. Firstly, we integrated multi-scale feature grids into our lightweight neural representation, and the use of higher resolution grids significantly improves the performance of INRs at low complexity. Secondly, we address the issue that existing INRs typically leverage autoregressive models for entropy coding: these are effective but impractical due to their slow coding speed. In this work, we propose an octree-based context model for entropy coding high-dimensional feature grids, which accelerates the entropy coding module of the model. Our experimental results demonstrate that NVRC-Lite outperforms C3, one of the best lightweight INR-based video codecs, with up to 21.03% and 23.06% BD-rate savings when measured in PSNR and MS-SSIM, respectively, while achieving 8.4x encoding and 2.5x decoding speedup. The implementation of NVRC-Lite will be made available.

</details>


### [112] [C3G: Learning Compact 3D Representations with 2K Gaussians](https://arxiv.org/abs/2512.04021)
*Honggyu An,Jaewoo Jung,Mungyeom Kim,Sunghwan Hong,Chaehyun Kim,Kazumi Fukuda,Minkyeong Jeon,Jisang Han,Takuya Narihira,Hyuna Ko,Junsu Kim,Yuki Mitsufuji,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出C3G框架，通过学习型token引导生成紧凑的3D高斯分布，减少冗余，提升多视角特征聚合与场景理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于每像素3D高斯点绘的方法生成过多冗余高斯，导致内存开销大、多视角特征聚合效果差，进而影响新视角合成和场景理解性能。

Method: 提出C3G框架：利用可学习token通过自注意力聚合多视角特征以指导关键空间位置上的紧凑高斯生成，并利用学习到的注意力模式进行高斯解码以高效提升特征。

Result: 在无姿态新视角合成、3D开放词汇分割和视角不变特征聚合任务上验证了方法有效性，实现了更高内存效率和特征保真度。

Conclusion: 紧凑但几何意义明确的3D高斯表示足以支持高质量场景重建与理解，优于现有方法。

Abstract: Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.

</details>


### [113] [RELIC: Interactive Video World Model with Long-Horizon Memory](https://arxiv.org/abs/2512.04040)
*Yicong Hong,Yiqun Mei,Chongjian Ge,Yiran Xu,Yang Zhou,Sai Bi,Yannick Hold-Geoffroy,Mike Roberts,Matthew Fisher,Eli Shechtman,Kalyan Sunkavalli,Feng Liu,Zhengqi Li,Hao Tan*

Main category: cs.CV

TL;DR: 本文提出了RELIC框架，旨在实现真正交互式世界模型所需的实时长时序流式处理、一致的空间记忆和精确的用户控制。通过结合压缩的历史潜在标记、相机姿态编码与自回归视频扩散蒸馏技术，RELIC在保持实时性能的同时实现了长时序一致性与空间记忆鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只解决实时长时序流式处理、空间记忆一致性或用户控制中的一个方面，难以三者兼顾；尤其长时记忆机制常损害实时性能。

Method: 基于自回归视频扩散蒸馏技术，使用KV缓存中压缩的历史潜在标记（含相对动作与绝对相机位姿）构建相机感知的记忆结构；提出内存高效自强制范式，将双向教师视频模型蒸馏为因果学生生成器，并支持长时教师上下文与学生自滚动蒸馏。

Result: RELIC作为14B参数模型，在Unreal Engine渲染数据集上训练，实现实时16FPS生成，在动作遵循准确性、长时序流稳定性及空间记忆检索鲁棒性上均优于先前工作。

Conclusion: RELIC统一解决了交互式世界建模的三大核心挑战，为下一代交互式世界模型奠定了坚实基础。

Abstract: A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.

</details>


### [114] [PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design](https://arxiv.org/abs/2512.04082)
*Jiazhe Wei,Ken Li,Tianyu Lao,Haofan Wang,Liang Wang,Caifeng Shan,Chenyang Si*

Main category: cs.CV

TL;DR: 本文提出了PosterCopilot框架，通过三阶段训练策略提升大图文模型在海报设计中的布局推理与可控编辑能力，实现几何准确、美观且支持图层迭代编辑的专业级设计。


<details>
  <summary>Details</summary>
Motivation: 现有基于大图文模型的自动化平面设计方法存在布局几何不准确、缺乏专业工作流所需的逐层迭代编辑能力等问题。

Method: 提出三阶段训练策略：扰动监督微调、面向视觉-现实对齐的强化学习、基于美学反馈的强化学习；并构建LMM与生成模型协同的工作流，支持图层可控的迭代编辑。

Result: PosterCopilot在布局几何准确性与美学质量上显著优于现有方法，并展现出前所未有的专业级迭代设计可控性。

Conclusion: PosterCopilot有效 bridged the gap between automated design and professional graphic design requirements，为AI驱动的高精度、可编辑视觉创作提供了新范式。

Abstract: Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.

</details>


### [115] [SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows](https://arxiv.org/abs/2512.04084)
*Qinyu Zhao,Guangting Zheng,Tao Yang,Rui Zhu,Xingjian Leng,Stephen Gould,Liang Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种简单而有效的改进方法，通过在VAE中固定编码器输出的方差（如设为0.5），同时联合训练VAE与归一化流（NF），从而避免了额外加噪/去噪步骤，并提升了重建与生成质量，在ImageNet 256×256上达到当前最优gFID 1.91。


<details>
  <summary>Details</summary>
Motivation: 解决现有归一化流方法中依赖随机加噪数据增强和使用预训练冻结VAE编码器所带来的复杂流程与次优性能问题。

Method: 固定VAE编码器预测的隐变量方差为常数（如0.5），使编码器输出更广分布的token，解码器直接学习从该增强分布重建干净图像；同时简化ELBO，实现VAE与NF端到端联合训练。

Result: 在ImageNet 256×256生成任务上，SimFlow取得gFID=2.15，优于STARFlow（2.40）；结合REPA-E后进一步降至gFID=1.91，刷新NF领域SOTA。

Conclusion: 固定VAE方差是一种极简却高效的设计，能同时缓解数据增强复杂性和VAE-NF联合训练不稳定性，显著提升生成质量。

Abstract: Normalizing Flows (NFs) learn invertible mappings between the data and a Gaussian distribution. Prior works usually suffer from two limitations. First, they add random noise to training samples or VAE latents as data augmentation, introducing complex pipelines including extra noising and denoising steps. Second, they use a pretrained and frozen VAE encoder, resulting in suboptimal reconstruction and generation quality. In this paper, we find that the two issues can be solved in a very simple way: just fixing the variance (which would otherwise be predicted by the VAE encoder) to a constant (e.g., 0.5). On the one hand, this method allows the encoder to output a broader distribution of tokens and the decoder to learn to reconstruct clean images from the augmented token distribution, avoiding additional noise or denoising design. On the other hand, fixed variance simplifies the VAE evidence lower bound, making it stable to train an NF with a VAE jointly. On the ImageNet $256 \times 256$ generation task, our model SimFlow obtains a gFID score of 2.15, outperforming the state-of-the-art method STARFlow (gFID 2.40). Moreover, SimFlow can be seamlessly integrated with the end-to-end representation alignment (REPA-E) method and achieves an improved gFID of 1.91, setting a new state of the art among NFs.

</details>


### [116] [Unique Lives, Shared World: Learning from Single-Life Videos](https://arxiv.org/abs/2512.04085)
*Tengda Han,Sayna Ebrahimi,Dilara Gokay,Li Yang Ku,Maks Ovsjanikov,Iva Babukova,Daniel Zoran,Viorica Patraucean,Joao Carreira,Andrew Zisserman,Dima Damen*

Main category: cs.CV

TL;DR: 本文提出了“单生命”学习范式，即仅使用单个个体的自我中心视频训练视觉模型，并利用其多视角特性进行自监督学习；实验表明不同个体数据训练的模型具有高度几何对齐性、学到的表征具备良好泛化与迁移能力，且少量单生命数据即可媲美大量网络数据的效果。


<details>
  <summary>Details</summary>
Motivation: 探索个体化、自我中心视频数据在视觉表征学习中的潜力，挑战依赖大规模多样化网络数据的传统范式，挖掘人类日常生活中自然多视角结构所蕴含的学习信号。

Method: 提出单生命学习范式，基于单一个体的自我中心视频构建自监督视觉编码器；设计基于交叉注意力的新指标量化不同模型内部表征的功能对齐程度；在室内/室外不同个体数据上训练并评估几何一致性、下游任务（如深度估计）迁移性能及数据效率。

Result: 1）不同个体数据训练的模型展现出高度一致的几何理解；2）单生命模型学到的表征可有效迁移到未见环境的下游任务；3）仅用同一人一周内30小时视频即可达到与30小时多样化网络数据相当的性能。

Conclusion: 世界固有的共享结构使得单生命数据足以支撑鲁棒、对齐且可迁移的视觉表征学习，为轻量级、个性化、隐私友好的视觉模型训练提供了新路径。

Abstract: We introduce the "single-life" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual. We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner. Our experiments demonstrate three key findings. First, models trained independently on different lives develop a highly aligned geometric understanding. We demonstrate this by training visual encoders on distinct datasets each capturing a different life, both indoors and outdoors, as well as introducing a novel cross-attention-based metric to quantify the functional alignment of the internal representations developed by different models. Second, we show that single-life models learn generalizable geometric representations that effectively transfer to downstream tasks, such as depth estimation, in unseen environments. Third, we demonstrate that training on up to 30 hours from one week of the same person's life leads to comparable performance to training on 30 hours of diverse web data, highlighting the strength of single-life representation learning. Overall, our results establish that the shared structure of the world, both leads to consistency in models trained on individual lives, and provides a powerful signal for visual representation learning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [117] [Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models](https://arxiv.org/abs/2512.03047)
*Samih Fadli*

Main category: cs.CL

TL;DR: 本文提出了一种基于'智能第二定律'的操作化框架，将大语言模型的伦理熵S(t)作为可测量的状态变量，通过行为分类器评估其动态变化，并构建实时监控管道以预警价值观漂移。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全性评估依赖静态基准，但关键失败（如分布偏移下的价值观漂移、越狱攻击、部署中对齐性缓慢退化）具有动态性，亟需动态监测框架。

Method: 基于智能第二定律，定义五维行为分类法，训练分类器从模型输出中估计伦理熵S(t)，并在多种压力测试下测量四个前沿模型基线版与指令微调版的熵动态；进而估算有效对齐做功速率gamma_eff，并将其嵌入实时监控管道。

Result: 基线模型呈现持续熵增长，而指令微调模型可抑制漂移、降低伦理熵约80%；基于熵轨迹成功估算gamma_eff，并实现超过稳定性阈值时自动告警的运行时监督。

Conclusion: 伦理熵可作为衡量和监控大语言模型价值观稳定性的实用动态指标，所提框架为部署中实时对齐监督提供了可行路径。

Abstract: Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.

</details>


### [118] [Watermarks for Embeddings-as-a-Service Large Language Models](https://arxiv.org/abs/2512.03079)
*Anudeex Shetty*

Main category: cs.CL

TL;DR: 本文研究Embeddings-as-a-Service（EaaS）中的水印技术，揭示了现有水印易被文本改写攻击绕过的漏洞，并提出一种基于线性变换的新水印方法WET，具备强鲁棒性和高可验证性。


<details>
  <summary>Details</summary>
Motivation: EaaS服务面临黑盒模仿攻击风险，现有嵌入水印易被输入文本改写绕过，亟需更鲁棒的水印机制保护模型知识产权。

Method: 首先实证分析现有水印在多种改写攻击下的失效现象；进而提出WET方法，通过嵌入空间的线性变换嵌入水印，并利用逆变换与相似度比对实现验证。

Result: 发现当前SOTA水印普遍可被改写攻击移除；WET在改写攻击下保持近100%水印可验证性，并通过消融实验验证各组件有效性。

Conclusion: 文本改写是EaaS水印的关键威胁，WET通过结构化嵌入变换显著提升水印鲁棒性，为商用大模型嵌入服务提供可行的知识产权保护方案。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.
  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.
  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.

</details>


### [119] [Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation](https://arxiv.org/abs/2512.03082)
*Nan Zhuang,Wenshuo Wang,Lekai Qian,Yuxiao Wang,Boyu Cao,Qi Liu*

Main category: cs.CL

TL;DR: 本文提出Reasoning Dependency Generation (RDG)框架，首次系统性解决大语言模型中的选择支持性偏差（CSB），通过生成平衡的推理问答数据进行微调，显著提升模型在记忆与评估任务中的客观性，同时保持基准性能。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法主要针对人口统计和社会偏见，而对大语言模型中的认知偏差（如选择支持性偏差CSB）缺乏有效应对，影响AI辅助决策的客观性。

Method: 提出Reasoning Dependency Generation (RDG)框架，自动构建平衡的推理问答对，显式建模或解耦选项、证据与理由之间的依赖关系，生成包含上下文依赖数据和依赖解耦数据的大规模跨领域数据集，并用于模型微调。

Result: 在记忆型实验中提升81.5%，评估型实验中提升94.3%，且在标准BBQ基准上性能基本不变。

Conclusion: RDG是首个针对LLM中认知偏差（特别是CSB）的系统性解决方案，为构建更可靠的AI辅助决策系统提供了新路径。

Abstract: Recent studies have demonstrated that some Large Language Models exhibit choice-supportive bias (CSB) when performing evaluations, systematically favoring their chosen options and potentially compromising the objectivity of AI-assisted decision making. While existing debiasing approaches primarily target demographic and social biases, methods for addressing cognitive biases in LLMs remain largely unexplored. In this work, we present the first solution to address CSB through Reasoning Dependency Generation (RDG), a novel framework for generating unbiased reasoning data to mitigate choice-supportive bias through fine-tuning. RDG automatically constructs balanced reasoning QA pairs, explicitly (un)modeling the dependencies between choices, evidences, and justifications. Our approach is able to generate a large-scale dataset of QA pairs across domains, incorporating Contextual Dependency Data and Dependency Decouple Data. Experiments show that LLMs fine-tuned on RDG-generated data demonstrate a 81.5% improvement in memory-based experiments and 94.3% improvement in the evaluation-based experiment, while maintaining similar performance on standard BBQ benchmarks. This work pioneers an approach for addressing cognitive biases in LLMs and contributes to the development of more reliable AI-assisted decision support systems.

</details>


### [120] [Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies](https://arxiv.org/abs/2512.03195)
*Stylianos Saroglou,Konstantinos Diamantaras,Francesco Preta,Marina Delianidi,Apostolos Benisis,Christian Johannes Meyer*

Main category: cs.CL

TL;DR: 本文研究了语言模型在提升劳动力市场信息分类方面的潜力，特别是将职位空缺文本链接到ESCO和EQF两大欧洲框架，并比较了句子链接与实体链接两种方法，发布了开源工具及两个新标注数据集，推动了工作实体提取和数字劳动市场分析的发展。


<details>
  <summary>Details</summary>
Motivation: 提升劳动力市场信息（如职位空缺）的自动分类能力，使其能精准链接至ESCO和EQF等标准化欧洲框架，以支持技能匹配、教育认证与就业政策制定。

Method: 比较Sentence Linking与Entity Linking两种主流方法；开发开源工具；构建两个面向职业与资格表示评估的标注数据集；探索生成式大语言模型在该任务中的不同应用方式。

Result: 提出了改进的职业与资格实体提取方法；发布了可复用的开源工具与高质量标注数据集；验证了大语言模型在深层语义链接任务中的有效性。

Conclusion: 本工作不仅推进了职位实体提取的技术前沿，还为数字时代下劳动、技能与就业话语的计算分析提供了基础设施与方法论支撑。

Abstract: This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier

</details>


### [121] [InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation](https://arxiv.org/abs/2512.03197)
*Faezeh Faez,Marzieh S. Tahaei,Yaochen Hu,Ali Pourranjbar,Mahdi Biparva,Mark Coates,Yingxue Zhang*

Main category: cs.CL

TL;DR: 本文提出InvertiTune框架，通过可控数据生成与监督微调（SFT）结合，提升文本到知识图谱（Text2KG）的效率与性能；其核心是利用大知识库生成高质量、长文本-大图对训练轻量模型，实现单次推理构建知识图谱，并在新基准CE12k和CrossEval-1200上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的Text2KG方法多依赖迭代提示，计算开销大且难以捕获文本中分布复杂的语义关系。

Method: 提出InvertiTune框架：首先从大型知识库系统抽取子图，经噪声过滤后，用LLM生成对应自然语言描述（逆向生成），构建高质量长文本-大KG配对数据集；再以此数据集对轻量模型进行监督微调（SFT），实现单次前向推理完成KG构建。

Result: 在自建基准CE12k上，InvertiTune优于未微调的大LLM及当前最优Text2KG方法；在跨数据集评测集CrossEval-1200上亦表现出更强泛化能力。

Conclusion: 高质量、贴近真实场景的训练数据对构建高效高质Text2KG系统至关重要；InvertiTune验证了‘逆向生成+轻量SFT’范式相较传统LLM提示更优。

Abstract: Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.

</details>


### [122] [Identifying attributions of causality in political text](https://arxiv.org/abs/2512.03214)
*Paulina Garcia-Corral*

Main category: cs.CL

TL;DR: 本文提出了一种用于检测和解析政治文本中解释的框架，通过训练一个轻量级因果语言模型，提取因果关系对（原因-结果），以支持大规模、系统化的政治解释分析。


<details>
  <summary>Details</summary>
Motivation: 解释是人们理解政治世界的基础，但政治学中对解释的系统性分析仍不充分，现有方法零散且多为议题特定。

Method: 训练一个轻量级因果语言模型，自动识别并结构化政治文本中的因果主张（即原因-结果对）。

Result: 该方法具有较低的人工标注需求、良好的泛化能力及与人工编码相比具有可接受的准确性。

Conclusion: 所提出的框架为政治文本中因果解释的大规模、可扩展、系统性研究提供了可行且稳健的计算工具。

Abstract: Explanations are a fundamental element of how people make sense of the political world. Citizens routinely ask and answer questions about why events happen, who is responsible, and what could or should be done differently. Yet despite their importance, explanations remain an underdeveloped object of systematic analysis in political science, and existing approaches are fragmented and often issue-specific. I introduce a framework for detecting and parsing explanations in political text. To do this, I train a lightweight causal language model that returns a structured data set of causal claims in the form of cause-effect pairs for downstream analysis. I demonstrate how causal explanations can be studied at scale, and show the method's modest annotation requirements, generalizability, and accuracy relative to human coding.

</details>


### [123] [Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs](https://arxiv.org/abs/2512.03310)
*Kunj Joshi,David A. Smith*

Main category: cs.CL

TL;DR: 本文提出了一种名为随机掩码微调（RMFT）的新方法，旨在减少大语言模型在微调过程中对个人身份信息（PII）的记忆，同时尽量保持模型性能。实验表明，RMFT在Enron邮件数据集上显著降低了PII提取率，并在隐私-效用权衡评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，自然语言模型尤其是大语言模型（LLMs）容易记忆训练数据中的个人身份信息（PIIs），带来严重的安全与隐私风险。

Method: 提出随机掩码微调（RMFT）技术，在微调过程中引入随机掩码机制以降低PII记忆；并构建MaxTER评估框架，采用AURC指标量化隐私-效用权衡。

Result: 在Enron邮件数据集上，RMFT相比基线微调将总提取率（TER）和已见提取率（SER）分别降低80.81%和80.17%，优于去重方法，且仅使困惑度上升5.73%。

Conclusion: RMFT是一种有效的隐私保护微调方法，在显著抑制PII记忆的同时保持模型实用性，MaxTER框架为评估隐私-效用权衡提供了新范式。

Abstract: The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.

</details>


### [124] [Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaraní](https://arxiv.org/abs/2512.03334)
*Nemika Tyagi,Nelvin Licona Guevara,Olga Kellert*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的标注流程，用于分析西班牙语-英语和西班牙语-瓜拉尼语双语语料中的社会语言学特征与话题分布，自动标注了3691个语码转换句子的主题、体裁和语用功能，并结合人口统计元数据揭示了性别、语言主导性与话语功能之间的系统性关联，以及巴拉圭语料中正式瓜拉尼语与非正式西班牙语的双语分层现象。


<details>
  <summary>Details</summary>
Motivation: 传统社会语言学分析依赖耗时的手动标注，尤其在低资源双语场景中面临数据稀缺和标注成本高的挑战；本研究旨在探索大语言模型能否可靠地支持跨语言、低资源双语的社会语言学量化研究。

Method: 构建LLM辅助标注流水线，对Miami双语语料库（西-英）和西班牙语-瓜拉尼语语料进行自动话题、体裁及话语-语用功能标注，并整合人口统计元数据；对瓜拉尼语部分新增话题标注以增强数据质量。

Result: 发现迈阿密语料中性别、语言主导性与话语功能存在系统性关联；巴拉圭语料呈现明显的语域分工：正式场合用瓜拉尼语、非正式场合用西班牙语；结果以大规模语料证据复现并拓展了既有互动与社会语言学观察。

Conclusion: 大语言模型可稳定提取可解释的社会语言学模式，为跨语言及低资源双语研究提供了高效、可扩展的计算方法新路径。

Abstract: This study presents an LLM-assisted annotation pipeline for the sociolinguistic and topical analysis of bilingual discourse in two typologically distinct contexts: Spanish-English and Spanish-Guaraní. Using large language models, we automatically labeled topic, genre, and discourse-pragmatic functions across a total of 3,691 code-switched sentences, integrated demographic metadata from the Miami Bilingual Corpus, and enriched the Spanish-Guaraní dataset with new topic annotations. The resulting distributions reveal systematic links between gender, language dominance, and discourse function in the Miami data, and a clear diglossic division between formal Guaraní and informal Spanish in Paraguayan texts. These findings replicate and extend earlier interactional and sociolinguistic observations with corpus-scale quantitative evidence. The study demonstrates that large language models can reliably recover interpretable sociolinguistic patterns traditionally accessible only through manual annotation, advancing computational methods for cross-linguistic and low-resource bilingual research.

</details>


### [125] [AR-Med: Automated Relevance Enhancement in Medical Search via LLM-Driven Information Augmentation](https://arxiv.org/abs/2512.03737)
*Chuyue Wang,Jie Feng,Yuxi Wu,Hang Zhang,Zhiguo Fan,Bing Cheng,Wei Lin*

Main category: cs.CL

TL;DR: 本文提出AR-Med框架，通过检索增强与知识蒸馏提升大模型在医疗搜索中的准确性、可靠性与效率，并在真实医疗平台大规模部署验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统医疗搜索方法难以理解复杂用户查询；大语言模型虽具语义理解潜力，但在高风险医疗场景中面临事实幻觉、专业知识缺失和高计算成本等挑战。

Method: 提出AR-Med框架：1）基于检索增强的LLM推理，确保医学知识准确性；2）设计知识蒸馏方案，将大教师模型压缩为高效学生模型；3）构建多专家标注基准LocalQSMed，对齐离线评估与线上效果。

Result: AR-Med离线准确率达93%以上，较原线上系统绝对提升24%，显著提升线上相关性与用户满意度。

Conclusion: AR-Med为现实医疗场景中构建可信、可扩展的LLM驱动系统提供了实用蓝图。

Abstract: Accurate and reliable search on online healthcare platforms is critical for user safety and service efficacy. Traditional methods, however, often fail to comprehend complex and nuanced user queries, limiting their effectiveness. Large language models (LLMs) present a promising solution, offering powerful semantic understanding to bridge this gap. Despite their potential, deploying LLMs in this high-stakes domain is fraught with challenges, including factual hallucinations, specialized knowledge gaps, and high operational costs. To overcome these barriers, we introduce \textbf{AR-Med}, a novel framework for \textbf{A}utomated \textbf{R}elevance assessment for \textbf{Med}ical search that has been successfully deployed at scale on the Online Medical Delivery Platforms. AR-Med grounds LLM reasoning in verified medical knowledge through a retrieval-augmented approach, ensuring high accuracy and reliability. To enable efficient online service, we design a practical knowledge distillation scheme that compresses large teacher models into compact yet powerful student models. We also introduce LocalQSMed, a multi-expert annotated benchmark developed to guide model iteration and ensure strong alignment between offline and online performance. Extensive experiments show AR-Med achieves an offline accuracy of over 93\%, a 24\% absolute improvement over the original online system, and delivers significant gains in online relevance and user satisfaction. Our work presents a practical and scalable blueprint for developing trustworthy, LLM-powered systems in real-world healthcare applications.

</details>


### [126] [PERCS: Persona-Guided Controllable Biomedical Summarization Dataset](https://arxiv.org/abs/2512.03340)
*Rohan Charudatt Salvi,Chirag Chawla,Dhruv Jain,Swapnil Panigrahi,Md Shad Akhtar,Shweta Yadav*

Main category: cs.CL

TL;DR: 本文提出了PERCS数据集，用于面向不同用户群体（如普通大众、医学生、非医学研究人员和医学专家）的可控生物医学摘要生成，并基于该数据集对多个大语言模型进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有医学文本简化方法通常假设单一通用受众，忽视了用户在医学素养和信息需求上的巨大差异，因此需要面向特定人群的可控摘要生成方法。

Method: 构建了PERCS数据集，包含四种 persona（普通大众、医学生、非医学研究人员、医学专家）对应的生物医学摘要及其人工撰写并经医生审核的个性化摘要；采用可读性、词汇复杂度和内容深度等指标进行技术验证，并用自动评估指标（全面性、可读性、忠实性）对四个大语言模型进行基准测试。

Result: PERCS数据集展示了不同persona摘要在可读性、词汇和内容深度上的显著差异；四个大语言模型在PERCS上的基准性能已建立，为后续研究提供参考。

Conclusion: PERCS是首个面向多persona的可控生物医学摘要数据集，支持更精准、个性化的健康信息传播，推动面向用户需求的可控文本生成研究。

Abstract: Automatic medical text simplification plays a key role in improving health literacy by making complex biomedical research accessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide variation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS (Persona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent different levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization. Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along with describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that assess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset, annotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication and controllable biomedical summarization.

</details>


### [127] [Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning](https://arxiv.org/abs/2512.03343)
*Darshan Fofadiya*

Main category: cs.CL

TL;DR: 本文提出Idea-Gated Transformer，通过引入‘Idea Head’预测未来语义分布并动态门控词汇生成，缓解自回归语言模型中的主题漂移问题，在保持困惑度的同时显著提升领域保持能力。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型在下一词预测目标下易出现主题漂移，因其依赖局部关联而非全局语义规划；扩大模型规模仅部分缓解，根本问题在于NTP目标的短视性。

Method: 设计Idea-Gated Transformer：新增Idea Head预测未来窗口的词袋分布，生成Concept Vector；采用可微门控机制实时抑制语义无关词，分离语义规划与句法生成。

Result: 在WikiText-103上验证：验证困惑度与GPT-2基线相当，但Domain Retention显著更优；定性与定量分析表明其能锁定特定语义簇（如金融、科学）并抵抗联想漂移。

Conclusion: Idea-Gated架构提供了一种参数高效、可控性强的语言建模新路径，有效缓解主题漂移，增强生成一致性。

Abstract: Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \citep{holtzman2019curious}. While scaling model size mitigates this \citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.

</details>


### [128] [From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation](https://arxiv.org/abs/2512.03360)
*Qingchuan Li,Mingyue Cheng,Zirui Liu,Daoyu Wang,Yuting Zeng,Tongxuan Liu*

Main category: cs.CL

TL;DR: 本文提出了一种假设驱动的反向逻辑推理框架（HBLR），结合置信度感知的符号化翻译与反向推理，提升大模型逻辑推理的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型多依赖前向推理，易产生冗余路径、幻觉步骤和语义漂移，导致推理低效且不可靠。

Method: HBLR包含两个核心阶段：1）置信度感知的符号翻译（仅高置信度文本转为一阶逻辑，辅以翻译反思模块保障语义保真）；2）假设结论为真、递归验证前提的反向推理（辅以推理反思模块修正错误步骤）。

Result: 在五个逻辑推理基准上，HBLR在准确率和推理效率上均显著优于强基线模型。

Conclusion: HBLR通过模拟人类演绎思维并引入双重反思机制，有效提升了逻辑推理的可靠性与效率，为LLM的可解释、可控推理提供了新范式。

Abstract: Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.

</details>


### [129] [Nexus: Higher-Order Attention Mechanisms in Transformers](https://arxiv.org/abs/2512.03377)
*Hanting Chen,Chu Zhong,Kai Han,Yuchuan Tian,Yuchen Liang,Tianyu Guo,Xinghao Chen,Dacheng Tao,Yunhe Wang*

Main category: cs.CL

TL;DR: 本文提出了一种高阶注意力网络（Hon），通过递归式自注意力机制动态优化Query和Key表示，突破标准注意力的低秩瓶颈，提升建模多跳关系的能力，且仅引入常数级额外参数。


<details>
  <summary>Details</summary>
Motivation: 标准的一阶注意力机制存在低秩瓶颈，难以在单层内捕捉复杂的多跳关系。

Method: 提出高阶注意力网络（Hon），采用嵌套自注意力机制动态生成Query和Key，并通过权重共享策略保证参数效率。

Result: 理论证明Hon能打破标准注意力的线性瓶颈；实验表明Hon在多个基准上优于标准Transformer。

Conclusion: Hon通过递归式高阶注意力显著增强了Transformer的表达能力，同时保持了参数高效性。

Abstract: Transformers have achieved significant success across various domains, relying on self-attention to capture dependencies. However, the standard first-order attention mechanism is often limited by a low-rank bottleneck, struggling to capture intricate, multi-hop relationships within a single layer. In this paper, we propose the \textbf{Higher-Order Attention Network (Hon)}, a novel architecture designed to enhance representational power through a recursive framework. Unlike standard approaches that use static linear projections for Queries and Keys, Hon dynamically refines these representations via nested self-attention mechanisms. Specifically, the Query and Key vectors are themselves outputs of inner attention loops, allowing tokens to aggregate global context and model high-order correlations \textit{prior} to the final attention computation. We enforce a parameter-efficient weight-sharing strategy across recursive steps, ensuring that this enhanced expressivity incurs $\mathcal{O}(1)$ additional parameters. We provide theoretical analysis demonstrating that our method breaks the linear bottleneck of standard attention. Empirically, Hon outperforms standard Transformers on multiple benchmarks.

</details>


### [130] [Characterizing Language Use in a Collaborative Situated Game](https://arxiv.org/abs/2512.03381)
*Nicholas Tomlin,Naitian Zhou,Eve Fleisig,Liangyuan,Chen,Téa Wright,Lauren Vinh,Laura X. Ma,Seun Eisape,Ellie French,Tingting Du,Tianjiao Zhang,Alexander Koller,Alane Suhr*

Main category: cs.CL

TL;DR: 本文介绍了Portal Dialogue Corpus，一个包含11.5小时多人合作解谜游戏《Portal 2》中玩家语音对话的数据集，共24.5K条语句，涵盖空间指代、澄清修复及临时约定形成等独特语言现象，并公开发布含视频、音频、转录文本、游戏状态及多类语言标注的完整资源。


<details>
  <summary>Details</summary>
Motivation: 现有闲聊或任务导向型对话语料库难以覆盖复杂协作场景下的语言现象，因此需要构建能反映真实协同问题解决中语言使用的新型语料库。

Method: 收集并整理《Portal 2》合作模式下玩家的自然对话数据，进行人工与自动标注，涵盖语言、行为与游戏状态信息，并系统分析其中的语言特征。

Result: 构建了Portal Dialogue Corpus，识别出复杂空间指代、澄清与修复、临时约定形成等在传统语料中罕见的语言现象，并提供了多模态、多层级标注的公开资源。

Conclusion: 该语料库为研究具身化、情境化、协作式问题解决中的语言使用提供了重要基础，推动人机协作与对话系统的发展。

Abstract: Cooperative video games, where multiple participants must coordinate by communicating and reasoning under uncertainty in complex environments, yield a rich source of language data. We collect the Portal Dialogue Corpus: a corpus of 11.5 hours of spoken human dialogue in the co-op mode of the popular Portal 2 virtual puzzle game, comprising 24.5K total utterances. We analyze player language and behavior, identifying a number of linguistic phenomena that rarely appear in most existing chitchat or task-oriented dialogue corpora, including complex spatial reference, clarification and repair, and ad-hoc convention formation. To support future analyses of language use in complex, situated, collaborative problem-solving scenarios, we publicly release the corpus, which comprises player videos, audio, transcripts, game state data, and both manual and automatic annotations of language data.

</details>


### [131] [Dual LoRA: Enhancing LoRA with Magnitude and Direction Updates](https://arxiv.org/abs/2512.03402)
*Yixing Xu,Chao Li,Xuanwu Yin,Spandan Tiwari,Dong Li,Ashish Sirasao,Emad Barsoum*

Main category: cs.CL

TL;DR: 本文提出Dual LoRA，通过将LoRA的低秩矩阵分解为控制更新幅度的'magnitude组'和决定更新方向的'direction组'，并分别引入ReLU和sign函数，增强其对全量微调中梯度更新过程的建模能力，在多个NLP任务和主流LLM上显著优于LoRA及其变体。


<details>
  <summary>Details</summary>
Motivation: LoRA因低秩假设限制，常导致下游任务性能不佳；需引入更符合真实梯度更新机制的归纳偏置以提升表现。

Method: 提出Dual LoRA：将LoRA中的低秩增量矩阵分解为magnitude组（施加ReLU）和direction组（施加sign函数），分别建模参数更新的大小与符号方向，从而更贴近全量微调的梯度更新行为。

Result: 在GPT-2、RoBERTa、DeBERTa及LLaMA系列模型上，于NLG、NLU和常识推理等多类任务中，Dual LoRA在相同可训练参数量下持续超越LoRA及其SOTA变体。

Conclusion: Dual LoRA通过解耦并显式建模参数更新的幅度与方向，有效缓解了原始LoRA的低秩瓶颈，是一种简单而高效的PEFT改进方案。

Abstract: Low-rank adaptation (LoRA) is one of the most popular methods among parameter-efficient fine-tuning (PEFT) methods to adapt pre-trained large language models (LLMs) to specific downstream tasks. However, the model trained based on LoRA often has an unsatisfactory performance due to its low-rank assumption. In this paper, we propose a novel method called Dual LoRA to improve the performance by incorporating an inductive bias into the original LoRA. Specifically, we separate low-rank matrices into two groups: the magnitude group to control whether or not and how far we should update a parameter and the direction group to decide whether this parameter should move forward or backward, to better simulate the parameter updating process of the full fine-tuning based on gradient-based optimization algorithms. We show that this can be simply achieved by adding a ReLU function to the magnitude group and a sign function to the direction group. We conduct several experiments over a wide range of NLP tasks, including natural language generation (NLG), understanding (NLU), and commonsense reasoning datasets on GPT-2, RoBERTa, DeBERTa, and LLaMA-1/2/3 as baseline models. The results show that we consistently outperform LoRA and its state-of-the-art variants with the same number of trainable parameters.

</details>


### [132] [PretrainZero: Reinforcement Active Pretraining](https://arxiv.org/abs/2512.03442)
*Xingrun Xing,Zhiyuan Fan,Jie Lou,Guoqi Li,Jiajun Zhang,Debing Zhang*

Main category: cs.CL

TL;DR: 本文提出PretrainZero框架，将强化学习（RL）从传统领域特定的后训练扩展到通用预训练阶段，通过主动学习和自监督方式，在无标注、无预训练奖励模型的情况下，直接在Wikipedia语料上对3B–30B规模的基础模型进行RL预训练，显著提升通用推理能力，并在MMLU-Pro、SuperGPQA和数学基准上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的大推理模型虽在特定领域（如软件、数学）表现出专家级能力，但严重依赖可验证的领域内奖励信号，难以拓展至通用推理，形成性能瓶颈。

Method: 提出PretrainZero：一种基于预训练语料的强化主动学习框架，包含三大特性——1）主动预训练：学习统一推理策略，主动识别并推理预训练语料中有信息量的内容；2）自监督学习：不依赖任何标签、预训练奖励模型或监督微调，直接在Wikipedia上对3B–30B基础模型进行RL预训练；3）验证尺度扩展：通过逐步增加被掩码片段难度，提升模型通用推理能力。

Result: 在强化预训练阶段，Qwen3-4B-Base模型在MMLU-Pro、SuperGPQA和数学平均基准上分别提升8.43、5.96和10.60分；预训练后的模型还可作为下游RLVR任务的推理基础模型。

Conclusion: PretrainZero成功将强化学习引入通用预训练阶段，突破了传统RL依赖可验证奖励的限制，为构建具备通用推理能力的人工通用智能提供了新范式。

Abstract: Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.

</details>


### [133] [A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention](https://arxiv.org/abs/2512.03494)
*Di Xiu,Hongyin Tang,Bolin Rong,Lizhi Yan,Jingang Wang,Yifan Lu,Xunliang Cai*

Main category: cs.CL

TL;DR: 本文研究了Top-k注意力机制在大语言模型长上下文建模中的有效性与理论机制，验证了其在解码和训练阶段的性能提升，并从信息熵角度提供理论解释。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文建模中推理计算成本过高，成为代理和多模态等应用发展的瓶颈。

Method: 通过实验验证精确Top-k解码的有效性；探索训练与推理一致的Top-k注意力训练策略；分析近似Top-k算法精度对下游任务的影响；并从信息熵角度进行理论解释。

Result: Top-k解码在HELMET和LongBench v2等任务上达到甚至超越全注意力性能；训练-推理一致的Top-k策略显著提升模型表现；下游任务性能与近似精度正相关；Top-k监督微调导致下游任务熵降低。

Conclusion: Top-k注意力机制可有效降低计算开销而不损性能，训练与推理一致性及低熵状态是其成功的关键因素。

Abstract: Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.

</details>


### [134] [Understanding LLM Reasoning for Abstractive Summarization](https://arxiv.org/abs/2512.03503)
*Haohan Yuan,Siu Cheung Hui,Haopeng Zhang*

Main category: cs.CL

TL;DR: 本文系统评估了大型语言模型（LLMs）在抽象摘要任务中推理能力的有效性，发现推理并非万能解法，不同策略在摘要质量与事实忠实性间存在权衡：显式推理提升流畅性但损害事实性，而大型推理模型（LRMs）的隐式推理则相反；增加内部推理预算反而可能降低事实一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学和代码等分析任务中推理能力强，但其在抽象摘要中的效用仍被广泛假设而缺乏实证验证，本文旨在填补这一空白。

Method: 将通用推理策略适配到摘要任务，系统比较8种推理策略与3种大型推理模型（LRMs）在8个多样化数据集上的表现，评估摘要质量与事实忠实性。

Result: 推理效果高度依赖具体策略与上下文；显式推理提升流畅性但降低事实性，隐式推理（LRMs）则相反；增加LRM内部推理预算不提升、甚至损害事实一致性。

Conclusion: 抽象摘要的关键在于忠实压缩而非过度推理，推理不是普适方案，需根据任务需求谨慎选择策略。

Abstract: While the reasoning capabilities of Large Language Models (LLMs) excel in analytical tasks such as mathematics and code generation, their utility for abstractive summarization remains widely assumed but largely unverified. To bridge this gap, we first tailor general reasoning strategies to the summarization domain. We then conduct a systematic, large scale comparative study of 8 reasoning strategies and 3 Large Reasoning Models (LRMs) across 8 diverse datasets, assessing both summary quality and faithfulness. Our findings show that reasoning is not a universal solution and its effectiveness is highly dependent on the specific strategy and context. Specifically, we observe a trade-off between summary quality and factual faithfulness: explicit reasoning strategies tend to improve fluency at the expense of factual grounding, while implicit reasoning in LRMs exhibits the inverse pattern. Furthermore, increasing an LRM's internal reasoning budget does not improve, and can even hurt, factual consistency, suggesting that effective summarization demands faithful compression rather than creative over-thinking.

</details>


### [135] [Fine-grained Narrative Classification in Biased News Articles](https://arxiv.org/abs/2512.03582)
*Zeba Afroz,Harsh Vardhan,Pawan Bhakuni,Aanchal Punia,Rajdeep Kumar,Md. Shad Akhtar*

Main category: cs.CL

TL;DR: 本文提出了一种针对印度新闻媒体中宣传叙事的细粒度分类方法，构建了首个意识形态驱动的多层级标注数据集INDI-PROP，并设计了两个基于GPT-4o-mini的多跳提示推理框架FANTA与TPTC，在偏见、叙事和说服技术分类任务上均显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对新闻宣传中细粒度叙事结构及其意识形态根基的系统建模，尤其在多元政治语境（如印度）中，亟需兼顾文章偏见、事件特定叙事框架与说服技巧的多层级分析工具。

Method: 构建多层级标注数据集INDI-PROP（含1266篇关于CAA法案与农民抗议的新闻），涵盖意识形态文章偏见、事件特异性叙事框架、说服技术三类标注；提出FANTA（融合信息抽取与上下文建模的多层推理）和TPTC（两阶段说服线索分解）两种GPT-4o-mini引导的多跳提示框架。

Result: FANTA与TPTC在文章偏见、叙事分类及说服技术识别三项任务上均显著超越基线模型，验证了多层级标注与多跳提示推理的有效性。

Conclusion: 细粒度、意识形态锚定的叙事建模是理解新闻宣传机制的关键；多层级标注数据集与大模型引导的分步推理框架为跨文化宣传分析提供了可扩展的方法论基础。

Abstract: Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.

</details>


### [136] [AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment](https://arxiv.org/abs/2512.03634)
*Ahmad Aghaebrahimian*

Main category: cs.CL

TL;DR: 本文提出了一种可解释的、基于原子事实分解的事实一致性评估框架，支持领域内与开放域文本，采用灵活无模式方法和加权指标，并可控制复杂度，已在通用与临床数据集上验证。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型易产生看似合理但错误的‘幻觉’内容，尤其在临床等高风险领域危害严重；而当前评估指标难以准确衡量事实一致性且缺乏可解释性，阻碍错误诊断与修正。

Method: 将文本分解为原子事实，设计无模式（schema-free）、可解释的评估框架；引入加权指标替代绝对指标以提升评估精度；并设计机制控制复杂领域下的评估复杂度。

Result: 在主流通用与临床数据集上完成基准测试，验证了框架的有效性与适用性；同时开源代码以支持后续事实感知模型训练研究。

Conclusion: 所提框架显著提升了事实一致性评估的可解释性与灵活性，为缓解大模型幻觉问题、特别是在高风险领域中提供了实用工具与新思路。

Abstract: Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.

</details>


### [137] [Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context](https://arxiv.org/abs/2512.03671)
*Beatrice Savoldi,Giuseppe Attanasio,Olga Gorodetskaya,Marta Marchiori Manerba,Elisa Bassignana,Silvia Casola,Matteo Negri,Tommaso Caselli,Luisa Bentivogli,Alan Ramponi,Arianna Muti,Nicoletta Balbo,Debora Nozza*

Main category: cs.CL

TL;DR: This paper presents the first comprehensive empirical study on GenAI adoption, usage patterns, and literacy in Italy, revealing widespread use for sensitive tasks despite low digital literacy and a significant gender divide in adoption, especially among older generations.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI chatbots risks widening digital divides due to uneven adoption and low awareness of their limitations; thus, there is a need to empirically map adoption, usage, and literacy to inform equitable policy and education.

Method: The study uses newly collected survey data from 1,906 Italian-speaking adults to analyze GenAI adoption, usage patterns, and digital literacy, with statistical analysis to identify demographic disparities and predictors of adoption.

Result: Widespread GenAI adoption for work and personal use—including sensitive domains—is observed; GenAI is replacing other information sources despite low user literacy; a pronounced gender gap exists, with women half as likely to adopt and use GenAI, especially among older generations; literacy predicts adoption only partially.

Conclusion: GenAI is rapidly becoming a primary information source in Italy, but its adoption is inequitable and poorly understood—highlighting an urgent need for targeted digital literacy initiatives and deeper investigation into non-literacy barriers (e.g., social, cultural, or design-related) to ensure inclusive participation.

Abstract: The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.

</details>


### [138] [Evaluating Hydro-Science and Engineering Knowledge of Large Language Models](https://arxiv.org/abs/2512.03672)
*Shiruo Hu,Wenbo Shan,Yingjia Li,Zhiqi Wan,Xinpeng Yu,Yunjia Qi,Haotian Xia,Yang Xiao,Dingxiao Liu,Jiaru Wang,Chenxu Gong,Ruixi Zhang,Shuyue Wu,Shibo Cui,Chee Hui Lai,Wei Luo,Yubin He,Bin Xu,Jianshi Zhao*

Main category: cs.CL

TL;DR: 本文提出了一个名为Hydro-SE Bench的评估基准，包含4000道多选题，用于系统评估大语言模型（LLMs）在水科学与工程（Hydro-SE）领域的知识掌握与应用能力；结果显示现有LLMs在基础科学相关子领域表现较好，但在行业标准、水工结构等专业领域仍显不足，模型参数规模提升主要增强推理与计算能力，而实际工程应用能力仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在水科学与工程（Hydro-SE）领域的知识覆盖与应用能力缺乏系统性评估，制约其在该关键跨学科工程领域的落地应用。

Method: 构建了涵盖9个子领域的Hydro-SE Bench评估基准（含4000道多选题），从基本概念知识、工程应用能力、推理与计算能力三方面对LLMs进行系统评测，并对比分析不同规模模型（商用大模型 vs 小参数模型）的表现差异。

Result: 商用LLMs准确率介于0.74–0.80，小参数LLMs为0.41–0.68；LLMs在自然与物理科学相关子领域表现良好，但在行业标准、水利结构等专业领域明显薄弱；模型缩放主要提升推理与计算能力，工程应用能力提升有限。

Conclusion: Hydro-SE Bench揭示了LLMs在水科学与工程任务中的优势与短板，为模型开发者指明训练优化方向，也为Hydro-SE研究者提供了LLM应用的实践参考。

Abstract: Hydro-Science and Engineering (Hydro-SE) is a critical and irreplaceable domain that secures human water supply, generates clean hydropower energy, and mitigates flood and drought disasters. Featuring multiple engineering objectives, Hydro-SE is an inherently interdisciplinary domain that integrates scientific knowledge with engineering expertise. This integration necessitates extensive expert collaboration in decision-making, which poses difficulties for intelligence. With the rapid advancement of large language models (LLMs), their potential application in the Hydro-SE domain is being increasingly explored. However, the knowledge and application abilities of LLMs in Hydro-SE have not been sufficiently evaluated. To address this issue, we propose the Hydro-SE LLM evaluation benchmark (Hydro-SE Bench), which contains 4,000 multiple-choice questions. Hydro-SE Bench covers nine subfields and enables evaluation of LLMs in aspects of basic conceptual knowledge, engineering application ability, and reasoning and calculation ability. The evaluation results on Hydro-SE Bench show that the accuracy values vary among 0.74 to 0.80 for commercial LLMs, and among 0.41 to 0.68 for small-parameter LLMs. While LLMs perform well in subfields closely related to natural and physical sciences, they struggle with domain-specific knowledge such as industry standards and hydraulic structures. Model scaling mainly improves reasoning and calculation abilities, but there is still great potential for LLMs to better handle problems in practical engineering application. This study highlights the strengths and weaknesses of LLMs for Hydro-SE tasks, providing model developers with clear training targets and Hydro-SE researchers with practical guidance for applying LLMs.

</details>


### [139] [Different types of syntactic agreement recruit the same units within large language models](https://arxiv.org/abs/2512.03676)
*Daria Kryvosheieva,Andrea de Varda,Evelina Fedorenko,Greta Tuckute*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）中语法知识的表征方式，发现不同类型的句法一致现象（如主谓、代词回指、限定词-名词一致）会激活重叠的神经元单元，表明‘一致’是LLM中一个有意义的功能性语法范畴，并在多语种中具有跨语言一致性。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型中语法知识如何被表征，特别是不同句法现象是否共享或依赖不同的模型组件。

Method: 采用受认知神经科学启发的功能定位方法，在7个开源大语言模型中识别对67种英语句法现象最敏感的神经元单元，并进行因果干预和跨语言（英语、俄语、中文及57种语言）分析。

Result: 不同句法一致现象显著激活重叠的神经元单元；该模式在英语、俄语、中文中均成立；跨语言分析显示结构相似的语言在主谓一致任务中共享更多单元。

Conclusion: 句法一致是大语言模型表征空间中一个具有功能性意义的语法范畴，反映了模型对句法依存关系的系统性编码。

Abstract: Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.

</details>


### [140] [AITutor-EvalKit: Exploring the Capabilities of AI Tutors](https://arxiv.org/abs/2512.03688)
*Numaan Naeem,Kaushal Kumar Maurya,Kseniia Petukhova,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: AITutor-EvalKit is a language technology-based application designed to evaluate the pedagogical quality of AI tutors, supporting education stakeholders and the ACL community through demonstration, evaluation, model inspection, and data visualization.


<details>
  <summary>Details</summary>
Motivation: To address the need for systematic evaluation of the pedagogical quality of AI tutors, especially for education stakeholders and the broader ACL community.

Method: Development of AITutor-EvalKit, an application incorporating language technology for evaluation, demonstration, model inspection, and data visualization.

Result: A functional tool enabling pedagogical evaluation of AI tutors, user feedback collection, and annotation support.

Conclusion: AITutor-EvalKit serves as a practical and scalable solution for evaluating AI tutor quality, bridging educational needs and NLP research communities.

Abstract: We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotations.

</details>


### [141] [DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue](https://arxiv.org/abs/2512.03704)
*Yijun Liao*

Main category: cs.CL

TL;DR: 本文提出DZ-TDPO框架，通过动态KL约束与可学习时间注意力偏置解决长上下文对话中的状态惯性问题，在MSC数据集上取得SOTA效果，并揭示了模型规模与对齐稳定性的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 长上下文对话系统存在“状态惯性”问题，即静态约束阻碍模型协调不断变化的用户意图与既定历史上下文之间的冲突。

Method: 提出DZ-TDPO非破坏性对齐框架，结合冲突感知的动态KL约束与可学习的时间注意力偏置。

Result: 在Multi-Session Chat（MSC）数据集上，DZ-TDPO在Phi-3.5上达到86.2%胜率；Qwen2.5-7B模型实现99.4%胜率且困惑度开销极小；验证了TAI可通过精准注意力调控缓解，而非破坏性权重更新。

Conclusion: DZ-TDPO有效缓解状态惯性，揭示‘容量-稳定性权衡’现象，证明大模型可通过注意力机制实现高对齐低代价，同时保持通用能力（如MMLU）。

Abstract: Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a "Capacity-Stability Trade-off": while smaller models incur an "alignment tax" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO

</details>


### [142] [Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective](https://arxiv.org/abs/2512.03759)
*Jingyang Ou,Jiaqi Han,Minkai Xu,Shaoxuan Xu,Jianwen Xie,Stefano Ermon,Yi Wu,Chongxuan Li*

Main category: cs.CL

TL;DR: 本文提出ESPO方法，通过使用ELBO作为序列级似然代理，在扩散大语言模型（dLLMs）中实现序列级强化学习优化，解决了传统token级RL在dLLMs中因缺乏条件概率分解而难以应用的问题，并在数学推理、编程和规划任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）缺乏自回归模型天然的token级条件概率，导致现有基于token的强化学习方法（如GRPO）难以直接应用，亟需适配dLLMs特性的序列级RL框架。

Method: 提出基于ELBO的序列级策略优化（ESPO），将整句生成视为单一动作，以ELBO作为可计算的序列级似然代理；引入token级重要性权重归一化与鲁棒KL散度估计以保障大规模训练稳定性。

Result: 在Countdown任务上提升20–40分，在数学推理与编程基准上也保持稳定增益，显著优于token级基线方法。

Conclusion: ESPO确立了序列级优化是dLLMs中强化学习的一种原理清晰且实证有效的范式。

Abstract: Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.

</details>


### [143] [In-Context Representation Hijacking](https://arxiv.org/abs/2512.03771)
*Itay Yona,Amir Sarid,Michael Karasik,Yossi Gandelsman*

Main category: cs.CL

TL;DR: 本文提出了一种名为Doublespeak的新型攻击方法，通过在上下文中系统性地将有害关键词替换为良性词汇，诱导大语言模型（LLM）内部表征发生语义覆盖，从而绕过安全对齐机制。


<details>
  <summary>Details</summary>
Motivation: 揭示当前LLM安全对齐策略在潜在表征层面的脆弱性，发现仅靠输出过滤或指令微调不足以防御语义层面的隐式攻击。

Method: 采用无优化的上下文内表征劫持（in-context representation hijacking）：在多个上下文示例中统一替换有害词为良性词，并附加前缀触发，使模型内部表征逐步收敛至有害语义。

Result: 该攻击在多种开源与闭源模型上均有效，尤其在Llama-3.3-70B-Instruct上单句上下文即可达到74%攻击成功率；可解释性分析证实语义覆盖呈逐层传播特性。

Conclusion: Doublespeak暴露了LLM对齐应从输出层延伸至表征层的根本缺陷，呼吁安全机制需直接监控和干预模型内部语义表征。

Abstract: We introduce \textbf{Doublespeak}, a simple \emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \textit{bomb}) with a benign token (e.g., \textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.

</details>


### [144] [Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5](https://arxiv.org/abs/2512.03803)
*Huey Sun,Anabel Yong,Lorenzo Gilly,Felipe Jin*

Main category: cs.CL

TL;DR: 本文将DoLa对比解码方法适配到T5和FLAN-T5等encoder-decoder架构，并首次评估其对指令遵循能力的影响，发现其在部分任务上提升生成忠实度，而在其他任务上则有负面影响；并通过逐层logit演化分析解释该现象。


<details>
  <summary>Details</summary>
Motivation: 现有对比解码方法（如DoLa）仅在decoder-only模型中实现并研究其对事实性的提升，尚未拓展至encoder-decoder架构，也未系统评估其对指令遵循能力的影响。

Method: 将DoLa方法适配到T5和FLAN-T5模型；在多种指令遵循任务上进行评测；开展FLAN-T5模型的逐层logit演化分析以量化DoLa对输出概率的影响。

Result: DoLa在部分任务类别中提升了生成文本的忠实度，但在其他类别中产生负面影响；层分析揭示了其对不同层logit扰动的机制差异。

Conclusion: 对比解码在encoder-decoder架构中效果具有任务依赖性，需结合模型结构与任务特性谨慎应用；该工作为理解对比解码在非decoder-only模型中的行为提供了新视角和实证基础。

Abstract: Contrastive decoding is a lightweight and effective inference-time method that improves the quality of text generation in Large Language Models. However, algorithms such as DoLa (Decoding by Contrastive Layers) have only been implemented in decoder-only architectures and studied for their impact on improving factuality. This work adapts DoLa for the T5 and FLAN-T5 model families and evaluates its impact on the models' instruction following capabilities, which to our knowledge is the first implementation of a contrastive decoding strategy in an encoder-decoder architecture. Our results show that DoLa improves the faithfulness of text generation for certain categories of tasks and harms others. To understand these results, we present a layer-by-layer analysis of logit evolution in a FLAN-T5 model to quantify DoLa's impact on token output probabilities.

</details>


### [145] [Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology](https://arxiv.org/abs/2512.03818)
*Kylie L. Anglin,Stephanie Milan,Brittney Hernandez,Claudia Ventura*

Main category: cs.CL

TL;DR: 本文提出了一种面向心理学构念文本识别的LLM提示工程实证框架，通过比较五种提示策略，发现基于编码手册指导的经验性提示选择与自动提示工程相结合的少样本提示效果最佳，并强调提示中构念定义和任务框架的关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在文本分类中表现优异，但其输出高度依赖提示词表述；而心理学等理论驱动领域中，构念定义精确且预训练数据中可能缺乏充分表征，亟需针对性的提示优化方法。

Method: 实验评估了五种提示策略（编码手册引导的经验提示选择、自动提示工程、角色提示、思维链推理、解释性提示）在零样本和少样本分类下的效果，涵盖三个心理学构念和两个LLM。

Result: 角色提示、思维链和解释性提示无法完全弥补不良提示导致的性能下降；提示中最关键因素是构念定义和任务框架，其次为示例；最优性能来自结合编码手册引导与自动提示工程的少样本提示。

Conclusion: 研究推荐采用人机协同生成大量提示变体，并基于训练集实证评估筛选、在预留集上验证，从而实现理论驱动、系统化且贴近专家判断的LLM提示优化。

Abstract: Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.

</details>


### [146] [Training and Evaluation of Guideline-Based Medical Reasoning in LLMs](https://arxiv.org/abs/2512.03838)
*Michael Staniek,Artem Sokolov,Stefan Riezler*

Main category: cs.CL

TL;DR: 本文提出了一种将医学共识指南（如Sepsis-3）以结构化语言规则形式注入LLM的方法，通过在电子健康记录上微调小模型，使其能分步推理并保证推导与数值正确性；实验表明该方法优于大模型的提示学习和通用医学文本训练，并指出早期预测瓶颈在于时序外推而非分布外泛化，可通过融合时间序列预测模型提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学早期预测模型过于关注准确率，忽视可解释性和对临床共识指南的忠实遵循，难以获得医生信任。

Method: 将医学共识指南（如Sepsis-3）转化为可执行的、面向电子健康记录的言语化推理规则，用于监督微调小型LLM；引入推导正确性（逻辑演绎）和数值正确性（预测值对比真实值）双维度自动评估；并结合时间序列预测模型构建多模态架构以改善稀疏不规则临床变量的未来预测。

Result: 微调后的小型LLM在推导正确性上接近完美，显著优于大模型的一次性提示学习及基于医学文本的训练；瓶颈被定位为‘面向未来的泛化’（即时序外推），而非传统OOD问题；融合时间序列模型后，对未来临床变量的预测性能得到提升。

Conclusion: 将医学共识规则显式建模并融入LLM训练流程，可兼顾准确性与可解释性；解决临床时序数据稀疏性与不规则采样问题，是提升早期预测能力的关键路径。

Abstract: Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.

</details>


### [147] [Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers](https://arxiv.org/abs/2512.03870)
*Hongzhan Lin,Zhiqi Bai,Xinmiao Zhang,Sen Yang,Xiang Li,Siran Yang,Yunlong Xu,Jiaheng Liu,Yongchi Zhao,Jiamang Wang,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文提出FusedKV和FusedKV-Lite两种跨层KV缓存融合方法，通过融合底层与中层的键值信息，在减少50% KV缓存内存的同时，优于标准Transformer解码器的验证困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有跨层KV缓存共享方法（如YOCO、CLA）性能弱于同层方法（如GQA），需探究其根本原因并提升性能。

Method: 分析顶层KV的信息来源分布，发现值主要来自底层、键来自底层与中层；据此提出FusedKV（可学习融合底层与中层KV，直接在RoPE后键上操作）和轻量版FusedKV-Lite（直接复用底层值与中层键）。

Result: 在332M至4B参数LLM上，KV缓存内存减少50%，验证困惑度低于标准Transformer解码器。

Conclusion: FusedKV系列方法是兼顾内存效率与高性能的Transformer解码器架构新选择。

Abstract: Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.

</details>


### [148] [BERnaT: Basque Encoders for Representing Natural Textual Diversity](https://arxiv.org/abs/2512.03903)
*Ekhi Azurmendi,Joseba Fernandez de Landa,Jaione Bengoetxea,Maite Heredia,Julen Etxaniz,Mikel Zubillaga,Ander Soraluze,Aitor Soroa*

Main category: cs.CL

TL;DR: 本文提出应将语言多样性（如方言、历史语言、非正式语言等）纳入语言模型训练，以提升包容性与泛化能力；以巴斯克语为例构建了多源语料库并预训练BERnaT系列模型，实验表明融合标准与多样语料的模型在标准与多样化NLU任务上均表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型依赖经过质量筛选的大规模文本，易排除非标准语言变体，削弱鲁棒性并加剧表征偏见；需系统性纳入语言多样性以构建更具包容性与泛化能力的模型。

Method: 针对巴斯克语，构建包含标准文本、社交媒体和历史文献的多源语料库；预训练三种配置的BERnaT编码器模型（标准、多样、融合）；设计区分标准与多样子集的NLU评估框架。

Result: 融合标准与多样数据训练的模型在所有任务类型（包括标准基准）上均优于仅用标准数据训练的模型，未牺牲标准性能。

Conclusion: 纳入语言多样性对提升语言模型的包容性、鲁棒性与跨变体泛化能力至关重要，应成为模型构建的基本原则。

Abstract: Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.

</details>


### [149] [Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions](https://arxiv.org/abs/2512.03943)
*Kazi Abrab Hossain,Jannatul Somiya Mahmud,Maria Hossain Tuli,Anik Mitra,S. M. Taiabul Haque,Farig Y. Sadeque*

Main category: cs.CL

TL;DR: 本文提出了一个名为BRAND的双语宗教可问责规范数据集，用于检测和分类多语言大模型在宗教语境中的偏见，尤其关注南亚四大宗教（佛教、基督教、印度教和伊斯兰教），发现模型在英语中表现优于孟加拉语，且对伊斯兰教存在持续偏见。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在宗教等敏感话题上仍存在严重偏见问题，尤其在多语言场景下易误表征宗教，可能引发严重误解。

Method: 构建了包含2400多条英文和孟加拉文双语样本的BRAND数据集，覆盖南亚四大宗教，并采用三类提示进行评估。

Result: 实验表明模型在英语上的表现优于孟加拉语，且在宗教中立问题上仍持续表现出对伊斯兰教的偏见，揭示了多语言模型在不同语言间存在不一致的偏见现象。

Conclusion: 多语言大模型在宗教相关任务中仍存在显著且不均衡的偏见，需结合HCI视角深入探讨宗教与灵性相关的技术伦理与设计问题。

Abstract: While recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality.

</details>


### [150] [Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study](https://arxiv.org/abs/2512.03976)
*Lifeng Chen,Ryan Lai,Tianming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段方法（持续预训练CPT + 监督微调SFT）将Qwen2.5-3B适配至低资源、形态丰富的藏语，显著提升语言建模与中→藏翻译性能，并揭示了模型内部适应主要发生在嵌入层、输出头及中后段MLP层。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言（如藏语）上因数据稀缺和跨语言漂移导致的适配难题。

Method: 采用两阶段适配：第一阶段为持续预训练（CPT）以建立藏语语言基础；第二阶段为监督微调（SFT）以增强任务（特别是中→藏翻译）能力。并进行层级别分析（基于Qwen3-4B的435层）。

Result: 困惑度显著下降（2.98 → 1.54）；中→藏翻译BLEU从0.046提升至0.261，chrF从2.2提升至6.6；层分析显示适应集中在嵌入层、输出头及中后段MLP层。

Conclusion: CPT构建藏语语义流形，SFT实现任务对齐且扰动最小；本工作首次定量刻画了LLM藏语适配动态，并提供开源可复现框架，适用于其他低资源语言扩展。

Abstract: Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\rightarrow$ 1.54) and substantial improvements in Chinese$\rightarrow$Tibetan translation quality (BLEU: 0.046 $\rightarrow$ 0.261; chrF: 2.2 $\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid--late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.

</details>


### [151] [Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models](https://arxiv.org/abs/2512.03989)
*Taido Purason,Pavel Chizhov,Ivan P. Yamshchikov,Mark Fishel*

Main category: cs.CL

TL;DR: 本文提出了一种持续BPE训练方法用于词表扩展，并引入基于叶节点的词表剪枝策略，以提升跨领域/语言迁移中预训练语言模型的分词效率和词表利用率。


<details>
  <summary>Details</summary>
Motivation: 现有词表扩展方法（如直接追加新词元）常导致大量不可达或未使用的词元，影响分词效率和模型性能；同时缺乏对冗余词元的有效剪枝机制。

Method: 提出持续BPE训练（continuing BPE merge learning on new data）进行词表扩展，并设计基于叶节点的词表剪枝（leaf-based vocabulary pruning）策略。

Result: 在多语言和多种模型家族上的实验表明，该方法提升了分词效率、增强了新增词元的利用率，并在剪枝后仍保持模型质量。

Conclusion: 所提方法为可控词表修改提供了实用、高效且开源的解决方案。

Abstract: Tokenizer adaptation plays an important role in transferring pre-trained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, which we release as an open-source package.

</details>


### [152] [AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving](https://arxiv.org/abs/2512.04013)
*Ying Wang,Zhen Jin,Jiexiong Xu,Wenhai Lin,Yiquan Chen,Wenzhi Chen*

Main category: cs.CL

TL;DR: 本文提出AugServe，一种面向增强型大语言模型（LLM）推理服务的高效框架，通过两阶段自适应请求调度与动态token批处理机制，显著提升有效吞吐量并降低首字延迟，大幅优于vLLM和InferCept。


<details>
  <summary>Details</summary>
Motivation: 现有增强型LLM推理服务系统面临FCFS调度导致的严重队头阻塞和静态token批处理限制，难以满足SLO要求、适应负载与硬件变化，从而损害有效吞吐量和服务质量。

Method: 提出AugServe框架，包含：（1）两阶段自适应请求调度策略——第一阶段基于请求推理特征优化调度顺序，第二阶段利用运行时信息持续调整；（2）根据硬件状态与实时负载动态调整token批处理大小。

Result: 实验表明，AugServe相较vLLM和InferCept，有效吞吐量提升4.7–33.1倍和3.3–13.2倍，首字延迟（TTFT）最多降低96.3%和95.0%。

Conclusion: AugServe通过联合优化调度策略与批处理机制，显著提升了增强型LLM服务在严苛SLO约束下的推理效率与服务质量，为实际部署提供了可行方案。

Abstract: As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality.
  This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7-33.1x and 3.3-13.2x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.

</details>


### [153] [Jina-VLM: Small Multilingual Vision Language Model](https://arxiv.org/abs/2512.04032)
*Andreas Koukounas,Georgios Mastrapas,Florian Hönicke,Sedigheh Eslami,Guillaume Roncari,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: Jina-VLM is a 2.4B-parameter multilingual vision-language model that achieves SOTA in multilingual VQA among open 2B-scale models, using SigLIP2 vision encoder, Qwen3 language backbone, and an attention-pooling connector for efficient arbitrary-resolution image processing.


<details>
  <summary>Details</summary>
Motivation: To develop a high-performing, multilingual vision-language model at the 2B parameter scale that supports arbitrary-resolution images and maintains strong text-only capabilities.

Method: Jina-VLM combines a SigLIP2 vision encoder with a Qwen3 language backbone, connected via an attention-pooling mechanism to enable token-efficient, resolution-agnostic visual encoding.

Result: Jina-VLM achieves state-of-the-art multilingual visual question answering performance on standard VQA benchmarks, outperforms comparable 2B-scale open VLMs, and retains competitive text-only performance.

Conclusion: Jina-VLM demonstrates that efficient architectural design—particularly attention-based pooling for vision-language alignment—enables strong multilingual and multimodal performance without sacrificing language-only capability, advancing scalable open VLM development.

Abstract: We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.

</details>


### [154] [SkillFactory: Self-Distillation For Learning Cognitive Behaviors](https://arxiv.org/abs/2512.04072)
*Zayne Sprague,Jack Lu,Manya Wadhwa,Sedrick Keh,Mengye Ren,Greg Durrett*

Main category: cs.CL

TL;DR: SkillFactory是一种在强化学习（RL）前通过监督微调（SFT）引导模型习得认知技能（如验证、回溯、重试等）的新方法，利用模型自身生成的‘银级’推理轨迹作为训练数据，无需更强教师模型蒸馏；实验证明其能提升RL后模型在难任务泛化性、技能实际使用率及跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 如何让基础语言模型习得其原本不具备的认知技能（如验证、回溯、重试），以便在后续强化学习中有效利用这些技能。

Method: 提出SkillFactory方法：在RL前进行监督微调（SFT），利用模型自身采样生成的、经人工重排构造的‘银级’推理轨迹（体现目标技能）作为训练数据，不依赖更强模型蒸馏。

Result: （1）SkillFactory初始化的模型经RL后，在更难任务上泛化更好（尽管SFT阶段性能更低）；（2）模型确实在推理中使用了目标认知技能；（3）相比基线RL模型，SkillFactory-RL模型在域外任务上退化更少、鲁棒性更高。

Conclusion: 在RL前通过SFT注入归纳偏置（即认知技能结构），有助于模型更鲁棒、更泛化地掌握和运用认知技能。

Abstract: Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These "silver" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.

</details>


### [155] [Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting](https://arxiv.org/abs/2506.23888)
*André de Souza Loureiro,Jorge Valverde-Rebaza,Julieta Noguez,David Escarcega,Ricardo Marcacini*

Main category: cs.CL

TL;DR: 本文提出MAPS框架，通过结合思维链、自我反思和自动提示技术，提升大语言模型在多步数学推理任务中的表现。该框架采用迭代优化方式，在检测到错误时动态生成提示以引导修正，实验证明其在多个基准测试中显著优于标准思维链方法，并使通用大模型达到接近专用推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理复杂多步推理任务时仍存在困难，需要更有效的推理增强方法。

Method: 提出多层自反射与自动提示（MAPS）框架，结合思维链（CoT）、自我反思和自动提示技术，采用迭代式错误检测与动态提示生成机制进行推理优化。

Result: 在四个主流基准测试中，MAPS显著优于标准CoT方法，并在多个LLM上达到与专门优化推理模型相当的性能；同时能平衡推理深度与计算开销。

Conclusion: MAPS是一种有效提升通用大语言模型多步数学推理能力的新范式，兼顾性能与效率。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly improved their problem-solving capabilities. However, these models still struggle when faced with complex multi-step reasoning tasks. In this paper, we propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework, a novel approach designed to enhance multi-step mathematical reasoning in LLMs by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an iterative refinement process. Initially, the model generates a solution using CoT prompting. When errors are detected, an adaptive self-reflection mechanism identifies and analyzes them, generating tailored prompts to guide corrections. These dynamically adjusted prompts enable the model to iteratively refine its reasoning. Experiments on four well-established benchmarks across multiple LLMs show that MAPS significantly outperforms standard CoT and achieves competitive results with reasoning-optimized models. In addition, MAPS enables general-purpose LLMs to reach performance levels comparable to specialized reasoning models. While deeper reflection layers improve accuracy, they also increase token usage and costs. To balance this trade-off, MAPS strategically limits reflection depth, ensuring an optimal balance between cost and reasoning performance.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [156] [LATTICE: Democratize High-Fidelity 3D Generation at Scale](https://arxiv.org/abs/2512.03052)
*Zeqiang Lai,Yunfei Zhao,Zibo Zhao,Haolin Liu,Qingxiang Lin,Jingwei Huang,Chunchao Guo,Xiangyu Yue*

Main category: cs.GR

TL;DR: LATTICE is a novel framework for high-fidelity 3D asset generation that introduces VoxSet—a semi-structured latent representation anchored to a coarse voxel grid—to bridge the scalability and quality gap between 2D and 3D generative models. It uses a two-stage pipeline (sparse voxel anchor + rectified flow transformer) for efficient, position-aware, and scalable 3D synthesis.


<details>
  <summary>Details</summary>
Motivation: 3D generative modeling lags behind 2D due to challenges in spatial structure prediction, geometric surface modeling, computational complexity of 3D representations, and lack of scalable, structured 3D encoding schemes.

Method: Proposes VoxSet: a semi-structured, voxel-anchored latent representation enabling positional embeddings and token-level test-time scaling; builds LATTICE as a two-stage pipeline—first generating sparse voxel geometry, then refining with a rectified flow transformer.

Result: Achieves state-of-the-art performance across multiple metrics, supports arbitrary-resolution decoding, low-cost training, flexible inference, and enables scalable, high-quality 3D asset generation.

Conclusion: LATTICE, powered by VoxSet, represents a significant step toward practical, scalable, and high-fidelity 3D generative modeling, effectively unifying structural efficiency with geometric fidelity.

Abstract: We present LATTICE, a new framework for high-fidelity 3D asset generation that bridges the quality and scalability gap between 3D and 2D generative models. While 2D image synthesis benefits from fixed spatial grids and well-established transformer architectures, 3D generation remains fundamentally more challenging due to the need to predict both spatial structure and detailed geometric surfaces from scratch. These challenges are exacerbated by the computational complexity of existing 3D representations and the lack of structured and scalable 3D asset encoding schemes. To address this, we propose VoxSet, a semi-structured representation that compresses 3D assets into a compact set of latent vectors anchored to a coarse voxel grid, enabling efficient and position-aware generation. VoxSet retains the simplicity and compression advantages of prior VecSet methods while introducing explicit structure into the latent space, allowing positional embeddings to guide generation and enabling strong token-level test-time scaling. Built upon this representation, LATTICE adopts a two-stage pipeline: first generating a sparse voxelized geometry anchor, then producing detailed geometry using a rectified flow transformer. Our method is simple at its core, but supports arbitrary resolution decoding, low-cost training, and flexible inference schemes, achieving state-of-the-art performance on various aspects, and offering a significant step toward scalable, high-quality 3D asset creation.

</details>


### [157] [Radiance Meshes for Volumetric Reconstruction](https://arxiv.org/abs/2512.04076)
*Alexander Mai,Trevor Hedstrom,George Kopanas,Janne Kontkanen,Falko Kuester,Jonathan T. Barron*

Main category: cs.GR

TL;DR: 本文提出了一种名为radiance meshes的新方法，利用Delaunay四面体化生成恒定密度的四面体单元来表示辐射场，支持快速精确的光栅化与光线追踪渲染，并结合Zip-NeRF风格主干网络解决顶点优化导致的拓扑不连续问题，实现实时高质量视图合成及多种扩展应用。


<details>
  <summary>Details</summary>
Motivation: 现有辐射场表示方法在硬件兼容性、渲染速度与精度之间存在权衡；Delaunay四面体化相比Voronoi图更易硬件实现，且能支持精确体积渲染。

Method: 采用Delaunay四面体化构建恒定密度四面体网格表示辐射场；设计新型光栅化算法；结合Zip-NeRF式网络建模辐射值以应对顶点优化引发的拓扑变化（如边翻转）；支持光栅化与光线追踪双路径渲染。

Result: 在多种平台上，渲染速度优于所有先前辐射场表示方法（同等原语数量与分辨率下）；可实现实时、高质量视图合成；支持鱼眼镜头畸变、物理仿真、编辑和网格提取等应用。

Conclusion: radiance meshes是一种高效、硬件友好、数学严谨且具扩展性的辐射场表示方法，在保持体积渲染方程精确求解的同时，显著提升实时渲染性能与应用灵活性。

Abstract: We introduce radiance meshes, a technique for representing radiance fields with constant density tetrahedral cells produced with a Delaunay tetrahedralization. Unlike a Voronoi diagram, a Delaunay tetrahedralization yields simple triangles that are natively supported by existing hardware. As such, our model is able to perform exact and fast volume rendering using both rasterization and ray-tracing. We introduce a new rasterization method that achieves faster rendering speeds than all prior radiance field representations (assuming an equivalent number of primitives and resolution) across a variety of platforms. Optimizing the positions of Delaunay vertices introduces topological discontinuities (edge flips). To solve this, we use a Zip-NeRF-style backbone which allows us to express a smoothly varying field even when the topology changes. Our rendering method exactly evaluates the volume rendering equation and enables high quality, real-time view synthesis on standard consumer hardware. Our tetrahedral meshes also lend themselves to a variety of exciting applications including fisheye lens distortion, physics-based simulation, editing, and mesh extraction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [158] [Physics-Informed Machine Learning for Steel Development: A Computational Framework and CCT Diagram Modelling](https://arxiv.org/abs/2512.03050)
*Peter Hedström,Victor Lamelas Cubero,Jón Sigurdsson,Viktor Österberg,Satish Kolli,Joakim Odqvist,Ziyong Hou,Wangzhong Mu,Viswanadh Gowtham Arigela*

Main category: cs.LG

TL;DR: 本文提出了一种结合物理机理与机器学习的物理信息连续冷却转变（CCT）模型，用于钢材料，具备高效率、强泛化性，并可扩展为热处理通用数字孪生平台。


<details>
  <summary>Details</summary>
Motivation: 通用机器学习框架难以准确刻画钢等复杂工业材料中成分、工艺参数与微观组织/性能之间的复杂关系。

Method: 构建融合物理洞察与机器学习的计算框架，开发物理信息CCT模型；基于4100张CCT图谱数据训练，并通过文献与实验数据验证。

Result: 生成完整CCT图（含100条冷却曲线）耗时<5秒；合金钢相分类F1分数>88%；各相变温度回归MAE<20°C（贝氏体为27°C）。

Conclusion: 该框架可扩展为通用热处理数字孪生平台，并通过耦合仿真与实验，加速材料设计流程。

Abstract: Machine learning (ML) has emerged as a powerful tool for accelerating the computational design and production of materials. In materials science, ML has primarily supported large-scale discovery of novel compounds using first-principles data and digital twin applications for optimizing manufacturing processes. However, applying general-purpose ML frameworks to complex industrial materials such as steel remains a challenge. A key obstacle is accurately capturing the intricate relationship between chemical composition, processing parameters, and the resulting microstructure and properties. To address this, we introduce a computational framework that combines physical insights with ML to develop a physics-informed continuous cooling transformation (CCT) model for steels. Our model, trained on a dataset of 4,100 diagrams, is validated against literature and experimental data. It demonstrates high computational efficiency, generating complete CCT diagrams with 100 cooling curves in under 5 seconds. It also shows strong generalizability across alloy steels, achieving phase classification F1 scores above 88% for all phases. For phase transition temperature regression, it attains mean absolute errors (MAE) below 20 °C across all phases except bainite, which shows a slightly higher MAE of 27 °C. This framework can be extended with additional generic and customized ML models to establish a universal digital twin platform for heat treatment. Integration with complementary simulation tools and targeted experiments will further support accelerated materials design workflows.

</details>


### [159] [Mitigating hallucinations and omissions in LLMs for invertible problems: An application to hardware logic design automation](https://arxiv.org/abs/2512.03053)
*Andrew S. Cassidy,Guillaume Garreau,Jay Sivagnaname,Mike Grassi,Bernard Brezzo,John V. Arthur,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 本文提出了一种利用大语言模型（LLMs）作为无损编解码器的方法，用于在逻辑条件表（LCTs）与硬件描述语言（HDL）代码之间进行可逆转换，从而缓解LLM常见的幻觉和遗漏问题，并验证了其在片上网络路由器设计中的有效性与实用性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在代码生成任务中易出现幻觉（hallucinations）和遗漏（omissions）的问题，特别是在形式化、可逆的硬件设计转换任务中提升可靠性与可验证性。

Method: 将LLM用作无损编码器（源域LCT → 目标域HDL）和无损解码器（HDL → LCT），通过比较原始LCT与从生成HDL反推的LCT来验证转换保真度；实验使用7种不同LLM生成二维NoC路由器（13个单元，1500–2000行HDL）并完成双向重建。

Result: 该方法能准确检测LLM生成逻辑的正确性与错误，显著提升开发效率，并辅助发现设计规格本身的错误；所有7个LLM均实现了高保真LCT-HDL-LCT闭环重建。

Conclusion: 将LLM视为无损编解码器是一种可行且有效的新范式，尤其适用于可逆、结构化、形式化强的领域（如硬件设计），可系统性抑制LLM不可靠输出，增强可信AI工程实践。

Abstract: We show for invertible problems that transform data from a source domain (for example, Logic Condition Tables (LCTs)) to a destination domain (for example, Hardware Description Language (HDL) code), an approach of using Large Language Models (LLMs) as a lossless encoder from source to destination followed by as a lossless decoder back to the source, comparable to lossless compression in information theory, can mitigate most of the LLM drawbacks of hallucinations and omissions. Specifically, using LCTs as inputs, we generate the full HDL for a two-dimensional network-on-chip router (13 units, 1500-2000 lines of code) using seven different LLMs, reconstruct the LCTs from the auto-generated HDL, and compare the original and reconstructed LCTs. This approach yields significant productivity improvements, not only confirming correctly generated LLM logic and detecting incorrectly generated LLM logic but also assisting developers in finding design specification errors.

</details>


### [160] [Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research](https://arxiv.org/abs/2512.03054)
*Ciro Benito Raggio,Lucia Migliorelli,Nils Skupien,Mathias Krohmer Zabaleta,Oliver Blanck,Francesco Cicone,Giuseppe Lucio Cascini,Paolo Zaffino,Maria Francesca Spadea*

Main category: cs.LG

TL;DR: 本文提出了一种面向绿色AI的自适应层冻结策略，用于降低联邦学习（FL）在医疗影像转换（MRI-to-CT）中的能耗与计算开销，同时保持模型性能。该策略基于编码器权重轮间相对变化动态冻结层，并引入耐心机制确保稳定性；实验表明其最多可降低23%训练时间、能耗与碳排放，且多数架构下图像转换精度（MAE）无显著下降，部分甚至提升。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽有望促进医疗公平，但其高资源消耗常将算力有限的医疗机构排除在外，加剧健康不平等；因此需兼顾模型性能与绿色可持续性。

Method: 提出一种自适应层冻结策略：根据每轮训练中编码器权重的相对变化量动态决定是否冻结；引入耐心机制，仅当权重更新持续微小时才冻结；使用CodeCarbon库量化能耗与CO2eq排放；在多种联邦架构下评估MRI-to-CT转换任务。

Result: 相比非冻结基线，训练时间、总能耗和CO2eq排放最多降低23%；MRI-to-CT转换的MAE仅小幅波动，其中3/5架构无统计学显著差异，2/5架构显著提升。

Conclusion: 该绿色AI导向策略在保障临床级模型性能的同时显著降低环境足迹，为构建兼顾隐私、公平与可持续性的联邦学习评估新范式奠定基础，推动AI驱动医疗中的气候正义、社会正义与经济正义。

Abstract: Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.

</details>


### [161] [Physics-informed self-supervised learning for predictive modeling of coronary artery digital twins](https://arxiv.org/abs/2512.03055)
*Xiaowu Sun,Thabo Mahendiran,Ortal Senouf,Denise Auberson,Bernard De Bruyne,Stephane Fournier,Olivier Muller,Pascal Frossard,Emmanuel Abbe,Dorina Thanou*

Main category: cs.LG

TL;DR: 本文提出PINS-CAD——一种物理信息自监督图神经网络框架，利用20万例合成冠状动脉数字孪生数据，结合1D Navier-Stokes方程与压降定律进行预训练，无需CFD仿真或标注数据；在FAME2临床数据上微调后，AUC达0.73，优于传统风险评分，并可生成空间分辨的压力与FFR曲线，实现无仿真、可解释、可扩展的预防性心脏病学分析。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉疾病（CAD）早期风险预测需求迫切，但现有基于计算流体力学（CFD）的方法计算成本高、难以扩展，而纯数据驱动方法受限于标注数据稀缺和缺乏生理先验知识。

Method: 提出PINS-CAD：一种物理信息自监督学习框架，使用图神经网络在20万例合成冠状动脉数字孪生上预训练，以1D Navier-Stokes方程和压力-流量关系为物理约束；随后在FAME2多中心临床数据（635例患者）上微调，用于预测心血管事件及生成压力/FFR曲线。

Result: 在FAME2数据集上预测未来心血管事件的AUC为0.73，显著优于临床风险评分（如CAD-PRO）和纯数据驱动基线模型；同时可输出空间分辨的压力分布与FFR曲线，提供可解释的生理标志物。

Conclusion: 将物理先验嵌入几何深度学习可大幅提升小样本下的泛化能力与生理可解释性，PINS-CAD实现了从常规血管造影到无仿真、生理感知的规模化预防性心脏评估范式转变。

Abstract: Cardiovascular disease is the leading global cause of mortality, with coronary artery disease (CAD) as its most prevalent form, necessitating early risk prediction. While 3D coronary artery digital twins reconstructed from imaging offer detailed anatomy for personalized assessment, their analysis relies on computationally intensive computational fluid dynamics (CFD), limiting scalability. Data-driven approaches are hindered by scarce labeled data and lack of physiological priors. To address this, we present PINS-CAD, a physics-informed self-supervised learning framework. It pre-trains graph neural networks on 200,000 synthetic coronary digital twins to predict pressure and flow, guided by 1D Navier-Stokes equations and pressure-drop laws, eliminating the need for CFD or labeled data. When fine-tuned on clinical data from 635 patients in the multicenter FAME2 study, PINS-CAD predicts future cardiovascular events with an AUC of 0.73, outperforming clinical risk scores and data-driven baselines. This demonstrates that physics-informed pretraining boosts sample efficiency and yields physiologically meaningful representations. Furthermore, PINS-CAD generates spatially resolved pressure and fractional flow reserve curves, providing interpretable biomarkers. By embedding physical priors into geometric deep learning, PINS-CAD transforms routine angiography into a simulation-free, physiology-aware framework for scalable, preventive cardiology.

</details>


### [162] [Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models](https://arxiv.org/abs/2512.03056)
*Zhidong Gao,Zimeng Pan,Yuhang Yao,Chenyue Xie,Wei Wei*

Main category: cs.LG

TL;DR: 本文提出Delta Sampling (DS)方法，通过在推理时利用适配前后模型预测的差异（delta），实现跨不同架构基础模型的知识迁移，无需原始训练数据，且具有即插即用特性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的适配组件（如LoRA、ControlNet）与特定基础模型强耦合，难以随基础模型升级（如SD 1.x→2.x）复用，因参数和架构变化大。

Method: Delta Sampling（DS）：在推理阶段计算旧适配模型与原基础模型的预测差（delta），并将该delta用于引导新基础模型的去噪过程，实现知识迁移。

Result: 在多个Stable Diffusion版本上验证，DS能稳定提升目标效果生成质量（如视觉风格、语义概念、结构），兼容不同采样策略。

Conclusion: DS是一种高效、即插即用的跨架构知识迁移机制，显著提升扩散模型适配组件的可复用性。

Abstract: Diffusion models like Stable Diffusion (SD) drive a vibrant open-source ecosystem including fully fine-tuned checkpoints and parameter-efficient adapters such as LoRA, LyCORIS, and ControlNet. However, these adaptation components are tightly coupled to a specific base model, making them difficult to reuse when the base model is upgraded (e.g., from SD 1.x to 2.x) due to substantial changes in model parameters and architecture. In this work, we propose Delta Sampling (DS), a novel method that enables knowledge transfer across base models with different architectures, without requiring access to the original training data. DS operates entirely at inference time by leveraging the delta: the difference in model predictions before and after the adaptation of a base model. This delta is then used to guide the denoising process of a new base model. We evaluate DS across various SD versions, demonstrating that DS achieves consistent improvements in creating desired effects (e.g., visual styles, semantic concepts, and structures) under different sampling strategies. These results highlight DS as an effective, plug-and-play mechanism for knowledge transfer in diffusion-based image synthesis. Code:~ https://github.com/Zhidong-Gao/DeltaSampling

</details>


### [163] [Dynamical Properties of Tokens in Self-Attention and Effects of Positional Encoding](https://arxiv.org/abs/2512.03058)
*Duy-Tung Pham,An The Nguyen,Viet-Hoang Tran,Nhan-Phu Chung,Xin T. Tong,Tan M. Nguyen,Thieu N. Vo*

Main category: cs.LG

TL;DR: 本文研究了预训练Transformer模型中token的动力学特性，并基于连续时间极限下的动力系统分析，提出了判断token收敛或发散的充分条件；进一步探讨了不同位置编码（绝对与旋转）对动力学行为的影响，并据此提出简单架构改进以缓解有害的收敛现象，从而提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作对Transformer中token动态行为的理解有限，且缺乏适用于真实模型的普适性理论条件；同时，观察到某些动力学行为（如token收敛）会损害模型性能，亟需从理论和实践两方面加以改进。

Method: 分析预训练Transformer在连续时间极限下的动力系统，刻画token间距离随时间变化的渐近行为；推导基于模型参数的token收敛至零或发散至无穷的充分条件；比较绝对位置编码与旋转位置编码对动力学模式的影响；基于理论发现设计轻量级架构改进方案。

Result: 获得了比先前工作更广泛、更实用的token收敛/发散判据；实证发现token收敛情形显著降低模型性能；提出的架构改进有效缓解了绝对和旋转位置编码下的有害收敛行为。

Conclusion: Transformer中token的动力学特性深刻影响模型性能，通过理论驱动的架构调整可稳健提升模型表现；本研究为Transformer的设计与优化提供了新的理论基础与实用原则。

Abstract: This paper investigates the dynamical properties of tokens in pre-trained Transformer models and explores their application to improving Transformers. To this end, we analyze the dynamical system governing the continuous-time limit of the pre-trained model and characterize the asymptotic behavior of its solutions. Specifically, we characterize when tokens move closer to or farther from one another over time, depending on the model parameters. We provide sufficient conditions, based on these parameters, to identify scenarios where tokens either converge to zero or diverge to infinity. Unlike prior works, our conditions are broader in scope and more applicable to real-world models. Furthermore, we investigate how different forms of positional encoding -- specifically absolute and rotary -- affect these dynamical regimes. Empirical evidence reveals that the convergence scenario adversely impacts model performance. Motivated by these insights, we propose simple refinements to Transformer architectures that mitigate convergence behavior in models with absolute or rotary positional encoding. These findings support theoretical foundations and design principles for improving Transformer models.

</details>


### [164] [Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL](https://arxiv.org/abs/2512.03059)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文提出了一种安全的分层深度强化学习（HDRL）框架，用于在多源不确定性下优化电动公交车（EBs）充电调度，兼顾成本最小化与电池安全，通过结合光伏能源与动态电价实现低碳公交运营。


<details>
  <summary>Details</summary>
Motivation: 电动公交车与可再生能源（如光伏）集成是实现低碳公交的关键，但实际中面临光伏发电波动、电价动态变化、行程时间不确定及充电设施有限等多重不确定性，亟需兼顾经济性与安全性的智能充电调度方法。

Method: 将问题建模为带约束的马尔可夫决策过程（CMDP），引入‘选项’机制支持时间抽象决策；提出新型分层HDRL算法DAC-MAPPO-Lagrangian：高层用集中式PPO-Lagrangian学习安全充电桩分配策略，低层用MAPPO-Lagrangian在CTDE范式下学习去中心化充电功率决策，并融合Lagrangian松弛处理安全约束。

Result: 在真实数据上的实验表明，该方法在降低运营成本和保障电池不耗尽（安全合规）两方面均优于现有基线方法，且收敛速度快。

Conclusion: 所提出的安全HDRL框架能有效应对多源不确定性，在保证电动公交车运行安全的前提下显著提升充电调度的经济性与实用性，为可持续智慧公交系统提供了可行的技术路径。

Abstract: The integration of Electric Buses (EBs) with renewable energy sources such as photovoltaic (PV) panels is a promising approach to promote sustainable and low-carbon public transportation. However, optimizing EB charging schedules to minimize operational costs while ensuring safe operation without battery depletion remains challenging - especially under real-world conditions, where uncertainties in PV generation, dynamic electricity prices, variable travel times, and limited charging infrastructure must be accounted for. In this paper, we propose a safe Hierarchical Deep Reinforcement Learning (HDRL) framework for solving the EB Charging Scheduling Problem (EBCSP) under multi-source uncertainties. We formulate the problem as a Constrained Markov Decision Process (CMDP) with options to enable temporally abstract decision-making. We develop a novel HDRL algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization Lagrangian (DAC-MAPPO-Lagrangian), which integrates Lagrangian relaxation into the Double Actor-Critic (DAC) framework. At the high level, we adopt a centralized PPO-Lagrangian algorithm to learn safe charger allocation policies. At the low level, we incorporate MAPPO-Lagrangian to learn decentralized charging power decisions under the Centralized Training and Decentralized Execution (CTDE) paradigm. Extensive experiments with real-world data demonstrate that the proposed approach outperforms existing baselines in both cost minimization and safety compliance, while maintaining fast convergence speed.

</details>


### [165] [Optimizing Life Sciences Agents in Real-Time using Reinforcement Learning](https://arxiv.org/abs/2512.03065)
*Nihir Chadderwala*

Main category: cs.LG

TL;DR: 本文提出了一种结合AWS Strands Agents与Thompson Sampling上下文赌博机的新框架，使生命科学领域的生成式AI代理能仅通过用户反馈自主学习最优决策策略，无需标注数据，在生成策略、工具选择和领域路由三方面实现自适应优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖固定规则或昂贵的标注数据，难以适应动态变化的用户需求和环境条件。

Method: 将AWS Strands Agents与Thompson Sampling上下文赌博机结合，基于用户反馈在线学习生成策略（直接生成 vs. 思维链）、工具选择（文献检索、药物数据库等）及领域路由（药理学、分子生物学、临床专家等）的最优组合。

Result: 在生命科学查询任务上，相比随机基线，用户满意度提升15–30%；20–30次交互后即显现清晰的学习模式；无需真实标签，支持持续自适应。

Conclusion: 该框架为具身AI系统中的探索-利用权衡提供了原理性解决方案，具备强实用性与可扩展性。

Abstract: Generative AI agents in life sciences face a critical challenge: determining the optimal approach for diverse queries ranging from simple factoid questions to complex mechanistic reasoning. Traditional methods rely on fixed rules or expensive labeled training data, neither of which adapts to changing conditions or user preferences. We present a novel framework that combines AWS Strands Agents with Thompson Sampling contextual bandits to enable AI agents to learn optimal decision-making strategies from user feedback alone. Our system optimizes three key dimensions: generation strategy selection (direct vs. chain-of-thought), tool selection (literature search, drug databases, etc.), and domain routing (pharmacology, molecular biology, clinical specialists). Through empirical evaluation on life science queries, we demonstrate 15-30\% improvement in user satisfaction compared to random baselines, with clear learning patterns emerging after 20-30 queries. Our approach requires no ground truth labels, adapts continuously to user preferences, and provides a principled solution to the exploration-exploitation dilemma in agentic AI systems.

</details>


### [166] [A Large Scale Heterogeneous Treatment Effect Estimation Framework and Its Applications of Users' Journey at Snap](https://arxiv.org/abs/2512.03060)
*Jing Pan,Li Shi,Paul Lo*

Main category: cs.LG

TL;DR: 本文提出了一种大规模工业级异质性处理效应（HTE）估计框架，利用数亿Snapchat用户实验数据，通过跨实验聚合发现潜在用户特征，并实现稳定、可扩展的CATE估计；该框架已成功应用于广告影响力与敏感性建模，并在线A/B测试中带来超6倍常规显著水平的业务指标提升。


<details>
  <summary>Details</summary>
Motivation: 传统处理效应模型假设所有用户响应一致，但现实中用户对干预（如广告）的反应存在显著异质性；现有方法难以在超大规模、稀疏、多源实验数据下稳定估计个体化因果效应。

Method: 构建端到端工业级HTE估计框架，核心包括：1）实验选择策略以保障数据质量与可比性；2）适配高维稀疏用户特征的base learner设计（如树模型或深度网络）；3）支持持续更新的增量训练机制；4）跨实验联合建模以挖掘不可观测的潜在用户特征。

Result: 在Snapchat数亿用户规模上实现了稳定、可部署的CATE估计；两个应用——用户广告影响力（influenceability）和广告敏感性（sensitivity）建模均取得实效；基于influenceability的定向投放在线A/B测试使关键业务指标提升幅度超过常规统计显著阈值的六倍。

Conclusion: 大规模HTE建模不仅可行，而且能释放深层用户因果洞察；跨实验联合学习是解锁不可观测用户维度、提升因果估计鲁棒性与业务价值的关键路径；该框架为工业界因果推断提供了可复用的系统范式。

Abstract: Heterogeneous Treatment Effect (HTE) and Conditional Average Treatment Effect (CATE) models relax the assumption that treatment effects are the same for every user. We present a large scale industrial framework for estimating HTE using experimental data from hundreds of millions of Snapchat users. By combining results across many experiments, the framework uncovers latent user characteristics that were previously unmeasurable and produces stable treatment effect estimates at scale.
  We describe the core components that enabled this system, including experiment selection, base learner design, and incremental training. We also highlight two applications: user influenceability to ads and user sensitivity to ads. An online A/B test using influenceability scores for targeting showed an improvement on key business metrics that is more than six times larger than what is typically considered significant.

</details>


### [167] [Globally optimized SVD compression of LLMs via Fermi-function-based rank selection and gauge fixing](https://arxiv.org/abs/2512.03062)
*Roman Rausch,David Jansen,Sukhbinder Singh,Román Orús*

Main category: cs.LG

TL;DR: 本文提出两种物理启发的SVD压缩方法：FermiGrad用于全局优化各层秩，PivGa用于无损压缩低秩因子。


<details>
  <summary>Details</summary>
Motivation: LLM计算资源需求高，SVD压缩虽有前景但存在层间秩选择困难和参数冗余问题。

Method: 提出FermiGrad（基于费米函数的连续优化算法）确定最优层秩；提出PivGa（利用低秩分解内在规范自由度）实现无损压缩。

Result: 实现了更高效、更紧凑的LLM低秩压缩，在保持性能的同时降低参数冗余。

Conclusion: 物理启发的方法可有效提升SVD在LLM压缩中的实用性与效率。

Abstract: Large Language Models (LLMs) are very demanding in terms of their computational resources. Low-rank decompositions of LLM weights, e.g. via Singular Value Decomposition (SVD), is a promising approach for LLM compression, but presents several practical hurdles, e.g. selecting appropriate layer-wise ranks and getting rid of its parameter redundancy. In this work, we present two physics-inspired improvements to SVD LLM compression: (1) \textbf{FermiGrad}, a gradient-descent algorithm that determines globally optimal layer-wise ranks by relaxing the discrete singular-value truncation into a continuous optimization using the Fermi function; (2) \textbf{PivGa}, an additional \textit{lossless} compression of the low-rank factors that exploits the intrinsic gauge freedom in their parametrization.

</details>


### [168] [ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms](https://arxiv.org/abs/2512.03476)
*Juan Diego Toscano,Daniel T. Chen,George Em Karniadakis*

Main category: cs.LG

TL;DR: ATHENA是一个面向科学计算与科学机器学习的自主实验室框架，通过HENA循环（一种基于上下文的多臂老虎机机制）实现从理论到代码的自动转化，在对称性识别、数值求解、混合建模等方面达到超人类精度（验证误差达10^-14），并支持人机协同优化。


<details>
  <summary>Details</summary>
Motivation: 弥合理论构想与计算实现之间的鸿沟是科学计算（SciC）和科学机器学习（SciML）的主要瓶颈。

Method: 提出ATHENA框架，核心为HENA循环——将知识驱动的诊断过程建模为上下文老虎机问题；系统基于专家蓝图（如通用逼近、物理约束）从组合空间中选择结构动作（A_n），生成可执行代码（S_n）以获得科学奖励（R_n）；支持符号-数值混合工作流（如PINNs与FEM耦合）及人机协同干预。

Result: 在SciC中实现数学对称性自动识别与稳定数值求解；在SciML中解决病态建模问题，达成10^-14量级验证误差；人机协同使结果提升一个数量级。

Conclusion: ATHENA标志着从实现细节转向方法论创新的范式转变，推动科学发现加速。

Abstract: Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' ($A_n$) from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code ($S_n$) to generate scientific rewards ($R_n$). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of $10^{-14}$. Furthermore, collaborative ``human-in-the-loop" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.

</details>


### [169] [Modal Logical Neural Networks](https://arxiv.org/abs/2512.03491)
*Antonin Sulc*

Main category: cs.LG

TL;DR: 本文提出了模态逻辑神经网络（MLNNs），将深度学习与模态逻辑的严格语义（特别是Kripke语义）结合，通过可微分的模态神经元实现对‘必然’和‘可能’的推理，并支持固定或学习世界间可达关系，从而在保持端到端可微的同时增强逻辑一致性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络缺乏形式逻辑支撑，难以保证推理的逻辑一致性与可解释性；而传统符号系统又难以从数据中学习结构。本文旨在弥合深度学习与模态逻辑之间的鸿沟，构建兼具学习能力与逻辑严谨性的神经符号框架。

Method: 基于Kripke语义设计可微分的□和◇模态神经元，定义在可能世界集合上；可达关系可预设（硬编码规则）或由神经网络参数化（从数据学习）；整体架构端到端可微，优化目标为最小化逻辑矛盾损失（logical contradiction loss）。

Result: 在语法守卫、公理化未知检测、多智能体信念信任建模、自然语言谈判中的建设性欺骗识别四个任务上验证了MLNNs的有效性：引入或学习可达关系显著提升了逻辑一致性与模型可解释性，且不改变原有任务架构。

Conclusion: MLNNs是一种灵活、可微、可解释的神经符号框架，能统一执行学习与演绎推理，在保持深度学习表达力的同时嵌入模态逻辑的语义约束，为可信AI提供新路径。

Abstract: We propose Modal Logical Neural Networks (MLNNs), a neurosymbolic framework that integrates deep learning with the formal semantics of modal logic, enabling reasoning about necessity and possibility. Drawing on Kripke semantics, we introduce specialized neurons for the modal operators $\Box$ and $\Diamond$ that operate over a set of possible worlds, enabling the framework to act as a differentiable ``logical guardrail.'' The architecture is highly flexible: the accessibility relation between worlds can either be fixed by the user to enforce known rules or, as an inductive feature, be parameterized by a neural network. This allows the model to optionally learn the relational structure of a logical system from data while simultaneously performing deductive reasoning within that structure.
  This versatile construction is designed for flexibility. The entire framework is differentiable from end to end, with learning driven by minimizing a logical contradiction loss. This not only makes the system resilient to inconsistent knowledge but also enables it to learn nonlinear relationships that can help define the logic of a problem space. We illustrate MLNNs on four case studies: grammatical guardrailing, axiomatic detection of the unknown, multi-agent epistemic trust, and detecting constructive deception in natural language negotiation. These experiments demonstrate how enforcing or learning accessibility can increase logical consistency and interpretability without changing the underlying task architecture.

</details>


### [170] [Hierarchical clustering of complex energy systems using pretopology](https://arxiv.org/abs/2512.03069)
*Loup-Noe Levy,Jeremie Bosom,Guillaume Guerard,Soufian Ben Amor,Marc Bui,Hai Tran*

Main category: cs.LG

TL;DR: 本文提出了一种基于预拓扑学的多准则分层分类算法，用于建模和分类大范围分布式区域的建筑能耗曲线，以优化能耗管理。该方法在合成数据和400个真实能耗站点数据上验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统逐个深入审计成千上万栋建筑耗时、费钱且依赖大量专业人员，亟需自动化方法构建高效能耗推荐系统。

Method: 采用预拓扑学建模能耗曲线，并基于预拓扑空间性质开发了多准则分层分类算法，集成于Python库中。

Result: 在二维点集和生成时间序列数据上，算法能准确识别簇结构；对生成时间序列，Pearson相关性聚类的调整兰德指数（ARI）达1；在400个法国真实能耗站点数据上完成验证。

Conclusion: 预拓扑建模与多准则分层分类方法可有效支持大规模建筑能耗分析与优化管理，具备实际应用潜力。

Abstract: This article attempts answering the following problematic: How to model and classify energy consumption profiles over a large distributed territory to optimize the management of buildings' consumption?
  Doing case-by-case in depth auditing of thousands of buildings would require a massive amount of time and money as well as a significant number of qualified people. Thus, an automated method must be developed to establish a relevant and effective recommendations system.
  To answer this problematic, pretopology is used to model the sites' consumption profiles and a multi-criterion hierarchical classification algorithm, using the properties of pretopological space, has been developed in a Python library.
  To evaluate the results, three data sets are used: A generated set of dots of various sizes in a 2D space, a generated set of time series and a set of consumption time series of 400 real consumption sites from a French Energy company.
  On the point data set, the algorithm is able to identify the clusters of points using their position in space and their size as parameter. On the generated time series, the algorithm is able to identify the time series clusters using Pearson's correlation with an Adjusted Rand Index (ARI) of 1.

</details>


### [171] [Mixed Data Clustering Survey and Challenges](https://arxiv.org/abs/2512.03070)
*Guillaume Guerard,Sonia Djebali*

Main category: cs.LG

TL;DR: 本文提出了一种基于前拓扑空间的混合数据聚类方法，以应对大数据背景下数值型与分类型异构数据的聚类挑战，并通过与经典数值聚类及现有前拓扑方法的基准对比验证其有效性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法难以有效处理混合数据（数值型与分类型变量共存）的复杂性，而大数据时代对可解释、结构化的聚类结果需求迫切。

Method: 提出一种基于前拓扑空间理论的新型混合数据聚类算法，强调层次性与可解释性。

Result: 该方法在基准测试中展现出相较于经典数值聚类算法和现有前拓扑方法的性能优势与有效性。

Conclusion: 基于前拓扑空间的聚类方法为混合数据提供了更适配、更具解释性的解决方案，适用于大数据环境下的实际决策支持。

Abstract: The advent of the big data paradigm has transformed how industries manage and analyze information, ushering in an era of unprecedented data volume, velocity, and variety. Within this landscape, mixed-data clustering has become a critical challenge, requiring innovative methods that can effectively exploit heterogeneous data types, including numerical and categorical variables. Traditional clustering techniques, typically designed for homogeneous datasets, often struggle to capture the additional complexity introduced by mixed data, underscoring the need for approaches specifically tailored to this setting. Hierarchical and explainable algorithms are particularly valuable in this context, as they provide structured, interpretable clustering results that support informed decision-making. This paper introduces a clustering method grounded in pretopological spaces. In addition, benchmarking against classical numerical clustering algorithms and existing pretopological approaches yields insights into the performance and effectiveness of the proposed method within the big data paradigm.

</details>


### [172] [PretopoMD: Pretopology-based Mixed Data Hierarchical Clustering](https://arxiv.org/abs/2512.03071)
*Loup-Noe Levy,Guillaume Guerard,Sonia Djebali,Soufian Ben Amor*

Main category: cs.LG

TL;DR: 本文提出了一种基于预拓扑学的新算法，利用析取范式构建可定制的逻辑规则和可调超参数，无需降维即可直接对混合数据进行可解释的层次聚类。


<details>
  <summary>Details</summary>
Motivation: 解决混合数据聚类中需依赖降维、缺乏可解释性及难以处理异质数据的问题。

Method: 基于预拓扑学，采用析取范式（DNF）构建可定制逻辑规则与可调超参数，支持用户定义的层次聚类结构，并通过层次树状图与对比聚类指标评估性能。

Result: 在原始数据上实现高精度、可解释的聚类，优于现有方法，保持数据完整性，提升聚类结果的鲁棒性与可解释性。

Conclusion: 该方法突破传统降维依赖，以逻辑规则增强聚类形成与解释能力，为混合数据聚类提供了重要新思路。

Abstract: This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm's robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.

</details>


### [173] [Model-Agnostic Fairness Regularization for GNNs with Incomplete Sensitive Information](https://arxiv.org/abs/2512.03074)
*Mahdi Tavassoli Kejani,Fadi Dornaika,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 本文提出了一种模型无关的公平性正则化框架，用于解决图神经网络（GNN）在敏感属性仅部分可得情况下的公平性问题，兼顾平等机会与统计均等，并在多个真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有公平性感知GNN方法依赖于所有节点的敏感属性完全可用这一强假设，但在现实中因隐私和数据收集限制难以满足，亟需适用于敏感属性部分缺失场景的新方法。

Method: 提出一种模型无关的公平性正则化框架，将平等机会和统计均等建模为可微正则项，融入GNN训练目标函数中，适用于敏感属性仅部分可观测的情形。

Result: 在五个真实世界基准数据集上的实验表明，该方法显著降低多种公平性指标偏差，同时保持有竞争力的节点分类准确率，公平性-准确性权衡优于基线模型。

Conclusion: 所提框架有效缓解了GNN在敏感属性部分缺失条件下的偏见问题，具备实际部署潜力，并将开源代码与数据集以促进后续研究。

Abstract: Graph Neural Networks (GNNs) have demonstrated exceptional efficacy in relational learning tasks, including node classification and link prediction. However, their application raises significant fairness concerns, as GNNs can perpetuate and even amplify societal biases against protected groups defined by sensitive attributes such as race or gender. These biases are often inherent in the node features, structural topology, and message-passing mechanisms of the graph itself. A critical limitation of existing fairness-aware GNN methods is their reliance on the strong assumption that sensitive attributes are fully available for all nodes during training--a condition that poses a practical impediment due to privacy concerns and data collection constraints. To address this gap, we propose a novel, model-agnostic fairness regularization framework designed for the realistic scenario where sensitive attributes are only partially available. Our approach formalizes a fairness-aware objective function that integrates both equal opportunity and statistical parity as differentiable regularization terms. Through a comprehensive empirical evaluation across five real-world benchmark datasets, we demonstrate that the proposed method significantly mitigates bias across key fairness metrics while maintaining competitive node classification performance. Results show that our framework consistently outperforms baseline models in achieving a favorable fairness-accuracy trade-off, with minimal degradation in predictive accuracy. The datasets and source code will be publicly released at https://github.com/mtavassoli/GNN-FC.

</details>


### [174] [Risk-Entropic Flow Matching](https://arxiv.org/abs/2512.03078)
*Vahid R. Ramezani,Benjamin Englard*

Main category: cs.LG

TL;DR: 本文提出将倾斜（熵）风险应用于流匹配（FM），通过在条件FM损失上应用对数-指数变换，得到一种自然的条件熵FM目标上界，并推导出协方差预处理与偏斜尾项两个可解释的一阶修正，从而更好捕捉数据流形的几何结构和少数分支。


<details>
  <summary>Details</summary>
Motivation: 标准矩形FM使用均方误差损失，仅学习条件均值，忽略高阶条件信息（如方差、偏度、多模态），导致无法刻画数据流形的精细几何结构和少数分支。

Method: 对条件FM损失应用标准风险敏感（对数-指数）变换，构建倾斜风险损失；理论证明其为条件熵FM目标的自然上界；通过小阶展开其梯度，导出协方差预处理和偏斜尾项两个修正项。

Result: 在探测模糊性和尾部特性的合成数据上，所提风险敏感损失相比标准矩形FM，在统计指标和几何结构恢复方面均有提升。

Conclusion: 倾斜风险为流匹配提供了更丰富的条件建模能力，能显式利用高阶条件统计信息，提升对复杂数据分布（尤其是多模态和稀疏结构）的建模精度。

Abstract: Tilted (entropic) risk, obtained by applying a log-exponential transform to a base loss, is a well established tool in statistics and machine learning for emphasizing rare or high loss events while retaining a tractable optimization problem. In this work, our aim is to interpret its structure for Flow Matching (FM). FM learns a velocity field that transports samples from a simple source distribution to data by integrating an ODE. In rectified FM, training pairs are obtained by linearly interpolating between a source sample and a data sample, and a neural velocity field is trained to predict the straight line displacement using a mean squared error loss. This squared loss collapses all velocity targets that reach the same space-time point into a single conditional mean, thereby ignoring higher order conditional information (variance, skewness, multi-modality) that encodes fine geometric structure about the data manifold and minority branches. We apply the standard risk-sensitive (log-exponential) transform to the conditional FM loss and show that the resulting tilted risk loss is a natural upper-bound on a meaningful conditional entropic FM objective defined at each space-time point. Furthermore, we show that a small order expansion of the gradient of this conditional entropic objective yields two interpretable first order corrections: covariance preconditioning of the FM residual, and a skew tail term that favors asymmetric or rare branches. On synthetic data designed to probe ambiguity and tails, the resulting risk-sensitive loss improves statistical metrics and recovers geometric structure more faithfully than standard rectified FM.

</details>


### [175] [ALARM: Automated MLLM-Based Anomaly Detection in Complex-EnviRonment Monitoring with Uncertainty Quantification](https://arxiv.org/abs/2512.03101)
*Congjing Zhang,Feng Lin,Xinyi Zhao,Pei Guo,Wei Li,Lin Chen,Chaoyue Zhao,Shuai Huang*

Main category: cs.LG

TL;DR: 本文提出了一种支持不确定性量化（UQ）的多模态大语言模型（MLLM）视觉异常检测（VAD）框架ALARM，融合推理链、自反思与MLLM集成等质量保障技术，并在真实智能家居与伤口图像数据上验证了其鲁棒性与跨领域适用性。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，视觉异常往往具有高度上下文依赖性和模糊性，因此不确定性量化（UQ）对MLLM-based VAD系统至关重要。

Method: 提出ALARM框架，将UQ与推理链、自反思、MLLM集成等质量保障技术结合，并基于严格的概率推理流程和计算过程进行设计。

Result: 在真实世界智能家庭基准数据和伤口图像分类数据上的大量实验证明ALARM具有优越性能和跨领域通用性。

Conclusion: ALARM通过引入UQ与多种质量保障机制，显著提升了MLLM-based VAD系统在复杂、模糊场景下的鲁棒性与可靠性，具备实际部署潜力。

Abstract: The advance of Large Language Models (LLMs) has greatly stimulated research interest in developing multi-modal LLM (MLLM)-based visual anomaly detection (VAD) algorithms that can be deployed in complex environments. The challenge is that in these complex environments, the anomalies are sometimes highly contextual and also ambiguous, and thereby, uncertainty quantification (UQ) is a crucial capacity for an MLLM-based VAD system to succeed. In this paper, we introduce our UQ-supported MLLM-based VAD framework called ALARM. ALARM integrates UQ with quality-assurance techniques like reasoning chain, self-reflection, and MLLM ensemble for robust and accurate performance and is designed based on a rigorous probabilistic inference pipeline and computational process. Extensive empirical evaluations are conducted using the real-world smart-home benchmark data and wound image classification data, which shows ALARM's superior performance and its generic applicability across different domains for reliable decision-making.

</details>


### [176] [Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration](https://arxiv.org/abs/2512.03102)
*Yiwei Shi,Hongnan Ma,Mengyue Yang,Cunjia Liu,Weiru Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散驱动的贝叶斯探索框架（DEPF），用于实时修正紧急响应等高风险场景中因初始状态估计偏差导致的后验支持锁定问题（S-PSI），通过熵正则化采样与协方差缩放扩散扩展后验支撑集，并以Metropolis-Hastings检验保障推理自适应性，在气体定位任务中显著优于传统SMC扰动与RL方法。


<details>
  <summary>Details</summary>
Motivation: 早期状态估计常因信息有限或有偏而严重失真，导致后续决策失效甚至灾难性后果；经典bootstrap粒子滤波在静态假设下存在后验支撑不变性（S-PSI），使初始先验排除的区域永久不可探索，无法自我纠正。

Method: 提出扩散驱动的贝叶斯探索框架（DEPF）：结合熵正则化采样与协方差缩放扩散以扩展后验支撑集，并引入Metropolis-Hastings校验机制确保对异常新证据的自适应推理。

Result: 在真实感强的危险气体定位任务中，当先验正确时性能媲美强化学习与规划基线；当先验失准时，显著优于经典SMC扰动和RL方法；理论证明DEPF可消除S-PSI并保持统计严谨性。

Conclusion: DEPF为高风险动态环境中鲁棒、可修正的早期状态估计提供了新范式，兼具理论保证与实证优势。

Abstract: In emergency response and other high-stakes societal applications, early-stage state estimates critically shape downstream outcomes. Yet, these initial state estimates-often based on limited or biased information-can be severely misaligned with reality, constraining subsequent actions and potentially causing catastrophic delays, resource misallocation, and human harm. Under the stationary bootstrap baseline (zero transition and no rejuvenation), bootstrap particle filters exhibit Stationarity-Induced Posterior Support Invariance (S-PSI), wherein regions excluded by the initial prior remain permanently unexplorable, making corrections impossible even when new evidence contradicts current beliefs. While classical perturbations can in principle break this lock-in, they operate in an always-on fashion and may be inefficient. To overcome this, we propose a diffusion-driven Bayesian exploration framework that enables principled, real-time correction of early state estimation errors. Our method expands posterior support via entropy-regularized sampling and covariance-scaled diffusion. A Metropolis-Hastings check validates proposals and keeps inference adaptive to unexpected evidence. Empirical evaluations on realistic hazardous-gas localization tasks show that our approach matches reinforcement learning and planning baselines when priors are correct. It substantially outperforms classical SMC perturbations and RL-based methods under misalignment, and we provide theoretical guarantees that DEPF resolves S-PSI while maintaining statistical rigor.

</details>


### [177] [Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%](https://arxiv.org/abs/2512.03107)
*Mainak Singha*

Main category: cs.LG

TL;DR: 本文提出ECLIPSE框架，通过联合建模语义熵与证据容量来检测大语言模型的幻觉，理论证明其目标函数严格凸且有唯一稳定最优解，在金融问答数据集上显著优于基线方法，并验证其依赖于校准的词元级不确定性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）常生成流利但缺乏依据的回答（即幻觉），限制其在高风险场景中的安全部署。

Method: ECLIPSE框架将幻觉建模为模型语义熵与可用证据容量之间的不匹配；结合多样本聚类估计语义熵，以及一种新颖的困惑度分解方法来衡量模型对检索证据的利用程度；并从理论上证明熵-容量目标函数在温和条件下严格凸且存在唯一稳定最优解。

Result: 在可控金融问答数据集（含200个带合成幻觉的平衡样本）上，ECLIPSE在GPT-3.5-turbo上达到ROC AUC 0.89、平均精度0.90，显著优于仅用语义熵的基线（AUC 0.50）；在缺乏词元级对数概率的Claude-3-Haiku上的消融实验显示AUC降至0.59、系数幅度下降95%，证实ECLIPSE是依赖词元级不确定性的原生机制；困惑度分解特征具有最大学习系数，表明证据利用是幻觉检测的核心。

Conclusion: ECLIPSE是一种受控机制研究，通过熵-容量匹配有效检测幻觉，其性能高度依赖校准的词元级不确定性；未来工作需在更广领域及真实幻觉场景中进一步验证。

Abstract: Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.

</details>


### [178] [E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing](https://arxiv.org/abs/2512.03109)
*Shuvom Sadhuka,Drew Prinster,Clara Fannjiang,Gabriele Scalia,Aviv Regev,Hanchen Wang*

Main category: cs.LG

TL;DR: 本文提出e-valuator方法，将黑盒验证器分数转化为具有可证明误报率控制的决策规则，通过e-processes实现对智能体轨迹的在线、逐步统计有效监控。


<details>
  <summary>Details</summary>
Motivation: 现有基于启发式评分（如LLM裁判）的智能体轨迹评估方法缺乏正确性保证，无法可靠判断轨迹是否最终成功。

Method: 将成功与失败轨迹的区分建模为序贯假设检验问题，利用e-processes理论构建在每一步都保持统计有效性（即严格控制误报率）的在线检验方法。

Result: 在六个数据集和三个智能体上，e-valuator相比其他策略展现出更高的统计功效和更优的误报率控制；并能快速中止问题轨迹以节省计算资源（token）。

Conclusion: e-valuator是一种轻量级、模型无关的框架，能将任意启发式验证器转化为具备统计保证的决策规则，提升智能体系统的可靠性与可部署性。

Abstract: Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.

</details>


### [179] [Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability](https://arxiv.org/abs/2512.03112)
*Jialai She*

Main category: cs.LG

TL;DR: 本文提出Sparse Isotonic Shapley Regression (SISR)，一种统一的非线性解释框架，通过学习单调变换恢复可加性，并施加L0稀疏约束，以解决Shapley值在非加性设定和高维稀疏解释中的两大挑战。


<details>
  <summary>Details</summary>
Motivation: Shapley值作为可解释AI的金标准面临两个主要问题：一是其经典框架假设收益函数可加，但现实中常因非高斯分布、重尾、特征依赖或领域特定损失尺度而违反该假设；二是高维下先计算稠密Shapley值再阈值截断以实现稀疏解释，计算昂贵且不一致。

Method: 提出Sparse Isotonic Shapley Regression (SISR)，联合学习一个单调变换（恢复可加性）和带L0约束的稀疏Shapley向量；优化采用Pool-Adjacent-Violators算法进行等张回归，结合归一化硬阈值选择支撑集。

Result: 理论分析表明SISR能在多种场景下恢复真实变换，并在高噪声下仍具强支撑恢复能力；实验显示其在回归、逻辑回归和树集成中稳定归因、准确过滤无关特征，而标准Shapley值易出现排序与符号严重失真。

Conclusion: SISR首次将非线性变换估计与稀疏性追求统一，为非线性可解释性提供了兼具理论严谨性与实用性的归因新范式。

Abstract: Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.

</details>


### [180] [Temporal Graph Neural Networks for Early Anomaly Detection and Performance Prediction via PV System Monitoring Data](https://arxiv.org/abs/2512.03114)
*Srijani Mukherjee,Laurent Vuillon,Liliane Bou Nassif,Stéphanie Giroux-Julien,Hervé Pabiou,Denys Dutykh,Ionnasis Tsanakas*

Main category: cs.LG

TL;DR: 本文提出了一种基于时序图神经网络（Temporal GNN）的新方法，用于预测光伏系统输出功率并检测异常，利用辐照度、组件与环境温度等参数建模时序图关系，并基于法国里昂屋顶实测数据验证。


<details>
  <summary>Details</summary>
Motivation: 光伏系统快速发展，亟需先进方法进行性能监控和异常检测以保障最优运行。

Method: 提出基于时序图神经网络（Temporal GNN）的模型，构建关键光伏参数（辐照度、组件温度、环境温度）之间的图结构及时序关系，用于功率预测与异常检测。

Result: 模型在法国里昂屋顶实测数据上成功实现了光伏输出功率预测与异常检测。

Conclusion: Temporal GNN能有效建模光伏系统多变量间的动态时空依赖关系，为光伏监控提供新思路。

Abstract: The rapid growth of solar photovoltaic (PV) systems necessitates advanced methods for performance monitoring and anomaly detection to ensure optimal operation. In this study, we propose a novel approach leveraging Temporal Graph Neural Network (Temporal GNN) to predict solar PV output power and detect anomalies using environmental and operational parameters. The proposed model utilizes graph-based temporal relationships among key PV system parameters, including irradiance, module and ambient temperature to predict electrical power output. This study is based on data collected from an outdoor facility located on a rooftop in Lyon (France) including power measurements from a PV module and meteorological parameters.

</details>


### [181] [Real-Time Structural Health Monitoring with Bayesian Neural Networks: Distinguishing Aleatoric and Epistemic Uncertainty for Digital Twin Frameworks](https://arxiv.org/abs/2512.03115)
*Hanbin Cho,Jecheon Yu,Hyeonbin Moon,Jiyoung Yoon,Junhyeong Lee,Giyoung Kim,Jinhyoung Park,Seunghwa Ryu*

Main category: cs.LG

TL;DR: 本文提出了一种结合PCA、贝叶斯神经网络（BNN）与哈密顿蒙特卡洛（HMC）推断的结构健康监测（SHM）框架，用于从稀疏应变片数据重建全场应变分布，并同步量化aleatoric与epistemic两类不确定性，提升数字孪生与风险感知诊断的可信度。


<details>
  <summary>Details</summary>
Motivation: 高价值结构的实时、可靠传感器数据分析对结构健康监测至关重要，但难以获得空间分辨的全场地 aleatoric 和 epistemic 不确定性，制约了可信决策。

Method: 将主成分分析（PCA）降维、贝叶斯神经网络（BNN）建模与哈密顿蒙特卡洛（HMC）推理相结合，将稀疏应变测量映射到主导PCA模态，实现带不确定性量化的全场应变重建。

Result: 在含不同裂纹长度的CFRP试件四点弯曲循环试验中验证，应变场重建R² > 0.9，并实时输出两类不确定性场；BNN能稳健处理含噪声与应变奇异性实验数据。

Conclusion: 该框架可区分局部低置信区域源于数据固有噪声还是模型局限，支撑可靠决策，推动可信数字孪生与风险感知结构诊断的发展。

Abstract: Reliable real-time analysis of sensor data is essential for structural health monitoring (SHM) of high-value assets, yet a major challenge is to obtain spatially resolved full-field aleatoric and epistemic uncertainties for trustworthy decision-making. We present an integrated SHM framework that combines principal component analysis (PCA), a Bayesian neural network (BNN), and Hamiltonian Monte Carlo (HMC) inference, mapping sparse strain gauge measurements onto leading PCA modes to reconstruct full-field strain distributions with uncertainty quantification. The framework was validated through cyclic four-point bending tests on carbon fiber reinforced polymer (CFRP) specimens with varying crack lengths, achieving accurate strain field reconstruction (R squared value > 0.9) while simultaneously producing real-time uncertainty fields. A key contribution is that the BNN yields robust full-field strain reconstructions from noisy experimental data with crack-induced strain singularities, while also providing explicit representations of two complementary uncertainty fields. Considered jointly in full-field form, the aleatoric and epistemic uncertainty fields make it possible to diagnose at a local level, whether low-confidence regions are driven by data-inherent issues or by model-related limitations, thereby supporting reliable decision-making. Collectively, the results demonstrate that the proposed framework advances SHM toward trustworthy digital twin deployment and risk-aware structural diagnostics.

</details>


### [182] [Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models](https://arxiv.org/abs/2512.03125)
*Xiwen Wei,Mustafa Munir,Radu Marculescu*

Main category: cs.LG

TL;DR: 本文提出Modality-Decoupled Experts (MoDE)架构，以解决统一多模态生成模型（UMGMs）在持续学习中面临的模态内与模态间灾难性遗忘问题，通过解耦模态更新和知识蒸馏来缓解梯度冲突并保留已有能力。


<details>
  <summary>Details</summary>
Motivation: 统一多模态生成模型（UMGMs）在持续学习新任务时面临严重的模态内（intra-modal）和模态间（inter-modal）灾难性遗忘，其中模态间遗忘尚未被充分研究。

Method: 提出Modality-Decoupled Experts (MoDE)，一种轻量、可扩展的架构：1）模态解耦以隔离各模态参数更新，缓解梯度冲突；2）结合知识蒸馏防止遗忘并保留预训练能力。

Result: 在多个基准上实验表明，MoDE显著缓解了模态内与模态间遗忘，性能优于现有持续学习基线方法。

Conclusion: MoDE通过显式模态解耦有效避免了模态间干扰，为UMGMs的持续学习提供了新思路，兼具有效性与可扩展性。

Abstract: Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git

</details>


### [183] [Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra](https://arxiv.org/abs/2512.03127)
*Ziyu Xiong,Yichi Zhang,Foyez Alauddin,Chu Xin Cheng,Joon Soo An,Mohammad R. Seyedsayamdost,Ellen D. Zhong*

Main category: cs.LG

TL;DR: ChefNMR 是一个基于原子扩散模型和非等变Transformer架构的端到端框架，仅凭1D NMR谱和分子式即可自动预测未知小分子（尤其是天然产物）结构，准确率超65%。


<details>
  <summary>Details</summary>
Motivation: NMR谱解析目前仍依赖耗时且需高度专业知识的手动过程，亟需自动化方法以加速小分子（特别是天然产物和临床药物）结构解析。

Method: 提出ChefNMR框架，将结构解析建模为条件生成任务，采用基于非等变Transformer的原子扩散模型；构建包含11.1万种天然产物的模拟1D NMR谱数据集用于训练。

Result: 在挑战性天然产物化合物上实现超过65%的结构预测准确率，性能优于现有方法。

Conclusion: ChefNMR显著推动了小分子结构解析的自动化进程，验证了深度学习在加速分子发现中的巨大潜力。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.

</details>


### [184] [Contrastive Deep Learning for Variant Detection in Wastewater Genomic Sequencing](https://arxiv.org/abs/2512.03158)
*Adele Chinda,Richmond Azumah,Hemanth Demakethepalli Venkateswara*

Main category: cs.LG

TL;DR: 本文提出了一种基于VQ-VAE的无监督病毒变异检测框架，用于污水基因组监测，无需参考基因组或变异标签，具有高准确率、强鲁棒性和良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统基于参考基因组的变异检测方法难以应对污水宏基因组数据中的高测序噪声、低病毒覆盖度、读段碎片化及缺乏标注等问题，亟需一种不依赖参考和标签的新型计算方法。

Method: 提出一种扩展的VQ-VAE框架：对k-mer分词后的序列进行无监督训练；引入掩码重建预训练以增强缺失数据鲁棒性；加入对比学习以提升嵌入判别力；采用离散码本表征病毒基因组模式。

Result: 在约10万条SARS-CoV-2污水测序读段上验证：token级平均准确率达99.52%，完整序列精确匹配率达56.33%，码本利用率为19.73%（101/512）；对比微调后，64维和128维嵌入分别使轮廓系数提升35%和42%。

Conclusion: 该无参考、无标签的VQ-VAE框架为污水病毒监测提供了可扩展、可解释、高鲁棒性的新范式，具备直接公共卫生应用价值。

Abstract: Wastewater-based genomic surveillance has emerged as a powerful tool for population-level viral monitoring, offering comprehensive insights into circulating viral variants across entire communities. However, this approach faces significant computational challenges stemming from high sequencing noise, low viral coverage, fragmented reads, and the complete absence of labeled variant annotations. Traditional reference-based variant calling pipelines struggle with novel mutations and require extensive computational resources. We present a comprehensive framework for unsupervised viral variant detection using Vector-Quantized Variational Autoencoders (VQ-VAE) that learns discrete codebooks of genomic patterns from k-mer tokenized sequences without requiring reference genomes or variant labels. Our approach extends the base VQ-VAE architecture with masked reconstruction pretraining for robustness to missing data and contrastive learning for highly discriminative embeddings. Evaluated on SARS-CoV-2 wastewater sequencing data comprising approximately 100,000 reads, our VQ-VAE achieves 99.52% mean token-level accuracy and 56.33% exact sequence match rate while maintaining 19.73% codebook utilization (101 of 512 codes active), demonstrating efficient discrete representation learning. Contrastive fine-tuning with different projection dimensions yields substantial clustering improvements: 64-dimensional embeddings achieve +35% Silhouette score improvement (0.31 to 0.42), while 128-dimensional embeddings achieve +42% improvement (0.31 to 0.44), clearly demonstrating the impact of embedding dimensionality on variant discrimination capability. Our reference-free framework provides a scalable, interpretable approach to genomic surveillance with direct applications to public health monitoring.

</details>


### [185] [Plantain: Plan-Answer Interleaved Reasoning](https://arxiv.org/abs/2512.03176)
*Anthony Liang,Jonathan Berant,Adam Fisch,Abhimanyu Goyal,Kalpesh Krishna,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 本文提出交错推理（IR）方法，让语言模型在思考过程中交替输出中间结果，以减少用户感知延迟并允许早期干预；其中Plantain变体通过首先生成明确的分步计划，进一步提升了数学推理和编程任务的性能（pass@1提升约6%）并大幅缩短首响应时间（减少超60%）。


<details>
  <summary>Details</summary>
Motivation: 解决现有推理模型‘先思考后回答’模式下用户无法获知推理进展、无法及时纠正错误前提所导致的时间浪费与体验不佳问题。

Method: 提出交错推理（IR），即模型在推理过程中交替进行思考与输出中间响应；进一步设计Plantain（Plan-Thought-Answer Interleaving），要求首个中间响应为显式的分步执行计划。

Result: Plantain在多个高难度数学推理与编程基准上实现约6%的pass@1提升，并将首响应时间相对基线减少超60%。

Conclusion: 交错推理，尤其是Plan-first的Plantain策略，能有效提升推理透明度、用户可控性与响应效率，且不牺牲最终答案质量。

Abstract: Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard "think-then-answer" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.

</details>


### [186] [Neighborhood density estimation using space-partitioning based hashing schemes](https://arxiv.org/abs/2512.03187)
*Aashi Jindal*

Main category: cs.LG

TL;DR: 本文提出了FiRE/FiRE.1算法用于单细胞RNA测序数据中的异常检测，以及Enhash方法用于流数据中的概念漂移检测，两者均在效率和准确性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对大规模单细胞RNA测序数据中罕见细胞亚群的快速识别需求，以及流数据中高效准确检测概念漂移的挑战。

Method: FiRE/FiRE.1是一种基于草图（sketching）的异常检测算法；Enhash是一种结合投影哈希的快速、资源高效集成学习器。

Result: FiRE/FiRE.1在异常检测任务中性能优于现有最先进方法；Enhash在多种概念漂移类型下均展现出高时效性和高准确性。

Conclusion: 所提出的两种方法分别在单细胞数据分析和流数据概念漂移检测中实现了效率与性能的显著提升，具有实际应用价值。

Abstract: This work introduces FiRE/FiRE.1, a novel sketching-based algorithm for anomaly detection to quickly identify rare cell sub-populations in large-scale single-cell RNA sequencing data. This method demonstrated superior performance against state-of-the-art techniques. Furthermore, the thesis proposes Enhash, a fast and resource-efficient ensemble learner that uses projection hashing to detect concept drift in streaming data, proving highly competitive in time and accuracy across various drift types.

</details>


### [187] [Scaling Internal-State Policy-Gradient Methods for POMDPs](https://arxiv.org/abs/2512.03204)
*Douglas Aberdeen,Jonathan Baxter*

Main category: cs.LG

TL;DR: 本文提出并比较了多种改进的策略梯度算法，用于在无限时间范围下学习具有记忆能力的策略，适用于部分可观测马尔可夫决策过程（POMDP）问题，尤其在机器人导航和多智能体系统等复杂场景中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在部分可观测环境中虽有潜力，但在需要记忆的场景中表现不佳，亟需提升其处理记忆依赖策略的能力。

Method: 提出了若干改进的策略梯度算法：在环境模型已知时直接优化带记忆策略；在模型未知时通过仿真进行学习。

Result: 在多个大规模POMDP问题（如噪声环境下的机器人导航和多智能体任务）上验证了所提算法的有效性与优越性。

Conclusion: 所提出的带记忆策略学习算法显著提升了策略梯度方法在部分可观测、需记忆任务中的性能，拓展了其适用范围。

Abstract: Policy-gradient methods have received increased attention recently as a mechanism for learning to act in partially observable environments. They have shown promise for problems admitting memoryless policies but have been less successful when memory is required. In this paper we develop several improved algorithms for learning policies with memory in an infinite-horizon setting -- directly when a known model of the environment is available, and via simulation otherwise. We compare these algorithms on some large POMDPs, including noisy robot navigation and multi-agent problems.

</details>


### [188] [A Multi-Agent, Policy-Gradient approach to Network Routing](https://arxiv.org/abs/2512.03211)
*Nigel Tao,Jonathan Baxter,Lex Weaver*

Main category: cs.LG

TL;DR: 本文提出了一种名为OLPOMDP的策略梯度强化学习算法，用于解决分布式网络路由问题，在多个网络模型中成功实现了多路由器协同优化，无需显式通信，并通过奖励塑形显著提升了收敛速度。


<details>
  <summary>Details</summary>
Motivation: 网络路由是一个天然具有数值性能指标（如数据包端到端平均传输时间）的分布式决策问题，需在无中心协调下实现多代理协同优化。

Method: 采用OLPOMDP策略梯度强化学习算法，在模拟网络环境中训练多个分布式路由器代理，通过设计包含显式惩罚的奖励信号进行奖励塑形。

Result: 多个路由器代理在无显式通信条件下学会协作行为，避免个体最优但全局次优的行为；奖励塑形显著加快了学习收敛速度。

Conclusion: OLPOMDP算法适用于分布式网络路由优化，奖励塑形是提升其效率与性能的关键手段。

Abstract: Network routing is a distributed decision problem which naturally admits numerical performance measures, such as the average time for a packet to travel from source to destination. OLPOMDP, a policy-gradient reinforcement learning algorithm, was successfully applied to simulated network routing under a number of network models. Multiple distributed agents (routers) learned co-operative behavior without explicit inter-agent communication, and they avoided behavior which was individually desirable, but detrimental to the group's overall performance. Furthermore, shaping the reward signal by explicitly penalizing certain patterns of sub-optimal behavior was found to dramatically improve the convergence rate.

</details>


### [189] [Perch 2.0 transfers 'whale' to underwater tasks](https://arxiv.org/abs/2512.03219)
*Andrea Burns,Lauren Harrell,Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0 是一个预训练的生物声学基础模型，虽未在海洋哺乳动物数据上训练，但通过少量样本迁移学习，在海洋哺乳动物音频分类任务中表现出色，优于多个现有模型。


<details>
  <summary>Details</summary>
Motivation: 评估Perch 2.0在缺乏海洋哺乳动物训练数据的情况下，能否通过少量样本迁移学习有效应用于海洋哺乳动物和水下音频任务。

Method: 采用线性探针（linear probing）方式，利用Perch 2.0生成的嵌入表示，并与其他开源生物声学预训练模型（如Perch 1.0、SurfPerch、AVES-bio等）进行对比实验。

Result: Perch 2.0的嵌入在多数海洋哺乳动物少样本分类任务中性能 consistently 最优，显著优于其他对比模型。

Conclusion: Perch 2.0是开发海洋哺乳动物少样本分类线性分类器的推荐基础模型。

Abstract: Perch 2.0 is a supervised bioacoustics foundation model pretrained on 14,597 species, including birds, mammals, amphibians, and insects, and has state-of-the-art performance on multiple benchmarks. Given that Perch 2.0 includes almost no marine mammal audio or classes in the training data, we evaluate Perch 2.0 performance on marine mammal and underwater audio tasks through few-shot transfer learning. We perform linear probing with the embeddings generated from this foundation model and compare performance to other pretrained bioacoustics models. In particular, we compare Perch 2.0 with previous multispecies whale, Perch 1.0, SurfPerch, AVES-bio, BirdAVES, and Birdnet V2.3 models, which have open-source tools for transfer-learning and agile modeling. We show that the embeddings from the Perch 2.0 model have consistently high performance for few-shot transfer learning, generally outperforming alternative embedding models on the majority of tasks, and thus is recommended when developing new linear classifiers for marine mammal classification with few labeled examples.

</details>


### [190] [SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning](https://arxiv.org/abs/2512.03244)
*Salman Rahman,Sruthi Gorantla,Arpit Gupta,Swastik Roy,Nanyun Peng,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出SPARK框架，通过三阶段方法（生成-验证-微调）构建无需真实标注的生成式过程奖励模型（PRM），在数学推理任务中超越基于真实答案的监督与强化学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型（PRMs）依赖昂贵的步骤级人工标注或真实参考答案，限制了其广泛应用；尤其在缺乏可验证答案的领域中难以部署。

Method: SPARK框架包含三阶段：1）用生成器与验证器（结合自一致性与元批判）对多样化解进行并行/顺序验证；2）以验证结果为合成标签微调生成式PRM；3）将训练好的PRM-CoT作为奖励模型用于RL训练，并引入格式约束防止奖励作弊。

Result: 在ProcessBench上PRM达67.5 F1（优于参考引导训练66.4和GPT-4o 61.9）；在六项数学推理基准上RL准确率达47.4%，超过RLVR（43.9%）。

Conclusion: SPARK实现了无需真实参考的答案、超越真实监督的RL训练范式，为缺乏可靠ground truth的复杂推理任务提供了新路径。

Abstract: Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.

</details>


### [191] [Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval](https://arxiv.org/abs/2512.03276)
*Constantin Venhoff,Ashkan Khakzar,Sonia Joseph,Philip Torr,Neel Nanda*

Main category: cs.LG

TL;DR: 本文研究了视觉语言模型（VLMs）在基于图像的事实回忆任务中性能下降的问题，发现多数VLMs因视觉实体表征生成过晚，无法有效复用LLM骨干网络中已有的事实回忆机制；通过归因修补、激活修补与探针分析验证该机制，并提出两种恢复性能的方法：将LLM骨干的实体表征注入VLM，以及使用思维链提示。


<details>
  <summary>Details</summary>
Motivation: 许多VLMs在事实回忆任务上表现不如其LLM骨干模型，作者旨在探究多模态微调是否能有效扩展LLM已有机制以支持视觉输入，并揭示其失败的内在机理。

Method: 对14种不同架构、规模和训练方式的VLMs进行事实回忆基准测试；选取高低性能退化代表模型，采用归因修补（attribution patching）、激活修补（activation patching）和探针（probing）分析其内部机制；并尝试两种性能恢复方法：实体表征注入与思维链提示。

Result: 11/14 VLMs出现事实回忆性能下降；退化主因是视觉实体表征生成过晚，导致无法及时激活LLM原有事实回忆回路；早期实体解析速度是决定VLM能否复用LLM机制的关键因素；两种恢复方法均能有效提升性能。

Conclusion: VLMs的事实回忆能力高度依赖于视觉实体表征生成的时效性；机械式分析可系统揭示多模态对齐中的结构性缺陷，并为改进提供可解释路径。

Abstract: Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.

</details>


### [192] [BlendedNet++: A Large-Scale Blended Wing Body Aerodynamics Dataset and Benchmark](https://arxiv.org/abs/2512.03280)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Matthew C. Jones,Faez Ahmed*

Main category: cs.LG

TL;DR: 本文提出了BlendedNet++，一个面向混合翼身（BWB）飞机的大规模气动数据集与基准测试框架，包含12,490个稳态RANS CFD仿真结果及对应的全场气动系数；基于该数据集，作者构建了前向代理模型（六类GNN/GNO/Transformer架构）和逆向设计（条件扩散模型）的标准化基准，并对比了纯扩散、梯度优化及混合方法的性能，旨在推动可复现的全场气动建模与逆设计研究。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习气动代理模型受限于缺乏大规模、场分辨率高的气动数据集，难以实现高精度逐点预测与可复现的逆向设计。

Method: 构建BlendedNet++数据集（12,490个BWB几何体+稳态RANS仿真结果），提供积分力矩系数与密集表面压力/摩阻系数场；建立前向代理基准（GraphSAGE、GraphUNet、PointNet、Transolver式坐标Transformer、FiLMNet、GNOT）；提出基于条件扩散模型的逆向设计任务，并与梯度优化及扩散-优化混合方法对比。

Result: 提供了统一的前向与逆向设计基准协议及多模型基线；验证了不同架构在点对点场预测上的性能差异；展示了条件扩散模型在L/D目标逆设计中的可行性，并发现混合方法优于纯扩散或纯梯度优化。

Conclusion: BlendedNet++填补了高质量、场分辨气动数据集的空白，为气动代理建模与逆设计提供了可复现、公平比较的基准平台，有望推动该领域研究进展。

Abstract: Despite progress in machine learning-based aerodynamic surrogates, the scarcity of large, field-resolved datasets limits progress on accurate pointwise prediction and reproducible inverse design for aircraft. We introduce BlendedNet++, a large-scale aerodynamic dataset and benchmark focused on blended wing body (BWB) aircraft. The dataset contains over 12,000 unique geometries, each simulated at a single flight condition, yielding 12,490 aerodynamic results for steady RANS CFD. For every case, we provide (i) integrated force/moment coefficients CL, CD, CM and (ii) dense surface fields of pressure and skin friction coefficients Cp and (Cfx, Cfy, Cfz). Using this dataset, we standardize a forward-surrogate benchmark to predict pointwise fields across six model families: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), a FiLMNet (coordinate MLP with feature-wise modulation), and a Graph Neural Operator Transformer (GNOT). Finally, we present an inverse design task of achieving a specified lift-to-drag ratio under fixed flight conditions, implemented via a conditional diffusion model. To assess performance, we benchmark this approach against gradient-based optimization on the same surrogate and a diffusion-optimization hybrid that first samples with the conditional diffusion model and then further optimizes the designs. BlendedNet++ provides a unified forward and inverse protocol with multi-model baselines, enabling fair, reproducible comparison across architectures and optimization paradigms. We expect BlendedNet++ to catalyze reproducible research in field-level aerodynamics and inverse design; resources (dataset, splits, baselines, and scripts) will be released upon acceptance.

</details>


### [193] [Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors](https://arxiv.org/abs/2512.03287)
*Dario Fenoglio,Mohan Li,Davide Casnici,Matias Laporte,Shkurta Gashi,Silvia Santini,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: 本文提出了一种面向多频率头戴设备的联邦学习框架（multi-frequency FL），用于隐私保护的人类活动识别（HAR），在两个数据集上优于单频方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于集中式数据的HAR方法存在用户隐私泄露风险，且现有研究多集中于智能手表/手机，头戴设备（如耳机、智能眼镜）这一新兴平台尚未被充分探索；同时，不同设备采样频率不一致，需支持异构频率下的联合建模。

Method: 设计了适用于多频率头戴设备的联邦学习框架，使各设备可在本地以自身采样频率训练模型，并通过服务器端聚合实现跨频率的联合模型学习，兼顾隐私保护与频率异构性。

Result: 在两个公开数据集上，所提方法相比频率特异性基线方法取得性能提升，验证了多频率FL-HAR的有效性；代码已开源。

Conclusion: 多频率联邦学习为头戴式HAR提供了兼顾隐私保护、设备异构性和建模性能的新范式，具有实际应用前景和进一步研究价值。

Abstract: Human Activity Recognition (HAR) benefits various application domains, including health and elderly care. Traditional HAR involves constructing pipelines reliant on centralized user data, which can pose privacy concerns as they necessitate the uploading of user data to a centralized server. This work proposes multi-frequency Federated Learning (FL) to enable: (1) privacy-aware ML; (2) joint ML model learning across devices with varying sampling frequency. We focus on head-worn devices (e.g., earbuds and smart glasses), a relatively unexplored domain compared to traditional smartwatch- or smartphone-based HAR. Results have shown improvements on two datasets against frequency-specific approaches, indicating a promising future in the multi-frequency FL-HAR task. The proposed network's implementation is publicly available for further research and development.

</details>


### [194] [ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics](https://arxiv.org/abs/2512.03290)
*Julian Evan Chrisnanto,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: 本文提出了一种新型物理信息神经网络ASPEN，通过在输入层引入可学习的自适应谱层（傅里叶特征），克服了传统PINN因频谱偏差难以求解刚性、多尺度和强非线性PDE（如复Ginzburg-Landau方程）的瓶颈；实验表明ASPEN显著优于标准PINN，在CGLE上实现高精度、物理一致的求解。


<details>
  <summary>Details</summary>
Motivation: 传统PINN受限于MLP的频谱偏差，难以有效表征高频成分，导致在求解刚性、多尺度和强非线性偏微分方程（如CGLE）时失败。

Method: 提出ASPEN架构，将可学习的自适应谱层（含动态调谐的傅里叶特征）嵌入网络输入端，使模型能在训练中自主调整谱基以匹配解的频率特性。

Result: ASPEN成功求解复Ginzburg-Landau方程，预测解与高分辨率真值视觉不可区分，中位物理残差低至5.10×10⁻³，并准确再现自由能快速弛豫和畴壁前沿长期稳定性等物理特性；而标准PINN完全发散。

Conclusion: 引入自适应谱机制可显著提升PINN对复杂动力学系统的建模能力，ASPEN为物理信息机器学习在强非线性、刚性系统中的应用提供了鲁棒且物理解一致的新范式。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.

</details>


### [195] [Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction](https://arxiv.org/abs/2512.03298)
*Echo Diyun LU,Charles Findling,Marianne Clausel,Alessandro Leite,Wei Gong,Pierric Kersaudy*

Main category: cs.LG

TL;DR: 本文提出了一种结合深度切换状态空间模型与自适应共形推断（ACI）及其聚合变体（AgACI）的方法，用于非平稳时间序列的分布无关不确定性预测，并设计了统一的共形包装器，适配多种强序列模型，在非平稳和模型误设下提供在线预测区间及有限样本边际保证。


<details>
  <summary>Details</summary>
Motivation: 时间序列中频繁发生的机制转换破坏了平稳性，使得校准不确定性与点预测精度同等重要。

Method: 将深度切换状态空间模型与自适应共形推断（ACI）及其聚合变体（AgACI）相结合，并设计一个统一的共形包装器，适配S4、MC-Dropout GRU、稀疏高斯过程和变点局部模型等强序列基线模型。

Result: 在合成与真实数据集上，共形化预测器实现了接近标称水平的覆盖率，同时保持竞争力的预测精度和普遍更优的区间效率。

Conclusion: 所提方法在非平稳和模型误设条件下，能有效提供具有有限样本理论保证的在线预测不确定性量化。

Abstract: Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.

</details>


### [196] [HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction](https://arxiv.org/abs/2512.03300)
*Pengfei Hu,Fan Ming,Xiaoxue Han,Chang Lu,Yue Ning,Dan Lu*

Main category: cs.LG

TL;DR: 本文提出HydroDCM框架，通过利用水库空间元数据构建伪域标签，结合对抗学习提取时序不变特征，并在推理阶段使用轻量级条件层进行位置自适应，有效解决了水文领域跨水库径流预测中的多域分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在水库入库流量预测中表现良好，但在不同水库间迁移时因分布差异（域偏移）导致性能下降；传统域泛化方法难以适应水文系统中各水库独特且受空间元数据间接影响的流入模式。

Method: 提出HydroDCM：1）利用水库空间元数据生成伪域标签；2）基于伪标签开展对抗学习以提取域不变时序特征；3）在推理阶段引入轻量级条件层，依据目标水库元数据动态调节特征，实现泛化性与局部适应性的统一。

Result: 在科罗拉多河上游流域30个真实水库上的实验表明，HydroDCM显著优于现有域泛化基线方法，尤其在多域场景下，并保持计算高效性。

Conclusion: HydroDCM为水文多域预测提供了一种可扩展、兼具域不变性与位置自适应能力的新范式，提升了跨水库流量预测的鲁棒性与实用性。

Abstract: Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.

</details>


### [197] [Robust Tabular Foundation Models](https://arxiv.org/abs/2512.03307)
*Matthew Peroni,Franck Le,Vadim Sheinin*

Main category: cs.LG

TL;DR: 本文提出了一种面向表格基础模型（TFM）的对抗训练框架RTFM，通过参数化合成数据生成器，在预训练中动态生成对当前模型最具挑战性的数据，以缩小其与强传统模型（如XGBoost）之间的性能差距，从而提升鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有TFM预训练多依赖高质量合成数据先验，但忽视了利用生成器的可学习性进行针对性对抗优化；作者认为，将生成器参数化后可从对抗鲁棒性视角出发，主动生成使模型表现最差的数据，以驱动其提升。

Method: 提出优化间隙（optimality gap）度量——即TFM性能与XGBoost等强基线估计的最优性能之差；基于此构建模型无关的对抗训练框架RTFM，联合优化TFM参数与合成数据生成器参数，使生成器偏向产出难样本。

Result: 在TabPFN V2上验证RTFM，平均归一化AUC较原模型及其他基线提升最高达6%，仅需额外不到10万合成数据集。

Conclusion: 参数化合成数据生成器支持高效、靶向的对抗训练，为纯合成数据驱动的TFM鲁棒性增强与微调提供了新范式。

Abstract: The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.

</details>


### [198] [Retrofitting Earth System Models with Cadence-Limited Neural Operator Updates](https://arxiv.org/abs/2512.03309)
*Aniruddha Bora,Shixuan Zhang,Khemraj Shukla,Bryce Harrop,George Em. Karniadakis,L. Ruby Leung*

Main category: cs.LG

TL;DR: 本文提出了一种基于算子学习的在线偏差校正框架，通过U-Net变体（IUNet和M&M）将地球系统模型（E3SM）的瞬时状态映射为偏差校正趋势，在线集成中提升预测精度与稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统数据同化方法在自由运行阶段对地球系统模型（ESM）偏差校正效果有限；而ESM受限于粗分辨率、参数化不完善及初值/强迫不确定性，亟需一种可在线应用、稳定且可扩展的偏差校正方法。

Method: 构建基于U-Net的两类算子学习架构——Inception U-Net（IUNet）和多尺度网络（M&M），融合多样化上采样策略与感受野以捕获多尺度非线性特征；在两年E3SM-ERA5 nudged数据上训练，并在线耦合至E3SM进行混合模拟。

Result: 两种架构均优于标准U-Net基线；M&M在网络变量和垂直层次上实现最一致的偏差降低；ML增强配置在多年模拟中保持数值稳定且计算可行。

Conclusion: 该框架强调长期稳定性、可移植性与低频更新，验证了高表达力ML算子在建模跨尺度结构关系及改造现有ESM方面的实用性，为可扩展混合建模提供了可行路径。

Abstract: Coarse resolution, imperfect parameterizations, and uncertain initial states and forcings limit Earth-system model (ESM) predictions. Traditional bias correction via data assimilation improves constrained simulations but offers limited benefit once models run freely. We introduce an operator-learning framework that maps instantaneous model states to bias-correction tendencies and applies them online during integration. Building on a U-Net backbone, we develop two operator architectures Inception U-Net (IUNet) and a multi-scale network (M\&M) that combine diverse upsampling and receptive fields to capture multiscale nonlinear features under Energy Exascale Earth System Model (E3SM) runtime constraints. Trained on two years E3SM simulations nudged toward ERA5 reanalysis, the operators generalize across height levels and seasons. Both architectures outperform standard U-Net baselines in offline tests, indicating that functional richness rather than parameter count drives performance. In online hybrid E3SM runs, M\&M delivers the most consistent bias reductions across variables and vertical levels. The ML-augmented configurations remain stable and computationally feasible in multi-year simulations, providing a practical pathway for scalable hybrid modeling. Our framework emphasizes long-term stability, portability, and cadence-limited updates, demonstrating the utility of expressive ML operators for learning structured, cross-scale relationships and retrofitting legacy ESMs.

</details>


### [199] [Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs](https://arxiv.org/abs/2512.03324)
*Ngoc Bui,Shubham Sharma,Simran Lamba,Saumitra Mishra,Rex Ying*

Main category: cs.LG

TL;DR: 本文提出TRIM-KV方法，通过在token生成时学习其内在重要性（轻量级保留门预测随时间衰减的保留分），动态裁剪KV缓存，在低内存下显著提升长程LLM推理性能，并兼具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法（如量化、卸载、启发式驱逐）存在调度开销高或依赖不可靠注意力代理的问题；而长程LLM推理受限于自注意力的二次复杂度和不断增长的KV缓存。

Method: 提出TRIM-KV：为每个token在生成时引入轻量级保留门，预测其在特定层/头下的长期效用得分（随时间衰减）；内存超限时按得分驱逐低分token；通过蒸馏+容量损失进行高效训练，仅微调门参数。

Result: 在数学推理（GSM8K等）、程序生成（LongProc）、长记忆对话（LongMemEval）及长文本理解（LongBench等）多个基准上，TRIM-KV在低内存下持续优于强基线，甚至在部分场景超越全缓存模型；定性分析显示保留分符合人类直觉，隐式恢复sink token、滑动窗口、主旨压缩等启发式策略。

Conclusion: TRIM-KV实现了高效、自适应、可解释的KV缓存管理；选择性保留不仅提升效率，还起正则化作用抑制噪声；保留分本身揭示了层/头特异性功能，为LLM可解释性提供新路径。

Abstract: Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.

</details>


### [200] [Single-Round Scalable Analytic Federated Learning](https://arxiv.org/abs/2512.03336)
*Alan T. L. Bacellar,Mustafa Munir,Felipe M. G. França,Priscila M. V. Lima,Radu Marculescu,Lizy K. John*

Main category: cs.LG

TL;DR: 本文提出SAFLe框架，通过结构化头部和稀疏分组嵌入实现非线性表达能力，并证明其数学等价于高维线性回归，从而支持单轮、数据分布无关的聚合，在联邦视觉任务中显著提升精度与效率。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中通信开销大和非独立同分布（non-IID）数据下性能下降两大挑战，突破现有Analytic FL仅限于线性模型或非线性方法需多轮迭代的局限。

Method: 提出SAFLe框架，引入桶化特征的结构化头部与稀疏分组嵌入；理论证明该非线性架构等价于高维线性回归，从而可沿用AFL的单轮、分布不变聚合机制。

Result: 在多个基准上显著超越线性AFL和多轮DeepAFL，建立分析型联邦学习新SOTA，验证了其高效性与可扩展性。

Conclusion: SAFLe成功打破非线性建模与单轮聚合之间的权衡，为联邦视觉任务提供兼具高精度、低通信与数据分布鲁棒性的新范式。

Abstract: Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.

</details>


### [201] [Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions](https://arxiv.org/abs/2512.03354)
*Hongseon Yeom,Jaeyoul Shin,Soojin Min,Jeongmin Yoon,Seunghak Yu,Dongyeop Kang*

Main category: cs.LG

TL;DR: 本文提出了一种面向确定性广告拍卖环境的离线策略评估（OPE）新框架，通过重用出价分布模型来近似倾向得分，使SNIPS等稳定估计器可用于反事实评估，并在仿真和真实A/B测试中验证了其高方向准确性（92% MDA）。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试成本高、风险大；而标准OPE方法在广告拍卖这种确定性（winner-takes-all）、非赢广告曝光概率为零的场景下失效，亟需适配OPE方案。

Method: 利用出价分布（bid landscape）模型近似构建倾向得分，进而支持Self-Normalized Inverse Propensity Scoring（SNIPS）等鲁棒离线评估方法。

Result: 在AuctionNet仿真和工业级两周线上A/B测试中验证，CTR预测的平均方向准确率（MDA）达92%，显著优于参数化基线。

Conclusion: 首次提出了适用于确定性广告拍卖的实用、可验证的OPE框架，为替代高成本、高风险的线上实验提供了可靠高效的离线评估方案。

Abstract: Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.

</details>


### [202] [A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning](https://arxiv.org/abs/2512.03363)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 本文提出A2G（双增益自适应聚合）框架，通过几何增益和QoS增益联合优化联邦学习在量子-经典混合网络中的模型聚合，提升异构与噪声环境下的稳定性与精度。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习聚合规则基于欧氏拓扑和均匀通信假设，难以适配量子使能、设备异构、信道随机（如量子隐形传态保真度波动）、设备不稳定及局部-全局模型几何失配等现实挑战。

Method: 提出A2G双增益框架：几何增益调控模型在流形上的加权融合，QoS增益依据隐形传态保真度、时延和设备不稳定性动态评估客户端重要性；推导A2G更新规则，并在平滑性和有界方差假设下证明收敛性。

Result: 理论证明A2G可退化为FedAvg、QoS感知平均及流形聚合等特例；在量子-经典混合实验平台上验证其在异构与噪声条件下相比基线方法具有更高准确率和更强稳定性。

Conclusion: A2G为量子联邦学习提供了首个兼顾几何结构与服务质量的自适应聚合范式，显著缓解了网络与设备异构性带来的性能退化问题。

Abstract: Federated learning (FL) deployed over quantum enabled and heterogeneous classical networks faces significant performance degradation due to uneven client quality, stochastic teleportation fidelity, device instability, and geometric mismatch between local and global models. Classical aggregation rules assume euclidean topology and uniform communication reliability, limiting their suitability for emerging quantum federated systems. This paper introduces A2G (Adaptive Aggregation with Two Gains), a dual gain framework that jointly regulates geometric blending through a geometry gain and modulates client importance using a QoS gain derived from teleportation fidelity, latency, and instability. We develop the A2G update rule, establish convergence guarantees under smoothness and bounded variance assumptions, and show that A2G recovers FedAvg, QoS aware averaging, and manifold based aggregation as special cases. Experiments on a quantum classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions.

</details>


### [203] [MAGE-ID: A Multimodal Generative Framework for Intrusion Detection Systems](https://arxiv.org/abs/2512.03375)
*Mahdi Arab Loodaricheh,Mohammad Hossein Manshaei,Anita Raja*

Main category: cs.LG

TL;DR: This paper proposes MAGE-ID, a diffusion-based multimodal generative framework that jointly synthesizes tabular network flow data and their corresponding images to address data imbalance in intrusion detection systems.


<details>
  <summary>Details</summary>
Motivation: Modern IDS face challenges from heterogeneous traffic, evolving threats, and severe data imbalance between benign and attack flows; existing generative models are limited to single modalities and cannot capture cross-domain dependencies.

Method: MAGE-ID is a diffusion-based generative framework that couples tabular flow features with their transformed images via a unified latent prior, using jointly trained Transformer and CNN-based variational encoders with an EDM-style denoiser.

Result: MAGE-ID achieves significant improvements in synthesis fidelity, diversity, and downstream intrusion detection performance over TabSyn and TabDDPM on CIC-IDS-2017 and NSL-KDD datasets.

Conclusion: Multimodal generative modeling—specifically coupling tabular and image representations through a unified diffusion framework—is effective for balanced and coherent data augmentation in intrusion detection.

Abstract: Modern Intrusion Detection Systems (IDS) face severe challenges due to heterogeneous network traffic, evolving cyber threats, and pronounced data imbalance between benign and attack flows. While generative models have shown promise in data augmentation, existing approaches are limited to single modalities and fail to capture cross-domain dependencies. This paper introduces MAGE-ID (Multimodal Attack Generator for Intrusion Detection), a diffusion-based generative framework that couples tabular flow features with their transformed images through a unified latent prior. By jointly training Transformer and CNN-based variational encoders with an EDM style denoiser, MAGE-ID achieves balanced and coherent multimodal synthesis. Evaluations on CIC-IDS-2017 and NSL-KDD demonstrate significant improvements in fidelity, diversity, and downstream detection performance over TabSyn and TabDDPM, highlighting the effectiveness of MAGE-ID for multimodal IDS augmentation.

</details>


### [204] [UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs](https://arxiv.org/abs/2512.03383)
*Hung-Yueh Chiang,Chi-Chih Chang,Yu-Chen Lu,Chien-Yu Lin,Kai-Chiang Wu,Mohamed S. Abdelfattah,Diana Marculescu*

Main category: cs.LG

TL;DR: UniQL 是一种统一的后训练量化与低秩压缩框架，支持在边缘设备上对 LLM（包括 Transformer、SSM 和混合模型）进行可配置剪枝，显著降低内存占用并提升推理速度，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 移动设备内存和计算资源有限，且受实时负载影响，导致大语言模型部署困难且不确定性高。

Method: 提出 UniQL 框架，融合量化与低秩压缩；引入结构化权重重排序、量化感知 SVD、SSM 状态感知排序、剪枝模型融合 RoPE 内核；云上单次完成权重排序、微调与量化，设备端支持最高 35% 可配置剪枝率。

Result: 量化与剪枝后模型内存减少 4x–5.7x，token 吞吐量提升 2.7x–3.4x，在 15% 剪枝率下精度损失控制在 5% 以内（覆盖 Llama3、Qwen2.5、Mamba2、Nemotron-H、Bamba-v2）。

Conclusion: UniQL 是一种高效、通用、可部署的边缘 LLM 压缩框架，兼顾压缩率、速度与精度，适用于多种前沿模型架构。

Abstract: Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.

</details>


### [205] [Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization](https://arxiv.org/abs/2512.03393)
*Lakshmi Jayalal,Sheetal Kalyani*

Main category: cs.LG

TL;DR: 本文提出了一种无需调参的隐式正则化框架，用于求解多测量向量（MMV）下的联合稀疏信号恢复问题，通过因子化重参数和梯度下降的内在动力学实现自动行稀疏性选择。


<details>
  <summary>Details</summary>
Motivation: 传统MMV稀疏恢复方法（如M-OMP、M-FOCUSS）依赖稀疏度或噪声方差等先验信息，需精细调参，限制了实用性。

Method: 将估计矩阵因子化以解耦共享行支撑与各向量分量；在标准最小二乘目标上对因子施加梯度下降；利用过参数化带来的隐式正则化效应。

Result: 理论证明：小而平衡的初始化下，优化过程产生‘类动量’效应，真支撑行范数增长更快，保证收敛至理想行稀疏解；实验表明性能媲美经典方法，且完全免调参。

Conclusion: 隐式正则化可替代显式稀疏先验，在MMV问题中实现鲁棒、自适应、免调参的联合稀疏恢复。

Abstract: Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a "momentum-like" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.

</details>


### [206] [VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing](https://arxiv.org/abs/2512.03394)
*Hamed Poursiami,Shay Snyder,Guojing Cong,Thomas Potok,Maryam Parsa*

Main category: cs.LG

TL;DR: VS-Graph 是一种基于超维计算（HDC）的轻量级图学习框架，通过 Spike Diffusion 和 Associative Message Passing 在高维向量空间中实现拓扑感知与多跳聚合，无需梯度优化即可达到接近现代 GNN 的精度，训练速度提升最高达 450 倍，并在低维（D=128）下保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNN）虽性能强但计算开销大，难以部署于资源受限设备；而超维计算（HDC）方法虽高效却表达能力不足，亟需兼顾效率与表现力的新框架。

Method: 提出 VS-Graph 框架：1）Spike Diffusion 机制用于拓扑驱动的节点标识；2）Associative Message Passing 方案在高维向量空间内完成多跳邻域聚合；全程无梯度下降或反向传播。

Result: 在 MUTAG 和 DD 等基准上较前序 HDC 方法提升 4–5% 准确率；在多个数据集上媲美或超越 GNN 基线；训练加速最高达 450 倍；在 D=128 超低维下仍保持高精度。

Conclusion: VS-Graph 成功弥合了 HDC 的效率优势与 GNN 的表达能力鸿沟，为边缘与类脑硬件上的高效图学习提供了新范式。

Abstract: Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.

</details>


### [207] [Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value](https://arxiv.org/abs/2512.03399)
*Joe Edelman,Tan Zhi-Xuan,Ryan Lowe,Oliver Klingefjord,Vincent Wang-Mascianica,Matija Franklin,Ryan Othniel Kearns,Ellie Hain,Atrisha Sarkar,Michiel Bakker,Fazl Barez,David Duvenaud,Jakob Foerster,Iason Gabriel,Joseph Gubbels,Bryce Goodman,Andreas Haupt,Jobst Heitzig,Julian Jara-Ettinger,Atoosa Kasirzadeh,James Ravi Kirkpatrick,Andrew Koh,W. Bradley Knox,Philipp Koralus,Joel Lehman,Sydney Levine,Samuele Marro,Manon Revel,Toby Shorin,Morgan Sutherland,Michael Henry Tessler,Ivan Vendrov,James Wilken-Smith*

Main category: cs.LG

TL;DR: 本文提出“全栈对齐”概念，强调不仅需对齐AI系统与其操作者意图，还需同步对齐塑造AI的制度与人类所珍视的价值；指出当前价值表征方式（如效用函数、偏好排序）存在局限，主张采用结构化的“厚价值模型”，以区分持久价值与临时偏好、建模社会语境并支持规范性推理，并在五个领域展示其应用。


<details>
  <summary>Details</summary>
Motivation: 现有AI对齐方法仅关注个体系统与操作者意图的一致性，忽视了组织目标可能与其他机构及个体价值冲突的问题，因而无法保障有益的社会结果；需一种能同时对齐AI系统及其制度环境的价值框架。

Method: 提出“厚价值模型”作为新型价值表征范式，强调结构化建模：区分持久价值与短暂偏好、嵌入社会语境、支持规范性推理；并在AI价值治理、规范胜任型智能体、共赢谈判系统、意义保持型经济机制和民主监管制度五个方向开展概念性验证。

Result: 论证了传统价值表征（效用函数、偏好排序、非结构化文本）在区分价值信号、支撑规范推理和建模集体善方面的根本不足；确立厚模型为实现全栈对齐的必要路径，并给出跨领域的可行性论证。

Conclusion: 实现真正有益的AI社会影响，必须推进全栈对齐——即AI系统与制度双重对齐于人类价值；厚价值模型是克服当前表征局限、支撑该目标的关键基础设施。

Abstract: Beneficial societal outcomes cannot be guaranteed by aligning individual AI systems with the intentions of their operators or users. Even an AI system that is perfectly aligned to the intentions of its operating organization can lead to bad outcomes if the goals of that organization are misaligned with those of other institutions and individuals. For this reason, we need full-stack alignment, the concurrent alignment of AI systems and the institutions that shape them with what people value. This can be done without imposing a particular vision of individual or collective flourishing. We argue that current approaches for representing values, such as utility functions, preference orderings, or unstructured text, struggle to address these and other issues effectively. They struggle to distinguish values from other signals, to support principled normative reasoning, and to model collective goods. We propose thick models of value will be needed. These structure the way values and norms are represented, enabling systems to distinguish enduring values from fleeting preferences, to model the social embedding of individual choices, and to reason normatively, applying values in new domains. We demonstrate this approach in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions.

</details>


### [208] [Better World Models Can Lead to Better Post-Training Performance](https://arxiv.org/abs/2512.03400)
*Prakhar Gupta,Henry Conklin,Sarah-Jane Leslie,Andrew Lee*

Main category: cs.LG

TL;DR: 本文研究了显式世界模型目标如何影响Transformer在不同训练阶段的内部表示和下游能力，发现显式世界建模能产生更线性可解码和因果可控的状态表示，并提升强化学习后训练（GRPO）的效果，尤其在处理更难的魔方状态时。


<details>
  <summary>Details</summary>
Motivation: 探究显式世界建模目标对Transformer内部表征及下游任务能力（特别是序列规划任务）的影响机制。

Method: 在可控的2x2x2魔方环境中，对比标准下一词预测与两种显式世界建模策略（状态预测预训练、状态预测+下一词联合目标），并在预训练后应用Group Relative Policy Optimization（GRPO）进行后训练；使用线性探针和因果干预评估表征质量。

Result: 显式世界建模得到更线性可解码、因果可操控的状态表征；高质量状态表征显著提升GRPO后训练效果，尤其在更难的魔方状态上。

Conclusion: 增强状态表征质量可提升序列规划类任务中后训练方法的有效性。

Abstract: In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.

</details>


### [209] [GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test](https://arxiv.org/abs/2512.03428)
*Ziyi Ding,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 本文提出GaussDetect-LiNGAM，一种无需显式高斯性检验的双变量因果发现新方法，利用前向模型噪声高斯性与反向回归中回归子与残差独立性之间的等价性，以鲁棒的核独立性检验替代脆弱的高斯性检验，提升LiNGAM在实际应用中的效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统LiNGAM方法依赖对噪声分布的高斯性检验，而该检验在小样本下不稳定且易出错，限制了方法的实际适用性。

Method: 基于LiNGAM的线性、无环和外生性假设，严格证明前向模型噪声高斯性与反向回归中 regressor 与 residual 独立性的等价性，并采用核独立性检验（如HSIC）替代高斯性检验来判定因果方向。

Result: 实验验证了该理论等价性；GaussDetect-LiNGAM在多种噪声类型和样本规模下保持高一致性，并显著减少每次因果决策所需的检验次数（TPD）。

Conclusion: GaussDetect-LiNGAM通过理论驱动的检验替代，提升了LiNGAM方法的鲁棒性、效率与实用性，推动其在真实场景中的落地应用。

Abstract: We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.

</details>


### [210] [Grokked Models are Better Unlearners](https://arxiv.org/abs/2512.03437)
*Yuanbang Liang,Yang Li*

Main category: cs.LG

TL;DR: 本文研究了模型在'grokking'（即训练后期才出现的泛化能力提升）之后进行机器遗忘（machine unlearning）的效果，发现相较于早期停止的模型，在grokking之后应用标准遗忘方法能更高效、更稳定地实现选择性遗忘，且对保留数据和测试性能的损害更小。


<details>
  <summary>Details</summary>
Motivation: 探究grokking这一训练现象是否有助于提升机器遗忘的效果，即能否在不完全重训练的前提下更有效地消除特定数据的影响。

Method: 在视觉（CIFAR、SVHN、ImageNet上的CNN/ResNet）与语言（TOFU风格设置下的Transformer）任务上，对比标准遗忘方法在grokking前与grokking后应用的效果，并分析特征表示与损失曲率变化。

Result: post-grokking模型展现出：(i) 更高效的遗忘（更少更新步数达目标遗忘水平），(ii) 更小的附带损害（保留集与测试集性能下降更少），(iii) 跨随机种子更稳定的更新行为；其内在机制源于更模块化的表征与遗忘/保留子集间梯度对齐度降低。

Conclusion: 模型训练所处阶段（pre- vs. post-grokking）是独立于遗忘算法设计的、可提升遗忘效果的关键因素，为改进现有遗忘方法提供了简单实用的新思路。

Abstract: Grokking-delayed generalization that emerges well after a model has fit the training data-has been linked to robustness and representation quality. We ask whether this training regime also helps with machine unlearning, i.e., removing the influence of specified data without full retraining. We compare applying standard unlearning methods before versus after the grokking transition across vision (CNNs/ResNets on CIFAR, SVHN, and ImageNet) and language (a transformer on a TOFU-style setup). Starting from grokked checkpoints consistently yields (i) more efficient forgetting (fewer updates to reach a target forget level), (ii) less collateral damage (smaller drops on retained and test performance), and (iii) more stable updates across seeds, relative to early-stopped counterparts under identical unlearning algorithms. Analyses of features and curvature further suggest that post-grokking models learn more modular representations with reduced gradient alignment between forget and retain subsets, which facilitates selective forgetting. Our results highlight when a model is trained (pre- vs. post-grokking) as an orthogonal lever to how unlearning is performed, providing a practical recipe to improve existing unlearning methods without altering their algorithms.

</details>


### [211] [Multi-Modal Opinion Integration for Financial Sentiment Analysis using Cross-Modal Attention](https://arxiv.org/abs/2512.03464)
*Yujing Liu,Chen Yang*

Main category: cs.LG

TL;DR: 本文提出了一种端到端深度学习框架，通过专为金融情感分析设计的跨模态注意力机制，融合时效性（recency）与流行度（popularity）两类金融舆情文本模态，显著提升情感分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效整合多样化的舆情模态并捕捉其细粒度交互，而金融 sentiment 分析对市场预测和风险评估日益重要。

Method: 采用中文BERT（Chinese-wwm-ext）进行文本嵌入，设计金融多头跨模态注意力（FMHCA）机制建模两类文本模态（时效性 vs. 流行度）间的交互，再经Transformer优化与多模态分解双线性池化融合，实现三类情感分类。

Result: 在覆盖837家公司的数据集上达到83.5%准确率，较BERT+Transformer基线提升21个百分点。

Conclusion: 所提框架能更精准地建模金融舆情多源异构特征，有望提升金融决策与风险管理效能。

Abstract: In recent years, financial sentiment analysis of public opinion has become increasingly important for market forecasting and risk assessment. However, existing methods often struggle to effectively integrate diverse opinion modalities and capture fine-grained interactions across them. This paper proposes an end-to-end deep learning framework that integrates two distinct modalities of financial opinions: recency modality (timely opinions) and popularity modality (trending opinions), through a novel cross-modal attention mechanism specifically designed for financial sentiment analysis. While both modalities consist of textual data, they represent fundamentally different information channels: recency-driven market updates versus popularity-driven collective sentiment. Our model first uses BERT (Chinese-wwm-ext) for feature embedding and then employs our proposed Financial Multi-Head Cross-Attention (FMHCA) structure to facilitate information exchange between these distinct opinion modalities. The processed features are optimized through a transformer layer and fused using multimodal factored bilinear pooling for classification into negative, neutral, and positive sentiment. Extensive experiments on a comprehensive dataset covering 837 companies demonstrate that our approach achieves an accuracy of 83.5%, significantly outperforming baselines including BERT+Transformer by 21 percent. These results highlight the potential of our framework to support more accurate financial decision-making and risk management.

</details>


### [212] [Bayesian Event-Based Model for Disease Subtype and Stage Inference](https://arxiv.org/abs/2512.03467)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯子类型变体事件模型（BEBMS），在合成与真实阿尔茨海默病数据上均优于现有SuStaIn方法，尤其在疾病进展排序、分期和亚型分配方面更鲁棒且符合科学共识。


<details>
  <summary>Details</summary>
Motivation: 评估SuStaIn模型在疾病亚型推断中的鲁棒性，并解决其在模型误设下的性能下降问题。

Method: 提出基于贝叶斯框架的子类型事件模型（BEBMS），通过合成数据实验（含不同误设程度）与真实阿尔茨海默病数据，系统比较其与SuStaIn在亚型数估计、进展顺序推断、患者分期及亚型分配上的性能。

Result: BEBMS在所有任务（排序、分期、亚型分配）中显著优于SuStaIn；在真实AD数据中，BEBMS结果更符合阿尔茨海默病进展的科学共识。

Conclusion: BEBMS是一种更鲁棒、更可靠的疾病亚型与进展建模方法，为临床研究提供了改进的计算工具。

Abstract: Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.

</details>


### [213] [SweetDeep: A Wearable AI Solution for Real-Time Non-Invasive Diabetes Screening](https://arxiv.org/abs/2512.03471)
*Ian Henriques,Lynda Elhassar,Sarvesh Relekar,Denis Walrave,Shayan Hassantabar,Vishu Ghanakota,Adel Laoui,Mahmoud Aich,Rafia Tir,Mohamed Zerguine,Samir Louafi,Moncef Kimouche,Emmanuel Cosson,Niraj K Jha*

Main category: cs.LG

TL;DR: 本文提出SweetDeep，一种轻量级神经网络，利用三星Galaxy Watch 7在真实生活场景下采集的生理与人口统计学数据，实现对2型糖尿病的非侵入式、高精度筛查。


<details>
  <summary>Details</summary>
Motivation: 全球2型糖尿病发病率上升，亟需可扩展、低成本的筛查方法；现有生化检测具有侵入性和高成本；可穿戴设备结合机器学习虽有探索，但此前研究多限于受控环境。

Method: 基于欧盟和中东-北非地区285名参与者（糖尿病与非糖尿病）在自由生活条件下使用三星Galaxy Watch 7连续6天采集的生理与人口统计学数据，构建轻量级神经网络SweetDeep（参数<3000），输入为每名参与者每日多个2分钟传感器记录（共约20条/人），采用三折交叉验证评估，并引入置信度阈值实现选择性预测。

Result: SweetDeep在患者级别达到82.5%准确率（宏F1=82.1%，敏感度=79.7%，特异度=84.6%，预期校准误差=5.5%）；允许模型对<10%低置信度样本拒绝预测时，剩余样本准确率达84.5%。

Conclusion: 融合人工设计特征与轻量架构可在真实世界可穿戴场景中实现快速、准确且泛化性强的2型糖尿病检测，为大规模无创筛查提供可行路径。

Abstract: The global rise in type 2 diabetes underscores the need for scalable and cost-effective screening methods. Current diagnosis requires biochemical assays, which are invasive and costly. Advances in consumer wearables have enabled early explorations of machine learning-based disease detection, but prior studies were limited to controlled settings. We present SweetDeep, a compact neural network trained on physiological and demographic data from 285 (diabetic and non-diabetic) participants in the EU and MENA regions, collected using Samsung Galaxy Watch 7 devices in free-living conditions over six days. Each participant contributed multiple 2-minute sensor recordings per day, totaling approximately 20 recordings per individual. Despite comprising fewer than 3,000 parameters, SweetDeep achieves 82.5% patient-level accuracy (82.1% macro-F1, 79.7% sensitivity, 84.6% specificity) under three-fold cross-validation, with an expected calibration error of 5.5%. Allowing the model to abstain on less than 10% of low-confidence patient predictions yields an accuracy of 84.5% on the remaining patients. These findings demonstrate that combining engineered features with lightweight architectures can support accurate, rapid, and generalizable detection of type 2 diabetes in real-world wearable settings.

</details>


### [214] [Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression](https://arxiv.org/abs/2512.03475)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: 本文提出了联合进展模型（JPM），一种用于建模神经退行性疾病中多种病理共存的事件型概率框架，通过将单病程视为偏序关系并构建联合进展先验，在合成数据和真实数据（NACC）上验证了其在排序准确性、校准性、可分离性和稳定性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 标准事件型模型（EBMs）假设个体仅患一种疾病，但神经退行性疾病中混合病理（如AD与VaD共存）十分常见，亟需能建模多病程交互的模型。

Method: 提出联合进展模型（JPM），将单病程建模为偏序，构建联合进展的先验分布；研究四种变体（Pairwise、Bradley-Terry、Plackett-Luce、Mallows），并从校准性、可分离性、尖锐性三方面评估；在合成数据和NACC真实数据上与SA-EBM基线对比。

Result: 所有JPM变体均具备良好校准性与近完美可分离性；尖锐性因变体而异，且可由输入偏序的简单特征（数量、长度、冲突、重叠）预测；在合成实验中排序准确率较SA-EBM提升约21%；在NACC数据中，Mallows-JPM与SA-EBM结果更符合AD-VaD混合病理的已有文献认知。

Conclusion: JPM为建模混合病理疾病进展提供了灵活、可解释且性能优越的概率框架，尤其Mallows变体在真实数据中展现出良好的生物学合理性。

Abstract: Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.

</details>


### [215] [Physics-Driven Learning Framework for Tomographic Tactile Sensing](https://arxiv.org/abs/2512.03512)
*Xuanxuan Yang,Xiuyang Zhang,Haofeng Chen,Gang Ma,Xiaojie Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为PhyDNN的物理驱动深度重建框架，用于解决电学阻抗断层成像（EIT）在大面积触觉传感中因非线性逆问题导致的伪影严重和接触重建不准确的问题。该方法将EIT前向模型嵌入学习目标，并设计可微分前向算子网络，实现了物理一致性与数据驱动的结合，在仿真和真实实验中均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: EIT在大面积触觉传感中具有布线少、形状灵活等优势，但其非线性逆问题常导致严重伪影和不准确的接触重建。

Method: 提出PhyDNN框架，将EIT前向物理模型直接嵌入深度学习目标函数；设计可微分前向算子网络以支持高效反向传播；联合最小化预测与真值电导率图的差异及前向PDE的一致性约束。

Result: 在16电极柔性传感器的仿真与真实触觉实验中，PhyDNN在接触形状、位置和压力分布重建上均优于NOSER、TV和标准DNN，表现出更少伪影、更清晰边界和更高评价指标得分。

Conclusion: PhyDNN通过融合物理模型与深度学习，显著提升了EIT触觉成像的质量、物理合理性和泛化能力，为高质量断层触觉传感提供了有效解决方案。

Abstract: Electrical impedance tomography (EIT) provides an attractive solution for large-area tactile sensing due to its minimal wiring and shape flexibility, but its nonlinear inverse problem often leads to severe artifacts and inaccurate contact reconstruction. This work presents PhyDNN, a physics-driven deep reconstruction framework that embeds the EIT forward model directly into the learning objective. By jointly minimizing the discrepancy between predicted and ground-truth conductivity maps and enforcing consistency with the forward PDE, PhyDNN reduces the black-box nature of deep networks and improves both physical plausibility and generalization. To enable efficient backpropagation, we design a differentiable forward-operator network that accurately approximates the nonlinear EIT response, allowing fast physics-guided training. Extensive simulations and real tactile experiments on a 16-electrode soft sensor show that PhyDNN consistently outperforms NOSER, TV, and standard DNNs in reconstructing contact shape, location, and pressure distribution. PhyDNN yields fewer artifacts, sharper boundaries, and higher metric scores, demonstrating its effectiveness for high-quality tomographic tactile sensing.

</details>


### [216] [Adaptive sampling using variational autoencoder and reinforcement learning](https://arxiv.org/abs/2512.03525)
*Adil Rasheed,Mikael Aleksander Jansen Shahly,Muhammad Faisal Aftab*

Main category: cs.LG

TL;DR: 本文提出了一种结合变分自编码器先验与强化学习的自适应稀疏感知框架，用于顺序选择最优测量，显著提升了稀疏采样下的重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有压缩感知方法依赖通用基和随机测量，效率与重建质量受限；最优传感器放置虽利用历史数据但基固定且无法适应非线性或样本特异性变化；生成模型方法仍使用次优随机采样。

Method: 将变分自编码器（VAE）作为生成先验，结合强化学习（RL）进行序列化测量选择，实现自适应稀疏感知。

Result: 在实验中，该方法在稀疏测量下的重建性能优于传统压缩感知（CS）、最优传感器放置（OSP）及基于生成模型的压缩感知方法。

Conclusion: 耦合生成先验与强化学习的自适应测量策略可有效提升稀疏感知的灵活性与重建精度，为数据驱动的智能采样提供了新范式。

Abstract: Compressed sensing enables sparse sampling but relies on generic bases and random measurements, limiting efficiency and reconstruction quality. Optimal sensor placement uses historcal data to design tailored sampling patterns, yet its fixed, linear bases cannot adapt to nonlinear or sample-specific variations. Generative model-based compressed sensing improves reconstruction using deep generative priors but still employs suboptimal random sampling. We propose an adaptive sparse sensing framework that couples a variational autoencoder prior with reinforcement learning to select measurements sequentially. Experiments show that this approach outperforms CS, OSP, and Generative model-based reconstruction from sparse measurements.

</details>


### [217] [Parameter-Efficient Augment Plugin for Class-Incremental Learning](https://arxiv.org/abs/2512.03537)
*Zhiming Xu,Baile Xu,Jian Zhao,Furao Shen,Suorong Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为DLC（Deployment of extra LoRA Components）的插件式扩展范式，用于非预训练的类增量学习（CIL），通过在基础模型上添加轻量级LoRA模块并引入权重单元来聚合任务特定表示，在极低参数开销下显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于重放或知识蒸馏的CIL方法易受遗忘或稳定性-可塑性困境限制；而扩展式方法虽精度高，却需大幅增加参数。本文旨在兼顾高精度与低参数开销。

Method: 将重放或蒸馏训练得到的特征提取器作为富含知识的基础模型；对每个新任务，使用LoRA在深层注入任务特定残差；推理时通过轻量级权重单元动态加权各LoRA模块输出的表示进行分类。

Result: 在ImageNet-100上，仅用ResNet-18 4%的参数即实现8%准确率提升；在固定内存预算下超越当前最优方法。

Conclusion: DLC是一种高效、即插即用的CIL扩展范式，显著缓解了精度与参数效率之间的权衡问题，尤其适用于资源受限的大规模增量学习场景。

Abstract: Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.

</details>


### [218] [Towards Irreversible Machine Unlearning for Diffusion Models](https://arxiv.org/abs/2512.03564)
*Xun Yuan,Zilong Zhao,Jiayu Li,Aryan Pasikhani,Prosanta Gope,Biplab Sikdar*

Main category: cs.LG

TL;DR: 本文提出了一种针对扩散模型微调式机器遗忘方法的新型重学习攻击（DiMRA），并设计了更具鲁棒性的遗忘方法DiMUM，通过‘记忆替代内容’而非简单‘遗忘’来提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的机器遗忘方法（尤其是基于微调的）存在被逆向攻击的风险，缺乏对遗忘效果的鲁棒性保障，亟需更安全的遗忘机制。

Method: 提出DiMRA攻击：在无目标遗忘信息先验下，利用辅助数据集优化已遗忘模型以恢复生成能力；提出DiMUM方法：不直接删除目标内容，而是通过有意识地记忆替代数据/特征来覆盖或抑制目标生成。

Result: 实验验证DiMRA能有效逆转多种SOTA微调式遗忘方法；DiMUM在保持生成质量的同时显著增强对DiMRA的抗性。

Conclusion: 微调式机器遗忘在扩散模型中存在本质脆弱性；‘以忆代忘’的DiMUM范式为构建更安全、鲁棒的生成模型遗忘机制提供了新思路。

Abstract: Diffusion models are renowned for their state-of-the-art performance in generating synthetic images. However, concerns related to safety, privacy, and copyright highlight the need for machine unlearning, which can make diffusion models forget specific training data and prevent the generation of sensitive or unwanted content. Current machine unlearning methods for diffusion models are primarily designed for conditional diffusion models and focus on unlearning specific data classes or features. Among these methods, finetuning-based machine unlearning methods are recognized for their efficiency and effectiveness, which update the parameters of pre-trained diffusion models by minimizing carefully designed loss functions. However, in this paper, we propose a novel attack named Diffusion Model Relearning Attack (DiMRA), which can reverse the finetuning-based machine unlearning methods, posing a significant vulnerability of this kind of technique. Without prior knowledge of the unlearning elements, DiMRA optimizes the unlearned diffusion model on an auxiliary dataset to reverse the unlearning, enabling the model to regenerate previously unlearned elements. To mitigate this vulnerability, we propose a novel machine unlearning method for diffusion models, termed as Diffusion Model Unlearning by Memorization (DiMUM). Unlike traditional methods that focus on forgetting, DiMUM memorizes alternative data or features to replace targeted unlearning data or features in order to prevent generating such elements. In our experiments, we demonstrate the effectiveness of DiMRA in reversing state-of-the-art finetuning-based machine unlearning methods for diffusion models, highlighting the need for more robust solutions. We extensively evaluate DiMUM, demonstrating its superior ability to preserve the generative performance of diffusion models while enhancing robustness against DiMRA.

</details>


### [219] [When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate](https://arxiv.org/abs/2512.03578)
*Florent Forest,Amaury Wei,Olga Fink*

Main category: cs.LG

TL;DR: 本文提出MAGNETS，一种无需标注即可学习人类可理解时间序列概念的内在可解释神经网络，通过掩码聚合和加性结构实现高精度与高透明度的时序外源回归。


<details>
  <summary>Details</summary>
Motivation: 现有TSER模型虽预测性能强，但多为黑箱；后验解释方法粗糙不稳定，而现有内在可解释方法依赖概念标注、难以建模特征交互、表达能力弱且难以扩展至高维多元时序数据。

Method: 提出MAGNETS架构：基于掩码（mask）的特征选择与聚合机制自动学习紧凑、可解释的时间序列概念；预测由这些概念线性叠加构成，保证完全透明的加性决策过程。

Result: MAGNETS在多个基准数据集上达到SOTA预测性能，同时提供稳定、精细、可验证的时间维度与特征维度归因，支持人类理解关键模式及其作用时机。

Conclusion: MAGNETS首次实现了无需监督、可扩展、能建模时序特征交互且保持高预测精度的内在可解释TSER框架，为可信时序建模提供了新范式。

Abstract: Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.
  To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.

</details>


### [220] [Optimal Transportation and Alignment Between Gaussian Measures](https://arxiv.org/abs/2512.03579)
*Sanjit Dandapanthula,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan,Aaditya Ramdas,Ziv Goldfeld*

Main category: cs.LG

TL;DR: 本文系统研究了高斯分布下最优传输（OT）与内积Gromov-Wasserstein（IGW）对齐的闭式解，解决了非中心高斯在希尔伯特空间中的IGW对齐这一开放问题，并给出解析界、中心情形的完全闭式解及IGW质心解；同时将多源高斯OT简化为带秩约束的可解优化问题，并应用于知识蒸馏与异构聚类。


<details>
  <summary>Details</summary>
Motivation: OT和GW对齐虽具几何可解释性，但计算昂贵；大规模应用常依赖高斯分布下的闭式解，而现有工作在非中心高斯、IGW质心、多源OT等方面仍存在理论空白。

Method: 推导非中心高斯在可分希尔伯特空间下的IGW对齐闭式解（含单位算子上的二次优化及其上下界）；给出中心高斯情形的完全解析解并推广至IGW质心；将高斯多源OT降维为带秩缺陷约束的优化问题并设计高效算法。

Result: 获得IGW对齐的紧致解析界；中心高斯IGW对齐与质心均有全解析解；多源高斯OT被转化为可高效求解的约束优化问题；在知识蒸馏和异构聚类任务中验证了方法有效性。

Conclusion: 本工作填补了高斯OT与IGW理论的关键空白，显著拓展了其在大规模、异构数据场景下的实用性与可解释性。

Abstract: Optimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets.

</details>


### [221] [Federated Learning and Trajectory Compression for Enhanced AIS Coverage](https://arxiv.org/abs/2512.03584)
*Thomas Gräupl,Andreas Reisenbauer,Marcel Hecko,Anil Rasouli,Anita Graser,Melitta Dragaschnig,Axel Weissenfeld,Gilles Dejaegere,Mahmoud Sakr*

Main category: cs.LG

TL;DR: VesselEdge系统利用联邦学习和带宽受限的轨迹压缩技术，提升海上态势感知能力，扩展AIS覆盖范围，特别适用于低带宽环境下的实时异常检测。


<details>
  <summary>Details</summary>
Motivation: 提升海上态势感知能力，解决AIS信号覆盖不足及低带宽环境下数据传输效率低的问题。

Method: 结合M3fed联邦学习模型与BWC-DR-A轨迹压缩算法，将船舶变为移动传感器，优先传输异常轨迹数据。

Result: 初步实验表明VesselEdge能有效扩展AIS覆盖范围并提升态势感知能力。

Conclusion: VesselEdge为资源受限的海上边缘环境提供了一种高效、自适应的分布式感知框架。

Abstract: This paper presents the VesselEdge system, which leverages federated learning and bandwidth-constrained trajectory compression to enhance maritime situational awareness by extending AIS coverage. VesselEdge transforms vessels into mobile sensors, enabling real-time anomaly detection and efficient data transmission over low-bandwidth connections. The system integrates the M3fed model for federated learning and the BWC-DR-A algorithm for trajectory compression, prioritizing anomalous data. Preliminary results demonstrate the effectiveness of VesselEdge in improving AIS coverage and situational awareness using historical data.

</details>


### [222] [Observation-driven correction of numerical weather prediction for marine winds](https://arxiv.org/abs/2512.03606)
*Matteo Peduto,Qidong Yang,Jonathan Giezendanner,Devis Tuia,Sherrie Wang*

Main category: cs.LG

TL;DR: 本文提出一种基于Transformer的深度学习模型，通过融合稀疏、异构、时变的海洋实测风速数据，对全球数值天气预报（GFS）结果进行局部校正，显著提升10米风速预测精度，尤其在近岸和航线区域效果突出。


<details>
  <summary>Details</summary>
Motivation: 海洋风速观测稀疏、异构且动态变化，导致传统直接预报方法精度受限；亟需一种能有效利用有限观测信息、实时校正NWP系统偏差的方法。

Method: 将风速预报重构为‘观测驱动的NWP输出校正’任务；设计一种新型Transformer架构：采用掩码与集合注意力处理不规则观测，通过交叉注意力利用最新观预报对，引入循环时间嵌入和坐标感知位置编码以支持任意时空坐标的单次前向推理。

Result: 在大西洋海域评估显示，该模型在1–48小时预报时效内均降低GFS的10米风速RMSE，1小时和48小时分别提升45%和13%；改进最显著区域为海岸线和主要航运路线；可统一处理船舶、浮标、验潮站等多种观测平台，并同步输出站点级和海盆尺度格点化产品。

Conclusion: 该方法是一种低延迟、实用性强的NWP后处理范式，通过学习系统性误差校正模式，有效弥补了传统数值预报在海洋区域的不足。

Abstract: Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.

</details>


### [223] [CoGraM: Context-sensitive granular optimization method with rollback for robust model fusion](https://arxiv.org/abs/2512.03610)
*Julius Lenz*

Main category: cs.LG

TL;DR: CoGraM是一种多阶段、上下文敏感、基于损失、逐层迭代优化的神经网络合并方法，通过损失差异对齐决策并防止有害更新，显著提升合并网络性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络合并方法（如权重平均、Fisher合并）常存在精度下降和跨随机种子不稳定的问题。

Method: 提出CoGraM方法，采用多阶段、上下文敏感、基于损失、逐层/神经元/权重级别迭代优化策略，结合损失差异对齐与阈值判断，并引入回滚机制防止有害更新。

Result: CoGraM能显著提升合并后网络的性能，克服Fisher等方法的固有缺陷。

Conclusion: CoGraM是一种更鲁棒、更有效的神经网络合并优化方法，适用于联邦学习与分布式学习场景。

Abstract: Merging neural networks without retraining is central to federated and distributed learning. Common methods such as weight averaging or Fisher merging often lose accuracy and are unstable across seeds. CoGraM (Contextual Granular Merging) is a multi-stage, context-sensitive, loss-based, and iterative optimization method across layers, neurons, and weight levels that aligns decisions with loss differences and thresholds and prevents harmful updates through rollback. CoGraM is an optimization method that addresses the weaknesses of methods such as Fisher and can significantly improve the merged network.

</details>


### [224] [The promising potential of vision language models for the generation of textual weather forecasts](https://arxiv.org/abs/2512.03623)
*Edward C. C. Steele,Dinesh Mane,Emilio Monti,Luis Orus,Rebecca Chantrill-Cheyette,Matthew Couch,Kirstine I. Dale,Simon Eaton,Govindarajan Rangarajan,Amir Majlesi,Steven Ramsdale,Michael Sharpe,Craig Smith,Jonathan Smith,Rebecca Yates,Holly Ellis,Charles Ewen*

Main category: cs.LG

TL;DR: 本文探索了使用视觉语言模型直接从视频编码的网格化天气数据生成航运预报文本的新方法，展示了提升气象服务生产效率和创新的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型展现出巨大潜力，但其在气象产品和服务生成中的应用仍处于初级阶段，亟需加速其应用和采纳。

Method: 本文提出了一种新颖的方法，即利用视觉语言模型，将视频编码的网格化天气数据直接转化为经典的航运预报文本。

Result: 初步结果表明，该方法具有可扩展的技术潜力，有望提高气象服务的生产效率并推动服务创新。

Conclusion: 视觉语言模型在气象领域具有广阔的应用前景，为气象服务的自动化和智能化提供了新的技术路径。

Abstract: Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.

</details>


### [225] [Conditional updates of neural network weights for increased out of training performance](https://arxiv.org/abs/2512.03653)
*Jan Saynisch-Wagner,Saran Rajendran Sari*

Main category: cs.LG

TL;DR: 本文提出一种方法，通过重训练神经网络、建立预测因子与权重异常之间的回归关系，并外推权重，以提升神经网络在训练数据与应用数据分布不一致（如OOD、模式/机制变化）场景下的性能。该方法在气候科学的时序、空间和跨域任务中验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在训练数据与应用数据不相似（如分布外、模式偏移、机制变化）时性能下降的问题。

Method: 三步法：1）基于训练数据的合理子集重训练网络并记录权重异常；2）选取合理预测因子，建立其与权重异常的回归模型；3）利用回归模型外推权重，从而实现神经网络对应用数据的适配。

Result: 在气候科学三个用例中成功实现了时间、空间和跨域的神经网络外推。

Conclusion: 该方法能有效提升神经网络在分布偏移场景下的泛化能力，具有实际应用潜力。

Abstract: This study proposes a method to enhance neural network performance when training data and application data are not very similar, e.g., out of distribution problems, as well as pattern and regime shifts. The method consists of three main steps: 1) Retrain the neural network towards reasonable subsets of the training data set and note down the resulting weight anomalies. 2) Choose reasonable predictors and derive a regression between the predictors and the weight anomalies. 3) Extrapolate the weights, and thereby the neural network, to the application data. We show and discuss this method in three use cases from the climate sciences, which include successful temporal, spatial and cross-domain extrapolations of neural networks.

</details>


### [226] [Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting](https://arxiv.org/abs/2512.03656)
*Salim Khazem,Houssam Kanso*

Main category: cs.LG

TL;DR: 本文提出了一种融合周期性时间编码与LSTM-CNN混合架构的统一深度学习框架，用于提升多步电力消耗预测精度。通过正余弦编码建模日历特征的周期性，并结合LSTM（捕获长期季节性）、CNN（提取局部模式）及MLP元学习器（按预测步长定制），在一年国家级用电数据上验证了该方法在7个预测步长上均优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 准确的电力消耗预测对需求管理和智能电网运行至关重要，而现有方法在有效融合周期性时间信息与多尺度时序模式方面仍存在不足。

Method: 采用正余弦编码对日历属性进行周期性时间编码；构建由LSTM、CNN和针对不同预测步长定制的MLP元学习器组成的混合集成模型；通过相关性分析评估特征重要性，并开展包含消融实验的系统性评估。

Result: 在一年国家级电力消费数据集上，所提模型在全部七个预测步长上均取得更低的RMSE和MAE，显著优于单一架构及现有基准方法。

Conclusion: 周期性时间表征与互补式深度学习结构的联合建模能有效提升短期电力负荷预测性能；本工作首次在统一框架中协同评估时间编码、日历特征与混合集成架构。

Abstract: Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.

</details>


### [227] [Dynamically Scaled Activation Steering](https://arxiv.org/abs/2512.03661)
*Alex Ferrando,Xavier Suau,Jordi Gonzàlez,Pau Rodriguez*

Main category: cs.LG

TL;DR: 本文提出了一种动态缩放激活引导（DSAS）框架，用于在生成模型中自适应地调控引导强度，以在毒性缓解与效用保持之间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法通常对所有输入统一干预，导致无需引导时性能下降，亟需一种能解耦‘何时引导’与‘如何引导’的自适应机制。

Method: DSAS是一种方法无关的引导框架，通过计算上下文相关的缩放因子，动态调节各层和各输入上的引导强度；支持与引导函数端到端联合优化。

Result: DSAS在多个基准上显著改善了毒性缓解与效用保留的Pareto前沿；成功扩展至文本到图像扩散模型，并具备低计算开销和高可解释性。

Conclusion: DSAS提供了一种通用、高效且可解释的自适应激活引导范式，提升了生成模型行为控制的灵活性与鲁棒性。

Abstract: Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.

</details>


### [228] [Feature-aware Modulation for Learning from Temporal Tabular Data](https://arxiv.org/abs/2512.03678)
*Hao-Run Cai,Han-Jia Ye*

Main category: cs.LG

TL;DR: 本文提出了一种面向时序表格数据的特征感知时间调制机制，通过根据时间上下文调节特征的统计属性（如尺度和偏度），缓解因特征语义随时间演化导致的概念漂移，从而在泛化性与适应性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 时序分布偏移使静态模型缺乏适应性、动态模型易过拟合，且特征语义（主客观含义）随时间演化引发概念漂移，亟需兼顾鲁棒性与适应性的建模方法。

Method: 提出特征感知的时间调制机制，利用时间上下文对特征表示进行条件化调制，重点调节其尺度、偏度等统计特性，以对齐跨时段的特征语义。

Result: 该方法在多个基准测试中验证了其有效应对表格数据中时序偏移的能力，实现了轻量级但强效的自适应。

Conclusion: 特征变换策略可缓解时间阶段间表征差异；基于时间上下文调制特征统计属性，是实现泛化性与适应性平衡的有效途径。

Abstract: While tabular machine learning has achieved remarkable success, temporal distribution shifts pose significant challenges in real-world deployment, as the relationships between features and labels continuously evolve. Static models assume fixed mappings to ensure generalization, whereas adaptive models may overfit to transient patterns, creating a dilemma between robustness and adaptability. In this paper, we analyze key factors essential for constructing an effective dynamic mapping for temporal tabular data. We discover that evolving feature semantics-particularly objective and subjective meanings-introduce concept drift over time. Crucially, we identify that feature transformation strategies are able to mitigate discrepancies in feature representations across temporal stages. Motivated by these insights, we propose a feature-aware temporal modulation mechanism that conditions feature representations on temporal context, modulating statistical properties such as scale and skewness. By aligning feature semantics across time, our approach achieves a lightweight yet powerful adaptation, effectively balancing generalizability and adaptability. Benchmark evaluations validate the effectiveness of our method in handling temporal shifts in tabular data.

</details>


### [229] [Quantum Topological Graph Neural Networks for Detecting Complex Fraud Patterns](https://arxiv.org/abs/2512.03696)
*Mohammad Doost,Mohammad Manthouri*

Main category: cs.LG

TL;DR: QTGNN是一种融合量子嵌入、变分图卷积与拓扑数据分析的新型框架，用于大规模金融网络中的欺诈交易检测，兼具理论严谨性、可解释性与NISQ设备实用性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模金融网络中复杂交易动态与结构异常导致的传统欺诈检测方法性能不足、缺乏可解释性及在噪声中尺度量子（NISQ）设备上训练不稳定的问题。

Method: 结合量子数据嵌入（含纠缠增强）、变分量子图卷积（含非线性动力学）、高阶拓扑不变量提取、自适应优化的混合量子-经典异常学习，以及基于拓扑归因的可解释决策机制，并通过电路简化与图采样实现NISQ友好型扩展。

Result: 在PaySim和Elliptic等金融数据集上，QTGNN在ROC-AUC、精确率和误报率等指标上优于经典与量子基线模型；消融实验验证了量子嵌入、拓扑特征、非线性通道和混合学习各自的关键贡献；理论保证了NISQ设备上的收敛性与拓扑签名稳定性。

Conclusion: QTGNN为金融欺诈检测提供了理论可靠、可解释且面向实际NISQ硬件部署的跨学科解决方案，成功融合量子机器学习、图神经网络与拓扑数据分析。

Abstract: We propose a novel QTGNN framework for detecting fraudulent transactions in large-scale financial networks. By integrating quantum embedding, variational graph convolutions, and topological data analysis, QTGNN captures complex transaction dynamics and structural anomalies indicative of fraud. The methodology includes quantum data embedding with entanglement enhancement, variational quantum graph convolutions with non-linear dynamics, extraction of higher-order topological invariants, hybrid quantum-classical anomaly learning with adaptive optimization, and interpretable decision-making via topological attribution. Rigorous convergence guarantees ensure stable training on noisy intermediate-scale quantum (NISQ) devices, while stability of topological signatures provides robust fraud detection. Optimized for NISQ hardware with circuit simplifications and graph sampling, the framework scales to large transaction networks. Simulations on financial datasets, such as PaySim and Elliptic, benchmark QTGNN against classical and quantum baselines, using metrics like ROC-AUC, precision, and false positive rate. An ablation study evaluates the contributions of quantum embeddings, topological features, non-linear channels, and hybrid learning. QTGNN offers a theoretically sound, interpretable, and practical solution for financial fraud detection, bridging quantum machine learning, graph theory, and topological analysis.

</details>


### [230] [Unlocking the Invisible Urban Traffic Dynamics under Extreme Weather: A New Physics-Constrained Hamiltonian Learning Algorithm](https://arxiv.org/abs/2512.03744)
*Xuhui Lin,Qiuchen Lu*

Main category: cs.LG

TL;DR: 本文提出一种基于物理约束的哈密顿学习算法，通过结构不可逆性检测和能量景观重建，识别城市交通系统在极端天气后的隐藏结构性损伤，避免传统方法中‘虚假恢复’的误判。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅依赖表层恢复指标，无法区分真实恢复与‘虚假恢复’（即表观交通指标恢复正常但系统内在动力学已永久退化），导致对基础设施真实健康状况的误判。

Method: 提出一种物理约束的哈密顿学习算法，包含三个核心步骤：提取低维状态表示、通过物理约束优化识别准哈密顿结构、通过能量景观对比量化结构性变化。

Result: 在2021年伦敦极端降雨事件分析中，传统指标显示完全恢复，而本方法检测出64.8%被忽略的结构性损伤。

Conclusion: 该框架支持前瞻性结构风险评估，使基础设施投资决策基于真实系统健康状态，而非具有误导性的表层指标。

Abstract: Urban transportation systems face increasing resilience challenges from extreme weather events, but current assessment methods rely on surface-level recovery indicators that miss hidden structural damage. Existing approaches cannot distinguish between true recovery and "false recovery," where traffic metrics normalize, but the underlying system dynamics permanently degrade. To address this, a new physics-constrained Hamiltonian learning algorithm combining "structural irreversibility detection" and "energy landscape reconstruction" has been developed. Our approach extracts low-dimensional state representations, identifies quasi-Hamiltonian structures through physics-constrained optimization, and quantifies structural changes via energy landscape comparison. Analysis of London's extreme rainfall in 2021 demonstrates that while surface indicators were fully recovered, our algorithm detected 64.8\% structural damage missed by traditional monitoring. Our framework provides tools for proactive structural risk assessment, enabling infrastructure investments based on true system health rather than misleading surface metrics.

</details>


### [231] [Universally Converging Representations of Matter Across Scientific Foundation Models](https://arxiv.org/abs/2512.03750)
*Sathya Edamadaka,Soojung Yang,Ju Li,Rafael Gómez-Bombarelli*

Main category: cs.LG

TL;DR: 本文研究了不同模态和架构的科学机器学习模型在分子、材料和蛋白质行为预测任务中是否学习到相似的内部表征，并发现近六十个模型在多种化学系统上表现出高度表征对齐，揭示了科学基础模型可能学习到物理现实的共同表征，但也指出当前模型在训练域外泛化能力仍受限。


<details>
  <summary>Details</summary>
Motivation: 理解不同科学机器学习模型是否学习到相似的内部表征，是构建可跨领域可靠泛化的科学基础模型的关键；此前语言与视觉领域已有表征收敛现象，但在科学领域尚未系统探索。

Method: 对近六十个涵盖字符串、图、3D原子结构及蛋白质等多模态的科学模型，在多种化学系统上进行表征空间对齐分析，结合性能对比与分布外泛化测试，识别表征收敛与坍缩模式。

Result: 1）不同模态模型在小分子上表征高度对齐；2）原子间势能模型性能提升时表征空间趋于收敛；3）训练域内高性能模型表征紧密对齐，低性能模型陷入局部次优；4）分布外输入下几乎所有模型坍缩至低信息表征。

Conclusion: 表征对齐可作为科学基础模型通用性的定量基准；当前模型尚未编码真正普适的物质结构，其表征能力仍受训练数据与归纳偏置限制；该框架可用于追踪模型规模化过程中普适表征的涌现，并指导跨模态、跨领域模型选择与蒸馏。

Abstract: Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.

</details>


### [232] [Origin-Conditional Trajectory Encoding: Measuring Urban Configurational Asymmetries through Neural Decomposition](https://arxiv.org/abs/2512.03755)
*Stephen Law,Tao Yang,Nanjiang Chen,Xuhui Lin*

Main category: cs.LG

TL;DR: 本文提出了一种条件轨迹编码器，联合学习空间与运动表征，保留起点依赖的导航不对称性，利用几何特征（如可见性比、曲率）和对比学习分解城市导航为共享认知模式与起点特异性空间叙事，揭示城市形态引发的认知不平等。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹分析方法存在三方面缺陷：空间与时间表征割裂、忽略导航方向不对称性（A→B ≠ B→A）、过度依赖POI等辅助数据而忽视城市空间基本几何属性。

Method: 设计基于双向LSTM的条件轨迹编码器，以可学习的起点嵌入为条件，处理可见性比和曲率等几何特征；通过对比学习将表征解耦为共享城市模式与起点特异性签名。

Result: 在六个合成城市及北京西城区真实数据上验证，证实城市形态导致系统性的认知不平等；为城市规划、建筑设计与导航系统提供可量化的起源感知分析工具。

Conclusion: 联合建模空间几何与轨迹动态、并显式建模起点依赖性，是实现公平、可解释城市智能分析的关键路径；该框架为评估体验公平性与优化空间认知负荷提供了新范式。

Abstract: Urban analytics increasingly relies on AI-driven trajectory analysis, yet current approaches suffer from methodological fragmentation: trajectory learning captures movement patterns but ignores spatial context, while spatial embedding methods encode street networks but miss temporal dynamics. Three gaps persist: (1) lack of joint training that integrates spatial and temporal representations, (2) origin-agnostic treatment that ignores directional asymmetries in navigation ($A \to B \ne B \to A$), and (3) over-reliance on auxiliary data (POIs, imagery) rather than fundamental geometric properties of urban space. We introduce a conditional trajectory encoder that jointly learns spatial and movement representations while preserving origin-dependent asymmetries using geometric features. This framework decomposes urban navigation into shared cognitive patterns and origin-specific spatial narratives, enabling quantitative measurement of cognitive asymmetries across starting locations. Our bidirectional LSTM processes visibility ratio and curvature features conditioned on learnable origin embeddings, decomposing representations into shared urban patterns and origin-specific signatures through contrastive learning. Results from six synthetic cities and real-world validation on Beijing's Xicheng District demonstrate that urban morphology creates systematic cognitive inequalities. This provides urban planners quantitative tools for assessing experiential equity, offers architects insights into layout decisions' cognitive impacts, and enables origin-aware analytics for navigation systems.

</details>


### [233] [Deep Unfolding: Recent Developments, Theory, and Design Guidelines](https://arxiv.org/abs/2512.03768)
*Nir Shlezinger,Santiago Segarra,Yi Zhang,Dvir Avrahami,Zohar Davidov,Tirza Routtenberg,Yonina C. Eldar*

Main category: cs.LG

TL;DR: 本文综述了深度展开（deep unfolding）这一融合优化与机器学习的框架，系统介绍其设计范式、训练策略及理论保障，并对比分析其在复杂度、可解释性与鲁棒性方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统优化算法虽具可解释性和理论保证，但依赖代理目标、超参调优且计算延迟高；而机器学习方法虽数据驱动能力强，却缺乏结构化、透明性和优化所需的效率。亟需一种能融合二者优势的框架。

Method: 以教程形式梳理深度展开的统一方法论，涵盖四类典型设计范式、适配迭代结构的训练策略，并综述其收敛性与泛化性理论进展。

Result: 提供了深度展开在信号处理中从优化到学习的系统性视角，总结了其概念内涵、理论支撑与实践表现，并通过定性与实证对比揭示了不同展开模型的性能权衡。

Conclusion: 深度展开是连接经典优化与现代机器学习的有效桥梁，兼具结构化建模能力与数据驱动适应性，在保持可解释性的同时提升效率与性能，具有广阔理论价值与应用前景。

Abstract: Optimization methods play a central role in signal processing, serving as the mathematical foundation for inference, estimation, and control. While classical iterative optimization algorithms provide interpretability and theoretical guarantees, they often rely on surrogate objectives, require careful hyperparameter tuning, and exhibit substantial computational latency. Conversely, machine learning (ML ) offers powerful data-driven modeling capabilities but lacks the structure, transparency, and efficiency needed for optimization-driven inference. Deep unfolding has recently emerged as a compelling framework that bridges these two paradigms by systematically transforming iterative optimization algorithms into structured, trainable ML architectures. This article provides a tutorial-style overview of deep unfolding, presenting a unified perspective of methodologies for converting optimization solvers into ML models and highlighting their conceptual, theoretical, and practical implications. We review the foundations of optimization for inference and for learning, introduce four representative design paradigms for deep unfolding, and discuss the distinctive training schemes that arise from their iterative nature. Furthermore, we survey recent theoretical advances that establish convergence and generalization guarantees for unfolded optimizers, and provide comparative qualitative and empirical studies illustrating their relative trade-offs in complexity, interpretability, and robustness.

</details>


### [234] [Forensic Activity Classification Using Digital Traces from iPhones: A Machine Learning-based Approach](https://arxiv.org/abs/2512.03786)
*Conor McCarthy,Jan Peter van Zandwijk,Marcel Worring,Zeno Geradts*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的方法，利用智能手机内置运动传感器的数字痕迹生成活动识别的似然比（LR），用于法医学调查，并公开了新数据集NFI_FARED及全部代码。


<details>
  <summary>Details</summary>
Motivation: 智能手机和智能手表中的运动传感器数据可作为法医调查中用户行为分析的重要数字痕迹来源，但缺乏系统化方法将其转化为可解释的法证证据（如似然比）。

Method: 采用机器学习方法，基于iPhone运动传感器数据（来自NFI_FARED数据集，含4种iPhone、19类活动）建模，输出不同活动对之间的似然比；进一步扩展为多活动联合分析与活动时间线构建。

Result: 在171种活动两两组合中，成功构建出167组有效的似然比系统；并实现了多活动同时评估与活动时间线生成，支持法医调查的早期与后期阶段。

Conclusion: 该方法为基于移动设备传感器数据的法证行为分析提供了可解释、可复现的量化框架，且数据与代码开源，推动该领域发展。

Abstract: Smartphones and smartwatches are ever-present in daily life, and provide a rich source of information on their users' behaviour. In particular, digital traces derived from the phone's embedded movement sensors present an opportunity for a forensic investigator to gain insight into a person's physical activities. In this work, we present a machine learning-based approach to translate digital traces into likelihood ratios (LRs) for different types of physical activities. Evaluating on a new dataset, NFI\_FARED, which contains digital traces from four different types of iPhones labelled with 19 activities, it was found that our approach could produce useful LR systems to distinguish 167 out of a possible 171 activity pairings. The same approach was extended to analyse likelihoods for multiple activities (or groups of activities) simultaneously and create activity timelines to aid in both the early and latter stages of forensic investigations. The dataset and all code required to replicate the results have also been made public to encourage further research on this topic.

</details>


### [235] [Adaptive Identification and Modeling of Clinical Pathways with Process Mining](https://arxiv.org/abs/2512.03787)
*Francesco Vitale,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 本文提出了一种基于过程挖掘的两阶段建模方法，利用一致性检查诊断扩展临床路径知识库，以应对疾病变体和组合带来的挑战，并在Synthea COVID-19数据集上验证了其高精度（AUC达95.62%）与模型简洁性（弧度复杂度67.11%）。


<details>
  <summary>Details</summary>
Motivation: 手动基于临床指南和领域知识构建临床路径困难，且难以反映不同疾病变体或组合下的真实最佳实践。

Method: 采用两阶段过程挖掘方法：第一阶段从历史数据中挖掘出某疾病的治疗流程模型；第二阶段通过一致性检查将新数据与参考模型比对，并依据结果扩展知识库，生成面向新疾病变体或组合的特化模型。

Result: 在Synthea模拟的SARS-CoV-2感染数据集上验证，该方法能高精度扩展临床路径知识库，AUC最高达95.62%，同时保持弧度复杂度为67.11%。

Conclusion: 所提方法可有效提升临床路径的知识覆盖广度与适应性，在保证模型简洁性的同时显著提高建模精度，适用于复杂多变的真实临床场景。

Abstract: Clinical pathways are specialized healthcare plans that model patient treatment procedures. They are developed to provide criteria-based progression and standardize patient treatment, thereby improving care, reducing resource use, and accelerating patient recovery. However, manual modeling of these pathways based on clinical guidelines and domain expertise is difficult and may not reflect the actual best practices for different variations or combinations of diseases. We propose a two-phase modeling method using process mining, which extends the knowledge base of clinical pathways by leveraging conformance checking diagnostics. In the first phase, historical data of a given disease is collected to capture treatment in the form of a process model. In the second phase, new data is compared against the reference model to verify conformance. Based on the conformance checking results, the knowledge base can be expanded with more specific models tailored to new variants or disease combinations. We demonstrate our approach using Synthea, a benchmark dataset simulating patient treatments for SARS-CoV-2 infections with varying COVID-19 complications. The results show that our method enables expanding the knowledge base of clinical pathways with sufficient precision, peaking to 95.62% AUC while maintaining an arc-degree simplicity of 67.11%.

</details>


### [236] [EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification](https://arxiv.org/abs/2512.03804)
*Hanhui Deng,Xinglin Li,Jie Luo,Zhanpeng Jin,Di Wu*

Main category: cs.LG

TL;DR: 本文提出了一种基于EfficientNet改进的轻量级深度学习模型EfficientECG，用于心电图（ECG）信号的自动特征提取与分类，并进一步引入跨注意力机制实现多导联ECG与患者多模态特征（如性别、年龄）的融合分析，在精度、多特征融合和模型轻量化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有ECG诊断模型误诊率高，且难以兼顾高精度、多特征融合与计算效率；亟需一种能自动提取特征、快速准确诊断并减轻医护人员负担的深度学习方案。

Method: 1）基于EfficientNet构建轻量高效的一维ECG分类模型EfficientECG，适配高频长序列多导联数据；2）在其基础上设计跨注意力机制的特征融合模块，整合ECG时序特征与患者静态属性（如性别、年龄）等多源信息。

Result: 在多个代表性ECG数据集上的实验表明，所提模型在分类精度、多特征融合能力及模型轻量化（参数量与推理延迟）方面均优于当前最优方法。

Conclusion: 本文验证了端到端轻量深度学习框架结合跨模态注意力融合在ECG智能诊断中的有效性与实用性，为临床辅助决策提供了高可靠、低开销的新路径。

Abstract: Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.

</details>


### [237] [Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA](https://arxiv.org/abs/2512.03805)
*Tai Nguyen,Phong Le,André Biedenkapp,Carola Doerr,Nguyen Dang*

Main category: cs.LG

TL;DR: 本文系统分析了深度强化学习（DDQN和PPO）在动态算法配置（DAC）任务中的表现，揭示了其面临的可扩展性下降与学习不稳定性两大挑战，并分别提出自适应奖励偏移机制（解决欠探索）和无折扣学习（解决规划范围覆盖），最终使DDQN在样本效率上远超现有DAC方法。


<details>
  <summary>Details</summary>
Motivation: 将强化学习应用于动态算法配置（DAC）面临挑战且依赖大量领域知识，亟需系统性分析与针对性改进。

Method: 以控制(1+(λ,λ))-GA在OneMax问题上的种群规模参数为案例，系统评估DDQN与PPO；提出自适应奖励偏移机制改善DDQN探索能力，并采用无折扣学习提升规划覆盖；对PPO进行超参敏感性分析。

Result: DDQN通过自适应奖励偏移和无折扣学习显著提升稳定性与可扩展性，性能媲美理论最优策略且样本效率高出若干数量级；PPO受方差和超参强依赖限制，难以泛化。

Conclusion: DDQN经针对性改进后成为DAC更优选择，而PPO在该任务中存在根本性局限；所提机制具有通用性，可推广至其他基于RL的算法配置任务。

Abstract: Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($λ$,$λ$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.

</details>


### [238] [Log Probability Tracking of LLM APIs](https://arxiv.org/abs/2512.03816)
*Timothée Chauvin,Erwan Le Merrer,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: 本文提出了一种基于LLM输出token的logprob均值进行低成本、高灵敏度API模型变更监测的方法，仅需单token输出即可检测到微小模型更新（如一次微调），比现有方法便宜1000倍，并构建TinyChange基准评估灵敏度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM API审计方法成本过高，难以定期、大规模实施，导致模型更新缺乏有效监控，影响下游应用可靠性与研究可复现性。

Method: 利用LLM非确定性但具统计规律性的logprobs，设计基于单token输出logprob均值的简单统计检验方法，实现持续、低成本监控。

Result: 该方法能检测到仅一步微调引起的模型变化，灵敏度高于现有方法，且计算开销降低1000倍；并发布了TinyChange基准用于量化评估审计方法对微小真实模型变更的敏感性。

Conclusion: logprobs虽非确定性，但其统计特性足以支撑高效、灵敏、低成本的LLM API持续监控，为保障模型一致性提供了实用新路径。

Abstract: When using an LLM through an API provider, users expect the served model to remain consistent over time, a property crucial for the reliability of downstream applications and the reproducibility of research. Existing audit methods are too costly to apply at regular time intervals to the wide range of available LLM APIs. This means that model updates are left largely unmonitored in practice. In this work, we show that while LLM log probabilities (logprobs) are usually non-deterministic, they can still be used as the basis for cost-effective continuous monitoring of LLM APIs. We apply a simple statistical test based on the average value of each token logprob, requesting only a single token of output. This is enough to detect changes as small as one step of fine-tuning, making this approach more sensitive than existing methods while being 1,000x cheaper. We introduce the TinyChange benchmark as a way to measure the sensitivity of audit methods in the context of small, realistic model changes.

</details>


### [239] [Transmit Weights, Not Features: Orthogonal-Basis Aided Wireless Point-Cloud Transmission](https://arxiv.org/abs/2512.03819)
*Junlin Chang,Yubo Han,Hnag Yue,John S Thompson,Rongke Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度联合信源信道编码（DeepJSCC）的语义无线传输框架，用于三维点云传输，通过发送组合权重而非原始特征，并采用折叠解码器重建3D点云，在带宽受限下显著优于SEPT。


<details>
  <summary>Details</summary>
Motivation: 深度传感器普及降低了点云获取门槛，但高效、鲁棒的无线点云传输仍面临挑战，尤其在带宽受限场景下。

Method: 基于DeepJSCC构建语义传输框架：发射端预测接收端语义正交特征池上的组合权重；接收端采用基于折叠的解码器将2D网格变形为3D点云；联合优化Chamfer距离与正交性正则项。

Result: 在ModelNet40数据集上，高带宽时性能与SEPT相当，低带宽时PSNR和CD指标均明显优于SEPT；消融实验证明正交化和折叠先验的有效性。

Conclusion: 语义驱动的权重传输与几何感知的折叠解码可兼顾紧凑性、鲁棒性与重建保真度，为带宽受限下的点云无线传输提供了有效新范式。

Abstract: The widespread adoption of depth sensors has substantially lowered the barrier to point-cloud acquisition. This letter proposes a semantic wireless transmission framework for three dimension (3D) point clouds built on Deep Joint Source - Channel Coding (DeepJSCC). Instead of sending raw features, the transmitter predicts combination weights over a receiver-side semantic orthogonal feature pool, enabling compact representations and robust reconstruction. A folding-based decoder deforms a 2D grid into 3D, enforcing manifold continuity while preserving geometric fidelity. Trained with Chamfer Distance (CD) and an orthogonality regularizer, the system is evaluated on ModelNet40 across varying Signal-to-Noise Ratios (SNRs) and bandwidths. Results show performance on par with SEmantic Point cloud Transmission (SEPT) at high bandwidth and clear gains in bandwidth-constrained regimes, with consistent improvements in both Peak Signal-to-Noise Ratio (PSNR) and CD. Ablation experiments confirm the benefits of orthogonalization and the folding prior.

</details>


### [240] [DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training](https://arxiv.org/abs/2512.03847)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Yuhui Wang,Sixian Li,Junjie Ye,Honglin Guo,Shichun Liu,Chenhao Huang,Yajie Yang,Junlin Shang,Senjie Jin,Ming Zhang,Jiazheng Zhang,Caishuang Huang,Yunke Zhang,Demei Yan,Yuran Wang,Tao Gui*

Main category: cs.LG

TL;DR: 本文提出DVPO框架，结合条件风险理论与分布值建模，在噪声监督下提升LLM后训练的鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实部署中监督信号常存在噪声或缺失，导致RL训练不稳定、泛化差；现有方法（如PPO、CQL）难以兼顾鲁棒性与泛化性，易产生过于保守的策略。

Method: DVPO采用分布式价值建模学习token级价值分布，并引入非对称风险正则化：压缩下尾以抑制噪声负偏差，扩展上尾以保留探索多样性。

Result: 在多轮对话、数学推理和科学问答等任务中，DVPO在噪声监督下持续优于PPO、GRPO及鲁棒Bellman式PPO。

Conclusion: DVPO有效平衡了鲁棒性与泛化性，为真实场景下的LLM后训练提供了新范式。

Abstract: Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.

</details>


### [241] [Scalable Decision Focused Learning via Online Trainable Surrogates](https://arxiv.org/abs/2512.03861)
*Gaetano Signorelli,Michele Lombardi*

Main category: cs.LG

TL;DR: 本文提出了一种加速决策导向学习（Decision Focused Learning）的 surrogate 方法，通过无偏估计器替代高成本损失函数评估，并支持黑箱优化与动态调整，显著减少内层求解器调用，同时保持解质量。


<details>
  <summary>Details</summary>
Motivation: 传统估计器用于优化问题参数估计时易导致次优解；决策导向学习虽可缓解该问题，但训练开销大、可扩展性差。

Method: 设计基于无偏估计器的高效 surrogate 损失函数，支持黑箱优化、置信度评估及必要时回退到精确计算。

Result: 在实验中显著减少了昂贵的内层求解器调用次数，解质量与当前最优方法相当。

Conclusion: 所提 surrogate 方法在保证决策性能的同时提升了训练效率，适用于含不确定性与实际约束的复杂优化场景。

Abstract: Decision support systems often rely on solving complex optimization problems that may require to estimate uncertain parameters beforehand. Recent studies have shown how using traditionally trained estimators for this task can lead to suboptimal solutions. Using the actual decision cost as a loss function (called Decision Focused Learning) can address this issue, but with a severe loss of scalability at training time. To address this issue, we propose an acceleration method based on replacing costly loss function evaluations with an efficient surrogate. Unlike previously defined surrogates, our approach relies on unbiased estimators reducing the risk of spurious local optima and can provide information on its local confidence allowing one to switch to a fallback method when needed. Furthermore, the surrogate is designed for a black-box setting, which enables compensating for simplifications in the optimization model and account- ing for recourse actions during cost computation. In our results, the method reduces costly inner solver calls, with a solution quality comparable to other state-of-the-art techniques.

</details>


### [242] [Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment](https://arxiv.org/abs/2512.03864)
*Danny Hoang,Anandkumar Patel,Ruimen Chen,Rajiv Malhotra,Farhad Imani*

Main category: cs.LG

TL;DR: This paper compares energy consumption, accuracy, and speed of common AI models versus HyperDimensional Computing (HDC) for geometric quality prediction in smart machining; HDC achieves comparable accuracy with drastically reduced energy use (200× less training, 175–1000× less inference) and faster execution (200× faster training, 300–600× faster inference).


<details>
  <summary>Details</summary>
Motivation: Smart manufacturing improves efficiency and reduces energy consumption, but the high energy demands of conventional AI models may offset these benefits.

Method: The study uses in-situ sensing-based prediction of geometric quality in smart machining to benchmark common AI models against HyperDimensional Computing (HDC).

Result: HDC achieves accuracy comparable to conventional AI models while reducing energy consumption by 200× for training and 175–1000× for inference; it also reduces training time by 200× and inference time by 300–600×.

Conclusion: HDC is a highly promising alternative for energy-efficient and fast AI deployment in smart manufacturing.

Abstract: Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200$\times$ for training and 175 to 1000$\times$ for inference. Furthermore, HDC reduces training times by 200$\times$ and inference times by 300 to 600$\times$, showcasing its potential for energy-efficient smart manufacturing.

</details>


### [243] [Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models](https://arxiv.org/abs/2512.03882)
*Haidong Kang,Wei Wu,Hanling Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为ACraft的自动化攻击方法，利用大语言模型（LLM）和基于PPO的强化学习，无需人工专家即可为少样本类增量学习（FSCIL）定制高效、低成本攻击，显著削弱现有FSCIL方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有FSCIL研究多关注提升模型性能，忽视其安全性；人工设计的攻击方法（如PGD、FGSM）对FSCIL效果差或成本高，亟需专用于FSCIL的自动化攻击方案。

Method: 提出ACraft方法：1）利用LLM自动搜索和生成针对FSCIL的攻击策略；2）引入基于PPO的强化学习优化LLM的攻击策略生成过程，通过正向反馈机制持续提升攻击质量。

Result: 在主流FSCIL基准上，ACraft显著降低SOTA方法性能，效果远超人工设计攻击方法，同时保持最低攻击成本。

Conclusion: ACraft验证了FSCIL系统的安全脆弱性，证明自动化、低开销、高性能的攻击是可行的，为FSCIL安全评估与防御提供了新范式。

Abstract: Few-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack.

</details>


### [244] [Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2512.03899)
*Janis Keck,Lukas Silvester Barth,Fatemeh,Fahimi,Parvaneh Joharinad,Jürgen Jost*

Main category: cs.LG

TL;DR: 本文提出了一种基于概率测度的模糊单纯复形集新框架，将UMAP等方法中的模糊权重解释为随机尺度下Vietoris-Rips滤过的累积分布函数，并统一了模糊集合与概率建模、信息散度及逻辑运算的关系，从而支持系统性地设计新降维算法。


<details>
  <summary>Details</summary>
Motivation: 模糊单纯复形集（如UMAP中所用）缺乏清晰的概率解释，使其与主流的统计学习和概率建模框架脱节，亟需一个统一的概率理论基础。

Method: 将模糊单纯复形集建模为单纯复形集上概率测度的边缘分布；引入随机尺度下的Vietoris-Rips/Čech滤过生成模型；结合面偏序集上的概率模型、模糊交叉熵与KL散度分析，以及布尔运算导出t-范数/t-余范数。

Result: 揭示UMAP模糊权重本质是距离的累积分布函数；建立模糊单纯复形集与概率模型、信息论及逻辑运算的统一联系；推导出基于Čech滤过与三元组采样的新嵌入方法并验证其有效性。

Conclusion: 该概率框架为模糊单纯复形集提供了坚实的理论基础，阐明了UMAP的生成机制，并为系统化构造新型流形学习与降维方法开辟了新路径。

Abstract: Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.

</details>


### [245] [Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations](https://arxiv.org/abs/2512.03923)
*Xiang Rao,Yina Liu,Yuxuan Shen*

Main category: cs.LG

TL;DR: 本文提出了一种离散变量（DV）电路量子-经典物理信息神经网络（QCPINN），用于求解油藏渗流三类典型偏微分方程，通过结合量子计算优势与物理约束，在参数效率和高维非线性拟合上优于传统PINNs。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法存在网格依赖误差和高计算成本，经典PINNs在参数效率、高维表达和强非线性拟合方面存在瓶颈。

Method: 提出DV-Circuit QCPINN架构，融合经典预/后处理网络与基于量子叠加和纠缠的DV量子核心，并嵌入物理约束；测试Cascade、Cross-mesh、Alternate三种量子电路拓扑结构。

Result: QCPINN在三类渗流模型（压力扩散方程、Buckley-Leverett方程、考虑吸附的对流-扩散方程）中均实现高预测精度，且参数量少于经典PINN；Alternate拓扑在单相与两相流中表现最优，Cascade在组分流动中更优。

Conclusion: QCPINN首次成功应用于油藏工程渗流模拟，验证了其可行性，推动了量子计算在油气工业实践中的落地。

Abstract: Solving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering.

</details>


### [246] [Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization](https://arxiv.org/abs/2512.03928)
*Michele Alessi,Alessio Ansuini,Alex Rodriguez*

Main category: cs.LG

TL;DR: 本文提出DiVAE，一种轻量级、数据驱动的正则化方法，通过将VAE的对数先验概率与数据估计的对数密度对齐，提升隐空间密度建模和OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 标准VAE使用简单先验（如标准正态分布），忽略了数据空间中潜在的密度结构，导致隐空间分布与真实数据密度不一致，影响OOD检测与解释性。

Method: DiVAE在ELBO中加入一个鲁棒的、精度加权的惩罚项，使隐变量后验质量按数据空间密度比例分配；若先验可学习，则引导其向高密度区域偏移。该正则项计算开销极小。

Result: 在合成数据上，DiVAE显著提升隐空间对数密度与真实密度的对齐度、先验覆盖度及OOD不确定性校准能力；在MNIST上，提升了先验与外部密度估计的一致性，并改善了可学习先验下的OOD检测性能。

Conclusion: DiVAE是一种高效且通用的VAE增强方法，无需修改模型架构，即可提升密度建模质量、OOD检测与可解释性。

Abstract: We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability $\log p_Z(z)$ with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors.

</details>


### [247] [Technical Report on Text Dataset Distillation](https://arxiv.org/abs/2512.03967)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Edson Bollis,Lucas Pellicer,Rosimeire Pereira Costa,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 本文综述了文本数据集蒸馏的研究进展，涵盖其动机、方法、结果与结论。


<details>
  <summary>Details</summary>
Motivation: 文本数据集蒸馏研究起步较晚，早期借鉴视觉领域方法，但因文本离散性等模态特性面临独特挑战，亟需独立发展和系统梳理。

Method: 采用文献综述法，梳理文本数据集蒸馏的发展脉络，包括基于Transformer模型、生成离散合成文本、扩展至十亿参数级解码器模型等关键策略。

Result: 总结了该领域的若干里程碑进展，并指出当前仍存在基准测试不统一、难以处理文本离散性、复杂任务适配不足及缺乏实际应用案例等挑战。

Conclusion: 文本数据集蒸馏尚处于成熟初期，需在标准化评估、建模离散性、任务泛化性和落地应用等方面进一步突破。

Abstract: In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.

</details>


### [248] [Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning](https://arxiv.org/abs/2512.03973)
*Franki Nguimatsia Tiofack,Théotime Le Hellard,Fabian Schramm,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.LG

TL;DR: 本文提出了Guided Flow Policy (GFP)方法，通过将多步流匹配策略与蒸馏的单步执行器耦合，并利用加权行为克隆聚焦于高价值动作，从而在离线强化学习中实现对数据集中最优转移的对齐和批评家最大化，显著提升了在多个基准测试上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习中的行为正则化方法无法区分高价值与低价值动作，导致策略优化效果受限。

Method: 提出Guided Flow Policy (GFP)，结合多步流匹配策略与蒸馏的单步actor；actor通过加权行为克隆优先模仿高价值动作，flow policy则约束actor保持与数据集中最优转移一致并最大化critic。

Result: 在OGBench、Minari和D4RL的144个状态和像素任务上达到SOTA性能，尤其在次优数据集和困难任务上提升显著。

Conclusion: GFP通过actor与flow policy之间的相互引导机制，有效解决了离线RL中行为正则化缺乏价值感知的问题，提升了策略泛化性与鲁棒性。

Abstract: Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/

</details>


### [249] [Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs](https://arxiv.org/abs/2512.03994)
*Oren Rachmil,Roy Betser,Itay Gershon,Omer Hofman,Nitay Yakoby,Yuval Meron,Idan Yankelev,Asaf Shabtai,Yuval Elovici,Roman Vainshtein*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练、高效的策略违规检测方法，将问题建模为分布外（OOD）检测，通过白化隐藏层激活并使用欧氏范数作为合规性评分，在政策基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 企业亟需可靠机制来检测大语言模型在法律、金融、医疗等敏感领域中对内部组织政策的违反，而现有方法（如通用安全过滤器、LLM-as-a-judge、微调）存在覆盖窄、延迟高、不可解释或部署重等问题。

Method: 提出一种训练无关的方法：对LLM隐藏层激活应用白化线性变换（去相关+零均值单位方差），使其协方差近似单位阵；在此空间中，用欧氏范数作为策略违规的合规性得分。仅需策略文本和少量示例样本。

Result: 在具有挑战性的政策基准测试中，该方法超越现有guardrails和微调推理模型，达到SOTA性能；具备轻量、可解释、低延迟和易部署优势。

Conclusion: 该工作为组织提供了实用、统计严谨的LLM策略合规监督框架，推动可部署AI治理的发展。

Abstract: Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection

</details>


### [250] [Physics-Embedded Gaussian Process for Traffic State Estimation](https://arxiv.org/abs/2512.04004)
*Yanlin Chen,Kehua Chen,Yinhai Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新型物理嵌入高斯过程（PEGP）框架，通过基于经典交通流模型设计多输出核函数，将物理先验与数据驱动方法结合，提升低渗透率下交通状态估计的准确性与不确定性校准能力。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动方法缺乏物理解释且泛化性差，而纯物理模型难以处理不确定性与真实交通复杂性；现有物理嵌入高斯过程方法依赖人工调参、不确定性校准不充分、对模型误设敏感。

Method: 提出Physics-Embedded Gaussian Process（PEGP），构建两种受ARZ和LWR交通模型启发的多输出核函数，显式应用线性化微分算子嵌入物理结构；在变分推断框架中实现无需伪观测或软约束的端到端物理融合。

Result: 在HighD与NGSIM数据集上，PEGP-ARZ在稀疏观测下更鲁棒，PEGP-LWR在密集观测下误差更低；消融实验表明PEGP-ARZ残差符合物理规律且不确定性可校准，PEGP-LWR残差更正交、方差场近似恒定。

Conclusion: PEGP框架成功融合物理先验与不确定性量化，在低渗透率交通状态估计中提供更可靠、可解释的支持。

Abstract: Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.

</details>


### [251] [Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics](https://arxiv.org/abs/2512.04006)
*Connall Garrod,Jonathan P. Keating,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 本文深入分析了多类交叉熵（CE）优化动力学，首次证明了CE梯度流在神经坍缩几何上的全局收敛性，并构建了一个显式的Lyapunov函数来克服非凸景观中的虚假临界点。


<details>
  <summary>Details</summary>
Motivation: 现有理论常简化交叉熵损失为平方损失或限制于凸模型，忽略了其本质行为；而CE与平方损失的动力学本质不同，且凸线性模型无法刻画非凸优化的复杂性。

Method: 通过分析一个具有标准基向量输入的典型两层线性神经网络（最简非凸扩展），结合Hadamard初始化使softmax算子对角化、冻结权重矩阵奇异向量等关键技巧，构造显式Lyapunov函数。

Result: 证明了CE梯度流在此模型下全局收敛至神经坍缩几何，首次确立了该几何在CE训练下的理论基础，并揭示了Hadamard初始化对分析CE动力学的普适价值。

Conclusion: 本文突破了传统理论对CE损失的简化假设，为理解深度学习中CE训练的非凸动力学提供了新工具和理论保证。

Abstract: Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.

</details>


### [252] [Efficient Public Verification of Private ML via Regularization](https://arxiv.org/abs/2512.04008)
*Zoë Ruha Bell,Anvith Thudi,Olive Franzese-McLaughlin,Nicolas Papernot,Shafi Goldwasser*

Main category: cs.LG

TL;DR: 本文提出了一种新型差分隐私（DP）随机凸优化（DP-SCO）算法，在保持近似最优隐私-效用权衡的同时，显著降低了DP保证的验证计算开销，使其低于模型训练本身的开销。


<details>
  <summary>Details</summary>
Motivation: 现有DP训练算法的隐私保证验证成本与训练成本相当，缺乏高效、低成本的验证方法，尤其对数据提供者和公众而言不实用。

Method: 通过私有地最小化一系列正则化目标函数，并仅使用标准DP组合界，实现紧致的隐私-效用权衡。

Result: 首次实现了DP-SCO中近似最优隐私-效用权衡且验证开销低于训练开销的算法；验证计算复杂度显著降低，尤其适用于大规模数据集。

Conclusion: 该工作为可验证差分隐私提供了新范式，弥合了理论最优性与实际可验证性之间的鸿沟，推动DP在现实系统中的可信部署。

Abstract: Training with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets.

</details>


### [253] [Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions](https://arxiv.org/abs/2512.04034)
*Hong Yang,Devroop Kar,Qi Yu,Alex Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 本文通过信息论视角首次理论解释了为何最先进的OOD检测方法在单域数据集上训练时会 catastrophically 失败，指出其根源在于监督学习导致的域特征坍缩（I(x_d; z) = 0），并提出通过域过滤保留域信息可缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 解释为何当前先进OOD检测方法在单域训练下表现极差这一令人困惑的经验现象，并揭示监督学习在窄域场景下的根本局限。

Method: 基于信息瓶颈理论进行理论推导，证明单域监督学习必然导致域特征坍缩；利用Fano不等式量化实际中的部分坍缩；构建Domain Bench单域基准并采用预训练表示进行域过滤实验验证。

Result: 理论证明单域训练导致I(x_d; z)=0，即域信息完全丢失；实验显示域过滤能有效恢复OOD检测性能（如MNIST上FPR@95从53%显著改善）；Domain Bench验证了该失效模式的普适性。

Conclusion: 单域监督学习固有地丢弃域信息，造成OOD检测失败；保留域信息（如通过域过滤）是解决该问题的关键；该发现对迁移学习及微调/冻结预训练模型策略具有重要启示。

Abstract: Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.

</details>


### [254] [MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking](https://arxiv.org/abs/2512.04044)
*Yizhou Zhao,Zhiwei Steven Wu,Adam Block*

Main category: cs.LG

TL;DR: 本文提出MarkTune，一种基于奖励驱动的微调框架，用于在开源大语言模型中嵌入高质量、高检测率的水印，显著优于现有方法GaussMark，并具备抗攻击与跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开放权重语言模型使得依赖推理时干预的传统水印方法失效，而现有权重修改类水印（如GaussMark）难以兼顾检测能力与生成质量。

Method: MarkTune是一种理论严谨的on-policy微调框架，将GaussMark水印信号建模为奖励函数，并联合正则化以维持文本生成质量；其通过表征空间内细粒度、水印感知的权重更新实现优化。

Result: MarkTune在保持生成质量的同时显著提升水印检测能力，逼近推理时水印性能；对改写和微调攻击鲁棒；且在未见数据集上具有良好泛化性。

Conclusion: MarkTune为开源大语言模型提供了一种通用、鲁棒、高质量的水印嵌入策略。

Abstract: Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.

</details>


### [255] [Convergence for Discrete Parameter Updates](https://arxiv.org/abs/2512.04051)
*Paul Wilson,Fabio Zanasi,George Constantinides*

Main category: cs.LG

TL;DR: 本文提出了一种离散更新规则的量化训练新方法，避免了传统方法中对连续更新进行量化的问题，并提供了收敛性保证和实证验证。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型需要巨大的计算资源，促使研究人员探索低精度训练方法；而现有量化训练通常依赖于对实值更新进行离散化，存在固有局限。

Method: 提出一种更新规则本身即为离散的方法，不依赖对连续更新的量化；建立一类通用离散方案的收敛性理论保证，并以多项式分布更新规则为具体实现。

Result: 理论证明了所提离散方案的收敛性，并通过实验验证了其有效性；为具有内在离散结构的模型提供了更高效的训练新路径。

Conclusion: 离散更新规则是一种有潜力的低精度训练范式，可绕过传统量化误差，提升训练效率与适用性。

Abstract: Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.

</details>


### [256] [Eval Factsheets: A Structured Framework for Documenting AI Evaluations](https://arxiv.org/abs/2512.04062)
*Florian Bordes,Candace Ross,Justine T Kao,Evangelia Spiliopoulou,Adina Williams*

Main category: cs.LG

TL;DR: 本文提出了Eval Factsheets，一个用于系统化记录AI系统评估方法的结构化框架，旨在提升评估的透明度、可复现性和可比性。


<details>
  <summary>Details</summary>
Motivation: 当前AI评估方法缺乏类似Datasheets和Model Cards的标准化文档框架，导致可复现性、透明度和决策困难。

Method: 提出包含五个维度（Context、Scope、Structure、Method、Alignment）的评估特征分类法，并设计为五部分问卷形式，含必填与推荐填写项。

Result: 通过多个基准的案例研究验证了该框架能统一描述传统基准与LLM-as-judge等新型评估范式，保持一致性与可比性。

Conclusion: Eval Factsheets有望被广泛采纳于新旧评估框架中，推动AI评估实践的标准化、透明化与可复现性提升。

Abstract: The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.

</details>


### [257] [Fare Comparison App of Uber, Ola and Rapido](https://arxiv.org/abs/2512.04065)
*Ashlesha Gopinath Sawant,Sahil S. Jadhav,Vidhan R. Jain,Shriraj S. Jagtap,Prachi Jadhav,Soham Jadhav,Ichha Raina*

Main category: cs.LG

TL;DR: 本文介绍了一个基于Web的应用程序，用于比较Ola、Uber和Rapido三家出行平台的车费与预估时间，帮助用户选择最经济高效的出行方式，后端使用Python实现数据获取与最优方案推荐，并探讨了API调用、模拟器测试及定位技术等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决用户在众多网约车平台中难以快速选择性价比最高服务的问题，提升出行效率与用户体验，并增强服务透明度。

Method: 开发一个Web应用程序，通过调用各平台API获取实时计价与预估时间数据；后端采用Python处理数据并进行比价分析；同时利用Android Studio模拟器、Appium及定位技术辅助测试与验证。

Result: 实现了跨平台（Ola/Uber/Rapido）的实时车费与时间对比功能，能为用户推荐最优出行选项，并验证了多源API集成与移动端测试的技术可行性。

Conclusion: 该系统有效提升了用户在网约车服务中的决策效率与体验，验证了多平台数据整合与自动化测试方法的实用性，为出行服务透明化提供了可行的技术路径。

Abstract: In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.

</details>


### [258] [Learning Steerable Clarification Policies with Collaborative Self-play](https://arxiv.org/abs/2512.04068)
*Jonathan Berant,Maximillian Chen,Adam Fisch,Reza Aghajani,Fantine Huot,Mirella Lapata,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 本文提出了一种基于自博弈和强化自训练（ReST）的可调节策略，用于AI助手在面对模糊或不明确查询时，根据上下文（如用户偏好、交互模态）和给定的澄清成本，动态选择直接回答、列举多种意图或提问澄清，从而最大化成本惩罚下的准确率。


<details>
  <summary>Details</summary>
Motivation: AI助手需根据上下文（如设备屏幕大小、语音交互等）灵活应对模糊查询，但现有不确定性管理策略缺乏可调节性和泛化性。

Method: 采用双智能体自博弈生成对话数据，将澄清成本和词生成成本作为输入，以成本惩罚准确率为奖励目标，使用强化自训练（ReST）优化策略模型。

Result: 所提方法实现了对成本参数敏感且可预测的行为调节，在训练未见的成本值上也具备良好泛化能力，显著提升奖励与准确率。

Conclusion: 基于自博弈与ReST的可调节不确定性管理策略，能有效适应多样化交互场景，并支持零样本成本泛化。

Abstract: To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [259] [Multi-Agent Reinforcement Learning and Real-Time Decision-Making in Robotic Soccer for Virtual Environments](https://arxiv.org/abs/2512.03166)
*Aya Taourirte,Md Sohag Mia*

Main category: cs.RO

TL;DR: 本文提出了一种统一的多智能体强化学习（MARL）框架，结合PPO、分层RL（HRL）与平均场理论，显著提升机器人足球等动态对抗环境中多智能体协作的实时性、策略性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法难以兼顾任务的多粒度性（长期策略 vs 瞬时动作）和大规模智能体交互的复杂性，尤其在动态对抗环境（如机器人足球）中面临实时决策、高效协作与维数灾难等挑战。

Method: 1）基于客户端-服务器架构构建PPO基准；2）引入基于options框架的分层RL（HRL），将问题分解为高层轨迹规划（Semi-Markov决策过程）与低层动作执行；3）在HRL中融合平均场理论，将多智能体交互简化为单智能体与群体均值的交互，并设计平均场Actor-Critic算法。

Result: 在Webots 4v4仿真中，平均进球数达5.93（基线4.32），控球率89.1%（基线82.9%），传球准确率92.3%，且训练稳定性增强。

Conclusion: 所提统一MARL框架有效平衡了多粒度任务建模、大规模协作与实时性需求，为复杂动态多智能体系统提供了可扩展、鲁棒且高效的解决方案。

Abstract: The deployment of multi-agent systems in dynamic, adversarial environments like robotic soccer necessitates real-time decision-making, sophisticated cooperation, and scalable algorithms to avoid the curse of dimensionality. While Reinforcement Learning (RL) offers a promising framework, existing methods often struggle with the multi-granularity of tasks (long-term strategy vs. instant actions) and the complexity of large-scale agent interactions. This paper presents a unified Multi-Agent Reinforcement Learning (MARL) framework that addresses these challenges. First, we establish a baseline using Proximal Policy Optimization (PPO) within a client-server architecture for real-time action scheduling, with PPO demonstrating superior performance (4.32 avg. goals, 82.9% ball control). Second, we introduce a Hierarchical RL (HRL) structure based on the options framework to decompose the problem into a high-level trajectory planning layer (modeled as a Semi-Markov Decision Process) and a low-level action execution layer, improving global strategy (avg. goals increased to 5.26). Finally, to ensure scalability, we integrate mean-field theory into the HRL framework, simplifying many-agent interactions into a single agent vs. the population average. Our mean-field actor-critic method achieves a significant performance boost (5.93 avg. goals, 89.1% ball control, 92.3% passing accuracy) and enhanced training stability. Extensive simulations of 4v4 matches in the Webots environment validate our approach, demonstrating its potential for robust, scalable, and cooperative behavior in complex multi-agent domains.

</details>


### [260] [GRAND: Guidance, Rebalancing, and Assignment for Networked Dispatch in Multi-Agent Path Finding](https://arxiv.org/abs/2512.03194)
*Johannes Gaber,Meshal Alharbi,Daniele Gammelli,Gioele Zardini*

Main category: cs.RO

TL;DR: 本文提出了一种结合图神经网络与轻量级优化的混合方法，用于大规模多智能体取送货（MAPD）的终身任务调度，在真实仓库场景中提升了吞吐量并满足实时性要求。


<details>
  <summary>Details</summary>
Motivation: 大型机器人车队在物流场景中日益普遍，微小的控制改进能带来显著的运营效益；现有调度方法在高密度、大规模场景下存在吞吐量瓶颈和实时性挑战。

Method: 采用强化学习训练的图神经网络生成全局自由机器人分布引导信号，通过最小成本流转化为区域间再平衡策略，并辅以局部小规模分配问题求解，兼顾精度与低延迟（<1秒）。

Result: 在League of Robot Runners（LRR）高拥塞仓库基准测试（最多500个机器人）中，相比2024年冠军调度器，吞吐量提升最高达10%，且保持实时执行能力。

Conclusion: 将图结构化学习引导与可解算优化器耦合，可有效缓解拥堵，为大规模机器人车队提供实用、可扩展的高吞吐调度方案。

Abstract: Large robot fleets are now common in warehouses and other logistics settings, where small control gains translate into large operational impacts. In this article, we address task scheduling for lifelong Multi-Agent Pickup-and-Delivery (MAPD) and propose a hybrid method that couples learning-based global guidance with lightweight optimization. A graph neural network policy trained via reinforcement learning outputs a desired distribution of free agents over an aggregated warehouse graph. This signal is converted into region-to-region rebalancing through a minimum-cost flow, and finalized by small, local assignment problems, preserving accuracy while keeping per-step latency within a 1 s compute budget. On congested warehouse benchmarks from the League of Robot Runners (LRR) with up to 500 agents, our approach improves throughput by up to 10% over the 2024 winning scheduler while maintaining real-time execution. The results indicate that coupling graph-structured learned guidance with tractable solvers reduces congestion and yields a practical, scalable blueprint for high-throughput scheduling in large fleets.

</details>


### [261] [KALIKO: Kalman-Implicit Koopman Operator Learning For Prediction of Nonlinear Dynamical Systems](https://arxiv.org/abs/2512.03256)
*Albert H. Li,Ivan Dario Jimenez Rodriguez,Joel W. Burdick,Yisong Yue,Aaron D. Ames*

Main category: cs.RO

TL;DR: 本文提出KALIKO方法，利用卡尔曼滤波隐式学习Koopman算子的嵌入表示，无需显式编码器，实现高精度长时序动力学预测与控制。


<details>
  <summary>Details</summary>
Motivation: 传统动力学建模难以处理非线性、混沌和高维系统；Koopman理论虽可将非线性系统线性化，但显式构造合适基函数困难且易导致预测不准或过拟合。

Method: 提出Kalman-Implicit Koopman Operator (KALIKO) 学习方法，结合Koopman理论与卡尔曼滤波，隐式学习状态嵌入及对应的线性潜在动力学，不依赖显式编码器。

Result: 在PDE生成的波浪数据上，KALIKO在开环预测和闭环控制（如抑制欠驱动机械臂载荷受强波扰动）任务中均优于多个基线方法；获得高保真重构与全局线性潜在动力学。

Conclusion: KALIKO为长时序动力学建模提供了一种鲁棒、可解释且无需手工设计基函数的新范式，在机器人预测与控制中具有实用价值。

Abstract: Long-horizon dynamical prediction is fundamental in robotics and control, underpinning canonical methods like model predictive control. Yet, many systems and disturbance phenomena are difficult to model due to effects like nonlinearity, chaos, and high-dimensionality. Koopman theory addresses this by modeling the linear evolution of embeddings of the state under an infinite-dimensional linear operator that can be approximated with a suitable finite basis of embedding functions, effectively trading model nonlinearity for representational complexity. However, explicitly computing a good choice of basis is nontrivial, and poor choices may cause inaccurate forecasts or overfitting. To address this, we present Kalman-Implicit Koopman Operator (KALIKO) Learning, a method that leverages the Kalman filter to implicitly learn embeddings corresponding to latent dynamics without requiring an explicit encoder. KALIKO produces interpretable representations consistent with both theory and prior works, yielding high-quality reconstructions and inducing a globally linear latent dynamics. Evaluated on wave data generated by a high-dimensional PDE, KALIKO surpasses several baselines in open-loop prediction and in a demanding closed-loop simulated control task: stabilizing an underactuated manipulator's payload by predicting and compensating for strong wave disturbances.

</details>


### [262] [GOMP: Grasped Object Manifold Projection for Multimodal Imitation Learning of Manipulation](https://arxiv.org/abs/2512.03347)
*William van den Bogert,Gregory Linkowski,Nima Fazeli*

Main category: cs.RO

TL;DR: 本文提出Grasped Object Manifold Projection (GOMP)，一种通过将非刚性抓取物体约束到低维流形来缓解模仿学习中累积误差的交互式方法，所有增强均从同一专家数据集学习，并结合n臂老虎机进行交互调整。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在工业装配等重复性操作任务中潜力巨大，但常受限于因累积误差导致的轨迹精度不足。

Method: 提出GOMP方法，将非刚性抓取物体约束到低维流形；基于同一专家数据集学习增强模块，并引入n臂老虎机实现交互式调整；提供理论分析以改进IL中经典的累积误差界。

Result: 在四个需高精度的装配任务上验证了GOMP的有效性，使用触觉反馈，且方法具有模态无关性。

Conclusion: GOMP能有效提升模仿学习在精细操作任务中的精度，无需额外专家数据，具备实用性和泛化能力。

Abstract: Imitation Learning (IL) holds great potential for learning repetitive manipulation tasks, such as those in industrial assembly. However, its effectiveness is often limited by insufficient trajectory precision due to compounding errors. In this paper, we introduce Grasped Object Manifold Projection (GOMP), an interactive method that mitigates these errors by constraining a non-rigidly grasped object to a lower-dimensional manifold. GOMP assumes a precise task in which a manipulator holds an object that may shift within the grasp in an observable manner and must be mated with a grounded part. Crucially, all GOMP enhancements are learned from the same expert dataset used to train the base IL policy, and are adjusted with an n-arm bandit-based interactive component. We propose a theoretical basis for GOMP's improvement upon the well-known compounding error bound in IL literature. We demonstrate the framework on four precise assembly tasks using tactile feedback, and note that the approach remains modality-agnostic. Data and videos are available at williamvdb.github.io/GOMPsite.

</details>


### [263] [Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing](https://arxiv.org/abs/2512.03397)
*Seungwon Choi,Dong-Gyu Park,Seo-Yeon Hwang,Tae-Wan Kim*

Main category: cs.RO

TL;DR: 本文提出Surfel-LIO，采用分层体素结构（hVox）与预计算的surfel表示，实现O(1)对应关系检索，避免运行时邻域枚举和平面拟合，并结合Z-order曲线提升缓存友好性，在保持定位精度的同时显著提升处理速度。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR惯性里程计（LIO）系统在最近邻搜索和平面参数重复计算方面仍存在效率瓶颈：（1）平面拟合需遍历多个空间单元以获取足够点；（2）地图几何不变时仍反复重算平面参数。

Method: 提出Surfel-LIO方法，核心包括：（1）构建分层体素结构（hVox）；（2）对每个体素预计算surfel（表面元素）表示；（3）利用Z-order曲线进行缓存友好的空间索引；从而实现O(1)级对应检索，无需实时邻域搜索或迭代平面拟合。

Result: 在M3DGR数据集上实验表明，Surfel-LIO相比当前SOTA方法显著提升处理速度，同时保持相当的状态估计精度。代码已开源。

Conclusion: Surfel-LIO通过预计算与高效空间索引设计，有效缓解了LIO中邻域搜索与重复拟合的计算开销，在实时性与精度间取得更好平衡，验证了基于surfel的静态地图表征在LIO中的有效性。

Abstract: LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry.

</details>


### [264] [What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models](https://arxiv.org/abs/2512.03422)
*Tianchen Deng,Yue Pan,Shenghai Yuan,Dong Li,Chen Wang,Mingrui Li,Long Chen,Lihua Xie,Danwei Wang,Jingchuan Wang,Javier Civera,Hesheng Wang,Weidong Chen*

Main category: cs.RO

TL;DR: 本文综述了机器人领域中各类3D场景表示方法（从传统点云、体素、SDF、场景图，到新兴的NeRF、3D高斯泼溅和基础模型），按感知、建图、定位、导航、操作五大模块分析其适用性，并探讨3D基础模型作为未来统一解决方案的发展趋势与挑战。


<details>
  <summary>Details</summary>
Motivation: 当前机器人SLAM与定位多依赖稀疏表示，而下游任务（如导航、避障）亟需更丰富的稠密表征；同时，神经表征可融合语义与语言先验，推动具身智能发展。

Method: 系统性分类与比较分析：将机器人核心模块划分为五类，逐一梳理各类场景表示方法（传统与神经）的标准形式、优劣及在各模块中的适用性；结合发展趋势，重点分析3D基础模型的潜力与瓶颈。

Result: 明确了不同场景表示方法在机器人各功能模块中的适用边界；指出3D基础模型有望成为未来统一表征范式；开源项目持续更新相关技术。

Conclusion: 尚无绝对‘最优’3D表示，选择应依具体任务与模块而定；3D基础模型是重要发展方向，但其实用化仍面临泛化性、实时性、可解释性等挑战。

Abstract: In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.

</details>


### [265] [World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations](https://arxiv.org/abs/2512.03429)
*Raul Steinmetz,Fabio Demo Rosa,Victor Augusto Kich,Jair Augusto Bottega,Ricardo Bedin Grando,Daniel Fernando Tello Gamarra*

Main category: cs.RO

TL;DR: 本文提出了一种基于DreamerV3的新型模型驱动强化学习框架，通过MLP-VAE将高维LIDAR数据压缩为紧凑隐表示，并结合学习的动力学预测器实现高效想象式策略优化，在TurtleBot3仿真导航任务中显著提升收敛速度与成功率（达100%），优于SAC、DDPG和TD3等无模型方法。


<details>
  <summary>Details</summary>
Motivation: 传统无模型强化学习在处理高维LIDAR数据时存在样本效率低、策略网络难以处理全分辨率输入的问题，导致空间感知弱、导航鲁棒性差。

Method: 在DreamerV3基础上构建模型驱动RL框架，引入MLP-VAE作为世界模型组件，将360维LIDAR数据编码为低维隐变量，并联合学习的动力学模型进行想象式策略优化。

Result: 在TurtleBot3仿真导航任务中，该方法比SAC、DDPG、TD3等模型更快收敛且成功率更高；使用完整360维LIDAR数据时达到100%成功率，而模型自由方法均低于85%。

Conclusion: 将预测性世界模型与学习到的隐表示相结合，可显著提升高维传感数据下自主导航的效率与鲁棒性。

Abstract: Autonomous navigation of terrestrial robots using Reinforcement Learning (RL) from LIDAR observations remains challenging due to the high dimensionality of sensor data and the sample inefficiency of model-free approaches. Conventional policy networks struggle to process full-resolution LIDAR inputs, forcing prior works to rely on simplified observations that reduce spatial awareness and navigation robustness. This paper presents a novel model-based RL framework built on top of the DreamerV3 algorithm, integrating a Multi-Layer Perceptron Variational Autoencoder (MLP-VAE) within a world model to encode high-dimensional LIDAR readings into compact latent representations. These latent features, combined with a learned dynamics predictor, enable efficient imagination-based policy optimization. Experiments on simulated TurtleBot3 navigation tasks demonstrate that the proposed architecture achieves faster convergence and higher success rate compared to model-free baselines such as SAC, DDPG, and TD3. It is worth emphasizing that the DreamerV3-based agent attains a 100% success rate across all evaluated environments when using the full dataset of the Turtlebot3 LIDAR (360 readings), while model-free methods plateaued below 85%. These findings demonstrate that integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data.

</details>


### [266] [PerFACT: Motion Policy with LLM-Powered Dataset Synthesis and Fusion Action-Chunking Transformers](https://arxiv.org/abs/2512.03444)
*Davood Soleymanzadeh,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: 本文提出PerFACT框架，包含MotionGeneralizer（利用大语言模型生成多样化工作空间以扩充数据集）和MpiNetsFusion（融合动作分块Transformer架构的通用运动规划网络），在3.5M轨迹数据上训练后显著提升规划速度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经运动规划器受限于小规模人工构建数据集，泛化性差，且单体网络难以有效编码关键规划信息。

Method: 提出MotionGeneralizer（LLM驱动的工作空间生成方法）和MpiNetsFusion（融合动作分块Transformer的通用规划网络），并基于前者合成3.5M轨迹数据进行训练。

Result: MpiNetsFusion在评估任务中规划速度比现有最先进方法快数倍。

Conclusion: 结合大语言模型数据合成与多模态融合Transformer架构，可有效提升神经运动规划器的泛化性与效率。

Abstract: Deep learning methods have significantly enhanced motion planning for robotic manipulators by leveraging prior experiences within planning datasets. However, state-of-the-art neural motion planners are primarily trained on small datasets collected in manually generated workspaces, limiting their generalizability to out-of-distribution scenarios. Additionally, these planners often rely on monolithic network architectures that struggle to encode critical planning information. To address these challenges, we introduce Motion Policy with Dataset Synthesis powered by large language models (LLMs) and Fusion Action-Chunking Transformers (PerFACT), which incorporates two key components. Firstly, a novel LLM-powered workspace generation method, MotionGeneralizer, enables large-scale planning data collection by producing a diverse set of semantically feasible workspaces. Secondly, we introduce Fusion Motion Policy Networks (MpiNetsFusion), a generalist neural motion planner that uses a fusion action-chunking transformer to better encode planning signals and attend to multiple feature modalities. Leveraging MotionGeneralizer, we collect 3.5M trajectories to train and evaluate MpiNetsFusion against state-of-the-art planners, which shows that the proposed MpiNetsFusion can plan several times faster on the evaluated tasks.

</details>


### [267] [MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization](https://arxiv.org/abs/2512.03522)
*Gihyeon Lee,Jungwoo Lee,Juwon Kim,Young-Sik Shin,Younggun Cho*

Main category: cs.RO

TL;DR: 本文提出了一种基于多标签似然的语义图匹配框架，用于解决未知物体类别和语义模糊环境下的机器人全局定位问题，通过上下文感知的似然传播提升语义匹配鲁棒性，并在闭集/开集检测及真实与合成场景中验证了其有效性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 机器人在语义模糊、物体类别未知的环境中进行全局定位时，高语义模糊性易导致物体误分类与错误数据关联，从而引发位姿估计显著误差。

Method: 提出多标签似然语义图匹配框架：构建多标签图表示以建模物体观测的语义上下文，并通过上下文感知的似然传播机制，将每个节点的似然与其邻居的最大似然相融合，增强图间语义对应。

Result: 在闭集与开集目标检测配置下，该方法显著提升了数据关联与位姿估计性能；并在大规模词汇量的真实室内场景和合成环境中验证了其良好可扩展性。

Conclusion: 多标签图表示与上下文感知似然传播能有效缓解语义模糊带来的定位误差，为开放世界语义定位提供了更鲁棒、可扩展的解决方案。

Abstract: Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.

</details>


### [268] [AdaPower: Specializing World Foundation Models for Predictive Manipulation](https://arxiv.org/abs/2512.03538)
*Yuhang Huang,Shilong Zou,Jiazhao Zhang,Xinwang Liu,Ruizhen Hu,Kai Xu*

Main category: cs.RO

TL;DR: AdaPower是一种轻量级适配框架，通过时-空测试时训练（TS-TTT）和记忆持久性（MP）将通用世界基础模型（WFMs）转化为专用世界模型，显著提升预训练视觉语言动作（VLA）策略在机器人控制任务中的成功率，且无需策略重训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法将世界基础模型（WFMs）仅用作合成数据生成器，存在计算开销大、未能充分利用预训练VLA策略的问题；亟需弥合WFMs的生成真实性与机器人控制所需的精度之间的鸿沟。

Method: 提出AdaPower框架，包含两个核心组件：1）时-空测试时训练（TS-TTT），用于推理阶段动态适配；2）记忆持久性（MP），保障长时序预测的一致性；并将其集成于模型预测控制（MPC）框架中，赋能预训练VLA策略。

Result: 在LIBERO基准上任务成功率提升超41%，无需策略重训练，同时保持计算高效性和模型通用能力。

Conclusion: AdaPower有效 bridged the gap between generative realism and control precision，证明了轻量级适配可释放WFMs在具身智能控制中的潜力。

Abstract: World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \textbf{AdaPower} (\textbf{Ada}pt and Em\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.

</details>


### [269] [A Learning-based Control Methodology for Transitioning VTOL UAVs](https://arxiv.org/abs/2512.03548)
*Zexin Lin,Yebin Zhong,Hanwen Wan,Jiu Cheng,Zhenglong Sun,Xiaoqiang Ji*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的耦合式过渡控制方法（ST3M），将巡航模式视为悬停的特例，解决了VTOL无人机在倾转旋翼过渡过程中因质心和推力方向变化导致的振动和控制解耦问题，在仿真与真实环境中均验证了其轨迹跟踪精度高、振动小、控制器可迁移性强。


<details>
  <summary>Details</summary>
Motivation: VTOL无人机在倾转旋翼过渡过程中因质心偏移和推力方向变化，现有解耦式高度与位置控制导致显著振动，且缺乏交互性与适应性。

Method: 提出基于强化学习（RL）的耦合式过渡控制方法ST3M，摒弃传统分阶段过渡思路，将巡航模式建模为悬停的特殊情形，实现统一控制框架。

Result: 在仿真与真实环境中验证成功：控制器开发高效、跨平台迁移性强，能精确控制位置与姿态，显著提升轨迹跟踪性能并降低过渡过程振动。

Conclusion: ST3M方法通过强化学习实现耦合控制与模式统一建模，有效提升了VTOL无人机过渡阶段的稳定性、精度与鲁棒性，为智能自主过渡控制提供了新范式。

Abstract: Transition control poses a critical challenge in Vertical Take-Off and Landing Unmanned Aerial Vehicle (VTOL UAV) development due to the tilting rotor mechanism, which shifts the center of gravity and thrust direction during transitions. Current control methods' decoupled control of altitude and position leads to significant vibration, and limits interaction consideration and adaptability. In this study, we propose a novel coupled transition control methodology based on reinforcement learning (RL) driven controller. Besides, contrasting to the conventional phase-transition approach, the ST3M method demonstrates a new perspective by treating cruise mode as a special case of hover. We validate the feasibility of applying our method in simulation and real-world environments, demonstrating efficient controller development and migration while accurately controlling UAV position and attitude, exhibiting outstanding trajectory tracking and reduced vibrations during the transition process.

</details>


### [270] [RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL](https://arxiv.org/abs/2512.03556)
*Yinzhou Tang,Yu Shang,Yinuo Chen,Bingwen Wei,Xin Zhang,Shu'ang Yu,Liangzhi Shi,Chao Yu,Chen Gao,Wei Wu,Yong Li*

Main category: cs.RO

TL;DR: 本文提出RoboScape-R框架，利用世界模型作为通用环境代理，并设计基于世界模型的内生奖励机制，显著提升具身策略在跨域场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习和强化学习难以实现具身策略在多样化场景中的泛化：IL易过拟合专家轨迹，RL缺乏统一通用的奖励信号；现有世界模型虽能预测观测，但仍依赖人工设计的任务特定奖励，无法提供真正通用的训练环境。

Method: 提出RoboScape-R框架，将世界模型用作强化学习中的通用环境代理，并设计一种新型基于世界模型的通用奖励机制，该机制从模型对真实世界状态转移动力学的内在理解中生成‘内生’奖励。

Result: 实验表明RoboScape-R有效克服传统RL方法局限，提供高效且通用的训练环境，显著提升具身策略泛化能力，在跨域场景下平均性能提升37.5%。

Conclusion: 世界模型可作为在线训练策略的关键组件，其内生奖励机制为构建通用、可泛化的具身智能体提供了新范式。

Abstract: Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.

</details>


### [271] [Multimodal Control of Manipulators: Coupling Kinematics and Vision for Self-Driving Laboratory Operations](https://arxiv.org/abs/2512.03630)
*Shifa Sulaiman,Amarnath H,Simon Bogh,Naresh Marturi*

Main category: cs.RO

TL;DR: 本文实现了三种基于雅可比矩阵的运动规划方案，结合RRT*轨迹规划和基于螺旋理论的正向运动学，对比分析了雅可比转置（JT）、伪逆（PI）和阻尼最小二乘（DLS）三种逆解方法在冗余机械臂带耦合手指夹爪任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 针对冗余机械臂与耦合手指夹爪系统的运动规划问题，需兼顾轨迹平滑性、关节运动连续性及逆运动学求解精度，亟需系统评估不同雅可比类逆解方法的适用性。

Method: 采用RRT*算法生成末端轨迹；基于螺旋理论建立正向运动学模型并计算空间雅可比矩阵与可操作性指标；分别使用雅可比转置（JT）、伪逆（PI）和阻尼最小二乘（DLS）三种方法求解逆运动学；通过仿真分析轨迹平滑度（RMSE）、关节速度/加速度/急动度/跃动度等指标。

Result: 三种雅可比方法在轨迹跟踪误差、关节运动平滑性和计算效率方面表现出差异：DLS在数值稳定性和轨迹精度上整体更优，JT计算快但误差较大，PI居中；可操作性分析揭示了构型对任务执行能力的影响。

Conclusion: DLS方法在综合性能上最适合该冗余机械臂-夹爪系统；研究为类似高自由度耦合机构的任务导向运动规划提供了方法选择依据与评估框架。

Abstract: Motion planning schemes are used for planning motions of a manipulator from an initial pose to a final pose during a task execution. A motion planning scheme generally comprises of a trajectory planning method and an inverse kinematic solver to determine trajectories and joints solutions respectively. In this paper, 3 motion planning schemes developed based on Jacobian methods are implemented to traverse a redundant manipulator with a coupled finger gripper through given trajectories. RRT* algorithm is used for planning trajectories and screw theory based forward kinematic equations are solved for determining joint solutions of the manipulator and gripper. Inverse solutions are computed separately using 3 Jacobian based methods such as Jacobian Transpose (JT), Pseudo Inverse (PI), and Damped Least Square (DLS) methods. Space Jacobian and manipulability measurements of the manipulator and gripper are obtained using screw theory formulations. Smoothness and RMSE error of generated trajectories and velocity continuity, acceleration profile, jerk, and snap values of joint motions are analysed for determining an efficient motion planning method for a given task. Advantages and disadvantages of the proposed motion planning schemes mentioned above are analysed using simulation studies to determine a suitable inverse solution technique for the tasks.

</details>


### [272] [Context-Triggered Contingency Games for Strategic Multi-Agent Interaction](https://arxiv.org/abs/2512.03639)
*Kilian Schweppe,Anne-Kathrin Schmuck*

Main category: cs.RO

TL;DR: 本文提出了一种名为上下文触发的应急博弈（context-triggered contingency games）的新框架，用于解决自主多智能体系统中长期战略目标与短期动态适应之间的平衡问题。该框架结合了基于时序逻辑规范的战略博弈和实时求解的动态应急博弈，并通过两层架构实现：上层使用策略模板保证高层目标满足，下层采用新型因子图求解器支持可扩展、实时的模型预测控制。实验在自动驾驶与机器人导航中验证了其高效性、可靠性与适应性。


<details>
  <summary>Details</summary>
Motivation: 自主多智能体系统需在不确定、交互性强的环境中兼顾长期战略目标与短期动态适应，现有方法难以同时保障安全性与进展性。

Method: 提出上下文触发的应急博弈，构建两层架构：上层基于时序逻辑生成策略模板以保证高层目标；下层设计因子图模型并开发实时求解器，实现动态交互的模型预测控制。

Result: 在自动驾驶和机器人导航的仿真与硬件实验中，所提方法实现了高效、可靠且自适应的多智能体交互，验证了安全性和进展性保障能力。

Conclusion: 上下文触发的应急博弈为自主多智能体系统提供了一个兼具理论保证与工程可行性的新范式，推动了复杂动态环境中协同决策的发展。

Abstract: We address the challenge of reliable and efficient interaction in autonomous multi-agent systems, where agents must balance long-term strategic objectives with short-term dynamic adaptation. We propose context-triggered contingency games, a novel integration of strategic games derived from temporal logic specifications with dynamic contingency games solved in real time. Our two-layered architecture leverages strategy templates to guarantee satisfaction of high-level objectives, while a new factor-graph-based solver enables scalable, real-time model predictive control of dynamic interactions. The resulting framework ensures both safety and progress in uncertain, interactive environments. We validate our approach through simulations and hardware experiments in autonomous driving and robotic navigation, demonstrating efficient, reliable, and adaptive multi-agent interaction.

</details>


### [273] [A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection](https://arxiv.org/abs/2512.03684)
*Shahid Ansari,Mahendra Kumar Gohil,Yusuke Maeda,Bishakh Bhattacharya*

Main category: cs.RO

TL;DR: 本文提出了一种基于混合式软体-刚性机械手的自主番茄采摘系统，结合视觉感知、力控抓取与优化轨迹规划，实现了在复杂环境下轻柔、可靠、全自动的番茄采摘。


<details>
  <summary>Details</summary>
Motivation: 解决传统农业采摘中人工成本高、效率低，以及现有机器人在复杂遮挡、光照变化和易损果实抓取方面存在的抓取不稳、易损伤等问题。

Method: 设计六指软体仿生夹持器（含刚性外骨架与乳胶篮）、Scotch-yoke伺服驱动机构、分离叶片与微型舵机剪切模块；采用RGB-D相机+Detectron2实现语义分割与关键点定位；基于虚功原理建立伺服扭矩-抓力解析模型；通过力敏电阻反馈+PID闭环控制实现抓力调节；结合PSO算法进行5-DOF机械臂轨迹规划。

Result: 实验完成完整采摘循环（接近、分离、剪茎、抓取、运输、释放），平均周期24.34秒，成功率约80%，抓力稳定控制在0.20–0.50 N，有效避免滑脱与果皮损伤。

Conclusion: 所提出的混合式夹持器与视觉-力觉协同控制架构，显著提升了在密集枝叶遮挡场景下的采摘鲁棒性与果实保护性，为软体机器人在农业自动化中的实用化提供了可行方案。

Abstract: This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping. The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting. For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination. An analytical model derived using the principle of virtual work relates servo torque to grasp force, enabling design-level reasoning about actuation requirements. During execution, closed-loop grasp-force regulation is achieved using a proportional--integral--derivative controller with feedback from force-sensitive resistors mounted on selected fingers to prevent slip and bruising. Motion execution is supported by Particle Swarm Optimization (PSO)--based trajectory planning for a 5-DOF manipulator. Experiments demonstrate complete picking cycles (approach, separation, cutting, grasping, transport, release) with an average cycle time of 24.34~s and an overall success rate of approximately 80\%, while maintaining low grasp forces (0.20--0.50~N). These results validate the proposed hybrid gripper and integrated vision--control pipeline for reliable harvesting in cluttered environments.

</details>


### [274] [ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration](https://arxiv.org/abs/2512.03707)
*Sundas Rafat Mulkana,Ronyu Yu,Tanaya Guha,Emma Li*

Main category: cs.RO

TL;DR: 本文提出ContactRL框架，通过强化学习结合力反馈奖励函数，实现人机协作中安全、有意的物理接触，并在仿真和真实实验中验证了其安全性和任务效率。


<details>
  <summary>Details</summary>
Motivation: 在人机协作任务中，安全不仅需要避免碰撞，还需确保安全且有意的物理接触。

Method: 提出ContactRL框架，将接触安全性直接融入基于力反馈的强化学习奖励函数，并结合动能型控制屏障函数（eCBF）进行部署安全保障。

Result: 仿真中安全违规率仅0.2%，任务成功率87.7%；真实UR3e平台360次手物交接实验中法向力始终低于10N。

Conclusion: ContactRL实现了安全与高效并重的人机物理协作，推动了协作机器人在接触密集型任务中的实际部署。

Abstract: In collaborative human-robot tasks, safety requires not only avoiding collisions but also ensuring safe, intentional physical contact. We present ContactRL, a reinforcement learning (RL) based framework that directly incorporates contact safety into the reward function through force feedback. This enables a robot to learn adaptive motion profiles that minimize human-robot contact forces while maintaining task efficiency. In simulation, ContactRL achieves a low safety violation rate of 0.2\% with a high task success rate of 87.7\%, outperforming state-of-the-art constrained RL baselines. In order to guarantee deployment safety, we augment the learned policy with a kinetic energy based Control Barrier Function (eCBF) shield. Real-world experiments on an UR3e robotic platform performing small object handovers from a human hand across 360 trials confirm safe contact, with measured normal forces consistently below 10N. These results demonstrate that ContactRL enables safe and efficient physical collaboration, thereby advancing the deployment of collaborative robots in contact-rich tasks.

</details>


### [275] [Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing](https://arxiv.org/abs/2512.03729)
*Samantha Chapin,Kenneth Stewart,Roxana Leontie,Carl Glen Henshaw*

Main category: cs.RO

TL;DR: 本文介绍了美国海军研究实验室（NRL）的APIARY实验，首次在国际空间站（ISS）上成功实现了基于强化学习（RL）对自由飞行机器人Astrobee的在轨控制，验证了RL在太空机器人自主性中的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 提升空间机器人在零重力环境下的自主控制能力，支持快速开发和部署面向空间探索、物流及实时任务的定制化行为。

Method: 采用PPO（近端策略优化） actor-critic 网络，在NVIDIA Isaac Lab仿真环境中训练6自由度鲁棒控制策略，并通过随机化目标位姿与质量分布增强泛化性；随后开展仿真测试、地面测试与在轨飞行验证。

Result: 2025年5月27日，首次成功实现RL控制自由飞行器Astrobee在ISS上的在轨运行，验证了所训练策略的有效性与鲁棒性。

Conclusion: 该实验标志着强化学习在太空机器人控制中迈出关键一步，为未来空间任务中高效、自适应的自主系统提供了可行路径和技术基础。

Abstract: The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.

</details>


### [276] [Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control](https://arxiv.org/abs/2512.03736)
*Kenneth Stewart,Samantha Chapin,Roxana Leontie,Carl Glen Henshaw*

Main category: cs.RO

TL;DR: This paper presents the first on-orbit demonstration of reinforcement learning (RL) for autonomous control of NASA's Astrobee robot aboard the ISS, using NVIDIA Omniverse simulation and curriculum learning to bridge the Sim2Real gap.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous robotic control in space by overcoming the challenges of training and deploying RL policies in microgravity environments, where real-world data collection is costly and risky.

Method: Trained a deep neural network using reinforcement learning in NVIDIA's Omniverse physics simulator with curriculum learning; deployed the trained policy onboard the free-flying Astrobee robot aboard the ISS to replace standard attitude and translation control.

Result: Successful on-orbit demonstration of RL-based autonomous navigation in microgravity; validation of a Sim2Real pipeline using GPU-accelerated, scientific-grade simulation for efficient Monte Carlo RL training.

Conclusion: Terrestrial RL training with high-fidelity simulation is feasible and transferable to space robotics, supporting future In-Space Servicing, Assembly, and Manufacturing (ISAM) missions requiring rapid on-orbit adaptation.

Abstract: Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.

</details>


### [277] [Cross-embodied Co-design for Dexterous Hands](https://arxiv.org/abs/2512.03743)
*Kehlani Fay,Darin Anthony Djapri,Anya Zorin,James Clinton,Ali El Lahib,Hao Su,Michael T. Tolley,Sha Yi,Xiaolong Wang*

Main category: cs.RO

TL;DR: 本文提出了一种用于灵巧操作的机器人手部形态与控制策略协同设计框架，支持形态搜索、跨实体控制评估及快速实物制造，实现24小时内完成从设计到部署的全流程。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作受限于控制与设计两方面，且尚无关于最优灵巧操纵器设计的共识，亟需一种能联合优化形态与控制的方法。

Method: 提出一种协同设计框架，包含：1）涵盖关节、手指和手掌生成的广泛形态搜索空间；2）基于形态条件的跨实体控制进行可扩展评估；3）使用易获取部件实现真实世界制造。

Result: 在仿真与真实场景的在手旋转等灵巧任务中验证了该框架有效性，实现了24小时内端到端设计、训练、制造与部署新机械手。

Conclusion: 该框架为面向任务的灵巧操纵器提供了高效、可扩展、可落地的协同设计范式，并将开源发布。

Abstract: Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.

</details>


### [278] [Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models](https://arxiv.org/abs/2512.03756)
*Marlon Steiner,Royden Wagner,Ömer Sahin Tas,Christoph Stiller*

Main category: cs.RO

TL;DR: 本文提出了一种将导航信息（如自车路径和目标位姿）融入基于注意力机制的运动预测模型的方法，以提升自动驾驶车辆与交通参与者交互中的预测与规划协同性能，并在nuPlan数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有运动预测与运动规划结合框架面临两大挑战：一是预测难以条件化于导航目标；二是生成的轨迹需保证运动学可行性和稳定性。本文聚焦于前者，旨在弥合多智能体运动预测与基于目标的运动规划之间的鸿沟。

Method: 将自车的意图路线和目标位姿作为导航信息，嵌入到基于注意力机制的运动预测模型架构中；设计并对比多种导航信息集成策略；在nuPlan数据集上进行实验评估。

Result: 实验结果表明，引入导航信息可显著提升运动预测精度，并有利于后续的运动规划任务，验证了预测驱动规划的潜力。

Conclusion: 导航信息的有效融合能够同时增强运动预测与运动规划性能，为实现更安全、鲁棒的自动驾驶交互提供了新思路。

Abstract: Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.

</details>


### [279] [Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control](https://arxiv.org/abs/2512.03772)
*Gabriele Fadini,Deepak Ingole,Tong Duy Son,Alisa Rupenyan*

Main category: cs.RO

TL;DR: 本文提出了一种基于扭矩的非线性模型预测控制（nMPC）自动调参框架，利用高维贝叶斯优化（SAASBO）结合数字孪生，在仿真中高效优化MPC参数，并安全迁移到UR10e机械臂硬件，显著提升轨迹跟踪精度与求解效率。


<details>
  <summary>Details</summary>
Motivation: 手动调参耗时低效且难以兼顾性能与安全性，亟需一种自动化、可迁移、高维的参数优化方法以提升nMPC在机器人实时控制中的表现。

Method: 采用稀疏轴对齐子空间贝叶斯优化（SAASBO）在数字孪生仿真环境中联合优化nMPC的成本函数权重和底层控制器增益，并确保优化结果安全迁移至真实UR10e机器人。

Result: 仿真中跟踪性能提升41.9%，求解时间减少2.5%；实物实验验证提升25.8%，证实方法有效性与迁移可靠性。

Conclusion: 数字孪生赋能的高维贝叶斯自动调参框架可显著提升nMPC在机器人实时控制中的性能与部署效率，为复杂机器人系统提供实用化调优范式。

Abstract: This paper presents an auto-tuning framework for torque-based Nonlinear Model Predictive Control (nMPC), where the MPC serves as a real-time controller for optimal joint torque commands. The MPC parameters, including cost function weights and low-level controller gains, are optimized using high-dimensional Bayesian Optimization (BO) techniques, specifically Sparse Axis-Aligned Subspace (SAASBO) with a digital twin (DT) to achieve precise end-effector trajectory real-time tracking on an UR10e robot arm. The simulation model allows efficient exploration of the high-dimensional parameter space, and it ensures safe transfer to hardware. Our simulation results demonstrate significant improvements in tracking performance (+41.9%) and reduction in solve times (-2.5%) compared to manually-tuned parameters. Moreover, experimental validation on the real robot follows the trend (with a +25.8% improvement), emphasizing the importance of digital twin-enabled automated parameter optimization for robotic operations.

</details>


### [280] [Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving](https://arxiv.org/abs/2512.03774)
*Johannes Fischer,Marlon Steiner,Ömer Sahin Tas,Christoph Stiller*

Main category: cs.RO

TL;DR: 本文提出了一种结合安全强化学习（SRL）与模型预测控制（MPC）的新方法，利用约束强化学习（CRL）生成更优、更安全的参考轨迹，克服传统凸近似MPC的局部最优限制，并在高速公路场景中验证了其在安全性和性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统MPC依赖凸近似，导致解被限制在局部子空间，难以达到全局最优；需提升规划器的安全性与全局探索能力。

Method: 将安全强化学习（SRL）嵌入MPC框架，以学习安全参考轨迹；采用基于手工设计能量函数的安全指数作为CRL约束；引入状态相关的拉格朗日乘子，与策略网络联合学习以求解CRL问题。

Result: 在高速公路仿真中，该方法在安全性（如碰撞率、约束违反次数）和性能（如轨迹平滑性、任务完成率）指标上均优于纯MPC和标准SRL方法。

Conclusion: 融合CRL与MPC的框架能有效拓展MPC的可行域，兼顾实时性与全局优化能力，同时通过可学习的安全约束机制保障自动驾驶中的安全性。

Abstract: Model predictive control (MPC) is widely used for motion planning, particularly in autonomous driving. Real-time capability of the planner requires utilizing convex approximation of optimal control problems (OCPs) for the planner. However, such approximations confine the solution to a subspace, which might not contain the global optimum. To address this, we propose using safe reinforcement learning (SRL) to obtain a new and safe reference trajectory within MPC. By employing a learning-based approach, the MPC can explore solutions beyond the close neighborhood of the previous one, potentially finding global optima. We incorporate constrained reinforcement learning (CRL) to ensure safety in automated driving, using a handcrafted energy function-based safety index as the constraint objective to model safe and unsafe regions. Our approach utilizes a state-dependent Lagrangian multiplier, learned concurrently with the safe policy, to solve the CRL problem. Through experimentation in a highway scenario, we demonstrate the superiority of our approach over both MPC and SRL in terms of safety and performance measures.

</details>


### [281] [MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving](https://arxiv.org/abs/2512.03795)
*Jia Hu,Zhexi Lian,Xuerun Yan,Ruiang Bi,Dou Shen,Yu Ruan,Haoran Wang*

Main category: cs.RO

TL;DR: 本文提出MPCFormer，一种可解释的、社会感知的自动驾驶方法，结合物理先验与数据驱动建模多车社会交互动力学，通过Transformer学习动力学系数，并在MPC框架下实现安全、高效、类人的决策规划。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶车辆在动态交互交通场景中难以表现类人行为，核心在于缺乏对多车社会交互机制的理解。

Method: 提出MPCFormer：将社会交互动力学建模为嵌入物理先验的离散状态空间表示，并采用Transformer编码器-解码器结构从真实驾驶数据中学习动力学系数；结合模型预测控制（MPC）进行闭环规划。

Result: 在NGSIM数据集开环评估中，5秒预测轨迹平均位移误差（ADE）低至0.86米；闭环实验显示规划成功率94.67%，效率提升15.75%，碰撞率由21.25%降至0.5%。

Conclusion: MPCFormer是首个显式建模多车社会交互动力学的方法，兼具可解释性、安全性与类人行为生成能力，显著优于前沿强化学习规划器。

Abstract: Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.

</details>


### [282] [IM HERE: Interaction Model for Human Effort Based Robot Engagement](https://arxiv.org/abs/2512.03828)
*Dominykas Strazdas,Magnus Jung,Jan Marquenie,Ingo Siegert,Ayoub Al-Hamadi*

Main category: cs.RO

TL;DR: 本文提出IM HERE框架，用于建模人-人、人-机器人及机器人-机器人交互中的参与度，通过双边关系的努力描述，简化为焦点放置和四个关键状态，整合主客观信息以识别误沟通，并指导自主系统遵循社会规范实现社会融合。


<details>
  <summary>Details</summary>
Motivation: 现有参与度定义和模型过于模糊或缺乏跨情境泛化能力，难以支持有效的人机交互。

Method: 提出IM HERE框架，基于双边关系的努力描述，将关系模式简化为焦点放置和四个关键状态，整合主观感知与客观状态来建模参与度。

Result: 该框架能准确刻画互惠关系、群体行为和社会规范一致性，并转化为自主系统的具体行为指令；可精准识别和描述误沟通。

Conclusion: IM HERE框架实现了社交行为的自动化分析与建模，使自主系统能在遵守社会规范的同时追求自身社交目标，推动其全面社会融合。

Abstract: The effectiveness of human-robot interaction often hinges on the ability to cultivate engagement - a dynamic process of cognitive involvement that supports meaningful exchanges. Many existing definitions and models of engagement are either too vague or lack the ability to generalize across different contexts. We introduce IM HERE, a novel framework that models engagement effectively in human-human, human-robot, and robot-robot interactions. By employing an effort-based description of bilateral relationships between entities, we provide an accurate breakdown of relationship patterns, simplifying them to focus placement and four key states. This framework captures mutual relationships, group behaviors, and actions conforming to social norms, translating them into specific directives for autonomous systems. By integrating both subjective perceptions and objective states, the model precisely identifies and describes miscommunication. The primary objective of this paper is to automate the analysis, modeling, and description of social behavior, and to determine how autonomous systems can behave in accordance with social norms for full social integration while simultaneously pursuing their own social goals.

</details>


### [283] [OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance](https://arxiv.org/abs/2512.03874)
*Lei Zhang,Diwen Zheng,Kaixin Bai,Zhenshan Bing,Zoltan-Csaba Marton,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: 本文提出OmniDexVLG框架，通过多模态语义感知与语言-视觉联合引导，实现结构多样、语义一致的灵巧抓取生成；包含语义丰富的数据生成流程OmniDexDataGen和多智能体协同推理模块OmniDexReasoner，并在仿真与真实场景中显著提升抓取多样性与语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有灵巧抓取生成方法难以统一建模抓取分类学、接触语义与功能可供性等多维语义，导致语义可控性差。

Method: 构建OmniDexDataGen数据生成流水线（融合分类学引导采样、功能可供性接触点采样、分类学感知力闭合采样及物理优化）；设计OmniDexReasoner多模态语义推理模块（结合多智能体协作、检索增强生成与思维链推理）；开发统一的视觉-语言抓取生成模型，显式嵌入抓取分类学、接触结构与功能可供性语义。

Result: 在仿真与真实世界抓取任务中，该方法在抓取多样性、接触语义多样性、功能可供性多样性及语义一致性方面均显著优于当前最优方法。

Conclusion: OmniDexVLG实现了语义可控、多模态引导的灵巧抓取生成，为任务驱动与人类可解释的机器人抓取提供了新范式。

Abstract: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.

</details>


### [284] [A Modular Architecture Design for Autonomous Driving Racing in Controlled Environments](https://arxiv.org/abs/2512.03886)
*Brais Fontan-Costas,M. Diaz-Cacho,Ruben Fernandez-Boullon,Manuel Alonso-Carracedo,Javier Perez-Robles*

Main category: cs.RO

TL;DR: 本文提出了一种用于封闭赛道车辆的自主系统（AS）架构，集成了计算机视觉、定位建图、路径规划和控制等模块，采用模块化设计与统一数据流水线，实现受控环境下的实时自主导航。


<details>
  <summary>Details</summary>
Motivation: 为在封闭赛道等受控环境中实现高精度、实时的车辆自主导航，需整合感知、定位、规划与控制等关键能力，并解决各子系统协同与实时性问题。

Method: 构建一个模块化的自主系统架构，各子系统（计算机视觉、定位与建图、路径规划、控制）独立运行，通过统一的数据流水线进行连接；融合当前最先进的技术以支持实时处理。

Result: 实现了可在封闭电路中执行高精度任务（如环境感知、精确定位、最优路径生成与精确车辆控制）的自主系统。

Conclusion: 该模块化、流水线驱动的AS架构有效支持了受控环境下实时、可靠的自主导航，验证了多技术融合与松耦合设计的可行性与实用性。

Abstract: This paper presents an Autonomous System (AS) architecture for vehicles in a closed circuit. The AS performs precision tasks including computer vision for environment perception, positioning and mapping for accurate localization, path planning for optimal trajectory generation, and control for precise vehicle actuation. Each subsystem operates independently while connecting data through a cohesive pipeline architecture. The system implements a modular design that combines state-of-the-art technologies for real-time autonomous navigation in controlled environments.

</details>


### [285] [Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning](https://arxiv.org/abs/2512.03891)
*Ying-Kuan Tsai,Yi-Ping Chen,Vispi Karkaria,Wei Chen*

Main category: cs.RO

TL;DR: 本文提出了一种基于数字孪生（DT）与深度强化学习（DRL）的控制-结构协同设计（CCD）框架，用于全车主动悬架系统，支持多代自优化与个性化驾驶场景适配。


<details>
  <summary>Details</summary>
Motivation: 传统主动悬架受限于固定硬件与控制策略，难以适应动态不确定工况；现有DT与DRL融合尚不成熟，缺乏统一的、支持全生命周期自适应优化的框架。

Method: 构建DT驱动的CCD框架，融合自动微分增强的DRL实现悬架物理参数与控制策略联合优化；引入分位数学习进行不确定性感知的模型在线更新；在部分可观测条件下直接从有限传感器数据中学习最优控制动作。

Result: 在温和与激进两种驾驶模式下分别降低控制能耗约43%和52%，同时保持乘坐舒适性与行驶稳定性；验证了个性化悬架优化的有效性。

Conclusion: 该框架首次将DT、DRL与不确定性建模深度融合，实现了主动悬架系统的实时协同优化、多代自进化与驾驶员个性化适配，为智能底盘系统提供了新范式。

Abstract: Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.

</details>


### [286] [Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware](https://arxiv.org/abs/2512.03911)
*Kenneth Stewart,Roxana Leontie,Samantha Chapin,Joe Hays,Sumit Bam Shrestha,Carl Glen Henshaw*

Main category: cs.RO

TL;DR: 本文提出了一种将强化学习训练的ANN端到端转换为适用于Intel Loihi 2的脉冲Sigma-Delta神经网络（SDNN）并部署于类脑硬件的流程，成功在Omniverse Isaac Lab中实现Astrobee机器人闭环控制，并对比验证了其低延迟、高能效优势。


<details>
  <summary>Details</summary>
Motivation: 为满足未来空间及地面机器人对实时性与高能效计算的需求，探索将仿真训练的强化学习策略迁移至类脑硬件的可行路径。

Method: 将仿真中训练的基于ReLU的ANN策略转换为适配Intel Loihi 2架构的脉冲Sigma-Delta神经网络（SDNN），并在NVIDIA Omniverse Isaac Lab中进行闭环控制仿真评估，同时对比GPU与Loihi 2的执行性能。

Result: 成功将ANN策略转换并部署于Loihi 2，在Astrobee机器人控制任务中实现低延迟、高能效的闭环推理；验证了类脑硬件用于机器人控制的可行性。

Conclusion: 该工作建立了从仿真RL训练到类脑硬件部署的完整流程，为未来空间与地面机器人应用中实现能量高效、实时的类脑计算提供了可行路径。

Abstract: We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.

</details>


### [287] [Hierarchical Vision Language Action Model Using Success and Failure Demonstrations](https://arxiv.org/abs/2512.03913)
*Jeongeun Park,Jihwan Yoon,Byungwoo Jeon,Juhan Park,Jinwoo Shin,Namhoon Cho,Kyungjae Lee,Sangdoo Yun,Sungjoon Choi*

Main category: cs.RO

TL;DR: 本文提出VINE模型，利用混合质量数据（包括失败案例）进行分层视觉-语言-动作建模，通过将高层推理（System 2）与低层控制（System 1）解耦，在规划阶段引入失败感知的可行性评估，显著提升VLA模型在操作任务中的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型仅使用成功演示数据，忽略自然采集中大量失败尝试；而失败数据蕴含策略脆弱性信息，可用于提升鲁棒性。

Method: 提出分层VINE模型：System 2基于2D场景图进行可行性引导的树搜索，融合成功与失败数据预测子目标过渡的成功概率并剪枝脆弱分支；System 1执行低层动作而不改变核心技能；全程基于离线遥操作数据训练。

Result: 在多个具挑战性的操作任务上，VINE持续提升成功率与鲁棒性。

Conclusion: 失败数据是将VLA广义能力转化为稳健执行的关键资源，VINE验证了在规划中显式建模失败可有效增强决策鲁棒性。

Abstract: Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.

</details>


### [288] [Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response](https://arxiv.org/abs/2512.03936)
*Aron Distelzweig,Yiwei Wang,Faris Janjoš,Marcel Hallgarten,Mihai Dobre,Alexander Langmann,Joschka Boedecker,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出BIBeR框架，将先进的运动预测器与博弈论规划（迭代最优响应）结合，实现双向交互式决策，并引入贝叶斯置信度估计动态调节更新强度，在高交互场景中显著提升规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶规划系统在常规场景表现良好，但在密集城市交通中难以处理需预判和影响他车的复杂交互（如变道、汇入）；当前预测-规划集成方式粗糙（仅筛除不安全方案）或端到端黑箱化，而博弈论方法虽原理严谨却应用有限。

Method: 提出贝叶斯迭代最优响应（BIBeR）框架：在迭代最优响应（IBR）循环中嵌入先进运动预测器，反复优化自车与周围智能体的策略；引入贝叶斯置信度估计量化预测可靠性，并据此动态调节策略更新强度；保持与现代预测器和规划器的兼容性。

Result: 在高交互的interPlan变道场景中，BIBeR相比最先进规划器提升11%；在标准nuPlan基准上也优于现有方法。

Conclusion: BIBeR首次实现了高精度预测器与博弈论规划的深度协同，通过双向适应和置信感知更新，兼顾可解释性与灵活性，为复杂交通交互规划提供了新范式。

Abstract: Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks.

</details>


### [289] [MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation](https://arxiv.org/abs/2512.03958)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: 本文提出了MDE-AgriVLN方法，将单目深度估计引入农业视觉-语言导航（AgriVLN）任务，显著提升了导航成功率并降低了导航误差。


<details>
  <summary>Details</summary>
Motivation: 现有农业机器人多依赖人工操作或轨道系统移动，且普遍采用单目视觉，空间感知能力受限，亟需更自主、鲁棒的导航方法。

Method: 提出MDE-AgriVLN方法，包含一个单目深度估计（MDE）模块，从RGB图像中生成深度特征，辅助导航决策模型进行推理。

Result: 在A2A基准上，成功率达0.32（提升0.09），导航误差降至4.08m（降低0.35m），达到农业VLN领域最优性能。

Conclusion: 单目深度估计可有效弥补农业机器人单目视觉的空间感知缺陷，显著提升视觉-语言导航性能，为无轨自主农业机器人提供了可行路径。

Abstract: Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN.

</details>


### [290] [Artificial Microsaccade Compensation: Stable Vision for an Ornithopter](https://arxiv.org/abs/2512.03995)
*Levi Burner,Guido de Croon,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: 本文提出了一种受生物微扫视启发的视频稳定方法——人工微扫视补偿（AMC），用于稳定尾部缺失扑翼飞行器拍摄的高频抖动视频，通过在SO(3)空间中优化3D旋转以最小化图像强度变化，实现实时、无失真、高质量的视频稳定，性能优于Adobe Premiere Pro的Warp Stabilizer。


<details>
  <summary>Details</summary>
Motivation: 尾部缺失的扑翼飞行器（ornithopter）因12–20 Hz高频抖动导致难以使用基于相机的感知，传统视频稳定方法效果不佳；受动物（如人类）微扫视可维持视觉稳定性的生物机制启发，作者寻求一种高效、实时且无畸变的稳定方案。

Method: 提出‘人工微扫视补偿’（AMC）方法，在SO(3)群上建模并优化三维旋转，以最小化帧间图像强度变化；支持两种模式：连续稳定（适合人眼观看）和固定朝向+偶发扫视（大幅降低帧间运动），并采用高效递归更新实现实时计算。

Result: 实现了对高频抖动视频的实时、无失真稳定，输出视频质量优于Adobe Premiere Pro Warp Stabilizer，且计算效率满足机载实时处理需求。

Conclusion: 生物启发的微扫视补偿是一种有效解决高频机械抖动下视频稳定问题的新范式，兼具理论合理性与工程实用性，尤其适用于资源受限、高动态平台（如微型扑翼飞行器）的视觉感知任务。

Abstract: Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for "Artificial Microsaccade Compensation". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [291] [Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation](https://arxiv.org/abs/2512.03048)
*Austin Spizzirri*

Main category: cs.AI

TL;DR: 本文主张将AI对齐问题重新构想为通过基于过程、多智能体、发展性机制来构建具有‘熵减’特性和理由响应能力的智能体，而非试图编码固定的人类价值内容；提出了‘规范陷阱’论证、‘熵减’信息理论框架，以及区分真实与模拟道德能力的功能性标准。


<details>
  <summary>Details</summary>
Motivation: 由于‘是- ought’鸿沟、价值多元主义和扩展框架问题的共同作用，基于内容的价值指定在结构上不稳定，因此需要重新思考AI对齐的根本路径。

Method: 采用哲学分析方法，提出‘规范陷阱’论证；引入‘熵减’作为信息论框架；结合兼容论的引导控制理论，构建区分真实与模拟道德能力的功能性标准，并辅以具身实验范式与验证机制。

Result: 确立了AI对齐应转向过程性、多智能体、发展性机制的新范式；提出‘熵减’概念刻画多智能体间状态对齐与不确定性递减；给出可操作、独立于现象学主张的道德能力验证标准。

Conclusion: AI对齐不应追求静态价值编码，而应致力于构建能通过交互演化出理由响应与道德能力的动态系统；该框架虽具可证伪性，但实证检验尚待后续研究完成。

Abstract: I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.

</details>


### [292] [Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI](https://arxiv.org/abs/2512.03072)
*Hu Keyi*

Main category: cs.AI

TL;DR: 本文提出一种基于第一性原理的认知架构'Weight-Calculatism'，通过逻辑原子和'指向'、'比较'两种基本操作实现可解释、可追溯价值对齐的通用人工智能路径。


<details>
  <summary>Details</summary>
Motivation: 当前AI在可解释性和价值对齐方面存在根本性挑战，亟需一种从第一性原理出发、具备内在可解释与价值可控性的新范式。

Method: 提出'Weight-Calculatism'架构：将认知分解为不可再分的逻辑原子及'指向'与'比较'两种基本操作；决策建模为Weight = Benefit * Probability，所有权重均可追溯至初始权重集；采用图算法引擎与全局工作空间流程实现。

Result: 初步实现与场景验证表明，该架构能实现透明、类人的推理能力，并在前所未见的情境中展现稳健学习能力。

Conclusion: Weight-Calculatism为构建可信、价值对齐的AGI提供了兼具理论严谨性与工程可行性的新基础。

Abstract: Current AI paradigms, as "architects of experience," face fundamental challenges in explainability and value alignment. This paper introduces "Weight-Calculatism," a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.

</details>


### [293] [When Do Symbolic Solvers Enhance Reasoning in Large Language Models?](https://arxiv.org/abs/2512.03272)
*Zhiyuan He,Dingmin Wang*

Main category: cs.AI

TL;DR: 本文探讨了大型推理模型（LRMs）在复杂推理任务中使用长思维链（CoT）带来的token开销问题，提出并验证了符号求解器集成方法的有效适用场景：适用于隐式推理需求少但搜索空间大的约束满足问题，而非深度演绎推理问题。


<details>
  <summary>Details</summary>
Motivation: 长思维链（CoT）虽提升复杂推理能力，但带来显著token开销，且‘过度思考’可能降低准确率；需探索符号求解器集成方法何时能真正增强传统长CoT。

Method: 通过对比实验，评估不同LLM（如GPT-4o、CodeLlama-13B）在演绎类问题与约束满足类问题（如Zebra谜题）上的表现，分析符号求解器集成方法在有/无声明式示例下的效果。

Result: 符号求解器集成法仅在隐式推理需求低、搜索空间大的问题（如需多次回溯的约束满足问题）中显著提升性能；GPT-4o在浅层演绎问题上更优；提供声明式示例时，CodeLlama-13B可超越GPT-4o解决困难Zebra谜题。

Conclusion: 符号求解器集成并非万能替代，其价值高度依赖问题结构；应根据推理深度与搜索复杂度选择推理范式，实现效率与准确性平衡。

Abstract: Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models "overthink" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.

</details>


### [294] [Prior preferences in active inference agents: soft, hard, and goal shaping](https://arxiv.org/abs/2512.03293)
*Filippo Torresan,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: 本文探讨了主动推理中偏好分布的四种定义方式对智能体规划与学习的影响，发现目标塑造能提升导航任务表现但会削弱对环境动态的学习。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少关注主动推理中偏好分布的设定方式及其对推断和学习的影响。

Method: 比较四种不同偏好分布定义（硬/软目标、是否包含目标塑造）下的智能体在网格世界导航任务中的表现。

Result: 目标塑造带来最佳性能（促进利用），但损害对环境转移动力学的学习（抑制探索）。

Conclusion: 偏好分布的设计需权衡利用与探索，目标塑造虽提升任务表现，却以牺牲环境建模能力为代价。

Abstract: Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).

</details>


### [295] [Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia](https://arxiv.org/abs/2512.03318)
*Chandler Smith,Marwa Abdulhai,Manfred Diaz,Marko Tesic,Rakshit S. Trivedi,Alexander Sasha Vezhnevets,Lewis Hammond,Jesse Clifton,Minsuk Chang,Edgar A. Duéñez-Guzmán,John P. Agapiou,Jayd Matyas,Danny Karmon,Akash Kundu,Aliaksei Korshuk,Ananya Ananya,Arrasy Rahman,Avinaash Anand Kulandaivel,Bain McHale,Beining Zhang,Buyantuev Alexander,Carlos Saith Rodriguez Rojas,Caroline Wang,Chetan Talele,Chenao Liu,Chichen Lin,Diana Riazi,Di Yang Shi,Emanuel Tewolde,Elizaveta Tennant,Fangwei Zhong,Fuyang Cui,Gang Zhao,Gema Parreño Piqueras,Hyeonggeun Yun,Ilya Makarov,Jiaxun Cui,Jebish Purbey,Jim Dilkes,Jord Nguyen,Lingyun Xiao,Luis Felipe Giraldo,Manuela Chacon-Chamorro,Manuel Sebastian Rios Beltran,Marta Emili García Segura,Mengmeng Wang,Mogtaba Alim,Nicanor Quijano,Nico Schiavone,Olivia Macmillan-Scott,Oswaldo Peña,Peter Stone,Ram Mohan Rao Kadiyala,Rolando Fernandez,Ruben Manrique,Sunjia Lu,Sheila A. McIlraith,Shamika Dhuri,Shuqing Shi,Siddhant Gupta,Sneheel Sarangi,Sriram Ganapathi Subramanian,Taehun Cha,Toryn Q. Klassen,Wenming Tu,Weijian Fan,Wu Ruiyang,Xue Feng,Yali Du,Yang Liu,Yiding Wang,Yipeng Kang,Yoonchang Sung,Yuxuan Chen,Zhaowei Zhang,Zhihan Wang,Zhiqiang Wu,Ziang Chen,Zilong Zheng,Zixia Jia,Ziyan Wang,Dylan Hadfield-Menell,Natasha Jaques,Tim Baarslag,Jose Hernandez-Orallo,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 本文提出了一种在零样本、混合动机环境中评估大语言模型（LLM）代理合作能力的新方法，基于Concordia多智能体自然语言仿真平台，并通过NeurIPS 2024 Concordia竞赛实证揭示了当前LLM代理在说服与规范执行等复杂合作任务上仍存在显著泛化能力不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法衡量LLM代理在新颖社会情境中合作能力的泛化性，而其实际部署正日益涉及人机及机机混合交互，亟需更贴近真实社会复杂性的评测框架。

Method: 构建基于Concordia自然语言多智能体仿真环境的零样本合作智能评测方法，通过跨多样化伙伴与场景（如谈判、集体行动问题）测试代理识别并实现互惠收益的能力。

Result: NeurIPS 2024 Concordia竞赛实证表明，当前LLM代理在多数合作场景中表现有限，尤其在需说服与规范执行的任务中鲁棒性明显不足，暴露出合作智能泛化能力的关键短板。

Conclusion: LLM代理的合作能力尚未达到可靠部署所需水平；未来研究应聚焦提升其在动态、异构社会交互中的推理、适应与协调能力。

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.

</details>


### [296] [Multimodal Reinforcement Learning with Agentic Verifier for AI Agents](https://arxiv.org/abs/2512.03438)
*Reuben Tan,Baolin Peng,Zhengyuan Yang,Hao Cheng,Oier Mees,Theodore Zhao,Andrea Tupini,Isar Meijier,Qianhui Wu,Yuncong Yang,Lars Liden,Yu Gu,Sheng Zhang,Xiaodong Liu,Lijuan Wang,Marc Pollefeys,Yong Jae Lee,Jianfeng Gao*

Main category: cs.AI

TL;DR: 本文提出Argos，一种用于多模态推理模型训练的智能体奖励机制，通过结合教师模型和基于规则的评分函数，对最终答案、时空定位及推理过程进行综合评估，显著提升了空间推理、视觉幻觉检测及具身AI等任务的性能，并从理论上证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态强化学习的智能体推理模型大多仅依赖稀疏的结果型奖励，缺乏对推理过程的细粒度指导；而设计更丰富的奖励信号面临评分函数选择困难和教师模型噪声干扰等问题。

Method: 提出Argos奖励智能体，针对每个样本从教师模型衍生与规则驱动的评分函数池中动态选择，同步评估响应准确性、时空实体/动作定位质量以及推理过程质量；并将该验证机制应用于监督微调（SFT）数据筛选与强化学习（RL）训练全过程。

Result: 在空间推理、视觉幻觉识别、机器人与具身AI等多个基准上达到SOTA；验证了仅靠高质量SFT后训练无法避免RL阶段的解耦失效；有效缓解MMRL中的奖励作弊问题。

Conclusion: Argos通过结构化、多维度、在线化的奖励评估，为多模态智能体训练提供了更鲁棒、可解释且理论可支撑的优化路径；强调推理过程监督与结果监督同等重要。

Abstract: Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.

</details>


### [297] [Multi-Agent Reinforcement Learning with Communication-Constrained Priors](https://arxiv.org/abs/2512.03528)
*Guang Yang,Tianpei Yang,Jingwen Qiao,Yanqing Wu,Jing Huo,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: 本文提出了一种通信受限的多智能体强化学习框架，通过广义通信约束模型、消息区分机制与双互信息估计器，提升在丢包环境下的鲁棒性与可扩展性，并在多个基准上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于通信的多智能体强化学习方法在真实场景中面临通信丢包、可扩展性差和鲁棒性不足的问题。

Method: 提出广义通信约束模型作为学习先验，区分丢包与无损消息；利用双互信息估计器解耦两类消息对分布式决策的影响；将通信消息影响量化并融入全局奖励函数。

Result: 所提方法在多个通信受限基准测试中展现出更优的性能与鲁棒性。

Conclusion: 该框架有效提升了多智能体系统在动态、复杂及丢包通信环境下的协同策略学习能力。

Abstract: Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.

</details>


### [298] [PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks](https://arxiv.org/abs/2512.03549)
*Yuki Orimo,Iori Kurata,Hodaka Mori,Ryuhei Okuno,Ryohto Sawada,Daisuke Okanohara*

Main category: cs.AI

TL;DR: PARC是一个基于分层多智能体架构的编码代理，具备任务规划、执行和自我评估反馈能力，能自主完成长周期计算任务，在材料科学和数据科学任务中展现出强大性能。


<details>
  <summary>Details</summary>
Motivation: 解决长周期计算任务中缺乏自主性、鲁棒性和错误纠正能力的问题，推动AI系统在科学计算与数据分析中实现真正独立工作。

Method: 提出PARC系统，采用分层多智能体架构，整合任务规划、执行模块，并引入基于独立上下文的自我评估与自我反馈机制。

Result: 在材料科学中成功复现锂离子传导与合金偏析的关键结果，协调数十个耗时约43小时的并行仿真任务；在Kaggle数据科学任务中，仅凭简短自然语言指令即可完成分析与搜索策略实现，结果媲美人工设计基线。

Conclusion: 分层多智能体系统结合自我评估与反馈机制，是实现AI系统独立开展大规模科研与分析工作的有效路径。

Abstract: We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own actions and their outcomes from an independent context and provides feedback, namely self-assessment and self-feedback. This design enables PARC to detect and correct high-level strategic errors and sustain progress without human intervention. We evaluate PARC across computational science and data science tasks. In materials science, it autonomously reproduces key results from studies on lithium-ion conduction and alloy segregation. In particular, it coordinates dozens of parallel simulation tasks, each requiring roughly 43 hours of computation, managing orchestration, monitoring, and error correction end-to-end. In Kaggle-based experiments, starting from minimal natural-language instructions, PARC conducts data analysis and implements search strategies, producing solutions competitive with human-engineered baselines. These results highlight the potential of integrating a hierarchical multi-agent system with self-assessment and self-feedback to enable AI systems capable of independent, large-scale scientific and analytical work.

</details>


### [299] [Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks](https://arxiv.org/abs/2512.03560)
*Gianni Molinari,Fabio Ciravegna*

Main category: cs.AI

TL;DR: 本文提出RP-ReAct多智能体框架，通过解耦高层规划（Reasoner Planner Agent）与底层执行（Proxy-Execution Agent），结合上下文节省策略，显著提升企业级复杂任务中的可靠性、效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体架构存在轨迹不稳定和本地开源模型上下文窗口小导致工具输出迅速耗尽上下文的问题，难以应对企业中需协调多工具、处理异构数据的复杂任务。

Method: 提出RP-ReAct：由具备强推理能力的大推理模型驱动的Reasoner Planner Agent负责分步规划与结果分析；一个或多个采用ReAct范式的Proxy-Execution Agent负责具体工具调用，并引入外部存储+按需加载的上下文节省策略以缓解上下文溢出。

Result: 在多领域ToolQA基准上，使用6种开源推理模型验证，RP-ReAct在性能、泛化性及不同模型尺度下的鲁棒性与稳定性均优于SOTA基线。

Conclusion: RP-ReAct通过规划与执行解耦及上下文优化，为隐私敏感的企业场景提供了更可靠、高效且可部署的智能体解决方案。

Abstract: Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.

</details>


### [300] [EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths](https://arxiv.org/abs/2512.03571)
*Zhening Li,Armando Solar-Lezama,Yisong Yue,Stephan Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种新的LLM智能体编程方法——'概率天使式非确定性'（PAN），通过分离工作流逻辑与推理时策略，提升智能体开发的灵活性与可靠性，并在EnCompass框架中实现验证。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体编程常将核心工作流逻辑与推理时策略（如树搜索）耦合，导致调试、优化和策略切换困难，亟需解耦设计。

Method: 提出PAN编程模型，利用Python装饰器将工作流程序编译为可搜索空间，并构建EnCompass框架支持多种推理策略的即插即用。

Result: 三个案例表明，该框架能显著提升智能体可靠性，并支持在几乎不修改代码的前提下灵活切换不同推理策略。

Conclusion: PAN模型及其EnCompass实现有效解耦了智能体设计中的逻辑与策略，为LLM智能体编程提供了更模块化、可实验性强的新范式。

Abstract: We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce "probabilistic angelic nondeterminism" ("PAN"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.

</details>


### [301] [DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization](https://arxiv.org/abs/2512.03607)
*Yusen Wu,Xiaotie Deng*

Main category: cs.AI

TL;DR: 本文提出DeepRule框架，通过融合大语言模型、博弈论约束优化和符号回归，实现零售业商品组合与定价策略的自动化业务规则生成，兼顾经济合理性与操作可行性。


<details>
  <summary>Details</summary>
Motivation: 现有理论模型与现实经济复杂性存在系统性错位，具体表现为：数据模态不匹配（非结构化文本难以支持精准客户画像）、动态特征纠缠（难以建模非线性价格弹性与时变属性）、以及多层级业务约束导致的操作不可行性。

Method: 构建三层架构：1）混合知识融合引擎，利用大语言模型对非结构化文本进行深度语义解析，生成结构化特征并融入管理经验；2）基于博弈论的约束优化机制，通过双边效用函数协调供应链多方利益，将厂商-分销商利润再分配建模为受层级约束的内生目标；3）可解释决策蒸馏接口，采用LLM引导的符号回归搜索满足经济先验（如非负弹性）的定价策略与可审计业务规则。

Result: 在真实零售环境中验证，DeepRule相较系统性B2C基线方法实现了更高利润，同时保障操作可行性。

Conclusion: DeepRule建立了闭环管道，统一了非结构化知识注入、多智能体优化与可解释策略合成，推动现实经济智能的发展。

Abstract: This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.
  Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.

</details>


### [302] [MemVerse: Multimodal Memory for Lifelong Learning Agents](https://arxiv.org/abs/2512.03627)
*Junming Liu,Yifei Sun,Weihua Cheng,Haodong Lei,Yirong Chen,Licheng Wen,Xuemeng Yang,Daocheng Fu,Pinlong Cai,Nianchen Deng,Yi Yu,Shuyue Hu,Botian Shi,Ding Wang*

Main category: cs.AI

TL;DR: MemVerse 是一种模型无关、即插即用的记忆框架，结合参数化快速回忆与分层检索式记忆，支持可扩展、自适应的多模态智能。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理缺乏可靠记忆，导致灾难性遗忘、长程推理困难及在多模态或交互环境中表现不佳。

Method: MemVerse 构建短时记忆处理近期上下文，并将原始多模态经验转化为结构化的长时记忆（分层知识图谱）；引入周期性蒸馏机制，将长时记忆中的关键知识压缩进参数模型以实现快速可微回忆。

Result: 实验表明 MemVerse 显著提升了多模态推理能力和持续学习效率，使AI代理能在长时间交互中保持记忆、适应与连贯推理。

Conclusion: MemVerse 为AI代理提供了可扩展、可控且可解释的记忆能力，是迈向真正具身与持续智能的重要一步。

Abstract: Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember. Without reliable memory, agents catastrophically forget past experiences, struggle with long-horizon reasoning, and fail to operate coherently in multimodal or interactive environments. We introduce MemVerse, a model-agnostic, plug-and-play memory framework that bridges fast parametric recall with hierarchical retrieval-based memory, enabling scalable and adaptive multimodal intelligence. MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. This design supports continual consolidation, adaptive forgetting, and bounded memory growth. To handle real-time demands, MemVerse introduces a periodic distillation mechanism that compresses essential knowledge from long-term memory into the parametric model, allowing fast, differentiable recall while preserving interpretability. Extensive experiments demonstrate that MemVerse significantly improves multimodal reasoning and continual learning efficiency, empowering agents to remember, adapt, and reason coherently across extended interactions.

</details>


### [303] [RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design](https://arxiv.org/abs/2512.03762)
*Jiawei Xu,Fengfeng Wei,Weineng Chen*

Main category: cs.AI

TL;DR: 本文提出RoCo，一种基于多智能体角色协作的自动启发式设计（AHD）系统，通过探索者、利用者、批评者和整合者四类LLM驱动智能体协同工作，在组合优化问题中生成高质量启发式规则，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based AHD研究多局限于单一角色，缺乏多样性与协作性，难以兼顾创新性与稳定性。

Method: 提出RoCo多角色协同框架，包含探索者（激发多样性）、利用者（优化短期性能）、批评者（评估反馈）和整合者（融合创新与利用），采用多轮反馈-精炼-精英变异机制，并结合短/长期反思指导演化。

Result: 在五类组合优化问题（白盒与黑盒设置）上实验表明，RoCo生成的启发式规则持续优于ReEvo和HSEvo等现有方法。

Conclusion: 基于角色分工与协作的多智能体范式为自动启发式设计提供了更鲁棒、高性能的新标准。

Abstract: Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.

</details>


### [304] [Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.03783)
*Dongchao Yang,Songxiang Liu,Disong Wang,Yuanyuan Wang,Guanglu Wan,Helen Meng*

Main category: cs.AI

TL;DR: 本文提出Omni-AutoThink框架，通过自适应监督微调和强化学习，使多模态大模型能根据任务难度动态调整推理深度，显著提升自适应推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有Omni模型推理行为僵化，无法根据任务复杂度灵活调整推理深度，导致简单任务过度推理或复杂任务缺乏推理。

Method: 提出两阶段自适应推理框架：(1) 自适应监督微调（Adaptive SFT），利用大规模推理增强数据赋予基础推理能力；(2) 自适应强化学习（Adaptive GRPO），依据任务复杂度与奖励反馈优化推理行为；并构建覆盖文本、文音、图文、文音图的多模态自适应推理基准。

Result: 在多模态自适应推理基准上，Omni-AutoThink显著优于现有基线方法。

Conclusion: Omni-AutoThink实现了对多模态任务难度的感知与推理深度的动态适配，为构建更智能、更高效的通用多模态模型提供了新范式。

Abstract: Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.

</details>


### [305] [A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)](https://arxiv.org/abs/2512.03887)
*Saurav Prateek*

Main category: cs.AI

TL;DR: 本文提出了一种静态深度研究代理（Static-DRA），通过可配置的树状工作流和用户可控的深度（Depth）与广度（Breadth）参数，在多跳检索与并行子主题探索中实现质量与计算成本的权衡，并在DeepResearch Bench上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为克服传统静态RAG在复杂、多轮研究任务中的局限性，需构建更灵活、可控且资源高效的深度研究系统。

Method: 设计基于树状结构的静态工作流，引入两个可调参数（Depth和Breadth）控制研究强度；采用三层代理架构（Supervisor、Independent、Worker）支持多跳检索与并行子主题分析。

Result: 在DeepResearch Bench上使用RACE框架评估，Static-DRA（depth=2, breadth=5, gemini-2.5-pro）取得34.72分；实验表明增大Depth和Breadth可提升研究深度与评分。

Conclusion: Static-DRA提供了一种透明、可控、资源感知的深度研究方案，兼顾结果质量与LLM调用开销，代码与结果已全部开源。

Abstract: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.
  The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.
  We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/

</details>


### [306] [Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties](https://arxiv.org/abs/2512.03931)
*Vineel Tummala,Daniela Inclezan*

Main category: cs.AI

TL;DR: 本文提出一种基于逻辑编程的策略感知自主智能体框架，通过扩展AOPL语言并结合ASP推理，支持对违规行为及其惩罚的建模与权衡，在保证高目标达成的同时提升可解释性与规划质量。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注策略合规性保障，而实际中有时需主动偏离策略以实现高风险高回报目标；同时，模拟真实人类的非合规决策有助于政策制定者优化策略设计。

Method: 扩展Gelfond和Lobo的AOPL语言以支持惩罚建模，结合Answer Set Programming（ASP）进行推理与规划；提出自动化AOPL到ASP的翻译方法，并改进ASP规划算法以量化并最小化违规惩罚；引入策略优先级与规则违反显式追踪机制。

Result: 在两个实验领域中，该框架生成的计划更高质量（避免有害行为），部分场景下还提升了计算效率；能明确识别违规规则及其后果，增强决策可解释性。

Conclusion: 所提框架不仅支持兼顾目标达成与策略约束的自主决策，还可为政策分析与优化提供仿真支持，具有理论价值与实践潜力。

Abstract: This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [307] [Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol](https://arxiv.org/abs/2512.03955)
*Niklas Jobs,Luis Miguel Vieira da Silva,Jayanth Somashekaraiah,Maximilian Weigand,David Kube,Felix Gehlhoff*

Main category: cs.AI

TL;DR: 本文提出了一个基于Blocksworld问题的可执行仿真环境基准测试，用于评估大语言模型（LLM）驱动的工业自动化代理在自适应规划与执行任务中的性能，并通过Model Context Protocol（MCP）实现不同代理架构的标准化接入与公平比较。


<details>
  <summary>Details</summary>
Motivation: 工业自动化需要灵活、可适应的控制策略，而现有LLM智能体缺乏统一、可复现的基准来系统评估其规划与执行能力。

Method: 构建了一个具有五个复杂度等级的Blocksworld可执行仿真环境基准；引入Model Context Protocol（MCP）作为标准化工具接口，支持多种LLM代理架构即插即用地接入和评估；实现并测试了一个单智能体基线方案。

Result: 成功开发出首个面向LLM智能体的可执行Blocksworld基准，支持定量评估（如成功率、步数、耗时等），并通过MCP验证了多架构兼容性；单智能体实验验证了基准的可行性与有效性。

Conclusion: 该基准填补了LLM智能体在工业自动化场景中缺乏标准化评估框架的空白，为未来自适应控制策略的研究提供了可扩展、可复现的评测基础设施。

Abstract: Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [308] [BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents](https://arxiv.org/abs/2512.03413)
*Shu Wang,Yingli Zhou,Yixiang Fang*

Main category: cs.IR

TL;DR: 本文提出BookRAG，一种专为具有层次结构文档（如书籍、手册）设计的检索增强生成方法，通过构建BookIndex索引结构并结合基于信息觅食理论的代理式查询策略，在问答任务中显著提升检索召回率与准确率。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法多面向通用文档，忽视书籍等真实文档固有的层次结构（如章、节、段），导致检索效果不佳。

Method: 提出BookRAG：1）构建BookIndex——从文档中抽取层次树（类目录）、用图建模实体关系、将实体映射至树节点；2）设计基于信息觅食理论的代理式动态查询方法，按查询类型自适应选择检索路径。

Result: 在三个主流基准上，BookRAG在检索召回率和QA准确率上均达到SOTA，同时保持较高效率。

Conclusion: 利用文档内在层次结构与实体关系可显著提升RAG在结构化长文档上的问答性能，BookIndex与代理式查询是关键创新。

Abstract: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information from external complex documents, has attracted tremendous attention from both industry and academia. Existing RAG approaches often focus on general documents, and they overlook the fact that many real-world documents (such as books, booklets, handbooks, etc.) have a hierarchical structure, which organizes their content from different granularity levels, leading to poor performance for the QA task. To address these limitations, we introduce BookRAG, a novel RAG approach targeted for documents with a hierarchical structure, which exploits logical hierarchies and traces entity relations to query the highly relevant information. Specifically, we build a novel index structure, called BookIndex, by extracting a hierarchical tree from the document, which serves as the role of its table of contents, using a graph to capture the intricate relationships between entities, and mapping entities to tree nodes. Leveraging the BookIndex, we then propose an agent-based query method inspired by the Information Foraging Theory, which dynamically classifies queries and employs a tailored retrieval workflow. Extensive experiments on three widely adopted benchmarks demonstrate that BookRAG achieves state-of-the-art performance, significantly outperforming baselines in both retrieval recall and QA accuracy while maintaining competitive efficiency.

</details>


### [309] [LLM as Explainable Re-Ranker for Recommendation System](https://arxiv.org/abs/2512.03439)
*Yaqi Wang,Haojia Sun,Shuting Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种将大语言模型（LLM）作为可解释重排序器（explainable re-ranker）的混合推荐方法，结合传统推荐模型提升准确性和可解释性，并通过两阶段训练显著提升NDCG指标，优于零样本基线。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统缺乏可解释性、存在流行度偏差；而纯LLM作为预测器时准确性又不如传统模型。

Method: 提出LLM作为可解释重排序器的混合框架；构建专用训练数据集并评估其与人类期望的一致性；采用两阶段训练策略优化LLM重排序器。

Result: 在NDCG等关键排序指标上显著提升；重排序器在排序准确性和可解释性上均优于零样本基线。

Conclusion: 将传统推荐模型与LLM结合的混合范式，能有效缓解现有推荐系统的可解释性与公平性缺陷，为构建更可靠、可解释的推荐系统提供新路径。

Abstract: The application of large language models (LLMs) in recommendation systems has recently gained traction. Traditional recommendation systems often lack explainability and suffer from issues such as popularity bias. Previous research has also indicated that LLMs, when used as standalone predictors, fail to achieve accuracy comparable to traditional models. To address these challenges, we propose to use LLM as an explainable re-ranker, a hybrid approach that combines traditional recommendation models with LLMs to enhance both accuracy and interpretability. We constructed a dataset to train the re-ranker LLM and evaluated the alignment between the generated dataset and human expectations. Leveraging a two-stage training process, our model significantly improved NDCG, a key ranking metric. Moreover, the re-ranker outperformed a zero-shot baseline in ranking accuracy and interpretability. These results highlight the potential of integrating traditional recommendation models with LLMs to address limitations in existing systems and pave the way for more explainable and fair recommendation frameworks.

</details>


### [310] [M3DR: Towards Universal Multilingual Multimodal Document Retrieval](https://arxiv.org/abs/2512.03514)
*Adithya S Kolavi,Vyoman Jain*

Main category: cs.IR

TL;DR: 本文提出了M3DR框架，用于解决多模态文档检索中的多语言问题，通过合成多语言文档数据和对比学习，实现了跨语言和跨模态的统一表征，在22种语言上验证了其有效性，并构建了全面的多语言基准。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多模态文档检索方法以英语为中心，难以适用于多语言场景，限制了其在不同语言和文化背景下的应用效果。

Method: 提出M3DR框架，利用合成的多语言文档数据，结合对比学习训练，实现文本与文档图像的统一跨语言跨模态表征；支持单向量（dense）和多向量（ColBERT-style）检索范式。

Result: 在22种类型学多样的语言上验证了模型的跨语言泛化能力；构建了覆盖单语、多语及混语场景的综合多语言基准；NetraEmbed和ColNetraEmbed模型在跨语言检索任务上取得约150%的相对性能提升。

Conclusion: M3DR有效解决了多模态文档检索中的多语言适配问题，具备良好的跨语言、跨模态对齐能力和架构通用性，为真实世界多语言场景提供了实用解决方案。

Abstract: Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.

</details>


### [311] [Algorithms for Boolean Matrix Factorization using Integer Programming and Heuristics](https://arxiv.org/abs/2512.03807)
*Christos Kolomvakis,Thomas Bobille,Arnaud Vandaele,Nicolas Gillis*

Main category: cs.IR

TL;DR: 本文提出了基于交替优化和整数规划的布尔矩阵分解（BMF）新算法，并设计了贪心与局部搜索启发式方法以提升可扩展性，同时构建了高效的C++布尔向量/矩阵数据结构，在多个真实数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 提升BMF的可解释性、降低近似误差，并解决现有方法在大规模数据上的可扩展性限制。

Method: 提出基于交替优化（AO）与整数规划（IP）的BMF算法；设计多轮运行后选择最优秩一因子子集的策略；引入贪心与局部搜索启发式方法；构建高性能C++布尔数据结构。

Result: 所提方法在含/不含缺失数据的真实数据集（如主题建模、图像处理）上优于当前最优方法，且新数据结构显著加速计算。

Conclusion: AO+IP框架结合启发式改进与高效数据结构，可在保持精度的同时大幅提升BMF的实用性与可扩展性。

Abstract: Boolean matrix factorization (BMF) approximates a given binary input matrix as the product of two smaller binary factors. Unlike binary matrix factorization based on standard arithmetic, BMF employs the Boolean OR and AND operations for the matrix product, which improves interpretability and reduces the approximation error. It is also used in role mining and computer vision. In this paper, we first propose algorithms for BMF that perform alternating optimization (AO) of the factor matrices, where each subproblem is solved via integer programming (IP). We then design different approaches to further enhance AO-based algorithms by selecting an optimal subset of rank-one factors from multiple runs. To address the scalability limits of IP-based methods, we introduce new greedy and local-search heuristics. We also construct a new C++ data structure for Boolean vectors and matrices that is significantly faster than existing ones and is of independent interest, allowing our heuristics to scale to large datasets. We illustrate the performance of all our proposed methods and compare them with the state of the art on various real datasets, both with and without missing data, including applications in topic modeling and imaging.

</details>


### [312] [Learning to Comparison-Shop](https://arxiv.org/abs/2512.04009)
*Jie Tang,Daochen Zha,Xin Liu,Huiji Gao,Liwei He,Stephanie Moyerman,Sanjeev Katariya*

Main category: cs.IR

TL;DR: 本文提出了一种名为Learning-to-Comparison-Shop（LTCS）的新排序架构，旨在建模和学习用户在在线平台上的比较购物行为，显著提升了NDCG和预订转化率。


<details>
  <summary>Details</summary>
Motivation: 传统搜索排序模型孤立评估商品，忽视用户在结果页上多商品比较的行为，导致与用户实际比较购物需求存在脱节。

Method: 提出LTCS系统，通过显式建模用户比较购物行为，并结合离线与在线实验验证其有效性。

Result: 在A/B测试中NDCG提升1.7%，预订转化率提升0.6%，且显著优于当前最优方法。

Conclusion: LTCS有效弥合了搜索排序与用户比较购物行为之间的鸿沟，兼顾业务指标提升与用户体验优化。

Abstract: In online marketplaces like Airbnb, users frequently engage in comparison shopping before making purchase decisions. Despite the prevalence of this behavior, a significant disconnect persists between mainstream e-commerce search engines and users' comparison needs. Traditional ranking models often evaluate items in isolation, disregarding the context in which users compare multiple items on a search results page. While recent advances in deep learning have sought to improve ranking accuracy, diversity, and fairness by encoding listwise context, the challenge of aligning search rankings with user comparison shopping behavior remains inadequately addressed. In this paper, we propose a novel ranking architecture - Learning-to-Comparison-Shop (LTCS) System - that explicitly models and learns users' comparison shopping behaviors. Through extensive offline and online experiments, we demonstrate that our approach yields statistically significant gains in key business metrics - improving NDCG by 1.7% and boosting booking conversion rate by 0.6% in A/B testing - while also enhancing user experience. We also compare our model against state-of-the-art approaches and demonstrate that LTCS significantly outperforms them.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [313] [AGENTSAFE: A Unified Framework for Ethical Assurance and Governance in Agentic AI](https://arxiv.org/abs/2512.03180)
*Rafflesia Khan,Declan Joyce,Mansura Habiba*

Main category: cs.MA

TL;DR: 本文提出了AGENTSAFE框架，旨在为基于大语言模型（LLM）的智能体系统提供端到端的实用治理方案，涵盖风险识别、设计控制、运行时保障与审计追踪，并通过预部署评估与动态运行治理机制提升安全性、隐私性、公平性与系统稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有AI治理框架碎片化、静态化，缺乏面向LLM智能体自主规划、多步工具调用及涌现交互特性的端到端治理能力，难以应对新型风险。

Method: 提出AGENTSAFE框架，将AI风险库结构化为设计层、运行时层和审计层三类控制；建模智能体循环（plan→act→observe→reflect）与工具链，扩展代理特有风险分类；引入行为约束、人工介入升级、场景化预部署评估（含安全/隐私/公平/系统安全）、语义遥测、动态授权、异常检测、可中断机制及密码学溯源等技术。

Result: 实现了统一的风险—控制映射治理框架；构建了可量化的Agent Safety Evaluation方法；落地了支持持续监控、响应与追责的运行时治理机制；在多个安全维度上验证了框架的可行性与有效性。

Conclusion: AGENTSAFE填补了LLM智能体系统全生命周期治理的空白，将抽象风险转化为可实施、可度量、可审计的工程控制，为可信智能体生态提供了系统性治理范式。

Abstract: The rapid deployment of large language model (LLM)-based agents introduces a new class of risks, driven by their capacity for autonomous planning, multi-step tool integration, and emergent interactions. It raises some risk factors for existing governance approaches as they remain fragmented: Existing frameworks are either static taxonomies driven; however, they lack an integrated end-to-end pipeline from risk identification to operational assurance, especially for an agentic platform. We propose AGENTSAFE, a practical governance framework for LLM-based agentic systems. The framework operationalises the AI Risk Repository into design, runtime, and audit controls, offering a governance framework for risk identification and assurance. The proposed framework, AGENTSAFE, profiles agentic loops (plan -> act -> observe -> reflect) and toolchains, and maps risks onto structured taxonomies extended with agent-specific vulnerabilities. It introduces safeguards that constrain risky behaviours, escalates high-impact actions to human oversight, and evaluates systems through pre-deployment scenario banks spanning security, privacy, fairness, and systemic safety. During deployment, AGENTSAFE ensures continuous governance through semantic telemetry, dynamic authorization, anomaly detection, and interruptibility mechanisms. Provenance and accountability are reinforced through cryptographic tracing and organizational controls, enabling measurable, auditable assurance across the lifecycle of agentic AI systems. The key contributions of this paper are: (1) a unified governance framework that translates risk taxonomies into actionable design, runtime, and audit controls; (2) an Agent Safety Evaluation methodology that provides measurable pre-deployment assurance; and (3) a set of runtime governance and accountability mechanisms that institutionalise trust in agentic AI ecosystems.

</details>


### [314] [Learning Network Sheaves for AI-native Semantic Communication](https://arxiv.org/abs/2512.03248)
*Enrico Grimaldi,Mario Edoardo Pandolfo,Gabriele D'Acunto,Sergio Barbarossa,Paolo Di Lorenzo*

Main category: cs.MA

TL;DR: 本文提出了一种面向AI原生6G网络的语义通信框架，通过学习通信拓扑与对齐映射，并结合语义去噪与压缩模块，实现异构AI代理间低噪声、高保真度的潜空间表征交换。


<details>
  <summary>Details</summary>
Motivation: 应对AI驱动下从比特中心向语义中心通信范式的转变，解决异构AI代理在潜空间交换中语义噪声大、任务相关性丢失的问题。

Method: 将问题建模为联合学习通信拓扑与正交对齐映射的网络层（learned network sheaf），并设计语义去噪与压缩模块构建共享全局语义空间，通过迭代闭式更新求解非凸字典学习问题。

Result: 在多个基于真实图像预训练的AI代理上验证：该方法有效提升代理间对齐与语义聚类能力，同时保持下游任务高准确率，并揭示了代理间语义异质性的可解释结构。

Conclusion: 所提框架为AI-native 6G提供了可解释、任务驱动的语义通信新范式，兼具理论严谨性与实际有效性。

Abstract: Recent advances in AI call for a paradigm shift from bit-centric communication to goal- and semantics-oriented architectures, paving the way for AI-native 6G networks. In this context, we address a key open challenge: enabling heterogeneous AI agents to exchange compressed latent-space representations while mitigating semantic noise and preserving task-relevant meaning. We cast this challenge as learning both the communication topology and the alignment maps that govern information exchange among agents, yielding a learned network sheaf equipped with orthogonal maps. This learning process is further supported by a semantic denoising end compression module that constructs a shared global semantic space and derives sparse, structured representations of each agent's latent space. This corresponds to a nonconvex dictionary learning problem solved iteratively with closed-form updates. Experiments with mutiple AI agents pre-trained on real image data show that the semantic denoising and compression facilitates AI agents alignment and the extraction of semantic clusters, while preserving high accuracy in downstream task. The resulting communication network provides new insights about semantic heterogeneity across agents, highlighting the interpretability of our methodology.

</details>


### [315] [A Gossip-Enhanced Communication Substrate for Agentic AI: Toward Decentralized Coordination in Large-Scale Multi-Agent Systems](https://arxiv.org/abs/2512.03285)
*Nafiul I. Khan,Mansura Habiba,Rafflesia Khan*

Main category: cs.MA

TL;DR: 本文提出将gossip协议作为多智能体系统中结构化通信的补充机制，以支持去中心化、自适应和容错的智能体协调，探讨其在上下文传播、不确定性下的鲁棒协调及全局意识涌现中的作用，并指出语义过滤、信任与知识衰减等开放问题。


<details>
  <summary>Details</summary>
Motivation: 随着智能体平台规模扩大，固定角色和预定义工具链已无法满足灵活、去中心化协调的需求；现有结构化通信协议难以支撑大规模自适应系统所需的涌现式和群体智能。

Method: 重新审视并分析gossip协议在多智能体通信中的适用性，探讨其在状态传播、不确定性协调和全局意识形成等方面的作用机制，并识别关键挑战与开放研究问题。

Result: 论证了gossip协议可有效弥补结构化协议的不足，支持上下文丰富、鲁棒且自组织的智能体协调，同时明确了语义相关性、时效性、动作一致性等核心挑战。

Conclusion: gossip协议是构建未来大规模、高自主性、自组织智能体生态系统的必要通信基底，需将其系统性地融入多智能体通信栈，并围绕语义、信任与知识演化开展深入研究。

Abstract: As agentic platforms scale, agents are moving beyond fixed roles and predefined toolchains, creating an urgent need for flexible and decentralized coordination. Current structured communication protocols such as direct agent-to-agent messaging or MCP-style tool calls offer reliability, but they struggle to support the emergent and swarm-like intelligence required in large adaptive systems. Distributed agents must learn continuously, share context fluidly, and coordinate without depending solely on central planners.
  This paper revisits gossip protocols as a complementary substrate for agentic communication. Gossip mechanisms, long valued in distributed systems for their decentralized and fault-tolerant properties, provide scalable and adaptive diffusion of knowledge and fill gaps that structured protocols alone cannot efficiently address. However, gossip also introduces challenges, including semantic relevance, temporal staleness, and limited guarantees on action consistency in rapidly changing environments.
  We examine how gossip can support context-rich state propagation, resilient coordination under uncertainty, and emergent global awareness. We also outline open problems around semantic filtering, trust, and knowledge decay. Rather than proposing a complete framework, this paper presents a research agenda for integrating gossip into multi-agent communication stacks and argues that gossip is essential for future agentic ecosystems that must remain robust, adaptive, and self-organizing as their scale and autonomy increase.

</details>


### [316] [Local Dominance in Mixed-Strength Populations -- Fast Maximal Independent Set](https://arxiv.org/abs/2512.03303)
*Michael Luby,Sandy Irani*

Main category: cs.MA

TL;DR: 本文研究了在混合强度代理模型中，Luby MIS协议是否仍能快速收敛到局部支配状态，并证明了其快速收敛性，同时揭示了异质性对过程动态的显著影响。


<details>
  <summary>Details</summary>
Motivation: 许多自然和工程系统中，代理通过局部竞争形成稳定的支配模式，且这种模式往往比种群规模更快地出现。这促使研究者寻找既能体现代理强度差异又能解释快速收敛的简单数学模型。

Method: 引入混合强度代理模型，其中每个代理从自身分布中抽取强度值；扩展Luby MIS协议，使每个代理按自身分布重复生成强度值；通过理论证明分析其收敛性及动态行为变化。

Result: 证明了扩展后的Luby MIS协议在混合强度下仍具有快速收敛性；发现异质性会显著改变过程动态，例如每轮消除的边比例不再恒定，并构造出渐进更慢进展的例子。

Conclusion: 混合强度下Luby MIS协议依然快速收敛，为自然界中观察到的快速支配现象提供了形式化支持；但强度异质性会导致全局行为发生质的变化，不能简单沿用等强度假设下的结论。

Abstract: In many natural and engineered systems, agents interact through local contests that determine which individuals become dominant within their neighborhoods. These interactions are shaped by inherent differences in strength, and they often lead to stable dominance patterns that emerge surprisingly quickly relative to the size of the population. This motivates the search for simple mathematical models that capture both heterogeneous agent strength and rapid convergence to stable local dominance.
  A widely studied abstraction of local dominance is the Maximal Independent Set (MIS) problem. In the Luby MIS protocol that provably converges quickly to an MIS, each agent repeatedly generates a strength value chosen uniformly and becomes locally dominant if its value is smaller than those of its neighbors. This provides a theoretical explanation for fast dominance convergence in populations of equal-strength agents and naturally raises the question of whether fast convergence also holds in the more realistic setting where agents are inherently mixed-strength.
  To investigate this question, we introduce the mixed-strength agents model, in which each agent draws its strength from its own distribution. We prove that the extension of the Luby MIS protocol where each agent repeatedly generates a strength value from its own distribution still exhibits fast dominance convergence, providing formal confirmation of the rapid convergence observed in many mixed-strength natural processes.
  We also show that heterogeneity can significantly change the dynamics of the process. In contrast to the equal-strength setting, a constant fraction of edges need not be eliminated per round. We construct a population and strength profile in which progress per round is asymptotically smaller, illustrating how inherent strength asymmetry produces qualitatively different global behavior.

</details>


### [317] [AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation](https://arxiv.org/abs/2512.03466)
*Xavier Cadet,Edward Koh,Peter Chin*

Main category: cs.MA

TL;DR: 本文提出AsymPuzl——一个用于评估大语言模型（LLM）在信息不对称下协同沟通能力的双智能体符号谜题环境，并通过多模型实验揭示其沟通策略差异与反馈设计敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体研究多聚焦开放角色扮演，缺乏可控、可量化的通信能力评估环境，尤其在信息不对称下的协作机制尚不清晰。

Method: 构建AsymPuzl：一个最小但富有表达力的双智能体符号谜题环境，两代理各自观察互补而不完整的谜题视图，需通过多轮消息交换协同求解；在多种主流及开源LLM上进行系统性实验，分析消息交互模式、收敛行为及不同反馈机制（如自反馈、联合反馈）的影响。

Result: （i）强模型（如GPT-5、Claude-4.0）能在两轮内共享完整信息并稳定求解；（ii）弱模型常忽略对方消息或过度修正假设；（iii）反馈设计影响显著：简单自反馈提升成功率，而详细联合反馈反而损害性能。

Conclusion: 即使在简单协同任务中，LLM的通信策略也存在显著分化，且高度依赖反馈信号的粒度；AsymPuzl为检验多轮协作极限与协调机制提供了可控、可解释的基准测试平台。

Abstract: Large Language Model (LLM) agents are increasingly studied in multi-turn, multi-agent scenarios, yet most existing setups emphasize open-ended role-play rather than controlled evaluation. We introduce AsymPuzl, a minimal but expressive two-agent puzzle environment designed to isolate communication under information asymmetry. Each agent observes complementary but incomplete views of a symbolic puzzle and must exchange messages to solve it cooperatively. Using a diverse set of current-generation and open-source LLMs, we show that (i) strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns, (ii) weaker models often ignore partner messages or over-correct their hypotheses, and (iii) feedback design is non-trivial: simple self-feedback improves success rates, while detailed joint feedback can hurt performance. These findings show that even in simple cooperative tasks, LLM communication strategies diverge and depend on the granularity of feedback signals. AsymPuzl thus provides a testbed for probing the limits of multi-turn cooperation and opens avenues for studying coordination mechanisms.

</details>


### [318] [SRPG: Semantically Reconstructed Privacy Guard for Zero-Trust Privacy in Educational Multi-Agent Systems](https://arxiv.org/abs/2512.03694)
*Shuang Guo,Zihui Li*

Main category: cs.MA

TL;DR: 本文提出SRPG，一种用于教育多智能体系统的隐私保护机制，通过双流重构机制在确保零PII泄露的同时恢复数学逻辑，显著提升隐私保护与教学效用的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有隐私方法难以在教育多智能体系统中兼顾未成年人PII安全与非结构化对话的教学实用性：基于角色的访问控制不适用于非结构化文本，而简单掩码会破坏教学语境。

Method: 提出SRPG隐私守卫机制，包含严格脱敏流（确保零PII泄露）和上下文重建流（由LLM驱动以恢复数学逻辑），实现教学内容与私人数据解耦。

Result: 在MathDial数据集上测试表明，SRPG在GPT-4o上实现0.0000攻击成功率（零泄露）和0.8267精确匹配率，远超零信任纯LLM基线（0.2138）。

Conclusion: SRPG能有效保护未成年人隐私，且不损害数学教学质量。

Abstract: Multi-Agent Systems (MAS) with large language models (LLMs) enable personalized education but risk leaking minors personally identifiable information (PII) via unstructured dialogue. Existing privacy methods struggle to balance security and utility: role-based access control fails on unstructured text, while naive masking destroys pedagogical context. We propose SRPG, a privacy guard for educational MAS, using a Dual-Stream Reconstruction Mechanism: a strict sanitization stream ensures zero PII leakage, and a context reconstruction stream (LLM driven) recovers mathematical logic. This decouples instructional content from private data, preserving teaching efficacy. Tests on MathDial show SRPG works across models; with GPT-4o, it achieves 0.0000 Attack Success Rate (ASR) (zero leakage) and 0.8267 Exact Match, far outperforming the zero trust Pure LLM baseline (0.2138). SRPG effectively protects minors privacy without sacrificing mathematical instructional quality.

</details>
