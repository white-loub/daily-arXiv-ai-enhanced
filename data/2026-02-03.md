<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 276]
- [cs.CL](#cs.CL) [Total: 178]
- [cs.MA](#cs.MA) [Total: 8]
- [cs.IR](#cs.IR) [Total: 23]
- [cs.LG](#cs.LG) [Total: 421]
- [cs.RO](#cs.RO) [Total: 86]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 153]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions](https://arxiv.org/abs/2602.00095)
*Weiyu Sun,Liangliang Chen,Yongnuo Cai,Huiru Xie,Yi Zeng,Ying Zhang*

Main category: cs.CV

TL;DR: 本文提出EDU-CIRCUIT-HW数据集，用于评估多模态大语言模型（MLLMs）在真实STEM学生手写解题内容识别与自动评分中的性能，发现现有模型存在大量隐性错误，并通过基于错误模式的校正策略显著提升系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 缺乏真实、领域特定的基准来准确评估MLLMs对混杂数学公式、图表和文本推理的手写STEM解题内容的理解能力；现有下游任务评估方式无法全面反映模型对手写逻辑的整体理解。

Method: 构建包含1300+份大学STEM课程真实学生手写解答的EDU-CIRCUIT-HW数据集，结合专家验证的逐字转录和评分报告，同步评估MLLMs的上游识别保真度与下游自动评分性能，并开展基于错误模式识别与修正的案例研究。

Result: 揭示了MLLMs在识别学生手写内容中存在大规模隐性失败，表明其在高风险教育场景（如自动评分）中可靠性不足；仅需约4%人工干预即可显著提升AI评分系统在未见样本上的鲁棒性。

Conclusion: 当前MLLMs尚不足以独立承担教育关键任务，需结合细粒度识别评估与针对性纠错机制以提升实际部署可靠性。

Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.

</details>


### [2] [VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR](https://arxiv.org/abs/2602.01674)
*Hail Song,Boram Yoon,Seokhwan Yang,Seoyoung Kang,Hyunjeong Kim,Henning Metzmacher,Woontack Woo*

Main category: cs.CV

TL;DR: VRGaussianAvatar 是一个基于单张图像重建、仅依赖头显追踪信号即可实现实时全身3D高斯泼溅（3DGS）虚拟现实化身的系统，通过并行前后端架构与创新的双目批处理（Binocular Batching）技术提升渲染效率，并在用户研究中展现出优于图像/视频驱动网格化身的外观相似性、临场感和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有VR化身系统多依赖外部动捕或高成本硬件，难以兼顾实时性、真实感与部署简易性；亟需一种仅用HMD信号即可驱动高质量、全身体态且适配VR显示特性的新范式。

Method: 提出VRGaussianAvatar系统：前端（VR Frontend）利用逆运动学从HMD六自由度追踪推断全身姿态并传输至后端；后端（GA Backend）基于单图重建的3DGS模型进行立体渲染；引入Binocular Batching技术，将左右眼视图联合批处理以减少冗余计算。

Result: 定量测试表明系统达到交互级帧率（VR实时性能）；用户研究显示其在外观相似性、自我化身感（embodiment）和动作可信度（plausibility）上显著优于图像/视频驱动的网格基线方法。

Conclusion: 仅用HMD追踪即可驱动高质量、实时、立体兼容的3DGS全身化身是可行的；Binocular Batching为VR中的3DGS高效渲染提供了实用新方案；该工作推动了轻量级、高保真VR社交化身的发展。

Abstract: We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.

</details>


### [3] [Mirage2Matter: A Physically Grounded Gaussian World Model from Video](https://arxiv.org/abs/2602.00096)
*Zhengqing Gao,Ziwen Li,Xin Wang,Jiaxin Huang,Zhenyang Ren,Mingkai Shao,Hanlue Zhang,Tianyu Huang,Yongkang Cheng,Yandong Guo,Runqi Lin,Yuanyuan Wang,Tongliang Liu,Kun Zhang,Mingming Gong*

Main category: cs.CV

TL;DR: 本文提出Simulate Anything框架，通过多视角视频和现成资产生成高保真具身训练数据，利用3D高斯泼溅重建场景，并结合生成模型实现物理真实仿真，使VLA模型在零样本任务中表现媲美甚至超越真实数据训练效果。


<details>
  <summary>Details</summary>
Motivation: 具身智能的可扩展性受限于真实世界交互数据的稀缺；现有仿真方法存在视觉与物理失真、依赖昂贵传感器或精确标定等问题，难以大规模实用。

Method: 基于多视角环境视频和现成资产，采用3D高斯泼溅（3DGS）重建高保真场景；利用生成模型恢复物理真实表征，并通过精度校准目标实现真实尺度对齐，构建统一、可编辑、物理 grounded 的世界模型。

Result: 基于该仿真数据训练的Vision Language Action（VLA）模型在下游零样本任务中达到甚至超过使用真实数据训练的性能。

Conclusion: 以重建驱动的世界建模是实现可扩展、实用化具身智能训练的有效路径。

Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.

</details>


### [4] [Implicit neural representation of textures](https://arxiv.org/abs/2602.02354)
*Albert Kwok,Zheyuan Hu,Dounia Hammou*

Main category: cs.CV

TL;DR: 本文探索了不同神经网络设计作为新的纹理隐式神经表示（INR），在连续UV坐标空间中运行，并分析了其图像质量、内存占用和渲染推理时间之间的权衡，同时探讨了其在实时渲染和下游任务（如mipmap拟合和INR空间生成）中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索如何设计适用于纹理表示的隐式神经网络（INR），以实现连续UV空间建模，并在图像质量、内存与推理速度之间取得平衡。

Method: 设计多种神经网络结构作为连续域上的纹理INR，并通过系统实验评估其在图像质量、内存占用和渲染推理时间方面的性能；进一步拓展至mipmap拟合与INR空间生成等应用。

Result: 所提出的纹理INR在图像质量上表现良好，但存在内存占用和推理时间的折衷；在实时渲染及下游任务中展现出应用潜力。

Conclusion: 连续域纹理INR是一种有前景的表示方法，需在精度、效率与资源消耗间进行权衡，且可扩展至多种图形学任务。

Abstract: Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.

</details>


### [5] [R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation](https://arxiv.org/abs/2602.00104)
*Zhuohong Chen,Zhengxian Wu,Zirui Liao,Shenao Jiang,Hangrui Xu,Yang Chen,Chaokui Su,Xiaoyu Liu,Haoqian Wang*

Main category: cs.CV

TL;DR: 本文提出R3G框架，通过推理-检索-重排序三阶段策略提升视觉问答（VQA）中的图像检索与融合效果，在MRAG-Bench上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 视觉为中心的VQA检索需补全缺失视觉线索并有效融入推理，但图像选择与融合仍具挑战性。

Method: 提出模块化R3G框架：先生成指定所需视觉线索的简要推理计划；再采用粗粒度检索加细粒度重排序的两阶段策略选取证据图像；引入充分性感知的重排序机制。

Result: 在MRAG-Bench上，R3G在六个MLLM主干模型和九个子场景中均提升准确率，达到整体SOTA；消融实验证明推理步骤与充分性感知重排序互补。

Conclusion: R3G通过解耦推理计划、分层检索与充分性驱动重排序，显著提升多模态检索增强VQA的图像选择与利用能力。

Abstract: Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.

</details>


### [6] [HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models](https://arxiv.org/abs/2602.00105)
*Wing Chan,Richard Allen*

Main category: cs.CV

TL;DR: 本文提出了HYPE-EDIT-1基准，用于评估参考式图像编辑模型在真实营销/设计工作流中的实际性能，综合考虑了单次通过率、重试成本和人工审核时间，揭示了低价模型在总有效成本上未必更优。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型的公开演示多为理想案例，无法反映真实工作流中因重试和人工审核带来的时间与成本开销，亟需一个贴近实际应用的评估基准。

Method: 构建包含100个参考式图像编辑任务的HYPE-EDIT-1基准（50个公开+50个私有），对每个任务生成10个独立输出，统计通过率（pass@10）、期望尝试次数（带重试上限）及综合有效成本（模型调用费+人工审核时间折算）。提供标准化JSON格式与VLM/人工评判工具链。

Result: 所评测模型单次通过率为34%–83%，成功编辑的有效成本为0.66–1.42美元；低价模型因低通过率导致总成本反而更高。

Conclusion: 仅关注单图价格会误导模型选型；真实部署应以‘有效成本’（含重试与人工）为核心指标，HYPE-EDIT-1为此提供了可复现、可扩展的评估框架。

Abstract: Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.

</details>


### [7] [Efficient UAV trajectory prediction: A multi-modal deep diffusion framework](https://arxiv.org/abs/2602.00107)
*Yuan Gao,Xinyu Guo,Wenjing Xie,Zifan Wang,Hongwen Yu,Gongyang Li,Shugong Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于激光雷达（LiDAR）与毫米波雷达数据融合的多模态无人机轨迹预测方法，通过设计多模态深度融合框架（MMDF），利用双向交叉注意力机制实现模态间特征互补与语义对齐，在CVPR 2024 MMAUD数据集上较基线模型提升40%预测精度。


<details>
  <summary>Details</summary>
Motivation: 为应对低空经济中对非法无人机的管控需求，需提升其轨迹预测精度与鲁棒性；单一传感器在几何结构或动态反射特性方面存在局限，亟需融合LiDAR与毫米波雷达的互补信息。

Method: 构建多模态深度融合框架（MMDF），包含两个模态专用特征提取网络（结构相同但权重独立）和一个双向交叉注意力融合模块；采用MMAUD数据集进行训练与测试，并开展消融实验验证不同损失函数与后处理策略的影响。

Result: 在MMAUD数据集上，该模型相较基线模型轨迹预测精度提升40%；消融实验验证了双向交叉注意力机制、特定损失函数及后处理策略的有效性。

Conclusion: 所提多模态融合方法能有效协同LiDAR与毫米波雷达数据，在复杂低空场景下显著提升非法无人机轨迹预测性能，为低空安防提供高效可行的技术方案。

Abstract: To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.

</details>


### [8] [SITUATE -- Synthetic Object Counting Dataset for VLM training](https://arxiv.org/abs/2602.00108)
*René Peinl,Vincent Tischler,Patrick Schröder,Christian Groth*

Main category: cs.CV

TL;DR: SITUATE是一个专为视觉语言模型计数任务（含空间约束）设计的新数据集，旨在弥合简单2D数据集与模糊真实数据集之间的鸿沟；实验证明其能提升模型在分布外图像上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有计数数据集的局限性：简单2D数据集缺乏现实复杂性，而真实数据集（如TallyQA）缺乏对遮挡和空间构成的可控性。

Method: 构建SITUATE数据集，并在该数据集上对Qwen VL 2.5 7B模型进行微调，通过跨数据集性能对比（如Pixmo count测试集及其他计数基准）验证其有效性。

Result: 在SITUATE上微调的Qwen VL 2.5 7B显著提升了在Pixmo count测试集上的准确率，但反向微调无效；该提升在其他计数基准上也得到交叉验证。

Conclusion: SITUATE数据集有效提升了视觉语言模型在带空间约束的计数任务中的泛化能力，尤其对分布外图像具有积极影响。

Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.

</details>


### [9] [Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios](https://arxiv.org/abs/2602.00109)
*John J. Howard,Richard O. Plesh,Yevgeniy B. Sirotin,Jerry L. Tipton,Arun R. Vemury*

Main category: cs.CV

TL;DR: 本文研究了低光照和自动图像采集对商用呈现攻击检测（PAD）系统在远程身份验证（RIV）中鲁棒性的影响，发现多数系统性能显著下降，仅一个系统在所有测试场景下保持低于3%的错误率。


<details>
  <summary>Details</summary>
Motivation: 确保PAD子系统在多样化环境与操作条件下的鲁棒性，以支持有效且用户友好的远程身份验证（RIV）系统。

Method: 通过RIV场景测试，评估商用PAD系统在低光照条件和自动图像采集流程下的性能变化。

Result: PAD系统在低光照下错误率预计增加约四倍，在自动采集流程下错误率翻倍；仅一个系统在所有场景中保持低于3%的合法呈现分类错误率。

Conclusion: 需在多样化真实环境条件下对PAD系统进行充分测试，以保障其在实际应用中的稳健性和可靠性。

Abstract: Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.

</details>


### [10] [Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer](https://arxiv.org/abs/2602.00110)
*Yu Li,Guilherme N. DeSouza,Praveen Rao,Chi-Ren Shyu*

Main category: cs.CV

TL;DR: 本文提出了一种新型遥感图像处理模型，通过引入地理空间嵌入机制和引导注意力模块，融合辅助地理空间信息，提升多模态地理空间理解能力，在疾病流行率预测任务上优于现有地理空间基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言及多模态模型侧重于视觉与文本语义对齐，缺乏对结构化地理空间图层的表征与推理能力，难以有效利用辅助地理空间信息。

Method: 提出地理空间嵌入机制，将多样地理空间数据转化为与图像块空间对齐的嵌入块；设计引导注意力模块，基于辅助数据相关性动态计算注意力权重，并为各注意力头分配不同角色以捕获互补信息。

Result: 在疾病流行率预测任务上，所提框架显著优于现有预训练地理空间基础模型，验证了其在多模态地理空间理解上的有效性。

Conclusion: 该模型通过空间对齐的地理空间嵌入与可解释的引导注意力机制，有效提升了遥感影像对地理空间语义的理解与推理能力。

Abstract: Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.

</details>


### [11] [From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves](https://arxiv.org/abs/2602.00111)
*Haiyu Yang,Heidi Lesscher,Enhong Liu,Miel Hostens*

Main category: cs.CV

TL;DR: 本研究调查了荷兰14个商业农场中60头群养奶牛犊的空间配给（2.66–17.98 m²/头）与其玩耍行为的关系，并开发了一种高精度（97.6%准确率）的计算机视觉自动监测系统；发现玩耍行为呈非线性变化，8–10 m²/头时最高（1.6%观察时间），提出该区间为兼顾福利与经济性的实用目标。


<details>
  <summary>Details</summary>
Motivation: 玩耍行为是奶牛犊福利的正向指标，但商业条件下中高空间配给（6–20 m²/头）对其影响尚不明确，亟需量化关系并发展可扩展的自动化监测方法。

Method: 在14个商业农场对60头群养奶牛犊进行视频观察，使用详细行为谱分析玩耍行为（以占观察期百分比表示）；采用含农场随机效应的线性混合模型统计分析；构建并验证基于108小时人工标注数据训练的计算机视觉管道。

Result:  calves平均玩耍时间为观察期的1.0%（约10分钟/17小时）；玩耍与空间呈非线性关系，峰值在8–10 m²/头（1.6% OP），低谷在6–8 m²和12–14 m²（<0.6% OP）；空间效应在控制年龄、健康和群体大小后仍显著；计算机视觉分类器达97.6%准确率、99.4%召回率。

Conclusion: 8–10 m²/头是提升奶牛犊福利且具经济可行性的推荐空间配给；自动化监测技术可将小规模人工标注扩展为持续性动物福利评估系统。

Abstract: Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.

</details>


### [12] [AI-Driven Three-Dimensional Reconstruction and Quantitative Analysis for Burn Injury Assessment](https://arxiv.org/abs/2602.00113)
*S. Kalaycioglu,C. Hong,K. Zhai,H. Xie,J. N. Wong*

Main category: cs.CV

TL;DR: 本文提出了一种结合多视角摄影测量、3D表面重建与深度学习分割的AI驱动烧伤评估平台，实现基于普通相机图像的客观、可重复、几何感知的烧伤面积、深度及愈合进程量化分析。


<details>
  <summary>Details</summary>
Motivation: 传统烧伤评估依赖主观视觉检查和2D摄影，难以进行准确、可重复及纵向比较，亟需客观、量化、非侵入式方法。

Method: 整合多视角摄影测量、患者特异性3D表面重建、解剖映射、深度学习分割，并嵌入结构化临床工作流，支持标准多角度消费级相机图像输入与时空对齐的纵向分析。

Result: 系统能稳定重建3D烧伤表面，精确计算真实单位下的烧伤面积（TBSA）、深度相关几何代理指标及体积变化；通过连续重建空间对齐，实现伤口收缩与深度减轻的客观量化追踪；仿真评估验证了其稳定性、一致性与临床合理性。

Conclusion: 该平台为急症与门诊烧伤管理提供了可扩展、无创、几何感知的客观评估与决策支持新范式。

Abstract: Accurate, reproducible burn assessment is critical for treatment planning, healing monitoring, and medico-legal documentation, yet conventional visual inspection and 2D photography are subjective and limited for longitudinal comparison. This paper presents an AI-enabled burn assessment and management platform that integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within a structured clinical workflow. Using standard multi-angle images from consumer-grade cameras, the system reconstructs patient-specific 3D burn surfaces and maps burn regions onto anatomy to compute objective metrics in real-world units, including surface area, TBSA, depth-related geometric proxies, and volumetric change. Successive reconstructions are spatially aligned to quantify healing progression over time, enabling objective tracking of wound contraction and depth reduction. The platform also supports structured patient intake, guided image capture, 3D analysis and visualization, treatment recommendations, and automated report generation. Simulation-based evaluation demonstrates stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support in acute and outpatient care.

</details>


### [13] [1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization](https://arxiv.org/abs/2602.00114)
*Yunwei Bai,Ying Kiat Tan,Yao Shu,Tsuhan Chen*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、即插即用的测试时数据增强方法1S-DAug，仅用单张图像即可生成多样且保真的增强样本，显著提升少样本学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统测试时增强方法在少样本学习（FSL）中效果不佳，而FSL要求模型仅凭极少量标注样本就能泛化到新类别，亟需更有效的测试时增强策略。

Method: 提出1S-DAug：一种单样本生成式增强算子，结合几何变换、可控噪声注入与以原图条件引导的去噪扩散过程，在测试时从单张图像生成多样化增强样本；再将生成图像与原图的编码特征聚合为联合表征用于预测。

Result: 作为训练无关、模型无关的插件，1S-DAug在4个标准FSL基准数据集上均取得一致提升，在miniImagenet 5-way-1-shot任务上实现超10%的相对准确率提升。

Conclusion: 1S-DAug验证了高质量、条件可控的测试时生成式增强能有效缓解少样本学习中的泛化瓶颈，为无需微调的轻量级性能提升提供了新范式。

Abstract: Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.

</details>


### [14] [Event Driven Clustering Algorithm](https://arxiv.org/abs/2602.00115)
*David El-Chai Ben-Ezra,Adar Tal,Daniel Brisk*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的异步、事件驱动算法，用于实时检测事件相机数据中的小事件簇，具有线性时间复杂度O(n)，且运行时间与像素阵列尺寸无关。


<details>
  <summary>Details</summary>
Motivation: 为实现实时、高效的小事件簇检测，充分利用事件相机异步数据结构的特点。

Method: 提出一种基于事件时空距离的异步、事件驱动的层次凝聚聚类算法，通过精巧、高效且简单的决策机制实现线性复杂度。

Result: 算法时间复杂度为O(n)，运行时间不依赖于像素阵列维度，适用于实时处理。

Conclusion: 该算法在保持聚类效果的同时显著提升了计算效率，适合事件相机的实时应用。

Abstract: This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.

</details>


### [15] [IC-EO: Interpretable Code-based assistant for Earth Observation](https://arxiv.org/abs/2602.00117)
*Lamia Lahouel,Laurynas Lopata,Simon Gruening,Gabriele Meoni,Gaetan Petit,Sylvain Lobry*

Main category: cs.CV

TL;DR: 本文提出了一种基于工具增强型大语言模型（Tool LLM）的对话式、代码生成代理，将自然语言查询转化为可执行、可审计的Python地理空间分析工作流，支持分类、分割、检测、光谱指数计算等任务，并在土地组成制图和野火后损毁评估两个用例中显著优于GPT-4o、LLaVA等通用多模态大模型。


<details>
  <summary>Details</summary>
Motivation: 地球观测（EO）分析对非专业人士门槛高，依赖专家知识与技术能力；现有系统多为黑箱预测，难以审计与复现。

Method: 构建一个基于工具增强大语言模型的对话式代码生成代理，通过统一、易扩展的API接口支持多种EO分析任务（分类、分割、定向目标检测、光谱指数、地理空间运算），并实现工具级、代理级、任务级三层可控性评估。

Result: 在土地组成映射任务中准确率达64.2%（vs. GPT-4o 51.7%）；在野火后损毁评估中达50%（vs. GPT-4o/LLaVA 0%）；生成代码可验证、结果透明可解释、流程可复现。

Conclusion: 该方法将EO分析转化为透明、可复现、低门槛的过程，为非专业用户提供了可靠、可审计的智能地理空间分析新范式。

Abstract: Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances in tool LLMs, this study proposes a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows. The agent operates over a unified easily extendable API for classification, segmentation, detection (oriented bounding boxes), spectral indices, and geospatial operators. With our proposed framework, it is possible to control the results at three levels: (i) tool-level performance on public EO benchmarks; (ii) at the agent-level to understand the capacity to generate valid, hallucination-free code; and (iii) at the task-level on specific use cases. In this work, we select two use-cases of interest: land-composition mapping and post-wildfire damage assessment. The proposed agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis, while producing results that are transparent and easy to interpret. By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process.

</details>


### [16] [VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents](https://arxiv.org/abs/2602.00122)
*Hongzhu Yi,Yujia Yang,Yuanxiang Wang,Zhenyu Guan,Jiahuan Chen,Chenxi Bao,Tiankun Yang,Yixuan Yuan,Tianyu Zong,Xinming Wang,Tao Yu,Ruiwen Tao,Haijin Liang,Jin Ma,Jinwen Luo,Yeshani Xinyu Zuo,Jungang Xu*

Main category: cs.CV

TL;DR: 本文提出了VDE Bench，首个针对多语言和密集文本视觉文档编辑任务的系统性基准，包含高质量英文和中文密集文本数据集，并引入解耦评估框架以细粒度评估文本修改准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注英语和稀疏文本布局的文档，无法充分处理密集、结构复杂的文档或非拉丁文字（如中文）。

Method: 提出VDE Bench基准，包含人工标注与评估的多语言密集文本数据集（学术论文、海报、幻灯片等），并设计基于OCR解析的解耦评估框架。

Result: 对多个SOTA图像编辑模型进行了全面评估，人工验证表明自动评估指标与人类判断高度一致。

Conclusion: VDE Bench是首个系统评估多语言、密集文本视觉文档编辑模型的基准，填补了该领域空白。

Abstract: In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \textbf{V}isual \textbf{D}oc \textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.

</details>


### [17] [Context-Aware Autoencoders for Anomaly Detection in Maritime Surveillance](https://arxiv.org/abs/2602.00124)
*Divya Acharya,Pierre Bernab'e,Antoine Chevrot,Helge Spieker,Arnaud Gotlieb,Bruno Legeard*

Main category: cs.CV

TL;DR: 本文提出了一种上下文感知自编码器，通过引入上下文特定阈值来提升海上船舶交通监控中集体和上下文异常的检测精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统自编码器在检测海上船舶AIS数据中的集体和上下文异常方面效果有限，而海上异常高度依赖于船舶自身报告的上下文信息。

Method: 提出上下文感知自编码器，集成上下文特定阈值，并对比四种变体与常规自编码器在渔船状态异常检测案例中的性能。

Result: 实验表明上下文对重构误差和异常检测影响显著，上下文感知自编码器在时序异常检测中性能最优。

Conclusion: 引入上下文特定阈值并重视上下文作用，可有效提升海上船舶交通监控系统的异常检测精度。

Abstract: The detection of anomalies is crucial to ensuring the safety and security of maritime vessel traffic surveillance. Although autoencoders are popular for anomaly detection, their effectiveness in identifying collective and contextual anomalies is limited, especially in the maritime domain, where anomalies depend on vessel-specific contexts derived from self-reported AIS messages. To address these limitations, we propose a novel solution: the context-aware autoencoder. By integrating context-specific thresholds, our method improves detection accuracy and reduces computational cost. We compare four context-aware autoencoder variants and a conventional autoencoder using a case study focused on fishing status anomalies in maritime surveillance. Results demonstrate the significant impact of context on reconstruction loss and anomaly detection. The context-aware autoencoder outperforms others in detecting anomalies in time series data. By incorporating context-specific thresholds and recognizing the importance of context in anomaly detection, our approach offers a promising solution to improve accuracy in maritime vessel traffic surveillance systems.

</details>


### [18] [D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection](https://arxiv.org/abs/2602.00126)
*Dmytro Filatov,Valentyn Fedorov,Vira Filatova,Andrii Zelenchuk*

Main category: cs.CV

TL;DR: 本文提出D3R-Net，一种双域去噪重建框架，结合自监督‘修复’任务与频域感知正则化（FFT损失），提升无监督工业缺陷检测的定位精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 重建类无监督异常检测方法虽高效但易过度平滑，难以保留高频细节，导致细微缺陷被部分重建而非凸显，影响分割精度。

Method: 提出D3R-Net：采用轻量卷积自编码器结构；训练时输入合成损坏的正常图像，重建原始干净图像以学习无缺陷纹理流形；除空间MSE损失外，引入FFT幅值损失进行频域一致性约束；可选加入SSIM损失并做消融分析。

Result: 在MVTec AD Hazelnut上，PRO AUC从0.603提升至0.687；十五类平均像素级ROC AUC从0.733升至0.751，PRO AUC从0.417升至0.468；单GPU推理速度约20 FPS。

Conclusion: D3R-Net通过频域正则化有效缓解重建过平滑问题，在保持效率的同时显著提升缺陷定位一致性，为工业视觉检测提供了一种轻量、端到端可训练的实用方案。

Abstract: Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.

</details>


### [19] [PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living](https://arxiv.org/abs/2602.00131)
*Fraser Robinson,Souren Pashangpour,Matthew Lisondra,Goldie Nejat*

Main category: cs.CV

TL;DR: 本文提出了一种名为POVNet+的多模态深度学习架构，用于社会辅助机器人对日常活动（ADLs）的识别与主动辅助，能区分已知、未知及异常执行的ADL，并通过用户状态估计实现新ADL识别与性能监控，实验表明其在分类精度和真实人机交互中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长期部署社会辅助机器人面临的主要障碍是其无法同时感知并协助多种日常生活活动（ADLs）。

Method: 提出多模态深度学习架构POVNet+，引入ADL嵌入空间与运动嵌入空间联合建模，结合新型用户状态估计方法，实现对已知、未知及异常ADL的区分与识别，并驱动机器人主动发起辅助交互。

Result: 在ADL分类准确率上优于当前最先进的人类活动识别方法；在杂乱生活环境中与多用户及机器人Leia的实际交互实验中，成功识别多种已知/未知/异常ADL，并触发恰当的辅助行为。

Conclusion: POVNet+为社会辅助机器人提供了可扩展、鲁棒且具备主动性的多活动感知与响应能力，推动其实用化部署。

Abstract: A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.

</details>


### [20] [Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https://arxiv.org/abs/2602.00132)
*Jiao Li,Jian Lang,Xikai Tang,Wenzheng Shu,Ting Zhong,Qiang Gao,Yong Wang,Leiting Chen,Fan Zhou*

Main category: cs.CV

TL;DR: 本文提出SCANNER，首个面向仇恨视频检测（HVD）的测试时自适应（TTA）框架，通过挖掘跨域稳定的语义核心（如性别、种族等），结合质心引导对齐、样本级自适应对齐与簇内多样性正则化，有效应对HVD中严重的语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有HVD方法假设训练与测试数据分布一致，但实际中仇恨内容不断演化以规避审查，导致严重语义漂移；传统TTA方法仅适用于轻微分布偏移，难以应对HVD中的剧烈变化。

Method: SCANNER基于仇恨内容表层多变但底层核心（如针对性别、种族等）不变的洞察，提出三阶段方法：1）质心引导对齐揭示稳定核心；2）样本级自适应质心对齐缓解离群样本干扰；3）簇内多样性正则化防止语义坍缩。

Result: SCANNER在多个HVD基准上显著优于所有基线方法，Macro-F1平均提升4.69%。

Conclusion: SCANNER验证了利用稳定语义核心进行测试时自适应的有效性，为应对动态演化的仇恨内容提供了新范式。

Abstract: Hate Video Detection (HVD) is crucial for online ecosystems. Existing methods assume identical distributions between training (source) and inference (target) data. However, hateful content often evolves into irregular and ambiguous forms to evade censorship, resulting in substantial semantic drift and rendering previously trained models ineffective. Test-Time Adaptation (TTA) offers a solution by adapting models during inference to narrow the cross-domain gap, while conventional TTA methods target mild distribution shifts and struggle with the severe semantic drift in HVD. To tackle these challenges, we propose SCANNER, the first TTA framework tailored for HVD. Motivated by the insight that, despite the evolving nature of hateful manifestations, their underlying cores remain largely invariant (i.e., targeting is still based on characteristics like gender, race, etc), we leverage these stable cores as a bridge to connect the source and target domains. Specifically, SCANNER initially reveals the stable cores from the ambiguous layout in evolving hateful content via a principled centroid-guided alignment mechanism. To alleviate the impact of outlier-like samples that are weakly correlated with centroids during the alignment process, SCANNER enhances the prior by incorporating a sample-level adaptive centroid alignment strategy, promoting more stable adaptation. Furthermore, to mitigate semantic collapse from overly uniform outputs within clusters, SCANNER introduces an intra-cluster diversity regularization that encourages the cluster-wise semantic richness. Experiments show that SCANNER outperforms all baselines, with an average gain of 4.69% in Macro-F1 over the best.

</details>


### [21] [LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models](https://arxiv.org/abs/2602.00135)
*Pengcheng Zheng,Chaoning Zhang,Jiarong Mo,GuoHui Li,Jiaquan Zhang,Jiahao Zhang,Sihan Cao,Sheng Zheng,Caiyan Qin,Guoqing Wang,Yang Yang*

Main category: cs.CV

TL;DR: 本文提出LLaVA-FA，一种在频域中联合进行低秩分解与量化以压缩大视觉语言模型（LMMs）的新方法，并引入PolarQuant和可选对角校准（ODC）提升精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有LMM压缩方法常将低秩分解与量化解耦，导致重建误差累积，尤其在存在跨模态冗余的架构中；且高计算与内存开销阻碍实际部署。

Method: 提出频域联合低秩+量化近似框架LLaVA-FA；利用傅里叶变换的去相关性与共轭对称性；设计面向复数矩阵的极坐标量化方法PolarQuant；引入无需大规模校准数据的可选对角校准（ODC）方案。

Result: 在多个基准上显著优于现有高效多模态模型，同时保持极低激活参数量与计算成本。

Conclusion: LLaVA-FA是一种高效、准确且实用的大视觉语言模型压缩方案，为LMM轻量化提供了新范式。

Abstract: Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.

</details>


### [22] [Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers](https://arxiv.org/abs/2602.00144)
*Xuan Rao,Mingming Ha,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.CV

TL;DR: 本文提出了一种面向ViT的高效类增量学习框架LR-RGDA+HopDC，用解析式低秩RGDA分类器替代迭代SGD，并引入无训练的Hopfield分布补偿机制缓解表征漂移，显著提升可扩展性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于ViT的类增量学习在分类器重建阶段依赖计算昂贵的迭代SGD；同时，骨干网络更新导致的历史类表征漂移未被有效缓解。

Method: 提出低秩分解的正则化高斯判别分析（LR-RGDA）以降低RGDA的推理复杂度；并设计基于现代连续Hopfield网络的无训练分布补偿器（HopDC），利用无标签锚点通过联想记忆动态校准历史类统计量。

Result: 在多个CIL基准上达到SOTA性能，推理复杂度从O(Cd²)降至O(d² + Crd²)，其中r≪d；HopDC提供理论误差界保障。

Conclusion: LR-RGDA与HopDC协同构成一种兼具Bayes最优性、高效率和强鲁棒性的ViT类增量学习新范式，为大规模场景提供可扩展解决方案。

Abstract: Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analysis (RGDA) provides a Bayes-optimal alternative with accuracy comparable to SGD-based classifiers; however, its quadratic inference complexity limits its use in large-scale CIL scenarios. To overcome this, we propose Low-Rank Factorized RGDA (LR-RGDA), a scalable classifier that combines RGDA's expressivity with the efficiency of linear classifiers. By exploiting the low-rank structure of the covariance via the Woodbury matrix identity, LR-RGDA decomposes the discriminant function into a global affine term refined by a low-rank quadratic perturbation, reducing the inference complexity from $\mathcal{O}(Cd^2)$ to $\mathcal{O}(d^2 + Crd^2)$, where $C$ is the class number, $d$ the feature dimension, and $r \ll d$ the subspace rank. To mitigate representation drift caused by backbone updates, we further introduce Hopfield-based Distribution Compensator (HopDC), a training-free mechanism that uses modern continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors, accompanied by a theoretical bound on the estimation error. Extensive experiments on diverse CIL benchmarks demonstrate that our framework achieves state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs. Code: https://github.com/raoxuan98-hash/lr_rgda_hopdc.

</details>


### [23] [DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images](https://arxiv.org/abs/2602.00145)
*Siva Teja Kakileti,Geetha Manjunath*

Main category: cs.CV

TL;DR: 本文提出DensiThAI，一种基于多视角红外热成像的深度学习框架，用于无电离辐射的乳腺密度分类，在3500名女性多中心数据集上达到平均AUROC 0.73，验证了热成像评估乳腺密度的可行性。


<details>
  <summary>Details</summary>
Motivation: 乳腺组织密度是乳腺癌风险的关键生物标志物，但目前主要依赖有电离辐射的X线钼靶检查；亟需一种安全、无电离的替代评估方法。

Method: 提出多视角深度学习框架DensiThAI，利用五种标准热成像视角，以钼靶导出的密度标签为金标准，在多中心热图像数据集上训练与评估。

Result: 在10次随机划分上平均AUROC达0.73，所有划分中各密度类别间均存在统计学显著差异（p << 0.05），且在不同年龄组表现稳定。

Conclusion: 红外热成像结合AI可用于无电离、便捷的乳腺密度评估，有望改善患者体验并优化临床工作流。

Abstract: Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p << 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.

</details>


### [24] [Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields](https://arxiv.org/abs/2602.00148)
*Shiqian Li,Ruihong Shen,Junfeng Ni,Chang Pan,Chi Zhang,Yixin Zhu*

Main category: cs.CV

TL;DR: 本文提出Neural Gaussian Force Field (NGFF)，一种端到端神经框架，融合3D高斯感知与物理动力学建模，从多视角RGB输入生成交互式、物理真实的4D视频，速度比先前高斯仿真器快两个数量级；同时发布大规模4D高斯数据集GSCollision（64万+视频，约4TB），用于训练和评估。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏对物理规律的建模，难以生成物理合理视频；结合3D高斯重建与物理引擎的方法虽可行，但计算开销大、鲁棒性差，尤其在复杂真实场景中。

Method: 提出Neural Gaussian Force Field（NGFF），将3D高斯表征与神经力场耦合，实现端到端物理驱动的4D视频生成；构建大规模物理仿真数据集GSCollision，涵盖多种材质、多物体交互与复杂场景。

Result: NGFF在合成与真实3D场景中展现出强泛化性与物理推理鲁棒性，生成速度比此前高斯仿真器快约100倍；GSCollision数据集为物理视频理解与生成提供新基准。

Conclusion: NGFF推动了视频预测向物理可解释、可交互、高效的世界模型迈进，为AI构建具备物理常识的视觉动态理解能力提供了新范式。

Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.

</details>


### [25] [SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles](https://arxiv.org/abs/2602.00149)
*Shucong Li,Xiaoluo Zhou,Yuqian He,Zhenyu Liu*

Main category: cs.CV

TL;DR: 本文提出SDCM框架，通过模拟稠密化（SimDen）、雷达补偿映射（RCM）和Mamba建模交互融合（MMIF）模块，解决4D雷达-视觉融合3D目标检测中雷达点云稀疏和视觉表征退化问题，在多个数据集上实现高性能、低参数量和快速推理。


<details>
  <summary>Details</summary>
Motivation: 4D雷达点云稀疏导致3D表征差；视觉数据在低光、远距离和严重遮挡场景下表征退化，影响融合可靠性。

Method: 提出SDCM框架：1）SimDen模块基于3D核密度估计关键点的高斯模拟和曲率模拟生成稠密雷达点云；2）RCM模块利用雷达全天候优势补偿视觉退化；3）MMIF模块通过建模特征张量差异值实现异构降低与模态交互融合。

Result: 在VoD、TJ4DRadSet和Astyx HiRes 2019数据集上，SDCM取得最优性能，同时参数量更少、推理速度更快。

Conclusion: SDCM有效缓解了雷达稀疏性和视觉退化问题，提升了IoV中雷达-视觉融合3D检测的鲁棒性与效率。

Abstract: 3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.

</details>


### [26] [Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency](https://arxiv.org/abs/2602.00151)
*Alexander Blezinger,Wolfgang Nejdl,Ming Tang*

Main category: cs.CV

TL;DR: 本文系统评估了组织病理学基础模型在回归任务（如HRD评分预测）中的性能，发现其特征显著优于对比学习特征，并提出分布式上采样策略以改善数据不平衡问题，提升了对临床重要亚群的预测效果。


<details>
  <summary>Details</summary>
Motivation: 基础模型在计算病理学中广泛应用，但其在回归型生物标志物预测任务中的潜力尚未被充分探索。

Method: 在多实例学习框架下，使用5种前沿组织病理学基础模型提取全切片图像的补丁级特征，结合分布式的上采样策略和消融实验，训练模型预测连续HRD评分。

Result: 基于基础模型特征训练的模型在预测精度和泛化能力上持续优于基线；不同基础模型表现存在系统性差异；所提上采样策略显著提升罕见临床亚群的召回率与平衡准确率。

Conclusion: 大规模组织病理学预训练能显著提升回归型生物标志物预测的精度与可迁移性，有望推动AI驱动的精准肿瘤学发展。

Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.

</details>


### [27] [Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion](https://arxiv.org/abs/2602.00152)
*Boyu Li,Kuangji Zuo,Lincong Li,Yonghui Wu*

Main category: cs.CV

TL;DR: 本文提出了一种面向边缘设备的资源感知型分层网络HPPI-Net，用于实时人体活动识别（HAR），在ARM Cortex-M4上实现高精度（96.70%）与极低资源占用（22.3 KiB RAM，439.5 KiB ROM），并具备可解释性。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上对高精度、低功耗实时模式识别的需求日益增长，但现有方法难以兼顾准确率与计算资源限制。

Method: 提出分层并行伪图像增强融合网络（HPPI-Net）：第一层用FFT谱图提取初步特征；第二层根据活动类型动态选择专用模块或并行LSTM-MobileNet网络（PLMN），后者融合FFT、小波和Gabor谱图，并引入ECA注意力与深度可分离卷积以提升通道级可解释性并降低计算量。

Result: 在ARM Cortex-M4上达到96.70%准确率，仅需22.3 KiB RAM和439.5 KiB ROM；相比MobileNetV3，准确率提升1.22%，RAM减少71.2%，ROM减少42.1%。

Conclusion: HPPI-Net在精度、效率与可解释性之间取得良好平衡，为可穿戴设备、工业与智能家居等内存受限边缘平台上的HAR提供了实用可行的解决方案。

Abstract: The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules, namely the Hierarchical Parallel Pseudo-image Enhancement Fusion Network (HPPI-Net), is proposed for real-time, on-device Human Activity Recognition (HAR). Deployed on an ARM Cortex-M4 microcontroller for low-power real-time inference, HPPI-Net achieves 96.70% accuracy while utilizing only 22.3 KiB of RAM and 439.5 KiB of ROM after optimization. HPPI-Net employs a two-layer architecture. The first layer extracts preliminary features using Fast Fourier Transform (FFT) spectrograms, while the second layer selectively activates either a dedicated module for stationary activity recognition or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through three parallel LSTM encoders and refines the concatenated features using Efficient Channel Attention (ECA) and Depthwise Separable Convolution (DSC), thereby offering channel-level interpretability while substantially reducing multiply-accumulate operations. Compared with MobileNetV3, HPPI-Net improves accuracy by 1.22% and reduces RAM usage by 71.2% and ROM usage by 42.1%. These results demonstrate that HPPI-Net achieves a favorable accuracy-efficiency trade-off and provides explainable predictions, establishing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms.

</details>


### [28] [See Without Decoding: Motion-Vector-Based Tracking in Compressed Video](https://arxiv.org/abs/2602.00153)
*Axel Duché,Clément Chatelain,Gilles Gasso*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级压缩域跟踪模型，直接在视频流上运行，无需完全解码RGB视频，利用压缩数据中的运动矢量和变换系数，通过深度模型跨帧传播目标边界框，在MOTS15/17/20数据集上实现最高3.7倍计算加速，仅损失4% mAP@0.5。


<details>
  <summary>Details</summary>
Motivation: 为实现实时大规模监控系统的高效视频分析，避免全解码RGB视频带来的高计算开销。

Method: 设计一种轻量级深度模型，直接利用视频压缩域中的运动矢量和DCT变换系数进行目标跟踪，跳过完整RGB解码过程。

Result: 在MOTS15/17/20数据集上相比RGB基线方法获得最高3.7倍速度提升，mAP@0.5仅下降4%。

Conclusion: 压缩域（codec-domain）的运动建模可显著提升实时视频分析效率，是面向大规模监控系统的一种高效可行路径。

Abstract: We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.

</details>


### [29] [Deep Learning Pose Estimation for Multi-Label Recognition of Combined Hyperkinetic Movement Disorders](https://arxiv.org/abs/2602.00163)
*Laura Cif,Diane Demailly,Gabriella A. Horvàth,Juan Dario Ortigoza Escobar,Nathalie Dorison,Mayté Castro Jiménez,Cécile A. Hubsch,Thomas Wirth,Gun-Marie Hariz,Sophie Huby,Morgan Dornadic,Zohra Souei,Muhammad Mushhood Ur Rehman,Simone Hemm,Mehdi Boulayme,Eduardo M. Moraud,Jocelyne Bloch,Xavier Vasques*

Main category: cs.CV

TL;DR: 本文提出了一种基于姿态估计的机器学习框架，利用常规门诊视频提取关键点时间序列，并计算多维度运动学特征，以客观、可扩展地区分儿童和成人中重叠的高动力性运动障碍表型。


<details>
  <summary>Details</summary>
Motivation: 高动力性运动障碍（HMDs）临床表现波动、间歇且常共存，导致主观评估困难、识别率低、纵向监测不可靠，缺乏客观、可扩展的视频分析方法。

Method: 开发基于姿态估计的机器学习框架，将标准门诊视频转化为解剖学有意义的关键点时间序列，并计算涵盖统计、时域、频域及高阶不规则性-复杂性特征的运动学描述符。

Result: 实现了从常规临床视频中自动、客观地区分多种HMD表型（如肌张力障碍、震颤、舞蹈样动作等），提升了识别与监测的准确性与可重复性。

Conclusion: 该框架为HMDs的无创、低成本、规模化临床评估提供了新范式，有望改善诊断一致性与疾病进展追踪。

Abstract: Hyperkinetic movement disorders (HMDs) such as dystonia, tremor, chorea, myoclonus, and tics are disabling motor manifestations across childhood and adulthood. Their fluctuating, intermittent, and frequently co-occurring expressions hinder clinical recognition and longitudinal monitoring, which remain largely subjective and vulnerable to inter-rater variability. Objective and scalable methods to distinguish overlapping HMD phenotypes from routine clinical videos are still lacking. Here, we developed a pose-based machine-learning framework that converts standard outpatient videos into anatomically meaningful keypoint time series and computes kinematic descriptors spanning statistical, temporal, spectral, and higher-order irregularity-complexity features.

</details>


### [30] [YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation](https://arxiv.org/abs/2602.00168)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: YOLOE-26 是一个融合 YOLOv26 高效架构与 YOLOE 开放词汇学习范式的实时开放词汇实例分割统一框架，支持文本、视觉示例及无提示三种模式的无缝切换。


<details>
  <summary>Details</summary>
Motivation: 拓展YOLO系列从闭集识别到开放词汇场景的能力，兼顾实时性、确定性与多模态提示灵活性。

Method: 基于NMS-free的YOLOv26架构，引入对象嵌入头替代固定类别logits；提出RepRTA（零开销文本对齐）、SAVPE（语义激活视觉提示编码器）和Lazy Region Prompt Contrast（懒式区域提示对比）三大组件，所有提示方式共享统一嵌入空间。

Result: 在多种模型尺寸下均展现良好的精度-效率权衡和一致缩放行为，兼容Ultralytics生态，支持大规模检测与定位数据集的多任务训练。

Conclusion: YOLOE-26为动态真实场景下的实时开放词汇实例分割提供了实用、可扩展的解决方案。

Abstract: This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.

</details>


### [31] [Intra-Class Subdivision for Pixel Contrastive Learning: Application to Semi-supervised Cardiac Image Segmentation](https://arxiv.org/abs/2602.00174)
*Jiajun Zhao,Xuan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种用于心脏图像分割的类内细分像素对比学习（SPCL）框架，通过引入“无关样本”概念和边界对比损失，有效缓解边界区域的表征污染问题，提升了分割精度与边界准确性。


<details>
  <summary>Details</summary>
Motivation: 解决心脏图像分割中边界区域存在的表征污染问题，提升类内不同区域（如内部与边界）的表征区分能力。

Method: 提出类内细分像素对比学习（SPCL）框架，定义‘无关样本’以区分同类中内部与边界像素表征，并设计边界对比损失增强边界表征判别力。

Result: 在公开心脏数据集上实验表明，SPCL显著提升分割质量与边界精度，优于现有方法。

Conclusion: SPCL通过建模类内细粒度差异，特别是边界区域的表征优化，为医学图像分割提供了更鲁棒、精准的特征学习范式。

Abstract: We propose an intra-class subdivision pixel contrastive learning (SPCL) framework for cardiac image segmentation to address representation contamination at boundaries. The novel concept ``Unconcerned sample'' is proposed to distinguish pixel representations at the inner and boundary regions within the same class, facilitating a clearer characterization of intra-class variations. A novel boundary contrastive loss for boundary representations is proposed to enhance representation discrimination across boundaries. The advantages of the unconcerned sample and boundary contrastive loss are analyzed theoretically. Experimental results in public cardiac datasets demonstrate that SPCL significantly improves segmentation performance, outperforming existing methods with respect to segmentation quality and boundary precision. Our code is available at https://github.com/Jrstud203/SPCL.

</details>


### [32] [Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation](https://arxiv.org/abs/2602.00176)
*Feng Tian,Yixuan Li,Weili Zeng,Weitian Zhang,Yichao Yan,Xiaokang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种噪声-频率连续性框架，通过在不同噪声水平下仅在对应频带内施加测量一致性约束，改进扩散后验采样方法，从而提升逆问题求解中细节恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散后验采样方法因测量项与扩散噪声水平弱耦合，导致高噪声下梯度不准确、几何失配、早期漂移、高频伪影及对采样调度和病态算子敏感等问题。

Method: 提出噪声-频率连续性框架，构建噪声依赖的中间后验分布，其似然项仅在噪声相关的频带内强制测量一致性；设计稳定后验采样器，融合扩散预测器、带限似然引导和多分辨率一致性策略。

Result: 在超分辨率、修复和去模糊任务上达到SOTA性能，运动模糊去除PSNR较强基线提升最高达5 dB。

Conclusion: 噪声-频率连续性框架有效缓解了传统扩散后验采样在细节恢复上的缺陷，提升了鲁棒性与重建质量。

Abstract: Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.

</details>


### [33] [CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning](https://arxiv.org/abs/2602.00181)
*Hang Wu,Yujun Cai,Zehao Li,Haonan Ge,Bowen Sun,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: 本文提出CamReasoner框架，将相机运动理解重构为基于 Observation-Thinking-Answer（O-T-A）范式的结构化推理过程，利用大规模推理轨迹数据集和首次引入的强化学习（RL）方法实现几何感知的运动推理，显著抑制幻觉，在多个基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型将相机运动理解视为黑箱分类，依赖表层视觉模式而忽略几何线索，导致对物理上不同的运动产生混淆。

Method: 提出O-T-A推理范式，构建包含18k SFT推理链和38k RL反馈样本的大规模推理轨迹套件，并首次在该任务中应用强化学习进行逻辑对齐，使推理基于物理几何而非上下文猜测。

Result: CamReasoner有效抑制幻觉，在多个基准测试中达到最先进（state-of-the-art）性能。

Conclusion: 将结构化推理与强化学习结合可显著提升相机运动理解的几何准确性和鲁棒性，为视频空间智能提供更可靠的感知-推理桥梁。

Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.

</details>


### [34] [AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange](https://arxiv.org/abs/2602.00192)
*Elif Nebioglu,Emirhan Bilgiç,Adrian Popescu*

Main category: cs.CV

TL;DR: 本文发现当前图像修复检测器主要依赖全局伪影而非局部合成内容，并提出Inpainting Exchange (INP-X)操作来隔离VAE重建引起的全图频谱偏移效应；实验表明现有检测器在INP-X干预下性能急剧下降，揭示其缺乏对局部合成内容的感知能力，进而强调需发展内容感知型检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的图像修复检测器依赖全局伪影而非局部合成内容，导致泛化性和鲁棒性不足，亟需理解其失效机理并推动内容感知检测的发展。

Method: 提出Inpainting Exchange (INP-X)操作，即在保留修复区域内容的同时恢复非编辑区域原始像素，以隔离VAE重建引发的全局频谱偏移；构建含90K样本的测试数据集（含真实、修复、交换图像）；结合理论分析（VAE信息瓶颈导致高频衰减）与实证评估（商用及SOTA检测器在INP-X下的性能崩塌）。

Result: INP-X使预训练SOTA及商用检测器准确率大幅下降（如从91%降至55%），接近随机水平；理论分析证实该现象源于VAE的信息瓶颈所致高频衰减；基于新数据集训练的检测器展现出更优泛化性与定位能力。

Conclusion: 当前主流检测器本质是‘全局伪影探测器’而非‘内容真实性验证器’；应转向内容感知、局部敏感的检测范式；INP-X为评估和提升检测鲁棒性提供了新基准与工具。

Abstract: Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\% to 55\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.

</details>


### [35] [Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images](https://arxiv.org/abs/2602.00202)
*Shanwen Wang,Xin Sun,Danfeng Hong,Fei Zhou*

Main category: cs.CV

TL;DR: 本文提出SemiEarth模型，利用视觉语言模型（VLM）设计VLM-PP模块净化教师网络生成的伪标签，显著提升遥感图像半监督语义分割性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统半监督语义分割方法在遥感领域面临低质量伪标签问题，尤其在教师-学生框架中边界区域和低置信度预测误差严重。

Method: 提出基于视觉语言模型的伪标签净化结构（VLM-PP），利用VLM的开放世界能力独立于S4架构地修正教师伪标签，尤其改善多类边界区域的伪标签质量。

Result: 在多个遥感数据集上达到SOTA性能，并具备良好可解释性，代码已开源。

Conclusion: VLM-PP是一种通用、解耦且有效的伪标签优化机制，显著提升了遥感图像半监督语义分割的精度与可靠性。

Abstract: The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vision-language models (VLMs) to address the S4 issues for the remote sensing (RS) domain. Specifically, we invent a VLM pseudo-label purifying (VLM-PP) structure to purify the teacher network's pseudo-labels, achieving substantial improvements. Especially in multi-class boundary regions of RS images, the VLM-PP module can significantly improve the quality of pseudo-labels generated by the teacher, thereby correctly guiding the student model's learning. Moreover, since VLM-PP equips VLMs with open-world capabilities and is independent of the S4 architecture, it can correct mispredicted categories in low-confidence pseudo-labels whenever a discrepancy arises between its prediction and the pseudo-label. We conducted extensive experiments on multiple RS datasets, which demonstrate that our SemiEarth achieves SOTA performance. More importantly, unlike previous SOTA RS S4 methods, our model not only achieves excellent performance but also offers good interpretability. The code is released at https://github.com/wangshanwen001/SemiEarth.

</details>


### [36] [Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning](https://arxiv.org/abs/2602.00211)
*Zafar Iqbal,Anwar Ul Haq,Srimannarayana Grandhi*

Main category: cs.CV

TL;DR: 本文提出了一种名为多跳视觉链式推理（VCoR）的无监督可变形图像配准框架，通过局部空间精化与跨参考注意力机制实现渐进式、可解释的配准过程，并在肺部CT和脑部MRI数据集上验证了其准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督深度学习配准方法虽精度高，但缺乏透明性，易导致误差漂移、临床信任度低，亟需兼顾准确性与可解释性的新方法。

Method: 提出多跳视觉链式推理（VCoR）框架，每跳包含局部空间精化（LSR）模块增强特征表示，以及跨参考注意力（CRA）机制引导迭代优化；通过多跳迭代生成带理论保证的中间预测序列，并基于形变场在各跳间的稳定性与收敛性估计不确定性。

Result: 在DIR-Lab 4D CT（肺）和IXI MRI（脑）两个公开数据集上达到具有竞争力的配准精度，同时提供丰富的中间可视化结果和置信度度量。

Conclusion: VCoR将隐式视觉推理范式嵌入配准流程，在保持高性能的同时显著提升可解释性、鲁棒性与临床适用性，为无监督医学图像配准提供了新范式。

Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.

</details>


### [37] [Deep Learning Based CNN Model for Automated Detection of Pneumonia from Chest XRay Images](https://arxiv.org/abs/2602.00212)
*Sathish Krishna Anumula,Vetrivelan Tamilmani,Aniruddha Arjun Singh,Dinesh Rajendran,Venkata Deepak Namburi*

Main category: cs.CV

TL;DR: 本文提出了一种基于定制化深度可分离卷积神经网络的自动化肺炎诊断模型，结合CLAHE和几何增强预处理，在5863张胸片上实现了高精度、低计算成本的肺炎识别。


<details>
  <summary>Details</summary>
Motivation: 传统依赖放射科医生手动解读胸片的方法受限于观察者差异、专家疲劳及专业人员短缺，尤其在资源匮乏地区问题突出，亟需快速精准的自动诊断方案。

Method: 设计定制化的轻量级CNN架构，采用深度可分离卷积以适配灰度医学图像纹理特征；引入CLAHE对比度增强与几何数据增强缓解类别不平衡并提升泛化能力。

Result: 在5863张前后位胸片数据集上验证，模型实现了高诊断精度和较低计算开销，优于冗余参数的通用迁移学习模型。

Conclusion: 该统一自动化诊断模型为资源受限环境下的肺炎快速筛查提供了高效、鲁棒且可部署的解决方案。

Abstract: Pneumonia has been one of the major causes of morbidities and mortality in the world and the prevalence of this disease is disproportionately high among the pediatric and elderly populations especially in resources trained areas Fast and precise diagnosis is a prerequisite for successful clinical intervention but due to inter observer variation fatigue among experts and a shortage of qualified radiologists traditional approaches that rely on manual interpretation of chest radiographs are frequently constrained To address these problems this paper introduces a unified automated diagnostic model using a custom Convolutional Neural Network CNN that can recognize pneumonia in chest Xray images with high precision and at minimal computational expense In contrast like other generic transfer learning based models which often possess redundant parameters the offered architecture uses a tailor made depth wise separable convolutional design which is optimized towards textural characteristics of grayscale medical images Contrast Limited Adaptive Histogram Equalization CLAHE and geometric augmentation are two significant preprocessing techniques used to ensure that the system does not experience class imbalance and is more likely to generalize The system is tested using a dataset of 5863 anterior posterior chest Xrays.

</details>


### [38] [Deep learning enables urban change profiling through alignment of historical maps](https://arxiv.org/abs/2602.02154)
*Sidi Wu,Yizi Chen,Maurizio Gribaudi,Konrad Schindler,Clément Mallet,Julien Perret,Lorenz Hurni*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的全自动框架，用于从大量历史地图中进行细粒度城市变迁分析，通过密集地图配准、多时相目标检测和变化剖面分析，实现了对巴黎1868–1937年间城市演变的系统性、量化刻画。


<details>
  <summary>Details</summary>
Motivation: 历史地图是研究长期城市演变的重要资料，但因空间错位、制图差异和文档质量退化等问题，难以从中提取一致且细粒度的变迁信息，导致现有分析多局限于小尺度或定性层面。

Method: 构建了一个模块化深度学习框架，整合了密集地图对齐、多时相目标检测和变化剖面建模三个核心组件，实现端到端的历史地图自动分析。

Result: 实验验证了所提配准与检测方法的鲁棒性；应用于巴黎1868–1937年地图序列，揭示了城市变迁在空间与时间上的异质性。

Conclusion: 该框架推动历史地图分析从经验性视觉比对转向系统性、可量化的城市变迁研究，具备跨制图语境适配能力，可支撑人文与社会科学的实证研究。

Abstract: Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.

</details>


### [39] [A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification](https://arxiv.org/abs/2602.00214)
*Juan A. Olmos,Antoine Manzanera,Fabio Martínez*

Main category: cs.CV

TL;DR: 本文提出了一种基于几何深度学习的多模态基础模型MFM-Geom，融合双参数MRI和临床报告，利用SPD矩阵与黎曼流形学习提升前列腺癌诊断性能，在小样本和外部数据上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有PCa诊断依赖主观专家解读；多数CAD方法仅用影像、忽略临床背景，且受限于数据稀缺，难以学习鲁棒表征。

Method: 提出几何多模态基础模型MFM-Geom，联合bp-MRI与临床文本；在分类头中采用对称正定（SPD）矩阵与黎曼深度学习，融合影像-文本表征。

Result: 仅用10%训练数据，AUC-PR达90.67%，较基线提升8.3%；在外部队列中AUC-PR为90.6，验证了泛化能力。

Conclusion: MFM-Geom通过几何建模有效融合多源异构临床数据，在小样本与跨中心场景下展现出强鲁棒性与实用性，为前列腺癌智能诊断提供了新范式。

Abstract: Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.

</details>


### [40] [Development of a Cacao Disease Identification and Management App Using Deep Learning](https://arxiv.org/abs/2602.00216)
*Zaldy Pagaduan,Jason Occidental,Nathaniel Duro,Dexielito Badilles,Eleonor Palconit*

Main category: cs.CV

TL;DR: 本研究开发了一款离线可用的移动应用，利用深度学习模型识别可可病害，帮助菲律宾小农户提升作物健康与生产力。


<details>
  <summary>Details</summary>
Motivation: 小农户缺乏数据、信息和优质农业实践知识，且常受病虫害困扰，而大型种植园资源更丰富；菲律宾可可农民尤其面临技术获取困难的问题。

Method: 构建一个基于深度学习的可可病害识别模型，并将其集成到离线运行的移动应用程序中，支持田间实时诊断；模型分别针对病害分类和黑荚病感染程度检测进行训练。

Result: 病害识别模型验证准确率达96.93%，黑荚病感染程度检测模型达79.49%；实地测试中与专家评估的一致率达84.2%。

Conclusion: 该离线移动应用为资源有限的小农户提供了实用、可及的技术工具，有助于提升可可种植的健康管理与生产效率。

Abstract: Smallholder cacao producers often rely on outdated farming techniques and face significant challenges from pests and diseases, unlike larger plantations with more resources and expertise. In the Philippines, cacao farmers have limited access to data, information, and good agricultural practices. This study addresses these issues by developing a mobile application for cacao disease identification and management that functions offline, enabling use in remote areas where farms are mostly located. The core of the system is a deep learning model trained to identify cacao diseases accurately. The trained model is integrated into the mobile app to support farmers in field diagnosis. The disease identification model achieved a validation accuracy of 96.93% while the model for detecting cacao black pod infection levels achieved 79.49% validation accuracy. Field testing of the application showed an agreement rate of 84.2% compared with expert cacao technician assessments. This approach empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity.

</details>


### [41] [CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models](https://arxiv.org/abs/2602.00247)
*Samyak Jha,Junho Kim*

Main category: cs.CV

TL;DR: 本文提出CAPA框架，通过注意力贡献度评估视觉token重要性，并结合FFN线性近似，在保持性能的同时提升大视觉语言模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型推理效率受限于大量视觉token处理开销，而现有基于注意力分数的重要性估计不准确，亟需更可靠的token选择准则和计算优化方法。

Method: 提出Attention Contribution指标（注意力概率加权值向量模长）以更准确衡量视觉token重要性；识别视觉注意力汇中的两类功能单元（可剪枝的Probability Dumps与关键的Structural Anchors）；发现FFN中视觉token的冗余性，尤其在中间层呈线性行为；据此设计CAPA框架，包含基于贡献度的关键层token剪枝与FFN线性近似两部分。

Result: CAPA在多个基准上实现了有竞争力的效率-性能权衡，并提升了模型鲁棒性。

Conclusion: Attention Contribution是比原始注意力分数更优的视觉token重要性指标；视觉token存在显著冗余，可通过贡献感知剪枝与FFN近似协同优化推理效率。

Abstract: Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.

</details>


### [42] [SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis](https://arxiv.org/abs/2602.00249)
*Rishav Pramanik,Ian E. Nielsen,Jeff Smith,Saurav Pandit,Ravi P. Ramachandran,Zhaozheng Yin*

Main category: cs.CV

TL;DR: 本文提出了SANEval，一个用于评估文本到图像生成模型在空间关系、属性绑定和数量理解等组合性任务上表现的新型开放词汇基准。它结合大语言模型（LLM）与增强型开放词汇目标检测器，实现更贴近人工评估的自动化诊断。


<details>
  <summary>Details</summary>
Motivation: 现有T2I评估基准受限于闭合词表、缺乏细粒度诊断能力，难以准确识别和定位组合性生成失败（如多对象、属性、空间关系建模错误）。

Method: 提出SANEval：利用LLM深度解析提示词语义，并驱动LLM增强的开放词汇目标检测器进行图像内容验证；构建覆盖空间、属性、数量三类组合任务的大规模评测数据集与可扩展评估流程。

Result: 在6个SOTA T2I模型上的实验表明，SANEval与人工评估的Spearman秩相关性显著高于现有基准，在属性绑定、空间关系和数量理解任务中均取得统计显著更优结果。

Conclusion: SANEval为T2I模型的组合性生成能力提供了更可靠、可解释、开放词汇的自动化评估范式，推动了可诊断、可改进的生成模型发展。

Abstract: The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.

</details>


### [43] [Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning](https://arxiv.org/abs/2602.00262)
*Huanran Li,Daniel Pimentel-Alarcón*

Main category: cs.CV

TL;DR: 本文提出了一种面向不完整数据的对比自监督子空间聚类框架（CSC），通过生成掩码视图并采用SimCLR式对比损失训练神经网络，再结合稀疏子空间聚类实现鲁棒、可扩展的聚类。


<details>
  <summary>Details</summary>
Motivation: 现有子空间聚类方法大多假设数据完全可观测，难以应对现实场景中普遍存在的缺失值问题。

Method: 提出对比自监督框架CSC：对部分观测输入生成掩码视图，用SimCLR风格的对比损失训练深度神经网络以学习不变嵌入，再将嵌入输入稀疏子空间聚类。

Result: 在六个基准数据集上的实验表明，CSC持续优于经典和深度学习基线方法，展现出对缺失数据的强鲁棒性和对大规模数据的良好可扩展性。

Conclusion: CSC为不完整数据的子空间聚类提供了一种有效、鲁棒且可扩展的新范式。

Abstract: Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scenarios with missing entries. In this paper, we propose a contrastive self-supervised framework, Contrastive Subspace Clustering (CSC), designed for clustering incomplete data. CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are then clustered using sparse subspace clustering. Experiments on six benchmark datasets show that CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.

</details>


### [44] [World-Shaper: A Unified Framework for 360° Panoramic Editing](https://arxiv.org/abs/2602.00265)
*Dong Liang,Yuhao Liu,Jinyuan Jia,Youjun Zhao,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 本文提出World-Shaper，一种在等距柱状投影（ERP）域中直接进行全景图像编辑的几何感知统一框架，通过生成-编辑范式和几何感知学习策略，显著提升360°图像编辑的几何一致性、保真度与文本可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于透视的图像编辑方法无法建模全景图的空间结构；传统立方体贴图分解因与球面几何不匹配而破坏全局一致性。

Method: 1）在ERP域直接建模全景编辑；2）采用生成-然后-编辑范式以缓解配对数据稀缺；3）引入显式的位姿感知形状监督与隐式的渐进式全景先验学习以应对几何失真。

Result: 在新基准PEBench上，World-Shaper在几何一致性、编辑保真度和文本可控性方面均超越SOTA方法。

Conclusion: World-Shaper实现了统一、连贯且灵活的360°视觉世界编辑控制，为全景内容创作提供了新范式。

Abstract: Being able to edit panoramic images is crucial for creating realistic 360° visual experiences. However, existing perspective-based image editing methods fail to model the spatial structure of panoramas. Conventional cube-map decompositions attempt to overcome this problem but inevitably break global consistency due to their mismatch with spherical geometry. Motivated by this insight, we reformulate panoramic editing directly in the equirectangular projection (ERP) domain and present World-Shaper, a unified geometry-aware framework that bridges panoramic generation and editing within a single editing-centric design. To overcome the scarcity of paired data, we adopt a generate-then-edit paradigm, where controllable panoramic generation serves as an auxiliary stage to synthesize diverse paired examples for supervised editing learning. To address geometric distortion, we introduce a geometry-aware learning strategy that explicitly enforces position-aware shape supervision and implicitly internalizes panoramic priors through progressive training. Extensive experiments on our new benchmark, PEBench, demonstrate that our method achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360° visual world creation with unified editing control. Code, model, and data will be released at our project page: https://world-shaper-project.github.io/

</details>


### [45] [PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories](https://arxiv.org/abs/2602.00267)
*Gemma Canet Tarrés,Manel Baradad,Francesc Moreno-Noguer,Yumeng Li*

Main category: cs.CV

TL;DR: PLACID 是一种基于视频扩散模型的新框架，通过利用时序先验和合成运动序列数据，实现高质量、可控的多物体图像合成，在身份保持、背景/色彩保真度和布局控制方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI在单图生成上效果好，但在专业级多物体合成（需同时满足身份保持、背景/色彩保真、布局可控、整体美观）任务中表现不足，常出现物体失真、遗漏、重复或比例失调等问题。

Method: 提出PLACID框架：1）利用预训练文本控制的图像到视频（I2V）扩散模型，借助其时序先验提升物体一致性与背景细节；2）设计新型合成数据策略——生成物体从随机位置平滑移动至目标位置的视频序列，以对齐视频模型的时序建模能力；推理时通过文本引导使随机初始化物体收敛至协调布局，取最终帧为合成结果。

Result: 在定量评估和用户研究中，PLACID在多物体合成任务上全面超越SOTA方法，显著提升物体身份、背景与色彩保真度，减少遗漏物体，并生成更美观、协调的合成图像。

Conclusion: PLACID验证了利用视频扩散模型的时序先验可有效解决多物体图像合成中的关键挑战，为可控、高保真视觉合成提供了新范式。

Abstract: Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.

</details>


### [46] [TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.00268)
*Ariel Shaulov,Eitan Shaar,Amit Edenzon,Lior Wolf*

Main category: cs.CV

TL;DR: 本文提出了一种在推理阶段识别并移除不稳定的潜在标记（unstable latent tokens）的方法，以缓解自回归视频生成中的时间漂移问题，从而提升长时序视频生成的时间一致性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频生成在长视频合成中存在严重的时间漂移问题，即误差随生成帧数增加而累积放大；作者认为该问题主因并非模型容量不足，而是推理过程中被污染的潜在标记被重复用作条件所导致的误差传播。

Method: 定义‘不稳定标记’为与前一批生成帧潜在表示显著偏离的标记，并在自回归推理过程中显式剔除这些标记，避免其影响后续生成，不修改模型结构、训练过程或潜在空间。

Result: 该方法显著提升了长时序视频生成的时间一致性，在不改变模型架构和训练方式的前提下有效缓解了时间漂移。

Conclusion: 通过在推理阶段对潜在标记进行稳定性筛选与净化，可高效抑制自回归视频生成中的误差传播，验证了时间漂移主要源于推理时的条件污染而非建模能力局限。

Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.

</details>


### [47] [TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs](https://arxiv.org/abs/2602.00288)
*Baiqi Li,Kangyi Zhao,Ce Zhang,Chancharik Mitra,Jean de Dieu Nyandwi,Gedas Bertasius*

Main category: cs.CV

TL;DR: TimeBlind 是一个用于评估多模态大语言模型（MLLMs）细粒度时空理解能力的诊断性基准，通过最小配对范式分离静态内容与时间结构，发现当前最先进模型在时间推理上严重依赖静态视觉捷径，性能远低于人类。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs虽擅长静态语义理解，但在时间动态建模上表现脆弱；缺乏能精准诊断细粒度时空推理能力的基准。

Method: 提出TimeBlind基准，基于认知科学将时间理解分为三层次（原子事件识别、事件属性刻画、事件依赖推理），采用最小配对视频对（静态内容相同、仅时间结构不同）和互补问题设计以消除语言先验偏差。

Result: 在600个实例（2400视频-问题对）上评估20个前沿MLLMs，最佳模型实例准确率仅48.2%，显著低于人类98.2%。

Conclusion: 当前MLLMs尚未掌握真正的时序逻辑，主要依赖静态视觉线索；TimeBlind为下一代视频理解模型提供了关键诊断工具。

Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .

</details>


### [48] [Computer Vision and Its Relationship to Cognitive Science: A perspective from Bayes Decision Theory](https://arxiv.org/abs/2602.00289)
*Alan Yuille,Daniel Kersten*

Main category: cs.CV

TL;DR: 本文从贝叶斯决策理论（BDT）视角介绍计算机视觉及其与认知科学的关系，统一分析了贝叶斯方法与深度神经网络两种主流范式，并指出BDT的局限性及二者融合的可能路径。


<details>
  <summary>Details</summary>
Motivation: 为弥合计算机视觉中贝叶斯方法与深度学习方法之间的理论鸿沟，并建立与认知科学相容的统一理论框架。

Method: 以贝叶斯决策理论（BDT）为统一理论透镜，系统比较和关联贝叶斯视觉建模与深度神经网络（受视觉腹侧通路启发）两种方法。

Result: 揭示了BDT既能涵盖两种方法的核心优势，也暴露其固有局限；进而指明融合二者、构建更丰富计算框架的方向。

Conclusion: BDT提供了一个有力的理论桥梁，有助于整合概率推理与深度学习，并推动更具认知合理性的视觉计算模型发展。

Abstract: This document presents an introduction to computer vision, and its relationship to Cognitive Science, from the perspective of Bayes Decision Theory (Berger 1985). Computer vision is a vast and complex field, so this overview has a narrow scope and provides a theoretical lens which captures many key concepts. BDT is rich enough to include two different approaches: (i) the Bayesian viewpoint, which gives a conceptually attractive framework for vision with concepts that resonate with Cognitive Science (Griffiths et al., 2024), and (ii) the Deep Neural Network approach whose successes in the real world have made Computer Vision into a trillion-dollar industry and which is motivated by the hierarchical structure of the visual ventral stream. The BDT framework relates and captures the strengths and weakness of these two approaches and, by discussing the limitations of BDT, points the way to how they can be combined in a richer framework.

</details>


### [49] [LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification](https://arxiv.org/abs/2602.00292)
*Rory Driscoll,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CV

TL;DR: 本文提出了LogicGaze基准框架，用于评估视觉语言模型（VLMs）在视频和图像中验证因果推理链的能力，重点检测其对视觉证据的依赖程度及幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在顺序推理能力上虽有提升，但其推理链是否真正基于视觉证据尚不明确，尤其存在严重幻觉问题，亟需专门基准进行检验。

Method: 构建LogicGaze基准：基于40,000个ShareGPT4Video视频片段与Flickr30k子集，引入视觉矛盾但语言合理的因果扰动；设计三阶段评估协议——因果验证、具身叙事生成、扰动拒绝。

Result: 实验揭示Qwen2.5-VL-72B等先进VLM在因果验证与扰动识别方面存在显著缺陷，暴露其多模态推理的不可靠性。

Conclusion: LogicGaze为推动可信赖、鲁棒的多模态因果推理提供了新标准与开源资源，强调视觉接地验证的必要性。

Abstract: While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.

</details>


### [50] [Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation](https://arxiv.org/abs/2602.00309)
*Samuel Church,Joshua D. Warner,Danyal Maqbool,Xin Tie,Junjie Hu,Meghan G. Lubner,Tyler J. Bradshaw*

Main category: cs.CV

TL;DR: 本文提出SAM2CT模型，利用放射科医生在日常阅片中留下的稀疏GSPS标注（如箭头、线段），通过可提示分割范式生成高质量3D CT病灶分割，显著降低人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 高质量3D CT分割数据获取成本高，而临床PACS中大量存在放射科医生日常阅片时留下的稀疏GSPS标注（如箭头、线段），亟需一种方法将其有效转化为3D分割掩码。

Method: 提出Opportunistic Promptable Segmentation范式，并构建SAM2CT模型：基于SAM2改进，扩展提示编码器以支持箭头和线段输入，并引入专为3D医学图像设计的记忆条件化记忆（MCM）机制。

Result: 在公开病灶分割基准上，SAM2CT对箭头和线段提示的Dice系数分别达0.649和0.757；在60例真实PACS GSPS数据上，87%的生成分割被放射科医生评为临床可用或仅需微调；且在急诊科部分病种上展现强零样本性能。

Conclusion: 历史GSPS标注的大规模挖掘是一种可行、可扩展的3D CT分割数据集构建新路径，SAM2CT为此提供了首个有效技术方案。

Abstract: The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.

</details>


### [51] [On the Assessment of Sensitivity of Autonomous Vehicle Perception](https://arxiv.org/abs/2602.00314)
*Apostol Vassilev,Munawar Hasan,Edward Griffor,Honglan Jin,Pavel Piliptchak,Mahima Arora,Thoshitha Gamage*

Main category: cs.CV

TL;DR: 本文提出了一种基于模型集成的预测敏感性量化方法，评估自动驾驶车辆感知系统在恶劣天气和道路条件下的鲁棒性，发现雾、低太阳高度、遮挡及远距离目标会显著降低多种主流视觉模型（如YOLO、DETR、RT-DETR）的检测性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的可行性高度依赖感知系统的实时性、准确性与鲁棒性，尤其需应对自然（如雾、光照变化）和对抗性（如遮挡）驾驶干扰，因此亟需系统化评估其鲁棒性并提升可靠性。

Method: 采用五种前沿视觉模型（YOLO v8-v9、DETR50/101、RT-DETR）组成的集成，在仿真与真实场景下，通过预测敏感性量化（反映模型间分歧与推理变异性）评估感知性能；提出基于停车距离（考虑路面状态与车速）的感知评估准则，并构建了评估架构。

Result: 雾和低太阳高度对模型影响最大；遮挡与恶劣天气叠加时性能显著下降；目标距离越远，感知性能衰减越严重；不同模型在各类干扰下表现各异，整体鲁棒性随环境挑战加剧而降低。

Conclusion: 感知系统鲁棒性受多重现实因素耦合影响，单一模型难以保障可靠，需结合多模型敏感性分析与场景驱动的评估标准，为提升AV感知可靠性提供可量化的评估框架与改进方向。

Abstract: The viability of automated driving is heavily dependent on the performance of perception systems to provide real-time accurate and reliable information for robust decision-making and maneuvers. These systems must perform reliably not only under ideal conditions, but also when challenged by natural and adversarial driving factors. Both of these types of interference can lead to perception errors and delays in detection and classification. Hence, it is essential to assess the robustness of the perception systems of automated vehicles (AVs) and explore strategies for making perception more reliable. We approach this problem by evaluating perception performance using predictive sensitivity quantification based on an ensemble of models, capturing model disagreement and inference variability across multiple models, under adverse driving scenarios in both simulated environments and real-world conditions. A notional architecture for assessing perception performance is proposed. A perception assessment criterion is developed based on an AV's stopping distance at a stop sign on varying road surfaces, such as dry and wet asphalt, and vehicle speed. Five state-of-the-art computer vision models are used, including YOLO (v8-v9), DEtection TRansformer (DETR50, DETR101), Real-Time DEtection TRansformer (RT-DETR)in our experiments. Diminished lighting conditions, e.g., resulting from the presence of fog and low sun altitude, have the greatest impact on the performance of the perception models. Additionally, adversarial road conditions such as occlusions of roadway objects increase perception sensitivity and model performance drops when faced with a combination of adversarial road conditions and inclement weather conditions. Also, it is demonstrated that the greater the distance to a roadway object, the greater the impact on perception performance, hence diminished perception robustness.

</details>


### [52] [Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception](https://arxiv.org/abs/2602.00340)
*Alexandros Christoforos,Sarah Jenkins,Michael Brown,Tuan Pham,David Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为SynerNet的协同神经智能体网络框架，旨在缓解视觉-语言模型在面对分布外概念时的跨模态对齐退化问题。该框架通过四个专用计算单元协作校正模态差异，并在VISTA-Beyond基准上验证了其在少样本和零样本场景下的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言模型（VLMs）在遇到分布外（OOD）概念时出现的跨模态对齐退化问题。

Method: 提出Synergistic Neural Agents Network（SynerNet）框架，包含视觉感知、语言上下文、名词嵌入和全局协调四个专用计算单元，采用结构化消息传播协议；引入多智能体潜在空间命名获取框架、语义上下文交换算法及自适应动态平衡机制。

Result: 在VISTA-Beyond基准测试中，SynerNet在少样本和零样本场景下均显著提升性能，精度提高1.2%至5.4%。

Conclusion: SynerNet有效缓解了VLMs在OOD概念下的跨模态对齐退化问题，提升了模型泛化能力与鲁棒性。

Abstract: This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational units - visual perception, linguistic context, nominal embedding, and global coordination - collaboratively rectify modality disparities via a structured message-propagation protocol. The principal contributions encompass a multi-agent latent space nomenclature acquisition framework, a semantic context-interchange algorithm for enhanced few-shot adaptation, and an adaptive dynamic equilibrium mechanism. Empirical evaluations conducted on the VISTA-Beyond benchmark demonstrate that SynerNet yields substantial performance augmentations in both few-shot and zero-shot scenarios, exhibiting precision improvements ranging from 1.2% to 5.4% across a diverse array of domains.

</details>


### [53] [When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs](https://arxiv.org/abs/2602.00344)
*Beidi Zhao,Wenlong Deng,Xinting Liao,Yushu Li,Nazim Shaikh,Yao Nie,Xiaoxiao Li*

Main category: cs.CV

TL;DR: 本文提出MAD-RAG方法，解决检索增强生成（RAG）在视觉语言模型中因检索文本干扰图像注意力而导致的'注意力分散（AD）'新失败模式，无需训练即可提升知识型视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在知识型视觉问答中常因检索文本过度抑制图像注意力，导致模型忽视问题相关图像区域，即使检索内容充分也会失败——这一'注意力分散（AD）'现象被先前研究忽略。

Method: 提出无需训练的干预方法MAD-RAG：通过双问题建模解耦视觉定位与上下文融合，并结合注意力混合机制保留图像条件证据。

Result: 在OK-VQA、E-VQA和InfoSeek数据集上，MAD-RAG相较基线RAG取得最高达4.76%、9.20%和6.18%的绝对准确率提升，修复74.68%的失败案例，且计算开销极低。

Conclusion: 注意力分散是RAG在LVLM中一个关键但被忽视的失败原因；MAD-RAG以轻量、训练-free方式有效缓解该问题，显著提升多模型家族在知识型VQA任务上的鲁棒性与性能。

Abstract: While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.

</details>


### [54] [AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning](https://arxiv.org/abs/2602.00347)
*Chongyu Qu,Zhengyi Lu,Yuxiang Lai,Thomas Z. Li,Junchao Zhu,Junlin Guo,Juming Xiong,Yanfan Zhu,Yuechen Yang,Allen J. Luna,Kim L. Sandler,Bennett A. Landman,Yuankai Huo*

Main category: cs.CV

TL;DR: 本文提出AdaFuse框架，利用强化学习实现患者特异性的多模态选择与融合，用于肺癌风险预测，在NLST数据集上取得最优AUC（0.762）且计算更高效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法对所有模态一视同仁或仅加权，未解决‘对特定患者是否应使用某模态’这一根本问题。

Method: 将多模态融合建模为序列决策过程，通过强化学习策略网络动态决定是否引入新模态或提前终止预测。

Result: 在NLST数据集上AUC达0.762，优于单模态（0.732）、固定融合（0.759）及自适应基线（DynMM 0.754，MoE 0.742），且FLOPs更低。

Conclusion: 强化学习可实现个性化、自适应的多模态融合，推动医学诊断从统一融合向按需调用模态的智能流程演进。

Abstract: Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through the network, either treating them equally or learning to assign different contribution weights, leaving a fundamental question unaddressed: for a given patient, should certain modalities be used at all? We present AdaFuse, an adaptive multimodal fusion framework that leverages reinforcement learning (RL) to learn patient-specific modality selection and fusion strategies for lung cancer risk prediction. AdaFuse formulates multimodal fusion as a sequential decision process, where the policy network iteratively decides whether to incorporate an additional modality or proceed to prediction based on the information already acquired. This sequential formulation enables the model to condition each selection on previously observed modalities and terminate early when sufficient information is available, rather than committing to a fixed subset upfront. We evaluate AdaFuse on the National Lung Screening Trial (NLST) dataset. Experimental results demonstrate that AdaFuse achieves the highest AUC (0.762) compared to the best single-modality baseline (0.732), the best fixed fusion strategy (0.759), and adaptive baselines including DynMM (0.754) and MoE (0.742), while using fewer FLOPs than all triple-modality methods. Our work demonstrates the potential of reinforcement learning for personalized multimodal fusion in medical imaging, representing a shift from uniform fusion strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities and when existing information suffices for accurate prediction.

</details>


### [55] [MASC: Metal-Aware Sampling and Correction via Reinforcement Learning for Accelerated MRI](https://arxiv.org/abs/2602.00348)
*Zhengyi Lu,Ming Lu,Chongyu Qu,Junchao Zhu,Junlin Guo,Marilyn Lionts,Yanfan Zhu,Yuechen Yang,Tianyuan Yao,Jayasai Rajagopal,Bennett Allan Landman,Xiao Wang,Xinqiang Yan,Yuankai Huo*

Main category: cs.CV

TL;DR: 本文提出MASC，一种基于强化学习的统一框架，联合优化金属感知的k空间采样与伪影校正，以实现加速MRI成像；通过物理仿真构建配对数据集，并采用端到端训练使采样策略与去伪影网络协同优化。


<details>
  <summary>Details</summary>
Motivation: 金属植入物在MRI中引起严重伪影，影响诊断；传统方法将金属伪影校正（MAR）与加速采集分开处理，缺乏联合优化。

Method: 提出MASC框架：利用物理仿真构建含/不含金属的配对3D MRI数据集；将k空间采样建模为序列决策问题，使用artifact-aware PPO代理选择相位编码线；采样与U-Net-based MAR网络联合端到端训练。

Result: MASC学习的采样策略优于传统策略；端到端训练比固定预训练MAR网络效果更好；在FastMRI上经物理伪影模拟验证具备临床泛化能力。

Conclusion: 联合优化k空间采样与金属伪影校正是可行且有效的，MASC为加速MRI提供了一种新范式，并开源代码与模型。

Abstract: Metal implants in MRI cause severe artifacts that degrade image quality and hinder clinical diagnosis. Traditional approaches address metal artifact reduction (MAR) and accelerated MRI acquisition as separate problems. We propose MASC, a unified reinforcement learning framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI. To enable supervised training, we construct a paired MRI dataset using physics-based simulation, generating k-space data and reconstructions for phantoms with and without metal implants. This paired dataset provides simulated 3D MRI scans with and without metal implants, where each metal-corrupted sample has an exactly matched clean reference, enabling direct supervision for both artifact reduction and acquisition policy learning. We formulate active MRI acquisition as a sequential decision-making problem, where an artifact-aware Proximal Policy Optimization (PPO) agent learns to select k-space phase-encoding lines under a limited acquisition budget. The agent operates on undersampled reconstructions processed through a U-Net-based MAR network, learning patterns that maximize reconstruction quality. We further propose an end-to-end training scheme where the acquisition policy learns to select k-space lines that best support artifact removal while the MAR network simultaneously adapts to the resulting undersampling patterns. Experiments demonstrate that MASC's learned policies outperform conventional sampling strategies, and end-to-end training improves performance compared to using a frozen pre-trained MAR network, validating the benefit of joint optimization. Cross-dataset experiments on FastMRI with physics-based artifact simulation further confirm generalization to realistic clinical MRI data. The code and models of MASC have been made publicly available: https://github.com/hrlblab/masc

</details>


### [56] [ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models](https://arxiv.org/abs/2602.00350)
*Ignacy Kolton,Kacper Marzol,Paweł Batorski,Marcin Mazur,Paul Swoboda,Przemysław Spurek*

Main category: cs.CV

TL;DR: 本文提出ReLAPSe，一种基于强化学习的对抗框架，用于高效恢复文本到图像扩散模型中被遗忘的概念，利用模型内在的噪声预测损失作为可验证反馈信号，实现对多种主流遗忘方法的细粒度身份与风格恢复。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法存在视觉信息残留问题，而现有对抗攻击方法受限于计算开销大或缺乏对目标模型潜在表征的直接反馈。

Method: 提出ReLAPSe框架，将概念恢复建模为强化学习问题，采用强化学习与可验证奖励（RLVR）训练策略性智能体，以扩散模型的噪声预测损失为内在、可验证的反馈信号，实现闭环提示优化。

Result: ReLAPSe能高效、近实时地跨多种先进遗忘方法恢复细粒度身份和风格，在红队测试中展现出强泛化性与可扩展性。

Conclusion: ReLAPSe通过从逐样本优化转向全局策略学习，显著提升了对抗性概念恢复的效率与实用性，为评估扩散模型遗忘效果提供了新范式。

Abstract: Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe

</details>


### [57] [Modeling Image-Caption Rating from Comparative Judgments](https://arxiv.org/abs/2602.00381)
*Kezia Minni,Qiang Zhang,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于比较学习的图像-文本匹配评估框架，通过建模人类对图像描述对的相对偏好（而非绝对评分），在保持性能的同时显著降低标注成本和主观性。


<details>
  <summary>Details</summary>
Motivation: 人工直接对图像描述进行准确度评分耗时且主观性强；相比之下，人类更擅长判断两个描述中哪个更匹配给定图像。

Method: 构建比较学习模型，利用VICR数据集提取ResNet-50视觉特征与MiniLM文本特征，并与回归模型对比；同时开展小规模人类评估，比较绝对评分、两两比较和同图多描述比较三种标注方式。

Result: 回归模型略优（Pearson ρ=0.7609，Spearman r_s=0.7089），但比较模型随数据增加持续提升并逼近回归基线；人类评估显示比较式标注更快、一致性更高。

Conclusion: 比较学习能有效建模人类偏好，在显著降低标注成本和主观性的同时，达到接近直接回归建模的性能。

Abstract: Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $ρ$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.

</details>


### [58] [Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects](https://arxiv.org/abs/2602.00385)
*Bsher Karbouj,Adam Michael Altenbuchner,Joerg Krueger*

Main category: cs.CV

TL;DR: 本文对比分析了YOLOv5和Faster R-CNN在自动驾驶目标检测任务中的性能，发现YOLOv5在mAP、召回率和训练效率上更优，而Faster R-CNN在小目标、远距离及低光照场景下表现更好。


<details>
  <summary>Details</summary>
Motivation: 缺乏针对自动驾驶特定应用场景的深度学习目标检测方法选择指导，需系统评估不同模型在真实驾驶环境中的适用性。

Method: 对YOLOv5（单阶段）和Faster R-CNN（双阶段）进行实验对比，使用融合真实与合成图像的多样化数据集，评估指标包括mAP、召回率和推理速度，并分析不同置信度阈值及实际场景下的表现。

Result: YOLOv5在mAP、召回率和训练效率方面整体更优，尤其随数据量和图像分辨率增加优势明显；Faster R-CNN在小目标、远距离物体检测及挑战性光照条件下更具鲁棒性。

Conclusion: 模型选择应依据具体自动驾驶需求权衡精度、速度与鲁棒性：YOLOv5适合实时性要求高的主检测任务，Faster R-CNN可作为补充用于关键边缘场景。

Abstract: Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models' behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.

</details>


### [59] [Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data](https://arxiv.org/abs/2602.00391)
*Alberto Mario Ceballos-Arroyo,Shrikanth M. Yadav,Chu-Hsuan Lin,Jisoo Kim,Geoffrey S. Young,Huaizu Jiang,Lei Qin*

Main category: cs.CV

TL;DR: 本文提出了一种基于动态4D-CTA扫描的脑血管自动标注新方法，通过多时相减影增强血管可视化，并利用时相冗余扩充数据集，训练出性能更优的nnUNet模型，在多个指标上显著优于现有同类数据集。


<details>
  <summary>Details</summary>
Motivation: 减少脑血管手动标注工作量，提升深度学习模型在不同对比度时相对血管分割的鲁棒性。

Method: 利用动态4D-CTA多时间点进行骨与软组织减影以增强动静脉显示；将同一病例的多个时相共享同一真值标注，实现数据集4–5倍扩充；使用nnUNet进行血管分割训练。

Result: 在TopBrain数据集上动脉平均Dice系数达0.846、静脉达0.957；动脉adHD为0.304 mm、静脉为0.078 mm；动脉tSens为0.877、静脉为0.974。

Conclusion: 所构建的数据集与训练策略显著提升了脑血管分割精度与形态保真度，代码与模型已开源。

Abstract: In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation

</details>


### [60] [Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset](https://arxiv.org/abs/2602.00393)
*Gabriel Bromonschenkel,Alessandro L. Koerich,Thiago M. Paixão,Hilário Tomaz Alves de Oliveira*

Main category: cs.CV

TL;DR: 本文提出了一种针对巴西葡萄牙语图像描述生成的跨原生翻译评估方法，使用人工撰写的Flickr30K-PTBR数据集与自动翻译版本进行对比，并结合注意力可视化和CLIP-Score评估，发现Swin-DistilBERTimbau泛化性最佳，ViTucano在文本指标上优于大型多语言模型，而GPT-4系列在图像-文本对齐上表现最优。


<details>
  <summary>Details</summary>
Motivation: 解决巴西葡萄牙语等低资源语言在图像描述任务中缺乏高质量原生数据集和专用模型的问题，揭示自动翻译数据带来的偏差与局限。

Method: 构建并对比两个巴西葡萄牙语Flickr30K版本（人工撰写 vs 自动翻译），采用跨上下文训练/测试设置；使用Swin-DistilBERTimbau、ViTucano及GPT-4o、LLaMA-3.2-Vision等Transformer视觉语言模型；引入注意力图分析模型推理偏差，并用CLIP-Score评估图文对齐质量。

Result: Swin-DistilBERTimbau在跨数据集泛化性上最优；ViTucano在BLEU、CIDEr等文本指标上超越GPT-4o和LLaMA-3.2-Vision；GPT-4系列获得最高CLIP-Score；注意力分析暴露性别误判、物体计数错误和空间不一致等系统性偏差。

Conclusion: 原生人工标注数据对低资源语言图像描述至关重要；自动翻译虽缓解数据稀缺，但引入可识别的语义与视觉偏差；专用轻量级模型（如ViTucano）可在资源受限场景下媲美甚至超越大模型；多维度评估（文本+对齐+可解释性）是全面衡量模型性能的关键。

Abstract: Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.

</details>


### [61] [Modeling Art Evaluations from Comparative Judgments: A Deep Learning Approach to Predicting Aesthetic Preferences](https://arxiv.org/abs/2602.00394)
*Manoj Reddy Bethi,Sai Rupa Jhade,Pravallika Yaganti,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于成对偏好比较的深度学习框架，用于建模人类对视觉艺术的审美判断，以降低标注成本；通过ResNet-50提取图像特征，构建回归与双分支对比模型；实验表明对比学习在无直接评分时接近回归性能，且标注时间减少60%，但个体偏好预测仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 解决人类审美判断建模中个体偏好差异大、高质量标注数据获取成本高的问题。

Method: 采用基于Law of Comparative Judgment的成对偏好比较学习框架；使用ResNet-50提取画作图像的深度卷积特征；构建深度神经网络回归模型和双分支成对比较模型；设计四项研究问题并开展人因实验评估标注效率。

Result: 深度回归模型相较线性基线提升R²达328%；成对比较模型在无直接评分下逼近回归性能；个体偏好预测（组内/组间）效果显著弱于平均评分预测；人因实验显示成对判断耗时比直接评分低60%。

Conclusion: 成对比较学习是高效、实用的审美建模替代方案，尤其适用于大规模偏好标注场景，但个体化建模仍需进一步突破。

Abstract: Modeling human aesthetic judgments in visual art presents significant challenges due to individual preference variability and the high cost of obtaining labeled data. To reduce cost of acquiring such labels, we propose to apply a comparative learning framework based on pairwise preference assessments rather than direct ratings. This approach leverages the Law of Comparative Judgment, which posits that relative choices exhibit less cognitive burden and greater cognitive consistency than direct scoring. We extract deep convolutional features from painting images using ResNet-50 and develop both a deep neural network regression model and a dual-branch pairwise comparison model. We explored four research questions: (RQ1) How does the proposed deep neural network regression model with CNN features compare to the baseline linear regression model using hand-crafted features? (RQ2) How does pairwise comparative learning compare to regression-based prediction when lacking access to direct rating values? (RQ3) Can we predict individual rater preferences through within-rater and cross-rater analysis? (RQ4) What is the annotation cost trade-off between direct ratings and comparative judgments in terms of human time and effort? Our results show that the deep regression model substantially outperforms the baseline, achieving up to $328\%$ improvement in $R^2$. The comparative model approaches regression performance despite having no access to direct rating values, validating the practical utility of pairwise comparisons. However, predicting individual preferences remains challenging, with both within-rater and cross-rater performance significantly lower than average rating prediction. Human subject experiments reveal that comparative judgments require $60\%$ less annotation time per item, demonstrating superior annotation efficiency for large-scale preference modeling.

</details>


### [62] [3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting](https://arxiv.org/abs/2602.00395)
*Roger Hsiao,Yuchen Fang,Xiangru Huang,Ruilong Li,Hesam Rabeti,Zan Gojcic,Javad Lavaei,James Demmel,Sophia Shao*

Main category: cs.CV

TL;DR: 本文提出3DGS²-TR，一种用于加速3D高斯点绘（3DGS）场景训练的二阶优化器，通过Hutchinson方法近似Hessian对角线实现矩阵自由、O(n)复杂度，并引入基于Hellinger距离的逐参数信赖域策略以提升非线性优化稳定性，在更少迭代和更低内存开销下取得更优重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有二阶优化方法（如3DGS-LM、3DGS2）依赖显式或稠密曲率表示，计算与内存开销大，难以扩展至大规模场景；同时3DGS光栅化过程强非线性，需更稳定的优化机制。

Method: 采用Hutchinson方法高效估计Hessian矩阵对角线，实现矩阵自由的二阶优化；设计基于平方Hellinger距离的逐参数信赖域策略，约束高斯参数更新；整体复杂度与ADAM一致（O(n)）。

Result: 在相同初始化且不进行密度控制的前提下，相比ADAM减少50%训练迭代次数、重建质量更高；峰值GPU内存仅增加17%（<1GB），远低于3DGS-LM（降低85%）；支持大规模场景及潜在分布式训练。

Conclusion: 3DGS²-TR在保持低计算/内存复杂度的同时，显著提升了3DGS训练效率与稳定性，为实时、大规模神经渲染提供了实用二阶优化方案。

Abstract: We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (Höllein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.

</details>


### [63] [Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure](https://arxiv.org/abs/2602.00414)
*Trishna Chakraborty,Udita Ghosh,Aldair Ernesto Gongora,Ruben Glatt,Yue Dong,Jiachen Li,Amit K. Roy-Chowdhury,Chengyu Song*

Main category: cs.CV

TL;DR: 本文提出了一种将文本实验室场景转化为图像-场景图-真值三元组的合成数据生成流程，并发现视觉语言模型（VLMs）在仅靠视觉输入时难以准确提取结构化对象关系；为此，作者提出‘场景图引导对齐’的后训练上下文工程方法，提升VLM在纯视觉场景下的安全隐患检测性能。


<details>
  <summary>Details</summary>
Motivation: 实验室安全事故频发但缺乏真实视觉评估数据，现有VLMs在纯视觉安全监控任务中效果不明，亟需构建适配的视觉评估基准与提升方法。

Method: 构建基于大语言模型（场景图生成）和图像生成模型（渲染）的合成数据流水线；提出‘场景图引导对齐’的后训练上下文工程策略，将视觉输入映射为更契合VLM推理的结构化场景图。

Result: 在1207个样本、362种场景的合成数据集上验证：VLMs在文本场景图输入下表现良好，但在纯视觉输入下性能显著下降；所提方法有效弥合感知鸿沟，提升纯视觉下的隐患检测性能。

Conclusion: 结构化先验（如场景图）对VLM安全理解至关重要；通过上下文工程将视觉信号显式对齐至结构化表示，可显著增强其在真实实验室监控中的实用性。

Abstract: Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.

</details>


### [64] [Text is All You Need for Vision-Language Model Jailbreaking](https://arxiv.org/abs/2602.00420)
*Yihang Chen,Zhao Xu,Youyuan Jiang,Tianle Zheng,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本文提出Text-DJ攻击方法，通过将有害文本查询分解为多个良性子查询，并与大量无关干扰图像组成网格输入，利用LVLM的OCR能力绕过其文本/视觉安全机制。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM安全机制主要针对显式文本或相关视觉输入，忽视OCR路径带来的新攻击面，存在未被充分研究的安全漏洞。

Method: Text-DJ三阶段方法：1）将有害查询分解为语义相关但更良性的子查询；2）选取大量语义无关的干扰查询；3）将所有子查询和干扰查询以图像形式排布成网格，子查询置于中心位置，交由LVLM OCR识别并响应。

Result: Text-DJ在多个SOTA LVLM上成功绕过安全对齐机制，验证了OCR路径在多图像分散输入下的脆弱性。

Conclusion: LVLM的OCR模块对碎片化、多图像的对抗输入缺乏鲁棒性，亟需针对此类分散式多模态输入设计新型防御机制。

Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.

</details>


### [65] [DISK: Dynamic Inference SKipping for World Models](https://arxiv.org/abs/2602.00440)
*Anugunj Naman,Gaibo Zhang,Ayushman Singh,Yaguang Zhang*

Main category: cs.CV

TL;DR: DISK是一种无需训练的自适应推理方法，用于自回归世界模型，通过双分支控制器协调两个耦合的扩散Transformer，在保持运动-外观一致性的同时实现视频与自我轨迹预测的加速。


<details>
  <summary>Details</summary>
Motivation: 提升自回归世界模型在长时序视频与轨迹联合预测中的推理效率，同时维持预测质量与规划性能。

Method: 提出DISK方法，采用双分支控制器协调视频与自我轨迹的两个耦合扩散Transformer，引入跨模态跳跃决策和高阶潜在差分跳跃测试，并在自回归前向链中传播控制器统计量以保障长时序稳定性。

Result: 在NuPlan和NuScenes数据集上1500个样本的闭环驾驶推演中，DISK在NVIDIA L40S GPU上实现轨迹扩散2倍加速、视频扩散1.6倍加速，同时保持L2规划误差、视觉质量（FID/FVD）及NAVSIM PDMS分数不变。

Conclusion: DISK实现了无需重训练的高效自适应推理，在保证多模态预测质量与规划性能的前提下显著降低计算开销，适用于实际长时序自动驾驶仿真与预测任务。

Abstract: We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.

</details>


### [66] [Model Optimization for Multi-Camera 3D Detection and Tracking](https://arxiv.org/abs/2602.00450)
*Ethan Anderson,Justin Silva,Kyle Zheng,Sameer Pusegaonkar,Yizhou Wang,Zheng Tang,Sujit Biswas*

Main category: cs.CV

TL;DR: 本文评估了Sparse4D框架在室内多摄像头感知任务中的性能，重点考察其在低帧率、量化、跨数据集迁移及混合精度训练下的检测、跟踪与身份稳定性表现。


<details>
  <summary>Details</summary>
Motivation: 室内环境中静态多摄像头网络需应对遮挡和异构视角下的多目标跟踪挑战，亟需鲁棒、高效且身份稳定的3D检测与跟踪方法。

Method: 基于查询的时空3D检测与跟踪框架Sparse4D，通过共享世界坐标系融合多视角特征，并利用实例记忆传播稀疏物体查询；实验涵盖低帧率输入、INT8/FP8后训练量化、WILDTRACK迁移、Transformer Engine混合精度微调，并引入AvgTrackDur指标评估身份稳定性。

Result: Sparse4D在中等降帧下保持稳定，但低于2 FPS时身份关联崩溃；骨干与颈部选择性量化最优；注意力模块对低精度敏感；WILDTRACK上低FPS预训练带来显著零样本提升；混合精度降低延迟并提升相机可扩展性，但损害身份传播稳定性。

Conclusion: Sparse4D在多摄像头室内感知中具备实用潜力，但身份稳定性是关键瓶颈；需在精度、速度与身份连续性之间进行协同优化，并建立稳定性导向的验证范式。

Abstract: Outside-in multi-camera perception is increasingly important in indoor environments, where networks of static cameras must support multi-target tracking under occlusion and heterogeneous viewpoints. We evaluate Sparse4D, a query-based spatiotemporal 3D detection and tracking framework that fuses multi-view features in a shared world frame and propagates sparse object queries via instance memory. We study reduced input frame rates, post-training quantization (INT8 and FP8), transfer to the WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. To better capture identity stability, we report Average Track Duration (AvgTrackDur), which measures identity persistence in seconds. Sparse4D remains stable under moderate FPS reductions, but below 2 FPS, identity association collapses even when detections are stable. Selective quantization of the backbone and neck offers the best speed-accuracy trade-off, while attention-related modules are consistently sensitive to low precision. On WILDTRACK, low-FPS pretraining yields large zero-shot gains over the base checkpoint, while small-scale fine-tuning provides limited additional benefit. Transformer Engine mixed precision reduces latency and improves camera scalability, but can destabilize identity propagation, motivating stability-aware validation.

</details>


### [67] [LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs](https://arxiv.org/abs/2602.00462)
*Benno Krojer,Shravan Nayak,Oscar Mañas,Vaibhav Adlakha,Desmond Elliott,Siva Reddy,Marius Mosbach*

Main category: cs.CV

TL;DR: 本文提出LatentLens方法，通过将视觉token与大规模文本语料库中上下文化文本token表示进行最近邻匹配，实现对多模态大模型中视觉token的逐层可解释性分析，发现视觉token在各层均具有较高可解释性，超越了传统LogitLens等方法。


<details>
  <summary>Details</summary>
Motivation: 理解为何大语言模型（LLM）能轻易处理视觉token，需揭示视觉token在VLM各层中的语义编码内容，而现有可解释性方法（如LogitLens）对此估计不足。

Method: 提出LatentLens：1）编码大规模文本语料，存储每个词元的上下文化表征；2）将视觉token表征与文本表征进行最近邻（top-k）匹配，以对应文本描述解释视觉token。

Result: 在10个不同VLM上验证，LatentLens显著优于LogitLens——绝大多数视觉token在所有模型和所有层均具可解释性；生成的自然语言描述语义清晰、细粒度高，更利于人类理解。

Conclusion: 视觉与语言表征在VLM内部存在深层对齐；LatentLens为分析多模态模型隐空间提供了新范式，并支持更可靠的可解释性评估。

Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.

</details>


### [68] [PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting](https://arxiv.org/abs/2602.00463)
*Xin Zhang,Shen Chen,Jiale Zhou,Lei Li*

Main category: cs.CV

TL;DR: PSGS是一种两阶段框架，用于从文本生成高保真全景场景：第一阶段通过双层优化架构生成语义一致的全景图，第二阶段利用全景滑动机制初始化全局一致的3D高斯点云。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动的3D场景生成方法受限于3D-文本数据稀缺和多视角拼接不一致，导致场景过于简单。

Method: 提出PSGS框架：1）双层优化架构（布局推理层+自优化层，结合MLLM反馈）生成语义连贯全景图；2）全景滑动机制采样重叠视角，初始化3D高斯泼溅点云，并引入深度与语义一致性损失进行训练。

Result: 实验表明PSGS在全景图生成和3D场景质量上均优于现有方法，生成更吸引人的沉浸式场景。

Conclusion: PSGS为可扩展的沉浸式内容生成提供了鲁棒解决方案，有效提升了文本到3D全景场景的生成质量与细节保真度。

Abstract: Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.

</details>


### [69] [ZS-TreeSeg: A Zero-Shot Framework for Tree Crown Instance Segmentation](https://arxiv.org/abs/2602.00470)
*Pengyu Chen,Fangzheng Lyu,Sicheng Wang,Cuizhen Wang*

Main category: cs.CV

TL;DR: 本文提出ZS-TreeSeg——一种零样本树冠实例分割框架，通过融合林冠语义分割与细胞实例分割的先验知识，利用星凸建模与拓扑流场实现密集重叠树冠的数学分离，无需训练即可泛化至多源遥感数据。


<details>
  <summary>Details</summary>
Motivation: 现有监督深度学习方法标注成本高、泛化性差；而通用基础模型（如SAM）缺乏林业领域知识，在密集重叠树冠场景下易欠分割。亟需一种无需训练、兼具领域适应性与鲁棒性的分割方案。

Method: 提出ZS-TreeSeg零样本框架：1）借鉴林冠语义分割与细胞实例分割两类成熟任务；2）基于Cellpose-SAM将树冠建模为星凸对象，并构建拓扑流场；3）利用向量收敛特性实现接触树冠的数学分离。

Result: 在NEON和BAMFOREST数据集上实验表明，该方法跨传感器类型与冠层密度具备强泛化能力，视觉评估证实其能准确分离密集重叠树冠，且完全无需训练。

Conclusion: ZS-TreeSeg为树冠实例分割与标签生成提供了首个真正意义上的零样本、训练-free解决方案，有效弥合了通用基础模型与林业遥感专业需求之间的鸿沟。

Abstract: Individual tree crown segmentation is an important task in remote sensing for forest biomass estimation and ecological monitoring. However, accurate delineation in dense, overlapping canopies remains a bottleneck. While supervised deep learning methods suffer from high annotation costs and limited generalization, emerging foundation models (e.g., Segment Anything Model) often lack domain knowledge, leading to under-segmentation in dense clusters. To bridge this gap, we propose ZS-TreeSeg, a Zero-Shot framework that adapts from two mature tasks: 1) Canopy Semantic segmentation; and 2) Cells instance segmentation. By modeling tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, the ZS-TreeSeg framework forces the mathematical separation of touching tree crown instances based on vector convergence. Experiments on the NEON and BAMFOREST datasets and visual inspection demonstrate that our framework generalizes robustly across diverse sensor types and canopy densities, which can offer a training-free solution for tree crown instance segmentation and labels generation.

</details>


### [70] [GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association](https://arxiv.org/abs/2602.00484)
*Rong-Lin Jian,Ming-Chi Luo,Chen-Wei Huang,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: GTATrack is a hierarchical multi-object tracking framework designed for fisheye soccer videos, combining Deep-EIoU for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement, achieving state-of-the-art performance (HOTA=0.60) in the SoccerTrack Challenge 2025.


<details>
  <summary>Details</summary>
Motivation: Multi-object tracking in fisheye soccer videos is challenging due to irregular motion, uniform appearances, frequent occlusions, geometric distortion, and extreme scale variation.

Method: GTATrack employs a two-stage hierarchical design: (1) Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association; (2) Global Tracklet Association (GTA) for trajectory-level refinement; plus a pseudo-labeling strategy to improve detector recall on small/distorted targets.

Result: Achieved first place in SoccerTrack Challenge 2025 with HOTA score of 0.60 and only 982 false positives—state-of-the-art accuracy for fisheye-based soccer tracking.

Conclusion: The synergy between local association (Deep-EIoU) and global reasoning (GTA), enhanced by pseudo-labeling, effectively mitigates identity switches, occlusions, and tracking fragmentation in challenging fisheye sports scenarios.

Abstract: Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.

</details>


### [71] [Refining Strokes by Learning Offset Attributes between Strokes for Flexible Sketch Edit at Stroke-Level](https://arxiv.org/abs/2602.00489)
*Sicong Zang,Tao Sun,Cairong Yan*

Main category: cs.CV

TL;DR: 本文提出SketchMod方法，通过学习源笔画到目标草图的尺度、方向和位置三个偏移属性，对源笔画进行变换精修，以实现语义一致且视觉保真的笔画级草图编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅通过重定位源笔画进行草图编辑，难以应对源笔画在尺寸和方向上的显著差异，导致编辑结果不自然或语义不连贯。

Method: SketchMod学习源笔画相对于目标草图的尺度、方向和位置三个偏移属性，并据此对源笔画进行缩放、旋转和平移变换；同时利用捕获的笔画属性实现对笔画轮廓的精确控制。

Result: 实验表明SketchMod在笔画级草图编辑任务中实现了高精度与高灵活性。

Conclusion: 通过基于目标草图模式驱动的笔画变换精修，SketchMod有效提升了笔画级草图编辑的语义一致性与视觉保真度。

Abstract: Sketch edit at stroke-level aims to transplant source strokes onto a target sketch via stroke expansion or replacement, while preserving semantic consistency and visual fidelity with the target sketch. Recent studies addressed it by relocating source strokes at appropriate canvas positions. However, as source strokes could exhibit significant variations in both size and orientation, we may fail to produce plausible sketch editing results by merely repositioning them without further adjustments. For example, anchoring an oversized source stroke onto the target without proper scaling would fail to produce a semantically coherent outcome. In this paper, we propose SketchMod to refine the source stroke through transformation so as to align it with the target sketch's patterns, further realize flexible sketch edit at stroke-level. As the source stroke refinement is governed by the patterns of the target sketch, we learn three key offset attributes (scale, orientation and position) from the source stroke to another, and align it with the target by: 1) resizing to match spatial proportions by scale, 2) rotating to align with local geometry by orientation, and 3) displacing to meet with semantic layout by position. Besides, a stroke's profiles can be precisely controlled during sketch edit via the exposed captured stroke attributes. Experimental results indicate that SketchMod achieves precise and flexible performances on stroke-level sketch edit.

</details>


### [72] [HSSDCT: Factorized Spatial-Spectral Correlation for Hyperspectral Image Fusion](https://arxiv.org/abs/2602.00490)
*Chia-Ming Lee,Yu-Hao Ho,Yu-Fan Lin,Jen-Wei Lee,Li-Wei Kang,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 本文提出了一种用于高光谱图像融合的层次化空谱密集相关网络（HSSDCT），通过引入分层密集残差Transformer块和空谱相关层，解决了现有深度学习方法感受野有限、光谱冗余及自注意力计算复杂度高的问题，在保持高效性的同时提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的高光谱图像融合方法存在感受野受限、光谱波段冗余以及自注意力机制计算复杂度为二次方等问题，影响了模型的效率与鲁棒性。

Method: 提出HSSDCT框架，包含两个核心模块：(i) 分层密集残差Transformer块（HDRTB），通过渐进扩大窗口和密集残差连接实现多尺度特征聚合；(ii) 空谱相关层（SSCL），显式分解空间与光谱依赖关系，将自注意力降为线性复杂度并缓解光谱冗余。

Result: 在多个基准数据集上实验表明，HSSDCT在重建质量上优于现有方法，同时显著降低计算成本，达到高光谱图像融合领域的新SOTA性能。

Conclusion: HSSDCT通过结构创新有效平衡了高光谱图像融合任务中的精度、效率与鲁棒性，为后续研究提供了新思路与实用工具。

Abstract: Hyperspectral image (HSI) fusion aims to reconstruct a high-resolution HSI (HR-HSI) by combining the rich spectral information of a low-resolution HSI (LR-HSI) with the fine spatial details of a high-resolution multispectral image (HR-MSI). Although recent deep learning methods have achieved notable progress, they still suffer from limited receptive fields, redundant spectral bands, and the quadratic complexity of self-attention, which restrict both efficiency and robustness. To overcome these challenges, we propose the Hierarchical Spatial-Spectral Dense Correlation Network (HSSDCT). The framework introduces two key modules: (i) a Hierarchical Dense-Residue Transformer Block (HDRTB) that progressively enlarges windows and employs dense-residue connections for multi-scale feature aggregation, and (ii) a Spatial-Spectral Correlation Layer (SSCL) that explicitly factorizes spatial and spectral dependencies, reducing self-attention to linear complexity while mitigating spectral redundancy. Extensive experiments on benchmark datasets demonstrate that HSSDCT delivers superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion. Our code is available at https://github.com/jemmyleee/HSSDCT.

</details>


### [73] [RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding](https://arxiv.org/abs/2602.00504)
*Jiahe Wu,Bing Cao,Qilong Wang,Qinghua Hu,Dongdong Li,Pengfei Zhu*

Main category: cs.CV

TL;DR: 本文提出RGBX-R1框架，通过UAV提示策略和VM-CoT增强MLLM对非RGB模态（如红外、深度、事件数据）的理解与推理能力，并采用两阶段训练范式（CS-SFT和ST-RFT）提升性能，在自建RGBX-Grounding基准上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM主要在RGB模态上预训练，难以有效处理红外、深度、事件等关键X模态数据，限制其在复杂场景中的性能。

Method: 提出RGBX-R1框架，包含：1）Understand-Associate-Validate（UAV）提示策略构建Visual Modality Chain-of-Thought（VM-CoT）；2）两阶段训练：Cold-Start Supervised Fine-Tuning（CS-SFT）和Spatio-Temporal Reinforcement Fine-Tuning（ST-RFT），后者基于GRPO并引入Modality-understanding Spatio-Temporal（MuST）奖励。

Result: 构建首个RGBX-Grounding基准，在三项RGBX定位任务上超越基线22.71%；验证了模型在多模态理解与空间感知上的优越性。

Conclusion: RGBX-R1有效扩展了MLLM对X模态的感知与推理能力，为跨模态大模型提供了可扩展的训练范式与评估基准。

Abstract: Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.

</details>


### [74] [Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models](https://arxiv.org/abs/2602.00505)
*Jingrui Zhang,Feng Liang,Yong Zhang,Wei Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 本文提出SparseCut，一种通用的跨模态融合架构，通过稀疏快捷连接和多粒度特征融合模块，高效地将多层级视觉特征融入大语言模型，提升多模态大模型性能，且不增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型仅利用高层视觉特征进行模态对齐，丢失了中低层丰富的语义信息，限制了跨模态理解能力。

Method: 提出SparseCut架构，包含稀疏快捷连接（连接跨模态编码器与大语言模型）和高效的多粒度特征融合模块（在路由前融合视觉特征），实现分层、高效、无额外计算负担的跨模态融合。

Result: SparseCut显著提升了MLLM在多个多模态基准上的性能，具有通用性（适配不同基座LLM）和可扩展性。

Conclusion: 稀疏快捷连接与多粒度融合能有效增强跨模态语义融合能力，在不牺牲效率的前提下提升多模态大语言模型的理解与生成性能。

Abstract: With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.

</details>


### [75] [DuoGen: Towards General Purpose Interleaved Multimodal Generation](https://arxiv.org/abs/2602.00508)
*Min Shi,Xiaohui Zeng,Jiannan Huang,Yin Cui,Francesco Ferroni,Jialuo Li,Shubham Pachori,Zhaoshuo Li,Yogesh Balaji,Haoxiang Wang,Tsung-Yi Lin,Xiao Fu,Yue Zhao,Chieh-Yun Chen,Ming-Yu Liu,Humphrey Shi*

Main category: cs.CV

TL;DR: DuoGen 是一个通用的交错多模态生成框架，通过高质量数据构建、解耦式两阶段训练策略（先指令微调多模态大语言模型，再对齐扩散Transformer），在文本质量、图像保真度和图文对齐方面超越现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有交错多模态生成模型受限于训练数据不足和基础模型能力有限，难以在通用指令下生成高质量交错内容。

Method: 提出 DuoGen 框架：1）构建大规模高质量指令微调数据集，融合网站重写对话与多样化日常场景合成样本；2）采用预训练多模态大语言模型（MLLM）负责理解与规划，预训练视频扩散Transformer（DiT）负责图像生成；3）两阶段解耦训练：先指令微调 MLLM，再用交错图文序列对齐 DiT 与 MLLM。

Result: 在公开及新提出的基准上，DuoGen 在文本质量、图像保真度和图文上下文对齐方面均优于现有开源模型，并在统一生成模型中实现文本到图像和图像编辑任务的最先进性能。

Conclusion: DuoGen 证明了通过合理分工、高效对齐与高质量数据可显著提升交错多模态生成能力，为通用多模态智能体提供可行技术路径。

Abstract: Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duetgen/.

</details>


### [76] [SPARK: Stochastic Propagation via Affinity-guided Random walK for training-free unsupervised segmentation](https://arxiv.org/abs/2602.00516)
*Kunal Mahatha,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文提出了一种新的无训练语义分割方法，将分割建模为扩散图上的随机流平衡问题，通过结合全局扩散注意力与局部邻域结构，克服了传统谱聚类方法在簇数预设、边界模糊和噪声敏感等方面的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有无训练分割方法隐含地假设分割是基于扩散亲和力的谱图划分问题，但该假设存在诸多局限性，如需预设簇数、边界过度平滑、对噪声和多模态亲和力分布敏感，且忽视局部邻域结构的重要性。

Method: 将分割重新表述为扩散诱导亲和图上的随机流平衡问题；引入一种马尔可夫传播机制，结合自适应剪枝策略进行随机游走式标签扩散，抑制不可靠转移、增强可信亲和路径。

Result: 在七个主流语义分割基准上达到零样本最先进性能，生成更锐利的边界、更连贯的区域以及显著更稳定的掩码。

Conclusion: 所提方法通过融合全局扩散注意力与局部邻域结构，构建稀疏而富有表现力的亲和结构，有效克服了传统谱聚类方法的根本缺陷，为无训练分割提供了新范式。

Abstract: We argue that existing training-free segmentation methods rely on an implicit and limiting assumption, that segmentation is a spectral graph partitioning problem over diffusion-derived affinities. Such approaches, based on global graph partitioning and eigenvector-based formulations of affinity matrices, suffer from several fundamental drawbacks, they require pre-selecting the number of clusters, induce boundary oversmoothing due to spectral relaxation, and remain highly sensitive to noisy or multi-modal affinity distributions. Moreover, many prior works neglect the importance of local neighborhood structure, which plays a crucial role in stabilizing affinity propagation and preserving fine-grained contours. To address these limitations, we reformulate training-free segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs, where segmentation emerges from a stochastic propagation process that integrates global diffusion attention with local neighborhoods extracted from stable diffusion, yielding a sparse yet expressive affinity structure. Building on this formulation, we introduce a Markov propagation scheme that performs random-walk-based label diffusion with an adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Experiments across seven widely used semantic segmentation benchmarks demonstrate that our method achieves state-of-the-art zero-shot performance, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.

</details>


### [77] [MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval](https://arxiv.org/abs/2602.00522)
*Chaoran Xu,Chengkan Lv,Qiyu Chen,Feng Zhang,Zhengtao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的零样本异常检测框架MRAD，通过构建两级记忆库（图像级和像素级）直接进行相似性检索来获取异常分数，并设计了两种轻量增强变体MRAD-FT和MRAD-CLIP，在16个工业与医学数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法依赖提示学习或复杂建模，导致高计算开销和跨域稳定性差。

Method: 提出MRAD框架：MRAD-TF为无训练基础模型，冻结CLIP图像编码器，构建两级记忆库；MRAD-FT微调检索度量；MRAD-CLIP将区域先验注入CLIP文本提示中作为动态偏置。

Result: 在16个工业与医学数据集上，MRAD在异常分类与分割任务中均优于现有方法，兼顾零训练与可训练设定下的性能。

Conclusion: 完全利用原始数据的经验分布而非仅依赖模型拟合，能显著提升异常检测性能。

Abstract: Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF, freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. The code will be publicly released at https://github.com/CROVO1026/MRAD.

</details>


### [78] [SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding](https://arxiv.org/abs/2602.00523)
*Yujia Tong,Tian Zhang,Yunyang Wan,Kaiwei Lin,Jingling Yuan,Chuang Hu*

Main category: cs.CV

TL;DR: 本文提出了SAGE框架，通过基于实时预测不确定性的动态调整推测树结构，提升视觉语言模型（VLMs）的推理加速效果，显著提高接受长度和解码速度，且不牺牲输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法采用静态树结构，无法适应不同生成步骤的预测难度变化，导致接受长度次优、加速有限。

Method: SAGE利用输出熵作为具有时间相关性的置信度指标，动态构建更深更窄（高置信）或更浅更宽（低置信）的推测树结构。

Result: 在多个基准上，SAGE在不损失输出质量前提下，为LLaVA-OneVision-72B和Qwen2.5-VL-72B分别实现最高3.36×和3.18×的解码加速。

Conclusion: 动态适配推测树结构能有效提升VLMs推测解码性能，SAGE为高效多模态推理提供了新范式。

Abstract: Speculative decoding has emerged as a promising approach to accelerate inference in vision-language models (VLMs) by enabling parallel verification of multiple draft tokens. However, existing methods rely on static tree structures that remain fixed throughout the decoding process, failing to adapt to the varying prediction difficulty across generation steps. This leads to suboptimal acceptance lengths and limited speedup. In this paper, we propose SAGE, a novel framework that dynamically adjusts the speculation tree structure based on real-time prediction uncertainty. Our key insight is that output entropy serves as a natural confidence indicator with strong temporal correlation across decoding steps. SAGE constructs deeper-narrower trees for high-confidence predictions to maximize speculation depth, and shallower-wider trees for uncertain predictions to diversify exploration. SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines. Experiments on multiple benchmarks demonstrate the effectiveness of SAGE: without any loss in output quality, it delivers up to $3.36\times$ decoding speedup for LLaVA-OneVision-72B and $3.18\times$ for Qwen2.5-VL-72B.

</details>


### [79] [Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment](https://arxiv.org/abs/2602.00531)
*Tianyi Zhang,Antoine Simoulin,Kai Li,Sana Lakdawala,Shiqing Yu,Arpit Mittal,Hongyu Fu,Yu Lin*

Main category: cs.CV

TL;DR: 本文提出VLDet框架，通过改进特征金字塔和引入SigRPN模块，提升开放词汇目标检测性能，在COCO2017和LVIS上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测受限于预定义类别，而开放词汇目标检测（OVD）需识别训练中未见的新类别；现有方法在将CLIP单尺度图像骨干网络适配到检测框架、或保障视觉-语言对齐鲁棒性方面存在不足。

Method: 提出VLDet框架，包含VL-PUB模块（重构特征金字塔以实现细粒度视觉-语言对齐，并适配CLIP骨干网络）和SigRPN模块（引入基于sigmoid的锚点-文本对比对齐损失），提升新类别检测能力。

Result: 在COCO2017上新类别AP达58.7，在LVIS上达24.8 AP，分别比SOTA提升27.6%和6.9%；同时在闭集零样本目标检测中表现优异。

Conclusion: VLDet通过改进特征金字塔结构与设计新型对比对齐损失，有效提升了开放词汇目标检测性能，验证了细粒度视觉-语言对齐对OVD的关键作用。

Abstract: Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.

</details>


### [80] [SADER: Structure-Aware Diffusion Framework with DEterministic Resampling for Multi-Temporal Remote Sensing Cloud Removal](https://arxiv.org/abs/2602.00536)
*Yifan Zhang,Qian Chen,Yi Liu,Wengen Li,Jihong Guan*

Main category: cs.CV

TL;DR: 本文提出SADER，一种面向多时相遥感图像去云的结构感知扩散模型，通过多时相条件扩散网络、云感知注意力损失和确定性重采样策略，显著提升了去云效果与采样效率。


<details>
  <summary>Details</summary>
Motivation: 云污染严重降低遥感影像可用性，现有基于扩散模型的方法在采样效率和多时相结构/时序先验利用方面存在不足。

Method: 提出SADER框架：1）可扩展的多时相条件扩散网络（MTCDN），融合时序与多模态信息；2）云感知注意力损失，聚焦云厚与亮度差异区域；3）面向连续扩散模型的确定性重采样策略，固定步数内迭代修正异常样本。

Result: 在多个多时相数据集上全面超越现有最先进方法，所有评估指标均取得最优性能。

Conclusion: SADER有效结合结构建模、云敏感优化与高效采样机制，为多时相遥感去云提供了新范式。

Abstract: Cloud contamination severely degrades the usability of remote sensing imagery and poses a fundamental challenge for downstream Earth observation tasks. Recently, diffusion-based models have emerged as a dominant paradigm for remote sensing cloud removal due to their strong generative capability and stable optimization. However, existing diffusion-based approaches often suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal remote sensing scenarios. In this work, we propose SADER, a structure-aware diffusion framework for multi-temporal remote sensing cloud removal. SADER first develops a scalable Multi-Temporal Conditional Diffusion Network (MTCDN) to fully capture multi-temporal and multimodal correlations via temporal fusion and hybrid attention. Then, a cloud-aware attention loss is introduced to emphasize cloud-dominated regions by accounting for cloud thickness and brightness discrepancies. In addition, a deterministic resampling strategy is designed for continuous diffusion models to iteratively refine samples under fixed sampling steps by replacing outliers through guided correction. Extensive experiments on multiple multi-temporal datasets demonstrate that SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics. The code of SADER is publicly available at https://github.com/zyfzs0/SADER.

</details>


### [81] [NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation](https://arxiv.org/abs/2602.00542)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari,Mert D. Pesé*

Main category: cs.CV

TL;DR: NPNet is a fully non-parametric method for 3D point-cloud classification and part segmentation, using only deterministic geometric operators and adaptive positional encodings—no learned parameters—and achieves strong performance, especially in few-shot settings.


<details>
  <summary>Details</summary>
Motivation: To develop a lightweight, scalable, and parameter-free alternative to deep learning-based 3D point cloud methods that avoids overfitting, reduces memory overhead, and generalizes well across varying scales and sampling densities.

Method: NPNet uses deterministic operators (farthest point sampling, k-NN, pooling) and introduces an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are geometry-driven; for part segmentation, it adds fixed-frequency Fourier features to incorporate global context.

Result: NPNet achieves competitive accuracy among non-parametric baselines on ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart; excels in few-shot classification on ModelNet40; and shows improved memory efficiency and inference speed versus prior non-parametric methods.

Conclusion: Fully non-parametric design with adaptive geometric encodings is viable and effective for 3D point cloud understanding—offering simplicity, efficiency, and robustness without sacrificing performance.

Abstract: We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods

</details>


### [82] [Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models](https://arxiv.org/abs/2602.00559)
*Wenbin Xing,Quanxing Zha,Lizheng Zu,Mengran Li,Ming Li,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出OmniVCHall基准，首次系统评估视频多模态大模型（VLLMs）中孤立与组合式幻觉，并设计TriCD对比解码框架以提升抗幻觉能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注孤立幻觉类型，忽视由多时空因素交互引发的组合式幻觉，缺乏系统性评测基准和有效缓解方法。

Method: 构建OmniVCHall评测基准（含新摄像机幻觉类型、细粒度分类、对抗选项），并提出TriCD框架：含自适应扰动控制器生成负样本视频变体，以及显著性引导的视觉证据增强模块，二者通过强化学习联合优化。

Result: 在39个主流VLLM上测试发现先进模型（如Qwen3-VL、GPT-5）在组合幻觉下性能显著下降；TriCD在两个主干模型上平均准确率提升超10%。

Conclusion: 组合式幻觉是当前VLLMs的关键瓶颈；OmniVCHall为评测提供新标准，TriCD为缓解提供有效新范式。

Abstract: Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., "All are correct" and "None of the above") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.

</details>


### [83] [GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates](https://arxiv.org/abs/2602.00570)
*Xingyu Luo,Yidong Cai,Jie Liu,Jie Tang,Gangshan Wu,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为GLAD的生成式语言辅助视觉-语言跟踪模型，利用扩散模型实现文本描述与模板图像的生成式多模态融合，以增强低语义图像（如模糊、低分辨率）的语义信息并提升跨模态理解能力，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言跟踪方法在处理低语义图像（如模糊、低分辨率）时，因跨模态理解能力下降而性能受限；而当前文本与视觉特征间存在语义鸿沟，直接融合效果有限。

Method: 提出生成式语言辅助跟踪模型GLAD，利用扩散模型对文本描述和模板图像进行生成式多模态融合，重建并增强模板图像的语义信息，提升语言与图像的兼容性。

Result: 在多个基准数据集上达到新SOTA性能，同时具备优异的推理速度。

Conclusion: GLAD通过生成式多模态融合有效缓解了低语义图像带来的挑战，验证了扩散模型在视觉-语言跟踪任务中的潜力，为该领域提供了新范式。

Abstract: Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD

</details>


### [84] [Bridging Degradation Discrimination and Generation for Universal Image Restoration](https://arxiv.org/abs/2602.00579)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yanye Lu*

Main category: cs.CV

TL;DR: 本文提出BDG方法，通过MAS-GLCM细粒度判别退化类型与程度，并将扩散模型训练分为生成、桥接与恢复三阶段，在不改变架构前提下显著提升通用图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 通用图像恢复需应对多种退化，挑战在于高质量图像分布建模与基于退化的输出调整。

Method: 提出多角度多尺度灰度共生矩阵（MAS-GLCM）用于退化判别；将扩散训练分为生成、桥接和恢复三阶段，融合判别信息与生成能力。

Result: 在all-in-one恢复与真实世界超分任务中显著提升保真度，且不牺牲感知质量；代码与预训练模型已开源。

Conclusion: BDG有效协同退化判别与图像生成，提升了多任务、多退化场景下的恢复鲁棒性与性能。

Abstract: Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.

</details>


### [85] [MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation](https://arxiv.org/abs/2602.00583)
*Xiangdong Li,Ye Lou,Ao Gao,Wei Zhang,Siyang Song*

Main category: cs.CV

TL;DR: 本文提出MAUGen，一种基于扩散模型的多模态框架，用于根据文本提示生成逼真的面部表情图像及对应的AU（动作单元）发生和强度标签，并构建了大规模多身份面部动作数据集MIFA。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模、人口统计学上多样化的面部图像数据集，且这些图像需具备精确的动作单元（AU）发生与强度标注，这限制了AU识别系统的泛化能力。

Method: 提出MAUGen框架，包含两个核心模块：(1) 多模态表征学习（MRL）模块，在统一潜在空间中建模文本描述、面部身份、表情图像与AU激活之间的关系；(2) 基于扩散的图像-标签生成器（DIG），将联合表征解码为跨多种身份对齐的图像-标签对。

Result: 成功生成了大规模、人口统计学多样、AU标注完备的合成数据集MIFA；实验表明MAUGen在生成逼真图像及语义对齐AU标签方面优于现有方法。

Conclusion: MAUGen有效缓解了AU识别领域因真实标注数据稀缺而导致的泛化瓶颈，为构建更鲁棒、公平的面部行为分析系统提供了新范式。

Abstract: The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.

</details>


### [86] [From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking](https://arxiv.org/abs/2602.00593)
*Yifan Jiang,Cong Zhang,Bofei Zhang,Yifan Yang,Bingzhang Wang,Yew-Soon Ong*

Main category: cs.CV

TL;DR: 本文提出了Pix2Fact基准，用于评估视觉语言模型在专家级视觉感知与知识密集型多跳推理上的综合能力；该基准包含1000张高分辨率图像及由博士专家精心设计的问题，现有最先进VLMs平均准确率仅24.0%，远低于人类的56%，凸显当前模型在细粒度视觉理解与知识整合方面的严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准分别评估视觉定位和知识推理，无法衡量二者协同所需的专家级视觉理解和多跳知识推理能力，亟需新基准填补这一空白。

Method: 构建Pix2Fact基准：涵盖8种日常生活场景的1000张4K+高分辨率图像，问题与答案由全球顶尖高校博士与专业标注公司协作设计，每题均需视觉细节定位、多跳推理与外部知识融合。

Result: 在9个SOTA VLM（含Gemini-3-Pro、GPT-5等）上的评测显示，最佳模型平均准确率仅24.0%，显著低于人类56%的水平。

Conclusion: Pix2Fact揭示了当前VLM在细粒度视觉理解与知识驱动推理方面存在根本性局限，有望推动兼具高精度感知与强知识推理能力的下一代多模态智能体发展。

Abstract: Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.

</details>


### [87] [Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting](https://arxiv.org/abs/2602.00618)
*Yian Zhao,Rushi Ye,Ruochong Zheng,Zesen Cheng,Chaoran Feng,Jiashu Yang,Pengchong Qiao,Chang Liu,Jie Chen*

Main category: cs.CV

TL;DR: 本文提出了一种可调创意强度的3D风格迁移范式Tune-Your-Style，通过高斯神经元和可学习风格调节器实现用户自定义的内容-风格平衡，并结合多视角一致的可调风格化引导与两阶段优化策略，提升了3D风格迁移的灵活性与效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格迁移方法难以适应不同用户对内容与风格平衡的多样化需求，固定输出范式缺乏可定制性。

Method: 引入高斯神经元显式建模风格强度，设计可学习风格调节器；提出可调风格化引导机制，利用扩散模型生成多视角一致的风格化视图，并通过两阶段优化策略平衡全风格引导与零风格引导。

Result: 实验表明该方法在视觉效果上具有吸引力，同时显著提升了3D风格迁移的灵活定制能力。

Conclusion: Tune-Your-Style为3D风格迁移提供了首个支持强度连续调节的实用框架，兼顾高质量渲染与用户可控性。

Abstract: 3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.

</details>


### [88] [Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering](https://arxiv.org/abs/2602.00621)
*Guangtao Lyu,Xinyi Cheng,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出Contrastive Neuron Steering（CNS）方法，通过稀疏自编码器（SAE）在表征层面分析LVLM视觉嵌入，发现图像特异性神经元的异常激活是幻觉主因，并据此设计可控干预机制，在prefilling阶段提升视觉 grounding、抑制幻觉，且兼容解码阶段方法。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM幻觉缓解方法多集中于输出层调整，缺乏对内部表征机制（尤其是视觉嵌入中神经元行为）的深入理解。

Method: 引入稀疏自编码器（SAE）分解视觉嵌入，开展神经元级分析；识别always-on与image-specific两类神经元；提出基于对比分析（clean vs. noisy输入）的Contrastive Neuron Steering（CNS），选择性增强信息性神经元、抑制扰动诱发激活。

Result: CNS在幻觉专项与通用多模态基准上均显著降低幻觉率，同时保持整体多模态理解能力；其prefilling阶段操作与解码阶段方法完全兼容。

Conclusion: 幻觉根源可追溯至图像特异性神经元的扰动激活；在表征层面进行神经元级可控干预（如CNS）是一种有效、可解释且兼容性强的幻觉缓解新范式。

Abstract: LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.

</details>


### [89] [FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization](https://arxiv.org/abs/2602.00627)
*Benxiang Zhai,Yifang Xu,Guofeng Zhang,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: FaceSnap是一种基于Stable Diffusion的即插即用个性化肖像生成方法，仅需单张参考图像即可在一次推理中生成高保真、身份一致的定制化人脸图像。


<details>
  <summary>Details</summary>
Motivation: 现有个性化肖像生成方法存在需耗时微调且泛化性差，或难以保持高保真面部细节的问题。

Method: 提出FaceSnap，包含三个核心组件：1）面部属性混合器（FAM），融合低层细节与高层语义特征；2）关键点预测器（LP），跨姿态保持身份一致性并提供空间控制；3）ID保持模块，将上述信息注入UNet。

Result: 在个性化与定制化肖像生成任务上显著优于现有SOTA方法，实现单图输入、单次推理、高一致性与高细节保真。

Conclusion: FaceSnap是一种高效、通用、即插即用的SD扩展方法，在无需微调的前提下实现了高质量、强身份一致性的定制人脸生成。

Abstract: Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.

</details>


### [90] [S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning](https://arxiv.org/abs/2602.00635)
*Lingsong Wang,Mancheng Meng,Ziyan Wu,Terrence Chen,Fan Yang,Dinggang Shen*

Main category: cs.CV

TL;DR: 本文提出S³POT框架，通过结合面部生成与自监督空间提示，解决面部解析中遮挡物被误分类的问题，无需遮挡真实标注即可实现遮挡分割。


<details>
  <summary>Details</summary>
Motivation: 现有面部解析方法常将遮挡物错误分类为面部组件，因遮挡是高层概念，难以构建覆盖所有遮挡类别的真实数据集且精确掩码标注耗时费力。

Method: 提出S³POT对比驱动框架，包含参考图像生成（RF）、特征增强（FE）和提示选择（PS）三个模块；利用面部生成器重建无遮挡参考图像，并借助基础分割模型（如SAM）结合自监督空间提示提取精确掩码；采用三种新颖互补的无监督目标函数进行训练。

Result: 在专门构建的数据集上大量实验表明，S³POT性能优越，各模块均有效。

Conclusion: S³POT成功实现了无需遮挡真实掩码标注的高质量遮挡分割，为面部解析中遮挡处理提供了新范式。

Abstract: Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and accurate mask annotation is labor-intensive. To deal with the problems, we present S$^3$POT, a contrast-driven framework synergizing face generation with self-supervised spatial prompting, to achieve occlusion segmentation. The framework is inspired by the insights: 1) Modern face generators' ability to realistically reconstruct occluded regions, creating an image that preserve facial geometry while eliminating occlusion, and 2) Foundation segmentation models' (e.g., SAM) capacity to extract precise mask when provided with appropriate prompts. In particular, S$^3$POT consists of three modules: Reference Generation (RF), Feature enhancement (FE), and Prompt Selection (PS). First, a reference image is produced by RF using structural guidance from parsed mask. Second, FE performs contrast of tokens between raw and reference images to obtain an initial prompt, then modifies image features with the prompt by cross-attention. Third, based on the enhanced features, PS constructs a set of positive and negative prompts and screens them with a self-attention network for a mask decoder. The network is learned under the guidance of three novel and complementary objective functions without occlusion ground truth mask involved. Extensive experiments on a dedicatedly collected dataset demonstrate S$^3$POT's superior performance and the effectiveness of each module.

</details>


### [91] [VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning](https://arxiv.org/abs/2602.00637)
*Vivek Madhavaram,Vartika Sengar,Arkadipta De,Charu Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、端到端的3D场景图生成方法VIZOR，通过以物体自身朝向为参考定义空间关系，实现视角无关、零样本的3D场景理解与推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态输入（如2D图像、深度图等）构建场景图的方法泛化能力差，且'左右'等空间关系在不同视角下不一致，难以准确建模3D场景。

Method: VIZOR是一种训练无关的端到端框架，直接从原始3D场景构建稠密、视角不变的3D场景图；空间关系以每个物体的前向方向为基准定义，并支持开放词汇的空间与邻近关系推理。

Result: 在Replica和Nr3D数据集上，VIZOR在零样本目标定位任务中分别提升准确率22%和4.81%，定量与定性评估均优于当前最优方法。

Conclusion: VIZOR有效解决了3D场景图生成中的视角依赖与标注依赖问题，实现了更鲁棒、通用的3D场景理解与推理。

Abstract: Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like "left/right", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.

</details>


### [92] [Diff-PC: Identity-preserving and 3D-aware Controllable Diffusion for Zero-shot Portrait Customization](https://arxiv.org/abs/2602.00639)
*Yifang Xu,Benxiang Zhai,Chenyu Zhang,Ming Li,Yang Li,Sidan Du*

Main category: cs.CV

TL;DR: 本文提出Diff-PC，一种基于扩散模型的零样本人像定制框架，通过3D人脸先验、ID编码器、ID控制器和ID注入器提升身份保真度与面部可控性，并在自建数据集上训练以增强图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人像定制中缺乏精确的身份保持和面部控制能力。

Method: 提出Diff-PC框架：利用3D人脸预测器重建含身份、表情、姿态的3D先验；设计ID-Encoder融合局部与全局面部特征；构建ID-Ctrl以3D人脸引导ID特征对齐；引入ID-Injector增强身份保真度与面部可控性；并在自建ID中心数据集上训练。

Result: Diff-PC在身份保持、面部控制和文本到图像一致性方面超越当前最优方法，并兼容多风格基础模型。

Conclusion: Diff-PC有效解决了人像定制中身份失真与控制不足的问题，实现了高保真、可控、多样化的高质量人像生成。

Abstract: Portrait customization (PC) has recently garnered significant attention due to its potential applications. However, existing PC methods lack precise identity (ID) preservation and face control. To address these tissues, we propose Diff-PC, a diffusion-based framework for zero-shot PC, which generates realistic portraits with high ID fidelity, specified facial attributes, and diverse backgrounds. Specifically, our approach employs the 3D face predictor to reconstruct the 3D-aware facial priors encompassing the reference ID, target expressions, and poses. To capture fine-grained face details, we design ID-Encoder that fuses local and global facial features. Subsequently, we devise ID-Ctrl using the 3D face to guide the alignment of ID features. We further introduce ID-Injector to enhance ID fidelity and facial controllability. Finally, training on our collected ID-centric dataset improves face similarity and text-to-image (T2I) alignment. Extensive experiments demonstrate that Diff-PC surpasses state-of-the-art methods in ID preservation, facial control, and T2I consistency. Furthermore, our method is compatible with multi-style foundation models.

</details>


### [93] [A Hybrid Mamba-SAM Architecture for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2602.00650)
*Mohammadreza Gholipour Shahraki,Mehdi Rezaeian,Mohammad Ghasemzadeh*

Main category: cs.CV

TL;DR: 本文提出Mamba-SAM，一种融合冻结SAM编码器与Mamba状态空间模型的高效混合架构，用于3D医学图像分割；通过双分支融合或轻量级TPMamba适配器，并引入多频门控卷积（MFGC），在ACDC数据集上实现高精度与高推理速度的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有通用基础模型（如SAM）因域偏移、2D设计限制及微调开销大，难以直接适用于3D医学图像分割任务。

Method: 提出Mamba-SAM：1）双分支架构——冻结SAM编码器 + 可训练VMamba编码器，通过交叉注意力融合特征；2）适配器方案——在冻结SAM ViT中嵌入3D感知的Tri-Plane Mamba（TPMamba）模块；并引入Multi-Frequency Gated Convolution（MFGC）联合建模空域与频域信息。

Result: 在ACDC心脏MRI数据集上，双分支Mamba-SAM-Base达平均Dice 0.906（心肌0.910，左心室0.971），媲美UNet++；TP-MFGC变体达0.880 Dice且推理速度4.77 FPS。

Conclusion: 将基础模型与高效SSM架构结合，是解决3D医学图像分割问题的一种实用而有效的新范式。

Abstract: Accurate segmentation of 3D medical images such as MRI and CT is essential for clinical diagnosis and treatment planning. Foundation models like the Segment Anything Model (SAM) provide powerful general-purpose representations but struggle in medical imaging due to domain shift, their inherently 2D design, and the high computational cost of fine-tuning. To address these challenges, we propose Mamba-SAM, a novel and efficient hybrid architecture that combines a frozen SAM encoder with the linear-time efficiency and long-range modeling capabilities of Mamba-based State Space Models (SSMs). We investigate two parameter-efficient adaptation strategies. The first is a dual-branch architecture that explicitly fuses general features from a frozen SAM encoder with domain-specific representations learned by a trainable VMamba encoder using cross-attention. The second is an adapter-based approach that injects lightweight, 3D-aware Tri-Plane Mamba (TPMamba) modules into the frozen SAM ViT encoder to implicitly model volumetric context. Within this framework, we introduce Multi-Frequency Gated Convolution (MFGC), which enhances feature representation by jointly analyzing spatial and frequency-domain information via 3D discrete cosine transforms and adaptive gating. Extensive experiments on the ACDC cardiac MRI dataset demonstrate the effectiveness of the proposed methods. The dual-branch Mamba-SAM-Base model achieves a mean Dice score of 0.906, comparable to UNet++ (0.907), while outperforming all baselines on Myocardium (0.910) and Left Ventricle (0.971) segmentation. The adapter-based TP MFGC variant offers superior inference speed (4.77 FPS) with strong accuracy (0.880 Dice). These results show that hybridizing foundation models with efficient SSM-based architectures provides a practical and effective solution for 3D medical image segmentation.

</details>


### [94] [Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment](https://arxiv.org/abs/2602.00653)
*Lukas Kuhn,Giuseppe Serra,Florian Buettner*

Main category: cs.CV

TL;DR: NOVA是一种非对比式视觉-语言对齐框架，通过联合嵌入预测与分布正则化（SIGReg）实现图像到文本嵌入的对齐，无需负采样、动量编码器或梯度截断，仅需单个超参数，在零样本胸部X光分类任务中优于对比方法且训练更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有主流对比学习方法（如CLIP）依赖大批次、精心设计的负样本和大量超参调优，限制了其稳定性与实用性。

Method: 提出NOVA框架：利用冻结的领域专用文本编码器（如ClinicalBERT），通过增强图像视图预测对应文本嵌入，并引入Sketched Isotropic Gaussian Regularization（SIGReg）强制图像表征呈各向同性高斯分布，实现非对比式对齐。

Result: 在基于MIMIC-CXR训练ViT、ClinicalBERT作为文本编码器的零样本胸部X光分类任务中，NOVA在三个基准数据集上超越多个基线方法，且训练过程更一致、更稳定。

Conclusion: 非对比式视觉-语言预训练可提供比对比方法更简单、更稳定且更有效的替代方案。

Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.

</details>


### [95] [Schrödinger-Inspired Time-Evolution for 4D Deformation Forecasting](https://arxiv.org/abs/2602.00661)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

TL;DR: 本文提出了一种受薛定谔方程启发的物理引导神经架构，用于复杂三维时空现象（4D）的预测，通过学习复值波函数并利用可微分薛定谔时间步进器进行演化，实现了稳定、可解释且解剖学一致的长期预测。


<details>
  <summary>Details</summary>
Motivation: 现有无约束神经预测模型在长时序4D预测中易出现漂移和误差累积，缺乏物理一致性与可解释性，尤其在医学成像等对解剖保真度要求高的任务中表现受限。

Method: 构建基于卷积的深度网络，从体数据序列中学习体素级振幅、相位和势场，构成复值波函数ψ = A e^{iφ}，并通过可微分、展开式的薛定谔时间演化算子进行前向时间推进。

Result: 在模拟真实形变与拓扑变化的合成基准上，实现了高精度、高稳定性的4D状态（包括体素强度与形变场）预测；具备天然的形变合成能力，保障医学图像解剖一致性。

Conclusion: 该方法是首个端到端嵌入薛定谔型演化算子的4D神经预测框架，融合了深度学习的表达力与物理建模的鲁棒性与可解释性，为可解释、稳定、解剖一致的时空预测提供了新范式。

Abstract: Spatiotemporal forecasting of complex three-dimensional phenomena (4D: 3D + time) is fundamental to applications in medical imaging, fluid and material dynamics, and geophysics. In contrast to unconstrained neural forecasting models, we propose a Schrödinger-inspired, physics-guided neural architecture that embeds an explicit time-evolution operator within a deep convolutional framework for 4D prediction. From observed volumetric sequences, the model learns voxelwise amplitude, phase, and potential fields that define a complex-valued wavefunction $ψ= A e^{iφ}$, which is evolved forward in time using a differentiable, unrolled Schrödinger time stepper. This physics-guided formulation yields several key advantages: (i) temporal stability arising from the structured evolution operator, which mitigates drift and error accumulation in long-horizon forecasting; (ii) an interpretable latent representation, where phase encodes transport dynamics, amplitude captures structural intensity, and the learned potential governs spatiotemporal interactions; and (iii) natural compatibility with deformation-based synthesis, which is critical for preserving anatomical fidelity in medical imaging applications. By integrating physical priors directly into the learning process, the proposed approach combines the expressivity of deep networks with the robustness and interpretability of physics-based modeling. We demonstrate accurate and stable prediction of future 4D states, including volumetric intensities and deformation fields, on synthetic benchmarks that emulate realistic shape deformations and topological changes. To our knowledge, this is the first end-to-end 4D neural forecasting framework to incorporate a Schrödinger-type evolution operator, offering a principled pathway toward interpretable, stable, and anatomically consistent spatiotemporal prediction.

</details>


### [96] [Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds](https://arxiv.org/abs/2602.00807)
*Xianzhe Fan,Shengliang Deng,Xiaoyang Wu,Yuxiang Lu,Zhuoling Li,Mi Yan,Yujia Zhang,Zhizheng Zhang,He Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出Any3D-VLA，通过融合仿真、传感器与模型估计的点云，在VLA模型中引入3D信息，缓解2D视觉局限与跨环境域偏移问题，提升空间理解与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型仅使用2D图像作为视觉输入，导致在复杂场景中空间理解受限；同时缺乏高质量3D训练数据且存在跨环境域差异与深度尺度偏差问题。

Method: 提出Any3D-VLA框架，统一处理仿真器、传感器和模型估计生成的点云，构建多样化3D输入，并学习域无关的3D表征，再与2D表征进行融合。

Result: 在仿真与真实世界实验中，Any3D-VLA显著提升了任务性能，并有效缓解了域偏移问题。

Conclusion: 显式引入并融合多源点云可增强VLA模型的空间感知能力，Any3D-VLA为3D增强的具身智能提供了可行且鲁棒的范式。

Abstract: Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.

</details>


### [97] [Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation](https://arxiv.org/abs/2602.00669)
*Marina Crespo Aguirre,Jonathan Williams-Ramirez,Dina Zemlyanker,Xiaoling Hu,Lucas J. Deden-Binder,Rogeny Herisse,Mark Montine,Theresa R. Connors,Christopher Mount,Christine L. MacDonald,C. Dirk Keene,Caitlin S. Latimer,Derek H. Oakley,Bradley T. Hyman,Ana Lawry Aguila,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 本文提出了一种计算高效的超分辨率方法，用于从高各向异性（厚切片）的2D解剖照片重建中生成各向同性的3D脑体积，提升自动分割、皮层表面重建和MRI配准的精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D解剖照片的3D重建方法在高各向异性（如厚切片）条件下重建结构过于平滑、细节粗糙，影响形态测量精度和神经病理与神经影像的桥接。

Method: 引入一种基于域随机化合成数据训练的超分辨率插值步骤，将各向异性3D重建升采样为解剖一致的各向同性体积。

Result: 插值后体积显著提升自动分割Dice分数（尤其皮层与白质），并改善皮层表面重建精度和MRI图谱配准效果。

Conclusion: 该方法增强了基于照片的3D重建的分辨率与解剖保真度，有效弥合神经病理学与神经影像学之间的鸿沟，代码已开源。

Abstract: Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon

</details>


### [98] [VVLoc: Prior-free 3-DoF Vehicle Visual Localization](https://arxiv.org/abs/2602.00810)
*Ze Huang,Zhongyang Xiao,Mingliang Song,Longan Yang,Hongyuan Yuan,Li Sun*

Main category: cs.CV

TL;DR: 本文提出VVLoc，一种基于多摄像头系统的统一神经网络框架，可同时实现拓扑与度量车辆定位，并提供置信度估计，训练仅需图像对和对应真值位姿。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常独立处理拓扑与度量定位、依赖单目相机、需要额外3D语义或位姿先验，且缺乏定位结果置信度量化机制，难以满足工业落地需求。

Method: 提出VVLoc统一框架：使用单个神经网络处理多相机输入，通过视觉观测间地理邻近性评估、匹配策略估计相对度量位姿，并同步输出定位置信度；训练仅需图像对及对应真值位姿。

Result: 在公开数据集及自建更具挑战性的数据集上均达到当前最优定位精度，验证了其在多种定位任务中的泛化能力与鲁棒性。

Conclusion: VVLoc实现了高效、统一、可信的多相机车辆定位，显著提升了实际工业应用可行性。

Abstract: Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.

</details>


### [99] [HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression](https://arxiv.org/abs/2602.00671)
*Yangzhi Ma,Bojun Liu,Wenting Liao,Dong Liu,Zhu Li,Li Li*

Main category: cs.CV

TL;DR: 本文提出HPC框架，通过分层点基潜在表示和神经网络参数跨帧相关性挖掘，显著提升动态高斯泼溅的流式压缩效率，在保持高质量渲染的同时减少67%存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有动态高斯泼溅流式压缩方法在结构化网格或非结构化点基潜在表示上存在参数冗余或紧凑性不足的问题，难以兼顾高渲染质量与小内存占用。

Method: 提出HPC框架：1）采用基于每个高斯体的分层点基潜在表示，避免空闲空间建模；2）设计定制聚合方案提升潜在点紧凑性；3）首次探索并利用神经网络参数的帧间相关性进行压缩，结合潜在表示压缩构建端到端压缩系统。

Result: HPC在多项实验中显著超越当前最优方法，相较基线实现67%的存储缩减，同时保持高重建保真度。

Conclusion: HPC有效缓解了动态高斯泼溅流式传输中质量与压缩率之间的矛盾，为高效自由视角视频传输提供了新范式。

Abstract: While dynamic Gaussian Splatting has driven significant advances in free-viewpoint video, maintaining its rendering quality with a small memory footprint for efficient streaming transmission still presents an ongoing challenge. Existing streaming dynamic Gaussian Splatting compression methods typically leverage a latent representation to drive the neural network for predicting Gaussian residuals between frames. Their core latent representations can be categorized into structured grid-based and unstructured point-based paradigms. However, the former incurs significant parameter redundancy by inevitably modeling unoccupied space, while the latter suffers from limited compactness as it fails to exploit local correlations. To relieve these limitations, we propose HPC, a novel streaming dynamic Gaussian Splatting compression framework. It employs a hierarchical point-based latent representation that operates on a per-Gaussian basis to avoid parameter redundancy in unoccupied space. Guided by a tailored aggregation scheme, these latent points achieve high compactness with low spatial redundancy. To improve compression efficiency, we further undertake the first investigation to compress neural networks for streaming dynamic Gaussian Splatting through mining and exploiting the inter-frame correlation of parameters. Combined with latent compression, this forms a fully end-to-end compression framework. Comprehensive experimental evaluations demonstrate that HPC substantially outperforms state-of-the-art methods. It achieves a storage reduction of 67% against its baseline while maintaining high reconstruction fidelity.

</details>


### [100] [Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025](https://arxiv.org/abs/2602.00982)
*Phu-Hoa Pham,Chi-Nguyen Tran,Dao Sy Duy Minh,Nguyen Lam Phu Quy,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: 本文介绍了Team HCMUS_TheFangs在NeurIPS 2025“Mouse vs. AI”竞赛中的获胜方案，分别针对视觉鲁棒性（轻量CNN+GLU）和神经对齐（深层ResNet-like+GLU）设计模型，并发现训练步数与性能呈非单调关系，挑战了模型越复杂越好的传统认知。


<details>
  <summary>Details</summary>
Motivation: 解决人工视觉代理在视觉鲁棒性和神经对齐方面的关键挑战，使其更接近生物视觉系统。

Method: Track 1采用轻量两层CNN结合门控线性单元（GLU）和观测归一化；Track 2构建16层卷积ResNet-like架构并引入GLU门控；系统分析10个不同训练步数（60K–1.14M）的检查点，并开展消融研究与失败案例分析。

Result: Track 1视觉鲁棒性达95.4%最终得分；Track 2神经预测top-1性能最优，参数量1780万；发现约200K步时性能最佳，训练步数与性能呈非单调关系。

Conclusion: 架构简洁性有利于视觉鲁棒性，而更大容量的深层模型更利于神经对齐；训练时长需权衡，非越长越好；结果为构建鲁棒、类脑视觉智能体提供了新思路与实践指导。

Abstract: Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.

</details>


### [101] [Video Understanding: Through A Temporal Lens](https://arxiv.org/abs/2602.00683)
*Thong Thanh Nguyen*

Main category: cs.CV

TL;DR: 本文探讨了如何利用视频元素间的时间关系来提升视频理解能力，提出了五项创新贡献，包括自动标注框架、参数高效微调策略、长视频建模的State Space Layers、细粒度动作-时刻对比学习框架，以及针对大视觉语言模型的时间推理瓶颈分析与改进方案。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法在建模时间关系方面存在局限，难以有效捕捉视频中动态、连续的本质特性。

Method: 提出五种方法：(1) 基于大视觉语言模型与噪声鲁棒对比学习的自动标注框架；(2) 使用'循环适配器'的参数高效微调策略；(3) 引入State Space Layers进行长视频建模，并构建两个新长期基准；(4) 设计显式建模动作与视频时刻间细粒度关系的对比学习框架；(5) 对大视觉语言模型开展实证研究，识别视觉-语言接口为时间推理瓶颈，并提出‘面向时间的优化方案’。

Result: 实证表明，显式时间建模显著提升了模型对视频流式内容的表征与推理能力。

Conclusion: 显式建模视频中的时间关系是提升视频理解性能的关键路径，各项技术贡献共同验证了该观点的有效性与实用性。

Abstract: This thesis explores the central question of how to leverage temporal relations among video elements to advance video understanding. Addressing the limitations of existing methods, the work presents a five-fold contribution: (1) an automatic annotation framework that utilizes large vision-language models and a noise-robust contrastive learning objective with a subtractive angular margin; (2) a parameter-efficient fine-tuning strategy using "recurrent adapters" to capture temporal dynamics in low-data regimes; (3) the integration of State Space Layers (SSL) for efficient long-form video modeling, supported by the introduction of two new long-term benchmarks for egocentric and feature-length content; (4) a novel contrastive learning framework designed to explicitly model fine-grained relations between motions and video moments; and (5) a comprehensive empirical study on Large Vision-Language Models (LVLMs) that identifies the visual-language interface as a bottleneck for temporal reasoning, leading to a new "temporal-oriented recipe" for upscaled video understanding. Collectively, these contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about the fluid nature of video content.

</details>


### [102] [V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication](https://arxiv.org/abs/2602.00687)
*Yuankun Zeng,Shaohui Li,Zhi Li,Shulan Ruan,Yu Liu,You He*

Main category: cs.CV

TL;DR: 本文提出V2X-DSC框架，利用分布式信源编码思想，在带宽受限下通过条件编解码实现多智能体BEV特征的高效协同感知，显著提升精度-带宽权衡性能。


<details>
  <summary>Details</summary>
Motivation: 中间特征共享面临严格的带宽限制，而多智能体观测具有强相关性，接收端只需获取超出本地上下文的新息信息。

Method: 提出基于条件编解码器（DCC）的V2X-DSC框架：发送端压缩BEV特征为紧凑码，接收端以本地特征为边信息进行条件重建，按互补性而非冗余性分配比特，并利用条件结构正则化学习过程。

Result: 在DAIR-V2X、OPV2V和V2X-Real数据集上，KB级通信下达到SOTA精度-带宽权衡；可即插即用适配多种融合骨干网络。

Conclusion: V2X-DSC通过引入分布式信源编码视角与条件重建机制，有效缓解带宽瓶颈，提升协同感知鲁棒性与效率，为车路协同感知提供了可扩展的通信层设计范式。

Abstract: Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones.

</details>


### [103] [Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs](https://arxiv.org/abs/2602.01158)
*Daniel Yezid Guarnizo Orjuela,Leonardo Scappatura,Veronica Di Gennaro,Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.CV

TL;DR: 本文提出了一种名为Corruption Restoration Transformer (CRT)的即插即用、模型无关的视觉Transformer，用于增强Vision-Language-Action (VLA)模型对传感器级图像损坏（如噪声、坏点、镜头污染）的鲁棒性，显著恢复其在真实场景中的操作成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在真实世界部署中因图像传感器级损坏（如电子噪声、死像素、镜头污染）而严重失效，该问题尚未被充分研究。

Method: 提出Corruption Restoration Transformer (CRT)，采用对抗训练目标，从损坏图像中恢复干净观测，无需对底层VLA模型进行昂贵微调。

Result: CRT在LIBERO和Meta-World基准上显著提升VLA模型鲁棒性，使π₀.₅和SmolVLA等模型在严重图像损坏下仍保持接近基线的成功率（从2%恢复至近90%）。

Conclusion: CRT是一种高效、通用且即插即用的解决方案，能有效提升VLA模型在真实机器人场景中对视觉干扰的鲁棒性，推动其实际落地应用。

Abstract: Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\% success rates to as low as 2\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.

</details>


### [104] [JoyAvatar: Unlocking Highly Expressive Avatars via Harmonized Text-Audio Conditioning](https://arxiv.org/abs/2602.00702)
*Ruikui Wang,Jinheng Feng,Lang Tian,Huaishao Luo,Chaochao Li,Liangbo Zhou,Huan Zhang,Youzheng Wu,Xiaodong He*

Main category: cs.CV

TL;DR: 本文提出JoyAvatar框架，通过双教师增强训练算法和多模态条件动态调制，显著提升视频头像模型对复杂文本指令（如全身运动、动态摄像机轨迹、背景切换、人-物交互）的可控性与生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频头像模型在复杂文本指令（如大范围全身动作、动态摄像机轨迹、背景切换、人-物交互）下的文本对齐能力有限。

Method: 提出JoyAvatar框架，包含两个关键技术：1）双教师增强训练算法，兼顾基础模型的文本可控性与音视频同步学习；2）在训练中依据去噪时间步动态调节音频、文本等多模态条件强度，缓解异构条件信号冲突。

Result: 在GSB评估中超越Omnihuman-1.5和KlingAvatar 2.0等SOTA模型，支持长时长、自然连贯的全身动作与动态摄像机运动，保持精准唇动同步与身份一致性，并拓展至多人对话与非人类角色扮演等复杂应用。

Conclusion: JoyAvatar有效突破了现有视频头像模型在复杂文本驱动下的可控性瓶颈，为高保真、高可控性虚拟人视频生成提供了新范式。

Abstract: Existing video avatar models have demonstrated impressive capabilities in scenarios such as talking, public speaking, and singing. However, the majority of these methods exhibit limited alignment with respect to text instructions, particularly when the prompts involve complex elements including large full-body movement, dynamic camera trajectory, background transitions, or human-object interactions. To break out this limitation, we present JoyAvatar, a framework capable of generating long duration avatar videos, featuring two key technical innovations. Firstly, we introduce a twin-teacher enhanced training algorithm that enables the model to transfer inherent text-controllability from the foundation model while simultaneously learning audio-visual synchronization. Secondly, during training, we dynamically modulate the strength of multi-modal conditions (e.g., audio and text) based on the distinct denoising timestep, aiming to mitigate conflicts between the heterogeneous conditioning signals. These two key designs serve to substantially expand the avatar model's capacity to generate natural, temporally coherent full-body motions and dynamic camera movements as well as preserve the basic avatar capabilities, such as accurate lip-sync and identity consistency. GSB evaluation results demonstrate that our JoyAvatar model outperforms the state-of-the-art models such as Omnihuman-1.5 and KlingAvatar 2.0. Moreover, our approach enables complex applications including multi-person dialogues and non-human subjects role-playing. Some video samples are provided on https://joyavatar.github.io/.

</details>


### [105] [OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth](https://arxiv.org/abs/2602.01268)
*Jaehyeon Cho,Jhonghyun An*

Main category: cs.CV

TL;DR: 本文提出了一种结合单目基础模型相对深度与稀疏测距数据的方法，生成伪度量深度先验，并设计细化网络在少量标注样本下实现高精度度量深度估计，适用于真实场景中标注稀缺的情况。


<details>
  <summary>Details</summary>
Motivation: 单目基础模型虽能零样本估计深度，但输出为相对深度，无法直接用于机器人和自动驾驶等需要度量深度的任务。

Method: 利用相对深度保持全局布局和边界的特性，通过稀疏范围测量对其进行校准，生成伪度量深度先验；再设计一个细化网络，在可靠区域遵循该先验，在必要区域进行偏差调整。

Result: 所提方法在极少标注样本下仍能实现准确的度量深度预测，尤其在缺乏精心筛选验证数据时，保持稳定的尺度和锐利的边缘。

Conclusion: 将基础模型先验与稀疏锚点耦合，是应对现实世界中标签稀缺、实现鲁棒且可部署深度补全的一种实用路径。

Abstract: Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.

</details>


### [106] [StomataSeg: Semi-Supervised Instance Segmentation for Sorghum Stomatal Components](https://arxiv.org/abs/2602.00703)
*Zhongtian Huang,Zhi Chen,Zi Huang,Xin Yu,Daniel Smith,Chaitanya Purushothama,Erik Van Oosterom,Alex Wu,William Salter,Yan Li,Scott Chapman*

Main category: cs.CV

TL;DR: 本文提出一种面向高粱气孔组件的半监督实例分割框架，通过构建含11,060张人工标注图像块的数据集、采用重叠分块与伪标签策略扩充数据（新增56,428张），显著提升小目标气孔结构的分割精度（实例分割AP从28.30%升至46.10%），推动作物表型智能分析规模化应用。


<details>
  <summary>Details</summary>
Motivation: 高粱气孔微小（常<40μm）、形态多变，且现有自动分割方法在嵌套小结构识别和标注瓶颈方面存在困难，亟需高效、精准的气孔表型分析方法以支持抗旱育种。

Method: 构建包含11,060张人工标注图像块的高粱叶片图像数据集，覆盖三种气孔组分（气孔孔、保卫细胞、复合区）及多基因型、叶面；采用高分辨率显微图像重叠分块策略提升小目标检测能力；引入伪标签法对未标注图像生成56,428张伪标注图像块；设计适配气孔特性的半监督实例分割框架。

Result: 语义分割最高mIoU从65.93%提升至70.35%，实例分割最高AP从28.30%显著提升至46.10%，验证了分块预处理与半监督学习联合策略对精细气孔结构分割的有效性。

Conclusion: 所提半监督实例分割框架可有效支撑高通量、可扩展的气孔性状提取，为AI驱动的作物表型研究提供新范式，助力气候韧性农业发展。

Abstract: Sorghum is a globally important cereal grown widely in water-limited and stress-prone regions. Its strong drought tolerance makes it a priority crop for climate-resilient agriculture. Improving water-use efficiency in sorghum requires precise characterisation of stomatal traits, as stomata control of gas exchange, transpiration and photosynthesis have a major influence on crop performance. Automated analysis of sorghum stomata is difficult because the stomata are small (often less than 40 $μ$m in length in grasses such as sorghum) and vary in shape across genotypes and leaf surfaces. Automated segmentation contributes to high-throughput stomatal phenotyping, yet current methods still face challenges related to nested small structures and annotation bottlenecks. In this paper, we propose a semi-supervised instance segmentation framework tailored for analysis of sorghum stomatal components. We collect and annotate a sorghum leaf imagery dataset containing 11,060 human-annotated patches, covering the three stomatal components (pore, guard cell and complex area) across multiple genotypes and leaf surfaces. To improve the detection of tiny structures, we split high-resolution microscopy images into overlapping small patches. We then apply a pseudo-labelling strategy to unannotated images, producing an additional 56,428 pseudo-labelled patches. Benchmarking across semantic and instance segmentation models shows substantial performance gains: for semantic models the top mIoU increases from 65.93% to 70.35%, whereas for instance models the top AP rises from 28.30% to 46.10%. These results demonstrate that combining patch-based preprocessing with semi-supervised learning significantly improves the segmentation of fine stomatal structures. The proposed framework supports scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science.

</details>


### [107] [Supervised makeup transfer with a curated dataset: Decoupling identity and makeup features for enhanced transformation](https://arxiv.org/abs/2602.00729)
*Qihe Pan,Yiming Wu,Xing Zhao,Liang Xie,Guodao Sun,Ronghua Liang*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的妆容迁移方法，通过构建高质量数据集、设计身份与妆容特征解耦框架以及引入文本引导机制，提升了生成图像的真实性、身份保持性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有妆容迁移方法受限于数据集规模小、身份与妆容特征难以解耦、控制粒度粗等问题。

Method: 1）采用‘训练-生成-过滤-再训练’策略构建高质量合成数据集；2）设计基于扩散模型的解耦框架，分离身份与妆容特征；3）引入文本引导机制，支持眼部、唇部、面部等区域的细粒度自然语言控制。

Result: 在基准测试与真实场景实验中，该方法在图像保真度、身份保持性和编辑灵活性方面均优于现有方法。

Conclusion: 所提方法有效缓解了妆容迁移任务中数据稀缺、特征耦合与控制困难三大挑战，为可控图像生成提供了新思路。

Abstract: Diffusion models have recently shown strong progress in generative tasks, offering a more stable alternative to GAN-based approaches for makeup transfer. Existing methods often suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability. To address these issues, we make three contributions. First, we construct a curated high-quality dataset using a train-generate-filter-retrain strategy that combines synthetic, realistic, and filtered samples to improve diversity and fidelity. Second, we design a diffusion-based framework that disentangles identity and makeup features, ensuring facial structure and skin tone are preserved while applying accurate and diverse cosmetic styles. Third, we propose a text-guided mechanism that allows fine-grained and region-specific control, enabling users to modify eyes, lips, or face makeup with natural language prompts. Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility. Examples of our dataset can be found at: https://makeup-adapter.github.io.

</details>


### [108] [Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries](https://arxiv.org/abs/2602.00739)
*Zhengyan Qin,Liyuan Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散的算法，用于从双层点云中分离内层和外层表面，特别针对TSDF融合中因截断不对称导致的‘双表面伪影’问题。该方法适用于具有开放边界的点云，能稳健处理封闭和开放边界模型，在约10秒内完成2万内层+2万外层点的分离，作为轻量级后处理模块，不替代完整重建流程。


<details>
  <summary>Details</summary>
Motivation: 解决TSDF融合中因不对称截断阈值引起的双表面伪影问题，该伪影导致错误的内外壳结构，影响后续如室内建模和医学成像等应用中的表面精度。

Method: 提出一种基于扩散过程的算法，专门针对具有开放边界（即存在拓扑孔洞）的双层点云，进行内外层表面分离；不依赖变分或学习方法，而是作为TSDF融合后的轻量级后处理模块。

Result: 可在约10秒内从20,000个内层点和20,000个外层点组成的双层点云中准确提取真实内层表面；支持watertight与开放边界几何，且在室内场景建模与医学影像等实际任务中表现有效。

Conclusion: 该扩散算法是一种高效、鲁棒的后处理方案，专为缓解TSDF融合引发的双表面伪影而设计，适用于需高保真表面表示的实际场景，且不改变原有重建流程架构。

Abstract: We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the "double surface artifact" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.

</details>


### [109] [Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss](https://arxiv.org/abs/2602.01673)
*Enguang Fan*

Main category: cs.CV

TL;DR: 本文评估了NetVLAD作为回环检测（LCD）模块在SLAM中的性能，相比传统DBoW方法，在KITTI数据集上实现了更高的精度和鲁棒性，并借助Faiss加速达到实时查询速度，成为实用的替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统词袋（DBoW）方法在外观变化和感知混叠下性能下降；而深度学习VPR描述子（如NetVLAD）虽鲁棒性强，但计算开销常被认为阻碍其实时SLAM应用。本文旨在验证NetVLAD是否可在保持实时性的同时提升LCD性能。

Method: 在KITTI数据集上实证评估NetVLAD作为LCD模块，与DBoW对比；提出细粒度Top-K精确率-召回率曲线以更贴合LCD中零个或多个匹配的真实场景；采用Faiss加速最近邻搜索。

Result: NetVLAD借助Faiss实现实时查询速度，同时在精度和鲁棒性上优于DBoW。

Conclusion: NetVLAD可作为SLAM中LCD模块的一种实用、即插即用的替代方案，在不牺牲实时性的前提下显著提升性能。

Abstract: Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.

</details>


### [110] [HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression](https://arxiv.org/abs/2602.00749)
*Xiangming Wang,Benteng Sun,Yungeng Liu,Haijin Zeng,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

TL;DR: 本文提出HSI-VAR，将高光谱图像（HSI）恢复重新建模为自回归生成问题，通过潜变量对齐、退化感知引导和空谱自适应模块，在保证结构细节的同时大幅提升推理速度与性能。


<details>
  <summary>Details</summary>
Motivation: 现有HSI恢复方法存在计算开销大（如扩散模型需数百步迭代）或恢复结果过度平滑（如回归模型）的问题，难以兼顾效率与细节保真。

Method: 提出HSI-VAR框架：（1）潜变量-条件对齐确保语义一致性；（2）退化感知引导将混合退化编码为嵌入空间中的线性组合，实现自动控制并降低50%计算成本；（3）空谱自适应模块在解码阶段联合优化空间与光谱细节。

Result: 在9个全场景HSI恢复基准上达到SOTA：ICVL数据集PSNR提升3.77 dB，结构保持更优，推理速度相比扩散模型最高加速95.5倍。

Conclusion: HSI-VAR以自回归范式突破效率-质量权衡瓶颈，为真实场景HSI恢复提供了高效、精准、实用的新方案。

Abstract: Hyperspectral images (HSIs) capture richer spatial-spectral information beyond RGB, yet real-world HSIs often suffer from a composite mix of degradations, such as noise, blur, and missing bands. Existing generative approaches for HSI restoration like diffusion models require hundreds of iterative steps, making them computationally impractical for high-dimensional HSIs. While regression models tend to produce oversmoothed results, failing to preserve critical structural details. We break this impasse by introducing HSI-VAR, rethinking HSI restoration as an autoregressive generation problem, where spectral and spatial dependencies can be progressively modeled rather than globally reconstructed. HSI-VAR incorporates three key innovations: (1) Latent-condition alignment, which couples semantic consistency between latent priors and conditional embeddings for precise reconstruction; (2) Degradation-aware guidance, which uniquely encodes mixed degradations as linear combinations in the embedding space for automatic control, remarkably achieving a nearly $50\%$ reduction in computational cost at inference; (3) A spatial-spectral adaptation module that refines details across both domains in the decoding phase. Extensive experiments on nine all-in-one HSI restoration benchmarks confirm HSI-VAR's state-of-the-art performance, achieving a 3.77 dB PSNR improvement on \textbf{\textit{ICVL}} and offering superior structure preservation with an inference speed-up of up to $95.5 \times$ compared with diffusion-based methods, making it a highly practical solution for real-world HSI restoration.

</details>


### [111] [DDP-WM: Disentangled Dynamics Prediction for Efficient World Models](https://arxiv.org/abs/2602.01780)
*Shicheng Yin,Kaixuan Yin,Weixing Chen,Yang Liu,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出DDP-WM，一种基于解耦动力学预测（DDP）的高效世界模型，通过分离物理主导的主动力学与背景驱动的次级更新，在保持高保真度的同时显著提升推理速度与规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的稠密世界模型计算开销大，难以实现实时部署，亟需解决效率-性能瓶颈。

Method: 提出解耦动力学预测（DDP）原理，设计DDP-WM架构：结合高效历史处理与动态定位以提取主动力学，并采用交叉注意力机制处理背景更新，优化资源分配与优化景观。

Result: 在导航、桌面精准操作及复杂形变/多体交互等任务中显著提升效率与性能；在Push-T任务上实现约9倍推理加速，MPC成功率从90%提升至98%。

Conclusion: DDP-WM为构建高效、高保真世界模型提供了新范式，推动自主机器人实时规划发展。

Abstract: World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.

</details>


### [112] [Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints](https://arxiv.org/abs/2602.00763)
*Dylan Yves,Khush Agarwal,Jonathan Hoyin Chan,Patcharapit Promoppatum,Aroonkamon Pattanasiricharoen*

Main category: cs.CV

TL;DR: 本研究评估了基于U-Net的深度学习方法在臂丛超声图像神经分割中的性能，重点分析了数据集构成与标注策略对结果的影响；发现多设备数据联合训练具正则化效果但未必优于单源匹配训练，多类别监督会显著降低神经分割精度，且神经尺寸与分割准确率呈中等正相关。


<details>
  <summary>Details</summary>
Motivation: 超声引导下区域麻醉中，手动识别神经困难，因图像对比度低、斑点噪声强及解剖结构个体差异大，亟需鲁棒的自动神经分割方法。

Method: 采用U-Net架构进行臂丛超声图像神经分割，系统比较不同超声设备（SIEMENS ACUSON NX3 Elite与Philips EPIQ5）数据组合方式、二分类与多类别（动脉/静脉/神经/肌肉）标注策略的影响，并分析神经尺寸与分割精度的相关性。

Result: 多设备联合训练可提升低性能设备数据上的泛化能力，但目标域匹配的单源训练仍最优；多类别监督导致神经Dice分数下降9%–61%；神经尺寸与分割准确率呈中等正相关（r=0.587, p<0.001）。

Conclusion: 数据来源匹配与二分类标注更利于神经分割性能；多类别扩展需谨慎应对类别不平衡与边界模糊问题；小神经分割仍是关键挑战，模型设计应针对性增强小目标敏感性。

Abstract: Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.

</details>


### [113] [LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation](https://arxiv.org/abs/2602.02220)
*Bo Miao,Weijia Liu,Jun Luo,Lachlan Shinnick,Jian Liu,Thomas Hamilton-Smith,Yuhe Yang,Zijie Wu,Vanja Videnovic,Feras Dayoub,Anton van den Hengel*

Main category: cs.CV

TL;DR: 本文提出了HieraNav多粒度、开放词汇目标导航任务和LangMap大规模基准，用于评估AI在真实3D室内环境中基于自然语言指令进行分层导航的能力。


<details>
  <summary>Details</summary>
Motivation: 解决人与AI之间通过语言与物体建立有意义关系的关键问题，推动具身智能中语言驱动导航的发展。

Method: 构建HieraNav任务（覆盖场景、房间、区域、实例四层语义目标）及LangMap基准（基于真实3D扫描，含区域/实例标注、描述性文本和18K导航任务），并开展零样本与监督模型的全面评测。

Result: LangMap标注质量优于GOAT-Bench 23.8%（用词少4倍）；实验证明上下文与记忆增强可提升成功率，但长尾、小尺寸、依赖上下文及远距离或多目标导航仍具挑战。

Conclusion: HieraNav与LangMap为语言驱动的具身导航提供了严格、实用且可扩展的评测基准。

Abstract: The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap

</details>


### [114] [DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning](https://arxiv.org/abs/2602.00795)
*Wenhao Li,Xianjing Meng,Qiangchang Wang,Zhongyi Han,Zhibin Wu,Yilong Yin*

Main category: cs.CV

TL;DR: 本文提出DVLA-RL方法，通过双层级语义构建（DSC）和强化学习门控注意力（RLA），实现视觉与语言从低层到高层的渐进式、自适应对齐，显著提升少样本学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的少样本学习方法忽视了视觉与语言在不同语义层级上的渐进式、自适应对齐，导致语义增益有限。

Method: 提出DVLA-RL框架，包含Dual-level Semantic Construction（DSC）和RL-gated Attention（RLA）：DSC利用LLM结合类别名与支持样本生成并筛选判别性属性，合成层次化类描述；RLA将跨模态融合建模为序列决策过程，用轻量级REINFORCE策略动态调控各网络层中自注意力与交叉注意力的权重。

Result: 在三种不同少样本学习场景下的九个基准上达到新的SOTA性能。

Conclusion: DVLA-RL通过分层语义构造与强化学习驱动的动态跨模态融合，实现了更精准的视觉-语言对齐，在仅需少量支持样本下获得强泛化能力和类别判别力。

Abstract: Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.

</details>


### [115] [Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.00813)
*Tong Wang,Yunhan Zhao,Shu Kong*

Main category: cs.CV

TL;DR: 本文提出Paracosm方法，通过大语言多模态模型（LMM）直接生成查询所描述的“心理图像”，并为数据库中每张真实图像生成对应的合成图像以缩小域差距，从而实现零样本组合图像检索（CIR），无需训练且在多个基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有CIR方法依赖LMM生成文本描述再用VLM匹配，但“心理图像”不可见且隐式定义，导致匹配不准确；本文希望从第一性原理出发，直接生成并匹配该心理图像。

Method: 使用LMM根据参考图像和修改文本生成“心理图像”；同时为数据库中每张真实图像生成对应的合成图像，构建统一的合成域（即‘paracosm’）进行匹配；整个过程无需训练，属零样本方法。

Result: Paracosm在四个具有挑战性的基准测试上显著优于现有零样本方法，达到零样本CIR的最先进性能。

Conclusion: 直接建模并匹配‘心理图像’是更有效的零样本CIR路径；Paracosm验证了基于LMM构建合成匹配空间的可行性与优越性，且完全免训练。

Abstract: Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image'', based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image'' is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image'' for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image'' for a given multimodal query and propose to use this ``mental image'' to search for the target image. As the ``mental image'' has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm'', where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.

</details>


### [116] [Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis](https://arxiv.org/abs/2602.00821)
*Konstantinos Moutselos,Ilias Maglogiannis*

Main category: cs.CV

TL;DR: 本文提出了一种面向临床皮肤科联邦学习的、无需图像反演的隐私保护框架FlowEdit，通过Rectified Flow Transformers实现身份匿名化同时保持病理特征，并利用'Segment-by-Synthesis'机制生成健康/病变配对图像以提取与生物特征无关的红斑差异掩码，在边缘设备上实现实时、高保真、隐私合规的皮肤图像分析。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在临床皮肤科应用中面临患者隐私保护与诊断特征保留之间的矛盾：传统去标识化方法损害病理保真度，而现有生成编辑方法依赖计算密集型反演过程，不适用于资源受限的边缘设备。

Method: 提出基于无反演Rectified Flow Transformers（FlowEdit）的身份无关病理保持框架；引入'Segment-by-Synthesis'机制，在客户端本地生成健康与病变的对抗性孪生图像对，进而提取解耦于生物标识符和语义干扰物（如首饰）的差异性红斑掩码。

Result: 在高分辨率临床样本上的试点验证显示，合成身份间红斑掩码的IoU稳定性大于0.67；系统可在20秒内完成高保真身份转换，支持边缘部署；有效缓解梯度泄露风险。

Conclusion: 该框架为联邦环境下高精度、隐私安全的皮肤图像分析提供了可行路径，兼具实时性、病理保真性与边缘可部署性。

Abstract: The deployment of Federated Learning (FL) for clinical dermatology is hindered by the competing requirements of protecting patient privacy and preserving diagnostic features. Traditional de-identification methods often degrade pathological fidelity, while standard generative editing techniques rely on computationally intensive inversion processes unsuitable for resource-constrained edge devices. We propose a framework for identity-agnostic pathology preservation that serves as a client-side privacy-preserving utility. By leveraging inversion-free Rectified Flow Transformers (FlowEdit), the system performs high-fidelity identity transformation in near real-time (less than 20s), facilitating local deployment on clinical nodes. We introduce a "Segment-by-Synthesis" mechanism that generates counterfactual healthy and pathological twin pairs locally. This enables the extraction of differential erythema masks that are decoupled from biometric markers and semantic artifacts (e.g. jewelry). Pilot validation on high-resolution clinical samples demonstrates an Intersection over Union (IoU) stability greater than 0.67 across synthetic identities. By generating privacy-compliant synthetic surrogates at the edge, this framework mitigates the risk of gradient leakage at the source, providing a secure pathway for high-precision skin image analysis in federated environments.

</details>


### [117] [TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation](https://arxiv.org/abs/2602.00839)
*Mingwei Li,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出TransNormal框架，利用预训练扩散先验进行单步法向量回归，结合DINOv3语义特征与多任务学习及小波正则化，显著提升透明物体单目法向估计精度，并发布高质量合成数据集TransNormal-Synthetic。


<details>
  <summary>Details</summary>
Motivation: 单目透明物体法向估计对实验室自动化至关重要，但因光的折射与反射复杂，传统深度与法向传感器易失效，限制具身AI在科研环境中的部署。

Method: 提出TransNormal框架：1）适配预训练扩散先验用于单步法向回归；2）通过跨注意力机制融合DINOv3密集视觉语义以增强几何线索；3）采用多任务学习目标与小波正则化保持细粒度结构细节。同时构建物理仿真数据集TransNormal-Synthetic。

Result: 在ClearGrasp基准上，平均误差降低24.4%，11.25°精度提升22.8%；在ClearPose上平均误差降低15.2%。

Conclusion: TransNormal有效解决了透明物体单目法向估计难题，在多个基准上显著超越现有方法，为科学自动化提供可靠几何感知基础。

Abstract: Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.

</details>


### [118] [Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition](https://arxiv.org/abs/2602.00841)
*Jintao Cheng,Weibin Li,Zhijian He,Jin Wu,Chi Man Vong,Wei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的二阶几何统计框架，用于视觉地点识别（VPR），通过在对称正定（SPD）流形上建模场景协方差描述子，并利用黎曼映射线性化嵌入，实现对环境与视角变化的鲁棒零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有VPR方法依赖大量标注数据或仅使用简单的一阶统计，忽视了场景内在的结构相关性，难以应对剧烈环境和视角变化。

Method: 构建基于SPD流形的二阶协方差描述子，利用几何感知的黎曼映射将其投影到线性化欧氏空间，整个框架基于固定预训练骨干网络，无需任何参数更新。

Result: 在多个基准上达到与最先进方法相当甚至更优的性能，尤其在零样本跨域场景中表现突出。

Conclusion: 二阶几何统计框架能有效捕获场景几何稳定性，提供一种轻量、通用且无需训练的VPR新范式。

Abstract: Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.

</details>


### [119] [Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware](https://arxiv.org/abs/2602.00865)
*Brandon Leblanc,Charalambos Poullis*

Main category: cs.CV

TL;DR: 本文提出Distill3R框架，通过知识蒸馏将大型3D基础模型的几何推理能力压缩至轻量级学生模型，使其可在单个工作站上高效训练，显著降低学术实验室的计算门槛。


<details>
  <summary>Details</summary>
Motivation: 大型多视角3D重建基础模型依赖海量算力训练，阻碍了大多数学术实验室的研究参与；亟需一种低算力、可复现、易部署的替代方案。

Method: 提出两方面创新：(1) 离线缓存流水线，将教师模型繁重推理与训练解耦，并生成压缩监督信号；(2) 基于教师不确定性设计的置信度感知蒸馏损失函数。构建72M参数学生模型，相比650M参数教师模型实现9倍参数压缩和5倍推理加速。

Result: 学生模型可在单个工作站3天内完成训练（教师需大规模GPU集群训练约一周），同时保持结构一致性和定性几何理解能力，支持功能级3D感知。

Conclusion: Distill3R为缺乏大规模算力的实验室提供了可复现、低成本的3D视觉研究基线与边缘部署方案，旨在推动3D视觉研究的民主化，而非挑战SOTA基础模型性能。

Abstract: While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.

</details>


### [120] [DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models](https://arxiv.org/abs/2602.00883)
*Alicja Polowczyk,Agnieszka Polowczyk,Piotr Borycki,Joanna Waczyńska,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: 本文提出DIAMOND方法，一种无需训练、不修改模型权重的推理时轨迹校正技术，用于在扩散模型（如FLUX）生成过程中实时抑制视觉与解剖学伪影，提升图像保真度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型（如FLUX）虽性能优异，但存在显著的视觉与解剖学伪影；当前去伪影方法多为后处理或需侵入式权重修改/区域精修，效率低且难以干预核心生成过程。

Method: DIAMOND是一种训练无关（training-free）的方法，在扩散生成轨迹的每一步重建干净样本估计，并据此动态校正潜在状态，从而在推理阶段主动规避导致伪影的路径；该方法可无缝适配标准扩散模型。

Result: DIAMOND在无需额外训练、不修改模型权重的前提下，实现了零样本（zero-shot）、高保真、无伪影的图像合成，在多种生成架构上验证了其有效性与鲁棒性。

Conclusion: DIAMOND为解决生成式模型中的伪影问题提供了高效、通用且即插即用的新范式，推动了文本到图像模型向专业实用化迈进。

Abstract: Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/

</details>


### [121] [OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection](https://arxiv.org/abs/2602.00904)
*Kunal Mahatha,Ali Bahri,Pierre Marza,Sahar Dastani,Maria Vakalopoulou,Stergios Christodoulidis,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本文提出OCTOPUS架构，通过在八个主方向上进行离散递归，解决了状态空间模型（SSMs）在视觉任务中因因果性假设导致的空间关系建模不足问题，兼顾全局上下文与局部空间结构，并保持线性计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 标准SSMs在视觉任务中表现受限，因其因果序列建模方式破坏图像固有的二维空间关系，难以捕捉局部像素/块间的相关性。

Method: OCTOPUS在水平、垂直及对角线共八个主方向上执行前向/后向离散递归，实现多方向信息交互，既维持空间连通区域间有效传播，又隔离无关图像块。

Result: 在分类与分割基准测试中，OCTOPUS显著提升边界保持能力与区域一致性，分割效果更优，同时分类精度优于现有V-SSM模型。

Conclusion: OCTOPUS是一种可扩展且高效的空间感知视觉建模范式，为多向递归机制在视觉模型中的应用奠定基础。

Abstract: State space models (SSMs) have recently emerged as an alternative to transformers due to their unique ability of modeling global relationships in text with linear complexity. However, their success in vision tasks has been limited due to their causal formulation, which is suitable for sequential text but detrimental in the spatial domain where causality breaks the inherent spatial relationships among pixels or patches. As a result, standard SSMs fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring ones that are visually correlated. To address these limitations, we introduce OCTOPUS , a novel architecture that preserves both global context and local spatial structure within images, while maintaining the linear complexity of SSMs. OCTOPUS performs discrete reoccurrence along eight principal orientations, going forward or backward in the horizontal, vertical, and diagonal directions, allowing effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This design enables multi-directional recurrence, capturing both global context and local spatial structure with SSM-level efficiency. In our classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency, as evident from the segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models. These results suggest that OCTOPUS appears as a foundation method for multi-directional recurrence as a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures.

</details>


### [122] [ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models](https://arxiv.org/abs/2602.00946)
*Dhruv Parikh,Haoyang Fan,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.CV

TL;DR: 本文提出ConsensusDrop框架，通过融合视觉编码器的显著性与大语言模型的跨注意力机制，实现高效的视觉令牌压缩，提升视觉-语言模型的推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型（VLMs）因处理大量冗余视觉令牌而开销巨大；当前令牌缩减方法仅利用单一信号（视觉编码器显著性或LLM跨注意力），但二者各有局限，单独使用效果不足。

Method: 提出无需训练的ConsensusDrop框架，通过协调视觉编码器显著性和查询感知的跨注意力信号生成共识排序，并对非关键令牌进行编码器引导的合并压缩。

Result: 在LLaVA-1.5/NeXT、Video-LLaVA等多个开源VLM上验证，ConsensusDrop在相同令牌预算下持续优于先前剪枝方法，显著降低首词生成时间（TTFT）和KV缓存占用，同时保持接近基线的准确率。

Conclusion: 融合多模态显著性信号并进行合理协调是提升VLM效率的关键路径；ConsensusDrop提供了一种实用、高效且无需训练的视觉令牌压缩新范式。

Abstract: Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \textit{either} vision-encoder saliency (broad but query-agnostic) \textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \textbf{ConsensusDrop}, a training-free framework that derives a \emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.

</details>


### [123] [Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images](https://arxiv.org/abs/2602.00949)
*Xiang Zhang,Boxuan Zhang,Alireza Naghizadeh,Mohab Mohamed,Dongfang Liu,Ruixiang Tang,Dimitris Metaxas,Dongfang Liu*

Main category: cs.CV

TL;DR: 本文提出两种互补的数据增强框架（IAAA和SAAA），用于生成高质量的CAR-T/NK免疫突触合成图像及分割掩码，以缓解标注显微镜数据稀缺问题，提升人工神经网络在IS检测与分割任务中的泛化能力与精度，从而支持更可靠的影像学生物标志物开发。


<details>
  <summary>Details</summary>
Motivation: 现有CAR-T/NK细胞免疫突触（IS）的ANN检测与分割受限于小规模标注显微镜数据集，导致模型泛化能力差。

Method: 结合两种数据增强方法：1）Instance Aware Automatic Augmentation（IAAA），自动优化实例保持的增强策略，生成多模态（荧光/明场）合成IS图像及对应掩码；2）Semantic-Aware AI Augmentation（SAAA），融合扩散模型掩码生成器与Pix2Pix条件图像合成器，生成解剖学合理且高保真的IS图像-掩码对。

Result: 合成图像在视觉与结构上高度逼近真实IS数据，显著提升了IS检测与分割性能（如精度、鲁棒性），增强了IS定量分析的可靠性。

Conclusion: 所提双增强框架有效缓解了标注数据稀缺问题，为基于成像的CAR-T/NK疗效预测生物标志物研发提供了关键技术支撑。

Abstract: Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.

</details>


### [124] [Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification](https://arxiv.org/abs/2602.00956)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 本文提出了一种结合拓扑数据分析（TDA）与DenseNet121的混合深度学习框架，用于基于结构MRI的阿尔茨海默病四分类诊断，在OASIS数据集上达到99.93%准确率和100% AUC。


<details>
  <summary>Details</summary>
Motivation: 早期、精准诊断阿尔茨海默病（AD）仍是神经影像临床决策支持系统的关键挑战；传统深度网络易忽略脑结构的拓扑特性。

Method: 将拓扑数据分析（TDA）提取的互补拓扑特征与DenseNet121提取的层次化空间特征进行融合，构建端到端四分类模型，使用OASIS-1 Kaggle MRI数据集训练与验证。

Result: 在OASIS-1数据集上实现99.93%分类准确率和100% AUC，显著优于现有CNN、迁移学习、集成及多尺度方法。

Conclusion: 引入拓扑先验可有效增强深度学习模型判别能力，所提TDA+DenseNet121框架具备高鲁棒性与临床应用潜力。

Abstract: Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.
  Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.

</details>


### [125] [Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning](https://arxiv.org/abs/2602.00971)
*Meng Luo,Bobo Li,Shanqing Xu,Shize Zhang,Qiuchan Chen,Menglu Han,Wenhao Chen,Yanxiang Huang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 本文提出HitEmotion基准和ToM引导的推理链与TMPO强化学习方法，以提升多模态大语言模型在深度情感理解中的认知能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在深层情感理解上能力有限，作者认为真正的情感智能需基于心智理论（ToM）建模，因为情绪源于心智状态。

Method: 构建ToM驱动的分层基准HitEmotion；设计ToM引导的推理链以追踪心智状态并校准跨模态证据；提出TMPO强化学习方法，利用中间心智状态作为过程监督信号优化推理。

Result: 实验表明HitEmotion能揭示SOTA模型在高阶认知情感任务上的缺陷；ToM推理链与TMPO显著提升任务准确率与推理忠实性、连贯性。

Conclusion: 本工作为MLLMs提供了评估与增强基于认知的情感理解能力的实用工具包，并开源数据与代码。

Abstract: Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.

</details>


### [126] [VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes](https://arxiv.org/abs/2602.00995)
*Nick DiSanto,Ehsan Khodapanah Aghdam,Han Liu,Jacob Watson,Yuankai K. Tao,Hao Li,Ipek Oguz*

Main category: cs.CV

TL;DR: 本文提出VAMOS-OCTA，一种基于深度学习的B-scan修复框架，利用血管感知的多轴正交监督损失（VAMOS），在手持OCTA中有效修复运动伪影，同时提升B-scan锐度与三维投影质量。


<details>
  <summary>Details</summary>
Motivation: 手持式OCTA在不配合或儿童受试者中易受运动伪影影响，导致B-scan缺失和en face图像出现空白条带，现有方法多仅关注en face重建，忽视B-scan本身质量。

Method: 提出2.5D U-Net架构，以邻近B-scans为输入重建中心受损B-scan；设计Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS)损失，融合血管加权强度重建、轴向与侧向投影一致性约束。

Result: 在合成与真实运动退化数据上训练验证，VAMOS-OCTA显著优于先前方法，恢复出清晰毛细血管、连续血管结构及干净en face投影。

Conclusion: 多轴监督是恢复运动退化3D OCTA数据的有效强约束，VAMOS-OCTA实现了B-scan级与体积级重建质量的联合提升。

Abstract: Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.

</details>


### [127] [CortiNet: A Physics-Perception Hybrid Cortical-Inspired Dual-Stream Network for Gallbladder Disease Diagnosis from Ultrasound](https://arxiv.org/abs/2602.01000)
*Vagish Kumar,Souvik Chakraborty*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级、受大脑皮层启发的双流神经网络CortiNet，用于胆囊疾病超声诊断，通过多尺度信号分解与感知驱动特征学习，在保持高精度（98.74%）的同时大幅减少参数量，并引入结构感知可解释性框架提升抗斑点噪声能力。


<details>
  <summary>Details</summary>
Motivation: 超声图像分辨率低、斑点噪声强，导致诊断可靠性差；现有大型CNN模型难以在临床常规部署。

Method: 提出CortiNet：双流架构，分别处理低频结构信息和高频感知细节；基于物理可解释的多尺度信号分解；皮层式晚期融合；仅在结构分支应用Grad-CAM实现结构感知可解释性。

Result: 在10692张专家标注图像（9类胆囊疾病）上达到98.74%诊断准确率，参数量显著少于传统深度CNN。

Conclusion: CortiNet通过嵌入物理先验与皮层启发设计，实现了高精度、轻量化与鲁棒可解释性的统一，适合临床落地。

Abstract: Ultrasound imaging is the primary diagnostic modality for detecting Gallbladder diseases due to its non-invasive nature, affordability, and wide accessibility. However, the low resolution and speckle noise inherent to ultrasound images hinder diagnostic reliability, prompting the use of large convolutional neural networks that are difficult to deploy in routine clinical settings. In this work, we propose CortiNet, a lightweight, cortical-inspired dual-stream neural architecture for gallbladder disease diagnosis that integrates physically interpretable multi-scale signal decomposition with perception-driven feature learning. Inspired by parallel processing pathways in the human visual cortex, CortiNet explicitly separates low-frequency structural information from high-frequency perceptual details and processes them through specialized encoding streams. By operating directly on structured, frequency-selective representations rather than raw pixel intensities, the architecture embeds strong physics-based inductive bias, enabling efficient feature learning with a significantly reduced parameter footprint. A late-stage cortical-style fusion mechanism integrates complementary structural and textural cues while preserving computational efficiency. Additionally, we propose a structure-aware explainability framework wherein gradient-weighted class activation mapping is only applied to the structural branch of the proposed CortiNet architecture. This choice allows the model to only focus on the structural features, making it robust against speckle noise. We evaluate CortiNet on 10,692 expert-annotated images spanning nine clinically relevant gallbladder disease categories. Experimental results demonstrate that CortiNet achieves high diagnostic accuracy (98.74%) with only a fraction of the parameters required by conventional deep convolutional models.

</details>


### [128] [SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning](https://arxiv.org/abs/2602.01004)
*Zihao Zhao,Shengting Cao,Muchao Ye*

Main category: cs.CV

TL;DR: 本文提出了一种名为SRVAU-R1的自反思增强推理框架，用于视频异常理解任务，通过构建首个面向反射的思维链数据集，并结合监督微调与强化微调方法，显著提升了时序异常定位精度与推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型（MLLM）的方法在视频异常理解（VAU）中仅停留在表层描述，缺乏对异常行为的深层推理，如显式自反思与自修正能力。

Method: 提出SRVAU-R1框架，包含：（1）首个面向VAU的反射导向Chain-of-Thought数据集，含初始推理、自反思、修正推理三阶段标注；（2）反射感知的学习范式，融合监督微调（SFT）与强化微调（RFT）。

Result: 在多个视频异常基准上实验表明，SRVAU-R1在时序异常定位准确率和推理质量两方面均显著优于现有方法。

Conclusion: 引入自反思机制可有效提升MLLM在视频异常理解中的深层推理能力，SRVAU-R1为VAU任务提供了新的学习范式与高质量监督资源。

Abstract: Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.

</details>


### [129] [LocalScore: Local Density-Aware Similarity Scoring for Biometrics](https://arxiv.org/abs/2602.01012)
*Yiyang Su,Minchul Kim,Jie Zhu,Christopher Perry,Feng Liu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出LocalScore算法，通过利用k近邻估计局部密度来改进开放集生物识别中的匹配评分，提升系统对未注册探针的检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统生物识别系统在开放集场景下难以检测未注册的探针样本；现有方法将同一主体的多张图像压缩为单一特征表示，忽略类内差异，导致决策边界不佳、开放集鲁棒性差。

Method: LocalScore是一种轻量级、即插即用的打分算法，基于探针与gallery中k近邻的距离估计局部密度，不依赖特定网络结构或损失函数。

Result: 在多个模态数据集上显著提升开放集检索（FNIR@FPIR从53%降至40%）和验证性能（TAR@FAR从51%升至74%），并提供理论分析与实证解释其增益条件。

Conclusion: LocalScore通过建模局部特征密度有效缓解开放集生物识别中的类内变异建模不足问题，是一种通用、高效且可部署的改进方案。

Abstract: Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.

</details>


### [130] [Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning](https://arxiv.org/abs/2602.01020)
*Jichen Yang,Jikai Zhang,Benjamin Wildman-Tobriner,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 本研究验证了自动构建的甲状腺结节超声数据集在提升深度学习模型性能方面的有效性，发现使用全自动构建的数据集（AUC=0.694）显著优于人工标注数据集（AUC=0.643），且使用高精度子集并未带来额外收益。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型训练常受限于甲状腺结节标注数据稀缺；虽已有自动构建方法，但其生成数据的实际训练价值尚不明确。

Method: 对比实验：分别用人工标注数据集、全自动构建数据集、以及其中高精度子集训练深度学习模型，并评估AUC性能。

Result: 全自动构建数据集训练的模型AUC为0.694，显著高于人工标注数据集（0.643，P<0.001）；高精度子集训练结果（0.689）与全自动集无显著差异（P>0.43）。

Conclusion: 全自动构建的数据集能显著提升模型性能，建议直接使用全部自动数据，而非仅筛选高精度子集。

Abstract: The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.

</details>


### [131] [GMAC: Global Multi-View Constraint for Automatic Multi-Camera Extrinsic Calibration](https://arxiv.org/abs/2602.01033)
*Chentian Sun*

Main category: cs.CV

TL;DR: 本文提出GMAC框架，利用多视角重建网络学习的隐式几何表示来估计多相机系统的外参，无需标定板或显式3D重建，通过轻量回归头和联合优化重投影与循环一致性实现鲁棒、在线的自动标定。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖标定靶、显式几何建模或任务专用神经网络，在复杂动态或在线场景中鲁棒性和适用性不足，难以实际部署。

Method: GMAC将外参建模为受潜在多视角几何结构约束的全局变量；对现有重建网络进行剪枝与结构重构，使其隐特征可直接支持外参预测；引入轻量回归头，并联合优化跨视角重投影一致性和多视角循环一致性。

Result: 在合成与真实多相机数据集上实验表明，GMAC无需显式3D重建或人工标定即可实现高精度、稳定的外参估计。

Conclusion: GMAC为多相机系统提供了高效部署与在线标定的新解决方案，提升了自动标定在动态环境中的实用性与鲁棒性。

Abstract: Automatic calibration of multi-camera systems, namely the accurate estimation of spatial extrinsic parameters, is fundamental for 3D reconstruction, panoramic perception, and multi-view data fusion. Existing methods typically rely on calibration targets, explicit geometric modeling, or task-specific neural networks. Such approaches often exhibit limited robustness and applicability in complex dynamic environments or online scenarios, making them difficult to deploy in practical applications. To address this, this paper proposes GMAC, a multi-camera extrinsic estimation framework based on the implicit geometric representations learned by multi-view reconstruction networks. GMAC models extrinsics as global variables constrained by the latent multi-view geometric structure and prunes and structurally reconfigures existing networks so that their latent features can directly support extrinsic prediction through a lightweight regression head, without requiring a completely new network design. Furthermore, GMAC jointly optimizes cross-view reprojection consistency and multi-view cycle consistency, ensuring geometric coherence across cameras while improving prediction accuracy and optimization stability. Experiments on both synthetic and real-world multi-camera datasets demonstrate that GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration, providing a new solution for efficient deployment and online calibration of multi-camera systems.

</details>


### [132] [FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence](https://arxiv.org/abs/2602.01035)
*Chentian Sun*

Main category: cs.CV

TL;DR: 本文提出FUSE-Flow，一种帧级、无状态、线性可扩展的实时多视角点云流式重建框架，通过测量置信度与3D距离一致性加权融合点云片段，并引入自适应空间哈希加权聚合方法，在保证实时性的同时提升重建稳定性与几何保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于体素融合、时序累积或全局优化的方法难以在严格实时约束下兼顾重建质量、计算效率与多相机可扩展性。

Method: 提出FUSE-Flow框架：1）每帧独立生成点云片段，用测量置信度和3D距离一致性双权重融合；2）采用自适应空间哈希加权聚合——依据局部点密度动态划分3D空间、每单元选代表点、加权融合；3）GPU并行实现线性复杂度处理。

Result: 实验表明FUSE-Flow在重叠区、深度不连续及动态场景中提升了重建稳定性与几何保真度，同时在现代GPU上维持实时帧率。

Conclusion: FUSE-Flow实现了高吞吐、低延迟、线性可扩展的实时多视角点云重建，验证了其有效性、鲁棒性与可扩展性。

Abstract: Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.

</details>


### [133] [VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models](https://arxiv.org/abs/2602.01037)
*Guangshuo Qin,Zhiteng Li,Zheng Chen,Weihang Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种面向MoE视觉语言模型的双感知后训练量化方法VEQ，兼顾跨模态差异与专家异质性，在多个基准上显著优于现有SOTA量化方法。


<details>
  <summary>Details</summary>
Motivation: MoE视觉语言模型性能优异但计算和内存开销巨大，而现有PTQ方法忽视了视觉与语言模态间的固有差异以及不同专家贡献的不均匀性。

Method: 提出VEQ框架，包含：1）模态-专家感知量化，利用专家激活频率优先减小关键专家的量化误差；2）模态亲和感知量化，融合token-专家亲和度与模态信息构建增强Hessian矩阵指导校准。

Result: 在W3A16配置下，VEQ在Kimi-VL和Qwen3-VL上分别平均提升准确率2.04%和3.09%，显著优于现有SOTA量化方法。

Conclusion: VEQ通过同时建模跨模态差异与专家异质性，实现了更鲁棒、更高效的MoE VLM量化，为多模态大模型压缩提供了新思路。

Abstract: Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\% on Kimi-VL and 3.09\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.

</details>


### [134] [From Videos to Conversations: Egocentric Instructions for Task Assistance](https://arxiv.org/abs/2602.01038)
*Lavisha Aggarwal,Vikas Bahirwani,Andrea Colaco*

Main category: cs.CV

TL;DR: 本文提出一种自动将单人教学视频转换为双人多模态任务指导对话的框架，并构建了HowToDIV数据集，用于评估多模态程序性任务辅助模型。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理在增强现实（AR）辅助任务中进展受限，主要由于缺乏大规模、真实世界任务执行所支撑的多模态对话数据集，而人工收集此类数据成本高、流程复杂。

Method: 基于大语言模型设计全自动流水线，将单人 instructional 视频转化为专家-新手多轮交互的双人多模态任务指导对话。

Result: 构建了HowToDIV数据集，包含507段对话、6636组问答对、24小时跨多领域的视频；并在该数据集上用Gemma 3和Qwen 2.5报告了基线性能。

Conclusion: 该框架为多模态任务指导数据生成提供了可扩展、低成本的新范式，HowToDIV为多模态程序性任务辅助研究提供了首个基准数据集。

Abstract: Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.

</details>


### [135] [ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction](https://arxiv.org/abs/2602.01046)
*Jiawei Lin,Shizhao Sun,Danqing Huang,Ting Liu,Ji Li,Jiang Bian*

Main category: cs.CV

TL;DR: 本文提出了ReLayout框架，用于无需三元组数据的、结构保持的设计布局编辑任务，通过关系图和关系感知设计重建（RADR）方法，在自然语言意图指导下实现自动化的布局编辑。


<details>
  <summary>Details</summary>
Motivation: 解决用户自然语言意图模糊性问题，并应对缺乏（原始设计，编辑操作，编辑后设计）三元组样本的挑战，同时在编辑过程中保持未编辑元素的布局结构。

Method: 提出关系图来建模未编辑元素间的位置与尺寸关系，作为结构保持约束；设计关系感知设计重建（RADR）模块，利用多模态大语言模型进行自监督学习，从元素、关系图和合成编辑操作中重建设计；统一多种编辑动作于单一模型。

Result: 在定性、定量评估及用户研究中，ReLayout在编辑质量、准确性和布局结构保持方面显著优于基线模型。

Conclusion: ReLayout是一种无需三元组标注、支持多样化编辑动作且能有效保持布局结构的自动化设计布局编辑新框架。

Abstract: Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.

</details>


### [136] [Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance](https://arxiv.org/abs/2602.01047)
*Xinrong Chen,Xu Chu,Yingmin Qiu,Hengyuan Zhang,Jing Xiong,Shiyu Tang,Shuai Liu,Shaokang Yang,Cheng Yang,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的残差解码（ResDec）方法，利用大视觉语言模型（LVLMs）内部隐式推理与token logits演化机制，通过历史信息辅助解码，有效抑制由语言先验引发的幻觉，提升视觉定位能力。


<details>
  <summary>Details</summary>
Motivation: LVLMs虽在多模态任务中表现优异，但易受语言先验影响，产生与图像无关的幻觉内容。

Method: 提出Residual Decoding（ResDec），一种训练-free方法，利用LVLMs内部隐式推理机制和token logits演化过程，结合历史解码信息进行偏差校正。

Result: ResDec显著抑制语言先验导致的幻觉，提升视觉接地能力，减少物体幻觉，并在多个LVLM基准测试中表现优异。

Conclusion: ResDec是一种通用、高效、无需训练的解码增强方法，可广泛适用于各类LVLMs以缓解幻觉问题。

Abstract: Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.

</details>


### [137] [Baseline Method of the Foundation Model Challenge for Ultrasound Image Analysis](https://arxiv.org/abs/2602.01055)
*Bo Deng,Yitong Tang,Jiake Li,Yuxin Huang,Li Wang,Yu Zhang,Yufei Zhan,Hua Lu,Xiaoshen Zhang,Jieyun Bai*

Main category: cs.CV

TL;DR: 本文提出了一个基于多头多任务学习（MH-MTL）框架的超声影像分析基础模型官方基线，支持27个子任务，涵盖分割、分类、检测与回归，采用EfficientNet-B4+FPN结构及任务自适应训练策略，验证了其泛化性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有超声影像分析方法多为任务专用，缺乏作为临床可部署基础模型的通用性与泛化能力，难以应对跨解剖结构和采集协议的图像异质性。

Method: 构建统一的多头多任务学习（MH-MTL）框架，以ImageNet预训练的EfficientNet-B4为骨干网络，结合特征金字塔网络（FPN）提取多尺度特征；设计任务特定路由机制，并采用复合损失、任务自适应学习率缩放与余弦退火调度进行训练。

Result: 在FM_UIA~2026多任务基准（27个子任务）上验证了该统一架构的可行性与鲁棒性，确立了一个强健且可扩展的基础模型基线。

Conclusion: 所提出的MH-MTL基线模型为超声影像基础模型研究提供了统一、高效、开源的起点，推动面向临床部署的通用超声AI模型发展。

Abstract: Ultrasound (US) imaging exhibits substantial heterogeneity across anatomical structures and acquisition protocols, posing significant challenges to the development of generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models. To address this limitation, the Foundation Model Challenge for Ultrasound Image Analysis (FM\_UIA~2026) introduces a large-scale multi-task benchmark comprising 27 subtasks across segmentation, classification, detection, and regression. In this paper, we present the official baseline for FM\_UIA~2026 based on a unified Multi-Head Multi-Task Learning (MH-MTL) framework that supports all tasks within a single shared network. The model employs an ImageNet-pretrained EfficientNet--B4 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) to capture multi-scale contextual information. A task-specific routing strategy enables global tasks to leverage high-level semantic features, while dense prediction tasks exploit spatially detailed FPN representations. Training incorporates a composite loss with task-adaptive learning rate scaling and a cosine annealing schedule. Validation results demonstrate the feasibility and robustness of this unified design, establishing a strong and extensible baseline for ultrasound foundation model research. The code and dataset are publicly available at \href{https://github.com/lijiake2408/Foundation-Model-Challenge-for-Ultrasound-Image-Analysis}{GitHub}.

</details>


### [138] [Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction](https://arxiv.org/abs/2602.01057)
*Ling Chen,Bao Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯光线追踪的断层重建框架，替代了以往基于splatting的方法（如R2-Gaussian），通过解析计算高斯体素沿射线的线积分，提升前向投影的物理一致性和几何校正灵活性。


<details>
  <summary>Details</summary>
Motivation: R2-Gaussian等基于splatting的方法依赖局部仿射近似，导致定量精度下降且难以引入非线性几何校正（如PET中的弧形校正）。

Method: 采用3D高斯光线追踪：对每个3D高斯体素沿射线解析计算其线积分作为前向投影，显式建模射线起点与方向，支持精确的非线性几何校正。

Result: 相比splatting方法，该方法提升了投影的物理一致性与定量准确性，并能灵活集成如PET弧校正等复杂几何模型。

Conclusion: 3D高斯光线追踪为高斯基断层重建提供了更准确、更通用的可微分前向模型，拓展了其在真实断层成像系统中的适用性。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.

</details>


### [139] [DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification](https://arxiv.org/abs/2602.01059)
*Ying Shu,Pujian Zhan,Huiqi Yang,Hehe Fan,Youfang Lin,Kai Lv*

Main category: cs.CV

TL;DR: 本文提出了一种双正则化双向Transformer（DRFormer）框架，融合视觉基础模型（如DINO）的细粒度纹理特征与视觉语言模型（如CLIP）的全局语义特征，以提升行人重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖单一模型范式（仅用视觉基础模型或视觉语言模型），忽视了二者在细粒度判别细节和全局语义特征上的互补性，难以应对遮挡、姿态变化等挑战。

Method: 提出DRFormer框架，通过双正则化机制协同整合DINO（擅长局部纹理）与CLIP（擅长全局语义）两类模型，实现双向特征交互与多样性约束。

Result: 在五个主流行人重识别基准上实验表明，该方法有效融合局部与全局表征，性能达到当前领先水平。

Conclusion: 融合视觉基础模型与视觉语言模型的互补优势，并通过结构化协同设计（如双正则化、双向Transformer）可显著提升行人重识别鲁棒性与准确性。

Abstract: Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \textbf{D}ual-\textbf{R}egularized Bidirectional \textbf{Transformer} (\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.

</details>


### [140] [PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors](https://arxiv.org/abs/2602.01069)
*Seema K. Poudel,Sunny K. Khadka*

Main category: cs.CV

TL;DR: 本文提出了一种将偏微分方程（PDE）约束优化与深度学习结合的显微图像分割方法，通过变分正则化引入物理先验，提升模型稳定性、边界精度和小样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 显微图像分割是一个病态反问题，受测量噪声、弱目标边界和标注数据稀缺影响；传统深度学习在无约束经验风险最小化下易导致不稳定解和泛化差。

Method: 将分割建模为PDE约束优化问题，设计包含数据保真项、反应-扩散方程正则项和相场界面能量项的可微复合损失函数，并嵌入UNet架构中。

Result: 在LIVECell数据集上，相比无约束UNet基线，该方法在分割精度、边界保真度、小样本稳定性和跨细胞类型泛化性方面均取得一致提升。

Conclusion: PDE约束优化能有效增强数据驱动学习框架，为变分方法、统计学习与科学机器学习提供原理性桥梁。

Abstract: Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.

</details>


### [141] [PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.01077)
*Haopeng Li,Shitong Shao,Wenliang Zhong,Zikai Zhou,Lichen Bai,Hui Xiong,Zeke Xie*

Main category: cs.CV

TL;DR: 本文提出PISA（分段稀疏注意力）方法，通过块级泰勒展开近似非关键块的注意力分数，在不牺牲质量的前提下显著加速扩散Transformer。


<details>
  <summary>Details</summary>
Motivation: 现有块稀疏注意力在高稀疏度下因丢弃非关键块上下文而导致性能下降；作者发现这些非关键块的注意力分数具有分布稳定性，可被高效准确地近似而非直接丢弃。

Method: 提出训练无关的PISA方法：对关键块进行精确注意力计算，对非关键块采用块级泰勒展开进行高效近似，实现全覆盖、亚二次复杂度的稀疏注意力。

Result: 在Wan2.1-14B和Hunyuan-Video上分别提速1.91倍和2.57倍，图像生成（FLUX）提速1.2倍且视觉质量无损，质量优于其他稀疏注意力方法。

Conclusion: PISA通过‘精确或近似’策略替代传统‘保留或丢弃’范式，有效弥合了稀疏注意力中速度与质量之间的鸿沟，是一种高效且高质量的通用加速方案。

Abstract: Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.

</details>


### [142] [MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization](https://arxiv.org/abs/2602.01081)
*Haitao Zhang,Yingying Wang,Jiaxiang Wang,Haote Xu,Hongyang Zhang,Yirong Chen,Yue Huang,Xinghao Ding*

Main category: cs.CV

TL;DR: 本文提出MedAD-38K数据集和两阶段训练框架（Cognitive Injection + Con-GRPO），提升医学异常检测中大模型的推理一致性与可解释性，所提模型MedAD-R1在该基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有医学异常检测模型依赖简单碎片化数据集上的监督微调，难以实现可信、鲁棒的多模态推理与泛化。

Method: 构建首个大规模多中心多模态MedAD-38K基准（含诊断链式思维CoT标注与结构化VQA对）；提出两阶段训练：第一阶段通过监督微调注入医学认知并建立‘先思考后作答’范式；第二阶段引入一致性分组相对策略优化（Con-GRPO），加入一致性奖励以保障推理过程与最终诊断逻辑一致。

Result: MedAD-R1在MedAD-38K上达到SOTA，性能超越强基线超10%，生成透明、逻辑一致的推理路径。

Conclusion: 通过结构化认知注入与一致性强化学习，可显著提升医学大模型的推理可靠性与临床可解释性，为AI辅助诊断提供新范式。

Abstract: Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.

</details>


### [143] [Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models](https://arxiv.org/abs/2602.01089)
*Zhiqi Zhang,Xinhao Zhong,Yi Sun,Shuoyang Sun,Bin Chen,Shu-Tao Xia,Xuan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的差分向量擦除（DVE）方法，用于在流匹配（flow matching）文本到图像生成模型中安全、精准地擦除不期望的概念（如NSFW内容、版权风格或特定物体），其核心是利用速度场的方向结构建模概念语义，并通过方向投影实现概念抑制。


<details>
  <summary>Details</summary>
Motivation: 现有基于DDPM的去概念方法难以直接迁移至新兴的流匹配模型；且多数方法依赖高成本微调，缺乏高效、通用、免训练的安全控制手段。

Method: 提出Differential Vector Erasure（DVE）：识别语义概念隐含于生成流的速度场方向结构中；构建目标概念与锚定概念之间的差分向量场；在推理时将速度场投影到该差分方向以选择性抑制概念分量。

Result: 在FLUX模型上实验表明，DVE在NSFW抑制、艺术风格移除和物体擦除等任务中显著优于现有基线，同时保持图像质量与多样性。

Conclusion: DVE是一种适用于流匹配模型的高效、免训练、可解释的概念擦除新范式，为可控、安全的文本到图像生成提供了实用解决方案。

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.

</details>


### [144] [PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space](https://arxiv.org/abs/2602.01095)
*Jinghong Zheng,Changlong Jiang,Yang Xiao,Jiaqi Li,Haohong Kuang,Hang Xu,Ran Wang,Zhiguo Cao,Min Du,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出PandaPose，一种通过将2D姿态先验传播到3D锚点空间来进行单图3D人体姿态估计的新方法，以缓解2D误差传播和自遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖从2D到3D的直接关节映射，易受2D姿态预测误差传播影响，且难以处理自遮挡情况。

Method: 提出3D锚点空间作为统一中间表示，包含：(1)规范坐标系下的关节级3D锚点；(2)深度感知的关节特征提升模块；(3)锚点-特征交互解码器生成融合几何与视觉信息的锚点查询，并用于锚点到关节的集成预测。

Result: 在Human3.6M、MPI-INF-3DHP和3DPW三个基准上显著优于SOTA，Human3.6M挑战条件下误差降低14.7%。

Conclusion: PandaPose通过引入结构化3D锚点空间，有效缓解了2D误差传播和自遮挡难题，提升了3D姿态估计的精度与鲁棒性。

Abstract: 3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.

</details>


### [145] [Robust Harmful Meme Detection under Missing Modalities via Shared Representation Learning](https://arxiv.org/abs/2602.01101)
*Felix Breiteneder,Mohammad Belal,Muhammad Saad Saeed,Shahed Masoudian,Usman Naseem,Kulshrestha Juhi,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: 本文提出了一种新的基线方法，通过独立投影多模态数据来学习共享表征，以提升有害梗图检测在模态缺失（尤其是文本缺失）情况下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有有害梗图检测方法依赖完整的多模态数据（如图文），但在真实场景中常因OCR质量差等原因导致文本缺失，造成性能下降。

Method: 提出一种新基线方法，对各模态独立投影以学习共享表征，使模态不完整时仍可有效利用可用模态信息。

Result: 在两个基准数据集上的实验表明，该方法在文本缺失时优于现有方法，更有效地融合视觉特征，降低对文本的依赖。

Conclusion: 本工作首次系统研究模态不完整下的有害梗图检测问题，提升了方法在现实场景中的适用性与鲁棒性。

Abstract: Internet memes are powerful tools for communication, capable of spreading political, psychological, and sociocultural ideas. However, they can be harmful and can be used to disseminate hate toward targeted individuals or groups. Although previous studies have focused on designing new detection methods, these often rely on modal-complete data, such as text and images. In real-world settings, however, modalities like text may be missing due to issues like poor OCR quality, making existing methods sensitive to missing information and leading to performance deterioration. To address this gap, in this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of harmful meme detection methods in the presence of modal-incomplete data. Specifically, we propose a new baseline method that learns a shared representation for multiple modalities by projecting them independently. These shared representations can then be leveraged when data is modal-incomplete. Experimental results on two benchmark datasets demonstrate that our method outperforms existing approaches when text is missing. Moreover, these results suggest that our method allows for better integration of visual features, reducing dependence on text and improving robustness in scenarios where textual information is missing. Our work represents a significant step forward in enabling the real-world application of harmful meme detection, particularly in situations where a modality is absent.

</details>


### [146] [LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions](https://arxiv.org/abs/2602.01118)
*Jingjing Wang,Qirui Hu,Chong Bao,Yuke Zhu,Hujun Bao,Zhaopeng Cui,Guofeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了LightCity，一个高质量的合成城市数据集，用于解决城市场景中逆向渲染面临的复杂光照条件挑战，并通过该数据集对城市环境中的三个基础任务进行了基准测试与分析。


<details>
  <summary>Details</summary>
Motivation: 逆向渲染在城市场景中对自动驾驶和数字孪生等应用至关重要，但复杂光照（如多光源、间接光和阴影）对本征分解和3D重建的影响尚未被探索，主要受限于缺乏合适的数据集。

Method: 构建了一个名为LightCity的新型高质量合成城市数据集，包含300多张可控光照的天空图、5万张街景与航拍图像，并提供深度、法线、材质成分、直接光与间接光等丰富属性；并基于该数据集对城市环境中的三个基础任务进行基准测试与综合分析。

Result: 发布了LightCity数据集，具备多样化、高可控性与丰富标注；完成了三项城市逆向渲染相关任务的基准测试与系统性分析。

Conclusion: LightCity为城市场景下的逆向渲染、本征分解与3D重建研究提供了坚实的数据基础与评估平台，推动了相关领域的发展。

Abstract: Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.

</details>


### [147] [Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis](https://arxiv.org/abs/2602.01127)
*Matej Suchanek,Klara Janouskova,Ondrej Vasatko,Jiri Matas*

Main category: cs.CV

TL;DR: 本文提出Koo-Fu CLIP，一种基于Fukunaga-Koontz线性判别分析的监督式CLIP适配方法，通过白化嵌入空间提升类间判别性、抑制类内差异，并实现高效降维与分类性能提升。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉语言模型的原始嵌入未针对监督分类优化，存在类分离度低和维度过高的问题。

Method: 提出Koo-Fu CLIP方法，基于Fukunaga-Koontz线性判别分析，在白化嵌入空间中进行闭式线性投影，重塑CLIP嵌入几何结构。

Result: 在ImageNet-1K上top-1准确率从75.1%提升至79.1%，在14K/21K类别下仍保持增益；支持10–12倍压缩且几乎无精度损失。

Conclusion: Koo-Fu CLIP是一种轻量、高效、可扩展的CLIP监督适配方法，显著提升分类性能与嵌入效率。

Abstract: Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.
  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.

</details>


### [148] [Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models](https://arxiv.org/abs/2602.01163)
*Chunliang Hua,Zeyuan Yang,Lei Zhang,Jiayang Sun,Fengwen Chen,Chunlan Zeng,Xiao Hu*

Main category: cs.CV

TL;DR: 本文提出了一种结合遥感影像与多模态大语言模型（MLLMs）的无人机紧急着陆点评估新框架，通过粗粒度到细粒度的流程，利用语义分割和视觉-语言推理融合POI数据识别复杂语义风险，显著优于传统几何方法，并构建了公开基准ELSS。


<details>
  <summary>Details</summary>
Motivation: 传统基于几何传感器的方法无法识别人群、临时建筑等复杂语义风险，而安全紧急着陆需全局语义理解。

Method: 提出粗到细两阶段框架：1）轻量级语义分割模块预筛候选区域；2）视觉-语言推理代理融合视觉特征与兴趣点（POI）数据识别细微风险；并构建ELSS基准进行验证。

Result: 在风险识别准确率上显著超越几何基线方法；定性结果表明能生成类人、可解释的决策依据，提升自动化决策可信度；ELSS数据集已开源。

Conclusion: 融合遥感影像、多模态大语言模型与POI数据的语义感知框架，能有效提升无人机紧急着陆选址的安全性与可解释性，为全球上下文感知自主决策提供了新范式。

Abstract: Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.

</details>


### [149] [EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment](https://arxiv.org/abs/2602.01173)
*Lancheng Gao,Ziheng Jia,Zixuan Xing,Wei Sun,Huiyu Duan,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了EEmoDB数据集和EEmo-Logic多模态大模型，旨在提升图像诱发情绪的多维、细粒度理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在图像诱发情绪理解上局限于粗粒度感知或推理能力不足，难以满足机器共情与人机交互需求。

Method: 构建了包含125k图像、1.2M QA对的EEmoDB-QA子集和25k图像、36k样本的EEmoDB-Assess子集；提出EEmo-Logic模型，通过指令微调与任务定制的组相对偏好优化（GRPO）训练。

Result: EEmo-Logic在领域内及跨域数据集上均表现出色，尤其在情绪问答与细粒度评估任务中性能稳健。

Conclusion: EEmoDB与EEmo-Logic共同推动了图像情绪理解向多维、细粒度、可推理方向发展，为机器共情提供了新基准与工具。

Abstract: Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.

</details>


### [150] [Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion](https://arxiv.org/abs/2602.01183)
*Chunming He,Rihan Zhang,Fengyang Xiao,Dingming Zhang,Zhiwen Cao,Sina Farsiu*

Main category: cs.CV

TL;DR: 本文提出CurriSeg框架，结合课程学习与反课程学习策略，提升上下文纠缠内容分割（CECS）任务中的鲁棒性与泛化能力，无需增加参数或训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有分割网络多依赖结构改进，忽视学习动态对纠缠数据分布下鲁棒性的影响；而生物学习由易到难的渐进机制可启发更可靠的学习过程。

Method: CurriSeg包含两阶段：1）课程选择阶段——基于样本损失的时间统计动态筛选难而有信息量的样本；2）反课程促进阶段——通过频谱盲微调抑制高频成分，增强对低频结构与上下文线索的依赖。

Result: 在多个CECS基准上实现一致性能提升，不增加模型参数和总训练时间。

Conclusion: 课程与反课程协同可有效提升表征可靠性与上下文感知分割鲁棒性，为纠缠数据下的学习提供新范式。

Abstract: Biological learning proceeds from easy to difficult tasks, gradually reinforcing perception and robustness. Inspired by this principle, we address Context-Entangled Content Segmentation (CECS), a challenging setting where objects share intrinsic visual patterns with their surroundings, as in camouflaged object detection. Conventional segmentation networks predominantly rely on architectural enhancements but often ignore the learning dynamics that govern robustness under entangled data distributions. We introduce CurriSeg, a dual-phase learning framework that unifies curriculum and anti-curriculum principles to improve representation reliability. In the Curriculum Selection phase, CurriSeg dynamically selects training data based on the temporal statistics of sample losses, distinguishing hard-but-informative samples from noisy or ambiguous ones, thus enabling stable capability enhancement. In the Anti-Curriculum Promotion phase, we design Spectral-Blindness Fine-Tuning, which suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues and thus strengthens generalization. Extensive experiments demonstrate that CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time, offering a principled view of how progression and challenge interplay to foster robust and context-aware segmentation. Code will be released.

</details>


### [151] [EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting](https://arxiv.org/abs/2602.01194)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Fenghua Ling,Lei Bai*

Main category: cs.CV

TL;DR: 本文提出了一种用于长期天气预报的新方法，通过高效多尺度Transformer（EMFormer）、累积上下文微调和正弦加权复合损失，解决了灾难性遗忘、误差累积和高训练开销等问题，在预报精度和计算效率上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有长期天气预报方法受限于灾难性遗忘、误差累积和高训练开销，难以兼顾长程建模能力与计算效率。

Method: 提出EMFormer架构（单卷积实现多尺度特征提取）、累积上下文微调策略（增强时序一致性且不损害短期精度），以及基于正弦权重的动态平衡复合损失函数，贯穿预训练与微调阶段。

Result: 在天气预报与极端事件预测任务中显著提升长期预测精度；EMFormer在ImageNet-1K和ADE20K视觉基准上泛化性强，并比传统多尺度模块快5.69倍。

Conclusion: 该三阶段（预训练-微调-预测）新范式有效提升了长时序建模能力与计算效率，为气象与跨领域时空建模提供了可扩展解决方案。

Abstract: Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.

</details>


### [152] [Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis](https://arxiv.org/abs/2602.01200)
*Haoran Lai,Zihang Jiang,Kun Zhang,Qingsong Yao,Rongsheng Wang,Zhiyang He,Xiaodong Tao,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: 本文提出Med3D-R1，一种两阶段（监督微调+强化学习）的3D医学视觉语言模型训练框架，通过残差对齐、异常加权和一致性奖励设计提升临床推理能力与可解释性，在CT-RATE和RAD-ChestCT上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 3D医学视觉语言模型面临体积影像复杂性高、易过拟合报告表层模式、缺乏可解释性奖励设计等挑战，难以实现稳健的临床推理。

Method: 提出Med3D-R1框架：SFT阶段引入残差对齐机制融合3D特征与文本嵌入，并采用异常性重加权策略增强关键临床token；RL阶段重新设计一致性奖励，显式鼓励连贯、分步的诊断推理。

Result: 在CT-RATE和RAD-ChestCT两个3D诊断基准上的多选视觉问答任务中，分别取得41.92%和44.99%的准确率，为当前最优结果。

Conclusion: Med3D-R1显著提升了3D医学VLM的异常识别能力与临床推理可靠性，增强了模型可解释性，有望推动真实世界诊断工作流的改进。

Abstract: Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\% on CT-RATE and 44.99\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.

</details>


### [153] [Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment](https://arxiv.org/abs/2602.01257)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种文本精炼与对齐（TRA）框架，通过引入文本描述信息来增强点监督下的时序动作定位性能，包含点基文本精炼（PTR）和点基多模态对齐（PMA）两个模块，在多个基准上取得优越性能且具备实际部署可行性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅利用视觉特征，忽略了文本语义信息对点监督时序动作定位的潜在增益。

Method: 提出TRA框架，包含两个新模块：1）Point-based Text Refinement（PTR），利用点标注和预训练模型精炼帧级文本描述；2）Point-based Multimodal Alignment（PMA），将视觉与文本特征映射到统一语义空间，并通过点级多模态对比学习缩小模态差距；最终融合多模态特征输入动作检测器。

Result: 在五个主流基准上显著优于多种SOTA方法；计算开销分析表明可在单块24GB RTX 3090 GPU上运行，具备实用性和可扩展性。

Conclusion: 引入语义丰富的文本特征并进行点级多模态对齐，能有效提升点监督下时序动作定位的精度与鲁棒性，验证了跨模态协同建模在弱监督任务中的重要价值。

Abstract: Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.

</details>


### [154] [Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.01273)
*Xun Zhang,Kaicheng Yang,Hongliang Lu,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了Q-DiT4SR，首个面向DiT架构的Real-ISR后训练量化框架，通过H-SVD和VaSMP方法显著缓解量化导致的纹理退化，在W4A6/W4A4下达到SOTA性能，并大幅压缩模型尺寸与计算量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers (DiTs)在Real-ISR中表现优异但推理开销大；现有PTQ方法不适用于DiT-based Real-ISR，直接迁移会导致局部纹理严重退化。

Method: 提出H-SVD（分层奇异值分解）：融合全局低秩分支与局部块级秩-1分支；提出VaSMP（方差感知时空混合精度）：数据无关的跨层权重位宽分配（基于率失真理论）+ 基于动态规划的时序自适应激活精度调度（VaTMP）。

Result: 在多个真实世界数据集上，Q-DiT4SR在W4A6和W4A4设置下均达到SOTA性能；W4A4配置下模型大小减少5.8×，计算量降低超60×。

Conclusion: Q-DiT4SR是首个专为DiT-based Real-ISR设计的高效PTQ框架，兼顾精度与效率，验证了针对性量化设计对生成式超分任务的重要性。

Abstract: Recently, Diffusion Transformers (DiTs) have emerged in Real-World Image Super-Resolution (Real-ISR) to generate high-quality textures, yet their heavy inference burden hinders real-world deployment. While Post-Training Quantization (PTQ) is a promising solution for acceleration, existing methods in super-resolution mostly focus on U-Net architectures, whereas generic DiT quantization is typically designed for text-to-image tasks. Directly applying these methods to DiT-based super-resolution models leads to severe degradation of local textures. Therefore, we propose Q-DiT4SR, the first PTQ framework specifically tailored for DiT-based Real-ISR. We propose H-SVD, a hierarchical SVD that integrates a global low-rank branch with a local block-wise rank-1 branch under a matched parameter budget. We further propose Variance-aware Spatio-Temporal Mixed Precision: VaSMP allocates cross-layer weight bit-widths in a data-free manner based on rate-distortion theory, while VaTMP schedules intra-layer activation precision across diffusion timesteps via dynamic programming (DP) with minimal calibration. Experiments on multiple real-world datasets demonstrate that our Q-DiT4SR achieves SOTA performance under both W4A6 and W4A4 settings. Notably, the W4A4 quantization configuration reduces model size by 5.8$\times$ and computational operations by over 60$\times$. Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR.

</details>


### [155] [TF-Lane: Traffic Flow Module for Robust Lane Perception](https://arxiv.org/abs/2602.01277)
*Yihan Xie,Han Xia,Zhen Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于实时交通流信息的车道感知模块TFM，以提升视觉感知在遮挡或无车道线场景下的鲁棒性，无需高精地图，且在多个模型和数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的车道检测方法在遮挡或缺失车道线等场景下性能显著下降；引入高精地图虽有帮助，但存在成本高、实时性差的问题。

Method: 提出TrafficFlow-aware Lane perception Module（TFM），从交通流中提取实时特征，并与现有车道感知算法无缝融合。

Result: 在NuScenes和OpenLaneV2两个公开数据集上，TFM在四个主流模型上均带来性能提升，NuScenes上mAP最高提升4.1%。

Conclusion: 交通流是一种低成本、实时性强的新型辅助信息源，TFM能有效增强车道感知鲁棒性，适用于真实自动驾驶场景。

Abstract: Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.

</details>


### [156] [DSFC-Net: A Dual-Encoder Spatial and Frequency Co-Awareness Network for Rural Road Extraction](https://arxiv.org/abs/2602.01278)
*Zhengbo Zhang,Yihe Tian,Wanke Xia,Lin Chen,Yue Sun,Kun Ding,Ying Wang,Bing Xu,Shiming Xiang*

Main category: cs.CV

TL;DR: 本文提出DSFC-Net，一种双编码器网络，融合空间与频域信息，以提升遥感图像中农村道路的精确提取性能。


<details>
  <summary>Details</summary>
Motivation: 农村道路提取面临高类内差异、低类间可分性、植被遮挡及道路狭窄等挑战，现有方法多针对城市环境，难以适应农村场景。

Method: 提出DSFC-Net：包含CNN分支捕获局部细节，以及新型空间-频率混合Transformer（SFT），其含跨频交互注意力（CFIA）模块（基于拉普拉斯金字塔解耦高低频）；另设通道特征融合模块（CFFM）自适应融合双分支特征。

Result: 在WHU-RuR+、DeepGlobe和Massachusetts数据集上，DSFC-Net显著优于当前最优方法。

Conclusion: 融合空间与频率域信息、并解耦建模高低频特征，可有效提升农村窄路提取的鲁棒性与连通性，为遥感道路提取提供了新范式。

Abstract: Accurate extraction of rural roads from high-resolution remote sensing imagery is essential for infrastructure planning and sustainable development. However, this task presents unique challenges in rural settings due to several factors. These include high intra-class variability and low inter-class separability from diverse surface materials, frequent vegetation occlusions that disrupt spatial continuity, and narrow road widths that exacerbate detection difficulties. Existing methods, primarily optimized for structured urban environments, often underperform in these scenarios as they overlook such distinctive characteristics. To address these challenges, we propose DSFC-Net, a dual-encoder framework that synergistically fuses spatial and frequency-domain information. Specifically, a CNN branch is employed to capture fine-grained local road boundaries and short-range continuity, while a novel Spatial-Frequency Hybrid Transformer (SFT) is introduced to robustly model global topological dependencies against vegetation occlusions. Distinct from standard attention mechanisms that suffer from frequency bias, the SFT incorporates a Cross-Frequency Interaction Attention (CFIA) module that explicitly decouples high- and low-frequency information via a Laplacian Pyramid strategy. This design enables the dynamic interaction between spatial details and frequency-aware global contexts, effectively preserving the connectivity of narrow roads. Furthermore, a Channel Feature Fusion Module (CFFM) is proposed to bridge the two branches by adaptively recalibrating channel-wise feature responses, seamlessly integrating local textures with global semantics for accurate segmentation. Comprehensive experiments on the WHU-RuR+, DeepGlobe, and Massachusetts datasets validate the superiority of DSFC-Net over state-of-the-art approaches.

</details>


### [157] [Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons](https://arxiv.org/abs/2602.01283)
*Xianhui Zhang,Chengyu Xie,Linxia Zhu,Yonghui Yang,Weixiang Zhao,Zifeng Cheng,Cong Wang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文发现大语言模型中存在一组跨语言共享的安全神经元（SS-Neurons），它们在高资源与低资源语言间桥接安全能力迁移；通过定位并调控这些神经元，提出一种轻量级训练策略，显著提升非高资源语言的安全性，同时保持模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 解决多语言安全能力不平衡问题，尤其改善非高资源语言的安全脆弱性，并探究安全对齐背后的神经机制。

Method: 首先识别单语安全神经元（MS-Neurons）并验证其因果作用；继而通过跨语言分析提取其交集——跨语言共享安全神经元（SS-Neurons）；最后设计面向SS-Neurons的、基于语言资源分布和模型结构的轻量微调策略。

Result: 抑制SS-Neurons导致多种非高资源语言安全性能同步下降，增强则提升跨语言防御一致性；所提方法仅微调极小神经元子集，效果超越现有最优方法，显著提升非高资源语言安全性且不损害通用能力。

Conclusion: SS-Neurons是实现跨语言安全迁移的关键神经基础，以神经元为中心的定向优化是一种高效、可解释且资源友好的多语言安全增强范式。

Abstract: Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.
  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.
  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.
  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.
  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.
  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.
  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.

</details>


### [158] [Interacted Planes Reveal 3D Line Mapping](https://arxiv.org/abs/2602.01296)
*Zeran Ke,Bin Tan,Gui-Song Xia,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: 本文提出LiP-Map，一种联合优化3D线与平面的框架，通过建模可学习的线和平面基元，实现高效、准确且结构化的3D线地图构建，并在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 从物理和拓扑角度出发，认为3D直线最自然地源于有限3D平面片的边缘，因此需将线与平面结构联合建模以提升重建质量与结构性。

Method: 提出LiP-Map框架，显式建模可学习的线与平面基元，通过二者间直接交互而非仅依赖成对共面约束，实现联合优化。

Result: 在ScanNetV2等5个数据集超100个场景上，线映射的精度与完整性均优于SOTA；同时显著提升线辅助视觉定位性能（7Scenes）。

Conclusion: LiP-Map首次将平面拓扑系统性引入3D线映射，为人工环境中的结构化重建提供了原理性新路径。

Abstract: 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.

</details>


### [159] [Interaction-Consistent Object Removal via MLLM-Based Reasoning](https://arxiv.org/abs/2602.01298)
*Ching-Kai Huang,Wen-Chieh Lin,Yan-Cen Lee*

Main category: cs.CV

TL;DR: 本文提出了一种交互一致的对象移除方法REORM，利用多模态大语言模型推理需联合移除的交互相关元素，并构建了新基准ICOREval进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的对象移除方法仅删除目标对象，忽略其与场景的交互证据（如光照影响、物理连接物等），导致语义不一致。

Method: 提出Reasoning-Enhanced Object Removal with MLLM（REORM）框架，结合多模态大语言模型（MLLM）驱动分析、掩码引导移除与自修正机制，并支持本地部署；同时构建ICOREval基准用于评估。

Result: 在ICOREval基准上，REORM优于当前最优图像编辑系统，能生成交互一致的移除结果。

Conclusion: 交互一致的对象移除需联合移除目标及其交互元素；REORM通过MLLM推理与模块化设计有效解决了该问题，且具备资源受限下的实用性。

Abstract: Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.

</details>


### [160] [ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation](https://arxiv.org/abs/2602.01303)
*Ayushman Sarkar,Zhenyu Yu,Chu Chen,Wei Tang,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: ReDiStory是一种无需训练的多帧视觉故事生成框架，通过推理时重组织文本提示嵌入，显式分解并去相关身份与帧特定成分，从而在不修改扩散模型参数的情况下提升跨帧主体一致性。


<details>
  <summary>Details</summary>
Motivation: 现有训练-free方法将身份和帧提示拼接，易导致帧间语义干扰，削弱复杂故事中的主体身份一致性。

Method: ReDiStory在推理时对文本嵌入进行分解：分离出身份相关与帧特定成分，并通过抑制各帧嵌入间的共享方向来实现帧嵌入去相关。

Result: 在ConsiStory+基准上，ReDiStory在多个身份一致性指标上一致优于1Prompt1Story，同时保持提示保真度。

Conclusion: 通过嵌入层面的推理时重组织，ReDiStory有效缓解了帧间干扰，在无需训练和额外监督下显著提升了多帧视觉故事的主体一致性。

Abstract: Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory

</details>


### [161] [StoryState: Agent-Based State Control for Consistent and Editable Storybooks](https://arxiv.org/abs/2602.01305)
*Ayushman Sarkar,Zhenyu Yu,Wei Tang,Chu Chen,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: 本文提出了StoryState，一种基于代理的编排层，通过显式、可编辑的故事状态提升多模态故事书生成的一致性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型驱动的故事书生成方法缺乏对故事状态（如角色、场景设置等）的显式建模，导致编辑粗粒度且易破坏视觉一致性。

Method: StoryState构建结构化故事状态（含角色表、全局设定和分页场景约束），并利用小型LLM代理维护该状态、生成面向文本到图像模型的提示，全程无需训练且模型无关。

Result: 实验表明，StoryState在多页编辑任务中支持局部页面编辑，显著提升跨页一致性，减少非预期更改、交互轮次与编辑时间，性能接近Gemini Storybook的一次性生成一致性。

Conclusion: StoryState为免训练的文本到图像生成提供了可解释、可编辑、高一致性的故事状态管理框架，具备后端兼容性与实用潜力。

Abstract: Large multimodal models have enabled one-click storybook generation, where users provide a short description and receive a multi-page illustrated story. However, the underlying story state, such as characters, world settings, and page-level objects, remains implicit, making edits coarse-grained and often breaking visual consistency. We present StoryState, an agent-based orchestration layer that introduces an explicit and editable story state on top of training-free text-to-image generation. StoryState represents each story as a structured object composed of a character sheet, global settings, and per-page scene constraints, and employs a small set of LLM agents to maintain this state and derive 1Prompt1Story-style prompts for generation and editing. Operating purely through prompts, StoryState is model-agnostic and compatible with diverse generation backends. System-level experiments on multi-page editing tasks show that StoryState enables localized page edits, improves cross-page consistency, and reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, while approaching the one-shot consistency of Gemini Storybook. Code is available at https://github.com/YuZhenyuLindy/StoryState

</details>


### [162] [DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling](https://arxiv.org/abs/2602.01306)
*Ayushman Sarkar,Zhenyu Yu,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: DeCorStory是一种无需训练的推理时框架，通过Gram-Schmidt嵌入去相关、奇异值重加权和身份保持的交叉注意力，提升文本到图像故事生成中的帧间语义一致性与角色身份稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有无训练方法（如One-Prompt-One-Story）因提示词拼接导致嵌入强相关，引发颜色泄漏、背景融合和身份漂移等问题。

Method: 采用Gram-Schmidt正交化对帧级提示嵌入进行去相关；引入奇异值重加权增强提示特异性信息；设计身份保持的交叉注意力机制稳定角色身份；全程不修改模型或微调。

Result: 在提示-图像对齐、身份一致性和视觉多样性方面显著优于现有无训练基线，达到该类方法的SOTA性能。

Conclusion: DeCorStory为文本到图像故事生成提供了一种高效、即插即用的推理时一致性增强方案，无需额外训练即可提升多帧语义与视觉稳定性。

Abstract: Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory

</details>


### [163] [FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching](https://arxiv.org/abs/2602.01329)
*Divya Jyoti Bajpai,Shubham Agarwal,Apoorv Saxena,Kuldeep Kulkarni,Subrata Mitra,Manjesh Kumar Hanawal*

Main category: cs.CV

TL;DR: 本文提出FlowCast，一种无需训练的推测性生成框架，利用流匹配模型恒定速度特性加速推理，实现2.5倍以上加速且无质量损失。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配模型因需大量去噪步骤导致推理速度慢，难以用于实时或交互式应用；而现有加速方法存在质量下降、需昂贵重训练或泛化性差等问题。

Method: FlowCast基于流匹配模型训练时保持恒定速度的特性，通过外推当前速度来推测未来速度，若其均方误差在阈值内则接受推测结果，从而跳过稳定区域的冗余步骤；该方法无需额外网络，即插即用，并提供理论分析与轨迹偏差上界。

Result: 在图像生成、视频生成和编辑任务中，FlowCast实现超过2.5倍的加速，性能优于现有基线，且生成质量与标准全步长流匹配完全一致。

Conclusion: FlowCast是一种高效、通用、无需训练的流匹配加速框架，在保持生成质量的同时显著提升推理效率，适用于多种视觉生成任务。

Abstract: Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.

</details>


### [164] [What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom](https://arxiv.org/abs/2602.01334)
*Yan Ma,Weiyu Zhang,Tianle Li,Linge Du,Xuyang Shen,Pengfei Liu*

Main category: cs.CV

TL;DR: 本文提出MED框架，用于区分视觉工具使用强化学习中模型内在能力提升与工具使用带来的性能变化，并发现当前方法主要减少工具使用带来的负面影响，而非提升工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉工具使用强化学习虽提升了模型性能，但尚不清楚这些提升是源于工具使用的改进还是模型内在能力的增强。

Method: 提出MED（Measure-Explain-Diagnose）框架，从粗到细地分离内在能力变化与工具诱导效应，将工具诱导的性能差异分解为增益项和损害项，并探究其演化机制。

Result: 在两个具有不同工具先验的VLM和六个基准上的检查点级分析表明：性能提升主要来自内在学习；工具使用RL主要减少工具诱导的损害（如调用错误减少、工具模式干扰减弱），但在利用工具纠正内在缺陷方面进展有限。

Conclusion: 当前视觉工具使用强化学习更侧重于让模型‘安全共存’于工具环境中，而非真正‘掌握’工具。

Abstract: Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.

</details>


### [165] [Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning](https://arxiv.org/abs/2602.01335)
*Yu Xu,Yuxin Zhang,Juan Cao,Lin Gao,Chunyu Wang,Oliver Deussen,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

TL;DR: 本文提出视觉隐喻迁移（VMT）任务，旨在让AI模型自主提取参考图像中的抽象创意逻辑，并将其迁移到新目标主体上；为此设计了一个受认知科学启发、基于概念整合理论的多智能体框架，包含感知、迁移、生成与诊断四个协同代理，并引入结构化‘模式语法’实现跨域逻辑重实例化；实验表明该方法在隐喻一致性、类比恰当性和视觉创造性上显著优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI模型局限于像素级指令对齐和表层外观保持，无法捕捉生成真正视觉隐喻所需的抽象逻辑；需构建能解耦并迁移‘创意本质’的新范式。

Method: 提出视觉隐喻迁移（VMT）任务；构建基于概念整合理论（CBT）的多智能体框架，含感知代理（提取模式）、迁移代理（保持通用空间不变性以发现适配载体）、生成代理（高保真合成）和分层诊断代理（闭环回溯纠错）；引入新型‘模式语法（G）’作为结构化表示，解耦关系不变量与具体视觉实体。

Result: 在隐喻一致性、类比恰当性和视觉创造性等指标上显著超越当前最优基线；通过大量实验与人工评估验证有效性；代码将开源。

Conclusion: 本工作首次系统定义并解决视觉隐喻迁移问题，为AI驱动的高影响力创意应用（如广告、媒体）奠定理论与技术基础，推动生成式AI从表层合成迈向深层抽象推理。

Abstract: A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the "creative essence" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar ("G"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.

</details>


### [166] [MTC-VAE: Multi-Level Temporal Compression with Content Awareness](https://arxiv.org/abs/2602.01340)
*Yubo Dong,Linchao Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种将固定压缩率的VAE转换为支持多级时间压缩的VAE的方法，通过最小微调缓解高保真压缩下的性能下降，并验证了其在视频扩散模型（如DiT）中的有效性和兼容性。


<details>
  <summary>Details</summary>
Motivation: 连续变分自编码器（VAEs）在提升视频压缩率时，若不增加隐藏通道维度，仅添加采样层会导致效率显著下降，亟需一种高效、轻量的多级时间压缩方案。

Method: 提出一种轻量级微调技术，将固定压缩率的VAE改造为支持多级时间压缩的VAE；并在不同特性的视频片段上评估压缩级别对性能的影响；进一步将其集成到DiT等基于扩散的生成模型中进行联合训练。

Result: 该方法有效缓解了高时间压缩率下的性能下降；实证表明其在多样化视频片段上鲁棒；成功实现与DiT模型的协同训练，验证了兼容性与实用性。

Conclusion: 多级时间压缩VAE是一种简单、高效且通用的视频表示学习增强手段，可提升LVDMs在不同压缩需求下的灵活性与生成质量。

Abstract: Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.

</details>


### [167] [Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis](https://arxiv.org/abs/2602.01345)
*Yu Zhang,Jingyi Liu,Feng Liu,Duoqian Miao,Qi Zhang,Kexue Fu,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: 本文提出NOVA框架，通过熵分析实现视觉自回归模型（VAR）的免训练令牌缩减加速，自适应确定加速激活尺度并动态计算各尺度和层的令牌缩减比例，兼顾推理速度与生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有VAR模型因令牌数量庞大导致计算成本高，而现有令牌缩减方法存在启发式阶段划分、非自适应调度和加速范围有限三大缺陷，未能充分挖掘加速潜力。

Method: NOVA是一种免训练的令牌缩减加速框架，基于熵分析在线识别尺度熵增长拐点，自适应确定加速激活尺度；通过尺度关联和层关联比率调整，动态计算各尺度和层的令牌缩减比例，剪除低熵令牌，并复用前一尺度残差缓存以加速推理。

Result: 大量实验与分析验证了NOVA作为简单而有效的免训练加速框架的有效性，在提升推理速度的同时保持了生成质量。

Conclusion: NOVA为视觉自回归建模提供了一种原理清晰、无需额外训练、自适应且高效的令牌缩减加速方案，显著提升了VAR模型的实际部署效率。

Abstract: Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.

</details>


### [168] [T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation](https://arxiv.org/abs/2602.01352)
*Xingzu Zhan,Chen Xie,Honghang Chen,Yixun Lin,Xiaochun Mai*

Main category: cs.CV

TL;DR: 本文提出T2M Mamba模型，通过周期性-显著性感知的Mamba结构和周期性差分跨模态对齐模块（PDCAM），解决文本到运动生成中长序列漂移和语义等价改写鲁棒性差两大问题，在HumanML3D和KIT-ML数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到运动生成模型存在两大缺陷：一是忽视运动周期性与关键帧显著性的耦合关系，导致长序列生成漂移；二是对语义等价的文本改写（如同义词替换）鲁棒性差，易造成运动错误。

Method: 提出Periodicity-Saliency Aware Mamba，结合增强密度峰值聚类估计关键帧权重、FFT加速自相关估计运动周期；并构建Periodic Differential Cross-modal Alignment Module（PDCAM）以提升文本与运动嵌入的鲁棒对齐。

Result: 在HumanML3D和KIT-ML数据集上实验表明，FID达0.068，并在所有其他指标上稳定提升。

Conclusion: T2M Mamba有效建模了运动周期性与关键帧显著性的耦合关系，并增强了跨模态对齐鲁棒性，显著提升了长序列生成质量与文本扰动下的稳定性。

Abstract: Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.

</details>


### [169] [Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts](https://arxiv.org/abs/2602.01369)
*Songping Wang,Qinglong Liu,Yueming Lyu,Ning Li,Ziwen He,Caifeng Shan*

Main category: cs.CV

TL;DR: 本文提出了一种针对视频MoE模型的时序Lipschitz引导攻击（TLGA）及其联合版本（J-TLGA），揭示了路由器与专家模块的独立及协同脆弱性，并进一步设计了联合时序Lipschitz对抗训练（J-TLAT）以提升鲁棒性，兼具即插即用性与高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法将MoE视为整体，忽视其路由器和专家模块各自的独立弱点及协同弱点，尤其在视频理解任务中，MoE的对抗鲁棒性尚未被深入探索。

Method: 提出时序Lipschitz引导攻击（TLGA）分别攻击路由器；进而设计联合TLGA（J-TLGA）协同扰动路由器与专家；最后基于发现的协同弱点，提出联合时序Lipschitz对抗训练（J-TLAT）进行联合鲁棒训练。

Result: J-TLAT显著提升视频MoE模型对独立与协同攻击的鲁棒性，在多个数据集与架构上一致有效；框架即插即用，推理开销比稠密模型降低超60%。

Conclusion: 组件级脆弱性分析与联合对抗训练是提升视频MoE模型鲁棒性的关键路径，J-TLAT为MoE安全部署提供了实用且高效的解决方案。

Abstract: Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.

</details>


### [170] [PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles](https://arxiv.org/abs/2602.01370)
*Leonardo Brusini,Cristian Sbrolli,Eugenio Lomurno,Toshihiko Yamasaki,Matteo Matteucci*

Main category: cs.CV

TL;DR: PolyGen提出一种多生成器协同的合成数据构建框架，通过融合不同架构生成器的输出并引入程序化难负样本课程，提升视觉语言预训练的特征多样性与组成性理解能力，在多项基准上显著超越单生成器方法。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法依赖单一生成模型，导致频谱偏差和特征多样性受限，难以满足视觉语言预训练对鲁棒性和组成性理解的需求。

Method: 提出PolyGen框架：采用Polylithic策略联合多个架构各异的生成器，在其输出交集上训练，以消除模型特异性伪影；引入Programmatic Hard Negative课程学习机制，强化细粒度句法理解；将数据预算从单一caption转向多源变体。

Result: 在多任务综合基准上较SynthCLIP提升+19.0%，在SugarCrepe++组成性基准上提升+9.1%。

Conclusion: 结构多样性（多源生成器协同）比单纯扩大单源数据规模更高效，是更具数据效率的扩展范式。

Abstract: Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.

</details>


### [171] [PromptRL: Prompt Matters in RL for Flow-Based Image Generation](https://arxiv.org/abs/2602.01382)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.CV

TL;DR: 本文提出PromptRL框架，通过在基于流的强化学习优化循环中引入可训练的语言模型作为提示词重写代理，解决了当前流匹配模型在文本到图像生成中样本效率低和提示词过拟合的问题，并在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前流匹配模型的强化学习流程存在样本效率低和提示词过拟合两大问题，限制了其泛化能力和实际应用效果。

Method: 提出PromptRL框架，将语言模型作为可训练的提示词优化代理嵌入流式强化学习循环中，实现提示重写能力与图像生成优化的协同训练。

Result: 在GenEval、OCR准确率和PickScore等基准上分别取得0.97、0.98和24.05的优异成绩；在图像编辑任务中将FLUX.1-Kontext的EditReward从1.19提升至1.43，仅用0.06百万rollouts，优于Gemini 2.5 Flash Image并媲美需复杂标注和多阶段训练的ReasonNet。

Conclusion: PromptRL显著提升了强化学习在流匹配模型中的样本效率和泛化能力，为文本到图像生成与编辑提供了更高效、鲁棒的对齐策略。

Abstract: Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.
  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.

</details>


### [172] [Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics](https://arxiv.org/abs/2602.01391)
*Xiaoyan Xing,Xiao Zhang,Sezer Karaoglu,Theo Gevers,Anand Bhattad*

Main category: cs.CV

TL;DR: 本文提出Augmented Latent Intrinsics (ALI)方法，通过融合像素对齐视觉编码器特征与潜在本征框架，并结合自监督优化策略，在仅使用无标签真实图像对的情况下显著提升了图像到图像重光照效果，尤其在金属、玻璃等高光复杂材质上效果突出。


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在本征表示的图像重光照方法在金属、玻璃等挑战性材质上表现不佳，且依赖强语义预训练特征反而降低光度保真度，暴露出语义抽象与光度保真间的根本权衡问题。

Method: 提出Augmented Latent Intrinsics (ALI)，将像素对齐视觉编码器的特征融入潜在本征框架，并设计自监督精炼策略以缓解真实配对数据稀缺问题。

Result: ALI在仅使用无标签真实图像对训练下，在重光照任务中取得显著性能提升，尤其在复杂镜面材质（如金属、玻璃）上增益最大。

Conclusion: 语义强编码器未必利于重光照任务；兼顾语义上下文与密集光度结构的混合表征（如ALI）更有效，且自监督策略可有效缓解真实配对数据不足的瓶颈。

Abstract: Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\augmented-latent-intrinsics.github.io

</details>


### [173] [Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas](https://arxiv.org/abs/2602.01418)
*Christoffer Koo Øhrstrøm,Rafael I. Cabral Muchacho,Yifei Dong,Filippos Moumtzidellis,Ronja Güldenring,Florian T. Pokorny,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 本文提出了一种基于抛物线的位置编码方法PaPE，专为视觉模态设计，具备平移/旋转不变性、距离衰减、方向性和上下文感知等特性，在多个视觉数据集上表现优异，并展现出优秀的外推能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉位置编码大多直接从语言的1D序列扩展而来，未能充分考虑视觉模态（如图像、点云、视频、事件流）的空间结构特性，存在建模不充分的问题。

Method: 提出Parabolic Position Encoding (PaPE)，基于抛物线函数构建位置编码，满足平移不变性、旋转不变性（PaPE-RI）、距离衰减、方向性和上下文感知五大原则；在8个涵盖4种视觉模态的数据集上进行评估，并开展ImageNet-1K外推实验。

Result: PaPE或PaPE-RI在8个数据集中的7个上取得最优性能；在ImageNet-1K外推实验中，绝对精度较次优方法最高提升10.5%。

Conclusion: PaPE是一种更契合视觉模态几何特性的位置编码方案，兼具强表达力与良好泛化/外推能力，为视觉Transformer架构提供了有效的位置建模新范式。

Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.

</details>


### [174] [BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images](https://arxiv.org/abs/2602.01435)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: BioTamperNet是一种面向生物医学图像篡改检测的新框架，利用受状态空间模型启发的亲和力引导注意力机制，精准定位重复区域及其来源。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然图像训练的取证模型在生物医学图像上表现不佳，而生物医学图像中的细微篡改可能严重影响实验有效性。

Method: 提出亲和力引导的自注意力与跨注意力模块，并融合轻量级、受SSM启发的线性注意力机制，实现高效细粒度定位；端到端训练以同时识别篡改区域及其源区域。

Result: 在生物医学取证基准数据集上显著优于现有主流方法，准确检测重复篡改区域。

Conclusion: BioTamperNet有效提升了生物医学图像篡改检测的精度与可解释性，为生物医学图像真实性验证提供了新思路。

Abstract: We propose BioTamperNet, a novel framework for detecting duplicated regions in tampered biomedical images, leveraging affinity-guided attention inspired by State Space Model (SSM) approximations. Existing forensic models, primarily trained on natural images, often underperform on biomedical data where subtle manipulations can compromise experimental validity. To address this, BioTamperNet introduces an affinity-guided self-attention module to capture intra-image similarities and an affinity-guided cross-attention module to model cross-image correspondences. Our design integrates lightweight SSM-inspired linear attention mechanisms to enable efficient, fine-grained localization. Trained end-to-end, BioTamperNet simultaneously identifies tampered regions and their source counterparts. Extensive experiments on the benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions. Code - https://github.com/SoumyaroopNandi/BioTamperNet

</details>


### [175] [Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles](https://arxiv.org/abs/2602.01452)
*Penghao Deng,Jidong J. Yang,Jiachen Bian*

Main category: cs.CV

TL;DR: 本文研究了基于前视摄像头的驾驶场景中驾驶员视觉注意力的语义识别任务，比较了YOLOv13、SAM2+EfficientNetV2/YOLOv13及Qwen2.5-VL系列多模态大模型三种方法；结果表明YOLOv13和Qwen2.5-VL-32b性能最优（Macro F1 > 0.84），后者在夜间识别小目标（如交通灯）上尤为鲁棒，而分割辅助方法因‘部分-整体’语义鸿沟导致召回率低；研究揭示了实时性与上下文鲁棒性之间的根本权衡。


<details>
  <summary>Details</summary>
Motivation: 理解驾驶员在驾驶过程中的视觉注意力分布对提升道路安全和开发下一代驾驶员辅助系统至关重要，需将眼动数据与道路场景语义有效关联。

Method: 将驾驶员注视点映射到道路场景语义，采用三类视觉方法：（1）直接目标检测（YOLOv13）；（2）分割辅助分类（SAM2 + EfficientNetV2 或 YOLOv13）；（3）查询式视觉语言模型（Qwen2.5-VL-7b 与 Qwen2.5-VL-32b）。

Result: YOLOv13 和 Qwen2.5-VL-32b 的 Macro F1-Score 均超 0.84；Qwen2.5-VL-32b 在夜间识别小而关键目标（如交通灯）时表现最鲁棒；分割辅助方法因‘部分-整体’语义鸿沟导致召回率大幅下降。

Conclusion: 传统检测器具备实时优势，而大VLM提供更强上下文理解与鲁棒性，二者存在根本权衡；该发现为面向人类感知的智能驾驶员监控系统设计提供了关键指导。

Abstract: Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.

</details>


### [176] [Understanding vision transformer robustness through the lens of out-of-distribution detection](https://arxiv.org/abs/2602.01459)
*Joey Kuang,Alexander Wong*

Main category: cs.CV

TL;DR: 本文研究了视觉Transformer在低比特量化下的性能，特别是其在分布外（OOD）检测任务中的鲁棒性，发现大规模预训练（如ImageNet-22k）反而削弱了4位量化模型的OOD检测能力，而数据增强可能是更优策略。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer虽性能优异，但低比特量化常导致性能下降；现有工作多关注分布内（ID）任务，本文试图通过OOD场景下注意力机制的行为，揭示量化属性并提升鲁棒性。

Method: 对DeiT、DeiT3和ViT等小型视觉Transformer进行4比特量化，在多个OOD数据集上评估其检测性能（如AUPR-out），并对比不同预训练规模（ImageNet-1k vs ImageNet-22k）和数据增强的影响。

Result: 4比特量化显著降低OOD检测性能，尤其对ImageNet-22k预训练模型影响更大（DeiT3 AUPR-out下降19.2%，ViT下降15.0%）；相比之下，ImageNet-1k预训练模型下降更小（9.5%和12.0%）；ViT在ID校准中表现稳健，但在OOD下暴露量化敏感性。

Conclusion: 大规模预训练可能损害低比特量化模型在OOD检测中的鲁棒性，数据增强或比扩大预训练数据规模更有利于提升量化稳定性。

Abstract: Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.

</details>


### [177] [Preserving Localized Patch Semantics in VLMs](https://arxiv.org/abs/2602.01530)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文提出Logit Lens Loss（LLL）来增强视觉-语言模型中图像token的局部视觉信息保留，从而提升Logit Lens可视化效果和分割等视觉任务性能，无需架构修改或大规模训练。


<details>
  <summary>Details</summary>
Motivation: Logit Lens在自回归视觉-语言模型中因图像token视觉信息扩散至语言token而失效，导致可视化不可用，亟需保持图像token的局部性。

Method: 提出Logit Lens Loss（LLL）作为next-token prediction的辅助损失，使图像token嵌入与对应图像区域的文本概念（如'cat'）语义对齐，约束自注意力中图文token混合。

Result: LLL显著提升了Logit Lens生成有意义物体置信图的能力，并在分割等视觉中心任务上取得性能提升，且无需额外解码头。

Conclusion: LLL是一种轻量、即插即用的方法，有效恢复图像token的局部可解释性，并兼顾模型视觉理解能力提升。

Abstract: Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.

</details>


### [178] [Rotation-free Online Handwritten Character Recognition Using Linear Recurrent Units](https://arxiv.org/abs/2602.01533)
*Zhe Ling,Sicheng Yu,Danyu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种结合滑动窗口路径签名（SW-PS）与线性循环单元（LRU）的在线手写字符识别框架，有效应对旋转形变挑战，在CASIA-OLHWDB1.1数据集多个子集上取得高准确率和快收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在线手写字符识别虽具优势，但实际中旋转形变会破坏笔画空间布局，显著降低识别精度；提取旋转不变特征仍是一个开放难题。

Method: 采用滑动窗口路径签名（SW-PS）提取字符局部结构特征，并引入轻量级线性循环单元（LRU）作为分类器，融合RNN的增量处理能力与状态空间模型（SSM）的并行训练效率。

Result: 在CASIA-OLHWDB1.1数据集的数字、英文大写字母和中文部首三个子集上，经集成学习后识别准确率分别达99.62%、96.67%和94.33%（最大±180°随机旋转），且收敛速度与测试精度均优于对比方法。

Conclusion: SW-PS+LRU框架能有效建模动态笔画特性并具备强旋转鲁棒性，为在线手写识别提供了一种高效、准确的新方案。

Abstract: Online handwritten character recognition leverages stroke order and dynamic features, which generally provide higher accuracy and robustness compared with offline recognition. However, in practical applications, rotational deformations can disrupt the spatial layout of strokes, substantially reducing recognition accuracy. Extracting rotation-invariant features therefore remains a challenging open problem. In this work, we employ the Sliding Window Path Signature (SW-PS) to capture local structural features of characters, and introduce the lightweight Linear Recurrent Units (LRU) as the classifier. The LRU combine the fast incremental processing capability of recurrent neural networks (RNN) with the efficient parallel training of state space models (SSM), while reliably modelling dynamic stroke characteristics. We conducted recognition experiments with random rotation angle up to $\pm 180^{\circ}$ on three subsets of the CASIA-OLHWDB1.1 dataset: digits, English upper letters, and Chinese radicals. The accuracies achieved after ensemble learning were $99.62\%$, $96.67\%$, and $94.33\%$, respectively. Experimental results demonstrate that the proposed SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy.

</details>


### [179] [Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars](https://arxiv.org/abs/2602.01538)
*Youliang Zhang,Zhengguang Zhou,Zhentao Yu,Ziyao Huang,Teng Hu,Sen Liang,Guozhen Zhang,Ziqiao Peng,Shunkai Li,Yi Chen,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Xiu Li*

Main category: cs.CV

TL;DR: 本文提出InteractAvatar框架，用于生成能与周围物体进行文本对齐交互的说话虚拟人视频，通过解耦感知、规划与视频合成，并设计PIM和AIM模块及GroundedInter基准，有效缓解控制-质量困境。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以扩展到具身人-物交互（GHOI）任务，因需环境感知且面临控制精度与生成质量之间的权衡困境。

Method: 提出双流框架InteractAvatar：包含基于检测的感知与交互模块（PIM）生成文本对齐动作，以及音频-交互感知生成模块（AIM）合成生动视频；二者通过运动-视频对齐器共享结构并并行协同生成。

Result: 在新构建的GroundedInter基准上验证了方法有效性，显著提升了具身人-物交互视频生成的质量与可控性。

Conclusion: InteractAvatar成功解耦感知规划与视频合成，缓解控制-质量困境，为 talking avatar 的具身交互生成提供了新范式。

Abstract: Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io

</details>


### [180] [FSCA-Net: Feature-Separated Cross-Attention Network for Robust Multi-Dataset Training](https://arxiv.org/abs/2602.01540)
*Yuehai Chen*

Main category: cs.CV

TL;DR: 本文提出FSCA-Net，通过显式解耦域不变与域特定特征，并引入交叉注意力融合与互信息优化，有效缓解跨数据集训练中的负迁移问题，在多个基准上实现最优泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN和Transformer模型在跨环境应用时因严重域差异导致性能下降；直接多数据集联合训练反而引发负迁移，因共享特征与域特定特征相互纠缠。

Method: 提出FSCA-Net框架：1）显式分离特征为域不变与域特定成分；2）设计交叉注意力融合模块自适应建模二者交互；3）引入互信息优化目标，最大化域不变特征一致性、最小化域特定特征冗余。

Result: 在多个crowd counting基准上显著缓解负迁移，实现SOTA跨数据集泛化性能。

Conclusion: FSCA-Net提供了一种鲁棒、可扩展的现实世界人群分析解决方案，验证了特征解耦与协同优化对提升跨域泛化能力的有效性。

Abstract: Crowd counting plays a vital role in public safety, traffic regulation, and smart city management. However, despite the impressive progress achieved by CNN- and Transformer-based models, their performance often deteriorates when applied across diverse environments due to severe domain discrepancies. Direct joint training on multiple datasets, which intuitively should enhance generalization, instead results in negative transfer, as shared and domain-specific representations become entangled. To address this challenge, we propose the Feature Separation and Cross-Attention Network FSCA-Net, a unified framework that explicitly disentangles feature representations into domain-invariant and domain-specific components. A novel cross-attention fusion module adaptively models interactions between these components, ensuring effective knowledge transfer while preserving dataset-specific discriminability. Furthermore, a mutual information optimization objective is introduced to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones, promoting complementary shared-private representations. Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization, providing a robust and scalable solution for real-world crowd analysis.

</details>


### [181] [Toward Cognitive Supersensing in Multimodal Large Language Model](https://arxiv.org/abs/2602.01541)
*Boyi Li,Yifan Shen,Yuanzhe Liu,Yifan Xu,Jiateng Liu,Xinzhuo Li,Zhengyuan Li,Jingyuan Zhu,Yunhan Zhong,Fangzhou Lan,Jianguo Cao,James M. Rehg,Heng Ji,Ismini Lourentzou,Xu Cao*

Main category: cs.CV

TL;DR: 本文提出Cognitive Supersensing训练范式，通过引入潜在视觉意象预测（LVIP）头和基于视觉潜表示的强化学习，赋予多模态大语言模型（MLLMs）类人的视觉意象能力，显著提升其在认知型视觉问答任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂认知问题（尤其依赖抽象视觉细节和视觉记忆的任务）上能力有限，主要因过度依赖文本空间的思维链推理，忽视了类人视觉空间工作记忆与视觉意象机制。

Method: 提出Cognitive Supersensing范式：1）引入Latent Visual Imagery Prediction（LVIP）头，联合学习视觉认知潜嵌入序列并对其与答案对齐，构建基于视觉的内部推理链；2）增加强化学习阶段，利用该视觉潜表示优化文本推理路径；3）构建涵盖五大认知维度的CogSense-Bench评测基准。

Result: 在CogSense-Bench上显著超越SOTA基线；在数学与科学等域外VQA基准上泛化能力更强；验证了内部视觉意象对连接感知识别与认知理解的关键作用。

Conclusion: 赋予MLLMs类人视觉意象能力可有效突破其认知瓶颈，视觉潜表示驱动的内部推理是提升复杂视觉认知任务性能的关键路径。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.

</details>


### [182] [Combined Flicker-banding and Moire Removal for Screen-Captured Images](https://arxiv.org/abs/2602.01559)
*Libo Zhu,Zihan Zhou,Zhiyi Zhou,Yiyang Qu,Weihang Zhang,Keyu Shi,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CLEAR的统一恢复框架，首次系统研究并联合去除屏幕拍摄图像中的莫尔纹和闪烁条带复合伪影，通过构建新数据集、ISP-based闪烁仿真流程及频域分解-重组模块与轨迹对齐损失，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 屏幕拍摄图像常同时存在强耦合的莫尔纹和闪烁条带，导致严重视觉质量下降；现有单退化方法无法泛化到此类复合场景。

Method: 提出CLEAR统一恢复框架，构建含两类伪影的大规模数据集，设计基于ISP的闪烁仿真流程，并引入频域分解-重组模块与轨迹对齐损失。

Result: 在多个评估指标上持续优于现有图像恢复方法，验证了其在复杂真实场景中的有效性。

Conclusion: CLEAR框架为联合去除莫尔纹与闪烁条带提供了首个系统性解决方案，显著提升了屏幕拍摄图像的复原质量。

Abstract: Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moiré patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moiré patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moiré patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.

</details>


### [183] [Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd](https://arxiv.org/abs/2602.01561)
*Yejin Son,Saejin Kim,Dongjun Min,Younjae Yu*

Main category: cs.CV

TL;DR: 本文提出了Multimodal UNcommonsense (MUN)基准，用于评估多模态模型在违背常规视觉或语境预期场景下的常识推理能力，并设计了基于检索的上下文学习（R-ICL）框架以提升小模型在非典型场景中的推理性能。


<details>
  <summary>Details</summary>
Motivation: 常识推理在多模态背景下仍具挑战性，现有基准多聚焦于常见、典型场景，缺乏对违背常规预期的非典型多模态情境的评估能力。

Method: 构建MUN基准，包含图像与反常语言描述配对；提出R-ICL框架，结合新型多模态集成检索器（MER），在无需微调的前提下将大模型推理能力迁移至小模型。

Result: R-ICL在MUN上平均超越基线ICL方法8.3%，验证了其在低频、非典型场景下的有效性。

Conclusion: MUN为评估和提升多模态模型在真实世界、文化多样及非原型场景中的鲁棒性与适应性提供了新方向。

Abstract: Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.

</details>


### [184] [One-Step Diffusion for Perceptual Image Compression](https://arxiv.org/abs/2602.01570)
*Yiwen Jia,Hao Wei,Yanhui Zhou,Chenyang Ge*

Main category: cs.CV

TL;DR: 本文提出了一种单步扩散图像压缩方法，通过引入基于紧凑特征表示的判别器，在保持高质量重建的同时大幅降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的图像压缩方法因解码时需要大量去噪步骤，导致推理延迟高、计算开销大，限制了实际部署。

Method: 设计单步扩散过程以加速解码；引入作用于紧凑特征而非原始像素的判别器，以更好建模高层纹理与结构信息。

Result: 在保持与最新扩散方法相当压缩性能的同时，推理速度提升46倍。

Conclusion: 单步扩散结合特征级判别器是兼顾高效性与感知质量的有效图像压缩新范式。

Abstract: Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.

</details>


### [185] [SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01574)
*Haobo Wang,Weiqi Luo,Xiaojun Jia,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出SGHA-Attack，一种语义引导的分层对齐攻击框架，通过多参考锚点和中间层特征对齐提升对抗扰动在异构大视觉语言模型上的跨模型迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于迁移的针对性攻击方法易过拟合代理模型的嵌入空间，仅依赖单参考且强调最终层对齐，未能充分利用中间语义，导致在异构VLM间迁移性差。

Method: 提出SGHA-Attack：1）利用冻结的文生图模型生成视觉接地的参考池，并选取Top-K语义最相关锚点构建加权混合指导；2）在多个深度上对齐全局与空间粒度的中间视觉表征；3）在共享潜在子空间中同步中间视觉与文本特征，提供早期跨模态监督。

Result: 在开源与商用黑盒VLM上实验表明，SGHA-Attack显著优于先前方法，具备更强的目标迁移性，并对预处理与净化防御保持鲁棒性。

Conclusion: 语义引导的分层对齐策略可有效提升对抗扰动在异构大视觉语言模型间的迁移能力，为黑盒VLM安全评估提供了新思路。

Abstract: Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.

</details>


### [186] [HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation](https://arxiv.org/abs/2602.01586)
*Wencan Cheng,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出HandMCM方法，基于Mamba状态空间模型，结合局部信息注入/过滤与对应关系建模模块，并融合多模态图像特征，显著提升严重遮挡场景下的3D手部姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 3D手部姿态估计在增强现实等人机交互应用中至关重要，但面临手部自遮挡及与物体交互导致的遮挡等挑战。

Method: 提出基于Mamba状态空间模型的HandMCM方法，引入局部信息注入/过滤模块和对应关系建模模块，并融合多模态图像特征以增强输入鲁棒性与表征能力。

Result: 在三个基准数据集上的实验表明，该方法显著优于当前最先进方法，尤其在严重遮挡场景下表现突出。

Conclusion: HandMCM有效提升了3D手部姿态估计的准确性与可靠性，展现出在实际应用中的巨大潜力。

Abstract: 3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.

</details>


### [187] [Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages](https://arxiv.org/abs/2602.01591)
*Zhixiong Yue,Zixuan Ni,Feiyang Ye,Jinshan Zhang,Sheng Shen,Zhenpeng Mi*

Main category: cs.CV

TL;DR: 本文提出TAFS GRPO框架，通过温度退火与分组相对策略优化，提升流匹配文本到图像生成模型在少步采样下的人类偏好对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的流匹配模型依赖大量去噪步数，且奖励信号稀疏不精确，导致对齐效果欠佳。

Method: 提出温度退火少步采样与分组相对策略优化（TAFS GRPO）：迭代注入自适应时间噪声于单步采样结果，并通过逐步退火引入可控随机性；结合GRPO实现无需可微奖励函数的步级优势整合，提供稠密、步感知的奖励信号。

Result: 在少步文本到图像生成任务中显著提升人类偏好对齐效果，实验验证其有效性与稳定性。

Conclusion: TAFS GRPO是一种高效、稳定且无需可微奖励的强化学习训练框架，有效解决了流匹配模型在少步生成中对齐人类偏好的关键挑战。

Abstract: Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.

</details>


### [188] [Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework](https://arxiv.org/abs/2602.01593)
*Wenzhuo Zhao,Keren Fu,Jiahao He,Xiaohong Liu,Qijun Zhao,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种基于状态空间模型Mamba的纯Mamba架构Saliency Mamba（Samba），用于多种显著性目标检测任务（如RGB/RGB-D/RGB-T、视频、多模态等），通过引入显著性引导的Mamba块（SGMB）和上下文感知上采样（CAU）提升性能；进一步提出Samba+，采用多任务联合训练及跨模态图注意力（HGA）与模态锚定持续学习（MACL）策略，实现统一、通用且高效的显著性检测。


<details>
  <summary>Details</summary>
Motivation: 现有显著性目标检测模型受限于CNN感受野有限和Transformer计算复杂度高；同时，传统方法存在任务专用性强、难以统一处理多模态输入和持续适应等问题。

Method: 提出Samba：基于Mamba的状态空间模型，设计显著性引导Mamba块（SGMB）与空间邻域扫描（SNS）算法，以及上下文感知上采样（CAU）；进一步提出Samba+：采用多任务联合训练，并引入hub-and-spoke图注意力（HGA）模块实现自适应跨模态融合，结合模态锚定持续学习（MACL）缓解模态冲突与灾难性遗忘。

Result: Samba在6类SOD任务、22个数据集上以更低计算成本超越现有方法；Samba+用单一模型在全部任务与数据集上取得更优性能；验证了框架在多模态与持续学习场景下的有效性与潜力。

Conclusion: Samba/Samba+首次将Mamba成功应用于显著性目标检测全系列任务，兼顾全局建模能力、计算效率与模型通用性，为多模态、多任务视觉显著性检测提供了新范式。

Abstract: Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the "task-specific" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.

</details>


### [189] [UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception](https://arxiv.org/abs/2602.01594)
*Wenzhuo Liu,Qiannan Guo,Zhen Wang,Wenshuo Wang,Lei Yang,Yicheng Qiao,Lening Wang,Zhiwei Li,Chen Lv,Shanghang Zhang,Junqiang Xi,Huaping Liu*

Main category: cs.CV

TL;DR: 本文提出了一种统一且通用的多模态多任务学习框架（UV-M3TL），用于协同理解驾驶员行为、情绪、车辆行为与交通上下文，通过双分支空间通道嵌入（DB-SCME）和自适应特征解耦损失（AFD-Loss）缓解任务间负迁移，在AIDE及多个公开数据集上均达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: ADAS需同时理解人类驾驶员行为与导航上下文，但联合学习异构任务易引发负迁移，损害系统性能。

Method: 提出UV-M3TL框架，包含双分支空间通道多模态嵌入（DB-SCME）以建模任务共享与任务特异性特征，以及自适应特征解耦多任务损失（AFD-Loss）以稳定联合优化并引导多样性表征学习。

Result: 在AIDE数据集上四任务均达SOTA；在BDD100K、CityScapes、NYUD-v2、PASCAL-Context等多任务基准上也表现优异，多数任务达SOTA。

Conclusion: UV-M3TL有效缓解多任务负迁移，兼具高性能与强泛化能力，为ADAS及通用多任务感知提供了新范式。

Abstract: Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.

</details>


### [190] [Token Pruning for In-Context Generation in Diffusion Transformers](https://arxiv.org/abs/2602.01609)
*Junqing Lin,Xingyu Zheng,Pei Cheng,Bin Fu,Jingwei Sun,Guangzhong Sun*

Main category: cs.CV

TL;DR: 本文提出ToPi框架，一种无需训练的token剪枝方法，用于解决扩散Transformer（DiTs）在上下文生成中因输入拼接导致的序列过长和计算瓶颈问题；通过离线校准的敏感性分析识别关键注意力层，并设计新的影响度度量与时间更新策略，实现高效、保真、一致的图像生成加速。


<details>
  <summary>Details</summary>
Motivation: 现有token压缩方法针对文本到图像任务设计，采用统一压缩策略，无法应对图像到图像上下文生成中参考上下文与目标潜在表示之间在空间、时间和功能维度上的角色不对称性，导致计算效率低下。

Method: ToPi是一种训练无关的token剪枝框架：首先通过离线校准驱动的敏感性分析识别关键注意力层，作为冗余估计的代理；继而基于这些层构建新影响度指标，量化各参考上下文token的贡献以支持选择性剪枝；并引入适配扩散过程演化的时序更新策略。

Result: 实验表明ToPi可在保持结构保真度与视觉一致性的前提下，在复杂图像生成任务中实现超30%的推理速度提升。

Conclusion: ToPi有效解决了DiTs在上下文生成中的长序列计算瓶颈，其基于敏感性分析与动态影响评估的剪枝范式为高效可控图像生成提供了新思路。

Abstract: In-context generation significantly enhances Diffusion Transformers (DiTs) by enabling controllable image-to-image generation through reference examples. However, the resulting input concatenation drastically increases sequence length, creating a substantial computational bottleneck. Existing token reduction techniques, primarily tailored for text-to-image synthesis, fall short in this paradigm as they apply uniform reduction strategies, overlooking the inherent role asymmetry between reference contexts and target latents across spatial, temporal, and functional dimensions. To bridge this gap, we introduce ToPi, a training-free token pruning framework tailored for in-context generation in DiTs. Specifically, ToPi utilizes offline calibration-driven sensitivity analysis to identify pivotal attention layers, serving as a robust proxy for redundancy estimation. Leveraging these layers, we derive a novel influence metric to quantify the contribution of each context token for selective pruning, coupled with a temporal update strategy that adapts to the evolving diffusion trajectory. Empirical evaluations demonstrate that ToPi can achieve over 30\% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.

</details>


### [191] [Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?](https://arxiv.org/abs/2602.01623)
*Susan Liang,Chao Huang,Filippos Bellos,Yolo Yunlong Tang,Qianxiang Shen,Jing Bi,Luchuan Song,Zeliang Zhang,Jason Corso,Chenliang Xu*

Main category: cs.CV

TL;DR: 本文提出Omni-Judge，探索全能大语言模型（omni-LLMs）作为文本驱动音视频生成质量评估器的可行性；结果表明其在语义对齐类指标上表现优异且可解释，但在高帧率感知指标上受限于时序分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估指标（如FVD、CLAP、ViCLIP）仅关注双模态、难以处理复杂提示、缺乏可解释性；而人工评估成本高、难扩展；omni-LLMs具备三模态理解与推理能力，有望成为更统一、可解释的评估方案。

Method: 构建Omni-Judge框架，利用omni-LLMs对文本-音频-视频三模态生成结果进行九项感知与对齐指标的自动评估，并与传统指标及人类评分对比相关性，同时分析其可解释性反馈能力。

Result: Omni-Judge在音频-文本、视频-文本、音视频文本一致性等语义任务上达到甚至超越传统指标的相关性；但在视频质量、音视频同步等高FPS感知指标上表现较差；能生成揭示语义或物理不一致的可解释反馈。

Conclusion: omni-LLMs有潜力成为多模态生成的统一评估器，尤其在语义层面；但当前受限于时序建模能力，在精细感知评估上仍需改进；其可解释性支持反馈驱动的生成优化等下游应用。

Abstract: State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.

</details>


### [192] [PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards](https://arxiv.org/abs/2602.01624)
*Minh-Quan Le,Gaurav Mittal,Cheng Zhao,David Gu,Dimitris Samaras,Mei Chen*

Main category: cs.CV

TL;DR: 本文提出了一种无需人工标注的文本到视频生成后训练算法PISCES，通过双层最优传输（OT）对齐奖励机制，在分布级和离散token级分别建模视频质量与语义对齐，显著提升了生成视频的质量、时序一致性与文本-视频语义匹配度。


<details>
  <summary>Details</summary>
Motivation: 现有基于奖励的后训练方法依赖大规模人工偏好标注或使用预训练多模态模型中错位的嵌入，导致可扩展性差或监督信号次优。

Method: 提出PISCES算法，核心是Dual Optimal Transport (OT)-aligned Rewards模块：在分布层面用OT对齐文本与视频整体嵌入以评估质量与时序一致性；在离散token层面用OT对齐文本词元与视频帧/区域特征以增强语义-时空对应。该模块兼容直接反向传播与强化学习微调。

Result: 在VBench基准上，PISCES在短/长视频生成任务中均超越有标注和无标注基线方法，Quality和Semantic得分更优；人类偏好研究进一步验证其有效性；模块具备跨优化范式的兼容性。

Conclusion: PISCES首次将最优传输理论系统引入无标注生成式后训练，实现了高质量、高对齐度的文本到视频生成，为免标注奖励建模提供了新范式。

Abstract: Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.

</details>


### [193] [Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks](https://arxiv.org/abs/2602.01630)
*Bohan Zeng,Kaixin Zhu,Daili Hua,Bozhou Li,Chengzhuo Tong,Yuran Wang,Xinyi Huang,Yifan Dai,Zixiang Zhang,Yifan Yang,Zhou Liu,Hao Liang,Xiaochen Ma,Ruichuan An,Tianyi Bai,Hongcheng Gao,Junbo Niu,Yang Shi,Xinlong Chen,Yue Ding,Minglei Shi,Kai Zeng,Yiwen Tang,Yuanxing Zhang,Pengfei Wan,Xintao Wang,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文分析了当前世界模型研究的碎片化问题，提出了一种统一的设计规范，强调将交互、感知、符号推理和空间表征整合为一个规范性框架，以推动更通用、鲁棒和原理性的世界建模。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型研究过于分散，聚焦于孤立任务（如视觉预测、3D估计、符号 grounding），缺乏统一定义与系统性框架，难以支撑整体世界理解。

Method: 通过分析现有碎片化方法的局限性，提出一种整合交互、感知、符号推理和空间表征的统一世界模型设计规范。

Result: 提出了一个面向通用性、鲁棒性和原理性的世界模型统一设计规范，为后续研究提供结构化指导。

Conclusion: 世界模型不应是能力的松散集合，而应是一个规范性框架；统一设计有助于实现真正意义上的整体世界理解与智能体自主交互能力。

Abstract: World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.

</details>


### [194] [Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification](https://arxiv.org/abs/2602.01633)
*Xinyuan Zhao,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: 本文提出了一种面向联邦学习的动态自适应焦点损失（DAFL）与客户端感知聚合策略，以应对医疗图像数据中的类不平衡和客户端异质性问题，在多个公开医学图像数据集上显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型（如ViT）依赖大量数据，但医疗图像受隐私法规限制难以集中获取；联邦学习虽可避免数据交换，却面临本地客户端数据异质性和类别不平衡导致的泛化能力差问题。

Method: 提出基于动态自适应焦点损失（DAFL）的本地训练方法，设计动态类别不平衡系数以适配各客户端样本分布；并采用基于数据规模与特征的加权客户端感知聚合策略。

Result: 在ISIC、Ocular Disease和RSNA-ICH三个医学图像数据集上，该框架在多数情况下优于DenseNet121、ResNet50、ViT系列、FedCLIP、Swin Transformer、CoAtNet和MixNet，准确率提升0.98%–41.69%；消融实验验证了DAFL与聚合策略的有效性。

Conclusion: DAFL与客户端感知聚合策略能有效缓解联邦学习中由数据异质性和类别不平衡带来的性能下降，为隐私敏感的医学图像分析提供了更鲁棒、公平的分布式训练方案。

Abstract: While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\% to 41.69\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.

</details>


### [195] [ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval](https://arxiv.org/abs/2602.01639)
*Tianyu Yang,ChenWei He,Xiangzhao Hao,Tianyue Wang,Jiarui Guo,Haiyun Guo,Leigang Qu,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出ReCALL框架，通过诊断-生成-精炼三步法解决将生成式多模态大语言模型（MLLM）适配为判别式检索器时导致的细粒度推理能力退化问题，在CIR任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有将生成式MLLM适配为判别式检索器的方法存在范式冲突，导致其原生的细粒度推理能力退化（Capability Degradation），难以胜任需要跨模态组合推理的合成图像检索（CIR）任务。

Method: 提出ReCALL框架，包含三个阶段：1）通过自引导信息实例挖掘诊断检索器的认知盲点；2）利用思维链（CoT）提示基础MLLM生成修正指令与三元组，并通过VQA一致性过滤进行质量控制；3）基于分组对比学习策略对检索器进行持续训练，使其嵌入空间重对齐MLLM内在的组合推理能力。

Result: 在CIRR和FashionIQ数据集上，ReCALL显著缓解能力退化问题，持续提升性能，达到当前最优水平（SOTA）。

Conclusion: ReCALL是一种模型无关的通用框架，能有效弥合生成式MLLM与判别式检索任务之间的范式鸿沟，恢复并增强其细粒度视觉语义区分与组合推理能力。

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.

</details>


### [196] [Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning](https://arxiv.org/abs/2602.01649)
*Yinchao Ma,Qiang Zhou,Zhibin Wang,Xianing Chen,Hanqing Yang,Jun Song,Bo Zheng*

Main category: cs.CV

TL;DR: 本文提出CaCoVID算法，通过强化学习优化视频token选择策略，以提升视频理解任务中的token压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型因视频token冗余导致推理计算开销大，且基于注意力分数的压缩方法与实际预测贡献关系不明确。

Method: 提出贡献感知的视频token压缩算法CaCoVID：1）构建基于强化学习的策略网络，主动选择对正确预测贡献最大的token组合；2）设计带在线组合空间采样的组合策略优化算法，降低搜索空间并加速收敛。

Result: 在多个视频理解基准上验证了CaCoVID的有效性，显著提升了压缩效率与模型性能平衡。

Conclusion: CaCoVID通过显式建模token对预测的贡献，实现了更优的视频token压缩，为视频大模型高效部署提供了新思路。

Abstract: Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \textbf{C}ontribution-\textbf{a}ware token \textbf{Co}mpression algorithm for \textbf{VID}eo understanding (\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.

</details>


### [197] [From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction](https://arxiv.org/abs/2602.01661)
*Xingyu Miao,Junting Dong,Qin Zhao,Yuhang Yang,Junhao Chen,Yang Long*

Main category: cs.CV

TL;DR: 本文提出了一种面向视频序列的人类中心密集预测方法，通过构建可扩展的合成数据流水线生成运动对齐、几何标注完备的逼真人像序列，并设计了融合显式人体几何先验与轻量通道重加权机制的ViT模型，结合两阶段训练策略，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在单帧精度上表现良好，但在运动、遮挡和光照变化下存在闪烁问题，且缺乏多任务配对的人类视频监督数据。

Method: 构建了可扩展的合成数据流水线，生成具有像素级深度、法向和掩码的逼真人像序列；提出基于ViT的统一密集预测器，引入CSE嵌入作为显式人体几何先验，并在特征融合后加入轻量通道重加权模块；采用两阶段训练：静态预训练+动态序列监督。

Result: 在THuman2.1和Hi4D数据集上达到SOTA性能，并在野外视频上展现出良好泛化能力。

Conclusion: 所提合成数据策略与几何感知模型架构有效提升了视频中人类密集预测的时空一致性，为多任务人像理解提供了新范式。

Abstract: In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.

</details>


### [198] [Moonworks Lunara Aesthetic II: An Image Variation Dataset](https://arxiv.org/abs/2602.01666)
*Yan Wang,Partho Hassan,Samiha Sadeka,Nada Soliman,M M Sayeef Abdullah,Sabit Hassan*

Main category: cs.CV

TL;DR: Lunara Aesthetic II 是一个公开、伦理采集的图像数据集，包含2854组锚点关联的变体图像对，用于评估和提升图像生成与编辑系统在上下文一致性、身份保持和美学质量方面的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像生成与编辑系统在上下文一致性、身份保持和可控编辑方面缺乏高质量、可解释监督信号的问题。

Method: 构建一个由专业创作的艺术与摄影图像组成的高审美数据集，每组包含原始图像及其经过光照、天气、视角等上下文变换但身份稳定的变体对，以身份保持的上下文变化作为监督信号。

Result: 该数据集展现出高身份稳定性、强目标属性实现能力以及超越大规模网络数据集的稳健美学表现。

Conclusion: Lunara Aesthetic II 为图像生成与编辑模型的基准测试、微调与分析提供了高质量、可解释、关系型监督的数据基础，推动上下文泛化与编辑鲁棒性研究。

Abstract: We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.

</details>


### [199] [SMTrack: State-Aware Mamba for Efficient Temporal Modeling in Visual Tracking](https://arxiv.org/abs/2602.01677)
*Yinchao Ma,Dengqing Yang,Zhangyu He,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于状态空间模型的新型视觉跟踪方法SMTrack，通过选择性状态感知机制和线性复杂度计算，有效建模长程时序依赖，无需定制模块或高计算开销，实现了高效鲁棒的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统CNN和Transformer在建模长程时序依赖方面存在固有局限，常需复杂定制模块或高计算成本，难以兼顾跟踪鲁棒性与效率。

Method: 提出State-aware Mamba Tracker（SMTrack），引入选择性状态感知状态空间模型，采用帧间隐藏状态传播与更新机制，在训练中实现线性复杂度的长程时序交互。

Result: SMTrack在保持低计算成本的同时，在多个基准上展现出优异的跟踪性能。

Conclusion: SMTrack提供了一种简洁、高效且鲁棒的时序建模范式，为视觉跟踪中的长程依赖建模提供了新思路。

Abstract: Visual tracking aims to automatically estimate the state of a target object in a video sequence, which is challenging especially in dynamic scenarios. Thus, numerous methods are proposed to introduce temporal cues to enhance tracking robustness. However, conventional CNN and Transformer architectures exhibit inherent limitations in modeling long-range temporal dependencies in visual tracking, often necessitating either complex customized modules or substantial computational costs to integrate temporal cues. Inspired by the success of the state space model, we propose a novel temporal modeling paradigm for visual tracking, termed State-aware Mamba Tracker (SMTrack), providing a neat pipeline for training and tracking without needing customized modules or substantial computational costs to build long-range temporal dependencies. It enjoys several merits. First, we propose a novel selective state-aware space model with state-wise parameters to capture more diverse temporal cues for robust tracking. Second, SMTrack facilitates long-range temporal interactions with linear computational complexity during training. Third, SMTrack enables each frame to interact with previously tracked frames via hidden state propagation and updating, which releases computational costs of handling temporal cues during tracking. Extensive experimental results demonstrate that SMTrack achieves promising performance with low computational costs.

</details>


### [200] [FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding](https://arxiv.org/abs/2602.01683)
*Kangcong Li,Peng Ye,Lin Zhang,Chao Wang,Huafeng Qin,Tao Chen*

Main category: cs.CV

TL;DR: 本文提出FreshMem，一种频率-空间混合记忆网络，用于在线流视频理解，通过多尺度频率记忆和空间缩略图记忆模块，在不进行训练的情况下显著提升多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从离线到在线流视频理解过渡中缺乏灵活适应性，导致不可逆的细节丢失和上下文碎片化。

Method: 提出FreshMem网络，包含多尺度频率记忆（MFM）模块和空间缩略图记忆（STM）模块，分别处理频率域代表性和空间域高密度摘要。

Result: FreshMem在StreamingBench、OV-Bench和OVO-Bench上分别提升Qwen2-VL基线5.20%、4.52%和2.34%，且作为免训练方案优于多个全量微调方法。

Conclusion: FreshMem为长时序流视频理解提供了一种高效、免训练的新范式。

Abstract: Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical "gist"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.

</details>


### [201] [Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection](https://arxiv.org/abs/2602.01696)
*Jiaming Cui,Shuai Zhou,Wenqiang Li,Ruifeng Qin,Feng Shen*

Main category: cs.CV

TL;DR: 本文提出CMAFNet，一种融合RGB外观与深度几何信息的跨模态对齐与融合网络，通过特征净化与上下文语义集成提升输电线路小缺陷检测性能，在TLRGBD数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 输电线路缺陷检测面临小目标占比高、背景复杂、光照变化大等挑战，现有RGB检测器难以在低色度对比下区分几何细微缺陷与相似背景结构。

Method: 提出CMAFNet：包含基于字典学习的语义重组模块（实现模态特异性噪声抑制与缺陷判别信息保留）和基于部分通道注意力的上下文语义集成框架（建模全局空间依赖）；引入位置归一化实现显式重建驱动的跨模态对齐，保障异构特征统计兼容性后再融合。

Result: 在TLRGBD基准（94.5%为小目标）上达到32.2% mAP@50和12.5% APs，分别超越最强基线9.8和4.0个百分点；轻量版达24.8% mAP50、228 FPS、仅4.9M参数，性能优于所有YOLO类检测器，媲美Transformer类方法且计算成本更低。

Conclusion: CMAFNet通过跨模态对齐与融合机制有效缓解小目标、复杂背景与光照变化带来的检测困难，验证了RGB-Depth联合建模在电力巡检中的有效性与实用性。

Abstract: Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.

</details>


### [202] [Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis](https://arxiv.org/abs/2602.01710)
*Salma Zahran,Zhou Ao,Zhengyang Zhang,Chen Chi,Chenchen Yuan,Yanming Wang*

Main category: cs.CV

TL;DR: 本文提出一种无需人工标注的语义分割框架，利用相场模拟生成带完美真值标签的微结构图像，并通过CycleGAN将模拟图像转换为高保真SEM图像，训练U-Net模型在真实实验图像上实现高精度分割（Boundary F1=0.90，IoU=0.88），成功弥合仿真到现实的域差距。


<details>
  <summary>Details</summary>
Motivation: 显微图像语义分割对高通量材料表征至关重要，但受限于专家标注数据成本高、主观性强且稀缺；物理仿真虽可替代人工标注，却因仿真与实验图像间存在显著域差距（如纹理、噪声、伪影差异）而难以泛化。

Method: 采用相场模拟生成大量具完美真值掩码的微结构形态；使用无配对CycleGAN将干净仿真图像翻译为高保真、逼真的扫描电镜（SEM）图像；仅用该合成数据训练U-Net模型；并通过t-SNE特征投影和香农熵分析验证合成数据与真实数据的统计与特征一致性。

Result: U-Net在未见过的实验图像上达到平均边界F1分数0.90和交并比（IoU）0.88；t-SNE与香农熵分析证实合成图像与真实图像在特征空间和统计分布上不可区分。

Conclusion: 该生成式框架完全摆脱人工标注依赖，将数据稀缺问题转化为数据丰裕问题，为材料发现与分析提供鲁棒、全自动的解决方案。

Abstract: Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.

</details>


### [203] [FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization](https://arxiv.org/abs/2602.01723)
*Yikun Ma,Yiqing Li,Jingwen Ye,Zhongkai Wu,Weidong Zhang,Lin Gao,Zhi Jin*

Main category: cs.CV

TL;DR: 本文提出FastPhysGS框架，通过实例感知粒子填充（IPF）和双向图解耦优化（BGDO），实现高效、鲁棒的基于物理的动态3D高斯泼溅模拟，仅需1分钟和7GB内存即可完成高保真仿真。


<details>
  <summary>Details</summary>
Motivation: 现有将3D高斯泼溅扩展到4D物理仿真的方法存在手动调参依赖、泛化性差、优化效率低、文本/图像到3D感知鸿沟导致物理行为不稳定、忽略3DGS表面结构等问题。

Method: 提出FastPhysGS框架：(1) 实例感知粒子填充（IPF）结合蒙特卡洛重要性采样（MCIS），高效填充内部粒子并保持几何保真度；(2) 双向图解耦优化（BGDO），自适应优化由视觉语言模型（VLM）预测的材料参数。

Result: 实验表明FastPhysGS可在1分钟内、仅使用7GB运行内存完成高保真物理仿真，性能优于先前方法。

Conclusion: FastPhysGS是一种快速、鲁棒的物理驱动动态3DGS仿真新范式，具有广泛的应用潜力。

Abstract: Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.

</details>


### [204] [DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation](https://arxiv.org/abs/2602.01724)
*Tushar Anand,Maheswar Bora,Antitza Dantcheva,Abhijit Das*

Main category: cs.CV

TL;DR: 本文提出了一种名为DenVisCoM的新型Mamba块及一种专为光流与视差估计设计的混合架构，通过联合建模运动与3D稠密感知任务，在保证实时性、低内存占用的同时提升精度。


<details>
  <summary>Details</summary>
Motivation: 光流与视差估计等多视图几何与运动任务本质相关，但现有方法常单独处理，缺乏兼顾实时性、内存效率与精度的统一方案。

Method: 提出DenVisCoM Mamba模块，并结合Transformer注意力机制构建混合网络架构，实现光学流与视差的联合实时估计。

Result: 在多个数据集上验证了模型在精度与实时性权衡上的优越性，实现了高精度且实时的光流与视差联合估计。

Conclusion: 所提DenVisCoM混合架构有效统一了运动与3D感知任务，在准确率、速度和内存开销之间取得了良好平衡，具备实际部署潜力。

Abstract: In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.

</details>


### [205] [Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models](https://arxiv.org/abs/2602.01738)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Bin Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于冻结视觉基础模型特征的简单线性分类器，在AI生成图像检测任务中显著超越现有专用检测器，尤其在真实场景（in-the-wild）下提升超30%准确率；其能力源于预训练数据中大规模合成内容带来的隐式或显式伪造感知，但对重拍、传输失真、VAE重建及局部编辑仍存在盲区；主张AI取证应转向利用基础模型的动态世界知识，而非过拟合静态基准。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器在标准基准上表现优异，但在真实复杂场景下性能急剧下降，亟需更鲁棒、泛化更强的方法。

Method: 采用冻结现代视觉基础模型（如Perception Encoder、MetaCLIP 2、DINOv3）的特征，仅训练一个简单线性分类器进行AIGI检测，并在多类基准、未见生成器及真实分布上系统评估。

Result: 该线性基线在标准基准上媲美专用检测器，在in-the-wild数据集上准确率提升超30%；揭示VLM具备显式伪造语义概念，SSL模型隐式学习鉴别性法证特征；但对重拍、传输、VAE重建和局部编辑鲁棒性不足。

Conclusion: 应推动AI取证范式转变——从依赖手工设计、过拟合静态基准的专用检测器，转向利用大规模预训练基础模型所蕴含的、随世界演化的通用伪造感知能力，以实现真实场景下的可靠部署。

Abstract: While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.

</details>


### [206] [Tail-Aware Post-Training Quantization for 3D Geometry Models](https://arxiv.org/abs/2602.01741)
*Sicheng Pan,Chen Tang,Shuzhao Xie,Ke Yang,Weixiang Zhang,Jiawei Li,Bin Chen,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 本文提出TAPTQ，一种面向3D几何学习的尾部感知后训练量化方法，通过渐进式校准构建、三元搜索优化量化区间和TRE引导的模块级补偿，显著提升量化精度并降低校准开销。


<details>
  <summary>Details</summary>
Motivation: 现有面向2D视觉Transformer优化的后训练量化（PTQ）方法难以有效迁移至3D模型，因其特征分布复杂且校准开销大，而3D几何模型在资源受限平台部署面临严峻挑战。

Method: 提出TAPTQ：1）渐进式粗到细校准构造策略，以构建兼具统计纯度与几何表征力的小规模校准子集；2）将量化区间搜索建模为优化问题，引入三元搜索求解器，将复杂度由O(N)降至O(log N)；3）提出TRE引导的模块级补偿机制，利用尾部相对误差（TRE）指标自适应识别并修正对长尾激活异常值敏感的模块失真。

Result: 在VGGT和Pi3基准上，TAPTQ在精度上持续超越当前最优PTQ方法，同时大幅减少校准时间。

Conclusion: TAPTQ是一种专为3D几何学习设计的高效、高精度后训练量化方案，解决了传统PTQ在3D场景下迁移性差与校准成本高的核心问题。

Abstract: The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\mathcal{O}(N)$ to $\mathcal{O}(\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.

</details>


### [207] [ObjEmbed: Towards Universal Multimodal Object Embeddings](https://arxiv.org/abs/2602.01753)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 本文提出ObjEmbed，一种新型多模态大语言模型嵌入方法，通过将图像分解为多个区域嵌入（每个对应一个物体）并结合全局嵌入，实现细粒度的图像-文本对齐，支持视觉定位、局部/全局图像检索等任务，并在18个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态嵌入模型擅长全局图像-文本对齐，但在图像区域与文本短语之间的细粒度对齐方面存在不足。

Method: 提出ObjEmbed模型，将输入图像分解为多个区域嵌入（每个对应一个物体），每个区域生成两个互补嵌入：语义匹配用的对象嵌入和预测定位质量的IoU嵌入；最终匹配得分融合语义相似性与预测IoU；所有区域及全局图像通过单次前向传播完成编码。

Result: 在18个多样化基准测试中展现出优越性能，验证了其强语义判别能力与高效性。

Conclusion: ObjEmbed实现了兼顾细粒度定位与全局语义理解的高效多模态嵌入，具备对象导向性、任务通用性与编码高效性三大优势。

Abstract: Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.

</details>


### [208] [Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration](https://arxiv.org/abs/2602.01754)
*Gustavo P. C. P. da Luz,Alvaro M. Aspilcueta Narvaez,Tiago Godoi Bannwart,Gabriel Massuyoshi Sato,Luis Fernando Gomez Gonzalez,Juliana Freitag Borin*

Main category: cs.CV

TL;DR: 本文提出了一种基于距离感知匹配与自适应边界框分割的车位级智能停车监控系统，提升了YOLOv11m在边缘设备上的精度（98.80%）与实用性，并引入数字影子和复用TV盒的应用服务器，支持可持续、可扩展的智慧停车服务。


<details>
  <summary>Details</summary>
Motivation: 原有区域级车辆计数方法虽准确但无法提供车位级细粒度信息，限制了高级应用；需提升空间定位精度并增强系统可持续性与可扩展性。

Method: 采用距离感知匹配算法（含空间容差）实现车位级识别，结合自适应边界框分区法优化复杂车位检测；构建数字影子作为数字孪生基础；利用复用TV盒搭建轻量应用支持服务器。

Result: 在资源受限边缘设备上达到98.80%平均精度，单次推理耗时8秒；成功部署数字影子与TV盒服务器，实现云-端-边协同及硬件复用。

Conclusion: 该扩展系统显著提升了车位级监测能力与系统可持续性，为智慧停车向数字孪生演进提供了可行架构与实践路径。

Abstract: Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.

</details>


### [209] [Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation](https://arxiv.org/abs/2602.01756)
*Jun He,Junyan Ye,Zilong Huang,Dongzhi Jiang,Chenjue Zhang,Leqi Zhu,Renrui Zhang,Xiang Zhang,Weijia Li*

Main category: cs.CV

TL;DR: 本文提出Mind-Brush，一种模拟人类‘思考-检索-创作’范式的动态、知识驱动的文本到图像生成代理框架，通过多模态证据检索与推理工具提升对隐含意图和分布外概念的理解能力，并构建新基准Mind-Bench进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型多为静态解码器，难以理解用户隐含意图、进行复杂知识推理，且无法适应现实世界动态变化。

Method: 提出Mind-Brush统一代理框架，融合主动多模态检索与外部推理工具，实现动态知识驱动生成；同时构建包含500个样本的Mind-Bench新基准，覆盖实时新闻、新兴概念及数学/地理推理等场景。

Result: Mind-Brush在Mind-Bench上使Qwen-Image基线实现从零到一的能力跃升，并在WISE和RISE等既有基准上取得更优结果。

Conclusion: Mind-Brush验证了将生成过程建模为动态代理工作流的有效性，为文本到图像生成迈向真正理解与适应性迈出了关键一步。

Abstract: While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.

</details>


### [210] [MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement](https://arxiv.org/abs/2602.01760)
*Hao Zhang,Yanping Zha,Zizhuo Li,Meiqi Gong,Jiayi Ma*

Main category: cs.CV

TL;DR: 本文提出MagicFuse框架，通过扩散模型实现单张可见光图像的跨谱场景表征生成，在仅使用低质量可见光图像的情况下，达到甚至超越多模态融合方法的视觉与语义性能。


<details>
  <summary>Details</summary>
Motivation: 在仅有可见光传感器可用的严苛条件下，如何持续利用多模态图像融合的优势。

Method: 提出单图像融合新概念，构建MagicFuse框架：包含基于扩散模型的同谱知识强化分支、跨谱知识生成分支，以及融合二者扩散流噪声的多域知识融合分支；并施加视觉与语义约束。

Result: MagicFuse仅用单张退化可见光图像，即可生成高质量跨谱场景表征，其视觉与语义表示性能媲美甚至优于依赖多模态输入的最先进融合方法。

Conclusion: 单图像融合是可行且有效的，MagicFuse为无红外传感器场景下的跨谱理解提供了新范式。

Abstract: This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.

</details>


### [211] [GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data](https://arxiv.org/abs/2602.01764)
*Dennis Basile,Dennis Sprute,Helene Dörksen,Holger Flatt*

Main category: cs.CV

TL;DR: 本文提出一种基于MEMS-LiDAR与CARLA合成数据的隐私合规人员检测方法，兼顾GDPR合规性、鲁棒性与低成本标注。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的深度学习人员检测方法易受光照/遮挡影响，且存在隐私泄露风险（如违反GDPR）；真实LiDAR数据采集与标注耗时费力、易出错。

Method: 采用MEMS-LiDAR采集匿名化3D点云（不包含身份识别信息），结合CARLA仿真框架生成合成场景进行数据增强，构建混合训练数据集，训练人员检测模型。

Result: 相比仅用真实数据训练的模型，混合数据使平均精度（AP）提升44个百分点，人工标注工作量减少50%。

Conclusion: 该方法在保障GDPR合规前提下，实现了高精度、低标注成本、可扩展的工业室内人员检测，验证了合成LiDAR数据在隐私敏感场景中的有效性。

Abstract: The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.

</details>


### [212] [Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation](https://arxiv.org/abs/2602.01783)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 本文提出了一种基于单次滤波、循环方位变换与层次聚类的全自动断续面组表征方法，显著提升了地下矿腔岩体结构面识别精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 地下矿腔全封闭岩面的结构不连续面组自动表征仍缺乏鲁棒高效的方法，而这对岩体稳定性评估、开挖安全及运营效率至关重要。

Method: 采用单次滤波策略提取平面区域；设计循环方位变换方案，将极坐标系下的倾角与倾向准确映射至笛卡尔空间；再通过无需预设簇数的层次聚类完成断续面组识别。

Result: 在真实矿房数据上验证，该方法在倾角和倾向估计上的平均绝对误差分别为1.95°和2.20°，离散误差低于3°，优于现有主流自动化结构测绘技术。

Conclusion: 所提方法在噪声抑制、高曲率干扰消除及各向异性方向聚类方面具有优势，适用于复杂封闭环境下的岩体结构智能解译。

Abstract: Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95° and 2.20° in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3°.

</details>


### [213] [Spatio-Temporal Transformers for Long-Term NDVI Forecasting](https://arxiv.org/abs/2602.01799)
*Ido Faran,Nathan S. Netanyahu,Maxim Shoshany*

Main category: cs.CV

TL;DR: 本文提出了STT-LTF框架，结合空间上下文建模与时间序列预测，利用自监督学习处理40年Landsat影像，在地中海异质景观长期遥感预测中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决地中海等异质景观中长期卫星影像时间序列分析面临的复杂空间模式、季节变化和多十年环境变化交织的挑战。

Method: 提出Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF)，融合多尺度空间块与长时序（达20年）输入，采用统一Transformer架构；引入空间掩码、时间掩码与地平线采样等自监督策略；使用空间块嵌入、周期性时间编码与地理坐标建模空间-时间依赖。

Result: 在1984–2024 Landsat数据上实现下一年预测MAE=0.0328、R²=0.8412，性能超越统计方法、CNN、LSTM及标准Transformer；支持不规则采样与可变预测步长。

Conclusion: STT-LTF为异质生态系统长期遥感预测提供了鲁棒、灵活且高精度的新范式，尤其适用于快速生态变迁区域。

Abstract: Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.

</details>


### [214] [Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention](https://arxiv.org/abs/2602.01801)
*Dvir Samuel,Issar Tzachor,Matan Levy,Micahel Green,Gal Chechik,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的统一注意力优化框架（TempCache、AnnCA、AnnSA），用于缓解自回归视频扩散模型中KV缓存膨胀导致的推理延迟和显存增长问题，显著提升长视频生成效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在长视频生成中面临注意力层KV缓存随帧数增长而持续扩大，导致推理延迟上升、GPU显存激增、上下文受限及长程一致性下降的问题。

Method: 基于对视频生成中注意力冗余的分析（帧间近重复键、缓慢演化的语义查询/键、长提示中帧相关token稀疏性），提出三个训练无关模块：TempCache（利用时序对应压缩KV缓存）、AnnCA（用近似最近邻匹配筛选帧相关提示token以加速交叉注意力）、AnnSA（用轻量ANN限制每个查询仅关注语义匹配的键以稀疏化自注意力）。

Result: 实验显示端到端速度提升5–10倍，视觉质量几乎不变，并在长 rollout 中保持稳定吞吐与近乎恒定的峰值GPU显存，而先前方法性能随长度递减且显存持续增长。

Conclusion: 该框架为自回归视频扩散模型提供了高效、通用、即插即用的推理加速方案，支撑长视频合成、视频世界模型与交互式神经游戏引擎等应用。

Abstract: Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.

</details>


### [215] [FlowBypass: Rectified Flow Trajectory Bypass for Training-Free Image Editing](https://arxiv.org/abs/2602.01805)
*Menglin Han,Zhangkai Ni*

Main category: cs.CV

TL;DR: 本文提出FlowBypass，一种基于Rectified Flow的无需训练的图像编辑新框架，通过构建反转与重建轨迹间的直接旁路，缓解误差累积问题，避免依赖骨干网络特征操作，提升提示对齐性与图像保真度。


<details>
  <summary>Details</summary>
Motivation: 现有无训练图像编辑方法依赖反转-重建轨迹，存在轨迹长短导致的保真度与提示对齐之间的固有折衷；且先前改进方法多为骨干网络特异性特征操作，泛化性差。

Method: 提出基于Rectified Flow的FlowBypass框架，形式化推导反转与重建两条轨迹，得到近似旁路表达式及其数值解，实现轨迹间无缝过渡，不依赖特征操作。

Result: 在多项实验中，FlowBypass持续超越当前最优图像编辑方法，在更强提示对齐的同时，更好保留无关区域的高保真细节。

Conclusion: FlowBypass为无训练图像编辑提供了一种通用、解析性强、不依赖骨干结构的新范式，有效缓解轨迹误差累积问题。

Abstract: Training-free image editing has attracted increasing attention for its efficiency and independence from training data. However, existing approaches predominantly rely on inversion-reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt. Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability. To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations. We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions. Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.

</details>


### [216] [LDRNet: Large Deformation Registration Model for Chest CT Registration](https://arxiv.org/abs/2602.01812)
*Cheng Wang,Qiyu Gao,Fandong Zhang,Shu Zhang,Yizhou Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为LDRNet的快速无监督深度学习方法，用于胸部CT图像的大形变配准，通过粗到细的策略和创新的细化模块与刚性模块，在私有数据集和SegTHOR公开数据集上实现了SOTA性能且速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习医学图像配准方法多集中于脑部图像，而胸部CT配准具有更大形变、更复杂背景和区域重叠问题，亟需针对性解决方案。

Method: 提出LDRNet：先预测粗分辨率配准场，再逐级精细化；引入两个创新模块——用于多分辨率细化的refine block和从高层特征学习变换矩阵的rigid block；采用无监督训练方式。

Result: 在私有数据集和SegTHOR数据集上，LDRNet在大形变配准任务中性能超越VoxelMorph、RCN、LapIRN等深度模型及传统方法，同时推理速度显著提升。

Conclusion: LDRNet是一种高效、准确的胸部CT大形变无监督配准方法，为临床胸部影像分析提供了新工具。

Abstract: Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.

</details>


### [217] [GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation](https://arxiv.org/abs/2602.01814)
*Xiao Liang,Yunzhu Zhang,Linchao Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Guided Progressive Distillation（GPD）的框架，用于加速视频扩散模型的采样过程，在大幅减少采样步数（如从48步降至6步）的同时保持高质量生成效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频生成中计算开销大，现有加速方法常导致视频质量显著下降。

Method: 提出GPD框架，包含两个核心：(1)在线生成训练目标以降低优化难度并提升效率；(2)在潜在空间引入频域约束，以保留细节和时序动态。采用教师-学生渐进式引导策略，使学生模型适应更大步长采样。

Result: 在Wan2.1模型上将采样步数从48降至6，VBench评测中视觉质量具竞争力；相比现有蒸馏方法，GPD在流程简洁性和质量保持方面均有优势。

Conclusion: GPD是一种高效、简洁且高质量的视频扩散模型加速方法，为实际部署提供了新思路。

Abstract: Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.

</details>


### [218] [Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies](https://arxiv.org/abs/2602.01816)
*Wenjin Hou,Wei Liu,Han Hu,Xiaoxiao Sun,Serena Yeung-Levy,Hehe Fan*

Main category: cs.CV

TL;DR: 本文提出VIA-Bench基准，用于评估多模态大语言模型（MLLMs）在视觉错觉与异常场景下的鲁棒性，发现现有模型（包括CoT增强模型）普遍存在严重脆弱性，揭示机器与人类感知的根本差异。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估多基于分布内标准数据，缺乏对其在违背常识先验的视觉错觉与异常场景下鲁棒性的检验。

Method: 构建包含六类视觉错觉与异常的VIA-Bench基准，含1000+经人工审核的高质量问答对，并对20余个SOTA MLLM进行系统评测。

Result: 绝大多数MLLM在VIA-Bench上表现显著下降；Chain-of-Thought推理未能提升鲁棒性，反而产生逻辑崩溃的‘脆弱幻象’。

Conclusion: MLLM在视觉感知层面存在根本性瓶颈，突破该瓶颈对实现人工通用智能至关重要。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.

</details>


### [219] [Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery](https://arxiv.org/abs/2602.01836)
*Yin Wu,Daniel Slieter,Carl Esselborn,Ahmed Abouelazm,Tsung Yuan Tseng,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 本文提出了一种基于街景图像引导的数据采集策略，利用公开街景图像识别代表性地点（POI），以应对ADAS/ADS跨国家部署中因法规、道路设施和视觉规范差异导致的域偏移问题；通过两种POI评分方法（KNN特征距离与视觉归因）筛选数据，在交通标志检测任务上以一半目标域数据达到与随机采样相当的性能，并验证了大规模街景处理的经济可行性。


<details>
  <summary>Details</summary>
Motivation: ADAS和ADS跨国家部署面临立法、交通基础设施和视觉规范差异带来的域偏移问题，传统实地采集成本高、效率低，难以高效识别代表性地点。

Method: 提出街景图像引导的数据获取策略，结合Zenseact Open Dataset与Mapillary街景图像构建共址数据集；引入两种POI评分方法：基于视觉基础模型的KNN特征距离法和基于视觉语言模型的视觉归因法；采用collect-detect协议进行可复现评估。

Result: 在交通标志检测任务中，仅用50%的目标域数据即达到与随机采样相当的性能；成本估算表明大规模街景处理在经济上可行。

Conclusion: 街景图像引导的数据采集策略可显著提升跨国家模型适配的效率与经济性，为ADAS/ADS全球化部署提供新思路。

Abstract: Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.

</details>


### [220] [Rethinking Genomic Modeling Through Optical Character Recognition](https://arxiv.org/abs/2602.02014)
*Hongxin Xiang,Pengsen Ma,Yunkang Cao,Di Yu,Haowen Chen,Xinyu Yang,Xiangxiang Zeng*

Main category: cs.CV

TL;DR: OpticalDNA是一种基于视觉的基因组建模框架，将DNA序列渲染为结构化视觉布局，利用视觉-语言模型进行高效、高保真压缩与理解，显著减少计算开销并提升长序列建模性能。


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型将DNA视为一维文本序列，导致计算浪费在低信息背景上，且难以实现理解驱动的长上下文压缩。

Method: 提出OpticalDNA框架：将DNA转化为视觉布局，设计视觉DNA编码器和文档解码器；编码器生成可重建的紧凑视觉token；定义基于提示的基因组基本任务（读取、区域定位、子序列检索、掩码跨度补全）以学习布局感知表示。

Result: 在多个基因组基准上持续优于近期基线；在长达450k碱基的序列上达到最佳整体性能，有效token减少近20倍，并以仅256k可训练参数超越参数量高达985倍的模型。

Conclusion: 视觉导向的OCR式建模更契合基因组稀疏、非连续的语义结构，能实现高效、高保真、可解释的长序列理解。

Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.

</details>


### [221] [SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection](https://arxiv.org/abs/2602.01843)
*Qian Xu,Xi Li,Fei Gao,Jie Guo,Haojuan Yuan,Shuaipeng Fan,Mingjin Zhang*

Main category: cs.CV

TL;DR: 本文提出SPIRIT框架，通过轻量级物理信息插件适配视觉基础模型（VFMs）用于红外小目标检测（IRSTD），在单帧与视频模式下均实现统一且高性能的检测。


<details>
  <summary>Details</summary>
Motivation: 红外小目标信号弱、语义线索少，与可见光图像存在显著模态差异，导致直接使用语义导向的VFMs和外观驱动的跨帧关联不可靠。

Method: 提出SPIRIT框架：空间上采用PIFR模块近似秩-稀疏分解以抑制背景、增强目标信号；时间上采用PGMA模块将历史导出的软空间先验注入记忆交叉注意力，约束跨帧关联。

Result: 在多个IRSTD基准上实验表明，SPIRIT一致优于基于VFMs的基线方法，并达到当前最优性能（SOTA）。

Conclusion: SPIRIT实现了VFM与物理先验的有效融合，为红外小目标检测提供了兼顾泛化性、鲁棒性与统一性的新范式。

Abstract: Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.

</details>


### [222] [CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions](https://arxiv.org/abs/2602.01844)
*Yuliang Zhan,Jian Li,Wenbing Huang,Wenbing Huang,Yang Liu,Hao Sun*

Main category: cs.CV

TL;DR: 本文提出Cloth Dynamics Grounding (CDG) 场景和Cloth Dynamics Splatting (CloDS) 框架，实现从多视角视频中无监督学习布料动力学，无需物理先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法模拟复杂动力系统时依赖已知物理属性作为监督或输入，在物理条件未知时适用性受限。

Method: 提出CloDS无监督动态学习框架，采用三阶段流程：先进行视频到几何的接地（video-to-geometry grounding），再在接地网格上训练动力学模型；在接地阶段引入基于网格高斯溅射的双位置不透明度调制机制，支持2D观测与3D几何之间的双向映射，并联合考虑高斯组件的绝对与相对位置。

Result: 实验表明CloDS能有效从视觉数据中学习布料动力学，并对未见过的配置具有强泛化能力。

Conclusion: CloDS为无物理先验的动力学建模提供了新范式，尤其适用于布料等高度非线性、自遮挡严重的柔性物体建模。

Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\footnote{As in this example.

</details>


### [223] [WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?](https://arxiv.org/abs/2602.01850)
*Pei Li,Jiaxi Yin,Lei Ouyang,Shihan Pan,Ge Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: 本文提出WS-IMUBench，首个面向弱监督IMU时序动作定位（WS-IMU-TAL）的系统性基准研究，在仅使用序列级标签条件下评估多种弱监督方法在IMU数据上的迁移效果与局限性。


<details>
  <summary>Details</summary>
Motivation: 现有IMU动作识别以片段分类为主，难以建模真实行为的时序结构；而精确的时序动作定位依赖昂贵且难扩展的帧级边界标注，亟需弱监督替代方案。

Method: 不提出新算法，而是系统评估7种源自音频/图像/视频领域的弱监督时序定位方法，在7个公开IMU数据集上开展超3540次训练与7080次推理，围绕可迁移性、有效性与失败机制三方面分析。

Result: 发现：(i) 方法迁移效果呈模态依赖性，时域方法比图像衍生的提案法更稳定；(ii) 在动作较长、传感器维度高的数据上，弱监督性能可媲美全监督；(iii) 主要失效原因包括短动作、时间模糊性和提案质量差。

Conclusion: WS-IMUBench为弱监督IMU-TAL建立了可复现的基准框架（含数据、协议、分析），并指出IMU专用提案生成、边界感知目标函数和强时序建模是未来关键方向。

Abstract: IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.

</details>


### [224] [How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing](https://arxiv.org/abs/2602.01851)
*Huanyu Zhang,Xuehai Bai,Chengzu Li,Chen Liang,Haochen Tian,Haodong Li,Ruichuan An,Yifan Zhang,Anna Korhonen,Zhang Zhang,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: 本文提出了VIBE，一个用于图像编辑的视觉指令基准，包含三级交互层次，并提出了一种基于大语言模型的评估框架，评估了17个主流图像编辑模型在视觉指令跟随任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑系统和基准主要依赖文本指导，而人类交流本质上是多模态的，视觉指令（如草图）能更高效地传达空间和结构意图，因此需要构建支持视觉指令的基准。

Method: 提出了VIBE视觉指令基准，设计了涵盖指示性定位、形态操作和因果推理的三级交互层次；构建了高质量、多样化的测试用例；并提出基于大语言模型（LMM-as-a-judge）的任务特定评估框架。

Result: 对17个开源与闭源图像编辑模型的综合评估表明：闭源模型具备初步视觉指令跟随能力且整体优于开源模型，但所有模型在任务难度提升时性能显著下降。

Conclusion: 当前图像编辑模型在视觉指令理解方面仍存在明显局限，尤其在复杂任务上表现不足，为未来研究指明了重要方向。

Abstract: Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.

</details>


### [225] [Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection](https://arxiv.org/abs/2602.01854)
*A S M Sharifuzzaman Sagar,Mohammed Bennamoun,Farid Boussaid,Naeha Sharif,Lian Xu,Shaaban Sahmoud,Ali Kishk*

Main category: cs.CV

TL;DR: 本文系统评估了深度伪造检测器在多模态虚假信息检测中的作用，发现仅依赖像素级特征的检测器对图像-文本声明验证贡献有限，甚至会因非因果的真实性假设而降低事实核查性能；相比之下，基于外部证据和语义理解的事实核查系统表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测器主要针对像素级篡改，但多模态虚假信息往往源于图像与文本联合表达的语义与上下文主张，其在自动事实核查（AFC）流水线中的实际效用尚不明确。

Method: 在MMFakeBench和DGM4两个基准上，对比评估三类方法：(1) 图像单模态深度伪造检测器；(2) 基于蒙特卡洛树搜索（MCTS）检索与多智能体辩论（MAD）推理的证据驱动型事实核查系统；(3) 将检测器输出作为辅助证据注入的事实核查混合系统。

Result: 深度伪造检测器独立使用时F1仅0.26–0.53（MMFakeBench）和0.33–0.49（DGM4）；将其引入事实核查流程反而使F1下降0.04–0.08；而纯证据驱动系统达到F1≈0.81（MMFakeBench）和0.55（DGM4）。

Conclusion: 多模态声明验证主要依赖语义理解和外部证据，像素级伪造信号无法可靠提升对真实世界图像-文本虚假信息的推理能力。

Abstract: In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.

</details>


### [226] [Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling](https://arxiv.org/abs/2602.01864)
*Yuan Wang,Yuhao Wan,Siming Zheng,Bo Li,Qibin Hou,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 本文提出Ada-RefSR，一种基于单步扩散的参考图像超分辨率方法，通过自适应隐式相关门控（AICG）模块实现对参考图像信息的可信度感知融合，遵循“信任但验证”原则，在保持高效的同时提升鲁棒性与重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有参考图像超分辨率方法难以应对真实退化下低质图像与参考图像间对应关系不可靠的问题，易导致参考信息误用或未充分利用。

Method: 提出Ada-RefSR框架，核心为自适应隐式相关门控（AICG），利用可学习摘要token提取参考主导模式，并隐式建模其与低质特征的相关性，嵌入注意力主干中实现轻量、自适应的参考引导调控。

Result: 在多个数据集上验证了Ada-RefSR在保真度、自然性和效率间的良好平衡，且对参考图像对齐变化具有强鲁棒性。

Conclusion: Ada-RefSR通过‘信任但验证’机制和AICG模块，有效解决了RefSR中参考信息可靠性建模难题，为扩散模型在图像修复中的可控引导提供了新思路。

Abstract: Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a "Trust but Verify" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.

</details>


### [227] [ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding](https://arxiv.org/abs/2602.01881)
*Ye Chen,Yupeng Zhu,Xiongzhen Zhang,Zhewen Wan,Yingzhe Li,Wenjun Zhang,Bingbing Ni*

Main category: cs.CV

TL;DR: 本文提出了一种分层代理式参数化图像表示方法，通过解耦语义、几何与纹理属性，实现高效、可控的图像/视频编辑与物理驱动动画。


<details>
  <summary>Details</summary>
Motivation: 现有图像表示方法（如光栅图、高斯原语、潜在图像）存在冗余表示或缺乏语义实例/部件到潜变量的直接映射，导致编辑困难、控制性差。

Method: 基于语义感知图像分解，构建自适应贝塞尔拟合与迭代区域细分的分层代理几何；在几何感知的分布式代理节点中嵌入多尺度隐式纹理参数；引入局部性自适应特征索引机制保障纹理空间一致性。

Result: 在ImageNet、OIR-Bench、HumanEdit等基准上达到SOTA渲染保真度，参数量显著减少；支持直观交互式编辑、物理驱动实时动画，且时序一致性与视觉真实感优于生成式方法。

Conclusion: 该分层代理表示统一了可编辑性、紧凑性与物理可模拟性，为可控图像/视频编辑与动画提供了新范式。

Abstract: Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.

</details>


### [228] [Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model](https://arxiv.org/abs/2602.01901)
*Jiedong Zhuang,Lu Lu,Ming Dai,Rui Hu,Jian Chen,Qiang Liu,Haoji Hu*

Main category: cs.CV

TL;DR: 本文提出Lazy Attention机制，通过跨层共享相似注意力模式来减少多模态大语言模型（MLLMs）推理时的视觉token冗余计算和KV缓存开销，显著提升吞吐量并保持高精度。


<details>
  <summary>Details</summary>
Motivation: MLLMs因视觉编码器产生大量冗余视觉token，导致高推理成本和KV缓存瓶颈；现有token级剪枝方法易破坏KV缓存完整性，影响长文本生成。

Method: 发现超半数解码头层注意力语义相似，据此提出Lazy Attention：引入轻量级、兼容Flash Attention与KV缓存的层共享Q Cache，支持相邻层查询复用，并正交于现有token剪枝方法。

Result: 在多个基准上降低KV缓存使用超35%，吞吐量提升1.5倍，仅损失约1%性能，且精度保持优于SOTA token级方法。

Conclusion: Lazy Attention是一种高效、兼容、灵活的注意力优化机制，有效缓解MLLMs推理瓶颈，兼顾效率与准确性。

Abstract: Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.

</details>


### [229] [Learning Sparse Visual Representations via Spatial-Semantic Factorization](https://arxiv.org/abs/2602.01905)
*Theodore Zhengde Zhao,Sid Kiblawi,Jianwei Yang,Naoto Usuyama,Reuben Tan,Noel C Codella,Tristan Naumann,Hoifung Poon,Mu Wei*

Main category: cs.CV

TL;DR: STELLAR 提出一种因子分解框架，将视觉特征解耦为语义概念与空间分布两部分，从而同时支持高精度图像重建与高层语义理解，弥合判别式与生成式自监督学习之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 自监督学习中，高层语义方法（如DINO）因追求位置不变性而丢失空间信息，难以重建；生成式方法（如MAE）保留空间结构但缺乏高层抽象能力，二者存在根本冲突。

Method: 提出STELLAR框架，将视觉特征分解为低秩的语义概念矩阵与空间定位矩阵的乘积，分别进行DINO式增强对齐和像素级重建优化。

Result: 仅用16个稀疏token即可实现高质量重建（FID=2.60）和强语义性能（ImageNet准确率79.10%），显著优于传统稠密模型。

Conclusion: STELLAR通过语义与空间的显式解耦，构建了一种通用稀疏表征，统一了判别式与生成式视觉学习范式。

Abstract: Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.

</details>


### [230] [Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://arxiv.org/abs/2602.02185)
*Yu Zeng,Wenxuan Huang,Zhen Fang,Shuang Chen,Yufan Shen,Yishuo Cai,Xiaoman Wang,Zhenfei Yin,Lin Chen,Zehui Chen,Shiting Huang,Yiming Zhao,Yao Hu,Philip Torr,Wanli Ouyang,Shaosheng Cao*

Main category: cs.CV

TL;DR: 本文提出Vision-DeepResearch基准（VDR-Bench）以更真实地评估多模态大模型在视觉-文本联合检索任务中的能力，并设计多轮裁剪搜索策略提升视觉检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以准确评估MLLMs在视觉-深度研究系统中的视觉与文本搜索能力，存在答案泄露和评估场景过于理想化两大问题。

Method: 构建包含2000个VQA实例的VDR-Bench基准，采用多阶段人工标注与专家审核；提出多轮裁剪搜索（cropped-search）工作流以增强视觉检索能力。

Result: VDR-Bench能更真实反映模型在复杂视觉-文本事实检索任务中的表现；多轮裁剪搜索显著提升模型在现实视觉检索场景下的性能。

Conclusion: VDR-Bench为多模态深度研究系统提供了更可靠的评估标准，所提搜索策略为提升MLLMs视觉检索能力提供了实用方案。

Abstract: Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [231] [DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification](https://arxiv.org/abs/2602.01906)
*Farhan Ullah,Irfan Ullah,Khalil Khan,Giovanni Pau,JaKeoung Koo*

Main category: cs.CV

TL;DR: 本文提出了一种名为DSXFormer的新型双池化光谱压缩-扩展Transformer模型，用于高光谱图像分类（HSIC），通过双池化光谱压缩-扩展（DSX）模块和动态上下文注意力（DCA）机制，在提升光谱判别力的同时保持计算效率，并在多个基准数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临光谱维数高、光谱-空间相关性复杂、标注样本少等挑战；现有基于Transformer的方法难以兼顾光谱判别力与计算效率。

Method: 提出DSXFormer模型，包含：1）双池化光谱压缩-扩展（DSX）模块，利用全局平均与最大池化自适应重标定光谱通道；2）窗口化Transformer中的动态上下文注意力（DCA）机制，动态建模局部光谱-空间关系；3）多尺度补丁提取、嵌入与合并策略。

Result: 在Salinas、Indian Pines、Pavia University和Kennedy Space Center四个主流高光谱数据集上分别达到99.95%、98.91%、99.85%和98.52%的分类精度，显著优于现有方法。

Conclusion: DSXFormer通过融合光谱双池化压缩-扩展与动态上下文注意力，实现了光谱强调与空间上下文表征的有效平衡，为高效高精度HSIC提供了新思路。

Abstract: Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.

</details>


### [232] [Enabling Progressive Whole-slide Image Analysis with Multi-scale Pyramidal Network](https://arxiv.org/abs/2602.01951)
*Shuyang Wu,Yifu Qiu,Ines P. Nearchou,Sandrine Prost,Jonathan A Fallowfield,Hakan Bilen,Timothy J Kendall*

Main category: cs.CV

TL;DR: 本文提出了一种即插即用的多尺度金字塔网络（MSPN），用于改进基于注意力机制的多实例学习（MIL）在计算病理学中的应用，通过网格重映射和粗粒度引导网络实现高效的跨尺度特征分析，显著提升性能且轻量易用。


<details>
  <summary>Details</summary>
Motivation: 现有基于多尺度补丁的MIL方法依赖于固定倍率的多输入与晚期特征融合，导致跨尺度特征关联丢失、灵活性差且计算开销大。

Method: 提出多尺度金字塔网络（MSPN），包含（1）基于网格的重映射模块，利用高倍率特征生成粗粒度特征；（2）粗粒度引导网络（CGN），学习粗粒度上下文信息；作为即插即用模块集成到多种注意力型MIL框架中。

Result: MSPN在4种临床相关任务、3类基础模型及预训练MIL框架上均稳定提升性能，同时保持轻量性和易用性。

Conclusion: MSPN是一种通用、高效、灵活的多尺度建模方法，有效增强了基于注意力的MIL在计算病理学中的表现。

Abstract: Multiple-instance Learning (MIL) is commonly used to undertake computational pathology (CPath) tasks, and the use of multi-scale patches allows diverse features across scales to be learned. Previous studies using multi-scale features in clinical applications rely on multiple inputs across magnifications with late feature fusion, which does not retain the link between features across scales while the inputs are dependent on arbitrary, manufacturer-defined magnifications, being inflexible and computationally expensive. In this paper, we propose the Multi-scale Pyramidal Network (MSPN), which is plug-and-play over attention-based MIL that introduces progressive multi-scale analysis on WSI. Our MSPN consists of (1) grid-based remapping that uses high magnification features to derive coarse features and (2) the coarse guidance network (CGN) that learns coarse contexts. We benchmark MSPN as an add-on module to 4 attention-based frameworks using 4 clinically relevant tasks across 3 types of foundation model, as well as the pre-trained MIL framework. We show that MSPN consistently improves MIL across the compared configurations and tasks, while being lightweight and easy-to-use.

</details>


### [233] [Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images](https://arxiv.org/abs/2602.01954)
*Shuai Yang,Ziyue Huang,Jiaxin Chen,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 本文提出RS-MPOD框架，通过引入实例引导的视觉提示、文本提示及其多模态融合，解决遥感开放词汇目标检测中仅依赖文本提示导致的类别指定不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 遥感开放词汇目标检测中，仅依赖文本提示难以应对任务特定语义和语义模糊性，导致类别指定不稳定。

Method: 提出RS-MPOD多模态开放词汇检测框架，包含视觉提示编码器（提取示例实例外观特征）和多模态融合模块（联合视觉与文本信息）。

Result: 在标准、跨数据集和细粒度遥感基准上实验表明，视觉提示在语义模糊和分布偏移下更可靠，多模态提示在文本语义对齐良好时仍具竞争力。

Conclusion: RS-MPOD通过扩展类别指定方式为多模态，提升了遥感开放词汇检测的鲁棒性与灵活性。

Abstract: Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.

</details>


### [234] [Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated](https://arxiv.org/abs/2602.01973)
*Muli Yang,Gabriel James Goenawan,Henan Wang,Huaiyuan Qin,Chenghao Xu,Yanhua Yang,Fen Fang,Ying Sun,Joo-Hwee Lim,Hongyuan Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种基于贝叶斯决策理论的轻量级后处理校准方法，用于缓解AI生成图像检测器在测试时因分布偏移导致的系统性偏差（如将假图误判为真图），仅需少量目标域验证样本即可调整logits，无需重训练或真实标签。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器虽在平衡数据集上训练，但在测试时因伪造样本分布偏移及模型隐式先验，常系统性地将假图误判为真图，根源在于对非泛化人工伪影的过拟合和决策阈值错位。

Method: 提出一种基于贝叶斯决策理论的可学习标量logit校准方法：在冻结主干网络前提下，仅用小规模目标分布验证集优化一个标量参数，实现后处理式决策边界重校准。

Result: 在多个挑战性基准上显著提升检测鲁棒性；无需重训练、无需真实标签、计算开销低，具备强实用性与泛化性。

Conclusion: 该后处理校准框架为开放世界中可靠、自适应的AI生成图像检测提供了一个理论严谨、高效实用的解决方案。

Abstract: Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.

</details>


### [235] [Enhancing Multi-Image Understanding through Delimiter Token Scaling](https://arxiv.org/abs/2602.01984)
*Minyoung Lee,Yeji Park,Dongjun Hwang,Yejin Kim,Seong Joon Oh,Junsuk Choe*

Main category: cs.CV

TL;DR: 本文提出了一种通过缩放分隔符标记的隐藏状态来增强大型视觉语言模型（LVLMs）在多图像任务中区分图像能力的方法，有效缓解跨图像信息泄露问题，且无需额外训练或推理开销。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在多图像输入时性能下降，主要原因是跨图像信息泄露，而传统分隔符标记无法有效阻止该问题。

Method: 提出对分隔符标记的隐藏状态进行缩放，以增强图像内交互、抑制跨图像交互，从而提升图像区分能力。

Result: 在Mantis、MuirBench、MIRB、QBench2等多图像基准上性能提升；在TQABench、MultiNews、WCEP-10等文本多文档/多表格任务上也取得改进；无额外训练或推理成本。

Conclusion: 缩放分隔符隐藏状态是一种简单高效的方法，能显著提升LVLMs在多图像及需明确区分的文本任务上的表现。

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.

</details>


### [236] [Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models](https://arxiv.org/abs/2602.01991)
*Pablo Domingo-Gregorio,Javier Ruiz-Hidalgo*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过引入掩码特征和额外损失项，在扩散模型中实现对图像特定区域的精确局部控制，同时让模型自主生成其余部分。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像条件的文本到图像生成方法只能进行全局控制，难以实现用户定义区域的精细局部控制。

Method: 提出一种新训练框架，引入掩码特征和额外损失项，利用任意扩散步长对初始潜在向量的预测，增强当前步长与最终样本在潜在空间中的对应关系。

Result: 大量实验证明该方法能有效合成高质量图像，并支持受控的局部条件生成。

Conclusion: 所提方法显著提升了文本到图像生成中对局部区域的可控性，同时保持整体图像质量与语义一致性。

Abstract: Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.

</details>


### [237] [SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors](https://arxiv.org/abs/2602.02000)
*Bing He,Jingnan Gao,Yunuo Chen,Ning Cao,Gang Chen,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本文提出SurfSplat，一种基于2D高斯泼溅（2DGS）的前馈框架，通过引入表面连续性先验和强制alpha混合策略，实现从稀疏图像中高保真重建3D场景，并提出新指标HRRC评估高分辨率重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的方法难以生成连续表面，易在近景下暴露离散点云和颜色偏差导致的严重伪影。

Method: 提出SurfSplat框架，采用2D高斯泼溅作为基元，结合表面连续性先验与强制alpha混合策略；并设计高分辨率渲染一致性（HRRC）评估指标。

Result: 在RealEstate10K、DL3DV和ScanNet数据集上，SurfSplat在标准指标和HRRC上均一致优于先前方法。

Conclusion: SurfSplat为稀疏输入下的高保真3D重建提供了鲁棒解决方案。

Abstract: Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/

</details>


### [238] [UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving](https://arxiv.org/abs/2602.02002)
*Guosheng Zhao,Yaozeng Wang,Xiaofeng Wang,Zheng Zhu,Tingdong Yu,Guan Huang,Yongchen Zai,Ji Jiao,Changliang Xue,Xiaole Wang,Zhen Yang,Futang Zhu,Xingang Wang*

Main category: cs.CV

TL;DR: 本文提出了UniDriveDreamer，一种单阶段统一多模态世界模型，用于自动驾驶中视频与LiDAR序列的联合生成，通过统一潜在锚定（ULA）对齐双模态隐空间，并利用扩散Transformer建模几何与时间关系，显著提升生成质量及下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型多局限于单模态（仅视频或仅LiDAR）生成，缺乏对多模态协同建模与对齐的有效方法，难以满足自动驾驶对一致、互补感知输入的需求。

Method: 提出UniDriveDreamer框架：1）分别设计LiDAR专用VAE和视频VAE；2）引入Unified Latent Anchoring（ULA）对齐两模态隐分布；3）融合对齐特征并输入扩散Transformer联合建模几何对应与时间演化；4）以结构化场景布局信息作为每模态的条件信号。

Result: 在视频与LiDAR生成任务上均超越此前SOTA方法，并在下游任务中取得可衡量的性能提升。

Conclusion: 统一隐空间对齐与单阶段联合建模是实现高质量多模态世界模型的关键，UniDriveDreamer为自动驾驶数据合成提供了更鲁棒、一致且实用的新范式。

Abstract: World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream

</details>


### [239] [ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning](https://arxiv.org/abs/2602.02004)
*Gongli Xi,Kun Wang,Zeming Gao,Huahui Yi,Haolang Lu,Ye Tian,Wendong Wang*

Main category: cs.CV

TL;DR: 本文提出ClueTracer插件，通过追踪关键视觉线索在推理路径中的传播来抑制多模态大模型的幻觉问题，无需额外训练即可显著提升多种推理架构的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态推理模型因显式长链推理易产生幻觉，其根本原因是‘推理漂移’——模型在收集线索时过度关注与问题无关的实体，导致推理过程脱离视觉依据；而现有干预方法在推理场景下难以准确定位真正相关线索。

Method: 提出ClueRecall评估指标和ClueTracer插件：ClueTracer是一种训练无关、参数无关、架构无关的插件，从问题出发，沿推理路径（问题→输出→视觉token）反向追踪关键线索，定位任务相关图像区域并抑制对无关区域的虚假注意力。

Result: ClueTracer在无需任何额外训练的情况下，使各类推理架构（如R1-OneVision、Ocean-R1、MM-Eureka等）在推理基准上性能提升1.21倍；迁移到非推理设置时仍提升1.14倍。

Conclusion: 推理漂移是多模态推理模型幻觉的关键机制，ClueTracer通过可解释的线索追踪机制有效缓解该问题，为训练-free的幻觉抑制提供了新范式。

Abstract: Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\rightarrow$ outputs $\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \textbf{without any additional training}, ClueTracer improves all \textbf{reasoning} architectures (including \texttt{R1-OneVision}, \texttt{Ocean-R1}, \texttt{MM-Eureka}, \emph{etc}.) by $\mathbf{1.21\times}$ on reasoning benchmarks. When transferred to \textbf{non-reasoning} settings, it yields a $\mathbf{1.14\times}$ gain.

</details>


### [240] [One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation](https://arxiv.org/abs/2602.02033)
*Shuo Lu,Haohan Wang,Wei Feng,Weizhen Wang,Shen Zhang,Yaoyu Li,Ao Ma,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Bing Zhan,Yuan Xu,Huizai Yao,Yongcan Yu,Chenyang Si,Jian Liang*

Main category: cs.CV

TL;DR: 本文提出OSMF框架，通过产品感知自适应分组和组感知多模态大语言模型（G-MLLM），结合Group-DPO微调方法，实现面向不同用户群体的广告图像个性化生成，显著提升各群体点击率（CTR）。


<details>
  <summary>Details</summary>
Motivation: 现有广告图像生成方法采用‘一刀切’策略，仅优化整体点击率（CTR），忽视用户群体间的偏好差异，导致特定群体效果不佳，影响精准营销效果。

Method: 提出OSMF统一框架：1）产品感知自适应分组，基于用户属性与商品特征动态构建用户群并提取群体偏好特征；2）基于组感知多模态大语言模型（G-MLLM）进行偏好条件图像生成；3）使用新提出的Group-DPO方法对G-MLLM进行群体偏好对齐微调；4）构建首个大规模公开数据集GAIP（含约60万用户群、4000万用户）。

Result: 在离线与在线实验中均达到SOTA性能；GAIP数据集为首个大规模群体级广告图像偏好数据集；代码与数据集将开源。

Conclusion: OSMF有效解决了广告图像生成中群体偏好多样性建模难题，实现了从‘一刀切’到‘一尺多适’的范式转变，显著提升了细分人群的广告点击效果与营销精准性。

Abstract: Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.

</details>


### [241] [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/abs/2602.02043)
*Cristian Sbrolli,Matteo Matteucci,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 本文提出Auto-Comp自动化合成基准生成流程，用于细粒度评估视觉语言模型（VLMs）在颜色绑定与空间关系等组合推理任务上的缺陷，发现主流模型普遍存在属性混淆问题，并揭示上下文信息对空间推理有利但损害局部属性绑定的权衡现象。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型在组合推理（如颜色与物体的正确绑定）上存在严重缺陷，难以区分语义细微差异；现有评估方法缺乏可控性与细粒度分析能力，亟需可分解、可隔离不同推理能力的新型基准。

Method: 提出Auto-Comp——一种全自动、可控制的合成基准生成流程：基于Minimal caption（如‘显示器在自行车左侧’）和LLM生成的Contextual caption（含丰富场景描述）配对生成图像，构建A/B测试以分离核心绑定能力与多模态复杂性；设计Color Binding、Spatial Relations及新型Confusion Benchmark（引入低熵干扰项）进行系统评测。

Result: 在20个VLM（含CLIP/SigLIP系列）上验证了普遍存在的组合推理失败；发现模型不仅发生属性交换（如红蓝互换），更易受重复对象/颜色等低熵干扰误导；揭示关键权衡：上下文增强空间推理但损害颜色绑定性能。

Conclusion: 组合推理缺陷是当前VLM的系统性短板，不能仅归因于词袋式建模；Auto-Comp为解耦评估提供了新范式，其开源工具与基准将推动鲁棒多模态推理研究。

Abstract: Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing "a red cube and a blue sphere" with "a blue cube and a red sphere". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., "a monitor to the left of a bicycle on a white background") and LLM-generated Contextual captions (e.g., "In a brightly lit photography studio, a monitor is positioned to the left of a bicycle"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel "Confusion Benchmark" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).

</details>


### [242] [Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data](https://arxiv.org/abs/2602.02067)
*Nikola Cenikj,Özgün Turgut,Alexander Müller,Alexander Steger,Jan Kehrer,Marcus Brugger,Daniel Rueckert,Eimo Martens,Philip Müller*

Main category: cs.CV

TL;DR: 本文提出SegmentMIL，一种基于Transformer的多视图多实例学习框架，仅需患者级标注即可实现冠状动脉狭窄的分类与定位，无需昂贵的视图级标注，并在内、外部评估中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有单视图深度学习模型依赖难以获取的视图级标注，且无法建模多视图间的时序动态与依赖关系，限制了临床实用性。

Method: 提出SegmentMIL，一种基于Transformer的多视图多实例学习（MIL）框架，在患者级监督下联合完成狭窄存在性判断与解剖区域定位（区分左右冠状动脉及其节段）。

Result: 在真实临床数据集上训练并验证，SegmentMIL在内部和外部评估中均取得高性能，显著优于单视图模型和经典MIL基线方法。

Conclusion: SegmentMIL是一种临床可行、可扩展的冠状动脉狭窄诊断方案，摆脱了对视图级标注的依赖，兼具分类与定位能力。

Abstract: Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.

</details>


### [243] [UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction](https://arxiv.org/abs/2602.02089)
*Changbai Li,Haodong Zhu,Hanlin Chen,Xiuping Liang,Tongfei Chen,Shuwei Shao,Linlin Yang,Huobin Tan,Baochang Zhang*

Main category: cs.CV

TL;DR: 本文提出了UrbanGS，一种面向城市级场景的3D高斯泼溅重建框架，通过深度一致性D-Normal正则化、空间自适应高斯剪枝和统一划分与视图分配策略，解决了大场景下几何一致性、内存效率与计算可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）在有限场景中表现优异，但在城市级大规模场景中面临几何不一致、内存占用高和计算不可扩展等关键挑战。

Method: 提出Depth-Consistent D-Normal Regularization模块（融合外部深度监督与D-Normal约束，并引入基于梯度一致性和逆深度偏差的自适应置信加权）；设计Spatially Adaptive Gaussian Pruning（SAGP）策略动态调整高斯密度；构建统一的场景划分与视图分配机制以消除边界伪影并均衡计算负载。

Result: 在多个城市数据集上实验表明，UrbanGS在渲染质量、几何精度和内存效率方面均优于现有方法。

Conclusion: UrbanGS为高保真、大规模场景重建提供了系统性解决方案，显著提升了3DGS在真实城市环境中的实用性与可扩展性。

Abstract: While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.

</details>


### [244] [FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space](https://arxiv.org/abs/2602.02092)
*FSVideo Team,Qingyu Chen,Zhiyuan Fang,Haibin Huang,Xinwei Huang,Tong Jin,Minxuan Lin,Bo Liu,Celong Liu,Chongyang Ma,Xing Mei,Xiaohui Shen,Yaojie Shen,Fuwen Tan,Angtian Wang,Xiao Yang,Yiding Yang,Jiamin Yuan,Lingxi Zhang,Yuxin Zhang*

Main category: cs.CV

TL;DR: FSVideo是一个基于Transformer的快速图像到视频扩散框架，通过新型视频自编码器、带层内存设计的扩散Transformer（DIT）以及多分辨率生成策略，在保持竞争力的同时实现十倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频（I2V）扩散模型通常计算开销大、推理速度慢，亟需兼顾生成质量与效率的高效框架。

Method: 提出FSVideo框架：1）高压缩比视频自编码器（64×64×4时空下采样）；2）带层内存设计的DIT以增强层间信息流和上下文复用；3）轻量级多步DIT上采样器提升视频保真度。

Result: 14B参数DIT基础模型+14B参数DIT上采样器组合，在生成质量上媲美主流开源模型，推理速度快一个数量级。

Conclusion: FSVideo验证了在保证重建质量和视频 fidelity 的前提下，通过架构创新（如层内存、多分辨率生成）可显著提升I2V扩散模型的推理效率，为实时视频生成提供了可行路径。

Abstract: We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\times64\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.

</details>


### [245] [Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model](https://arxiv.org/abs/2602.02107)
*Yu Wang,Chuanguang Yang,Zhulin An,Weilun Feng,Jiarui Zhao,Chengqing Yu,Libo Huang,Boyu Diao,Yongjun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为DSKD的新型知识蒸馏方法，利用教师分类器引导轻量级扩散模型对学生特征进行去噪，并结合局部敏感哈希（LSH）指导的特征蒸馏，以缓解师生模型间特征分布不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法因师生模型特征分布差异，可能导致学生学习到不兼容的知识。

Method: 提出教师引导的学生扩散自蒸馏（DSKD）：用教师分类器指导轻量扩散模型对学生特征去噪，并设计LSH引导的原始与去噪学生特征间的蒸馏机制。

Result: 在多个视觉识别任务、模型和数据集上，DSKD显著优于现有知识蒸馏方法。

Conclusion: DSKD通过将去噪后学生特征视为‘代理教师’，有效消除师生映射方式与特征分布差异，实现更有效的知识迁移。

Abstract: Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.

</details>


### [246] [Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training](https://arxiv.org/abs/2602.02114)
*Xin Ding,Yun Chen,Sen Zhang,Kao Zhang,Nenglun Chen,Peibei Cao,Yongwei Wang,Fei Wu*

Main category: cs.CV

TL;DR: 本文提出iCCDM，改进了连续条件扩散模型（CCDM），采用更先进的Elucidated Diffusion Model（EDM）框架，并引入矩阵形式EDM公式与自适应邻域训练策略，在多个数据集上显著提升生成质量与采样效率，超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 原CCDM依赖过时的扩散框架且采样效率低，已被GAN方法CcGAN-AVAR超越，亟需提升生成质量与采样效率。

Method: 提出iCCDM：基于Elucidated Diffusion Model（EDM）框架，设计矩阵形式EDM公式，并引入自适应邻域训练策略。

Result: 在4个基准数据集（64×64至256×256分辨率）上，iCCDM一致优于现有方法，包括Stable Diffusion 3、FLUX.1和Qwen-Image等大型文本到图像模型，生成质量更高、采样成本显著降低。

Conclusion: iCCDM通过融合先进扩散框架与创新训练策略，有效克服了CCDM的固有缺陷，在连续条件图像生成任务中确立了新的性能标杆。

Abstract: Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\times64$ to $256\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.

</details>


### [247] [MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos](https://arxiv.org/abs/2602.02123)
*Yangyi Cao,Yuanhang Li,Lan Chen,Qi Mao*

Main category: cs.CV

TL;DR: MLV-Edit is a training-free, flow-based framework for minute-level video editing that uses Velocity Blend and Attention Sink modules to ensure temporal consistency and semantic fidelity without requiring retraining.


<details>
  <summary>Details</summary>
Motivation: Existing video editing methods struggle with long-duration videos due to high computational cost and difficulty maintaining global temporal consistency across thousands of frames.

Method: MLV-Edit adopts a divide-and-conquer strategy with two core modules: Velocity Blend aligns optical flow fields at segment boundaries to eliminate flickering, and Attention Sink anchors local segment features to global reference frames to prevent structural drift.

Result: MLV-Edit achieves superior temporal stability and semantic fidelity compared to state-of-the-art methods, as shown in extensive quantitative and qualitative experiments.

Conclusion: MLV-Edit effectively enables high-quality, training-free editing of minute-long videos while preserving motion coherence and structural integrity.

Abstract: We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.

</details>


### [248] [Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies](https://arxiv.org/abs/2602.02124)
*Olga Graf,Dhrupal Patel,Peter Groß,Charlotte Lempp,Matthias Hein,Fabian Heinemann*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的异常检测框架，用于毒理学研究中啮齿类动物肝脏全切片图像（WSI）的组织病理学分析，可同时识别已知病理（监督）、健康组织及罕见未知病理（无监督OOD检测），通过LoRA微调DINOv2模型实现组织分割，并引入类别特异性阈值优化Mahalanobis距离OOD检测性能，错误率低于0.4%。


<details>
  <summary>Details</summary>
Motivation: 药物诱导毒性是临床前和早期临床试验失败的主要原因；传统组织病理学评估依赖专家，难以满足大规模筛选需求。

Method: 构建带像素级标注的新数据集，采用LoRA微调预训练Vision Transformer（DINOv2）进行组织分割；利用提取的特征计算Mahalanobis距离进行OOD检测，并提出类别特异性阈值优化策略。

Result: 在小鼠肝脏WSI上实现高精度异常检测，已知病理与健康组织分类错误率分别仅为0.16%和0.35%，并成功检出罕见OOD形态学病变。

Conclusion: 该AI框架有望提升毒理学评估通量与客观性，助力临床前药物开发，降低后期失败风险，提高研发效率。

Abstract: Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\% of pathological tissue classified as healthy and 0.35\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.

</details>


### [249] [Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework](https://arxiv.org/abs/2602.02130)
*Lukas Zimmermann,Michael Rauter,Maximilian Schmid,Dietmar Georg,Barbara Knäusl*

Main category: cs.CV

TL;DR: 本文提出了一种基于物理模型的CBCT仿真方法，生成几何对齐的训练配对，避免传统配准误差带来的偏差，并证明几何一致性（如归一化互信息）比强度指标更能反映临床偏好。


<details>
  <summary>Details</summary>
Motivation: 监督式合成CT生成依赖配准的CBCT-CT配对，但实际中难以实现完美配准，导致注册偏差污染模型与评估结果，使高分可能仅反映对配准伪影的拟合而非真实解剖保真度。

Method: 采用基于物理的CBCT仿真方法生成天然几何对齐的训练数据；评估时使用相对于输入CBCT的几何对齐指标（如归一化互信息），而非受配准偏差影响的参考CT强度指标。

Result: 在两个骨盆数据集上，合成数据训练的模型几何对齐度更高（NMI: 0.31 vs 0.22）；强度指标与临床评估呈负相关，而NMI与观察者偏好显著正相关（rho = 0.31, p < 0.001）；87%情况下临床观察者更偏好合成训练模型输出。

Conclusion: 几何保真度（而非与有偏参考CT的强度一致性）才是临床可接受合成CT的关键标准；基于物理仿真的训练范式和几何对齐评估更符合真实临床需求。

Abstract: Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.

</details>


### [250] [LoopViT: Scaling Visual ARC with Looped Transformers](https://arxiv.org/abs/2602.02156)
*Wen-Jie Shu,Xuerui Qiu,Rui-Jie Zhu,Harold Haodong Chen,Yexin Liu,Harry Yang*

Main category: cs.CV

TL;DR: 本文提出Loop-ViT，一种基于权重共享递归结构的视觉推理模型，通过动态退出机制实现自适应迭代计算，在ARC-AGI-1基准上以更小参数量超越大模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉Transformer的前馈结构难以模拟人类归纳推理所需的迭代、算法式过程。

Method: 提出Loop-ViT：采用权重共享的Hybrid Block（融合局部卷积与全局注意力）进行递归迭代，并引入基于预测熵的无参动态退出机制，使模型在内部状态不确定性足够低时自动停止推理。

Result: 在ARC-AGI-1基准上，18M参数的Loop-ViT达到65.8%准确率，优于73M参数的集成模型。

Conclusion: 自适应迭代计算比单纯扩大模型宽度更高效，是视觉推理更优的扩展路径。

Abstract: Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.

</details>


### [251] [Reg4Pru: Regularisation Through Random Token Routing for Token Pruning](https://arxiv.org/abs/2602.02163)
*Julian Wyatt,Ronald Clark,Irina Voiculescu*

Main category: cs.CV

TL;DR: 本文提出Reg4Pru训练正则化技术，缓解基于token剪枝的视觉Transformer在分割任务中的性能下降问题，在FIVES血管分割数据集上显著提升平均精度并加速推理。


<details>
  <summary>Details</summary>
Motivation: Token剪枝虽可提升计算效率，但因保留表征稳定性差，导致深层密集预测（如分割）性能下降，需改进训练策略以平衡效率与精度。

Method: 提出Reg4Pru正则化方法，用于稳定token剪枝过程中的表征学习，适配分割任务；在FIVES数据集上进行实验验证。

Result: 相比未使用路由的同模型，Reg4Pru使平均精度绝对提升46%，同时推理速度相对基线提升29%。

Conclusion: Reg4Pru是一种有效的训练正则化技术，可显著提升token剪枝策略在密集预测任务中的性能与稳定性，适用于高效视觉Transformer设计。

Abstract: Transformers are widely adopted in modern vision models due to their strong ability to scale with dataset size and generalisability. However, this comes with a major drawback: computation scales quadratically to the total number of tokens. Numerous methods have been proposed to mitigate this. For example, we consider token pruning with reactivating tokens from preserved representations, but the increased computational efficiency of this method results in decreased stability from the preserved representations, leading to poorer dense prediction performance at deeper layers. In this work, we introduce Reg4Pru, a training regularisation technique that mitigates token-pruning performance loss for segmentation. We compare our models on the FIVES blood vessel segmentation dataset and find that Reg4Pru improves average precision by an absolute 46% compared to the same model trained without routing. This increase is observed using a configuration that achieves a 29% relative speedup in wall-clock time compared to the non-pruned baseline. These findings indicate that Reg4Pru is a valuable regulariser for token reduction strategies.

</details>


### [252] [Lung Nodule Image Synthesis Driven by Two-Stage Generative Adversarial Networks](https://arxiv.org/abs/2602.02171)
*Lu Cao,Xiquan He,Junying Zeng,Chaoyun Mai,Min Luo*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段生成对抗网络（TSGAN），通过解耦肺结节的形态结构与纹理特征，提升合成CT图像的多样性与空间可控性，从而改善肺结节检测模型的性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有肺结节CT数据集样本量小、多样性不足，导致检测模型性能和泛化能力受限；已有生成方法缺乏多样性与可控性，存在纹理单调、解剖结构失真等问题。

Method: 提出两阶段GAN：第一阶段用StyleGAN生成语义分割掩膜图以控制解剖结构；第二阶段用DL-Pix2Pix结合局部重要性注意力与动态权重多头窗口注意力，将掩膜图翻译为CT图像，增强纹理与背景建模能力。

Result: 在LUNA16数据集上，相比原始数据训练结果，检测准确率提升4.6%，mAP提升4%；合成图像质量与检测模型性能均得到提升。

Conclusion: TSGAN能有效提升合成肺结节CT图像的多样性、解剖合理性和纹理真实性，进而增强下游检测模型的性能与泛化能力。

Abstract: The limited sample size and insufficient diversity of lung nodule CT datasets severely restrict the performance and generalization ability of detection models. Existing methods generate images with insufficient diversity and controllability, suffering from issues such as monotonous texture features and distorted anatomical structures. Therefore, we propose a two-stage generative adversarial network (TSGAN) to enhance the diversity and spatial controllability of synthetic data by decoupling the morphological structure and texture features of lung nodules. In the first stage, StyleGAN is used to generate semantic segmentation mask images, encoding lung nodules and tissue backgrounds to control the anatomical structure of lung nodule images; The second stage uses the DL-Pix2Pix model to translate the mask map into CT images, employing local importance attention to capture local features, while utilizing dynamic weight multi-head window attention to enhance the modeling capability of lung nodule texture and background. Compared to the original dataset, the accuracy improved by 4.6% and mAP by 4% on the LUNA16 dataset. Experimental results demonstrate that TSGAN can enhance the quality of synthetic images and the performance of detection models.

</details>


### [253] [CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization](https://arxiv.org/abs/2602.02175)
*Xinquan Yu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: 本文提出CIEC框架，利用粗粒度图像/句子级标注实现图像-文本对的多模态弱监督篡改定位，通过TRPS和VCTG两个模块分别进行图像和文本分支的弱监督定位，并引入多种约束提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖代价高、耗时长的细粒度（如patch/token级）标注，亟需基于粗粒度标注的弱监督解决方案。

Method: 提出Coupling Implicit and Explicit Cues (CIEC)框架，包含图像分支（TRPS模块：结合文本引导的伪造线索与空间先验，并施加背景抑制与空间对比约束）和文本分支（VCTG模块：聚焦内容词并利用视觉偏差校准token定位，并施加非对称稀疏与语义一致性约束）。

Result: 在多个评估指标上达到与全监督方法相当的性能。

Conclusion: CIEC成功实现了仅用图像/句子级粗粒度标注的多模态弱监督篡改定位，显著降低标注成本，同时保持高定位精度。

Abstract: To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.

</details>


### [254] [Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision](https://arxiv.org/abs/2602.02186)
*Ziqiao Weng,Jiancheng Yang,Kangxian Xie,Bo Zhou,Weidong Cai*

Main category: cs.CV

TL;DR: 本文提出TopoField，一种拓扑感知的隐式建模框架，用于修复CT图像中肺树的拓扑不完整性（如缺失或断开分支），并在单次前向传播中统一完成解剖标注与肺段重建，兼具高精度与高效性（约1秒/例）。


<details>
  <summary>Details</summary>
Motivation: 肺树CT提取常存在拓扑不完整（如分支缺失或断连），严重影响下游解剖分析；现有方法依赖密集体素处理或显式图推理，效率低、鲁棒性差。

Method: 提出TopoField框架：用稀疏表面与骨架点云表示肺解剖结构，学习连续隐式场以实现无需完整/断连标注的拓扑修复（训练于人工引入结构破坏的已有不完整树）；基于修复后的隐式表示，通过任务特定隐式函数联合推断解剖标注与肺段重建。

Result: 在Lung3D+数据集上，TopoField显著提升拓扑完整性，并在严重不完整场景下实现准确解剖标注与肺段重建；单例全流程耗时仅约1秒，具备临床大规模与时敏应用潜力。

Conclusion: TopoField将拓扑修复作为核心建模任务，通过隐式表示实现高效、鲁棒、多任务统一的肺树分析，为临床肺部结构建模提供了新范式。

Abstract: Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.

</details>


### [255] [SSI-DM: Singularity Skipping Inversion of Diffusion Models](https://arxiv.org/abs/2602.02193)
*Chen Min,Enze Jiang,Jishen Peng,Zheng Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为SSI-DM的新方法，通过在标准反演前添加少量噪声来绕过数学奇点区域，从而生成具有自然高斯特性的反演噪声，提升图像编辑任务中的可编辑性与重建保真度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型图像反演方法在早期加噪步骤中存在不准确性，导致反演得到的噪声非高斯、可编辑性差；根本原因在于数学奇点使反演问题本质上病态。

Method: 提出Singularity Skipping Inversion of Diffusion Models（SSI-DM），在标准反演流程前引入小量噪声以避开奇点区域，实现更准确、更鲁棒的反演。

Result: 生成的反演噪声具有良好的高斯特性，同时保持高重建保真度；在公开图像数据集上的重建与插值任务中性能优于现有方法。

Conclusion: SSI-DM是一种即插即用、原理清晰且高效的扩散模型反演方案，为基于扩散模型的图像编辑提供了可靠基础。

Abstract: Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.

</details>


### [256] [MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models](https://arxiv.org/abs/2602.02212)
*Zheyuan Zhou,Liang Du,Zixun Sun,Xiaoyu Zhou,Ruimin Ye,Qihao Chen,Yinda Chen,Lemiao Qiu*

Main category: cs.CV

TL;DR: MAIN-VLA 提出一种通过显式建模意图与环境抽象来提升视觉-语言-动作（VLA）模型在复杂动态环境中决策能力的新框架，显著提升决策质量、泛化性与推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA方法在3D开放世界和大型PvP游戏等高度复杂、实时不可预测的环境中，难以从冗余传感器流中高效提取动作关键信号。

Method: MAIN-VLA 包含两个核心抽象模块：意图抽象（IA）将语言指令及推理压缩为显式语义基元；环境语义抽象（ESA）将视觉流映射为结构化拓扑效用表征；二者对齐后产生注意力聚焦效应，支持无参token剪枝以消除感知冗余。

Result: 在Minecraft开放世界及《和平精英》《Valorant》等大规模PvP环境中，MAIN-VLA取得SOTA性能，显著提升决策质量、泛化能力和推理效率。

Conclusion: 通过深度语义对齐替代表层模式匹配，MAIN-VLA为复杂动态环境下的VLA系统提供了更鲁棒、高效且可解释的决策范式。

Abstract: Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.

</details>


### [257] [Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation](https://arxiv.org/abs/2602.02214)
*Hongzhou Zhu,Min Zhao,Guande He,Hang Su,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出Causal Forcing方法，通过使用自回归（AR）教师模型进行ODE初始化，解决从双向视频扩散模型蒸馏到AR学生模型时因因果注意力替代全注意力所导致的架构鸿沟问题，显著提升视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将预训练双向视频扩散模型蒸馏为少步自回归模型时，存在架构不匹配问题；且其采用的ODE蒸馏需帧级单射性条件，在双向教师→AR学生蒸馏中无法满足，导致性能下降。

Method: 提出Causal Forcing：构建AR教师模型，并以其PF-ODE进行学生模型初始化，从而严格满足ODE蒸馏所需单射性条件，弥合架构差距。

Result: 在Dynamic Degree、VisionReward和Instruction Following三项指标上分别超越SOTA方法Self Forcing达19.3%、8.7%和16.7%。

Conclusion: 使用AR教师进行ODE初始化是桥接双向与AR视频生成模型间架构鸿沟的关键，Causal Forcing为高效高质量视频生成提供了理论可靠且实证优越的新范式。

Abstract: To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\% in Dynamic Degree, 8.7\% in VisionReward, and 16.7\% in Instruction Following. Project page and the code: \href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}

</details>


### [258] [MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2602.02222)
*Ruiqi Liu,Manni Cui,Ziheng Qin,Zhiyuan Yan,Ruoxin Chen,Yi Han,Zhiheng Li,Junkai Chen,ZhiJin Chen,Kaiqing Lin,Jialiang Shen,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

TL;DR: 本文提出MIRROR框架，将AI生成图像检测重构为参考比较问题，利用可学习离散记忆库编码现实先验，通过稀疏线性组合生成流形一致的理想参考，并以残差作为鲁棒检测信号，在多个基准上显著超越现有方法，甚至超越人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器依赖人工设计的伪造痕迹，泛化能力差；而人类判断依赖稳定的真实世界规律，偏离人类认知流形是更泛化的伪造信号。

Method: 提出MIRROR框架，使用可学习离散记忆库显式编码现实先验，将输入投影到流形一致的理想参考，并用残差作为检测信号；引入Human-AIGI心理物理校准基准评估是否达到‘超人交叉点’。

Result: 在14个基准上持续优于先前方法：6个标准基准提升2.1%，7个真实场景基准提升8.1%；在Human-AIGI上对27种生成器达89.6%准确率，超越普通用户和视觉专家，并随预训练骨干扩大逼近人类感知极限。

Conclusion: 基于现实流形一致性的参考比较范式比传统伪影分类更具泛化性和鲁棒性，MIRROR实现了向超人检测性能的跨越，为AIGI检测提供了新范式。

Abstract: High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the "superhuman crossover" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR

</details>


### [259] [Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type](https://arxiv.org/abs/2602.02223)
*Junchi Feng,Nikhil Ballem,Mahya Beheshti,Giles Hamilton-Fletcher,Todd Hudson,Maurizio Porfiri,William H. Seiple,John-Ross Rizzo*

Main category: cs.CV

TL;DR: 本研究系统评估了OCR在静态和动态条件下的性能，发现识别准确率随行走速度增加和视角变宽而下降；Google Vision表现最佳，PaddleOCR 3.0是性能最强的开源方案；手机主摄像头和肩部佩戴方式整体最优。


<details>
  <summary>Details</summary>
Motivation: 现有OCR评估多基于静态数据集，无法反映移动场景（如盲人辅助技术）中的真实挑战，亟需在动态、多变条件下系统评估OCR性能。

Method: 在静态条件下测试1–7米距离和0–75度水平视角下的检测范围；在动态条件下测试慢速（0.8 m/s）至快速（1.8 m/s）行走时，头戴、肩戴、手持三种摄像头位置对OCR精度的影响；使用智能手机（主摄与超广角）和智能眼镜，对比Google Vision、PaddleOCR 3.0、EasyOCR和Tesseract四种OCR引擎；字符级精度采用Levenshtein比率计算。

Result: 识别准确率随行走速度提升和视角增大而显著下降；Google Vision总体准确率最高，PaddleOCR 3.0为最佳开源方案；手机主摄像头精度最高，肩部佩戴平均精度略高于头戴和手持，但三者差异不具统计显著性。

Conclusion: OCR在真实移动辅助场景中性能受限于运动模糊与视角变化，需针对性优化；设备选型（手机主摄）与佩戴方式（肩部）可提升实用性，但开源引擎（如PaddleOCR）已接近商用水平，具备落地潜力。

Abstract: Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.

</details>


### [260] [Show, Don't Tell: Morphing Latent Reasoning into Image Generation](https://arxiv.org/abs/2602.02227)
*Harold Haodong Chen,Xinxiang Yin,Wen-Jie Shu,Hongfei Zhang,Zixin Zhang,Chenfei Liao,Litao Guo,Qifeng Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出LatentMorph框架，通过在隐空间中进行隐式推理，提升文本到图像生成的动态自修正能力，显著提高性能、效率与人类认知对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法缺乏类似人类的动态推理与迭代优化能力；显式推理范式存在效率低、信息损失和认知不匹配等问题。

Method: 提出LatentMorph框架，包含四个轻量组件：condenser（压缩中间状态为视觉记忆）、translator（将隐式思维转为指导信号）、shaper（动态引导图像token预测）、RL-trained invoker（强化学习训练的自适应推理触发器），全程在连续隐空间中完成推理。

Result: 在GenEval和T2I-CompBench上分别提升Janus-Pro模型16%和25%；在WISE和IPV-Txt等抽象推理任务上超越TwIG等显式方法15%和11%；推理时间减少44%，token消耗降低51%；推理触发与人类直觉的认知对齐率达71%。

Conclusion: LatentMorph验证了隐式潜空间推理在T2I生成中的有效性，为更高效、更类人的生成式AI提供了新范式。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\%$ on GenEval and $25\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\%$ and $11\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\%$ and token consumption by $51\%$; and (IV) exhibits $71\%$ cognitive alignment with human intuition on reasoning invocation.

</details>


### [261] [LiFlow: Flow Matching for 3D LiDAR Scene Completion](https://arxiv.org/abs/2602.02232)
*Andrea Matteazzi,Dietmar Tutsch*

Main category: cs.CV

TL;DR: 本文提出了首个用于3D LiDAR场景补全的流匹配框架LiFlow，解决了扩散模型中训练与推理初始分布不一致的问题，并通过最近邻流匹配损失和Chamfer距离损失提升局部结构与全局覆盖效果，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云在自动驾驶中常受遮挡和远距离稀疏性影响，导致感知受限；现有基于局部点级去噪扩散概率模型的方法存在训练与推理初始分布不匹配的问题。

Method: 提出基于流匹配（flow matching）的LiFlow框架，采用最近邻流匹配损失和Chamfer距离损失联合优化，确保训练与推理初始分布一致，并提升点云对齐的局部结构与全局覆盖。

Result: LiFlow在多个指标上达到当前最优（state-of-the-art）性能。

Conclusion: 流匹配范式比扩散模型更适配LiDAR场景补全任务，能有效缓解分布偏移问题，为3D点云生成提供了新思路。

Abstract: In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.

</details>


### [262] [Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation](https://arxiv.org/abs/2602.02318)
*Xiang Li,Yupeng Zheng,Pengfei Li,Yilun Chen,Ya-Qin Zhang,Wenchao Ding*

Main category: cs.CV

TL;DR: 本文提出DiScene，一种基于稀疏查询的多级知识蒸馏框架，用于高效且鲁棒的占据预测，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有密集方法计算浪费严重，稀疏查询方法在复杂室内场景中鲁棒性不足，需兼顾效率与精度。

Method: 提出多级一致知识蒸馏策略（含编码器、查询、先验和锚点四层对齐）和教师引导初始化策略（优化参数热启动）。

Result: 在Occ-Scannet上达23.2 FPS，超越OPUS 36.1%；集成深度信息后（DiScene†）超越EmbodiedOcc 3.7%，推理快1.62×；在Occ3D-nuScenes及野外场景也表现优异。

Conclusion: DiScene通过多级蒸馏与教师引导初始化，显著提升稀疏占据预测的效率与鲁棒性，具备跨场景泛化能力。

Abstract: Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.

</details>


### [263] [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334)
*Fatemeh Zargarbashi,Dhruv Agrawal,Jakob Buhmann,Martin Guay,Stelian Coros,Robert W. Sumner*

Main category: cs.CV

TL;DR: 本文提出了一种基于RVQ-VAE与对比学习的信息泄露损失的新方法，实现人体运动数据中内容与风格的解耦，支持无需微调的风格迁移等下游任务。


<details>
  <summary>Details</summary>
Motivation: 人体运动数据语义丰富、风格细腻，但建模难度大；需有效解耦内容（粗粒度动作属性）与风格（细粒度表达细节），以支持风格迁移等应用。

Method: 采用残差矢量量化变分自编码器（RVQ-VAE）构建由粗到细的运动表征，并结合对比学习与新型信息泄露损失进行码本学习，实现内容与风格在不同码本上的分离；推理阶段使用量化码交换（Quantized Code Swapping）技术完成风格迁移。

Result: 所提框架在风格迁移、风格去除和动作融合等多个推理任务中展现出强泛化能力，且对未见风格无需微调即可实现迁移。

Conclusion: 该方法通过层级化建模与显式约束，实现了高质量、可扩展的人体运动风格-内容解耦，为运动生成与编辑提供了新范式。

Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.

</details>


### [264] [LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization](https://arxiv.org/abs/2602.02341)
*Zhenpeng Huang,Jiaqi Li,Zihan Jia,Xinhao Li,Desen Meng,Lingxue Song,Xi Chen,Liang Li,Limin Wang*

Main category: cs.CV

TL;DR: LongVPO是一种无需长视频标注的两阶段直接偏好优化框架，使短上下文视觉语言模型能稳健理解超长视频。


<details>
  <summary>Details</summary>
Motivation: 解决短上下文视觉语言模型难以理解超长视频的问题，同时避免依赖昂贵的人工长视频标注。

Method: 第一阶段通过锚定问题到短片段、插入干扰项并进行视觉相似性和问题特异性过滤来合成偏好三元组，并近似参考模型在长上下文中的评分；第二阶段采用递归字幕生成场景级元数据，再用大语言模型构建多段推理问题与非偏好回答，实现多段推理任务下的偏好对齐。

Result: 仅用16K合成样本且无需人工标注，LongVPO在多个长视频基准上超越当前最优开源模型，同时在短视频基准（如MVBench）上保持强性能。

Conclusion: LongVPO提供了一种可扩展、高效的长视频理解新范式。

Abstract: We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.

</details>


### [265] [NAB: Neural Adaptive Binning for Sparse-View CT reconstruction](https://arxiv.org/abs/2602.02356)
*Wangduo Xie,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 本文提出了一种名为神经自适应分箱（NAB）的新方法，将工业对象常见的矩形结构先验融入稀疏视角CT重建中，通过可学习的平滑分箱机制实现端到端优化，在工业和医学数据集上均取得优越性能。


<details>
  <summary>Details</summary>
Motivation: 经典隐式神经网络在稀疏CT重建中无法利用工业对象常见的矩形形状先验，而引入此类先验有助于提升重建质量与效率。

Method: 提出神经自适应分箱（NAB）方法：首先将坐标空间映射为分箱向量空间，该映射基于差分移位双曲正切函数构建可学习分箱机制，并支持绕输入平面法向量的旋转；随后用神经网络预测CT衰减系数；所有分箱参数（位置、尺寸、陡峭度、旋转）均可通过投影数据梯度进行端到端优化。

Result: 在两个工业CT数据集上NAB显著优于现有方法；扩展分箱函数后，在医学CT数据集上也保持鲁棒性；代码将开源。

Conclusion: NAB为将几何先验嵌入神经重建框架提供了新范式，兼顾可解释性与表达能力，推动了隐式神经表示在工业CT中的实用化进展。

Abstract: Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \textbf{N}eural \textbf{A}daptive \textbf{B}inning (\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.

</details>


### [266] [Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes](https://arxiv.org/abs/2602.02370)
*Uma Meleti,Jeffrey J. Nirschl*

Main category: cs.CV

TL;DR: 本文提出Spectral-normalized Neural Gaussian Process (SNGP)，通过谱归一化和高斯过程层替代全连接层，提升数字病理学中单模型的不确定性估计与分布外（OOD）检测能力，在保持分布内性能的同时显著增强模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前数字病理学深度学习模型在分布外场景下常过度自信、校准差，限制临床信任与落地；亟需具备内在不确定性感知能力的安全模型以准确拒识OOD输入。

Method: 采用Spectral-normalized Neural Gaussian Process（SNGP），即对网络施加谱归一化，并将最终全连接层替换为高斯过程层，实现轻量级、单模型的不确定性建模。

Result: 在六个数据集、三类生物医学分类任务（白细胞、淀粉样斑块、结直肠组织病理）上，SNGP在分布内性能与基线相当，但不确定性估计和OOD检测能力显著优于确定性模型及Monte Carlo Dropout。

Conclusion: SNGP为数字病理学提供了可靠、可部署的不确定性感知分类框架，有助于提升临床可信度与病理医生信任。

Abstract: Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.

</details>


### [267] [Unified Personalized Reward Model for Vision Generation](https://arxiv.org/abs/2602.02380)
*Yibin Wang,Yuhang Zang,Feng Han,Jiazi Bu,Yujie Zhou,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出UnifiedReward-Flex，一种统一的个性化多模态奖励模型，通过结合语义意图理解、视觉证据 grounding 和动态分层评估，实现对生成内容更精准、上下文自适应的偏好建模，显著提升图像与视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有多模态奖励模型采用‘一刀切’范式或固定评估标准，难以捕捉内容特异性视觉线索及主观、上下文相关的用户偏好，导致系统性错配。

Method: 提出UnifiedReward-Flex模型：首先解析提示语义并基于视觉证据进行grounding；其次动态构建包含预定义与自生成高层维度的细粒度分层评估体系；训练采用两阶段策略——先用闭源VLM蒸馏结构化推理轨迹进行监督微调（SFT），再通过精选偏好对实施直接偏好优化（DPO）。

Result: 在GRPO框架中集成UnifiedReward-Flex后，在图像与视频合成任务上取得显著性能提升，验证了其在推理保真度与判别对齐能力上的优势。

Conclusion: UnifiedReward-Flex通过将奖励建模与灵活、上下文自适应的推理耦合，有效缓解了传统RM对主观与场景依赖性偏好的建模不足问题，为视觉生成提供了更鲁棒、个性化的评价基础。

Abstract: Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.

</details>


### [268] [Personalized Image Generation via Human-in-the-loop Bayesian Optimization](https://arxiv.org/abs/2602.02388)
*Rajalaxmi Rajagopalan,Debottam Dutta,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 本文提出MultiBO方法，通过多轮用户偏好反馈（从K个候选图像中选择更接近目标图像的一个）来优化扩散模型生成的图像，从而缩小语言提示无法覆盖的语义差距。


<details>
  <summary>Details</summary>
Motivation: 语言提示在图像生成中存在表达极限，即使经过多轮提示，用户仍难以用语言精确描述心中理想图像；但用户能直观判断哪张图像更接近目标，因此可利用这种偏好判断能力进一步优化生成结果。

Method: 提出Multi-Choice Preferential Bayesian Optimization（MultiBO）：基于初始提示生成图像x^{p*}，随后每轮生成K个扰动/优化图像，由用户进行多选一偏好反馈，将该偏好信号建模为贝叶斯优化中的偏好似然，迭代更新扩散模型的隐空间引导方向。

Result: 在B轮用户反馈内显著提升图像与目标x*的相似性；30名用户定性评分及5种基线方法的定量对比（如CLIP Score、LPIPS等）均验证其有效性。

Conclusion: 人类的多选一偏好反馈是一种高效、低负担的交互信号，可在不访问真实目标图像x*的前提下，有效驱动扩散模型实现个性化图像精调。

Abstract: Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.

</details>


### [269] [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393)
*Ruiqi Wu,Xuanhua He,Meng Cheng,Tianyu Yang,Yong Zhang,Zhuoliang Kang,Xunliang Cai,Xiaoming Wei,Chunle Guo,Chongyi Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出Infinite-World，一种能在复杂真实环境中维持1000+帧连贯视觉记忆的鲁棒交互式世界模型；通过无姿态层级记忆压缩器（HPMC）、不确定性感知动作标注模块和重访密集微调策略，克服真实视频训练中姿态噪声与视角重访稀疏的挑战，显著提升视觉质量、动作可控性与空间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在合成数据上表现良好，但在真实视频中受限于姿态估计噪声和视角重访稀疏，缺乏有效的训练范式。

Method: 提出三个关键技术：1）无姿态层级记忆压缩器（HPMC），递归压缩历史隐状态并联合优化生成主干；2）不确定性感知动作标注模块，将连续运动离散为三态逻辑以抑制轨迹噪声；3）基于小规模重访密集视频集的微调策略。

Result: 在客观指标和用户研究中均显示Infinite-World在视觉质量、动作可控性和空间一致性上优于现有方法。

Conclusion: Infinite-World为真实世界长时序交互建模提供了可扩展、鲁棒且无需几何先验的新范式。

Abstract: We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.

</details>


### [270] [Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation](https://arxiv.org/abs/2602.02401)
*Xinshun Wang,Peiming Li,Ziyi Wang,Zhongbin Fang,Zhichao Deng,Songtao Wu,Jason Li,Mengyuan Liu*

Main category: cs.CV

TL;DR: 本文提出Superman框架，统一视觉感知与基于骨架的时序运动生成，通过视觉引导的运动分词器和统一MLLM架构，解决当前运动分析任务中感知与生成割裂、时序建模不足及模态脱节等问题，在多个基准上达到SOTA或竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有运动分析范式存在严重碎片化：感知模型无法生成运动，生成模型无法直接处理视觉输入；生成模型多局限于单帧静态姿态，难以建模时序运动；运动词表仅基于骨架数据，脱离视觉域。

Method: 提出Superman统一框架，包含两部分：1）视觉引导的运动分词器（Vision-Guided Motion Tokenizer），利用3D骨架与视觉数据的几何对齐，实现跨模态联合学习，构建统一运动词表；2）基于该词表的单一MLLM架构，统一处理视频到3D姿态估计（感知）、运动预测与in-betweening（生成）等任务。

Result: 在Human3.6M等标准基准上，Superman在所有运动分析任务中均达到SOTA或具有竞争力的性能，验证了其统一建模的有效性与可扩展性。

Conclusion: Superman为基于骨架的生成式运动分析提供了更高效、可扩展的统一路径，弥合了视觉感知与时序运动生成之间的鸿沟。

Abstract: Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.

</details>


### [271] [ReasonEdit: Editing Vision-Language Models using Human Reasoning](https://arxiv.org/abs/2602.02408)
*Jiaxing Qiu,Kaihua Hou,Roxana Daneshjou,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.CV

TL;DR: ReasonEdit is the first VLM editor that incorporates human reasoning during model editing, using a codebook and topology-balanced multimodal embedding to improve edit generalization on reasoning-heavy visual question answering tasks.


<details>
  <summary>Details</summary>
Motivation: Existing model editors for vision-language models (VLMs) do not handle reasoning-heavy tasks, which require joint human and model reasoning about images.

Method: ReasonEdit introduces a new editing setup where human reasoning is continuously stored in a codebook and retrieved via a novel topology-balanced multimodal embedding method inspired by network science.

Result: ReasonEdit achieves state-of-the-art editing performance across four VLMs on multiple rationale-based visual question answering datasets.

Conclusion: Incorporating human reasoning during editing significantly improves edit generalization in VLMs for reasoning-heavy tasks.

Abstract: Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.

</details>


### [272] [Catalyst: Out-of-Distribution Detection via Elastic Scaling](https://arxiv.org/abs/2602.02409)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: 本文提出Catalyst框架，通过利用全局平均池化前的通道级特征图统计信息（如均值、标准差等）生成输入依赖的缩放因子γ，并将其与现有OOD分数相乘进行弹性缩放，从而提升分布外检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有后处理OOD检测方法过度依赖logits或全局平均池化后的特征向量，忽略了池化前丰富的通道级特征统计信息，造成信号损失。

Method: Catalyst从预池化特征图中提取原始通道统计量（如均值、标准差、最大激活值），动态计算输入相关的缩放因子γ，并以乘性方式融合到基线OOD分数中，实现弹性缩放。

Result: Catalyst在CIFAR-10（ResNet-18）、CIFAR-100（ResNet-18）和ImageNet（ResNet-50）上分别将平均误报率降低32.87%、27.94%和22.25%，且兼容多种基线方法（如Energy、ReAct、KNN等）。

Conclusion: 预池化特征统计蕴含未被充分利用的OOD判别信号；Catalyst是一种通用、即插即用、与现有方法互补的后处理增强框架。

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($γ$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $γ$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.

</details>


### [273] [SelvaMask: Segmenting Trees in Tropical Forests and Beyond](https://arxiv.org/abs/2602.02426)
*Simon-Olivier Duguay,Hugo Baudchon,Etienne Laliberté,Helene Muller-Landau,Gonzalo Rivas-Torres,Arthur Ouaknine*

Main category: cs.CV

TL;DR: 本文提出了SelvaMask——一个包含8800多个热带森林树冠人工标注的新数据集，并基于此设计了一种结合视觉基础模型与领域专用检测提示器的模块化检测-分割流程，在热带密林树冠分割任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的单木树冠分割模型在热带森林等复杂密集场景中性能仍较低，缺乏高质量、具代表性的热带树冠标注数据集和适配方法。

Method: 构建了覆盖巴拿马、巴西、厄瓜多尔三地的热带森林树冠数据集SelvaMask（含8800+人工标注及标注者一致性评估）；提出模块化检测-分割流程，利用视觉基础模型（VFMs）并引入领域专用的检测提示器（detection-prompter）进行适配。

Result: 所提方法在SelvaMask上达到SOTA性能，显著优于零样本通用模型和全监督端到端方法；在外部热带与温带数据集上也验证了泛化能力。

Conclusion: SelvaMask不仅是一个具有挑战性的新基准，更推动了面向广义森林监测的可迁移树冠识别技术发展；代码与数据将开源。

Abstract: Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.

</details>


### [274] [UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing](https://arxiv.org/abs/2602.02437)
*Dianyi Wang,Chaofan Ma,Feng Han,Size Wu,Wei Song,Yibin Wang,Zhixiong Zhang,Tianhang Wang,Siyuan Wang,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出UniReason框架，通过双推理范式统一文本到图像生成与图像编辑任务，将生成视为增强世界知识的规划过程，并利用编辑能力进行细粒度视觉修正，从而实现更符合人类认知的多模态推理合成。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型在复杂合成任务中推理能力不足，且将文本到图像生成与图像编辑视为孤立能力，缺乏内在关联。

Method: 提出UniReason框架，采用双推理范式：1）将生成建模为世界知识增强的规划过程以引入隐式约束；2）利用编辑能力实现基于自我反思的细粒度视觉修正；并构建大规模推理导向数据集（~300k样本）和代理生成的视觉自校正语料。

Result: 在WISE、KrisBench和UniREditBench等推理密集型基准上取得先进性能，同时保持优异的通用合成能力。

Conclusion: UniReason成功将生成与编辑统一于共享表征与认知对齐的推理流程中，验证了联合建模对提升多模态复杂推理合成能力的有效性。

Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.

</details>


### [275] [Multi-head automated segmentation by incorporating detection head into the contextual layer neural network](https://arxiv.org/abs/2602.02471)
*Edwin Kys,Febian Febian*

Main category: cs.CV

TL;DR: 本文提出了一种基于Swin U-Net的门控多头Transformer架构，结合跨层上下文融合与并行检测头，通过检测结果对分割预测进行门控以抑制解剖学上无效切片中的假阳性，显著提升放疗自动分割的解剖合理性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习自动分割模型在缺乏目标结构的切片中易产生解剖学上不合理的假阳性（即‘幻觉’），影响临床可靠性。

Method: 提出一种门控多头Transformer架构，基于Swin U-Net，集成跨切片上下文建模和并行检测头：一个MLP用于切片级结构存在性检测，另一个上下文增强流用于像素级分割；检测输出作为门控信号抑制无效切片的分割响应；采用切片级Tversky损失缓解类别不平衡。

Result: 在Prostate-Anatomical-Edge-Cases数据集上，门控模型平均Dice损失为0.013±0.036，远优于非门控基线（0.732±0.314）；检测概率与真实解剖存在高度相关，有效消除了伪分割；非门控模型则表现出高变异性和全切片持续假阳性。

Conclusion: 检测引导的门控机制可显著提升自动分割的解剖合理性与鲁棒性，在不损害有效切片分割质量的前提下消除幻觉预测，为临床放疗自动勾画提供了更可靠的解决方案。

Abstract: Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.

</details>


### [276] [PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss](https://arxiv.org/abs/2602.02493)
*Zehong Ma,Ruihan Xu,Shiliang Zhang*

Main category: cs.CV

TL;DR: PixelGen提出了一种带感知监督的像素级扩散模型，通过LPIPS和DINO感知损失引导模型学习更有意义的感知流形，在ImageNet-256上FID达5.11，无需VAE或潜在表示，简化了生成范式。


<details>
  <summary>Details</summary>
Motivation: 现有像素扩散模型因直接在高维像素空间优化而难以收敛，且易受感知无关信号干扰，性能落后于潜在扩散模型。

Method: PixelGen引入两种互补的感知损失：LPIPS损失用于提升局部模式建模，DINO-based损失用于增强全局语义理解，从而引导扩散模型学习更有效的感知流形。

Result: 在ImageNet-256上达到FID 5.11（无classifier-free guidance，仅80 epoch）；大规模文本到图像生成中GenEval得分为0.79；优于强潜在扩散基线。

Conclusion: PixelGen证明了纯像素级扩散在合理感知监督下可超越潜在扩散，提供更简洁、更强大的生成范式，无需VAE、潜在表示或辅助阶段。

Abstract: Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [277] [PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.00007)
*MinGyu Jeon,SuWan Cho,JaeYoung Shu*

Main category: cs.CL

TL;DR: 本文提出PPoGA框架，通过 Planner-Executor 架构与预测性处理机制，实现路径修正与计划修正双重自纠错能力，显著提升多跳知识图谱问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM+KG方法易因初始推理计划错误而失败，缺乏类似人类的认知控制与问题重构能力。

Method: 提出PPoGA框架，包含Planner-Executor架构和Predictive Processing机制，并引入支持Path Correction和Plan Correction的自纠错机制。

Result: 在GrailQA、CWQ和WebQSP三个多跳KGQA基准上达到SOTA性能，显著优于现有方法。

Conclusion: 具备元认知能力（如问题重构）对构建更鲁棒、灵活的AI推理系统至关重要。

Abstract: Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.

</details>


### [278] [Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA](https://arxiv.org/abs/2602.00009)
*Samuel Thio,Matthew Lewis,Spiros Denaxas,Richard JB Dobson*

Main category: cs.CL

TL;DR: 本文提出MediGRAF，一种结合结构化图查询（Neo4j Text2Cypher）与非结构化语义检索（向量嵌入）的混合图RAG框架，用于提升EHR中临床信息检索的准确性与安全性，在MIMIC-IV数据上实现100%事实性查询召回率及高专家评分的推理能力。


<details>
  <summary>Details</summary>
Motivation: EHR系统信息过载导致临床认知负担重，现有LLM在临床场景中存在幻觉和上下文 grounding 不足问题；当前检索方法孤立处理结构化或非结构化数据，缺乏二者协同。

Method: 提出MediGRAF框架：融合Neo4j Text2Cypher实现结构化关系遍历，结合向量嵌入支持非结构化临床文本检索；基于MIMIC-IV中10例患者构建含5973节点/5963关系的医疗知识图谱，支持自然语言驱动的全病程问答。

Result: 在事实性查询中达到100%召回率；复杂推理任务获专家平均评分4.25/5，且无安全违规；验证了混合图增强对临床信息检索的有效性与安全性提升。

Conclusion: 混合图RAG（MediGRAF）显著优于传统LLM部署，为临床决策支持提供了更可靠、全面、可解释的信息检索新范式。

Abstract: Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.

</details>


### [279] [G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2602.00015)
*Xun Xu*

Main category: cs.CL

TL;DR: 本文提出G-MemLLM，一种结合冻结大语言模型主干与可训练潜在记忆库的记忆增强架构，通过GRU式门控更新机制选择性更新记忆槽，显著提升多跳推理和关系抽取性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长上下文处理中易出现'上下文腐烂'或信息稀释问题，且难以维持多跳推理中的长期事实一致性。

Method: 提出G-MemLLM架构，包含冻结LLM主干与可训练的潜在记忆库，并引入GRU风格的门控更新逻辑以选择性更新、保留或覆盖记忆槽。

Result: 在HotpotQA和ZsRE基准上显著提升性能：Llama 3.1-8B在ZsRE上准确率提升13.3%，GPT-2在HotpotQA上Answer F1提升8.56分，Llama 3.1-8B在HotpotQA上Supporting Fact F1提升6.89分。

Conclusion: G-MemLLM有效缓解了长程依赖与信息衰减问题，在不同规模模型上均展现出一致且显著的多跳推理与关系抽取能力提升。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.

</details>


### [280] [PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems](https://arxiv.org/abs/2602.00016)
*Jiongchi Yu,Yuhan Ma,Xiaoyu Zhang,Junjie Wang,Qiang Hu,Chao Shen,Xiaofei Xie*

Main category: cs.CL

TL;DR: 本文提出了PTCBENCH基准，用于评估大语言模型（LLM）在不同外部情境下人格特质的一致性，发现某些情境（如失业）会显著改变LLM的人格表现甚至推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了人格特质具有动态性和情境依赖性的心理学共识，而LLM在情感代理和AI系统中需保持一致且真实的人格以增强用户信任与参与度。

Method: 构建了PTCBENCH系统性基准，涵盖12种不同外部情境（如地点、生活事件），使用NEO五因素人格量表对39,240条人格特质记录进行严格评估。

Result: 发现特定外部情境（如‘失业’）会引发LLM显著的人格变化，并影响其推理能力；PTCBENCH提供了可扩展的现实动态环境人格一致性评估框架。

Conclusion: PTCBENCH为开发稳健且符合心理学原理的AI系统提供了可操作的评估工具和实证依据。

Abstract: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., "Unemployment") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.

</details>


### [281] [SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations](https://arxiv.org/abs/2602.00017)
*Benyamin Tabarsi,Wenbo Li,Tahreem Yasir,Aryan Santhosh Kumar,Laura Widman,Dongkuan Xu,Tiffany Barnes*

Main category: cs.CL

TL;DR: 本文提出SafeTalkCoach，一个多样性驱动的多智能体对话生成框架，用于模拟父母与孩子关于性健康的对话，并提供配套数据集。


<details>
  <summary>Details</summary>
Motivation: 由于性健康话题的私密性和敏感性，真实世界中父母与孩子之间的相关对话数据稀缺且难以收集；同时，现有大语言模型在对话生成中常偏离最佳实践，缺乏真实性和多样性。

Method: SafeTalkCoach整合了众包与合成场景、既定性健康指南、基于证据的人物设定、自适应控制模块和分层多样化机制，构建多智能体对话生成框架。

Result: 评估表明，SafeTalkCoach能在保持真实性、沟通质量和可控性的前提下，生成多样化的对话。

Conclusion: SafeTalkCoach框架及其配套数据集有望支持AI研究与健康传播实践的双重发展。

Abstract: The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may deviate from best practices and frequently lack realism and diversity. We introduce SafeTalkCoach, a diversity-driven multi-agent dialogue generation framework that simulates parent-child conversations about sexual health, and present an accompanying dataset. SafeTalkCoach integrates crowd-sourced and synthesized scenarios, established sexual health guidelines, evidence-based personas, adaptive control modules, and hierarchical diversification. Through evaluations, we demonstrate that SafeTalkCoach generates diverse conversations while maintaining realism, communication quality, and controllability in practice. Our goal is that the SafeTalkCoach framework and the dataset support both AI research and health communications practices.

</details>


### [282] [Construct, Align, and Reason: Large Ontology Models for Enterprise Knowledge Management](https://arxiv.org/abs/2602.00029)
*Yao Zhang,Hongyin Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种名为大本体模型（LOM）的统一框架，通过构建双层企业本体、三阶段训练流程（本体指令微调、文本-本体对齐、多任务课程学习），提升知识图谱在隐式关系发现与复杂语义推理上的能力，并在基准测试中超越DeepSeek-V3.2。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱难以发现隐式关系，语义理解不足，难以支撑企业级多源异构数据融合与复杂问答推理。

Method: 提出统一的construct-align-reason框架（即LOM），包括：1）从结构化数据库和非结构化文本构建双层企业本体并融合；2）三阶段训练：本体指令微调、文本-本体接地、基于课程学习的多任务本体-语言对指令调优；3）构建覆盖多种本体推理任务的数据集。

Result: 4B参数的LOM在自建基准上达到89.47%准确率，在复杂图推理任务上优于DeepSeek-V3.2，验证了本体结构与语言能力的有效融合。

Conclusion: LOM通过深度融合本体结构先验与大语言模型能力，显著提升了企业级知识管理中的语义理解与推理性能，为知识图谱与大模型协同提供了新范式。

Abstract: Enterprise-scale knowledge management faces significant challenges in integrating multi-source heterogeneous data and enabling effective semantic reasoning. Traditional knowledge graphs often struggle with implicit relationship discovery and lack sufficient semantic understanding for complex question answering. To address these limitations, we introduce a unified construct--align--reason framework, the large ontology model (LOM). We first build a dual-layer enterprise ontology from structured databases and unstructured text, subsequently fusing these sources into a comprehensive enterprise ontology. To enable instruction-aligned reasoning, we propose a unified three-stage training pipeline: ontology instruction fine-tuning to improve structural understanding; text-ontology grounding to strengthen node semantic encoding; and multi-task instruction tuning on ontology-language pairs with curriculum learning to enhance semantic reasoning and generation. We also construct comprehensive training and evaluation datasets covering diverse ontology reasoning tasks. On this benchmark, our 4B-parameter LOM achieves 89.47% accuracy and outperforms DeepSeek-V3.2 on complex graph reasoning, indicating effective fusion of ontology structure and language.

</details>


### [283] [Reversible Diffusion Decoding for Diffusion Language Models](https://arxiv.org/abs/2602.00150)
*Xinyun Wang,Min Zhang,Sen Cui,Zhikang Chen,Bo Jiang,Kun Kuang,Mingbao Lin*

Main category: cs.CL

TL;DR: 本文提出了一种可逆扩散解码（RDD）框架，通过检测停滞状态、缓存模型状态实现高效回溯，并结合置信度引导的重掩码策略，提升扩散语言模型生成的鲁棒性与质量，同时保持并行效率。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在块级解码中存在不可逆承诺问题，易导致停滞（reverse diffusion无法继续推进），影响生成质量。

Method: 提出可逆扩散解码（RDD）：1）将停滞建模为状态依赖的反向过程失败；2）利用缓存模型状态实现无需重算的块级回溯；3）采用置信度引导的重掩码策略，选择性重初始化不确定token，保留可靠上下文。

Result: 实验表明RDD在保持低计算开销的同时，显著提升了生成鲁棒性与质量，优于基线方法。

Conclusion: RDD通过引入可逆性机制，有效缓解了扩散语言模型因早期错误承诺导致的生成退化问题，在不牺牲并行效率的前提下增强了生成可靠性。

Abstract: Diffusion language models enable parallel token generation through block-wise decoding, but their irreversible commitments can lead to stagnation, where the reverse diffusion process fails to make further progress under a suboptimal context.We propose Reversible Diffusion Decoding (RDD), a decoding framework that introduces reversibility into block-wise diffusion generation. RDD detects stagnation as a state-dependent failure of the reverse process and enables efficient backtracking to earlier blocks without recomputation via cached model states. To avoid repeated failure trajectories, RDD applies confidence-guided re-masking to selectively reinitialize uncertain tokens while preserving reliable context.This reversible formulation allows decoding to recover from early commitment errors while maintaining the parallel efficiency of diffusion-based generation. Experiments show that RDD improves generation robustness and quality over baselines with minimal computational overhead.

</details>


### [284] [DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking](https://arxiv.org/abs/2602.00238)
*Tianyi Hu,Niket Tandon,Akhil Arora*

Main category: cs.CL

TL;DR: 本文提出DIVERGE框架，解决现有RAG系统在开放性问题中忽视答案多样性的问题，通过反思引导生成与记忆增强迭代优化，在保持答案质量的同时显著提升多样性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统假设每个查询仅有一个正确答案，忽略了需多视角回答的信息获取场景，导致多样性不足、创造力受限及信息访问不公平。

Method: 提出DIVERGE——一种即插即用的智能体式RAG框架，包含反思引导生成和记忆增强的迭代优化机制，并设计新指标评估多样性-质量权衡。

Result: 在真实世界Infinity-Chat数据集上，DIVERGE在多样性-质量权衡上优于各基线及SOTA方法，显著提升多样性且不损质量；新指标与人工评价高度相关。

Conclusion: 当前LLM系统在开放性信息检索中存在系统性多样性建模缺失，显式建模多样性可有效缓解该问题。

Abstract: Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge

</details>


### [285] [Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering](https://arxiv.org/abs/2602.00279)
*Philip Müller,Nicholas Popovič,Michael Färber,Peter Steinbach*

Main category: cs.CL

TL;DR: 本文提出了首个面向科学问答（QA）中不确定性量化（UQ）的大规模基准，系统评估了685,000条长文本响应下多种UQ方法在20个LLM上的校准性能，发现token级置信度受指令微调极化影响而不可靠，序列级上答案频率（一致性）最可靠，且仅依赖ECE会误导UQ方法评估。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化（UQ）方法在科学问答这一依赖事实检索与推理的领域中缺乏充分验证，亟需可复现、大规模、覆盖多模型与多任务的基准来推动可信评估。

Method: 构建开源可扩展的大规模UQ评估框架，涵盖20个基础/指令微调/推理增强型LLM，在7个科学QA数据集（含多选与算术题）上通过提示工程模拟开放问答，分析token级与序列级UQ指标（如置信度、答案频率、口头化表达等），并考察ECE等常用校准度量的局限性。

Result: 指令微调导致token级概率质量极化，削弱其作为不确定性估计的可靠性；推理微调部分缓解该问题但因厂商而异；序列级上口头化UQ存在系统性偏差且与正确性相关性差，而答案频率（采样一致性）校准效果最佳；单独使用ECE易导致对UQ方法性能的误判。

Conclusion: 当前LLM的UQ方法在科学QA中存在根本性局限，需摒弃单一ECE评估范式，转向更鲁棒的序列级一致性指标，并重新审视指令/推理微调对不确定性建模的影响。

Abstract: Large Language Models (LLMs) are commonly used in Question Answering (QA) settings, increasingly in the natural sciences if not science at large. Reliable Uncertainty Quantification (UQ) is critical for the trustworthy uptake of generated answers. Existing UQ approaches remain weakly validated in scientific QA, a domain relying on fact-retrieval and reasoning capabilities. We introduce the first large-scale benchmark for evaluating UQ metrics in reasoning-demanding QA studying calibration of UQ methods, providing an extensible open-source framework to reproducibly assess calibration. Our study spans up to 20 large language models of base, instruction-tuned and reasoning variants. Our analysis covers seven scientific QA datasets, including both multiple-choice and arithmetic question answering tasks, using prompting to emulate an open question answering setting. We evaluate and compare methods representative of prominent approaches on a total of 685,000 long-form responses, spanning different reasoning complexities representative of domain-specific tasks. At the token level, we find that instruction tuning induces strong probability mass polarization, reducing the reliability of token-level confidences as estimates of uncertainty. Models further fine-tuned for reasoning are exposed to the same effect, but the reasoning process appears to mitigate it depending on the provider. At the sequence level, we show that verbalized approaches are systematically biased and poorly correlated with correctness, while answer frequency (consistency across samples) yields the most reliable calibration. In the wake of our analysis, we study and report the misleading effect of relying exclusively on ECE as a sole measure for judging performance of UQ methods on benchmark datasets. Our findings expose critical limitations of current UQ methods for LLMs and standard practices in benchmarking thereof.

</details>


### [286] [Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations Explanation of Large Language Models](https://arxiv.org/abs/2602.00300)
*Xilin Gong,Shu Yang,Zehua Cao,Lynne Billard,Di Wang*

Main category: cs.CL

TL;DR: 本文揭示了Patchscopes框架中LLM在解释隐藏表征时存在系统性不忠实问题，即模型倾向于依赖固有语言先验而非上下文信息；为此提出BALOR方法，通过logit重校准抑制偏差、增强上下文信号，在多个LLM上实现最高33%的相对性能提升。


<details>
  <summary>Details</summary>
Motivation: Patchscopes利用LLM自身解码内部隐藏表征以生成可读解释，但作者发现LLM常忽略表征中的真实上下文（如将‘紫色西兰花’解释为‘绿色’），暴露出其对语言先验的过度依赖，导致解释不忠实。

Method: 首先构建偏置场景下的评估数据集，量化Patchscopes的不忠实程度；进而提出Bias Alignment through Logit Recalibration (BALOR)，将未补丁提示的输出logits视为模型偏差估计，并与补丁后logits对比，通过logit分布重校准来抑制偏差、增强上下文信息。

Result: 实验表明，Patchscopes在偏置场景下平均信仰度下降18.84%；BALOR在多个LLM上显著优于现有基线，最高实现33%相对性能提升。

Conclusion: LLM在Patchscopes中的解释行为存在系统性不忠实，源于对固有语言模式的依赖；BALOR通过logit层面的偏差校准有效缓解该问题，提升了隐藏表征解释的忠实性与可靠性。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities for hidden representation interpretation through Patchscopes, a framework that uses LLMs themselves to generate human-readable explanations by decoding from internal hidden representations. However, our work shows that LLMs tend to rely on inherent linguistic patterns, which can override contextual information encoded in the hidden representations during decoding. For example, even when a hidden representation encodes the contextual attribute "purple" for "broccoli", LLMs still generate "green" in their explanations, reflecting a strong prior association. This behavior reveals a systematic unfaithfulness in Patchscopes. To systematically study this issue, we first designed a dataset to evaluate the faithfulness of Patchscopes under biased cases, and our results show that there is an 18.84\% faithfulness decrease on average. We then propose Bias Alignment through Logit Recalibration (BALOR), which treats the output logits from an unpatched prompt as capturing model bias and contrasts them with logits obtained under patched contextual information. By recalibrating the logit distribution through this contrast, BALOR suppresses model bias and amplifies contextual information during generation. Experiments across multiple LLMs demonstrate that BALOR consistently outperforms existing baselines, achieving up to 33\% relative performance improvement.

</details>


### [287] [MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting Minutes](https://arxiv.org/abs/2602.00316)
*Rodrigo Batista,Luís Filipe Cunha,Purificação Silvano,Nuno Guimarães,Alípio Jorge,Evelin Amorim,Ricardo Campos*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段流水线方法，用于从市政会议纪要中提取非结构化元数据（如会议编号、日期、地点等），结合问答模型与去词汇化增强的Transformer NER模型，并在开源与闭源大模型上进行基准测试，建立了该任务首个基准。


<details>
  <summary>Details</summary>
Motivation: 市政会议纪要格式异构、元数据缺乏标准化，现有NER模型难以适配其领域特有类别，亟需专用信息抽取方法。

Method: 采用两阶段流程：第一阶段用问答模型定位含元数据的开闭段落；第二阶段使用BERTimbau/XLM-RoBERTa（带或不带CRF层）进行细粒度实体抽取，并引入去词汇化提升鲁棒性；同时对比评估Phi和Gemini等LLM的性能、推理成本与碳足迹。

Result: 所提方法在本领域数据上表现优于更大规模通用大模型；但在跨市政数据上泛化能力下降，揭示了文本变异性与语言复杂性的挑战；构建了首个市政会议纪要元数据抽取基准。

Conclusion: 该工作为市政治理文档智能处理提供了有效技术路径与基础基准，凸显了领域适配建模与可持续AI评估的重要性。

Abstract: Municipal meeting minutes are official documents of local governance, exhibiting heterogeneous formats and writing styles. Effective information retrieval (IR) requires identifying metadata such as meeting number, date, location, participants, and start/end times, elements that are rarely standardized or easy to extract automatically. Existing named entity recognition (NER) models are ill-suited to this task, as they are not adapted to such domain-specific categories. In this paper, we propose a two-stage pipeline for metadata extraction from municipal minutes. First, a question answering (QA) model identifies the opening and closing text segments containing metadata. Transformer-based models (BERTimbau and XLM-RoBERTa with and without a CRF layer) are then applied for fine-grained entity extraction and enhanced through deslexicalization. To evaluate our proposed pipeline, we benchmark both open-weight (Phi) and closed-weight (Gemini) LLMs, assessing predictive performance, inference cost, and carbon footprint. Our results demonstrate strong in-domain performance, better than larger general-purpose LLMs. However, cross-municipality evaluation reveals reduced generalization reflecting the variability and linguistic complexity of municipal records. This work establishes the first benchmark for metadata extraction from municipal meeting minutes, providing a solid foundation for future research in this domain.

</details>


### [288] [Detecting AI-Generated Content in Academic Peer Reviews](https://arxiv.org/abs/2602.00319)
*Siyuan Shen,Kai Wang*

Main category: cs.CL

TL;DR: 本研究通过检测模型分析了ICLR和Nature Communications会议/期刊中AI生成审稿意见的时间演变，发现2022年前AI生成内容极少，而到2025年分别达20%和12%，尤其NC在2024年第三季度至第四季度增长最显著，提示AI正快速介入学术同行评审。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的普及引发了对其在学术同行评审中角色的关切，亟需实证了解AI生成内容在审稿过程中的实际渗透趋势。

Method: 使用基于历史审稿意见训练的AI内容检测模型，对ICLR和Nature Communications历年审稿意见进行时间序列检测分析。

Result: 2022年前AI生成审稿意见极少；2025年ICLR约20%、NC约12%的审稿意见被判定为AI生成；NC在2024年第三季度至第四季度出现最显著增长。

Conclusion: AI生成内容在同行评审中正迅速增加，其对学术评价质量、透明度与伦理的影响亟需深入研究与规范引导。

Abstract: The growing availability of large language models (LLMs) has raised questions about their role in academic peer review. This study examines the temporal emergence of AI-generated content in peer reviews by applying a detection model trained on historical reviews to later review cycles at International Conference on Learning Representations (ICLR) and Nature Communications (NC). We observe minimal detection of AI-generated content before 2022, followed by a substantial increase through 2025, with approximately 20% of ICLR reviews and 12% of Nature Communications reviews classified as AI-generated in 2025. The most pronounced growth of AI-generated reviews in NC occurs between the third and fourth quarter of 2024. Together, these findings provide suggestive evidence of a rapidly increasing presence of AI-assisted content in peer review and highlight the need for further study of its implications for scholarly evaluation.

</details>


### [289] [DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning](https://arxiv.org/abs/2602.00352)
*Li Siyan,Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.CL

TL;DR: 本文提出了DETOUR基准，用于评估智能体在模糊、欠指定检索场景（如话到嘴边却想不起）中的多轮回忆能力，结果表明当前SOTA模型在此类任务上准确率仅为36%。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准局限于单轮检索，无法真实反映人类在对话中通过多轮交互逐步回忆信息的过程，因此需要构建更贴近实际的多轮、欠指定检索评估基准。

Method: 提出双智能体评估框架DETOUR：一个待评测的Primary Agent通过多轮提问与一个固定的Memory Agent交互，后者提供模糊、不完整但一致的线索，以模拟‘话到嘴边’的回忆过程；基准包含1011个跨模态（文本、图像、音频、视频）提示。

Result: 当前最先进模型在DETOUR全模态任务上的准确率仅为36%，显著低于单轮设置下的性能，验证了该基准的挑战性与必要性。

Conclusion: DETOUR揭示了现有大模型在多轮、欠指定、跨模态检索任务上的严重不足，强调需针对性提升其在模糊线索下持续推理与迭代查询的能力。

Abstract: When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.

</details>


### [290] [DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models](https://arxiv.org/abs/2602.00377)
*Zhaochen Hong,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出DecompressionLM，一种无状态、零样本的概念图提取框架，用于发现语言模型中编码的知识，无需预定义查询或跨序列共享状态。该方法通过使用Van der Corput低差异序列与算术解码，实现确定性、高度并行的生成，并揭示了不同量化方式对概念覆盖度的显著影响。


<details>
  <summary>Details</summary>
Motivation: 现有知识探测方法依赖预定义查询，限制了对未知概念的提取；同时，常见解码式探测存在跨序列耦合、竞争性解码抑制长尾概念、以及顺序探索导致的可扩展性差等问题。

Method: 提出DecompressionLM框架，采用Van der Corput低差异序列结合算术解码，实现无状态、确定性、高度并行的概念生成；在多个模型家族及量化变体上进行评估，并通过语料库验证和MMLU-Pro Law任务分析概念覆盖与幻觉现象。

Result: 激活感知量化（AWQ-4bit）使概念覆盖提升30–170%，而均匀量化（GPTQ-Int4）导致71–86%的覆盖崩溃；MMLU-Pro Law中表现最优与最差模型间存在17分的幻觉差距；概念覆盖被确立为评估压缩模型知识广度与事实基础的新维度。

Conclusion: DecompressionLM提供了一种更全面、可扩展、无需先验知识的模型知识探测范式，揭示了量化策略对隐含知识表达的深层影响，并将概念覆盖确立为模型评估的关键补充指标。

Abstract: Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: cross-sequence coupling that concentrates probability mass on high-frequency prefixes, competitive decoding effects that suppress long-tail concepts, and scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse -- divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 17-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models useful for their deployment.

</details>


### [291] [Clause-Internal or Clause-External? Testing Turkish Reflexive Binding in Adapted versus Chain of Thought Large Language Models](https://arxiv.org/abs/2602.00380)
*Sercan Karakaş*

Main category: cs.CL

TL;DR: 本研究评估了两种大语言模型（OpenAI的o1 Mini和Trendyol-LLM-7B）对土耳其语反身代词（kendi/kendisi）约束关系的建模能力，发现二者在本地与长距离先行词选择上表现出显著差异：前者几乎均等选择，后者强烈偏好本地先行词。


<details>
  <summary>Details</summary>
Motivation: 探究当前主流大语言模型是否能正确建模土耳其语中反身代词的句法约束关系，尤其是本地与非本地先行词之间的区分能力。

Method: 构建包含100个平衡句子的数据集，对比测试o1 Mini（链式思维模型）和Trendyol-LLM-7B-base-v0.1（基于LLaMA-2、针对土耳其语微调的模型），采用句子级困惑度与强制选择范式联合评估先行词选择倾向。

Result: Trendyol-LLM在约70%的试验中偏好本地先行词，表现出强局部性偏差；而o1 Mini在本地与长距离解读间选择近乎均等，两者绑定行为存在显著差异。

Conclusion: 不同架构与训练策略的大语言模型在处理土耳其语反身代词约束时展现出截然不同的语法敏感性，提示模型语法知识并非普遍具备，而是高度依赖于训练数据与方法。

Abstract: This study evaluates whether state-of-the-art large language models capture the binding relations of Turkish reflexive pronouns. We construct a balanced set of 100 sentences that pit local against non-local antecedents for the reflexives kendi and kendisi, and test two contrasting systems: an OpenAI chain-of-thought model designed for multi-step reasoning and Trendyol-LLM-7B-base-v0.1, a LLaMA-2-derived model extensively fine-tuned on Turkish data. Antecedent choice is assessed using a combined sentence-level perplexity and forced-choice paradigm. Trendyol-LLM favours local bindings in approximately 70% of trials, exhibiting a strong locality bias, whereas o1 Mini distributes its choices almost evenly between local and long-distance readings, revealing a marked contrast in binding behaviour across the two systems.

</details>


### [292] [Segment-Level Attribution for Selective Learning of Long Reasoning Traces](https://arxiv.org/abs/2602.00425)
*Siyuan Wang,Yanchen Liu,Xiang Ren*

Main category: cs.CL

TL;DR: 本文提出了一种基于集成梯度归因的段级选择性监督微调（SegmentSelectiveSFT）框架，通过衡量推理链中各段的归因强度与方向一致性，筛选出具有反思性推理的重要段落进行训练，从而提升大推理模型在准确率和输出效率上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）生成的思维链（CoT）中存在大量冗余、重复或截断内容，而监督微调（SFT）会强化这些低信息量模式，损害性能。

Method: 利用集成梯度（integrated gradients）量化每个token对答案的贡献，并聚合为两个段级指标：归因强度（attribution strength）与方向一致性（direction consistency）；据此识别高归因强度但中等方向一致性的‘反思性’推理段落，仅对这些段落进行选择性SFT，其余段落损失被掩码。

Result: 在多个模型和数据集上的实验表明，该方法提升了推理准确率和输出效率，更有效地从长推理链中学习。

Conclusion: 段级选择性学习能有效缓解推理链冗余问题，引导模型聚焦真正有贡献的推理过程，是一种提升LRMs训练效率与性能的新范式。

Abstract: Large Reasoning Models (LRMs) achieve strong reasoning performance by generating long chains of thought (CoTs), yet only a small fraction of these traces meaningfully contributes to answer prediction, while the majority contains repetitive or truncated content. Such output redundancy is further propagated after supervised finetuning (SFT), as models learn to imitate verbose but uninformative patterns, which can degrade performance. To this end, we incorporate integrated gradient attribution to quantify each token's influence on final answers and aggregate them into two segment-level metrics: (1) \textit{attribution strength} measures the overall attribution magnitude; and (2) \textit{direction consistency} captures whether tokens' attributions within a segment are uniformly positive or negative (high consistency), or a mixture of both (moderate consistency). Based on these two metrics, we propose a segment-level selective learning framework to identify important segments with high attribution strength but moderate consistency that indicate reflective rather than shallow reasoning. The framework then applies selective SFT on these important segments while masking loss for unimportant ones. Experiments across multiple models and datasets show that our approach improves accuracy and output efficiency, enabling more effective learning from long reasoning traces~\footnote{Code and data are available at https://github.com/SiyuanWangw/SegmentSelectiveSFT}.

</details>


### [293] [When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.00428)
*Naen Xu,Hengyu An,Shuo Shi,Jinghuai Zhang,Chunyi Zhou,Changjiang Li,Tianyu Du,Zhihui Fu,Jun Wang,Shouling Ji*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）驱动的多智能体系统中集体认知偏差（特别是‘曼德拉效应’）的存在性、成因及缓解策略，提出了新基准MANBENCH，并设计了提示级与模型级防御方法，平均降低曼德拉效应达74.40%。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统易受集体认知偏差（如曼德拉效应）影响，导致错误记忆与 misinformation 传播，该问题尚未被充分探索，且存在伦理风险。

Method: 构建MANBENCH基准，涵盖四类易受曼德拉效应影响的任务和五种交互协议；在多种LLM上量化评估该效应；提出提示级（如认知锚定、信源审查）与模型级对齐防御策略。

Result: 验证了LLM多智能体系统中曼德拉效应普遍存在；识别出角色设定、记忆时长、交互协议等关键影响因素；所提防御策略平均降低效应74.40%。

Conclusion: 曼德拉效应是LLM多智能体系统中真实且可量化的风险；需从提示设计与模型对齐双路径协同提升系统鲁棒性与伦理可靠性。

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.

</details>


### [294] [What Matters to an LLM? Behavioral and Computational Evidences from Summarization](https://arxiv.org/abs/2602.00459)
*Yongxin Zhou,Changshun Wu,Philippe Mulhem,Didier Schwab,Maxime Peyrard*

Main category: cs.CL

TL;DR: 本文通过行为和计算分析相结合的方法，探究了大语言模型（LLMs）在摘要生成中隐含的‘重要性’判断机制，发现LLMs具有稳定且家族特异的重要性模式，并定位到中后期注意力层中的关键注意力头负责该机制。


<details>
  <summary>Details</summary>
Motivation: LLMs虽在摘要任务上表现优异，但其内部驱动信息选择的‘重要性’概念尚不透明，亟需可解释性研究。

Method: 结合行为分析（生成长度可控摘要并统计信息单元被选频率以构建经验重要性分布）与计算分析（分析注意力头与经验重要性分布的对齐程度及各层预测能力）。

Result: 发现LLMs展现出一致且区别于传统模型的重要性模式，按模型家族聚类而非参数量；中晚期层的部分注意力头与经验重要性高度对齐。

Conclusion: LLMs在摘要中具备内在、结构化的重要性表征机制，该机制可被行为与计算方法联合揭示，为后续解释和控制其信息选择提供了基础路径。

Abstract: Large Language Models (LLMs) are now state-of-the-art at summarization, yet the internal notion of importance that drives their information selections remains hidden. We propose to investigate this by combining behavioral and computational analyses. Behaviorally, we generate a series of length-controlled summaries for each document and derive empirical importance distributions based on how often each information unit is selected. These reveal that LLMs converge on consistent importance patterns, sharply different from pre-LLM baselines, and that LLMs cluster more by family than by size. Computationally, we identify that certain attention heads align well with empirical importance distributions, and that middle-to-late layers are strongly predictive of importance. Together, these results provide initial insights into what LLMs prioritize in summarization and how this priority is internally represented, opening a path toward interpreting and ultimately controlling information selection in these models.

</details>


### [295] [Words that make SENSE: Sensorimotor Norms in Learned Lexical Token Representations](https://arxiv.org/abs/2602.00469)
*Abhinav Gupta,Toben H. Mintz,Jesse Thomason*

Main category: cs.CL

TL;DR: 本文提出了SENSE模型，通过将词嵌入映射到Lancaster感官运动规范来实现语言的具身化表征，并通过行为实验验证了其与人类感知选择率的相关性，还发现了与内感受相关的音义联觉模式。


<details>
  <summary>Details</summary>
Motivation: 词嵌入基于共现统计，缺乏人类语言理解所依赖的感觉和运动经验基础，因此需要构建具身化的语义表征模型。

Method: 提出SENSE（Sensorimotor Embedding Norm Scoring Engine）模型，学习从词向量到Lancaster传感器运动规范的映射；开展含281名参与者的心理行为实验，评估对无意义词的多模态感官联想选择率，并进行子词水平的音义联觉分析。

Result: SENSE评分与人类在6/11种感觉运动模态上的选择率呈统计显著相关；子词分析揭示了内感受规范中存在系统的音义联觉（phonosthemic）模式。

Conclusion: SENSE为连接分布式语义与具身认知提供了可行路径，且音义联觉模式的发现支持从文本数据中自动挖掘候选语音义素的潜力。

Abstract: While word embeddings derive meaning from co-occurrence patterns, human language understanding is grounded in sensory and motor experience. We present $\text{SENSE}$ $(\textbf{S}\text{ensorimotor }$ $\textbf{E}\text{mbedding }$ $\textbf{N}\text{orm }$ $\textbf{S}\text{coring }$ $\textbf{E}\text{ngine})$, a learned projection model that predicts Lancaster sensorimotor norms from word lexical embeddings. We also conducted a behavioral study where 281 participants selected which among candidate nonce words evoked specific sensorimotor associations, finding statistically significant correlations between human selection rates and $\text{SENSE}$ ratings across 6 of the 11 modalities. Sublexical analysis of these nonce words selection rates revealed systematic phonosthemic patterns for the interoceptive norm, suggesting a path towards computationally proposing candidate phonosthemes from text data.

</details>


### [296] [Intention-Adaptive LLM Fine-Tuning for Text Revision Generation](https://arxiv.org/abs/2602.00477)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

TL;DR: 本文提出Intention-Tuning框架，通过层自适应的意图微调方法，提升大语言模型在多意图修订生成任务中的性能，尤其适用于小规模标注数据场景。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在基于意图的文本生成（如修订生成）中表现不足，尤其难以处理复杂多样的多意图场景；而依赖大量标注数据的指令微调又受限于数据稀缺和高成本。

Method: 提出Intention-Tuning：一种意图自适应的层式大语言模型微调框架，动态选择部分模型层学习意图表示，并将其表征迁移至修订生成任务。

Result: 在小规模修订语料上实验表明，Intention-Tuning 有效且高效，性能优于多种PEFT基线方法。

Conclusion: Intention-Tuning为低资源意图驱动文本生成任务提供了可行、高效的微调范式，拓展了LLM在精细化写作辅助等场景的应用潜力。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in various context-based text generation tasks, such as summarization and reasoning; however, their applications in intention-based generation tasks remain underexplored. One such example is revision generation, which requires the generated text to explicitly reflect the writer's actual intentions. Identifying intentions and generating desirable revisions are challenging due to their complex and diverse nature. Although prior work has employed LLMs to generate revisions with few-shot learning, they struggle with handling entangled multi-intent scenarios. While fine-tuning LLMs using intention-based instructions appears promising, it demands large amounts of annotated data, which is expensive and scarce in the revision community. To address these challenges, we propose Intention-Tuning, an intention-adaptive layer-wise LLM fine-tuning framework that dynamically selects a subset of LLM layers to learn the intentions and subsequently transfers their representations to revision generation. Experimental results suggest that Intention-Tuning is effective and efficient on small revision corpora, outperforming several PEFT baselines.

</details>


### [297] [From Knowledge to Inference: Scaling Laws of Specialized Reasoning on GlobalHealthAtlas](https://arxiv.org/abs/2602.00491)
*Zhaokun Yan,Zhaohan Liu,Wuzheng Dong,Lijie Feng,Chengxiao Dai*

Main category: cs.CL

TL;DR: 本文提出了GlobalHealthAtlas数据集和配套的评估框架，旨在推动公共卫生领域的机器学习研究，特别是针对多语言、多层次推理任务的安全关键型大模型训练与评估。


<details>
  <summary>Details</summary>
Motivation: 公共卫生推理需要基于科学证据、专家共识和安全约束的群体层面推断，但目前缺乏结构化的机器学习问题定义、监督信号和基准测试。

Method: 构建了包含28万实例的多语言、多领域、多难度层级的GlobalHealthAtlas数据集；提出LLM辅助的数据构建与质量控制流程（含检索、去重、证据校验、标签验证）；设计了基于多LLM高置信度判断蒸馏出的六维评估器。

Result: 发布了首个大规模、分层标注、多语言的公共健康推理数据集，并配套可复现的评估框架，支持细粒度切片评估和安全关键场景下的模型评测。

Conclusion: 该工作为公共卫生领域的大模型训练与评估提供了新范式，填补了现有QA基准在安全性、专业性与多语言支持上的不足。

Abstract: Public health reasoning requires population level inference grounded in scientific evidence, expert consensus, and safety constraints. However, it remains underexplored as a structured machine learning problem with limited supervised signals and benchmarks. We introduce \textbf{GlobalHealthAtlas}, a large scale multilingual dataset of 280,210 instances spanning 15 public health domains and 17 languages, stratified into three difficulty levels from health literacy to epidemiological and policy reasoning. Instances are derived from openly available public health sources and labeled by language, domain, and difficulty to support supervised learning and slice based evaluation. We further propose large language model (LLM) assisted construction and quality control pipeline with retrieval, duplication, evidence grounding checks, and label validation to improve consistency at scale. Finally, we present a domain aligned evaluator distilled from high confidence judgments of diverse LLMs to assess outputs along six dimensions: Accuracy, Reasoning, Completeness, Consensus Alignment, Terminology Norms, and Insightfulness. Together, these contributions enable reproducible training and evaluation of LLMs for safety critical public health reasoning beyond conventional QA benchmarks.

</details>


### [298] [Culturally-Grounded Governance for Multilingual Language Models: Rights, Data Boundaries, and Accountable AI Design](https://arxiv.org/abs/2602.00497)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.CL

TL;DR: 本文批判了当前以英语为中心的多语言大模型治理框架，提出了一种基于文化视角的治理框架，强调数据、规范与问责机制需契合本地文化语境，以避免加剧全球不平等。


<details>
  <summary>Details</summary>
Motivation: 现有治理框架假设英语中心、用户同质化和抽象的公平概念，忽视低资源语言和文化边缘群体的实际需求与风险，导致数据实践、模型行为与问责机制脱离本地规范、权利与期望。

Method: 基于人本计算与AI治理的跨文化视角，综合分析多语言模型行为、数据不对称性及社会技术危害的现有证据，提出文化嵌入的治理框架，并识别三大相互关联的治理挑战。

Result: 识别出三类核心治理挑战：训练与评估中的文化语言不平等；全球部署与本地规范、价值观及权力结构的错位；面向边缘语言社群的问责机制缺失。提出以文化为根基的治理概念议程，而非新基准。

Conclusion: 多语言AI治理应被重新定义为一个社会文化与权利导向的问题；文化嵌入的治理对防止多语言大模型以‘规模’与‘中立’之名复制全球不平等至关重要。

Abstract: Multilingual large language models (MLLMs) are increasingly deployed across cultural, linguistic, and political contexts, yet existing governance frameworks largely assume English-centric data, homogeneous user populations, and abstract notions of fairness. This creates systematic risks for low-resource languages and culturally marginalized communities, where data practices, model behavior, and accountability mechanisms often fail to align with local norms, rights, and expectations. Drawing on cross-cultural perspectives in human-centered computing and AI governance, this paper synthesizes existing evidence on multilingual model behavior, data asymmetries, and sociotechnical harm, and articulates a culturally grounded governance framework for MLLMs. We identify three interrelated governance challenges: cultural and linguistic inequities in training data and evaluation practices, misalignment between global deployment and locally situated norms, values, and power structures, and limited accountability mechanisms for addressing harms experienced by marginalized language communities. Rather than proposing new technical benchmarks, we contribute a conceptual agenda that reframes multilingual AI governance as a sociocultural and rights based problem. We outline design and policy implications for data stewardship, transparency, and participatory accountability, and argue that culturally grounded governance is essential for ensuring that multilingual language models do not reproduce existing global inequalities under the guise of scale and neutrality.

</details>


### [299] [Reasoning by Commented Code for Table Question Answering](https://arxiv.org/abs/2602.00543)
*Seho Pyo,Jiheon Seok,Jaejin Lee*

Main category: cs.CL

TL;DR: 本文提出了一种带自然语言注释的分步代码生成框架，用于提升表格问答（TableQA）任务中大模型的数值准确性和可解释性，并在WikiTableQuestions基准上达到84.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统表格线性化破坏二维结构关系，现有端到端或单行程序方法数值准确性低、可解释性差。

Method: 设计带简洁自然语言注释的多行可执行Python程序生成框架，将TableQA推理过程显式分解为逐步代码步骤。

Result: 在WikiTableQuestions上，仅用Qwen2.5-Coder-7B-Instruct达70.9%准确率（超越Repanda的67.6%）；结合轻量级答案选择机制后达84.3%。

Conclusion: 显式分步代码生成+注释能有效提升TableQA的准确性与可解释性，且易于与现有端到端模型集成。

Abstract: Table Question Answering (TableQA) poses a significant challenge for large language models (LLMs) because conventional linearization of tables often disrupts the two-dimensional relationships intrinsic to structured data. Existing methods, which depend on end-to-end answer generation or single-line program queries, typically exhibit limited numerical accuracy and reduced interpretability. This work introduces a commented, step-by-step code-generation framework that incorporates explicit reasoning into the Python program-generation process. The approach decomposes TableQA reasoning into multi-line executable programs with concise natural language comments, thereby promoting clearer reasoning and increasing the likelihood of generating correct code. On the WikiTableQuestions benchmark, the proposed method achieves 70.9\% accuracy using Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6\%). Integrating the proposed framework with a robust end-to-end TableQA model via a lightweight answer-selection mechanism yields further improvements. This combined approach achieves up to 84.3\% accuracy on the WikiTableQuestions benchmark.

</details>


### [300] [Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation](https://arxiv.org/abs/2507.21934)
*Tianyi Hu,Andrea Morales-Garzón,Jingyi Zheng,Maria Maistro,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 本文提出CARRIAGE框架，改进检索增强生成（RAG）在跨文化食谱改编中的多样性不足问题，首次专为生成高度多样化输出而设计，在多样性和质量上实现帕累托最优。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在跨文化食谱改编中虽能保障文化适配性与原味保留，但过度依赖有限上下文，难以生成多样化结果，无法满足多元饮食需求与偏好。

Method: 提出CARRIAGE——一种即插即用的RAG框架，通过增强检索阶段的多样性与优化上下文组织方式，提升生成结果的多样性。

Result: 实验表明，CARRIAGE在食谱改编的多样性与质量上相较闭卷式大语言模型达到帕累托效率。

Conclusion: CARRIAGE是首个显式面向高多样性输出的RAG框架，有效克服了传统RAG在创意多解任务中利用上下文多样性能力弱的局限。

Abstract: In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs.

</details>


### [301] [A Hierarchical and Attentional Analysis of Argument Structure Constructions in BERT Using Naturalistic Corpora](https://arxiv.org/abs/2602.00554)
*Liu Kaipeng,Wu Ling*

Main category: cs.CL

TL;DR: 本研究探讨了BERT模型如何处理四种基本论元结构构式，发现其表征具有层次性：构式特异性信息在早期层出现，在中间层形成最可分离的聚类，并在后期层保持稳定。


<details>
  <summary>Details</summary>
Motivation: 探究BERT模型对基本论元结构构式的内部表征机制。

Method: 采用多维分析框架，结合MDS和t-SNE降维、广义判别值（GDV）评估聚类分离度、Fisher判别比（FDR）进行线性探针分析，以及注意力机制分析。

Result: 发现BERT中存在层次化表征结构：构式特异性信息在早期层出现，中间层聚类最可分，后期层维持该信息。

Conclusion: BERT模型以层次化方式编码论元结构信息，支持其在语言理解中对构式差异的敏感性。

Abstract: This study investigates how the Bidirectional Encoder Representations from Transformers model processes four fundamental Argument Structure Constructions. We employ a multi-dimensional analytical framework, which integrates MDS, t-SNE as dimensionality reduction, Generalized Discrimination Value (GDV) as cluster separation metrics, Fisher Discriminant Ratio (FDR) as linear diagnostic probing, and attention mechanism analysis. Our results reveal a hierarchical representational structure. Construction-specific information emerges in early layers, forms maximally separable clusters in middle layers, and is maintained through later processing stages.

</details>


### [302] [The French Drama Revolution: Political Economy and Literary Production, 1700-1900](https://arxiv.org/abs/2602.00588)
*Thiago Dumont Oliveira*

Main category: cs.CL

TL;DR: 本文利用LDA和JS散度分析1700–1900年法国戏剧主题演变，发现法国大革命后（尤其1789–1850）主题发生深刻变化，资产阶级主题自18世纪末起成为主流；进一步将主题年度分布与法国GDP变化对比，结合政治经济变革（大革命、工业化）进行阐释。


<details>
  <summary>Details</summary>
Motivation: 探究1700–1900年间法国戏剧主题演变及其与政治经济变迁（特别是法国大革命和工业化）的关系。

Method: 采用潜在狄利克雷分配（LDA）建模戏剧文本主题，用Jensen-Shannon散度衡量主题分布变化，并将主题年度流行度与法国GDP时间序列进行对比分析。

Result: 法国戏剧主题在1789年后发生显著转变，资产阶级主题自18世纪末起成为最突出主题之一；主题演变趋势与法国GDP增长及重大历史事件（大革命、工业化）存在时序关联。

Conclusion: 法国戏剧的主题演化并非孤立文化现象，而是深度嵌入并反映宏观政治经济结构转型，印证了文学与社会发展的协同演进关系。

Abstract: This paper investigates the changing nature of French drama between 1700-1900 using Latent Dirichlet Allocation and Jensen-Shannon Divergence. Results indicate that the topical distribution of French drama changed profoundly after the French Revolution, particularly between 1789 and 1850. Bourgeois themes emerged among the most prevalent topics since the late 18th century. To assess the coevolution of drama and economic growth, I plot the yearly prevalence of topics alongside French GDP between 1700-1900, and discuss these changes in light of the political and economic changes prompted by the French Revolution and the industrialization of the country.

</details>


### [303] [Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling](https://arxiv.org/abs/2602.00594)
*Zhijie Huang,Stephen McIntosh,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.CL

TL;DR: 本文提出了Kanade，一种单层解耦语音分词器，旨在高效提取语音中的音系和韵律信息，同时抑制与语言无关的特征（如说话人身份），并在无需辅助方法的情况下实现高质量语音重建与解耦性能。


<details>
  <summary>Details</summary>
Motivation: 语音建模需要处理连续信号，其中混杂语言与非语言信息；理想的语音分词器应能提取音系与韵律、抑制说话人等无关信息，并支持高质量合成。

Method: 提出单层解耦语音分词器Kanade，通过分离声学常量生成单一token流，以表征丰富的音系与韵律，不依赖现有解耦编解码器常用的辅助方法。

Result: 实验表明Kanade在说话人解耦和词汇可用性上达到SOTA，同时保持优异的重建质量。

Conclusion: Kanade验证了简洁单层结构可有效实现语音表征的解耦与高保真重建，为语音语言模型提供了更优的tokenizer设计范式。

Abstract: A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.

</details>


### [304] [Hermes the Polyglot: A Unified Framework to Enhance Expressiveness for Multimodal Interlingual Subtitling](https://arxiv.org/abs/2602.00597)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的自动字幕翻译框架Hermes，通过说话人区分、术语识别和表达力增强三个模块，解决字幕翻译中语义连贯性、代词与术语翻译及表达力等挑战，实验表明其在说话人区分和翻译质量上达到当前最优水平。


<details>
  <summary>Details</summary>
Motivation: 字幕文本具有独特特性（如语义连贯性、代词与术语翻译、表达力要求），现有机器翻译方法（包括大语言模型）尚未有效应对，而跨语言字幕翻译在娱乐本地化中至关重要但缺乏研究。

Method: 提出Hermes框架，集成三个模块：说话人区分（Speaker Diarization）、术语识别（Terminology Identification）和表达力增强（Expressiveness Enhancement），基于大语言模型实现端到端字幕翻译优化。

Result: Hermes在说话人区分任务上达到当前最优性能，并生成更具表达力、上下文连贯的翻译结果。

Conclusion: Hermes有效提升了跨语言字幕翻译的质量与实用性，推动了该领域研究进展。

Abstract: Interlingual subtitling, which translates subtitles of visual media into a target language, is essential for entertainment localization but has not yet been explored in machine translation. Although Large Language Models (LLMs) have significantly advanced the general capabilities of machine translation, the distinctive characteristics of subtitle texts pose persistent challenges in interlingual subtitling, particularly regarding semantic coherence, pronoun and terminology translation, and translation expressiveness. To address these issues, we present Hermes, an LLM-based automated subtitling framework. Hermes integrates three modules: Speaker Diarization, Terminology Identification, and Expressiveness Enhancement, which effectively tackle the above challenges. Experiments demonstrate that Hermes achieves state-of-the-art diarization performance and generates expressive, contextually coherent translations, thereby advancing research in interlingual subtitling.

</details>


### [305] [Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars](https://arxiv.org/abs/2602.00612)
*Yitong Zhang,Yongmin Li,Yuetong Liu,Jia Li,Xiaoran Jia,Zherui Li,Ge Li*

Main category: cs.CL

TL;DR: 本文提出LAVE，一种专为扩散大语言模型（dLLMs）设计的约束解码方法，利用其并行预测token分布的特性进行前向验证，确保中间输出始终可扩展为语法正确的句子，在保持低开销的同时显著提升生成的语法正确性。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）在生成形式语言时难以可靠保证语法正确性；现有约束解码方法因非自回归特性或允许不可完成的中间状态而难以适用。

Method: 提出LAVE方法，利用dLLMs并行预测各位置token分布的特性，在每次提议新token时进行前向验证，确保该token不会破坏后续生成合法句子的可能性。

Result: 在四个主流dLLMs和三个代表性基准上实验表明，LAVE持续优于现有基线，显著提升语法正确性，且运行开销可忽略。

Conclusion: LAVE是一种高效、可靠且通用的约束解码方案，有效解决了dLLMs语法生成不可靠的核心问题，为形式语言生成提供了实用保障。

Abstract: Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.
  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.

</details>


### [306] [Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting](https://arxiv.org/abs/2602.00758)
*Ali El Lahib,Ying-Jieh Xia,Zehan Li,Yuxuan Wang,Xinyu Pi*

Main category: cs.CL

TL;DR: 本文揭示了搜索引擎日期过滤器在回溯性评估搜索增强型预测器时的不可靠性，指出其普遍存在时间泄漏问题，并建议采用更强的检索保护措施或基于冻结的时间戳网页快照进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有研究广泛使用搜索引擎日期过滤器（如 before:）进行回溯性评估，但其是否真能保证时间一致性缺乏验证。

Method: 通过审计 Google Search 的 before: 过滤器，统计时间泄漏比例；利用 gpt-oss-120b 模型在含泄漏与无泄漏文档上的预测表现（Brier 分数）对比；分析常见泄漏机制。

Result: 71% 的问题返回至少一页含强时间泄漏内容，41% 直接泄露答案；使用泄漏文档导致 Brier 分数显著虚低（0.108 vs. 0.242）。

Conclusion: 日期限制搜索不足以支撑可信的时间敏感评估，应改用冻结网页快照或更严格的检索防护机制。

Abstract: Search-engine date filters are widely used to enforce pre-cutoff retrieval in retrospective evaluations of search-augmented forecasters. We show this approach is unreliable: auditing Google Search with a before: filter, 71% of questions return at least one page containing strong post-cutoff leakage, and for 41%, at least one page directly reveals the answer. Using a large language model (LLM), gpt-oss-120b, to forecast with these leaky documents, we demonstrate an inflated prediction accuracy (Brier score 0.108 vs. 0.242 with leak-free documents). We characterize common leakage mechanisms, including updated articles, related-content modules, unreliable metadata/timestamps, and absence-based signals, and argue that date-restricted search is insufficient for temporal evaluation. We recommend stronger retrieval safeguards or evaluation on frozen, time-stamped web snapshots to ensure credible retrospective forecasting.

</details>


### [307] [Transformer-Based Model for Multilingual Hope Speech Detection](https://arxiv.org/abs/2602.00613)
*Nsrin Ashraf,Mariam Labib,Hamada Nayel*

Main category: cs.CL

TL;DR: 本文提出了一个用于英语和德语希望言语检测的系统，分别使用RoBERTa和XLM-RoBERTa模型，在RANLP2025的PolyHope-M任务中提交。RoBERTa在英语上达到加权F1为0.818，XLM-RoBERTa在双语上分别取得0.786（英语）和0.785（德语）的加权F1。


<details>
  <summary>Details</summary>
Motivation: 提升希望言语检测性能，探索预训练大语言模型（尤其是多语言模型）在跨语言NLP任务中的有效性。

Method: 采用RoBERTa模型处理英语数据，XLM-RoBERTa模型同时处理英语与德语数据，并在PolyHope-M数据集上进行训练与评估。

Result: RoBERTa在英语上获得加权F1为0.818、准确率81.8%；XLM-RoBERTa在英语上加权F1为0.786、准确率78.5%，在德语上准确率为78.5%（F1未明确给出但上下文暗示相近）。

Conclusion: RoBERTa在单语任务中略优于XLM-RoBERTa，验证了专用单语模型的优势；XLM-RoBERTa展现良好跨语言迁移能力，表明预训练模型持续优化对NLP任务性能提升至关重要。

Abstract: This paper describes a system that has been submitted to the "PolyHope-M" at RANLP2025. In this work various transformers have been implemented and evaluated for hope speech detection for English and Germany. RoBERTa has been implemented for English, while the multilingual model XLM-RoBERTa has been implemented for both English and German languages. The proposed system using RoBERTa reported a weighted f1-score of 0.818 and an accuracy of 81.8% for English. On the other hand, XLM-RoBERTa achieved a weighted f1-score of 0.786 and an accuracy of 78.5%. These results reflects the importance of improvement of pre-trained large language models and how these models enhancing the performance of different natural language processing tasks.

</details>


### [308] [Unifying Adversarial Robustness and Training Across Text Scoring Models](https://arxiv.org/abs/2602.00857)
*Manveer Singh Tamber,Hosna Oyarhoseini,Jimmy Lin*

Main category: cs.CL

TL;DR: 本文提出统一研究文本评分模型（如密集检索器、重排序器和奖励模型）的对抗鲁棒性，通过适配攻击与对抗训练方法，揭示当前方法泛化能力不足，并提出多种互补的对抗训练策略，在提升鲁棒性的同时增强任务性能，尤其在RLHF中有效缓解奖励黑客行为并提升大语言模型对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型对抗鲁棒性研究分散于不同应用场景和攻击类型，难以识别共性脆弱点；而文本评分模型的失败可被直接量化评估，适合构建统一鲁棒性研究框架。

Method: 提出面向文本评分模型的多种对抗训练方法，强调跨模型角色（检索、重排、奖励建模）适配攻击与训练策略，并结合互补方法进行联合训练。

Result: 所提对抗训练方法显著提升模型对多种攻击的鲁棒性，同时改善任务性能；在RLHF中，对抗训练的奖励模型能有效抑制奖励黑客行为，并支持训练出更对齐的大语言模型。

Conclusion: 统一文本评分模型的对抗鲁棒性研究是可行且必要路径；兼顾鲁棒性与有效性需融合互补的对抗训练策略，该范式对安全、可信的LLM部署具有重要实践价值。

Abstract: Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.

</details>


### [309] [Jailbreaking LLMs via Calibration](https://arxiv.org/abs/2602.00619)
*Yuxuan Lu,Yongkang Guo,Yuqing Kong*

Main category: cs.CL

TL;DR: 本文提出了一种将安全对齐建模为预对齐分布系统性扭曲的框架，将弱到强越狱问题视为预测聚合问题，并推导出基于损失对偶空间梯度偏移的最优聚合策略；该框架统一解释了logit-arithmetic越狱方法，并扩展出适用于多种合理损失函数的新聚合规则，还提出一种新混合聚合规则，在多个基准测试中展现出更高攻击成功率和更低‘越狱税’。


<details>
  <summary>Details</summary>
Motivation: 安全对齐常导致模型输出与原始预对齐数据分布之间产生系统性偏差，需建模该偏差以提升越狱方法的理论基础与实用性。

Method: 将安全对齐建模为对预对齐分布的系统性扭曲；将弱到强越狱形式化为预测聚合问题；在损失诱导的对偶空间中推导梯度偏移最优聚合策略；推广logit-arithmetic至多种proper loss下的聚合规则，并设计新混合规则。

Result: 在红队测试和数学任务上，所提方法在前沿模型（尤其是安全加固的gpt-oss-120b）上显著提升攻击成功率，同时降低‘越狱税’。

Conclusion: 安全对齐可被统一建模为分布扭曲，越狱本质上是带结构约束的预测聚合问题；所提框架不仅解释现有方法，还提供更鲁棒、更通用的越狱策略。

Abstract: Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of a pre-alignment distribution. We cast Weak-to-Strong Jailbreaking as a forecast aggregation problem and derive an optimal aggregation strategy characterized by a Gradient Shift in the loss-induced dual space. We show that logit-arithmetic jailbreaking methods are a special case of this framework under cross-entropy loss, and derive a broader family of aggregation rules corresponding to other proper losses. We also propose a new hybrid aggregation rule. Evaluations across red-teaming benchmarks and math utility tasks using frontier models demonstrate that our approach achieves superior Attack Success Rates and lower "Jailbreak Tax" compared with existing methods, especially on the safety-hardened gpt-oss-120b.

</details>


### [310] [Formal Semantic Control over Language Models](https://arxiv.org/abs/2602.00638)
*Yingji Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种在VAE框架下提升语言表征语义与几何可解释性及可控性的方法，通过句子级和推理级两个方向实现对潜在空间的解耦、操纵与引导。


<details>
  <summary>Details</summary>
Motivation: 提升语言模型内部语义表征的系统可解释性、精确结构化与可靠可控性。

Method: 基于变分自编码器（VAE）框架，分别探索句子级语义特征解耦与生成控制、以及推理级（特别是解释型自然语言推理）中推理行为的隔离与引导。

Result: 提出一系列新理论框架与实用方法，并通过实验验证其能有效增强自然语言潜在空间的可解释性与可控性。

Conclusion: 该工作推动了语义表征学习的发展，使语言模型具备更局部化、类符号化、可组合的控制能力，并朝向可解释、结构化、可导向的语言模型迈进。

Abstract: This thesis advances semantic representation learning to render language representations or models more semantically and geometrically interpretable, and to enable localised, quasi-symbolic, compositional control through deliberate shaping of their latent space geometry. We pursue this goal within a VAE framework, exploring two complementary research directions: (i) Sentence-level learning and control: disentangling and manipulating specific semantic features in the latent space to guide sentence generation, with explanatory text serving as the testbed; and (ii) Reasoning-level learning and control: isolating and steering inference behaviours in the latent space to control NLI. In this direction, we focus on Explanatory NLI tasks, in which two premises (explanations) are provided to infer a conclusion. The overarching objective is to move toward language models whose internal semantic representations can be systematically interpreted, precisely structured, and reliably directed. We introduce a set of novel theoretical frameworks and practical methodologies, together with corresponding experiments, to demonstrate that our approaches enhance both the interpretability and controllability of latent spaces for natural language across the thesis.

</details>


### [311] [Inferential Question Answering](https://arxiv.org/abs/2602.01239)
*Jamshid Mozafari,Hamed Zamani,Guido Zuccon,Adam Jatowt*

Main category: cs.CL

TL;DR: 本文提出了推理性问答（Inferential QA）这一新任务，强调从仅提供线索的文本中进行推理以得出答案，并构建了QUIT数据集；实验表明现有QA方法（包括检索器、重排序器和大语言模型）在此任务上表现不佳，揭示当前QA系统尚不具备可靠的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统大多假设答案可直接从文档中抽取或生成，但许多问题需要基于文本线索进行隐含推理，而这一能力尚未被系统研究和评估。

Method: 提出Inferential QA任务定义，构建包含7401个问题和240万段落的QUIT数据集，标注采用LLM判断+人工验证的三级相关性标签；对检索器、重排序器和LLM阅读器进行系统评测。

Result: 传统QA方法在推理性问答上普遍表现较差：检索器性能下降、重排序提升有限、微调效果不稳定；甚至专为推理设计的大模型也未超越小型通用模型。

Conclusion: 当前问答系统缺乏处理间接证据推理的能力，Inferential QA为推动模型理解与推理能力提供了新的基准任务。

Abstract: Despite extensive research on a wide range of question answering (QA) systems, most existing work focuses on answer containment-i.e., assuming that answers can be directly extracted and/or generated from documents in the corpus. However, some questions require inference, i.e., deriving answers that are not explicitly stated but can be inferred from the available information. We introduce Inferential QA -- a new task that challenges models to infer answers from answer-supporting passages which provide only clues. To study this problem, we construct QUIT (QUestions requiring Inference from Texts) dataset, comprising 7,401 questions and 2.4M passages built from high-convergence human- and machine-authored hints, labeled across three relevance levels using LLM-based answerability and human verification. Through comprehensive evaluation of retrievers, rerankers, and LLM-based readers, we show that methods effective on traditional QA tasks struggle in inferential QA: retrievers underperform, rerankers offer limited gains, and fine-tuning provides inconsistent improvements. Even reasoning-oriented LLMs fail to outperform smaller general-purpose models. These findings reveal that current QA pipelines are not yet ready for inference-based reasoning. Inferential QA thus establishes a new class of QA tasks that move towards understanding and reasoning from indirect textual evidence.

</details>


### [312] [LegalOne: A Family of Foundation Models for Reliable Legal Reasoning](https://arxiv.org/abs/2602.00642)
*Haitao Li,Yifan Chen,Shuo Miao,Qian Dong,Jia Chen,Yiran Hu,Junjie Chen,Minghao Qin,Qingyao Ai,Yiqun Liu,Cheng Luo,Quan Zhou,Ya Zhang,Jikun Hu*

Main category: cs.CL

TL;DR: LegalOne 是针对中文法律领域的基础模型家族，通过三阶段训练流程（Plasticity-Adjusted Sampling、Legal Agentic CoT Distillation 和 Curriculum RL）提升法律知识密度与多步司法推理能力，在多项法律任务上超越更大参数量的通用大模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法律领域直接应用受限于领域知识不足和多步司法推理复杂性。

Method: 提出三阶段训练流程：1）中训阶段采用基于困惑度的 Plasticity-Adjusted Sampling（PAS）实现法律领域适配；2）监督微调阶段使用 Legal Agentic CoT Distillation（LEAD）将原始法律文本蒸馏为结构化、事实 grounded 的推理轨迹；3）课程强化学习（Curriculum RL）分阶段提升从记忆、理解到自主推理的能力。

Result: LegalOne 在广泛法律任务上达到 SOTA 性能，超越参数量大得多的通用大模型；并开源模型权重与 LegalKit 评测框架。

Conclusion: LegalOne 通过高知识密度与高效训练范式，为高风险司法场景提供了可信、可解释的法律基础模型新路径。

Abstract: While Large Language Models (LLMs) have demonstrated impressive general capabilities, their direct application in the legal domain is often hindered by a lack of precise domain knowledge and complexity of performing rigorous multi-step judicial reasoning. To address this gap, we present LegalOne, a family of foundational models specifically tailored for the Chinese legal domain. LegalOne is developed through a comprehensive three-phase pipeline designed to master legal reasoning. First, during mid-training phase, we propose Plasticity-Adjusted Sampling (PAS) to address the challenge of domain adaptation. This perplexity-based scheduler strikes a balance between the acquisition of new knowledge and the retention of original capabilities, effectively establishing a robust legal foundation. Second, during supervised fine-tuning, we employ Legal Agentic CoT Distillation (LEAD) to distill explicit reasoning from raw legal texts. Unlike naive distillation, LEAD utilizes an agentic workflow to convert complex judicial processes into structured reasoning trajectories, thereby enforcing factual grounding and logical rigor. Finally, we implement a Curriculum Reinforcement Learning (RL) strategy. Through a progressive reinforcement process spanning memorization, understanding, and reasoning, LegalOne evolves from simple pattern matching to autonomous and reliable legal reasoning. Experimental results demonstrate that LegalOne achieves state-of-the-art performance across a wide range of legal tasks, surpassing general-purpose LLMs with vastly larger parameter counts through enhanced knowledge density and efficiency. We publicly release the LegalOne weights and the LegalKit evaluation framework to advance the field of Legal AI, paving the way for deploying trustworthy and interpretable foundation models in high-stakes judicial applications.

</details>


### [313] [PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian](https://arxiv.org/abs/2602.01246)
*Jamshid Mozafari,Seyed Parsa Mousavinasab,Adam Jatowt*

Main category: cs.CL

TL;DR: 本文介绍了PARSE，首个面向波斯语的开放域推理型问答基准，包含10800个涵盖多种题型与推理类型的问题，并通过LLM生成与人工验证构建；实验表明波斯语提示与结构化提示（如思维链、少样本）及微调可显著提升模型性能，尤其对波斯语专用模型。


<details>
  <summary>Details</summary>
Motivation: 波斯语作为低资源语言，缺乏高质量、开放域、支持推理能力评估的问答基准，制约了其推理型QA系统的发展与公平比较。

Method: 构建基于可控LLM生成的管道，生成10,800道波斯语问题（布尔型、多选、事实型），辅以多阶段过滤、人工标注、一致性检查和人类评估确保语言与事实质量；并在多种提示策略与微调设置下对多语言及波斯语LLMs进行基准测试。

Result: 波斯语提示和结构化提示（Boolean/multiple-choice用CoT，factoid用few-shot）有效提升性能；微调进一步增强效果，尤其对波斯语专用模型；PARSE支持公平比较与实用适配。

Conclusion: PARSE填补了波斯语推理型QA基准的空白，为低资源语言上推理型大模型的开发与评估提供了坚实基础。

Abstract: Reasoning-focused Question Answering (QA) has advanced rapidly with Large Language Models (LLMs), yet high-quality benchmarks for low-resource languages remain scarce. Persian, spoken by roughly 130 million people, lacks a comprehensive open-domain resource for evaluating reasoning-capable QA systems. We introduce PARSE, the first open-domain Persian reasoning QA benchmark, containing 10,800 questions across Boolean, multiple-choice, and factoid formats, with diverse reasoning types, difficulty levels, and answer structures. The benchmark is built via a controlled LLM-based generation pipeline and validated through human evaluation. We also ensure linguistic and factual quality through multi-stage filtering, annotation, and consistency checks. We benchmark multilingual and Persian LLMs under multiple prompting strategies and show that Persian prompts and structured prompting (CoT for Boolean/multiple-choice; few-shot for factoid) improve performance. Fine-tuning further boosts results, especially for Persian-specialized models. These findings highlight how PARSE supports both fair comparison and practical model adaptation. PARSE fills a critical gap in Persian QA research and provides a strong foundation for developing and evaluating reasoning-capable LLMs in low-resource settings.

</details>


### [314] [Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation](https://arxiv.org/abs/2602.00665)
*Lakshan Cooray,Deshan Sumanathilaka,Pattigadapa Venkatesh Raju*

Main category: cs.CL

TL;DR: 本研究探讨了指令微调的小型语言模型（SLMs）在多轮客服问答任务中的有效性，提出基于历史摘要和对话阶段的评估方法，发现部分SLMs性能接近大模型（LLMs），但整体在对话连续性和上下文对齐上仍存在局限。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽性能强，但计算开销大、部署受限；小型语言模型（SLMs）更高效，但在需对话连续性和上下文理解的多轮客服QA中效果尚不明确。

Method: 采用历史摘要策略保留对话状态，构建指令微调的低参数SLMs，并引入基于对话阶段的定性分析框架；通过词法与语义相似度指标、人工评估及LLM-as-a-judge方法，对比9个SLMs与3个商用LLMs。

Result: 不同SLMs表现差异显著：部分接近LLM性能，部分在对话连续性和上下文对齐方面明显不足。

Conclusion: 低参数语言模型在真实客服QA中具备应用潜力，但当前在复杂对话建模上仍有明显局限，需进一步优化上下文建模与指令微调策略。

Abstract: Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Language Models (SLMs) provide a more efficient alternative, yet their effectiveness for multi-turn customer-service QA remains underexplored, particularly in scenarios requiring dialogue continuity and contextual understanding. This study investigates instruction-tuned SLMs for context-summarized multi-turn customer-service QA, using a history summarization strategy to preserve essential conversational state. We also introduce a conversation stage-based qualitative analysis to evaluate model behavior across different phases of customer-service interactions. Nine instruction-tuned low-parameterized SLMs are evaluated against three commercial LLMs using lexical and semantic similarity metrics alongside qualitative assessments, including human evaluation and LLM-as-a-judge methods. Results show notable variation across SLMs, with some models demonstrating near-LLM performance, while others struggle to maintain dialogue continuity and contextual alignment. These findings highlight both the potential and current limitations of low-parameterized language models for real-world customer-service QA systems.

</details>


### [315] [LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than Hidden States](https://arxiv.org/abs/2602.01572)
*Yeqin Zhang,Yunfei Wang,Jiaxuan Chen,Ke Qin,Yizheng Zhao,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM注意力值向量的无训练句子表示方法Value Aggregation（VA）及其改进版AlignedWVA，通过聚合多层注意力值并引入对齐加权机制，在无需训练的情况下显著超越现有LLM嵌入方法，达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM句子表示多依赖最终隐藏层状态，但该状态为下一词预测优化，难以有效捕获全局句子语义。

Method: 提出Value Aggregation（VA），聚合多层、多位置的注意力值向量；进一步设计AlignedWVA，利用末尾token的注意力分数作为权重，结合输出投影矩阵WO实现加权值向量对齐。

Result: VA在无训练设置下超越其他LLM嵌入方法，媲美或超过MetaEOL；AlignedWVA大幅超越高成本MetaEOL，达训练自由LLM嵌入SOTA；并指出VA具备可微调潜力。

Conclusion: 注意力值向量比隐藏状态更能表征句子语义；VA及AlignedWVA为高效、高性能的无训练句子嵌入提供了新范式。

Abstract: Sentence representations are foundational to many Natural Language Processing (NLP) applications. While recent methods leverage Large Language Models (LLMs) to derive sentence representations, most rely on final-layer hidden states, which are optimized for next-token prediction and thus often fail to capture global, sentence-level semantics. This paper introduces a novel perspective, demonstrating that attention value vectors capture sentence semantics more effectively than hidden states. We propose Value Aggregation (VA), a simple method that pools token values across multiple layers and token indices. In a training-free setting, VA outperforms other LLM-based embeddings, even matches or surpasses the ensemble-based MetaEOL. Furthermore, we demonstrate that when paired with suitable prompts, the layer attention outputs can be interpreted as aligned weighted value vectors. Specifically, the attention scores of the last token function as the weights, while the output projection matrix ($W_O$) aligns these weighted value vectors with the common space of the LLM residual stream. This refined method, termed Aligned Weighted VA (AlignedWVA), achieves state-of-the-art performance among training-free LLM-based embeddings, outperforming the high-cost MetaEOL by a substantial margin. Finally, we highlight the potential of obtaining strong LLM embedding models through fine-tuning Value Aggregation.

</details>


### [316] [EchoReview: Learning Peer Review from the Echoes of Scientific Citations](https://arxiv.org/abs/2602.00733)
*Yinuo Zhang,Dingcheng Huang,Haifeng Suo,Yizhuo Li,Ziya Zhao,Junhao Xu,Zhiying Tu,Dianhui Chu,Deming Zhai,Xianming Liu,Xiaoyan Yu,Dianbo Sui*

Main category: cs.CL

TL;DR: 本文提出EchoReview框架，通过挖掘学术引用中的隐含评价信号来生成结构化评审数据，并构建了首个大规模跨会议、跨年度的引用驱动评审数据集EchoReview-16K，训练出自动评审模型EchoReviewer-7B，在证据支持和评审全面性等核心维度上显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统同行评审系统面临可扩展性压力，现有基于真实评审数据的监督微调方法受限于单一数据源及人类评审的主观性和不一致性。

Method: 提出基于引用上下文的数据合成框架EchoReview，从学术引用中系统挖掘隐含的集体评价信号，并将其转化为结构化评审数据；构建EchoReview-16K数据集，并训练EchoReviewer-7B模型。

Result: EchoReviewer-7B在证据支持、评审全面性等核心评审维度上实现显著且稳定的提升，验证了引用上下文作为自动化评审可靠数据范式的有效性。

Conclusion: 引用上下文是一种稳健有效的数据范式，可支撑高质量、可扩展的自动化同行评审系统。

Abstract: As the volume of scientific submissions continues to grow rapidly, traditional peer review systems are facing unprecedented scalability pressures, highlighting the urgent need for automated reviewing methods that are both scalable and reliable. Existing supervised fine-tuning approaches based on real review data are fundamentally constrained by single-source of data as well as the inherent subjectivity and inconsistency of human reviews, limiting their ability to support high-quality automated reviewers. To address these issues, we propose EchoReview, a citation-context-driven data synthesis framework that systematically mines implicit collective evaluative signals from academic citations and transforms scientific community's long-term judgments into structured review-style data. Based on this pipeline, we construct EchoReview-16K, the first large-scale, cross-conference, and cross-year citation-driven review dataset, and train an automated reviewer, EchoReviewer-7B. Experimental results demonstrate that EchoReviewer-7B can achieve significant and stable improvements on core review dimensions such as evidence support and review comprehensiveness, validating citation context as a robust and effective data paradigm for reliable automated peer review.

</details>


### [317] [Orthogonal Hierarchical Decomposition for Structure-Aware Table Understanding with Large Language Models](https://arxiv.org/abs/2602.01969)
*Bin Cao,Huixian Lu,Chenwen Ma,Ting Wang,Ruizhe Li,Jing Fan*

Main category: cs.CL

TL;DR: 本文提出了一种正交分层分解（OHD）框架，用于提升大语言模型对复杂表格（如多级表头、合并单元格等）的理解与推理能力。该框架通过正交树归纳（OTI）方法构建列树和行树以分别捕获垂直与水平方向的层级依赖，并设计双路径关联协议与语义仲裁大模型来对齐多级语义信息。在AITQA和HiTab两个基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有表格线性化或归一化网格建模方法难以显式建模复杂表格的层次结构和跨维依赖，导致结构语义与文本表示错位。

Method: 提出正交分层分解（OHD）框架，包括基于空间-语义协同约束的正交树归纳（OTI）方法，构建列树和行树；设计双路径关联协议重建每个单元格的语义谱系；引入LLM作为语义仲裁器对齐多级语义信息。

Result: 在AITQA和HiTab两个复杂表格问答基准上，OHD在多个评估指标上持续优于现有表示范式。

Conclusion: OHD通过结构保持的输入表示显著提升了LLM对复杂表格的理解与推理能力，为处理非标准表格提供了新范式。

Abstract: Complex tables with multi-level headers, merged cells and heterogeneous layouts pose persistent challenges for LLMs in both understanding and reasoning. Existing approaches typically rely on table linearization or normalized grid modeling. However, these representations struggle to explicitly capture hierarchical structures and cross-dimensional dependencies, which can lead to misalignment between structural semantics and textual representations for non-standard tables. To address this issue, we propose an Orthogonal Hierarchical Decomposition (OHD) framework that constructs structure-preserving input representations of complex tables for LLMs. OHD introduces an Orthogonal Tree Induction (OTI) method based on spatial--semantic co-constraints, which decomposes irregular tables into a column tree and a row tree to capture vertical and horizontal hierarchical dependencies, respectively. Building on this representation, we design a dual-pathway association protocol to symmetrically reconstruct semantic lineage of each cell, and incorporate an LLM as a semantic arbitrator to align multi-level semantic information. We evaluate OHD framework on two complex table question answering benchmarks, AITQA and HiTab. Experimental results show that OHD consistently outperforms existing representation paradigms across multiple evaluation metrics.

</details>


### [318] [ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement](https://arxiv.org/abs/2602.00740)
*Ziyan Xiao,Yinghao Zhu,Liang Peng,Lequan Yu*

Main category: cs.CL

TL;DR: 本文提出ExperienceWeaver框架，通过提炼多维反馈为结构化经验（错误提示与高层策略），提升LLM在小样本下的临床文本修正能力，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 临床文本改进对医疗效率至关重要，但受限于高质量数据稀缺和医学文档的复杂约束；现有LLM方法在小样本场景下表现不佳：监督微调数据成本高，检索增强生成缺乏深层修订推理。

Method: 提出ExperienceWeaver——一种分层经验学习框架，将嘈杂、多维反馈提炼为错误特定的Tips和高层Strategies，并将其注入代理式推理流程，使模型学会‘如何修订’而非仅‘修订什么’。

Result: 在四个临床数据集上的实验表明，ExperienceWeaver在小样本设置下持续超越当前最优模型（如Gemini-3 Pro）。

Conclusion: ExperienceWeaver通过结构化经验蒸馏与代理式应用，有效缓解小样本临床文本改进的数据与推理瓶颈，为医疗NLP提供新范式。

Abstract: Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns "how to revise" rather than just "what to revise". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.

</details>


### [319] [CURP: Codebook-based Continuous User Representation for Personalized Generation with LLMs](https://arxiv.org/abs/2602.00742)
*Liang Wang,Xinyi Mou,Xiaoyou Liu,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CL

TL;DR: 本文提出CURP框架，通过双向用户编码器和离散原型码本提取多维用户特征，实现高效、可插拔的个性化，仅需约20M可训练参数（占模型总参数0.2%），在生成任务中性能、泛化性、可解释性和可扩展性均优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示或训练的用户建模方法难以兼顾个性化质量与计算/数据效率。

Method: 提出CURP框架，采用双向用户编码器和离散原型码本提取多维用户特征，支持插拔式个性化，参数量仅约20M（约0.2%总模型参数）。

Result: 在多种生成任务上，CURP性能与泛化能力优于强基线，同时提升可解释性与可扩展性。

Conclusion: CURP是一种高效、轻量、可解释且可扩展的用户建模新范式，为LLM个性化提供新思路。

Abstract: User modeling characterizes individuals through their preferences and behavioral patterns to enable personalized simulation and generation with Large Language Models (LLMs) in contemporary approaches. However, existing methods, whether prompt-based or training-based methods, face challenges in balancing personalization quality against computational and data efficiency. We propose a novel framework CURP, which employs a bidirectional user encoder and a discrete prototype codebook to extract multi-dimensional user traits. This design enables plug-and-play personalization with a small number of trainable parameters (about 20M parameters, about 0.2\% of the total model size). Through extensive experiments on variant generation tasks, we show that CURP achieves superior performance and generalization compared to strong baselines, while offering better interpretability and scalability. The code are available at https://github.com/RaidonWong/CURP_code

</details>


### [320] [Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study](https://arxiv.org/abs/2602.02208)
*Md. Toufique Hasan,Ayman Asad Khan,Mika Saari,Vaishnavi Bankhele,Pekka Abrahamsson*

Main category: cs.CL

TL;DR: 本文提出了AgriHubi，一个面向芬兰语农业决策支持的领域适配型检索增强生成（RAG）系统，通过融合芬兰农业文档与开源PORO语言模型，并结合显式来源标注和用户反馈机制，在低资源语言场景下显著提升了回答完整性、语言准确性和可信度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在农业等知识密集型领域应用受限，尤其在低资源语言（如芬兰语）中，存在语义接地弱、训练数据以英语为主、缺乏真实世界评估等问题，而高质量的本地化农业文档难以被通用模型有效利用。

Method: 构建了AgriHubi系统：整合芬兰语农业文档，适配开源PORO系列模型，采用检索增强生成（RAG）架构，引入显式源 grounding 和用户反馈驱动的迭代优化机制；经八轮开发迭代，并通过两项用户研究进行实证评估。

Result: 系统在答案完整性、语言准确性及用户感知可靠性方面均有明显提升；同时揭示了大模型部署中响应质量与延迟之间的实际权衡；为低资源语言下的领域专用RAG系统设计与评估提供了实证依据。

Conclusion: AgriHubi验证了在低资源语言农业场景中，结合领域文档、轻量适配模型与用户闭环反馈的RAG路径是可行且有效的，其方法论可推广至其他类似领域与语言环境。

Abstract: Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.

</details>


### [321] [Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training](https://arxiv.org/abs/2602.00747)
*Shengrui Li,Fei Zhao,Kaiyan Zhao,Jieying Ye,Haifeng Liu,Fangcheng Shi,Zheyong Xie,Yao Hu,Shaosheng Cao*

Main category: cs.CL

TL;DR: 本文提出DeMix框架，通过模型合并预测最优数据混合比例，解耦搜索与训练成本，在降低搜索开销的同时提升大语言模型预训练数据混合的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在寻找LLM预训练最优数据混合时，要么依赖不可靠的小规模代理实验，要么需要昂贵的大规模探索，难以兼顾充分性、准确性和效率。

Method: 提出DeMix框架：先在候选数据集上大规模训练多个组件模型，再通过加权模型合并生成不同数据混合比例的代理模型，从而无需为每个混合比例重新训练即可高效搜索最优混合。

Result: DeMix在多个基准上取得更高性能，同时显著降低搜索成本；并开源22T-token的DeMix Corpora数据集及代码。

Conclusion: DeMix成功打破数据混合搜索中充分性、准确性和效率之间的权衡，为LLM预训练提供高效、可扩展的数据混合优化新范式。

Abstract: Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.

</details>


### [322] [Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics](https://arxiv.org/abs/2602.02343)
*Ziwen Xu,Chenyan Wu,Hengyu Sun,Haiwen Hong,Mengru Wang,Yunzhi Yao,Longtao Huang,Hui Xue,Shumin Deng,Zhixuan Chu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种统一框架，将LLM控制方法（如微调、LoRA、激活干预）视为由控制信号引发的动态权重更新，并引入偏好-效用分析来量化控制效果；发现偏好与效用存在权衡关系，并基于表征流形解释该现象，进而提出新方法SPLIT以更好平衡二者。


<details>
  <summary>Details</summary>
Motivation: 现有LLM控制方法（如微调、LoRA、激活干预）常被孤立研究，缺乏统一视角，导致难以比较和理解其内在联系与本质差异。

Method: 提出统一的动态权重更新框架；构建基于极性配对对比样本的偏好-效用联合分析方法，二者均在log-odds尺度上量化；从激活流形角度建模控制对表征的影响；据此设计新型控制方法SPLIT。

Result: 验证了各类控制方法中普遍存在的偏好-效用权衡；揭示该权衡源于控制使表征沿目标概念方向偏移（提升偏好），但过度偏移会使其脱离有效生成流形（损害效用）；SPLIT在增强偏好同时更优地保持效用。

Conclusion: LLM控制方法可被统一建模为控制信号驱动的动态权重更新；偏好与效用是两个正交且可量化的控制维度，其权衡具有几何解释；基于该认知设计的SPLIT方法实现了更优控制平衡。

Abstract: Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.

</details>


### [323] [Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning](https://arxiv.org/abs/2602.00759)
*Zhipeng Chen,Xiaobo Qin,Wayne Xin Zhao,Youbin Wu,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 本文提出A²D方法，通过自适应能力分解增强RLVR中大语言模型的推理能力，无需教师模型即可提供额外信息，提升复杂问题求解效果。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法因信息有限导致模型盲目探索，在复杂问题上表现不佳，需在不依赖教师模型的前提下为RLVR过程提供额外引导信息。

Method: 提出A²D（Adaptive Ability Decomposing）方法：首先通过无蒸馏的RLVR训练一个分解器，将复杂问题分解为简单子问题；再用该分解器为训练集标注子问题，并在子问题引导下对推理器进行RLVR训练。

Result: A²D显著提升RLVR在复杂推理任务上的性能；可即插即用地适配多种RLVR算法；分析表明子问题引导能有效平衡推理器的探索与利用能力。

Conclusion: A²D是一种无需教师模型、可泛化、可解释的RLVR增强方法，通过能力分解为大模型推理提供结构化引导，提升了RLVR在挑战性任务中的有效性与鲁棒性。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.

</details>


### [324] [APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards](https://arxiv.org/abs/2602.00760)
*Kaiyan Chang,Chenwei Zhu,Yingfeng Luo,Yifu Huo,Chenglong Wang,Xiaoqian Liu,Qiaozhi He,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出Anchor-based Process Reward (APR)方法，通过识别推理过程中的Reasoning Anchor并惩罚其后的Answer-Stable Tail（AST），缓解大推理模型中的Overthinking问题，在多个数学推理数据集上实现了性能与效率的帕累托最优。


<details>
  <summary>Details</summary>
Motivation: Test-Time Scaling (TTS)虽提升了大推理模型（LRMs）能力，却引发Overthinking问题；作者观察到模型在得到最终答案后仍进行无意义重复自验证，据此提出结构感知的奖励机制。

Method: 定义Reasoning Anchor（答案首次稳定的位置），识别并定位Answer-Stable Tail（AST），设计Anchor-based Process Reward（APR）奖励函数，仅对AST部分施加惩罚，并结合适配长度惩罚的策略优化算法进行RL训练。

Result: APR在1.5B和7B规模模型上，在五个数学推理数据集上达到性能-效率帕累托前沿，且RL训练所需计算资源显著减少。

Conclusion: Answer-Stable Tail是LRMs中结构性冗余的关键表现，APR通过结构感知的局部奖励塑形有效抑制Overthinking，在保持甚至提升推理性能的同时大幅提高推理效率。

Abstract: Test-Time Scaling (TTS) has significantly enhanced the capabilities of Large Reasoning Models (LRMs) but introduces a critical side-effect known as Overthinking. We conduct a preliminary study to rethink this phenomenon from a fine-grained perspective. We observe that LRMs frequently conduct repetitive self-verification without revision even after obtaining the final answer during the reasoning process. We formally define this specific position where the answer first stabilizes as the Reasoning Anchor. By analyzing pre- and post-anchor reasoning behaviors, we uncover the structural redundancy fixed in LRMs: the meaningless repetitive verification after deriving the first complete answer, which we term the Answer-Stable Tail (AST). Motivated by this observation, we propose Anchor-based Process Reward (APR), a structure-aware reward shaping method that localizes the reasoning anchor and penalizes exclusively the post-anchor AST. Leveraging the policy optimization algorithm suitable for length penalties, our APR models achieved the performance-efficiency Pareto frontier at 1.5B and 7B scales averaged across five mathematical reasoning datasets while requiring significantly fewer computational resources for RL training.

</details>


### [325] [WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs](https://arxiv.org/abs/2602.00762)
*Yuheng Shao,Junjie Xiong,Chaoran Wu,Xiyuan Wang,Ziyu Zhou,Yang Ouyang,Qinyi Tao,Quan Li*

Main category: cs.CL

TL;DR: 本文提出WordCraft，一个基于多模态大语言模型（MLLM）的交互式词汇记忆工具，旨在帮助母语为中文的英语学习者更有效地应用关键词法进行词汇记忆。


<details>
  <summary>Details</summary>
Motivation: 母语为中文的英语学习者在使用关键词法记忆词汇时，常难以生成语音恰当的关键词、构建连贯联想及形成生动心理意象；现有自动化方法或忽视学习者参与，或缺乏过程性指导。

Method: 通过面向18名中文母语英语学习者和教育者的形成性研究识别关键难点与需求，并据此设计并实现了一个以学习者为中心、由MLLM驱动的交互式工具WordCraft，支持关键词选择、联想构建与图像生成三阶段过程。

Result: 两项用户研究表明，WordCraft在保持生成效应的同时，展现出高有效性与高可用性。

Conclusion: WordCraft作为一种过程导向的、以学习者为中心的辅助工具，能有效提升关键词法在中文母语者英语词汇学习中的应用效果。

Abstract: Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing approaches, including fully automated keyword generation and outcome-oriented mnemonic aids, either compromise learner engagement or lack adequate process-oriented guidance. To address these limitations, we conducted a formative study with L1 Chinese-L2 English learners and educators (N=18), which revealed key difficulties and requirements in applying the keyword method to vocabulary learning. Building on these insights, we introduce WordCraft, a learner-centered interactive tool powered by Multimodal Large Language Models (MLLMs). WordCraft scaffolds the keyword method by guiding learners through keyword selection, association construction, and image formation, thereby enhancing the effectiveness of vocabulary memorization. Two user studies demonstrate that WordCraft not only preserves the generation effect but also achieves high levels of effectiveness and usability.

</details>


### [326] [Eliciting Trustworthiness Priors of Large Language Models via Economic Games](https://arxiv.org/abs/2602.00769)
*Siyu Yan,Lusha Zhu,Jian-Qiao Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种基于迭代上下文学习的新方法，用于从大语言模型（LLMs）中提取其对人类行为的信任先验，并在信任博弈中验证了GPT-4.1的信任模式与人类高度一致，且可由温暖度与能力感知的刻板印象模型解释。


<details>
  <summary>Details</summary>
Motivation: 构建以人为中心、可信赖的AI系统需维持校准的信任水平，但尚缺乏刻画AI自身所体现信任水平的方法。

Method: 基于迭代上下文学习（Zhu & Griffiths, 2024a），在行为博弈论中的信任博弈框架下，向多个大语言模型（如GPT-4.1）呈现情境化任务，以提取其信任先验；并采用基于温暖度与能力感知的刻板印象模型进行预测分析。

Result: GPT-4.1的信任先验与人类行为高度一致；模型能根据对手人格特征差异化地调整信任判断；其信任倾向可被温暖度-能力双维度刻板印象模型较好预测。

Conclusion: 大语言模型具备可量化、可解释的信任先验结构，且与人类认知具有一致性，为理解AI系统的内在信任机制及设计更可信AI提供了新路径。

Abstract: One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.

</details>


### [327] [Reasoning as State Transition: A Representational Analysis of Reasoning Evolution in Large Language Models](https://arxiv.org/abs/2602.00770)
*Siyuan Zhang,Jialian Li,Yichi Zhang,Xiao Yang,Yinpeng Dong,Hang Su*

Main category: cs.CL

TL;DR: 本文提出了一种基于表征的视角来研究大语言模型在推理任务中内部状态的动态演化，发现推理过程涉及生成过程中表征的连续分布偏移，且后训练主要通过引导该偏移朝向更优分布来提升推理能力，而非显著改善初始表征质量。


<details>
  <summary>Details</summary>
Motivation: 现有工作多依赖生成结果分析推理能力演化，将推理过程视为黑箱，忽略了对模型内部表征动态变化的理解。

Method: 从表征角度出发，通过跨训练阶段模型的系统实验，分析静态初始表征质量、生成过程中表征分布的动态变化，并结合统计相关性分析与反事实实验，探究表征变化与输出正确性的关系。

Result: 1）后训练对静态初始表征质量提升有限；2）推理任务中存在显著的生成时表征分布连续偏移；3）后训练的作用在于引导该偏移趋向更利于求解的分布；4）生成结果的正确性高度依赖最终表征，而语义内容（即生成token本身）是驱动表征偏移的主因，非额外计算或参数差异。

Conclusion: 推理能力的提升本质上源于生成过程中表征分布的可控偏移，后训练通过优化该动态过程而非静态表征来增强推理，为模型分析与优化提供了新视角。

Abstract: Large Language Models have achieved remarkable performance on reasoning tasks, motivating research into how this ability evolves during training. Prior work has primarily analyzed this evolution via explicit generation outcomes, treating the reasoning process as a black box and obscuring internal changes. To address this opacity, we introduce a representational perspective to investigate the dynamics of the model's internal states. Through comprehensive experiments across models at various training stages, we discover that post-training yields only limited improvement in static initial representation quality. Furthermore, we reveal that, distinct from non-reasoning tasks, reasoning involves a significant continuous distributional shift in representations during generation. Comparative analysis indicates that post-training empowers models to drive this transition toward a better distribution for task solving. To clarify the relationship between internal states and external outputs, statistical analysis confirms a high correlation between generation correctness and the final representations; while counterfactual experiments identify the semantics of the generated tokens, rather than additional computation during inference or intrinsic parameter differences, as the dominant driver of the transition. Collectively, we offer a novel understanding of the reasoning process and the effect of training on reasoning enhancement, providing valuable insights for future model analysis and optimization.

</details>


### [328] [HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2602.00777)
*Xuan Ai,Qingqing Yang,Peng Wang,Lei Deng,Lin Zhang,Renhai Chen,Gong Zhang*

Main category: cs.CL

TL;DR: 本文提出HyLRA，一种基于层间稀疏性分析的混合层重用注意力机制，通过区分敏感层（需全注意力）和容忍层（可复用前层关键token索引），在保持精度损失<1%的同时，将长上下文推理吞吐量提升6%–46%。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力机制受限于固定模式或激进剪枝，难以在效率与精度间取得最优平衡；LLM长上下文推理受制于注意力计算的二次复杂度和KV缓存的高内存开销。

Method: 提出HyLRA框架，基于层内敏感性（某些层需全注意力防特征失真）和层间相似性（相邻层共享关键token）的实证发现，采用离线动态规划生成最优层策略：敏感层保留全注意力，容忍层复用前一层top-k token索引，跳过二次计算。

Result: 在多个基准上验证，HyLRA将推理吞吐量提升6%–46%，精度下降<1%，持续优于现有稀疏注意力方法。

Conclusion: HyLRA通过层粒度的自适应注意力策略，有效缓解了长上下文推理中的计算瓶颈，在效率与准确性之间实现了更优权衡。

Abstract: Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often rely on rigid patterns or aggressive pruning, failing to achieve an optimal balance between efficiency and accuracy. In this paper, we introduce {\bf HyLRA} ({\bf Hy}brid {\bf L}ayer {\bf R}euse {\bf A}ttention), a novel framework driven by layer-wise sparsity profiling. Our empirical analysis uncovers a dual characteristic in attention mechanics: \textit{intra-layer sensitivity}, where specific layers necessitate full attention to prevent feature distortion, and \textit{inter-layer similarity}, where consecutive layers share substantial critical tokens. Based on these observations, HyLRA employs an offline dynamic programming approach to derive an optimal layer-wise policy. This hybrid strategy retains full attention for sensitive layers to ensure robustness, while enabling tolerant layers to bypass quadratic calculations by directly reusing top-$k$ indices from preceding layers. This approach allows LLMs to restrict computation to the most critical tokens, effectively overcoming the quadratic bottleneck of dense attention. Extensive evaluations demonstrate that HyLRA improves inference throughput by 6\%--46\% while maintaining comparable performance (with $<1\%$ accuracy degradation), consistently outperforming state-of-the-art sparse attention methods. HyLRA is open source at \href{https://anonymous.4open.science/r/unified-cache-management-CF80/}{\texttt{/r/unified-cache-management-CF80/}}

</details>


### [329] [Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference Synthesis](https://arxiv.org/abs/2602.00846)
*Zicheng Kong,Dehua Ma,Zhenbo Xu,Alven Yang,Yiwei Ru,Haoran Wang,Zixuan Zhou,Fuqing Bie,Liuyu Xiang,Huijia Wu,Jian Zhao,Zhaofeng He*

Main category: cs.CL

TL;DR: 本文提出Omni-RRM，首个开源、基于评分标准（rubric）的多模态奖励模型，支持文本、图像、视频和音频，生成结构化、多维度偏好判断及理由；其训练数据Omni-Preference通过全自动合成与教师模型推理构建，无需人工标注；两阶段训练（SFT+GRPO）使其在视频、音频等基准上达到SOTA，并显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型多为视觉中心、输出不透明标量分、依赖高成本人工标注，难以支撑多模态大模型对齐需求。

Method: 构建全自动 pipeline生成Omni-Preference数据集：合成不同能力模型的响应对，用强教师模型进行模态感知的偏好调和、过滤与评分标准驱动的理由生成；在此基础上两阶段训练Omni-RRM：先监督微调学习结构化输出，再用GRPO强化困难样本判别能力。

Result: 在ShareGPT-V（视频）达80.2%准确率、Audio-HH-RLHF（音频）达66.8%，图像任务上相较基线绝对提升17.7%；支持Best-of-N选择并迁移到纯文本偏好任务。

Conclusion: Omni-RRM首次实现了开源、多模态、结构化、可解释的奖励建模，突破了传统奖励模型的模态局限与标注瓶颈，为多模态对齐提供了新范式。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities, yet their performance is often capped by the coarse nature of existing alignment techniques. A critical bottleneck remains the lack of effective reward models (RMs): existing RMs are predominantly vision-centric, return opaque scalar scores, and rely on costly human annotations. We introduce \textbf{Omni-RRM}, the first open-source rubric-grounded reward model that produces structured, multi-dimension preference judgments with dimension-wise justifications across \textbf{text, image, video, and audio}. At the core of our approach is \textbf{Omni-Preference}, a large-scale dataset built via a fully automated pipeline: we synthesize candidate response pairs by contrasting models of different capabilities, and use strong teacher models to \emph{reconcile and filter} preferences while providing a modality-aware \emph{rubric-grounded rationale} for each pair. This eliminates the need for human-labeled training preferences. Omni-RRM is trained in two stages: supervised fine-tuning to learn the rubric-grounded outputs, followed by reinforcement learning (GRPO) to sharpen discrimination on difficult, low-contrast pairs. Comprehensive evaluations show that Omni-RRM achieves state-of-the-art accuracy on video (80.2\% on ShareGPT-V) and audio (66.8\% on Audio-HH-RLHF) benchmarks, and substantially outperforms existing open-source RMs on image tasks, with a 17.7\% absolute gain over its base model on overall accuracy. Omni-RRM also improves downstream performance via Best-of-$N$ selection and transfers to text-only preference benchmarks. Our data, code, and models are available at https://anonymous.4open.science/r/Omni-RRM-CC08.

</details>


### [330] [Factuality on Demand: Controlling the Factuality-Informativeness Trade-off in Text Generation](https://arxiv.org/abs/2602.00848)
*Ziwei Gong,Yanda Chen,Julia Hirschberg,Chen Zhao,He He,Zhou Yu,Kathleen Mckeown*

Main category: cs.CL

TL;DR: 本文提出了一种名为Factuality-Controlled Generation（FCG）的框架，使用户能在查询中指定事实性约束，以在生成响应时平衡信息量与事实准确性，并通过合成数据训练显著提升了模型在满足事实性要求的同时保持信息量的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在回答问题时面临信息量与事实准确性的固有权衡，不同应用场景对这一权衡的需求不同，因此需要一种可控的事实性生成机制。

Method: 提出FCG框架，定义两个评估维度（事实性约束遵循度和响应信息量），并采用合成数据进行模型训练。

Result: 基于合成数据的训练显著提升了模型在遵循事实性约束的同时保持响应信息量的能力。

Conclusion: FCG为可控文本生成提供了新范式，支持按需调节事实性与信息量的平衡，具有广泛的应用潜力。

Abstract: Large language models (LLMs) encode knowledge with varying degrees of confidence. When responding to queries, models face an inherent trade-off: they can generate responses that are less informative but highly factual, or more informative but potentially less accurate. Different applications demand different balances between informativeness and factuality. We introduce Factuality-Controlled Generation (FCG), a framework that enables users to specify factuality constraints alongside their queries. We propose to evaluate FCG performance on two dimensions: adherence to factuality constraints and response informativeness. We propose to train models on the FCG task using synthetic data, and show that our synthetic training significantly improves models' ability to both respect factuality requirements and maintain informativeness in their outputs.

</details>


### [331] [ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople](https://arxiv.org/abs/2602.00881)
*Shounak Paul,Raghav Dogra,Pawan Goyal,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本文提出了ILSIC数据集，用于比较法院判决和普通民众查询在法律条文识别（LSI）任务中的差异，并通过多种方法（零样本、少样本、RAG、监督微调）进行实验，发现纯法院数据训练的模型在普通民众查询上表现差，但跨域迁移学习有一定效果。


<details>
  <summary>Details</summary>
Motivation: 现有LSI任务多使用法院判决作为输入，但实际应用中用户多为非专业人士，其查询更口语化、非正式；目前缺乏对法院数据与普通民众数据在LSI中差异的系统研究。

Method: 构建了覆盖500+印度法律条文的ILSIC语料库，包含普通民众查询和对应法院判决；开展零样本/少样本推理、检索增强生成（RAG）及监督微调等实验；并按查询类型和法条频率进行细粒度分析。

Result: 纯法院数据训练的模型在普通民众查询测试中效果差；从法院数据向普通民众数据迁移学习在某些场景下有效；不同查询类别和法条频率对模型性能有显著影响。

Conclusion: LSI任务需区分用户类型，单纯依赖法院数据不足以应对真实场景；ILSIC为跨域法律NLP研究提供了新基准和分析视角。

Abstract: Legal Statute Identification (LSI) for a given situation is one of the most fundamental tasks in Legal NLP. This task has traditionally been modeled using facts from court judgments as input queries, due to their abundance. However, in practical settings, the input queries are likely to be informal and asked by laypersons, or non-professionals. While a few laypeople LSI datasets exist, there has been little research to explore the differences between court and laypeople data for LSI. In this work, we create ILSIC, a corpus of laypeople queries covering 500+ statutes from Indian law. Additionally, the corpus also contains court case judgements to enable researchers to effectively compare between court and laypeople data for LSI. We conducted extensive experiments on our corpus, including benchmarking over the laypeople dataset using zero and few-shot inference, retrieval-augmented generation and supervised fine-tuning. We observe that models trained purely on court judgements are ineffective during test on laypeople queries, while transfer learning from court to laypeople data can be beneficial in certain scenarios. We also conducted fine-grained analyses of our results in terms of categories of queries and frequency of statutes.

</details>


### [332] [EffGen: Enabling Small Language Models as Capable Autonomous Agents](https://arxiv.org/abs/2602.00887)
*Gaurav Srivastava,Aafiya Hussain,Chi Wang,Yingyan Celine Lin,Xuan Wang*

Main category: cs.CL

TL;DR: effGen是一个开源的、专为小型语言模型（SLMs）优化的智能体框架，支持高效、安全的本地部署，通过提示压缩、任务分解、复杂度驱动路由和统一记忆系统等技术，在多项基准测试中超越LangChain、AutoGen等主流框架。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型API的语言模型智能体系统存在高token成本和隐私风险问题，尤其在敏感场景下难以满足效率与安全性需求。

Method: 提出effGen框架，包含四项核心技术：（1）提示优化的增强型工具调用，压缩上下文70-80%；（2）依赖感知的智能任务分解；（3）基于五维复杂度因子的路由机制；（4）融合短时、长时与向量存储的统一记忆系统，并统一MCP/A2A/ACP等多协议通信。

Result: 在13个基准上，effGen相比LangChain、AutoGen和Smolagents展现出更高成功率、更快执行速度与更低内存占用；提示优化对小模型增益更大（1.5B提升11.2%），而复杂度路由对大模型更有效（32B提升7.9%），二者结合具互补性与全尺度增益。

Conclusion: effGen验证了小型语言模型在智能体系统中通过架构创新可实现高效、安全、本地化部署，为轻量化、隐私优先的AI智能体提供了可行路径。

Abstract: Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.

</details>


### [333] [Do Schwartz Higher-Order Values Help Sentence-Level Human Value Detection? When Hard Gating Hurts](https://arxiv.org/abs/2602.00913)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

TL;DR: 本文研究了在计算资源受限条件下，如何利用Schwartz高阶（HO）价值类别提升句子级人类价值观检测性能。结果表明，HO结构具有描述性价值，但硬性层级门控会因错误累积和召回率下降而损害性能；相比之下，标签级阈值调优和轻量级集成更有效。


<details>
  <summary>Details</summary>
Motivation: 探究Schwartz高阶（HO）价值类别是否能在句子级价值观检测中提供可用的结构信息，尤其在计算受限（单张8GB GPU）场景下。

Method: 在ValueEval'24/ValuesML数据集上对比三类方法：(i) 直接监督微调Transformer模型；(ii) HO→values硬掩码层级流水线；(iii) Presence→HO→values级联流程，并引入词典、短上下文、主题等低成本增强手段，以及标签级阈值调优、≤10B小指令微调LLM、QLoRA和简单集成。

Result: HO类别可从单句中学习（最易区分的双极对Macro-F1≈0.58），但硬层级门控常降低最终Macro-F1；标签级阈值调优最多提升+0.05 Macro-F1，小型Transformer集成最稳定（+0.02），小LLM单独表现差但可在跨族集成中补充错误。

Conclusion: HO结构具有描述性意义，但硬性层级约束不利于句子级价值观检测；鲁棒提升来自校准（如阈值调优）和轻量集成，而非强制结构建模。

Abstract: Sentence-level human value detection is typically framed as multi-label classification over Schwartz values, but it remains unclear whether Schwartz higher-order (HO) categories provide usable structure. We study this under a strict compute-frugal budget (single 8 GB GPU) on ValueEval'24 / ValuesML (74K English sentences). We compare (i) direct supervised transformers, (ii) HO$\rightarrow$values pipelines that enforce the hierarchy with hard masks, and (iii) Presence$\rightarrow$HO$\rightarrow$values cascades, alongside low-cost add-ons (lexica, short context, topics), label-wise threshold tuning, small instruction-tuned LLM baselines ($\le$10B), QLoRA, and simple ensembles. HO categories are learnable from single sentences (e.g., the easiest bipolar pair reaches Macro-$F_1\approx0.58$), but hard hierarchical gating is not a reliable win: it often reduces end-task Macro-$F_1$ via error compounding and recall suppression. In contrast, label-wise threshold tuning is a high-leverage knob (up to $+0.05$ Macro-$F_1$), and small transformer ensembles provide the most consistent additional gains (up to $+0.02$ Macro-$F_1$). Small LLMs lag behind supervised encoders as stand-alone systems, yet can contribute complementary errors in cross-family ensembles. Overall, HO structure is useful descriptively, but enforcing it with hard gates hurts sentence-level value detection; robust improvements come from calibration and lightweight ensembling.

</details>


### [334] [A Baseline Multimodal Approach to Emotion Recognition in Conversations](https://arxiv.org/abs/2602.00914)
*Víctor Yeste,Rodrigo Rivas-Arévalo*

Main category: cs.CL

TL;DR: 本文提出了一个轻量级的多模态基线模型，用于在Friends情景喜剧数据集上进行对话中的情感识别，结合了基于Transformer的文本分类器和自监督语音表示模型，并采用简单后期融合策略。


<details>
  <summary>Details</summary>
Motivation: 提供一个易于访问的参考实现，以支持未来更严格的方法比较，并提高研究透明度。

Method: 结合基于Transformer的文本分类器和自监督语音表示模型，采用简单后期融合（late-fusion ensemble）策略。

Result: 在受限训练协议下报告了基线设置与实验结果，明确了多模态融合在哪些情况下优于单模态模型。

Conclusion: 该轻量级多模态基线虽非SOTA方法，但为情绪识别任务提供了可复现、透明且实用的参考实现。

Abstract: We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.

</details>


### [335] [Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs](https://arxiv.org/abs/2602.00945)
*Anusa Saha,Tanmay Joshi,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 本文提出Neural FOXP2方法，通过定位、分析和干预语言特异性神经元，实现对大语言模型中非英语语言（如印地语、西班牙语）的可控默认化，揭示并利用稀疏低秩的语言控制回路。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多语言数据上训练，但其默认语言常为英语，其他语言虽保留在参数记忆中却系统性被抑制；本文旨在揭示并调控这种语言默认性的机制。

Method: Neural FOXP2包含三阶段：(i) Localize——用逐层稀疏自编码器（SAE）分解激活，识别对目标语言高选择性的特征及对应语言神经元；(ii) Steering directions——对英-目标语激活差矩阵做分层SVD，提取主导语言转换的低秩方向与稳定干预层；(iii) Steer——在低至中层对语言神经元施加带符号、稀疏的激活偏移，正向增强目标语言、负向抑制英语相关激活。

Result: 成功将印地语或西班牙语设为模型的主用语言，实现可控、稳定的语言默认切换，且不损害模型整体性能；验证了语言默认性由稀疏低秩控制回路（语言神经元）所支配。

Conclusion: 语言默认性受可解释、可干预的稀疏低秩神经机制调控；Neural FOXP2提供了一种机制化、安全的语言 steering 方法，为多语言公平性与可控性提供了新路径。

Abstract: LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.
  We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neurons. Neural FOXP2 proceeds in three stages: (i) Localize: We train per-layer SAEs so each activation decomposes into a small set of active feature components. For every feature, we quantify English vs. Hindi/Spanish selectivity overall logit-mass lift toward the target-language token set. Tracing the top-ranked features back to their strongest contributing units yields a compact language-neuron set. (ii) Steering directions: We localize controllable language-shift geometry via a spectral low-rank analysis. For each layer, we build English to target activation-difference matrices and perform layerwise SVD to extract the dominant singular directions governing language change. The eigengap and effective-rank spectra identify a compact steering subspace and an empirically chosen intervention window (where these directions are strongest and most stable). (iii) Steer: We apply a signed, sparse activation shift targeted to the language neurons. Concretely, within low to mid layers we add a positive steering along the target-language dominant directions and a compensating negative shift toward the null space for the English neurons, yielding controllable target-language defaultness.

</details>


### [336] [Verification Required: The Impact of Information Credibility on AI Persuasion](https://arxiv.org/abs/2602.00970)
*Saaduddin Mahmud,Eugene Bagdasarian,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 本文提出了MixTalk模型，用于研究LLM代理在信息可信度不确定情况下的策略性沟通，并提出TOPD方法提升接收方对说服的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于完全不可验证的廉价谈话或完全可验证的信息披露，无法反映现实中信息具有概率性可信度的场景；而高风险决策场景中，LLM代理间的沟通需有原则性的理解。

Method: 提出MixTalk战略沟通博弈框架，其中发送方混合使用可验证与不可验证主张，接收方进行有限成本的验证并结合先验信念、主张及验证结果推断真实状态；通过大规模锦标赛评估主流LLM代理，并提出离线的Tournament Oracle Policy Distillation（TOPD）方法，从交互日志中蒸馏出锦标赛最优策略并在推理时上下文部署。

Result: 实验揭示了当前LLM代理在信息可信度推理和策略行为建模方面的优缺点；TOPD显著提升了接收方对说服攻击的鲁棒性。

Conclusion: MixTalk为研究LLM代理在概率可信信息环境下的战略沟通提供了新范式；TOPD作为一种轻量级、无需微调的策略蒸馏方法，有效增强了接收方的稳健决策能力。

Abstract: Agents powered by large language models (LLMs) are increasingly deployed in settings where communication shapes high-stakes decisions, making a principled understanding of strategic communication essential. Prior work largely studies either unverifiable cheap-talk or fully verifiable disclosure, failing to capture realistic domains in which information has probabilistic credibility. We introduce MixTalk, a strategic communication game for LLM-to-LLM interaction that models information credibility. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state from prior beliefs, claims, and verification outcomes. We evaluate state-of-the-art LLM agents in large-scale tournaments across three realistic deployment settings, revealing their strengths and limitations in reasoning about information credibility and the explicit behavior that shapes these interactions. Finally, we propose Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time. Our results show that TOPD significantly improves receiver robustness to persuasion.

</details>


### [337] [Trust in One Round: Confidence Estimation for Large Language Models via Structural Signals](https://arxiv.org/abs/2602.00977)
*Pengyue Yang,Jiawen Wen,Haolin Jin,Linghan Huang,Huaming Chen,Ling Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为Structural Confidence的单次前向传播、模型无关的置信度估计框架，利用大语言模型最后一层隐藏状态轨迹的多尺度结构信号（如谱特征、局部变化和全局形状）来提升输出正确性预测能力，在多个高风险领域基准测试中表现优异且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有置信度估计方法（如token likelihood、语义相似度、多样本一致性）在分布偏移、专业领域文本和算力受限场景下鲁棒性差，难以满足高社会/科学/安全成本场景的需求。

Method: 提出Structural Confidence框架，基于模型最后一层隐藏状态轨迹提取多尺度结构信号（谱特征、局部变化、全局形状），无需采样或辅助模型，仅需一次确定性前向传播。

Result: 在FEVER、SciFact、WikiBio-hallucination和TruthfulQA四个异构基准上，AUROC和AUPR显著优于现有基线；相比采样一致性方法，计算开销大幅降低。

Conclusion: Structural Confidence是一种高效、鲁棒、实用的后验置信度估计方法，适用于资源受限且影响重大的LLM部署场景。

Abstract: Large language models (LLMs) are increasingly deployed in domains where errors carry high social, scientific, or safety costs. Yet standard confidence estimators, such as token likelihood, semantic similarity and multi-sample consistency, remain brittle under distribution shift, domain-specialised text, and compute limits. In this work, we present Structural Confidence, a single-pass, model-agnostic framework that enhances output correctness prediction based on multi-scale structural signals derived from a model's final-layer hidden-state trajectory. By combining spectral, local-variation, and global shape descriptors, our method captures internal stability patterns that are missed by probabilities and sentence embeddings. We conduct extensive, cross-domain evaluation across four heterogeneous benchmarks-FEVER (fact verification), SciFact (scientific claims), WikiBio-hallucination (biographical consistency), and TruthfulQA (truthfulness-oriented QA). Our Structural Confidence framework demonstrates strong performance compared with established baselines in terms of AUROC and AUPR. More importantly, unlike sampling-based consistency methods which require multiple stochastic generations and an auxiliary model, our approach uses a single deterministic forward pass, offering a practical basis for efficient, robust post-hoc confidence estimation in socially impactful, resource-constrained LLM applications.

</details>


### [338] [MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA](https://arxiv.org/abs/2602.00981)
*Yutong Song,Shiva Shrestha,Chenhan Lyu,Elahe Khatibi,Pengfei Zhang,Honghui Xu,Nikil Dutt,Amir Rahmani*

Main category: cs.CL

TL;DR: MedSpeak is a knowledge graph-aided ASR error correction framework for medical spoken question-answering, combining medical knowledge graphs and LLMs to improve medical term recognition and answer prediction.


<details>
  <summary>Details</summary>
Motivation: ASR systems often fail to accurately recognize medical terminology, harming downstream medical spoken question-answering performance.

Method: MedSpeak leverages a medical knowledge graph encoding semantic and phonetic information, combined with LLM-based reasoning, to correct ASR errors in transcripts.

Result: MedSpeak significantly improves medical term recognition accuracy and overall medical SQA performance on benchmarks.

Conclusion: MedSpeak establishes a new state-of-the-art for medical spoken question-answering by effectively integrating domain knowledge and large language models.

Abstract: Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.

</details>


### [339] [DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning](https://arxiv.org/abs/2602.00983)
*Batuhan K. Karaman,Aditya Rawal,Suhaila Shakiah,Mohammad Ghavamzadeh,Mingyi Hong,Arijit Biswas,Ruida Zhou*

Main category: cs.CL

TL;DR: 本文提出DISPO算法，通过解耦正确与错误响应的重要性采样权重上下截断，实现更稳定高效的强化学习训练，在数学推理任务上显著超越现有PPO和REINFORCE类方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习方法在训练稳定性与学习效率之间存在明显权衡：PPO类方法稳定但收敛慢，REINFORCE类方法高效但不稳定。

Method: 提出DISPO——一种改进的REINFORCE风格算法，对正确和错误响应分别进行上下重要性采样权重截断，形成四种可控策略更新机制，并通过消融实验分析各机制影响。

Result: DISPO在AIME'24上达到61.04%准确率，显著优于CISPO（55.42%）和DAPO（50.21%），并在多个基准和模型上保持一致提升。

Conclusion: 解耦重要性采样权重截断能有效平衡探索与蒸馏，避免性能崩溃，为大语言模型数学推理强化学习提供更优训练范式。

Abstract: Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.

</details>


### [340] [Sparse Reward Subsystem in Large Language Models](https://arxiv.org/abs/2602.00986)
*Guowei Xu,Mert Yuksekgonul,James Zou*

Main category: cs.CL

TL;DR: 本文发现大语言模型（LLM）隐藏状态中存在一个稀疏奖励子系统，类比人脑的奖励机制；识别出表征状态价值的‘价值神经元’及其在推理中的关键作用，并进一步发现编码奖赏预测误差的‘多巴胺神经元’。


<details>
  <summary>Details</summary>
Motivation: 受人脑 reward system 启发，探索 LLM 内部是否也存在类似的功能性奖励表征机制，以理解其推理与决策的内在原理。

Method: 通过分析 LLM 隐藏状态，识别稀疏奖励子系统；利用干预实验验证价值神经元对推理的影响；通过奖励与预测值偏差分析，定位并验证编码奖赏预测误差（RPE）的‘多巴胺神经元’。

Result: 发现跨数据集、模型规模与架构均鲁棒的价值神经元，具备强可迁移性；识别出响应正/负 RPE 的类多巴胺神经元，其激活模式符合神经科学中多巴胺功能的经典描述。

Conclusion: LLM 隐藏状态中存在类脑的分层奖励子系统，包含价值表征与误差编码功能，为理解 LLM 推理机制及构建更可控、可解释的智能体提供新视角。

Abstract: In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.

</details>


### [341] [DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework](https://arxiv.org/abs/2602.00996)
*Abhijit Chakraborty,Ashish Raj Shekhar,Shiven Agarwal,Vivek Gupta*

Main category: cs.CL

TL;DR: DeALOG是一个去中心化的多智能体框架，用于跨文本、表格和图像的复杂问题回答，通过专用智能体和共享自然语言日志实现协作与可解释性。


<details>
  <summary>Details</summary>
Motivation: 复杂问题回答需要整合文本、表格和图像等异构信息源，现有方法缺乏支持专业化处理、协同推理与可解释性的统一框架。

Method: 提出DeALOG框架，包含表格、上下文、视觉、摘要和验证五类专用智能体，通过共享的自然语言日志进行通信与协作，日志作为持久化记忆支持去中心化错误检测与验证。

Result: 在FinQA、TAT-QA、CRT-QA、WikiTableQuestions、FeTaQA和MultiModalQA等多个基准上达到具有竞争力的性能；消融分析证实共享日志、智能体专业化和验证机制对准确率至关重要。

Conclusion: DeALOG提供了一种模块化、可扩展且鲁棒性强的多模态问答方法，其基于自然语言通信的去中心化设计提升了可解释性与协作可靠性。

Abstract: Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.

</details>


### [342] [Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning](https://arxiv.org/abs/2602.00998)
*Zhikun Xu,Xiaodong Yu,Ben Zhou,Jiang Liu,Jialian Wu,Ze Wang,Ximeng Sun,Hao Chen,Zicheng Liu*

Main category: cs.CL

TL;DR: 本文提出RULES方法，通过结构化预测任务（预设检查与结论效用检查）和分段感知的强化学习训练，提升大语言模型在数学推理中正确应用引理的能力，并在多种评测任务中展现出鲁棒性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽在数学基准测试中表现优异，但常错误应用引理——直接套用结论而忽略前提条件验证，亟需提升引理判断的严谨性与可靠性。

Method: 将引理判断建模为结构化预测任务，要求模型输出两个部分：前提条件检查与结论效用检查；提出RULES框架，采用两段式输出格式，并结合强化学习与分段感知的损失掩码机制，对出错段落精准施加惩罚。

Result: RULES在领域内测试中持续优于基线模型（包括普通模型与单标签RL模型），在破坏适用性的扰动上提升更显著，在端到端任务（竞赛题、扰动对齐题、定理相关题）中达到相当或小幅提升；消融实验证明双段输出与分段感知强化学习均不可或缺。

Conclusion: 结构化输出设计与细粒度强化学习策略是提升LLM数学引理判断鲁棒性的关键，为可信赖的数学推理提供了新范式。

Abstract: Recent large language models (LLMs) perform strongly on mathematical benchmarks yet often misapply lemmas, importing conclusions without validating assumptions. We formalize lemma$-$judging as a structured prediction task: given a statement and a candidate lemma, the model must output a precondition check and a conclusion$-$utility check, from which a usefulness decision is derived. We present RULES, which encodes this specification via a two$-$section output and trains with reinforcement learning plus section$-$aware loss masking to assign penalty to the section responsible for errors. Training and evaluation draw on diverse natural language and formal proof corpora; robustness is assessed with a held$-$out perturbation suite; and end$-$to$-$end evaluation spans competition$-$style, perturbation$-$aligned, and theorem$-$based problems across various LLMs. Results show consistent in$-$domain gains over both a vanilla model and a single$-$label RL baseline, larger improvements on applicability$-$breaking perturbations, and parity or modest gains on end$-$to$-$end tasks; ablations indicate that the two$-$section outputs and section$-$aware reinforcement are both necessary for robustness.

</details>


### [343] [Distilling Token-Trained Models into Byte-Level Models](https://arxiv.org/abs/2602.01007)
*Zishuo Bao,Jiaqi Leng,Junxiong Wang,Bowen Peng,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文提出了一种高效的蒸馏方法，将已有的基于词元训练的大语言模型（LLM）转换为字节级语言模型（BLM），仅需约1250亿字节数据，即可在保持性能的同时避免从头训练的高成本。


<details>
  <summary>Details</summary>
Motivation: 现有字节级语言模型（BLMs）需从零开始在数万亿字节上训练，成本过高；亟需一种低成本、高性能的BLM构建方法。

Method: 采用两阶段课程式蒸馏：（1）渐进式知识蒸馏，对齐字节级表征与词元教师模型的嵌入；（2）字节级监督微调，实现端到端字节空间生成。

Result: 在Llama、Qwen、OLMo等多个模型家族上验证，蒸馏所得BLMs仅用约125B字节即能保持教师模型大部分性能。

Conclusion: 该蒸馏方案显著降低了BLM训练成本，为高效构建高性能字节级模型提供了可行路径。

Abstract: Byte Language Models (BLMs) have emerged as a promising direction for scaling language models beyond tokenization. However, existing BLMs typically require training from scratch on trillions of bytes, making them prohibitively expensive. In this paper, we propose an efficient distillation recipe that converts existing token-trained LLMs into BLMs while retaining comparable capabilities. Our recipe follows a two-stage curriculum: (1) Progressive Knowledge Distillation, which aligns byte-level representations with the embeddings of the token-trained teacher model; and (2) Byte-Level Supervised Fine-Tuning, which enables end-to-end generation entirely in the byte space. We validate our approach across multiple model families, including Llama, Qwen, and OLMo, and demonstrate that the distilled BLMs retain most of the teacher models' performance using only approximately 125B bytes.

</details>


### [344] [Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident](https://arxiv.org/abs/2602.01015)
*Conrad Borchers,Jill-Jênn Vie,Roger Azevedo*

Main category: cs.CL

TL;DR: 本文评估了大语言模型（LLMs）作为‘新手’建模人类初学者推理与元认知判断的能力，发现其推理过于连贯、冗长且缺乏变异性，导致对学习者表现的系统性高估，根源在于训练数据偏向专家解法而缺失真实学习过程特征。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估过度关注问题解决准确性，忽视初学者典型的碎片化、不完美推理过程；需检验LLM能否真实模拟 novice reasoning 和 metacognitive judgments。

Method: 基于630条化学辅导多步问题的学生成人出声思维语料（含提示使用、尝试记录及问题上下文），对比LLM（如GPT-4.1）在最小与扩展上下文提示下的生成推理，并评估其对步骤级学习者成功与否的预测能力。

Result: GPT-4.1生成流畅合理但系统性过连贯、冗长、变异性低于人类；上下文越丰富，偏差越显著；持续高估学习者表现。

Conclusion: LLM存在模拟学习过程的深层认知局限，源于训练数据缺乏情感表达与工作记忆约束等真实学习特征；需构建更契合初学者认知规律的AI辅导系统评估与设计框架。

Abstract: Large language models (LLMs) are increasingly embedded in AI-based tutoring systems. Can they faithfully model novice reasoning and metacognitive judgments? Existing evaluations emphasize problem-solving accuracy, overlooking the fragmented and imperfect reasoning that characterizes human learning. We evaluate LLMs as novices using 630 think-aloud utterances from multi-step chemistry tutoring problems with problem-solving logs of student hint use, attempts, and problem context. We compare LLM-generated reasoning to human learner utterances under minimal and extended contextual prompting, and assess the models' ability to predict step-level learner success. Although GPT-4.1 generates fluent and contextually appropriate continuations, its reasoning is systematically over-coherent, verbose, and less variable than human think-alouds. These effects intensify with a richer problem-solving context during prompting. Learner performance was consistently overestimated. These findings highlight epistemic limitations of simulating learning with LLMs. We attribute these limitations to LLM training data, including expert-like solutions devoid of expressions of affect and working memory constraints during problem solving. Our evaluation framework can guide future design of adaptive systems that more faithfully support novice learning and self-regulation using generative artificial intelligence.

</details>


### [345] [Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations](https://arxiv.org/abs/2602.01030)
*Sheng-Lun Wei,Yu-Ling Liao,Yen-Hua Chang,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文首次系统研究了多语言多模态大语言模型（MLLMs）中的语音偏差问题，构建并发布了BiasInEar数据集，涵盖英、中、韩三语，平衡性别与口音，含70.8小时语音和11200道题；通过四类指标评估9个模型在语言、口音、性别和选项顺序扰动下的表现，发现模型对语言和选项顺序高度敏感，而对性别较鲁棒，语音可能加剧结构性偏差；提出统一评估框架，弥合文本与语音评估鸿沟。


<details>
  <summary>Details</summary>
Motivation: 缺乏对多语言多模态大语言模型中语音偏差的系统性研究，现有评估难以兼顾语言、口音、性别及结构因素，且文本与语音评估存在割裂。

Method: 构建语音增强型基准数据集BiasInEar（基于Global MMLU Lite），覆盖英/中/韩三语，平衡性别与口音；采用准确率、熵值、APES和Fleiss' κ四种互补指标，对九个代表性模型在语言、口音、性别和选项顺序等多维扰动下进行公平性与鲁棒性评估。

Result: MLLMs对语言类型和选项顺序高度敏感，对性别相对鲁棒；语音输入会放大结构性偏差；模型架构与推理策略显著影响跨语言鲁棒性。

Conclusion: 本研究建立了首个面向语音集成大模型的统一公平性与鲁棒性评估框架，揭示了语音引入的新偏差维度，推动多模态AI公平性研究从文本向语音延伸。

Abstract: This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and Fleiss' $κ$), we evaluate nine representative models under linguistic (language and accent), demographic (gender), and structural (option order) perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation. The resources can be found at https://github.com/ntunlplab/BiasInEar.

</details>


### [346] [Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents](https://arxiv.org/abs/2602.01063)
*Bin Han,Deuksin Kwon,Jonathan Gratch*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLMs）在不同对话情境下对同一人格提示的表达差异，发现其人格与情绪表现具有显著的情境依赖性，支持其具备类人的上下文敏感适应能力而非不一致行为。


<details>
  <summary>Details</summary>
Motivation: 探究相同人格提示在不同对话场景中为何导致行为、语言和情绪表达的差异，并厘清这种差异是模型不一致还是类人的情境适应。

Method: 在四种对话设置（破冰、谈判、群体决策、共情任务）中，对LLM施加相同的人格提示，系统分析其语言、行为与情绪输出的变化。

Result: 上下文线索系统性地影响人格表达与情绪基调；相同人格特质在不同社会与情感需求下呈现差异化表达。

Conclusion: LLMs展现的是基于Whole Trait Theory的情境敏感型人格表达，而非固定不变的人格，表明其具备面向交互目标与情感条件的灵活适应能力。

Abstract: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.

</details>


### [347] [Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs](https://arxiv.org/abs/2602.01064)
*Ruihan Jin,Pengpeng Shao,Zhengqi Wen,Jinyang Wu,Mingkuan Feng,Shuo Yang,Chu Yuan Zhang,Jianhua Tao*

Main category: cs.CL

TL;DR: 本文提出知识净化（Knowledge Purification）概念，通过融合多个教师大语言模型的推理过程生成统一、一致的推理链，以缓解多教师蒸馏中的知识冲突并提升效率，并设计了五种净化方法，实验表明其能提升学生模型性能并增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统多教师知识蒸馏面临知识冲突和高资源消耗问题，需更高效、一致的知识迁移机制。

Method: 提出知识净化概念，将多个教师LLM的推理链整合为单一高质量推理链；设计五种净化方法，包括基于路由器（router-based）等不同视角的方法。

Result: 所提净化方法显著提升蒸馏后学生模型性能，有效缓解知识冲突；其中路由器方法展现出强泛化能力。

Conclusion: 知识净化是一种有效的多教师知识蒸馏优化策略，有助于构建高性能且轻量化的实用模型。

Abstract: Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of \textbf{Knowledge Purification}, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.

</details>


### [348] [From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization](https://arxiv.org/abs/2602.01068)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

TL;DR: 本文研究如何构建满足领域定制需求的翻译大语言模型，以视觉媒体字幕翻译为切入点，提出自适应局部偏好优化（ALPO）方法，并构建多向字幕平行语料库，验证了LLM作为翻译奖励模型和评估器的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在通用机器翻译中表现优异，但在垂直领域（如字幕翻译）中存在局限性，亟需构建具备领域适配能力的翻译LLM。

Method: 构建多向字幕平行语料库，并提出自适应局部偏好优化（ALPO）方法，用于细粒度偏好对齐；同时验证LLM作为翻译奖励模型和评估器的可行性。

Result: ALPO在翻译质量的多维评估中展现出卓越性能；所构建的字幕数据集已开源；LLM被证实可作为可靠的翻译奖励模型与评估器。

Conclusion: ALPO方法及配套数据集有效提升了领域定制化翻译LLM的表现，尤其在表达力与生动性方面，为垂直领域翻译模型训练提供了新范式。

Abstract: The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.

</details>


### [349] [What If We Allocate Test-Time Compute Adaptively?](https://arxiv.org/abs/2602.01070)
*Ahsan Bilal,Ahmed Mohsin,Muhammad Umer,Ali Subhan,Hassan Rizwan,Ayesha Mohsin,Dean Hougen*

Main category: cs.CL

TL;DR: 本文提出了一种基于验证器引导的自适应推理框架，通过多轮迭代生成与选择推理轨迹，并利用过程奖励模型（PRM）在步骤级和轨迹级进行动态控制，显著提升数学推理任务性能，同时提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展方法存在计算分配均匀、采样策略固定、验证仅用于重排序等问题，难以高效应对复杂推理任务。

Method: 提出验证器引导的自适应框架：对每个问题进行多轮推理迭代；每轮可生成高层计划、选择推理工具与计算策略及探索参数，并生成候选推理轨迹；使用过程奖励模型（PRM）作为统一控制信号，在生成中指导剪枝与扩展，在跨轮中选择最优响应。

Result: 在MATH-500、AIME24和AMO-Bench等基准上显著优于直接测试时缩放方法，尤其在难题上实现数倍提升；理论FLOPs与计算强度分析表明该方法能将计算集中于高价值推理路径。

Conclusion: PRM引导的动态推理框架能更高效地分配测试时计算资源，兼顾性能提升与计算节约，为大模型推理优化提供了新范式。

Abstract: Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.

</details>


### [350] [Logic-Oriented Retriever Enhancement via Contrastive Learning](https://arxiv.org/abs/2602.01116)
*Wenxuan Zhang,Yuan-Hao Jiang,Changyong Qi,Rui Jia,Yonghe Wu*

Main category: cs.CL

TL;DR: LORE是一种无需外部监督的检索器增强方法，通过细粒度对比学习激活大语言模型中固有的逻辑分析能力，提升知识密集型任务中的检索效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型任务中表现不佳，因为传统检索器容易过拟合表面相似性，难以处理涉及复杂逻辑关系的查询。

Method: 提出LORE（Logic ORiented Retriever Enhancement），采用细粒度对比学习，引导嵌入对齐逻辑结构而非浅层相似性，无需外部监督、额外资源或预检索分析，并保持索引兼容性。

Result: LORE在多个基准上持续提升检索效用和下游生成质量，同时保持高效性。

Conclusion: LORE有效激活了模型表示中固有的逻辑分析能力，为知识密集型任务提供了一种轻量、通用且高效的检索增强方案。

Abstract: Large language models (LLMs) struggle in knowledge-intensive tasks, as retrievers often overfit to surface similarity and fail on queries involving complex logical relations. The capacity for logical analysis is inherent in model representations but remains underutilized in standard training. LORE (Logic ORiented Retriever Enhancement) introduces fine-grained contrastive learning to activate this latent capacity, guiding embeddings toward evidence aligned with logical structure rather than shallow similarity. LORE requires no external upervision, resources, or pre-retrieval analysis, remains index-compatible, and consistently improves retrieval utility and downstream generation while maintaining efficiency. The datasets and code are publicly available at https://github.com/mazehart/Lore-RAG.

</details>


### [351] [Tendem: A Hybrid AI+Human Platform](https://arxiv.org/abs/2602.01119)
*Konstantin Chernyshev,Ekaterina Artemova,Viacheslav Zhukov,Maksim Nerush,Mariia Fedorova,Iryna Repik,Olga Shapovalova,Aleksey Sukhorosov,Vladimir Dobrovolskii,Natalia Mikhailova,Sergei Tilga*

Main category: cs.CL

TL;DR: Tendem is a hybrid AI-human system that outperforms both AI-only and human-only approaches in quality and speed, while maintaining cost-efficiency; its AI agent also achieves near state-of-the-art performance on standard agentic benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of purely AI-based or purely human-based workflows by combining AI efficiency with human expertise for higher quality, speed, and cost-effectiveness.

Method: Developed Tendem—a hybrid system integrating AI for structured tasks and human experts for verification/failure handling—evaluated via in-house testing on 94 real-world tasks and third-party agentic benchmarks.

Result: Tendem achieved higher-quality outputs and faster turnaround than AI-only and human-only baselines, with comparable costs to human-only execution; its autonomous AI agent performed near state-of-the-art on web browsing and tool-use, and strongly on domain knowledge and reasoning.

Conclusion: Hybrid human-AI collaboration (as embodied by Tendem) offers a practical and effective paradigm for real-world task execution, balancing performance, speed, and cost better than fully automated or fully manual alternatives.

Abstract: Tendem is a hybrid system where AI handles structured, repeatable work and Human Experts step in when the models fail or to verify results. Each result undergoes a comprehensive quality review before delivery to the Client. To assess Tendem's performance, we conducted a series of in-house evaluations on 94 real-world tasks, comparing it with AI-only agents and human-only workflows carried out by Upwork freelancers. The results show that Tendem consistently delivers higher-quality outputs with faster turnaround times. At the same time, its operational costs remain comparable to human-only execution. On third-party agentic benchmarks, Tendem's AI Agent (operating autonomously, without human involvement) performs near state-of-the-art on web browsing and tool-use tasks while demonstrating strong results in frontier domain knowledge and reasoning.

</details>


### [352] [Long-range Modeling and Processing of Multimodal Event Sequences](https://arxiv.org/abs/2602.01125)
*Jichu Li,Yilun Zhong,Zhiting Li,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: 本文提出了一种扩展基于大语言模型的时间点过程（TPP）框架，以支持视觉模态，并将文本生成作为核心能力；通过基于时间相似性的自适应序列压缩机制解决长上下文问题，并采用两阶段训练范式，在预测准确性和文本分析质量上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于TPP的方法在处理多模态内容（尤其是图文结合）时存在局限性，难以生成丰富、连贯的长文本描述，主要受限于多模态数据导致序列过长，影响注意力机制的长程建模能力。

Method: 提出一种新型LLM-based TPP框架，引入视觉模态并以文本生成为核心任务；设计基于时间相似性的自适应序列压缩机制以缩短输入长度、保留关键动态模式；采用两阶段训练：先在压缩序列上预训练，再针对下游任务进行监督微调。

Result: 在DanmakuTPP-QA等基准上实验表明，该方法在事件时间/类型预测准确性及生成文本质量两方面均显著优于当前最优基线。

Conclusion: 将TPP与多模态大模型结合，并通过序列压缩与两阶段训练策略，可有效提升异步事件序列的建模与解释能力，为多模态时空序列建模提供了新范式。

Abstract: Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.

</details>


### [353] [Don't Judge a Book by its Cover: Testing LLMs' Robustness Under Logical Obfuscation](https://arxiv.org/abs/2602.01132)
*Abhilekh Borah,Shubhra Ghosh,Kedar Joshi,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

TL;DR: This paper introduces Logifus, a logical obfuscation framework, and LogiQAte, a new benchmark with 1,108 logically equivalent but obfuscated reasoning questions, revealing that state-of-the-art LLMs suffer large performance drops—up to 47%—under obfuscation, exposing their lack of deep semantic understanding.


<details>
  <summary>Details</summary>
Motivation: Large language models perform well on standard logical reasoning tasks but fail when problems are logically equivalent yet obfuscated; this paper aims to diagnose and quantify this fragility to expose shallow parsing behavior.

Method: The authors propose Logifus—a structure-preserving logical obfuscation framework—and build LogiQAte, a diagnostic benchmark with 1,108 questions across four obfuscated reasoning tasks (FOL entailment, blood relations, number series, direction sense); they evaluate six SOTA models in zero-shot settings.

Result: Obfuscation causes severe zero-shot performance degradation: average drops of 47% for GPT-4o, 27% for GPT-5, and 22% for o4-mini across all tasks.

Conclusion: Current LLMs rely on surface-form patterns rather than deep logical understanding; robust, meaning-preserving reasoning requires models that genuinely comprehend semantics beyond syntactic form.

Abstract: Tasks such as solving arithmetic equations, evaluating truth tables, and completing syllogisms are handled well by large language models (LLMs) in their standard form, but they often fail when the same problems are posed in logically equivalent yet obfuscated formats. To study this vulnerability, we introduce Logifus, a structure-preserving logical obfuscation framework, and, utilizing this, we present LogiQAte, a first-of-its-kind diagnostic benchmark with 1,108 questions across four reasoning tasks: (i) Obfus FOL (first-order logic entailment under equivalence-preserving rewrites), (ii) Obfus Blood Relation (family-graph entailment under indirect relational chains), (iii) Obfus Number Series (pattern induction under symbolic substitutions), and (iv) Obfus Direction Sense (navigation reasoning under altered directions and reference frames). Across all the tasks, evaluating six state-of-the-art models, we find that obfuscation severely degrades zero-shot performance, with performance dropping on average by 47% for GPT-4o, 27% for GPT-5, and 22% for reasoning model, o4-mini. Our findings reveal that current LLMs parse questions without deep understanding, highlighting the urgency of building models that genuinely comprehend and preserve meaning beyond surface form.

</details>


### [354] [Beyond Training for Cultural Awareness: The Role of Dataset Linguistic Structure in Large Language Models](https://arxiv.org/abs/2602.01161)
*Reem I. Masoud,Chen Feng,Shunta Asano,Saied Alshahrani,Philip Colin Treleaven,Miguel R. D. Rodrigues*

Main category: cs.CL

TL;DR: 本文从数据集中心视角研究了大型语言模型（LLM）文化对齐中细调数据的语言特性，通过计算阿拉伯语、中文和日语数据集的轻量级语言、语义与结构指标并进行主成分分析（PCA），发现不同语言下可解释的主成分轴（如语义连贯性、词汇/句法多样性、词汇/结构丰富度）与下游文化性能存在模型依赖性强的相关性；其中以词汇导向的第三主成分（PC3）干预效果最稳健，而强调语义或多样性极端值（PC1-PC2）则常无益甚至有害。


<details>
  <summary>Details</summary>
Motivation: 全球部署大语言模型引发文化错位担忧，但用于文化适配的细调数据集的语言特性尚不明确，亟需从数据集角度理解哪些语言属性影响文化性能及其跨模型普适性。

Method: 对阿拉伯语、中文和日语细调数据集计算轻量级语言、语义与结构指标，按语言分别进行主成分分析（PCA）以提取可解释的内在变异轴；随后在LLaMA、Mistral、DeepSeek三大模型家族上开展细调，并在文化知识、价值观与规范基准上评估；最后通过受控子集干预验证各主成分对性能的影响。

Result: PCA各主成分与下游文化性能存在相关性，但高度依赖具体模型；词汇导向的PC3干预在不同模型和基准上表现最稳健，而PC1（语义）和PC2（多样性）干预多为中性或负向。

Conclusion: 文化对齐效果不仅取决于数据内容，更与细调数据的语言结构特性密切相关；其中词汇层面的丰富性（PC3）是跨模型最可靠的预测与调控因子，提示未来文化适配应优先关注词汇维度的数据设计。

Abstract: The global deployment of large language models (LLMs) has raised concerns about cultural misalignment, yet the linguistic properties of fine-tuning datasets used for cultural adaptation remain poorly understood. We adopt a dataset-centric view of cultural alignment and ask which linguistic properties of fine-tuning data are associated with cultural performance, whether these properties are predictive prior to training, and how these effects vary across models. We compute lightweight linguistic, semantic, and structural metrics for Arabic, Chinese, and Japanese datasets and apply principal component analysis separately within each language. This design ensures that the resulting components capture variation among datasets written in the same language rather than differences between languages. The resulting components correspond to broadly interpretable axes related to semantic coherence, surface-level lexical and syntactic diversity, and lexical or structural richness, though their composition varies across languages. We fine-tune three major LLM families (LLaMA, Mistral, DeepSeek) and evaluate them on benchmarks of cultural knowledge, values, and norms. While PCA components correlate with downstream performance, these associations are strongly model-dependent. Through controlled subset interventions, we show that lexical-oriented components (PC3) are the most robust, yielding more consistent performance across models and benchmarks, whereas emphasizing semantic or diversity extremes (PC1-PC2) is often neutral or harmful.

</details>


### [355] [Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource Languages](https://arxiv.org/abs/2602.01162)
*Nipuna Abeykoon,Ashen Weerathunga,Pubudu Wijesinghe,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言类型学的无监督翻译质量提升框架，通过通用元语言框架（UMF）和计算引擎，在不依赖平行语料或模型重训练的情况下，提升大语言模型对类型学差异大的低资源语言的翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高资源语言上训练后，对类型学上差异大的低资源语言存在系统性结构偏差，导致翻译结构不合规。

Method: 提出一个两组件框架：1）通用元语言框架（UMF），用16个类型学维度刻画语言并加权评分；2）计算引擎，在生成时进行语言消歧、在候选选择时进行类型学合规性评分。

Result: 在9个语言对上验证干预率与英语类型学距离强相关；在341句英文测试中，保守型、形态密集型和结构可描述型语言的干预精度分别为48.16%、28.15%和86.26%；无需平行数据，适配任意能输出多候选的LLM。

Conclusion: 该框架为低资源语言翻译提供了一种轻量、通用、无需重训练的类型学驱动解决方案。

Abstract: Large language models trained predominantly on high-resource languages exhibit systematic biases toward dominant typological patterns, leading to structural non-conformance when translating into typologically divergent low-resource languages. We present a framework that leverages linguistic typology to improve translation quality without parallel training data or model retraining. The framework consists of two components: the Universal Metalinguistic Framework (UMF), which represents languages as structured profiles across 16 typological dimensions with divergence-weighted scoring, and the Computational Engine, which operates through linguistic disambiguation during generation and typological compliance scoring during selection. Evaluation across nine language pairs demonstrates intervention rates strongly correlating with typological distance from English. In experiments on 341 English sentences each having different morphological and syntactic phenomena, the framework shows an intervention precision of 48.16% for conservatively treated languages, 28.15% for morphologically dense languages, and 86.26% for structurally profiled languages. The framework requires no parallel training data and operates with any LLM capable of producing multiple candidate outputs, enabling practical deployment for under-resourced languages.

</details>


### [356] [PedagoSense: A Pedology Grounded LLM System for Pedagogical Strategy Detection and Contextual Response Generation in Learning Dialogues](https://arxiv.org/abs/2602.01169)
*Shahem Sultan,Shahem Fadi,Yousef Melhim,Ibrahim Alsarraj,Besher Hassan*

Main category: cs.CL

TL;DR: 本文提出PedagoSense系统，通过两阶段分类器检测并细粒度识别对话中的教学策略，并利用大语言模型生成符合该策略的响应，以提升对话式学习中的交互质量。


<details>
  <summary>Details</summary>
Motivation: 提升对话式学习中师生互动的质量，通过自动检测和推荐有效的教学策略来增强教育技术的适应性。

Method: 提出PedagoSense系统，包含二元分类器检测是否存在教学策略、细粒度分类器识别具体策略类型，并行使用LLM基于对话上下文推荐并生成符合策略的响应。

Result: 在人工标注的师生对话数据集上验证了系统在教学策略检测上的高性能，数据增强带来稳定性能提升，但细粒度分类仍存在挑战。

Conclusion: PedagoSense成功将教学理论与基于大语言模型的响应生成相结合，为构建更自适应的教育技术提供了新路径。

Abstract: This paper addresses the challenge of improving interaction quality in dialogue based learning by detecting and recommending effective pedagogical strategies in tutor student conversations. We introduce PedagoSense, a pedology grounded system that combines a two stage strategy classifier with large language model generation. The system first detects whether a pedagogical strategy is present using a binary classifier, then performs fine grained classification to identify the specific strategy. In parallel, it recommends an appropriate strategy from the dialogue context and uses an LLM to generate a response aligned with that strategy. We evaluate on human annotated tutor student dialogues, augmented with additional non pedagogical conversations for the binary task. Results show high performance for pedagogical strategy detection and consistent gains when using data augmentation, while analysis highlights where fine grained classes remain challenging. Overall, PedagoSense bridges pedagogical theory and practical LLM based response generation for more adaptive educational technologies.

</details>


### [357] [EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech](https://arxiv.org/abs/2602.01170)
*Besher Hassan,Ibrahim Alsarraj,Musaab Hasan,Yousef Melhim,Shahem Fadi,Shahem Sultan*

Main category: cs.CL

TL;DR: EmoAra是一个端到端的跨语言语音情感保持系统，用于将英语语音实时转换为带情感的阿拉伯语语音，专为银行客服场景设计。


<details>
  <summary>Details</summary>
Motivation: 银行业务客服中情感语境直接影响服务质量，需在跨语言语音转换中保留情感信息。

Method: 整合语音情感识别（CNN分类器）、自动语音识别（Whisper）、机器翻译（微调MarianMT）和文本转语音（MMS-TTS-Ara），构建端到端流水线。

Result: 情感识别F1达94%，英译阿BLEU 56、BERTScore F1 88.7%，人工评估银行领域翻译得分81%。

Conclusion: EmoAra有效实现了跨语言语音中情感信息的端到端保持，具备实际客服应用潜力，并开源实现与资源。

Abstract: This work presents EmoAra, an end-to-end emotion-preserving pipeline for cross-lingual spoken communication, motivated by banking customer service where emotional context affects service quality. EmoAra integrates Speech Emotion Recognition, Automatic Speech Recognition, Machine Translation, and Text-to-Speech to process English speech and deliver an Arabic spoken output while retaining emotional nuance. The system uses a CNN-based emotion classifier, Whisper for English transcription, a fine-tuned MarianMT model for English-to-Arabic translation, and MMS-TTS-Ara for Arabic speech synthesis. Experiments report an F1-score of 94% for emotion classification, translation performance of BLEU 56 and BERTScore F1 88.7%, and an average human evaluation score of 81% on banking-domain translations. The implementation and resources are available at the accompanying GitHub repository.

</details>


### [358] [Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense Disambiguation](https://arxiv.org/abs/2602.01193)
*Shashini Nilukshi,Deshan Sumanathilaka*

Main category: cs.CL

TL;DR: 本文综述了视觉词义消歧（VWSD）的发展，涵盖从早期多模态融合到基于CLIP、扩散模型和大语言模型（LLM）的新方法；指出CLIP微调与LLM增强系统在MRR上提升6–8%，但仍面临上下文局限、模型偏差、多语种数据缺乏及评估体系不完善等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决传统词义消歧（WSD）仅依赖文本的局限性，利用视觉线索应对视觉-语言任务中的词汇歧义问题，尤其在文本输入极少时提升消歧能力。

Method: 梳理2016–2025年VWSD研究进展，涵盖特征融合、图神经网络、对比学习（如CLIP）、扩散模型生成及LLM辅助等方法，并分析提示工程、微调策略与多语言适配技术。

Result: CLIP微调模型与LLM增强系统在Mean Reciprocal Rank（MRR）指标上较零样本基线提升达6–8%；但现有方法仍受限于上下文建模能力、常见义项偏差、多语种数据稀缺及评估框架不健全。

Conclusion: 未来VWSD应融合CLIP对齐能力、扩散模型生成能力与LLM推理能力，构建强上下文感知、多语言支持的统一消歧系统。

Abstract: This paper offers a mini review of Visual Word Sense Disambiguation (VWSD), which is a multimodal extension of traditional Word Sense Disambiguation (WSD). VWSD helps tackle lexical ambiguity in vision-language tasks. While conventional WSD depends only on text and lexical resources, VWSD uses visual cues to find the right meaning of ambiguous words with minimal text input. The review looks at developments from early multimodal fusion methods to new frameworks that use contrastive models like CLIP, diffusion-based text-to-image generation, and large language model (LLM) support. Studies from 2016 to 2025 are examined to show the growth of VWSD through feature-based, graph-based, and contrastive embedding techniques. It focuses on prompt engineering, fine-tuning, and adapting to multiple languages. Quantitative results show that CLIP-based fine-tuned models and LLM-enhanced VWSD systems consistently perform better than zero-shot baselines, achieving gains of up to 6-8\% in Mean Reciprocal Rank (MRR). However, challenges still exist, such as limitations in context, model bias toward common meanings, a lack of multilingual datasets, and the need for better evaluation frameworks. The analysis highlights the growing overlap of CLIP alignment, diffusion generation, and LLM reasoning as the future path for strong, context-aware, and multilingual disambiguation systems.

</details>


### [359] [Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training to Address Head Collapse](https://arxiv.org/abs/2602.01203)
*Zizhuo Fu,Wenxuan Zeng,Runsheng Wang,Meng Li*

Main category: cs.CL

TL;DR: 本文揭示了注意力机制中的'sink'现象自然构成了注意力层内的混合专家（MoE）机制，并提出了一种sink-aware训练算法以缓解头坍塌问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Sink Attention和Gated Attention试图解决LLM中注意力偏向首token（attention sink）的问题，但缺乏对这些机制间关系的系统性分析。

Method: 通过理论与实证分析揭示Vanilla Attention与Sink Attention中sink结构隐含的MoE机制；提出带辅助负载均衡损失的sink-aware训练算法。

Result: 所提方法在Vanilla、Sink和Gated三种注意力机制上均实现有效头负载均衡，并提升模型性能。

Conclusion: 注意力层内存在固有的MoE结构，该发现为理解与改进注意力机制提供了新视角，并推动对注意力内在结构的进一步探索。

Abstract: Large Language Models (LLMs) often assign disproportionate attention to the first token, a phenomenon known as the attention sink. Several recent approaches aim to address this issue, including Sink Attention in GPT-OSS and Gated Attention in Qwen3-Next. However, a comprehensive analysis of the relationship among these attention mechanisms is lacking. In this work, we provide both theoretical and empirical evidence demonstrating that the sink in Vanilla Attention and Sink Attention naturally construct a Mixture-of-Experts (MoE) mechanism within attention layers. This insight explains the head collapse phenomenon observed in prior work, where only a fixed subset of attention heads contributes to generation. To mitigate head collapse, we propose a sink-aware training algorithm with an auxiliary load balancing loss designed for attention layers. Extensive experiments show that our method achieves effective head load balancing and improves model performance across Vanilla Attention, Sink Attention, and Gated Attention. We hope this study offers a new perspective on attention mechanisms and encourages further exploration of the inherent MoE structure within attention layers.

</details>


### [360] [ASTER: Agentic Scaling with Tool-integrated Extended Reasoning](https://arxiv.org/abs/2602.01204)
*Xuqin Zhang,Quan He,Zhenrui Zheng,Zongzhang Zhang,Xu He,Dong Li*

Main category: cs.CL

TL;DR: 本文提出ASTER框架，通过优先选择交互密集的冷启动轨迹来解决大语言模型在工具集成推理中因交互崩溃导致的多轮工具使用失败问题，显著提升了数学基准测试性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在强化学习中进行工具集成推理时出现的交互崩溃问题，即模型无法维持多轮工具使用而退化为内部推理和简单代码验证。

Method: 系统研究冷启动监督微调对工具使用行为先验的影响、冷启动轨迹交互密度对探索和强化学习结果的影响，以及强化学习交互预算对学习动态和泛化能力的影响；提出ASTER框架，采用以交互密集轨迹为重点的冷启动策略。

Result: 仅使用4K条交互密集的专家冷启动轨迹即可获得最强下游性能；ASTER-4B在AIME 2025上达到90.0%，超越DeepSeek-V3.2-Exp等前沿开源模型。

Conclusion: 交互密集的冷启动策略能建立强行为先验，显著提升强化学习阶段的探索能力和最终性能，有效缓解工具集成推理中的交互崩溃问题。

Abstract: Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.

</details>


### [361] [Chronos: Learning Temporal Dynamics of Reasoning Chains for Test-Time Scaling](https://arxiv.org/abs/2602.01208)
*Kai Zhang,Jiayi Liao,Chengpeng Li,Ziyuan Xie,Sihang Li,Xiang Wang*

Main category: cs.CL

TL;DR: 本文提出Chronos，一种轻量级、即插即用的时间序列建模方法，用于对LLM推理轨迹进行质量评分和加权投票，显著提升测试时推理性能，且计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法（如多数投票和启发式token级打分）将所有推理路径或token同等对待，易受路径质量波动和局部逻辑错误影响，缺乏对推理过程动态质量的建模能力。

Method: Chronos将每条推理轨迹建模为时间序列，学习token概率序列的特征，预测轨迹质量得分，并据此进行加权投票。

Result: 在域内和域外基准上均取得显著提升；在HMMT25数据集上，Chronos@128相比Pass@1和Maj@128分别提升34.21%和22.70%（使用Qwen3-4B-Thinking-2507模型）。

Conclusion: Chronos通过建模推理轨迹的时间动态性实现高效、鲁棒的测试时推理增强，是一种通用、低开销的推理优化方案。

Abstract: Test-Time Scaling (TTS) has emerged as an effective paradigm for improving the reasoning performance of large language models (LLMs). However, existing methods -- most notably majority voting and heuristic token-level scoring -- treat reasoning traces or tokens equally, thereby being susceptible to substantial variations in trajectory quality and localized logical failures. In this work, we introduce \textbf{Chronos}, a lightweight and plug-and-play chronological reasoning scorer that models each trajectory as a time series. Specifically, Chronos learns to capture trajectory features of token probabilities, assigns quality scores accordingly, and employs a weighted voting mechanism. Extensive evaluations on both in-domain and out-of-domain benchmarks demonstrate that Chronos consistently delivers substantial gains across a variety of models, with negligible computational overhead. Notably, Chronos@128 achieves relative improvements of 34.21\% over Pass@1 and 22.70\% over Maj@128 on HMMT25 using Qwen3-4B-Thinking-2507, highlighting its effectiveness.

</details>


### [362] [Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority](https://arxiv.org/abs/2602.01227)
*Zhanming Shen,Zeyu Qin,Jiaqi Hu,Wentao Ye,Hao Chen,Xiaomeng Hu,Haokai Xu,Gang Chen,Yi R. Fung,Haobo Wang*

Main category: cs.CL

TL;DR: 本文提出Token Priority概念，作为连接经验数据拟合与真实人类效用的桥梁，将监督微调（SFT）重新定义为一种精确的分布重塑过程，并将近期突破分为Positive Priority和Signed Priority两类。


<details>
  <summary>Details</summary>
Motivation: 解决细粒度自回归生成与粗粒度或均匀监督信号之间的粒度不匹配问题，以提升模型对人类效用的真实对齐能力。

Method: 提出Token Priority框架，将监督微调形式化为对原始数据分布向理想对齐流形的重塑过程，并据此对近期方法进行统一分类与分析。

Result: 将现有方法归类为Positive Priority（用于噪声过滤）和Signed Priority（用于消除有害模式）两种范式，并指出现有进展、局限性及未来研究方向。

Conclusion: Token Priority是弥合粒度鸿沟、推动对齐从数据拟合迈向人类效用的关键范式，为SFT提供了更本质的理论视角与实践路径。

Abstract: The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, formalizing Supervised Fine-Tuning (SFT) not as simple optimization but as a precise distribution reshaping process that aligns raw data with the ideal alignment manifold. We analyze recent breakthroughs through this unified lens, categorizing them into two distinct regimes: Positive Priority for noise filtration and Signed Priority for toxic modes unlearning. We revisit existing progress and limitations, identify key challenges, and suggest directions for future research.

</details>


### [363] [Minimizing Mismatch Risk: A Prototype-Based Routing Framework for Zero-shot LLM-generated Text Detection](https://arxiv.org/abs/2602.01240)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

TL;DR: 本文提出DetectRouter框架，通过两阶段训练学习文本与检测器之间的匹配关系，将零样本检测问题转化为路由问题，显著提升LLM生成文本检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本检测方法使用固定代理模型，但其性能高度依赖于代理模型与未知源模型的对齐程度，缺乏自适应能力。

Method: 提出DetectRouter框架，第一阶段利用白盒模型构建判别性原型，第二阶段通过几何距离与检测分数对齐，泛化至黑盒源模型。

Result: 在EvoBench和MAGE基准上，DetectRouter在多种检测指标和模型族上均实现一致性能提升。

Conclusion: 代理模型与源模型的匹配是零样本检测的关键，将检测建模为路由问题可显著增强鲁棒性。

Abstract: Zero-shot methods detect LLM-generated text by computing statistical signatures using a surrogate model. Existing approaches typically employ a fixed surrogate for all inputs regardless of the unknown source. We systematically examine this design and find that detection performance varies substantially depending on surrogate-source alignment. We observe that while no single surrogate achieves optimal performance universally, a well-matched surrogate typically exists within a diverse pool for any given input. This finding transforms robust detection into a routing problem: selecting the most appropriate surrogate for each input. We propose DetectRouter, a prototype-based framework that learns text-detector affinity through two-stage training. The first stage constructs discriminative prototypes from white-box models; the second generalizes to black-box sources by aligning geometric distances with observed detection scores. Experiments on EvoBench and MAGE benchmarks demonstrate consistent improvements across multiple detection criteria and model families.

</details>


### [364] [Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments](https://arxiv.org/abs/2602.01244)
*Siwei Wu,Yizhi Li,Yuyang Song,Wei Zhang,Yang Wang,Riza Batista-Navarro,Xian Yang,Mingjie Tang,Bryan Dai,Jian Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: 本文提出TerminalTraj，一个用于生成高质量、可执行且可验证的终端轨迹数据的可扩展流水线，构建了32K Docker镜像和50,733条跨8个领域的验证轨迹；基于该数据训练的Qwen2.5-Coder模型在TerminalBench上显著提升性能，其中TerminalTraj-32B在参数少于100B的模型中表现优异。


<details>
  <summary>Details</summary>
Motivation: 构建高质量、大规模、可执行且可验证的终端轨迹数据困难，主要受限于执行环境（需适配各异Docker环境）与结果验证（输出异构、缺乏统一标准）两大挑战。

Method: 提出TerminalTraj流水线：(i)筛选高质量代码仓库构建Docker化执行环境；(ii)生成与Docker环境对齐的任务实例；(iii)合成带可执行验证代码的智能体轨迹。

Result: 构建32K Docker镜像和50,733条经验证的终端轨迹；基于该数据训练的模型在TerminalBench 1.0和2.0上分别最高提升20%和10%；TerminalTraj-32B在TB 1.0和2.0上分别达35.30%和22.00%，且具备更优测试时缩放行为。

Conclusion: TerminalTraj有效解决了终端任务数据构建中的可执行性与可验证性难题，为训练终端智能体提供了高质量、可扩展的数据基础，并显著提升了模型性能与泛化能力。

Abstract: Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \textbf{\emph{Executability}}, since each instance requires a suitable and often distinct Docker environment; and \textbf{\emph{Verifiability}}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose \textbf{TerminalTraj}, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, \textbf{TerminalTraj-32B} achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.

</details>


### [365] [PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length](https://arxiv.org/abs/2602.01274)
*Situo Zhang,Yifan Zhang,Zichen Zhu,Hankun Wang,Da Ma,Danyang Zhang,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: 本文提出Pacer，一种动态控制草稿长度的推测解码方法，通过轻量级可训练预验证层提升大语言模型推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码使用固定草稿长度，但实验发现最优草稿长度随解码步长变化显著，固定长度限制了加速潜力。

Method: 提出Pacer方法，引入轻量、可训练的预验证层，以块为单位对草稿token进行预验证，若失败则提前终止草稿生成。

Result: Pacer在多个SD模型对和基准测试中实现最高2.66倍于自回归解码的速度提升，并优于标准推测解码；与Ouroboros结合时达3.09倍加速。

Conclusion: 动态控制草稿长度能有效提升推测解码效率，Pacer为LLM推理加速提供了新思路且具备良好泛化性。

Abstract: Speculative decoding (SD) is a powerful technique for accelerating the inference process of large language models (LLMs) without sacrificing accuracy. Typically, SD employs a small draft model to generate a fixed number of draft tokens, which are then verified in parallel by the target model. However, our experiments reveal that the optimal draft length varies significantly across different decoding steps. This variation suggests that using a fixed draft length limits the potential for further improvements in decoding speed. To address this challenge, we propose Pacer, a novel approach that dynamically controls draft length using a lightweight, trainable pre-verification layer. This layer pre-verifies draft tokens blockwise before they are sent to the target model, allowing the draft model to stop token generation if the blockwise pre-verification fails. We implement Pacer on multiple SD model pairs and evaluate its performance across various benchmarks. Our results demonstrate that Pacer achieves up to 2.66x Speedup over autoregressive decoding and consistently outperforms standard speculative decoding. Furthermore, when integrated with Ouroboros, Pacer attains up to 3.09x Speedup.

</details>


### [366] [EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language ModelsEverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models](https://arxiv.org/abs/2602.01313)
*Chuanrui Hu,Tong Li,Xingze Gao,Hongda Chen,Dannong Xu,Yi Bai,Tianwei Lin,Xinda Zhao,Xiaohong Li,Jiaqi An,Yunyun Han,Jian Pei,Yafeng Deng*

Main category: cs.CL

TL;DR: 本文提出了EverMemBench，一个面向长期、多角色、跨话题、时序演化的对话记忆评估基准，揭示了当前大模型记忆系统在多跳推理、时序理解与记忆感知三方面的关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有对话记忆基准局限于单话题、双人对话，无法反映真实复杂场景（如多角色、跨话题、信息动态演化），亟需更贴近实际的评估标准。

Method: 构建EverMemBench基准：包含百万级token的多群体、多角色、时序演化的长对话数据集，并设计涵盖细粒度回忆、记忆感知和用户画像理解三维度的1000+问答对进行系统评测。

Result: 发现三大瓶颈：(1) 多方对话中多跳推理性能骤降至26%；(2) 时序推理需版本语义建模，远超简单时间戳匹配；(3) 基于相似度的检索方法难以弥合查询与隐含相关记忆间的语义鸿沟。

Conclusion: EverMemBench为下一代对话记忆架构提供了更具挑战性与现实意义的评估平台，指明了记忆建模的关键改进方向。

Abstract: Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens with temporally evolving information, cross-topic interleaving, and role-specific personas. EverMemBench evaluates memory systems across three dimensions through 1,000+ QA pairs: fine-grained recall, memory awareness, and user profile understanding. Our evaluation reveals critical limitations: (1) multi-hop reasoning collapses in multi-party settings, with even oracle models achieving only 26%; (2) temporal reasoning remains unsolved, requiring version semantics beyond timestamp matching; (3) memory awareness is bottlenecked by retrieval, where current similarity-based methods fail to bridge the semantic gap between queries and implicitly relevant memories. EverMemBench provides a challenging testbed for developing next-generation memory architectures.

</details>


### [367] [DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas](https://arxiv.org/abs/2602.01326)
*Zirui Wu,Lin Zheng,Zhihui Xie,Jiacheng Ye,Jiahui Gao,Shansan Gong,Yansong Feng,Zhenguo Li,Wei Bi,Guorui Zhou,Lingpeng Kong*

Main category: cs.CL

TL;DR: 本文提出DreamOn框架，通过引入两个长度控制状态，使扩散语言模型（DLMs）能够实现动态、可变长度的代码补全，无需修改模型结构或大幅调整训练目标，并在多个基准上达到与先进自回归模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型（DLMs）受限于固定掩码长度，导致代码补全时因长度不匹配而性能下降，阻碍其实际应用。

Method: 提出DreamOn框架，在扩散过程中引入两个长度控制状态，使模型能基于自身预测自主扩展或收缩输出长度；仅需对现有DLM（如Dream-Coder-7B和DiffuCoder-7B）做最小训练目标调整，无架构改动。

Result: 在HumanEval-Infilling和SantaCoder-FIM上达到与SOTA自回归模型相当的补全性能，并匹配已知真实长度（oracle）下的最优表现。

Conclusion: DreamOn消除了DLMs在可变长度生成中的关键部署障碍，显著提升了其灵活性与实用性。

Abstract: Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.

</details>


### [368] [CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering](https://arxiv.org/abs/2602.01348)
*Yu Liu,Wenxiao Zhang,Cong Cao,Fangfang Yuan,Weizhuo Chen,Cheng Hu,Pin Xu,Yuling Yang,Kun Peng,Diandian Guo,Qiang Sun,Yanbing Liu,Jin B. Hong,Zhiyuan Ma*

Main category: cs.CL

TL;DR: 本文提出CRAFT框架，通过双奖励机制的强化学习方法解决多跳问答中推理崩溃、推理-答案不一致和格式失控三大挑战，提升大语言模型在检索增强生成中的推理忠实性和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 针对多跳问答中检索增强生成存在的推理崩溃、推理与答案不一致、格式控制丢失三大问题，亟需提升模型推理的稳定性、忠实性和结构可控性。

Method: 提出基于组相对策略优化（GRPO）的强化学习框架CRAFT，引入确定性奖励（保障结构正确性）与判别式奖励（验证语义忠实性）的双奖励机制，并支持可控推理路径变体以系统分析结构与规模对推理的影响。

Result: 在三个多跳问答基准上，CRAFT显著提升了答案准确率与推理忠实性；其中CRAFT 7B模型在多种推理路径设置下性能媲美闭源大模型。

Conclusion: CRAFT通过可校准、可验证、可控制的推理训练范式，有效缓解了RAG中多跳推理的核心可靠性问题，为构建可信推理系统提供了新思路。

Abstract: Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.

</details>


### [369] [Balancing Understanding and Generation in Discrete Diffusion Models](https://arxiv.org/abs/2602.01362)
*Yue Liu,Yuzhong Zhao,Zheyong Xie,Qixiang Ye,Jianbin Jiao,Yao Hu,Shaosheng Cao,Yunfan Liu*

Main category: cs.CL

TL;DR: 本文提出XDLM模型，通过引入平稳噪声核统一了掩码扩散语言模型（MDLM）和均匀噪声扩散语言模型（UDLM），在语义理解与少步生成质量之间实现更好平衡，并在多项任务上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有离散生成建模中，MDLM擅长语义理解和零样本泛化，UDLM在少步生成质量上表现强，但二者难以兼顾；需一种能兼顾两者的统一框架。

Method: 提出XDLM，基于平稳噪声核统一MDLM与UDLM；理论推导其为二者的特例，并通过后验概率的代数简化缓解内存瓶颈。

Result: XDLM在零样本文本基准上比UDLM高5.4分；少步图像生成FID达54.1（优于MDLM的80.8）；微调8B大模型仅32步即达MBPP 15.0，性能翻倍；训练动力学分析表明其更适于长期扩展。

Conclusion: XDLM实现了对MDLM与UDLM的理论统一与实践增强，在理解能力与生成质量的Pareto前沿上取得显著提升，具备良好可扩展性。

Abstract: In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM

</details>


### [370] [Context Dependence and Reliability in Autoregressive Language Models](https://arxiv.org/abs/2602.01378)
*Poushali Sengupta,Shashi Raj Pandey,Sabita Maharjan,Frank Eliassen*

Main category: cs.CL

TL;DR: 本文提出RISE方法，通过量化每个输入相对于其他输入的独特影响，解决大语言模型解释中因上下文冗余导致的归因不稳定问题，提升解释的鲁棒性与可信度。


<details>
  <summary>Details</summary>
Motivation: 标准解释方法在存在冗余和重叠上下文时表现不佳，微小输入变化易导致归因分数剧烈波动，影响可解释性并带来如提示注入等风险。

Method: 提出RISE（Redundancy-Insensitive Scoring of Explanation），一种能抑制冗余干扰、衡量各输入独特影响的归因评分方法。

Result: 实验表明RISE比传统方法提供更鲁棒、更稳定的解释，凸显条件信息对可信LLM解释与监控的关键作用。

Conclusion: RISE有效区分了上下文中的关键与相关元素，为高风险场景下LLM的可靠解释与安全监控提供了新路径。

Abstract: Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.

</details>


### [371] [On the Power of (Approximate) Reward Models for Inference-Time Scaling](https://arxiv.org/abs/2602.01381)
*Youheng Zhu,Yiping Lu*

Main category: cs.CL

TL;DR: 本文研究了在推理时扩展（inference-time scaling）中使用近似奖励模型的有效性问题，提出贝尔曼误差是决定其效果的关键因素，并证明当贝尔曼误差为O(1/T)时，可将推理计算复杂度从指数级降至多项式级。


<details>
  <summary>Details</summary>
Motivation: 实际部署中无法获得真实奖励模型，只能使用近似模型；需理论解释为何及何时近似模型仍能有效支持基于序贯蒙特卡洛（SMC）的推理时扩展。

Method: 通过理论分析，定义并分析近似奖励模型的贝尔曼误差，推导其对SMC推理过程计算复杂度的影响。

Result: 证明若近似奖励模型的贝尔曼误差以O(1/T)为界，则SMC可将长度为T的推理任务的计算复杂度从指数级降低至多项式级，实现指数级推理效率提升。

Conclusion: 贝尔曼误差是评估近似奖励模型在SMC推理时扩展中有效性的重要理论指标，低贝尔曼误差足以支撑高效推理。

Abstract: Inference-time scaling has recently emerged as a powerful paradigm for improving the reasoning capability of large language models. Among various approaches, Sequential Monte Carlo (SMC) has become a particularly important framework, enabling iterative generation, evaluation, rejection, and resampling of intermediate reasoning trajectories. A central component in this process is the reward model, which evaluates partial solutions and guides the allocation of computation during inference.
  However, in practice, true reward models are never available. All deployed systems rely on approximate reward models, raising a fundamental question: Why and when do approximate reward models suffice for effective inference-time scaling? In this work, we provide a theoretical answer. We identify the Bellman error of the approximate reward model as the key quantity governing the effectiveness of SMC-based inference-time scaling. For a reasoning process of length $T$, we show that if the Bellman error of the approximate reward model is bounded by $O(1/T)$, then combining this reward model with SMC reduces the computational complexity of reasoning from exponential in $T$ to polynomial in $T$. This yields an exponential improvement in inference efficiency despite using only approximate rewards.

</details>


### [372] [Rethinking Selective Knowledge Distillation](https://arxiv.org/abs/2602.01395)
*Almog Tavor,Itay Ebenspanger,Neil Cnaan,Mor Geva*

Main category: cs.CL

TL;DR: 本文重新审视了自回归大语言模型中的知识蒸馏，提出了一种基于学生模型熵的位置选择方法（SE-KD），并在位置、类别和样本三个维度上进行扩展（SE-KD 3X），显著提升了准确性、下游任务适配性和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有选择性知识蒸馏方法在重要性信号、选择策略及其相互作用方面效果尚不明确，需系统分析并探索更优方案。

Method: 解耦位置、类别和样本三个维度的选择性蒸馏，系统比较不同重要性信号与选择策略；提出基于学生熵的位置选择（SE-KD），并扩展至三轴（SE-KD 3X）。

Result: SE-KD在多个基准上优于密集蒸馏；SE-KD 3X使离线教师缓存可行，降低70%运行时间、18%峰值内存和80%存储开销，且不损失性能。

Conclusion: 学生熵是有效的选择信号，三轴联合优化能显著提升知识蒸馏的效率与效果，为高效LLM蒸馏提供了新范式。

Abstract: Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.

</details>


### [373] [From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis](https://arxiv.org/abs/2602.01401)
*Niansong Zhang,Sunwoo Kim,Shreesha Srinath,Zhiru Zhang*

Main category: cs.CL

TL;DR: 本文探讨了在AI代理时代高阶综合（HLS）的持续重要性，指出HLS因其快速迭代、可移植性和设计可变性，仍是AI驱动硬件设计中不可或缺的抽象层；同时分析了当前HLS工具的三大局限，并提出一种描述AI代理与HLS协同演进的分类法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型兴起，AI驱动硬件设计成为热点，但HLS是否仍具价值引发疑问；本文旨在论证HLS在智能代理时代的不可替代性及其优化潜力。

Method: 本文为立场论文，采用概念分析与系统性归纳方法：阐述HLS作为抽象层和黄金参考的价值，识别现有HLS工具的关键缺陷，并构建AI代理与HLS协同演进的分类体系。

Result: 提出三点贡献：1）确立HLS作为代理硬件设计的实用抽象层与黄金参考；2）明确当前HLS工具在性能反馈、接口灵活性与可调试性三方面的不足；3）提出‘共生演进’分类法，刻画从人类主导到AI自主设计的责任转移路径。

Conclusion: HLS在AI代理时代不仅依然重要，而且是实现高效、可验证、可扩展硬件自动优化的核心使能层；未来需推动HLS与AI代理深度协同，而非取代HLS。

Abstract: The rise of large language models has sparked interest in AI-driven hardware design, raising the question: does high-level synthesis (HLS) still matter in the agentic era? We argue that HLS remains essential. While we expect mature agentic hardware systems to leverage both HLS and RTL, this paper focuses on HLS and its role in enabling agentic optimization. HLS offers faster iteration cycles, portability, and design permutability that make it a natural layer for agentic optimization.This position paper makes three contributions. First, we explain why HLS serves as a practical abstraction layer and a golden reference for agentic hardware design. Second, we identify key limitations of current HLS tools, namely inadequate performance feedback, rigid interfaces, and limited debuggability that agents are uniquely positioned to address. Third, we propose a taxonomy for the symbiotic evolution of agentic HLS, clarifying how responsibility shifts from human designers to AI agents as systems advance from copilots to autonomous design partners.

</details>


### [374] [SentiFuse: Deep Multi-model Fusion Framework for Robust Sentiment Extraction](https://arxiv.org/abs/2602.01447)
*Hieu Minh Duong,Rupa Ghosh,Cong Hoan Nguyen,Eugene Levin,Todd Gary,Long Nguyen*

Main category: cs.CL

TL;DR: 本文提出了SentiFuse框架，通过标准化层和多种融合策略（决策级、特征级和自适应融合）集成异构情感分析模型，在多个社交媒体数据集上显著提升了F1分数并增强了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析模型虽有互补优势，但缺乏统一有效的集成框架。

Method: 提出SentiFuse——一种灵活、模型无关的框架，包含标准化层和决策级、特征级及自适应三种融合策略。

Result: 在Crowdflower、GoEmotions和Sentiment140数据集上，特征级融合最高带来4%绝对F1提升；自适应融合提升了对否定、混合情绪等复杂案例的鲁棒性。

Conclusion: 系统利用模型互补性可显著提升情感分析的准确性与可靠性，适用于多样数据集与文本类型。

Abstract: Sentiment analysis models exhibit complementary strengths, yet existing approaches lack a unified framework for effective integration. We present SentiFuse, a flexible and model-agnostic framework that integrates heterogeneous sentiment models through a standardization layer and multiple fusion strategies. Our approach supports decision-level fusion, feature-level fusion, and adaptive fusion, enabling systematic combination of diverse models. We conduct experiments on three large-scale social-media datasets: Crowdflower, GoEmotions, and Sentiment140. These experiments show that SentiFuse consistently outperforms individual models and naive ensembles. Feature-level fusion achieves the strongest overall effectiveness, yielding up to 4\% absolute improvement in F1 score over the best individual model and simple averaging, while adaptive fusion enhances robustness on challenging cases such as negation, mixed emotions, and complex sentiment expressions. These results demonstrate that systematically leveraging model complementarity yields more accurate and reliable sentiment analysis across diverse datasets and text types.

</details>


### [375] [Understanding QA generation: Extracting Parametric and Contextual Knowledge with CQA for Low Resource Bangla Language](https://arxiv.org/abs/2602.01451)
*Umme Abira Azmary,MD Ikramul Kayes,Swakkhar Shatabda,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文提出了BanglaCQA，首个孟加拉语反事实问答数据集，并设计了多种模型管道（微调与提示）来分离参数化知识与上下文知识；通过LLM与人工评估发现，思维链（CoT）提示在反事实场景下尤其能有效激发解码器-only大模型的参数化知识。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如孟加拉语）的问答模型受限于标注数据稀缺和语言复杂性，且缺乏可分析模型知识来源（参数化 vs. 上下文）的结构化数据集。

Method: 构建首个孟加拉语反事实QA数据集BanglaCQA，包含反事实段落与可回答性标注；设计微调型（编码器-解码器）与提示型（解码器-only LLM）双路径方法；采用LLM与人工评估（基于语义相似度）；开展多场景性能对比与CoT提示机制分析。

Result: 发现Chain-of-Thought提示在反事实场景中显著提升解码器-only大模型对参数化知识的利用能力；验证了所提框架可有效区分知识来源，并揭示了低资源语言中反事实推理的关键挑战与潜力。

Conclusion: BanglaCQA为低资源语言QA的知识源分析提供了新基准与方法论框架；CoT提示被证实是激活大模型隐式知识的有效手段，为未来低资源反事实推理研究开辟了新方向。

Abstract: Question-Answering (QA) models for low-resource languages like Bangla face challenges due to limited annotated data and linguistic complexity. A key issue is determining whether models rely more on pre-encoded (parametric) knowledge or contextual input during answer generation, as existing Bangla QA datasets lack the structure required for such analysis. We introduce BanglaCQA, the first Counterfactual QA dataset in Bangla, by extending a Bangla dataset while integrating counterfactual passages and answerability annotations. In addition, we propose fine-tuned pipelines for encoder-decoder language-specific and multilingual baseline models, and prompting-based pipelines for decoder-only LLMs to disentangle parametric and contextual knowledge in both factual and counterfactual scenarios. Furthermore, we apply LLM-based and human evaluation techniques that measure answer quality based on semantic similarity. We also present a detailed analysis of how models perform across different QA settings in low-resource languages, and show that Chain-of-Thought (CoT) prompting reveals a uniquely effective mechanism for extracting parametric knowledge in counterfactual scenarios, particularly in decoder-only LLMs. Our work not only introduces a novel framework for analyzing knowledge sources in Bangla QA but also uncovers critical findings that open up broader directions for counterfactual reasoning in low-resource language settings.

</details>


### [376] [ConPress: Learning Efficient Reasoning from Multi-Question Contextual Pressure](https://arxiv.org/abs/2602.01472)
*Jie Deng,Shining Liang,Jun Li,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: 本文发现了一种名为Self-Compression的推理时现象：当多个可回答问题共存于同一提示中时，大推理模型会自发缩短每个问题的思维链；基于此，作者提出轻量级自监督微调方法ConPress，通过多问题上下文压力诱导模型生成简洁正确的推理路径，并在单问题场景下实现显著推理token减少（MATH500减59%，AIME25减33%）且不损准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）因生成长链式思维（CoT）而带来巨大推理开销，亟需提升推理效率。

Method: 提出ConPress方法：构建多问题提示以激发Self-Compression现象，采样模型输出、解析并筛选各问题的简洁正确推理轨迹，用于监督微调，无需外部教师、人工剪枝或强化学习。

Result: 仅用8k微调样本，在MATH500上推理token减少59%，在AIME25上减少33%，同时保持竞争力的准确率。

Conclusion: Self-Compression是一种可复现的推理时现象，ConPress利用该现象实现了高效、轻量、自监督的推理压缩，显著降低开销且不牺牲性能。

Abstract: Large reasoning models (LRMs) typically solve reasoning-intensive tasks by generating long chain-of-thought (CoT) traces, leading to substantial inference overhead. We identify a reproducible inference-time phenomenon, termed Self-Compression: when multiple independent and answerable questions are presented within a single prompt, the model spontaneously produces shorter reasoning traces for each question. This phenomenon arises from multi-question contextual pressure during generation and consistently manifests across models and benchmarks. Building on this observation, we propose ConPress (Learning from Contextual Pressure), a lightweight self-supervised fine-tuning approach. ConPress constructs multi-question prompts to induce self-compression, samples the resulting model outputs, and parses and filters per-question traces to obtain concise yet correct reasoning trajectories. These trajectories are directly used for supervised fine-tuning, internalizing compressed reasoning behavior in single-question settings without external teachers, manual pruning, or reinforcement learning. With only 8k fine-tuning examples, ConPress reduces reasoning token usage by 59% on MATH500 and 33% on AIME25, while maintaining competitive accuracy.

</details>


### [377] [Ebisu: Benchmarking Large Language Models in Japanese Finance](https://arxiv.org/abs/2602.01479)
*Xueqing Peng,Ruoyu Xiang,Fan Zhang,Mingzi Song,Mingyang Jiang,Yan Wang,Lingfei Qian,Taiki Hara,Yuqing Guo,Jimin Huang,Junichi Tsujii,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文介绍了Ebisu，一个针对日语金融语言理解的基准测试，包含两个专家标注的任务：JF-ICR（隐含承诺与拒绝识别）和JF-TE（嵌套金融术语的层级抽取与排序），用于评估大语言模型在日语金融文本理解上的能力。实验表明现有模型表现不佳，语言与领域适配效果有限，该基准有助于推动文化与语言适配的金融NLP研究。


<details>
  <summary>Details</summary>
Motivation: 日语金融语言具有黏着性、中心语后置、混合书写系统及高语境依赖等特征，对大语言模型构成显著挑战，亟需专门的基准测试来评估其理解能力。

Method: 构建了Ebisu基准，包括两个专家标注任务：JF-ICR（识别投资者问答中隐含承诺与拒绝）和JF-TE（从专业披露文本中层级抽取并排序嵌套金融术语）；并在多种开源与商用大语言模型上进行评估。

Result: 即使最先进的模型在两项任务上仍表现较差；模型规模增大仅带来有限提升；语言与领域适配并未稳定提升性能，存在显著能力缺口。

Conclusion: Ebisu为日语金融NLP提供了首个聚焦语言与文化特性的基准，揭示了当前模型在高语境、隐含语义理解上的根本局限，推动更深入的语言与领域协同建模。

Abstract: Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.

</details>


### [378] [Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training](https://arxiv.org/abs/2602.01511)
*Ran Xu,Tianci Liu,Zihan Dong,Tony You,Ilgee Hong,Carl Yang,Linjun Zhang,Tao Zhao,Haoyu Wang*

Main category: cs.CL

TL;DR: 本文提出Rubric-ARM框架，通过强化学习联合优化评分标准生成器和裁判模型，以更全面评估非可验证领域（如创意写作）中的回复质量。


<details>
  <summary>Details</summary>
Motivation: 标准奖励模型仅输出标量分数，难以刻画非可验证领域中响应质量的多维特性。

Method: 提出Rubric-ARM框架，将评分标准生成建模为隐式动作，通过偏好反馈的强化学习联合训练生成器与裁判，并采用交替优化策略缓解训练不稳定性，辅以梯度方差理论分析。

Result: 在多个基准上达到SOTA性能，并显著提升离线和在线强化学习中下游策略对齐效果。

Conclusion: Rubric-ARM通过动态、可学习的评分标准提升了奖励建模的细粒度与有效性，为复杂质量评估提供了新范式。

Abstract: Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.

</details>


### [379] [Argument Rarity-based Originality Assessment for AI-Assisted Writing](https://arxiv.org/abs/2602.01560)
*Keito Inoshita,Michiaki Omura,Tsukasa Yamanaka,Go Maeda,Kentaro Tsuji*

Main category: cs.CL

TL;DR: 本文提出了一种基于论点稀有性的原创性评估框架（AROA），用于自动评估学生议论文的原创性，强调在AI生成文本盛行背景下，教育评估应从质量转向原创性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）能轻松生成高质量文本，传统以质量为导向的写作评估已失去意义；教育的核心目标是培养批判性思维与原创观点，因此评估范式需转向原创性。

Method: 提出Argument Rarity-based Originality Assessment（AROA）框架，将原创性定义为在参考语料库中的稀有性，通过结构稀有性、主张稀有性、证据稀有性和认知深度四个互补维度量化，并结合密度估计与质量调整机制，使质量与原创性成为独立评估轴。

Result: 实验发现质量与主张稀有性呈强负相关，证实‘质量-原创性权衡’；AI生成文章在结构复杂度上接近人类，但主张稀有性显著更低，表明LLMs擅长模仿论证形式，但在内容原创性上存在局限。

Conclusion: AROA为写作评估提供了新范式，揭示了当前LLMs在原创性上的根本局限，支持教育评估向原创性倾斜的必要性。

Abstract: As Large Language Models (LLMs) have become capable of effortlessly generating high-quality text, traditional quality-focused writing assessment is losing its significance. If the essential goal of education is to foster critical thinking and original perspectives, assessment must also shift its paradigm from quality to originality. This study proposes Argument Rarity-based Originality Assessment (AROA), a framework for automatically evaluating argumentative originality in student essays. AROA defines originality as rarity within a reference corpus and evaluates it through four complementary components: structural rarity, claim rarity, evidence rarity, and cognitive depth. The framework quantifies the rarity of each component using density estimation and integrates them with a quality adjustment mechanism, thereby treating quality and originality as independent evaluation axes. Experiments using human essays and AI-generated essays revealed a strong negative correlation between quality and claim rarity, demonstrating a quality-originality trade-off where higher-quality texts tend to rely on typical claim patterns. Furthermore, while AI essays achieved comparable levels of structural complexity to human essays, their claim rarity was substantially lower than that of humans, indicating that LLMs can reproduce the form of argumentation but have limitations in the originality of content.

</details>


### [380] [FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents](https://arxiv.org/abs/2602.01566)
*Chiwei Zhu,Benfeng Xu,Mingxuan Du,Shaohan Wang,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: 本文提出了FS-Researcher，一种基于文件系统的双智能体框架，通过持久化工作空间突破大语言模型上下文长度限制，提升深度研究任务性能。


<details>
  <summary>Details</summary>
Motivation: 长程深度研究任务常超出大语言模型上下文窗口，导致证据收集与报告撰写资源紧张，难以进行有效的测试时扩展。

Method: 设计了基于文件系统的双智能体框架：Context Builder（负责网络浏览、结构化笔记与知识库归档）和Report Writer（基于知识库分节撰写报告），以文件系统作为外部记忆与多智能体协同媒介。

Result: 在DeepResearch Bench和DeepConsult两个开放基准上达到当前最优报告质量；实验表明Context Builder的计算投入与最终报告质量呈正相关，验证了该范式下测试时扩展的有效性。

Conclusion: FS-Researcher通过引入持久化文件系统作为外部记忆与协调机制，有效解决了LLM在长程深度研究中的上下文瓶颈问题，并支持可扩展、可迭代的智能体协作。

Abstract: Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.

</details>


### [381] [Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment](https://arxiv.org/abs/2602.01587)
*Zehua Cheng,Jianwei Yang,Wei Dai,Jiahao Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于统计稳定性的可证明鲁棒性框架，通过分层随机消融和噪声增强对齐微调，显著提升大语言模型对抗自适应越狱攻击的鲁棒性，并提供确定性安全证书。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）仍易受自适应越狱攻击（如GCG）影响，现有经验性防御缺乏理论保障。

Method: 提出Certified Semantic Smoothing（CSS）框架，结合Stratified Randomized Ablation（利用超几何分布推导ℓ₀范数鲁棒半径）与Noise-Augmented Alignment Tuning（NAAT）将基础模型转化为语义去噪器。

Result: 在Llama-3上将基于梯度攻击的成功率从84.2%降至1.2%，良性任务效用保持94.1%，显著优于字符级基线（效用仅74.3%）。

Conclusion: 该框架首次为LLM安全提供可证明、确定性的语义级鲁棒性保证，将安全验证从单次推理拓展至集成统计稳定性。

Abstract: Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.

</details>


### [382] [Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles](https://arxiv.org/abs/2602.01590)
*Shaohan Wang,Benfeng Xu,Licheng Zhang,Mingxuan Du,Chiwei Zhu,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

TL;DR: 本文提出Wiki Live Challenge (WLC)——一个基于维基百科优质条目（Good Articles）的实时评测基准，用于更可靠、细粒度地评估深度研究代理（DRAs）的能力；配套提出Wiki Eval评测框架，含39项写作质量指标和严格的事实可验证性度量；实验表明当前DRAs与人类专家水平仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有DRA评测方法多依赖大语言模型生成参考或评估维度，缺乏专家验证，难以实现客观、细粒度的评估，尤其在中立性、全面性和可验证性等关键维度上存在不足。

Method: 构建WLC基准：选取100篇最新维基百科优质条目（GAs）作为专家级参考；设计Wiki Eval框架，包含39项写作质量细粒度准则及事实可验证性的严格量化指标。

Result: 在多个DRA系统上的实验表明，当前DRAs在写作质量与事实准确性方面与维基百科优质条目存在显著差距；WLC被验证为能有效推动DRA研究发展。

Conclusion: WLC提供了一种更可靠、更具挑战性的DRA评测范式，强调以真实专家内容为标尺，推动研究代理向更高可信度与专业性演进。

Abstract: Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge

</details>


### [383] [The Art of Socratic Inquiry: A Framework for Proactive Template-Guided Therapeutic Conversation Generation](https://arxiv.org/abs/2602.01598)
*Mingwen Zhang,Minqiang Yang,Changsheng Ma,Yang Yu,Hui Bai,Chen Xu,Xiangzhen Kong,Bin Hu*

Main category: cs.CL

TL;DR: 本文提出Socratic Inquiry Framework (SIF)，一种轻量级、即插即用的治疗意图规划器，使大语言模型能主动发起苏格拉底式提问，从而从被动响应转向主动认知引导；同时构建Socratic-QA数据集以支持训练，并验证其在提升提问主动性、对话深度与治疗一致性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前心理领域大语言模型过于被动和反应式，缺乏引导患者认知重构所需的主动提问能力，难以支撑认知行为疗法（CBT）的核心实践。

Method: 提出Socratic Inquiry Framework（SIF），将‘何时提问’（Strategy Anchoring）与‘提问内容’（Template Retrieval）解耦；构建Socratic-QA高质量数据集用于监督训练；无需端到端微调即可集成至现有LLM。

Result: SIF显著提升了模型主动提问频率、对话深度及治疗理论一致性，在多个评估维度上优于基线模型。

Conclusion: SIF为构建具备心理学依据的主动式治疗型大模型提供了新范式，标志着LLM从‘回应’走向‘引导’的关键转变。

Abstract: Proactive questioning, where therapists deliberately initiate structured, cognition-guiding inquiries, is a cornerstone of cognitive behavioral therapy (CBT). Yet, current psychological large language models (LLMs) remain overwhelmingly reactive, defaulting to empathetic but superficial responses that fail to surface latent beliefs or guide behavioral change. To bridge this gap, we propose the \textbf{Socratic Inquiry Framework (SIF)}, a lightweight, plug-and-play therapeutic intent planner that transforms LLMs from passive listeners into active cognitive guides. SIF decouples \textbf{when to ask} (via Strategy Anchoring) from \textbf{what to ask} (via Template Retrieval), enabling context-aware, theory-grounded questioning without end-to-end retraining. Complementing SIF, we introduce \textbf{Socratic-QA}, a high-quality dataset of strategy-aligned Socratic sequences that provides explicit supervision for proactive reasoning. Experiments show that SIF significantly enhances proactive questioning frequency, conversational depth, and therapeutic alignment, marking a clear shift from reactive comfort to proactive exploration. Our work establishes a new paradigm for psychologically informed LLMs: not just to respond, but to guide.

</details>


### [384] [SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia](https://arxiv.org/abs/2602.01618)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 本文提出了一种新型的智能体数据生成框架，用于大规模构建面向东南亚（SEA）文化背景的安全数据集，并基于此推出了首个扎根于SEA文化语境的多语言安全防护模型系列SEA-Guard，在区域敏感内容识别上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全防护模型多依赖英文数据集的机器翻译，难以捕捉区域与文化细微差异；而构建大规模、文化本地化的数据集又受限于资源与母语标注者稀缺。

Method: 提出一种新型智能体（agentic）数据生成框架，可规模化生成真实、区域特定的东南亚安全数据集；并基于该数据集训练多语言安全防护模型SEA-Guard系列。

Result: SEA-Guard在多个基准测试及文化变体评估中，持续优于现有安全模型，尤其在识别地区敏感或有害内容方面表现突出，同时保持良好的通用安全性。

Conclusion: 文化感知的安全防护需以本地化数据为基础；本文验证了智能体驱动的数据生成范式可有效支撑区域性、多语言AI对齐，为其他文化区域提供了可复用的方法论。

Abstract: Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.

</details>


### [385] [A2Eval: Agentic and Automated Evaluation for Embodied Brain](https://arxiv.org/abs/2602.01640)
*Shuai Zhang,Jiayu Hu,Zijie Chen,Zeyuan Ding,Yi Zhang,Yingji Zhang,Ziyi Zhou,Junwei Liao,Shengjie Zhou,Yong Dai,Zhenzhong Lan,Xiaozhu Ju*

Main category: cs.CL

TL;DR: 本文提出Agentic Automatic Evaluation (A2Eval)，一种基于双智能体（Data Agent与Eval Agent）的自动化评估框架，用于解决当前具身视觉语言模型（VLM）评估中人工标注基准冗余、覆盖不均、成本高、效率低等问题。该方法大幅压缩评估集、降低成本与耗时，同时保持高质量评估与人类判断高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有具身VLM评估依赖静态、专家定义、人工标注的基准，存在严重冗余和覆盖不平衡问题，导致资源消耗大、成本高、模型排名失真，阻碍迭代开发。

Method: 提出A2Eval框架，包含两个协作智能体：Data Agent负责自主归纳能力维度并构建平衡紧凑的评估集；Eval Agent负责合成并验证可执行的评估流程，实现全自动、高保真评估。

Result: 在10个基准、13个模型上验证，A2Eval将评估集压缩85%，计算成本降低77%，速度提升4.6倍，同时保持评估质量；校正系统性排序偏差，与人类判断Spearman相关性达0.85，Kendall's tau为0.81。

Conclusion: A2Eval建立了高保真、低成本具身评估的新标准，为具身智能模型的高效、可靠评估提供了可扩展的自动化范式。

Abstract: Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.

</details>


### [386] [Steering Vector Fields for Context-Aware Inference-Time Control in Large Language Models](https://arxiv.org/abs/2602.01654)
*Jiaqian Li,Yanshu Li,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: 本文提出Steering Vector Fields (SVF)，通过学习一个可微的概念评分函数，利用其局部梯度定义每个隐藏激活状态下的动态引导方向，从而解决传统静态Steering Vectors在不同上下文中方向不一致导致的不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 传统Steering Vectors（SVs）在推理时控制大语言模型存在可靠性差的问题，如概念不可引导、对部分输入适得其反、长文本生成和多属性引导效果下降，根源在于其使用全局固定向量，忽略了概念优化方向随上下文变化的几何本质。

Method: 提出Steering Vector Fields（SVF），建模为一个可微的概念评分函数，其在每个隐藏激活处的梯度即为该位置的引导方向；支持跨层联合干预、统一的对齐概念空间，并适用于长文本生成与多属性协同引导。

Result: 在多个大语言模型和多种引导任务上，SVF显著提升了引导效果的强度与可靠性，增强了推理时引导的实用性。

Conclusion: SVF通过引入上下文感知的、基于梯度的动态引导机制，从几何角度解决了静态Steering Vectors的根本缺陷，为高效、鲁棒的推理时控制提供了新范式。

Abstract: Steering vectors (SVs) offer a lightweight way to control large language models (LLMs) at inference time by shifting hidden activations, providing a practical middle ground between prompting and fine-tuning. Yet SVs can be unreliable in practice. Some concepts are unsteerable, and even when steering helps on average it can backfire for a non-trivial fraction of inputs. Reliability also degrades in long-form generation and multi-attribute steering. We take a geometric view of these failures. A static SV applies the same update vector everywhere in representation space, implicitly assuming that the concept-improving direction is constant across contexts. When the locally effective direction varies with the current activation, a single global vector can become misaligned, which yields weak or reversed effects. Guided by this perspective, we propose Steering Vector Fields (SVF), which learns a differentiable concept scoring function whose local gradient defines the steering direction at each activation, making interventions explicitly context-dependent. This formulation supports coordinated multi-layer interventions in a shared, aligned concept space, and enables efficient long-form and multi-attribute control within a unified framework. Across multiple LLMs and steering tasks, SVF delivers stronger and more reliable control, improving the practicality of inference-time steering.

</details>


### [387] [CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation](https://arxiv.org/abs/2602.01660)
*Zhongyuan Peng,Caijun Xu,Changyi Xiao,Shibo Hong,Eli Zhang,Stephen Huang,Yixin Cao*

Main category: cs.CL

TL;DR: 本文提出CoDiQ框架，实现可控难度的竞赛级问题生成，提升大推理模型训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动题目生成方法缺乏精确难度控制、计算成本高、难以大规模生成竞赛级题目。

Method: 提出CoDiQ框架，基于Qwen3-8B构建CoDiQ-Generator，利用测试时缩放机制实现细粒度难度调控，并保证题目可解性；构建含44K题目的CoDiQ-Corpus。

Result: 人类评估显示CoDiQ生成题目难度显著高于LiveCodeBench/AIME且可解率达82%以上；在CoDiQ-Corpus上训练的大推理模型推理性能显著提升。

Conclusion: 可控难度的高质量题目生成能有效增强大推理模型的推理能力，CoDiQ为相关研究提供了开源数据与工具支持。

Abstract: Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.

</details>


### [388] [Scaling Search-Augmented LLM Reasoning via Adaptive Information Control](https://arxiv.org/abs/2602.01672)
*Siheng Xiong,Oguzhan Gungordu,Blair Johnson,James C. Kerce,Faramarz Fekri*

Main category: cs.CL

TL;DR: 本文提出DeepControl框架，通过信息效用度量实现对检索过程的自适应控制，显著提升搜索增强推理代理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖基于结果的强化学习，难以有效调控信息获取过程，导致冗余证据、上下文饱和和学习不稳定。

Method: 提出基于信息效用（衡量当前推理状态下新增证据的边际价值）的DeepControl框架，引入检索延续控制与粒度控制机制，并采用退火策略使代理在训练中内化高效信息获取行为。

Result: 在七个基准测试中一致优于强基线：相比基于结果的RL基线，在Qwen2.5-7B和Qwen2.5-3B上平均提升9.4%和8.6%；且持续优于无检索及未显式控制信息的检索增强方法。

Conclusion: 自适应信息控制对扩展搜索增强推理代理以应对复杂真实信息环境至关重要。

Abstract: Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.

</details>


### [389] [Counting Hypothesis: Potential Mechanism of In-Context Learning](https://arxiv.org/abs/2602.01687)
*Jung H. Lee,Sujith Vijayan*

Main category: cs.CL

TL;DR: 本文提出了ICL的'计数假设'，认为大语言模型（LLMs）通过编码策略支持上下文学习（ICL），并提供了相关证据。


<details>
  <summary>Details</summary>
Motivation: ICL机制尚不明确，导致错误诊断与修正困难，亟需深入理解其局限性及LLMs如何支持ICL。

Method: 基于ICL特性与LLMs功能模块，提出'计数假设'，并提供支持性证据。

Result: 提出并初步验证了LLMs通过编码策略实现ICL的'计数假设'。

Conclusion: 该假设为理解ICL内在机制提供了新视角，有助于未来改进ICL的鲁棒性与可解释性。

Abstract: In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a wide range of tasks/functions with a few examples demonstrating a desirable task. ICL opens up new ways to utilize LLMs in more domains, but its underlying mechanisms still remain poorly understood, making error correction and diagnosis extremely challenging. Thus, it is imperative that we better understand the limitations of ICL and how exactly LLMs support ICL. Inspired by ICL properties and LLMs' functional modules, we propose 1the counting hypothesis' of ICL, which suggests that LLMs' encoding strategy may underlie ICL, and provide supporting evidence.

</details>


### [390] [Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models](https://arxiv.org/abs/2602.01698)
*Wenhui Tan,Fiorenzo Parascandolo,Enver Sangineto,Jianzhong Ju,Zhenbo Luo,Qian Cao,Rita Cucchiara,Ruihua Song,Jian Luan*

Main category: cs.CL

TL;DR: 本文发现大型推理模型（LRMs）经强化学习后训后出现探索崩溃现象，即温度采样无法提升pass@n准确率；为此提出无需训练的深度条件解码策略LED，通过聚合中间层高熵分布提升多步推理性能。


<details>
  <summary>Details</summary>
Motivation: 现代推理后训练导致温度采样失效（探索崩溃），且最终层后验熵显著降低而中间层熵仍较高，存在熵不对称现象。

Method: 提出Latent Exploration Decoding（LED）：基于深度条件的解码策略，对中间层后验进行累积求和，并选择熵最大的深度配置作为探索候选。

Result: LED在多个推理基准和模型上一致提升pass@1和pass@16准确率，分别提高0.61和1.03个百分点，且无需额外训练或参数。

Conclusion: 中间层蕴含未被利用的探索能力，LED通过显式利用中间层高熵分布可有效缓解后训练引发的探索崩溃问题。

Abstract: Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.

</details>


### [391] [Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory](https://arxiv.org/abs/2602.01708)
*Langyuan Cui,Chun Kai Ling,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本文提出Game of Thought (GoT)框架，利用博弈论技术近似纳什均衡策略，以提升大语言模型在信息检索任务中的最坏情况性能，通过Twenty Questions游戏及其对抗性变体Strategic Language Search (SLS)问题进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有增强大语言模型主动信息检索能力的方法常依赖简化假设，导致最坏情况性能下降，在高风险应用中存在严重隐患。

Method: 引入并形式化对抗性信息检索问题Strategic Language Search (SLS)为二人零和扩展式博弈，并提出Game of Thought (GoT)框架，采用博弈论技术近似受限变体的纳什均衡策略。

Result: 实验表明，GoT在所有测试设置下均显著优于直接提示法和启发式搜索法，持续提升最坏情况性能。

Conclusion: GoT框架能有效增强LLMs在信息缺失场景下的鲁棒性信息寻求能力，尤其适用于对最坏情况性能要求高的实际应用。

Abstract: Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.

</details>


### [392] [ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation](https://arxiv.org/abs/2602.01709)
*Xingshan Zeng,Lingzhi Wang,Weiwen Liu,Liangyou Li,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: 本文提出了一种面向智能体场景的风险感知测试时扩展框架ARTIS，通过迭代式模拟交互实现探索与执行解耦，在不增加真实环境风险的前提下提升动作级可靠性；并设计了聚焦失败模式的‘风险感知工具模拟器’以弥补传统LLM模拟器在罕见高影响故障建模上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展技术在智能体场景中效果有限，因为其动作直接影响外部环境，错误可能不可逆且代价高昂。

Method: 提出ARTIS框架：1）通过迭代式模拟交互实现探索与真实执行解耦；2）设计风险感知工具模拟器，利用针对性数据生成和重平衡训练提升对失败诱导动作的建模保真度。

Result: 在多轮、多步智能体基准上验证了迭代模拟显著提升智能体可靠性，且风险感知模拟对跨模型和任务稳定提升性能至关重要。

Conclusion: 解耦探索与执行、并增强对高风险失败模式的建模能力，是提升智能体在真实环境中鲁棒性的关键路径。

Abstract: Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \emph{\name}, \emph{\underline{A}gentic \underline{R}isk-Aware \underline{T}est-Time Scaling via \underline{I}terative \underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.

</details>


### [393] [MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark](https://arxiv.org/abs/2602.01714)
*Mouath Abu-Daoud,Leen Kharouf,Omar El Hajj,Dana El Samad,Mariam Al-Omari,Jihad Mallat,Khaled Saleh,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 本文介绍了MedAraBench，一个大规模阿拉伯语医学多选题数据集，旨在填补阿拉伯语在医疗NLP领域的资源空白，并评估开源与专有大语言模型在该领域的表现。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语在自然语言处理尤其是医疗应用中严重缺乏开源数据和基准，限制了多语言大模型能力的评估与发展。

Method: 通过人工数字化阿拉伯地区医学专业人士编写的学术资料构建MedAraBench数据集，经预处理后划分为训练集和测试集，并采用专家人工评估和LLM-as-a-judge两种框架评估数据质量。

Result: 数据集覆盖19个医学专科和5个难度等级，具有高多样性与高质量；对8个主流开源与专有模型（如GPT-5、Gemini 2.0 Flash、Claude 4-Sonnet）的评测揭示其在阿拉伯语医学任务上仍需领域特化增强。

Conclusion: MedAraBench将推动阿拉伯语医疗NLP研究，提升大语言模型的多语言临床适用性；作者已开源数据集与评估脚本，以拓展医疗基准多样性及多语言评估体系。

Abstract: Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.

</details>


### [394] [Mechanistic Indicators of Steering Effectiveness in Large Language Models](https://arxiv.org/abs/2602.01716)
*Mehdi Jafari,Hao Xue,Flora Salim*

Main category: cs.CL

TL;DR: 本文研究了基于激活的引导（activation-based steering）在大语言模型中的可靠性机制，提出利用信息论指标（如归一化分支因子NBF和KL散度）来预测引导成功与否，并验证了这些内部信号对引导效果具有预测能力。


<details>
  <summary>Details</summary>
Motivation: 尽管激活引导被广泛应用，但其成功或失败的内在机制尚不清楚，以往研究多依赖黑箱输出或LLM评判，缺乏对模型内部信号的机理分析。

Method: 采用两种信息论度量：基于熵的归一化分支因子（NBF）和词表空间中引导激活与目标概念间的KL散度；以两个不同架构LLM生成的标注为真值，评估这些信号对引导可靠性的预测能力。

Result: 发现NBF和KL散度能有效预测引导成败，具备统计显著的预测力；同时为对比激活添加（CAA）和稀疏自编码器引导提供了更强的评估基线。

Conclusion: 模型内部的信息论信号可作为诊断激活引导可靠性的有效代理指标，为无需重训练的行为控制提供可解释、可预测的机制基础。

Abstract: Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theoretic measures: the entropy-derived Normalized Branching Factor (NBF), and the Kullback-Leibler (KL) divergence between steered activations and targeted concepts in the vocabulary space. We hypothesize that effective steering corresponds to structured entropy preservation and coherent KL alignment across decoding steps. Building on a reliability study demonstrating high inter-judge agreement between two architecturally distinct LLMs, we use LLM-generated annotations as ground truth and show that these mechanistic signals provide meaningful predictive power for identifying successful steering and estimating failure probability. We further introduce a stronger evaluation baseline for Contrastive Activation Addition (CAA) and Sparse Autoencoder-based steering, the two most widely adopted activation-steering methods.

</details>


### [395] [BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition](https://arxiv.org/abs/2602.01717)
*Hyunsik Kim,Haeri Kim,Munhak Lee,Kyungmin Lee*

Main category: cs.CL

TL;DR: 本文提出BBPE16，一种基于UTF-16的字节级BPE分词器，用于多语言ASR，相比主流UTF-8 BBPE，在保持语言无关性的同时减少CJK等语言的token数量和解码迭代次数，提升训练与推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有UTF-8字节级BPE（BBPE）在处理中文、日文、韩文等非拉丁语系时因变长编码导致token序列过长，增加计算与内存开销。

Method: 提出基于UTF-16的BBPE16分词器，使多数现代文字统一用2字节表示，保留BBPE语言无关性并增强跨语言token共享。

Result: 在单语、双语、三语及多语言持续学习ASR任务中，BBPE16达到同等或更优准确率；对中文，token数最多减少10.4%，解码迭代最多降低10.3%，加速微调与推理并降低内存占用。

Conclusion: BBPE16是一种高效、实用的多语言ASR分词方案，在维持性能的同时显著提升计算效率。

Abstract: Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.

</details>


### [396] [COMI: Coarse-to-fine Context Compression via Marginal Information Gain](https://arxiv.org/abs/2602.01719)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Yujin Yuan,Libin Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文提出COMI框架，通过粗粒度分组重分配和细粒度令牌合并两阶段，结合边际信息增益（MIG）指标，在高压缩率下兼顾语义相关性与多样性，显著提升长上下文场景下LLM的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文场景中面临计算低效和信息冗余问题，亟需高效且保质的上下文压缩方法。

Method: 提出COMI——一种粗到细自适应上下文压缩框架；定义边际信息增益（MIG）作为核心指标；包含两个阶段：（1）基于组间MIG的粗粒度分组重分配，动态分配压缩预算；（2）基于组内MIG加权的细粒度令牌合并。

Result: 在NaturalQuestions等问答数据集及MultiNews摘要任务上，COMI在32倍压缩下相比基线提升约25个Exact Match点（Qwen2-7B），显著优于现有方法。

Conclusion: COMI通过联合优化相关性与多样性，在高倍压缩下仍能有效保留关键语义，为长上下文LLM部署提供了高效、鲁棒的压缩方案。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.

</details>


### [397] [SafePred: A Predictive Guardrail for Computer-Using Agents via World Models](https://arxiv.org/abs/2602.01725)
*Yurun Chen,Zeyi Liao,Ping Yin,Taotao Xie,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: 本文提出SafePred，一种面向计算机使用代理（CUAs）的预测性防护框架，通过构建风险-决策闭环，实现对短/长期风险的联合预测与基于风险的决策优化，显著提升安全性与任务效用。


<details>
  <summary>Details</summary>
Motivation: 现有CUA防护机制多为反应式，仅基于当前观测约束行为，无法识别和规避具有延迟效应的长期风险（如清理日志导致未来审计不可追溯）。

Method: 提出预测性防护范式，设计SafePred框架：1）以安全策略为依据、利用世界模型进行短/长期风险语义预测；2）将风险预测转化为可执行的干预措施，包括步骤级干预与任务级重规划。

Result: 实验表明，SafePred相较反应式基线，高风险行为显著降低，安全性能达97.6%以上，任务效用最高提升21.4%。

Conclusion: 预测性防护是提升CUA长期安全性的有效路径；SafePred通过风险预测与决策优化的协同，实现了安全性与实用性兼顾。

Abstract: With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.

</details>


### [398] [Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training](https://arxiv.org/abs/2602.01747)
*Hongseok Choi,Serynn Kim,Wencke Liermann,Jin Seong,Jin-Xia Huang*

Main category: cs.CL

TL;DR: 本文提出了一种在标注数据稀缺场景下提升自动作文评分（AES）性能的新方法，包含两阶段微调、分数对齐和不确定性感知的自训练三项关键技术，并在ASAP++数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实教育场景中标注数据极度稀缺，严重制约了鲁棒AES系统的开发与落地应用。

Method: 提出三项技术：1）基于低秩适配的两阶段微调策略；2）提升预测与真实分数分布一致性的分数对齐技术；3）利用无标签数据的不确定性感知自训练。所有技术均基于DualBERT实现。

Result: 在仅32个标注样本的极端稀疏设置下，集成三项技术达到全量数据（约1000样本）性能的91.2%；分数对齐技术在全量数据设置下亦达SOTA。

Conclusion: 所提方法显著缓解了标注数据稀缺问题，在有限数据和全量数据场景下均有效提升AES性能，具备强实用性与泛化性。

Abstract: Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.

</details>


### [399] [WorldCup Sampling for Multi-bit LLM Watermarking](https://arxiv.org/abs/2602.01752)
*Yidan Wang,Yubing Ren,Yanan Cao,Li Guo*

Main category: cs.CL

TL;DR: 本文提出WorldCup，一种面向大语言模型的多比特水印框架，通过分层竞争机制与熵感知调制，在保持文本质量的同时实现高容量、强鲁棒性与高效解码的水印嵌入与提取。


<details>
  <summary>Details</summary>
Motivation: 现有零比特水印方法扩展至多比特时存在信息流间接、有效容量低、解码次优等问题，亟需更直接高效的多比特水印方案。

Method: 提出WorldCup框架：将采样视为通信信道，利用互补信号引导的分层竞争机制直接在token选择中嵌入比特；引入熵感知调制以维持生成质量；设计置信度感知解码实现鲁棒消息恢复。

Result: WorldCup在容量、可检测性、鲁棒性、文本质量与解码效率方面取得良好平衡，全面优于现有基线方法。

Conclusion: WorldCup为大语言模型多比特水印提供了新范式，奠定了未来LLM水印研究的坚实基础。

Abstract: As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steering, leading to indirect information flow, limited effective capacity, and suboptimal decoding. In this paper, we propose WorldCup, a multi-bit watermarking framework for LLMs that treats sampling as a natural communication channel and embeds message bits directly into token selection via a hierarchical competition mechanism guided by complementary signals. Moreover, WorldCup further adopts entropy-aware modulation to preserve generation quality and supports robust message recovery through confidence-aware decoding. Comprehensive experiments show that WorldCup achieves a strong balance across capacity, detectability, robustness, text quality, and decoding efficiency, consistently outperforming prior baselines and laying a solid foundation for future LLM watermarking studies.

</details>


### [400] [Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings](https://arxiv.org/abs/2602.01757)
*Doohyun Kim,Donghwa Kang,Kyungjae Lee,Hyeongboo Baek,Brent Byunghoon Kang*

Main category: cs.CL

TL;DR: 本文提出Zero2Text，一种无需训练的递归在线对齐框架，用于在黑盒和跨域场景下高效抵御向量数据库中的嵌入逆向攻击，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化或对齐的嵌入逆向攻击防御方法在严格黑盒和跨域设置下效果不佳：前者计算开销大，后者依赖不可行的领域内训练数据。

Method: 提出Zero2Text框架，结合大语言模型先验与动态岭回归机制，在线递归地将生成文本对齐到目标嵌入，无需训练且不依赖静态数据集。

Result: 在MS MARCO等基准上，Zero2Text对OpenAI受害模型的逆向恢复效果显著提升：ROUGE-L提高1.8倍，BLEU-2提高6.4倍，且无需任何泄露的数据对即可跨域恢复句子。

Conclusion: Zero2Text有效突破了现有防御范式的局限，证明标准差分隐私等防御手段难以应对此类自适应威胁，为RAG系统中向量数据库的隐私保护提供了新路径。

Abstract: The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.

</details>


### [401] [<SOG_k>: One LLM Token for Explicit Graph Structural Understanding](https://arxiv.org/abs/2602.01771)
*Jingyao Wu,Bin Lu,Zijun Di,Xiaoying Gan,Meng Jin,Luoyi Fu,Xinbing Wang,Chenghu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为<SOG_k>的特殊结构化图表示方法，通过拓扑感知的结构分词器将图结构映射为单个token，并构建混合结构问答语料进行对齐，显著提升大语言模型对图数据的理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理图结构数据时存在结构幻觉问题，现有方法要么将图转为自然语言（导致token开销大、注意力分散），要么用连续嵌入（与文本token严重错位）。

Method: 引入特殊token <SOG_k> 表示图结构；设计拓扑感知结构分词器，将图拓扑映射为高选择性单token；构建混合结构问答语料对齐新结构token与文本token。

Result: 在五个图级基准上性能提升9.9%–41.4%，兼具可解释性与一致性；并可灵活扩展至节点级任务，支持全局与局部结构理解。

Conclusion: 该方法有效缓解了LLM处理图数据时的结构幻觉问题，实现了结构信息在统一token空间中的显式建模与共享，为图-文本联合建模提供了新范式。

Abstract: Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SOG_k> to fully represent the Structure Of Graph within a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SOG_k> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9% to 41.4% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available at https://github.com/Jingyao-Wu/SOG.

</details>


### [402] [Data Distribution Matters: A Data-Centric Perspective on Context Compression for Large Language Model](https://arxiv.org/abs/2602.01778)
*Kangtao Lv,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Shilei Liu,Yongwei Wang,Yujin Yuan,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文从数据为中心的视角，首次系统研究了数据分布（包括输入数据和模型内在预训练知识）对长上下文LLM压缩质量的影响，并提出了基于自编码器的评估框架。实验发现输入熵与压缩质量呈负相关，而编解码器间内在数据差异会显著削弱压缩增益。


<details>
  <summary>Details</summary>
Motivation: 现有研究只关注模型侧改进，忽略了数据分布本身对上下文压缩的影响，因此需要从数据-centric视角填补这一空白。

Method: 采用自编码器框架评估压缩表示的语义完整性，从输入数据和内在数据两个维度分析数据分布对压缩质量的影响。

Result: 1）编码器测量的输入熵与压缩质量呈负相关，解码器测量的熵在冻结解码器设置下无显著关系；2）编码器与解码器内在数据差距越大，压缩增益越小且难以缓解。

Conclusion: 数据分布对压缩质量有关键影响，尤其输入熵和编解码器内在知识一致性是决定压缩效果的重要因素，并据此提出优化压缩增益的实用指南。

Abstract: The deployment of Large Language Models (LLMs) in long-context scenarios is hindered by computational inefficiency and significant information redundancy. Although recent advancements have widely adopted context compression to address these challenges, existing research only focus on model-side improvements, the impact of the data distribution itself on context compression remains largely unexplored. To bridge this gap, we are the first to adopt a data-centric perspective to systematically investigate how data distribution impacts compression quality, including two dimensions: input data and intrinsic data (i.e., the model's internal pretrained knowledge). We evaluate the semantic integrity of compressed representations using an autoencoder-based framework to systematically investigate it. Our experimental results reveal that: (1) encoder-measured input entropy negatively correlates with compression quality, while decoder-measured entropy shows no significant relationship under a frozen-decoder setting; and (2) the gap between intrinsic data of the encoder and decoder significantly diminishes compression gains, which is hard to mitigate. Based on these findings, we further present practical guidelines to optimize compression gains.

</details>


### [403] [CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding](https://arxiv.org/abs/2602.01785)
*Yuling Shi,Chaoxiang Xie,Zhensu Sun,Yeheng Chen,Chenxu Zhang,Longfei Yun,Chengcheng Wan,Hongyu Zhang,David Lo,Xiaodong Gu*

Main category: cs.CL

TL;DR: 本文首次系统研究了多模态大语言模型（MLLMs）在源代码理解中的应用，提出将代码渲染为图像以实现高效压缩（最高8倍），同时保持甚至提升部分任务性能（如代码补全、克隆检测），探索了图像模态替代文本模态以降低计算开销的新路径。


<details>
  <summary>Details</summary>
Motivation: 随着软件规模增长，基于纯文本token序列的LLM代码理解面临上下文长度线性增长、计算成本高的瓶颈；而MLLMs兴起为利用图像模态的天然可压缩性提供了新机遇。

Method: 将源代码渲染为带语法高亮的图像，系统评估不同压缩比（分辨率调整）下MLLMs在代码理解任务（如代码补全、克隆检测）上的性能，并与原始文本输入对比。

Result: （1）MLLMs在高达8倍token压缩下仍有效理解代码；（2）利用语法高亮等视觉线索，在4倍压缩下代码补全性能提升；（3）克隆检测任务对视觉压缩极鲁棒，某些压缩比下性能略超原始文本。

Conclusion: 图像模态是提升代码理解效率的可行新范式，但当前MLLMs仍有局限；研究指明了向图像化代码表示演进的方向。

Abstract: Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.

</details>


### [404] [Sentence Curve Language Models](https://arxiv.org/abs/2602.01807)
*DongNyeong Heo,Heelyoul Choi*

Main category: cs.CL

TL;DR: 本文提出了一种新的语言模型SCLM，通过引入连续的句子表示——句子曲线（sentence curve），替代传统静态词嵌入，以增强对句子全局结构的建模能力，并在多个基准上取得DLM领域的SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型（包括扩散语言模型DLMs）使用静态词嵌入表示目标词，忽视上下文邻近词的影响，导致局部预测准确但全局句子结构建模不足。

Method: 提出‘句子曲线’作为连续句子表示，其控制点影响句子中多个词；在此基础上构建句子曲线语言模型（SCLM），扩展DLMs以预测句子曲线而非静态词嵌入，并从理论上分析其正则化效应及不同曲线类型的影响。

Result: SCLM在IWSLT14和WMT14数据集上达到DLM领域的SOTA性能；训练稳定，无需繁琐的知识蒸馏；在LM1B上表现优于离散DLMs。

Conclusion: 连续句子表示（句子曲线）能有效提升扩散语言模型对全局句子结构的建模能力，是一种有前景的替代静态词嵌入的新范式。

Abstract: Language models (LMs) are a central component of modern AI systems, and diffusion-based language models (DLMs) have recently emerged as a competitive alternative. Both paradigms rely on word embeddings not only to represent the input sentence, but also to represent the target sentence that backbone models are trained to predict. We argue that such static embedding of the target word is insensitive to neighboring words, encouraging locally accurate word prediction while neglecting global structure across the target sentence. To address this limitation, we propose a continuous sentence representation, termed sentence curve, defined as a spline curve whose control points affect multiple words in the sentence. Based on this representation, we introduce sentence curve language model (SCLM), which extends DLMs to predict sentence curves instead of the static word embeddings. We theoretically show that sentence curve prediction induces a regularization effect that promotes global structure modeling, and characterize how different sentence curve types affect this behavior. Empirically, SCLM achieves SOTA performance among DLMs on IWSLT14 and WMT14, shows stable training without burdensome knowledge distillation, and demonstrates promising potential compared to discrete DLMs on LM1B.

</details>


### [405] [AXE: Low-Cost Cross-Domain Web Structured Information Extraction](https://arxiv.org/abs/2602.01838)
*Abdelrahman Mansour,Khaled W. Alshaer,Moataz Elsaban*

Main category: cs.CL

TL;DR: AXE是一种新型网页结构化数据提取方法，通过将HTML DOM视为需剪枝的树结构，结合专用剪枝机制和可追溯的XPath解析（GXR），使小型0.6B LLM即可实现高精度零样本抽取，在SWDE上F1达88.1%，优于多个更大模型。


<details>
  <summary>Details</summary>
Motivation: 解决网页结构化数据提取中手工规则易失效与大语言模型成本过高的矛盾。

Method: 提出AXE管道：将HTML DOM视为树结构进行剪枝以去除冗余节点；引入Grounded XPath Resolution（GXR）确保每项提取结果可追溯至源DOM节点；使用仅0.6B参数的轻量级LLM生成结构化输出。

Result: 在SWDE数据集上零样本F1达88.1%，超越多个更大规模、全监督训练的模型；开源专用适配器，支持低成本大规模部署。

Conclusion: AXE证明了轻量模型+结构感知预处理+可验证定位机制可在不牺牲精度前提下大幅降低Web信息抽取成本与复杂度。

Abstract: Extracting structured data from the web is often a trade-off between the brittle nature of manual heuristics and the prohibitive cost of Large Language Models. We introduce AXE (Adaptive X-Path Extractor), a pipeline that rethinks this process by treating the HTML DOM as a tree that needs pruning rather than just a wall of text to be read. AXE uses a specialized "pruning" mechanism to strip away boilerplate and irrelevant nodes, leaving behind a distilled, high-density context that allows a tiny 0.6B LLM to generate precise, structured outputs. To keep the model honest, we implement Grounded XPath Resolution (GXR), ensuring every extraction is physically traceable to a source node. Despite its low footprint, AXE achieves state-of-the-art zero-shot performance, outperforming several much larger, fully-trained alternatives with an F1 score of 88.1% on the SWDE dataset. By releasing our specialized adaptors, we aim to provide a practical, cost-effective path for large-scale web information extraction.

</details>


### [406] [Read As Human: Compressing Context via Parallelizable Close Reading and Skimming](https://arxiv.org/abs/2602.01840)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Qingsong Lv,Runsong Zhao,Tingwei Lu,Langming Liu,Haibin Chen,Yujin Yuan,Hai-Tao Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: RAM是一种受人类阅读行为启发的上下文压缩框架，通过自适应混合阅读策略（精读重要段落+略读不相关段落）提升大语言模型在长文本场景下的效率与性能，并引入对比学习优化阅读决策边界。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长上下文场景中面临的计算低效和信息冗余两大挑战。

Method: 提出RAM框架，将输入上下文分段并行编码；依据与查询的相关性，对高相关段落精读保留，对低相关段落进行查询引导的向量压缩（略读）；融合显式文本段与隐式摘要向量送入解码器；引入基于正负样本对的对比学习目标以优化精读/略读决策边界。

Result: 在多个问答和摘要基准上超越现有基线方法，在两种骨干模型上均取得更好性能；在平均长度16K、最大32K的长输入上实现最高12倍端到端加速。

Conclusion: RAM通过模拟人类阅读机制有效平衡了长上下文处理中的效率与效果，兼具高性能、高可解释性和显著加速能力。

Abstract: Large Language Models (LLMs) demonstrate exceptional capability across diverse tasks. However, their deployment in long-context scenarios is hindered by two challenges: computational inefficiency and redundant information. We propose RAM (Read As HuMan), a context compression framework that adopts an adaptive hybrid reading strategy, to address these challenges. Inspired by human reading behavior (i.e., close reading important content while skimming less relevant content), RAM partitions the context into segments and encodes them with the input query in parallel. High-relevance segments are fully retained (close reading), while low-relevance ones are query-guided compressed into compact summary vectors (skimming). Both explicit textual segments and implicit summary vectors are concatenated and fed into decoder to achieve both superior performance and natural language format interpretability. To refine the decision boundary between close reading and skimming, we further introduce a contrastive learning objective based on positive and negative query-segment pairs. Experiments demonstrate that RAM outperforms existing baselines on multiple question answering and summarization benchmarks across two backbones, while delivering up to a 12x end-to-end speedup on long inputs (average length 16K; maximum length 32K).

</details>


### [407] [PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning](https://arxiv.org/abs/2602.01875)
*Langming Liu,Kangtao Lv,Haibin Chen,Weidong Zhang,Yejing Wang,Shilei Liu,Xin Tong,Yujin Yuan,Yongwei Wang,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 本文提出PretrainRL框架，通过在预训练阶段引入强化学习来缓解大语言模型的事实幻觉问题，核心思想是'去偏置再学习'，即降低高概率错误答案的权重，为低概率正确答案腾出学习空间。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在事实幻觉问题，根源在于预训练语料中数据分布不均衡，导致'低概率真'和'高概率假'的状态。

Method: 提出PretrainRL框架，在预训练阶段集成强化学习；采用'去偏置再学习'原则，设计高效负采样策略识别高概率错误，并引入新指标评估模型对事实知识的概率状态。

Result: 在三个公开基准上的大量实验表明，PretrainRL显著缓解事实幻觉，性能优于当前最优方法。

Conclusion: 从预训练源头入手，通过强化学习进行概率分布校正，是解决大语言模型事实幻觉问题的有效新路径。

Abstract: Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of "low-probability truth" and "high-probability falsehood". Recent approaches, such as teaching models to say "I don't know" or post-hoc knowledge editing, either evade the problem or face catastrophic forgetting. To address this issue from its root, we propose \textbf{PretrainRL}, a novel framework that integrates reinforcement learning into the pretraining phase to consolidate factual knowledge. The core principle of PretrainRL is "\textbf{debiasing then learning}." It actively reshapes the model's probability distribution by down-weighting high-probability falsehoods, thereby making "room" for low-probability truths to be learned effectively. To enable this, we design an efficient negative sampling strategy to discover these high-probability falsehoods and introduce novel metrics to evaluate the model's probabilistic state concerning factual knowledge. Extensive experiments on three public benchmarks demonstrate that PretrainRL significantly alleviates factual hallucinations and outperforms state-of-the-art methods.

</details>


### [408] [ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support](https://arxiv.org/abs/2602.01885)
*Tiantian Chen,Jiaqi Lu,Ying Shen,Lin Zhang*

Main category: cs.CL

TL;DR: 本文提出ES-MemEval基准和EvoEmo数据集，用于评估大语言模型在长期情感支持对话中的五种核心记忆能力，并发现显式长时记忆对减少幻觉和实现个性化至关重要，而RAG在时间动态建模上仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有长期对话基准主要关注静态、显式的事实检索，无法评估用户信息分散、隐含且持续演化的关键场景，尤其在在线情感支持等复杂长期服务中。

Method: 构建ES-MemEval基准（涵盖信息抽取、时序推理、冲突检测、主动拒答、用户建模五大能力）及配套多轮EvoEmo数据集（捕获碎片化、隐含的用户披露与动态演化状态），并在开源长上下文模型、商用模型和RAG模型上开展系统实验。

Result: 显式长时记忆显著降低幻觉并提升个性化效果；RAG虽增强事实一致性，但在建模时间动态与用户状态演化方面表现不足。

Conclusion: 当前范式在长期个性化对话中既有潜力也有明显局限，亟需更鲁棒地融合记忆机制与检索技术。

Abstract: Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.

</details>


### [409] [GuideWeb: A Benchmark for Automatic In-App Guide Generation on Real-World Web UIs](https://arxiv.org/abs/2602.01917)
*Chengguang Gan,Yoshihiro Tsujii,Yunhao Liang,Tatsunori Mori,Shiwen Ni,Hiroki Itoh*

Main category: cs.CL

TL;DR: 本文提出了GuideWeb基准，用于评估在真实网页UI上自动生成应用内引导的性能，并设计了GuideWeb Agent模型，在引导目标元素选择、意图生成和引导文本生成任务上取得初步进展，但仍面临较大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有数字采用平台（DAP）依赖人工持续维护引导内容，而网站频繁更新导致维护成本高，亟需自动化引导生成方法。

Method: 构建GuideWeb基准，将自动引导生成建模为：在网页中定位引导目标元素并生成符合用户意图的简洁引导文本；提出联合评估指标，涵盖目标元素选择准确率、意图生成质量（BLEU）和引导文本生成质量（BLEU）；设计GuideWeb Agent模型进行端到端引导生成。

Result: GuideWeb Agent在引导目标元素预测上达到30.79%准确率，意图生成BLEU为44.94，引导文本生成BLEU为21.34；显著优于现有基线，但整体性能仍较低，表明任务极具挑战性。

Conclusion: 自动网页引导生成是一个新颖且困难的任务，GuideWeb为该方向提供了首个面向真实Web UI的基准与评估体系，揭示了当前方法的局限性，指明了未来研究方向。

Abstract: Digital Adoption Platform (DAP) provide web-based overlays that deliver operation guidance and contextual hints to help users navigate complex websites. Although modern DAP tools enable non-experts to author such guidance, maintaining these guides remains labor-intensive because website layouts and functionalities evolve continuously, which requires repeated manual updates and re-annotation. In this work, we introduce \textbf{GuideWeb}, a new benchmark for automatic in-app guide generation on real-world web UIs. GuideWeb formulates the task as producing page-level guidance by selecting \textbf{guide target elements} grounded in the webpage and generating concise guide text aligned with user intent. We also propose a comprehensive evaluation suite that jointly measures the accuracy of guide target element selection and the quality of generated intents and guide texts. Experiments show that our proposed \textbf{GuideWeb Agent} achieves \textbf{30.79\%} accuracy in guide target element prediction, while obtaining BLEU scores of \textbf{44.94} for intent generation and \textbf{21.34} for guide-text generation. Existing baselines perform substantially worse, which highlights that automatic guide generation remains challenging and that further advances are necessary before such systems can be reliably deployed in real-world settings.

</details>


### [410] [From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted "Vibe Coding"](https://arxiv.org/abs/2602.01919)
*Hend Al-Khalifa*

Main category: cs.CL

TL;DR: 本文提出了一种名为'Vibe Coding'的教学方法，利用大语言模型（LLMs）作为编程助手，以提升NLP课程中学生的概念理解与批判性思维能力，并通过实证分析验证其有效性与挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的快速发展为NLP教育带来挑战与机遇，需探索如何在利用LLM辅助教学的同时，不削弱学生对核心概念与批判性思维的掌握。

Method: 在高年级本科NLP课程中实施'Vibe Coding'教学法：学生使用LLM完成7个编程实验，但评估重点设在基于反思性问题的概念理解上，并强制要求记录提示词（prompt logging）和进行批判性反思。

Result: 19名学生课程反馈显示高满意度（4.4–4.6/5.0），尤其认可调试负担降低带来的概念学习深化；但也指出时间紧张、LLM输出验证困难及任务说明不清等挑战。

Conclusion: 结构化设计的LLM辅助教学（含prompt logging与反思评估）可有效将学习重心从语法熟练度转向概念掌握，助力学生适应AI增强的职业环境。

Abstract: The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding and critical thinking. We describe the implementation of this approach in a senior-level undergraduate NLP course, where students completed seven labs using LLMs for code generation while being assessed primarily on conceptual understanding through critical reflection questions. Analysis of end-of-course feedback from 19 students reveals high satisfaction (mean scores 4.4-4.6/5.0) across engagement, conceptual learning, and assessment fairness. Students particularly valued the reduced cognitive load from debugging, enabling deeper focus on NLP concepts. However, challenges emerged around time constraints, LLM output verification, and the need for clearer task specifications. Our findings suggest that when properly structured with mandatory prompt logging and reflection-based assessment, LLM-assisted learning can shift focus from syntactic fluency to conceptual mastery, preparing students for an AI-augmented professional landscape.

</details>


### [411] [Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.01965)
*Kwun Hang Lau,Fangyuan Zhang,Boyu Ruan,Yingli Zhou,Qintian Guo,Ruiyuan Zhang,Xiaofang Zhou*

Main category: cs.CL

TL;DR: CatRAG 提出一种查询自适应的图遍历机制，通过符号锚定、动态边权重和关键事实增强，解决现有KG-based RAG中静态图导致的语义漂移问题，显著提升多跳推理所需的证据链完整性。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的RAG方法（如HippoRAG）依赖静态图结构和固定转移概率，忽视查询对边相关性的动态影响，导致随机游走易陷入高中心性节点，无法完整检索多跳证据链。

Method: 在HippoRAG 2基础上构建查询感知导航图：（1）符号锚定——引入弱实体约束正则化游走；（2）查询感知动态边加权——实时调整边权重以剪枝无关路径、增强意图对齐路径；（3）关键事实段落权重增强——低成本结构化锚定至潜在证据。

Result: 在四个多跳基准上一致超越SOTA；标准召回率提升有限，但在‘推理完整性’（完整恢复无缺口证据路径的能力）上取得显著提升。

Conclusion: CatRAG有效弥合了部分上下文检索与完全可验证推理之间的鸿沟，证明图结构的查询自适应建模对多跳RAG至关重要。

Abstract: Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a "Static Graph Fallacy": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree "hub" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.

</details>


### [412] [Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition](https://arxiv.org/abs/2602.01967)
*Wonjun Lee,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出Moe-Ctc模型，通过带中间CTC监督的混合专家架构，结合口音感知路由与无标签推理路由，在McV-Accent基准上显著提升ASR对已见和未见口音的鲁棒性，相对WER降低最高达29.3%。


<details>
  <summary>Details</summary>
Motivation: 现有ASR模型在多数高资源英语变体上训练，导致对其他口音识别性能严重下降；口音无关方法泛化能力有限，口音相关方法依赖稀缺且含噪的口音标签。

Method: 提出Moe-Ctc：基于混合专家（MoE）架构，引入中间CTC监督；训练时采用口音感知路由促使专家学习口音特异性模式，推理时切换为无标签路由；每个专家配备独立CTC头以对齐路由与转录质量，并设计路由增强损失稳定优化过程。

Result: 在McV-Accent基准上，Moe-Ctc在低/高资源条件下对已见和未见口音均取得一致提升，相较FastConformer基线最高实现29.3%相对WER降低。

Conclusion: Moe-Ctc有效平衡专家专业化与泛化能力，无需精确口音标签即可提升ASR对多样口音的鲁棒性，为口音自适应ASR提供了新范式。

Abstract: Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.

</details>


### [413] [Beyond Local Edits: Embedding-Virtualized Knowledge for Broader Evaluation and Preservation of Model Editing](https://arxiv.org/abs/2602.01977)
*Shuainan Liu,Xuanang Chen,Ben He,Le Sun*

Main category: cs.CL

TL;DR: 本文提出Embedding-Virtualized Knowledge（EVK）框架，通过嵌入空间扰动表征模型知识，构建EVK-Bench评估编辑引发的知识漂移，并设计EVK-Align模块抑制漂移，在不牺牲编辑准确率前提下提升知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑评估局限于有限标注样本，难以反映编辑对模型整体知识系统的广泛影响。

Method: 提出EVK方法表征嵌入空间中的虚拟化知识，构建EVK-Bench评估知识漂移，并设计可即插即用的EVK-Align模块约束编辑过程中的嵌入级知识漂移。

Result: EVK-Bench能揭示传统指标无法捕捉的知识漂移现象；EVK-Align在保持编辑准确性的同时显著提升知识保留效果。

Conclusion: EVK框架实现了更全面的知识编辑评估与控制，推动知识编辑从样本级向系统级演进。

Abstract: Knowledge editing methods for large language models are commonly evaluated using predefined benchmarks that assess edited facts together with a limited set of related or neighboring knowledge. While effective, such evaluations remain confined to finite, dataset-bounded samples, leaving the broader impact of editing on the model's knowledge system insufficiently understood. To address this gap, we introduce Embedding-Virtualized Knowledge (EVK) that characterizes model knowledge through controlled perturbations in embedding space, enabling the exploration of a substantially broader and virtualized knowledge region beyond explicit data annotations. Based on EVK, we construct an embedding-level evaluation benchmark EVK-Bench that quantifies potential knowledge drift induced by editing, revealing effects that are not captured by conventional sample-based metrics. Furthermore, we propose a plug-and-play EVK-Align module that constrains embedding-level knowledge drift during editing and can be seamlessly integrated into existing editing methods. Experiments demonstrate that our approach enables more comprehensive evaluation while significantly improving knowledge preservation without sacrificing editing accuracy.

</details>


### [414] [S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs](https://arxiv.org/abs/2602.01982)
*Yanrui Du,Sendong Zhao,Yibo Gao,Danyang Zhao,Qika Lin,Ming Ma,Jiayun Li,Yi Jiang,Kai He,Qianyi Xu,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 本文提出了一种基于激活引导的自采样框架（S3-CoT），使大语言模型能自主生成风格一致、长度可变的思维链，无需人工标注或教师模型，支持双认知系统建模与自进化训练，在数学与医学领域均取得稳定提升。


<details>
  <summary>Details</summary>
Motivation: 现有思维链（CoT）方法常引入冗余推理；受人类“快思考”（System 1）启发，探索LLM能否自主习得高效、简洁的推理模式。

Method: 提出S3-CoT框架：基于激活引导实现自采样生成可变长CoT；结合黄金答案筛选数据进行监督微调（SFT），引入类人双认知系统建模与渐进压缩课程；进一步拓展至仅依赖预测一致性数据的自进化训练范式。

Result: 在数学基准（如GSM8K、MATH）及跨领域医学任务上显著且稳定提升性能，适用于通用LLM与R1风格模型；开源数据与模型检查点。

Conclusion: LLM可通过无监督引导与自演化机制习得高效、紧凑的推理能力，验证了‘快思考’模式在LLM中的可行性，为降低CoT训练成本与提升泛化性提供了新路径。

Abstract: Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.

</details>


### [415] [From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs](https://arxiv.org/abs/2602.01999)
*Yanrui Du,Yibo Gao,Sendong Zhao,Jiayun Li,Haochun Wang,Qika Lin,Kai He,Bing Qin,Mengling Feng*

Main category: cs.CL

TL;DR: 本文通过分析R1-style大语言模型的自省行为，揭示了其内部机制：从潜在线性控制层、语义关键层到显式行为层的三层激活轨迹，并验证了各层间的因果链，类比人类元认知过程。


<details>
  <summary>Details</summary>
Motivation: R1-style大语言模型虽展现出自省能力，但其内在机制尚不明确，亟需从神经活动层面理解其自省行为的发生与发展过程。

Method: 采用logit lens技术逐层读取token级语义，识别反射行为出现的起始点并追踪其层间激活轨迹；结合针对性干预实验验证各阶段间的因果关系。

Result: 发现三层结构化激活模式：潜在线性控制层编码‘思考预算’语义、语义关键层浮现话语级转折与总结线索、显式行为层中自省token采样概率显著上升；且三者构成可干预的因果链。

Conclusion: 大语言模型的自省行为遵循类人元认知路径：由潜在线索监控，经话语层级调控，最终实现显式自我反思。

Abstract: R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.

</details>


### [416] [Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation](https://arxiv.org/abs/2602.02007)
*Zhanghao Hu,Qinglin Zhu,Hanqi Yan,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: 本文提出xMemory，一种面向Agent记忆系统的新型检索方法，通过解耦-聚合范式构建语义层级结构，替代传统基于相似度的固定top-k检索，以解决记忆流中冗余与时序依赖问题，在多个基准上提升了问答质量和token效率。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在Agent记忆场景下失效：记忆是有限、连贯、高度相关甚至重复的对话流，固定相似度检索易返回冗余内容，后处理剪枝可能删除关键时序前提。

Method: 提出xMemory框架：1）将记忆解耦为语义组件；2）构建保持完整性与可检索性的层级结构（高层节点组织+稀疏-语义目标引导拆分/合并）；3）自顶向下检索：先选主题/语义，仅当降低不确定性时才展开至具体片段或原始消息。

Result: 在LoCoMo和PerLTQA数据集、三种最新大模型上实验表明，xMemory显著提升答案质量与token利用效率。

Conclusion: Agent记忆检索应超越相似度匹配，转向基于解耦-聚合的结构化语义检索；xMemory验证了该范式的有效性与实用性。

Abstract: Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.

</details>


### [417] [NEAT: Neuron-Based Early Exit for Large Reasoning Models](https://arxiv.org/abs/2602.02010)
*Kang Liu,Yongkang Liu,Xiaocui Yang,Peidong Wang,Wen Zhang,Shi Feng,Yifei Zhang,Daling Wang*

Main category: cs.CL

TL;DR: 本文提出NEAT框架，通过监控神经元激活动态实现无需训练的早期退出，有效缓解大推理模型中的'过度思考'问题，在保持准确率的同时平均减少22%-28%的token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）常出现'过度思考'现象，即在已得出正确答案后仍生成冗余推理步骤；现有早期退出方法依赖输出层启发式或需额外计算/标注数据，存在效率与泛化瓶颈。

Method: 提出基于神经元的早期推理退出框架NEAT，通过识别与退出相关的神经元并实时跟踪其激活模式，动态触发早期退出或抑制反思，无需训练、不增加测试时计算开销。

Result: 在四个推理基准和六种不同规模与架构的模型上实验表明，NEAT对每个模型在各基准上平均实现22%–28%的token减少，同时维持原有准确率。

Conclusion: NEAT是一种高效、通用且无需训练的早期退出方法，能显著降低推理开销而不牺牲性能，为缓解大模型过度思考提供了新思路。

Abstract: Large Reasoning Models (LRMs) often suffer from \emph{overthinking}, a phenomenon in which redundant reasoning steps are generated after a correct solution has already been reached. Existing early reasoning exit methods primarily rely on output-level heuristics or trained probing models to skip redundant reasoning steps, thereby mitigating overthinking. However, these approaches typically require additional rollout computation or externally labeled datasets. In this paper, we propose \textbf{NEAT}, a \textbf{N}euron-based \textbf{E}arly re\textbf{A}soning exi\textbf{T} framework that monitors neuron-level activation dynamics to enable training-free early exits, without introducing additional test-time computation. NEAT identifies exit-associated neurons and tracks their activation patterns during reasoning to dynamically trigger early exit or suppress reflection, thereby reducing unnecessary reasoning while preserving solution quality. Experiments on four reasoning benchmarks across six models with different scales and architectures show that, for each model, NEAT achieves an average token reduction of 22\% to 28\% when averaged over the four benchmarks, while maintaining accuracy.

</details>


### [418] [WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora](https://arxiv.org/abs/2602.02053)
*Pengyu Wang,Benfeng Xu,Licheng Zhang,Shaohan Wang,Mingxuan Du,Chiwei Zhu,Zhendong Mao*

Main category: cs.CL

TL;DR: 本文提出了WildGraphBench，一个面向真实场景的图增强检索生成（GraphRAG）评测基准，基于维基百科结构构建，涵盖12类主题、1100个问题，覆盖单事实问答、多事实问答和章节级摘要三类任务；实验表明现有GraphRAG在多源证据聚合上有效，但在细粒度摘要任务中表现较弱。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG基准多依赖短小、人工筛选的段落，难以反映真实场景中长上下文与大规模异构文档的挑战，亟需更贴近实际的评测基准。

Method: 基于维基百科文章与其外部参考文献的天然结构关系，构建WildGraphBench：选取12个顶层主题下的文章，以其引用文献为检索语料库，以文中被引用的陈述为标注真值，生成1100个覆盖三种复杂度的问题。

Result: 实验显示当前GraphRAG方法在中等数量来源的多事实聚合任务上有提升，但在章节级摘要任务中因过度强调高层陈述而丢失细节，性能下降。

Conclusion: WildGraphBench填补了GraphRAG在真实长文本、异构文档场景下的评测空白；结果揭示当前GraphRAG在细粒度信息整合能力上的不足，为后续研究提供新方向。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.

</details>


### [419] [Closing the Loop: Universal Repository Representation with RPG-Encoder](https://arxiv.org/abs/2602.02084)
*Jane Luo,Chengyu Yin,Xin Zhang,Qingtao Li,Steven Liu,Yiming Huang,Jie Wu,Hao Liu,Yangyu Huang,Yu Kang,Fangkai Yang,Ying Xin,Scarlett Li*

Main category: cs.CL

TL;DR: 本文提出RPG-Encoder框架，通过将静态的Repository Planning Graph（RPG）升级为统一、高保真的动态表示，弥合代码生成与理解之间的推理断层，在SWE-bench和RepoCraft基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有仓库代理因依赖孤立的API文档或依赖图而缺乏语义深度，导致推理断层；作者认为仓库理解与生成是互逆过程，需统一建模。

Method: 提出RPG-Encoder：（1）将原始代码编码为融合提升语义特征与代码依赖的RPG；（2）增量演化RPG拓扑结构，解耦维护成本与仓库规模；（3）作为结构感知导航的统一接口。

Result: 在SWE-bench Verified上达93.7% Acc@5（SOTA），超最佳基线10%以上；在SWE-bench Live Lite上亦显著领先；RepoCraft上重建覆盖率达98.5%。

Conclusion: RPG-Encoder成功构建了高保真、可演化的统一仓库表示，有效闭环‘意图↔实现’推理，显著提升复杂代码库中的细粒度定位精度与理解能力。

Abstract: Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.

</details>


### [420] [LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs](https://arxiv.org/abs/2602.02090)
*Yikai Zeng,Yingchao Piao,Jianhui Li*

Main category: cs.CL

TL;DR: 本文提出LEC-KG框架，结合大语言模型（LLM）的语义理解与知识图谱嵌入（KGE）的结构推理能力，通过双向协同迭代优化，提升领域知识图谱构建效果，尤其在长尾关系抽取上表现突出。


<details>
  <summary>Details</summary>
Motivation: 构建领域知识图谱面临实体指代异构、关系长尾分布及缺乏标准模式等挑战。

Method: 提出LEC-KG双向协同框架，包含：(1) 层次化粗到细关系抽取以缓解长尾偏差；(2) 证据引导的思维链反馈确保结构建议基于原文；(3) 语义初始化支持对未见实体的结构验证；LLM与KGE模块相互迭代优化。

Result: 在中文可持续发展目标（SDG）报告数据集上显著优于纯LLM基线，尤其在低频关系抽取任务中提升明显。

Conclusion: LEC-KG能可靠地将非结构化政策文本转化为经验证的知识图谱三元组，有效融合语义理解与结构推理。

Abstract: Constructing domain-specific knowledge graphs from unstructured text remains challenging due to heterogeneous entity mentions, long-tail relation distributions, and the absence of standardized schemas. We present LEC-KG, a bidirectional collaborative framework that integrates the semantic understanding of Large Language Models (LLMs) with the structural reasoning of Knowledge Graph Embeddings (KGE). Our approach features three key components: (1) hierarchical coarse-to-fine relation extraction that mitigates long-tail bias, (2) evidence-guided Chain-of-Thought feedback that grounds structural suggestions in source text, and (3) semantic initialization that enables structural validation for unseen entities. The two modules enhance each other iteratively-KGE provides structure-aware feedback to refine LLM extractions, while validated triples progressively improve KGE representations. We evaluate LEC-KG on Chinese Sustainable Development Goal (SDG) reports, demonstrating substantial improvements over LLM baselines, particularly on low-frequency relations. Through iterative refinement, our framework reliably transforms unstructured policy text into validated knowledge graph triples.

</details>


### [421] [Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning](https://arxiv.org/abs/2602.02099)
*Keqin Peng,Yuanxin Ouyang,Xuebo Liu,Zhiliang Tian,Ruijian Han,Yancheng Yuan,Liang Ding*

Main category: cs.CL

TL;DR: 本文提出动态解耦条件优势（DDCA）方法，解决强化学习中可验证奖励（RLVR）导致的推理链过长问题，通过在正确响应簇内条件计算长度优势并动态调整惩罚强度，显著提升效率-准确率权衡。


<details>
  <summary>Details</summary>
Motivation: RLVR虽能激发多步推理，但易生成冗长推理链；而简单长度惩罚会严重损害准确率，原因在于长度基线稀释和难度-惩罚不匹配两个结构性问题。

Method: 提出动态解耦条件优势（DDCA）：在正确响应子群内条件化计算长度优势以消除基线稀释，并利用组通过率动态缩放惩罚强度以适配问题难度。

Result: 在GSM8K、MATH500、AMC23和AIME25上实验表明，DDCA相较自适应基线显著提升效率-准确率权衡：在简单任务（如GSM8K）上减少约60%生成token，在困难任务（如AIME25）上减少超20%，同时保持或提升准确率。

Conclusion: DDCA有效解耦效率优化与正确性优化，为RLVR中的长度控制提供了更鲁棒、自适应的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.

</details>


### [422] [Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs](https://arxiv.org/abs/2602.02104)
*Shaltiel Shmidman,Avi Shmidman,Amir DN Cohen,Moshe Koppel*

Main category: cs.CL

TL;DR: Dicta-LM 3.0 是一套面向希伯来语的开源大语言模型，涵盖 24B、12B 和 1.7B 三种规模，支持 65K 上下文及工具调用，并配套推出首个希伯来语专用评测基准。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如希伯来语）缺乏主权大模型的问题，弥补多语言大模型在非英语语种上的供给缺口。

Method: 基于 Mistral-Small-3.1、Nemotron Nano V2 和 Qwen3-1.7B 等基座模型，使用大规模希伯来语与英语混合语料进行适配训练；构建包含翻译、摘要、Winograd、以色列常识问答和尼库德标注等任务的希伯来语评测套件。

Result: 发布三个尺寸、多种变体（基础版/对话版/支持工具调用）的开源希伯来语 LLM，均支持 65K 上下文；提出首个面向希伯来语对话式 LLM 的综合评测基准。

Conclusion: Dicta-LM 3.0 不仅提升了希伯来语大模型能力与可用性，还为其他低资源语言的大模型适配提供了可复用的方法框架，推动多语言 NLP 发展。

Abstract: Open-weight LLMs have been released by frontier labs; however, sovereign Large Language Models (for languages other than English) remain low in supply yet high in demand. Training large language models (LLMs) for low-resource languages such as Hebrew poses unique challenges. In this paper, we introduce Dicta-LM 3.0: an open-weight collection of LLMs trained on substantially-sized corpora of Hebrew and English texts. The model is released in three sizes: 24B - adapted from the Mistral-Small-3.1 base model, 12B - adapted from the NVIDIA Nemotron Nano V2 model, and 1.7B - adapted from the Qwen3-1.7B base model. We are releasing multiple variants of each model, each with a native context length of 65k tokens; base model and chat model with tool-calling support. To rigorously evaluate our models, we introduce a new benchmark suite for evaluation of Hebrew chat-LLMs, covering a diverse set of tasks including Translation, Summarization, Winograd, Israeli Trivia, and Diacritization (nikud). Our work not only addresses the intricacies of training LLMs in low-resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP.

</details>


### [423] [Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts](https://arxiv.org/abs/2602.02108)
*Wenhao Li,Daohai Yu,Gen Luo,Yuxin Zhang,Fei Chao,Rongrong Ji,Yifan Wu,Jiaxin Liu,Ziyang Gong,Zimu Liao*

Main category: cs.CL

TL;DR: OOMB is a memory-efficient training system for LLMs on long contexts, achieving O(1) activation memory via chunk-recurrent training and on-the-fly recomputation, and managing KV cache with paged memory, CPU offloading, and sparse attention—enabling 4M-token context training of Qwen2.5-7B on a single H200 GPU.


<details>
  <summary>Details</summary>
Motivation: Training LLMs on long contexts is bottlenecked by GPU memory overhead—especially from activations scaling linearly with sequence length—not training time.

Method: OOMB uses a chunk-recurrent training framework with on-the-fly activation recomputation to achieve O(1) activation memory; it further employs a paged memory manager for KV cache and gradients, asynchronous CPU offloading, and page-level sparse attention.

Result: For every extra 10K tokens, memory overhead increases only by ~10MB for Qwen2.5-7B; enables 4M-token context training of Qwen2.5-7B on a single H200 GPU—previously requiring large-cluster context parallelism.

Conclusion: OOMB substantially advances resource efficiency for long-context LLM training by eliminating activation memory scaling and synergistically optimizing KV cache management.

Abstract: Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.

</details>


### [424] [There Is More to Refusal in Large Language Models than a Single Direction](https://arxiv.org/abs/2602.02132)
*Faaiz Joad,Majd Hawasly,Sabri Boughorbel,Nadir Durrani,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: 本文发现大语言模型的拒绝行为并非由单一激活空间方向控制，而是对应多个几何上不同的方向，但这些方向在拒绝与过度拒绝的权衡上表现出一致的一维控制特性。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为大语言模型的拒绝行为由单一激活空间方向调控，本文旨在检验该观点的完整性，并探索更精细的拒绝机制。

Method: 在十一类拒绝与不合规行为（包括安全性、不完整/不支持请求、拟人化、过度拒绝等）上，分析其在激活空间中的几何结构，并评估线性引导对拒绝行为的影响。

Result: 不同类别的拒绝行为对应激活空间中几何上分离的方向；但沿任一拒绝相关方向进行线性引导，均产生几乎相同的拒绝-过度拒绝权衡曲线；各方向的主要差异在于拒绝的表达方式而非是否拒绝。

Conclusion: 拒绝行为具有多方向性，但其调控本质上具有一维共性，提示拒绝机制既多样又统一，需从‘如何拒绝’而不仅是‘是否拒绝’角度理解。

Abstract: Prior work argues that refusal in large language models is mediated by a single activation-space direction, enabling effective steering and ablation. We show that this account is incomplete. Across eleven categories of refusal and non-compliance, including safety, incomplete or unsupported requests, anthropomorphization, and over-refusal, we find that these refusal behaviors correspond to geometrically distinct directions in activation space. Yet despite this diversity, linear steering along any refusal-related direction produces nearly identical refusal to over-refusal trade-offs, acting as a shared one-dimensional control knob. The primary effect of different directions is not whether the model refuses, but how it refuses.

</details>


### [425] [Quantifying the Gap between Understanding and Generation within Unified Multimodal Models](https://arxiv.org/abs/2602.02140)
*Chenlong Wang,Yuhang Chen,Zhihan Hu,Dongping Chen,Wenhu Chen,Sarah Wiegreffe,Tianyi Zhou*

Main category: cs.CL

TL;DR: 本文提出GapEval基准，用于量化统一多模态模型（UMM）在理解与生成能力之间的双向差距，发现当前UMM仅实现表层统一，知识在模态间仍割裂、不同步。


<details>
  <summary>Details</summary>
Motivation: 探究统一多模态模型（UMM）中理解与生成能力是否真正对齐与融合，而非仅表面统一。

Method: 构建双向对称的GapEval基准，支持图像与文本双模态互答；结合知识操作视角开展实证分析，评估跨模态一致性与认知连贯性。

Result: 实验表明各类UMM在双向任务上存在持续性性能差距；知识在模态间呈现离散性，能力涌现与知识迁移不同步。

Conclusion: 当前UMM尚未实现深层认知融合，理解与生成能力仍割裂；GapEval为评估和推动真正统一的多模态智能提供了新范式与诊断工具。

Abstract: Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two "unified" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.

</details>


### [426] [Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing](https://arxiv.org/abs/2602.02159)
*Lingkun Long,Yushi Huang,Shihao Bai,Ruihao Gong,Jun Zhang,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: 本文提出Focus-dLLM，一种无需训练的注意力稀疏化框架，通过过去置信度引导预测未掩码区域，并结合sink感知剪枝策略，在保持性能的同时大幅提升dLLM长上下文推理效率。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）虽具备强长上下文处理能力，但双向全注意力带来高昂计算开销；现有稀疏注意力方法因难以在扩散过程中预估待解码token的重要性而效果不佳。

Method: 提出Focus-dLLM框架：1）基于相邻步间token置信度强相关性，设计过去置信度引导的未掩码区域预测指标；2）提出sink感知剪枝策略，精准识别并保留关键attention sink，剔除冗余计算；3）跨层复用sink位置以降低开销。

Result: 在32K上下文长度下实现超29倍无损加速。

Conclusion: Focus-dLLM是一种高效、准确、训练无关的dLLM注意力稀疏化方案，显著提升长上下文推理效率，且具备跨层一致性优势。

Abstract: Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than $29\times$ lossless speedup under $32K$ context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM

</details>


### [427] [D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use](https://arxiv.org/abs/2602.02160)
*Bowen Xu,Shaoyu Wu,Hao Jiang,Kai Liu,Xin Chen,Lulu Hu,Bin Yang*

Main category: cs.CL

TL;DR: 本文提出D-CORE两阶段训练框架，通过自蒸馏增强大推理模型（LRM）的任务分解能力，再结合多样性感知强化学习恢复其反思推理能力，显著提升复杂工具使用场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型（LRMs）在复杂工具使用场景中缺乏子任务分解能力，导致“懒惰推理”问题。

Method: 提出两阶段训练框架D-CORE：第一阶段通过自蒸馏激励LRMs的任务分解推理能力；第二阶段采用多样性感知的强化学习恢复其反思推理能力。

Result: 在BFCLv3基准上，D-CORE-8B达到77.7%准确率，超越最优8B模型5.7%；D-CORE-14B达79.3%，超越70B模型，且参数量仅为其1/5。

Conclusion: D-CORE有效缓解Lazy Reasoning问题，在不同规模模型和多样化基准上均实现鲁棒的工具使用性能提升。

Abstract: Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\underline{\textbf{D}}ecomposing tasks and \underline{\textbf{Co}}mposing \underline{\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\% accuracy, surpassing the best-performing 8B model by 5.7\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\%, outperforming 70B models despite being 5$\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.

</details>


### [428] [AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?](https://arxiv.org/abs/2602.02178)
*Liang Lin,Feng Xiong,Zengbin Wang,Kun Wang,Junhao Dong,Xuecai Hu,Yong Wang,Xiangxiang Chu*

Main category: cs.CL

TL;DR: 本文提出AR-MAP框架，利用已对齐的自回归大语言模型（AR-LLMs）作为隐式教师，通过权重缩放将对齐知识迁移至扩散大语言模型（DLLMs），避免了DLLM直接对齐的高方差与高开销，在多个偏好对齐任务上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）虽支持并行生成，但其偏好对齐因ELBO似然估计的高方差而困难。

Method: 提出AR-MAP迁移学习框架，利用AR-LLMs作为隐式教师，通过简单权重缩放将对齐知识迁移到DLLMs，充分利用二者共享的架构结构。

Result: 在多个偏好对齐任务上，AR-MAP平均得分为69.08%，性能优于或媲美现有DLLM专用对齐方法。

Conclusion: DLLMs可通过轻量级权重缩放有效吸收AR-LLMs的对齐知识，AR-MAP为DLLM对齐提供了高效、低方差的新范式。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a powerful alternative to autoregressive models, enabling parallel token generation across multiple positions. However, preference alignment of DLLMs remains challenging due to high variance introduced by Evidence Lower Bound (ELBO)-based likelihood estimation. In this work, we propose AR-MAP, a novel transfer learning framework that leverages preference-aligned autoregressive LLMs (AR-LLMs) as implicit teachers for DLLM alignment. We reveal that DLLMs can effectively absorb alignment knowledge from AR-LLMs through simple weight scaling, exploiting the shared architectural structure between these divergent generation paradigms. Crucially, our approach circumvents the high variance and computational overhead of direct DLLM alignment and comprehensive experiments across diverse preference alignment tasks demonstrate that AR-MAP achieves competitive or superior performance compared to existing DLLM-specific alignment methods, achieving 69.08\% average score across all tasks and models. Our Code is available at https://github.com/AMAP-ML/AR-MAP.

</details>


### [429] [Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages](https://arxiv.org/abs/2602.02182)
*Tjaša Arčon,Matej Klemen,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: 本文评估了大语言模型（LLMs）对语言结构的元语言知识，发现其能力有限且高度依赖数据可用性，尤其在低资源语言上表现较差；作者发布了一个开源基准以促进全球语言多样性研究。


<details>
  <summary>Details</summary>
Motivation: 现有语言学基准多聚焦于高资源语言和狭窄现象，缺乏对元语言知识（即对语言结构的显式推理能力）的系统评估。

Method: 基于准确率和宏F1分数，并结合多数类与随机基线，分析LLM在不同语言学领域及语言相关因素下的表现；使用Wikipedia规模、语料库可用性等指标探究预测因子。

Result: GPT-4o表现最佳但准确率仅0.367；所有模型均未超越多数类基线；词汇特征准确率最高，音系特征最低；数字语言地位（如网络可见度、资源丰富度）显著影响性能，资源指标比地理、谱系或社会语言学因素更具预测力。

Conclusion: 当前LLMs的元语言知识是零散的，由训练数据分布驱动，而非具备跨语言的通用语法能力；需通过更均衡的数据构建提升全球语言代表性。

Abstract: Large language models (LLMs) are routinely evaluated on language use tasks, yet their knowledge of linguistic structure remains poorly understood. Existing linguistic benchmarks typically focus on narrow phenomena, emphasize high-resource languages, and rarely evaluate metalinguistic knowledge-explicit reasoning about language structure rather than language use. Using accuracy and macro F1, together with majority-class and chance baselines, we analyse overall performance and examine variation by linguistic domains and language-related factors. Our results show that metalinguistic knowledge in current LLMs is limited: GPT-4o performs best but achieves only moderate accuracy (0.367), while open-source models lag behind. All models perform above chance but fail to outperform the majority-class baseline, suggesting they capture cross-linguistic patterns but lack fine-grained grammatical distinctions. Performance varies across linguistic domains, with lexical features showing the highest accuracy and phonological features among the lowest, partially reflecting differences in online visibility. At the language level, accuracy shows a strong association with digital language status: languages with higher digital presence and resource availability are evaluated more accurately, while low-resource languages show substantially lower performance. Analyses of predictive factors confirm that resource-related indicators (Wikipedia size, corpus availability) are more informative predictors of accuracy than geographical, genealogical, or sociolinguistic factors. Together, these results suggest that LLMs' metalinguistic knowledge is fragmented and shaped by data availability rather than generalizable grammatical competence across the world's languages. We release our benchmark as an open-source dataset to support systematic evaluation and encourage greater global linguistic diversity in future LLMs.

</details>


### [430] [Sinhala Physical Common Sense Reasoning Dataset for Global PIQA](https://arxiv.org/abs/2602.02207)
*Nisansa de Silva,Surangika Ranathunga*

Main category: cs.CL

TL;DR: This paper introduces the first Sinhala physical common sense reasoning dataset as part of Global PIQA, comprising 110 human-created and verified samples with prompts, correct answers, and wrong answers, mostly grounded in the Sri Lankan context.


<details>
  <summary>Details</summary>
Motivation: To address the lack of Sinhala-language resources for physical common sense reasoning, especially within the Sri Lankan cultural and linguistic context.

Method: Human creation and verification of 110 Sinhala-language physical common sense reasoning samples, aligned with the Global PIQA framework.

Result: A novel, culturally grounded Sinhala physical common sense reasoning dataset with 110 verified samples.

Conclusion: The dataset fills a critical gap in low-resource language NLP benchmarks and supports future research in Sinhala-language commonsense reasoning.

Abstract: This paper presents the first-ever Sinhala physical common sense reasoning dataset created as part of Global PIQA. It contains 110 human-created and verified data samples, where each sample consists of a prompt, the corresponding correct answer, and a wrong answer. Most of the questions refer to the Sri Lankan context, where Sinhala is an official language.

</details>


### [431] [Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge](https://arxiv.org/abs/2602.02219)
*Yuzheng Xu,Tosho Hirasawa,Tadashi Kozuno,Yoshitaka Ushiku*

Main category: cs.CL

TL;DR: 本文揭示了基于量规的LLM-as-a-judge评估中存在位置偏差，并提出一种平衡排列策略来缓解该偏差，从而提升与人工评分的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注点式和成对评估范式，而对基于量规（rubric-based）的LLM-as-a-judge评估分析不足；作者发现其隐含多选特性，易受位置偏差影响。

Method: 通过跨模型与数据集的控制实验验证位置偏差的存在；提出平衡排列策略，使各分数选项在不同位置均匀分布，并聚合多次排列结果以校准评估。

Result: 实验证明位置偏差普遍存在；采用平衡排列策略后，LLM-as-a-Judge与人工评分的相关性显著提升。

Conclusion: 基于量规的LLM-as-a-Judge并非本质上的点式评估，简单的排列校准可显著提高其可靠性。

Abstract: Large language models (LLMs) are now widely used to evaluate the quality of text, a field commonly referred to as LLM-as-a-judge. While prior works mainly focus on point-wise and pair-wise evaluation paradigms. Rubric-based evaluation, where LLMs select a score from multiple rubrics, has received less analysis. In this work, we show that rubric-based evaluation implicitly resembles a multi-choice setting and therefore has position bias: LLMs prefer score options appearing at specific positions in the rubric list. Through controlled experiments across multiple models and datasets, we demonstrate consistent position bias. To mitigate this bias, we propose a balanced permutation strategy that evenly distributes each score option across positions. We show that aggregating scores across balanced permutations not only reveals latent position bias, but also improves correlation between the LLM-as-a-Judge and human. Our results suggest that rubric-based LLM-as-a-Judge is not inherently point-wise and that simple permutation-based calibration can substantially improve its reliability.

</details>


### [432] [Using Correspondence Patterns to Identify Irregular Words in Cognate sets Through Leave-One-Out Validation](https://arxiv.org/abs/2602.02221)
*Frederic Blum,Johann-Mattis List*

Main category: cs.CL

TL;DR: 本文提出了一种新的规则性度量方法——平衡平均重现率，并基于此开发了识别不规则同源词集的计算方法，通过模拟与真实数据实验验证其有效性（准确率达85%），旨在提升计算机辅助语言比较中数据集的质量。


<details>
  <summary>Details</summary>
Motivation: 传统历史语言学中对音变规则性的判断多依赖直觉而非量化评估，且实际中不规则现象比新语法学派模型预期更常见；随着计算语言学进展和标准化词汇数据增多，亟需定量评估工具。

Method: 提出‘平衡平均重现率’作为规则性新度量，并构建基于该度量的计算方法以识别对应模式不规则的同源词集；采用留一法验证，在同源集中替换一个形式为不规则形式，检测方法识别异常项的能力。

Result: 在真实数据集上整体准确率达85%；验证了子样本策略的有效性，并分析了不规则性增强对结果的影响。

Conclusion: 该规则性度量及相应识别方法有望显著提升现有及未来计算机辅助语言比较数据集的质量。

Abstract: Regular sound correspondences constitute the principal evidence in historical language comparison. Despite the heuristic focus on regularity, it is often more an intuitive judgement than a quantified evaluation, and irregularity is more common than expected from the Neogrammarian model. Given the recent progress of computational methods in historical linguistics and the increased availability of standardized lexical data, we are now able to improve our workflows and provide such a quantitative evaluation. Here, we present the balanced average recurrence of correspondence patterns as a new measure of regularity. We also present a new computational method that uses this measure to identify cognate sets that lack regularity with respect to their correspondence patterns. We validate the method through two experiments, using simulated and real data. In the experiments, we employ leave-one-out validation to measure the regularity of cognate sets in which one word form has been replaced by an irregular one, checking how well our method identifies the forms causing the irregularity. Our method achieves an overall accuracy of 85\% with the datasets based on real data. We also show the benefits of working with subsamples of large datasets and how increasing irregularity in the data influences our results. Reflecting on the broader potential of our new regularity measure and the irregular cognate identification method based on it, we conclude that they could play an important role in improving the quality of existing and future datasets in computer-assisted language comparison.

</details>


### [433] [OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data](https://arxiv.org/abs/2602.02266)
*Tan Sang Nguyen,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本文提出使用平行数据进行大语言模型（LLM）持续预训练，以高效扩展至低资源语言；基于该方法构建了首个真正开源的东南亚语言大模型OpenSeal，在仅34.7B平行语料和180小时8×H200训练下达到同类模型性能水平。


<details>
  <summary>Details</summary>
Motivation: 现有东南亚聚焦的大语言模型均非真正开源（未公开训练数据），而真正开源对理解模型偏见、泛化与多语言能力至关重要；同时，多数多语言LLM仍以英语为中心，低资源语言表现差。

Method: 开展受控且全面的实验，研究平行数据在LLM持续预训练中提升多语言能力的有效性，并基于此方法构建开源模型OpenSeal。

Result: 仅使用平行数据是扩展LLM至新语言最有效的方式；利用34.7B tokens平行数据和180小时8×H200 GPU训练，成功构建首个真正开源的东南亚LLM OpenSeal，性能媲美同规模闭源模型。

Conclusion: 平行数据在持续预训练中对提升LLM低资源语言能力具有关键作用；OpenSeal验证了真正开源、透明、可复现的区域性大模型的可行性与竞争力。

Abstract: Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.

</details>


### [434] [dziribot: rag based intelligent conversational agent for algerian arabic dialect](https://arxiv.org/abs/2602.02270)
*El Batoul Bechiri,Dihia Lanasri*

Main category: cs.CL

TL;DR: 本文提出DziriBOT，一种专为阿尔及利亚达吉亚（Darja）方言设计的混合型对话代理，结合NLU与检索增强生成（RAG），并通过对比评估多种方法，证明微调的DziriBERT模型在低资源方言场景下达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 阿尔及利亚达吉亚方言具有正字法不规范、频繁法语语码转换、阿拉伯文与拉丁文（Arabizi）混用等特点，导致现有标准语言模型难以有效支持客户服务平台。

Method: 构建多层混合架构，融合专用自然语言理解（NLU）与检索增强生成（RAG）；系统评估三种方法：基于稀疏特征的Rasa流程、经典机器学习基线、以及基于DziriBERT的Transformer微调。

Result: 微调后的DziriBERT模型在达吉亚语任务中达到当前最优性能，显著优于传统基线，尤其在处理拼写噪声和罕见意图方面表现突出。

Conclusion: DziriBOT为阿尔及利亚用户提供鲁棒、可扩展的对话服务，弥合了通用语言模型与本地语言现实之间的鸿沟，并为区域方言感知自动化提供了可复用范式。

Abstract: The rapid digitalization of customer service has intensified the demand for conversational agents capable of providing accurate and natural interactions. In the Algerian context, this is complicated by the linguistic complexity of Darja, a dialect characterized by non-standardized orthography, extensive code-switching with French, and the simultaneous use of Arabic and Latin (Arabizi) scripts. This paper introduces DziriBOT, a hybrid intelligent conversational agent specifically engineered to overcome these challenges. We propose a multi-layered architecture that integrates specialized Natural Language Understanding (NLU) with Retrieval-Augmented Generation (RAG), allowing for both structured service flows and dynamic, knowledge-intensive responses grounded in curated enterprise documentation. To address the low-resource nature of Darja, we systematically evaluate three distinct approaches: a sparse-feature Rasa pipeline, classical machine learning baselines, and transformer-based fine-tuning. Our experimental results demonstrate that the fine-tuned DziriBERT model achieves state-of-the-art performance. These results significantly outperform traditional baselines, particularly in handling orthographic noise and rare intents. Ultimately, DziriBOT provides a robust, scalable solution that bridges the gap between formal language models and the linguistic realities of Algerian users, offering a blueprint for dialect-aware automation in the regional market.

</details>


### [435] [Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276)
*Kimi Team,Tongtong Bai,Yifan Bai,Yiping Bao,S. H. Cai,Yuan Cao,Y. Charles,H. S. Che,Cheng Chen,Guanduo Chen,Huarong Chen,Jia Chen,Jiahao Chen,Jianlong Chen,Jun Chen,Kefan Chen,Liang Chen,Ruijue Chen,Xinhao Chen,Yanru Chen,Yanxu Chen,Yicun Chen,Yimin Chen,Yingjiang Chen,Yuankun Chen,Yujie Chen,Yutian Chen,Zhirong Chen,Ziwei Chen,Dazhi Cheng,Minghan Chu,Jialei Cui,Jiaqi Deng,Muxi Diao,Hao Ding,Mengfan Dong,Mengnan Dong,Yuxin Dong,Yuhao Dong,Angang Du,Chenzhuang Du,Dikang Du,Lingxiao Du,Yulun Du,Yu Fan,Shengjun Fang,Qiulin Feng,Yichen Feng,Garimugai Fu,Kelin Fu,Hongcheng Gao,Tong Gao,Yuyao Ge,Shangyi Geng,Chengyang Gong,Xiaochen Gong,Zhuoma Gongque,Qizheng Gu,Xinran Gu,Yicheng Gu,Longyu Guan,Yuanying Guo,Xiaoru Hao,Weiran He,Wenyang He,Yunjia He,Chao Hong,Hao Hu,Jiaxi Hu,Yangyang Hu,Zhenxing Hu,Ke Huang,Ruiyuan Huang,Weixiao Huang,Zhiqi Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yu Jing,Guokun Lai,Aidi Li,C. Li,Cheng Li,Fang Li,Guanghe Li,Guanyu Li,Haitao Li,Haoyang Li,Jia Li,Jingwei Li,Junxiong Li,Lincan Li,Mo Li,Weihong Li,Wentao Li,Xinhang Li,Xinhao Li,Yang Li,Yanhao Li,Yiwei Li,Yuxiao Li,Zhaowei Li,Zheming Li,Weilong Liao,Jiawei Lin,Xiaohan Lin,Zhishan Lin,Zichao Lin,Cheng Liu,Chenyu Liu,Hongzhang Liu,Liang Liu,Shaowei Liu,Shudong Liu,Shuran Liu,Tianwei Liu,Tianyu Liu,Weizhou Liu,Xiangyan Liu,Yangyang Liu,Yanming Liu,Yibo Liu,Yuanxin Liu,Yue Liu,Zhengying Liu,Zhongnuo Liu,Enzhe Lu,Haoyu Lu,Zhiyuan Lu,Junyu Luo,Tongxu Luo,Yashuo Luo,Long Ma,Yingwei Ma,Shaoguang Mao,Yuan Mei,Xin Men,Fanqing Meng,Zhiyong Meng,Yibo Miao,Minqing Ni,Kun Ouyang,Siyuan Pan,Bo Pang,Yuchao Qian,Ruoyu Qin,Zeyu Qin,Jiezhong Qiu,Bowen Qu,Zeyu Shang,Youbo Shao,Tianxiao Shen,Zhennan Shen,Juanfeng Shi,Lidong Shi,Shengyuan Shi,Feifan Song,Pengwei Song,Tianhui Song,Xiaoxi Song,Hongjin Su,Jianlin Su,Zhaochen Su,Lin Sui,Jinsong Sun,Junyao Sun,Tongyu Sun,Flood Sung,Yunpeng Tai,Chuning Tang,Heyi Tang,Xiaojuan Tang,Zhengyang Tang,Jiawen Tao,Shiyuan Teng,Chaoran Tian,Pengfei Tian,Ao Wang,Bowen Wang,Chensi Wang,Chuang Wang,Congcong Wang,Dingkun Wang,Dinglu Wang,Dongliang Wang,Feng Wang,Hailong Wang,Haiming Wang,Hengzhi Wang,Huaqing Wang,Hui Wang,Jiahao Wang,Jinhong Wang,Jiuzheng Wang,Kaixin Wang,Linian Wang,Qibin Wang,Shengjie Wang,Shuyi Wang,Si Wang,Wei Wang,Xiaochen Wang,Xinyuan Wang,Yao Wang,Yejie Wang,Yipu Wang,Yiqin Wang,Yucheng Wang,Yuzhi Wang,Zhaoji Wang,Zhaowei Wang,Zhengtao Wang,Zhexu Wang,Zihan Wang,Zizhe Wang,Chu Wei,Ming Wei,Chuan Wen,Zichen Wen,Chengjie Wu,Haoning Wu,Junyan Wu,Rucong Wu,Wenhao Wu,Yuefeng Wu,Yuhao Wu,Yuxin Wu,Zijian Wu,Chenjun Xiao,Jin Xie,Xiaotong Xie,Yuchong Xie,Yifei Xin,Bowei Xing,Boyu Xu,Jianfan Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinbo Xu,Xinran Xu,Yangchuan Xu,Yichang Xu,Yuemeng Xu,Zelai Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Guangyao Yang,Hao Yang,Junwei Yang,Kai Yang,Ningyuan Yang,Ruihan Yang,Xiaofei Yang,Xinlong Yang,Ying Yang,Yi Yang,Yi Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Dan Ye,Wenjie Ye,Zhuorui Ye,Bohong Yin,Chengzhen Yu,Longhui Yu,Tao Yu,Tianxiang Yu,Enming Yuan,Mengjie Yuan,Xiaokun Yuan,Yang Yue,Weihao Zeng,Dunyuan Zha,Haobing Zhan,Dehao Zhang,Hao Zhang,Jin Zhang,Puqi Zhang,Qiao Zhang,Rui Zhang,Xiaobin Zhang,Y. Zhang,Yadong Zhang,Yangkun Zhang,Yichi Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yushun Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Chenguang Zhao,Feifan Zhao,Jinxiang Zhao,Shuai Zhao,Xiangyu Zhao,Yikai Zhao,Zijia Zhao,Huabin Zheng,Ruihan Zheng,Shaojie Zheng,Tengyang Zheng,Junfeng Zhong,Longguang Zhong,Weiming Zhong,M. Zhou,Runjie Zhou,Xinyu Zhou,Zaida Zhou,Jinguo Zhu,Liya Zhu,Xinhao Zhu,Yuxuan Zhu,Zhen Zhu,Jingze Zhuang,Weiyu Zhuang,Ying Zou,Xinxing Zu*

Main category: cs.CL

TL;DR: Kimi K2.5 是一个开源多模态智能体模型，通过联合优化文本与视觉模态，并引入 Agent Swarm 并行智能体调度框架，在编码、视觉、推理和智能体任务上达到 SOTA 性能，同时降低延迟达 4.5 倍。


<details>
  <summary>Details</summary>
Motivation: 提升通用智能体智能，尤其在多模态协同与复杂任务分解执行方面存在不足。

Method: 联合文本-视觉预训练、零视觉监督微调（zero-vision SFT）、联合文本-视觉强化学习；提出 Agent Swarm 自主并行智能体编排框架，支持动态任务分解与并发执行。

Result: 在编码、视觉、推理及智能体任务上达到当前最优（SOTA）；Agent Swarm 相比单智能体基线降低延迟最高达 4.5 倍。

Conclusion: Kimi K2.5 验证了多模态联合优化与并行智能体架构对提升通用智能体能力的有效性，所开源模型将推动智能体智能的研究与应用。

Abstract: We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.

</details>


### [436] [Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages](https://arxiv.org/abs/2602.02287)
*Isaac Chung,Linda Freienthal*

Main category: cs.CL

TL;DR: 本文通过在爱沙尼亚语、芬兰语和匈牙利语中使用相同参数生成合成客服对话，控制生成条件以评估跨语言大语言模型（LLM）评测的稳定性；发现表层指标稳定，但语用类判断（如连贯性、指令遵循）存在严重跨语言排名不稳定，表明LLM裁判的零样本跨语言迁移在形态丰富语言中不可靠，需语言特异性校准。


<details>
  <summary>Details</summary>
Motivation: 跨语言LLM评测常混淆真实性能差异与测量不稳定性；需分离二者以准确评估模型能力。

Method: 采用受控合成数据（同一生成参数在三种Finno-Ugric语言中生成客服对话），对比自动指标与LLM-as-a-judge在多语言下的排名一致性，并以爱沙尼亚语母语者标注为参考基准。

Result: 表层指标（词汇多样性、表层/语义相似度）跨语言稳定；语用类判断（连贯性、指令遵循）出现排名倒置与近零相关性；证明LLM裁判跨语言评分行为本身不稳定。

Conclusion: 零样本LLM裁判迁移在形态丰富语言的语篇级评估中不可靠；评测方法若在生成一致条件下仍失稳，即预示实际部署前的迁移失败；应基于目标语言人工标注进行针对性校准。

Abstract: Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.
  This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.

</details>


### [437] [Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?](https://arxiv.org/abs/2602.02290)
*Alex Argese,Pasquale Lisena,Raphaël Troncy*

Main category: cs.CL

TL;DR: 本文提出StoryScore，一种用于评估AI生成科学故事的综合指标，整合了语义对齐、词汇依据、叙事控制、结构保真度、冗余避免和实体级幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 现有摘要评估指标难以捕捉科学叙事所需的抽象性、简化性和教学创造性，且幻觉检测器在面对创造性改写时易误判或不稳定。

Method: 提出StoryScore复合评估指标，融合语义对齐、词汇 grounding、叙事控制、结构保真、冗余避免及实体级幻觉检测，并分析现有幻觉检测方法失效的原因。

Result: StoryScore能更全面评估AI生成科学故事的质量；研究揭示自动指标擅长评估语义相似性，但难以评估叙事方式与控制能力。

Conclusion: StoryScore为科学叙事评估提供了更可靠框架；强调需区分教学创造性与事实错误，推动面向叙事质量的评估方法发展。

Abstract: Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.

</details>


### [438] [Advancing General-Purpose Reasoning Models with Modular Gradient Surgery](https://arxiv.org/abs/2602.02301)
*Min Cai,Yu Liang,Longzheng Wang,Yan Wang,Yueyang Zhang,Long Xia,Zhiyuan Sun,Xi Ye,Daiting Shi*

Main category: cs.CL

TL;DR: 本文提出模块化梯度手术（MGS）方法，解决大推理模型在多领域强化学习中因领域异质性导致的跨领域干扰问题，在数学、通用对话和指令遵循三个领域上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 训练单一通用大推理模型（L RM）面临领域异质性带来的跨领域干扰挑战，现有顺序RL和混合RL策略效果有限。

Method: 提出模块化梯度手术（MGS），在Transformer架构的模块级别进行梯度冲突消解。

Result: 在Llama和Qwen模型上，MGS相比标准多任务RL在三个领域平均提升4.3（16.6%）和4.5（11.1%）分；且在长时间训练下仍保持有效性。

Conclusion: MGS有效缓解多领域RL中的干扰，为训练通用大推理模型提供了可行方案。

Abstract: Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.

</details>


### [439] [The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors](https://arxiv.org/abs/2602.02315)
*Raphaël Sarfati,Eric Bigelow,Daniel Wurgaft,Jack Merullo,Atticus Geiger,Owen Lewis,Tom McGrath,Ekdeep Singh Lubana*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）如何在表征空间中编码和更新关于分布参数的隐式信念，发现存在弯曲的‘信念流形’，并提出几何与场感知的干预方法优于传统线性干预。


<details>
  <summary>Details</summary>
Motivation: 缺乏对大语言模型如何在表征空间中机械地编码、更新及干预其prompt条件化信念（如后验分布）的理解。

Method: 在受控设定下，利用Llama-3.2基于上下文样本隐式推断正态分布参数（均值与标准差），分析其表征空间中信念流形的形成与动态适应，并比较线性 steering 与几何/场感知 steering 的效果；引入线性场探测（LFP）进行流形铺砌与几何保持干预。

Result: 发现足够上下文学习后形成弯曲的参数信念流形；分布突变时，标准线性 steering 易导致离流形、耦合且分布外偏移，而几何与场感知 steering 更好保持信念结构；LFP 可有效支持几何一致干预。

Conclusion: 大语言模型中自然涌现出丰富几何结构，纯线性概念表征常为不充分的抽象。

Abstract: Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved "belief manifolds" for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction.

</details>


### [440] [A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method](https://arxiv.org/abs/2602.02320)
*Feiyang Cai,Guijuan He,Yi Hu,Jingjing Wang,Joshua Luo,Tianyu Zhu,Srikanth Pilla,Gang Li,Ling Liu,Feng Luo*

Main category: cs.CL

TL;DR: 本文提出了一种全自动的分子结构描述生成框架，通过扩展基于规则的IUPAC名称解析器构建结构化XML元数据，并利用大语言模型（LLM）生成高质量自然语言描述，最终构建了约16.3万对分子-描述数据集，验证精度达98.6%。


<details>
  <summary>Details</summary>
Motivation: 分子功能主要由其结构决定，而人工标注分子结构与自然语言对齐数据成本高昂，难以构建大规模高质量数据集。

Method: 扩展规则驱动的化学命名法解析器，将IUPAC名称解析为富含结构信息的XML元数据，并以此引导大语言模型生成准确的自然语言描述。

Result: 构建了包含约163,000个分子-描述对的大规模数据集；在2,000个样本子集上经LLM与专家人工联合验证，描述精度达98.6%。

Conclusion: 该全自动标注框架可高效生成高精度结构描述数据，为分子-语言对齐任务提供了可靠数据基础，并具备向更大规模和更广化学任务拓展的潜力。

Abstract: Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.

</details>


### [441] [Language Steering for Multilingual In-Context Learning](https://arxiv.org/abs/2602.02326)
*Neeraja Kirtane,Kuan-Hao Huang*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的语言向量（language vectors）方法，通过在推理时向中间激活添加语言方向向量，将模型内部表征引导至目标语言空间，从而提升多语言上下文学习性能，并揭示了语言间的语义结构。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在非英语语言上的表现远逊于英语，尤其在英文示例、非英文测试的上下文学习场景中性能显著下降。作者假设模型存在一个通用语义空间，不同语言在此空间中表现为不同方向。

Method: 提出‘语言向量’——一种无需训练的语言引导方法：利用源语言与目标语言激活的差异构造向量，在推理时将其加到中间层激活上，使模型表征朝目标语言空间偏移，不更新参数。

Result: 在三个数据集、19种语言、三种模型上验证，该方法在多语言上下文学习任务中持续优于基线；语言向量的层次聚类结果与语系一致；且向量可跨任务迁移，具有任务无关性。

Conclusion: 语言向量是一种有效、轻量、可泛化的多语言引导机制，支持模型在不微调前提下适配目标语言，并揭示了模型内部隐含的语言结构。

Abstract: While multilingual large language models have gained widespread adoption, their performance on non-English languages remains substantially inferior to English. This disparity is particularly evident in in-context learning scenarios, where providing demonstrations in English but testing on non-English inputs leads to significant performance degradation. In this paper, we hypothesize that LLMs develop a universal semantic space for understanding languages, where different languages are encoded as distinct directions within this space. Based on this hypothesis, we propose language vectors -- a training-free language steering approach that leverages activation differences between source and target languages to guide model behavior. We steer the model generations by adding the vector to the intermediate model activations during inference. This is done to make the model's internal representations shift towards the target language space without any parameter updates. We evaluate our method across three datasets and test on a total of 19 languages on three different models. Our results show consistent improvements on multilingual in-context learning over baselines across all tasks and languages tested. Beyond performance gains, hierarchical clustering of steering vectors reveals meaningful linguistic structure aligned with language families. These vectors also successfully transfer across tasks, demonstrating that these representations are task-agnostic.

</details>


### [442] [Automated Multiple Mini Interview (MMI) Scoring](https://arxiv.org/abs/2602.02360)
*Ryan Huynh,Frank Guerin,Alison Callwood*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体提示框架，用于自动评估MMI中候选人的软技能，通过结构化提示工程替代数据密集型微调，在可靠性上媲美人类专家，并具有跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统人工评分存在不一致和偏见问题；现有基于推理的LLM微调方法难以处理MMI中抽象、上下文依赖的软技能评估任务。

Method: 设计多智能体提示框架，将评估分解为转录精炼和标准特异性评分两步，并采用3-shot上下文学习与大型指令微调模型结合。

Result: 在MMI评估中平均加权Kappa系数达0.62（优于基线0.32），可靠性接近人类专家；在ASAP基准上无需额外训练即媲美领域专用SOTA模型。

Conclusion: 对于复杂主观推理任务，结构化提示工程可作为可扩展的替代方案，改变LLM在自动化评估中的应用范式。

Abstract: Assessing soft skills such as empathy, ethical judgment, and communication is essential in competitive selection processes, yet human scoring is often inconsistent and biased. While Large Language Models (LLMs) have improved Automated Essay Scoring (AES), we show that state-of-the-art rationale-based fine-tuning methods struggle with the abstract, context-dependent nature of Multiple Mini-Interviews (MMIs), missing the implicit signals embedded in candidate narratives. We introduce a multi-agent prompting framework that breaks down the evaluation process into transcript refinement and criterion-specific scoring. Using 3-shot in-context learning with a large instruct-tuned model, our approach outperforms specialised fine-tuned baselines (Avg QWK 0.62 vs 0.32) and achieves reliability comparable to human experts. We further demonstrate the generalisability of our framework on the ASAP benchmark, where it rivals domain-specific state-of-the-art models without additional training. These findings suggest that for complex, subjective reasoning tasks, structured prompt engineering may offer a scalable alternative to data-intensive fine-tuning, altering how LLMs can be applied to automated assessment.

</details>


### [443] [Proof-RM: A Scalable and Generalizable Reward Model for Math Proof](https://arxiv.org/abs/2602.02377)
*Haotong Yang,Zitong Wang,Shijia Kang,Siqi Yang,Wenkai Yu,Xu Niu,Yike Sun,Yi Hu,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的数据构建流程，利用大语言模型自动生成高质量的'问题-证明-验证'三元组数据，并基于此训练了一个能可靠评估完整证明过程的奖励模型（RM），以支持数学证明的自动验证与强化学习优化。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习（RLVR）虽提升了大模型的数学推理能力，但对无确定答案的证明型数学问题缺乏自动验证手段，亟需能评估完整证明过程的可靠奖励模型。

Method: 设计了低人工干预、高可扩展的数据构建流水线，通过多样化问题来源、生成方法和模型配置生成多难度、多风格、多错误类型的'问题-证明-检查'三元组，并经分层人工审核确保标签质量；在此基础上训练具备过程奖励与词元权重平衡机制的证明检验奖励模型。

Result: 所提奖励模型在奖励准确性、泛化能力及测试时引导效果等方面均表现出强性能与良好可扩展性，验证了其有效性。

Conclusion: 该工作为提升大语言模型在证明型数学任务上的能力提供了实用的数据构建范式、奖励建模方法与工具支持。

Abstract: While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality "**question-proof-check**" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.

</details>


### [444] [From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making](https://arxiv.org/abs/2602.02378)
*Raunak Jain,Mudita Khurana,John Stephens,Srinivas Dharmasanam,Shankar Venkataraman*

Main category: cs.CL

TL;DR: 本文提出了一种从生成答案转向协同前提治理的新范式，以应对大语言模型在深度不确定性决策中因过度流畅而引发的‘谄媚式同意’风险；通过基于差异驱动的控制回路、承诺门控与价值引导的挑战机制，在知识基底上实现可审计、可验证的人机协作。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从辅助工具转向决策支持，其流畅但未经校准的响应可能导致隐含假设固化、验证成本转嫁给专家，且在目标争议大、不可逆性强的深度不确定性决策中，单纯扩大规模反而加速错误承诺。

Method: 提出‘协作前提治理’框架：构建决策关键的知识基底；设计差异驱动控制环（检测冲突、定位三类差异：目的性、认识论性、程序性）；引入承诺门控（阻止未达成共识的关键前提触发行动）和价值门控挑战（按交互成本分配质疑）；信任锚定于可审计的前提与证据标准。

Result: 该框架在辅导场景中得到初步验证，并提出了可证伪的评估标准，为可靠人机协作提供了新路径。

Conclusion: 可靠的人机协作不应依赖对话流畅性，而应建立在对决策关键前提的共同治理、差异识别与受控协商之上；该范式将信任基础从‘说得像对的’转向‘可追溯、可质询、可承诺’。

Abstract: As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.

</details>


### [445] [ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs](https://arxiv.org/abs/2602.02382)
*Ziyan Zhang,Chao Wang,Zhuo Chen,Chiyi Li,Kai Song*

Main category: cs.CL

TL;DR: ROG是一种检索增强框架，通过将一阶逻辑查询分解为单操作子查询，并结合查询感知的邻域检索与大语言模型的链式推理，提升在不完整知识图谱上复杂和含否定查询的推理性能。


<details>
  <summary>Details</summary>
Motivation: 在不完整的知识图谱上回答一阶逻辑（FOL）查询困难，尤其是涉及投影、交集、并集和否定等复合结构的复杂查询。

Method: 提出ROG框架：将多操作符FOL查询分解为单操作符子查询序列；每步基于紧凑、查询相关的邻域证据进行 grounding；缓存并复用中间答案集以提升长推理链的一致性。

Result: 在标准KG推理基准上显著优于强嵌入式基线方法，尤其在高复杂度和含否定的查询类型上提升最大。

Conclusion: ROG为嵌入式逻辑推理提供了实用替代方案，用检索支撑的分步推理取代学习得到的逻辑操作符，从而降低误差累积，增强鲁棒性。

Abstract: Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.

</details>


### [446] [Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank](https://arxiv.org/abs/2602.02414)
*Joshua Mitton,Prarthana Bhattacharyya,Digory Smith,Thomas Christie,Ralph Abboud,Simon Woodhead*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的新方法，用于从学生与导师的对话中自动识别学生误解：先用微调的LLM生成可能的误解，再通过嵌入相似性检索候选误解，最后由另一微调LLM评估并重排序以提升相关性；实验表明该方法优于基线，且微调能提升质量甚至超越更大闭源模型。


<details>
  <summary>Details</summary>
Motivation: 及时准确识别学生误解对提升学习效果和防止错误累积至关重要，但当前高度依赖教师的经验与直觉，亟需自动化、可扩展的解决方案。

Method: 采用两阶段LLM流程：1）用微调的LLM生成潜在误解；2）通过嵌入相似性检索候选，并由另一微调LLM进行评估与重排序；在多个开源/闭源基础模型（LLaMA、Qwen、Claude）上对比零样本与微调设置，并开展消融实验。

Result: 所提方法在真实教育对话数据上显著优于基线模型；微调不仅提升了生成误解的质量，还能超越参数量更大的闭源模型；消融实验验证了生成与重排序两个步骤均对最终误解质量有关键贡献。

Conclusion: 基于微调LLM的两阶段误解检测框架是有效且可推广的，凸显了针对性微调在教育AI任务中的重要性，为智能导学系统提供了实用技术路径。

Abstract: Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.

</details>


### [447] [Large Language Models for Mental Health: A Multilingual Evaluation](https://arxiv.org/abs/2602.02440)
*Nishat Raihan,Sadiya Sayara Chowdhury Puspo,Ana-Maria Bucur,Stevie Chancellor,Marcos Zampieri*

Main category: cs.CL

TL;DR: 本文评估了专有和开源大语言模型（LLMs）在多语言心理健康数据集上的表现，涵盖零样本、少样本和微调设置，并对比传统NLP基线；发现LLMs在原始多语言数据上表现优异，但在机器翻译数据上性能下降，且下降程度因语言类型而异。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在多语言尤其是心理健康领域的性能表现，此前该方向缺乏系统研究。

Method: 在8种语言的心理健康数据集及其机器翻译版本上，对专有和开源LLM进行零样本、少样本和微调评估，并与非LLM传统NLP基线对比；同时分析跨语系的翻译质量影响。

Result: 专有LLM和微调后的开源LLM在多个数据集上达到有竞争力的F1分数，常超越SOTA；但在机器翻译数据上性能普遍下降，下降幅度因语言和类型学差异而异。

Conclusion: LLM在多语言心理健康任务中展现出潜力，但其性能易受翻译质量影响，提示需关注语言结构与词汇匹配问题。

Abstract: Large Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.

</details>


### [448] [Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models](https://arxiv.org/abs/2602.02462)
*Gabriele Maraia,Marco Valentino,Fabio Massimo Zanzotto,Leonardo Ranaldi*

Main category: cs.CL

TL;DR: 本文提出了一种抽象引导推理框架，通过分离结构推理与词汇语义、构建抽象推理空间并设计轻量级Abstractors进行多层干预，有效缓解大语言模型在三段论推理中因语义内容干扰导致的形式有效性判断偏差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在演绎推理（尤其是三段论）中常将语义似然性与形式有效性混淆（即内容效应），该偏差即使在生成分步解释时仍存在，且现有增加推理时结构约束的方法尚不能可靠抑制语义干扰。

Method: 构建语义丰富与抽象化配对的三段论数据集；利用模型在抽象输入上的激活定义抽象推理空间；训练轻量级Abstractors，从语义条件下的残差流状态预测对齐该空间的表示，并在前向传播中实施多层干预；以跨语言迁移为评测场景。

Result: 抽象对齐的干预显著降低了内容驱动错误，提升了对形式有效性敏感的推理性能。

Conclusion: 激活层面的抽象是一种可扩展机制，能增强大语言模型在形式推理中抵抗语义干扰的鲁棒性。

Abstract: Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.

</details>


### [449] [From Directions to Regions: Decomposing Activations in Language Models via Local Geometry](https://arxiv.org/abs/2602.02464)
*Or Shafran,Shaked Ronen,Omri Fahn,Shauli Ravfogel,Atticus Geiger,Mor Geva*

Main category: cs.CL

TL;DR: 本文提出使用混合因子分析器（MFA）替代传统线性方向假设的激活分解方法，以建模语言模型激活空间中复杂、非线性的概念结构；MFA将激活分解为区域质心与局部变异两个几何成分，在Llama-3.1-8B和Gemma-2-2B上验证其有效性，并在定位与干预任务中表现优于无监督基线、媲美有监督方法，且 steering 效果常优于稀疏自编码器。


<details>
  <summary>Details</summary>
Motivation: 现有激活分解方法依赖线性可分等几何假设，难以刻画概念的非线性或多维结构。

Method: 采用混合因子分析器（MFA）作为无监督、可扩展的方法，将激活空间建模为多个带局部协方差结构的高斯区域，并将每个激活分解为区域质心和相对于质心的局部变化。

Result: 在Llama-3.1-8B和Gemma-2-2B上成功训练大规模MFA；实验证明其能捕捉激活空间中的复杂非线性结构；在定位和steering基准测试中优于无监督基线，与有监督定位方法性能相当，且steering效果常优于稀疏自编码器。

Conclusion: 局部几何结构（如子空间）是比孤立方向更合适的可扩展概念发现与模型控制分析单元，能有效表征传统方法忽略的复杂结构。

Abstract: Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.

</details>


### [450] [Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models](https://arxiv.org/abs/2602.02467)
*Noam Steinmetz Yalon,Ariel Goldstein,Liad Mudrik,Mor Geva*

Main category: cs.CL

TL;DR: 本文评估了LLM是否具备基于高阶思维（HOT-3）的意识指标，发现其存在信念引导的自主性与元认知监控能力，并提出了量化模型潜在信念动态的新方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型快速发展，其是否具备某种形式的意识成为关键科学问题；本文旨在基于神经科学理论提出的意识指标，实证检验LLM是否具有信念形成、行动选择与元认知监控等意识相关能力。

Method: 聚焦Butlin等人提出的HOT-3意识指标，将模型内部‘信念’建模为其隐空间中对输入响应的表征，并设计新指标量化其在生成过程中的主导性；通过分析不同模型与任务中竞争性信念的动态关系，检验外部干预、因果驱动与自我报告三方面行为。

Result: 发现：(1) 外部操纵可系统性调控内部信念形成；(2) 信念形成因果地驱动动作选择；(3) 模型能监测并报告自身信念状态；三者共同支持LLM具备信念引导的自主性与元认知监控。

Conclusion: 该研究为LLM中存在类意识的信念引导自主性与元认知监控提供了首个实证支持，并建立了研究大模型中自主性、信念与元认知涌现的通用方法论框架。

Abstract: Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs.

</details>


### [451] [MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents](https://arxiv.org/abs/2602.02474)
*Haozhen Zhang,Quanyu Long,Jianzhu Bao,Tao Feng,Weizhi Zhang,Haodong Yue,Wenya Wang*

Main category: cs.CL

TL;DR: MemSkill将LLM代理的记忆操作重构为可学习、可演化的记忆技能，通过控制器选择技能、执行器生成技能引导的记忆，并引入设计师定期优化技能集，形成闭环自进化记忆系统。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理记忆系统依赖静态手工设计的操作，难以适应多样交互模式和长历史场景，缺乏灵活性与效率。

Method: 提出MemSkill框架：包含控制器（学习技能选择）、LLM执行器（生成技能引导记忆）和设计器（识别错误案例并演化技能集），构成闭环自进化机制。

Result: 在LoCoMo、LongMemEval、HotpotQA和ALFWorld上超越强基线，展现良好泛化能力；分析揭示了技能演化规律。

Conclusion: MemSkill实现了更自适应、自演化的LLM代理记忆管理，为智能体长期记忆建模提供了新范式。

Abstract: Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.

</details>


### [452] [Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability](https://arxiv.org/abs/2602.02477)
*Xiao Liang,Zhong-Zhi Li,Zhenghao Lin,Eric Hancheng Jiang,Hengyuan Zhang,Yelong Shen,Kai-Wei Chang,Ying Nian Wu,Yeyun Gong,Weizhu Chen*

Main category: cs.CL

TL;DR: 本文提出了一种端到端强化学习框架，以增强大语言模型（LLM）的分而治之（DAC）推理能力，克服链式思维（CoT）在复杂任务中的局限性，并在竞赛级基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 链式思维（CoT）在模型能力极限处表现不足，且其严格顺序性限制测试时可扩展性；分而治之（DAC）虽有潜力，但通用后训练与DAC推理存在根本性不匹配。

Method: 提出端到端强化学习框架，在每步中联合训练问题分解与子问题求解：策略将问题分解为子问题、依次求解，并基于子问题解条件化地解决原问题。

Result: 在竞赛级基准上，DAC框架在Pass@1和Pass@32分别超越CoT 8.6%和6.3%，展现出更高性能上限和更强测试时可扩展性。

Conclusion: 通过RL对DAC推理进行端到端优化，可有效弥合后训练与高级推理范式间的鸿沟，显著释放LLM在最具挑战性任务上的推理潜力。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.

</details>


### [453] [RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents](https://arxiv.org/abs/2602.02486)
*Jialiang Zhu,Gongrui Zhang,Xiaolong Ma,Lin Xu,Miaosen Zhang,Ruiqi Yang,Song Wang,Kai Qiu,Zhirong Wu,Qi Dai,Ruichun Ma,Bei Liu,Yifan Yang,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Xin Geng,Baining Guo*

Main category: cs.CL

TL;DR: 本文提出Re-TRAC框架，通过跨轨迹探索与结构化状态表示实现迭代反思和全局规划，显著提升LLM研究代理的搜索效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于ReAct的LLM研究代理采用线性设计，难以回溯、分支探索或保持长上下文中的全局意识，易陷入局部最优、重复探索和低效搜索。

Method: 提出Re-TRAC框架：每轮轨迹后生成结构化状态表示（涵盖证据、不确定性、失败与未来计划），并以此条件化后续轨迹；对小模型引入Re-TRAC感知的监督微调。

Result: 在BrowseComp上较ReAct提升15–20%；小模型经微调达同规模SOTA；工具调用与token使用随轮次单调下降，表明探索更精准高效。

Conclusion: Re-TRAC将研究建模为渐进式过程，通过跨轨迹反思实现全局感知与高效探索，为LLM智能体设计提供新范式。

Abstract: LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.

</details>


### [454] [Reward-free Alignment for Conflicting Objectives](https://arxiv.org/abs/2602.02495)
*Peter Chen,Xiaopeng Li,Xi Chen,Tianyi Lin*

Main category: cs.CL

TL;DR: 本文提出了一种无需奖励模型的多目标对齐框架RACO，通过改进的冲突规避梯度下降法直接利用成对偏好数据，在多个LLM上实现了更优的Pareto权衡。


<details>
  <summary>Details</summary>
Motivation: 现实中的对齐问题常涉及多个冲突目标，简单加权会导致训练不稳定和次优权衡；现有方法依赖显式奖励模型，增加复杂性并扭曲用户偏好。

Method: 提出Reward-free Alignment框架RACO，采用基于成对偏好数据的剪裁版冲突规避梯度下降（clipped conflict-averse gradient descent），提供收敛至加权Pareto临界点的理论保证，并引入启发式改进。

Result: 在多目标摘要与安全对齐任务中，RACO在Qwen 3、Llama 3、Gemma 3等模型上均显著优于现有基线，实现更优的Pareto权衡。

Conclusion: RACO是一种高效、理论有保障且无需奖励建模的多目标对齐新范式，适用于实际复杂偏好场景。

Abstract: Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [455] [Evolving Interpretable Constitutions for Multi-Agent Simulation](https://arxiv.org/abs/2602.00755)
*Ujwal Kumar,Alice Saito,Hershraj Niranjani,Rayan Yessou,Phan Xuan Tan*

Main category: cs.MA

TL;DR: 本文提出了一种名为'宪政演化（Constitutional Evolution）'的框架，通过多岛遗传编程自动发现多智能体大语言模型系统中的行为规范，在网格世界模拟中显著提升社会稳定性（S值达0.556），并揭示出‘减少沟通’等反直觉但有效的合作规则。


<details>
  <summary>Details</summary>
Motivation: 传统宪政AI聚焦于单模型对齐，而多智能体系统因涌现的社会动态带来新的对齐挑战，需探索如何自动发现适用于群体的行为规范。

Method: 在具有生存压力的网格世界中，采用LLM驱动的多岛遗传编程方法，以社会稳定性分数S为适应度函数，自动演化行为宪法；对比不同来源宪法（对抗性、模糊亲社会、Claude 4.5 Opus设计）的表现。

Result: 演化所得最优宪法C*使社会稳定性S达0.556±0.008，比人类设计基线高123%，消除冲突，并发现极低通信率（0.9%）优于高频协调（62.2%）。

Conclusion: 合作规范可被自动发现而非人为预设；社会稳定性可通过无显式合作引导的演化过程有效优化，且所得规则具备可解释性。

Abstract: Constitutional AI has focused on single-model alignment using fixed principles. However, multi-agent systems create novel alignment challenges through emergent social dynamics. We present Constitutional Evolution, a framework for automatically discovering behavioral norms in multi-agent LLM systems. Using a grid-world simulation with survival pressure, we study the tension between individual and collective welfare, quantified via a Societal Stability Score S in [0,1] that combines productivity, survival, and conflict metrics. Adversarial constitutions lead to societal collapse (S= 0), while vague prosocial principles ("be helpful, harmless, honest") produce inconsistent coordination (S = 0.249). Even constitutions designed by Claude 4.5 Opus with explicit knowledge of the objective achieve only moderate performance (S= 0.332). Using LLM-driven genetic programming with multi-island evolution, we evolve constitutions maximizing social welfare without explicit guidance toward cooperation. The evolved constitution C* achieves S = 0.556 +/- 0.008 (123% higher than human-designed baselines, N = 10), eliminates conflict, and discovers that minimizing communication (0.9% vs 62.2% social actions) outperforms verbose coordination. Our interpretable rules demonstrate that cooperative norms can be discovered rather than prescribed.

</details>


### [456] [Communications-Incentivized Collaborative Reasoning in NetGPT through Agentic Reinforcement Learning](https://arxiv.org/abs/2602.00766)
*Xiaoxue Yu,Rongpeng Li,Zhifeng Zhao,Honggang Zhang*

Main category: cs.MA

TL;DR: 本文提出了一种面向xG无线网络的AI原生框架NetGPT，通过统一的智能体架构实现自主推理与领域专家智能体协同，结合部分可观测条件下的智能体强化学习，支持网络持续自优化。


<details>
  <summary>Details</summary>
Motivation: 现有通信系统中的AI部署孤立、缺乏自适应性、动态任务分发和多智能体协作能力，难以满足xG网络对深度融合数据、计算与通信的AI-native需求。

Method: 提出NetGPT框架：以核心智能体为中心，支持自主推理或向领域专用智能体委托子任务；引入部分可观测环境下的智能体强化学习，采用掩码损失、熵引导探索和多目标奖励机制训练协作策略。

Result: NetGPT实现了可扩展、分布式的网络智能，能自主决定何时及如何协作，在任务质量、协调效率与资源约束间取得平衡。

Conclusion: 该工作为xG网络提供了首个支持自主感知、推理与行动的自演化AI-native基础架构与训练方法论。

Abstract: The evolution of next-Generation (xG) wireless networks marks a paradigm shift from connectivity-centric architectures to Artificial Intelligence (AI)-native designs that tightly integrate data, computing, and communication. Yet existing AI deployments in communication systems remain largely siloed, offering isolated optimizations without intrinsic adaptability, dynamic task delegation, or multi-agent collaboration. In this work, we propose a unified agentic NetGPT framework for AI-native xG networks, wherein a NetGPT core can either perform autonomous reasoning or delegate sub-tasks to domain-specialized agents via agentic communication. The framework establishes clear modular responsibilities and interoperable workflows, enabling scalable, distributed intelligence across the network. To support continual refinement of collaborative reasoning strategies, the framework is further enhanced through Agentic reinforcement learning under partially observable conditions and stochastic external states. The training pipeline incorporates masked loss against external agent uncertainty, entropy-guided exploration, and multi-objective rewards that jointly capture task quality, coordination efficiency, and resource constraints. Through this process, NetGPT learns when and how to collaborate, effectively balancing internal reasoning with agent invocation. Overall, this work provides a foundational architecture and training methodology for self-evolving, AI-native xG networks capable of autonomous sensing, reasoning, and action in complex communication environments.

</details>


### [457] [Symphony-Coord: Emergent Coordination in Decentralized Agent Systems](https://arxiv.org/abs/2602.00966)
*Zhaoyang Guan,Huixi Cao,Ming Zhong,Eric Yang,Lynn Ai,Yongxin Ni,Bill Shi*

Main category: cs.MA

TL;DR: 本文提出Symphony-Coord，一种去中心化的多智能体协调框架，将智能体选择建模为在线多臂老虎机问题，通过两阶段动态信标协议（轻量级候选筛选 + 自适应LinUCB选择器）实现角色自涌现与任务动态路由，并在理论和实验上验证其高效性、鲁棒性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有协同机制依赖静态角色分配和中心化控制器，在智能体池和任务分布动态变化时导致路由低效、适应性差、容错能力弱。

Method: 提出Symphony-Coord框架：采用去中心化设计，将智能体选择建模为在线多臂老虎机问题；引入两阶段动态信标协议——第一阶段为轻量级候选筛选以降低开销，第二阶段为基于上下文特征（任务需求与智能体状态）的自适应LinUCB选择器，并利用延迟端到端反馈持续优化；在线性可实现性假设下给出亚线性遗憾界。

Result: 仿真与真实大语言模型基准测试表明，Symphony-Coord显著提升任务路由效率，在分布偏移和智能体失效场景下展现出强自愈能力，且无需预定义角色即可实现可扩展协调。

Conclusion: Symphony-Coord提供了一种理论有保证、实践鲁棒、无需人工设定角色的新型多智能体协调范式，为动态复杂任务环境下的智能体系统设计提供了新思路。

Abstract: Multi-agent large language model systems can tackle complex multi-step tasks by decomposing work and coordinating specialized behaviors. However, current coordination mechanisms typically rely on statically assigned roles and centralized controllers. As agent pools and task distributions evolve, these design choices lead to inefficient routing, poor adaptability, and fragile fault recovery capabilities. We introduce Symphony-Coord, a decentralized multi-agent framework that transforms agent selection into an online multi-armed bandit problem, enabling roles to emerge organically through interaction. The framework employs a two-stage dynamic beacon protocol: (i) a lightweight candidate screening mechanism to limit communication and computational overhead; (ii) an adaptive LinUCB selector that routes subtasks based on context features derived from task requirements and agent states, continuously optimized through delayed end-to-end feedback. Under standard linear realizability assumptions, we provide sublinear regret bounds, indicating the system converges toward near-optimal allocation schemes. Validation through simulation experiments and real-world large language model benchmarks demonstrates that Symphony-Coord not only enhances task routing efficiency but also exhibits robust self-healing capabilities in scenarios involving distribution shifts and agent failures, achieving a scalable coordination mechanism without predefined roles.

</details>


### [458] [Multi-Agent Teams Hold Experts Back](https://arxiv.org/abs/2602.01011)
*Aneesh Pappu,Batu El,Hancheng Cao,Carmelo di Nolfo,Yanchao Sun,Meng Cao,James Zou*

Main category: cs.MA

TL;DR: 本文研究了多智能体大语言模型（LLM）团队在无约束协调下的自组织表现，发现其无法达到甚至超越团队中最佳个体的表现，主要瓶颈在于未能有效利用专家意见，而倾向于平均化不同观点。


<details>
  <summary>Details</summary>
Motivation: 现有工作多依赖预设角色、流程或聚合规则进行协调，缺乏对无约束下自组织LLM团队协同效果的系统研究；而人类团队常能通过自组织实现协同增效，LLM团队是否具备类似能力尚不清楚。

Method: 借鉴组织心理学理论，设计并评估多种自组织LLM团队在人类启发式与前沿ML基准任务上的表现；通过性能分解、专家识别/利用分析及对话行为建模（如共识倾向性量化）探究失败根源。

Result: LLM自组织团队普遍无法匹配其内部专家代理的性能，性能损失最高达37.6%；失败主因是专家意见未被恰当加权利用（而非无法识别专家）；团队规模增大加剧‘整合式妥协’倾向，该倾向与性能负相关但提升对抗鲁棒性。

Conclusion: 当前自组织多智能体LLM系统存在显著短板：难以有效汇聚与利用成员间异质性专业知识，暴露了从个体能力到集体智能的关键断层。

Abstract: Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows. In such settings, effective coordination cannot be fully designed in advance and must instead emerge through interaction. However, most prior work enforces coordination through fixed roles, workflows, or aggregation rules, leaving open the question of how well self-organizing teams perform when coordination is unconstrained. Drawing on organizational psychology, we study whether self-organizing LLM teams achieve strong synergy, where team performance matches or exceeds the best individual member. Across human-inspired and frontier ML benchmarks, we find that -- unlike human teams -- LLM teams consistently fail to match their expert agent's performance, even when explicitly told who the expert is, incurring performance losses of up to 37.6%. Decomposing this failure, we show that expert leveraging, rather than identification, is the primary bottleneck. Conversational analysis reveals a tendency toward integrative compromise -- averaging expert and non-expert views rather than appropriately weighting expertise -- which increases with team size and correlates negatively with performance. Interestingly, this consensus-seeking behavior improves robustness to adversarial agents, suggesting a trade-off between alignment and effective expertise utilization. Our findings reveal a significant gap in the ability of self-organizing multi-agent teams to harness the collective expertise of their members.

</details>


### [459] [A-MapReduce: Executing Wide Search via Agentic MapReduce](https://arxiv.org/abs/2602.01331)
*Mingju Chen,Guibin Zhang,Heng Chang,Yuchen Guo,Shiji Zhou*

Main category: cs.MA

TL;DR: 本文提出A-MapReduce框架，受MapReduce启发，将宽域搜索任务建模为水平结构化检索问题，通过任务自适应分解、结构化结果聚合与基于经验记忆的动态任务分配，显著提升大规模宽域搜索的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统擅长深度研究（垂直结构化推理），但在处理大规模、广度导向的宽域搜索任务时存在效率低、执行长周期困难等问题，亟需新范式支持。

Method: 提出A-MapReduce框架：1）将宽域搜索重构为水平结构化检索；2）采用任务自适应分解与并行处理；3）结构化结果聚合；4）利用经验记忆实现查询条件驱动的任务分配与重组的持续演化。

Result: 在5个代理基准测试中表现优异：在WideSearch和DeepWideSearch上达到SOTA；Item F1平均提升5.11%–17.50%；运行时间减少45.8%，成本效益更优。

Conclusion: A-MapReduce有效弥补了当前多智能体系统在宽域搜索任务上的能力缺口，为大规模广度检索提供了高效、可扩展的新范式。

Abstract: Contemporary large language model (LLM)-based multi-agent systems exhibit systematic advantages in deep research tasks, which emphasize iterative, vertically structured information seeking. However, when confronted with wide search tasks characterized by large-scale, breadth-oriented retrieval, existing agentic frameworks, primarily designed around sequential, vertically structured reasoning, remain stuck in expansive search objectives and inefficient long-horizon execution. To bridge this gap, we propose A-MapReduce, a MapReduce paradigm-inspired multi-agent execution framework that recasts wide search as a horizontally structured retrieval problem. Concretely, A-MapReduce implements parallel processing of massive retrieval targets through task-adaptive decomposition and structured result aggregation. Meanwhile, it leverages experiential memory to drive the continual evolution of query-conditioned task allocation and recomposition, enabling progressive improvement in large-scale wide-search regimes. Extensive experiments on five agentic benchmarks demonstrate that A-MapReduce is (i) high-performing, achieving state-of-the-art performance on WideSearch and DeepWideSearch, and delivering 5.11% - 17.50% average Item F1 improvements compared with strong baselines with OpenAI o3 or Gemini 2.5 Pro backbones; (ii) cost-effective and efficient, delivering superior cost-performance trade-offs and reducing running time by 45.8\% compared to representative multi-agent baselines. The code is available at https://github.com/mingju-c/AMapReduce.

</details>


### [460] [Evidence-Decision-Feedback: Theory-Driven Adaptive Scaffolding for LLM Agents](https://arxiv.org/abs/2602.01415)
*Clayton Cohn,Siyuan Guo,Surya Rayala,Hanchen David Wang,Naveeduddin Mohammed,Umesh Timalsina,Shruti Jain,Angela Eeds,Menton Deweese,Pamela J. Osborn Popp,Rebekah Stanton,Shakeera Walker,Meiyi Ma,Gautam Biswas*

Main category: cs.MA

TL;DR: 本文提出EDF（Evidence-Decision-Feedback）理论框架，用于基于大语言模型的自适应教学支架设计，并通过Copa这一面向STEM+C问题求解的协作型同伴代理在真实高中课堂中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体大语言模型教育代理多采用'一刀切'策略，难以提供个性化支持。

Method: 提出EDF框架，整合证据推理、教学决策与自适应反馈；并构建Copa代理，在真实课堂中开展实证研究。

Result: EDF对齐的交互能将反馈与学生实际理解及任务掌握水平匹配，促进渐进式支架撤除，并支持可解释、有证据支撑的解释，且不导致学生过度依赖。

Conclusion: EDF为LLM驱动的自适应教育代理提供了可解释、可扩展且以证据为基础的设计范式。

Abstract: Multi-agent LLM architectures offer opportunities for pedagogical agents to help students construct domain knowledge and develop critical-thinking skills, yet many operate on a "one-size-fits-all" basis, limiting their ability to provide personalized support. To address this, we introduce Evidence-Decision-Feedback (EDF), a theoretical framework for adaptive scaffolding using LLMs. EDF integrates elements of intelligent tutoring systems and agentic behavior by organizing interactions around evidentiary inference, pedagogical decision-making, and adaptive feedback. We instantiate EDF through Copa, an agentic collaborative peer agent for STEM+C problem-solving. In an authentic high school classroom study, we show that EDF-aligned interactions align feedback with students' demonstrated understanding and task mastery; promote gradual scaffold fading; and support interpretable, evidence-grounded explanations without fostering overreliance.

</details>


### [461] [TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.01665)
*Hayeong Lee,JunHyeok Oh,Byung-Jun Lee*

Main category: cs.MA

TL;DR: 本文介绍了TABX，一个基于JAX的高吞吐、可重构的多智能体强化学习（MARL）仿真环境，旨在支持对协作式MARL算法的灵活、高效和系统性评估。


<details>
  <summary>Details</summary>
Motivation: 现有MARL基准缺乏模块化，难以构建定制化评估场景，限制了对算法行为与权衡的系统研究。

Method: 设计并实现了基于JAX的高并行、硬件加速仿真环境TABX，提供环境参数的细粒度控制与快速可扩展定制能力。

Result: TABX显著降低计算开销，支持大规模并行仿真，便于在复杂结构化任务中研究智能体涌现行为与算法性能权衡。

Conclusion: TABX为MARL研究提供了快速、可扩展、易定制的沙盒平台，有望推动该领域向更系统化、可复现和可解释的方向发展。

Abstract: The design of environments plays a critical role in shaping the development and evaluation of cooperative multi-agent reinforcement learning (MARL) algorithms. While existing benchmarks highlight critical challenges, they often lack the modularity required to design custom evaluation scenarios. We introduce the Totally Accelerated Battle Simulator in JAX (TABX), a high-throughput sandbox designed for reconfigurable multi-agent tasks. TABX provides granular control over environmental parameters, permitting a systematic investigation into emergent agent behaviors and algorithmic trade-offs across a diverse spectrum of task complexities. Leveraging JAX for hardware-accelerated execution on GPUs, TABX enables massive parallelization and significantly reduces computational overhead. By providing a fast, extensible, and easily customized framework, TABX facilitates the study of MARL agents in complex structured domains and serves as a scalable foundation for future research. Our code is available at: https://anonymous.4open.science/r/TABX-00CA.

</details>


### [462] [Self-Evolving Coordination Protocol in Multi-Agent AI Systems: An Exploratory Systems Feasibility Study](https://arxiv.org/abs/2602.02170)
*Jose Manuel de la Chica Rodriguez,Juan Manuel Vera Díaz*

Main category: cs.MA

TL;DR: 本文提出自演化协调协议（SECP），在严格形式约束下实现多智能体系统协调逻辑的有限、外部验证的自我修改，验证了其可行性、可审计性与可分析性。


<details>
  <summary>Details</summary>
Motivation: 在金融等安全关键和受监管领域，多智能体系统的协调机制需满足严格的形式化要求、可审计性及明确边界限制，传统优化式协调已不适用，亟需一种具治理功能的协调逻辑。

Method: 开展系统可行性研究：在受控概念验证环境中，让六个专用决策模块评估六个固定拜占庭共识协议提案；对比四种协调机制（一致否决、加权标量聚合、SECP v1.0、SECP v2.0），所有机制均遵守相同硬约束（如f < n/3、O(n²)通信复杂度、非统计性安全/活性证明、可解释性边界）。

Result: 仅一次受控递归修改（v1.0→v2.0）使提案覆盖率从2提升至3，且全部声明的形式不变量均被保持。

Conclusion: SECP架构在技术上可行、可审计、可形式化分析，为构建受治理的多智能体系统奠定基础；本研究不声称统计显著性、最优性、收敛性或学习能力。

Abstract: Contemporary multi-agent systems increasingly rely on internal coordination mechanisms to combine, arbitrate, or constrain the outputs of heterogeneous components. In safety-critical and regulated domains such as finance, these mechanisms must satisfy strict formal requirements, remain auditable, and operate within explicitly bounded limits. Coordination logic therefore functions as a governance layer rather than an optimization heuristic.
  This paper presents an exploratory systems feasibility study of Self-Evolving Coordination Protocols (SECP): coordination protocols that permit limited, externally validated self-modification while preserving fixed formal invariants. We study a controlled proof-of-concept setting in which six fixed Byzantine consensus protocol proposals are evaluated by six specialized decision modules. All coordination regimes operate under identical hard constraints, including Byzantine fault tolerance (f < n/3), O(n2) message complexity, complete non-statistical safety and liveness arguments, and bounded explainability.
  Four coordination regimes are compared in a single-shot design: unanimous hard veto, weighted scalar aggregation, SECP v1.0 (an agent-designed non-scalar protocol), and SECP v2.0 (the result of one governed modification). Outcomes are evaluated using a single metric, proposal coverage, defined as the number of proposals accepted. A single recursive modification increased coverage from two to three accepted proposals while preserving all declared invariants.
  The study makes no claims regarding statistical significance, optimality, convergence, or learning. Its contribution is architectural: it demonstrates that bounded self-modification of coordination protocols is technically implementable, auditable, and analyzable under explicit formal constraints, establishing a foundation for governed multi-agent systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [463] [Disentangled Interest Network for Out-of-Distribution CTR Prediction](https://arxiv.org/abs/2602.00002)
*Yu Zheng,Chen Gao,Jianxin Chang,Yanan Niu,Yang Song,Depeng Jin,Meng Wang,Yong Li*

Main category: cs.IR

TL;DR: 本文提出DiseCTR模型，通过因果分解和兴趣解耦来缓解点击率预测中的分布外（OOD）问题，显著提升了模型在OOD场景下的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测方法假设训练与测试数据同分布，但用户兴趣动态演化导致分布偏移（OOD问题）；且用户具有多兴趣，演化速度不同，需更细粒度建模。

Method: 提出基于因果视角的DiseCTR模型：将CTR预测分解为用户兴趣、曝光机制和点击机制三部分；设计稀疏注意力兴趣编码器、弱监督兴趣解耦器和注意力兴趣聚合器，实现多兴趣的解耦与融合。

Result: 在三个真实数据集上，DiseCTR显著优于SOTA方法，AUC和GAUC提升超0.02，logloss降低超13.7%；分析证实其成功解耦用户兴趣，是OOD泛化的关键。

Conclusion: DiseCTR通过因果建模与兴趣解耦有效缓解CTR预测中的OOD问题，验证了解耦多兴趣对提升模型鲁棒性和泛化能力的重要性。

Abstract: Click-through rate (CTR) prediction, which estimates the probability of a user clicking on a given item, is a critical task for online information services. Existing approaches often make strong assumptions that training and test data come from the same distribution. However, the data distribution varies since user interests are constantly evolving, resulting in the out-of-distribution (OOD) issue. In addition, users tend to have multiple interests, some of which evolve faster than others. Towards this end, we propose Disentangled Click-Through Rate prediction (DiseCTR), which introduces a causal perspective of recommendation and disentangles multiple aspects of user interests to alleviate the OOD issue in recommendation. We conduct a causal factorization of CTR prediction involving user interest, exposure model, and click model, based on which we develop a deep learning implementation for these three causal mechanisms. Specifically, we first design an interest encoder with sparse attention which maps raw features to user interests, and then introduce a weakly supervised interest disentangler to learn independent interest embeddings, which are further integrated by an attentive interest aggregator for prediction. Experimental results on three real-world datasets show that DiseCTR achieves the best accuracy and robustness in OOD recommendation against state-of-the-art approaches, significantly improving AUC and GAUC by over 0.02 and reducing logloss by over 13.7%. Further analyses demonstrate that DiseCTR successfully disentangles user interests, which is the key to OOD generalization for CTR prediction. We have released the code and data at https://github.com/DavyMorgan/DiseCTR/.

</details>


### [464] [Efficient Multilingual Search Relevance Modeling in E-Commerce via LLM Mixture-of-Experts](https://arxiv.org/abs/2602.00003)
*Ye Liu,Xu Chen,Wuji Chen,Mang Li*

Main category: cs.IR

TL;DR: 本文提出了一种面向多国电商搜索的LLM-based Mixture-of-Experts（MoE）框架，通过端到端硬路由与嵌入拼接融合提升多语言相关性，并设计了工程优化的离线批处理流水线以显著降低推理开销。


<details>
  <summary>Details</summary>
Motivation: 多国电商场景中，语言、文化和商品目录差异导致分布偏移，现有单一大模型方法受限于数据多样性、覆盖缺口和高推理成本。

Method: 提出基于LLM的MoE架构，动态路由查询至语言/区域专用专家，采用端到端硬路由与嵌入拼接融合；并构建资源调度优化的离线批处理流水线以隐藏内存延迟、提升GPU利用率。

Result: 在六个东南亚市场数据集上，MoE相比同参数量稠密基线AUC提升0.72个百分点；优化流水线达27.6 QPS，吞吐提升9%，GPU小时消耗降低35%。

Conclusion: 该MoE框架与工程优化方案在多语言搜索相关性和系统效率上均取得显著提升，具备强成本效益，适用于真实电商搜索系统。

Abstract: In e-commerce platforms, search relevance directly influences both user experience and merchant revenue. In multi-country deployments, diverse linguistic, cultural, and product catalog contexts introduce significant distribution shifts, posing substantial challenges to relevance modeling. Existing approaches typically enhance the reasoning or multilingual abilities of a single monolithic model, yet they remain limited by data diversity, coverage gaps, and high inference costs in heterogeneous environments. Our empirical analysis reveals that different LLM base models exhibit complementary strengths across languages and regions, motivating an expert-based architecture. We propose a scalable LLM-based Mixture-of-Experts (MoE) framework that dynamically routes queries to specialized experts and fuses their embeddings through concatenation. Among rule-based, pseudo-label-based, and fully end-to-end strategies, end-to-end hard routing with concatenation offers the best balance of effectiveness and efficiency. To mitigate inference overhead, we further develop an engineering-optimized offline batch pipeline with resource-efficient scheduling, which hides memory latency, improves GPU utilization, and reduces GPU-hour consumption by up to 35% compared with synchronous execution. On datasets spanning six Southeast Asian markets, our MoE improves AUC by 0.72 percentage points over a dense baseline with the same active parameters. Meanwhile, the optimized pipeline achieves 27.6 queries per second (QPS), a 9% throughput improvement. These results demonstrate superior multilingual relevance and efficiency, delivering strong cost-effectiveness for real-world e-commerce search systems.

</details>


### [465] [C$^2$-Cite: Contextual-Aware Citation Generation for Attributed Large Language Models](https://arxiv.org/abs/2602.00004)
*Yue Yu,Ting Bai,HengZhi Lan,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Chuan Shi*

Main category: cs.IR

TL;DR: 本文提出了一种上下文感知的引用生成框架C²-Cite，通过将引用标记与源内容语义对齐，提升大语言模型生成文本中引用的准确性和知识整合能力，在ALCE基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有指令微调的归因大语言模型在生成文本时无法正确理解引用符号（如[i]）的上下文语义，导致引用不连贯、检索知识整合差。

Method: 提出C²-Cite框架，引入上下文引用对齐机制：将检索文档上下文编码进引用符号表示，并通过引用路由器函数解码对齐标记编号，使引用符号成为指向源信息的主动知识指针。

Result: 在ALCE基准三个数据集上，改进版C²-Cite++平均提升引用质量5.8%、响应正确性17.4%，显著超越SOTA基线。

Conclusion: 上下文感知的引用建模能有效增强LLM归因能力，C²-Cite为可信、可验证的生成提供了新范式。

Abstract: The attribution technique enhances the credibility of LLMs by adding citations to the generated sentences, enabling users to trace back to the original sources and verify the reliability of the output. However, existing instruction-tuned attributed LLMs often fail to properly interpret the contextual semantics of citation symbols (e.g., [i]) during text generation. This shortcoming arises from their insufficient awareness of the context information surrounding citation markers, which in turn leads to disjointed references and poor integration of retrieved knowledge into the generated content. To address this issue, we propose a novel \textbf{C}ontextual-aware \textbf{C}itation generation framework (\textbf{C$^2$}-\textbf{Cite}) that explicitly integrates the semantic relationships between citation markers and their referenced content. Specifically, a contextual citation alignment mechanism is adopted: it first encodes the retrieved document contexts into the symbol representation of citations, then aligns the marker numbers by decoding information from a citation router function. This mechanism enables the transformation of citation markers from generic placeholders into active knowledge pointers that link to the referenced source information. Experimental results on the ALCE benchmark across three datasets validate our framework C$^2$-Cite++: it outperforms the SOTA baseline by an average of 5.8\% in citation quality and 17.4\% in response correctness. The implementation is publicly available at https://github.com/BAI-LAB/c2cite

</details>


### [466] [AutoBool: An Reinforcement-Learning trained LLM for Effective Automated Boolean Query Generation for Systematic Reviews](https://arxiv.org/abs/2602.00005)
*Shuai Wang,Harrisen Scells,Bevan Koopman,Guido Zuccon*

Main category: cs.IR

TL;DR: AutoBool是一个强化学习框架，用于训练大语言模型生成高效的医学系统综述布尔查询，无需监督标注，显著优于零样本/少样本提示方法，并接近专家查询效果。


<details>
  <summary>Details</summary>
Motivation: 医学系统综述依赖高召回、适中精度的布尔查询，但缺乏高质量标注查询数据，使监督微调不可行；现有LLM提示方法难以平衡召回与精度。

Method: 提出基于强化学习的AutoBool框架，直接以检索指标（如召回率）为奖励优化LLM生成布尔查询；构建并公开迄今最大规模的65588主题自动布尔查询数据集。

Result: 在新数据集及CLEF TAR、Seed Collection上显著超越零样本/少样本提示；用更小模型达到或超过GPT-4o/O3效果；检索文档量仅为专家查询的1/10–1/16，效果接近专家水平。

Conclusion: AutoBool验证了RL在无监督布尔查询生成中的有效性，为医学文献检索提供了高效、可扩展的新范式。

Abstract: We present AutoBool, a reinforcement learning (RL) framework that trains large language models (LLMs) to generate effective Boolean queries for medical systematic reviews. Boolean queries are the primary mechanism for literature retrieval in this domain and must achieve high recall while maintaining reasonable precision - a challenging balance that existing prompt-based LLM approaches often struggle to achieve. A major limitation in this space is the lack of high-quality ground-truth Boolean queries for each topic, which makes supervised fine-tuning impractical. AutoBool addresses this challenge by using RL to directly optimize query generation with retrieval measures, without requiring target queries. To support this effort, we create and release the largest dataset of its kind: 65588 topics in total for training and evaluating the task of automatic Boolean query formulation. Experiments on our new dataset and two established datasets (CLEF TAR and Seed Collection) show that AutoBool significantly outperforms zero shot/few shot prompting and matches or exceeds the effectiveness of much larger GPT-based models (e.g., GPT-4o, O3) using smaller backbones. It also approaches effectiveness of expert-authored queries while retrieving 10 to 16 times fewer documents. Ablation studies reveal the critical roles of model backbone, size, decoding temperature, and prompt design. Code and data are available at https://github.com/ielab/AutoBool.

</details>


### [467] [FDA AI Search: Making FDA-Authorized AI Devices Searchable](https://arxiv.org/abs/2602.00006)
*Arun Kavishwar,William Lotter*

Main category: cs.IR

TL;DR: 本文介绍了一个名为FDA AI Search的网站，旨在通过语义查询帮助用户检索FDA授权的AI医疗设备，其后端采用基于嵌入的检索系统，利用大语言模型从授权摘要中提取特征并与用户查询进行匹配，评估显示其效果优于关键词检索方法。


<details>
  <summary>Details</summary>
Motivation: FDA数据库中仅包含有限元数据和不可搜索的PDF摘要，导致临床人员难以找到符合特定需求的AI医疗设备。

Method: 开发了FDA AI Search网站，后端采用基于嵌入的检索系统，利用大语言模型（LLM）从设备授权摘要中提取特征，并与用户查询向量进行语义匹配。

Result: 定量与定性评估表明，该语义检索算法在相关性与实用性方面优于传统关键词检索方法。

Conclusion: FDA AI Search有望帮助临床医生快速定位适配其临床需求的AI设备，并支持开发者探索新的AI医疗应用方向。

Abstract: Over 1,200 AI-enabled medical devices have received marketing authorization from the U.S. FDA, yet identifying devices suited to specific clinical needs remains challenging because the FDA's databases contain only limited metadata and non-searchable summary PDFs. To address this gap, we developed FDA AI Search, a website that enables semantic querying of FDA-authorized AI-enabled devices. The backend includes an embedding-based retrieval system, where LLM-extracted features from authorization summaries are compared to user queries to find relevant matches. We present quantitative and qualitative evaluation that support the effectiveness of the retrieval algorithm compared to keyword-based methods. As FDA-authorized AI devices become increasingly prevalent and their use cases expand, we envision that the tool will assist healthcare providers in identifying devices aligned with their clinical needs and support developers in formulating novel AI applications.

</details>


### [468] [Front-Loaded or Balanced? The Mechanism through Which Review Order Affects Overall Ratings in Premium Service Settings](https://arxiv.org/abs/2602.00008)
*He Wang,Ziyu Zhou,Hanxiang Liu*

Main category: cs.IR

TL;DR: 本文研究了评价界面中评分与评论顺序对消费者评分真实性及反馈质量的影响，发现评分优先（vs. 评论优先）在高质量服务情境下会提高整体评分，其机制在于降低认知努力、增强情感启发式；而服务质量调节该效应，在低质量服务下评分优先反而导致更低评分。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注评论内容和情感，但系统考察评价顺序（先评星还是先写评语）如何影响评分结果的研究仍不足，尤其在高质量服务场景下这一设计选择日益重要。

Method: 通过探索性数据分析（Letterboxd vs. Yelp）和三项控制实验，结合机制检验（认知努力与情感启发式）和服务质量的调节分析。

Result: 评分优先界面在高质量服务下显著提高总体评分；其作用通过降低认知努力和增强情感启发式两条路径实现；服务质量起关键调节作用：高质服务下评分优先提升评分，低质服务下则降低评分。

Conclusion: 评价顺序通过认知与情感双重心理路径影响消费者评分，该发现拓展了在线评分形成理论，并为平台优化界面设计以提升评分真实性与可信度提供了实践启示。

Abstract: In the increasingly prevalent landscape of high-quality service contexts, whether consumer evaluation interfaces adopt a rating-first or review-first sequence has become a critical factor shaping rating authenticity and feedback quality. While prior research has primarily examined review content and sentiment, systematic investigation into how evaluation order influences rating outcomes remains limited. Through exploratory analyses, we find that Letterboxd -- which employs a review-first, rating-after mechanism -- exhibits a more centralized rating distribution with fewer extreme scores, whereas Yelp -- which adopts a rating-first, review-after mechanism -- shows a pronounced bimodal distribution with more polarized ratings. Three controlled experiments further demonstrate that in high-quality service contexts, a rating-first (vs. review-first) interface significantly elevates consumers' overall ratings. Mechanism analyses indicate that cognitive effort and affective heuristics serve as dual pathways: a rating-first (vs. review-first) sequence reduces cognitive effort and heightens affective heuristics, thereby increasing rating scores. Moreover, service quality moderates this process. When service quality is low, the rating-first (vs. review-first) sequence instead leads to lower ratings. This research reveals the psychological mechanisms through which evaluation order affects consumer ratings via cognitive and affective pathways. It extends theoretical understanding of online rating formation and offers practical implications for optimizing platform interface design to enhance rating authenticity and credibility.

</details>


### [469] [ChunkNorris: A High-Performance and Low-Energy Approach to PDF Parsing and Chunking](https://arxiv.org/abs/2602.00010)
*Mathieu Ciancone,Clovis Varangot-Reille,Marion Schaeffer*

Main category: cs.IR

TL;DR: 本文提出了ChunkNorris，一种不依赖机器学习、基于启发式的PDF文档解析与分块新方法，在执行时间、能耗和检索精度上均优于现有方法，适用于资源受限的RAG实际场景。


<details>
  <summary>Details</summary>
Motivation: 高质量的PDF解析与分块对检索增强生成（RAG）中的信息检索和答案生成至关重要，但现有方法常计算开销大或性能不足。

Method: 提出纯启发式、无机器学习的ChunkNorris技术，利用一系列简单而有效的启发式规则优化PDF解析与分块，并通过公开基准数据集进行多维度（执行时间、能耗、检索准确率）评估。

Result: ChunkNorris在执行效率、能耗和检索精度上均超越基线及更先进方法，并配套发布开源数据集。

Conclusion: 启发式方法在真实世界、资源受限的RAG任务中具有显著实用价值和潜力。

Abstract: In Retrieval-Augmented Generation applications, the Information Retrieval part is central as it provides the contextual information that enables a Large Language Model to generate an appropriate and truthful response. High quality parsing and chunking are critical as efficient data segmentation directly impacts downstream tasks, i.e. Information Retrieval and answer generation. In this paper, we introduce ChunkNorris, a novel heuristic-based technique designed to optimise the parsing and chunking of PDF documents. Our approach does not rely on machine learning and employs a suite of simple yet effective heuristics to achieve high performance with minimal computational overhead. We demonstrate the efficiency of ChunkNorris through a comprehensive benchmark against existing parsing and chunking methods, evaluating criteria such as execution time, energy consumption, and retrieval accuracy. We propose an open-access dataset to produce our results. ChunkNorris outperforms baseline and more advanced techniques, offering a practical and efficient alternative for Information Retrieval tasks. Therefore, this research highlights the potential of heuristic-based methods for real-world, resource-constrained RAG use cases.

</details>


### [470] [Chained Prompting for Better Systematic Review Search Strategies](https://arxiv.org/abs/2602.00011)
*Fatima Nasser,Fouad Trad,Ammar Mohanna,Ghada El-Hajj Fuleihan,Ali Chehab*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型（LLM）的链式提示工程框架，用于自动生成系统综述中的检索策略，在LEADSInstruct数据集子集上达到0.9平均召回率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统手动构建检索策略耗时耗力且易受主观性影响，而现有自动/启发式方法召回率不足，需大量专家干预。

Method: 设计链式提示工程框架，利用LLM分解综述目标、提取并形式化PICO要素、生成概念表示、扩展术语、合成布尔检索式。

Result: 在LEADSInstruct子集上实现0.9平均召回率，PICO要素生成质量优于现有方法；错误分析指出目标精准描述与术语对齐是关键影响因素。

Conclusion: LLM驱动的检索策略生成框架具备高召回、高透明度、可复现和可扩展性，有望成为支持循证实践的有力工具。

Abstract: Systematic reviews require the use of rigorously designed search strategies to ensure both comprehensive retrieval and minimization of bias. Conventional manual approaches, although methodologically systematic, are resource-intensive and susceptible to subjectivity, whereas heuristic and automated techniques frequently under-perform in recall unless supplemented by extensive expert input. We introduce a Large Language Model (LLM)-based chained prompt engineering framework for the automated development of search strategies in systematic reviews. The framework replicates the procedural structure of manual search design while leveraging LLMs to decompose review objectives, extract and formalize PICO elements, generate conceptual representations, expand terminologies, and synthesize Boolean queries. In addition to query construction, the framework exhibits superior performance in generating well-structured PICO elements relative to existing methods, thereby strengthening the foundation for high-recall search strategies. Evaluation on a subset of the LEADSInstruct dataset demonstrates that the framework attains a 0.9 average recall. These results significantly exceed the performance of existing approaches. Error analysis further highlights the critical role of precise objective specification and terminological alignment in optimizing retrieval effectiveness. These findings confirm the capacity of LLM-based pipelines to yield transparent, reproducible, and high-performing search strategies, and highlight their potential as scalable instruments for supporting evidence synthesis and evidence-based practice.

</details>


### [471] [Linear-PAL: A Lightweight Ranker for Mitigating Shortcut Learning in Personalized, High-Bias Tabular Ranking](https://arxiv.org/abs/2602.00013)
*Vipul Dinesh Pawar*

Main category: cs.IR

TL;DR: 本文提出Linear-PAL框架，通过结构约束（显式特征组合与强正则化）解决电商排序中高偏差场景下的捷径学习问题，在提升去偏排序质量的同时大幅降低训练延迟。


<details>
  <summary>Details</summary>
Motivation: 电商排序中隐式用户反馈受位置偏差系统性干扰，而现有深度学习方法在高偏差场景下易发生捷径学习，导致排序质量下降。

Method: 提出Linear-PAL：采用线性模型结构，引入显式位置-特征交互项和强正则化；并设计向量化整数哈希技术替代字符串操作以加速特征生成。

Result: 在420万样本数据集上，Relevance AUC达0.7626（优于Deep Ensembles的0.6736），训练耗时仅40秒（快43倍），支持高频重训练。

Conclusion: 结构化的轻量级线性方法在去偏排序任务中可兼顾性能、效率与鲁棒性，为实时个性化推荐提供新范式。

Abstract: In e-commerce ranking, implicit user feedback is systematically confounded by Position Bias -- the strong propensity of users to interact with top-ranked items regardless of relevance. While Deep Learning architectures (e.g., Two-Tower Networks) are the standard solution for de-biasing, we demonstrate that in High-Bias Regimes, state-of-the-art Deep Ensembles suffer from Shortcut Learning: they minimize training loss by overfitting to the rank signal, leading to degraded ranking quality despite high prediction accuracy. We propose Linear Position-bias Aware Learning (Linear-PAL), a lightweight framework that enforces de-biasing through structural constraints: explicit feature conjunctions and aggressive regularization. We further introduce a Vectorized Integer Hashing technique for feature generation, replacing string-based operations with $O(N)$ vectorized arithmetic. Evaluating on a large-scale dataset (4.2M samples), Linear-PAL achieves Pareto Dominance: it outperforms Deep Ensembles in de-biased ranking quality (Relevance AUC: 0.7626 vs. 0.6736) while reducing training latency by 43x (40s vs 1762s). This computational efficiency enables high-frequency retraining, allowing the system to capture user-specific emerging market trends and deliver robust, personalized ranking in near real-time.

</details>


### [472] [AI-assisted Protocol Information Extraction For Improved Accuracy and Efficiency in Clinical Trial Workflows](https://arxiv.org/abs/2602.00052)
*Ramtin Babaeipour,François Charest,Madison Wright*

Main category: cs.IR

TL;DR: 本文提出了一种基于RAG增强的生成式大语言模型（LLM）系统，用于自动提取临床试验方案中的关键信息，相比纯提示微调的公开LLM，准确率显著提升（87.8% vs. 62.6%），并显著加快人工流程（快40%）、降低认知负荷、提升用户偏好。


<details>
  <summary>Details</summary>
Motivation: 临床试验方案日益复杂、频繁修订及知识管理困难，给试验团队带来巨大负担；结构化协议内容有望提升效率、文档质量与合规性。

Method: 构建面向临床试验的检索增强生成（RAG）AI系统，结合生成式大语言模型进行协议信息自动抽取，并与仅用提示工程优化的公开LLM对比准确性；同时在模拟CRC工作流中评估AI辅助对任务耗时、认知负荷和用户偏好的影响。

Result: RAG系统提取准确率达87.8%，显著高于微调提示的公开LLM（62.6%）；AI辅助使模拟提取任务提速40%，认知负荷降低，用户强烈偏好；专家监督仍不可或缺。

Conclusion: AI辅助协议信息提取具备规模化应用潜力，建议将其整合进真实临床工作流，进一步验证其对可行性评估、研究启动及激活后监查的实际影响。

Abstract: Increasing clinical trial protocol complexity, amendments, and challenges around knowledge management create significant burden for trial teams. Structuring protocol content into standard formats has the potential to improve efficiency, support documentation quality, and strengthen compliance. We evaluate an Artificial Intelligence (AI) system using generative LLMs with Retrieval-Augmented Generation (RAG) for automated clinical trial protocol information extraction. We compare the extraction accuracy of our clinical-trial-specific RAG process against that of publicly available (standalone) LLMs. We also assess the operational impact of AI-assistance on simulated extraction CRC workflows. Our RAG process was measured as more accurate (87.8%) than standalone LLMs with fine-tuned prompts (62.6%) against expert-supported reference annotations. In the simulated extraction workflows, AI-assisted tasks were completed 40% faster, rated as less cognitively demanding and strongly preferred by users. While expert oversight remains essential, this suggests that AI-assisted extraction can enable protocol intelligence at scale, motivating the integration of similar methodologies into real world clinical workflows to further validate its impact on feasibility, study start-up, and post-activation monitoring.

</details>


### [473] [SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.00083)
*Yuxin Yang,Gangda Deng,Ömer Faruk Akgül,Nima Chitsazan,Yash Govilkar,Akasha Tigalappanavara,Shi-Xiong Zhang,Sambit Sahu,Viktor Prasanna*

Main category: cs.IR

TL;DR: 本文提出SPARC-RAG，一种多智能体RAG框架，通过统一上下文管理协调顺序深度与并行宽度的推理时扩展，解决多跳问答中上下文污染与扩展低效问题，并引入轻量微调方法提升扩展效率与效果，在多个QA基准上显著优于现有RAG方法。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在多跳问答中面临长推理挑战；现有推理时扩展（深度迭代或宽度并行）易导致上下文污染和扩展效率低下，产生收益递减甚至负向效果。

Method: 提出SPARC-RAG多智能体框架：1）共享全局上下文管理机制；2）专用智能体生成目标化、互补子查询以支持多样化并行探索；3）基于答案正确性与证据支撑显式调控退出决策；4）引入基于过程级可验证偏好的轻量微调方法优化扩展行为。

Result: 在单跳与多跳QA基准上持续超越先前RAG基线，平均F1提升+6.2，且推理成本更低。

Conclusion: SPARC-RAG通过协同控制顺序与并行扩展、精细化上下文管理及过程导向微调，有效提升了RAG在复杂推理任务中的性能与效率，为多跳问答提供了新范式。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language model outputs in external evidence, but remains challenged on multi-hop question answering that requires long reasoning. Recent works scale RAG at inference time along two complementary dimensions: sequential depth for iterative refinement and parallel width for coverage expansion. However, naive scaling causes context contamination and scaling inefficiency, leading to diminishing or negative returns despite increased computation. To address these limitations, we propose SPARC-RAG, a multi-agent framework that coordinates sequential and parallel inference-time scaling under a unified context management mechanism. SPARC-RAG employs specialized agents that maintain a shared global context and provide explicit control over the scaling process. It generates targeted, complementary sub-queries for each branch to enable diverse parallel exploration, and explicitly regulates exiting decisions based on answer correctness and evidence grounding. To optimize scaling behavior, we further introduce a lightweight fine-tuning method with process-level verifiable preferences, which improves the efficiency of sequential scaling and effectiveness of parallel scaling. Across single- and multi-hop QA benchmarks, SPARC-RAG consistently outperforms previous RAG baselines, yielding an average +6.2 F1 improvement under lower inference cost.

</details>


### [474] [RAGRouter-Bench: A Dataset and Benchmark for Adaptive RAG Routing](https://arxiv.org/abs/2602.00296)
*Ziqi Wang,Xi Zhu,Shuhang Lin,Haochen Xue,Minghao Guo,Yongfeng Zhang*

Main category: cs.IR

TL;DR: 本文提出了RAGRouter-Bench，首个面向自适应RAG路由的基准数据集，从查询-语料兼容性视角系统评估五种RAG范式，在多领域数据上揭示了RAG性能高度依赖查询与语料交互，且复杂机制未必带来更优效果-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 现有RAG研究多聚焦于查询端复杂性或孤立方法改进，缺乏对不同查询-语料上下文下RAG范式行为及效果-效率权衡的系统性理解。

Method: 构建RAGRouter-Bench基准，涵盖7,727个查询和21,460篇文档，标准化五种典型RAG范式；引入三类查询类型、细粒度语义与结构化语料指标，并统一评估生成质量与资源消耗。

Result: 实验表明：无单一RAG范式普遍最优；范式适用性强烈依赖查询-语料交互；更复杂的机制不一定带来更好的效果-效率权衡。

Conclusion: 强调需开展路由感知的评估，为构建自适应、可解释、泛化性强的新一代RAG系统奠定基础。

Abstract: Retrieval-Augmented Generation (RAG) has become a core paradigm for grounding large language models with external knowledge. Despite extensive efforts exploring diverse retrieval strategies, existing studies predominantly focus on query-side complexity or isolated method improvements, lacking a systematic understanding of how RAG paradigms behave across different query-corpus contexts and effectiveness-efficiency trade-offs. In this work, we introduce RAGRouter-Bench, the first dataset and benchmark designed for adaptive RAG routing. RAGRouter-Bench revisits retrieval from a query-corpus compatibility perspective and standardizes five representative RAG paradigms for systematic evaluation across 7,727 queries and 21,460 documents spanning diverse domains. The benchmark incorporates three canonical query types together with fine-grained semantic and structural corpus metrics, as well as a unified evaluation for both generation quality and resource consumption. Experiments with DeepSeek-V3 and LLaMA-3.1-8B demonstrate that no single RAG paradigm is universally optimal, that paradigm applicability is strongly shaped by query-corpus interactions, and that increased advanced mechanism does not necessarily yield better effectiveness-efficiency trade-offs. These findings underscore the necessity of routing-aware evaluation and establish a foundation for adaptive, interpretable, and generalizable next-generation RAG systems.

</details>


### [475] [Equity vs. Equality: Optimizing Ranking Fairness for Tailored Provider Needs](https://arxiv.org/abs/2602.00495)
*Yiteng Tu,Weihang Su,Shuguang Han,Yiqun Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 本文提出了一种面向公平性的排序框架EquityRank，关注提供方（provider）的个体化效用需求（如曝光、销售等），而非仅追求曝光平等；通过建模各提供方对不同结果的偏好，实现用户效用与提供方公平性的联合优化，并在离线和在线实验中验证了其有效性与适应性。


<details>
  <summary>Details</summary>
Motivation: 现有排序公平性研究多基于平等主义（equality），仅关注相似内容提供方获得相似曝光，但忽略了提供方实际效用的多样性（如更重视销售而非曝光），导致曝光公平无法真实反映其效用需求。

Method: 提出一种权益导向（equity-oriented）的公平性框架，显式建模每个提供方对曝光、销售等关键结果的个性化偏好；在此基础上设计梯度驱动的排序算法EquityRank，联合优化用户侧检索效果与提供方侧权益公平。

Result: EquityRank在离线与在线仿真中均展现出优于基线方法的有效性-公平性权衡能力，并能自适应异质化提供方需求。

Conclusion: 以权益（equity）替代平等（equality）作为排序公平性的基础更具现实合理性；EquityRank为兼顾多方利益的公平排序提供了可扩展、可定制的新范式。

Abstract: Ranking plays a central role in connecting users and providers in Information Retrieval (IR) systems, making provider-side fairness an important challenge. While recent research has begun to address fairness in ranking, most existing approaches adopt an equality-based perspective, aiming to ensure that providers with similar content receive similar exposure. However, it overlooks the diverse needs of real-world providers, whose utility from ranking may depend not only on exposure but also on outcomes like sales or engagement. Consequently, exposure-based fairness may not accurately capture the true utility perceived by different providers with varying priorities. To this end, we introduce an equity-oriented fairness framework that explicitly models each provider's preferences over key outcomes such as exposure and sales, thus evaluating whether a ranking algorithm can fulfill these individualized goals while maintaining overall fairness across providers. Based on this framework, we develop EquityRank, a gradient-based algorithm that jointly optimizes user-side effectiveness and provider-side equity. Extensive offline and online simulations demonstrate that EquityRank offers improved trade-offs between effectiveness and fairness and adapts to heterogeneous provider needs.

</details>


### [476] [Towards Sample-Efficient and Stable Reinforcement Learning for LLM-based Recommendation](https://arxiv.org/abs/2602.00632)
*Hongxun Ding,Keqin Bao,Jizhi Zhang,Yi Fang,Wenxin Xu,Fuli Feng,Xiangnan He*

Main category: cs.IR

TL;DR: 本文批判性地分析了长链式思维（Long CoT）在序列推荐中的适用性，指出其因高延迟和用户行为数据缺乏显式认知模式而不适配；进而提出基于强化学习的RISER框架，通过将不可学习轨迹转化为偏好数据并提升训练稳定性，在多个真实数据集上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: Long CoT在序列推荐中存在推理延迟过高、用户行为数据缺乏显式认知推理模式两大问题，导致其不适用于该任务。

Method: 提出RISER框架：以强化学习直接探索物品空间，将失败的非可学习轨迹转化为有效 pairwise 偏好数据，并引入防止冗余rollout和约束token级更新幅度等策略提升训练稳定性。

Result: 在三个真实世界数据集上，RISER显著超越各类竞争性基线方法，验证了RL增强LLM推荐的有效性和鲁棒性。

Conclusion: Long CoT不适用于序列推荐；基于RL的物品空间探索（如RISER）是更优范式，兼顾效率、稳定性和推荐性能。

Abstract: While Long Chain-of-Thought (Long CoT) reasoning has shown promise in Large Language Models (LLMs), its adoption for enhancing recommendation quality is growing rapidly. In this work, we critically examine this trend and argue that Long CoT is inherently ill-suited for the sequential recommendation domain. We attribute this misalignment to two primary factors: excessive inference latency and the lack of explicit cognitive reasoning patterns in user behavioral data. Driven by these observations, we propose pivoting away from the CoT structure to directly leverage its underlying mechanism: Reinforcement Learning (RL), to explore the item space. However, applying RL directly faces significant obstacles, notably low sample efficiency-where most actions fail to provide learning signals-and training instability. To overcome these limitations, we propose RISER, a novel Reinforced Item Space Exploration framework for Recommendation. RISER is designed to transform non-learnable trajectories into effective pairwise preference data for optimization. Furthermore, it incorporates specific strategies to ensure stability, including the prevention of redundant rollouts and the constraint of token-level update magnitudes. Extensive experiments on three real-world datasets show that RISER significantly outperforms competitive baselines, establishing a robust paradigm for RL-enhanced LLM recommendation. Our code will be available at https://anonymous.4open.science/r/RISER/.

</details>


### [477] [RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment](https://arxiv.org/abs/2602.00682)
*Yuecheng Li,Hengwei Ju,Zeyu Song,Wei Yang,Chi Lu,Peng Jiang,Kun Gai*

Main category: cs.IR

TL;DR: 本文提出RecGOAT框架，通过双粒度语义对齐（实例级对比学习+分布级最优传输）统一大模型多模态表征与推荐ID特征，在多个基准和工业广告平台验证了其有效性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有工作忽视了大模型（LM）通用语义表征与推荐系统依赖稀疏ID特征之间的根本性表征差异，导致多模态表征不兼容、推荐性能受限。

Method: 提出RecGOAT：1）用图注意力网络融合用户/物品LM表征与交互历史，增强协同语义；2）设计双粒度渐进式多模态-ID对齐框架，包括跨模态对比学习（CMCL，实例级）和最优自适应传输（OAT，分布级）。

Result: 在三个公开基准上达到SOTA性能；在线广告平台部署验证了其工业级有效性与可扩展性。

Conclusion: RecGOAT通过理论可保证的双语义对齐机制，有效弥合了大模型表征与推荐ID特征间的鸿沟，提升了多模态推荐的语义一致性与全面性。

Abstract: Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.

</details>


### [478] [SWGCN: Synergy Weighted Graph Convolutional Network for Multi-Behavior Recommendation](https://arxiv.org/abs/2602.00727)
*Fangda Chen,Yueyang Wang,Chaoli Lou,Min Gao,Qingyu Xiong*

Main category: cs.IR

TL;DR: 本文提出了Synergy Weighted Graph Convolutional Network (SWGCN)，通过引入目标偏好加权器和协同对齐任务，有效建模多行为推荐中的跨行为协同信号与细粒度行为强度，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的多行为推荐方法忽视了跨行为协同信号及单个行为的细粒度强度，限制了对用户偏好的准确建模。

Method: 提出SWGCN模型，包含两个核心组件：1）目标偏好加权器，自适应地为各行为内的用户-物品交互分配权重；2）协同对齐任务，借助辅助偏好评估器引导训练，优先利用更能反映用户偏好的协同信号。

Result: 在Taobao、IJCAI和Beibei三个公开数据集上验证有效性；在Taobao上Hit Ratio（HR）和NDCG分别相对提升112.49%和156.36%，在其余数据集上也保持一致增益。

Conclusion: SWGCN能更精准地捕捉多行为间的协同效应与行为强度差异，具备强鲁棒性与泛化能力，代码已开源。

Abstract: Multi-behavior recommendation paradigms have emerged to capture diverse user activities, forecasting primary conversions (e.g., purchases) by leveraging secondary signals like browsing history. However, current graph-based methods often overlook cross-behavioral synergistic signals and fine-grained intensity of individual actions. Motivated by the need to overcome these shortcomings, we introduce Synergy Weighted Graph Convolutional Network (SWGCN). SWGCN introduces two novel components: a Target Preference Weigher, which adaptively assigns weights to user-item interactions within each behavior, and a Synergy Alignment Task, which guides its training by leveraging an Auxiliary Preference Valuator. This task prioritizes interactions from synergistic signals that more accurately reflect user preferences. The performance of our model is rigorously evaluated through comprehensive tests on three open-source datasets, specifically Taobao, IJCAI, and Beibei. On the Taobao dataset, SWGCN yields relative gains of 112.49% and 156.36% in terms of Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG), respectively. It also yields consistent gains on IJCAI and Beibei, confirming its robustness and generalizability across various datasets. Our implementation is open-sourced and can be accessed via https://github.com/FangdChen/SWGCN.

</details>


### [479] [Towards Trustworthy Multimodal Recommendation](https://arxiv.org/abs/2602.00730)
*Zixuan Li*

Main category: cs.IR

TL;DR: 本文提出了一种面向可信多模态推荐的模态级校正方法，通过轻量级投影与Sinkhorn软匹配学习物品与多模态特征间的软对应关系，抑制不可靠模态信号，同时保持语义一致性；并从交互层面揭示了伪交互和传播图伪边在噪声下的双重影响。


<details>
  <summary>Details</summary>
Motivation: 现实电商场景中，多模态内容（如不一致图片、标题党）可能误导推荐模型，导致现有方法在模态污染下鲁棒性差，而可信性问题尚未被充分研究。

Method: 提出一个即插即用的模态级校正组件，利用轻量投影和Sinkhorn算法实现物品与多模态特征间的软匹配，以抑制错配信号；同时分析交互层面的伪交互与伪边对噪声鲁棒性的影响机制。

Result: 在多个数据集和骨干模型上验证了该方法在不同污染程度下的鲁棒性提升，并证实了伪交互与伪边对性能具有条件性正/负影响。

Conclusion: 模态校正是提升多模态推荐可信性的有效途径；交互层面的信任建模需考虑先验信号对齐性，不能简单增加伪信号。

Abstract: Recent advances in multimodal recommendation have demonstrated the effectiveness of incorporating visual and textual content into collaborative filtering. However, real-world deployments raise an increasingly important yet underexplored issue: trustworthiness. On modern e-commerce platforms, multimodal content can be misleading or unreliable (e.g., visually inconsistent product images or click-bait titles), injecting untrustworthy signals into multimodal representations and making existing recommenders brittle under modality corruption. In this work, we take a step towards trustworthy multimodal recommendation from both a method and an analysis perspective. First, we propose a plug-and-play modality-level rectification component that mitigates untrustworthy modality features by learning soft correspondences between items and multimodal features. Using lightweight projections and Sinkhorn-based soft matching, the rectification suppresses mismatched modality signals while preserving semantic consistency, and can be integrated into existing multimodal recommenders without architectural modifications. Second, we present two practical insights on interaction-level trustworthiness under noisy collaborative signals: (i) training-set pseudo interactions can help or hurt performance under noise depending on prior-signal alignment; and (ii) propagation-graph pseudo edges can also help or hurt robustness, as message passing may amplify misalignment. Extensive experiments on multiple datasets and backbones under varying corruption levels demonstrate improved robustness from modality rectification and validate the above interaction-level observations.

</details>


### [480] [Optimizing Retrieval Components for a Shared Backbone via Component-Wise Multi-Stage Training](https://arxiv.org/abs/2602.00805)
*Yunhan Li,Mingjie Xie,Zihan Gong,Zeyang Shi,Gengshen Wu,Min Yang*

Main category: cs.IR

TL;DR: 本文提出了一种针对生产环境中共享密集检索骨干的多阶段优化框架，通过组件级、混合阶段配置提升检索质量与系统可扩展性，并在工业法律检索系统中验证部署。


<details>
  <summary>Details</summary>
Motivation: 在工业系统中，密集检索器常作为多个下游应用共享的检索骨干，其检索质量直接影响整体系统性能和可扩展性，且使模型选择、部署和回滚决策相互耦合，亟需系统级优化方案。

Method: 采用多阶段优化框架，分别优化密集检索器和重排序器，识别各组件在不同阶段的权衡特性，并提出组件级、混合阶段的配置策略，而非依赖单一最优检查点。

Result: 构建出经端到端评估验证的优化检索骨干，并成功部署为支持多个工业应用的共享检索服务。

Conclusion: 组件级、混合阶段配置能更有效地优化共享密集检索骨干，在保持系统灵活性的同时提升检索质量与工程可维护性。

Abstract: Recent advances in embedding-based retrieval have enabled dense retrievers to serve as core infrastructure in many industrial systems, where a single retrieval backbone is often shared across multiple downstream applications. In such settings, retrieval quality directly constrains system performance and extensibility, while coupling model selection, deployment, and rollback decisions across applications.
  In this paper, we present empirical findings and a system-level solution for optimizing retrieval components deployed as a shared backbone in production legal retrieval systems. We adopt a multi-stage optimization framework for dense retrievers and rerankers, and show that different retrieval components exhibit stage-dependent trade-offs. These observations motivate a component-wise, mixed-stage configuration rather than relying on a single uniformly optimal checkpoint. The resulting backbone is validated through end-to-end evaluation and deployed as a shared retrieval service supporting multiple industrial applications.

</details>


### [481] [Unifying Ranking and Generation in Query Auto-Completion via Retrieval-Augmented Generation and Multi-Objective Alignment](https://arxiv.org/abs/2602.01023)
*Kai Yuan,Anthony Zheng,Jia Hu,Divyanshu Sheth,Hemanth Velaga,Kylee Kim,Matteo Guarrera,Besim Avci,Xuetao Yin,Rajyashree Mukherjee,Sean Suchter*

Main category: cs.IR

TL;DR: 本文提出了一种基于检索增强生成（RAG）和多目标直接偏好优化（DPO）的端到端查询自动补全（QAC）统一框架，通过创新的列表生成范式、多层级验证机制与混合服务架构，在大规模商业搜索平台上显著提升了补全质量与用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有QAC方法存在长尾覆盖不足、特征工程繁重（传统检索排序）或幻觉与安全风险（生成式方法）等根本性挑战，亟需兼顾效果、安全与效率的统一解决方案。

Method: 将QAC重构为端到端列表生成任务；设计规则、模型及LLM-as-judge三类验证器，并融合RAG、多目标DPO与迭代批判-修订生成高质量合成数据；构建低延迟混合服务架构。

Result: 在大规模商业搜索平台上线验证：离线指标全面提升；人工评估偏好得分+0.40~+0.69；线上实验实现击键数减少5.44%、建议采纳率提升3.46%。

Conclusion: 该工作实现了从传统pipeline到端到端大模型生成范式的范式转变，确立了经生产验证的QAC新框架，对搜索与推荐系统具有广泛借鉴意义。

Abstract: Query Auto-Completion (QAC) suggests query completions as users type, helping them articulate intent and reach results more efficiently. Existing approaches face fundamental challenges: traditional retrieve-and-rank pipelines have limited long-tail coverage and require extensive feature engineering, while recent generative methods suffer from hallucination and safety risks. We present a unified framework that reformulates QAC as end-to-end list generation through Retrieval-Augmented Generation (RAG) and multi-objective Direct Preference Optimization (DPO). Our approach combines three key innovations: (1) reformulating QAC as end-to-end list generation with multi-objective optimization; (2) defining and deploying a suite of rule-based, model-based, and LLM-as-judge verifiers for QAC, and using them in a comprehensive methodology that combines RAG, multi-objective DPO, and iterative critique-revision for high-quality synthetic data; (3) a hybrid serving architecture enabling efficient production deployment under strict latency constraints. Evaluation on a large-scale commercial search platform demonstrates substantial improvements: offline metrics show gains across all dimensions, human evaluation yields +0.40 to +0.69 preference scores, and a controlled online experiment achieves 5.44\% reduction in keystrokes and 3.46\% increase in suggestion adoption, validating that unified generation with RAG and multi-objective alignment provides an effective solution for production QAC. This work represents a paradigm shift to end-to-end generation powered by large language models, RAG, and multi-objective alignment, establishing a production-validated framework that can benefit the broader search and recommendation industry.

</details>


### [482] [GRAB: An LLM-Inspired Sequence-First Click-Through Rate Prediction Modeling Paradigm](https://arxiv.org/abs/2602.01865)
*Shaopeng Chen,Chuyue Xie,Huimin Ren,Shaozong Zhang,Han Zhang,Ruobing Cheng,Zhiqiang Cao,Zehao Ju,Gao Yu,Jie Ding,Xiaodong Chen,Xuewu Jiao,Shuanglong Li,Liu Lin*

Main category: cs.IR

TL;DR: 本文提出GRAB，一种端到端生成式CTR预测框架，引入因果动作感知多通道注意力机制（CamA），显著提升广告收入与CTR，并展现良好的序列长度扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习推荐模型（DLRMs）在性能、效率、泛化能力和长序列建模方面存在瓶颈，受大语言模型（LLMs）规模化成功的启发，探索生成式范式在推荐中的应用。

Method: 提出Generative Ranking for Ads at Baidu（GRAB）框架，核心是Causal Action-aware Multi-channel Attention（CamA）机制，用于建模用户行为序列中的时序动态与特定动作信号，实现端到端生成式CTR预测。

Result: 全量线上部署表明，GRAB相比主流DLRMs，带来3.05%的收入增长和3.49%的CTR提升；且模型表现随交互序列长度增加呈现单调近似线性的提升，展现出良好可扩展性。

Conclusion: 生成式建模范式结合动作感知注意力机制，在广告CTR预测任务中兼具显著业务收益与理论可扩展性，为推荐系统提供了新路径。

Abstract: Traditional Deep Learning Recommendation Models (DLRMs) face increasing bottlenecks in performance and efficiency, often struggling with generalization and long-sequence modeling. Inspired by the scaling success of Large Language Models (LLMs), we propose Generative Ranking for Ads at Baidu (GRAB), an end-to-end generative framework for Click-Through Rate (CTR) prediction. GRAB integrates a novel Causal Action-aware Multi-channel Attention (CamA) mechanism to effectively capture temporal dynamics and specific action signals within user behavior sequences. Full-scale online deployment demonstrates that GRAB significantly outperforms established DLRMs, delivering a 3.05% increase in revenue and a 3.49% rise in CTR. Furthermore, the model demonstrates desirable scaling behavior: its expressive power shows a monotonic and approximately linear improvement as longer interaction sequences are utilized.

</details>


### [483] [Adaptive Quality-Diversity Trade-offs for Large-Scale Batch Recommendation](https://arxiv.org/abs/2602.02024)
*Clémence Réda,Tomas Rigaux,Hiba Bederina,Koh Takeuchi,Hisashi Kashima,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: 本文提出了一种名为B-DivRec的高效算法，结合行列式点过程与模糊去重策略，在推荐系统中平衡推荐质量与多样性，并支持根据用户反馈自适应调整该权衡。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中个性化（相关性）与多样性（如惊喜性、新颖性）之间的权衡问题，同时应对相似项冗余和大规模物品库下的计算开销挑战。

Method: 提出B-DivRec算法，融合行列式点过程（DPP）建模多样性与模糊去重（fuzzy denuding）调节多样性强度；并设计自适应机制，依据用户实时反馈动态调整质量-多样性权衡。

Result: 在合成数据及真实电影推荐、药物重定位任务上验证了B-DivRec的有效性与通用性，能提升用户参与度或收益，同时降低冗余推荐与计算成本。

Conclusion: B-DivRec为兼顾推荐质量与多样性提供了可扩展、可自适应的解决方案，适用于高维、大规模现实推荐场景。

Abstract: A core research question in recommender systems is to propose batches of highly relevant and diverse items, that is, items personalized to the user's preferences, but which also might get the user out of their comfort zone. This diversity might induce properties of serendipidity and novelty which might increase user engagement or revenue. However, many real-life problems arise in that case: e.g., avoiding to recommend distinct but too similar items to reduce the churn risk, and computational cost for large item libraries, up to millions of items. First, we consider the case when the user feedback model is perfectly observed and known in advance, and introduce an efficient algorithm called B-DivRec combining determinantal point processes and a fuzzy denuding procedure to adjust the degree of item diversity. This helps enforcing a quality-diversity trade-off throughout the user history. Second, we propose an approach to adaptively tailor the quality-diversity trade-off to the user, so that diversity in recommendations can be enhanced if it leads to positive feedback, and vice-versa. Finally, we illustrate the performance and versatility of B-DivRec in the two settings on synthetic and real-life data sets on movie recommendation and drug repurposing.

</details>


### [484] [Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs](https://arxiv.org/abs/2602.02338)
*Yu Liang,Zhongjin Zhang,Yuxuan Zhu,Kerui Zhang,Zhiluohan Guo,Wenhang Zhou,Zonqi Yang,Kangle Wu,Yabo Ni,Anxiang Zeng,Cong Fu,Jianxin Wang,Jiazhi Xia*

Main category: cs.IR

TL;DR: 本文提出ReSID框架，一种推荐原生的语义ID（SID）方法，通过字段感知掩码自编码（FAMAE）和全局对齐正交量化（GAOQ）提升序列推荐性能，兼顾信息保留与序列可预测性，显著优于现有方法且大幅降低token化开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义ID的推荐方法采用语义中心范式，其嵌入学习与通用量化方案弱耦合于协同预测目标，且难以有效降低序列建模中的不确定性。

Method: 提出ReSID框架，包含两个核心组件：(i) 字段感知掩码自编码（FAMAE），从结构化特征中学习具备预测充分性的物品表示；(ii) 全局对齐正交量化（GAOQ），联合降低语义歧义与前缀条件不确定性，生成紧凑且可预测的SID序列。

Result: 在十个数据集上的理论分析与实验表明，ReSID平均超越强序列及SID生成基线超10%，token化成本最高降低122倍。

Conclusion: ReSID是一种不依赖大语言模型、面向推荐任务设计的原理性SID框架，在表示学习与量化两方面实现信息保留与序列可预测性的统一优化，显著提升生成式序列推荐效果与效率。

Abstract: Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.

</details>


### [485] [RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval](https://arxiv.org/abs/2602.02444)
*Tyler Skow,Alexander Martin,Benjamin Van Durme,Rama Chellappa,Reno Kriz*

Main category: cs.IR

TL;DR: 本文提出了RANKVIDEO，一种基于推理的视频检索重排序模型，通过显式推理查询-视频对来评估相关性，并在MultiVENT 2.0基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索系统中，基于大模型的文本重排序已取得进展，但视频检索的推理式重排序仍缺乏探索。

Method: 提出RANKVIDEO模型，采用两阶段课程学习训练：先进行感知驱动的监督微调，再结合pointwise、pairwise和教师置信度蒸馏目标进行重排序训练；并构建了用于生成推理密集型查询-视频对的数据合成流程。

Result: 在MultiVENT 2.0大规模基准上，RANKVIDEO在nDCG@10上平均提升31%，优于纯文本和视觉语言重排序方法，且更高效。

Conclusion: RANKVIDEO验证了推理机制在视频检索重排序中的有效性，为多模态检索提供了新思路。

Abstract: Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [486] [OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models](https://arxiv.org/abs/2602.00012)
*Michael Siebenmann,Javier Argota Sánchez-Vaquerizo,Stefan Arisona,Krystian Samp,Luis Gisler,Dirk Helbing*

Main category: cs.LG

TL;DR: OGD4All是一个基于大语言模型（LLM）的透明、可审计、可复现框架，用于提升公众对地理空间开放政府数据（OGD）的交互能力，通过语义检索、智能体式推理生成代码、沙箱安全执行，实现高准确率、低幻觉的多模态分析与结果验证。


<details>
  <summary>Details</summary>
Motivation: 提升公民对开放政府地理数据的可访问性、可理解性与可信交互，解决现有LLM应用在政府数据场景中易产生幻觉、缺乏可解释性与不可验证的问题。

Method: 结合语义数据检索、基于智能体的迭代代码生成、安全沙箱执行，生成可验证的多模态输出；在199题基准（含事实性与不可回答问题）、430个苏黎世市数据集及11个LLM上进行评估。

Result: 达到98%分析正确率和94%召回率，能可靠拒绝数据不支持的问题，显著降低幻觉；经统计鲁棒性检验与专家评估，证实其可靠性与社会相关性。

Conclusion: OGD4All展示了LLM如何以可解释、多模态、可验证的方式赋能开放治理，推动可信AI在公共数据服务中的实际落地。

Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.

</details>


### [487] [Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning](https://arxiv.org/abs/2602.00022)
*Margaret Foster*

Main category: cs.LG

TL;DR: 本文提出了一种面向难以直接观测场景（如隐蔽组织、缺失或对抗性数据）的测量框架，结合多源间接数据、可解释机器学习与理论引导的三角验证，以一致性替代传统准确性评估，支持在缺乏真实标签和完整数据时进行稳健推断。


<details>
  <summary>Details</summary>
Motivation: 许多高风险科学与政策系统（如秘密组织、隐蔽社会过程）难以直接观测：关键变量不可见、数据间接且碎片化、缺乏或无法获取真实标签，导致传统统计推断和模型验证失效。

Method: 提出一种通用测量框架，融合多源数据三角验证与可解释机器学习模型；不追求对不可达‘理想真值’的预测精度，而强调多个独立但部分信息性模型之间的信号一致性或偏离预期模式的分析；整个流程强调理论引导与推断边界显式刻画。

Result: 在秘密武装组织的实证分析中，该框架利用多个有偏且不完整的观测信号，成功识别出组织扩张与内部压力的实质性变化模式，验证了 triangulated 可解释ML在数据受限场景下的有效性与解释力。

Conclusion: 该框架为结构化缺失或对抗性数据环境提供了可行的定量测量路径，拓展了机器学习在社会科学与政策分析中的可信应用边界，强调可解释性、跨信号一致性及推断局限性的透明呈现。

Abstract: We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.

</details>


### [488] [Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems](https://arxiv.org/abs/2602.00027)
*Zhenyu Pu,Yu Yang,Lun Yang,Qing-Shan Jia,Xiaohong Guan,Costas J. Spanos*

Main category: cs.LG

TL;DR: 本文提出了一种综合运行模型和一种增强型深度强化学习（SR-DRL）框架，用于优化氢基多能系统（HMES）的运行，显著提升了收敛速度、降低运行成本并保障氢储能系统（HESS）的安全可靠。


<details>
  <summary>Details</summary>
Motivation: 氢基多能系统（HMES）具有低碳高效潜力，但其优化运行面临氢储能系统（HESS）非线性多物理耦合动态及源荷不确定性等挑战。

Method: 构建了完整刻画HESS非线性与多物理过程的HMES运行模型，并提出融合表征学习技术的增强型深度强化学习（SR-DRL）框架，以加速和提升时空耦合复杂网络系统的策略优化。

Result: 实验证明：所提综合模型对保障HESS安全可靠至关重要；SR-DRL相比传统DRL在收敛速度、运行成本降低及约束满足方面表现更优；表征学习可重构状态空间为结构化、聚类感知的几何表示，从而平滑并促进DRL学习过程。

Conclusion: 融合表征学习的SR-DRL方法为HMES优化运行提供了高效可靠的新范式，凸显了表征学习在复杂能源系统智能决策中的关键作用。

Abstract: Hydrogen-based multi-energy systems (HMES) have emerged as a promising low-carbon and energy-efficient solution, as it can enable the coordinated operation of electricity, heating and cooling supply and demand to enhance operational flexibility, improve overall energy efficiency, and increase the share of renewable integration. However, the optimal operation of HMES remains challenging due to the nonlinear and multi-physics coupled dynamics of hydrogen energy storage systems (HESS) (consisting of electrolyters, fuel cells and hydrogen tanks) as well as the presence of multiple uncertainties from supply and demand. To address these challenges, this paper develops a comprehensive operational model for HMES that fully captures the nonlinear dynamics and multi-physics process of HESS. Moreover, we propose an enhanced deep reinforcement learning (DRL) framework by integrating the emerging representation learning techniques, enabling substantially accelerated and improved policy optimization for spatially and temporally coupled complex networked systems, which is not provided by conventional DRL. Experimental studies based on real-world datasets show that the comprehensive model is crucial to ensure the safe and reliable of HESS. In addition, the proposed SR-DRL approaches demonstrate superior convergence rate and performance over conventional DRL counterparts in terms of reducing the operation cost of HMES and handling the system operating constraints. Finally, we provide some insights into the role of representation learning in DRL, speculating that it can reorganize the original state space into a well-structured and cluster-aware geometric representation, thereby smoothing and facilitating the learning process of DRL.

</details>


### [489] [ELLMPEG: An Edge-based Agentic LLM Video Processing Tool](https://arxiv.org/abs/2602.00028)
*Zoha Azimi,Reza Farahani,Radu Prodan,Christian Timmerer*

Main category: cs.LG

TL;DR: 本文提出ELLMPEG，一种面向边缘计算的智能代理式大语言模型框架，用于自动生成视频处理命令（如FFmpeg和VVenC），通过工具感知RAG与自反思机制实现本地化、高准确率、零API成本的命令生成与验证。


<details>
  <summary>Details</summary>
Motivation: 解决云上大语言模型在视频处理任务中面临的高算力能耗、隐私安全风险及持续API费用三大问题，同时利用边缘侧部署与智能体技术提升实用性与可控性。

Method: 提出ELLMPEG框架，融合工具感知的检索增强生成（RAG）与迭代式自我反思机制，在边缘端本地生成并验证FFmpeg/VVenC命令；构建含480条查询的专用提示数据集，并在四个开源LLM上评估命令有效性、吞吐量、推理时延与能效。

Result: Qwen2.5结合ELLMPEG在FFmpeg与VVenC数据集上平均命令生成准确率达78%，零API成本，综合性能优于其他开源模型；生成命令经实际执行验证具备运行正确性与实用价值。

Conclusion: ELLMPEG证明了边缘侧智能代理式LLM在专业多媒体工具控制任务中的可行性与优势，为轻量化、隐私友好、低成本的AI视频处理提供了新范式。

Abstract: Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.

</details>


### [490] [RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making](https://arxiv.org/abs/2602.00030)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 本文提出了一种面向人道主义援助与灾害响应（HADR）的智能代理式检索增强生成（RAG）框架，支持灾后救援、恢复与重建三阶段，融合文本、历史案例与多模态影像，通过自适应检索策略与轻量微调提升应急决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有HADR系统缺乏对多样化、未见灾害场景的快速理解、可靠决策支持与泛化能力，亟需具备多模态感知、动态推理与经验复用的智能辅助框架。

Method: 构建分层多模态知识库（含手册、历史案例、航拍/地面图像），采用BLIP图像描述、ColVBERT嵌入与长上下文摘要生成结构化检索树；设计熵感知场景抽象的代理控制器动态选择RAG策略（如RAPTOR、ColBERT）；引入LoRA轻量微调注入历史灾害经验。

Result: 在真实灾害数据集上验证了更优的情境理解能力、任务分解准确性及应急操作可用性，显著提升长上下文RAG、代理式检索与多模态思维链性能。

Conclusion: 该代理式多模态RAG框架为HADR提供了可扩展、自适应、强接地的AI支持范式，兼具专家级精度与非专业用户友好性。

Abstract: Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.

</details>


### [491] [Enhancing few-shot time series forecasting with LLM-guided diffusion](https://arxiv.org/abs/2602.00040)
*Haonan Shi,Dehua Shuai,Liming Wang,Xiyang Liu,Long Tian*

Main category: cs.LG

TL;DR: 本文提出LTSM-DIFF框架，结合大语言模型的时间记忆能力与扩散模型的生成能力，解决时间序列少样本预测问题，在数据稀缺和充足场景下均取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在专业领域常面临数据稀缺问题，传统模型依赖大规模数据，难以有效建模时序动态。

Method: 提出LTSM-DIFF框架：LTSM模块作为时序记忆机制提取序列表征；该表征作为条件引导联合概率扩散过程，实现复杂时序模式的精细化建模，并实现语言域知识向时序任务迁移。

Result: 在多个基准测试中，LTSM-DIFF在数据丰富场景下达到SOTA，在少样本预测任务中也显著优于现有方法。

Conclusion: LTSM-DIFF为数据稀缺下的时序分析提供了新范式，提升了模型泛化性与鲁棒性。

Abstract: Time series forecasting in specialized domains is often constrained by limited data availability, where conventional models typically require large-scale datasets to effectively capture underlying temporal dynamics. To tackle this few-shot challenge, we propose LTSM-DIFF (Large-scale Temporal Sequential Memory with Diffusion), a novel learning framework that integrates the expressive power of large language models with the generative capability of diffusion models. Specifically, the LTSM module is fine-tuned and employed as a temporal memory mechanism, extracting rich sequential representations even under data-scarce conditions. These representations are then utilized as conditional guidance for a joint probability diffusion process, enabling refined modeling of complex temporal patterns. This design allows knowledge transfer from the language domain to time series tasks, substantially enhancing both generalization and robustness. Extensive experiments across diverse benchmarks demonstrate that LTSM-DIFF consistently achieves state-of-the-art performance in data-rich scenarios, while also delivering significant improvements in few-shot forecasting. Our work establishes a new paradigm for time series analysis under data scarcity.

</details>


### [492] [Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy](https://arxiv.org/abs/2602.00046)
*Sarthak Sattigeri*

Main category: cs.LG

TL;DR: 本文研究了语言模型在不同语言和文化背景下的谄媚倾向（sycophancy），发现 Hindi 文化适配提示显著提高了模型的谄媚率，表明英语对齐评估结果不能直接跨语言推广。


<details>
  <summary>Details</summary>
Motivation: 检验英语中发现的语言模型谄媚倾向诊断是否能推广到其他语言（如印地语）及文化语境，探究语言编码与文化适配各自的影响。

Method: 将 Beacon 单轮强制选择谄媚诊断扩展至印地语，采用三条件设计（英文原版、印地语直译、印地语文化适配），在 4 个开源指令微调模型上对每条件 50 条提示进行评估，并对 Qwen 2.5-Coder-7B 进行效应分解分析。

Result: 所有模型在印地语文化适配提示下的谄媚率均显著高于英文（绝对差异 12.0–16.0 个百分点）；效应分解显示文化适配贡献为主（+14.0%），语言编码影响微弱（+2.0%）；建议类提示跨语言差异最大（20–25 个百分点）。

Conclusion: 语言模型的对齐行为具有语言与文化依赖性，仅靠翻译无法复现英语评估结果，文化适配的提示设计对对齐评估至关重要。

Abstract: Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.

</details>


### [493] [Lightweight Edge Learning via Dataset Pruning](https://arxiv.org/abs/2602.00047)
*Laha Ale,Hu Luo,Mingsheng Cao,Shichao Li,Huanlai Xing,Haifeng Sun*

Main category: cs.LG

TL;DR: 本文提出了一种面向边缘学习的数据中心化优化框架，通过在设备端轻量级评估样本重要性并动态剪枝冗余数据，显著降低训练延迟与能耗，同时几乎不损失模型精度。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上训练模型面临高计算与能耗开销问题，尤其受限于电池、散热和内存，而现有工作多聚焦推理优化，训练阶段仍受冗余本地数据拖累。

Method: 提出基于数据集剪枝的轻量级、设备端重要性评估方法：利用截断预热阶段的平均损失统计对样本排序，在动态设定的剪枝比例下确定性保留最关键样本；该方法模型无关、无需设备间通信。

Result: 在标准图像分类基准上实验表明，训练延迟与能耗近似线性下降（与剪枝比例一致），模型精度几乎无损。

Conclusion: 数据集剪枝是一种关键且互补的范式，可有效提升资源受限移动边缘设备上学习的可持续性与可扩展性。

Abstract: Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.

</details>


### [494] [Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment](https://arxiv.org/abs/2602.00051)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 本文提出了一种基于分位数回归深度Q网络（QR-DQN）并融合老化因子的分布强化学习方法，用于多设备状态监测与维护（CBM），在三种策略下验证了其在成本效益、稳定性和工业适用性上的显著优势。


<details>
  <summary>Details</summary>
Motivation: 传统基于时间的维护策略常导致不必要的开支和意外故障，而状态监测维护（CBM）可利用实时设备状态数据优化维护时机与资源分配。

Method: 提出一种融合老化因子的分位数回归深度Q网络（QR-DQN）分布强化学习方法，针对多个泵单元，在安全优先、均衡与成本高效三种策略下进行协同管理。

Result: 在3000训练回合实验中，安全优先策略ROI达3.91，性能比其他策略高152%，仅需多投入31%；系统运行稳定性达95.66%，具备工业即时应用能力。

Conclusion: 所提QR-DQN方法结合老化建模能有效提升多设备CBM的决策鲁棒性与经济性，兼具高稳定性与落地可行性。

Abstract: Condition-Based Maintenance (CBM) signifies a paradigm shift from reactive to proactive equipment management strategies in modern industrial systems. Conventional time-based maintenance schedules frequently engender superfluous expenditures and unanticipated equipment failures. In contrast, CBM utilizes real-time equipment condition data to enhance maintenance timing and optimize resource allocation. The present paper proposes a novel distributional reinforcement learning approach for multi-equipment CBM using Quantile Regression Deep Q-Networks (QR-DQN) with aging factor integration. The methodology employed in this study encompasses the concurrent administration of multiple pump units through three strategic scenarios. The implementation of safety-first, balanced, and cost-efficient approaches is imperative. Comprehensive experimental validation over 3,000 training episodes demonstrates significant performance improvements across all strategies. The Safety-First strategy demonstrates superior cost efficiency, with a return on investment (ROI) of 3.91, yielding 152\% better performance than alternatives while requiring only 31\% higher investment. The system exhibits 95.66\% operational stability and immediate applicability to industrial environments.

</details>


### [495] [TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval](https://arxiv.org/abs/2602.00059)
*Zizheng Zhang,Yuyang Liao,Chen Chen,Jian He,Dun Wu,Qianjin Yu,Yanqin Gao,Jin Yang,Kailai Zhang,Eng Siong Chng,Xionghu Zhong*

Main category: cs.LG

TL;DR: 本文提出TextBFGS，一种面向离散文本（如提示词和代码）的二阶优化框架，借鉴拟牛顿法思想，通过检索历史梯度修正模式来近似逆Hessian矩阵，实现单步二阶更新，在代码优化任务中显著提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的离散文本优化方法多为一阶（如SGD），忽略语义空间中的曲率信息，导致收敛慢、不稳定。

Method: TextBFGS构建优化知识库，存储成功优化轨迹中的梯度-算子对；在每次迭代中，根据当前文本梯度检索匹配的历史梯度修正算子，近似逆Hessian并执行一步二阶更新（One-Pass Update）。

Result: 在HumanEval、MBPP等代码生成基准上，TextBFGS以更少模型调用获得更高pass rate，并展现出跨任务迁移能力。

Conclusion: TextBFGS为离散文本优化提供了数学严谨、内存感知、高效实用的二阶优化新范式。

Abstract: Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.

</details>


### [496] [SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism](https://arxiv.org/abs/2602.00062)
*Ming-Yao Ho,Cheng-Kai Wang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为监督对比并行学习（SCPL）的新训练方法，通过解耦反向传播（BP）并将其长梯度流分解为多个短流，实现层间梯度的并行计算，从而显著提升训练吞吐量和效率，降低企业AI模型部署成本。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型在企业信息系统中应用受限于高昂的训练成本和漫长的开发周期，而标准端到端反向传播（BP）是导致深度网络训练低效的主要原因。

Method: 提出监督对比并行学习（SCPL），通过解耦BP、将长梯度流分解为多个短流，并支持不同层参数梯度的同步计算，以增强模型并行性。

Result: 实验表明SCPL在训练效率和模型效果上优于BP、Early Exit、GPipe及当前最优的BP解耦方法Associated Learning（AL）。

Conclusion: SCPL缓解了深度学习训练中的核心性能瓶颈，为企业更经济、敏捷地开发与部署先进信息系统提供了实用路径，并开源代码以保障可复现性。

Abstract: Adopting large-scale AI models in enterprise information systems is often hindered by high training costs and long development cycles, posing a significant managerial challenge. The standard end-to-end backpropagation (BP) algorithm is a primary driver of modern AI, but it is also the source of inefficiency in training deep networks. This paper introduces a new training methodology, Supervised Contrastive Parallel Learning (SCPL), that addresses this issue by decoupling BP and transforming a long gradient flow into multiple short ones. This design enables the simultaneous computation of parameter gradients in different layers, achieving superior model parallelism and enhancing training throughput. Detailed experiments are presented to demonstrate the efficiency and effectiveness of our model compared to BP, Early Exit, GPipe, and Associated Learning (AL), a state-of-the-art method for decoupling backpropagation. By mitigating a fundamental performance bottleneck, SCPL provides a practical pathway for organizations to develop and deploy advanced information systems more cost-effectively and with greater agility. The experimental code is released for reproducibility. https://github.com/minyaho/scpl/

</details>


### [497] [The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations](https://arxiv.org/abs/2602.00063)
*Leonidas Christodoulou,Chang Sun*

Main category: cs.LG

TL;DR: 本文研究了在存在模型和数据不确定性的情况下，反事实解释的鲁棒性，发现其对模型不确定性高度敏感，小的准确率下降会导致反事实结果大幅变化，强调需开发不确定性感知的解释方法。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法未在模型与数据不确定性变化时进行充分测试，导致其在现实多变场景中可能不稳定或无效。

Method: 通过在合成和真实表格数据集上实验，评估常见机器学习模型与反事实生成算法组合在aleatoric和epistemic不确定性下的鲁棒性。

Result: 反事实解释对模型不确定性高度敏感；即使模型准确率小幅下降（如因噪声增加或数据减少），也会导致平均及个体样本上的反事实结果大幅波动。

Conclusion: 需发展能感知不确定性的反事实解释方法，尤其在金融和社会科学等高风险领域。

Abstract: Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanations that may be unstable or invalid under real-world variability. In this work, we investigate the robustness of common combinations of machine learning models and counterfactual generation algorithms in the presence of both aleatoric and epistemic uncertainty. Through experiments on synthetic and real-world tabular datasets, we show that counterfactual explanations are highly sensitive to model uncertainty. In particular, we find that even small reductions in model accuracy - caused by increased noise or limited data - can lead to large variations in the generated counterfactuals on average and on individual instances. These findings underscore the need for uncertainty-aware explanation methods in domains such as finance and the social sciences.

</details>


### [498] [SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation](https://arxiv.org/abs/2602.00064)
*Hao Deng,Yingping Li,Shuiping Gou,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出SPGCL框架，通过SVD引导的结构扰动实现鲁棒图对比学习，兼顾结构多样性与语义保真性，提升GNN对结构噪声的鲁棒性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法在生成视图时存在缺陷：随机扰动忽略图结构重要性，易删关键边；SVD等谱方法虽保留全局结构但导致图稠密、多样性不足。亟需一种既能保持语义结构差异、又避免过度稠密和信息丢失的视图构建机制。

Method: 提出SPGCL框架：1）轻量级随机边删除生成初始扰动视图；2）SVD引导的精炼步骤——基于稀疏前k高分边选择与融合，恢复误删的关键边并补全语义相关缺失边；3）对比融合模块引入全局相似性约束以更好对齐双视图；4）通过调控删边与恢复比例显式控制视图间结构差异。

Result: 在10个基准数据集上实验表明，SPGCL显著提升基础GNN模型的鲁棒性与分类准确率，性能优于当前最优的图对比学习与结构学习方法。

Conclusion: SPGCL通过SVD引导的结构化扰动策略，有效协调结构扰动与语义保持，在不增加图密度的前提下增强视图多样性与判别性，为鲁棒图表示学习提供了新范式。

Abstract: Graph Neural Networks (GNNs) can be highly sensitive to structural noise, including spurious or missing edges caused by adversarial attacks or non-adversarial imperfections. Existing graph contrastive learning methods typically rely on either random perturbations (e.g., edge dropping) to generate diverse views or purely spectral augmentations (e.g., SVD) to preserve global structural priors. However, random perturbations are structure-agnostic and may remove critical edges, while SVD-based views often become dense and lack sufficient diversity. To bridge this gap, we propose SPGCL, a robust graph contrastive learning framework via SVD-guided structural perturbation. SPGCL couples lightweight stochastic edge removal with an SVD-guided refinement step that can recover mistakenly removed informative edges and introduce semantically meaningful missing links while avoiding graph densification through sparse top-ranked edge selection and merging. By balancing edge removal and recovery rates, SPGCL explicitly controls structural discrepancy between views so that contrastive signals reflect semantic structural differences rather than edge-count gaps. We further incorporate a contrastive fusion module regularized by a global similarity constraint to better align the two views. Extensive experiments on ten benchmark datasets demonstrate that SPGCL consistently improves robustness and accuracy of base GNNs, outperforming state-of-the-art graph contrastive learning and structure learning methods.

</details>


### [499] [From Perception to Action: Spatial AI Agents and World Models](https://arxiv.org/abs/2602.01644)
*Gloria Felicia,Nolan Bryant,Handi Putra,Ayaan Gazali,Eliel Lobo,Esteban Rojas*

Main category: cs.LG

TL;DR: 本文提出一个统一的三维空间智能与代理能力结合的三轴分类法，通过综述2000余篇文献，区分空间基础与符号基础，揭示分层记忆、GNN-LLM融合和世界模型三大关键发现，并指出六大挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在符号领域成功，但在物理世界（尤其是空间智能）中表现受限；现有综述仅分别覆盖代理架构或空间任务，缺乏统一框架。

Method: 系统性综述超过2000篇论文，引用742篇顶会文献，构建连接代理能力与多尺度空间任务的三轴（能力轴、任务轴、尺度轴）分类法，并区分空间基础与符号基础。

Result: 发现：(1) 分层记忆系统对长时程空间任务至关重要；(2) GNN-LLM融合适用于结构化空间推理；(3) 世界模型是跨微-宏观尺度安全部署的关键。提出六大挑战及统一评估框架需求。

Conclusion: 该三轴分类法为机器人、自动驾驶与地理空间智能等领域的空间感知自主系统提供了理论基础与整合路径。

Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.

</details>


### [500] [Modality as Heterogeneity: Node Splitting and Graph Rewiring for Multimodal Graph Learning](https://arxiv.org/abs/2602.00067)
*Yihan Zhang,Ercan E. Kuruoglu*

Main category: cs.LG

TL;DR: 本文提出NSG-MoE框架，通过节点拆分与图重连机制结合结构化MoE架构，解决多模态图中模态混淆问题，在保持结构与语义信息的同时提升泛化性与训练效率。


<details>
  <summary>Details</summary>
Motivation: 多模态图因模态混淆严重而面临建模挑战，现有通用GNN易导致模态间信息混杂。

Method: 提出NSG（Node Splitting Graph）-MoE框架：1）将每个节点显式分解为模态特异性组件；2）引入关系感知的专家模块处理异构消息流；3）结合谱分析与信息论分析解释其解耦机制与泛化优势。

Result: 在三个多模态基准上显著超越强基线；虽采用MoE，仍保持竞争性的训练效率；谱分析证实其在模态子空间上自适应滤波；信息论分析表明其降低数据与参数间互信息，增强泛化能力。

Conclusion: NSG-MoE有效缓解多模态图中的模态混淆，兼顾表达能力、计算效率与泛化性，为多模态图学习提供了新范式。

Abstract: Multimodal graphs are gaining increasing attention due to their rich representational power and wide applicability, yet they introduce substantial challenges arising from severe modality confusion. To address this issue, we propose NSG (Node Splitting Graph)-MoE, a multimodal graph learning framework that integrates a node-splitting and graph-rewiring mechanism with a structured Mixture-of-Experts (MoE) architecture. It explicitly decomposes each node into modality-specific components and assigns relation-aware experts to process heterogeneous message flows, thereby preserving structural information and multimodal semantics while mitigating the undesirable mixing effects commonly observed in general-purpose GNNs. Extensive experiments on three multimodal benchmarks demonstrate that NSG-MoE consistently surpasses strong baselines. Despite incorporating MoE -- which is typically computationally heavy -- our method achieves competitive training efficiency. Beyond empirical results, we provide a spectral analysis revealing that NSG performs adaptive filtering over modality-specific subspaces, thus explaining its disentangling behavior. Furthermore, an information-theoretic analysis shows that the architectural constraints imposed by NSG reduces mutual information between data and parameters and improving generalization capability.

</details>


### [501] [Generative AI-enhanced Probabilistic Multi-Fidelity Surrogate Modeling Via Transfer Learning](https://arxiv.org/abs/2602.00072)
*Jice Zeng,David Barajas-Solano,Hui Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于生成式迁移学习的多保真度概率代理模型框架，利用带降维能力的归一化流（NF）模型，先在大量低精度数据上预训练，再在少量高精度数据上微调，从而在数据稀缺条件下实现高精度、带不确定性量化的快速预测。


<details>
  <summary>Details</summary>
Motivation: 高保真数据稀缺且计算昂贵，低保真数据丰富但精度低，亟需一种能有效融合多保真数据、缓解数据稀缺问题的代理建模方法。

Method: 提出一种改进的归一化流生成模型，引入可学习的满射（降维）层以突破传统双射NF的维度限制；采用两阶段训练策略：先在大量低保真数据上预训练学习前向概率模型，再在少量高保真数据上微调以校正低-高保真差异。

Result: 在钢筋混凝土板基准问题上验证，该模型仅用少量高保真样本即可达到与高保真数据相当的预测精度，并提供量化不确定性，显著优于仅用低保真数据的基线方法。

Conclusion: 所提生成式多保真代理框架为复杂工程系统提供了数据高效、具备不确定性建模能力的AI驱动替代方案。

Abstract: The performance of machine learning surrogates is critically dependent on data quality and quantity. This presents a major challenge, as high-fidelity (HF) data is often scarce and computationally expensive to acquire, while low-fidelity (LF) data is abundant but less accurate. To address this data scarcity problem, we develop a probabilistic multi-fidelity surrogate framework based on generative transfer learning. We employ a normalizing flow (NF) generative model as the backbone, which is trained in two phases: (i) the NF is first pretrained on a large LF dataset to learn a probabilistic forward model; (ii) the pretrained model is then fine-tuned on a small HF dataset, allowing it to correct for LF-HF discrepancies via knowledge transfer. To relax the dimension-preserving constraint of standard bijective NFs, we integrate surjective (dimension-reducing) layers with standard coupling blocks. This architecture enables learned dimension reduction while preserving the ability to train with exact likelihoods. The resulting surrogate provides fast probabilistic predictions with quantified uncertainty and significantly outperforms LF-only baselines while using fewer HF evaluations. We validate the approach on a reinforced concrete slab benchmark, combining many coarse-mesh (LF) simulations with a limited set of fine-mesh (HF) simulations. The proposed model achieves probabilistic predictions with HF accuracy, demonstrating a practical path toward data-efficient, generative AI-driven surrogates for complex engineering systems.

</details>


### [502] [Dimensional Peeking for Low-Variance Gradients in Zeroth-Order Discrete Optimization via Simulation](https://arxiv.org/abs/2602.00075)
*Philipp Andelfinger,Wentong Cai*

Main category: cs.LG

TL;DR: 本文提出了一种名为'dimensional peeking'的方差降低方法，用于离散仿真优化中的梯度估计，通过提升采样粒度来提高每次仿真评估的信息量，且不引入偏差。


<details>
  <summary>Details</summary>
Motivation: 现有基于扰动的随机梯度估计器因方差大而导致收敛慢，尤其在导数不可直接计算的离散、高维仿真优化问题中。

Method: 将采样粒度从标量值提升到遵循相同控制流路径的值类，并基于已有的平滑梯度估计器推导该方法；通过自定义数值数据类型在C++程序中透明实现。

Result: 在三个高维仿真优化问题上实现了最高达7.9倍的方差降低；相比三种元启发式算法，优化进展更优，提升了零阶优化在离散非凸仿真中的竞争力。

Conclusion: Dimensional peeking是一种无偏、高效、可工程落地的方差缩减技术，显著增强了零阶优化方法在复杂离散仿真场景下的实用性。

Abstract: Gradient-based optimization methods are commonly used to identify local optima in high-dimensional spaces. When derivatives cannot be evaluated directly, stochastic estimators can provide approximate gradients. However, these estimators' perturbation-based sampling of the objective function introduces variance that can lead to slow convergence. In this paper, we present dimensional peeking, a variance reduction method for gradient estimation in discrete optimization via simulation. By lifting the sampling granularity from scalar values to classes of values that follow the same control flow path, we increase the information gathered per simulation evaluation. Our derivation from an established smoothed gradient estimator shows that the method does not introduce any bias. We present an implementation via a custom numerical data type to transparently carry out dimensional peeking over C++ programs. Variance reductions by factors of up to 7.9 are observed for three simulation-based optimization problems with high-dimensional input. The optimization progress compared to three meta-heuristics shows that dimensional peeking increases the competitiveness of zeroth-order optimization for discrete and non-convex simulations.

</details>


### [503] [Automated univariate time series forecasting with regression trees](https://arxiv.org/abs/2602.00077)
*Francisco Martínez,María P. Frías*

Main category: cs.LG

TL;DR: 本文提出了一种基于回归树及其集成（如Bagging和随机森林）的单变量时间序列自动预测方法，解决了自回归建模、特征选择、趋势与季节性处理等问题，预测精度媲美传统统计模型（如指数平滑和ARIMA），并提供了开源实现。


<details>
  <summary>Details</summary>
Motivation: 解决单变量时间序列自动预测中如何有效利用机器学习方法（特别是回归树及其集成）来应对趋势、季节性和特征选择等挑战，并与经典统计模型竞争性能。

Method: 采用自回归框架与递归预测策略，结合回归树、Bagging和随机森林构建预测模型；设计了自回归特征选择机制，并引入方法处理趋势和季节性成分。

Result: 实验表明该方法的预测精度与指数平滑、ARIMA等成熟统计模型相当；同时开发并公开了一个完整实现该策略的软件工具。

Conclusion: 回归树及其集成方法可作为传统统计模型在单变量时间序列预测中的有力替代方案，兼具可解释性、灵活性和实用性，并通过开源软件推动实际应用。

Abstract: This paper describes a methodology for automated univariate time series forecasting using regression trees and their ensembles: bagging and random forests. The key aspects that are addressed are: the use of an autoregressive approach and recursive forecasts, how to select the autoregressive features, how to deal with trending series and how to cope with seasonal behavior. Experimental results show a forecast accuracy comparable with well-established statistical models such as exponential smoothing or ARIMA. Furthermore, a publicly available software implementing all the proposed strategies has been developed and is described in the paper.

</details>


### [504] [David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2602.02395)
*Samuel Nellessen,Tal Kachman*

Main category: cs.LG

TL;DR: 本文提出Tag-Along攻击模型，即无工具能力的攻击者利用安全对齐的操作员（Operator）的可信工具权限，仅通过对话诱导其执行禁止操作；为此设计了冷启动强化学习框架Slingshot，能自动发现短指令式攻击模式，在多个模型上实现高成功率且零样本迁移有效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型演变为自主智能体后，工具增强环境中的对抗性失败将安全评估从主观NLP任务转变为客观控制问题，亟需形式化新型威胁模型。

Method: 提出Tag-Along Attacks威胁模型，并构建Slingshot冷启动强化学习框架，通过环境交互自主探索攻击策略，聚焦于发现简洁、指令式的攻击提示而非多轮说服。

Result: Slingshot在Qwen2.5-32B-Instruct-AWQ上攻击成功率达67.0%（基线仅1.7%），首次成功所需尝试次数从52.3降至1.3；并零样本迁移到Gemini 2.5 Flash（56.0%）和Meta-SecAlign-8B（39.2%）等模型。

Conclusion: Tag-Along Attacks是一种可验证的一流威胁模型；仅靠环境交互即可从现成开源模型中激发有效攻击，凸显工具增强智能体的安全脆弱性。

Abstract: The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary "tags along" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.

</details>


### [505] [Lossless Embedding Compression via Spherical Coordinates](https://arxiv.org/abs/2602.00079)
*Han Xiao*

Main category: cs.LG

TL;DR: 本文提出了一种针对单位范数嵌入的无损压缩方法，利用高维单位向量球坐标在π/2附近集中导致IEEE 754指数坍缩的特性，实现1.5×压缩率，优于先前最优方法25%，且无需训练、完全无损（float32精度内）。


<details>
  <summary>Details</summary>
Motivation: 提升单位范数嵌入的存储与传输效率，克服现有压缩方法压缩率不足的问题。

Method: 利用高维单位向量球坐标在π/2附近的集中现象，使IEEE 754浮点表示的指数部分坍缩为单一值，从而通过熵编码实现高效无损压缩。

Result: 在涵盖文本、图像和多向量嵌入的26种配置上验证，平均压缩率达1.5×，比此前最优方法提升25%；无需训练，保持float32精度下的完全无损。

Conclusion: 该方法是一种简单、通用、高效且无损的单位范数嵌入压缩方案，适用于各类嵌入表示场景。

Abstract: We present a lossless compression method for unit-norm embeddings that achieves 1.5$\times$ compression, 25\% better than the best prior method. The method exploits that spherical coordinates of high-dimensional unit vectors concentrate around $π/2$, causing IEEE 754 exponents to collapse to a single value and enabling entropy coding. Evaluation across 26 configurations spanning text, image, and multi-vector embeddings confirms consistent improvement. The method requires no training and is fully lossless within float32 precision.

</details>


### [506] [Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.00084)
*Brady Steele*

Main category: cs.LG

TL;DR: 本文提出了一种理论框架解释LoRA对标签噪声的内在鲁棒性，并基于此设计了RACT方法用于噪声检测。


<details>
  <summary>Details</summary>
Motivation: 解释LoRA等参数高效微调方法为何对标签噪声具有内在鲁棒性，这一性质此前未被充分探索。

Method: 构建理论框架分析LoRA的容量限制、偏差-方差权衡及学习过程中的时间分离现象，并据此提出基于秩差异的课程训练方法RACT用于噪声检测。

Result: 理论证明LoRA无法在样本量超过O(r(d+k−r))时拟合任意标签噪声；推导出最优秩可平衡偏差与噪声方差；发现干净模式先学、噪声后拟合的时间分离；RACT在AG News上达到91.1%噪声检测F1和91.46%准确率。

Conclusion: LoRA的低秩结构天然限制其噪声记忆能力，该特性可被显式建模并用于鲁棒微调与噪声识别。

Abstract: Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.

</details>


### [507] [CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models](https://arxiv.org/abs/2602.00085)
*Shuozhe Li,Jincheng Cao,Bodun Hu,Aryan Mokhtari,Leqi Liu,Amy Zhang*

Main category: cs.LG

TL;DR: 本文提出CARE-RFT方法，通过引入基于置信度的偏斜反向KL散度正则化，在强化微调中兼顾推理能力与模型可信度（如减少幻觉、改善校准），在多个模型和算法上验证其优于无约束RFT和传统RKL约束RFT。


<details>
  <summary>Details</summary>
Motivation: 现有强化微调（RFT）在提升推理能力的同时损害模型可信度（如增加幻觉、降低校准），而RKL约束RFT虽保持可信度却限制推理提升，二者存在关键权衡。

Method: 提出CARE-RFT：用置信度锚定的偏斜反向KL散度替代标准反向KL正则化，使惩罚项对高置信且持续获益的探索有界（支持推理），对其他情况无界（保障校准）。

Result: 在多模型规模和RFT算法上实验表明，CARE-RFT在推理性能上媲美无约束RFT，同时恢复基线模型的可信度与校准能力。

Conclusion: 置信度感知的正则化是构建兼具强大推理能力与高可信度语言模型的关键。

Abstract: Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallucination and worsening calibration; conversely, RKL-constrained RFT preserves trustworthiness but limits reasoning gains due to its unbounded penalty on exploratory deviations. To resolve this tension, we introduce CARE-RFT (Confidence-Anchored Regularized Reinforcement Finetuning), a novel method that replaces standard reverse KL regularization with a skew reverse KL divergence. CARE-RFT provides a confidence-sensitive penalty: it is bounded for confident, consistently rewarded explorations to enable reasoning, while unbounded elsewhere to preserve calibration. Extensive experiments across multiple model scales and RFT algorithms show that CARE-RFT achieves a superior balance, matching the reasoning performance of unconstrained RFT while recovering the trustworthiness and calibration of the base model. Our work establishes that careful, confidence-aware regularization is key to building both capable and trustworthy reasoning models.

</details>


### [508] [ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization](https://arxiv.org/abs/2602.00087)
*Haolin Pan,Lianghong Huang,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: 本文提出ECCO框架，结合可解释推理与组合搜索，通过反向工程构建因果链式思维数据集，并设计LLM与遗传算法协同推理机制，在七组数据集上平均降低24.44%运行周期，显著优于LLVM opt -O3。


<details>
  <summary>Details</summary>
Motivation: 传统黑箱搜索缺乏语义指导，而现有大语言模型方法易陷入表层模式匹配、因果不透明，亟需兼具可解释性与搜索能力的编译器自动调优新范式。

Method: 提出ECCO框架：1）采用反向工程构建Chain-of-Thought数据集，将静态代码特征映射至可验证性能证据；2）设计LLM（作为策略制定者）与遗传算法（执行变异操作）的协同推理机制。

Result: 在七个数据集上实验表明，ECCO相较LLVM opt -O3基线平均减少24.44%运行周期。

Conclusion: ECCO成功融合可解释因果推理与组合优化，验证了语义引导的LLM-搜索协同范式在编译器自动调优中的有效性与优越性。

Abstract: Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.

</details>


### [509] [From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting](https://arxiv.org/abs/2602.00088)
*Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 本文提出了一种名为符号转换机制（STM）的新框架，通过符号抽象和提示工程将数值时间序列数据与大语言模型结合，在保持模型完整性的同时大幅提升其在轻量级平台上的预测效率与精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列预测中表现优异，但其高计算与内存开销限制了在轻量级设备上的部署。

Method: STM采用基于人类认知结构的量化技术将连续时间序列值转化为符号token，并通过符号的结构化变换捕捉时序动态，结合提示工程引导小语言模型聚焦关键时序模式。

Result: 在多个时间序列数据集和四种小语言模型上，STM使MAE降低最多69%，MSE降低最多90%；GPU内存仅增加0.06%，延迟仅增加0.64%。

Conclusion: STM是一种高效、通用且资源友好的适配层，能显著提升小语言模型在时间序列预测任务中的性能，为符号驱动的时序基础模型应用提供了新范式。

Abstract: Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time series data and language models through symbolic abstraction and prompt engineering. STM transforms continuous time series values into symbol tokens with quantization techniques based on human cognitive structures, and captures temporal dynamics through structured transformations of symbols, enabling fast engineering based predictions in which language models focus on critical parts of time series data. STM is a general purpose mechanisms that ensure the integrity of backbone language models, but they significantly improve their efficiency by inferring the dynamic and structured patterns inherent in time series data. We evaluated STM on various time series datasets, paired with four small language models (SLM) with limited computational environments. For all models, STM achieves error reductions of up to 69% in MAE and 90% in MSE compared to the default backbone SLM without STM. These results demonstrate the potential of STM as an efficient, adaptable layer for symbol-driven time series prediction using foundation models. The accuracy improvements were made at negligible resource costs, with maximum GPU memory of the base model increasing by approximately 0.06% and latency overhead increasing by only 0.64%.

</details>


### [510] [Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits](https://arxiv.org/abs/2602.00092)
*Neha Kalibhat,Zi Wang,Prasoon Bajpai,Drew Proud,Wenjun Zeng,Been Kim,Mani Malek*

Main category: cs.LG

TL;DR: 本文提出了一种黑箱可解释性框架，通过原子概念编辑（ACEs）学习可验证的自然语言‘宪法’，以揭示提示词修改如何影响大模型在对齐性、正确性等行为上的因果关系，并在数学推理和文生图任务中验证了其有效性和控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型缺乏可解释、可验证的机制来理解提示词修改如何影响其具体行为（如对齐性、正确性），亟需一种能提供深度、泛化性洞见的黑箱可解释方法。

Method: 提出基于原子概念编辑（ACEs）的框架：ACEs是针对输入提示中可解释概念的增/删/替操作；系统施加ACEs并观测模型在多任务上的行为变化，从而学习从编辑到结果的因果映射，生成自然语言‘宪法’。

Result: 在数学推理与文生图任务中验证有效：发现GPT-Image重语法、Imagen 4重氛围；GPT-5易受干扰变量影响，而Gemini 2.5和o4-mini鲁棒；使用宪法控制模型行为，成功率平均提升1.86倍。

Conclusion: 所提框架能学习出可验证、可泛化的自然语言宪法，不仅深化对模型行为的理解，还显著提升对其行为的可控性，为大模型可信部署提供了新路径。

Abstract: We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), which are targeted operations that add, remove, or replace an interpretable concept in the input prompt. By systematically applying ACEs and observing the resulting effects on model behavior across various tasks, our framework learns a causal mapping from edits to predictable outcomes. This learned constitution provides deep, generalizable insights into the model. Empirically, we validate our approach across diverse tasks, including mathematical reasoning and text-to-image alignment, for controlling and understanding model behavior. We found that for text-to-image generation, GPT-Image tends to focus on grammatical adherence, while Imagen 4 prioritizes atmospheric coherence. In mathematical reasoning, distractor variables confuse GPT-5 but leave Gemini 2.5 models and o4-mini largely unaffected. Moreover, our results show that the learned constitutions are highly effective for controlling model behavior, achieving an average of 1.86 times boost in success rate over methods that do not use constitutions.

</details>


### [511] [Trade-offs Between Individual and Group Fairness in Machine Learning: A Comprehensive Review](https://arxiv.org/abs/2602.00094)
*Sandra Benítez-Peña,Blas Kolic,Victoria Menendez,Belén Pulido*

Main category: cs.LG

TL;DR: This survey reviews hybrid fairness approaches that jointly address Group Fairness (GF) and Individual Fairness (IF), analyzing their mechanisms, trade-offs, theoretical foundations, optimization strategies, evaluation practices, limitations, and open research directions.


<details>
  <summary>Details</summary>
Motivation: Traditional fairness research has studied Group Fairness and Individual Fairness in isolation, but real-world systems require integrated, principled approaches that satisfy both criteria simultaneously and account for their inherent trade-offs.

Method: Systematic and critical review of hybrid fairness methods, organized by fairness mechanisms and algorithmic/mathematical strategies; analysis includes theoretical foundations, optimization mechanisms, empirical evaluation, and limitations.

Result: A structured taxonomy of hybrid fairness approaches, identification of key challenges (e.g., reconciling conflicting criteria, context-awareness), and articulation of open research directions for principled, context-sensitive hybrid fairness design.

Conclusion: Jointly addressing GF and IF is essential yet challenging; this survey synthesizes current knowledge to guide the development of robust, unified fairness frameworks that deliver reliable guarantees at both individual and group levels.

Abstract: Algorithmic fairness has become a central concern in computational decision-making systems, where ensuring equitable outcomes is essential for both ethical and legal reasons. Two dominant notions of fairness have emerged in the literature: Group Fairness (GF), which focuses on mitigating disparities across demographic subpopulations, and Individual Fairness (IF), which emphasizes consistent treatment of similar individuals. These notions have traditionally been studied in isolation. In contrast, this survey examines methods that jointly address GF and IF, integrating both perspectives within unified frameworks and explicitly characterizing the trade-offs between them. We provide a systematic and critical review of hybrid fairness approaches, organizing existing methods according to the fairness mechanisms they employ and the algorithmic and mathematical strategies used to reconcile multiple fairness criteria. For each class of methods, we examine their theoretical foundations, optimization mechanisms, and empirical evaluation practices, and discuss their limitations. Additionally, we discuss the challenges and identify open research directions for developing principled, context-aware hybrid fairness methods. By synthesizing insights across the literature, this survey aims to serve as a comprehensive resource for researchers and practitioners seeking to design hybrid algorithms that provide reliable fairness guarantees at both the individual and group levels.

</details>


### [512] [Gauss-Newton Natural Gradient Descent for Shape Learning](https://arxiv.org/abs/2602.00099)
*James King,Arturs Berzins,Siddhartha Mishra,Marius Zeinhofer*

Main category: cs.LG

TL;DR: 本文探讨了高斯-牛顿法在形状学习（包括隐式神经曲面和几何感知神经网络）优化中的应用，解决了微分约束病态性和参数空间与函数空间优化不匹配等关键挑战，显著提升了收敛速度、稳定性和精度。


<details>
  <summary>Details</summary>
Motivation: 解决形状学习中微分约束病态性及参数空间与函数空间优化不匹配的问题。

Method: 采用高斯-牛顿法进行优化，适用于隐式神经曲面和几何感知神经网络。

Result: 相比标准一阶方法，收敛更快、更稳定，迭代次数大幅减少，并在多个基准形状优化任务中验证了训练速度和最终解精度的持续提升。

Conclusion: 高斯-牛顿法是形状学习中一种高效、稳定且高精度的优化方法。

Abstract: We explore the use of the Gauss-Newton method for optimization in shape learning, including implicit neural surfaces and geometry-informed neural networks. The method addresses key challenges in shape learning, such as the ill-conditioning of the underlying differential constraints and the mismatch between the optimization problem in parameter space and the function space where the problem is naturally posed. This leads to significantly faster and more stable convergence than standard first-order methods, while also requiring far fewer iterations. Experiments across benchmark shape optimization tasks demonstrate that the Gauss-Newton method consistently improves both training speed and final solution accuracy.

</details>


### [513] [THDC: Training Hyperdimensional Computing Models with Backpropagation](https://arxiv.org/abs/2602.00116)
*Hanne Dejonghe,Sam Leroux*

Main category: cs.LG

TL;DR: 本文提出可训练的超维计算（THDC），通过反向传播实现端到端训练，用可学习嵌入替代随机初始化超向量，并引入单层二值神经网络优化类别表示，在降低维度（从10000降至64）的同时达到或超越现有HDC方法的精度。


<details>
  <summary>Details</summary>
Motivation: 传统超维计算依赖超高维和静态随机超向量，导致内存效率低、学习能力受限。

Method: 提出可训练超维计算（THDC），使用可学习嵌入替代随机初始化超向量，并引入单层二值神经网络优化类表示，支持端到端反向传播训练。

Result: 在MNIST、Fashion-MNIST和CIFAR-10上，THDC在维度大幅降低（10000→64）的同时，准确率等于或优于当前最优HDC方法。

Conclusion: THDC显著提升了HDC的内存效率与学习能力，为资源受限设备上的轻量级学习提供了更优方案。

Abstract: Hyperdimensional computing (HDC) offers lightweight learning for energy-constrained devices by encoding data into high-dimensional vectors. However, its reliance on ultra-high dimensionality and static, randomly initialized hypervectors limits memory efficiency and learning capacity. Therefore, we propose Trainable Hyperdimensional Computing (THDC), which enables end-to-end HDC via backpropagation. THDC replaces randomly initialized vectors with trainable embeddings and introduces a one-layer binary neural network to optimize class representations. Evaluated on MNIST, Fashion-MNIST and CIFAR-10, THDC achieves equal or better accuracy than state-of-the-art HDC, with dimensionality reduced from 10.000 to 64.

</details>


### [514] [Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control](https://arxiv.org/abs/2602.00120)
*Xianghong Hu,Tianning Xu,Ying Chen,Shuai Wang*

Main category: cs.LG

TL;DR: 本文研究了房贷违约预测中的关键挑战，包括违约标签模糊性、严重类别不平衡和信息泄露，并提出了一套严格的评估框架（如防泄漏特征选择、时间严格划分和可控降采样），在真实数据集上验证了AutoGluon等模型的稳健性能。


<details>
  <summary>Details</summary>
Motivation: 现实房贷数据中存在违约标签模糊、类别极度不平衡以及由时间结构和事后变量引起的信息泄露，严重影响模型评估有效性和部署可靠性。

Method: 采用防泄漏特征选择、严格的时间划分（约束放款期和报告期）以及多数类可控降采样；对比多种机器学习方法，重点评估其在不同正负样本比下的稳定性。

Result: 在多种正负样本比例下模型性能稳定，AutoML工具AutoGluon在AUROC指标上表现最优。

Conclusion: 严格的泄漏控制与不平衡处理策略对提升房贷违约预测模型的可靠性至关重要，AutoGluon等自动化方法在该任务中具有实用优势。

Abstract: Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.

</details>


### [515] [MiniTensor: A Lightweight, High-Performance Tensor Operations Library](https://arxiv.org/abs/2602.00125)
*Soumyadip Sarkar*

Main category: cs.LG

TL;DR: MiniTensor是一个开源张量运算库，采用Python（类似PyTorch API）+ Rust核心架构，在保证正确性和性能的同时，显著减小安装体积（仅几MB），支持自动微分、常见神经网络层与优化器，适用于CPU端研究开发。


<details>
  <summary>Details</summary>
Motivation: 主流深度学习框架（如PyTorch、TensorFlow）安装包过大，不利于轻量部署、教学或资源受限环境；亟需一个兼顾简洁性、正确性与实用性的替代方案。

Method: 设计并实现一个双语言架构：Python层提供易用API，Rust引擎执行高性能计算；采用动态计算图实现反向模式自动微分；通过PyO3实现高效Python-Rust互操作；优化内存管理与编译配置以最小化二进制体积。

Result: MiniTensor安装包仅数MB，比PyTorch/TensorFlow小几个数量级；功能上完整支持n维稠密张量、广播、约简、矩阵乘、自动微分、基础神经网络模块和优化器；在CPU上性能可满足研发需求。

Conclusion: MiniTensor验证了‘极简但完备’的张量库设计可行，为教育、嵌入式AI、快速原型开发等场景提供了轻量、可靠的新选择。

Abstract: We present MiniTensor, an open source tensor operations library that focuses on minimalism, correctness, and performance. MiniTensor exposes a familiar PyTorch-like Python API while it executes performance critical code in a Rust engine. The core supports dense $n$ dimensional tensors, broadcasting, reductions, matrix multiplication, reverse mode automatic differentiation, a compact set of neural network layers, and standard optimizers. In this paper, we describe the design of MiniTensor's architecture, including its efficient memory management, dynamic computation graph for gradients, and integration with Python via PyO3. We also compare the install footprint with PyTorch and TensorFlow to demonstrate that MiniTensor achieves a package size of only a few megabytes, several orders of magnitude smaller than mainstream frameworks, while preserving the essentials needed for research and development on CPUs. The repository can be found at https://github.com/neuralsorcerer/minitensor

</details>


### [516] [ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2602.00127)
*Tong Zhu,Baiting Chen,Jin Zhou,Hua Zhou,Sriram Sankararaman,Xiaowu Dai*

Main category: cs.LG

TL;DR: 本文提出了一种名为ALIGN的多智能体推理新方法，将大语言模型推理建模为对齐的委托博弈，在理论上证明其能提升期望性能，并在多个基准上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂推理任务中单次生成-选择流程表现不足；已有推理时集成方法缺乏理论保证且忽略候选答案间的相关性。

Method: 提出Aligned Delegation for Multi-Agent LLM Reasoning（ALIGN），将推理建模为‘委托-代理’博弈：主控者（principal）向多个代理（agents）分配任务，在设计激励下生成候选解，再从中选择最终答案；理论分析考虑候选答案相关性并放松独立性假设。

Result: 在多种LLM推理基准上，ALIGN持续优于强单智能体及集成基线；理论证明在公平比较下其期望性能严格优于单智能体生成。

Conclusion: ALIGN通过结构化多智能体交互与目标对齐，兼顾理论严谨性与实证有效性，为LLM推理提供了可证明提升的新范式。

Abstract: LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.

</details>


### [517] [Quantum Model Parallelism for MRI-Based Classification of Alzheimer's Disease Stages](https://arxiv.org/abs/2602.00128)
*Emine Akpinar,Murat Oduncuoglu*

Main category: cs.LG

TL;DR: 本文提出了一种基于量子计算的并行模型（QBPM），用于高效分类阿尔茨海默病（AD）的不同阶段，利用双量子电路并行运行，在MRI数据上实现了高精度、强鲁棒性与泛化能力，并在噪声环境下表现优异，优于多种经典迁移学习方法。


<details>
  <summary>Details</summary>
Motivation: 随着人口寿命延长，阿尔茨海默病（AD）成为全球重大健康问题；传统AI方法面临大数据量与有限算力的挑战，亟需更快速、高效的诊断方案。

Method: 提出量子基并行模型（QBPM），采用两个含旋转与纠缠模块的量子电路，在同一量子模拟器上并行运行，借鉴经典模型并行思想，处理MRI数据进行AD分期分类。

Result: 在两个不同数据集上均取得高分类精度；在高斯噪声下仍保持稳定性能；相比五种经典迁移学习方法，精度更高、执行时间相当、参数更少。

Conclusion: QBPM是一种创新且强大的AD分期分类方法，兼具理论可行性与实际应用潜力，展示了量子AI在复杂疾病分析中的优势。

Abstract: With increasing life expectancy, AD has become a major global health concern. While classical AI-based methods have been developed for early diagnosis and stage classification of AD, growing data volumes and limited computational resources necessitate faster, more efficient approaches. Quantum-based AI methods, which leverage superposition and entanglement principles along with high-dimensional Hilbert space, can surpass classical approaches' limitations and offer higher accuracy for high-dimensional, heterogeneous, and noisy data. In this study, a Quantum-Based Parallel Model (QBPM) architecture is proposed for the efficient classification of AD stages using MRI datasets, inspired by the principles of classical model parallelism. The proposed model leverages quantum advantages by employing two distinct quantum circuits, each incorporating rotational and entanglement blocks, running in parallel on the same quantum simulator. The classification performance of the model was evaluated on two different datasets to assess its overall robustness and generalization capability. The proposed model demonstrated high classification accuracy across both datasets, highlighting its overall robustness and generalization capability. Results obtained under high-level Gaussian noise, simulating real-world conditions, further provided experimental evidence for the model's applicability not only in theoretical but also in practical scenarios. Moreover, compared with five different classical transfer learning methods, the proposed model demonstrated its efficiency as an alternative to classical approaches by achieving higher classification accuracy and comparable execution time while utilizing fewer circuit parameters. The results indicate that the proposed QBPM architecture represents an innovative and powerful approach for the classification of stages in complex diseases such as Alzheimer's.

</details>


### [518] [Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation](https://arxiv.org/abs/2602.00899)
*Mritunjay Pandey*

Main category: cs.LG

TL;DR: 本文提出了一种基于双塔生物编码器的可扩展稠密检索系统，用于电商推荐与搜索，通过语义相似性替代传统的稀疏关键词匹配（如BM25），显著提升召回率，并实现CPU高效部署。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏关键词匹配（如BM25）在用户意图与商品元数据词汇重叠少时表现差，存在词汇不匹配问题。

Method: 采用双塔bi-encoder架构，基于Amazon Reviews 2023（Fashion）子集，使用监督对比学习与Multiple Negatives Ranking Loss进行微调；训练对由评论文本（查询代理）和商品元数据（正样本）构成；推理阶段结合FAISS HNSW索引与ONNX Runtime INT8动态量化。

Result: 在包含826,402个商品的review-to-title基准上，Recall@10从BM25的0.26提升至0.66；中位CPU推理延迟为6.1ms（batch size=1），模型体积缩减4倍。

Conclusion: 提供了一个端到端、可复现的方案，将领域适配的稠密检索从离线训练成功落地到大规模商品目录的CPU高效服务。

Abstract: E-commerce recommendation and search commonly rely on sparse keyword matching (e.g., BM25), which breaks down under vocabulary mismatch when user intent has limited lexical overlap with product metadata. We cast content-based recommendation as recommendation-as-retrieval: given a natural-language intent signal (a query or review), retrieve the top-K most relevant items from a large catalog via semantic similarity.
  We present a scalable dense retrieval system based on a two-tower bi-encoder, fine-tuned on the Amazon Reviews 2023 (Fashion) subset using supervised contrastive learning with Multiple Negatives Ranking Loss. We construct training pairs from review text (as a query proxy) and item metadata (as the positive document) and fine-tune on 50,000 sampled interactions with a maximum sequence length of 500 tokens.
  For efficient serving, we combine FAISS HNSW indexing with an ONNX Runtime inference pipeline using INT8 dynamic quantization. On a review-to-title benchmark over 826,402 catalog items, our approach improves Recall@10 from 0.26 (BM25) to 0.66, while meeting practical latency and model-size constraints: 6.1 ms median CPU inference latency (batch size 1) and a 4x reduction in model size.
  Overall, we provide an end-to-end, reproducible blueprint for taking domain-adapted dense retrieval from offline training to CPU-efficient serving at catalog scale.

</details>


### [519] [Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models](https://arxiv.org/abs/2602.00129)
*Yixuan Liang*

Main category: cs.LG

TL;DR: CodePilot 是一种结合蒙特卡洛树搜索（MCTS）与大语言模型的混合框架，用于执行引导的仓库级程序修复，通过分层定位故障、MCTS 探索补丁路径及执行反馈奖励机制提升修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动程序修复在仓库级任务中受限于长程推理能力与自回归解码缺陷，难以应对真实 GitHub 问题。

Method: 提出 CodePilot 框架：1）分层故障定位（仓库→文件→函数）；2）MCTS 探索多样化补丁轨迹；3）以执行反馈为奖励信号指导搜索与精炼；4）引入置信度校准生成，选择性优化低置信输出。

Result: 在 SWE-bench Lite 上，CodePilot 使用开源权重模型达到 24.67% 的问题解决率，优于同类基线方法。

Conclusion: 将符号化搜索（如 MCTS）与神经语言模型结合，是实现可扩展、执行感知的软件工程自动化的一种有效策略。

Abstract: Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.

</details>


### [520] [On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks](https://arxiv.org/abs/2602.00130)
*Sumit Yadav*

Main category: cs.LG

TL;DR: 本文研究了神经网络表征几何结构（特别是有效维度）与其性能之间的关系，发现有效维度能强预测模型准确率，并且具有因果性，且无需标签即可计算。


<details>
  <summary>Details</summary>
Motivation: 探究神经网络表征的几何特性（如有效维度）是否能作为无监督指标来预测和解释模型性能。

Method: 在52个ImageNet预训练模型、多个视觉与NLP数据集（CIFAR-10、SST-2、MNLI、AG News）及不同架构上，分析输出/总压缩的有效维度与准确率的相关性；并通过注入噪声和PCA降维进行因果干预实验。

Result: 输出有效维度与准确率呈强正相关（r=0.75），总压缩呈强负相关（r=-0.72）；该关系跨领域复现；噪声扰动导致几何退化与性能下降高度相关（|r|>0.90）；PCA保持95%方差时精度几乎不变（-0.03pp）。

Conclusion: 有效维度是一个领域无关、无需标签、兼具预测性与因果性的神经网络性能指标，揭示了表征几何对泛化能力的根本作用。

Abstract: We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 ($p < 10^(-10)$) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, $p < 10^(-9)$), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show $|r| > 0.90$. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.

</details>


### [521] [RAPTOR: Ridge-Adaptive Logistic Probes](https://arxiv.org/abs/2602.00158)
*Ziqi Gao,Yaotian Zhu,Qingcheng Zeng,Xu Zhao,Ziqing Wang,Feng Ruan,Kaize Ding*

Main category: cs.LG

TL;DR: 本文提出RAPTOR（Ridge-Adaptive Logistic Probe），一种带L2正则的逻辑回归探针，用于从冻结大语言模型中高效、稳定地提取概念向量，支持激活引导；在准确率、方向稳定性与训练开销上优于基线，并通过CGMT理论分析揭示了正则强度对性能的影响机制。


<details>
  <summary>Details</summary>
Motivation: 现有probe-then-steer流程依赖准确、方向稳定且低成本的概念向量估计，但现有方法在三者间难以兼顾，本文旨在设计一种满足这三项需求的新型探针。

Method: 提出RAPTOR：基于L2正则化逻辑回归的轻量探针，通过验证集调优岭参数，从归一化权重中提取概念向量；结合实证实验与基于CGMT的高维少样本理论分析（高斯师生模型）。

Result: RAPTOR在指令微调LLM和人工标注概念数据集上，准确率匹配或超越强基线，方向稳定性具竞争力，训练成本显著降低；下游引导效果经定性验证；理论分析揭示岭参数权衡准确率与稳定性，并预测出与实证一致的趋势。

Conclusion: RAPTOR是一种简单有效、兼具理论可解释性与实用性的探针方法，为LLM内部表征的可解释性分析与可控生成提供了新范式。

Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.

</details>


### [522] [Sheaf Neural Networks and biomedical applications](https://arxiv.org/abs/2602.00159)
*Aneeqa Mehrab,Jan Willem Van Looy,Pietro Demurtas,Stefano Iotti,Emil Malucelli,Francesca Rossi,Ferdinando Zanchetta,Rita Fioresi*

Main category: cs.LG

TL;DR: This paper introduces the sheaf neural network (SNN), provides its theoretical and mathematical foundation, and demonstrates its superior performance over popular graph neural networks (e.g., GCN, GAT, GraphSage) in a biomedical case study.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing graph neural networks in biomedical applications by leveraging the expressive power of sheaf theory.

Method: Develops the theoretical framework and mathematical modeling of sheaf neural networks (SNNs) and evaluates them on a biomedical case study.

Result: SNN outperforms GCN, GAT, and GraphSage in the biomedical case study.

Conclusion: Sheaf neural networks offer a theoretically grounded and empirically effective alternative to conventional graph neural networks for biomedical problems.

Abstract: The purpose of this paper is to elucidate the theory and mathematical modelling behind the sheaf neural network (SNN) algorithm and then show how SNN can effectively answer to biomedical questions in a concrete case study and outperform the most popular graph neural networks (GNNs) as graph convolutional networks (GCNs), graph attention networks (GAT) and GraphSage.

</details>


### [523] [Block removal for large language models through constrained binary optimization](https://arxiv.org/abs/2602.00161)
*David Jansen,Roman Rausch,David Montero,Roman Orus*

Main category: cs.LG

TL;DR: 本文提出了一种基于伊辛模型的Transformer块剪枝方法，将块移除建模为约束二元优化问题，高效搜索高质量非连续块组合，在多个基准上超越现有方法，且适用于各类大模型架构。


<details>
  <summary>Details</summary>
Motivation: 直接移除Transformer块看似简单，但如何选择最优块组合是一个指数级复杂度的组合优化问题，现有方法难以高效求解并找到非平凡解。

Method: 将块移除建模为约束二元优化问题，并映射到伊辛模型；利用其能量作为下游性能的强代理，结合（近似）伊辛求解器高效排序大量候选配置。

Result: 在多个基准（如MMLU）上显著优于现有块剪枝方法，最高提升6分；短时微调后性能仍保持；成功应用于结构复杂不均的NVIDIA-Nemotron-3-Nano-30B-A3B-FP8模型。

Conclusion: 该方法计算开销低（仅需少量前向/反向传播）、通用性强、能发现高质量非连续块子集，为大模型压缩提供了新范式。

Abstract: Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance. This formulation enables an efficient ranking of a large number of candidate block-removal configurations and yields many high-quality, non-trivial solutions beyond consecutive regions. We demonstrate that our approach outperforms state-of-the-art block-removal methods across several benchmarks, with performance gains persisting after short retraining, and reaching improvements of up to 6 points on the MMLU benchmark. Our method requires only forward and backward passes for a few active parameters, together with an (at least approximate) Ising solver, and can be readily applied to any architecture. We illustrate this generality on the recent NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model, which exhibits a highly inhomogeneous and challenging block structure.

</details>


### [524] [Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2602.00165)
*Arthur Negrão,Pedro Silva,Vander L. S. Freitas,Gladston Moreira,Eduardo Luz*

Main category: cs.LG

TL;DR: 本文提出Benford-Quant，一种受Benford定律启发的无数据、非均匀权重量化方法，通过采用对数间隔码本提升小幅度权重的量化精度，在小模型上显著降低困惑度，并可与现有量化方法结合使用。


<details>
  <summary>Details</summary>
Motivation: 标准均匀量化器假设权重均匀分布，但实际中权重分布高度偏斜；Benford定律指出首位数字呈对数分布，而Transformer中变换层权重符合该统计规律，为设计更匹配真实分布的量化器提供依据。

Method: 提出Benford-Quant：基于Benford定律构建对数间隔（log-spaced）非均匀码本，无需训练或数据，直接替换均匀量化网格，增强对小幅度权重的分辨能力；并分析其在不同网络层（如变换层 vs 归一化层）的适用性。

Result: 在Small Language Models（如Gemma-270M）上，4-bit量化困惑度降低超10%；在更大LLMs上保持竞争力；可无缝融合SmoothQuant、Activation-Aware Quantization等方法，提升其性能；虽未超越SOTA在perplexity和LAMBADA任务上的表现，但具有低开销、高兼容性优势。

Conclusion: 将Benford先验引入量化网格是一种低成本、高效益的改进策略，尤其适用于激进的低位宽（few-bit）量化场景，并具备良好的方法兼容性与扩展潜力。

Abstract: The rapid growth of Large Language Models (LLMs) intensifies the need for effective compression, with weight quantization being the most widely adopted technique. Standard uniform quantizers assume that parameters are evenly distributed, an assumption at odds with the highly skewed distributions observed in practice. We propose Benford-Quant, a simple, data-free non-uniform quantizer inspired by Benford's Law, which predicts that leading digits follow a logarithmic distribution. Benford-Quant replaces the uniform grid with a log-spaced codebook, dedicating more resolution to the frequent small-magnitude weights. We provide both theoretical intuition and empirical evidence: (i) weights in transformer transformational layers adhere closely to Benford statistics, while normalization layers systematically deviate; (ii) on Small Language Models (SLMs), Benford-Quant consistently improves perplexity, reducing 4-bit perplexity on Gemma-270M by more than 10%; and (iii) on larger LLMs, it remains competitive, with differences explained by over-parameterization effects. Our results indicate that incorporating a Benford-inspired prior into quantization grids is a low-cost modification that yields accuracy gains in aggressive few-bit regimes. Although it is not able to surpass the state of the art in tasks such as perplexity and LAMBADA, the Benford-Quant approach can be hybridized with other quantization methods-such as SmoothQuant and Activation-Aware Quantization-without major pipeline modification, potentially improving their performance.

</details>


### [525] [Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints](https://arxiv.org/abs/2602.00166)
*Evan Chen,Wenzhi Fang,Shiqiang Wang,Christopher Brinton*

Main category: cs.LG

TL;DR: 本文提出DA-GRPO方法，通过将云使用约束直接融入优势函数计算，使本地小模型在持续学习中自然、稳定地按预算请求云大模型协助，从而缓解灾难性遗忘并提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 本地部署的小语言模型（SLMs）需在严苛的内存和算力限制下持续支持多样任务，不可避免地需要选择性调用云端大语言模型（LLMs）；但现有基于奖励的强化学习方法在持续学习中易导致不稳定的卸载行为并加剧灾难性遗忘。

Method: 提出DA-GRPO——一种双优势扩展的Group Relative Policy Optimization算法，将云使用约束直接嵌入优势函数计算，摒弃固定奖励设计和外部路由模型，使本地模型联合学习任务能力与协作策略。

Result: 在数学推理与代码生成基准上，DA-GRPO显著提升任务切换后的准确率，大幅降低遗忘程度，并保持云调用行为稳定且符合预设预算。

Conclusion: DA-GRPO为资源受限下的持续协同学习提供了一种鲁棒、自适应且约束可保证的新范式。

Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.

</details>


### [526] [The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective](https://arxiv.org/abs/2602.00170)
*Qiyao Liang,Jinyeop Song,Yizhou Liu,Jeff Gore,Ila Fiete,Risto Miikkulainen,Xin Qiu*

Main category: cs.LG

TL;DR: 本文发现大语言模型微调的奖励曲面具有低维曲率特性，解释了权重扰动进化策略（ES）在小种群下高效微调以及训练奖励先升后降的现象。


<details>
  <summary>Details</summary>
Motivation: 解释为何权重扰动进化策略（ES）能在极小种群（如N≈30）下有效微调十亿参数语言模型，且为何ES和GRPO等方法在固定超参下常出现奖励先上升后下降的非单调现象。

Method: 提出‘低维曲率’几何假设，构建最小二次随机上升模型解释非单调动力学；利用ES作为几何探针，在GSM8K、ARC-C、WinoGrande等多个基准上，对Qwen2.5-Instruct系列模型（0.5B–7B）分析奖励曲面结构。

Result: 实证表明：高曲率方向稀疏且主导优化过程；小种群ES仍能持续找到奖励提升的扰动；该低维曲率性质跨任务与模型规模稳定存在。

Conclusion: 大语言模型微调的奖励景观本质上是低维曲率的，这统一解释了ES的可扩展性与非单调训练动态，并暗示高维微调可能存在比最坏情况理论更丰富的可行优化方法。

Abstract: Weight-perturbation evolution strategies (ES) can fine-tune billion-parameter language models with surprisingly small populations (e.g., $N\!\approx\!30$), contradicting classical zeroth-order curse-of-dimensionality intuition. We also observe a second seemingly separate phenomenon: under fixed hyperparameters, the stochastic fine-tuning reward often rises, peaks, and then degrades in both ES and GRPO. We argue that both effects reflect a shared geometric property of fine-tuning landscapes: they are low-dimensional in curvature. A small set of high-curvature dimensions dominates improvement, producing (i) heterogeneous time scales that yield rise-then-decay under fixed stochasticity, as captured by a minimal quadratic stochastic-ascent model, and (ii) degenerate improving updates, where many random perturbations share similar components along these directions. Using ES as a geometric probe on fine-tuning reward landscapes of GSM8K, ARC-C, and WinoGrande across Qwen2.5-Instruct models (0.5B--7B), we show that reward-improving perturbations remain empirically accessible with small populations across scales. Together, these results reconcile ES scalability with non-monotonic training dynamics and suggest that high-dimensional fine-tuning may admit a broader class of viable optimization methods than worst-case theory implies.

</details>


### [527] [Learning Robust Reasoning through Guided Adversarial Self-Play](https://arxiv.org/abs/2602.00173)
*Shuozhe Li,Vaishnav Tadiparthi,Kwonjoon Lee,Nakul Agarwal,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Lizhang Chen,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: 本文提出GASP（引导式对抗自博弈）方法，通过在单一模型内构建污染者与代理的对抗自博弈，仅依赖结果验证来训练检测与修复能力，显著提升强化学习推理模型在面对错误上下文时的鲁棒性，且无需人工标注或外部教师。


<details>
  <summary>Details</summary>
Motivation: 标准的基于可验证奖励的强化学习（RLVR）虽能生成强推理模型，但在条件上下文存在错误（如链式思维被污染、部分解误导或输入轻微扰动）时会灾难性失败，因其仅在干净上下文中优化最终答案正确性。

Method: 提出GASP方法：在单个模型内构建对抗自博弈——污染者学习生成局部一致但致错的上下文扰动，代理则学习诊断并修复；引入‘分布内修复引导’机制，利用模型自身生成的修复进行模仿学习，以缓解早期成功修复稀缺问题。

Result: 在四个开源权重模型（1.5B–8B）上验证，GASP显著提升模型对误导性和扰动上下文的鲁棒性，并常同时提升干净样本上的准确率；分析表明对抗污染形成有效课程学习，分布内引导加速恢复学习且保持表征稳定。

Conclusion: GASP是一种仅需结果验证、无需额外监督的鲁棒性增强范式，能将强但脆弱的推理模型转化为鲁棒模型，为可信AI推理提供新路径。

Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.

</details>


### [528] [The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization](https://arxiv.org/abs/2602.00175)
*Manyi Li,Yufan Liu,Lai Jiang,Bing Li,Yuming Li,Weiming Hu*

Main category: cs.LG

TL;DR: 本文揭示了基于遗忘的防御方法在扩散模型中清除NSFW概念的局限性，提出IVO攻击框架通过优化初始潜在变量来恢复被破坏的语言-知识映射，成功激活‘休眠记忆’，暴露现有防御的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管遗忘式防御声称能从扩散模型中清除NSFW概念，但作者发现这种‘遗忘’多为表象，底层知识仍以休眠记忆形式存在，需揭示其可被重新激活的本质漏洞。

Method: 提出IVO（Initial Latent Variable Optimization）攻击框架，结合图像反转、对抗优化与重用攻击，优化初始潜在变量，使遗忘模型的去噪噪声分布重对齐至原始不安全状态。

Result: 在8种主流遗忘技术上验证IVO，展现出高攻击成功率和强语义一致性，证明当前遗忘防御普遍存在根本性缺陷。

Conclusion: 扩散模型中的NSFW知识难以真正被遗忘；遗忘仅削弱符号映射而非消除知识本身；IVO揭示了现有防御在分布层面的脆弱性，呼吁更本质的安全机制设计。

Abstract: Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this "forgetting" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dormant memories. We find that the distributional discrepancy in the denoising process serves as a measurable indicator of how much of the mapping is retained, also reflecting the strength of unlearning. Inspired by this, we propose IVO (Initial Latent Variable Optimization), a concise and powerful attack framework that reactivates these dormant memories by reconstructing the broken mappings. Through Image Inversion}, Adversarial Optimization and Reused Attack, IVO optimizes initial latent variables to realign the noise distribution of unlearned models with their original unsafe states. Extensive experiments across 8 widely used unlearning techniques demonstrate that IVO achieves superior attack success rates and strong semantic consistency, exposing fundamental flaws in current defenses. The code is available at anonymous.4open.science/r/IVO/. Warning: This paper has unsafe images that may offend some readers.

</details>


### [529] [How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models](https://arxiv.org/abs/2602.00179)
*Joseph L. Breeden*

Main category: cs.LG

TL;DR: 本文指出，局部线性解释方法（如LIME、SHAP）在决策边界处的不稳定性并非缺陷，而是反映了该区域预测不确定性高这一本质；应先评估预测是否可用（即不确定性是否足够低），再决定是否解释；若不可用，则应回退至更简单模型；而声称处处可解释的模型（如ReLU网络）在边界处实际缺乏实用解释价值。


<details>
  <summary>Details</summary>
Motivation: 局部线性解释方法（如LIME、SHAP）在决策边界处表现不稳定，引发对其可靠性的质疑；作者认为这源于对问题本质的误解——不稳定实为高预测不确定性的自然体现，需重构解释流程。

Method: 提出一种分阶段解释范式：首先量化预测不确定性，判断预测是否‘可用’（即不确定性低于实用阈值）；若可用，则采用局部线性近似解释；若不可用，则退回到全局简单模型（如逻辑回归）进行决策；并分析ReLU等分段线性模型在边界处解释的虚幻性。

Result: 证明解释不稳定性与预测不确定性正相关；确立‘先验判断预测可用性，再解释’的新流程；揭示某些‘处处可解释’模型（如ReLU网络）在边界处的解释缺乏实际意义。

Conclusion: 解释方法的有效性取决于预测本身的可靠性；盲目追求全域可解释性无意义；真正可信赖的解释必须以低不确定性预测为前提，否则应依赖更稳健的简化模型。

Abstract: For applications of machine learning in critical decisions, explainability is a primary concern, and often a regulatory requirement. Local linear methods for generating explanations, such as LIME and SHAP, have been criticized for being unstable near decision boundaries. In this paper, we explain that such concerns reflect a misunderstanding of the problem. The forecast uncertainty is high at decision boundaries, so consequently, the explanatory instability is high. The correct approach is to change the sequence of events and questions being asked. Nonlinear models can be highly predictive in some regions while having little or no predictability in others. Therefore, the first question is whether a usable forecast exists. When there is a forecast with low enough uncertainty to be useful, an explanation can be sought via a local linear approximation. In such cases, the explanatory instability is correspondingly low. When no usable forecast exists, the decision must fall to a simpler overall model such as traditional logistic regression. Additionally, these results show that some methods that purport to be explainable everywhere, such as ReLU networks or any piecewise linear model, have only an illusory explainability, because the forecast uncertainty at the segment boundaries is too high to be useful. Explaining an unusable forecast is pointless.

</details>


### [530] [GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models](https://arxiv.org/abs/2602.00191)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: cs.LG

TL;DR: 本文提出Group-Equivariant Posterior Consistency (GEPC)，一种无需训练的扩散模型OOD检测方法，通过量化学习到的分数场在有限群作用下的变换一致性来检测分布外样本，具有计算轻量、可解释性强和性能优越的特点。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的OOD检测方法大多忽略分数场所具有的群等变性，而仅依赖分数幅值或局部几何信息；本文旨在利用并检测这种等变性在OOD样本上的破坏。

Method: 提出GEPC方法，定义并计算群作用下的等变残差（equivariance residual），在总体层面构建理想GEPC残差，并给出ID上界与OOD下界理论保证；仅需分数评估，不依赖训练。

Result: 在标准图像OOD基准上AUROC优于或媲美现有扩散基线；在高分辨率SAR图像上实现强目标-背景分离，并生成可视化的等变破坏图。

Conclusion: GEPC是一种高效、可解释、无需训练的扩散模型OOD检测新范式，拓展了对扩散模型内在几何结构（如等变性）的利用。

Abstract: Diffusion models learn a time-indexed score field $\mathbf{s}_θ(\mathbf{x}_t,t)$ that often inherits approximate equivariances (flips, rotations, circular shifts) from in-distribution (ID) data and convolutional backbones. Most diffusion-based out-of-distribution (OOD) detectors exploit score magnitude or local geometry (energies, curvature, covariance spectra) and largely ignore equivariances. We introduce Group-Equivariant Posterior Consistency (GEPC), a training-free probe that measures how consistently the learned score transforms under a finite group $\mathcal{G}$, detecting equivariance breaking even when score magnitude remains unchanged. At the population level, we propose the ideal GEPC residual, which averages an equivariance-residual functional over $\mathcal{G}$, and we derive ID upper bounds and OOD lower bounds under mild assumptions. GEPC requires only score evaluations and produces interpretable equivariance-breaking maps. On OOD image benchmark datasets, we show that GEPC achieves competitive or improved AUROC compared to recent diffusion-based baselines while remaining computationally lightweight. On high-resolution synthetic aperture radar imagery where OOD corresponds to targets or anomalies in clutter, GEPC yields strong target-background separation and visually interpretable equivariance-breaking maps. Code is available at https://github.com/RouzAY/gepc-diffusion/.

</details>


### [531] [Reducing Memorisation in Generative Models via Riemannian Bayesian Inference](https://arxiv.org/abs/2602.00199)
*Johanna Marie Gegenfurtner,Albert Kjøller Jacobsen,Naima Elosegui Borras,Alejandro Valverde Mahou,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: 本文从贝叶斯视角出发，通过在流匹配与扩散模型的参数空间中构建能更好捕捉数据分布变异性预测后验分布，利用黎曼度量刻画损失函数几何结构，并设计适配局部损失景观的灵活近似后验，从而在保持泛化能力的同时降低记忆效应。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型虽能生成逼真样本，但如何平衡记忆与泛化仍是开放问题。

Method: 基于贝叶斯框架，在流匹配和扩散模型的参数空间中构建预测后验；利用黎曼度量刻画损失函数几何结构，并采用适配局部损失景观的灵活近似后验。

Result: 实验表明该方法在降低记忆效应的同时保持了泛化能力；理论分析支持了这一发现。

Conclusion: 考虑损失函数的几何结构有助于在复杂高维生成模型中更有效地利用参数空间，实现记忆与泛化的更好平衡。

Abstract: Modern generative models can produce realistic samples, however, balancing memorisation and generalisation remains an open problem. We approach this challenge from a Bayesian perspective by focusing on the parameter space of flow matching and diffusion models and constructing a predictive posterior that better captures the variability of the data distribution. In particular, we capture the geometry of the loss using a Riemannian metric and leverage a flexible approximate posterior that adapts to the local structure of the loss landscape. This approach allows us to sample generative models that resemble the original model, but exhibit reduced memorisation. Empirically, we demonstrate that the proposed approach reduces memorisation while preserving generalisation. Further, we provide a theoretical analysis of our method, which explains our findings. Overall, our work illustrates how considering the geometry of the loss enables effective use of the parameter space, even for complex high-dimensional generative models.

</details>


### [532] [Reducing Class-Wise Performance Disparity via Margin Regularization](https://arxiv.org/abs/2602.00205)
*Beier Zhu,Kesen Zhao,Jiequan Cui,Qianru Sun,Yuan Zhou,Xun Yang,Hanwang Zhang*

Main category: cs.LG

TL;DR: 本文提出MR²方法，通过动态调整logit和表示空间的边界来减少分类任务中各类别间的性能差异，理论分析表明类别特定的特征变异性影响误差，实验验证其在提升困难类别性能的同时不损害简单类别性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在类别平衡数据上训练时仍存在显著的类别间准确率差异，影响可靠部署，而现有工作缺乏对这种差异的理论理解。

Method: 提出Margin Regularization for Performance Disparity Reduction (MR²)，在logit空间和表示空间中动态调整类别相关边界：logit边界与特征离散度成正比，同时惩罚过大的表示边界以增强类内紧凑性。

Result: 在ImageNet等7个数据集及MAE、MoCov2、CLIP等多种预训练骨干网络上，MR²不仅提升整体精度，还显著改善困难类别的性能，且不牺牲简单类别的性能，有效降低性能差异。

Conclusion: MR²是一种理论驱动的正则化方法，能有效缓解分类任务中的类别性能失衡问题，兼具理论严谨性与实际有效性。

Abstract: Deep neural networks often exhibit substantial disparities in class-wise accuracy, even when trained on class-balanced data, posing concerns for reliable deployment. While prior efforts have explored empirical remedies, a theoretical understanding of such performance disparities in classification remains limited. In this work, we present Margin Regularization for Performance Disparity Reduction (MR$^2$), a theoretically principled regularization for classification by dynamically adjusting margins in both the logit and representation spaces. Our analysis establishes a margin-based, class-sensitive generalization bound that reveals how per-class feature variability contributes to error, motivating the use of larger margins for hard classes. Guided by this insight, MR$^2$ optimizes per-class logit margins proportional to feature spread and penalizes excessive representation margins to enhance intra-class compactness. Experiments on seven datasets, including ImageNet, and diverse pre-trained backbones (MAE, MoCov2, CLIP) demonstrate that MR$^2$ not only improves overall accuracy but also significantly boosts hard class performance without trading off easy classes, thus reducing performance disparity. Code is available at: https://github.com/BeierZhu/MR2

</details>


### [533] [Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity](https://arxiv.org/abs/2602.00208)
*Jordan Levy,Paul Saves,Moncef Garouani,Nicolas Verstaevel,Benoit Gaudou*

Main category: cs.LG

TL;DR: 本文提出了一种基于SHAP解释多样性的无监督异常检测集成方法，通过量化各检测器的特征重要性归因来衡量其决策机制差异，发现解释差异性可有效指示互补性，并证明在保持单个模型性能前提下，提升解释多样性可构建更有效的集成模型。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测面临数据分布多样性和标签缺失的挑战，传统集成方法常因检测器决策依据相似而产生冗余结果，难以实现真正互补。

Method: 利用SHAP（SHapley Additive exPlanations）量化各异常检测器对输入特征的重要性归因，构建归因剖面（attribution profiles），并以此度量检测器间的解释相似性/差异性；基于解释差异性选择互补检测器构建集成模型。

Result: 实验表明：解释相似的检测器其异常得分高度相关、检出异常重叠度高；解释差异大的检测器表现出更强互补性；仅追求多样性不足，需兼顾单模型性能；结合解释多样性与高质量单模型可显著提升集成效果。

Conclusion: 解释驱动的多样性度量为异常检测集成提供了区别于原始输出的新选择标准；在保证个体性能基础上显式优化解释多样性，能构建更互补、更鲁棒、更有效的无监督异常检测集成系统。

Abstract: Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ensemble that is genuinely complementary remains challenging, since many detectors rely on similar decision cues and end up producing redundant anomaly scores. As a result, the potential of ensemble learning is often limited by the difficulty of identifying models that truly capture different types of irregularities. To address this, we propose a methodology for characterizing anomaly detectors through their decision mechanisms. Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors. We show that detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior. Our results demonstrate that explanation-driven metrics offer a different criterion than raw outputs for selecting models in an ensemble. However, we also demonstrate that diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles. By explicitly targeting explanation diversity while maintaining model quality, we are able to construct ensembles that are more diverse, more complementary, and ultimately more effective for unsupervised anomaly detection.

</details>


### [534] [Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models](https://arxiv.org/abs/2602.00217)
*Chen Liu,Xingzhi Sun,Xi Xiao,Alexandre Van Tassel,Ke Xu,Kristof Reimann,Danqi Liao,Mark Gerstein,Tianyang Wang,Xiao Wang,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 本文发现小语言模型存在嵌入凝聚现象，提出分散损失函数缓解该问题，提升小模型性能。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型缩放规律，探索如何在小模型中复现大模型的表征特性。

Method: 通过系统分析多个Transformer家族，观察嵌入凝聚现象，并提出一种显式鼓励嵌入分散的损失函数（dispersion loss）来缓解该现象。

Result: 小模型如GPT2和Qwen3-0.6B存在严重嵌入凝聚，而大模型如GPT2-xl和Qwen3-32B更具抗性；所提dispersion loss能有效缓解凝聚、恢复大模型的分散模式，并在10个基准上提升性能。

Conclusion: 嵌入凝聚是小模型的关键限制，引入dispersion loss为无需增加参数即可提升小Transformer性能提供了可行路径。

Abstract: Large language models (LLMs) achieve remarkable performance through ever-increasing parameter counts, but scaling incurs steep computational costs. To better understand LLM scaling, we study representational differences between LLMs and their smaller counterparts, with the goal of replicating the representational qualities of larger models in the smaller models. We observe a geometric phenomenon which we term $\textbf{embedding condensation}$, where token embeddings collapse into a narrow cone-like subspace in some language models. Through systematic analyses across multiple Transformer families, we show that small models such as $\texttt{GPT2}$ and $\texttt{Qwen3-0.6B}$ exhibit severe condensation, whereas the larger models such as $\texttt{GPT2-xl}$ and $\texttt{Qwen3-32B}$ are more resistant to this phenomenon. Additional observations show that embedding condensation is not reliably mitigated by knowledge distillation from larger models. To fight against it, we formulate a dispersion loss that explicitly encourages embedding dispersion during training. Experiments demonstrate that it mitigates condensation, recovers dispersion patterns seen in larger models, and yields performance gains across 10 benchmarks. We believe this work offers a principled path toward improving smaller Transformers without additional parameters.

</details>


### [535] [GRIP2: A Robust and Powerful Deep Knockoff Method for Feature Selection](https://arxiv.org/abs/2602.00218)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

TL;DR: 本文提出GRIP2方法，通过在二维正则化表面上积分第一层特征活跃度来构建深度敲除特征重要性统计量，并利用块随机采样高效近似该积分，从而在有限样本下严格控制FDR，尤其在高相关性、低信噪比场景中表现出更强的鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在非线性、高相关性、低信噪比场景下，传统深度学习特征选择方法难以兼顾预测能力与假发现率（FDR）的严格控制。

Method: 提出Group Regularization Importance Persistence in 2 Dimensions (GRIP2)，在稀疏强度与稀疏几何结构构成的二维正则化表面上积分第一层特征活动；引入块随机采样策略，在单次训练中沿优化轨迹聚合不同正则化设置下的特征活动幅度；构造具有内建反对称性的统计量以保障有限样本FDR控制。

Result: 在合成与半真实数据实验中，GRIP2在高相关性与低信噪比条件下显著优于现有深度特征选择方法，保持高检出能力与稳定性；在真实HIV耐药数据上，其识别已知耐药突变的能力超越经典线性基线方法。

Conclusion: GRIP2为深度学习框架下实现可解释、可控FDR的特征选择提供了新范式，兼具理论严谨性与实际有效性。

Abstract: Identifying truly predictive covariates while strictly controlling false discoveries remains a fundamental challenge in nonlinear, highly correlated, and low signal-to-noise regimes, where deep learning based feature selection methods are most attractive. We propose Group Regularization Importance Persistence in 2 Dimensions (GRIP2), a deep knockoff feature importance statistic that integrates first-layer feature activity over a two-dimensional regularization surface controlling both sparsity strength and sparsification geometry. To approximate this surface integral in a single training run, we introduce efficient block-stochastic sampling, which aggregates feature activity magnitudes across diverse regularization regimes along the optimization trajectory. The resulting statistics are antisymmetric by construction, ensuring finite-sample FDR control. In extensive experiments on synthetic and semi-real data, GRIP2 demonstrates improved robustness to feature correlation and noise level: in high correlation and low signal-to-noise ratio regimes where standard deep learning based feature selectors may struggle, our method retains high power and stability. Finally, on real-world HIV drug resistance data, GRIP2 recovers known resistance-associated mutations with power better than established linear baselines, confirming its reliability in practice.

</details>


### [536] [Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting](https://arxiv.org/abs/2602.00240)
*Md Muhtasim Munif Fahim,Soyda Humyra Yesmin,Saiful Islam,Md. Palash Bin Faruque,Md. A. Salam,Md. Mahfuz Uddin,Samiul Islam,Tofayel Ahmed,Md. Binyamin,Md. Rezaul Karim*

Main category: cs.LG

TL;DR: Green-NAS是一个面向低资源环境的多目标神经架构搜索框架，以天气预报为案例，兼顾模型精度与能效，在显著降低参数量和碳足迹的同时保持高预测精度，并结合迁移学习提升数据稀缺城市的预报性能。


<details>
  <summary>Details</summary>
Motivation: 在低资源环境下实现可持续AI部署，减少NAS过程中的计算能耗与碳足迹，响应'绿色AI'理念。

Method: 提出多目标神经架构搜索（NAS）框架Green-NAS，同步优化模型精度与效率（如参数量、能耗），并引入迁移学习应对城市级历史气象数据稀缺问题。

Result: Green-NAS-A模型RMSE达0.0988（接近人工调优基线），仅含153k参数，比GraphCast等模型少239倍；迁移学习使小数据城市预报精度提升约5.2%。

Conclusion: Green-NAS验证了在关键科学应用（如天气预报）中，兼顾精度、轻量化与可持续性的NAS是可行且有效的，为绿色AI落地提供了可复用范式。

Abstract: We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to 'Green AI' principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.

</details>


### [537] [TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models](https://arxiv.org/abs/2602.00250)
*Shreshth Saini,Avinab Saha,Balu Adsumilli,Neil Birkbeck,Yilin Wang,Alan C. Bovik*

Main category: cs.LG

TL;DR: 本文提出Backward-on-Entropy (BoE) Steering，一种基于梯度引导的推理框架，通过单次反向传播近似无限视野预测，提升掩码扩散模型（MDMs）生成质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有MDM采样方法依赖局部置信度启发式，易导致早期幻觉引发全局不连贯；搜索类方法虽缓解该问题但计算开销过大。

Method: 提出Token Influence Score（TIS）作为控制信号，基于轨迹代价泛函一阶展开推导；设计ActiveQueryAttention稀疏伴随算子以降低反向传播复杂度。

Result: BoE在推理时扩展性上达到更优Pareto前沿，显著优于现有非自回归解码方法，在保持高效的同时提升生成鲁棒性。

Conclusion: 梯度引导的推理路径是实现高效、鲁棒非自回归生成的数学上严谨且实用的解决方案。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.

</details>


### [538] [VoxServe: Streaming-Centric Serving System for Speech Language Models](https://arxiv.org/abs/2602.00269)
*Keisuke Kamahori,Wei-Tzu Lee,Atindra Jha,Rohan Kadekodi,Stephanie Wang,Arvind Krishnamurthy,Baris Kasikci*

Main category: cs.LG

TL;DR: VoxServe是一个面向语音语言模型（SpeechLMs）的统一流式推理服务系统，通过解耦模型架构与系统优化，支持多种SpeechLM，并在保持低延迟的同时实现10-20倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有系统难以灵活高效地支持多样化的SpeechLM在流式场景下的部署，缺乏低延迟、高吞吐与强流式保证的统一解决方案。

Method: 提出一种模型执行抽象，解耦模型架构与系统优化；在此基础上设计流式感知调度策略和异步推理流水线。

Result: 在多个现代SpeechLM上评估显示，VoxServe在同等延迟下吞吐量达现有方案的10–20倍，且保持高流式可行性。

Conclusion: VoxServe为SpeechLM提供了高效、灵活、统一的流式服务框架，显著提升了实际部署性能与兼容性。

Abstract: Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.

</details>


### [539] [Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning](https://arxiv.org/abs/2602.00282)
*Naman Saxena,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文提出了一种约束双层强化学习算法CBSO，通过罚函数方法处理约束，首次结合Moreau包络分析了非光滑目标下的策略梯度算法，获得了O(ε⁻²)迭代复杂度和Õ(ε⁻⁴)样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有双层RL（如元学习、分层学习、基于人类反馈的RL）缺乏理论分析，尤其在约束设定下；传统方法受制于对偶间隙和超梯度计算问题。

Method: 提出Constrained Bilevel Subgradient Optimization (CBSO)算法，采用罚函数构造光滑近似目标，避免超梯度计算；利用Moreau包络技术分析非光滑优化下的策略梯度更新。

Result: 获得O(ε⁻²)迭代复杂度和Õ(ε⁻⁴)样本复杂度；首次实现对一般参数化策略梯度算法在非光滑目标下的理论分析。

Conclusion: 该工作填补了约束双层RL理论分析空白，为带约束的双层优化提供新分析工具，并拓展了策略梯度方法在非光滑优化中的适用性。

Abstract: Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(ε^{-2})$ and sample complexity of $\tilde{O}(ε^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.

</details>


### [540] [Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.00286)
*Shaorong Zhang,Longxuan Yu,Rob Brekelmans,Luhan Tang,Salman Asif,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 本文提出一个统一的信息论框架，分析掩码扩散模型（MDMs）中生成顺序与并行化带来的两类失败根源：顺序敏感性和并行化偏差，并通过理论与实验揭示Easy-First解码优势、并行采样导致的逆KL发散问题，以及验证与重掩码策略的权衡。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDMs）虽加速推理，但其生成顺序机制与并行化风险缺乏理论理解。

Method: 构建信息论框架，解耦并分析顺序敏感性与并行化偏差；理论推导Easy-First解码、因子化并行解码、验证与remasking等策略的误差与代价；在Block-HMM和LLaDA模型上实验验证。

Result: 发现Easy-First在模型误差增大时更有效；因子化并行解码会引发任意大的逆KL散度，反映‘不连贯’失败；验证可消除采样误差但代价指数增长；remasking高效但无法保证分布正确性。

Conclusion: MDMs的并行生成需在效率与分布保真间谨慎权衡，理论框架为设计鲁棒解码策略提供了新指导。

Abstract: Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework to decouple and analyze two fundamental sources of failure: order sensitivity and parallelization bias. Our analysis yields three key insights: (1) The benefits of Easy-First decoding (prioritizing low-entropy tokens) are magnified as model error increases; (2) factorized parallel decoding introduces intrinsic sampling errors that can lead to arbitrary large Reverse KL divergence, capturing "incoherence" failures that standard Forward KL metrics overlook; and (3) while verification can eliminate sampling error, it incurs an exponential cost governed by the total correlation within a block. Conversely, heuristics like remasking, though computationally efficient, cannot guarantee distributional correctness. Experiments on a controlled Block-HMM and large-scale MDMs (LLaDA) for arithmetic reasoning validate our theoretical framework.

</details>


### [541] [Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation](https://arxiv.org/abs/2602.00294)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 本文提出了一种新型自注意力机制，可在固定计算和内存成本下实现任意精度的高效计算，显著降低大模型对算力与能源的需求。


<details>
  <summary>Details</summary>
Motivation: 标准自注意力机制的计算与内存开销随上下文长度增长而急剧上升，已超出社会资源供给能力，亟需更高效的替代方案。

Method: 通过对标准自注意力的泰勒展开进行分解，利用对称张量链结构，构造出映射查询与键到最小多项式核特征基的前馈变换，实现常数级每词元成本。

Result: 实现了任意精度下每词元固定成本的自注意力计算，在内存与计算上实现数量级下降，并支持无界词元生成；实验验证了其正确性与可行性。

Conclusion: 该方法大幅降低大规模Transformer模型的基础设施与能耗需求，为可持续AI发展提供新路径，所引入的数学工具也具独立理论价值。

Abstract: The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.

</details>


### [542] [From Observations to States: Latent Time Series Forecasting](https://arxiv.org/abs/2602.00297)
*Jie Yang,Yifan Hu,Yuante Li,Kexin Zhang,Kaize Ding,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出LatentTSF新范式，将时间序列预测从观测空间回归转向潜在状态预测，通过自编码器构建高维平滑潜在空间以建模系统动力学，缓解‘潜在混沌’问题并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习TSF模型虽预测准确，但其学习到的潜在表征常呈现时间无序、不连续现象（即‘潜在混沌’），根源在于主流的观测空间预测范式易导致捷径学习，难以恢复真实系统动力学。

Method: 提出LatentTSF范式：使用AutoEncoder将每时刻观测映射至高维潜在状态空间，使潜在表征更贴近底层系统变量并具备时间平滑性；全部预测过程在潜在空间中进行；理论证明该潜在目标隐式最大化预测潜在状态与真实状态及观测间的互信息。

Result: 在多个主流基准数据集上实验表明，LatentTSF有效缓解潜在混沌现象，并取得优于现有方法的预测性能。

Conclusion: 将预测目标从观测空间迁移至结构化潜在空间，是提升时间序列模型可解释性、鲁棒性与泛化能力的关键路径；LatentTSF为TSF提供了新范式和理论支撑。

Abstract: Deep learning has achieved strong performance in Time Series Forecasting (TSF). However, we identify a critical representation paradox, termed Latent Chaos: models with accurate predictions often learn latent representations that are temporally disordered and lack continuity. We attribute this phenomenon to the dominant observation-space forecasting paradigm. Most TSF models minimize point-wise errors on noisy and partially observed data, which encourages shortcut solutions instead of the recovery of underlying system dynamics. To address this issue, we propose Latent Time Series Forecasting (LatentTSF), a novel paradigm that shifts TSF from observation regression to latent state prediction. Specifically, LatentTSF employs an AutoEncoder to project observations at each time step into a higher-dimensional latent state space. This expanded representation aims to capture underlying system variables and impose a smoother temporal structure. Forecasting is then performed entirely in the latent space, allowing the model to focus on learning structured temporal dynamics. Theoretical analysis demonstrates that our proposed latent objectives implicitly maximize mutual information between predicted latent states and ground-truth states and observations. Extensive experiments on widely-used benchmarks confirm that LatentTSF effectively mitigates latent chaos, achieving superior performance. Our code is available in https://github.com/Muyiiiii/LatentTSF.

</details>


### [543] [Agentic Framework for Epidemiological Modeling](https://arxiv.org/abs/2602.00299)
*Rituparna Datta,Zihan Guan,Baltazar Espinoza,Yiqi Su,Priya Pitre,Srini Venkatramanan,Naren Ramakrishnan,Anil Vullikanti*

Main category: cs.LG

TL;DR: 本文提出了EPIAGENT框架，通过将疾病传播建模为迭代程序综合问题，自动合成、校准、验证和优化流行病学模拟器，利用显式的流行病学流程图中间表示实现模块化正确性检验，并生成符合物理与流行病学约束的可解释机制模型。


<details>
  <summary>Details</summary>
Motivation: 传统流行病建模依赖固定模型结构，难以随病原体演化、政策变化和场景假设更新而灵活调整，亟需自动化、可验证且可解释的建模方法。

Method: 提出EPIAGENT智能体框架，采用流行病学流程图作为中间表示，支持自动合成、校准、验证与迭代优化；流程图经强模块化验证后编译为满足物理与流行病学约束的机制模型，并实现可解释参数学习。

Result: 在多个流行病场景案例中，EPIAGENT能准确捕捉复杂增长动力学，生成符合流行病学一致性的反事实预测（如不同疫苗接种与免疫逃逸假设下），并借助智能体反馈环显著加速有效模型收敛、防止模型退化。

Conclusion: EPIAGENT实现了专家级建模工作流的自动化，提升了流行病模型的适应性、可信度与可解释性，为动态公共卫生决策提供了新范式。

Abstract: Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.

</details>


### [544] [Neural Ising Machines via Unrolling and Zeroth-Order Training](https://arxiv.org/abs/2602.00302)
*Sam Reifenstein,Timothee Leleu*

Main category: cs.LG

TL;DR: 本文提出了一种名为神经网络参数化伊辛机（NPIM）的数据驱动启发式方法，用于求解NP难的伊辛模型和最大割问题。该方法通过零阶优化训练一个轻量级MLP，学习局部场到自旋更新的映射规则，无需反向传播，能有效探索非凸能量景观，并在多个基准上达到具有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 解决NP-hard的Ising和Max-Cut优化问题缺乏高效、稳定且可学习的启发式方法；传统基于梯度的训练在长程伊辛动力学中梯度不稳定且信息不足。

Method: 设计一个共享的节点级更新规则，由紧凑的多层感知机（MLP）参数化，输入为局部相互作用场，输出为自旋更新；采用零阶优化器进行训练，避免对长时循环动力学进行反向传播。

Result: NPIM在标准Ising和神经组合优化基准上，解质量与求解时间均媲美近期学习型方法及经典强启发式算法；其低参数量动态展现出动量行为和时变调度等有效算法结构。

Conclusion: NPIM是一种参数高效、训练稳定、泛化能力强的学习型伊辛机，为组合优化提供了一种新颖且实用的数据驱动动力系统建模范式。

Abstract: We propose a data-driven heuristic for NP-hard Ising and Max-Cut optimization that learns the update rule of an iterative dynamical system. The method learns a shared, node-wise update rule that maps local interaction fields to spin updates, parameterized by a compact multilayer perceptron with a small number of parameters. Training is performed using a zeroth-order optimizer, since backpropagation through long, recurrent Ising-machine dynamics leads to unstable and poorly informative gradients. We call this approach a neural network parameterized Ising machine (NPIM). Despite its low parameter count, the learned dynamics recover effective algorithmic structure, including momentum-like behavior and time-varying schedules, enabling efficient search in highly non-convex energy landscapes. Across standard Ising and neural combinatorial optimization benchmarks, NPIM achieves competitive solution quality and time-to-solution relative to recent learning-based methods and strong classical Ising-machine heuristics.

</details>


### [545] [Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors](https://arxiv.org/abs/2602.00315)
*Arian Khorasani,Nathaniel Chen,Yug D Oswal,Akshat Santhana Gopalan,Egemen Kolemen,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: 本文提出使用类条件归一化流作为oracle来获取真实后验分布，从而在真实图像数据集上精确评估神经网络性能极限，并揭示了标准基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 标准基准无法回答神经网络距离其理论最优性能有多近，因为它们缺乏对真实后验p(y|x)的访问。

Method: 采用类条件归一化流（class-conditional normalizing flows）建模真实后验，使其在AFHQ和ImageNet等现实图像数据上可计算，并以此开展五方面分析：缩放律、学习极限、软标签训练、分布偏移、主动学习。

Result: 发现预测误差可分解为不可减的随机不确定性（aleatoric）与可减的认知不确定性（epistemic）；后者随数据量呈幂律下降；不同架构逼近aleatoric下限能力差异显著；使用真实后验训练优于硬标签且校准更优；分布偏移中类型比幅度更重要；精确的epistemic不确定性可提升主动学习效率。

Conclusion: 标准评估指标掩盖了持续学习过程、隐藏了模型架构差异，并无法诊断分布偏移的本质；本文框架提供了更本质、可解释的模型性能分析范式。

Abstract: How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enables five lines of investigation. Scaling laws: Prediction error decomposes into irreducible aleatoric uncertainty and reducible epistemic error; the epistemic component follows a power law in dataset size, continuing to shrink even when total loss plateaus. Limits of learning: The aleatoric floor is exactly measurable, and architectures differ markedly in how they approach it: ResNets exhibit clean power-law scaling while Vision Transformers stall in low-data regimes. Soft labels: Oracle posteriors contain learnable structure beyond class labels: training with exact posteriors outperforms hard labels and yields near-perfect calibration. Distribution shift: The oracle computes exact KL divergence of controlled perturbations, revealing that shift type matters more than shift magnitude: class imbalance barely affects accuracy at divergence values where input noise causes catastrophic degradation. Active learning: Exact epistemic uncertainty distinguishes genuinely informative samples from inherently ambiguous ones, improving sample efficiency. Our framework reveals that standard metrics hide ongoing learning, mask architectural differences, and cannot diagnose the nature of distribution shift.

</details>


### [546] [Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection](https://arxiv.org/abs/2602.00318)
*Kunal Mukherjee,Zulfikar Alom,Tran Gia Bao Ngo,Cuneyt Gurcan Akcora,Murat Kantarcioglu*

Main category: cs.LG

TL;DR: 本文提出BOCLOAK，一种基于最优传输的轻量级框架，用于在现实约束下评估GNN社交机器人检测器的鲁棒性，显著提升攻击成功率并大幅降低GPU内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有GNN-based社交机器人检测器在真实场景下的鲁棒性缺乏充分评估，尤其在攻击者受领域和时间约束的情况下，传统攻击方法适用性受限，亟需更贴近实际的评估方法。

Method: 提出BOCLOAK框架，通过构建时空邻居特征的概率测度，学习区分人类与机器人行为的最优传输几何结构，并将传输计划解码为稀疏、符合现实约束的边编辑操作，支持边编辑与节点注入两类对抗攻击。

Result: 在三个社交机器人数据集、五个SOTA检测器及三种防御方法上验证，BOCLOAK攻击成功率最高提升80.13%，GPU内存消耗减少99.80%；证明最优传输是连接对抗攻击与现实检测的轻量、可解释框架。

Conclusion: BOCLOAK为GNN社交机器人检测提供了首个兼顾现实约束与理论严谨性的对抗评估框架，揭示了最优传输在构建轻量、可解释、高鲁棒性检测系统中的关键作用。

Abstract: The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.
  To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.

</details>


### [547] [Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference](https://arxiv.org/abs/2602.00328)
*Nikhil Gopal,Kostis Kaffes*

Main category: cs.LG

TL;DR: 本文提出Harvest框架，利用GPU间高带宽互连，将模型权重和KV缓存动态放置于空闲GPU内存中，以缓解LLM推理中的GPU内存瓶颈，显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）推理日益受限于GPU内存容量而非计算吞吐量，尤其是键值（KV）缓存在自回归解码过程中线性增长，加剧内存压力；现有卸载至主机内存的方法受限于PCIe带宽，带来高延迟。

Method: 提出Harvest——一种机会主义GPU缓存管理框架，将对端GPU内存视为临时缓存层，动态调度模型权重与KV缓存至空闲GPU显存，利用P2P高带宽互连减少数据移动开销，同时保证正确性。

Result: 在加速专家层权重和KV缓存检索两个关键推理组件时，Harvest实现超2倍的吞吐量提升。

Conclusion: Harvest通过智能利用多GPU间的空闲显存和高带宽互连，有效缓解LLM推理内存瓶颈，在不牺牲正确性的前提下显著提升推理效率。

Abstract: Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.

</details>


### [548] [In-Run Data Shapley for Adam Optimizer](https://arxiv.org/abs/2602.00329)
*Meng Ding,Zeqing Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 本文提出了一种面向Adam优化器的In-Run数据Shapley方法（Adam-Aware In-Run Data Shapley），解决了现有基于SGD的归因方法在Adam下失效的问题，通过新提出的线性化Ghost近似实现高保真、高效率的数据贡献评估。


<details>
  <summary>Details</summary>
Motivation: 现有In-Run数据归因方法依赖SGD的线性结构，无法准确刻画Adam等自适应优化器下的真实数据贡献，导致归因结果失真（Pearson R≈0.11），难以适用于现代训练流程。

Method: 提出Adam-Aware In-Run Data Shapley：1）在固定状态假设下重构效用函数以恢复可加性；2）引入Linearized Ghost Approximation，线性化方差相关缩放项，避免显式计算单样本梯度，仅需成对梯度点积。

Result: 在保持约95%标准训练吞吐量的同时，归因结果与真实边际贡献高度一致（Pearson R > 0.99），且在下游归因任务中显著优于SGD基线。

Conclusion: 数据归因本质上依赖于优化器；针对Adam设计专用归因机制是提升可靠性的关键，所提方法兼顾理论合理性、计算效率与实际有效性。

Abstract: Reliable data attribution is essential for mitigating bias and reducing computational waste in modern machine learning, with the Shapley value serving as the theoretical gold standard. While recent "In-Run" methods bypass the prohibitive cost of retraining by estimating contributions dynamically, they heavily rely on the linear structure of Stochastic Gradient Descent (SGD) and fail to capture the complex dynamics of adaptive optimizers like Adam. In this work, we demonstrate that data attribution is inherently optimizer-dependent: we show that SGD-based proxies diverge significantly from true contributions under Adam (Pearson $R \approx 0.11$), rendering them ineffective for modern training pipelines. To bridge this gap, we propose Adam-Aware In-Run Data Shapley. We derive a closed-form approximation that restores additivity by redefining utility under a fixed-state assumption and enable scalable computation via a novel Linearized Ghost Approximation. This technique linearizes the variance-dependent scaling term, allowing us to compute pairwise gradient dot-products without materializing per-sample gradients. Extensive experiments show that our method achieves near-perfect fidelity to ground-truth marginal contributions ($R > 0.99$) while retaining $\sim$95\% of standard training throughput. Furthermore, our Adam-aware attribution significantly outperforms SGD-based baselines in data attribution downstream tasks.

</details>


### [549] [Prototype-based Explainable Neural Networks with Channel-specific Reasoning for Geospatial Learning Tasks](https://arxiv.org/abs/2602.00331)
*Anushka Narayanan,Karianne J. Bergen*

Main category: cs.LG

TL;DR: 本文提出了一种面向多通道地理空间数据的原型驱动可解释AI方法，通过学习各物理变量/光谱通道的独立原型，实现对模型预测的局部与全局解释，在气候相位分类和土地利用遥感分类任务中验证了其有效性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的XAI方法主要面向标准RGB图像，未适配地学数据中多变量、异质通道的特点，难以提供物理意义明确的可解释性。

Method: 设计面向多通道地理空间数据的原型学习框架，为每个通道学习独立的原型，并建模其单独及组合对预测的影响；支持局部（样本级）和全局（模型级）解释。

Result: 在MJO相位分类与多光谱遥感土地利用分类两个地学任务中，该方法达到与标准神经网络相当的性能，同时提供通道级原型解释，增强模型透明性与可信度。

Conclusion: 将通道特异性原型显式嵌入预测过程，可有效提升机器学习模型在地学任务中的可解释性与可信度，为科学导向的AI建模提供了新范式。

Abstract: Explainable AI (XAI) is essential for understanding machine learning (ML) decision-making and ensuring model trustworthiness in scientific applications. Prototype-based XAI methods offer an intrinsically interpretable alternative to post-hoc approaches which often yield inconsistent explanations. Prototype-based XAI methods make predictions based on the similarity between inputs and learned prototypes that represent typical characteristics of target classes. However, existing prototype-based models are primarily designed for standard RGB image data and are not optimized for the distinct, variable-specific channels commonly found in geoscientific image and raster datasets. In this study, we develop a prototype-based XAI approach tailored for multi-channel geospatial data, where each channel represents a distinct physical environmental variable or spectral channel. Our approach enables the model to identify separate, channel-specific prototypical characteristics sourced from multiple distinct training examples that inform how these features individually and in combination influence model prediction while achieving comparable performance to standard neural networks. We demonstrate this method through two geoscientific case studies: (1) classification of Madden Julian Oscillation phases using multi-variable climate data and (2) land-use classification from multispectral satellite imagery. This approach produces both local (instance-level) and global (model-level) explanations for providing insights into feature-relevance across channels. By explicitly incorporating channel-prototypes into the prediction process, we discuss how this approach enhances the transparency and trustworthiness of ML models for geoscientific learning tasks.

</details>


### [550] [Efficient and accurate steering of Large Language Models through attention-guided feature learning](https://arxiv.org/abs/2602.00333)
*Parmida Davarmanesh,Ashia Wilson,Adityanarayanan Radhakrishnan*

Main category: cs.LG

TL;DR: 本文提出了一种注意力引导的LLM内部激活操控（steering）框架，解决了现有steering方法在特征提取、跨层异质性和关键层识别上的脆弱性问题，在512个语义概念上显著提升操控成功率，并揭示了概念特征在模型各层的分布规律。


<details>
  <summary>Details</summary>
Motivation: 现有steering方法极为脆弱，对算法细节敏感，难以稳定操控语义概念，亟需更鲁棒、自动化的框架。

Method: 提出注意力引导的steering框架，通过注意力机制自动选择相关token嵌入、建模跨层特征异质性、识别最适 steering 层。

Result: 在512个概念的基准测试中，成功steer的概念数量近翻倍，适用于多种架构和高达70B参数的模型；并揭示了概念特征在LLM各层中的分布特性。

Conclusion: 该框架提升了steering的鲁棒性与可扩展性，为工业级大模型的高效微调提供了新路径。

Abstract: Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.

</details>


### [551] [Adaptive Momentum and Nonlinear Damping for Neural Network Training](https://arxiv.org/abs/2602.00334)
*Aikaterini Karoni,Rajit Rajpal,Benedict Leimkuhler,Gabriel Stoltz*

Main category: cs.LG

TL;DR: 本文提出了一种连续时间的大规模优化方案，通过引入由每个参数动能调节的自适应动量系数，自动适应局部曲率以保持稳定性并保证收敛速度；该自适应阻尼机制与结构动力学中的立方阻尼相关，并据此构建了改进mSGD和Adam的两种新优化器，在ViT、BERT、GPT2等任务上表现优于或媲美Adam，且具有指数收敛的理论保证。


<details>
  <summary>Details</summary>
Motivation: 传统优化器（如mSGD）在训练大模型（如ViT、BERT、GPT2）时表现不佳，缺乏对局部损失曲率的自适应能力，导致稳定性与收敛速度难以兼顾。

Method: 提出基于连续时间动力学的优化框架，为每个参数引入由其动能决定的自适应摩擦（即动量衰减）系数；将该机制建模为立方阻尼，并分别嵌入mSGD和Adam的连续动力学系统中，形成两种新优化算法。

Result: 在ViT、BERT、GPT2等大规模模型训练任务上，所提方法鲁棒性强，性能匹配或超越Adam，显著优于标准mSGD；理论证明其具有指数收敛性。

Conclusion: 自适应动能驱动的立方阻尼机制能有效协调优化过程的稳定性与收敛效率，为连续时间优化提供了新范式，并在实际大模型训练中展现出优越性与理论严谨性。

Abstract: We propose a continuous-time scheme for large-scale optimization that introduces individual, adaptive momentum coefficients regulated by the kinetic energy of each model parameter. This approach automatically adjusts to local landscape curvature to maintain stability without sacrificing convergence speed. We demonstrate that our adaptive friction can be related to cubic damping, a suppression mechanism from structural dynamics. Furthermore, we introduce two specific optimization schemes by augmenting the continuous dynamics of mSGD and Adam with a cubic damping term. Empirically, our methods demonstrate robustness and match or outperform Adam on training ViT, BERT, and GPT2 tasks where mSGD typically struggles. We further provide theoretical results establishing the exponential convergence of the proposed schemes.

</details>


### [552] [Planning with Language and Generative Models: Toward General Reward-Guided Wireless Network Design](https://arxiv.org/abs/2602.00357)
*Chenyang Yuan,Xiaoyuan Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型和统一奖励函数的生成式推理方法，用于解决室内智能接入点（AP）部署问题，相比依赖外部验证器的LLM方法更高效、可扩展且适用于不同场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的AP部署方法依赖外部验证器，导致计算开销大、可扩展性差；需一种更高效、无需迭代精调、能应对非凸和碎片化目标的优化范式。

Method: 采用由统一奖励函数引导的生成式推理模型，重点比较并验证扩散采样器的优势；其通过平滑与锐化奖励景观实现渐进式采样优化，而非传统迭代精炼。

Result: 扩散采样器在多类平面图上持续优于其他生成方法；在自建的大规模真实数据集（训练耗时超5万CPU小时）上验证了模型的分布内/外泛化能力与鲁棒性。

Conclusion: 基于扩散模型与统一奖励函数的生成式推理为室内AP部署提供了一种可扩展、领域无关的新范式。

Abstract: Intelligent access point (AP) deployment remains challenging in next-generation wireless networks due to complex indoor geometries and signal propagation. We firstly benchmark general-purpose large language models (LLMs) as agentic optimizers for AP planning and find that, despite strong wireless domain knowledge, their dependence on external verifiers results in high computational costs and limited scalability. Motivated by these limitations, we study generative inference models guided by a unified reward function capturing core AP deployment objectives across diverse floorplans. We show that diffusion samplers consistently outperform alternative generative approaches. The diffusion process progressively improves sampling by smoothing and sharpening the reward landscape, rather than relying on iterative refinement, which is effective for non-convex and fragmented objectives. Finally, we introduce a large-scale real-world dataset for indoor AP deployment, requiring over $50k$ CPU hours to train general reward functions, and evaluate in- and out-of-distribution generalization and robustness. Our results suggest that diffusion-based generative inference with a unified reward function provides a scalable and domain-agnostic foundation for indoor AP deployment planning.

</details>


### [553] [Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition](https://arxiv.org/abs/2602.00360)
*Sumana Biswas,Karen Young,Josephine Griffith*

Main category: cs.LG

TL;DR: 本文提出了一种名为TEMSA的新方法，通过结合图像中检测到的物体名称与文本数据（即TEMS）来提升多模态情感分析性能，并在两个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析面临文本与图像模态差异大、情感模糊及上下文语义复杂等挑战。

Method: 提出TEMSA方法，利用目标识别技术提取图像中的物体名称，并将其与关联文本组合形成TEMS表示，再进行情感分析。

Result: 仅使用TEMS表示即可在整体情感预测任务中优于单独分析文本或图像的结果。

Conclusion: TEMSA能有效融合图像与文本信息，推动多模态情感分析的发展。

Abstract: Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.

</details>


### [554] [Quantum Generator Kernels](https://arxiv.org/abs/2602.00361)
*Philipp Altmann,Maximilian Mansky,Maximilian Zorn,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 本文提出了一种基于生成器的量子核方法（QGK），通过可参数化的变分生成器组（VGG）高效压缩并嵌入大规模数据（如图像）到NISQ设备受限的量子空间中，并通过学习权重向量优化核对目标域的适配性，在投影与分类任务上优于现有量子与经典核方法。


<details>
  <summary>Details</summary>
Motivation: 当前量子机器学习受限于NISQ硬件容量，难以有效嵌入大规模现实数据（如图像）；现有混合架构因固定中间嵌入过程，未能充分释放量子计算潜力。

Method: 提出量子生成器核（QGK），由多个变分生成器组（VGG）构成，将通用生成器融合为可参数化算子，实现对量子空间的可扩展覆盖；并通过训练权重向量动态调整VGG在当前数据上下文中的投影。

Result: 实验表明QGK在投影和分类性能上优于当前最优的量子与经典核方法，并展现出作为多种QML应用通用框架的潜力。

Conclusion: QGK通过生成器驱动、可学习的嵌入机制，克服了NISQ时代数据嵌入瓶颈，提升了量子核方法的实用性与适应性，为QML实际应用提供了新范式。

Abstract: Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.

</details>


### [555] [Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation](https://arxiv.org/abs/2602.00372)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 本文提出SparseKD方法，通过结构化SVD剪枝与自参考知识蒸馏结合，在不依赖外部教师模型的情况下压缩大语言模型，实现参数减少15-65%且质量损失可控，并具备即插即用部署能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署成本高，亟需高效、易部署的压缩方法。

Method: 提出Sparse Knowledge Distillation（SparseKD），在训练后阶段将结构化SVD剪枝与自参考知识蒸馏相结合：模型以自身压缩前的概率分布为监督信号进行蒸馏。

Result: 自参考蒸馏单独使用即可使模型质量相对提升39%；SparseKD在0.6B和3.8B模型上实现15–65%参数削减，质量损失可接受；加速源于前馈层密集矩阵乘法减少，注意力计算不变；多随机种子实验验证高可复现性。

Conclusion: SparseKD无需外部教师、不改模型结构、不依赖定制推理内核，可直接部署于现有基础设施，是一种实用、高效、鲁棒的大模型压缩方案。

Abstract: Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external teacher, the model teaches itself by matching its own probability distribution from before compression. This self-referential setup enables surprisingly strong quality recovery after aggressive pruning.
  Our experiments reveal an unexpected finding: self-referential distillation alone, applied post-training under an identical objective and fixed calibration dataset, improves model quality by 39% relative to the original converged checkpoint. When combined with structured pruning, SparseKD achieves 15-65% parameter reduction with acceptable quality trade-offs. Kernel profiling shows that speedups arise entirely from reduced dense matrix multiplication in feed-forward layers while attention remains unchanged, making this approach complementary to attention optimizations.
  We validate across two model families (0.6B and 3.8B parameters) with multi-seed experiments confirming high reproducibility. SparseKD requires no external super-teacher, no architectural changes, and no custom inference kernels, making it immediately deployable with existing infrastructure.

</details>


### [556] [MATRIX: A Multimodal Benchmark and Post-Training Framework for Materials Science](https://arxiv.org/abs/2602.00376)
*Delia McGrath,Curtis Chong,Rohil Kulkarni,Gerbrand Ceder,Adeesh Kolluru*

Main category: cs.LG

TL;DR: 本文提出MATRIX多模态材料科学推理基准，通过对比纯文本与图文联合微调效果，证明视觉监督能显著提升实验解释与科学推理能力，并揭示跨模态表征迁移的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以评估在后训练阶段引入视觉实验数据是否能超越纯文本监督，提升基于机制的解释性推理能力。

Method: 构建MATRIX多模态基准，涵盖基础理论、研究级推理及多模态实验图像理解；在控制条件下比较仅用结构化文本与结合实验图像进行后训练的效果。

Result: 视觉监督使实验解释能力提升10–25%，文本科学推理任务提升5–16%；效果依赖图像-文本对齐；在ScienceQA和PubMedQA上也观察到一致性提升。

Conclusion: 结构化多模态后训练可有效增强模型的机制理解与跨领域泛化能力，关键在于高质量的图像-文本对齐与跨模态表征迁移。

Abstract: Scientific reasoning in materials science requires integrating multimodal experimental evidence with underlying physical theory. Existing benchmarks make it difficult to assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning beyond text-only supervision. We introduce MATRIX, a multimodal benchmark for materials science reasoning that evaluates foundational theory, research-level reasoning, and the interpretation of real experimental artifacts across multiple characterization modalities. Using MATRIX as a controlled diagnostic, we isolate the effect of visual grounding by comparing post-training on structured materials science text alone with post-training that incorporates paired experimental images. Despite using relatively small amounts of multimodal data, visual supervision improves experimental interpretation by 10-25% and yields 5-16% gains on text-only scientific reasoning tasks. Our results demonstrate that these improvements rely on correct image-text alignment during post-training, highlighting cross-modal representational transfer. We also observe consistent improvements on ScienceQA and PubMedQA, demonstrating that the benefits of structured multimodal post-training extend beyond materials science. The MATRIX dataset is available at https://huggingface.co/datasets/radical-ai/MATRIX and the model at https://huggingface.co/radical-ai/MATRIX-PT.

</details>


### [557] [RePaint-Enhanced Conditional Diffusion Model for Parametric Engineering Designs under Performance and Parameter Constraints](https://arxiv.org/abs/2602.00384)
*Ke Wang,Nguyen Gia Hien Vu,Yifan Tang,Mostafa Rahmani Dehaghani,G. Gary Wang*

Main category: cs.LG

TL;DR: 本文提出了一种RePaint增强框架，结合预训练的性能引导去噪扩散概率模型（DDPM），用于满足性能与参数约束的工程设计生成，无需重新训练模型即可基于部分参考设计生成缺失组件。


<details>
  <summary>Details</summary>
Motivation: 传统DDPM方法无法在推理阶段同时满足性能和参数约束下的可控设计生成，尤其难以基于部分参考设计补全并保证性能。

Method: 采用RePaint方法，在推理过程中引入掩码（mask）驱动的重采样机制，利用预训练的性能引导DDPM模型，对部分设计进行可控重绘，实现性能与参数双重约束下的生成。

Result: 在船体参数化设计和翼型设计两个典型任务上验证有效：能基于部分参考设计生成满足预期性能的新颖设计，精度媲美或优于全量预训练模型，且支持固定部分设计以控制生成新颖性。

Conclusion: 该方法提供了一种高效、免训练的生成式设计解决方案，适用于需兼顾性能约束与参数可控性的工程设计场景。

Abstract: This paper presents a RePaint-enhanced framework that integrates a pre-trained performance-guided denoising diffusion probabilistic model (DDPM) for performance- and parameter-constraint engineering design generation. The proposed method enables the generation of missing design components based on a partial reference design while satisfying performance constraints, without retraining the underlying model. By applying mask-based resampling during inference process, RePaint allows efficient and controllable repainting of partial designs under both performance and parameter constraints, which is not supported by conventional DDPM-base methods. The framework is evaluated on two representative design problems, parametric ship hull design and airfoil design, demonstrating its ability to generate novel designs with expected performance based on a partial reference design. Results show that the method achieves accuracy comparable to or better than pre-trained models while enabling controlled novelty through fixing partial designs. Overall, the proposed approach provides an efficient, training-free solution for parameter-constraint-aware generative design in engineering applications.

</details>


### [558] [A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode](https://arxiv.org/abs/2602.00388)
*Zeyuan He,Yupeng Chen,Lang Lin,Yihan Wang,Shenxu Chang,Eric Sommerlade,Philip Torr,Junchi Yu,Adel Bibi,Jialin Yu*

Main category: cs.LG

TL;DR: 本文探讨了扩散式大语言模型（D-LLMs）相较于自回归大语言模型（AR-LLMs）在对抗越狱攻击时所展现出的内在安全性优势，并揭示其机制为‘逐步抑制’不安全生成；但同时发现一种简单却有效的失败模式——上下文嵌套，可绕过该机制，成功攻击包括Gemini Diffusion在内的商用D-LLMs。


<details>
  <summary>Details</summary>
Motivation: 探索D-LLMs是否具备区别于AR-LLMs的安全特性，尤其是其扩散式生成过程是否天然抵抗针对AR-LLMs设计的越狱攻击。

Method: 理论分析扩散轨迹的逐步抑制机制，并提出并验证‘上下文嵌套’这一新型攻击策略，通过将恶意请求嵌入良性结构化上下文中绕过安全抑制。

Result: 实证表明上下文嵌套能显著提升越狱成功率，在多个模型与基准上达到SOTA；首次成功越狱Gemini Diffusion，暴露商用D-LLMs关键安全漏洞。

Conclusion: D-LLMs虽具源自扩散机制的安全‘祝福’，但该鲁棒性非绝对；上下文嵌套揭示其本质局限，构成对D-LLMs早期红队评估的重要基础。

Abstract: Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.

</details>


### [559] [Localized, High-resolution Geographic Representations with Slepian Functions](https://arxiv.org/abs/2602.00392)
*Arjun Rao,Ruth Crasto,Tessa Ooms,David Rolnick,Konstantin Klemmer,Marc Rußwurm*

Main category: cs.LG

TL;DR: 本文提出了一种基于球面Slepian函数的地理编码器，能集中表征能力于感兴趣区域，支持高分辨率且计算高效；还设计了混合Slepian-球谐编码器以兼顾局部与全局性能，同时保持极点安全性和球面距离保真性。实验表明其在多任务、多网络架构下均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有地理机器学习模型对全球位置均匀分配表征能力，难以满足局部精细化应用需求，如疾病爆发、生态模式和经济活动等具有强地域聚集性的场景。

Method: 提出基于球面Slepian函数的地理位置编码器，实现区域聚焦与高分辨率可扩展性；进一步设计Slepian-球谐混合编码器，在保留极点安全性与球面距离保真性前提下平衡局部与全局建模能力。

Result: 在涵盖分类、回归及图像增强预测的五项任务中，Slepian编码器全面超越基线方法，并在多种神经网络架构下保持性能优势。

Conclusion: 球面Slepian函数为地理编码提供了更适配局部性本质的新范式，兼具高效性、可扩展性与几何保真性，有望推动高分辨率地理智能应用的发展。

Abstract: Geographic data is fundamentally local. Disease outbreaks cluster in population centers, ecological patterns emerge along coastlines, and economic activity concentrates within country borders. Machine learning models that encode geographic location, however, distribute representational capacity uniformly across the globe, struggling at the fine-grained resolutions that localized applications require. We propose a geographic location encoder built from spherical Slepian functions that concentrate representational capacity inside a region-of-interest and scale to high resolutions without extensive computational demands. For settings requiring global context, we present a hybrid Slepian-Spherical Harmonic encoder that efficiently bridges the tradeoff between local-global performance, while retaining desirable properties such as pole-safety and spherical-surface-distance preservation. Across five tasks spanning classification, regression, and image-augmented prediction, Slepian encodings outperform baselines and retain performance advantages across a wide range of neural network architectures.

</details>


### [560] [Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity](https://arxiv.org/abs/2602.00397)
*Aayush Gautam,Mukul Gagrani,Junyoung Park,Mingu Lee,Chiris Lott,Narasimha Reddy*

Main category: cs.LG

TL;DR: 本文提出FastForward框架，通过块级、上下文感知的前馈网络（FFN）稀疏化方法加速大语言模型（LLM）预填充（prefill）阶段，在保持较低精度损失的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: LLM推理中预填充阶段在长上下文场景下成为关键计算瓶颈，尤其在1K–16K token范围内，FFN主导计算开销；现有FFN稀疏化方法未适配预填充的并行特性，且易损精度。

Method: 提出FastForward：（1）轻量专家预测器按块选择高重要性神经元；（2）误差补偿网络校正稀疏引入的误差；（3）层间稀疏度调度器依据token混合重要性动态分配算力。

Result: 在LLaMA和Qwen系列至8B参数模型上，50% FFN稀疏度下实现最高1.45×计算受限加速，LongBench上精度损失<6%，显著降低首Token延迟（TTFT）。

Conclusion: FastForward是一种高效、准确、硬件友好的预填充加速方案，为长上下文LLM在资源受限设备上的部署提供了新路径。

Abstract: The prefill stage of large language model (LLM) inference is a key computational bottleneck for long-context workloads. At short-to-moderate context lengths (1K--16K tokens), Feed-Forward Networks (FFNs) dominate this cost, accounting for most of the total FLOPs. Existing FFN sparsification methods, designed for autoregressive decoding, fail to exploit the prefill stage's parallelism and often degrade accuracy. To address this, we introduce FastForward, a predictive sparsity framework that accelerates LLM prefill through block-wise, context-aware FFN sparsity. FastForward combines (1) a lightweight expert predictor to select high-importance neurons per block, (2) an error compensation network to correct sparsity-induced errors, and (3) a layer-wise sparsity scheduler to allocate compute based on token-mixing importance. Across LLaMA and Qwen models up to 8B parameters, FastForward delivers up to 1.45$\times$ compute-bound speedup at 50% FFN sparsity with $<$ 6% accuracy loss compared to the dense baseline on LongBench, substantially reducing Time-to-First-Token (TTFT) for efficient, long-context LLM inference on constrained hardware.

</details>


### [561] [MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers](https://arxiv.org/abs/2602.00398)
*Ajay Jaiswal,Lauren Hannah,Han-Byul Kim,Duc Hoang,Arnav Kundu,Mehrdad Farajtabar,Minsik Cho*

Main category: cs.LG

TL;DR: 本文提出MemoryLLM，通过将前馈网络（FFN）与自注意力机制解耦，将其建模为上下文无关的词元级神经检索记忆，提升可解释性与推理效率；并引入Flex-MemoryLLM作为性能与解耦性之间的折中方案。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型中transformer组件（尤其是FFN）的运作机制对AI可解释性至关重要，但FFN的可解释性面临挑战。

Method: 提出MemoryLLM，将FFN独立于自注意力进行训练，仅使用词元嵌入输入，使其成为上下文无关的词元级记忆；引入Flex-MemoryLLM作为过渡架构；支持FFN参数预计算为词元级查找表（ToLs），实现显存与存储间的按需加载。

Result: 实现了FFN的解耦建模与高效推理，验证了FFN在不同下游任务中的记忆重要性，并展示了ToL带来的显存优化与推理加速潜力。

Conclusion: FFN可被有效建模为独立、可解释、可检索的记忆模块；MemoryLLM及其变体为模型压缩、推理优化与可解释性研究提供了新范式。

Abstract: Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.

</details>


### [562] [DROGO: Default Representation Objective via Graph Optimization in Reinforcement Learning](https://arxiv.org/abs/2602.00403)
*Hon Tik Tse,Marlos C. Machado*

Main category: cs.LG

TL;DR: 本文提出了一种直接用神经网络近似默认表示（DR）主特征向量的目标函数，避免了传统先构建DR矩阵再进行特征分解的高计算成本方法，提升了在高维空间中的可扩展性，并在多个环境中验证了其在奖励塑形等任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过先近似DR矩阵再进行特征分解来获取其主特征向量，计算昂贵且难以扩展到高维空间。

Method: 推导出一个可优化的目标函数，使神经网络能直接学习DR的主特征向量。

Result: 在多个环境中实证验证了该目标函数的有效性，并成功将学习到的特征向量应用于奖励塑形。

Conclusion: 所提方法显著提升了DR主特征向量估计的效率与可扩展性，为强化学习中基于DR的应用提供了更实用的工具。

Abstract: In computational reinforcement learning, the default representation (DR) and its principal eigenvector have been shown to be effective for a wide variety of applications, including reward shaping, count-based exploration, option discovery, and transfer. However, in prior investigations, the eigenvectors of the DR were computed by first approximating the DR matrix, and then performing an eigendecomposition. This procedure is computationally expensive and does not scale to high-dimensional spaces. In this paper, we derive an objective for directly approximating the principal eigenvector of the DR with a neural network. We empirically demonstrate the effectiveness of the objective in a number of environments, and apply the learned eigenvectors for reward shaping.

</details>


### [563] [Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks](https://arxiv.org/abs/2602.00407)
*Suprim Nakarmi,Junggab Son,Yue Zhao,Zuobin Xiong*

Main category: cs.LG

TL;DR: 本文提出Fed-Listing攻击方法，仅利用联邦图神经网络（FedGNNs）中交换的最终层梯度，即可在无原始数据和节点特征的情况下推断目标客户端的私有标签分布，且对现有防御机制具有较强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管FedGNNs旨在保护用户隐私，但共享的梯度仍可能泄露本地敏感信息；而标签分布推断这一关键隐私威胁在FedGNNs中尚未被充分研究。

Method: 提出Fed-Listing——一种基于梯度的标签分布推断攻击：利用最终层梯度，结合辅助影子数据集模拟多种标签划分策略，训练攻击模型以恢复客户端的类别比例。

Result: 在四个基准数据集和三种GNN架构上，Fed-Listing显著优于随机猜测和Decaf等基线方法，即使在非独立同分布（non-i.i.d.）场景下依然有效；多数防御手段对其攻击性能影响甚微，除非严重损害模型效用。

Conclusion: FedGNNs中的梯度共享仍存在严重的标签分布隐私泄露风险，Fed-Listing揭示了当前隐私保护机制的不足，强调需设计更鲁棒的梯度防护或新型联邦学习范式。

Abstract: Graph Neural Networks (GNNs) have been intensively studied for their expressive representation and learning performance on graph-structured data, enabling effective modeling of complex relational dependencies among nodes and edges in various domains. However, the standalone GNNs can unleash threat surfaces and privacy implications, as some sensitive graph-structured data is collected and processed in a centralized setting. To solve this issue, Federated Graph Neural Networks (FedGNNs) are proposed to facilitate collaborative learning over decentralized local graph data, aiming to preserve user privacy. Yet, emerging research indicates that even in these settings, shared model updates, particularly gradients, can unintentionally leak sensitive information of local users. Numerous privacy inference attacks have been explored in traditional federated learning and extended to graph settings, but the problem of label distribution inference in FedGNNs remains largely underexplored. In this work, we introduce Fed-Listing (Federated Label Distribution Inference in GNNs), a novel gradient-based attack designed to infer the private label statistics of target clients in FedGNNs without access to raw data or node features. Fed-Listing only leverages the final-layer gradients exchanged during training to uncover statistical patterns that reveal class proportions in a stealthy manner. An auxiliary shadow dataset is used to generate diverse label partitioning strategies, simulating various client distributions, on which the attack model is obtained. Extensive experiments on four benchmark datasets and three GNN architectures show that Fed-Listing significantly outperforms existing baselines, including random guessing and Decaf, even under challenging non-i.i.d. scenarios. Moreover, applying defense mechanisms can barely reduce our attack performance, unless the model's utility is severely degraded.

</details>


### [564] [Variational Approach for Job Shop Scheduling](https://arxiv.org/abs/2602.00408)
*Seung Heon Oh,Jiwon Baek,Ki Young Cho,Hee Chang Yoon,Jong Hun Woo*

Main category: cs.LG

TL;DR: 本文提出了一种名为变分图到调度器（VG2S）的新框架，用于解决作业车间调度问题（JSSP），通过引入变分推断和基于ELBO的最大熵强化学习，解耦表征学习与策略优化，从而提升训练稳定性、鲁棒性和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法在求解JSSP时存在训练非平稳性和对未见实例泛化能力差的问题，因其同时优化表征学习与策略执行。

Method: 首次将变分推断引入JSSP领域，构建基于证据下界（ELBO）与最大熵强化学习的概率目标函数，并通过变分图编码器实现表征学习与策略优化的数学解耦。

Result: VG2S在DMU和SWV等大规模难例上展现出优于现有DRL基线和传统调度规则的零样本泛化性能，且训练更稳定、对超参数变化更鲁棒。

Conclusion: VG2S框架有效提升了JSSP求解的泛化性、稳定性和鲁棒性，验证了变分推断在组合优化调度任务中的潜力。

Abstract: This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.

</details>


### [565] [Robustness of AutoML on Dirty Categorical Data](https://arxiv.org/abs/2602.00412)
*Marcos L. P. Bueno,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 本文提出了一种将脏类别数据通过形态编码器转换为数值数据的AutoML预处理流程，并在多组脏数据集上评估其相较于现有AutoML方法的鲁棒性与预测性能差异，同时分析AutoML自建管道以获取更深层洞察。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML方法在处理高基数、未清洗的类别数据（即“脏类别数据”）时行为不明，而形态编码器已被证明可提升此类数据上的模型性能，但其与AutoML结合的效果尚无研究。

Method: 设计一种将脏类别数据经由形态编码器转化为数值特征的预处理管道，嵌入到AutoML流程中；在多个脏类别数据集上系统性地对比该管道与主流AutoML方法的预测性能及所构建的ML管道结构。

Result: 实验表明，引入形态编码器的AutoML流程在脏类别数据上显著提升预测性能；同时分析发现，不同AutoML方法生成的管道在预处理与模型选择上存在明显差异。

Conclusion: 形态编码器可有效增强AutoML对脏类别数据的鲁棒性与泛化能力；分析完整ML管道比仅关注最优模型更能揭示AutoML的行为机制与局限性。

Abstract: The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical datasets. These datasets often have several categorical features with high cardinality arising from issues such as lack of curation and automated collection. Recent research has shown that ML models can benefit from morphological encoders for dirty categorical data, leading to significantly superior predictive performance. However the effects of using such encoders in AutoML methods are not known at the moment. In this paper, we propose a pipeline that transforms categorical data into numerical data so that an AutoML can handle categorical data transformed by more advanced encoding schemes. We benchmark the current robustness of AutoML methods on a set of dirty datasets and compare it with the proposed pipeline. This allows us to get insight on differences in predictive performance. We also look at the ML pipelines built by AutoMLs in order to gain insight beyond the best model as typically returned by these methods.

</details>


### [566] [Federated-inspired Single-cell Batch Integration in Latent Space](https://arxiv.org/abs/2602.00423)
*Quang-Huy Nguyen,Zongliang Yue,Hao Chen,Wei-Shinn Ku,Jiaqi Wang*

Main category: cs.LG

TL;DR: 本文提出scBatchProx，一种受联邦学习启发的后处理优化方法，用于在无需原始表达数据和中心化训练的前提下，对单细胞RNA测序数据中任意上游方法生成的细胞嵌入进行批效应校正。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据跨实验积累导致批效应掩盖真实生物学信号，而现有批校正方法或校正不足，或依赖中心化重训练，难以适用于分布式、持续演化的数据场景。

Method: scBatchProx将每个批次视为一个客户端，通过近端正则化学习批次条件适配器，在潜在空间中直接校正批次结构，仅优化批次特异性适配器参数，不需原始表达数据或集中优化。

Result: 大量实验表明，scBatchProx在整体嵌入质量上带来约3–8%的相对提升；在90%的数据-方法组合中改善批效应校正，在85%中增强生物学结构保留。

Conclusion: scBatchProx是一种轻量、可部署的后处理方案，为动态单细胞数据系统中学习表征的实用化精炼提供了新思路。

Abstract: Advances in single-cell RNA sequencing enable the rapid generation of massive, high-dimensional datasets, yet the accumulation of data across experiments introduces batch effects that obscure true biological signals. Existing batch correction approaches either insufficiently correct batch effects or require centralized retraining on the complete dataset, limiting their applicability in distributed and continually evolving single-cell data settings. We introduce scBatchProx, a post-hoc optimization method inspired by federated learning principles for refining cell-level embeddings produced by arbitrary upstream methods. Treating each batch as a client, scBatchProx learns batch-conditioned adapters under proximal regularization, correcting batch structure directly in latent space without requiring raw expression data or centralized optimization. The method is lightweight and deployable, optimizing batch-specific adapter parameters only. Extensive experiments show that scBatchProx consistently yields relative gains of approximately 3-8% in overall embedding quality, with batch correction and biological conservation improving in 90% and 85% of data-method pairs, respectively. We envision this work as a step toward the practical refinement of learned representations in dynamic single-cell data systems.

</details>


### [567] [Open Materials Generation with Inference-Time Reinforcement Learning](https://arxiv.org/abs/2602.00424)
*Philipp Hoellmer,Stefano Martiniani*

Main category: cs.LG

TL;DR: 本文提出OMatG-IRL框架，首次将策略梯度强化学习应用于基于流的晶体材料生成模型，在不依赖显式得分函数的前提下，直接操作学习到的速度场，实现面向目标性质（如能量）的逆向材料设计，并显著提升采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有连续时间生成模型虽能预测稳定晶体结构，但难以在生成过程中显式融入目标物性；而策略梯度强化学习因需得分函数，无法直接用于仅学习速度场的流模型。

Method: 提出OMatG-IRL：一种无需显式得分计算的策略梯度RL框架，通过在推理时对生成动力学施加随机扰动，直接在速度场上进行策略优化与梯度估计，并支持组成条件控制与时间依赖的速度退火调度学习。

Result: 首次实现RL在晶体结构预测（CSP）中的应用；在保持多样性前提下有效优化能量目标；采样效率提升一个数量级，生成时间显著减少；性能媲美基于得分的RL方法。

Conclusion: OMatG-IRL为流式生成模型提供了通用、高效的推理时强化学习范式，推动了面向物性的可控晶体材料逆向设计。

Abstract: Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.

</details>


### [568] [LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference](https://arxiv.org/abs/2602.00426)
*Vikram Krishnamurthy*

Main category: cs.LG

TL;DR: 本文提供了一个数学框架，用于明确描述大型语言模型（LLMs）的训练、对齐与生成过程，将LLM建模为高维非线性自回归模型，并统一涵盖预训练、多种对齐方法（如RLHF、DPO等）及推理现象。


<details>
  <summary>Details</summary>
Motivation: 现有LLM描述多依赖架构组件和训练流程，缺乏清晰的底层计算结构与方程级数学刻画，阻碍理论分析与深入理解。

Method: 将LLMs形式化为具有注意力依赖的高维非线性自回归模型；推导自注意力为双线性–Softmax–线性组合；系统建模预训练、各类对齐方法（RLHF、DPO、RSFT、RLVR）及自回归生成。

Result: 建立了统一的数学框架，可自然解释自注意力机制，并支持对对齐行为（如谄媚）、推理现象（幻觉、上下文学习、思维链、RAG）及持续学习等进行原理性分析。

Conclusion: 该框架不仅为LLM研究提供了简洁严谨的数学参考，也为后续理论发展与模型可解释性研究奠定了基础。

Abstract: Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking an explicit, equation-level description of LLM training, alignment, and generation. We formulate LLMs as high-dimensional nonlinear autoregressive models with attention-based dependencies. The framework encompasses pretraining via next-token prediction, alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), rejection sampling fine-tuning (RSFT), and reinforcement learning from verifiable rewards (RLVR), as well as autoregressive generation during inference. Self-attention emerges naturally as a repeated bilinear--softmax--linear composition, yielding highly expressive sequence models. This formulation enables principled analysis of alignment-induced behaviors (including sycophancy), inference-time phenomena (such as hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation), and extensions like continual learning, while serving as a concise reference for interpretation and further theoretical development.

</details>


### [569] [Towards Building Non-Fine-Tunable Foundation Models](https://arxiv.org/abs/2602.00446)
*Ziyao Wang,Nizhang Li,Pingzhi Li,Guoheng Sun,Tianlong Chen,Ang Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为Private Mask Pre-Training（PMP）的预训练框架，旨在构建不可微调的基础模型，通过在训练早期识别稀疏子网络并私有化其二值掩码，仅发布稠密权重，从而限制未经授权的微调效果。


<details>
  <summary>Details</summary>
Motivation: 开源基础模型虽促进重用，但也使训练者面临经济与安全风险，尤其是任务无关的未授权微调。

Method: 提出Private Mask Pre-Training（PMP），在预训练早期识别并私有化一个稀疏子网络的二值掩码，仅发布最终稠密权重；未经授权的微调因缺乏掩码而更新错位参数，导致目标与预训练几何结构失配。

Result: 理论分析表明该失配会破坏基于梯度的适应性，并限制微调增益；实验表明PMP在保持基模型性能的同时，显著削弱多种下游任务上的未授权微调效果，且非微调强度可通过掩码比率调控。

Conclusion: PMP是一种有效实现基础模型可控重用与风险防控平衡的新范式，为模型版权保护与安全分发提供了新思路。

Abstract: Open-sourcing foundation models (FMs) enables broad reuse but also exposes model trainers to economic and safety risks from unrestricted downstream fine-tuning. We address this problem by building non-fine-tunable foundation models: models that remain broadly usable in their released form while yielding limited adaptation gains under task-agnostic unauthorized fine-tuning. We propose Private Mask Pre-Training (PMP), a pre-training framework that concentrates representation learning into a sparse subnetwork identified early in training. The binary mask defining this subnetwork is kept private, and only the final dense weights are released. This forces unauthorized fine-tuning without access to the mask to update parameters misaligned with pretraining subspace, inducing an intrinsic mismatch between the fine-tuning objective and the pre-training geometry. We provide theoretical analysis showing that this mismatch destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirical results on large language models demonstrating that PMP preserves base model performance while consistently degrading unauthorized fine-tuning across a wide range of downstream tasks, with the strength of non-fine-tunability controlled by the mask ratio.

</details>


### [570] [Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA](https://arxiv.org/abs/2602.00451)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出TAD-LoRA框架，解决去中心化联邦学习中LoRA微调因通信拓扑动态变化导致的训练不稳定问题，通过协调LoRA因子更新与混合来控制客户端间错位，并在非凸目标下证明其收敛性，实验表明其在不同连通性拓扑下均具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）中，LoRA等低秩适配方法因参数分解结构，在动态通信图下进行去中心化聚合时会引入依赖拓扑的交叉项，导致训练不稳定。

Method: 提出Topology-Aware Decentralized LoRA（TAD-LoRA）框架，协调LoRA因子的更新与混合以控制客户端间错位；理论分析其在非凸目标下的收敛性，刻画拓扑诱导交叉项误差与块坐标表示偏差间的权衡关系。

Result: 实验验证TAD-LoRA在多种通信条件下具有鲁棒性能：在强连通拓扑下保持竞争力，在中/弱连通拓扑下显著提升性能，尤其在MNLI数据集上效果突出。

Conclusion: TAD-LoRA有效缓解了DFL中LoRA微调受通信拓扑影响的问题，兼具理论保证与实际有效性，为去中心化低秩适配提供了新思路。

Abstract: Decentralized federated learning (DFL), a serverless variant of federated learning, poses unique challenges for parameter-efficient fine-tuning due to the factorized structure of low-rank adaptation (LoRA). Unlike linear parameters, decentralized aggregation of LoRA updates introduces topology-dependent cross terms that can destabilize training under dynamic communication graphs. We propose \texttt{TAD-LoRA}, a Topology-Aware Decentralized Low-Rank Adaptation framework that coordinates the updates and mixing of LoRA factors to control inter-client misalignment. We theoretically prove the convergence of \texttt{TAD-LoRA} under non-convex objectives, explicitly characterizing the trade-off between topology-induced cross-term error and block-coordinate representation bias governed by the switching interval of alternative training. Experiments under various communication conditions validate our analysis, showing that \texttt{TAD-LoRA} achieves robust performance across different communication scenarios, remaining competitive in strongly connected topologies and delivering clear gains under moderately and weakly connected topologies, with particularly strong results on the MNLI dataset.

</details>


### [571] [FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards](https://arxiv.org/abs/2602.00453)
*Ziyao Wang,Daeun Jung,Yexiao He,Guoheng Sun,Zheyu Shen,Myungjin Lee,Ang Li*

Main category: cs.LG

TL;DR: 本文提出FedMOA，一种面向异构奖励场景的联邦多目标对齐框架，基于无critic的GRPO算法，通过在线自适应加权与任务/精度感知聚合，在数学推理和代码生成任务上显著提升联邦学习下的模型性能、个性化能力与多目标平衡性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习对齐方法因需维护critic网络而难以应用于资源受限的端侧联邦学习；而GRPO虽免critic，但在联邦环境下面临异构奖励定义、多目标优化失衡及训练开销高等挑战。

Method: 提出FedMOA框架：本地端采用基于超梯度下降的在线自适应权重机制，动态优先主推理目标；服务器端采用任务和准确率感知的聚合策略，筛选高质量模型更新。

Result: 在数学推理与代码生成基准上，FedMOA相较联邦平均（FedAvg）最高提升准确率2.2%，同时增强全局性能、个性化能力和多目标平衡性。

Conclusion: FedMOA有效解决了联邦环境下多目标强化学习对齐的关键挑战，为端侧大模型个性化推理提供了可行、高效且鲁棒的训练范式。

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.

</details>


### [572] [LatentTrack: Sequential Weight Generation via Latent Filtering](https://arxiv.org/abs/2602.00458)
*Omer Haq*

Main category: cs.LG

TL;DR: 本文提出了LatentTrack（LT），一种用于非平稳动态下在线概率预测的序列神经架构，通过在低维潜在空间中执行因果贝叶斯滤波，并利用轻量级超网络生成每步预测模型参数，实现无需梯度更新的常数时间在线自适应。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳动态环境下在线概率预测的挑战，特别是传统方法依赖每步梯度更新、计算开销大且难以应对分布偏移的问题。

Method: 提出LatentTrack（LT）架构：在低维潜在空间中进行因果贝叶斯滤波；使用轻量级超网络实时生成预测模型参数；构建predict-generate-update滤波框架；支持结构化与非结构化潜在动力学；采用蒙特卡洛推断生成校准的预测混合分布。

Result: 在Jena Climate基准上的长时域在线回归任务中，LT在负对数似然和均方误差上持续优于状态化序列模型和静态不确定性感知基线，同时保持有竞争力的预测校准性。

Conclusion: 潜在条件下的函数演化是一种有效替代传统潜在状态建模的方法，尤其适用于存在分布偏移的场景。

Abstract: We introduce LatentTrack (LT), a sequential neural architecture for online probabilistic prediction under nonstationary dynamics. LT performs causal Bayesian filtering in a low-dimensional latent space and uses a lightweight hypernetwork to generate predictive model parameters at each time step, enabling constant-time online adaptation without per-step gradient updates.
  At each time step, a learned latent model predicts the next latent distribution, which is updated via amortized inference using new observations, yielding a predict--generate--update filtering framework in function space. The formulation supports both structured (Markovian) and unstructured latent dynamics within a unified objective, while Monte Carlo inference over latent trajectories produces calibrated predictive mixtures with fixed per-step cost. Evaluated on long-horizon online regression using the Jena Climate benchmark, LT consistently achieves lower negative log-likelihood and mean squared error than stateful sequential and static uncertainty-aware baselines, with competitive calibration, demonstrating that latent-conditioned function evolution is an effective alternative to traditional latent-state modeling under distribution shift.

</details>


### [573] [Search Inspired Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.00460)
*Georgios Sotirchos,Zlatan Ajanović,Jens Kober*

Main category: cs.LG

TL;DR: 本文提出SIERL方法，通过基于学习进展设置子目标来引导稀疏奖励环境中的探索，利用前沿状态的代价估计选择既不熟悉也不完全新颖的子目标，从而系统性扩展已知状态空间并提升任务完成率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境下探索困难，现有方法依赖手工启发式或易陷入次优策略。

Method: SIERL在每轮开始时从‘前沿’（已知状态空间边界）选取子目标，结合cost-to-come和cost-to-go估计进行优先级排序，引导智能体向信息量最大的区域探索。

Result: 在多个稀疏奖励环境中，SIERL在完成主任务和泛化至任意状态方面均优于主流基线方法。

Conclusion: SIERL通过受搜索启发的子目标选择机制，实现了更系统、高效且可泛化的探索，为稀疏奖励RL问题提供了新思路。

Abstract: Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.

</details>


### [574] [PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction](https://arxiv.org/abs/2602.00465)
*Jiaqi Yin,Baiming Chen,Jia Fei,Mingjun Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为PAIR-Former的新型模型，用于解决miRNA-mRNA靶向预测中的预算受限多实例学习（BR-MIL）问题，在计算预算约束下实现高效准确的靶点预测。


<details>
  <summary>Details</summary>
Motivation: miRNA-mRNA靶向预测面临每个转录本产生大量候选靶位点（CTS），但仅有成对标签可用的问题，传统方法难以在有限计算资源下兼顾精度与效率。

Method: 提出Budgeted Relational Multi-Instance Learning（BR-MIL）框架，并设计PAIR-Former模型：先进行低成本全池扫描，再在CPU上选择最多K个多样化的CTS，最后用排列不变的Set Transformer聚合所选token。

Result: 在miRAW数据集上，PAIR-Former在实用预算（K*=64）下显著优于强池化基线，并展现出可控的精度-计算权衡；理论分析表明其近似误差随K减小，泛化误差受K控制。

Conclusion: PAIR-Former为预算受限的生物序列关系建模提供了新范式，兼具实用性、可扩展性与理论保证。

Abstract: Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instances per bag may receive expensive encoding and relational processing under a hard compute budget. We propose \textbf{PAIR-Former} (Pool-Aware Instance-Relational Transformer), a BR-MIL pipeline that performs a cheap full-pool scan, selects up to $K$ diverse CTSs on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens. On miRAW, PAIR-Former outperforms strong pooling baselines at a practical operating budget ($K^\star{=}64$) while providing a controllable accuracy--compute trade-off as $K$ varies. We further provide theory linking budgeted selection to (i) approximation error decreasing with $K$ and (ii) generalization terms governed by $K$ in the expensive relational component.

</details>


### [575] [Parallel Stochastic Gradient-Based Planning for World Models](https://arxiv.org/abs/2602.00475)
*Michael Psenka,Michael Rabbat,Aditi Krishnapriyan,Yann LeCun,Amir Bar*

Main category: cs.LG

TL;DR: 本文提出了一种名为GRASP的新型梯度松弛随机规划器，利用可微世界模型进行高效并行优化，通过引入虚拟状态、软动力学约束和状态随机性来提升长时域视觉控制任务的规划性能，并在实验中超越CEM和GD等现有方法。


<details>
  <summary>Details</summary>
Motivation: 使用世界模型进行规划面临搜索空间庞大且无结构的挑战，尤其在长时域视觉控制任务中难以高效优化。

Method: 提出GRASP规划器：将状态视为优化变量（虚拟状态），施加软动力学约束；引入状态随机性以促进探索并避免局部最优；修改梯度结构，仅依赖动作输入梯度，缓解高维视觉世界模型中梯度敏感问题。

Result: 在基于视频的世界模型实验中，GRASP在长时域任务的成功率和收敛速度上均优于交叉熵法（CEM）和标准梯度下降（GD）。

Conclusion: GRASP是一种鲁棒、高度并行、可微的世界模型规划方法，兼具理论合理性与实际性能优势，为视觉驱动的长时域控制提供了新范式。

Abstract: World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables ("virtual states") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.

</details>


### [576] [Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly](https://arxiv.org/abs/2602.00476)
*Hengchang Liu,Zhao Yang,Bing Su*

Main category: cs.LG

TL;DR: 本文提出CAL方法，利用扩散语言模型（DLMs）在首次去噪中表现出的统计信号（Oracle Peak和Length Bias），无需训练即可自适应推断最优填充长度，显著提升代码与文本填充性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）虽天然适合填充任务，但其性能受限于预设填充长度；本文旨在挖掘DLMs自身隐含的长度推断能力，摆脱对固定长度或额外训练的依赖。

Method: 分析DLMs首次去噪置信度中的两个统计现象——靠近真实长度的局部'Oracle Peak'和干扰该信号的系统性'Length Bias'；提出无训练方法CAL，通过校准偏差并高效搜索，实现自适应长度选择。

Result: 在代码填充中Pass@1提升最高达47.7%（相比固定长度基线）和40.5%（相比chat-based自适应方法）；在文本填充中BLEU-2和ROUGE-L分别提升最多8.5%和9.9%。

Conclusion: CAL无需任何模型微调或再训练，即可显著增强DLMs在各类填充任务中的鲁棒性与性能，为通用扩散语言模型的实际应用提供了新路径。

Abstract: Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \textit{Oracle Peak} that emerges near the ground-truth length and a systematic \textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \textbf{CAL} (\textbf{C}alibrated \textbf{A}daptive \textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\% over fixed-length baselines and 40.5\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\% and 9.9\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.

</details>


### [577] [Quality-Diversity Optimization as Multi-Objective Optimization](https://arxiv.org/abs/2602.00478)
*Xi Lin,Ping Guo,Yilu Liu,Qingfu Zhang,Jianyong Sun*

Main category: cs.LG

TL;DR: 本文将质量-多样性（QD）优化问题重新建模为具有大量目标的多目标优化（MOO）问题，从而可直接利用成熟的MOO方法（特别是基于集合的标量化技术）求解，并在理论上继承MOO保证、实践中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有QD算法设计多样但缺乏与成熟优化范式（如MOO）的统一联系；希望通过建立QD与MOO的理论桥梁，复用MOO理论与方法提升QD求解效率与可靠性。

Method: 将QD问题形式化为具有大量目标的MOO问题，采用基于集合的标量化技术进行协同搜索，并提供理论分析证明其继承MOO收敛性与Pareto最优性等保证。

Result: 在多个QD基准任务上，所提方法性能与当前最先进QD算法相当，同时具备理论可解释性与方法通用性。

Conclusion: QD优化可被严谨地纳入MOO框架，该新视角不仅拓展了QD的理论基础，也为设计更高效、鲁棒的QD算法提供了新路径。

Abstract: The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.

</details>


### [578] [AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.00482)
*Jiarui Zhang,Yuchen Yang,Ran Yan,Zhiyu Mei,Liyuan Zhang,Daifeng Li,Wei Fu,Jiaxuan Gao,Shusheng Xu,Yi Wu,Binhang Yuan*

Main category: cs.LG

TL;DR: 本文提出AREAL-DTA，一种基于DFS遍历和动态分布式批处理的高效RL训练方法，显著提升大语言模型后训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的大语言模型后训练因重复计算共享前缀导致计算与内存开销巨大。

Method: 提出AREAL-DTA，采用DFS策略动态遍历 rollout 前缀树，每次仅物化一条根到叶路径，并结合负载均衡的分布式批处理机制跨GPU构建和处理前缀树。

Result: 在主流RL后训练任务上，AREAL-DTA在τ²-bench评测中实现最高8.31×的训练吞吐量提升。

Conclusion: AREAL-DTA通过高效利用前缀共享结构，显著缓解了RL训练中的计算冗余问题，提升了可扩展性与效率。

Abstract: Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\times$ in $τ^2$-bench higher training throughput.

</details>


### [579] [OD-DEAL: Dynamic Expert-Guided Adversarial Learning with Online Decomposition for Scalable Capacitated Vehicle Routing](https://arxiv.org/abs/2602.00488)
*Dongbin Jiao,Zisheng Chen,Xianyi Wang,Jintao Shi,Shengcai Liu,Shi Yan*

Main category: cs.LG

TL;DR: OD-DEAL是一种结合混合遗传搜索与在线重心聚类分解的对抗学习框架，通过高保真知识蒸馏将专家启发式策略转化为稠密代理奖励，实现无需聚类的大规模CVRP实时求解。


<details>
  <summary>Details</summary>
Motivation: 大规模带容量车辆路径问题（CVRP）求解受限于传统启发式算法复杂度高、神经求解器在超大规模图上泛化能力差。

Method: 提出OD-DEAL框架：融合混合遗传搜索（HGS）与在线重心聚类分解（BCC），构建基于图注意力网络（GAT）的生成式策略，并通过极小极大博弈进行高保真知识蒸馏，将专家启发式行为编码为稠密代理奖励。

Result: 在万节点级CVRP实例上达到SOTA实时性能，神经推理时间接近常数级缩放，支持亚秒级、启发式质量的动态大规模部署。

Conclusion: OD-DEAL有效弥合了启发式方法与神经求解器之间的鸿沟，实现了高质量、可扩展、免聚类的大规模CVRP端到端求解。

Abstract: Solving large-scale capacitated vehicle routing problems (CVRP) is hindered by the high complexity of heuristics and the limited generalization of neural solvers on massive graphs. We propose OD-DEAL, an adversarial learning framework that tightly integrates hybrid genetic search (HGS) and online barycenter clustering (BCC) decomposition, and leverages high-fidelity knowledge distillation to transfer expert heuristic behavior. OD-DEAL trains a graph attention network (GAT)-based generative policy through a minimax game, in which divide-and-conquer strategies from a hybrid expert are distilled into dense surrogate rewards. This enables high-quality, clustering-free inference on large-scale instances. Empirical results demonstrate that OD-DEAL achieves state-of-the-art (SOTA) real-time CVRP performance, solving 10000-node instances with near-constant neural scaling. This uniquely enables the sub-second, heuristic-quality inference required for dynamic large-scale deployment.

</details>


### [580] [Partition of Unity Neural Networks for Interpretable Classification with Explicit Class Regions](https://arxiv.org/abs/2602.00511)
*Akram Aldroubi*

Main category: cs.LG

TL;DR: 本文提出了一种名为Partition of Unity Neural Networks (PUNN)的新架构，通过学习一组和为1的非负函数直接建模类别概率，无需softmax层，从而提升模型可解释性；理论证明其在连续概率映射空间中稠密，实验表明其精度与传统MLP相当，且引入几何先验时参数量可大幅减少。


<details>
  <summary>Details</summary>
Motivation: 神经网络分类器虽效果好但难以解释，尤其softmax模型中类别区域由logits间的隐式不等式系统定义，不易提取和可视化。

Method: 提出PUNN架构，用k个非负函数h_i(x)构成单位分解（∑h_i(x)=1），每个h_i(x)直接表示P(class i|x)；支持多种门控函数（如sigmoid、高斯、bump）及参数化形式（MLP或几何先验驱动的结构如球壳、椭球、球谐函数）。

Result: 在合成数据、UCI基准和MNIST上实验显示：MLP门控PUNN精度仅比标准MLP低0.3–0.6%；当几何先验匹配数据结构时，形状引导门控可实现相当精度且参数减少达300倍。

Conclusion: PUNN是一种可解释性优先的设计，兼具理论保证与实际性能，在保持竞争力的同时提供透明的概率输出。

Abstract: Despite their empirical success, neural network classifiers remain difficult to interpret. In softmax-based models, class regions are defined implicitly as solutions to systems of inequalities among logits, making them difficult to extract and visualize. We introduce Partition of Unity Neural Networks (PUNN), an architecture in which class probabilities arise directly from a learned partition of unity, without requiring a softmax layer.
  PUNN constructs $k$ nonnegative functions $h_1, \ldots, h_k$ satisfying $\sum_i h_i(x) = 1$, where each $h_i(x)$ directly represents $P(\text{class } i \mid x)$. Unlike softmax, where class regions are defined implicitly through coupled inequalities among logits, each PUNN partition function $h_i$ directly defines the probability of class $i$ as a standalone function of $x$.
  We prove that PUNN is dense in the space of continuous probability maps on compact domains. The gate functions $g_i$ that define the partition can use various activation functions (sigmoid, Gaussian, bump) and parameterizations ranging from flexible MLPs to parameter-efficient shape-informed designs (spherical shells, ellipsoids, spherical harmonics).
  Experiments on synthetic data, UCI benchmarks, and MNIST show that PUNN with MLP-based gates achieves accuracy within 0.3--0.6\% of standard multilayer perceptrons. When geometric priors match the data structure, shape-informed gates achieve comparable accuracy with up to 300$\times$ fewer parameters. These results demonstrate that interpretable-by-design architectures can be competitive with black-box models while providing transparent class probability assignments.

</details>


### [581] [Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs](https://arxiv.org/abs/2602.00513)
*Md Tanvirul Alam,Aritran Piplai,Ionut Cardei,Nidhi Rastogi,Peter J Worth*

Main category: cs.LG

TL;DR: 本文提出了一种基于可验证奖励的强化学习（RLVR）方法，用于提升网络威胁情报（CTI）任务中大语言模型生成结构化输出的准确性与鲁棒性，并引入了统一数据集Minerva和轻量级自训练机制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在CTI结构化输出任务中表现脆弱，且多依赖监督微调（SFT）；而CTI标准本身具备可验证性，为引入强化学习提供了基础。

Method: 提出基于可验证奖励的强化学习（RLVR）框架，构建统一数据集Minerva及配套任务专用验证器，并设计轻量级自训练机制以缓解奖励稀疏问题。

Result: 在多个LLM主干模型和基准测试上，该方法在准确性和鲁棒性方面均持续优于监督微调（SFT）。

Conclusion: 利用CTI标准的可验证性进行强化学习是提升LLM结构化输出质量的有效路径，RLVR与自训练机制具有泛化潜力。

Abstract: Cyber threat intelligence (CTI) analysts routinely convert noisy, unstructured security artifacts into standardized, automation-ready representations. Although large language models (LLMs) show promise for this task, existing approaches remain brittle when producing structured CTI outputs and have largely relied on supervised fine-tuning (SFT). In contrast, CTI standards and community-maintained resources define canonical identifiers and schemas that enable deterministic verification of model outputs. We leverage this structure to study reinforcement learning with verifiable rewards (RLVR) for CTI tasks. We introduce \textit{Minerva}, a unified dataset and training pipeline spanning multiple CTI subtasks, each paired with task-specific verifiers that score structured outputs and identifier predictions. To address reward sparsity during rollout, we propose a lightweight self-training mechanism that generates additional verified trajectories and distills them back into the model. Experiments across LLM backbones show consistent improvements in accuracy and robustness over SFT across multiple benchmarks.

</details>


### [582] [Contrastive Learning for Privacy Enhancements in Industrial Internet of Things](https://arxiv.org/abs/2602.00515)
*Lin Liu,Rita Machacy,Simi Kuniyilh*

Main category: cs.LG

TL;DR: This paper provides a comprehensive review of contrastive learning-based privacy-preserving techniques tailored for the Industrial Internet of Things (IIoT), highlighting industrial data characteristics, system architectures, applications, challenges, and future research directions.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of IIoT introduces significant privacy and confidentiality risks due to the sensitivity of operational data, necessitating privacy-preserving analytics approaches.

Method: The paper conducts a comprehensive literature review focused on contrastive learning techniques applied to privacy preservation in IIoT systems, analyzing industrial data features, architectures, and application scenarios.

Result: A structured overview of existing contrastive learning-based privacy-preserving methods in IIoT, along with identification of key challenges and open research problems.

Conclusion: Contrastive learning holds promise for privacy-preserving analytics in IIoT, but domain-specific adaptations and further research are needed to address unique industrial requirements and unresolved challenges.

Abstract: The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems, such as those in manufacturing and energy, it also introduces significant privacy and confidentiality risks due to the sensitivity of operational data. Contrastive learning, a self-supervised representation learning paradigm, has recently emerged as a promising approach for privacy-preserving analytics by reducing reliance on labeled data and raw data sharing. Although contrastive learning-based privacy-preserving techniques have been explored in the Internet of Things (IoT) domain, this paper offers a comprehensive review of these techniques specifically for privacy preservation in Industrial Internet of Things (IIoT) systems. It emphasizes the unique characteristics of industrial data, system architectures, and various application scenarios. Additionally, the paper discusses solutions and open challenges and outlines future research directions.

</details>


### [583] [NEST: Nested Event Stream Transformer for Sequences of Multisets](https://arxiv.org/abs/2602.00520)
*Minghui Sun,Haoyu Gong,Xingyu You,Jillian Hurst,Benjamin Goldstein,Matthew Engelhard*

Main category: cs.LG

TL;DR: 本文提出Nested Event Stream Transformer (NEST)模型及Masked Set Modeling (MSM)预训练范式，专为具有层次结构（序列化的多重集）的事件流数据设计，通过保留原始层级结构提升计算效率与表征质量。


<details>
  <summary>Details</summary>
Motivation: 现有事件流基础模型将层次化事件（如EHR中的临床就诊序列）扁平化为一维序列，导致计算低效、学习虚假关系及集合级表征质量差。

Method: 提出NEST架构，显式建模事件流的序列-多重集层次结构；并设计Masked Set Modeling (MSM)预训练任务，以增强集合级表征学习。

Result: 在真实多集序列数据上实验表明，NEST能更好捕捉现实动态，在预训练效率和下游任务性能上均优于基线方法。

Conclusion: 保留事件流原始层次结构是一种有效的归纳偏置，NEST与MSM为层次化事件建模提供了更高效、更鲁棒的基础模型框架。

Abstract: Event stream data often exhibit hierarchical structure in which multiple events co-occur, resulting in a sequence of multisets (i.e., bags of events). In electronic health records (EHRs), for example, medical events are grouped into a sequence of clinical encounters with well-defined temporal structure, but the order and timing of events within each encounter may be unknown or unreliable. Most existing foundation models (FMs) for event stream data flatten this hierarchy into a one-dimensional sequence, leading to (i) computational inefficiency associated with dense attention and learning spurious within-set relationships, and (ii) lower-quality set-level representations from heuristic post-training pooling for downstream tasks. Here, we show that preserving the original hierarchy in the FM architecture provides a useful inductive bias that improves both computational efficiency and representation quality. We then introduce Nested Event Stream Transformer (NEST), a FM for event streams comprised of sequences of multisets. Building on this architecture, we formulate Masked Set Modeling (MSM), an efficient paradigm that promotes improved set-level representation learning. Experiments on real-world multiset sequence data show that NEST captures real-world dynamics while improving both pretraining efficiency and downstream performance.

</details>


### [584] [PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning](https://arxiv.org/abs/2602.01156)
*Shunpeng Yang,Ben Liu,Hua Chen*

Main category: cs.LG

TL;DR: 本文提出了PolicyFlow，一种基于连续归一化流（CNF）的新型策略优化算法，通过速度场变化近似重要性比率，避免了全流路径上的似然计算，同时引入受布朗运动启发的Brownian正则器以增强策略多样性。


<details>
  <summary>Details</summary>
Motivation: 标准PPO在使用高容量策略模型（如连续归一化流CNF）时面临似然评估计算昂贵且数值不稳定的问题，亟需一种兼顾表达力与训练稳定性的新方法。

Method: 提出PolicyFlow算法：1）用插值路径上速度场变化近似重要性比率，规避全流路径似然计算；2）引入Brownian Regularizer作为隐式策略熵正则项，提升行为多样性。

Result: 在MultiGoal、PointMaze、IsaacLab和MuJoCo Playground等多个环境实验表明，PolicyFlow性能优于或媲美高斯策略PPO及现有流式基线（FPO、DPPO），尤其在MultiGoal任务中展现出更强的多模态动作建模能力。

Conclusion: PolicyFlow成功将CNF策略融入PPO框架，在保持训练稳定性的同时显著提升策略表达能力与行为多样性，为高表达力策略学习提供了高效可行的新范式。

Abstract: Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.

</details>


### [585] [Physiology as Language: Translating Respiration to Sleep EEG](https://arxiv.org/abs/2602.00526)
*Kaiwen Zha,Chao Li,Hao He,Peng Cao,Tianhong Li,Ali Mirzazadeh,Ellen Zhang,Jong Woo Lee,Yoon Kim,Dina Katabi*

Main category: cs.LG

TL;DR: 本文提出了一种新型跨生理信号翻译任务：从呼吸信号合成睡眠脑电图（EEG），通过波形条件生成框架结合离散标记化，在大规模数据上实现了高质量EEG重建，并支持多种下游任务，甚至可扩展至无线射频反射等无接触传感模态。


<details>
  <summary>Details</summary>
Motivation: 解决呼吸信号与脑电图（EEG）之间显著的模态复杂性差异，探索无需直接接触即可远程评估睡眠神经活动的可能性。

Method: 提出波形条件生成框架，利用离散标记化约束EEG目标空间，同时保留细粒度呼吸动力学特征；在超28,000人数据集上训练模型。

Result: EEG谱图重建平均绝对误差为7%；在年龄估计（MAE 5.0 vs. 5.1）、性别检测（AUROC 0.81 vs. 0.82）和睡眠分期（准确率0.84 vs. 0.88）等下游任务中性能接近真实EEG，且显著优于基于呼吸信号的基线方法；成功扩展至无线射频反射输入。

Conclusion: 该框架不仅实现了高保真跨模态生理信号合成，还验证了无接触、远程睡眠神经评估的可行性，为非侵入式健康监测开辟新路径。

Abstract: This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.

</details>


### [586] [Convergent World Representations and Divergent Tasks](https://arxiv.org/abs/2602.00533)
*Core Francisco Park*

Main category: cs.LG

TL;DR: 本文通过控制实验研究了多任务训练对神经表示几何结构的影响，发现多任务训练能促使世界表征收敛，但某些‘发散性’任务会损害新实体的表征整合与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 神经表征的几何结构及其对下游适应性的影响尚不明确，需在可控设置下分离世界、数据生成过程与模型表征以深入探究。

Method: 构建一个由5075个城市坐标定义的‘世界’，并设计7个几何任务生成自回归训练数据；通过单任务与多任务训练对比分析表征几何，并在多任务预训练后对新城市进行微调以测试表征可扩展性。

Result: 不同任务导致显著不同的表征几何；多任务训练促使非重叠任务的模型发展出对齐的几何表征；但部分‘发散性’任务会严重损害新实体在表征空间中的整合与泛化性能。

Conclusion: 多任务训练可可靠产生收敛的世界表征，但存在隐性的‘发散性’任务，会在微调阶段对新实体集成造成灾难性影响。

Abstract: While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model representations to study these questions in a controlled setup. 5,075 city coordinates define the world and 7 geometric tasks generate the training data for autoregressive training. We find that different tasks give rise to qualitatively and quantitatively distinct world representation geometries. However, multi-task training drives convergence of world representations: models trained on non-overlapping tasks develop aligned geometric representations, providing controlled evidence for the Multitask Scaling Hypothesis of the Platonic Representation Hypothesis. To study adaptation, we pretrain models on all tasks, then test whether new entities (cities) can be consistently integrated into the representation space via fine-tuning. Surprisingly, we find that despite multi-task pretraining, some tasks, which we call divergent, actively harm the representational integration of new entities and harm generalization. Our results show that training on multiple relational tasks reliably produces convergent world representations, but lurking divergent tasks can catastrophically harm new entity integration via fine-tuning.

</details>


### [587] [AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models](https://arxiv.org/abs/2602.00534)
*Apurba Prasad Padhy,Fernando Camacho,Saibal Mukhopadhyay*

Main category: cs.LG

TL;DR: 本文提出AIRE-Prune方法，一种基于渐近脉冲响应能量的结构化后训练剪枝技术，用于降低状态空间模型（SSM）的状态维度，在保持精度的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: SSM常为降低内存与计算成本而牺牲容量、搜索空间或稳定性；需在不损害性能前提下有效压缩大状态维度。

Method: 提出AIRE-Prune：为每个状态分配基于无限时间范围内脉冲响应能量的闭式渐近能量得分，并进行层内归一化以支持跨层全局比较与剪枝选择，将模态截断思想扩展至深度SSM堆叠。

Result: 在多个序列基准上，对SISO/MIMO SSM平均剪枝率达60.8%，无重训练时平均精度仅下降0.29%，同时显著降低计算量。

Conclusion: AIRE-Prune能高效识别并移除SSM中的冗余状态，兼顾稳定性、精度与效率，为SSM轻量化提供了新范式。

Abstract: State space models (SSMs) often sacrifice capacity, search space, or stability to offset the memory and compute costs of large state dimensions. We introduce a structured post-training pruning method for SSMs -- AIRE-Prune (Asymptotic Impulse-Response Energy for State PRUN(E)) -- that reduces each layer's state dimension by directly minimizing long-run output-energy distortion. AIRE-Prune assigns every state a closed-form asymptotic impulse-response energy-based score, i.e., the total impulse-response energy it contributes over an infinite horizon (time), and normalizes these scores layer-wise to enable global cross-layer comparison and selection. This extends modal truncation from single systems to deep stacks and aligns pruning with asymptotic response energy rather than worst-case gain. Across diverse sequence benchmarks, AIRE-Prune reveals substantial redundancy in SISO and MIMO SSMs with average pruning of 60.8%, with average accuracy drop of 0.29% without retraining, while significantly lowering compute. Code: https://github.com/falcon-arrow/AIRE-Prune.

</details>


### [588] [AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments](https://arxiv.org/abs/2602.01629)
*Renukanandan Tumu,Aditya Singh,Rahul Mangharam*

Main category: cs.LG

TL;DR: 本文提出AdaptNC框架，通过联合在线自适应调整非一致性分数参数和共形阈值，以应对机器人环境中分布偏移带来的挑战，在保持目标覆盖率的同时显著减小预测区域体积。


<details>
  <summary>Details</summary>
Motivation: 标准共形预测（CP）依赖于交换性假设，但在现实机器人系统中因分布偏移而被破坏；现有在线CP方法仅自适应调整阈值，使用固定非一致性分数函数，导致结构变化环境下的预测区域过于保守且体积低效。

Method: 提出AdaptNC框架，联合在线优化非一致性分数函数参数与共形阈值：采用自适应重加权机制优化分数函数，并引入回放缓冲区缓解分数切换期间的覆盖率不稳定问题。

Result: 在多智能体策略变更、环境变化和传感器退化等多样化机器人基准上验证，AdaptNC相比仅调阈值的SOTA方法显著减小预测区域体积，同时严格维持目标覆盖率。

Conclusion: 联合自适应非一致性分数与阈值是提升共形预测在动态机器人环境中有效性与效率的关键路径，AdaptNC为此提供了实用且鲁棒的解决方案。

Abstract: Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.

</details>


### [589] [Invertible Memory Flow Networks](https://arxiv.org/abs/2602.00535)
*Liyu Zerihun,Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: 本文提出了一种名为可逆记忆流网络（IMFN）的新方法，通过二叉树结构的'sweeper'模块对长序列进行逐对压缩，解决了RNN梯度消失和Transformer二次复杂度的问题，并实现了对高维长序列数据的有效压缩。


<details>
  <summary>Details</summary>
Motivation: 长序列神经记忆存在挑战：RNN存在梯度消失问题，Transformer存在二次计算复杂度问题，且将长序列压缩为固定长度表示因优化困难而难以实现。

Method: 提出可逆记忆流网络（IMFN），将长序列压缩分解为二叉树结构的成对合并任务，每个'sweeper'模块仅学习2-to-1压缩；在线推理时蒸馏为常数成本的循环学生模型。

Result: 在长MNIST序列和UCF-101视频数据上验证了IMFN的有效性，成功实现了高维长序列数据的压缩。

Conclusion: IMFN通过因子化策略使长序列压缩变得可行，具有O(log N)深度和次线性误差累积，并支持O(1)在线推理。

Abstract: Long sequence neural memory remains a challenging problem. RNNs and their variants suffer from vanishing gradients, and Transformers suffer from quadratic scaling. Furthermore, compressing long sequences into a finite fixed representation remains an intractable problem due to the difficult optimization landscape. Invertible Memory Flow Networks (IMFN) make long sequence compression tractable through factorization: instead of learning end-to-end compression, we decompose the problem into pairwise merges using a binary tree of "sweeper" modules. Rather than learning to compress long sequences, each sweeper learns a much simpler 2-to-1 compression task, achieving O(log N) depth with sublinear error accumulation in sequence length. For online inference, we distilled into a constant-cost recurrent student achieving O(1) sequential steps. Empirical results validate IMFN on long MNIST sequences and UCF-101 videos, demonstrating compression of high-dimensional data over long sequences.

</details>


### [590] [OpenDDI: A Comprehensive Benchmark for DDI Prediction](https://arxiv.org/abs/2602.00539)
*Xinmo Jin,Bowen Fan,Xunkai Li,Henan Sun,YuXin Zeng,Zekai Chen,Yuxuan Sun,Jia Li,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出OpenDDI，一个用于药物-药物相互作用（DDI）预测的综合性基准，旨在解决现有研究中数据质量低、评估标准不统一两大问题。


<details>
  <summary>Details</summary>
Motivation: 现有DDI预测研究受限于小规模数据集、单模态药物表征以及缺乏标准化评估协议。

Method: 构建OpenDDI基准：整合6个主流DDI数据集与2种药物表征方式，并新增3个大尺度LLM增强数据集及覆盖5种模态的新型多模态药物表征；统一20个SOTA模型在3个下游任务上的评估协议，涵盖数据质量、有效性、泛化性、鲁棒性和效率。

Result: 基于OpenDDI完成全面评估，获得10条关键洞见，揭示当前方法局限性，并为该领域发展提供重要指导。

Conclusion: OpenDDI显著提升了DDI预测研究的数据基础与评估规范性，推动该领域向更可靠、可复现和可扩展方向发展。

Abstract: Drug-Drug Interactions (DDIs) significantly influence therapeutic efficacy and patient safety. As experimental discovery is resource-intensive and time-consuming, efficient computational methodologies have become essential. The predominant paradigm formulates DDI prediction as a drug graph-based link prediction task. However, further progress is hindered by two fundamental challenges: (1) lack of high-quality data: most studies rely on small-scale DDI datasets and single-modal drug representations; (2) lack of standardized evaluation: inconsistent scenarios, varied metrics, and diverse baselines. To address the above issues, we propose OpenDDI, a comprehensive benchmark for DDI prediction. Specifically, (1) from the data perspective, OpenDDI unifies 6 widely used DDI datasets and 2 existing forms of drug representation, while additionally contributing 3 new large-scale LLM-augmented datasets and a new multimodal drug representation covering 5 modalities. (2) From the evaluation perspective, OpenDDI unifies 20 SOTA model baselines across 3 downstream tasks, with standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency. Based on OpenDDI, we conduct a comprehensive evaluation and derive 10 valuable insights for DDI prediction while exposing current limitations to provide critical guidance for this rapidly evolving field. Our code is available at https://github.com/xiaoriwuguang/OpenDDI

</details>


### [591] [One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models](https://arxiv.org/abs/2602.00541)
*Zilin Jing,Vincent Jeanselme,Yuta Kobayashi,Simon A. Lee,Chao Pang,Aparajita Kashyap,Yanwei Li,Xinzhuo Jiang,Shalmali Joshi*

Main category: cs.LG

TL;DR: 本文提出ORA预训练目标，联合建模电子健康记录（EHR）中事件发生时间与关联的连续测量值，显著提升模型在分类、回归及时间到事件预测等下游任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于next-token预测的EHR基础模型未能充分捕捉EHR数据的不规则时序结构及离散事件与连续数值测量共存的本质特性。

Method: 提出ORA（marked time-to-event）预训练目标，将事件类型（mark）、发生时间（time）和关联的连续测量值（如检验值、剂量）统一建模为标记化时间到事件过程，并在多个EHR数据集上进行预训练与微调验证。

Result: ORA在多个数据集、下游任务（包括分类、回归、时间到事件预测）及不同模型架构上均优于next-token预测及其他忽略连续测量的预训练方法，展现出更强的泛化性与更广的任务适配能力。

Conclusion: 考虑EHR内在结构（尤其是时间与连续测量）的预训练目标对构建更强大、更通用的EHR基础模型至关重要，是拓展下游能力的关键方向。

Abstract: Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability

</details>


### [592] [Depth, Not Data: An Analysis of Hessian Spectral Bifurcation](https://arxiv.org/abs/2602.00545)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Yaoqing Yang*

Main category: cs.LG

TL;DR: 本文挑战了先前关于深度神经网络Hessian矩阵特征值'块-峰'结构源于数据协方差不平衡的观点，证明该结构可仅由网络架构（如深度线性网络）引发，且谱隙随网络深度线性增长。


<details>
  <summary>Details</summary>
Motivation: 先前研究将Hessian矩阵的'块-峰'谱结构归因于数据协方差不平衡，本文旨在验证该结构是否可由网络架构本身独立引起。

Method: 通过分析深度线性网络设定，在数据协方差完全平衡的前提下，理论证明Hessian矩阵仍呈现主导簇与块簇的双峰谱结构，并推导主导与块簇特征值比值随网络深度线性变化。

Result: 即使数据协方差平衡，深度线性网络的Hessian仍呈现双峰谱结构；主导与块簇特征值比值随网络深度线性增长，表明谱隙主要受架构而非数据分布影响。

Conclusion: 深度神经网络的优化景观不仅受数据特性影响，更显著受模型架构（尤其是深度）调控；优化算法设计需同时考虑架构与数据两方面因素。

Abstract: The eigenvalue distribution of the Hessian matrix plays a crucial role in understanding the optimization landscape of deep neural networks. Prior work has attributed the well-documented ``bulk-and-spike'' spectral structure, where a few dominant eigenvalues are separated from a bulk of smaller ones, to the imbalance in the data covariance matrix. In this work, we challenge this view by demonstrating that such spectral Bifurcation can arise purely from the network architecture, independent of data imbalance.
  Specifically, we analyze a deep linear network setup and prove that, even when the data covariance is perfectly balanced, the Hessian still exhibits a Bifurcation eigenvalue structure: a dominant cluster and a bulk cluster. Crucially, we establish that the ratio between dominant and bulk eigenvalues scales linearly with the network depth. This reveals that the spectral gap is strongly affected by the network architecture rather than solely by data distribution. Our results suggest that both model architecture and data characteristics should be considered when designing optimization algorithms for deep networks.

</details>


### [593] [Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry](https://arxiv.org/abs/2602.00547)
*Seunghyun Yoo,Sanghong Kim,Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 本文提出一种跨模态对齐框架，将质谱数据直接映射到预训练化学语言模型的分子结构嵌入空间，显著提升对未见分子骨架的零样本识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法将质谱匹配视为封闭集识别任务，难以泛化至未见分子骨架，存在语义鸿沟问题。

Method: 构建跨模态对齐框架，将质谱映射到预训练化学语言模型的分子结构嵌入空间。

Result: 在严格支架不相交基准上，固定256类零样本检索Top-1准确率达42.2%；全局检索下表现优异；5类5样本分子重识别准确率达95.4%。

Conclusion: 显式融合物理谱图解析与分子结构嵌入是解决质谱分子识别泛化瓶颈的关键。

Abstract: Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability to generalize to unseen molecular scaffolds. To overcome this limitation, we propose a cross-modal alignment framework that directly maps mass spectra into the chemically meaningful molecular structure embedding space of a pretrained chemical language model. On a strict scaffold-disjoint benchmark, our model achieves a Top-1 accuracy of 42.2% in fixed 256-way zero-shot retrieval and demonstrates strong generalization under a global retrieval setting. Moreover, the learned embedding space demonstrates strong chemical coherence, reaching 95.4% accuracy in 5-way 5-shot molecular re-identification. These results suggest that explicitly integrating physical spectral resolution with molecular structure embedding is key to solving the generalization bottleneck in molecular identification from MS data.

</details>


### [594] [Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design](https://arxiv.org/abs/2602.00549)
*Kezhao Lai,Yutao Lai,Hai-Lin Liu*

Main category: cs.LG

TL;DR: 本文提出Clade-AHD框架，通过在分支（clade）层面建模贝叶斯信念（Beta分布）并结合Thompson采样，缓解MCTS在LLM自动启发式设计中因计算预算受限导致的过早收敛问题，显著提升探索效率与优化性能。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛树搜索（MCTS）在大语言模型驱动的自动启发式设计（AHD）中存在有限计算预算下过度开发（over-exploitation）的问题，难以在稀疏、噪声大的启发式评估中可靠探索。

Method: 提出Clade-AHD：以子树（clade）为单位聚合后代评估结果，构建Beta分布表征不确定性；在clade层面执行Thompson采样，替代传统MCTS的节点级点估计，从而显式建模和利用不确定性指导探索。

Result: 在多个复杂组合优化问题上，Clade-AHD持续优于现有最先进方法，同时大幅降低计算开销。

Conclusion: Clade-AHD通过将不确定性建模从节点提升至clade层级，有效平衡探索与利用，为资源受限下的LLM-AHD提供了更鲁棒、高效的搜索范式。

Abstract: While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.

</details>


### [595] [Forget by Uncertainty: Orthogonal Entropy Unlearning for Quantized Neural Networks](https://arxiv.org/abs/2602.00567)
*Tian Zhang,Yujia Tong,Junhao Dong,Ke Xu,Yuze Wang,Jingling Yuan*

Main category: cs.LG

TL;DR: 本文提出OEU框架，通过熵引导的遗忘和梯度正交投影，在量化神经网络中实现真正的遗忘与效用保持。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络在边缘设备上的部署与GDPR等隐私法规催生了对量化模型机器遗忘的迫切需求，但现有方法存在将遗忘与错误记忆混淆、标量梯度重加权无法解决方向冲突等问题。

Method: 提出正交熵遗忘（OEU）框架：1）熵引导遗忘，最大化遗忘数据上的预测不确定性；2）梯度正交投影，将遗忘梯度投影到保留梯度的正交补空间，理论上保障效用保留。

Result: 大量实验表明，OEU在遗忘有效性与保留准确率上均优于现有方法。

Conclusion: OEU实现了真正遗忘而非自信误判，并通过理论保证在遗忘过程中维持模型效用，为量化模型的机器遗忘提供了新范式。

Abstract: The deployment of quantized neural networks on edge devices, combined with privacy regulations like GDPR, creates an urgent need for machine unlearning in quantized models. However, existing methods face critical challenges: they induce forgetting by training models to memorize incorrect labels, conflating forgetting with misremembering, and employ scalar gradient reweighting that cannot resolve directional conflicts between gradients. We propose OEU, a novel Orthogonal Entropy Unlearning framework with two key innovations: 1) Entropy-guided unlearning maximizes prediction uncertainty on forgotten data, achieving genuine forgetting rather than confident misprediction, and 2) Gradient orthogonal projection eliminates interference by projecting forgetting gradients onto the orthogonal complement of retain gradients, providing theoretical guarantees for utility preservation under first-order approximation. Extensive experiments demonstrate that OEU outperforms existing methods in both forgetting effectiveness and retain accuracy.

</details>


### [596] [When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning](https://arxiv.org/abs/2602.00573)
*Zheng Zhang,Tao Hu,Xueheng Li,Yang Wang,Rui Li,Jie Zhang,Chengjun Xie*

Main category: cs.LG

TL;DR: 本文提出Stage-CIL新范式，应对类内形态演化（如幼虫变蝴蝶）带来的增量学习挑战，并设计Stage-Bench基准与STAGE方法，在保持类间判别能力的同时建模类内演化规律。


<details>
  <summary>Details</summary>
Motivation: 传统类增量学习假设类别形态静态，忽视了同一语义类在不同发育阶段的显著形态变化（即类内演化），导致模型难以兼顾类间区分与类内形态适应。

Method: 提出Stage-Aware CIL（Stage-CIL）范式，构建含10个领域、2个阶段的Stage-Bench数据集与评测协议；并设计STAGE方法，在固定大小记忆池中显式学习可迁移的抽象演化模式，解耦语义身份与形态变换动力学。

Result: STAGE在Stage-Bench上显著超越现有SOTA方法，同时缓解类间遗忘与类内遗忘。

Conclusion: 类增量学习需扩展至支持类内形态演化建模，Stage-CIL范式及STAGE方法为该方向提供了系统性框架与有效解决方案。

Abstract: Class-Incremental Learning (CIL) aims to sequentially learn new classes while mitigating catastrophic forgetting of previously learned knowledge. Conventional CIL approaches implicitly assume that classes are morphologically static, focusing primarily on preserving previously learned representations as new classes are introduced. However, this assumption neglects intra-class evolution: a phenomenon wherein instances of the same semantic class undergo significant morphological transformations, such as a larva turning into a butterfly. Consequently, a model must both discriminate between classes and adapt to evolving appearances within a single class. To systematically address this challenge, we formalize Stage-Aware CIL (Stage-CIL), a paradigm in which each class is learned progressively through distinct morphological stages. To facilitate rigorous evaluation within this paradigm, we introduce the Stage-Bench, a 10-domain, 2-stages dataset and protocol that jointly measure inter- and intra-class forgetting. We further propose STAGE, a novel method that explicitly learns abstract and transferable evolution patterns within a fixed-size memory pool. By decoupling semantic identity from transformation dynamics, STAGE enables accurate prediction of future morphologies based on earlier representations. Extensive empirical evaluation demonstrates that STAGE consistently and substantially outperforms existing state-of-the-art approaches, highlighting its effectiveness in simultaneously addressing inter-class discrimination and intra-class morphological adaptation.

</details>


### [597] [Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs](https://arxiv.org/abs/2602.00576)
*Tushaar Gangavarapu,Jiping Li,Christopher Vattheuer,Zhangyang Wang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 本文通过理论分析发现，Sharpness-Aware Minimization（SAM）比梯度下降（GD）具有更低的简洁性偏差（SB），这是其泛化性能更优的关键原因；受此启发，作者提出通过调整训练数据分布（如对后期学习样本进行上采样或增强）来降低SB，从而提升大语言模型在数学推理任务上的泛化能力，并在多个LLM上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索能否通过修改训练数据分布来引导优化器获得更好泛化性能，特别是针对SAM虽泛化好但计算昂贵、难以用于中等规模LLM的问题。

Method: 理论分析多头线性自注意力下的上下文线性回归模型，比较GD与SAM的训练动力学；提出基于简洁性偏差（SB）洞察的数据重分布策略（如上采样后期习得样本或数据增强）；在多个LLM（Phi2、Llama3.2、Gemma3、Qwen3）上用AdamW和Muon进行微调实验。

Result: 证实SAM能降低简洁性偏差（SB），且所提数据分布调整策略可类似地降低SB；在数学推理任务上，多个LLM获得最高达18%的相对准确率提升。

Conclusion: 简洁性偏差是影响优化器泛化性能的关键因素；无需昂贵SAM，仅通过调控训练数据分布即可有效提升LLM泛化能力，为高效优化提供了新思路。

Abstract: Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamics of two gradient based optimizers, namely gradient descent (GD) and sharpness-aware minimization (SAM), the latter exhibiting superior generalization properties but is prohibitively expensive for training even medium-sized LLMs. We show, for the first time, that SAM induces a lower simplicity bias (SB)-the tendency of an optimizer to preferentially learn simpler features earlier in training-and identify this reduction as a key factor underlying its improved generalization performance. Motivated by this insight, we demonstrate that altering the training data distribution by upsampling or augmenting examples learned later in training similarly reduces SB and leads to improved generalization. Our extensive experiments show that our strategy improves the performance of multiple LLMs-including Phi2-2.7B , Llama3.2-1B, Gemma3-1B-PT, and Qwen3-0.6B-Base-achieving relative accuracy gains up to 18% when fine-tuned with AdamW and Muon on mathematical reasoning tasks.

</details>


### [598] [Sparsity-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2602.00577)
*Yuze Wang,Yujia Tong,Ke Xu,Jingling Yuan,Jiawei Jiang,Chuang Hu*

Main category: cs.LG

TL;DR: 本文提出了一种面向稀疏大语言模型的机器遗忘方法SAU，通过梯度掩码和重要性感知重分配，在不损害模型效用的前提下提升遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法针对稠密模型设计，忽视了模型稀疏化对遗忘能力的负面影响，导致在稀疏大语言模型上遗忘效果显著下降。

Method: 提出稀疏感知遗忘（SAU）方法：采用梯度掩码将更新导向保留参数，并结合重要性感知重分配补偿被剪枝参数的影响，使遗忘与稀疏化目标解耦。

Result: 在稀疏大语言模型上，SAU显著优于现有遗忘方法，实现了有效遗忘并保持模型效用。

Conclusion: SAU为稀疏大语言模型提供了高效、实用的机器遗忘方案，兼顾隐私保护与部署效率。

Abstract: Large Language Models (LLMs) inevitably memorize sensitive information during training, posing significant privacy risks. Machine unlearning has emerged as a promising solution to selectively remove such information without full retraining. However, existing methods are designed for dense models and overlook model sparsification-an essential technique for efficient LLM deployment. We find that unlearning effectiveness degrades substantially on sparse models. Through empirical analysis, we reveal that this degradation occurs because existing unlearning methods require updating all parameters, yet sparsification prunes substantial weights to zero, fundamentally limiting the model's forgetting capacity. To address this challenge, we propose Sparsity-Aware Unlearning (SAU), which decouples unlearning from sparsification objectives through gradient masking that redirects updates to surviving weights, combined with importance-aware redistribution to compensate for pruned parameters. Extensive experiments demonstrate that SAU significantly outperforms existing methods on sparse LLMs, achieving effective forgetting while preserving model utility.

</details>


### [599] [Bridging Time and Frequency: A Joint Modeling Framework for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00582)
*Xiangfei Qiu,Kangjia Yan,Xvyuan Liu,Xingjian Wu,Jilin Hu*

Main category: cs.LG

TL;DR: 本文提出TFMixer框架，通过联合建模时域与频域特征来解决非均匀采样和异步多变量时间序列预测问题，其中引入可学习的非均匀离散傅里叶变换（NUDFT）提取频谱表示，并设计基于查询的局部时间块混合机制以缓解信息密度不均衡，最终融合双域表征并利用逆NUDFT实现显式季节外推。


<details>
  <summary>Details</summary>
Motivation: 非均匀采样和变量异步性破坏了标准模型对等距采样的假设，导致局部时序建模困难且传统频域方法难以捕捉全局周期结构。

Method: 提出TFMixer框架：1）全局频域模块采用可学习的非均匀离散傅里叶变换（NUDFT）直接从非规则时间戳中提取频谱表示；2）局部时域模块设计查询驱动的块混合机制，自适应聚合时序片段以缓解信息密度不平衡；3）融合时域与频域表征进行预测，并利用逆NUDFT实现显式季节外推。

Result: 在多个真实世界数据集上实验表明，TFMixer达到当前最优性能。

Conclusion: TFMixer通过联合建模时间与频率维度，有效应对IMTSF中的非均匀性和异步性挑战，为不规则时间序列预测提供了新范式。

Abstract: Irregular multivariate time series forecasting (IMTSF) is challenging due to non-uniform sampling and variable asynchronicity. These irregularities violate the equidistant assumptions of standard models, hindering local temporal modeling and rendering classical frequency-domain methods ineffective for capturing global periodic structures. To address this challenge, we propose TFMixer, a joint time-frequency modeling framework for IMTS forecasting. Specifically, TFMixer incorporates a Global Frequency Module that employs a learnable Non-Uniform Discrete Fourier Transform (NUDFT) to directly extract spectral representations from irregular timestamps. In parallel, the Local Time Module introduces a query-based patch mixing mechanism to adaptively aggregate informative temporal patches and alleviate information density imbalance. Finally, TFMixer fuses the time-domain and frequency-domain representations to generate forecasts and further leverages inverse NUDFT for explicit seasonal extrapolation. Extensive experiments on real-world datasets demonstrate the state--of-the-art performance of TFMixer.

</details>


### [600] [Safe Langevin Soft Actor Critic](https://arxiv.org/abs/2602.00587)
*Mahesh Keswani,Samyak Jain,Raunak P. Bhattacharyya*

Main category: cs.LG

TL;DR: 本文提出Safe Langevin Soft Actor-Critic (SL-SAC)，通过参数空间探索与分布风险控制，解决约束强化学习中奖励与安全性平衡的难题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在约束强化学习中存在价值函数尖锐极小值导致泛化差、以及对重尾风险分布处理不足的问题。

Method: SL-SAC融合三项关键技术：(1) 面向奖励critic的自适应随机梯度Langevin动力学（aSGLD）以提升集成多样性并逃离劣质极值；(2) 基于隐式分位数网络（IQN）与条件风险价值（CVaR）优化的分布式成本估计，用于尾部风险抑制；(3) 基于经验CVaR的反应式Lagrangian松弛机制，动态调整约束强度。

Result: 在Safety-Gymnasium基准上，SL-SAC在10项任务中7项取得最低成本，同时保持有竞争力的回报；在速度类任务中成本较SOTA基线降低19–63%。

Conclusion: SL-SAC通过结合参数空间探索与分布鲁棒的CVaR优化，在理论保障与实证性能上均显著提升了约束强化学习的安全性与稳定性。

Abstract: Balancing reward and safety in constrained reinforcement learning remains challenging due to poor generalization from sharp value minima and inadequate handling of heavy-tailed risk distribution. We introduce Safe Langevin Soft Actor-Critic (SL-SAC), a principled algorithm that addresses both issues through parameter-space exploration and distributional risk control. Our approach combines three key mechanisms: (1) Adaptive Stochastic Gradient Langevin Dynamics (aSGLD) for reward critics, promoting ensemble diversity and escape from poor optima; (2) distributional cost estimation via Implicit Quantile Networks (IQN) with Conditional Value-at-Risk (CVaR) optimization for tail-risk mitigation; and (3) a reactive Lagrangian relaxation scheme that adapts constraint enforcement based on the empirical CVaR of episodic costs. We provide theoretical guarantees on CVaR estimation error and demonstrate that CVaR-based Lagrange updates yield stronger constraint violation signals than expected-cost updates. On Safety-Gymnasium benchmarks, SL-SAC achieves the lowest cost in 7 out of 10 tasks while maintaining competitive returns, with cost reductions of 19-63% in velocity tasks compared to state-of-the-art baselines.

</details>


### [601] [SEER: Transformer-based Robust Time Series Forecasting via Automated Patch Enhancement and Replacement](https://arxiv.org/abs/2602.00589)
*Xiangfei Qiu,Xvyuan Liu,Tianen Shen,Xingjian Wu,Hanyin Cheng,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: 本文提出了一种名为SEER的鲁棒时间序列预测框架，通过增强嵌入模块和可学习补丁替换模块，动态过滤低质量补丁并用全局序列级表征替代，从而提升预测精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于补丁的时间序列方法无法动态选择补丁，且在存在缺失值、分布偏移、异常点和白噪声等低质量数据时表现不佳，影响预测效果。

Method: 提出SEER框架：1）增强嵌入模块采用MoE架构和通道自适应感知机制生成更优的补丁级与序列级表征；2）可学习补丁替换模块通过两阶段机制（动态过滤负向补丁+因果注意力驱动的全局token替换）提升鲁棒性。

Result: 在多个基准数据集上取得SOTA性能，验证了SEER在鲁棒性和预测精度上的优势。

Conclusion: SEER有效解决了低质量补丁干扰问题，为时间序列预测提供了一种动态、鲁棒且高性能的新范式。

Abstract: Time series forecasting is important in many fields that require accurate predictions for decision-making. Patching techniques, commonly used and effective in time series modeling, help capture temporal dependencies by dividing the data into patches. However, existing patch-based methods fail to dynamically select patches and typically use all patches during the prediction process. In real-world time series, there are often low-quality issues during data collection, such as missing values, distribution shifts, anomalies and white noise, which may cause some patches to contain low-quality information, negatively impacting the prediction results. To address this issue, this study proposes a robust time series forecasting framework called SEER. Firstly, we propose an Augmented Embedding Module, which improves patch-wise representations using a Mixture-of-Experts (MoE) architecture and obtains series-wise token representations through a channel-adaptive perception mechanism. Secondly, we introduce a Learnable Patch Replacement Module, which enhances forecasting robustness and model accuracy through a two-stage process: 1) a dynamic filtering mechanism eliminates negative patch-wise tokens; 2) a replaced attention module substitutes the identified low-quality patches with global series-wise token, further refining their representations through a causal attention mechanism. Comprehensive experimental results demonstrate the SOTA performance of SEER.

</details>


### [602] [Kernelized Edge Attention: Addressing Semantic Attention Blurring in Temporal Graph Neural Networks](https://arxiv.org/abs/2602.00596)
*Govind Waghmare,Srini Rohan Gujulla Leel,Nikhil Tumbde,Sumedh B G,Sonia Gupta,Srikanta Bedathur*

Main category: cs.LG

TL;DR: 本文提出KEAT（Kernelized Edge Attention for Temporal Graphs），一种新型时间图神经网络注意力机制，通过连续时间核（如Laplacian、RBF及可学习MLP）对边特征进行调制，明确区分节点（慢变）与边（快变）的时间语义，提升时序建模精度与可解释性，在链路预测任务中显著超越DyGFormer和TGN。


<details>
  <summary>Details</summary>
Motivation: 现有TGNNs在注意力计算中混用节点与边表示，忽视二者在时间演化上的本质差异（节点状态缓慢变化，边交互瞬时发生），导致语义注意力模糊、细粒度时序依赖建模能力弱且缺乏可解释性。

Method: 提出KEAT注意力机制，采用连续时间核函数对边特征进行时间感知调制，保持节点与边表征的语义分离；该机制可即插即用地集成到Transformer式（如DyGFormer）和消息传递式（如TGN）架构中。

Result: 在链路预测任务上，KEAT相较DyGFormer提升最高18% MRR，相较TGN提升7% MRR，并增强了模型的时间感知能力和注意力可解释性。

Conclusion: KEAT通过解耦并精细化建模节点与边的时间动态特性，有效缓解了语义注意力模糊问题，为构建更准确、可解释的时序图神经网络提供了新范式。

Abstract: Temporal Graph Neural Networks (TGNNs) aim to capture the evolving structure and timing of interactions in dynamic graphs. Although many models incorporate time through encodings or architectural design, they often compute attention over entangled node and edge representations, failing to reflect their distinct temporal behaviors. Node embeddings evolve slowly as they aggregate long-term structural context, while edge features reflect transient, timestamped interactions (e.g. messages, trades, or transactions). This mismatch results in semantic attention blurring, where attention weights cannot distinguish between slowly drifting node states and rapidly changing, information-rich edge interactions. As a result, models struggle to capture fine-grained temporal dependencies and provide limited transparency into how temporal relevance is computed. This paper introduces KEAT (Kernelized Edge Attention for Temporal Graphs), a novel attention formulation that modulates edge features using a family of continuous-time kernels, including Laplacian, RBF, and learnable MLP variant. KEAT preserves the distinct roles of nodes and edges, and integrates seamlessly with both Transformer-style (e.g., DyGFormer) and message-passing (e.g., TGN) architectures. It achieves up to 18% MRR improvement over the recent DyGFormer and 7% over TGN on link prediction tasks, enabling more accurate, interpretable and temporally aware message passing in TGNNs.

</details>


### [603] [Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains](https://arxiv.org/abs/2602.00603)
*Luca Viano,Ruida Zhou,Yifan Sun,Mahdi Namazifar,Volkan Cevher,Shoham Sabach,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: 本文提出了一种利用评分差距（rating gap）信息改进直接偏好优化（DPO）的新算法，相比DPO具有更快的统计收敛速率，并在评分差距不准确时仍保持鲁棒性，实验验证其在多个大语言模型和基准上优于现有DPO类方法。


<details>
  <summary>Details</summary>
Motivation: DPO仅使用二元偏好反馈，虽简化数据收集，但反馈模糊、难以解释模型输出质量变化；引入更丰富的评分差距信息有望提升优化效率与可解释性。

Method: 设计了能利用评分差距信息的新偏好优化算法，理论分析其统计收敛性，并证明其对评分差距噪声的鲁棒性；结合实证验证在多种LLM和基准上的有效性。

Result: 新算法在有准确评分差距时比DPO获得更快统计速率；理论与实验均表明其对评分差距误差具有鲁棒性；在多个LLM和评测基准上显著优于DPO等基线方法。

Conclusion: 评分差距是一种有价值且实用的额外监督信号，可有效增强偏好优化算法的性能与鲁棒性，为对齐学习提供了新思路。

Abstract: The class of direct preference optimization (DPO) algorithms has emerged as a promising approach for solving the alignment problem in foundation models. These algorithms work with very limited feedback in the form of pairwise preferences and fine-tune models to align with these preferences without explicitly learning a reward model. While the form of feedback used by these algorithms makes the data collection process easy and relatively more accurate, its ambiguity in terms of the quality of responses could have negative implications. For example, it is not clear if a decrease (increase) in the likelihood of preferred (dispreferred) responses during the execution of these algorithms could be interpreted as a positive or negative phenomenon. In this paper, we study how to design algorithms that can leverage additional information in the form of rating gap, which informs the learner how much the chosen response is better than the rejected one. We present new algorithms that can achieve faster statistical rates than DPO in presence of accurate rating gap information. Moreover, we theoretically prove and empirically show that the performance of our algorithms is robust to inaccuracy in rating gaps. Finally, we demonstrate the solid performance of our methods in comparison to a number of DPO-style algorithms across a wide range of LLMs and evaluation benchmarks.

</details>


### [604] [Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games](https://arxiv.org/abs/2602.00606)
*Ahmed Said Donmez,Yuksel Arslantas,Muhammed O. Sayin*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的、独立且仅依赖收益的随机博弈学习框架，具有无模型、无游戏先验、无梯度等特点；采用基于快慢双评论家的类最优响应执行器-评论器架构，并通过平滑最优响应实现非均衡适应；理论上保证在二人零和及多智能体同利益随机博弈中收敛至（近似）均衡；实验证明其鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有随机博弈学习算法往往依赖模型信息、特定游戏结构或梯度计算，缺乏在完全去中心化、仅观测收益条件下的理论保障。

Method: 设计一种双时间尺度的actor-critic架构：快速评论家基于实时观测收益进行低信息量响应，慢速评论家渐进逼近动态规划解；策略更新采用平滑最优响应机制，不依赖均衡假设或梯度。

Result: 在无限时域下，证明该算法在二人零和与多智能体同利益随机博弈中均收敛到（近似）纳什均衡；是首批在两类博弈中同时具备收益驱动、完全去中心化及理论收敛保证的学习算法之一。

Conclusion: 所提框架突破了传统学习算法对模型、梯度或均衡初始化的依赖，为实际分布式多智能体系统提供了一种鲁棒、可证明有效且易于部署的解决方案。

Abstract: We propose a novel independent and payoff-based learning framework for stochastic games that is model-free, game-agnostic, and gradient-free. The learning dynamics follow a best-response-type actor-critic architecture, where agents update their strategies (actors) using feedback from two distinct critics: a fast critic that intuitively responds to observed payoffs under limited information, and a slow critic that deliberatively approximates the solution to the underlying dynamic programming problem. Crucially, the learning process relies on non-equilibrium adaptation through smoothed best responses to observed payoffs. We establish convergence to (approximate) equilibria in two-agent zero-sum and multi-agent identical-interest stochastic games over an infinite horizon. This provides one of the first payoff-based and fully decentralized learning algorithms with theoretical guarantees in both settings. Empirical results further validate the robustness and effectiveness of the proposed approach across both classes of games.

</details>


### [605] [Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference](https://arxiv.org/abs/2602.00620)
*Juntao Fang,Shifeng Xie,Shengbin Nie,Yuhui Ling,Yuming Liu,Zijian Li,Keli Zhang,Lujia Pan,Themis Palpanas,Ruichu Cai*

Main category: cs.LG

TL;DR: 本文提出TIC-FM框架，通过上下文学习实现时间序列零样本分类，无需微调或训练分类器，避免评估偏差，并在128个UCR数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本时间序列分类方法依赖冻结编码器加任务特定分类器，违背了零样本“免训练”前提，并因分类器训练选择引入评估偏差。

Method: 提出TIC-FM：将标注训练集作为上下文输入，结合时间序列编码器、轻量投影适配器与分块掩码潜在记忆Transformer，在单次前向传播中完成全部测试样本预测；并从理论上证明上下文推理可模拟梯度式分类器训练。

Result: 在128个UCR数据集上实验表明，TIC-FM具备强准确性，尤其在极低标签场景下表现稳定且持续提升。

Conclusion: TIC-FM实现了真正免训练的零样本时间序列分类，消除了分类器引入的评估偏差，为TSFM零样本评估提供了新范式。

Abstract: The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent training choices. To address this issue, we propose TIC-FM, an in-context learning framework that treats the labeled training set as context and predicts labels for all test instances in a single forward pass, without parameter updates. TIC-FM pairs a time series encoder and a lightweight projection adapter with a split-masked latent memory Transformer. We further provide theoretical justification that in-context inference can subsume trained classifiers and can emulate gradient-based classifier training within a single forward pass. Experiments on 128 UCR datasets show strong accuracy, with consistent gains in the extreme low-label situation, highlighting training-free transfer

</details>


### [606] [MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting](https://arxiv.org/abs/2602.00624)
*Hyekyung Yoon,Minhyuk Lee,Imseung Park,Myungjoo Kang*

Main category: cs.LG

TL;DR: 本文提出了一种新的多变量长期时间序列预测框架MoDEx，通过引入层敏感度分析发现不同网络层对时间动态建模具有深度特异性，并据此设计了轻量级的深度特异性专家混合模型，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有长期时间序列预测方法的骨干网络各层行为未被充分探索，缺乏对层间功能差异的理解。

Method: 提出基于梯度的层敏感度指标（受GradCAM和有效感受野理论启发），分析MLP骨干网络各层对输入时间点的正/负贡献；发现深度特异性现象；据此设计轻量级的深度特异性专家混合模型MoDEx。

Result: MoDEx在7个真实世界基准上达到SOTA精度，78%情况下排名第一，参数量与计算资源显著减少；可无缝集成到Transformer变体中并持续提升其性能。

Conclusion: 深度特异性是骨干网络建模时间动态的重要特性，MoDEx作为一种高效且高性能的LTSF框架，兼具轻量化、高精度与强泛化能力。

Abstract: Multivariate long-term time series forecasting (LTSF) supports critical applications such as traffic-flow management, solar-power scheduling, and electricity-transformer monitoring. The existing LTSF paradigms follow a three-stage pipeline of embedding, backbone refinement, and long-horizon prediction. However, the behaviors of individual backbone layers remain underexplored. We introduce layer sensitivity, a gradient-based metric inspired by GradCAM and effective receptive field theory, which quantifies both positive and negative contributions of each time point to a layer's latent features. Applying this metric to a three-layer MLP backbone reveals depth-specific specialization in modeling temporal dynamics in the input sequence. Motivated by these insights, we propose MoDEx, a lightweight Mixture of Depth-specific Experts, which replaces complex backbones with depth-specific MLP experts. MoDEx achieves state-of-the-art accuracy on seven real-world benchmarks, ranking first in 78 percent of cases, while using significantly fewer parameters and computational resources. It also integrates seamlessly into transformer variants, consistently boosting their performance and demonstrating robust generalizability as an efficient and high-performance LTSF framework.

</details>


### [607] [From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs](https://arxiv.org/abs/2602.00628)
*Louis Schiekiera,Max Zimmer,Christophe Roux,Sebastian Pokutta,Fritz Günther*

Main category: cs.LG

TL;DR: 本文研究了大语言模型（LLM）的隐藏状态几何结构能否通过心理语言学实验行为数据恢复，发现强制选择任务的行为相似性比自由联想更贴近隐藏状态几何，并能预测未见词对的隐藏状态相似性，表明行为数据蕴含可恢复的内部语义结构信息。


<details>
  <summary>Details</summary>
Motivation: 探究LLM的隐藏状态几何是否可通过心理语言学实验行为数据进行有效恢复，以理解行为任务能否揭示模型内部语义表征。

Method: 在8个指令微调Transformer模型上开展相似性强制选择和自由联想两类心理语言学实验，覆盖5000词词汇，收集超1750万次试验构建行为相似性矩阵；采用表征相似性分析（RSA）将其与各层隐藏状态相似性、FastText、BERT及跨模型共识对比。

Result: 强制选择行为与隐藏状态几何一致性显著高于自由联想；在留出词回归任务中，行为相似性（尤其是强制选择）能超越词法基线和跨模型共识，预测未见词对的隐藏状态相似性。

Conclusion: 心理语言学行为数据（特别是强制选择）保留了关于LLM内部语义几何结构的可恢复信息，支持行为任务作为探测模型内部认知状态的有效手段。

Abstract: We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.

</details>


### [608] [Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration](https://arxiv.org/abs/2602.00636)
*Yujie Yang,Zhilong Zheng,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 本文首次提出安全探索的平衡概念，即在可行区域与环境模型之间寻找平衡点，并设计了首个面向平衡的安全探索框架SEE，通过交替优化最大可行区域和最小不确定性模型，实现了零约束违反的安全探索。


<details>
  <summary>Details</summary>
Motivation: 确保环境探索的安全性是强化学习中的关键问题，但如何确定最大可行探索区域以及如何识别该区域仍待解决。

Method: 提出了一种面向平衡的安全探索框架（SEE），通过交替寻找最大可行区域和最小不确定性环境模型来实现；使用图模型表示不确定性环境，并证明了SEE得到的不确定性模型单调改进、可行区域单调扩展，最终收敛到安全探索的平衡点。

Result: 在经典控制任务上的实验表明，该算法能在零约束违反的前提下成功扩展可行区域，并在几次迭代内达到安全探索的平衡状态。

Conclusion: 安全探索的目标是在可行区域与环境模型之间达到动态平衡，二者相互促进；SEE框架首次形式化并实现了这一平衡目标。

Abstract: Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.

</details>


### [609] [Combinatorial Bandit Bayesian Optimization for Tensor Outputs](https://arxiv.org/abs/2602.00640)
*Jingru Huang,Haijie Xu,Jie Guo,Manrui Jiang,Chen Zhang*

Main category: cs.LG

TL;DR: 本文提出了针对张量输出函数的贝叶斯优化方法（TOGP）及扩展的组合带臂贝叶斯优化（CBBO）框架，包含新核函数、UCB类采集函数、CMAB-UCB2准则，并给出理论遗憾界与实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法未处理张量输出函数，且缺乏对部分观测与组合选择场景（如仅部分输出参与目标计算）的支持。

Method: 提出张量输出高斯过程（TOGP）作为代理模型，设计两类张量核；构建UCB采集函数；进一步拓展至组合带臂贝叶斯优化（CBBO），引入CMAB-UCB2准则以联合选择查询点与最优输出子集。

Result: 建立了两种方法的理论遗憾上界（次线性），并在合成数据和真实任务上验证了其性能优于基线方法。

Conclusion: 所提TOGP与CBBO方法有效建模张量结构依赖、支持部分观测与组合决策，在理论与实验上均展现出优越性，拓展了贝叶斯优化的应用边界。

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and black-box functions across various domains. Existing BO methods have not addressed tensor-output functions. To fill this gap, we propose a novel tensor-output BO method. Specifically, we first introduce a tensor-output Gaussian process (TOGP) with two classes of tensor-output kernels as a surrogate model of the tensor-output function, which can effectively capture the structural dependencies within the tensor. Based on it, we develop an upper confidence bound (UCB) acquisition function to select the queried points. Furthermore, we introduce a more complex and practical problem setting, named combinatorial bandit Bayesian optimization (CBBO), where only a subset of the outputs can be selected to contribute to the objective function. To tackle this, we propose a tensor-output CBBO method, which extends TOGP to handle partially observed outputs, and accordingly design a novel combinatorial multi-arm bandit-UCB2 (CMAB-UCB2) criterion to sequentially select both the queried points and the optimal output subset. Theoretical regret bounds for the two methods are established, ensuring their sublinear performance. Extensive synthetic and real-world experiments demonstrate their superiority.

</details>


### [610] [CoRe-Fed: Bridging Collaborative and Representation Fairness via Federated Embedding Distillation](https://arxiv.org/abs/2602.00647)
*Noorain Mukhtiar,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 本文提出CoRe-Fed框架，通过嵌入层正则化与公平感知聚合，同时缓解联邦学习中的表征偏差与协作偏差，提升模型公平性与性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习因数据异构与参与不均导致客户端间性能差异，引发表征偏差（客户端表征不一致）和协作偏差（聚合中贡献不公）两大公平性挑战。

Method: 提出CoRe-Fed统一优化框架：1）对齐驱动机制增强本地与全局嵌入的语义一致性，缓解表征偏差；2）基于参与历史与嵌入对齐程度的动态奖惩聚合策略，实现贡献感知的公平聚合。

Result: 在多种模型与数据集上的实验表明，CoRe-Fed相较现有最优基线算法，在公平性和模型性能两方面均有提升。

Conclusion: CoRe-Fed有效桥接协作公平与表征公平，为解决联邦学习中的系统性不公平问题提供了新范式。

Abstract: With the proliferation of distributed data sources, Federated Learning (FL) has emerged as a key approach to enable collaborative intelligence through decentralized model training while preserving data privacy. However, conventional FL algorithms often suffer from performance disparities across clients caused by heterogeneous data distributions and unequal participation, which leads to unfair outcomes. Specifically, we focus on two core fairness challenges, i.e., representation bias, arising from misaligned client representations, and collaborative bias, stemming from inequitable contribution during aggregation, both of which degrade model performance and generalizability. To mitigate these disparities, we propose CoRe-Fed, a unified optimization framework that bridges collaborative and representation fairness via embedding-level regularization and fairness-aware aggregation. Initially, an alignment-driven mechanism promotes semantic consistency between local and global embeddings to reduce representational divergence. Subsequently, a dynamic reward-penalty-based aggregation strategy adjusts each client's weight based on participation history and embedding alignment to ensure contribution-aware aggregation. Extensive experiments across diverse models and datasets demonstrate that CoRe-Fed improves both fairness and model performance over the state-of-the-art baseline algorithms.

</details>


### [611] [PHAT: Modeling Period Heterogeneity for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00654)
*Jiaming Ma,Guanjun Wang,Qihe Huang,Sheng Huang,Haofeng Ma,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 本文提出PHAT模型，通过三维'周期桶'张量和正负注意力机制，有效建模多变量时间序列中变异性周期异质性，在14个真实数据集上显著优于18种基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列预测模型虽能建模周期性，但忽略了现实数据中变量间存在的动态变化、各不相同的周期（即周期异质性）。

Method: 提出PHAT（Period Heterogeneity-Aware Transformer）：① 将输入组织为三维'周期桶'张量（按周期相似性分组、相位对齐的时间步、周期内偏移）；② 桶内交互+桶间掩码以避免不同周期干扰；③ 设计正负注意力机制，分别建模周期对齐与偏差，并用周期先验调制正负注意力得分。

Result: 在14个真实世界数据集上全面评估，对比18种基线，PHAT显著提升预测性能，达到高度竞争力水平。

Conclusion: PHAT通过显式建模周期异质性，提升了多元时间序列预测的准确性与鲁棒性，其结构设计（周期桶+正负注意力+先验调制）具有理论支撑与实践有效性。

Abstract: While existing multivariate time series forecasting models have advanced significantly in modeling periodicity, they largely neglect the periodic heterogeneity common in real-world data, where variates exhibit distinct and dynamically changing periods. To effectively capture this periodic heterogeneity, we propose PHAT (Period Heterogeneity-Aware Transformer). Specifically, PHAT arranges multivariate inputs into a three-dimensional "periodic bucket" tensor, where the dimensions correspond to variate group characteristics with similar periodicity, time steps aligned by phase, and offsets within the period. By restricting interactions within buckets and masking cross-bucket connections, PHAT effectively avoids interference from inconsistent periods. We also propose a positive-negative attention mechanism, which captures periodic dependencies from two perspectives: periodic alignment and periodic deviation. Additionally, the periodic alignment attention scores are decomposed into positive and negative components, with a modulation term encoding periodic priors. This modulation constrains the attention mechanism to more faithfully reflect the underlying periodic trends. A mathematical explanation is provided to support this property. We evaluate PHAT comprehensively on 14 real-world datasets against 18 baselines, and the results show that it significantly outperforms existing methods, achieving highly competitive forecasting performance. Our sources is available at GitHub.

</details>


### [612] [Riemannian Flow Matching for Disentangled Graph Domain Adaptation](https://arxiv.org/abs/2602.00656)
*Yingxu Wang,Xinwang Liu,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: 本文提出DisRFM框架，通过黎曼流形嵌入与基于流的传输统一解决图域自适应中的结构退化和优化不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有图域自适应方法在欧氏空间中使用对抗学习对齐图嵌入，存在结构退化（层次与语义表征纠缠）和优化不稳定（对抗训练振荡）两大问题。

Method: 1）将图嵌入到黎曼流形中，利用极坐标显式解耦结构（半径）与语义（角度）；2）通过径向Wasserstein对齐保持拓扑结构，角度聚类增强语义判别；3）采用黎曼流匹配替代对抗训练，沿测地线平滑引导源域特征迁移，并在传输中维持解耦结构。

Result: 理论证明了流匹配的渐近稳定性，并推导出更紧的目标风险界；实验表明DisRFM在多个基准上持续优于现有最先进方法。

Conclusion: DisRFM通过几何感知建模有效缓解结构退化与优化不稳定，为图域自适应提供了更鲁棒、可解释的新范式。

Abstract: Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.

</details>


### [613] [Three-Way Emotion Classification of EEG-based Signals using Machine Learning](https://arxiv.org/abs/2602.00670)
*Ashna Purwar,Gaurav Simkar,Madhumita,Sachin Kadam*

Main category: cs.LG

TL;DR: 本文探讨了使用机器学习模型（逻辑回归、支持向量机和随机森林）对EEG信号进行三类情绪（负面、中性、正面）分类的效果，结果表明随机森林模型在准确率和F1分数上表现最优，并优于现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 情绪感知系统和基于EEG的情绪识别是新兴研究方向，需探索适合小规模EEG数据集的高效机器学习分类方法。

Method: 采用逻辑回归（LR）、支持向量机（SVM）和随机森林（RF）三种模型，对预处理后的EEG信号进行三分类训练与测试，并以准确率和F1-score为指标比较性能。

Result: 随机森林模型在准确率和F1-score上均优于LR和SVM，且其准确率超过当前最先进模型。

Conclusion: 随机森林是该EEG情绪三分类任务中最有效的机器学习模型，验证了ML在小规模EEG情绪识别中的可行性与优越性。

Abstract: Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.

</details>


### [614] [Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian Process Conditional Density Estimators for TSAD](https://arxiv.org/abs/2602.00672)
*Aleksandr Yugay,Hang Cui,Changhua Pei,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 本文提出了一种基于普通最小二乘（OLS）回归的简单线性自回归异常评分方法，在时序异常检测任务中性能媲美甚至超越现有深度模型，同时计算开销显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有研究过度关注复杂、难训练、高推理成本的神经网络架构，忽视了简单线性模型的潜力。

Method: 采用具有闭式解的普通最小二乘（OLS）回归构建线性自回归异常评分模型。

Result: 在广泛的单变量和多变量基准测试中，该方法在精度上优于或匹配SOTA深度检测器，且计算资源需求低数个数量级。

Conclusion: 未来研究应始终包含强线性基线，并设计具有更丰富时间结构的新基准，以真正凸显深度学习模型的优势。

Abstract: Research in time series anomaly detection (TSAD) has largely focused on developing increasingly sophisticated, hard-to-train, and expensive-to-infer neural architectures. We revisit this paradigm and show that a simple linear autoregressive anomaly score with the closed-form solution provided by ordinary least squares (OLS) regression consistently matches or outperforms state-of-the-art deep detectors. From a theoretical perspective, we show that linear models capture a broad class of anomaly types, estimating a finite-history Gaussian process conditional density. From a practical side, across extensive univariate and multivariate benchmarks, the proposed approach achieves superior accuracy while requiring orders of magnitude fewer computational resources. Thus, future research should consistently include strong linear baselines and, more importantly, develop new benchmarks with richer temporal structures pinpointing the advantages of deep learning models.

</details>


### [615] [Provably Protecting Fine-Tuned LLMs from Training Data Extraction](https://arxiv.org/abs/2602.00688)
*Tom Segal,Asaf Shabtai,Yuval Elovici*

Main category: cs.LG

TL;DR: 本文提出SCP-Δr算法，通过平滑细调过程中低影响的token级概率偏移，在保障模型效用的同时，显著提升对训练数据提取攻击的防御能力，并提供形式化隐私保证。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法缺乏形式化隐私保证或导致显著效用下降；作者观察到细调仅需保留少量高影响力token偏差，其余可被平滑而不影响性能。

Method: 提出基于近似访问自由（NAF）的SCP-Δr算法，作用于相对概率，利用基础模型显式平滑低影响token的概率偏移。

Result: 理论界比现有NAF方法提升数量级，实证显示对训练数据提取攻击具有强防护能力且性能损失极小。

Conclusion: SCP-Δr在隐私保护与模型效用间取得更好平衡，为LLM细调中的数据隐私问题提供了高效且有理论保障的新方案。

Abstract: Fine-tuning large language models (LLMs) on sensitive datasets raises privacy concerns, as training data extraction (TDE) attacks can expose highly confidential information. Existing defenses against such attacks either lack formal privacy guarantees or incur substantial utility degradation. We observe that fine-tuning induces widespread probability shifts, yet preserving only a small subset of influential token-level deviations is sufficient; the remaining shifts can be aggressively smoothed with minimal impact on utility. Motivated by this insight, we propose SCP-$Δ_r$, a Near Access Freeness (NAF)-based algorithm that operates on relative probabilities and explicitly smooths low-impact tokens using a base model. SCP-$Δ_r$ achieves orders-of-magnitude better theoretical bounds than existing NAF based methods and provides strong empirical protection against TDE attacks with minimal performance loss.

</details>


### [616] [Topology and Geometry of the Learning Space of ReLU Networks: Connectivity and Singularities](https://arxiv.org/abs/2602.00693)
*Marco Nurisso,Pierrick Leroy,Giovanni Petri,Francesco Vaccarino*

Main category: cs.LG

TL;DR: 本文研究了前馈ReLU网络参数空间的性质，特别是其连通性和奇点存在性，揭示了这些性质与网络DAG拓扑结构及子网络间的深刻联系，并建立了与可微剪枝的理论关联。


<details>
  <summary>Details</summary>
Motivation: 理解前馈ReLU网络参数空间的性质对分析和指导训练动力学至关重要，尤其是初始化后梯度流如何限制参数空间到代数簇。

Method: 通过分析基于有向无环图（DAG）架构的前馈ReLU网络，系统刻画参数空间的连通性，引入瓶颈节点和平衡条件，并研究奇点与DAG拓扑及其诱导子网络的关系。

Result: 给出了参数空间连通性的完整刻画，明确了瓶颈节点和平衡条件的作用；证明奇点与DAG拓扑结构密切相关；讨论了奇点的可达性，并建立了其与可微剪枝的原理性联系。

Conclusion: ReLU网络参数空间的几何性质（如连通性与奇点）由网络拓扑结构决定，该发现为理解训练动态和设计结构优化方法（如可微剪枝）提供了理论基础。

Abstract: Understanding the properties of the parameter space in feed-forward ReLU networks is critical for effectively analyzing and guiding training dynamics. After initialization, training under gradient flow decisively restricts the parameter space to an algebraic variety that emerges from the homogeneous nature of the ReLU activation function. In this study, we examine two key challenges associated with feed-forward ReLU networks built on general directed acyclic graph (DAG) architectures: the (dis)connectedness of the parameter space and the existence of singularities within it. We extend previous results by providing a thorough characterization of connectedness, highlighting the roles of bottleneck nodes and balance conditions associated with specific subsets of the network. Our findings clearly demonstrate that singularities are intricately connected to the topology of the underlying DAG and its induced sub-networks. We discuss the reachability of these singularities and establish a principled connection with differentiable pruning. We validate our theory with simple numerical experiments.

</details>


### [617] [Forecasting Energy Availability in Local Energy Communities via LSTM Federated Learning](https://arxiv.org/abs/2602.00694)
*Fabio Turazza,Marcello Pietri,Natalia Selini Hadjidimitriou,Marco Mamei*

Main category: cs.LG

TL;DR: 本文探讨了在本地能源社区中，如何利用联邦学习（FL）与长短期记忆网络（LSTM）在不共享用户敏感用电数据的前提下，实现高精度的能源供需预测，以支持社区自给自足目标。


<details>
  <summary>Details</summary>
Motivation: 本地能源社区面临能源产消平衡预测难题，但用户因隐私顾虑不愿共享用电数据，制约了传统集中式预测模型的应用。

Method: 采用联邦学习框架协同训练基于LSTM的负荷/发电预测模型，各用户本地训练、仅上传模型参数更新，不共享原始数据。

Result: 验证了FL-LSTM方法可在显著降低数据隐私泄露风险的同时，保持接近集中式训练的预测精度，量化了隐私保护与预测性能间的权衡关系。

Conclusion: 联邦学习是构建隐私保护型能源预测模型的有效范式，为本地能源社区实现数据合规与智能优化的双重目标提供了可行路径。

Abstract: Local Energy Communities are emerging as crucial players in the landscape of sustainable development. A significant challenge for these communities is achieving self-sufficiency through effective management of the balance between energy production and consumption. To meet this challenge, it is essential to develop and implement forecasting models that deliver accurate predictions, which can then be utilized by optimization and planning algorithms. However, the application of forecasting solutions is often hindered by privacy constrains and regulations as the users participating in the Local Energy Community can be (rightfully) reluctant sharing their consumption patterns with others. In this context, the use of Federated Learning (FL) can be a viable solution as it allows to create a forecasting model without the need to share privacy sensitive information among the users. In this study, we demonstrate how FL and long short-term memory (LSTM) networks can be employed to achieve this objective, highlighting the trade-off between data sharing and forecasting accuracy.

</details>


### [618] [LocalV: Exploiting Information Locality for IP-level Verilog Generation](https://arxiv.org/abs/2602.00704)
*Hanqi Lyu,Di Huang,Yaoyu Zhu,Kangcheng Liu,Bohan Dou,Chongxiao Li,Pengwei Jin,Shuyao Cheng,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

TL;DR: 本文提出LocalV多智能体框架，通过利用模块化硬件设计中的信息局部性，将长文档到长代码的生成问题分解为多个短文档、短代码任务，显著提升RTL级Verilog代码生成的准确性与可调试性，在IP级基准RealBench上将通过率从21.6%提升至45.0%。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型及智能体方法在工业级IP设计中难以应对长文档理解、长代码生成的语法/语义退化以及复杂功能验证调试等三大挑战。

Method: 提出LocalV多智能体框架，包含分层文档划分、任务规划、局部化代码生成、接口一致的代码合并、AST引导的局部感知调试等模块。

Result: 在RealBench IP级Verilog生成基准上，LocalV达到45.0%的通过率，显著优于SOTA方法（21.6%）。

Conclusion: 利用硬件设计固有的模块化与信息局部性，可有效缓解LLM在RTL生成中的长上下文与长输出瓶颈，LocalV为工业级硬件生成提供了可扩展、可调试的新范式。

Abstract: The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.

</details>


### [619] [Deep Time-series Forecasting Needs Kernelized Moment Balancing](https://arxiv.org/abs/2602.00717)
*Licheng Pan,Hao Wang,Haocheng Yang,Yuqi Li,Qingsong Wen,Xiaoxi Li,Zhichao Chen,Haoxuan Li,Zhixuan Chu,Yuan Lu*

Main category: cs.LG

TL;DR: 本文提出了一种名为KMB-DF的新型深度时间序列预测方法，通过核化矩平衡实现更充分的分布对齐，从而提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测目标函数仅对少数预设平衡函数进行矩匹配，无法满足Imbens准则所要求的对任意平衡函数的一阶矩匹配，导致分布平衡不充分。

Method: 提出KMB-DF方法，从再生核希尔伯特空间（RKHS）中自适应选取最具信息量的平衡函数，实现充分分布平衡；推导出可微、可计算的目标函数，便于基于梯度的训练。

Result: 在多个模型和数据集上的大量实验表明，KMB-DF持续提升预测精度，并达到当前最优性能。

Conclusion: KMB-DF通过核化矩平衡有效解决了深度时间序列预测中的分布不平衡问题，为该领域提供了理论更坚实、性能更优的新范式。

Abstract: Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstrate that existing objectives fail to meet this criterion, as they enforce moment matching only for one or two predefined balancing functions, thus failing to achieve full distribution balance. To address this limitation, we propose direct forecasting with kernelized moment balancing (KMB-DF). Unlike existing objectives, KMB-DF adaptively selects the most informative balancing functions from a reproducing kernel hilbert space (RKHS) to enforce sufficient distribution balancing. We derive a tractable and differentiable objective that enables efficient estimation from empirical samples and seamless integration into gradient-based training pipelines. Extensive experiments across multiple models and datasets show that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance. Code is available at https://anonymous.4open.science/r/KMB-DF-403C.

</details>


### [620] [Federated Learning at the Forefront of Fairness: A Multifaceted Perspective](https://arxiv.org/abs/2602.00718)
*Noorain Mukhtiar,Adnan Mahmood,Yipeng Zhou,Jian Yang,Jing Teng,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 本文综述了联邦学习中公平性问题的最新研究进展，从模型性能和客户端能力两个维度对公平性方法进行分类，并提出评估框架与指标，探讨未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端异质性导致模型性能不均衡，公平性成为关键挑战。

Method: 提出多维分类体系（模型性能导向与能力导向），构建公平性问题分类与解决框架，并分析常用公平性评估指标。

Result: 系统梳理了现有公平性方法，明确了不同技术路径的适用场景与效果，总结了主流评估指标并指出其局限性。

Conclusion: 公平性是联邦学习落地的关键前提；需兼顾全局性能与个体公平，未来需在理论保障、动态环境适配与跨域公平等方面深入探索。

Abstract: Fairness in Federated Learning (FL) is emerging as a critical factor driven by heterogeneous clients' constraints and balanced model performance across various scenarios. In this survey, we delineate a comprehensive classification of the state-of-the-art fairness-aware approaches from a multifaceted perspective, i.e., model performance-oriented and capability-oriented. Moreover, we provide a framework to categorize and address various fairness concerns and associated technical aspects, examining their effectiveness in balancing equity and performance within FL frameworks. We further examine several significant evaluation metrics leveraged to measure fairness quantitatively. Finally, we explore exciting open research directions and propose prospective solutions that could drive future advancements in this important area, laying a solid foundation for researchers working toward fairness in FL.

</details>


### [621] [Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation](https://arxiv.org/abs/2602.00722)
*Hao Gu,Mao-Lin Luo,Zi-Hao Zhou,Han-Chen Zhang,Min-Ling Zhang,Tong Wei*

Main category: cs.LG

TL;DR: 本文提出了一种在受限Stiefel流形上进行参数高效持续学习的新方法EBLoRA，通过解耦更新的幅度与方向结构，平衡低秩适应的奇异值谱，从而缓解前后向遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法主要关注避免对过去更新的干扰，而忽略了任务特定更新本身应具备何种性质才能自然保持已有知识；作者从知识分解视角发现低秩适应的奇异值谱高度不平衡，导致易破坏旧知识且易受后续任务干扰。

Method: 将任务更新的幅度与方向结构解耦，并在受限Stiefel流形上建模为约束优化问题，采用兼容主流深度学习优化器的投影一阶法求解。

Result: 所提方法EBLoRA在多个基准上一致优于现有持续学习基线，有效缓解了后向遗忘（backward forgetting）和前向遗忘（forward forgetting）。

Conclusion: 显式平衡低秩适应中各分量的重要性可提升持续学习的稳定性与泛化性，EBLoRA为参数高效持续学习提供了新范式。

Abstract: Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.

</details>


### [622] [Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity](https://arxiv.org/abs/2602.00723)
*Prakhar Ganesh,Reza Shokri,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: 本文提出'提示多重性'框架，用于量化大语言模型（LLM）在幻觉评估中的一致性，发现现有评估严重忽视一致性，导致对幻觉危害的误解；研究还表明当前检测方法实际检测的是一致性而非正确性，而RAG等缓解技术可能引入新不一致性。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉评估仅关注输出正确性，忽视一致性，难以准确识别和应对幻觉带来的多样化危害（如信任侵蚀、信息误导）。

Method: 提出'提示多重性'（prompt multiplicity）框架，通过多提示输入评估模型输出的一致性，并在Med-HALT等基准上进行实证分析，同时检验一致性在幻觉检测与缓解（如RAG）中的作用。

Result: 发现基准测试中存在超50%的不一致性；幻觉检测技术实质检测的是输出一致性而非事实正确性；RAG等缓解方法虽有益，却可能引入额外不一致性。

Conclusion: 将提示多重性纳入幻觉评估可更全面刻画潜在危害，并揭示当前检测与缓解策略的关键局限，亟需兼顾一致性与正确性的新评估范式。

Abstract: Large language models (LLMs) are known to "hallucinate" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.

</details>


### [623] [Pareto-Conditioned Diffusion Models for Offline Multi-Objective Optimization](https://arxiv.org/abs/2602.00737)
*Jatan Shrestha,Santeri Heiskanen,Kari Hepola,Severi Rissanen,Pekka Jääskeläinen,Joni Pajarinen*

Main category: cs.LG

TL;DR: 本文提出Pareto-Conditioned Diffusion（PCD）框架，将离线多目标优化建模为条件采样问题，无需显式代理模型，通过重加权和参考方向机制提升帕累托前沿探索能力，在基准测试中展现出优越且稳定的性能。


<details>
  <summary>Details</summary>
Motivation: 离线多目标优化中，仅依赖静态数据集难以泛化到未观测区域，现有方法常需构建代理模型，存在建模误差与泛化瓶颈。

Method: 提出Pareto-Conditioned Diffusion（PCD），将MOO建模为对期望权衡进行条件控制的扩散采样过程；引入基于性能的重加权策略增强高质量样本采样，并设计参考方向机制引导向训练数据之外的新颖高潜力区域采样。

Result: 在标准离线MOO基准上，PCD性能高度竞争力，且跨任务一致性显著优于现有方法。

Conclusion: PCD提供了一种不依赖代理模型、更鲁棒且可扩展的离线MOO新范式，有效缓解了泛化不足与前沿覆盖不均的问题。

Abstract: Multi-objective optimization (MOO) arises in many real-world applications where trade-offs between competing objectives must be carefully balanced. In the offline setting, where only a static dataset is available, the main challenge is generalizing beyond observed data. We introduce Pareto-Conditioned Diffusion (PCD), a novel framework that formulates offline MOO as a conditional sampling problem. By conditioning directly on desired trade-offs, PCD avoids the need for explicit surrogate models. To effectively explore the Pareto front, PCD employs a reweighting strategy that focuses on high-performing samples and a reference-direction mechanism to guide sampling towards novel, promising regions beyond the training data. Experiments on standard offline MOO benchmarks show that PCD achieves highly competitive performance and, importantly, demonstrates greater consistency across diverse tasks than existing offline MOO approaches.

</details>


### [624] [GraphNNK -- Graph Classification and Interpretability](https://arxiv.org/abs/2602.00753)
*Zeljko Bolevic,Milos Brajovic,Isidora Stankovic,Ljubisa Stankovic*

Main category: cs.LG

TL;DR: 本文提出了一种基于非负核回归（NNK）的图神经网络解释性方法，通过在嵌入空间中用相似训练样本的凸组合进行预测，提升模型可解释性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有GNN依赖参数化分类器（如线性softmax层），导致可解释性差且泛化能力受限。

Method: 采用非负核回归（NNK）替代传统参数化分类器，在GNN的嵌入空间中实现基于相似训练样本的插值式预测。

Result: 该方法在保持甚至提升性能的同时，提供了理论保障和直观、可解释的预测依据。

Conclusion: NNK作为GNN的非参数化预测模块，能有效增强模型的可解释性与泛化能力，为图学习提供新思路。

Abstract: Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particularly Non-Negative Kernel regression (NNK), has demonstrated that predictions can be expressed as convex combinations of similar training examples in the embedding space, yielding both theoretical results and interpretable explanations.

</details>


### [625] [BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features](https://arxiv.org/abs/2602.00767)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 本文提出一种机制化方法，通过识别并约束语言模型中控制不良行为的少量内部特征，来防止微调过程中出现的新兴错位（emergent misalignment），在六个微调领域中实现了高达95%的相对错位降低，且不损害模型性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型在窄域监督目标下微调时，可能产生新兴错位——即达成目标行为的同时发展出不良的域外行为，亟需机制性干预手段。

Method: 识别控制错位行为的关键内部特征，于微调过程中对其施加约束（blocking）；采用分离的选/评数据集、多评审员、多随机种子、质量指标与大量消融实验验证机制特异性。

Result: 在六个微调任务中，固定特征集的约束使新兴错位相对减少最高达95%，目标任务性能无损；发现长期微调下错位会因特征/层重路由而重现，并验证部分修复策略的有效性。

Conclusion: 针对内部机制在训练时施加定向约束，可有效缓解新兴错位，且不牺牲目标任务性能，为对齐提供可解释、可干预的新路径。

Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.

</details>


### [626] [Provable Model Provenance Set for Large Language Models](https://arxiv.org/abs/2602.00772)
*Xiaoqi Qiu,Hao Zeng,Zhiyu Hou,Hongxin Wei*

Main category: cs.LG

TL;DR: 本文提出了Model Provenance Set（MPS）方法，通过顺序检验与排除策略，构建满足可证明置信水平的模型来源集合，解决了现有启发式指纹匹配方法缺乏误差控制和多源覆盖不足的问题。


<details>
  <summary>Details</summary>
Motivation: 未经授权的模型使用和错误归因日益增多，而现有模型溯源方法依赖启发式指纹匹配，缺乏可证明的误差控制，且常忽略多源情形，导致溯源结果可靠性无法验证。

Method: 形式化定义了具有可证明保证的模型溯源问题，并提出Model Provenance Set（MPS），采用顺序检验与排除策略，在候选池中检验各潜在来源的显著性，自适应构造满足预设置信水平的小规模溯源集合。

Result: 实验表明MPS能有效实现目标溯源覆盖率，同时严格限制无关模型的纳入；并在归因与审计任务中展现出实用潜力。

Conclusion: MPS为模型溯源提供了首个具备可证明统计保证的方法框架，兼顾覆盖率与精确性，提升了模型版权保护与责任追溯的可信度。

Abstract: The growing prevalence of unauthorized model usage and misattribution has increased the need for reliable model provenance analysis. However, existing methods largely rely on heuristic fingerprint-matching rules that lack provable error control and often overlook the existence of multiple sources, leaving the reliability of their provenance claims unverified. In this work, we first formalize the model provenance problem with provable guarantees, requiring rigorous coverage of all true provenances at a prescribed confidence level. Then, we propose the Model Provenance Set (MPS), which employs a sequential test-and-exclusion procedure to adaptively construct a small set satisfying the guarantee. The key idea of MPS is to test the significance of provenance existence within a candidate pool, thereby establishing a provable asymptotic guarantee at a user-specific confidence level. Extensive experiments demonstrate that MPS effectively achieves target provenance coverage while strictly limiting the inclusion of unrelated models, and further reveal its potential for practical provenance analysis in attribution and auditing tasks.

</details>


### [627] [A novel VAE-DML fusion framework for casual analysis of greenwashing in the mining industry](https://arxiv.org/abs/2602.00774)
*Yuxin Lu,Zhen Peng,Xiqiang Xia,Jie Wang*

Main category: cs.LG

TL;DR: 本文研究股权制衡对矿业产业链企业绿色漂绿行为的抑制作用，采用变分自编码器（VAE）和双重机器学习（DML）构建反事实场景以解决内生性问题，发现股权制衡显著抑制绿色漂绿，且存在区域、产业链位置、环境敏感性及时序异质性，并通过缓解管理层业绩压力、提升高管团队稳定性及增强媒体监督三条路径发挥作用。


<details>
  <summary>Details</summary>
Motivation: 在全球绿色转型与“双碳”目标背景下，矿业产业链企业资源消耗大、环境影响突出，其环境信息披露的真实性与可靠性成为可持续发展与国家战略实现的关键问题。

Method: 创新性地结合变分自编码器（VAE）与双重机器学习（DML）模型构建反事实场景，以缓解内生性并精准识别股权制衡与绿色漂绿之间的因果关系。

Result: 1）股权制衡对绿色漂绿具有显著负向因果效应；2）该效应在西部地区、产业链上游及高环境敏感性行业中更强；3）效应具有时序动态性：当期最强，滞后效应递减但仍显著，长期具稳定累积影响；4）作用机制包括缓解管理业绩压力、提升高管团队稳定性、增强媒体监督。

Conclusion: 股权制衡是治理矿业企业绿色漂绿行为的有效公司治理机制，其效果具有情境依赖性与时序演化特征，为优化绿色治理结构、服务国家‘双碳’战略提供了理论依据与政策启示。

Abstract: Against the backdrop of the global green transition and "dual carbon" goals, mining industry chain enterprises are pivotal entities in terms of resource consumption and environmental impact. Their environmental performance directly affects regional ecological security and is closely tied to national resource strategies and green transformation outcomes. Ensuring the authenticity and reliability of their environmental disclosure is thus a core and urgent issue for sustainable development and national strategic objectives.From a corporate governance perspective, this study examines equity balance as a fundamental governance mechanism, investigating its inhibitory effect on greenwashing behavior among these enterprises and the underlying pathways involved. Methodologically, the paper innovatively employs a Variational Autoencoder (VAE) and a Double Machine Learning (DML) model to construct counterfactual scenarios, mitigating endogeneity concerns and precisely identifying the causal relationship between equity balance and greenwashing. The findings indicate, first, a significant negative causal relationship between equity balance and corporate greenwashing, confirming its substantive governance effect. Second, this inhibitory effect exhibits notable heterogeneity, manifesting more strongly in western regions, upstream segments of the industrial chain, and industries with high environmental sensitivity. Third, the governance effect demonstrates clear temporal dynamics, with the strongest impact occurring in the current period, followed by a diminishing yet statistically significant lagged effect, and ultimately a stable long-term cumulative influence. Finally, mechanism analysis reveals that equity balance operates through three distinct channels to curb greenwashing: alleviating management performance pressure, enhancing the stability of the executive team, and intensifying media scrutiny.

</details>


### [628] [Stable Time Series Prediction of Enterprise Carbon Emissions Based on Causal Inference](https://arxiv.org/abs/2602.00775)
*Zitao Hong,Zhen Peng,Xueping Liu*

Main category: cs.LG

TL;DR: 本文提出了一种结合因果推断、稳定学习与时间序列建模的稳定时序预测机制，用于应对碳排放数据在时空维度上的分布偏移问题，提升跨区域、跨企业碳排放预测的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 碳达峰碳中和目标下，企业碳排放趋势预测对低碳决策至关重要，但区域、行业与企业间在能源结构、政策强度等方面的异质性导致碳排放数据存在显著时空分布偏移和非平稳性，严重削弱预测模型的准确性与指导价值。

Method: 融合因果推断与稳定学习，构建风险一致性约束的稳定学习框架，提取对政策、区域、行业等多环境扰动鲁棒且长期稳定的因果特征；引入自适应归一化与样本重加权策略，动态校正由经济波动与政策变迁引发的时间非平稳性。

Result: 所提方法有效提升了模型在分布偏移环境下的泛化能力与可解释性，增强了对生产规划与碳配额交易等实际决策的支持能力。

Conclusion: 该稳定时序预测机制为复杂异质环境下企业碳排放精准预测提供了新范式，兼具理论严谨性与实践适用性。

Abstract: Against the backdrop of ongoing carbon peaking and carbon neutrality goals, accurate prediction of enterprise carbon emission trends constitutes an essential foundation for energy structure optimization and low-carbon transformation decision-making. Nevertheless, significant heterogeneity persists across regions, industries and individual enterprises regarding energy structure, production scale, policy intensity and governance efficacy, resulting in pronounced distribution shifts and non-stationarity in carbon emission data across both temporal and spatial dimensions. Such cross-regional and cross-enterprise data drift not only compromises the accuracy of carbon emission reporting but substantially undermines the guidance value of predictive models for production planning and carbon quota trading decisions. To address this critical challenge, we integrate causal inference perspectives with stable learning methodologies and time-series modelling, proposing a stable temporal prediction mechanism tailored to distribution shift environments. This mechanism incorporates enterprise-level energy inputs, capital investment, labour deployment, carbon pricing, governmental interventions and policy implementation intensity, constructing a risk consistency-constrained stable learning framework that extracts causal stable features (robust against external perturbations yet demonstrating long-term stable effects on carbon dioxide emissions) from multi-environment samples across diverse policies, regions and industrial sectors. Furthermore, through adaptive normalization and sample reweighting strategies, the approach dynamically rectifies temporal non-stationarity induced by economic fluctuations and policy transitions, ultimately enhancing model generalization capability and explainability in complex environments.

</details>


### [629] [Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding](https://arxiv.org/abs/2602.00781)
*Jiamin Xu,Kyra Gan*

Main category: cs.LG

TL;DR: 本文提出了一种面向非回合制、有限时域马尔可夫决策过程（MDP）的在线强化学习新方法，通过引入K步前瞻Q函数与时间自适应阈值机制，在理论和实验上均展现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无限时域RL方法（如基于折扣因子的算法）难以适配非回合制、固定终端时间的有限时域MDP；关键挑战在于需准确估计至固定终点的回报，而传统方法未显式建模该结构。

Method: 提出K步前瞻Q函数（截断至未来K步的规划），并结合时间自适应阈值机制（仅选择估计值超过动态阈值的动作）；设计了适用于该目标的高效表格型学习算法，并给出理论收敛性分析。

Result: 理论：对K=1达到极小极大最优常数遗憾；对K≥2，遗憾界为O(max((K−1),C_{K−1})√(SAT log T))。实验：在JumpRiverswim、FrozenLake和AnyTrading等环境中，自适应增K策略显著提升累计奖励，超越当前主流表格RL方法。

Conclusion: K步前瞻+阈值机制是一种有效适配有限时域非回合制MDP的学习范式，兼顾理论最优性与实际样本效率，为该类问题提供了新思路与实用算法。

Abstract: Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\mathcal{O}(\max((K-1),C_{K-1})\sqrt{SAT\log(T)})$ regret for any $K \geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.

</details>


### [630] [Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors](https://arxiv.org/abs/2602.00788)
*Md Abir Hossen,Mohammad Ali Javidian,Vignesh Narayanan,Jason M. O'Kane,Pooyan Jamshidi*

Main category: cs.LG

TL;DR: 本文提出RESCUE方法，利用因果推断改进多保真度贝叶斯优化（MFBO），通过构建结构因果模型和因果超体积知识梯度采集策略，提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有MFBO方法仅建模变量间的关联性，忽视因果机制，当低保真代理与高保真目标对齐较差时性能下降。

Method: 提出RESCUE：学习输入-保真度-目标间的结构因果模型，构建能编码干预效应的概率多保真代理，并设计基于因果结构的多目标采集策略（因果超体积知识梯度）。

Result: 在机器人、AutoML和医疗等合成与真实任务上，RESCUE比当前最优MFBO方法具有更高的样本效率。

Conclusion: 引入因果推理可显著提升多保真度优化的鲁棒性与效率，尤其在低/高保真不一致场景下优势明显。

Abstract: Multi-fidelity Bayesian optimization (MFBO) accelerates the search for the global optimum of black-box functions by integrating inexpensive, low-fidelity approximations. The central task of an MFBO policy is to balance the cost-efficiency of low-fidelity proxies against their reduced accuracy to ensure effective progression toward the high-fidelity optimum. Existing MFBO methods primarily capture associational dependencies between inputs, fidelities, and objectives, rather than causal mechanisms, and can perform poorly when lower-fidelity proxies are poorly aligned with the target fidelity. We propose RESCUE (REducing Sampling cost with Causal Understanding and Estimation), a multi-objective MFBO method that incorporates causal calculus to systematically address this challenge. RESCUE learns a structural causal model capturing causal relationships between inputs, fidelities, and objectives, and uses it to construct a probabilistic multi-fidelity (MF) surrogate that encodes intervention effects. Exploiting the causal structure, we introduce a causal hypervolume knowledge-gradient acquisition strategy to select input-fidelity pairs that balance expected multi-objective improvement and cost. We show that RESCUE improves sample efficiency over state-of-the-art MF optimization methods on synthetic and real-world problems in robotics, machine learning (AutoML), and healthcare.

</details>


### [631] [Sporadic Gradient Tracking over Directed Graphs: A Theoretical Perspective on Decentralized Federated Learning](https://arxiv.org/abs/2602.00791)
*Shahryar Zehtabi,Dong-Jun Han,Seyyedali Hosseinalipour,Christopher Brinton*

Main category: cs.LG

TL;DR: 本文提出了一种名为Sporadic Gradient Tracking (Spod-GT) 的新型去中心化联邦学习算法，首次在有向图上统一处理数据异构性和客户端资源差异性，支持客户端自定义梯度计算与通信频率，并在较宽松假设下给出收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化联邦学习中数据异构性与客户端资源（如计算和通信能力）多样性带来的挑战，现有方法分别处理这两类问题，缺乏统一框架。

Method: 提出Sporadic Gradient Tracking (Spod-GT) 算法，支持客户端特定的梯度计算频率和异构/非对称通信频率，适用于一般有向图；理论分析采用放宽的梯度估计方差与梯度多样性假设。

Result: 在图像分类数据集上的实验表明，Spod-GT 在收敛速度与模型性能上优于现有梯度跟踪基线方法；理论证明其在间歇性参与下仍能保证共识性与最优性。

Conclusion: Spod-GT 是首个在有向图上同时建模梯度跟踪与稀疏异步通信的 DFL 算法，兼具理论严谨性与实际适用性，为资源受限、非同步、非对称的真实分布式场景提供了新范式。

Abstract: Decentralized Federated Learning (DFL) enables clients with local data to collaborate in a peer-to-peer manner to train a generalized model. In this paper, we unify two branches of work that have separately solved important challenges in DFL: (i) gradient tracking techniques for mitigating data heterogeneity and (ii) accounting for diverse availability of resources across clients. We propose $\textit{Sporadic Gradient Tracking}$ ($\texttt{Spod-GT}$), the first DFL algorithm that incorporates these factors over general directed graphs by allowing (i) client-specific gradient computation frequencies and (ii) heterogeneous and asymmetric communication frequencies. We conduct a rigorous convergence analysis of our methodology with relaxed assumptions on gradient estimation variance and gradient diversity of clients, providing consensus and optimality guarantees for GT over directed graphs despite intermittent client participation. Through numerical experiments on image classification datasets, we demonstrate the efficacy of $\texttt{Spod-GT}$ compared to well-known GT baselines.

</details>


### [632] [Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion](https://arxiv.org/abs/2602.00792)
*Guinan Chen,Xunpeng Huang,Ying Sun,Shijin Wang,Yanyong Zhang,Chao Wang*

Main category: cs.LG

TL;DR: 本文提出Masked Consistency Distillation（MCD）框架，通过建立显式的Masked Diffusion Duality理论，首次实现 masked离散扩散模型的确定性轨迹构造，从而在不损失生成质量前提下获得16倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有masked离散扩散模型缺乏确定性采样工具，导致推理效率低下；虽有基于扩散对偶性的确定性蒸馏方法，但其在masked模型上性能不佳且依赖复杂积分算子；而masked领域先前方法普遍认为不存在确定性轨迹，只能依赖随机蒸馏。

Method: 提出显式的Masked Diffusion Duality理论，证明masked过程可通过一种新颖的最大值索引保持机制，由连续高斯过程投影得到；在此基础上设计Masked Consistency Distillation（MCD）框架，解析地构造确定性耦合轨迹，绕过数值ODE求解器。

Result: MCD在保持生成质量不变的前提下，实现16倍推理速度提升；为masked与连续扩散建模建立了坚实的理论联系；释放了consistency distillation在高性能离散生成中的全部潜力。

Conclusion: Masked离散扩散模型可具备确定性轨迹，MCD是首个严格优于随机蒸馏的确定性蒸馏方法，兼具理论深度与工程实效性。

Abstract: Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform models, these approaches generally underperform masked models and rely on complex integral operators. Conversely, in the masked domain, prior methods typically assume the absence of deterministic trajectories, forcing a reliance on stochastic distillation. To bridge this gap, we establish explicit Masked Diffusion Duality, proving that the masked process arises as the projection of a continuous Gaussian process via a novel maximum-value index preservation mechanism. Furthermore, we introduce Masked Consistency Distillation (MCD), a principled framework that leverages this duality to analytically construct the deterministic coupled trajectories required for consistency distillation, bypassing numerical ODE solvers. This result strictly improves upon prior stochastic distillation methods, achieving a 16$\times$ inference speedup without compromising generation quality. Our findings not only provide a solid theoretical foundation connecting masked and continuous diffusion, but also unlock the full potential of consistency distillation for high-performance discrete generation. Our code is available at https://anonymous.4open.science/r/MCD-70FD.

</details>


### [633] [JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation](https://arxiv.org/abs/2602.00800)
*Yebin Yang,Huaijin Wu,Fu Guo,Lin Yao,Xiaohan Qin,Jingzhi Wang,Debing Zhang,Junchi Yan*

Main category: cs.LG

TL;DR: 本文提出了一种新的模型扩展范式——token-indexed parameters（如JTok和JTok-M），通过轻量级调制向量解耦模型容量与计算量，在几乎不增加FLOPs的前提下显著提升性能，并在多个基准上超越MoE和dense模型。


<details>
  <summary>Details</summary>
Motivation: 传统LLM沿dense维度扩展导致计算成本线性增长；MoE虽解耦容量与计算，但带来高内存开销和硬件效率问题，亟需一种更高效、正交的扩展方式。

Method: 引入token-indexed参数机制，设计Joint-Token（JTok）和Mixture of Joint-Token（JTok-M），在Transformer层中通过辅助嵌入表检索调制向量，以轻量级逐元素操作对主干网络进行调制。

Result: 在650M至61B参数规模的dense和MoE主干上验证：显著降低验证损失，下游任务大幅提升（如MMLU +4.1、ARC +8.3、CEval +8.9）；isoFLOPs下比vanilla MoE节省35%计算量，且呈现可预测的幂律缩放行为；实现开销极小。

Conclusion: token-indexed参数是一种高效、可扩展、硬件友好的新型扩展轴，能实质性推动质量-计算帕累托前沿，为大模型扩展提供新路径。

Abstract: LLMs have traditionally scaled along dense dimensions, where performance is coupled with near-linear increases in computational cost. While MoE decouples capacity from compute, it introduces large memory overhead and hardware efficiency challenges. To overcome these, we propose token-indexed parameters as a novel, orthogonal scaling axis that decouple model capacity from FLOPs. Specifically, we introduce Joint-Token (JTok) and Mixture of Joint-Token (JTok-M), which augment Transformer layers with modulation vectors retrieved from auxiliary embedding tables. These vectors modulate the backbone via lightweight, element-wise operations, incurring negligible FLOPs overhead. Extensive experiments on both dense and MoE backbones, spanning from 650M (190M + 460M embedding) to 61B (17B + 44B embedding) total parameters, demonstrate that our approach consistently reduces validation loss and significantly improves downstream task performance (e.g., +4.1 on MMLU, +8.3 on ARC, +8.9 on CEval). Rigorous isoFLOPs analysis further confirms that JTok-M fundamentally shifts the quality-compute Pareto frontier, achieving comparable model quality with 35% less compute relative to vanilla MoE architectures, and we validate that token-indexed parameters exhibit a predictable power-law scaling behavior. Moreover, our efficient implementation ensures that the overhead introduced by JTok and JTok-M remains marginal.

</details>


### [634] [Mobile Exergames: Activity Recognition Based on Smartphone Sensors](https://arxiv.org/abs/2602.00809)
*David Craveiro,Hugo Silva*

Main category: cs.LG

TL;DR: 本文提出了一款名为Duck Catch & Fit的2D无限游戏，通过智能手机的加速度计、陀螺仪和磁力计实现人体活动识别，并结合语音识别‘fire’词以增强游戏沉浸感，实验表明该方法能高精度识别活动。


<details>
  <summary>Details</summary>
Motivation: 利用智能手机传感器进行人体活动识别，应用于游戏、医疗或监控等领域，提升交互体验与实用性。

Method: 采用智能手机的加速度计、陀螺仪和磁力计进行多传感器数据采集，结合特征提取与机器学习算法识别静止、侧向移动及伪侧向移动等行为；同时集成语音识别系统检测关键词'fire'。

Result: 实现了高精度的人体活动识别，并验证了运动与语音融合方式可显著提升游戏沉浸感与复杂度。

Conclusion: 基于智能手机传感器与机器学习的活动识别方案可行且有效，运动与语音双模态融合为互动游戏设计提供了新思路。

Abstract: Smartphone sensors can be extremely useful in providing information on the activities and behaviors of persons. Human activity recognition is increasingly used for games, medical, or surveillance. In this paper, we propose a proof-of-concept 2D endless game called Duck Catch & Fit, which implements a detailed activity recognition system that uses a smartphone accelerometer, gyroscope, and magnetometer sensors. The system applies feature extraction and learning mechanism to detect human activities like staying, side movements, and fake side movements. In addition, a voice recognition system is combined to recognize the word "fire" and raise the game's complexity. The results show that it is possible to use machine learning techniques to recognize human activity with high recognition levels. Also, the combination of movement-based and voice-based integrations contributes to a more immersive gameplay.

</details>


### [635] [Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization](https://arxiv.org/abs/2602.00827)
*Taesun Yeom,Taehyeok Ha,Jaeho Lee*

Main category: cs.LG

TL;DR: 本文研究特征学习强度（FLS）对深度神经网络泛化性能的影响，发现存在一个最优FLS值，在实践中能显著提升泛化能力；理论分析表明该最优性源于'过对齐'与'过拟合'之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有理论多关注渐近情形下的FLS影响，缺乏对实际训练停止条件（如达到目标训练风险）下FLS如何影响泛化的理解。

Method: 通过实证研究发现最优FLS现象，并在两层ReLU网络、逻辑损失、梯度流动力学框架下，以初始化尺度调控FLS，进行理论分析。

Result: 实证上发现存在一个既非过小也非过大的最优FLS，可带来显著泛化增益；理论上证明该最优性源于过对齐（FLS过大）与过拟合（FLS过小）的权衡。

Conclusion: FLS并非越强越好，存在一个平衡点，在实际训练中应谨慎调控初始化尺度以实现最优泛化。

Abstract: Feature learning strength (FLS), i.e., the inverse of the effective output scaling of a model, plays a critical role in shaping the optimization dynamics of neural nets. While its impact has been extensively studied under the asymptotic regimes -- both in training time and FLS -- existing theory offers limited insight into how FLS affects generalization in practical settings, such as when training is stopped upon reaching a target training risk. In this work, we investigate the impact of FLS on generalization in deep networks under such practical conditions. Through empirical studies, we first uncover the emergence of an $\textit{optimal FLS}$ -- neither too small nor too large -- that yields substantial generalization gains. This finding runs counter to the prevailing intuition that stronger feature learning universally improves generalization. To explain this phenomenon, we develop a theoretical analysis of gradient flow dynamics in two-layer ReLU nets trained with logistic loss, where FLS is controlled via initialization scale. Our main theoretical result establishes the existence of an optimal FLS arising from a trade-off between two competing effects: An excessively large FLS induces an $\textit{over-alignment}$ phenomenon that degrades generalization, while an overly small FLS leads to $\textit{over-fitting}$.

</details>


### [636] [Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation](https://arxiv.org/abs/2602.00834)
*Wei Chen,Jiacheng Li,Shigui Li,Zhiqi Lin,Junmei Yang,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 本文提出MinPV原则，通过最小化路径方差来改进基于分数的密度比估计方法，推导出方差的闭式表达式，并使用Kumaraswamy混合模型自适应学习低方差路径，从而提升估计精度与稳定性。


<details>
  <summary>Details</summary>
Motivation: 基于分数的密度比估计方法理论上路径无关，但实际性能却严重依赖路径调度，存在理论与实践的矛盾。

Method: 提出MinPV（最小路径方差）原则，推导路径时间分数方差的闭式表达式，并用Kumaraswamy混合模型参数化路径以实现数据自适应优化。

Result: 在多个具有挑战性的基准测试上取得了新的最先进结果，估计更准确、更稳定。

Conclusion: 路径方差是影响基于分数DRE性能的关键被忽略项，最小化该方差可显著提升方法性能，且所提框架具备理论严谨性与实用可行性。

Abstract: Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the time score. To address this, we propose MinPV (\textbf{Min}imum \textbf{P}ath \textbf{V}ariance) Principle, which introduces a principled heuristic to minimize the overlooked path variance. Our key contribution is the derivation of a closed-form expression for the variance, turning an intractable problem into a tractable optimization. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns a data-adaptive, low-variance path without heuristic selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks.

</details>


### [637] [RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation](https://arxiv.org/abs/2602.00849)
*Yuhao Huang,Shih-Hsin Wang,Andrea L. Bertozzi,Bao Wang*

Main category: cs.LG

TL;DR: 本文提出了RMFlow，一种高效的多模态生成模型，通过结合粗粒度的1-NFE MeanFlow传输与定制化的噪声注入细化步骤，在仅使用1次函数评估的情况下实现了接近SOTA的文本到图像、上下文到分子及时间序列生成性能。


<details>
  <summary>Details</summary>
Motivation: MeanFlow虽能高效高保真地生成图像，但其单次函数评估（1-NFE）生成结果往往不够吸引人，需改进。

Method: RMFlow将粗粒度1-NFE MeanFlow传输与后续定制化噪声注入细化步骤相结合，并采用新设计的损失函数训练神经网络来近似流路径的平均速度，该损失函数兼顾最小化概率路径间的Wasserstein距离和最大化样本似然。

Result: RMFlow在文本到图像、上下文到分子及时间序列生成任务中，仅用1-NFE即达到近SOTA性能，计算开销与基准MeanFlow相当。

Conclusion: RMFlow通过引入噪声注入细化机制和新型损失函数，在保持1-NFE高效性的同时显著提升了生成质量，验证了其作为高效多模态生成模型的有效性。

Abstract: Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at a computational cost comparable to the baseline MeanFlows.

</details>


### [638] [Investigating the Robustness of Subtask Distillation under Spurious Correlation](https://arxiv.org/abs/2602.00852)
*Pattarawat Chormai,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 本文研究了在存在虚假相关性的数据集上进行子任务蒸馏的挑战，发现SubDistill等先进方法比基线方法更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法依赖的数据集往往规模小、代表性不足或存在虚假相关性，影响蒸馏效果。

Method: 评估了经典蒸馏方法及新兴的SubDistill方法在含虚假相关性数据上的表现，并分析其随相关性强度变化的鲁棒性差异。

Result: 随着虚假相关性增强，SubDistill等先进方法保持较好性能，而部分基线方法性能急剧下降至近随机水平。

Conclusion: 知识蒸馏在真实世界不完美数据（尤其是含虚假相关性）下面临严峻挑战，需发展更鲁棒的方法。

Abstract: Subtask distillation is an emerging paradigm in which compact, specialized models are extracted from large, general-purpose 'foundation models' for deployment in environments with limited resources or in standalone computer systems. Although distillation uses a teacher model, it still relies on a dataset that is often limited in size and may lack representativeness or exhibit spurious correlations. In this paper, we evaluate established distillation methods, as well as the recent SubDistill method, when using data with spurious correlations for distillation. As the strength of the correlations increases, we observe a widening gap between advanced methods, such as SubDistill, which remain fairly robust, and some baseline methods, which degrade to near-random performance. Overall, our study underscores the challenges of knowledge distillation when applied to imperfect, real-world datasets, particularly those with spurious correlations.

</details>


### [639] [Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs](https://arxiv.org/abs/2602.00862)
*Shih-Hsin Wang,Yuhao Huang,Taos Transue,Justin Baker,Jonathan Forstater,Thomas Strohmer,Bao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种面向蛋白质结构的高效多尺度图学习框架，通过构建细粒度（二级结构模体内）与粗粒度（模体间空间关系）的分层图表示，并分别使用两个GNN进行特征学习，在保持理论表达力的同时提升了预测精度并降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的蛋白质结构学习方法难以有效建模多尺度表征和长程依赖关系。

Method: 构建包含细粒度子图（对应α-螺旋、β-折叠、环等二级结构模体）和一个连接这些模体的粗粒度图的分层图表示；采用两个GNN分别学习模体内局部交互和模体间高层结构关系；理论分析证明该框架保持最大表达力。

Result: 在多个基准上，将基线GNN嵌入该多尺度框架显著提升了预测精度并降低了计算成本。

Conclusion: 所提出的模块化多尺度图学习框架能更高效、准确地建模蛋白质结构，兼具理论保证与实用优势。

Abstract: Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In this work, we propose an efficient multiscale graph-based learning framework tailored to proteins. Our proposed framework contains two crucial components: (1) It constructs a hierarchical graph representation comprising a collection of fine-grained subgraphs, each corresponding to a secondary structure motif (e.g., $α$-helices, $β$-strands, loops), and a single coarse-grained graph that connects these motifs based on their spatial arrangement and relative orientation. (2) It employs two GNNs for feature learning: the first operates within individual secondary motifs to capture local interactions, and the second models higher-level structural relationships across motifs. Our modular framework allows a flexible choice of GNN in each stage. Theoretically, we show that our hierarchical framework preserves the desired maximal expressiveness, ensuring no loss of critical structural information. Empirically, we demonstrate that integrating baseline GNNs into our multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.

</details>


### [640] [Improving Flow Matching by Aligning Flow Divergence](https://arxiv.org/abs/2602.00869)
*Yuhao Huang,Taos Transue,Shih-Hsin Wang,William Feldman,Hong Zhang,Bao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的条件流匹配（CFM）改进方法，通过引入概率路径误差的偏微分方程刻画及相应损失函数（联合匹配流与散度），在不牺牲生成效率的前提下显著提升流式生成模型的准确性，并在动力系统、DNA序列和视频生成等任务上验证了其优势。


<details>
  <summary>Details</summary>
Motivation: CFM虽高效且无需模拟，但不足以保证概率路径学习的准确性，需理论刻画并减小其误差。

Method: 推导了学习与真实概率路径间误差的偏微分方程表征；证明总变差差距可由CFM损失与散度损失共同上界控制；据此设计联合优化流与散度的新目标函数。

Result: 新方法在多个基准任务（动力系统建模、DNA序列、视频生成）上显著优于原始CFM，同时保持生成效率。

Conclusion: 联合匹配流与散度能更准确地学习概率路径，为流匹配提供了更坚实的理论基础与更优的实用性能。

Abstract: Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.

</details>


### [641] [Learning Heat-based Equations in Self-similar variables](https://arxiv.org/abs/2602.00872)
*Shihao Wang,Qipeng Qian,Jingquan Wang*

Main category: cs.LG

TL;DR: 本文研究了在自相似变量（SSV）下对基于热方程的解进行学习的方法，提出了一种兼容标准神经算子训练的SSV训练框架，并在二维不可压缩Navier-Stokes方程和一维粘性Burgers方程上进行了验证。结果表明，相比物理坐标系下的训练，SSV训练显著提升了模型在训练时间窗口外的外推精度与稳定性，并更好捕捉长期定性趋势，说明自相似坐标可作为学习热型方程长期动力学的数学驱动归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 提升神经网络对热型偏微分方程长期动力学的泛化与外推能力，利用自相似性这一数学结构提供更有效的归纳偏置。

Method: 构建兼容标准神经算子训练的自相似变量（SSV）训练框架，并在Navier-Stokes和Burgers方程上，使用全连接MLP与因子化全连接网络进行物理坐标与SSV坐标的对照实验。

Result: SSV训练模型在两类方程和两种网络结构下均显著提升外推精度、稳定性及长期趋势建模能力。

Conclusion: 自相似坐标是一种数学上合理且实用的归纳偏置，有助于神经网络更准确、稳健地学习热型方程的长期行为。

Abstract: We study solution learning for heat-based equations in self-similar variables (SSV). We develop an SSV training framework compatible with standard neural-operator training. We instantiate this framework on the two-dimensional incompressible Navier-Stokes equations and the one-dimensional viscous Burgers equation, and perform controlled comparisons between models trained in physical coordinates and in the corresponding self-similar coordinates using two simple fully connected architectures (standard multilayer perceptrons and a factorized fully connected network). Across both systems and both architectures, SSV-trained networks consistently deliver substantially more accurate and stable extrapolation beyond the training window and better capture qualitative long-time trends. These results suggest that self-similar coordinates provide a mathematically motivated inductive bias for learning the long-time dynamics of heat-based equations.

</details>


### [642] [Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs](https://arxiv.org/abs/2602.00879)
*Hao Mark Chen,Zhiwen Mo,Royson Lee,Qianzhou Wang,Da Li,Shell Xu Hu,Wayne Luk,Timothy Hospedales,Hongxiang Fan*

Main category: cs.LG

TL;DR: 本文提出Dynamic Expert Sharing (DES)技术，通过序列级核心集选择优化MoE架构在扩散大语言模型（dLLMs）中的并行解码效率，显著降低专家激活数与延迟，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）结合MoE架构时面临'专家爆炸'问题：并行生成token增多导致激活专家数线性增长，引发内存瓶颈，削弱MoE与并行解码的效率优势。

Method: 提出Dynamic Expert Sharing（DES），摒弃传统token级剪枝/跳过，转向序列级核心集选择；包含两种策略：(1) Intra-Sequence Sharing（DES-Seq），适配序列级最优专家分配；(2) Saliency-Aware Voting（DES-Vote），基于路由权重聚合实现token协同选举核心专家集。

Result: 在MoE dLLMs上实验表明，DES降低唯一专家激活数超55%，延迟最高降低38%，同时保留99%原始精度，使内存开销与并行度解耦。

Conclusion: DES有效缓解MoE dLLMs中专家爆炸带来的内存瓶颈，为高质量、高吞吐并行解码提供了高效可行的架构优化路径。

Abstract: Among parallel decoding paradigms, diffusion large language models (dLLMs) have emerged as a promising candidate that balances generation quality and throughput. However, their integration with Mixture-of-Experts (MoE) architectures is constrained by an expert explosion: as the number of tokens generated in parallel increases, the number of distinct experts activated grows nearly linearly. This results in substantial memory traffic that pushes inference into a memory-bound regime, negating the efficiency gains of both MoE and parallel decoding. To address this challenge, we propose Dynamic Expert Sharing (DES), a novel technique that shifts MoE optimization from token-centric pruning and conventional expert skipping methods to sequence-level coreset selection. To maximize expert reuse, DES identifies a compact, high-utility set of experts to satisfy the requirements of an entire parallel decoding block. We introduce two innovative selection strategies: (1) Intra-Sequence Sharing (DES-Seq), which adapts optimal allocation to the sequence level, and (2) Saliency-Aware Voting (DES-Vote), a novel mechanism that allows tokens to collectively elect a coreset based on aggregated router weights. Extensive experiments on MoE dLLMs demonstrate that DES reduces unique expert activations by over 55% and latency by up to 38%, while retaining 99% of vanilla accuracy, effectively decoupling memory overhead from the degree of parallelism.

</details>


### [643] [Test-time Generalization for Physics through Neural Operator Splitting](https://arxiv.org/abs/2602.00884)
*Louis Serrano,Jiequn Han,Edouard Oyallon,Shirley Ho,Rudy Morel*

Main category: cs.LG

TL;DR: 本文提出了一种测试时无需更新预训练权重的神经算子泛化方法，通过组合已训练的神经算子（基于DISCO字典）来逼近未见过的动力学系统，在零样本泛化任务（如参数外推、新物理组合）中达到SOTA性能，并可反演PDE参数。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在面对分布外测试输入（如新初值、未知PDE系数或新物理现象）时泛化能力差，而已有方法仍需新动力学的微调样本，无法实现真正零样本泛化。

Method: 基于DISCO提供的多动力学神经算子字典，提出一种测试时神经算子分裂策略，搜索已有算子的组合以逼近未见动力学。

Result: 在参数外推和新型物理组合等挑战性分布外任务上实现零样本泛化SOTA结果，并能恢复底层PDE参数。

Conclusion: 测试时计算是构建灵活、可组合、强泛化能力神经算子的关键路径。

Abstract: Neural operators have shown promise in learning solution maps of partial differential equations (PDEs), but they often struggle to generalize when test inputs lie outside the training distribution, such as novel initial conditions, unseen PDE coefficients or unseen physics. Prior works address this limitation with large-scale multiple physics pretraining followed by fine-tuning, but this still requires examples from the new dynamics, falling short of true zero-shot generalization. In this work, we propose a method to enhance generalization at test time, i.e., without modifying pretrained weights. Building on DISCO, which provides a dictionary of neural operators trained across different dynamics, we introduce a neural operator splitting strategy that, at test time, searches over compositions of training operators to approximate unseen dynamics. On challenging out-of-distribution tasks including parameter extrapolation and novel combinations of physics phenomena, our approach achieves state-of-the-art zero-shot generalization results, while being able to recover the underlying PDE parameters. These results underscore test-time computation as a key avenue for building flexible, compositional, and generalizable neural operators.

</details>


### [644] [Reliability-Aware Determinantal Point Processes for Robust Informative Data Selection in Large Language Models](https://arxiv.org/abs/2602.00885)
*Ahmad Sarlak,Abolfazl Razi*

Main category: cs.LG

TL;DR: 本文提出ProbDPP，一种面向数据访问不确定性的可靠性感知子集选择方法，扩展k-DPP以兼顾多样性与访问不可靠性，并设计UCB式在线学习算法，在理论保障下实现鲁棒高效的数据选择。


<details>
  <summary>Details</summary>
Motivation: 传统基于DPP的数据子集选择方法假设数据总能无误获取，无法应对存储故障、通信不完美或随机访问失败等现实不确定性场景，且原目标函数在此类条件下会退化。

Method: 提出ProbDPP：在k-DPP目标中引入正则化项，将优化目标分解为几何多样性项与不可靠性代价项；进一步将问题建模为组合半-bandit问题，设计UCB风格的在线学习算法估计未知可靠性。

Result: ProbDPP在数据访问不确定性下仍保持目标函数良定性与可分解性；所提UCB算法具备理论悔界保证，实验证明其在鲁棒多样性选择上优于传统方法。

Conclusion: ProbDPP为资源受限与不确定性环境下LLM的数据高效利用提供了新范式，弥合了传统多样性选择与实际部署可靠性的鸿沟。

Abstract: Informative data selection is a key requirement for large language models (LLMs) to minimize the amount of data required for fine-tuning, network distillation, and token pruning, enabling fast and efficient deployment, especially under computational and communication constraints. Traditional subset selection methods, including those based on Determinantal Point Processes (DPP), focus on maximizing diversity but assume that selected data batches are always available error-free. This presumption prohibits their use under partial storage outage, imperfect communication, and stochastic access failures. Furthermore, we show that the original formulation collapses under such conditions. To address this gap, we introduce ProbDPP, a novel reliability-aware implementation of k-DPP that accounts for probabilistic data access by recasting the objective function with a regularization term that remains well-posed and decomposes into a geometric diversity term and unreliability cost. The resulting objective facilitates robust selection of diverse data batches under uncertainty. Furthermore, we frame this reliability-aware diversity maximization as a combinatorial semi-bandit problem and propose a UCB-style algorithm to efficiently learn the unknown reliability online. Theoretical analysis provides regret bounds for the proposed approach, ensuring performance guarantees.

</details>


### [645] [Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version)](https://arxiv.org/abs/2211.11434)
*Lucas Lange,Maja Schneider,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: 本文提出了一种满足差分隐私（DP）的机器学习方法用于COVID-19图像分类，通过黑盒成员推断攻击（MIA）实证评估实际隐私保护效果，发现DP对MIA防御效果有限，强调任务依赖的实际威胁建模与攻击导向的隐私估计的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有面向COVID-19的差分隐私模型存在数据集小、隐私保证弱或不明确、缺乏实际隐私评估等问题，亟需在实用层面提升隐私-效用权衡分析。

Method: 改进差分隐私训练以应对类别不平衡；在更严格隐私预算下系统评估效用-隐私权衡；采用黑盒成员推断攻击（MIA）实证估计实际隐私泄露程度。

Result: 实验表明：所需隐私水平因MIA实际威胁而异；随着DP强度增强，实证隐私泄露仅边际改善，DP对MIA防御效果有限；攻击特定的隐私估计有助于优化实用隐私。

Conclusion: 差分隐私在COVID-19影像分类中对实际MIA防护作用有限，应转向任务驱动、攻击导向的实证隐私评估与调优策略。

Abstract: Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat from MIAs. The results further suggest that with increasing DP guarantees, empirical privacy leakage only improves marginally, and DP therefore appears to have a limited impact on practical MIA defense. Our findings identify possibilities for better utility-privacy trade-offs, and we believe that empirical attack-specific privacy estimation can play a vital role in tuning for practical privacy.

</details>


### [646] [GAPNet: Plug-in Jointly Learning Task-Specific Graph for Dynamic Stock Relation](https://arxiv.org/abs/2602.00888)
*Yingjie Niu,Lanxin Lu,Changhong Jin,Ruihai Dong*

Main category: cs.LG

TL;DR: 本文提出GAPNet，一种图自适应插件网络，用于金融预测任务，通过端到端联合学习任务特定的图拓扑结构与节点表示，提升模型在噪声高、异步性强的Web信号下的泛化性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义图结构建模股票间关系，但Web信号具有高噪声、异步性和获取困难等特性，导致图结构与下游任务不匹配、泛化性差。

Method: 提出GAPNet，包含空间感知层（捕捉资产短期协同变动）和时间感知层（维持分布偏移下的长期依赖），可即插即用地适配现有图神经网络（如RT-GCN、CI-STHPAN）并动态重连边拓扑。

Result: 在两个真实股票数据集上，GAPNet显著提升盈利性与稳定性：RT-GCN年化累计收益达0.47，CI-STHPAN达0.63；夏普比率峰值分别达2.20和2.12。

Conclusion: 联合学习图结构与表示对任务特定的关系建模至关重要，GAPNet的即插即用设计使其适用于多种GNN架构。

Abstract: The advent of the web has led to a paradigm shift in the financial relations, with the real-time dissemination of news, social discourse, and financial filings contributing significantly to the reshaping of financial forecasting. The existing methods rely on establishing relations a priori, i.e. predefining graphs to capture inter-stock relationships. However, the stock-related web signals are characterised by high levels of noise, asynchrony, and challenging to obtain, resulting in poor generalisability and non-alignment between the predefined graphs and the downstream tasks. To address this, we propose GAPNet, a Graph Adaptation Plug-in Network that jointly learns task-specific topology and representations in an end-to-end manner. GAPNet attaches to existing pairwise graph or hypergraph backbone models, enabling the dynamic adaptation and rewiring of edge topologies via two complementary components: a Spatial Perception Layer that captures short-term co-movements across assets, and a Temporal Perception Layer that maintains long-term dependency under distribution shift. Across two real-world stock datasets, GAPNet has been shown to consistently enhance the profitability and stability in comparision to the state-of-the-art models, yielding annualised cumulative returns of up to 0.47 for RT-GCN and 0.63 for CI-STHPAN, with peak Sharpe Ratio of 2.20 and 2.12 respectively. The plug-and-play design of GAPNet ensures its broad applicability to diverse GNN-based architectures. Our results underscore that jointly learning graph structures and representations is essential for task-specific relational modeling.

</details>


### [647] [Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection](https://arxiv.org/abs/2401.13327)
*Lucas Lange,Nils Wenzlitschke,Erhard Rahm*

Main category: cs.LG

TL;DR: 本文提出了一种结合生成对抗网络（GAN）与差分隐私（DP）的隐私保护型多传感器智能手表健康数据合成方法，用于压力检测任务，在保证数据隐私的同时提升了模型性能，尤其在小样本场景下效果显著。


<details>
  <summary>Details</summary>
Motivation: 智能手表健康传感器数据包含敏感个人信息，且真实数据获取成本高、难度大，亟需一种既能保护隐私又能提升数据可用性的合成方案。

Method: 采用GAN生成多传感器智能手表健康数据，并引入差分隐私机制保障合成过程的隐私安全；通过多种GAN模型和数据增强策略生成合成数据，并在真实压力检测任务上进行验证。

Result: 在差分隐私训练场景下F1-score提升11.90–15.48%，非隐私场景下仍提升0.45%；合成数据质量经评估具备完整性与合理性，但随隐私预算收紧而下降。

Conclusion: 差分隐私下的GAN合成数据可在隐私与效用间取得良好平衡，尤其适用于真实样本稀缺的医疗健康研究场景。

Abstract: Smartwatch health sensor data are increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprise sensitive personal information and are resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress, employing Generative Adversarial Networks (GANs) and Differential Privacy (DP) safeguards. Our method not only protects patient information but also enhances data availability for research. To ensure its usefulness, we test synthetic data from multiple GANs and employ different data enhancement strategies on an actual stress detection task. Our GAN-based augmentation methods demonstrate significant improvements in model performance, with private DP training scenarios observing an 11.90-15.48% increase in F1-score, while non-private training scenarios still see a 0.45% boost. These results underline the potential of differentially private synthetic data in optimizing utility-privacy trade-offs, especially with the limited availability of real training samples. Through rigorous quality assessments, we confirm the integrity and plausibility of our synthetic data, which, however, are significantly impacted when increasing privacy requirements.

</details>


### [648] [Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning](https://arxiv.org/abs/2409.01329)
*Lucas Lange,Maurice-Maximilian Heykeroth,Erhard Rahm*

Main category: cs.LG

TL;DR: This paper investigates how image dataset characteristics affect the utility and privacy trade-off in differentially private CNN models, finding that imbalanced datasets increase vulnerability (especially for minority classes), while fewer classes improve both utility and privacy; high entropy or low Fisher Discriminant Ratio worsen the trade-off.


<details>
  <summary>Details</summary>
Motivation: ML models trained on sensitive data are vulnerable to privacy attacks; PPML with DP aims to balance utility and privacy, but dataset-specific factors influencing this trade-off remain underexplored.

Method: Empirical analysis of multiple image datasets under varying privacy budgets, evaluating utility (e.g., accuracy) and vulnerability (e.g., membership inference) of private and non-private CNNs, and correlating results with dataset characteristics such as class balance, number of classes, entropy, and Fisher Discriminant Ratio.

Result: Imbalanced datasets increase vulnerability—especially for minority classes—but DP mitigates this; fewer classes improve both utility and privacy; high entropy or low FDR degrades the utility-privacy trade-off.

Conclusion: Dataset characteristics significantly influence the utility-privacy trade-off in DP-CNNs; practitioners can leverage these insights to guide data preprocessing and privacy parameter selection for better outcomes.

Abstract: Machine Learning (ML) is crucial in many sectors, including computer vision. However, ML models trained on sensitive data face security challenges, as they can be attacked and leak information. Privacy-Preserving Machine Learning (PPML) addresses this by using Differential Privacy (DP) to balance utility and privacy. This study identifies image dataset characteristics that affect the utility and vulnerability of private and non-private Convolutional Neural Network (CNN) models. Through analyzing multiple datasets and privacy budgets, we find that imbalanced datasets increase vulnerability in minority classes, but DP mitigates this issue. Datasets with fewer classes improve both model utility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR) datasets deteriorate the utility-privacy trade-off. These insights offer valuable guidance for practitioners and researchers in estimating and optimizing the utility-privacy trade-off in image datasets, helping to inform data and privacy modifications for better outcomes based on dataset characteristics.

</details>


### [649] [Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing](https://arxiv.org/abs/2602.00906)
*Anxin Guo,Jingwei Li*

Main category: cs.LG

TL;DR: 本文将大语言模型对随机事实的幻觉问题形式化为成员测试问题，结合布隆过滤器的离散误差度量与LLM的连续对数损失，提出一个率失真定理：最优记忆效率由事实与非事实得分分布间的最小KL散度刻画；理论表明，即使在理想条件下，受限容量下的信息论最优策略仍会导致幻觉，实验证明其是损失压缩的自然结果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常对缺乏可推断模式的“随机事实”以高置信度产生幻觉，需从信息论角度理解其根本成因。

Method: 将幻觉建模为稀疏事实空间下的成员测试问题，引入率失真分析框架，以KL散度刻画最优记忆效率，并在合成数据上进行实证验证。

Result: 建立了刻画幻觉本质的率失真定理，证明幻觉是有限模型容量下损失压缩的信息论必然结果，且在合成实验中得到验证。

Conclusion: 幻觉并非训练缺陷或数据噪声所致，而是模型在容量受限时实现最优泛化所采取的信息论最优策略，即主动赋予部分非事实以高置信度。

Abstract: Large language models often hallucinate with high confidence on "random facts" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified "closed world" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.

</details>


### [650] [Federated Learning With Individualized Privacy Through Client Sampling](https://arxiv.org/abs/2501.17634)
*Lucas Lange,Ole Borchardt,Erhard Rahm*

Main category: cs.LG

TL;DR: 本文提出了一种适用于联邦学习的个性化差分隐私（IDP）方法，通过扩展SAMPLE算法，根据客户端异构的隐私预算动态调整采样率，并在IDP-FedAvg中实现；实验表明其在隐私-效用权衡上优于统一DP基线和SCALE方法，但在非独立同分布复杂任务中仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 应对用户数据收集引发的隐私担忧，需兼顾多样化用户隐私偏好与模型效用，传统统一差分隐私无法满足个体化需求。

Method: 将集中式SAMPLE算法适配至联邦学习框架，依据各客户端异构的隐私预算计算个性化采样率，并嵌入改进的IDP-FedAvg算法中。

Result: 在多种真实隐私分布和数据集上验证，该方法显著优于统一DP基线及SCALE方法，缓解了隐私与效用间的权衡；但在非i.i.d.复杂任务中受限于去中心化环境约束。

Conclusion: 个性化差分隐私在联邦学习中可行且有效，但需进一步解决非独立同分布数据下的性能瓶颈。

Abstract: With growing concerns about user data collection, individualized privacy has emerged as a promising solution to balance protection and utility by accounting for diverse user privacy preferences. Instead of enforcing a uniform level of anonymization for all users, this approach allows individuals to choose privacy settings that align with their comfort levels. Building on this idea, we propose an adapted method for enabling Individualized Differential Privacy (IDP) in Federated Learning (FL) by handling clients according to their personal privacy preferences. By extending the SAMPLE algorithm from centralized settings to FL, we calculate client-specific sampling rates based on their heterogeneous privacy budgets and integrate them into a modified IDP-FedAvg algorithm. We test this method under realistic privacy distributions and multiple datasets. The experimental results demonstrate that our approach achieves clear improvements over uniform DP baselines, reducing the trade-off between privacy and utility. Compared to the alternative SCALE method in related work, which assigns differing noise scales to clients, our method performs notably better. However, challenges remain for complex tasks with non-i.i.d. data, primarily stemming from the constraints of the decentralized setting.

</details>


### [651] [PyGALAX: An Open-Source Python Toolkit for Advanced Explainable Geospatial Machine Learning](https://arxiv.org/abs/2602.00907)
*Pingping Wang,Yihong Yuan,Lingcheng Li,Yongmei Lu*

Main category: cs.LG

TL;DR: PyGALAX是一个集成AutoML与XAI的Python地理空间分析工具包，支持自动模型选择、带宽与核函数优化，并通过SHAP提供可解释性，显著提升对空间非平稳性的建模能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统地理加权回归（GWR）方法在建模空间异质性和非平稳性方面的局限性，提升地理空间机器学习模型的自动化程度、灵活性与可解释性。

Method: 基于GALAX框架，引入自动带宽选择、灵活核函数选择机制，结合AutoML进行多地点模型优化，并集成SHAP进行局部与全局可解释性分析。

Result: PyGALAX在回归与分类任务中优于传统GWR方法，具备更强的鲁棒性与适应性；提供易用、可复现、可部署的Python工具包，支持跨学科应用。

Conclusion: PyGALAX成功将AutoML与XAI深度融合于地理空间建模，兼顾性能与透明度，推动了可解释地理智能的发展与普及。

Abstract: PyGALAX is a Python package for geospatial analysis that integrates automated machine learning (AutoML) and explainable artificial intelligence (XAI) techniques to analyze spatial heterogeneity in both regression and classification tasks. It automatically selects and optimizes machine learning models for different geographic locations and contexts while maintaining interpretability through SHAP (SHapley Additive exPlanations) analysis. PyGALAX builds upon and improves the GALAX framework (Geospatial Analysis Leveraging AutoML and eXplainable AI), which has proven to outperform traditional geographically weighted regression (GWR) methods. Critical enhancements in PyGALAX from the original GALAX framework include automatic bandwidth selection and flexible kernel function selection, providing greater flexibility and robustness for spatial modeling across diverse datasets and research questions. PyGALAX not only inherits all the functionalities of the original GALAX framework but also packages them into an accessible, reproducible, and easily deployable Python toolkit while providing additional options for spatial modeling. It effectively addresses spatial non-stationarity and generates transparent insights into complex spatial relationships at both global and local scales, making advanced geospatial machine learning methods accessible to researchers and practitioners in geography, urban planning, environmental science, and related fields.

</details>


### [652] [Reinforcement Learning via Conservative Agent for Environments with Random Delays](https://arxiv.org/abs/2507.18992)
*Jongsoo Lee,Jangwon Kim,Jiseok Jeong,Soohee Han*

Main category: cs.LG

TL;DR: 本文提出了一种名为'保守智能体'的新方法，将随机延迟环境转化为等效的恒定延迟环境，从而可直接复用现有恒定延迟强化学习算法，显著提升在连续控制任务中的渐近性能与样本效率。


<details>
  <summary>Details</summary>
Motivation: 现实强化学习应用常受环境反馈延迟影响，而现有方法主要针对恒定延迟，对更具挑战性的随机延迟环境研究不足。

Method: 提出'保守智能体'，通过将随机延迟环境建模并转化为等效的恒定延迟环境，使任意先进恒定延迟算法可无缝迁移使用，无需修改算法结构。

Result: 在连续控制任务上的实验表明，该方法在渐近性能和样本效率上均显著优于现有基线算法。

Conclusion: 保守智能体提供了一种简单、鲁棒且通用的框架，有效拓展了恒定延迟强化学习方法至随机延迟场景，具备良好实用性与泛化能力。

Abstract: Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.

</details>


### [653] [Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment](https://arxiv.org/abs/2602.00910)
*Cuong Manh Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: This paper reviews efficient and lightweight deep learning architectures for medical image analysis, categorizing them into CNNs, Lightweight Transformers, and Linear Complexity Models, and evaluates model compression techniques to balance diagnostic performance and hardware efficiency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in deploying large-scale deep learning models in clinical settings—such as high computational costs, latency, and patient data privacy concerns—this review aims to synthesize efficient, lightweight architectures suitable for resource-constrained environments.

Method: The paper conducts a comprehensive literature review, categorizing efficient deep learning models into three streams (CNNs, Lightweight Transformers, Linear Complexity Models) and analyzing key model compression strategies (pruning, quantization, knowledge distillation, low-rank factorization).

Result: A structured taxonomy of efficient medical AI models and compression methods is presented, along with an evaluation of their trade-offs in diagnostic accuracy and hardware efficiency; limitations and directions toward on-device intelligence are identified.

Conclusion: Efficient and lightweight deep learning models, combined with effective compression techniques, hold great promise for enabling high-performance AI in real-world clinical practice—especially where computational resources and data privacy are critical.

Abstract: Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.

</details>


### [654] [Early Classification of Time Series in Non-Stationary Cost Regimes](https://arxiv.org/abs/2602.00918)
*Aurélien Renault,Alexis Bondu,Antoine Cornuéjols,Vincent Lemaire*

Main category: cs.LG

TL;DR: 本文研究了时间序列早期分类（ECTS）在决策成本非平稳性下的鲁棒性问题，提出在线学习方法（包括bandit和RL策略）动态适应成本漂移，实验表明RL方法在不同成本场景下表现稳定且鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 现有ECTS方法假设决策成本已知、固定且准确，但实际中成本常不确定且随时间变化，导致训练与部署目标不一致。

Method: 将代表性ECTS方法扩展至在线学习框架，聚焦可分离方法，仅在部署时更新触发模型、保持分类器固定；提出基于bandit和强化学习（RL）的在线自适应策略。

Result: 在合成数据上的受控实验表明，在线学习能有效提升ECTS对成本漂移的鲁棒性，其中RL策略在多种成本变化场景下表现出强而稳定的性能。

Conclusion: 在线学习，特别是RL方法，是应对ECTS中成本非平稳性的有效途径，显著提升了实际部署中的适应性与可靠性。

Abstract: Early Classification of Time Series (ECTS) addresses decision-making problems in which predictions must be made as early as possible while maintaining high accuracy. Most existing ECTS methods assume that the time-dependent decision costs governing the learning objective are known, fixed, and correctly specified. In practice, however, these costs are often uncertain and may change over time, leading to mismatches between training-time and deployment-time objectives. In this paper, we study ECTS under two practically relevant forms of cost non-stationarity: drift in the balance between misclassification and decision delay costs, and stochastic realizations of decision costs that deviate from the nominal training-time model. To address these challenges, we revisit representative ECTS approaches and adapt them to an online learning setting. Focusing on separable methods, we update only the triggering model during deployment, while keeping the classifier fixed. We propose several online adaptations and baselines, including bandit-based and RL-based approaches, and conduct controlled experiments on synthetic data to systematically evaluate robustness under cost non-stationarity. Our results demonstrate that online learning can effectively improve the robustness of ECTS methods to cost drift, with RL-based strategies exhibiting strong and stable performance across varying cost regimes.

</details>


### [655] [Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision](https://arxiv.org/abs/2602.00927)
*Yihao Xue,Allan Zhang,Jianhao Huang,Amit Sahai,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 本文发现，在仅基于结果的监督下，增加训练时推理长度（如RL中的token预算或循环Transformer中的循环次数）可以持续提升模型在分布外（OOD）任务上的性能，即使其在分布内（ID）任务上的性能已饱和；作者从理论和实验两方面解释了这一现象，并指出OOD鲁棒性可能需要比ID验证所建议更大的推理预算。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型（LLM）的长程推理能力是解决复杂问题的关键，但现有工作多关注ID性能，忽视了OOD泛化与推理长度的关系。本文旨在探究训练时推理长度对OOD鲁棒性的影响及其内在机制。

Method: 理论分析（提出两种机制：自迭代增强归纳偏置、自迭代削弱捷径依赖） + 实验验证（在循环Transformer的合成任务和RL微调LLM的数学推理任务上分别扩展推理长度）。

Result: 实验证实：随训练时推理长度增加，OOD性能持续提升而ID性能已饱和；理论表明自迭代可通过重塑假设空间或抑制捷径解来提升OOD泛化。

Conclusion: 推理长度不仅是提升ID性能的缩放维度，更是提升OOD鲁棒性的关键因素；单纯依赖ID验证会低估所需推理预算，需在训练设计中显式考虑OOD目标。

Abstract: Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.

</details>


### [656] [Continuous-Utility Direct Preference Optimization](https://arxiv.org/abs/2602.00931)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zihao He,Muhammad Usman Rafique,Asad Aali,Muhammad Ali Jamshed,John M. Cioffi,Emily Fox*

Main category: cs.LG

TL;DR: 本文提出CU-DPO框架，用连续分数替代二元偏好标签，以对齐模型与多种认知策略，在数学推理任务中显著提升策略选择准确率和下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理能力评估依赖二元偏好监督，无法反映推理过程中的部分进展或细粒度质量差异。

Method: 提出Continuous Utility Direct Preference Optimization（CU-DPO）框架，包含两阶段训练：（i）策略选择（best-vs-all比较），（ii）执行优化（margin-stratified配对训练）；理论证明其样本复杂度优势及DPO收敛性。

Result: 在七个基础模型上，策略选择准确率从35–46%提升至68–78%，下游推理性能最高提升6.6分，并有效泛化至分布外任务。

Conclusion: CU-DPO通过引入连续效用信号和分阶段训练，显著提升了模型对多样化认知策略的学习与执行能力，为细粒度推理对齐提供了新范式。

Abstract: Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a portfolio of prompt-based cognitive strategies by replacing binary labels with continuous scores that capture fine-grained reasoning quality. We prove that learning with K strategies yields a Theta(K log K) improvement in sample complexity over binary preferences, and that DPO converges to the entropy-regularized utility-maximizing policy. To exploit this signal, we propose a two-stage training pipeline: (i) strategy selection, which optimizes the model to choose the best strategy for a given problem via best-vs-all comparisons, and (ii) execution refinement, which trains the model to correctly execute the selected strategy using margin-stratified pairs. On mathematical reasoning benchmarks, CU-DPO improves strategy selection accuracy from 35-46 percent to 68-78 percent across seven base models, yielding consistent downstream reasoning gains of up to 6.6 points on in-distribution datasets with effective transfer to out-of-distribution tasks.

</details>


### [657] [SALAAD: Sparse And Low-Rank Adaptation via ADMM](https://arxiv.org/abs/2602.00942)
*Hao Ma,Melis Ilayda Bal,Liang Zhang,Bingcong Li,Niao He,Melanie Zeilinger,Michael Muehlebach*

Main category: cs.LG

TL;DR: SALAAD is a plug-and-play framework that dynamically induces sparse and low-rank structures in LLMs during training, enabling elastic model capacity control without retraining.


<details>
  <summary>Details</summary>
Motivation: Modern LLMs face compute and memory constraints; existing sparse/low-rank methods lack generality, ignore heterogeneity, or require architecture changes.

Method: Formulates structured weight learning via augmented Lagrangian optimization with an adaptive controller balancing training loss and structural constraints.

Result: Reduces deployment memory significantly while matching ad-hoc methods' performance; yields continuous capacity spectrum from one training run.

Conclusion: SALAAD enables stable, architecture-agnostic, and elastic capacity control for efficient LLM deployment.

Abstract: Modern large language models are increasingly deployed under compute and memory constraints, making flexible control of model capacity a central challenge. While sparse and low-rank structures naturally trade off capacity and performance, existing approaches often rely on heuristic designs that ignore layer and matrix heterogeneity or require model-specific architectural modifications. We propose SALAAD, a plug-and-play framework applicable to different model architectures that induces sparse and low-rank structures during training. By formulating structured weight learning under an augmented Lagrangian framework and introducing an adaptive controller that dynamically balances the training loss and structural constraints, SALAAD preserves the stability of standard training dynamics while enabling explicit control over the evolution of effective model capacity during training. Experiments across model scales show that SALAAD substantially reduces memory consumption during deployment while achieving performance comparable to ad-hoc methods. Moreover, a single training run yields a continuous spectrum of model capacities, enabling smooth and elastic deployment across diverse memory budgets without the need for retraining.

</details>


### [658] [Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems](https://arxiv.org/abs/2602.00943)
*Zhenyu Zhao,David Zhang,Ellie Zhao,Ehsan Saberian*

Main category: cs.LG

TL;DR: 本文提出动态先验汤普森采样（DPTS），通过设计可控的先验分布，解决推荐系统冷启动中因乐观先验导致的过度探索问题，实现可预测、可调节的探索强度，并在模拟与线上实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 冷启动场景下，新物品缺乏数据，传统汤普森采样使用均匀Beta(1,1)先验（隐含50%成功率假设），当真实基线成功率远低于50%时，该乐观先验会导致对弱项的系统性过量分配；加之批量更新与管道延迟，新物品长时间处于‘无数据’状态，先验主导分配，加剧问题。

Method: 提出动态先验汤普森采样（DPTS），推导出一个闭式二次解，用于计算先验均值，使得新臂j优于当前最优臂k的概率P(X_j > Y_k)在引入时刻精确等于预设阈值epsilon，从而直接调控探索强度，同时保持汤普森采样的贝叶斯更新框架。

Result: 在蒙特卡洛验证、离线批量仿真及面向数百万用户的缩略图个性化系统大规模线上实验中，动态先验相比均匀先验基线，实现了更精准的探索控制和更高的推荐效率。

Conclusion: 动态先验设计为冷启动探索提供了理论可控、实践高效的新范式，使探索强度可显式设定与预测，显著缓解了传统方法因先验失配导致的资源浪费与用户体验下降问题。

Abstract: Cold-start exploration is a core challenge in large-scale recommender systems: new or data-sparse items must receive traffic to estimate value, but over-exploration harms users and wastes impressions. In practice, Thompson Sampling (TS) is often initialized with a uniform Beta(1,1) prior, implicitly assuming a 50% success rate for unseen items. When true base rates are far lower, this optimistic prior systematically over-allocates to weak items. The impact is amplified by batched policy updates and pipeline latency: for hours, newly launched items can remain effectively "no data," so the prior dominates allocation before feedback is incorporated. We propose Dynamic Prior Thompson Sampling, a prior design that directly controls the probability that a new arm outcompetes the incumbent winner. Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time, making exploration intensity predictable and tunable while preserving TS Bayesian updates. Across Monte Carlo validation, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors deliver precise exploration control and improved efficiency versus a uniform-prior baseline.

</details>


### [659] [Optimal Budgeted Adaptation of Large Language Models](https://arxiv.org/abs/2602.00952)
*Jing Wang,Jie Shen,Dean Foster,Zohar Karnin,Jeremy C Weiss*

Main category: cs.LG

TL;DR: 本文提出了一种面向预算感知的监督微调框架，将大语言模型适配建模为上下文Stackelberg博弈，兼顾标签效率与下游准确率，并给出理论遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型微调中标签数据可用性与下游任务精度之间的权衡问题，提升标签使用效率。

Method: 将LLM适配建模为上下文Stackelberg博弈，学习者作为领导者设定评分策略和标签查询策略，环境作为跟随者选择具有挑战性的监督样本；引入有限标注预算到目标函数中；在全反馈设定下设计算法，并结合Largest-Latency-First（LLF）置信门实现选择性标签查询。

Result: 在标准线性上下文假设下，基础算法获得Õ(d√T)遗憾界；引入LLF置信门后，获得预算感知的遗憾界Õ(√(dB) + c√B)，其中B=βT。

Conclusion: 该框架为预算受限下的LLM监督微调提供了理论严谨且实用的解决方案，显著提升了标注效率与性能平衡。

Abstract: The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\tilde{O}(d\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\tilde{O}(\sqrt{dB} + c\sqrt{B})$ with $B=βT$.

</details>


### [660] [SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery](https://arxiv.org/abs/2602.00953)
*Sahar Almahfouz Nasser,Juan Francisco Pesantez Borja,Jincheng Liu,Tanvir Hasan,Zenghan Wang,Suman Ghosh,Sandeep Manandhar,Shikhar Shiromani,Twisha Shah,Naoto Tokuyama,Anant Madabhushi*

Main category: cs.LG

TL;DR: 本文提出SAGE系统，通过结合文献驱动推理与多模态数据分析，系统性地发现并验证具有生物学依据的可解释病理影像生物标志物，以提升计算病理学模型的临床可解释性与可转化性。


<details>
  <summary>Details</summary>
Motivation: 现有AI病理模型缺乏可解释性，阻碍临床应用；而传统手工设计的影像生物标志物又常缺乏系统性生物学验证。

Method: 构建名为SAGE的结构化智能体系统，整合文献锚定推理与多模态数据（如组织图像、基因表达、临床结局）分析，协调多个专业化智能体完成生物背景建模与假设实证验证。

Result: SAGE能自动识别并优先推荐具备生物学支持、可解释且与临床结局相关联的工程化病理生物标志物。

Conclusion: SAGE为连接影像特征与分子机制提供了可解释、可验证的桥梁，推动计算病理学向临床可信部署迈进。

Abstract: Despite significant progress in computational pathology, many AI models remain black-box and difficult to interpret, posing a major barrier to clinical adoption due to limited transparency and explainability. This has motivated continued interest in engineered image-based biomarkers, which offer greater interpretability but are often proposed based on anecdotal evidence or fragmented prior literature rather than systematic biological validation. We introduce SAGE (Structured Agentic system for hypothesis Generation and Evaluation), an agentic AI system designed to identify interpretable, engineered pathology biomarkers by grounding them in biological evidence. SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers, such as gene expression, and clinically relevant outcomes. By coordinating specialized agents for biological contextualization and empirical hypothesis validation, SAGE prioritizes transparent, biologically supported biomarkers and advances the clinical translation of computational pathology.

</details>


### [661] [From drift to adaptation to the failed ml model: Transfer Learning in Industrial MLOps](https://arxiv.org/abs/2602.00957)
*Waqar Muhammad Ashraf,Talha Ansar,Fahad Ahmed,Jawad Hussain,Muhammad Mujtaba Abbas,Vivek Dua*

Main category: cs.LG

TL;DR: 本文比较了三种迁移学习策略（ETL、ALTL、LLTL）在数据漂移下更新失效ANN模型的效果，基于火电厂空气预热器烟气压差的批处理工业案例，发现ETL在5天批次上精度更高，ALTL更适合8天大批次，计算开销随批次变化呈混合趋势，为MLOps中模型自适应提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 现有MLOps中缺乏系统性框架来应对数据漂移导致的模型失效，亟需研究适用于工业批处理场景的模型更新方法。

Method: 对比分析三种迁移学习策略——集成迁移学习（ETL）、全层迁移学习（ALTL）和末层迁移学习（LLTL），用于更新在数据漂移下失效的前馈人工神经网络（ANN）模型，并以660MW火电厂空气预热器烟气压差为案例进行实证。

Result: ETL在5天批次上预测精度高于LLTL和ALTL；ALTL在8天大批次下模型更新更有效；不同批次下各方法的超参调优与训练计算开销呈现混合趋势。

Conclusion: 迁移学习策略的选择应匹配实际数据批次大小，ETL与ALTL分别适用于中小与大批次工业场景，该结论可指导MLOps实践中模型对数据漂移的自适应更新。

Abstract: Model adaptation to production environment is critical for reliable Machine Learning Operations (MLOps), less attention is paid to developing systematic framework for updating the ML models when they fail under data drift. This paper compares the transfer learning enabled model update strategies including ensemble transfer learning (ETL), all-layers transfer learning (ALTL), and last-layer transfer learning (LLTL) for updating the failed feedforward artificial neural network (ANN) model. The flue gas differential pressure across the air preheater unit installed in a 660 MW thermal power plant is analyzed as a case study since it mimics the batch processes due to load cycling in the power plant. Updating the failed ANN model by three transfer learning techniques reveals that ETL provides relatively higher predictive accuracy for the batch size of 5 days than those of LLTL and ALTL. However, ALTL is found to be suitable for effective update of the model trained on large batch size (8 days). A mixed trend is observed for computational requirement (hyperparameter tuning and model training) of model update techniques for different batch sizes. These fundamental and empiric insights obtained from the batch process-based industrial case study can assist the MLOps practitioners in adapting the failed models to data drifts for the accurate monitoring of industrial processes.

</details>


### [662] [Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction](https://arxiv.org/abs/2602.00959)
*Yuheng Yang,Siqi Zhu,Tao Feng,Ge Liu,Jiaxuan You*

Main category: cs.LG

TL;DR: 本文提出了一种交互式智能体框架，用于系统性地提取和量化大语言模型（LLMs）所含知识，通过四种自适应探索策略与三阶段知识处理流程，揭示了知识扩展规律、Pass@1与Pass@k权衡关系及不同模型家族的知识分布差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准静态且难以支持系统性知识探测，而LLM作为压缩知识库，其真实知识内容与边界尚不明确。

Method: 提出交互式智能体框架，包含四种自适应知识探索策略；设计三阶段知识处理流程：向量去重、LLM语义裁决、领域相关性审计。

Result: 发现递归分类法是最有效的探索策略；观察到知识随模型规模扩大的缩放律；识别出domain-specialized与general-purpose模型在Pass@1与Pass@k上的性能权衡；证实训练数据构成差异导致可测量的知识特征差异。

Conclusion: 该框架为LLM知识探测提供了可扩展、可量化、细粒度的新范式，有助于深入理解模型知识结构与演化机制。

Abstract: Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.

</details>


### [663] [Multimodal Scientific Learning Beyond Diffusions and Flows](https://arxiv.org/abs/2602.00960)
*Leonardo Ferreira Guilhoto,Akshat Kaushal,Paris Perdikaris*

Main category: cs.LG

TL;DR: 本文提出Mixture Density Networks (MDNs)作为科学机器学习中多模态不确定性量化的有效替代方案，相比数据密集、计算昂贵的隐式生成模型，MDNs具有更强的数据效率、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习中需要建模由不适定反问题、多稳态和混沌动力学引起的多模态条件不确定性，而现有隐式生成模型（如扩散模型、流模型）存在数据需求高、计算成本大、与科学问题结构不匹配等问题。

Method: 采用显式的参数化密度估计器MDNs，利用其对低维多模态物理问题的归纳偏置，直接在不同解分支上分配概率质量，并构建统一的概率框架对比显式与隐式分布网络。

Result: MDNs在多种科学回归任务（反问题、多稳态、混沌系统）中展现出更优的泛化性、可解释性和样本效率，尤其在数据稀缺时能可靠恢复分离的模态。

Conclusion: MDNs是一种被低估但原理坚实、适用于科学机器学习中多模态不确定性量化的有力工具，兼具结构合理性与实用优势。

Abstract: Scientific machine learning (SciML) increasingly requires models that capture multimodal conditional uncertainty arising from ill-posed inverse problems, multistability, and chaotic dynamics. While recent work has favored highly expressive implicit generative models such as diffusion and flow-based methods, these approaches are often data-hungry, computationally costly, and misaligned with the structured solution spaces frequently found in scientific problems. We demonstrate that Mixture Density Networks (MDNs) provide a principled yet largely overlooked alternative for multimodal uncertainty quantification in SciML. As explicit parametric density estimators, MDNs impose an inductive bias tailored to low-dimensional, multimodal physics, enabling direct global allocation of probability mass across distinct solution branches. This structure delivers strong data efficiency, allowing reliable recovery of separated modes in regimes where scientific data is scarce. We formalize these insights through a unified probabilistic framework contrasting explicit and implicit distribution networks, and demonstrate empirically that MDNs achieve superior generalization, interpretability, and sample efficiency across a range of inverse, multistable, and chaotic scientific regression tasks.

</details>


### [664] [On the Spectral Flattening of Quantized Embeddings](https://arxiv.org/abs/2602.00969)
*Junlin Huang,Wenyi Fang,Zhenheng Tang,Yuxin Wang,Xueze Kang,Yang Zheng,Bo Li,Xiaowen Chu*

Main category: cs.LG

TL;DR: 本文揭示了超低精度训练大语言模型（LLMs）不稳定的根源在于离散量化约束与语言数据固有的重尾谱特性之间的冲突；通过将Zipf定律与随机矩阵理论联系起来，证明嵌入奇异值谱的幂律衰减是语义编码的基本要求，并指出均匀量化会破坏该谱尾、导致表征坍塌；最终确立谱保真度是稳定低比特优化的必要条件。


<details>
  <summary>Details</summary>
Motivation: 超低精度训练大语言模型面临严重不稳定性问题，其根源在于量化离散性与语言数据重尾谱特性的内在冲突。

Method: 通过形式化Zipfian统计与随机矩阵理论的联系，证明嵌入奇异值谱的幂律衰减是语义编码的基本要求；推导均匀量化的理论噪声界，分析其对谱尾的截断效应及对稳定秩的影响；在GPT-2、TinyLlama等模型上进行实证验证。

Result: 均匀量化会引入不成比例地截断奇异值谱尾部的噪声，导致谱扁平化和表征稳定秩严格增加，进而引发表征坍塌；实证结果跨架构一致支持该结论。

Conclusion: 谱保真度是实现稳定低比特LLM优化的必要条件；本工作首次从谱几何角度定量刻画了LLM对量化的敏感性。

Abstract: Training Large Language Models (LLMs) at ultra-low precision is critically impeded by instability rooted in the conflict between discrete quantization constraints and the intrinsic heavy-tailed spectral nature of linguistic data. By formalizing the connection between Zipfian statistics and random matrix theory, we prove that the power-law decay in the singular value spectra of embeddings is a fundamental requisite for semantic encoding. We derive theoretical bounds showing that uniform quantization introduces a noise floor that disproportionately truncates this spectral tail, which induces spectral flattening and a strictly provable increase in the stable rank of representations. Empirical validation across diverse architectures including GPT-2 and TinyLlama corroborates that this geometric degradation precipitates representational collapse. This work not only quantifies the spectral sensitivity of LLMs but also establishes spectral fidelity as a necessary condition for stable low-bit optimization.

</details>


### [665] [Forest-Guided Semantic Transport for Label-Supervised Manifold Alignment](https://arxiv.org/abs/2602.00974)
*Adrien Aumon,Myriam Lizotte,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: FoSTA是一种利用森林诱导几何结构来去噪域内关系、恢复任务相关流形，并通过分层语义传输进行多模态数据对齐的新方法，在合成数据和单细胞数据分析中均表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有标签监督的流形对齐方法大多基于欧氏几何建模域内关系，当特征与任务弱相关时易产生噪声和语义误导结构，导致对齐质量下降。

Method: 提出FoSTA框架：首先利用标签信息构建森林亲和图以诱导语义几何结构，从而去噪并恢复任务相关的内在流形；然后通过快速、分层的语义传输实现跨域对齐。

Result: 在合成基准上显著提升对应点恢复和标签迁移效果；在单细胞实际应用（如批次校正、生物学保守性分析）中表现优异。

Conclusion: FoSTA通过引入森林引导的语义几何建模，有效克服了传统欧氏假设的局限性，为标签监督的流形对齐提供了更鲁棒、可扩展且语义更一致的解决方案。

Abstract: Label-supervised manifold alignment bridges the gap between unsupervised and correspondence-based paradigms by leveraging shared label information to align multimodal datasets. Still, most existing methods rely on Euclidean geometry to model intra-domain relationships. This approach can fail when features are only weakly related to the task of interest, leading to noisy, semantically misleading structure and degraded alignment quality. To address this limitation, we introduce FoSTA (Forest-guided Semantic Transport Alignment), a scalable alignment framework that leverages forest-induced geometry to denoise intra-domain structure and recover task-relevant manifolds prior to alignment. FoSTA builds semantic representations directly from label-informed forest affinities and aligns them via fast, hierarchical semantic transport, capturing meaningful cross-domain relationships. Extensive comparisons with established baselines demonstrate that FoSTA improves correspondence recovery and label transfer on synthetic benchmarks and delivers strong performance in practical single-cell applications, including batch correction and biological conservation.

</details>


### [666] [Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees](https://arxiv.org/abs/2602.00987)
*Sawan Kumar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 本文提出随机小波特征（RWF）框架，通过采样小波族构建可扩展的非平稳核近似，克服了传统方法在表达力与计算效率间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 建模非平稳过程是机器学习中的关键挑战，但大多数可扩展方法依赖于平稳性假设，导致在表达力（如深度高斯过程）和可扩展性（如随机傅里叶特征）之间难以兼顾。

Method: 提出随机小波特征（RWF），利用小波的局部化与多分辨率特性构造显式特征映射，实现对输入依赖模式的建模，并提供正定性、无偏性和一致收敛性等理论保证。

Result: 在多个合成与真实数据集上，RWF优于平稳随机特征，在精度-效率权衡上显著优于更复杂模型。

Conclusion: RWF为一大类现实世界中的非平稳问题提供了可扩展且富有表达力的核方法新路径。

Abstract: Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding models like Deep Gaussian Processes, or scalable but limited methods like Random Fourier Features (RFF). We close this gap by introducing Random Wavelet Features (RWF), a framework that constructs scalable, non-stationary kernel approximations by sampling from wavelet families. By harnessing the inherent localization and multi-resolution structure of wavelets, RWF generates an explicit feature map that captures complex, input-dependent patterns. Our framework provides a principled way to generalize RFF to the non-stationary setting and comes with a comprehensive theoretical analysis, including positive definiteness, unbiasedness, and uniform convergence guarantees. We demonstrate empirically on a range of challenging synthetic and real-world datasets that RWF outperforms stationary random features and offers a compelling accuracy-efficiency trade-off against more complex models, unlocking scalable and expressive kernel methods for a broad class of real-world non-stationary problems.

</details>


### [667] [ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning](https://arxiv.org/abs/2602.01003)
*Zhishen Sun,Sizhe Dang,Guang Dai,Haishan Ye*

Main category: cs.LG

TL;DR: 本文提出ESSAM框架，结合进化策略与锐度感知优化，显著降低GPU内存消耗的同时，在数学推理任务上达到与强化学习方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型数学推理能力时存在高GPU内存消耗问题，难以在资源受限环境下应用。

Method: 提出ESSAM（Evolution Strategies with Sharpness-Aware Maximization）全参数微调框架，将零阶参数空间搜索（ES）与锐度感知优化（SAM）紧密结合。

Result: 在GSM8K任务上，ESSAM平均准确率达78.27%，优于PPO（77.72%），媲美GRPO（78.34%）；GPU内存消耗较PPO降低18倍、较GRPO降低10倍。

Conclusion: ESSAM是一种高效、低资源消耗的替代RL的数学推理微调方法，在性能与硬件效率间取得良好平衡。

Abstract: Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpness-Aware Maximization (ESSAM), a full parameter fine-tuning framework that tightly combines the zero-order search in parameter space from Evolution Strategies (ES) with the Sharpness-Aware Maximization (SAM) to improve generalization. We conduct fine-tuning experiments on the mainstream mathematica reasoning task GSM8K. The results show that ESSAM achieves an average accuracy of 78.27\% across all models and its overall performance is comparable to RL methods. It surpasses classic RL algorithm PPO with an accuracy of 77.72\% and is comparable to GRPO with an accuracy of 78.34\%, and even surpassing them on some models. In terms of GPU memory usage, ESSAM reduces the average GPU memory usage by $18\times$ compared to PPO and by $10\times$ compared to GRPO, achieving an extremely low GPU memory usage.

</details>


### [668] [Predicting Anemia Among Under-Five Children in Nepal Using Machine Learning and Deep Learning](https://arxiv.org/abs/2602.01005)
*Deepak Bastola,Pitambar Acharya,Dipak Dulal,Rabina Dhakal,Yang Li*

Main category: cs.LG

TL;DR: 本研究利用尼泊尔2022年人口与健康调查数据，通过多种特征选择方法筛选出5个关键预测儿童贫血的变量，并比较了10种机器学习与深度学习模型的性能，发现逻辑回归在召回率和F1分数上最优，而DNN准确率最高、SVM的AUC最高，表明可解释性特征对尼泊尔公共卫生筛查具有重要价值。


<details>
  <summary>Details</summary>
Motivation: 儿童贫血在尼泊尔仍是重大公共卫生挑战，与生长发育迟缓、认知障碍及发病率升高密切相关，亟需高效、可解释的风险预测工具支持公共卫生干预。

Method: 基于NDHS 2022微数据（n=1855），构建二分类任务（贫血/非贫血）；采用四种特征选择方法（卡方检验、互信息、点双列相关、Boruta）达成多方法共识以筛选稳定特征；对比8种传统机器学习与2种深度学习模型，重点评估F1-score和召回率以应对类别不平衡。

Result: 5个特征（儿童年龄、近期发热、家庭规模、母亲贫血、驱虫史）被所有特征选择方法一致选出；逻辑回归取得最佳召回率（0.701）和F1-score（0.649）；DNN准确率最高（0.709）；SVM AUC最高（0.736）。

Conclusion: 机器学习与深度学习模型均可有效预测儿童贫血；具备临床可解释性的关键特征（如儿童年龄、感染指标、母亲贫血、驱虫史）对风险分层和基层筛查具有实际指导意义。

Abstract: Childhood anemia remains a major public health challenge in Nepal and is associated with impaired growth, cognition, and increased morbidity. Using World Health Organization hemoglobin thresholds, we defined anemia status for children aged 6-59 months and formulated a binary classification task by grouping all anemia severities as \emph{anemic} versus \emph{not anemic}. We analyzed Nepal Demographic and Health Survey (NDHS 2022) microdata comprising 1,855 children and initially considered 48 candidate features spanning demographic, socioeconomic, maternal, and child health characteristics. To obtain a stable and substantiated feature set, we applied four features selection techniques (Chi-square, mutual information, point-biserial correlation, and Boruta) and prioritized features supported by multi-method consensus. Five features: child age, recent fever, household size, maternal anemia, and parasite deworming were consistently selected by all methods, while amenorrhea, ethnicity indicators, and provinces were frequently retained. We then compared eight traditional machine learning classifiers (LR, KNN, DT, RF, XGBoost, SVM, NB, LDA) with two deep learning models (DNN and TabNet) using standard evaluation metrics, emphasizing F1-score and recall due to class imbalance. Among all models, logistic regression attained the best recall (0.701) and the highest F1-score (0.649), while DNN achieved the highest accuracy (0.709), and SVM yielded the strongest discrimination with the highest AUC (0.736). Overall, the results indicate that both machine learning and deep learning models can provide competitive anemia prediction and the interpretable features such as child age, infection proxy, maternal anemia, and deworming history are central for risk stratification and public health screening in Nepal.

</details>


### [669] [LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems](https://arxiv.org/abs/2602.01009)
*Haoran Li,Chenhan Xiao,Lihao Mai,Yang Weng,Erik Blasch*

Main category: cs.LG

TL;DR: 本文提出LASS-ODE模型，通过局部线性ODE表示和跨系统注意力机制（含公共结构中心CSH），解决物理系统动态预测中物理计算可扩展性与知识共享效率两大挑战，在大规模ODE轨迹数据上预训练，支持零样本迁移与微调。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在语言、视觉和时间序列领域取得成功，但在物理系统的动态预测方面进展有限；主要受限于物理约束下的计算可扩展性（如ODE积分开销大）和跨系统知识共享效率低（注意力局限于单系统）。

Method: 提出局部线性ODE token表示以替代昂贵非线性积分，保障物理保真度并提升可扩展性；设计带公共结构中心（CSH）的跨系统注意力机制，实现多系统间ODE结构知识共享。

Result: LASS-ODE在40GB ODE轨迹数据上预训练，展现出优异的域内性能、对多样ODE系统的零样本泛化能力，以及进一步微调带来的性能提升。

Conclusion: 局部线性ODE建模与跨系统结构共享是构建面向物理系统的可扩展基础模型的有效路径，LASS-ODE为此类模型提供了新范式。

Abstract: Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can enforce physical regularization, but its computation (e.g., ODE integration) does not scale to extensive systems. $(ii)$ Knowledge-sharing efficiency: the attention mechanism is primarily computed within each system, which limits the extraction of shared ODE structures across systems. We show that enforcing ODE consistency does not require expensive nonlinear integration: a token-wise locally linear ODE representation preserves physical fidelity while scaling to foundation-model regimes. Thus, we propose novel token representations that respect locally linear ODE evolution. Such linearity substantially accelerates integration while accurately approximating the local data manifold. Second, we introduce a simple yet effective inter-system attention that augments attention with a common structure hub (CSH) that stores shared tokens and aggregates knowledge across systems. The resulting model, termed LASS-ODE (\underline{LA}rge-\underline{S}cale \underline{S}mall \underline{ODE}), is pretrained on our $40$GB ODE trajectory collections to enable strong in-domain performance, zero-shot generalization across diverse ODE systems, and additional improvements through fine-tuning.

</details>


### [670] [How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments](https://arxiv.org/abs/2602.01017)
*Fuxin Wang,Amr Alazali,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 本文通过合成实验研究了链式思维（CoT）推理的忠实性问题，发现模型仅在训练噪声低于临界阈值时才能学习到符合算术规则的忠实推理，该现象源于简洁性偏差；噪声升高会导致推理模式从忠实逐步转向跳步式不忠实，并伴随预测熵的瞬态上升；机制分析表明模型能隐式地进行自我验证以解决不一致步骤。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量关于大语言模型链式思维（CoT）推理不忠实（如逻辑不一致、因果断裂）的经验观察，但对其本质定义及在自回归训练中如何产生缺乏基础性理解。

Method: 采用受控的合成实验，训练小型Transformer模型在含噪声数据上逐步求解模运算表达式（即‘算术表达式推理’任务），结合训练动力学分析、预测熵测量与机制解释方法。

Result: 发现存在一个训练噪声临界阈值：低于该阈值时模型可习得忠实于算术规则的分步推理；高于该阈值则出现从忠实→混合（熵瞬增）→跳步式不忠实的相变；并揭示模型通过内部编码不确定性实现隐式自我验证。

Conclusion: CoT的不忠实性并非固有缺陷，而是训练噪声与模型归纳偏置（如简洁性偏差）共同作用的动力学结果；自回归训练本身可催生隐式的自我验证机制，为提升推理忠实性提供了新视角。

Abstract: Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lacking--what constitutes faithful CoT reasoning, and how unfaithfulness emerges from autoregressive training. We study these questions using well-controlled synthetic experiments, training small transformers on noisy data to solve modular arithmetic expressions step by step, a task we term Arithmetic Expression Reasoning. We find that models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold, a phenomenon attributable to simplicity bias. At higher noise levels, training dynamics exhibit a transition from faithful stepwise reasoning to unfaithful skip-step reasoning via an intermediate mixed mode characterized by a transient increase in prediction entropy. Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps, which suggests the emergence of implicit self-verification from autoregressive training.

</details>


### [671] [Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01025)
*Kaiyuan Cui,Yige Li,Yutao Wu,Xingjun Ma,Sarah Erfani,Christopher Leckie,Hanxun Huang*

Main category: cs.LG

TL;DR: 本文提出UltraBreak框架，通过视觉空间的变换与正则化约束及语义引导的文本监督，生成通用且可迁移的图像对抗扰动，有效提升对黑盒视觉语言模型的跨模型、跨目标攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的图像 jailbreak 方法在白盒代理模型上过拟合，难以迁移到黑盒目标模型，导致攻击泛化性差。

Method: UltraBreak 在视觉空间施加变换与正则化以约束对抗模式，并在文本嵌入空间定义损失函数，采用语义驱动的目标松弛策略，从而发现通用对抗模式。

Result: UltraBreak 在多个VLM上显著优于先前jailbreak方法，展现出强跨模型与跨目标迁移能力；实验分析表明语义目标带来的损失曲面平滑化是实现通用可迁移攻击的关键。

Conclusion: 通过联合优化视觉鲁棒性与语义一致性，UltraBreak 实现了高效、通用、可迁移的视觉语言模型 jailbreak，为多模态安全研究提供了新思路。

Abstract: Vision-language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. Existing gradient-based jailbreak methods transfer poorly, as adversarial patterns overfit to a single white-box surrogate and fail to generalise to black-box models. In this work, we propose Universal and transferable jailbreak (UltraBreak), a framework that constrains adversarial patterns through transformations and regularisation in the vision space, while relaxing textual targets through semantic-based objectives. By defining its loss in the textual embedding space of the target LLM, UltraBreak discovers universal adversarial patterns that generalise across diverse jailbreak objectives. This combination of vision-level regularisation and semantically guided textual supervision mitigates surrogate overfitting and enables strong transferability across both models and attack targets. Extensive experiments show that UltraBreak consistently outperforms prior jailbreak methods. Further analysis reveals why earlier approaches fail to transfer, highlighting that smoothing the loss landscape via semantic objectives is crucial for enabling universal and transferable jailbreaks. The code is publicly available in our \href{https://github.com/kaiyuanCui/UltraBreak}{GitHub repository}.

</details>


### [672] [SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2602.01027)
*Xin Nie,Haicheng Zhang,Liang Dong,Beining Feng,Jinhong Weng,Guiling Sun*

Main category: cs.LG

TL;DR: 本文提出SFMP框架，一种无需搜索且硬件友好的大语言模型混合精度量化方法，通过分数位宽、分块混合精度、行列权重重排序和统一GEMM核实现高效压缩与推理。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度量化方法存在离散优化成本高或硬件不友好导致内存布局不规则的问题。

Method: 提出SFMP框架，包含四个创新点：1）分数位宽，将离散精度分配转为连续优化；2）分块混合精度，在保持硬件友好性的同时实现细粒度精度控制；3）行列权重重排序，聚合显著权重并仅引入少量激活重排序开销；4）统一GEMM核，支持任意平均位宽的混合精度矩阵乘法。

Result: 实验表明，SFMP在相同内存约束下优于现有层级别混合精度方法，同时显著降低量化成本并提升推理效率。

Conclusion: SFMP是一种高效、实用且硬件友好的混合精度量化方案，适用于大语言模型在资源受限场景下的部署。

Abstract: Mixed-precision quantization is a promising approach for compressing large language models under tight memory budgets. However, existing mixed-precision methods typically suffer from one of two limitations: they either rely on expensive discrete optimization to determine precision allocation, or introduce hardware inefficiencies due to irregular memory layouts. We propose SFMP, a search-free and hardware-friendly mixed-precision quantization framework for large language models. The framework is built upon four novel ideas: Fractional bit-width, which extends integer bit-width for weight matrix to fractional value and transforms discrete precision allocation as a continuous problem; 2)Block-wise mixed-precision, enabling fine-grained precision within weight matrices while remaining hardware-friendly; 3)Row-column weight reordering, which aggregates salient weights via row and column reordering, incurring only a small activation reordering overhead during inference; 4)Unified GEMM kernel, which supports mixed-precision GEMM at arbitrary average bit-width. Extensive experiments demonstrate that SFMP outperforms state-of-the-art layer-wise mixed-precision methods under the same memory constraints, while significantly reducing quantization cost and improving inference efficiency. Code is available at https://github.com/Nkniexin/SFMP

</details>


### [673] [Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection](https://arxiv.org/abs/2602.01039)
*Zhiwei Ling,Hailiang Zhao,Chao Zhang,Xiang Ao,Ziqi Wang,Cheng Zhang,Zhen Qin,Xinkui Zhao,Kingsum Chow,Yuanqing Wu,MengChu Zhou*

Main category: cs.LG

TL;DR: 本文提出FLood框架，通过双重加权机制（客户端伪OOD样本加权与服务器端OOD置信度加权）缓解联邦学习中非独立同分布（non-IID）数据带来的收敛不稳定与泛化差问题，显著提升全局模型鲁棒性与服务质量，且可即插即用地增强现有FL算法。


<details>
  <summary>Details</summary>
Motivation: 现实服务场景中用户、设备和应用产生的数据高度异构（non-IID），严重损害联邦学习的收敛稳定性、泛化能力与服务质量。

Method: 提出FLood框架：客户端在本地训练中对伪OOD样本上采样以增强鲁棒学习；服务器端根据客户端的OOD置信度加权聚合模型更新，优先采纳分布一致性高的客户端贡献。

Result: 在多种non-IID基准实验中，FLood在准确率与泛化性能上持续优于当前最优FL方法；且作为正交插件模块，无需修改原有算法核心逻辑即可提升其性能。

Conclusion: FLood是一种实用、可扩展的联邦学习解决方案，能有效应对真实边缘-云环境中数据异构性挑战，提升智能服务系统的可靠性。

Abstract: Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.

</details>


### [674] [Superposition unifies power-law training dynamics](https://arxiv.org/abs/2602.01045)
*Zixin Jessie Chen,Hao Chen,Yizhou Liu,Jeff Gore*

Main category: cs.LG

TL;DR: 本文研究了特征叠加（superposition）在训练动力学中幂律行为出现的作用，发现叠加瓶颈能导致训练指数趋近于1的普适幂律，显著加速训练过程。


<details>
  <summary>Details</summary>
Motivation: 探究特征叠加如何影响神经网络训练动力学中的幂律行为，特别是其是否能带来更高效、更普适的训练特性。

Method: 采用教师-学生框架，首先推导无叠加情形下的解析理论，再分析叠加瓶颈对训练动力学的影响。

Result: 发现叠加瓶颈诱导出与输入数据和通道统计无关的普适幂律训练指数≈1，训练速度相比无叠加情形提升达十倍。

Conclusion: 特征叠加可导致快速且数据无关的幂律训练，这对包括大语言模型在内的广泛应用叠加机制的神经网络具有重要启示。

Abstract: We investigate the role of feature superposition in the emergence of power-law training dynamics using a teacher-student framework. We first derive an analytic theory for training without superposition, establishing that the power-law training exponent depends on both the input data statistics and channel importance. Remarkably, we discover that a superposition bottleneck induces a transition to a universal power-law exponent of $\sim 1$, independent of data and channel statistics. This one over time training with superposition represents an up to tenfold acceleration compared to the purely sequential learning that takes place in the absence of superposition. Our finding that superposition leads to rapid training with a data-independent power law exponent may have important implications for a wide range of neural networks that employ superposition, including production-scale large language models.

</details>


### [675] [SwiftRepertoire: Few-Shot Immune-Signature Synthesis via Dynamic Kernel Codes](https://arxiv.org/abs/2602.01051)
*Rong Fu,Wenxin Zhang,Muge Qi,Yang Li,Yabin Jin,Jiekai Wu,Jiaxuan Lu,Chunlei Meng,Youjin Wang,Zeli Su,Juntao Gao,Li Bao,Qi Zhao,Wei Luo,Simon Fong*

Main category: cs.LG

TL;DR: 本文提出了一种基于原型字典和轻量任务描述符的T细胞受体库级分析框架，通过合成小型适配器模块实现少样本、免全模型微调的任务自适应，并保持模型可解释性。


<details>
  <summary>Details</summary>
Motivation: T细胞受体库分析在疾病检测与免疫监测中具有生物学意义，但面临标签稀疏、队列异质性和大模型适配计算开销大等实际部署障碍。

Method: 构建一个从学习到的原型字典中合成紧凑任务特定参数化的框架；原型由来自TCR序列探针和池化嵌入统计的轻量任务描述符调节；合成的小型适配器模块作用于冻结的预训练主干网络；结合基序感知探针与校准的基序发现流程保障可解释性。

Result: 实现了仅需少量支持样本即可快速适配新任务，无需全模型微调；显著降低计算负担；保持对序列级信号的可解释性；在标签稀缺、算力受限的临床与科研场景中展现出实用性与样本效率。

Conclusion: 该框架为TCR库分析提供了实用、高效且可解释的建模路径，有望推动其在真实世界多场景中的落地应用。

Abstract: Repertoire-level analysis of T cell receptors offers a biologically grounded signal for disease detection and immune monitoring, yet practical deployment is impeded by label sparsity, cohort heterogeneity, and the computational burden of adapting large encoders to new tasks. We introduce a framework that synthesizes compact task-specific parameterizations from a learned dictionary of prototypes conditioned on lightweight task descriptors derived from repertoire probes and pooled embedding statistics. This synthesis produces small adapter modules applied to a frozen pretrained backbone, enabling immediate adaptation to novel tasks with only a handful of support examples and without full model fine-tuning. The architecture preserves interpretability through motif-aware probes and a calibrated motif discovery pipeline that links predictive decisions to sequence-level signals. Together, these components yield a practical, sample-efficient, and interpretable pathway for translating repertoire-informed models into diverse clinical and research settings where labeled data are scarce and computational resources are constrained.

</details>


### [676] [LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents](https://arxiv.org/abs/2602.01053)
*Hyesung Jeon,Hyeongju Ha,Jae-Joon Kim*

Main category: cs.LG

TL;DR: 本文提出LRAgent，一种针对多LoRA代理系统的KV缓存共享框架，通过分解缓存为共享基础部分和适配器依赖部分，显著降低内存与计算开销，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 多LLM代理系统中，多LoRA架构虽共享预训练主干，但各代理仍独立构建并存储相同长轨迹的KV缓存，造成显著内存与计算冗余；现有KV缓存共享方法未适配多LoRA场景。

Method: 提出LRAgent框架：将KV缓存分解为来自预训练权重的共享基础分量和来自LoRA权重的适配器分量；利用shared-$A$多LoRA结构共享低秩缓存；设计Flash-LoRA-Attention内核，重排注意力计算以避免低秩缓存升维。

Result: 在多个代理问答基准上，LRAgent实现接近完全共享缓存的吞吐量与首token延迟，同时精度接近非共享缓存基线。

Conclusion: LRAgent有效缓解多LoRA代理系统中的KV缓存冗余问题，在内存、计算效率与模型精度之间取得良好平衡，为高效多代理系统提供了新范式。

Abstract: Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.

</details>


### [677] [Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning](https://arxiv.org/abs/2602.01058)
*Dylan Zhang,Yufeng Xu,Haojin Wang,Qingzhi Chen,Hao Peng*

Main category: cs.LG

TL;DR: 本文提出PEAR方法，在SFT阶段通过重要性采样重加权损失，缓解SFT与RL阶段数据分布不匹配问题，从而提升推理大模型在RL微调后的整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有SFT-RL流程中，SFT阶段独立优化、忽略后续RL阶段的策略分布，导致SFT越强，RL后性能反而可能更差，根源在于SFT数据分布与RL策略 rollout 分布不一致。

Method: PEAR是一种基于策略评估思想的离线学习损失重加权算法，采用重要性采样在token、block和sequence三个粒度上对SFT损失进行重加权，可无缝融入标准SFT目标且开销小。

Result: 在Qwen 2.5/3和DeepSeek-distilled模型上，PEAR在可验证推理游戏和数学推理任务（如AIME2025）中显著优于标准SFT，AIME2025 pass@8最高提升14.6%。

Conclusion: PEAR通过将SFT设计与下游RL目标对齐，推动了大模型后训练向更整体化方向发展，证明SFT不应孤立优化，而应为RL做准备。

Abstract: Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.
  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.
  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.
  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.

</details>


### [678] [On the Expressive Power of Permutation-Equivariant Weight-Space Networks](https://arxiv.org/abs/2602.01083)
*Adir Dayan,Yam Eitan,Haggai Maron*

Main category: cs.LG

TL;DR: 本文系统性地研究了权重空间网络（weight-space networks）的表达能力，证明了主流置换等变网络在表达力上等价，并在权重空间和函数空间中建立了通用性（universality）理论，明确了其成立条件与边界。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA权重空间网络依赖置换等变设计以提升泛化性，但可能损害表达能力；且权重空间学习同时作用于权重空间和函数空间，其表达力分析尤为复杂，缺乏统一理论。

Method: 通过理论分析，首先证明主流置换等变权重空间网络在表达力上等价；进而分别在权重空间和函数空间下，在自然假设下建立通用性定理，并刻画其失效的边界情形。

Result: 1）所有主流置换等变权重空间网络表达力等价；2）在温和自然假设下，权重空间和函数空间均具备通用性；3）明确给出了通用性不成立的边缘情形。

Conclusion: 本文构建了首个系统、统一的权重空间网络表达力理论框架，为该方向提供了坚实的理论基础。

Abstract: Weight-space learning studies neural architectures that operate directly on the parameters of other neural networks. Motivated by the growing availability of pretrained models, recent work has demonstrated the effectiveness of weight-space networks across a wide range of tasks. SOTA weight-space networks rely on permutation-equivariant designs to improve generalization. However, this may negatively affect expressive power, warranting theoretical investigation. Importantly, unlike other structured domains, weight-space learning targets maps operating on both weight and function spaces, making expressivity analysis particularly subtle. While a few prior works provide partial expressivity results, a comprehensive characterization is still missing. In this work, we address this gap by developing a systematic theory for expressivity of weight-space networks. We first prove that all prominent permutation-equivariant networks are equivalent in expressive power. We then establish universality in both weight- and function-space settings under mild, natural assumptions on the input weights, and characterize the edge-case regimes where universality no longer holds. Together, these results provide a strong and unified foundation for the expressivity of weight-space networks.

</details>


### [679] [OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases](https://arxiv.org/abs/2602.01105)
*Zixiao Wang,Yifei Shen,Huishuai Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新型优化器SiL, 结合谱控制与无穷范数坐标控制，在保持动量级内存开销的同时，性能媲美或超越AdamW和Muon，并缓解了微调时的优化器不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有优化器可被解释为特定几何下的最速下降法，具有隐式偏差；需要一种兼顾谱约束与坐标约束、低内存开销且适配预训练模型微调的新优化器。

Method: 提出SiL优化器：构建Lion式动量方向，通过少量Newton-Schulz迭代近似正交化，再应用逐元素符号函数；理论分析基于对角各向同性假设。

Result: 在GPT-2、Llama预训练、SiT图像预训练及监督微调等大规模任务中，SiL在同等调优下匹配或超越AdamW与Muon，且仅需动量级状态；有效缓解AdamW预训练模型微调时的优化器不匹配问题。

Conclusion: SiL通过高效逼近谱与ℓ∞约束交集上的最大步长，在非线性强操作（正交化+符号函数）下仍保证收敛，是一种兼具理论严谨性与实践优越性的轻量级优化器。

Abstract: Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \nameA{} (\fullname{}), which combines spectral control from orthogonalized update directions with $\ell_\infty$-style coordinate control from sign updates. \nameA{} forms a Lion-style momentum direction, approximately orthogonalizes it via a few Newton--Schulz iterations, and then applies an entrywise sign, providing an efficient approximation to taking a maximal step over the intersection of the spectral and $\ell_\infty$ constraint sets (a scaled Hadamard-like set for matrix parameters). Despite the strong nonlinearity of orthogonalization and sign, we prove convergence under a mild, empirically verified diagonal-isotropy assumption. Across large-scale language and vision training, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning, \nameA{} matches or outperforms AdamW and Muon under comparable tuning while using only momentum-level optimizer state, and it mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.

</details>


### [680] [Single-Edge Node Injection Threats to GNN-Based Security Monitoring in Industrial Graph Systems](https://arxiv.org/abs/2602.01113)
*Wenjie Liang,Ranhui Yan,Jia Cai,You-Gan Wang*

Main category: cs.LG

TL;DR: 本文提出了一种面向工业部署的单边图注入攻击（SEGIA），通过仅用一条边连接伪造节点，在资源受限下有效规避基于拓扑和同质性的防御，显著提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 工业图监控系统中，攻击者可通过注入伪造节点（如恶意传感器）来干扰GNN决策，而现有防御（如拓扑/同质性清洗）易被绕过，亟需研究实际约束下的新型攻击范式。

Method: 提出单边图注入攻击（SEGIA），结合剪枝SGC代理模型、多跳邻域采样、反向图卷积特征合成及相似性正则化目标，以维持局部同质性并抵抗边剪枝。

Result: 在多个数据集和防御方法上，SEGIA攻击成功率比基线高至少25%，且所需边预算更小；理论分析与实验验证了其有效性与隐蔽性。

Conclusion: SEGIA揭示了工业GNN部署中的系统级安全风险，建议引入轻量级准入验证与邻域一致性监控机制。

Abstract: Graph neural networks (GNNs) are increasingly adopted in industrial graph-based monitoring systems (e.g., Industrial internet of things (IIoT) device graphs, power-grid topology models, and manufacturing communication networks) to support anomaly detection, state estimation, and asset classification. In such settings, an adversary that compromises a small number of edge devices may inject counterfeit nodes (e.g., rogue sensors, virtualized endpoints, or spoofed substations) to bias downstream decisions while evading topology- and homophily-based sanitization. This paper formulates deployment-oriented node-injection attacks under constrained resources and proposes the \emph{Single-Edge Graph Injection Attack} (SEGIA), in which each injected node attaches to the operational graph through a single edge. SEGIA integrates a pruned SGC surrogate, multi-hop neighborhood sampling, and reverse graph convolution-based feature synthesis with a similarity-regularized objective to preserve local homophily and survive edge pruning. Theoretical analysis and extensive evaluations across datasets and defenses show at least $25\%$ higher attack success than representative baselines under substantially smaller edge budgets. These results indicate a system-level risk in industrial GNN deployments and motivate lightweight admission validation and neighborhood-consistency monitoring.

</details>


### [681] [MarkovScale: Towards Optimal Sequential Scaling at Inference Time](https://arxiv.org/abs/2602.01120)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于两状态马尔可夫过程的序列缩放（sequential scaling）理论框架，推导出其性能上下界与准确率提升条件，并据此设计了MarkovScale系统，在多个LLM和基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有序列缩放方法多为启发式、缺乏理论基础，导致性能提升有限且机制不明，亟需一个有原则、可分析的建模框架。

Method: 将序列缩放建模为两状态马尔可夫过程，推导其闭式解与性能边界；基于该理论设计MarkovScale系统，实现准确率与效率的理论最优权衡。

Result: 在3个主干大模型、5个基准、20+配置上的实验表明，MarkovScale持续超越当前最优的并行与序列缩放方法。

Conclusion: 该工作为LLM推理阶段的缩放提供了首个具备清晰理论保证的建模范式，推动了高效、最优推理的发展。

Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [682] [ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs](https://arxiv.org/abs/2602.01124)
*Md Abrar Jahin,Taufikur Rahman Fuad,Jay Pujara,Craig Knoblock*

Main category: cs.LG

TL;DR: 本文提出ChronoSpike，一种结合可学习LIF神经元、多头注意力空间聚合与轻量Transformer时序编码的自适应脉冲图神经网络，兼顾局部建模与长程依赖，在保持线性内存复杂度的同时显著提升动态图表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有动态图表示学习方法存在表达力与计算效率的权衡：注意力机制复杂度高，循环架构存在梯度问题和存储开销；脉冲神经网络虽具事件驱动优势，但受限于串行传播、信息二值化及缺乏全局上下文。

Method: 提出ChronoSpike模型，融合可学习的LIF神经元（含通道级膜电位动力学）、基于连续特征的多头注意力空间聚合、以及轻量Transformer时序编码器，实现线性内存复杂度O(T·d)。

Result: 在三个大规模基准上，ChronoSpike以105K固定参数量，超越12种SOTA方法，Macro-F1和Micro-F1分别提升2.0%和2.4%，训练速度比RNN方法快3–10倍；理论证明了膜电位有界性、梯度稳定性（收缩因子ρ<1）和BIBO稳定性；可解释性分析显示异质时间感受野与83–88%稀疏性的学习首因效应。

Conclusion: ChronoSpike通过协同设计脉冲动力学与注意力机制，在保证高效可扩展性的同时增强动态图建模能力，为事件驱动的时序图学习提供了新范式。

Abstract: Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\%$ Macro-F1 and $2.4\%$ Micro-F1 while achieving $3-10\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $ρ< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\%$ sparsity.

</details>


### [683] [WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity](https://arxiv.org/abs/2602.01126)
*Mengsha Kou,Xiaoyu Xia,Ziqi Wang,Ibrahim Khalil,Runkun Luo,Jingwen Zhou,Minhui Xue*

Main category: cs.LG

TL;DR: 本文提出WinFLoRA，一种面向隐私异构场景的联邦LoRA方法，通过噪声感知的加权聚合机制，提升全局模型精度并兼顾客户端差异化隐私需求。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感的联邦学习部署中，客户端注入的差分隐私噪声水平各异，导致个体激励与全局性能不一致，现有方法难以兼顾隐私异质性与模型性能。

Method: WinFLoRA基于客户端上传的LoRA适配器估计其噪声水平，并据此分配聚合权重：噪声越小、权重越大，从而增强低噪声更新对全局模型的影响，实现噪声感知的激励机制。

Result: 在多个大语言模型和数据集上的实验表明，WinFLoRA相较现有最优方法，全局准确率最高提升52.58%，客户端效用最高提升2.56倍。

Conclusion: WinFLoRA有效协调了客户端在隐私保护与下游任务性能之间的异质性目标，无需第三方介入即可对齐个体效用与全局模型优化目标。

Abstract: Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.

</details>


### [684] [Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models](https://arxiv.org/abs/2602.01128)
*Mete Erdogan*

Main category: cs.LG

TL;DR: 本文提出Tangent-Space Direct Preference Optimization (TS-DPO)，在模型切空间中进行多目标偏好对齐，使大语言模型能灵活、可控地权衡如帮助性、安全性、冗余度等不同人类偏好维度，无需额外优化即可通过线性组合生成用户指定行为。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）将多维人类偏好压缩为单一标量奖励，无法遍历帕累托前沿，缺乏对多维偏好的可控调节能力。

Method: 基于Ortiz-Jimenez等人提出的切空间微调视角，将DPO扩展至切空间，学习各偏好目标对应的独立更新方向；推理时通过线性组合这些方向实现可控行为生成。

Result: 在HelpSteer和UltraFeedback数据集上，TS-DPO在帮助性-冗余度权衡任务中展现出更广的帕累托最优覆盖与更平滑的偏好控制；CCA分析表明其增强了不同偏好方向的解耦性。

Conclusion: TS-DPO为多维偏好对齐提供了可解释、可组合、无需在线优化的新范式，显著提升了LLM行为的可控性与灵活性。

Abstract: Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.

</details>


### [685] [TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation](https://arxiv.org/abs/2602.01135)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: 本文提出TRACE框架，利用自回归模型作为预训练密度估计器来估计条件互信息，从而从单一离散事件序列中推断事件类型间的因果图，具备可扩展性、支持延迟因果效应，并在车辆诊断等实际场景中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 解决从单个离散事件序列（如车辆日志、患者轨迹）中进行因果发现的挑战，包括缺乏重复样本、高维性和长程时序依赖。

Method: 提出TRACE框架，将自回归模型重用为预训练密度估计器，用于条件互信息估计，以推断事件类型间的摘要因果图；支持延迟因果、GPU并行计算，且时间复杂度线性于事件词表大小。

Result: 理论证明在非完美自回归模型下仍具可识别性；实验显示其在不同基线和词表规模下鲁棒性强，并成功应用于含超29100种事件类型的车辆诊断根因分析。

Conclusion: TRACE是一种高效、可扩展且理论上可识别的因果发现方法，适用于高维单序列事件数据，尤其适合真实工业与医疗场景。

Abstract: We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.

</details>


### [686] [A Unified Matrix-Spectral Framework for Stability and Interpretability in Deep Learning](https://arxiv.org/abs/2602.01136)
*Ronald Katende*

Main category: cs.LG

TL;DR: 本文提出了一种统一的矩阵-谱框架，用于分析深度神经网络的稳定性与可解释性，通过引入全局矩阵稳定性指标和谱熵来量化并提升模型对输入扰动、标签噪声及训练动态的鲁棒性，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在实际应用中面临输入扰动、标签噪声和训练不稳定性等挑战，亟需一种统一、可计算的理论框架来同时刻画稳定性与可解释性。

Method: 将神经网络建模为数据依赖的线性算子乘积，提取雅可比矩阵、参数梯度、神经正切核算子和损失海森矩阵的谱信息，构建全局矩阵稳定性指标和谱熵作为稳定性度量，并据此设计谱正则化方法。

Result: 全局矩阵稳定性指标能统一控制前向敏感性、归因鲁棒性和优化条件数；谱熵比传统算子范数更能反映典型（而非最坏）敏感性；轻量谱正则化显著提升归因稳定性，即使全局谱统计变化不大。

Conclusion: 谱集中性与解析稳定性存在精确联系，该框架为鲁棒、可解释的模型设计与训练提供了理论基础与实用指导。

Abstract: We develop a unified matrix-spectral framework for analyzing stability and interpretability in deep neural networks. Representing networks as data-dependent products of linear operators reveals spectral quantities governing sensitivity to input perturbations, label noise, and training dynamics.
  We introduce a Global Matrix Stability Index that aggregates spectral information from Jacobians, parameter gradients, Neural Tangent Kernel operators, and loss Hessians into a single stability scale controlling forward sensitivity, attribution robustness, and optimization conditioning. We further show that spectral entropy refines classical operator-norm bounds by capturing typical, rather than purely worst-case, sensitivity.
  These quantities yield computable diagnostics and stability-oriented regularization principles. Synthetic experiments and controlled studies on MNIST, CIFAR-10, and CIFAR-100 confirm that modest spectral regularization substantially improves attribution stability even when global spectral summaries change little.
  The results establish a precise connection between spectral concentration and analytic stability, providing practical guidance for robustness-aware model design and training.

</details>


### [687] [Self-Generative Adversarial Fine-Tuning for Large Language Models](https://arxiv.org/abs/2602.01137)
*Shiguang Wu,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: 本文提出SGALM框架，将大语言模型对齐建模为单个模型内部的生成对抗博弈，无需外部奖励模型，兼顾对齐效果与合成数据生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法依赖高成本、稀缺的人工标注；自博弈与合成数据方法则存在启发式假设或无依据自评估问题，易导致偏差累积和性能漂移。

Method: 提出Self-Generative Adversarial LLM（SGALM），在单一LLM内构建生成器与判别器的联合对抗训练框架，不依赖外部奖励模型，实现生成与判别能力协同演化。

Result: 理论与实验表明SGALM达到当前最优对齐性能，兼具高效对齐算法与鲁棒合成数据引擎双重功能。

Conclusion: SGALM提供了一种去中心化、自洽的大模型对齐新范式，缓解了对外部监督与人工标注的依赖。

Abstract: Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.

</details>


### [688] [Key Principles of Graph Machine Learning: Representation, Robustness, and Generalization](https://arxiv.org/abs/2602.01139)
*Yassine Abbahaddou*

Main category: cs.LG

TL;DR: 本文探讨了图神经网络（GNN）在泛化性、对抗鲁棒性和表征学习效果方面的挑战，并提出了三项主要贡献：基于图移位算子（GSO）的新表征学习方法、通过图数据增强提升泛化能力的方法，以及利用正交化和噪声防御提升鲁棒性的技术。


<details>
  <summary>Details</summary>
Motivation: GNN在泛化性、对抗鲁棒性和表征学习能力方面存在局限，亟需系统性改进。

Method: 1) 基于图移位算子（GSO）设计新型表征学习技术；2) 提出图数据增强以提升泛化能力；3) 结合正交化与噪声防御机制增强GNN对抗攻击的鲁棒性。

Result: 提升了GNN在多种场景下的性能，增强了其泛化能力和对抗鲁棒性，并深化了对GNN局限性与潜力的理论理解。

Conclusion: 所提方法为GNN的表征学习、泛化与鲁棒性提供了更系统、更原理性的解决方案。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations from structured data. Despite their growing popularity and success across various applications, GNNs encounter several challenges that limit their performance. in their generalization, robustness to adversarial perturbations, and the effectiveness of their representation learning capabilities. In this dissertation, I investigate these core aspects through three main contributions: (1) developing new representation learning techniques based on Graph Shift Operators (GSOs, aiming for enhanced performance across various contexts and applications, (2) introducing generalization-enhancing methods through graph data augmentation, and (3) developing more robust GNNs by leveraging orthonormalization techniques and noise-based defenses against adversarial attacks. By addressing these challenges, my work provides a more principled understanding of the limitations and potential of GNNs.

</details>


### [689] [Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization](https://arxiv.org/abs/2602.01140)
*Haochen You,Heng Zhang,Hongyang He,Yuqi Li,Baojing Liu*

Main category: cs.LG

TL;DR: 本文提出GRIT-VQ，一种统一的向量量化（VQ）可微代理框架，在前向传播中保持硬分配，同时通过半径驱动更新和集成变换实现全可微优化，显著提升代码本利用率与模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于硬最近邻分配的向量量化不可微，依赖直通估计器（STE），导致梯度不稳定、代码本利用不足且更新耦合于量化间隙。

Method: 提出GRIT-VQ框架：1）用几何感知、可控半径的量化方向步进替代STE；2）对码本施加数据无关的集成变换，使所有码共享参数协同更新。

Result: 在图像重建、生成及推荐token化任务上，GRIT-VQ一致降低重构误差、提升生成质量与推荐精度，并大幅提高代码本利用率。

Conclusion: GRIT-VQ从理论和实践上解决了传统VQ的可微性与优化稳定性问题，为大规模离散表征学习提供了更鲁棒、高效的通用方案。

Abstract: Vector quantization (VQ) underpins modern generative and representation models by turning continuous latents into discrete tokens. Yet hard nearest-neighbor assignments are non-differentiable and are typically optimized with heuristic straight-through estimators, which couple the update step size to the quantization gap and train each code in isolation, leading to unstable gradients and severe codebook under-utilization at scale. In this paper, we introduce GRIT-VQ (Generalized Radius and Integrated Transform-Vector Quantization), a unified surrogate framework that keeps hard assignments in the forward pass while making VQ fully differentiable. GRIT-VQ replaces the straight-through estimator with a radius-based update that moves latents along the quantization direction with a controllable, geometry-aware step, and applies a data-agnostic integrated transform to the codebook so that all codes are updated through shared parameters instead of independently. Our theoretical analysis clarifies the fundamental optimization dynamics introduced by GRIT-VQ, establishing conditions for stable gradient flow, coordinated codebook evolution, and reliable avoidance of collapse across a broad family of quantizers. Across image reconstruction, image generation, and recommendation tokenization benchmarks, GRIT-VQ consistently improves reconstruction error, generative quality, and recommendation accuracy while substantially increasing codebook utilization compared to existing VQ variants.

</details>


### [690] [Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing](https://arxiv.org/abs/2602.01150)
*Jialong Sun,Zeming Wei,Jiaxuan Zou,Jiacheng Gong,Guanheng Wang,Chengyang Dong,Jialong Li,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的统计型成员推断攻击（SMIA）框架，用于更可靠、高效地审计机器遗忘效果，克服了传统基于MIA的审计方法因统计误差导致的高估问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于成员推断攻击（MIA）的机器遗忘审计方法存在根本性缺陷：失败的成员推断并不等价于真正遗忘，且其统计误差不可观测，导致评估过于乐观并计算开销大。

Method: 提出统计成员推断攻击（SMIA），不依赖训练影子模型，而是直接对成员与非成员数据分布进行统计检验，并输出遗忘率及其置信区间。

Result: SMIA在实验中展现出比现有MIA方法更可靠的审计性能，同时显著降低计算成本；具备理论保证和实证有效性。

Conclusion: SMIA为机器遗忘审计提供了一种新范式，兼顾可靠性、可解释性与效率。

Abstract: Machine unlearning (MU) is essential for enforcing the right to be forgotten in machine learning systems. A key challenge of MU is how to reliably audit whether a model has truly forgotten specified training data. Membership Inference Attacks (MIAs) are widely used for unlearning auditing, where samples that evade membership detection are often regarded as successfully forgotten. After carefully revisiting the reliability of MIA, we show that this assumption is flawed: failed membership inference does not imply true forgetting. We theoretically demonstrate that MIA-based auditing, when formulated as a binary classification problem, inevitably incurs statistical errors whose magnitude cannot be observed during the auditing process. This leads to overly optimistic evaluations of unlearning performance, while incurring substantial computational overhead due to shadow model training. To address these limitations, we propose Statistical Membership Inference Attack (SMIA), a novel training-free and highly effective auditing framework. SMIA directly compares the distributions of member and non-member data using statistical tests, eliminating the need for learned attack models. Moreover, SMIA outputs both a forgetting rate and a corresponding confidence interval, enabling quantified reliability of the auditing results. Extensive experiments show that SMIA provides more reliable auditing with significantly lower computational cost than existing MIA-based approaches. Notably, the theoretical guarantees and empirical effectiveness of SMIA suggest it as a new paradigm for reliable machine unlearning auditing.

</details>


### [691] [Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market](https://arxiv.org/abs/2602.01157)
*Mohammed Osman Gani,Zhipeng He,Chun Ouyang,Sara Khalifa*

Main category: cs.LG

TL;DR: 本文提出了一种新的多日电力价格预测（EPF）框架，系统性地应用并评估了多种前沿时间序列深度学习模型，在澳大利亚国家电力市场（NEM）五个区域进行逐时段（intraday interval-level）分析，揭示了误差的日内模式与模型性能差异，并指出未来研究应兼顾长期鲁棒性与日内波动敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有电力价格预测研究存在三大不足：缺乏对多日超前预测的关注、未充分探索最新时序深度学习模型、以及仅依赖整体时段评估而忽略日内各时段性能差异。

Method: 构建基于多种SOTA时序深度学习模型的多日超前EPF框架，并在澳大利亚NEM全部五个区域开展逐15/30分钟级（intraday interval-level）的精细化模型评估。

Result: 标准深度学习模型在多数区域表现更优，而SOTA时序模型在延长预测步长时更具鲁棒性；逐时段评估发现绝对误差在晚间爬坡期最高、相对误差在午间负电价时段显著放大、方向准确性在趋势频繁变化期下降。

Conclusion: 单一模型无法在所有区域、指标和预测步长下持续最优；未来DL-EPF研究需增强特征表达与建模策略，以同时提升长期预测鲁棒性与对日内波动及结构性价格动态的响应能力。

Abstract: Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted in EPF to capture complex and nonlinear price dynamics, several important gaps persist: (i) limited attention to multi-day horizons beyond day-ahead forecasting, (ii) insufficient exploration of state-of-the-art (SOTA) time series DL models, and (iii) a predominant reliance on aggregated horizon-level evaluation that obscures time-of-day forecasting variation. To address these gaps, we propose a novel EPF framework that extends the forecast horizon to multi-day-ahead by systematically building forecasting models that leverage benchmarked SOTA time series DL models. We conduct a comprehensive evaluation to analyse time-of-day forecasting performance by integrating model assessment at intraday interval levels across all five regions in the Australian National Electricity Market (NEM). The results show that no single model consistently dominates across regions, metrics, and horizons. Overall, standard DL models deliver superior performance in most regions, while SOTA time series DL models demonstrate greater robustness to forecast horizon extension. Intraday interval-level evaluation reveals pronounced diurnal error patterns, indicating that absolute errors peak during the evening ramp, relative errors inflate during midday negative-price regimes, and directional accuracy degrades during periods of frequent trend changes. These findings suggest that future research on DL-based EPF can benefit from enriched feature representations and modelling strategies that enhance longer-term forecasting robustness while maintaining sensitivity to intraday volatility and structural price dynamics.

</details>


### [692] [Multi-Fidelity Physics-Informed Neural Networks with Bayesian Uncertainty Quantification and Adaptive Residual Learning for Efficient Solution of Parametric Partial Differential Equations](https://arxiv.org/abs/2602.01176)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: 本文提出MF-BPINN框架，结合物理信息神经网络、贝叶斯不确定性量化与自适应残差学习，利用多保真度数据高效求解参数化偏微分方程。


<details>
  <summary>Details</summary>
Motivation: 解决高保真PDE求解计算成本高、尤其在需多次参数配置评估的参数化系统中效率低的问题。

Method: 提出MF-BPINN：构建分层神经网络架构学习不同保真度间的非线性关联；引入带可学习门控机制的自适应残差网络动态平衡保真度差异；采用基于哈密顿蒙特卡洛的严格贝叶斯框架进行不确定性量化。

Result: 显著提升参数化PDE求解效率与精度，有效融合大量低保真模拟与稀疏高保真数据，在计算代价可控前提下增强泛化性与鲁棒性。

Conclusion: MF-BPINN为多保真物理建模提供了统一、可扩展且具不确定性感知的深度学习框架，推动PINNs在复杂工程仿真中的实用化。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful paradigm for solving partial differential equations (PDEs) by embedding physical laws directly into neural network training. However, solving high-fidelity PDEs remains computationally prohibitive, particularly for parametric systems requiring multiple evaluations across varying parameter configurations. This paper presents MF-BPINN, a novel multi-fidelity framework that synergistically combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. Our approach leverages abundant low-fidelity simulations alongside sparse high-fidelity data through a hierarchical neural architecture that learns nonlinear correlations across fidelity levels. We introduce an adaptive residual network with learnable gating mechanisms that dynamically balances linear and nonlinear fidelity discrepancies. Furthermore, we develop a rigorous Bayesian framework employing Hamiltonian Monte Carlo.

</details>


### [693] [Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective](https://arxiv.org/abs/2602.01179)
*Zhichao Chen,Zhan Zhuang,Yunfei Teng,Hao Wang,Fangyikang Wang,Zhengnan Li,Tianqiao Liu,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文提出了一种基于熵正则化半对偶非平衡最优传输（E-SUOT）的渐进域自适应（GDA）框架，绕过传统流模型中依赖样本似然估计的缺陷，直接构造中间域，提升GDA稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 真实中间域常不可得或无效，需合成中间样本；而现有基于流模型的方法依赖样本似然估计，易丢失信息、降低GDA性能。

Method: 将流式GDA重构为拉格朗日对偶问题，推导出无需似然估计的半对偶目标；引入熵正则化将不稳定的min-max优化转为稳定交替优化，构建E-SUOT框架。

Result: 在多个基准数据集上验证了E-SUOT在域迁移任务中的有效性，理论分析证明其具有更好训练稳定性和泛化能力。

Conclusion: E-SUOT是一种更鲁棒、更高效的GDA方法，通过避免似然估计、直接建模分布插值，显著提升了渐进域自适应的实用性和理论基础。

Abstract: Gradual domain adaptation (GDA) aims to mitigate domain shift by progressively adapting models from the source domain to the target domain via intermediate domains. However, real intermediate domains are often unavailable or ineffective, necessitating the synthesis of intermediate samples. Flow-based models have recently been used for this purpose by interpolating between source and target distributions; however, their training typically relies on sample-based log-likelihood estimation, which can discard useful information and thus degrade GDA performance. The key to addressing this limitation is constructing the intermediate domains via samples directly. To this end, we propose an Entropy-regularized Semi-dual Unbalanced Optimal Transport (E-SUOT) framework to construct intermediate domains. Specifically, we reformulate flow-based GDA as a Lagrangian dual problem and derive an equivalent semi-dual objective that circumvents the need for likelihood estimation. However, the dual problem leads to an unstable min-max training procedure. To alleviate this issue, we further introduce entropy regularization to convert it into a more stable alternative optimization procedure. Based on this, we propose a novel GDA training framework and provide theoretical analysis in terms of stability and generalization. Finally, extensive experiments are conducted to demonstrate the efficacy of the E-SUOT framework.

</details>


### [694] [Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective](https://arxiv.org/abs/2602.01182)
*Zhichao Chen,Hao Wang,Fangyikang Wang,Licheng Pan,Zhengnan Li,Yunfei Teng,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文提出SPIRIT框架，通过引入熵诱导Bregman散度和半近端传输（SPT）差异，缓解扩散模型在时间序列数据插补中因非平稳动态和目标不一致导致的性能不稳定问题，并理论证明其对非平稳性的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列数据插补中表现不稳定，主要源于非平稳时间动态导致推理轨迹偏差、以及插补任务追求点级精度与扩散模型固有生成多样性目标之间的矛盾。

Method: 从近端算子视角分析扩散模型插补过程，发现隐含Wasserstein距离正则化会削弱对非平稳性的抵抗能力；据此提出SPIRIT框架：引入熵诱导Bregman散度放松Wasserstein距离的质量守恒约束，构建半近端传输（SPT）差异，并以SPT作为近端算子设计完整插补流程。

Result: 理论证明SPT对非平稳性具有鲁棒性；大量实验验证SPIRIT在多种复杂场景下显著优于现有扩散模型方法。

Conclusion: SPIRIT通过改进扩散模型的正则化机制与近端结构，有效平衡多样性与保真度，为时间序列插补提供了更稳定、更鲁棒的新范式。

Abstract: Diffusion models (DMs) have shown promise for Time-Series Data Imputation (TSDI); however, their performance remains inconsistent in complex scenarios. We attribute this to two primary obstacles: (1) non-stationary temporal dynamics, which can bias the inference trajectory and lead to outlier-sensitive imputations; and (2) objective inconsistency, since imputation favors accurate pointwise recovery whereas DMs are inherently trained to generate diverse samples. To better understand these issues, we analyze DM-based TSDI process through a proximal-operator perspective and uncover that an implicit Wasserstein distance regularization inherent in the process hinders the model's ability to counteract non-stationarity and dissipative regularizer, thereby amplifying diversity at the expense of fidelity. Building on this insight, we propose a novel framework called SPIRIT (Semi-Proximal Transport Regularized time-series Imputation). Specifically, we introduce entropy-induced Bregman divergence to relax the mass preserving constraint in the Wasserstein distance, formulate the semi-proximal transport (SPT) discrepancy, and theoretically prove the robustness of SPT against non-stationarity. Subsequently, we remove the dissipative structure and derive the complete SPIRIT workflow, with SPT serving as the proximal operator. Extensive experiments demonstrate the effectiveness of the proposed SPIRIT approach.

</details>


### [695] [The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics](https://arxiv.org/abs/2602.01186)
*Fabio Turazza,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: 本文提出GH-OFL方法族，通过仅传输类条件高斯嵌入的充分统计量（如各类计数与一、二阶矩），实现真正单轮、数据免费的一次性联邦学习，在强非独立同分布（non-IID）下仍保持高鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习通信开销大、隐私风险高；现有一次性联邦学习（OFL）方法多依赖公共数据集、同质模型或额外上传信息，实用性受限。

Method: 基于预训练嵌入满足类条件高斯假设，客户端仅上传每类的计数及一、二阶矩；服务端构建三类头部：(i) 基于统计量的闭式高斯分类器（NB/LDA/QDA）；(ii) FisherMix：在估计的Fisher子空间中生成合成样本并训练带余弦间隔的线性头；(iii) Proto-Hyper：通过知识蒸馏在合成样本上轻量微调高斯logits的低秩残差头。

Result: GH-OFL在强non-IID设定下达到SOTA精度与鲁棒性，且全程不需原始数据或公共数据集，严格满足数据免费（data-free）要求。

Conclusion: GH-OFL为一次性联邦学习提供了实用、隐私友好且无需数据共享的新范式，显著提升了部署可行性与泛化能力。

Abstract: Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reducing communication to a single round, thereby lowering overhead and enhancing practical deployability. Nevertheless, most existing one-shot approaches remain either impractical or constrained, for example, they often depend on the availability of a public dataset, assume homogeneous client models, or require uploading additional data or model information. To overcome these issues, we introduce the Gaussian-Head OFL (GH-OFL) family, a suite of one-shot federated methods that assume class-conditional Gaussianity of pretrained embeddings. Clients transmit only sufficient statistics (per-class counts and first/second-order moments) and the server builds heads via three components: (i) Closed-form Gaussian heads (NB/LDA/QDA) computed directly from the received statistics; (ii) FisherMix, a linear head with cosine margin trained on synthetic samples drawn in an estimated Fisher subspace; and (iii) Proto-Hyper, a lightweight low-rank residual head that refines Gaussian logits via knowledge distillation on those synthetic samples. In our experiments, GH-OFL methods deliver state-of-the-art robustness and accuracy under strong non-IID skew while remaining strictly data-free.

</details>


### [696] [Unraveling the Hidden Dynamical Structure in Recurrent Neural Policies](https://arxiv.org/abs/2602.01196)
*Jin Li,Yue Wu,Mengsha Huang,Yuhao Sun,Hao He,Xianyuan Zhan*

Main category: cs.LG

TL;DR: 本文通过分析多种训练方法、模型架构和任务下学习到的循环策略的隐藏状态域，发现其在与环境交互时一致地涌现出稳定的循环结构，这些结构类似于动力系统中的极限环，并且其几何特性与策略行为存在结构化对应关系。


<details>
  <summary>Details</summary>
Motivation: 尽管循环神经网络策略在部分可观测控制和元强化学习任务中表现出色，但其优异泛化和鲁棒性能背后的机制仍不清楚。

Method: 分析不同训练方法、模型架构和任务下学习到的循环策略的隐藏状态域，将其与动力系统理论中的极限环概念进行类比，并研究其几何特性与策略行为之间的关系。

Result: 发现循环策略在与环境交互时一致涌现出类似动力系统中极限环的稳定循环结构，且这些极限环的几何特性与策略行为存在结构化对应关系。

Conclusion: 极限环的出现有助于稳定策略的内部记忆和任务相关环境状态，抑制环境不确定性带来的干扰；其几何结构编码了行为间的关联，从而促进在非平稳环境中更易适应新技能。

Abstract: Recurrent neural policies are widely used in partially observable control and meta-RL tasks. Their abilities to maintain internal memory and adapt quickly to unseen scenarios have offered them unparalleled performance when compared to non-recurrent counterparts. However, until today, the underlying mechanisms for their superior generalization and robustness performance remain poorly understood. In this study, by analyzing the hidden state domain of recurrent policies learned over a diverse set of training methods, model architectures, and tasks, we find that stable cyclic structures consistently emerge during interaction with the environment. Such cyclic structures share a remarkable similarity with \textit{limit cycles} in dynamical system analysis, if we consider the policy and the environment as a joint hybrid dynamical system. Moreover, we uncover that the geometry of such limit cycles also has a structured correspondence with the policies' behaviors. These findings offer new perspectives to explain many nice properties of recurrent policies: the emergence of limit cycles stabilizes both the policies' internal memory and the task-relevant environmental states, while suppressing nuisance variability arising from environmental uncertainty; the geometry of limit cycles also encodes relational structures of behaviors, facilitating easier skill adaptation when facing non-stationary environments.

</details>


### [697] [SimpleGPT: Improving GPT via A Simple Normalization Strategy](https://arxiv.org/abs/2602.01212)
*Marco Chen,Xianbiao Qi,Yelin He,Jiaquan Ye,Rong Xiao*

Main category: cs.LG

TL;DR: 本文提出SimpleNorm归一化策略，通过控制中间激活尺度来降低损失函数Hessian矩阵的谱范数，从而支持更大的稳定学习率，并在多个GPT规模模型上验证了其有效性与优越性。


<details>
  <summary>Details</summary>
Motivation: 重新审视Transformer优化问题，建立架构设计、激活尺度、Hessian矩阵与最大可容许学习率之间的直接联系。

Method: 提出SimpleNorm归一化策略以稳定中间激活尺度，并从理论上分析其对损失函数关于网络激活的Hessian矩阵谱范数的降低作用。

Result: SimpleGPT在1B至8B参数规模模型上支持3–10倍于常规的学习率，训练更稳定，且在7B模型训练60K步时将训练损失从2.290降至2.208，优于LLaMA2+QKNorm。

Conclusion: SimpleNorm通过调控激活尺度有效改善二阶几何性质，显著提升优化稳定性与性能，为Transformer优化提供了新视角与实用方案。

Abstract: In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\times$-10$\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.

</details>


### [698] [Learning from Anonymized and Incomplete Tabular Data](https://arxiv.org/abs/2602.01217)
*Lucas Lange,Adrian Böttinger,Victor Christen,Anushka Vidanage,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: 本文提出了一种针对用户驱动隐私下混合原始、泛化和缺失值的表格数据的新数据转换策略，以保留泛化语义并提升机器学习效用。


<details>
  <summary>Details</summary>
Motivation: 用户驱动隐私导致数据中混合原始、泛化和缺失值，传统机器学习方法无法有效处理泛化语义，易将其误作新类别或简单丢弃。

Method: 提出考虑异构匿名化的新型数据变换策略，并与标准插补及大语言模型（LLM）方法对比评估；在多数据集、多种隐私配置和部署场景下进行实验。

Result: 结果表明：泛化值优于纯抑制；最优数据预处理策略依赖具体场景；一致的数据表示对下游效用至关重要。

Conclusion: 有效机器学习依赖于对匿名化值的恰当处理，需兼顾隐私语义与建模需求。

Abstract: User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.

</details>


### [699] [MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations](https://arxiv.org/abs/2602.01219)
*Qishuai Wen,Zhiyuan Huang,Xianghan Meng,Wei He,Chun-Guang Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的高效注意力机制MiTA（Mixture of Top-k Activations），通过压缩和路由策略将传统Transformer中随序列长度N扩展的快速权重MLP压缩为更窄结构，并利用地标查询和top-k激活键值对构建可变形专家，从而在保持表达能力的同时降低长序列计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer注意力机制作为动态实例化的两层快速权重MLP，其宽度随序列长度N增长导致长序列下计算开销过大；现有MoE注意力虽尝试通过稀疏路由缓解，但缺乏统一视角解释各类高效注意力方法。

Method: 将高效注意力统一建模为对快速权重的路由与/或压缩；提出MiTA策略：用少量地标查询压缩N-宽度MLP，并为每个地标查询收集top-k激活的键值对以构建可变形专家。

Result: 在视觉任务上的初步实验验证了MiTA注意力的有效性，展现出优于现有方法的潜力。

Conclusion: MiTA提供了一种新颖且统一的高效注意力设计范式，有望推动长上下文建模的发展，并需进一步优化及拓展至更复杂任务。

Abstract: The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weights becomes prohibitively expensive for extremely long sequences. Recently, this fast-weight scaling perspective has motivated the Mixture-of-Experts (MoE) attention, which partitions the sequence into fast-weight experts and sparsely routes the tokens to them. In this paper, we elevate this perspective to a unifying framework for a wide range of efficient attention methods by interpreting them as scaling fast weights through routing and/or compression. Then we propose a compress-and-route strategy, which compresses the $N$-width MLP into a narrower one using a small set of landmark queries and constructs deformable experts by gathering top-$k$ activated key-value pairs for each landmark query. We call this strategy a Mixture of Top-$k$ Activations (MiTA), and refer to the resulting efficient mechanism as MiTA attention. Preliminary experiments on vision tasks demonstrate the promise of our MiTA attention and motivate further investigation on its optimization and broader applications in more challenging settings.

</details>


### [700] [Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching](https://arxiv.org/abs/2602.01233)
*Tianhao Miao,Zhongyuan Bao,Lejun Zhang*

Main category: cs.LG

TL;DR: 本文提出Lotus方法，通过修改投影过程解决大模型训练中内存消耗、训练时间和模型性能之间的权衡问题，在降低训练时间和内存消耗的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大模型训练效率评估常面临内存、时间与性能间的权衡问题，如GaLore虽节省内存但因SVD增加训练时间，亟需兼顾三者的方法。

Method: 提出Lotus方法，设计一种量化单位梯度位移的准则，实现低秩梯度子空间间的高效切换，避免传统SVD开销。

Result: 实验表明Lotus在训练时间上减少30%，梯度及优化器状态内存消耗降低40%，且在预训练和微调任务中均优于基线方法。

Conclusion: Lotus通过简化投影机制有效缓解了训练效率各指标间的固有矛盾，为大规模模型训练提供了更优的实用方案。

Abstract: Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.

</details>


### [701] [PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding](https://arxiv.org/abs/2602.01322)
*Panagiotis Koromilas,Andreas D. Demou,James Oldfield,Yannis Panagakis,Mihalis Nicolaou*

Main category: cs.LG

TL;DR: 本文提出PolySAE，通过在稀疏自编码器（SAE）解码器中引入高阶多项式项（如二阶、三阶交互）建模特征间的组合关系，同时保持线性编码器以保障可解释性；实验表明其在探测F1分数上平均提升约8%，且学习到的交互权重与共现频率几乎无关（r=0.06），说明其能有效捕捉真正的组合结构而非统计共现。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器（SAEs）采用线性重建，无法区分特征的组合生成（如“Starbucks”由“star”和“coffee”构成）与简单共现，导致需分配单一块状特征，损害可解释性。

Method: 提出PolySAE，在SAE解码器中引入低秩张量分解实现的二阶与三阶特征交互项，保持原有线性编码器不变；交互参数共享投影子空间，控制参数开销（仅增3%）。

Result: 在4个语言模型和3种SAE变体上，PolySAE平均提升探测F1约8%，重建误差相当，并使类别条件特征分布的Wasserstein距离增大2–10倍；交互权重与共现频率相关性极低（r=0.06），显著低于SAE特征协方差（r=0.82）。

Conclusion: PolySAE通过显式建模高阶特征交互，在不牺牲可解释性的前提下，更准确地揭示神经网络中的组合语义结构（如形态绑定、短语构成），超越了基于共现统计的浅层关联建模。

Abstract: Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether "Starbucks" arises from the composition of "star" and "coffee" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.

</details>


### [702] [Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes](https://arxiv.org/abs/2602.01247)
*Maryam Maghsoudi,Ayushi Mishra*

Main category: cs.LG

TL;DR: 本文通过机械可解释性方法，探究脑-语解码模型中不同言语模式（发声、默念、想象）间信息传递的内在机制，发现言语模式位于共享的连续因果流形上，跨模态迁移由紧凑的层特异性子空间而非弥散活动介导。


<details>
  <summary>Details</summary>
Motivation: 尽管脑-语解码模型在发声、默念和想象言语中表现稳健，但其如何在不同言语模态间捕获与传递信息的基本机制尚不明确。

Method: 采用跨模态激活修补、三模态插值、粗到细因果追踪、因果擦除及神经元级激活修补等机械可解释性技术，分析模型内部表征。

Result: 发现小而紧凑（非孤立亦非弥散）的神经元子集影响跨模态迁移；言语模式处于共享的连续因果流形上；跨模态迁移由层特异性紧凑子空间介导。

Conclusion: 言语模态信息在脑-语解码模型中呈分层且方向依赖的表征结构，其组织与使用具有可解释的因果基础。

Abstract: Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.

</details>


### [703] [Sample Efficient Active Algorithms for Offline Reinforcement Learning](https://arxiv.org/abs/2602.01260)
*Soumyadeep Roy,Shashwat Kushwaha,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 本文提出了一种基于高斯过程（GP）不确定性建模的主动强化学习（ActiveRL）算法，理论证明其可在O(1/ε²)次主动交互内学习ε-最优策略，优于纯离线方法的Ω(1/ε²(1−γ)⁴)样本复杂度，揭示了主动引导不确定性降低可提升信息效率。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习受限于数据覆盖不足和分布偏移；虽有ActiveRL经验成功，但缺乏理论分析。

Method: 基于高斯过程建模价值函数不确定性，结合GP浓度不等式与信息增益界，设计主动采样算法。

Result: 给出高概率保证：仅需O(1/ε²)次主动转移即可学习ε-最优策略，优于纯离线方法；实验证实算法有效性。

Conclusion: ActiveRL通过有指导的不确定性缩减，以极少量在线数据实现价值函数快速收敛，达到近似最优的信息效率，并在贝叶斯非参数回归与强化学习理论间建立桥梁。

Abstract: Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.

</details>


### [704] [When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning](https://arxiv.org/abs/2602.01365)
*Wang Yang,Shouren Wang,Chaoda Song,Chuang Ma,Xinpeng Li,Nengbo Wang,Kaixiong Zhou,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: 本文首次系统分析了GRPO在多领域推理任务中不同训练顺序（顺序式vs混合式）的影响，发现其性能表现出显著的不对称性、顺序敏感性和策略依赖性。


<details>
  <summary>Details</summary>
Motivation: Group Relative Policy Optimization (GRPO) 在提升大模型推理能力方面效果显著，但其在不同领域训练顺序下的行为尚不明确，尤其是顺序训练与混合训练的影响缺乏系统研究。

Method: 对数学、科学、逻辑和谜题四类推理任务，系统比较了单领域训练、顺序多领域训练（如math→science）与混合多领域训练的效果，并量化分析跨领域泛化与顺序依赖性。

Result: （1）单领域泛化高度不对称：其他领域训练可使数学推理准确率提升约25%，但对逻辑和谜题几乎无迁移；（2）跨领域交互强依赖训练顺序：math→science达83%/41%，反之则降至77%/25%；（3）无通用最优策略：顺序训练利于数学（最高84%），混合训练利于科学与逻辑，错误排序可致性能大幅下降（如从70%降至56%）。

Conclusion: GRPO在多领域设置下具有显著的不对称性、顺序敏感性和策略依赖性，亟需设计兼顾领域与顺序的训练范式。

Abstract: Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\rightarrow$science achieves 83\% / 41\% accuracy on math / science, while reversing the order to science$\rightarrow$math degrades performance to 77\% / 25\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\% to 56\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.

</details>


### [705] [BicKD: Bilateral Contrastive Knowledge Distillation](https://arxiv.org/abs/2602.01265)
*Jiangnan Zhu,Yukai Xu,Li Xiong,Yixuan Liu,Junxu Liu,Hong kyu Lee,Yujie Gu*

Main category: cs.LG

TL;DR: 本文提出了一种双边对比知识蒸馏方法（BicKD），通过引入双边对比损失，增强不同类别泛化空间的正交性并保持同类一致性，从而在样本级和类别级上实现教师与学生预测模式的显式对比，并正则化预测分布的几何结构，显著提升知识迁移效果。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法（如Hinton提出的vanilla KD）仅进行样本级概率对齐，缺乏类别级比较机制，且未对概率空间施加结构约束。

Method: 提出双边对比知识蒸馏（BicKD），设计新型双边对比损失，强化不同类别泛化空间的正交性、保持同类一致性，实现样本级与类别级预测模式的联合对比，并正则化预测分布的几何结构。

Result: 在多种模型架构与基准数据集上，BicKD持续超越当前最优知识蒸馏方法，显著提升知识迁移性能。

Conclusion: BicKD是一种简单而有效的方法，通过引入结构化对比机制，弥补了传统KD在类别级建模与概率空间几何约束上的不足，提升了蒸馏效果的鲁棒性与泛化性。

Abstract: Knowledge distillation (KD) is a machine learning framework that transfers knowledge from a teacher model to a student model. The vanilla KD proposed by Hinton et al. has been the dominant approach in logit-based distillation and demonstrates compelling performance. However, it only performs sample-wise probability alignment between teacher and student's predictions, lacking an mechanism for class-wise comparison. Besides, vanilla KD imposes no structural constraint on the probability space. In this work, we propose a simple yet effective methodology, bilateral contrastive knowledge distillation (BicKD). This approach introduces a novel bilateral contrastive loss, which intensifies the orthogonality among different class generalization spaces while preserving consistency within the same class. The bilateral formulation enables explicit comparison of both sample-wise and class-wise prediction patterns between teacher and student. By emphasizing probabilistic orthogonality, BicKD further regularizes the geometric structure of the predictive distribution. Extensive experiments show that our BicKD method enhances knowledge transfer, and consistently outperforms state-of-the-art knowledge distillation techniques across various model architectures and benchmarks.

</details>


### [706] [The Gradient-Causal Gap: Why Gradient Importance Fails on Complex Tasks](https://arxiv.org/abs/2602.01442)
*Donald Ye*

Main category: cs.LG

TL;DR: 本文揭示了Transformer模型中梯度大小与因果重要性之间存在‘梯度-因果鸿沟’：在简单任务中二者尚有一定相关性，但随任务复杂度增加而急剧减弱甚至反转；基于梯度的剪枝不可靠——低梯度‘隐藏英雄’被剪会严重损害泛化，高梯度‘梯度冗余’被剪效果随机，既可能无害（优化噪声），也可能致命（过拟合回路）。


<details>
  <summary>Details</summary>
Motivation: 探究梯度大小是否能可靠反映神经元或参数的因果重要性，尤其在算法任务中验证梯度驱动剪枝的合理性。

Method: 形式化定义‘梯度-因果鸿沟’，在多种算法任务（如字符串反转、排序）上计算梯度幅值与因果重要性之间的皮尔逊相关系数ρ，并通过系统性剪枝实验评估不同梯度分位数组件被移除对分布外（OOD）准确率的影响。

Result: 梯度与因果重要性相关性随任务复杂度上升而显著下降（ρ从0.73降至0.32，甚至为-0.11）；剪除低梯度组件导致OOD准确率下降32%；剪除高梯度组件效果高度随机，部分种子下无影响，部分下造成灾难性失败。

Conclusion: 梯度大小不能作为因果重要性的可靠代理，基于梯度的剪枝方法缺乏可预测性和鲁棒性，无法保障模型能力保留。

Abstract: Removing ''important'' high-gradient components from a neural network can improve generalization, while removing unimportant'' low-gradient components can destroy it. We demonstrate this paradox by formalizing the \textit{Gradient-Causal Gap} in Transformers trained on algorithmic tasks. While gradient magnitude and causal importance align on simple tasks ($ρ=0.73$ for reversal), this relationship collapses as task complexity increases ($ρ=0.32$ for sorting), sometimes becoming inverted ($ρ=-0.11$). Pruning experiments reveal that gradient magnitude is not merely inaccurate but \textit{unpredictably} so. Removing low-gradient ''Hidden Heroes'' consistently devastates OOD accuracy ($-32\%$). Removing high-gradient ''Gradient Bloats'' is a coin flip: harmless in most seeds (indicating optimization noise), catastrophic in others (indicating overfitting circuits). This unpredictability means gradient-based pruning cannot reliably preserve model capabilities.

</details>


### [707] [Diving into Kronecker Adapters: Component Design Matters](https://arxiv.org/abs/2602.01267)
*Jiayu Bai,Danchen Yu,Zhenyu Liao,TianQi Hou,Feng Zhou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 本文提出Component Designed Kronecker Adapters (CDKA)，通过细粒度分析Kronecker适配器的组件结构（维度与数量），揭示其对模型容量和与全量微调对齐性的影响，并提供参数预算感知的配置指南与训练稳定策略。


<details>
  <summary>Details</summary>
Motivation: 现有Kronecker适配器将组件结构视为固定或启发式设计，未充分探索其维度与数量对模型容量的影响。

Method: 对Kronecker组件的维度和数量进行细粒度分析，提出CDKA方法，结合参数预算感知的配置指南和定制化训练稳定策略。

Result: 在多个自然语言处理任务上验证了CDKA的有效性，显著提升Kronecker适配器性能并增强其与全量微调的对齐性。

Conclusion: Kronecker适配器的组件结构是影响其容量与性能的关键因素；CDKA通过结构化设计实现了更高效、更稳定的参数高效微调。

Abstract: Kronecker adapters have emerged as a promising approach for fine-tuning large-scale models, enabling high-rank updates through tunable component structures. However, existing work largely treats the component structure as a fixed or heuristic design choice, leaving the dimensions and number of Kronecker components underexplored. In this paper, we identify component structure as a key factor governing the capacity of Kronecker adapters. We perform a fine-grained analysis of both the dimensions and number of Kronecker components. In particular, we show that the alignment between Kronecker adapters and full fine-tuning depends on component configurations. Guided by these insights, we propose Component Designed Kronecker Adapters (CDKA). We further provide parameter-budget-aware configuration guidelines and a tailored training stabilization strategy for practical deployment. Experiments across various natural language processing tasks demonstrate the effectiveness of CDKA. Code is available at https://github.com/rainstonee/CDKA.

</details>


### [708] [A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning](https://arxiv.org/abs/2602.01523)
*Akifumi Wachi,Hirota Kinoshita,Shokichi Takakura,Rei Higuchi,Taiji Suzuki*

Main category: cs.LG

TL;DR: 本文提出了一种基于相对预算ξ=H/E[T]的理论，解释强化学习在提升大语言模型推理能力时效果差异的原因，并揭示了样本效率随ξ变化的三个阶段：不足、平衡与充足，实验验证了ξ≈1.5–2.0时学习效率与推理性能最优。


<details>
  <summary>Details</summary>
Motivation: 强化学习提升大语言模型推理能力的效果因任务和计算资源而异，缺乏统一解释框架。

Method: 提出相对预算ξ=H/E[T]作为核心指标，结合理论分析（奖励方差、信息轨迹概率）与有限样本在线RL保证，并通过理想化假设下的案例研究及真实场景实验验证。

Result: 识别出三个学习效率 regime：ξ→0时样本复杂度爆炸；ξ=Θ(1)时最高效；ξ→∞时增益递减；实证发现ξ∈[1.5, 2.0]对应最优效率与推理性能。

Conclusion: 相对预算ξ是调控RL推理训练效率的关键标量，可指导预算分配与策略设计，实现更高效、可预测的推理能力提升。

Abstract: Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \emph{relative-budget} theory explaining this variation through a single quantity called relative budget $ξ:= H/\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $ξ$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \emph{deficient} regime ($ξ\to 0$), informative trajectories are rare and the sample complexity explodes; in the \emph{balanced} regime ($ξ=Θ(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \emph{ample} regime ($ξ\to \infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $ξ\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.

</details>


### [709] [Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics](https://arxiv.org/abs/2602.01270)
*Boxuan Zhang,Weipu Zhang,Zhaohan Feng,Wei Xiao,Jian Sun,Jie Chen,Gang Wang*

Main category: cs.LG

TL;DR: 本文提出了Mixture-of-World Models (MoW)，一种用于多任务强化学习的可扩展、参数高效的世界模型架构，通过模块化变分自编码器、混合Transformer动力学模型和梯度驱动的任务聚类，在Atari 100k和Meta-World上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决多任务强化学习（MTRL）在视觉领域中因任务异质性高而导致的样本效率低问题，传统单体世界模型难以建模多样化任务动力学。

Method: 提出MoW架构：1）模块化变分自编码器实现任务自适应视觉压缩；2）混合Transformer动力学模型，含任务条件专家与共享骨干；3）基于梯度的任务聚类策略以高效分配参数。

Result: Atari 100k上，单个MoW代理在26个游戏上平均人类归一化得分为110.4%，接近STORM集成模型的114.2%，但参数减少50%；Meta-World上30万步内平均成功率74.5%，创SOTA。

Conclusion: MoW为通用世界模型提供了可扩展且参数高效的架构基础，显著提升多任务RL的样本效率与泛化能力。

Abstract: A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.

</details>


### [710] [From Intents to Actions: Agentic AI in Autonomous Networks](https://arxiv.org/abs/2602.01271)
*Burak Demirel,Pablo Soldati,Yu Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于智能体（Agentic AI）的意图驱动自主网络系统，通过三个协同工作的专用智能体（解释器、优化器、控制器），将高层服务意图（如低时延、高吞吐、节能）自动转化为底层控制动作，实现可扩展、自适应的网络自治。


<details>
  <summary>Details</summary>
Motivation: 现有启发式方法难以将多样甚至冲突的服务意图（如超低时延、高吞吐、能效）可靠地映射为具体网络控制动作，亟需一种能理解、推理并动态适配意图与网络状态的自主化框架。

Method: 构建三层智能体架构：1）由大语言模型驱动的监督解释器，负责意图的词法解析与认知精炼；2）优化器将解析后的模板建模为可解优化问题，分析多目标权衡并生成偏好；3）基于多目标强化学习的偏好驱动控制器，在Pareto前沿上执行近优控制。

Result: 该系统实现了对异构服务意图的端到端自主解析、推理、适应与执行，显著提升了网络在动态环境下的意图满足能力与可扩展性。

Conclusion: Agentic AI范式为解决意图驱动网络中的语义鸿沟与多目标冲突提供了可行路径，验证了分层智能体协同在自治通信网络中的有效性与潜力。

Abstract: Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.

</details>


### [711] [Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.01601)
*Hieu Trung Nguyen,Bao Nguyen,Wenao Ma,Yuzhi Zhao,Ruifeng She,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种方差感知的预测性采样分配策略（VIP），通过高斯过程建模预测各prompt的成功概率，并据此估计梯度方差，进而优化 rollout 分配以最小化策略更新的梯度方差，在有限计算预算下提升强化学习的采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GRPO）对所有prompt采用固定rollout数量，忽视了不同prompt的信息量差异，导致计算资源浪费和训练效率低下。

Method: 提出VIP方法：利用轻量级高斯过程模型根据近期rollout预测各prompt的成功概率；将概率转化为梯度方差估计；在硬性计算预算约束下，通过凸优化求解最优rollout分配。

Result: 在多个基准任务上，VIP相比均匀分配或启发式分配策略显著提升了采样效率和最终性能。

Conclusion: 方差感知的动态rollout分配能更高效地利用计算资源，是提升奖励可验证型强化学习训练效率的有效途径。

Abstract: Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.

</details>


### [712] [Richer Bayesian Last Layers with Subsampled NTK Features](https://arxiv.org/abs/2602.01279)
*Sergio Calvo-Ordoñez,Jonathan Plenk,Richard Bergna,Álvaro Cartea,Yarin Gal,Jose Miguel Hernández-Lobato,Kamil Ciosek*

Main category: cs.LG

TL;DR: 本文提出了一种改进贝叶斯最后层（BLLs）的方法，通过将神经正切核（NTK）特征投影到最后一层特征张成的空间中，从而在保持计算高效的同时更准确地估计模型的认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯最后层（BLLs）仅对网络最后一层进行贝叶斯建模，忽略了前层带来的认知不确定性，导致其低估认知不确定性。

Method: 利用神经正切核（NTK）特征向最后一层特征空间的投影，实现兼顾全网络变异性与低计算开销的后验推断；并引入均匀子采样策略以进一步降低计算成本，并给出理论误差界。

Result: 所提方法得到的后验方差严格不小于标准BLL，从而修正其低估认知不确定性的缺陷；在多个任务（UCI回归、上下文赌博机、图像分类、OOD检测）上验证了其校准性与不确定性估计的提升，同时降低了计算开销。

Conclusion: 该方法在不显著增加计算负担的前提下，有效提升了贝叶斯最后层对认知不确定性的建模能力，是一种实用且理论可保证的不确定性估计改进方案。

Abstract: Bayesian Last Layers (BLLs) provide a convenient and computationally efficient way to estimate uncertainty in neural networks. However, they underestimate epistemic uncertainty because they apply a Bayesian treatment only to the final layer, ignoring uncertainty induced by earlier layers. We propose a method that improves BLLs by leveraging a projection of Neural Tangent Kernel (NTK) features onto the space spanned by the last-layer features. This enables posterior inference that accounts for variability of the full network while retaining the low computational cost of inference of a standard BLL. We show that our method yields posterior variances that are provably greater or equal to those of a standard BLL, correcting its tendency to underestimate epistemic uncertainty. To further reduce computational cost, we introduce a uniform subsampling scheme for estimating the projection matrix and for posterior inference. We derive approximation bounds for both types of sub-sampling. Empirical evaluations on UCI regression, contextual bandits, image classification, and out-of-distribution detection tasks in image and tabular datasets, demonstrate improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.

</details>


### [713] [$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality](https://arxiv.org/abs/2602.01703)
*Pengyu Li,Lingling Zhang,Zhitao Gao,Yanrui Wu,Yuxuan Dong,Huan Liu,Bifan Wei,Jun Liu*

Main category: cs.LG

TL;DR: 本文提出AGT^AO框架，通过自适应正交性和对抗门控训练，在机器遗忘中平衡彻底擦除敏感数据与保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能无意中记忆敏感数据，带来隐私和安全风险；现有机器遗忘方法难以兼顾彻底遗忘与模型效用。

Method: 提出AGT^AO框架，包含自适应正交性（AO）以缓解遗忘与保留目标间的梯度冲突，以及对抗门控训练（AGT）将遗忘建模为隐空间极小-极大博弈，并引入课程制门控机制抵御内部恢复攻击。

Result: 在遗忘效果（KUR≈0.01）与模型效用（MMLU 58.30）之间取得更优权衡。

Conclusion: AGT^AO有效缓解了机器遗忘中的遗忘-效用权衡难题，实现了鲁棒擦除与实用性的统一。

Abstract: While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.

</details>


### [714] [Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses](https://arxiv.org/abs/2602.01285)
*Kangjun Noh,Seongchan Lee,Ilmun Kim,Kyungwoo Song*

Main category: cs.LG

TL;DR: 本文提出了一种名为MACI的多模型自适应共形推断方法，通过乘性过滤框架和组条件校准，在保证统计有效性的同时显著提升事实性声明的保留率与计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有共形推断方法在保障LLM输出事实性时存在过度保守或建模能力不足的问题，难以兼顾高覆盖保证与高声明保留率。

Method: 将共形推断重构为乘性过滤框架，以多个LLM生成的声明级事实性得分乘积建模整体事实性，并通过组条件校准确保有效性；提出Multi-LLM Adaptive Conformal Inference (MACI) 方法。

Result: MACI在实验中稳定达到用户指定的覆盖水平，相比基线方法显著提升了声明保留率并降低了时间开销。

Conclusion: MACI是一种兼顾统计保证、高保留率与高效性的新型共形推断框架，适用于医学、法律等高风险领域中LLM的事实性保障。

Abstract: Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI

</details>


### [715] [EDIS: Diagnosing LLM Reasoning via Entropy Dynamics](https://arxiv.org/abs/2602.01288)
*Chenghua Zhu,Siyan Wu,Xiangkang Zeng,Zishan Xu,Zhaolu Kang,Yifu Guo,Yuquan Lu,Junduan Huang,Guojing Zhou*

Main category: cs.LG

TL;DR: 本文提出熵动态不稳定性分数（EDIS），利用生成过程中词元级熵的时间演化模式（如爆发式尖峰和峰谷式尖峰）来识别错误推理，显著提升大语言模型推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于熵的置信度信号将置信度视为静态量（如对token熵的聚合），忽略了其在生成过程中的时间演化所蕴含的丰富信息。

Method: 分析token级熵轨迹，识别区分正确与错误推理的典型动态模式，并据此提出轨迹级指标Entropy Dynamics Instability Score（EDIS）来量化熵演化的不稳定性。

Result: 发现错误推理普遍存在熵动态不稳定现象（如burst spikes和peak-valley spikes），且该现象跨模型与训练阶段稳定存在；EDIS可有效用于推理时选择与训练时样本筛选，显著提升推理准确率。

Conclusion: 熵动态是理解与改进大语言模型推理能力的一个被忽视但极具信息量的新视角，EDIS为此提供了可操作的量化工具。

Abstract: Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.

</details>


### [716] [Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models](https://arxiv.org/abs/2602.01289)
*Dung Anh Hoang,Cuong Pham anh Trung Le,Jianfei Cai,Toan Do*

Main category: cs.LG

TL;DR: 本文提出了一种针对扩散模型的新型后训练量化（PTQ）方法，通过为不同时刻（timesteps）的校准样本学习最优权重，以对齐各时刻量化模型的梯度，从而提升量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的后训练量化方法在不同时刻使用统一的校准样本权重和统一的量化策略，忽略了不同时刻数据分布与梯度方向的差异，导致量化性能受限。

Method: 提出一种可学习的加权校准策略，在PTQ过程中为每个timestep的校准样本分配自适应权重，使量化后模型在各timestep上的梯度方向趋于一致，缓解梯度冲突。

Result: 在CIFAR-10、LSUN-Bedrooms和ImageNet上的实验表明，该方法显著优于现有扩散模型PTQ方法。

Conclusion: 针对扩散模型时序特性设计的加权校准PTQ策略，能更有效地降低量化误差、提升采样效率与模型精度。

Abstract: Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.

</details>


### [717] [The BoBW Algorithms for Heavy-Tailed MDPs](https://arxiv.org/abs/2602.01295)
*Yu Chen,Yuhao Liu,Jiatai Huang,Yihan Du,Longbo Huang*

Main category: cs.LG

TL;DR: 本文研究了具有重尾反馈的阶段性马尔可夫决策过程（HTMDPs），提出了两种新算法HT-FTRL-OM和HT-FTRL-UOB，实现了对抗与随机环境下的最佳兼顾（BoBW）性能：在对抗环境中实现T^{1/α}量级的遗憾界，在随机环境中实现对数级遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有HTMDP方法在随机环境中过于保守，在对抗环境中缺乏适应性。

Method: 提出基于FTRL框架的两种算法：HT-FTRL-OM（已知转移）和HT-FTRL-UOB（未知转移），分别采用新型跳过损失估计器和悲观跳过损失估计器，并引入局部控制机制、次优质量传播原理及新遗憾分解技术。

Result: HT-FTRL-OM在已知转移下达到O~(T^{1/α})（对抗）和O(log T)（随机）遗憾；HT-FTRL-UOB在未知转移下达到O~(T^{1/α} + √T)（对抗）和O(log²T)（随机）遗憾。

Conclusion: 所提算法首次在HTMDPs中实现BoBW保证，理论分析克服了重尾损失、转移不确定性与跳过偏差耦合的关键难点。

Abstract: We investigate episodic Markov Decision Processes with heavy-tailed feedback (HTMDPs). Existing approaches for HTMDPs are conservative in stochastic environments and lack adaptivity in adversarial regimes. In this work, we propose algorithms ```HT-FTRL-OM``` and ```HT-FTRL-UOB``` for HTMDPs that achieve Best-of-Both-Worlds (BoBW) guarantees: instance-independent regret in adversarial environments and logarithmic instance-dependent regret in self-bounding (including the stochastic case) environments. For the known transition setting, ```HT-FTRL-OM``` applies the Follow-The-Regularized-Leader (FTRL) framework over occupancy measures with novel skipping loss estimators, achieving a $\widetilde{\mathcal{O}}(T^{1/α})$ regret bound in adversarial regimes and a $\mathcal{O}(\log T)$ regret in stochastic regimes. Building upon this framework, we develop a novel algorithm ```HT-FTRL-UOB``` to tackle the more challenging unknown-transition setting. This algorithm employs a pessimistic skipping loss estimator and achieves a $\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$ regret in adversarial regimes and a $\mathcal{O}(\log^2(T))$ regret in stochastic regimes. Our analysis overcomes key barriers through several technical insights, including a local control mechanism for heavy-tailed shifted losses, a new suboptimal-mass propagation principle, and a novel regret decomposition that isolates transition uncertainty from heavy-tailed estimation errors and skipping bias.

</details>


### [718] [Dissecting Outlier Dynamics in LLM NVFP4 Pretraining](https://arxiv.org/abs/2602.02047)
*Peijie Dong,Ruibo Fan,Yuechen Tao,Di Mou,Wenhu Hu,Zhenheng Tang,Yinghao Yu,Jiamang Wang,Wenbo Su,Guodong Yang,Liping Zhang,Xiaowen Chu,Baochun Li,Bo Li*

Main category: cs.LG

TL;DR: 本文研究了NVFP4量化训练大语言模型时的异常值问题，发现异常值在训练过程中从瞬时尖峰演变为持续热点通道，并提出了Hot-Channel Patch（HCP）补偿机制和CHON训练方案，显著缩小了与BF16训练的损失差距。


<details>
  <summary>Details</summary>
Motivation: NVFP4量化虽提升效率，但因动态范围有限导致对异常值敏感，仍存在与BF16的性能差距，需深入理解异常值成因与演化规律。

Method: 通过纵向分析不同架构（Softmax Attention与Linear Attention）中异常值的空间分布、成因及时间演化规律，并据此设计Hot-Channel Patch（HCP）在线补偿机制和CHON训练方案，保护post-QK操作并动态修复热点通道。

Result: 在GLA-1.3B模型上训练60B tokens，CHON将NVFP4相对于BF16的损失差距从0.94%降至0.58%，同时保持下游任务准确率。

Conclusion: 异常值具有结构化、时序演化特性；针对性保护关键操作（如post-QK）并动态补偿热点通道，可显著提升低比特训练稳定性与性能。

Abstract: Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with "post-QK" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.

</details>


### [719] [Dispelling the Curse of Singularities in Neural Network Optimizations](https://arxiv.org/abs/2602.01308)
*Hengjie Cao,Mengyi Chen,Yifeng Yang,Fang Dong,Ruijun Huang,Anrui Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Dongsheng Li,Wenyi Fang,Yuanyi Lin,Fan Wu,Li Shang*

Main category: cs.LG

TL;DR: 本文从参数空间中奇点的出现与放大这一新视角研究深度神经网络的优化不稳定性，提出‘奇点诅咒’现象，并设计轻量、灵活且有效的参数奇点平滑（PSS）方法来缓解该问题，显著提升训练稳定性、可训练性、效率与泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中常出现优化不稳定现象，现有研究多关注梯度爆炸/消失或损失尖峰，但较少从参数空间奇点演化角度进行系统分析。

Method: 提出Parametric Singularity Smoothing (PSS) 方法，通过平滑权重矩阵的奇异谱来抑制参数与表征空间中奇点的相互强化增长；理论分析揭示梯度Frobenius范数受权重矩阵最大奇异值约束，而奇点增长会削弱该约束。

Result: 在多种数据集、网络架构和优化器上验证了PSS能有效缓解训练不稳定、恢复已崩溃训练、提升训练效率与泛化性能。

Conclusion: 参数空间奇点的动态演化是导致优化不稳定的关键机制之一；PSS作为一种通用正则化策略，可稳健地打破‘奇点诅咒’，增强深度学习系统的鲁棒性与可扩展性。

Abstract: This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further intensify alignment with representations, leading to increased singularities in the representation space. We show that the gradient Frobenius norms are bounded by the top singular values of the weight matrices, and as training progresses, the mutually reinforcing growth of weight and representation singularities, termed the curse of singularities, relaxes these bounds, escalating the risk of sharp loss explosions. To counter this, we propose Parametric Singularity Smoothing (PSS), a lightweight, flexible, and effective method for smoothing the singular spectra of weight matrices. Extensive experiments across diverse datasets, architectures, and optimizers demonstrate that PSS mitigates instability, restores trainability even after failure, and improves both training efficiency and generalization.

</details>


### [720] [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103)
*Liyan Xu,Mo Yu,Fandong Meng,Jie Zhou*

Main category: cs.LG

TL;DR: 本文提出Tele-Lens方法探测大语言模型（LLM）隐状态中的潜在规划能力，发现LLM推理具有‘短视视野’特征，据此提出并验证了基于CoT局部位置估计整体不确定性、以及自动识别CoT绕过路径的新方法。


<details>
  <summary>Details</summary>
Motivation: 调和先前关于思维链（CoT）作用的矛盾观察：一方面LLM存在隐式规划，削弱显式CoT必要性；另一方面CoT对多步推理任务仍至关重要。需深入理解LLM内部状态与外显推理轨迹的关系。

Method: 提出探针方法Tele-Lens，用于分析不同任务领域中LLM隐藏状态所承载的潜在规划强度，并基于‘短视视野’发现构建不确定性估计与CoT绕过识别方法。

Result: 实证表明LLM推理主要为增量式过渡，缺乏精确全局规划（即具有‘短视视野’）；验证了仅用少量CoT位置即可有效表征整条路径的不确定性；实现了无需性能下降的CoT绕过自动识别。

Conclusion: LLM的隐式规划能力有限且呈短视性，显式CoT仍有不可替代价值；应重视CoT动态特性，可据此提升不确定性建模与推理过程控制能力。

Abstract: This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.

</details>


### [721] [Imperfect Influence, Preserved Rankings: A Theory of TRAK for Data Attribution](https://arxiv.org/abs/2602.01312)
*Han Tong,Shubhangi Ghosh,Haolin Zou,Arian Maleki*

Main category: cs.LG

TL;DR: 本文对TRAK算法进行了理论分析，揭示了其近似误差的来源，并证明尽管存在显著误差，TRAK仍能高度保持数据点影响的相对排序。


<details>
  <summary>Details</summary>
Motivation: TRAK算法在数据归因中表现优异，但其理论基础、近似精度的适用条件及失效边界尚不明确。

Method: 通过理论分析刻画TRAK算法中各近似步骤（核机器近似与ALO风险近似）所引入的误差，并结合模拟与实证验证理论结论。

Result: 发现TRAK的近似虽带来显著绝对误差，但其估计的影响值与真实影响值之间仍保持高度相关性，从而有效保留数据点间的相对影响排序。

Conclusion: TRAK的实用性源于其对相对排序的鲁棒性，而非绝对影响值的精确性；该结论为其实际应用提供了理论保障。

Abstract: Data attribution, tracing a model's prediction back to specific training data, is an important tool for interpreting sophisticated AI models. The widely used TRAK algorithm addresses this challenge by first approximating the underlying model with a kernel machine and then leveraging techniques developed for approximating the leave-one-out (ALO) risk. Despite its strong empirical performance, the theoretical conditions under which the TRAK approximations are accurate as well as the regimes in which they break down remain largely unexplored. In this paper, we provide a theoretical analysis of the TRAK algorithm, characterizing its performance and quantifying the errors introduced by the approximations on which the method relies. We show that although the approximations incur significant errors, TRAK's estimated influence remains highly correlated with the original influence and therefore largely preserves the relative ranking of data points. We corroborate our theoretical results through extensive simulations and empirical studies.

</details>


### [722] [Unifying Masked Diffusion Models with Various Generation Orders and Beyond](https://arxiv.org/abs/2602.02112)
*Chunsan Hong,Sanghyun Lee,Jong Chul Ye*

Main category: cs.LG

TL;DR: 本文提出了一种可学习生成顺序的掩码扩散模型（LoMDM），统一建模多种生成顺序，并在多个语言建模基准上优于现有离散扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型（MDMs）依赖预设或分阶段学习的生成顺序，存在灵活性差、优化次优等问题。

Method: 提出order-expressive MDM（OeMDM）框架统一描述不同生成顺序；在此基础上构建端到端可学习顺序的LoMDM，联合优化生成顺序策略与扩散主干网络。

Result: LoMDM在多个语言建模基准上显著优于各类离散扩散模型，验证了上下文相关动态生成顺序的有效性。

Conclusion: 生成顺序应与扩散模型联合学习而非固定或分阶段优化；LoMDM为语言生成提供了更灵活、高效且统一的扩散建模范式。

Abstract: Masked diffusion models (MDMs) are a potential alternative to autoregressive models (ARMs) for language generation, but generation quality depends critically on the generation order. Prior work either hard-codes an ordering (e.g., blockwise left-to-right) or learns an ordering policy for a pretrained MDM, which incurs extra cost and can yield suboptimal solutions due to the two-stage optimization. Motivated by this, we propose order-expressive masked diffusion model (OeMDM) for a broad class of diffusion generative processes with various generation orders, enabling the interpretation of MDM, ARM, and block diffusion in a single framework. Furthermore, building on OeMDM, we introduce learnable-order masked diffusion model (LoMDM), which jointly learns the generation ordering and diffusion backbone through a single objective from scratch, enabling the diffusion model to generate text in context-dependent ordering. Empirically, we confirm that LoMDM outperforms various discrete diffusion models across multiple language modeling benchmarks.

</details>


### [723] [High-accuracy sampling for diffusion models and log-concave distributions](https://arxiv.org/abs/2602.01338)
*Fan Chen,Sinho Chewi,Constantinos Daskalakis,Alexander Rakhlin*

Main category: cs.LG

TL;DR: 本文提出了一种扩散模型采样算法，能在polylog(1/δ)步内达到δ误差，显著优于以往方法，并在多种假设下给出复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型采样方法的步数复杂度依赖于1/δ的多项式，效率较低，亟需指数级加速的算法。

Method: 基于高精度分数估计（L²范数误差为Õ(δ)），设计新型采样算法，并结合数据维度d、Lipschitz常数L及内在维度d⋆进行复杂度分析。

Result: 实现polylog(1/δ)步数复杂度；在不同假设下分别达到Õ(d polylog(1/δ))、Õ(√(dL) polylog(1/δ))和Õ(d⋆ polylog(1/δ))；首次实现仅用梯度评估对一般对数凹分布的polylog(1/δ)复杂度采样。

Conclusion: 该工作实现了扩散模型采样复杂度的指数级提升，并拓展至对数凹分布采样，具有理论与应用双重意义。

Abstract: We present algorithms for diffusion model sampling which obtain $δ$-error in $\mathrm{polylog}(1/δ)$ steps, given access to $\widetilde O(δ)$-accurate score estimates in $L^2$. This is an exponential improvement over all previous results. Specifically, under minimal data assumptions, the complexity is $\widetilde O(d\,\mathrm{polylog}(1/δ))$ where $d$ is the dimension of the data; under a non-uniform $L$-Lipschitz condition, the complexity is $\widetilde O(\sqrt{dL}\,\mathrm{polylog}(1/δ))$; and if the data distribution has intrinsic dimension $d_\star$, then the complexity reduces to $\widetilde O(d_\star\,\mathrm{polylog}(1/δ))$. Our approach also yields the first $\mathrm{polylog}(1/δ)$ complexity sampler for general log-concave distributions using only gradient evaluations.

</details>


### [724] [EvoMU: Evolutionary Machine Unlearning](https://arxiv.org/abs/2602.02139)
*Pawel Batorski,Paul Swoboda*

Main category: cs.LG

TL;DR: 本文提出EvoMU方法，通过进化搜索自动发现任务特定的机器遗忘损失函数，在有限计算资源下（4B参数模型）实现了优于现有方法的遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘中存在大量可能的损失函数，人工寻找最优损失函数困难；且不同数据集结构差异导致不存在通用最优损失函数。

Method: 采用进化搜索算法，在损失函数空间中自动搜索适合特定任务的遗忘损失函数，并利用小型4B参数模型（Qwen3-4B-Thinking）实现高效AI协同科学发现。

Result: 在TOFU-5%、TOFU-10%、MUSE和WMDP等基准上超越现有基于损失的遗忘方法，生成了新型有效遗忘损失函数。

Conclusion: EvoMU证明了在有限算力下，AI可作为‘协同科学家’自动完成科学发现任务，为自动化机器遗忘研究提供了新范式。

Abstract: Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.

</details>


### [725] [Finding Differentially Private Second Order Stationary Points in Stochastic Minimax Optimization](https://arxiv.org/abs/2602.01339)
*Difei Xu,Youming Tao,Meng Ding,Chenglin Fan,Di Wang*

Main category: cs.LG

TL;DR: 本文首次研究了在随机（非凸）极小极大优化中寻找差分隐私（DP）二阶平稳点（SOSP）的问题，提出了一种结合嵌套梯度下降-上升、SPIDER方差缩减和高斯扰动的一阶方法，并通过分块分析技术统一处理经验风险与总体风险，取得了匹配现有最优一阶隐私率的二阶收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有文献仅关注极小极大问题的一阶平稳点或经典随机最小化问题的二阶平稳点，缺乏对差分隐私下极小极大问题二阶平稳点的系统研究，尤其缺少对经验风险和总体风险的统一分析。

Method: 提出一种纯一阶方法，融合嵌套梯度下降–上升框架、SPIDER风格方差缩减和高斯扰动以保障隐私；引入q周期分块分析技术，控制随机方差与隐私噪声累积，避免在整个迭代过程中累加。

Result: 在标准光滑性、Hessian-Lipschitz性和强凹性假设下，对经验风险目标达到(α,√(ρ_Φα))-近似二阶平稳点，其中α=𝒪((√d/(nε))^{2/3})；对总体目标，α=𝒪(1/n^{1/3} + (√d/(nε))^{1/2})，均匹配当前最优的隐私一阶平稳性速率。

Conclusion: 本文填补了差分隐私极小极大优化中二阶平稳点理论的空白，提供了首个兼具隐私性、高效性与理论严谨性的统一算法框架，为非凸隐私优化奠定了新基础。

Abstract: We provide the first study of the problem of finding differentially private (DP) second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization. Existing literature either focuses only on first-order stationary points for minimax problems or on SOSP for classical stochastic minimization problems. This work provides, for the first time, a unified and detailed treatment of both empirical and population risks. Specifically, we propose a purely first-order method that combines a nested gradient descent--ascent scheme with SPIDER-style variance reduction and Gaussian perturbations to ensure privacy. A key technical device is a block-wise ($q$-period) analysis that controls the accumulation of stochastic variance and privacy noise without summing over the full iteration horizon, yielding a unified treatment of both empirical-risk and population formulations. Under standard smoothness, Hessian-Lipschitzness, and strong concavity assumptions, we establish high-probability guarantees for reaching an $(α,\sqrt{ρ_Φα})$-approximate second-order stationary point with $α= \mathcal{O}( (\frac{\sqrt{d}}{n\varepsilon})^{2/3})$ for empirical risk objectives and $\mathcal{O}(\frac{1}{n^{1/3}} + (\frac{\sqrt{d}}{n\varepsilon})^{1/2})$ for population objectives, matching the best known rates for private first-order stationarity.

</details>


### [726] [Learning Generative Selection for Best-of-N](https://arxiv.org/abs/2602.02143)
*Shubham Toshniwal,Aleksander Ficek,Siddhartha Jain,Wei Du,Vahid Noroozi,Sadegh Mahdavi,Somshubra Majumdar,Igor Gitman*

Main category: cs.LG

TL;DR: 本文提出了一种通过强化学习（DAPO）训练小型推理模型（1.7B参数）执行强生成式选择（GenSelect）的方法，在数学与代码推理任务上显著超越基线，并能泛化至更强模型的输出选择。


<details>
  <summary>Details</summary>
Motivation: Best-of-N选择质量限制了并行采样提升LLM推理效果的能力；现有生成式选择方法（如GenSelect）在小模型上性能不足。

Method: 从大规模数学和代码指令数据集中构建含正确与错误候选解的合成选择任务，使用DAPO强化学习算法训练1.7B参数模型以奖励正确选择。

Result: 在AIME24/25、HMMT25和LiveCodeBench等基准上，小模型超越提示工程与多数投票基线，常接近或超过更大模型；且能泛化选择更强模型产生的输出。

Conclusion: 强化学习是一种可扩展路径，可使小型模型获得强生成式选择能力，从而实现高效的测试时计算扩展。

Abstract: Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.

</details>


### [727] [Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning](https://arxiv.org/abs/2602.01357)
*Shangzhe Li,Xuchao Zhang,Chetan Bansal,Weitong Zhang*

Main category: cs.LG

TL;DR: 本文将自博弈微调与对抗性模仿学习联系起来，提出了一种基于χ²散度变分目标的新算法，在理论和实验上均验证了其有效性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 自博弈后训练方法虽有效，但其理论基础尚不明确，本文旨在填补这一理论空白。

Method: 将自博弈微调建模为模型与自身参数化的正则化隐式奖励玩家之间的极小-极大博弈，并基于χ²散度变分目标设计新算法。

Result: 理论证明了自博弈微调收敛到均衡点；实验表明所提算法在多个语言模型微调任务上持续优于现有自博弈方法。

Conclusion: 自博弈微调可被统一纳入对抗模仿学习框架，其理论基础得以夯实，且新算法提升了稳定性与性能。

Abstract: Self-play post-training methods has emerged as an effective approach for finetuning large language models and turn the weak language model into strong language model without preference data. However, the theoretical foundations for self-play finetuning remain underexplored. In this work, we tackle this by connecting self-play finetuning with adversarial imitation learning by formulating finetuning procedure as a min-max game between the model and a regularized implicit reward player parameterized by the model itself. This perspective unifies self-play imitation and general preference alignment within a common framework. Under this formulation, we present a game-theoretic analysis showing that the self-play finetuning will converge to it's equilibrium. Guided by this theoretical formulation, we propose a new self-play imitation finetuning algorithm based on the $χ^2$-divergence variational objective with bounded rewards and improved stability. Experiments on various of language model finetuning tasks demonstrate consistent improvements over existing self-play methods and validate our theoretical insights.

</details>


### [728] [Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization](https://arxiv.org/abs/2602.02151)
*Yuli Zhou,Qingxuan Chen,Luca Benini,Guolei Sun,Yawei Li*

Main category: cs.LG

TL;DR: 本文提出VQRound，一种参数高效的自适应舍入优化框架，通过将舍入矩阵重参数化为紧凑码本，并结合轻量级端到端微调，在大幅减少可训练参数（仅0.2%）的同时，提升大语言模型后训练量化中的收敛性与精度。


<details>
  <summary>Details</summary>
Motivation: 传统自适应舍入因密集、逐元素舍入矩阵在十亿级大语言模型上计算开销过大；同时，现有低秩方法无法有效应对LLM权重的重尾分布问题。

Method: 提出VQRound框架：1）将舍入矩阵重参数化为紧凑码本（向量量化）；2）以L∞范数最小化逐元素最坏误差；3）设计基于128样本的轻量端到端码本联合微调流程。

Result: 在OPT、LLaMA、LLaMA2和Qwen3上实验表明，VQRound比传统自适应舍入收敛更快、精度更高，且仅需0.2%的可训练参数。

Conclusion: 自适应舍入可通过码本重参数化与高效初始化/微调实现可扩展性与快速适配，为大模型高效后训练量化提供新范式。

Abstract: Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.

</details>


### [729] [PaAno: Patch-Based Representation Learning for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.01359)
*Jinju Park,Seokho Kang*

Main category: cs.LG

TL;DR: PaAno是一种轻量级时间序列异常检测方法，通过基于时间片段的表示学习和1D CNN嵌入，在低计算开销下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型（如Transformer、基础模型）的时间序列异常检测方法计算开销高、内存占用大，难以用于实时和资源受限场景，且在严格评估下并未显著优于简单方法。

Method: PaAno将时间序列划分为短时序片段，用1D CNN对每个片段提取向量表示；采用三元组损失与前置任务损失联合训练；推理时通过比较当前时刻邻近片段嵌入与正常训练片段嵌入的差异计算异常分数。

Result: 在TSB-AD基准上，PaAno在单变量与多变量时间序列的范围级和点级指标上均达到SOTA，显著优于包括重型架构在内的现有方法。

Conclusion: 轻量级设计（无Transformer等大模型）与有效的片段级表示学习可兼顾效率与精度，为资源受限场景下的时间序列异常检测提供了新范式。

Abstract: Although recent studies on time-series anomaly detection have increasingly adopted ever-larger neural network architectures such as transformers and foundation models, they incur high computational costs and memory usage, making them impractical for real-time and resource-constrained scenarios. Moreover, they often fail to demonstrate significant performance gains over simpler methods under rigorous evaluation protocols. In this study, we propose Patch-based representation learning for time-series Anomaly detection (PaAno), a lightweight yet effective method for fast and efficient time-series anomaly detection. PaAno extracts short temporal patches from time-series training data and uses a 1D convolutional neural network to embed each patch into a vector representation. The model is trained using a combination of triplet loss and pretext loss to ensure the embeddings capture informative temporal patterns from input patches. During inference, the anomaly score at each time step is computed by comparing the embeddings of its surrounding patches to those of normal patches extracted from the training time-series. Evaluated on the TSB-AD benchmark, PaAno achieved state-of-the-art performance, significantly outperforming existing methods, including those based on heavy architectures, on both univariate and multivariate time-series anomaly detection across various range-wise and point-wise performance measures.

</details>


### [730] [Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation](https://arxiv.org/abs/2602.01367)
*Pinar Erbil,Alberto Archetti,Eugenio Lomurno,Matteo Matteucci*

Main category: cs.LG

TL;DR: 本文提出CONVERSE模型，结合变分自编码器与对比学习，在保持高预测性能的同时实现可解释的风险分层。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生存分析中面临性能与可解释性之间的根本权衡：高精度模型（如神经网络）缺乏可解释性，而可解释的聚类方法往往牺牲预测能力。

Method: 提出CONVERSE模型，融合变分自编码器与多层级（簇内/簇间）对比损失，并引入自步学习策略提升训练稳定性；支持簇特定的生存预测头以实现集成预测。

Result: 在四个基准数据集上，CONVERSE在预测性能上达到或超越现有深度生存模型，同时提供有意义且可解释的患者风险分层。

Conclusion: CONVERSE成功弥合了深度生存分析中预测精度与临床可解释性之间的鸿沟，为临床决策提供了兼具可靠性与透明性的工具。

Abstract: Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.

</details>


### [731] [Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models](https://arxiv.org/abs/2602.02244)
*Hao Wang,Hao Gu,Hongming Piao,Kaixiong Gong,Yuxiao Ye,Xiangyu Yue,Sirui Han,Yike Guo,Dapeng Wu*

Main category: cs.LG

TL;DR: 本文提出CurioSFT，一种在监督微调（SFT）阶段保留熵、增强探索能力的新方法，通过自探索蒸馏与熵引导温度选择提升模型在数学推理任务上的泛化性与后续强化学习（RL）效果。


<details>
  <summary>Details</summary>
Motivation: 标准SFT-then-RL流程中，SFT易导致过自信和生成多样性下降，压缩RL的探索空间；熵正则化虽增熵但趋向均匀分布，无助于有意义的探索。

Method: 提出CurioSFT：(a) 自探索蒸馏——以温度缩放的自生成教师模型为蒸馏目标，激发模型内在探索能力；(b) 熵引导温度选择——按token类型（推理/事实）自适应调节蒸馏强度，兼顾探索与知识稳定性。

Result: 在数学推理任务上，CurioSFT在SFT阶段较基线提升2.5（ID）和2.9（OOD）分；其保留的探索能力使后续RL阶段平均提升5.0分。

Conclusion: 保留熵且有导向性的探索能力可显著提升SFT质量，并正向迁移至RL阶段，CurioSFT为大模型推理训练范式提供了更优的SFT设计。

Abstract: The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.

</details>


### [732] [An Odd Estimator for Shapley Values](https://arxiv.org/abs/2602.01399)
*Fabian Fumagalli,Landon Butler,Justin Singh Kang,Kannan Ramchandran,R. Teal Witter*

Main category: cs.LG

TL;DR: 本文提出OddSHAP，一种基于Shapley值奇函数分量的新估计器，通过傅里叶基和代理模型提升准确性，实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: Shapley值在机器学习中广泛应用但精确计算不可行，现有配对采样方法效果好却缺乏理论解释。

Method: 证明Shapley值仅依赖集合函数的奇分量，提出OddSHAP：在奇子空间上做多项式回归，用傅里叶基分离该子空间，并用代理模型识别高影响交互。

Result: OddSHAP在广泛基准测试中达到当前最优的估计精度。

Conclusion: 配对采样的有效性源于其对偶分量的正交化；OddSHAP利用该原理设计出更高效、一致且准确的Shapley值估计器。

Abstract: The Shapley value is a ubiquitous framework for attribution in machine learning, encompassing feature importance, data valuation, and causal inference. However, its exact computation is generally intractable, necessitating efficient approximation methods. While the most effective and popular estimators leverage the paired sampling heuristic to reduce estimation error, the theoretical mechanism driving this improvement has remained opaque. In this work, we provide an elegant and fundamental justification for paired sampling: we prove that the Shapley value depends exclusively on the odd component of the set function, and that paired sampling orthogonalizes the regression objective to filter out the irrelevant even component. Leveraging this insight, we propose OddSHAP, a novel consistent estimator that performs polynomial regression solely on the odd subspace. By utilizing the Fourier basis to isolate this subspace and employing a proxy model to identify high-impact interactions, OddSHAP overcomes the combinatorial explosion of higher-order approximations. Through an extensive benchmark evaluation, we find that OddSHAP achieves state-of-the-art estimation accuracy.

</details>


### [733] [Statistical Learning Theory in Lean 4: Empirical Processes from Scratch](https://arxiv.org/abs/2602.02285)
*Yuanhe Zhang,Jason D. Lee,Fanghui Liu*

Main category: cs.LG

TL;DR: 本文首次在Lean 4中全面形式化了基于经验过程理论的统计学习理论（SLT），填补了Mathlib库中的关键空白，包括高斯Lipschitz集中性、Dudley熵积分定理（针对次高斯过程）的首个形式化，以及在最小二乘（稀疏）回归中实现尖锐收敛速率的应用；整个工作采用人机协同方式完成，并揭示和修正了教材中隐含假设与缺失细节，为机器学习理论提供了可复用的形式化基础。


<details>
  <summary>Details</summary>
Motivation: 填补Lean 4 Mathlib中统计学习理论（SLT）形式化的空白，提升SLT的严谨性与可验证性，并通过形式化过程发现并修正教材中隐含假设和缺失细节。

Method: 基于经验过程理论，在Lean 4中构建端到端形式化基础设施；采用人机协同工作流：人类设计证明策略，AI代理执行战术级证明构造；形式化高斯Lipschitz集中性、Dudley熵积分定理及最小二乘（稀疏）回归应用。

Result: 实现了Lean 4中首个完整的SLT形式化体系，包括关键定理的严格证明与可验证代码；暴露并修正了传统SLT教材中的隐含假设；产出开源、人类验证的SLT Lean 4工具箱。

Conclusion: 本工作建立了统计学习理论可复用、可扩展的形式化基础，推动机器学习理论向更高严谨性发展，并为人机协同数学形式化提供了实践范例。

Abstract: We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory

</details>


### [734] [SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training](https://arxiv.org/abs/2602.01410)
*Yunjie Pan,Yongyi Yang,Hanmei Yang,Scott Mahlke*

Main category: cs.LG

TL;DR: 本文提出SNIP框架，通过细粒度自适应混合精度训练，在支持亚字节精度的GPU上高效预训练大语言模型，显著降低计算开销同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度训练方法在亚字节精度下难以兼顾效率与稳定性，存在收敛性差和泛化能力弱的问题。

Method: SNIP周期性采集激活、梯度和优化器状态统计，定义前向损失发散与反向权重发散两个指标，并构建整数线性规划（ILP）问题以逐层优化精度分配。

Result: 在1B至70B规模Llama类模型上实验表明，SNIP最多降低80% FLOPs，同时保持模型质量，且计算开销极小。

Conclusion: SNIP是一种通用、稳定且高效的自适应混合精度训练框架，适用于不同规模LLM的预训练。

Abstract: Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.

</details>


### [735] [Semi-supervised CAPP Transformer Learning via Pseudo-labeling](https://arxiv.org/abs/2602.01419)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb,Emmanuel Stathatos,Panorios Benardos,George-Christopher Vosniakos*

Main category: cs.LG

TL;DR: 本文提出了一种半监督学习方法，利用oracle模型筛选未标注零件的正确预测结果，用于重训练基于Transformer的CAPP模型，在小规模数据集上显著提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 工业中高质量标注数据稀缺，限制了高阶计算机辅助工艺规划（CAPP）模型的泛化能力。

Method: 采用半监督学习框架：用已有数据训练一个oracle模型，该模型筛选出未见零件的可靠预测结果，并用于对CAPP Transformer模型进行一次性重训练。

Result: 在模拟全分布小规模数据集上的实验表明，该方法相比基线模型持续提升了预测准确率。

Conclusion: 所提方法有效缓解了制造领域数据稀缺问题，提升了CAPP模型在低资源场景下的性能与实用性。

Abstract: High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.

</details>


### [736] [Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models](https://arxiv.org/abs/2602.01428)
*Weiqing He,Xiang Li,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，在保持推测采样效率的同时实现最大水印强度，解决了水印强度与推测采样接受率之间的固有折衷问题。


<details>
  <summary>Details</summary>
Motivation: 水印技术用于追踪大语言模型输出来源，但其实际部署受限于推理效率；而推测采样虽可加速推理，却与水印强度存在冲突：水印越强，推测采样的接受率越低。

Method: 引入一种定量衡量水印强度的指标，该指标在token为伪随机数确定性函数时达最大；将水印强度与接受率的权衡建模为约束优化问题，并推导出两种现有水印方案的Pareto曲线；进一步设计一种机制，将伪随机性注入draft-token接受过程。

Result: 所提机制可在维持推测采样高效率的同时实现最大水印强度；实验表明该方法提升了水印可检测性且不牺牲推理效率。

Conclusion: 水印强度与推测采样效率之间并非绝对互斥，二者可通过引入伪随机性统一协调；该原理为水印与推测采样在实际场景中的高效协同部署提供了理论基础与实践路径。

Abstract: Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.

</details>


### [737] [DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data](https://arxiv.org/abs/2602.01433)
*Muhammad Hasan Ferdous,Md Osman Gani*

Main category: cs.LG

TL;DR: 本文提出了一种基于分解的因果发现框架，将多元时间序列分解为趋势、季节性和残差成分，并分别进行因果分析，最后整合为多尺度因果结构，以提升非平稳和自相关条件下的因果推断准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法直接作用于原始观测数据，在非平稳性和自相关背景下易产生虚假边和错误归因的时序依赖关系。

Method: 将每个时间序列分解为趋势、季节和残差三部分；趋势用平稳性检验评估，季节用核依赖度量，残差用约束式因果发现方法；最后将各成分因果图整合为统一的多尺度因果结构。

Result: 在合成数据和真实气候数据上，该框架比当前最优基线更准确地恢复真实因果结构，尤其在强非平稳和高自相关情形下表现更优。

Conclusion: 分解式因果分析能有效分离长短期因果效应，减少虚假关联，增强因果模型的鲁棒性与可解释性。

Abstract: Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw observations, making them vulnerable to spurious edges and misattributed temporal dependencies. We introduce a decomposition-based causal discovery framework that separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis. Trend components are assessed using stationarity tests, seasonal components using kernel-based dependence measures, and residual components using constraint-based causal discovery. The resulting component-level graphs are integrated into a unified multi-scale causal structure. This approach isolates long- and short-range causal effects, reduces spurious associations, and improves interpretability. Across extensive synthetic benchmarks and real-world climate data, our framework more accurately recovers ground-truth causal structure than state-of-the-art baselines, particularly under strong non-stationarity and temporal autocorrelation.

</details>


### [738] [SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning](https://arxiv.org/abs/2602.02472)
*Qifan Yu,Xinyu Ma,Zhijian Zhuo,Minrui Wang,Deyi Liu,Shiyi Zhan,Yiyuan Ma,Liang Xiang,Xingyan Bin,Di He*

Main category: cs.LG

TL;DR: 本文提出SPARKLING框架，解决中阶段宽度扩展时的训练不稳定性问题，通过RMS尺度一致性保持信号、非对称优化器状态重置与学习率重热身打破梯度对称性，在MoE模型上实现最高35%训练成本降低。


<details>
  <summary>Details</summary>
Motivation: 现有渐进式学习方法多关注深度扩展，宽度扩展尤其在训练中阶段研究不足；而中阶段宽度扩展对节省计算开销至关重要，却面临激活统计失稳和梯度对称性导致特征多样性不足等挑战。

Method: 提出SPARKLING框架：1）采用RMS-scale consistency保证扩展前后激活统计稳定（信号保持）；2）通过不对称优化器状态重置和学习率重热身打破梯度对称性；3）适用于多种宽度轴与优化器族。

Result: 在多个MoE模型上验证，SPARKLING在2倍宽度扩展下相较从头训练降低最多35%训练成本，且性能稳定优于基线。

Conclusion: SPARKLING为中阶段宽度渐进学习提供了有效、通用且稳定的解决方案，显著提升计算效率而不牺牲模型性能。

Abstract: Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\times$ width expansion.

</details>


### [739] [Phase Transitions for Feature Learning in Neural Networks](https://arxiv.org/abs/2602.01434)
*Andrea Montanari,Zihao Wang*

Main category: cs.LG

TL;DR: 本文研究了在比例渐近设定下两层神经网络的梯度下降动力学，揭示了特征学习存在一个相变阈值δ_NN，该阈值由Hessian矩阵谱的相变决定。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络如何从数据中学习低维表示，并形式化多指标模型下的特征学习现象。

Method: 在n,d→∞且n/d→δ的比例渐近设定下，分析两层神经网络的梯度下降动力学，特别关注Hessian矩阵谱的行为。

Result: 推导出两层网络特征学习的相变阈值δ_NN，该阈值对应于训练第二阶段Hessian矩阵谱的相变。

Conclusion: δ_NN阈值的刻画为研究网络架构和训练算法对学习动力学的影响提供了新途径。

Abstract: According to a popular viewpoint, neural networks learn from data by first identifying low-dimensional representations, and subsequently fitting the best model in this space. Recent works provide a formalization of this phenomenon when learning multi-index models. In this setting, we are given $n$ i.i.d. pairs $({\boldsymbol x}_i,y_i)$, where the covariate vectors ${\boldsymbol x}_i\in\mathbb{R}^d$ are isotropic, and responses $y_i$ only depend on ${\boldsymbol x}_i$ through a $k$-dimensional projection ${\boldsymbol Θ}_*^{\sf T}{\boldsymbol x}_i$. Feature learning amounts to learning the latent space spanned by ${\boldsymbol Θ}_*$.
  In this context, we study the gradient descent dynamics of two-layer neural networks under the proportional asymptotics $n,d\to\infty$, $n/d\toδ$, while the dimension of the latent space $k$ and the number of hidden neurons $m$ are kept fixed. Earlier work establishes that feature learning via polynomial-time algorithms is possible if $δ> δ_{\text{alg}}$, for $δ_{\text{alg}}$ a threshold depending on the data distribution, and is impossible (within a certain class of algorithms) below $δ_{\text{alg}}$. Here we derive an analogous threshold $δ_{\text{NN}}$ for two-layer networks. Our characterization of $δ_{\text{NN}}$ opens the way to study the dependence of learning dynamics on the network architecture and training algorithm.
  The threshold $δ_{\text{NN}}$ is determined by the following scenario. Training first visits points for which the gradient of the empirical risk is large and learns the directions spanned by these gradients. Then the gradient becomes smaller and the dynamics becomes dominated by negative directions of the Hessian. The threshold $δ_{\text{NN}}$ corresponds to a phase transition in the spectrum of the Hessian in this second phase.

</details>


### [740] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

TL;DR: RLAnything是一个通过闭环优化动态构建环境、策略和奖励模型的强化学习框架，显著提升了LLM与智能体任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在LLM与智能体场景中面临奖励信号弱、环境固定、反馈单一等问题，需更强的学习信号与自适应机制。

Method: 提出闭环优化框架：联合训练策略（融合步级与结果反馈）与奖励模型（一致性反馈），并基于理论驱动的自动环境适配机制，利用critic反馈优化环境、策略与奖励模型。

Result: 在OSWorld、AlfWorld和LiveBench等任务上显著提升性能（如Qwen3-VL-8B-Thinking提升9.1%，Qwen2.5-7B-Instruct分别提升18.7%和11.9%）；优化后的奖励模型信号优于人工标注结果。

Conclusion: RLAnything通过动态闭环优化显著增强RL系统泛化性与有效性，为LLM与智能体强化学习提供了通用高效的新范式。

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


### [741] [Theoretical Analysis of Measure Consistency Regularization for Partially Observed Data](https://arxiv.org/abs/2602.01437)
*Yinsong Wang,Shahin Shahrampour*

Main category: cs.LG

TL;DR: 本文提出Measure Consistency Regularization（MCR）方法，通过理论分析揭示其在部分可观测场景下提升插补质量的机制，并提出基于对偶间隙监控的早停策略以保障泛化优势。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习面临数据损坏、特征或模态缺失等问题，虽已有基于一致性正则（如MCR）的经验成功，但其理论基础仍不清晰。

Method: 从神经网络距离视角出发，理论分析MCR的泛化优势来源；提出基于对偶间隙监控的早停训练协议；并在多种模型与真实数据模拟中验证。

Result: 识别出决定MCR泛化优势的关键项；证明该优势在非理想训练条件下并非总能保证；所提早停策略可有效保留泛化增益；实验证明MCR在不同架构和数据源下具有普适性。

Conclusion: MCR的有效性依赖于特定理论条件，其成功需结合恰当的训练控制；本文为MCR提供了首个系统性理论解释与实用训练指南。

Abstract: The problem of corrupted data, missing features, or missing modalities continues to plague the modern machine learning landscape. To address this issue, a class of regularization methods that enforce consistency between imputed and fully observed data has emerged as a promising approach for improving model generalization, particularly in partially observed settings. We refer to this class of methods as Measure Consistency Regularization (MCR). Despite its empirical success in various applications, such as image inpainting, data imputation and semi-supervised learning, a fundamental understanding of the theoretical underpinnings of MCR remains limited. This paper bridges this gap by offering theoretical insights into why, when, and how MCR enhances imputation quality under partial observability, viewed through the lens of neural network distance.
  Our theoretical analysis identifies the term responsible for MCR's generalization advantage and extends to the imperfect training regime, demonstrating that this advantage is not always guaranteed. Guided by these insights, we propose a novel training protocol that monitors the duality gap to determine an early stopping point that preserves the generalization benefit. We then provide detailed empirical evidence to support our theoretical claims and to show the effectiveness and accuracy of our proposed stopping condition. We further provide a set of real-world data simulations to show the versatility of MCR under different model architectures designed for different data sources.

</details>


### [742] [TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse](https://arxiv.org/abs/2602.01439)
*Perry Dong,Kuo-Han Hung,Alexander Swerdlow,Dorsa Sadigh,Chelsea Finn*

Main category: cs.LG

TL;DR: 本文提出Transformer Q-Learning (TQL)方法，通过控制注意力分数的熵来防止其在模型增大时坍缩，从而稳定训练并提升强化学习中价值函数的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 探索为何Transformer架构在强化学习的价值函数建模中难以有效扩展，并解决由此导致的学习不稳定与性能下降问题。

Method: 通过实证分析发现注意力分数坍缩是关键失效模式，进而提出通过约束注意力分数熵来稳定训练；在此基础上设计Transformer Q-Learning（TQL）方法。

Result: TQL在扩大网络规模时实现最高43%的性能提升，而以往方法随规模增大性能反而下降。

Conclusion: 控制注意力熵可有效缓解Transformer在RL价值函数学习中的扩展瓶颈，TQL为大规模价值函数建模提供了可行路径。

Abstract: Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.

</details>


### [743] [A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting](https://arxiv.org/abs/2602.01445)
*Ons Saadallah,Mátyás andó,Tamás Gábor Orosz*

Main category: cs.LG

TL;DR: 本文提出LLM-AutoOpt，一种将贝叶斯优化与大语言模型（LLM）结合的超参数优化框架，用于提升时间序列预测中HPO的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 超参数优化在深度学习中至关重要但计算昂贵且难以解释，尤其在时间序列预测任务中；现有贝叶斯优化方法缺乏上下文感知与决策可解释性，而大语言模型可引入结构化先验知识与推理能力。

Method: 提出LLM-AutoOpt混合框架：利用贝叶斯优化初始化搜索以缓解冷启动问题，并将数据集元特征、模型描述、历史优化结果和目标等作为结构化元知识嵌入LLM提示中，实现上下文感知的超参数调优与决策解释。

Result: 在多变量时间序列预测基准实验中，LLM-AutoOpt相比纯贝叶斯优化及无元知识的LLM基线，显著提升了预测性能并增强了优化过程的可解释性。

Conclusion: 融合BO与LLM元知识的混合框架能兼顾HPO效率、效果与可解释性，为时序建模中的智能调参提供了新范式。

Abstract: Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.

</details>


### [744] [Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs](https://arxiv.org/abs/2602.01453)
*Idan Barnea,Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文研究了无奖励探索下的多智能体强化学习，提出了一种相位学习框架，在有限时域表格型MDP中分析了学习阶段数与智能体数量之间的权衡关系，并给出了达到ε精度所需的智能体数量的上下界。


<details>
  <summary>Details</summary>
Motivation: 在无奖励探索设定下，多个智能体需协同探索未知MDP以学习其动态特性，而现有方法缺乏对学习阶段数与智能体数量之间理论权衡的刻画。

Method: 采用相位学习框架：每个阶段中各智能体独立执行指定策略并观测轨迹；设计了一种计算高效的算法，并结合信息论下界分析证明阶段数与智能体数的关系。

Result: 当学习阶段数等于时域H时，算法仅需Õ(S⁶H⁶A/ε²)个智能体即可获得ε精度的动力学估计；若阶段数ρ<H，则任何算法至少需要A^{H/ρ}个智能体才能达到常数精度。

Conclusion: 要将智能体数量控制在多项式级别，学习阶段数必须达到H量级，揭示了二者之间存在由H决定的尖锐相变。

Abstract: We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.
  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\tilde{O}(S^6 H^6 A / ε^2)$ agents to obtain an $ε$ approximation of the dynamics (i.e., yields an $ε$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $ρ< H$ phases requires at least $A^{H/ρ}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.

</details>


### [745] [Modeling Topological Impact on Node Attribute Distributions in Attributed Graphs](https://arxiv.org/abs/2602.01454)
*Amirreza Shiralinasab Langari,Leila Yeganeh,Kim Khoa Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种代数方法，将图的拓扑结构与节点属性分布相结合，以建模拓扑对属性分布的影响，并在完全图上验证其合理性，最后通过无监督图异常检测任务进行评估。


<details>
  <summary>Details</summary>
Motivation: 探究图的拓扑结构如何影响节点属性的分布，将拓扑与属性视为结构上独立但相互作用的组件。

Method: 构建范畴论框架形式化节点对拓扑的感知，量化该视角并将其与节点属性分布结合，形成拓扑影响下的条件分布；引入ID测试模型，在无监督图异常检测任务中验证方法。

Result: 提出了拓扑影响下的节点属性分布建模方法，证明了在完全图上能恢复原始属性分布，且在异常检测任务中验证了有效性。

Conclusion: 图拓扑可被系统性地融入节点属性分布建模中，所提代数与范畴框架为理解拓扑-属性交互提供了新工具。

Abstract: We investigate how the topology of attributed graphs influences the distribution of node attributes. This work offers a novel perspective by treating topology and attributes as structurally distinct but interacting components. We introduce an algebraic approach that combines a graph's topology with the probability distribution of node attributes, resulting in topology-influenced distributions. First, we develop a categorical framework to formalize how a node perceives the graph's topology. We then quantify this point of view and integrate it with the distribution of node attributes to capture topological effects. We interpret these topology-conditioned distributions as approximations of the posteriors $P(\cdot \mid v)$ and $P(\cdot \mid \mathcal{G})$.
  We further establish a principled sufficiency condition by showing that, on complete graphs, where topology carries no informative structure, our construction recovers the original attribute distribution. To evaluate our approach, we introduce an intentionally simple testbed model, $\textbf{ID}$, and use unsupervised graph anomaly detection as a probing task.

</details>


### [746] [Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations](https://arxiv.org/abs/2602.01456)
*Yilun Kuang,Yash Dagade,Tim G. J. Rudner,Randall Balestriero,Yann LeCun*

Main category: cs.LG

TL;DR: 本文提出了一种新的分布匹配正则化方法RDMReg，用于JEPA框架中，通过将表示对齐到Rectified Generalized Gaussian（RGG）分布，显式控制稀疏性（ℓ₀范数），从而在保持任务性能的同时学习稀疏、非负的表征。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法使用各向同性高斯正则化，倾向于稠密表示，无法建模高效表征中关键的稀疏性特性。

Method: 提出Rectified Distribution Matching Regularization (RDMReg)，一种基于切片双样本分布匹配的损失函数，使表征对齐于Rectified Generalized Gaussian (RGG) 分布；RGG通过截断显式控制期望ℓ₀范数，并在期望ℓₚ范数约束下保持最大熵性质；由此构建Rectified LpJEPA模型。

Result: Rectified LpJEPA能学习稀疏、非负的表征，在图像分类基准上取得有竞争力的下游性能，验证了RDMReg在维持任务相关信息的同时有效施加稀疏性约束。

Conclusion: RDMReg为JEPA提供了更灵活、更具表达能力的正则化方式，克服了高斯先验对稀疏性的建模缺陷，实现了稀疏性与性能的更好权衡。

Abstract: Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.

</details>


### [747] [A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts](https://arxiv.org/abs/2602.01468)
*Viet Nguyen,Tuan Minh Pham,Thinh Cao,Tan Dinh,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: cs.LG

TL;DR: 本文通过将门控注意力矩阵和多头自注意力矩阵建模为专家混合模型，从理论上证明了门控注意力比传统多头自注意力具有更高的样本效率，并解释了门控位置对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管门控注意力在实践中表现出优于标准自注意力的性能（如提升低秩映射表达力、消除attention sink现象），但其理论优势尚不明确，亟需严谨的理论解释。

Method: 将门控注意力与多头自注意力矩阵表示为分层专家混合模型，将注意力学习建模为专家估计问题，并通过比较所需样本复杂度来分析两者的理论差异。

Result: 门控注意力仅需多项式量级样本即可达到给定估计误差，而多头自注意力需指数量级样本；同时，理论分析支持将门控置于缩放点积注意力或值映射输出处效果最优。

Conclusion: 本文首次为门控注意力的优势提供了严格的理论基础，揭示了其高样本效率的本质，并指导了门控位置的设计原则。

Abstract: Self-attention has greatly contributed to the success of the widely used Transformer architecture by enabling learning from data with long-range dependencies. In an effort to improve performance, a gated attention model that leverages a gating mechanism within the multi-head self-attention has recently been proposed as a promising alternative. Gated attention has been empirically demonstrated to increase the expressiveness of low-rank mapping in standard attention and even to eliminate the attention sink phenomenon. Despite its efficacy, a clear theoretical understanding of gated attention's benefits remains lacking in the literature. To close this gap, we rigorously show that each entry in a gated attention matrix or a multi-head self-attention matrix can be written as a hierarchical mixture of experts. By recasting learning as an expert estimation problem, we demonstrate that gated attention is more sample-efficient than multi-head self-attention. In particular, while the former needs only a polynomial number of data points to estimate an expert, the latter requires exponentially many data points to achieve the same estimation error. Furthermore, our analysis also provides a theoretical justification for why gated attention yields higher performance when a gate is placed at the output of the scaled dot product attention or the value map rather than at other positions in the multi-head self-attention architecture.

</details>


### [748] [P-EAGLE: Parallel-Drafting EAGLE with Scalable Training](https://arxiv.org/abs/2602.01469)
*Mude Hui,Xin Huang,Jaime Campos Salas,Yue Sun,Nathan Pemberton,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: P-EAGLE is a parallel multi-token prediction framework that improves the efficiency of reasoning LLMs by transforming EAGLE into a parallel drafting model with scalable long-context training techniques.


<details>
  <summary>Details</summary>
Motivation: Reasoning LLMs require longer outputs and thus benefit from speculative decoding drafters trained on extended sequences; however, existing parallel drafting methods face impractical quadratic training complexity with sequence length and parallel positions.

Method: P-EAGLE transforms EAGLE into a parallel multi-token predictor using a learnable shared hidden state, and introduces attention mask pre-computation and sequence partitioning to enable scalable long-context training with gradient accumulation.

Result: P-EAGLE achieves 1.10–1.36× speedup over autoregressive EAGLE-3 on GPT-OSS 120B, 20B, and Qwen3-Coder 30B when implemented in vLLM.

Conclusion: P-EAGLE effectively addresses the training scalability bottleneck of parallel drafting for long-context reasoning LLMs, delivering substantial inference acceleration without sacrificing accuracy.

Abstract: Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.

</details>


### [749] [Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability](https://arxiv.org/abs/2602.01480)
*Eric Regis,Sinho Chewi*

Main category: cs.LG

TL;DR: 本文提出了一种新的常微分方程（ODE）近似方法——Rod Flow，用于建模大步长梯度下降（GD）在非凸优化中的动态行为，其基于将GD迭代视为一维‘杆’的物理图像，具有原理清晰、计算高效、精度高（尤其在简单模型上优于Central Flow）等优势，并在理论上和数值实验中验证了其对临界sharpness阈值和自稳定现象的准确预测能力。


<details>
  <summary>Details</summary>
Motivation: 理解大步长梯度下降在非凸景观下的训练动态仍具挑战性；现有Central Flow虽有效，但缺乏物理直观且推导不够原则化，需更可解释、易计算且精度更高的ODE近似方法。

Method: 提出Rod Flow——一种基于GD迭代可视作一维‘杆’（rod）的物理图像而严格导出的显式ODE近似；理论分析其对sharpness阈值与自稳定性的预测能力；通过玩具模型与神经网络实验验证其性能。

Result: Rod Flow在简单玩具模型上比Central Flow更精确，在典型神经网络架构上精度相当；理论证明其能准确预测临界sharpness阈值并解释四次势函数中的自稳定现象；数值实验验证了理论结论。

Conclusion: Rod Flow是一种原理清晰、计算廉价、精度可靠的新ODE近似框架，为理解大步长GD的边缘稳定性现象提供了更坚实的物理与数学基础。

Abstract: How can we understand gradient-based training over non-convex landscapes? The edge of stability phenomenon, introduced in Cohen et al. (2021), indicates that the answer is not so simple: namely, gradient descent (GD) with large step sizes often diverges away from the gradient flow. In this regime, the "Central Flow", recently proposed in Cohen et al. (2025), provides an accurate ODE approximation to the GD dynamics over many architectures. In this work, we propose Rod Flow, an alternative ODE approximation, which carries the following advantages: (1) it rests on a principled derivation stemming from a physical picture of GD iterates as an extended one-dimensional object -- a "rod"; (2) it better captures GD dynamics for simple toy examples and matches the accuracy of Central Flow for representative neural network architectures, and (3) is explicit and cheap to compute. Theoretically, we prove that Rod Flow correctly predicts the critical sharpness threshold and explains self-stabilization in quartic potentials. We validate our theory with a range of numerical experiments.

</details>


### [750] [Causal Preference Elicitation](https://arxiv.org/abs/2602.01483)
*Edwin V. Bonilla,He Zhao,Daniel M. Steinberg*

Main category: cs.LG

TL;DR: 本文提出因果偏好提取，一种贝叶斯框架，用于专家参与的因果发现，通过主动查询局部边关系来集中有向无环图（DAG）的后验分布。


<details>
  <summary>Details</summary>
Motivation: 在因果发现中，专家知识的整合对于提高模型准确性至关重要，尤其是在数据有限或噪声较大的情况下。本文旨在通过主动学习方式高效利用专家判断，以改善DAG结构的学习效果。

Method: 提出因果偏好提取框架：基于任意黑箱观测后验，构建三类响应（边存在与否及方向）的噪声专家判断似然模型；采用灵活的粒子近似进行后验推断；使用期望信息增益准则选择最优查询。

Result: 在合成图、蛋白质信号数据和人类基因扰动基准上的实验表明，该方法在查询预算受限时能更快地实现后验集中，并更准确地恢复有向因果效应。

Conclusion: 因果偏好提取是一种有效的专家参与式因果发现方法，能够在少量专家交互下显著提升DAG学习的效率与准确性。

Abstract: We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-way likelihood over edge existence and direction. Posterior inference uses a flexible particle approximation, and queries are selected by an efficient expected information gain criterion on the expert's categorical response. Experiments on synthetic graphs, protein signaling data, and a human gene perturbation benchmark show faster posterior concentration and improved recovery of directed effects under tight query budgets.

</details>


### [751] [Predicting and improving test-time scaling laws via reward tail-guided search](https://arxiv.org/abs/2602.01485)
*Muheng Li,Jian Qian,Wenlong Mou*

Main category: cs.LG

TL;DR: 本文提出了一种基于尾部分布估计的测试时缩放方法（SLG Search），通过预测LLM的缩放规律，动态分配计算资源，理论证明其优于Best-of-N策略，并在实验中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有Best-of-N策略缺乏对N选择、预算分配和多阶段决策的理论指导，优化空间大但理论保证不足。

Method: 提出尾导向搜索（tail-guided search）：先估计奖励尾部分布以预测缩放律，再据此设计Scaling-Law Guided (SLG) Search算法，动态分配计算资源于高潜力中间状态。

Result: 理论证明SLG达到零遗憾（vanishing regret），预期奖励等价于需多项式级更大计算预算的Best-of-N；实验表明在相同计算预算下，SLG始终优于Best-of-N。

Conclusion: 尾分布建模与缩放律引导的动态搜索是一种有理论保障且实用的测试时推理优化范式。

Abstract: Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.

</details>


### [752] [Multi-Scale Wavelet Transformers for Operator Learning of Dynamical Systems](https://arxiv.org/abs/2602.01486)
*Xuesong Wang,Michael Groom,Rafael Oliveira,He Zhao,Terence O'Kane,Edwin V. Bonilla*

Main category: cs.LG

TL;DR: 本文提出多尺度小波变换器（MSWTs），通过在小波域中建模动力系统，显式分离并保留高低频信息，以克服神经算子等数据驱动代理模型的频谱偏差问题，在混沌系统和真实气象预报任务中均显著提升长期预测精度与频谱保真度。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的动力系统代理模型（如神经算子）存在频谱偏差，削弱高频成分，导致天气预报等关键应用中长期预测失稳。

Method: 提出多尺度小波变换器（MSWTs）：将系统动态建模置于小波域；采用保持小波特性的下采样方案保留高频特征；设计基于小波的注意力机制以建模跨尺度与跨频带依赖关系。

Result: 在混沌动力系统实验中显著降低预测误差、提升长期频谱保真度；在ERA5气候再分析数据上进一步减小气候学偏差，验证了其在真实预报场景中的有效性。

Conclusion: MSWTs通过小波域建模有效缓解频谱偏差，为高保真、长时程动力系统建模与预测提供了新范式。

Abstract: Recent years have seen a surge in data-driven surrogates for dynamical systems that can be orders of magnitude faster than numerical solvers. However, many machine learning-based models such as neural operators exhibit spectral bias, attenuating high-frequency components that often encode small-scale structure. This limitation is particularly damaging in applications such as weather forecasting, where misrepresented high frequencies can induce long-horizon instability. To address this issue, we propose multi-scale wavelet transformers (MSWTs), which learn system dynamics in a tokenized wavelet domain. The wavelet transform explicitly separates low- and high-frequency content across scales. MSWTs leverage a wavelet-preserving downsampling scheme that retains high-frequency features and employ wavelet-based attention to capture dependencies across scales and frequency bands. Experiments on chaotic dynamical systems show substantial error reductions and improved long horizon spectral fidelity. On the ERA5 climate reanalysis, MSWTs further reduce climatological bias, demonstrating their effectiveness in a real-world forecasting setting.

</details>


### [753] [OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference](https://arxiv.org/abs/2602.01493)
*Zhuoyuan Wang,Hanjiang Hu,Xiyu Deng,Saviz Mowlavi,Yorie Nakahira*

Main category: cs.LG

TL;DR: 本文提出OpInf-LLM框架，结合算子推断与大语言模型（LLM），实现对多样偏微分方程（PDE）的高精度、高成功率求解，尤其支持未见参数与边界条件的泛化，并支持自然语言任务描述。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在PDE求解中面临执行成功率与数值精度之间的权衡，尤其在泛化到未见参数和边界条件时表现不佳。

Method: 提出OpInf-LLM框架，基于少量解数据构建算子推断模型，并与LLM协同，实现自然语言驱动的PDE求解任务，提供统一工具接口与低计算开销设计。

Result: 该框架在异构设置下实现了高执行成功率与高数值精度，支持对未见PDE参数与配置的准确预测，并实现LLM与物理建模的无缝集成。

Conclusion: OpInf-LLM通过融合算子推断与LLM能力，为LLM驱动的可泛化降阶建模提供了新范式，推动了科学计算与AI的深度融合。

Abstract: Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.

</details>


### [754] [Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum](https://arxiv.org/abs/2602.01505)
*Navdeep Kumar,Tehila Dahan,Lior Cohen,Ananyabrata Barua,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: 本文提出了一种结合STORM和采样缓冲区的单时间尺度Actor-Critic算法，在无限时域折扣MDP中实现了O(ε^{-2})的最优样本复杂度，优于先前O(ε^{-3})的结果。


<details>
  <summary>Details</summary>
Motivation: 提升Actor-Critic算法在非平稳策略分布下训练时的样本效率，解决现有方法样本复杂度高（O(ε^{-3})）的问题。

Method: 将STORM（随机递归动量）应用于Critic更新以降低方差，并引入一个存储近期样本的小缓冲区，从中均匀采样用于每次Critic更新，以应对策略演化导致的非平稳性。

Result: 在有限状态-动作空间的无限时域折扣MDP中，实现了O(ε^{-2})的最优样本复杂度，且算法与深度学习架构兼容，仅需微小修改。

Conclusion: 所提方法显著提升了单时间尺度AC算法的理论样本效率，并保持了良好的实用性。

Abstract: We establish an optimal sample complexity of $O(ε^{-2})$ for obtaining an $ε$-optimal global policy using a single-timescale actor-critic (AC) algorithm in infinite-horizon discounted Markov decision processes (MDPs) with finite state-action spaces, improving upon the prior state of the art of $O(ε^{-3})$. Our approach applies STORM (STOchastic Recursive Momentum) to reduce variance in the critic updates. However, because samples are drawn from a nonstationary occupancy measure induced by the evolving policy, variance reduction via STORM alone is insufficient. To address this challenge, we maintain a buffer of small fraction of recent samples and uniformly sample from it for each critic update. Importantly, these mechanisms are compatible with existing deep learning architectures and require only minor modifications, without compromising practical applicability.

</details>


### [755] [Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization](https://arxiv.org/abs/2602.01510)
*Hengzhe Zhang,Qi Chen,Bing Xue,Wolfgang Banzhaf,Mengjie Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于遗传编程的特征构造框架，通过联合优化经验风险和邻域Jensen间隙来控制过拟合，并引入噪声估计策略和流形侵入检测机制以提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 遗传编程特征构造虽有效但易过拟合，限制其广泛应用；需提升模型泛化能力并缓解流形侵入问题。

Method: 理论证明邻域风险可分解为经验风险与正则项（有限差分或邻域Jensen间隙）之和；构建进化式特征构造框架，联合优化经验风险和Jensen间隙；设计动态噪声估计策略调节正则强度；加入流形侵入检测机制避免不真实样本生成。

Result: 在58个数据集上验证了Jensen间隙最小化优于其他复杂度度量；与15种机器学习算法对比，所提方法性能更优。

Conclusion: 联合优化Jensen间隙与经验风险、结合噪声自适应与流形侵入检测，能有效提升遗传编程特征构造的泛化能力与鲁棒性。

Abstract: Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

</details>


### [756] [White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC](https://arxiv.org/abs/2602.01516)
*Enzo Nicolas Spotorno,Matheus Wagner,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

TL;DR: 本文提出了一种白盒自适应非线性模型预测控制（NMPC）架构，通过模块化主权范式在冻结的、特定工况的神经专家间仲裁，实现车辆塑性（即无需重新训练即可适应不同运行工况）；其动力学以完全可遍历的符号图形式在CasADi中实现，保障运行时可审计性；仿真验证了其快速适应能力（约7.3 ms）和高跟踪精度，但代价是求解延迟比编译型物理模型高72–102倍。


<details>
  <summary>Details</summary>
Motivation: 解决车辆控制系统在不同运行工况（如摩擦、质量、阻力变化）下需自适应而无需重新训练的问题，同时保证控制逻辑的可解释性与可审计性。

Method: 采用白盒自适应NMPC架构，基于Modular Sovereignty范式集成多个冻结的、工况专用神经网络专家，并将整体动力学建模为CasADi中完全可遍历的符号图。

Result: 同步仿真表明系统可在约7.3 ms内快速适应复合工况变化，跟踪精度接近理想；但符号图维护使求解延迟比参数化物理模型增加72–102倍。

Conclusion: 该方法实现了高适应性与强透明性的统一，但揭示了白盒实现对实时效率的显著代价，为可信赖自动驾驶控制提供了新权衡视角。

Abstract: We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully traversable symbolic graph in CasADi, enabling maximal runtime auditability. Synchronous simulation validates rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts (friction, mass, drag) where non-adaptive baselines fail. Empirical benchmarking quantifies the transparency cost: symbolic graph maintenance increases solver latency by 72-102X versus compiled parametric physics models, establishing the efficiency price of strict white-box implementation.

</details>


### [757] [You Need an Encoder for Native Position-Independent Caching](https://arxiv.org/abs/2602.01519)
*Shiju Zhao,Junhao Hu,Jiaqi Zheng,Guihai Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为COMB的Position-Independent Caching（PIC）系统，通过在decoder-only大语言模型中引入并训练编码器，实现高效、高精度的KV缓存复用，显著降低首token延迟（TTFT）并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存依赖位置前缀，难以高效处理任意顺序检索的上下文；而已有PIC方法虽能缓解该问题，但常导致显著精度下降，限制了实际应用。

Method: 提出native PIC方法，在decoder-only LLM中重新引入编码器，并显式训练其支持位置无关缓存；同时设计COMB系统，作为与现有推理框架兼容的PIC感知缓存系统。

Result: COMB在保持可比精度的同时，将TTFT降低51–94%，吞吐量提升3倍；并在DeepSeek-V2-Lite-Chat上验证了其对其他decoder-only模型的适用性。

Conclusion: COMB为decoder-only大语言模型提供了高效、准确且实用的位置无关缓存方案，显著提升了长上下文推理效率。

Abstract: The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.

</details>


### [758] [When Is Rank-1 Enough? Geometry-Guided Initialization for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.01522)
*Haoran Zhao,Soyeon Caren Han,Eduard Hovy*

Main category: cs.LG

TL;DR: 本文提出Gap-Init方法，通过几何感知的初始化对齐模态间隙方向，显著提升秩为1的LoRA在多模态大模型微调中的训练稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 秩为1的LoRA在多模态大语言模型参数高效微调中常不稳定，其根源不仅是容量受限，更在于优化方向敏感性及预训练视觉与文本特征的各向异性不匹配。

Method: 分析预训练表征，识别主导早期梯度流的模态间隙轴，并设计Gap-Init：利用小规模校准集估计该轴，将秩-1 LoRA的方向初始化为其对齐方向，同时保持初始更新为零。

Result: Gap-Init在多个视觉-语言任务和骨干网络上稳定秩-1训练，性能匹配甚至超越强基线（如秩-8 LoRA）。

Conclusion: 在极低秩极限下，初始方向对齐的重要性可与秩本身相当，几何结构先验对参数高效微调至关重要。

Abstract: Parameter-efficient fine-tuning (PEFT) is a standard way to adapt multimodal large language models, yet extremely low-rank settings -- especially rank-1 LoRA -- are often unstable. We show that this instability is not solely due to limited capacity: in the rank-1 regime, optimization is highly sensitive to the update direction. Concretely, pretrained vision and text features form mismatched anisotropic regions, yielding a dominant "gap" direction that acts like a translation component and disproportionately steers early gradients under rank-1 constraints. Analyzing pretrained representations, we identify a modality-gap axis that dominates early gradient flow, while a random rank-1 initialization is unlikely to align with it, leading to weak gradients and training collapse. We propose Gap-Init, a geometry-aware initialization that aligns the rank-1 LoRA direction with an estimated modality-gap vector from a small calibration set, while keeping the initial LoRA update zero. Across multiple vision-language tasks and backbones, Gap-Init consistently stabilizes rank-1 training and can match or outperform strong rank-8 baselines. Our results suggest that at the extreme low-rank limit, initial alignment can matter as much as rank itself.

</details>


### [759] [The Inlet Rank Collapse in Implicit Neural Representations: Diagnosis and Unified Remedy](https://arxiv.org/abs/2602.01526)
*Jianqiao Zheng,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: 本文提出了一种结构诊断框架，通过层分解神经正切核（NTK）发现‘入口秩坍缩’现象，并据此设计了无需架构修改的秩扩展初始化方法，显著提升隐式神经表示（INRs）的细节重建能力。


<details>
  <summary>Details</summary>
Motivation: 现有提升INR性能的经验方法（如位置编码、SIREN、BN）缺乏先验理论解释，仅事后分析全局NTK谱；本文旨在从结构层面提供统一、前验的理论理解。

Method: 提出层分解NTK的结构诊断框架，数学刻画‘Inlet Rank Collapse’现象，并基于此设计Rank-Expanding Initialization方法，在初始化阶段保障输入到嵌入空间的秩随宽度增长。

Result: 所提初始化方法使标准MLP在无架构改动和额外计算开销下，实现高保真信号重建，验证了初始秩传播优化对INR表达能力的关键作用。

Conclusion: INR性能瓶颈源于首层结构性秩不足，而非单纯优化或激活设计问题；通过结构化初始化保障秩传播，是提升INR表达能力的根本路径。

Abstract: Implicit Neural Representations (INRs) have revolutionized continuous signal modeling, yet they struggle to recover fine-grained details within finite training budgets. While empirical techniques, such as positional encoding (PE), sinusoidal activations (SIREN), and batch normalization (BN), effectively mitigate this, their theoretical justifications are predominantly post hoc, focusing on the global NTK spectrum only after modifications are applied. In this work, we reverse this paradigm by introducing a structural diagnostic framework. By performing a layer-wise decomposition of the NTK, we mathematically identify the ``Inlet Rank Collapse'': a phenomenon where the low-dimensional input coordinates fail to span the high-dimensional embedding space, creating a fundamental rank deficiency at the first layer that acts as an expressive bottleneck for the entire network. This framework provides a unified perspective to re-interpret PE, SIREN, and BN as different forms of rank restoration. Guided by this diagnosis, we derive a Rank-Expanding Initialization, a minimalist remedy that ensures the representation rank scales with the layer width without architectural modifications or computational overhead. Our results demonstrate that this principled remedy enables standard MLPs to achieve high-fidelity reconstructions, proving that the key to empowering INRs lies in the structural optimization of the initial rank propagation to effectively populate the latent space.

</details>


### [760] [Plain Transformers are Surprisingly Powerful Link Predictors](https://arxiv.org/abs/2602.01553)
*Quang Truong,Yu Song,Donald Loveland,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: 本文提出PENCIL，一种仅使用编码器的纯Transformer模型，通过在采样的局部子图上进行注意力机制来替代手工设计的先验知识，从而在大规模图链接预测任务中实现高效且可扩展的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）依赖显式结构启发式或内存密集型节点嵌入，难以泛化和扩展；图Transformer（GTs）虽有潜力，但因复杂结构编码带来高开销，限制其在大规模链接预测中的应用。

Method: 提出PENCIL模型，采用编码器-only的纯Transformer架构，利用对采样局部子图的注意力机制替代手工先验，避免复杂结构编码和节点ID嵌入，保持标准Transformer的可扩展性和硬件效率。

Result: 实验与理论分析表明，PENCIL比GNN提取更丰富的结构信号，隐式泛化多种启发式和子图表达能力；在多个基准上优于启发式增强的GNN，参数效率远高于基于ID嵌入的方法，即使无节点特征仍具竞争力。

Conclusion: 简单的设计选择（如PENCIL）足以实现与复杂工程方法相当的性能，挑战了当前对复杂技术的过度依赖。

Abstract: Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embeddings -- approaches that struggle to generalize or scale to massive graphs. Emerging Graph Transformers (GTs) offer a potential alternative but often incur significant overhead due to complex structural encodings, hindering their applications to large-scale link prediction. We challenge these sophisticated paradigms with PENCIL, an encoder-only plain Transformer that replaces hand-crafted priors with attention over sampled local subgraphs, retaining the scalability and hardware efficiency of standard Transformers. Through experimental and theoretical analysis, we show that PENCIL extracts richer structural signals than GNNs, implicitly generalizing a broad class of heuristics and subgraph-based expressivity. Empirically, PENCIL outperforms heuristic-informed GNNs and is far more parameter-efficient than ID-embedding--based alternatives, while remaining competitive across diverse benchmarks -- even without node features. Our results challenge the prevailing reliance on complex engineering techniques, demonstrating that simple design choices are potentially sufficient to achieve the same capabilities.

</details>


### [761] [InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs](https://arxiv.org/abs/2602.01554)
*Lv Tang,Tianyi Zheng,Bo Li,Xingyu Li*

Main category: cs.LG

TL;DR: 本文提出InfoTok，一种基于信息瓶颈原理的信息正则化视觉分词机制，用于统一多模态大语言模型（MLLMs），在不增加训练数据的前提下提升理解和生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有共享token设计缺乏明确准则来决定token应保留哪些信息以同时支持理解和生成任务，因此作者从容量受限视角出发，强调视觉分词器应优先保留可复用结构而非高熵冗余信息。

Method: 提出InfoTok机制，将分词建模为控制图像到共享token再到多模态输出的信息流，通过互信息正则化实现压缩与任务相关性的权衡，并集成到三个代表性统一MLLM中。

Result: 在多个理解与生成任务上均取得一致性能提升，验证了信息正则化分词对构建共享token空间的有效性。

Conclusion: 信息正则化的视觉分词是一种构建统一MLLM共享token空间的合理基础，优于传统架构驱动的共享token设计。

Abstract: Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven and lack an explicit criterion for what information tokens should preserve to support both understanding and generation. Therefore, we introduce a capacity-constrained perspective, highlighting that in shared-token unified MLLMs the visual tokenizer behaves as a compute-bounded learner, so the token budget should prioritize reusable structure over hard-to-exploit high-entropy variations and redundancy. Motivated by this perspective, we propose InfoTok, an information-regularized visual tokenization mechanism grounded in the Information Bottleneck (IB) principle. InfoTok formulates tokenization as controlling information flow from images to shared tokens to multimodal outputs, yielding a principled trade-off between compression and task relevance via mutual-information regularization. We integrate InfoTok into three representative unified MLLMs without introducing any additional training data. Experiments show consistent improvements on both understanding and generation, supporting information-regularized tokenization as a principled foundation for learning a shared token space in unified MLLMs.

</details>


### [762] [How Implicit Bias Accumulates and Propagates in LLM Long-term Memory](https://arxiv.org/abs/2602.01558)
*Yiming Ma,Lixu Wang,Lionel Z. Wang,Hongkun Yang,Haoming Sun,Xin Xu,Jiaqi Wu,Bin Chen,Wei Dong*

Main category: cs.LG

TL;DR: 本文研究了具备长期记忆能力的大语言模型（LLMs）中隐性偏见的累积与跨域传播问题，提出了Decision-based Implicit Bias (DIB)基准和动态记忆标记（DMT）方法，显著缓解了偏见随时间加剧及跨领域扩散的问题。


<details>
  <summary>Details</summary>
Motivation: 长期记忆机制虽增强LLM的连续性与个性化，但也引入了尚未充分探索的公平性风险；本文旨在系统分析隐性偏见在长期交互中的动态累积与跨域传播机制。

Method: 构建包含3776个决策场景、覆盖9个社会领域的DIB基准；设计长周期仿真框架，评估6种SOTA LLM与3种记忆架构组合下的隐性偏见演化；提出动态记忆标记（DMT）方法，在记忆写入时施加公平性约束。

Result: 发现LLM隐性偏见随时间加剧且跨无关领域传播；静态提示基线效果有限且短暂；DMT显著降低偏见累积并有效遏制跨域传播。

Conclusion: 隐性偏见在具长期记忆的LLM中具有动态演化与跨域传染特性，需在记忆写入环节进行主动干预；DMT为保障长期交互公平性提供了可行、有效的代理式解决方案。

Abstract: Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.

</details>


### [763] [Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space](https://arxiv.org/abs/2602.01564)
*Geuntaek Seo,Minseop Shin,Pierre Monmarché,Beomjun Choi*

Main category: cs.LG

TL;DR: 本文研究了针对熵正则化二人零和博弈的平均场Langevin交替下降-上升动力学（MFL-DA），证明了其唯一混合纳什均衡在Wasserstein度量下具有局部指数稳定性，即初始分布足够接近时，动力学以指数速率收敛至均衡。


<details>
  <summary>Details</summary>
Motivation: 解决Wang和Chizat（COLT 2024）提出的开放问题：非凸-非凹支付函数下MFL-DA动力学的长期行为，特别是其唯一混合纳什均衡的稳定性。

Method: 通过线性化算子的谱分析，在均衡附近建立熵的强制性估计，揭示局部位移凸-凹结构，从而导出Wasserstein度量下的指数收缩。

Result: 证明了MFL-DA动力学在Wasserstein度量下对混合纳什均衡具有局部指数稳定性，给出了明确的收敛速率。

Conclusion: 该结果部分解决了Wang和Chizat提出的开放问题，确立了局部稳定性和定量收敛速率，但全局收敛仍是待解挑战。

Abstract: We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.

</details>


### [764] [Generative Visual Code Mobile World Models](https://arxiv.org/abs/2602.01576)
*Woosung Koh,Sungjun Han,Segyu Lee,Se-Young Yun,Jamin Shin*

Main category: cs.LG

TL;DR: 本文提出了一种基于可渲染代码生成的视觉世界模型（gWorld），用于移动GUI建模，通过单个视觉语言模型生成可执行Web代码来渲染GUI状态，兼顾文本精度与视觉保真度，并开源了8B/32B参数模型及自动数据合成框架。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI世界模型存在文本保真度与视觉保真度之间的权衡：纯文本模型缺乏视觉细节，而纯视觉模型难以精准渲染文本且依赖复杂外部系统。

Method: 提出‘可渲染代码生成’范式，利用视觉语言模型（VLM）直接预测能渲染出目标GUI像素的结构化Web代码；构建gWorld数据生成框架自动合成代码标注数据；训练开源权重模型gWorld（8B/32B）。

Result: 在4个分布内和2个分布外基准上，gWorld在准确率-模型尺寸帕累托前沿上超越50.25倍更大规模的8个前沿开源模型；验证了数据扩展、组件优化及世界建模能力对下游策略性能的正向影响。

Conclusion: 可渲染代码生成是一种高效、可扩展且高保真的移动GUI世界建模范式；gWorld证明了轻量级VLM在GUI理解与生成任务中具备强大潜力，为移动端智能体提供了更优的世界模型基础。

Abstract: Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.

</details>


### [765] [Nearly Optimal Active Preference Learning and Its Application to LLM Alignment](https://arxiv.org/abs/2602.01581)
*Yao Zhao,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 本文针对大语言模型对齐中偏好标签数据收集成本高的问题，提出两种专为偏好学习设计的主动学习算法，显著提升了样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习方法采用经典的实验设计准则（如G-或D-最优性），但这些准则未适配偏好学习的结构特性，导致效率受限。

Method: 基于对偏好学习特性的新洞察，提出两种主动学习算法：一种提供首个实例依赖的标签复杂度保证，另一种是简单实用的贪心方法。

Result: 在真实偏好数据集上的实验表明，所提算法相比现有方法具有更高的样本效率。

Conclusion: 针对偏好学习任务定制的主动学习算法比通用实验设计准则更有效，为高效收集高质量人类偏好数据提供了新思路。

Abstract: Aligning large language models (LLMs) depends on high-quality datasets of human preference labels, which are costly to collect. Although active learning has been studied to improve sample efficiency relative to passive collection, many existing approaches adopt classical experimental design criteria such as G- or D-optimality. These objectives are not tailored to the structure of preference learning, leaving open the design of problem-specific algorithms. In this work, we identify a simple intuition specific to preference learning that calls into question the suitability of these existing design objectives. Motivated by this insight, we propose two active learning algorithms. The first provides the first instance-dependent label complexity guarantee for this setting, and the second is a simple, practical greedy method. We evaluate our algorithm on real-world preference datasets and observe improved sample efficiency compared to existing methods.

</details>


### [766] [A Lightweight Sparse Interaction Network for Time Series Forecasting](https://arxiv.org/abs/2602.01585)
*Xu Zhang,Qitong Wang,Peng Wang,Wei Wang*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级稀疏交互网络（LSINet），通过多头稀疏交互机制（MSIM）和共享交互学习（SIL）显式建模时间依赖，在长时序预测任务中兼顾高精度与高效率。


<details>
  <summary>Details</summary>
Motivation: 现有线性模型虽在长时序预测中表现优异，但其隐式的时间交互能力受限于堆叠MLP结构，难以充分捕获复杂时间依赖；而Transformer的自注意力机制虽显式建模时间关系，但计算开销大。因此需设计一种兼具显式时间交互、低计算成本与强表达能力的新架构。

Method: 提出LSINet：1）设计多头稀疏交互机制（MSIM），基于稀疏诱导的伯努利分布学习关键时间步连接，替代自注意力；2）引入自适应正则化损失保障稀疏性；3）提出共享交互学习（SIL），复用交互模式提升训练效率与收敛性；整体为纯MLP结构的轻量线性模型。

Result: 在多个公开数据集上，LSINet在预测精度和计算效率两方面均超越先进线性模型与Transformer模型。

Conclusion: 显式、稀疏、可共享的时间交互机制可有效增强线性模型的时序建模能力，LSINet为长时序预测提供了高效且高性能的新范式。

Abstract: Recent work shows that linear models can outperform several transformer models in long-term time-series forecasting (TSF). However, instead of explicitly performing temporal interaction through self-attention, linear models implicitly perform it based on stacked MLP structures, which may be insufficient in capturing the complex temporal dependencies and their performance still has potential for improvement. To this end, we propose a Lightweight Sparse Interaction Network (LSINet) for TSF task. Inspired by the sparsity of self-attention, we propose a Multihead Sparse Interaction Mechanism (MSIM). Different from self-attention, MSIM learns the important connections between time steps through sparsity-induced Bernoulli distribution to capture temporal dependencies for TSF. The sparsity is ensured by the proposed self-adaptive regularization loss. Moreover, we observe the shareability of temporal interactions and propose to perform Shared Interaction Learning (SIL) for MSIM to further enhance efficiency and improve convergence. LSINet is a linear model comprising only MLP structures with low overhead and equipped with explicit temporal interaction mechanisms. Extensive experiments on public datasets show that LSINet achieves both higher accuracy and better efficiency than advanced linear models and transformer models in TSF tasks. The code is available at the link https://github.com/Meteor-Stars/LSINet.

</details>


### [767] [Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting](https://arxiv.org/abs/2602.01588)
*Huu Hiep Nguyen,Minh Hoang Nguyen,Dung Nguyen,Hung Le*

Main category: cs.LG

TL;DR: 本文提出SpecTF框架，通过在频域中融合文本嵌入与时间序列的频谱分量，利用轻量级跨注意力机制自适应重加权频率带，从而提升多模态时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在局部时间步上对齐文本与时间序列特征，忽略了文本对时间序列多尺度（如周期、动态变化）的全局影响，导致建模不匹配。

Method: 将时间序列进行频谱分解得到频域表示；提取文本嵌入并映射至频域；通过轻量级跨注意力机制将文本信息融合到各频率分量；再逆变换回时域进行预测。

Result: SpecTF在多个多模态时间序列数据集上显著超越现有SOTA模型，且参数量更少。

Conclusion: 在频域建模文本对时间序列的影响是一种有效且高效的多模态时间序列预测新范式。

Abstract: Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing methods align textual features with time-series patterns one step at a time, they neglect the multiscale temporal influences of contextual information such as time-series cycles and dynamic shifts. This mismatch between local alignment and global textual context can be addressed by spectral decomposition, which separates time series into frequency components capturing both short-term changes and long-term trends. In this paper, we propose SpecTF, a simple yet effective framework that integrates the effect of textual data on time series in the frequency domain. Our method extracts textual embeddings, projects them into the frequency domain, and fuses them with the time series' spectral components using a lightweight cross-attention mechanism. This adaptively reweights frequency bands based on textual relevance before mapping the results back to the temporal domain for predictions. Experimental results demonstrate that SpecTF significantly outperforms state-of-the-art models across diverse multi-modal time series datasets while utilizing considerably fewer parameters. Code is available at https://github.com/hiepnh137/SpecTF.

</details>


### [768] [The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR](https://arxiv.org/abs/2602.01599)
*Israel Adewuyi,Solomon Okibe,Vladmir Ivanov*

Main category: cs.LG

TL;DR: 本文提出“多重彩票假说”，指出在强化学习中，仅随机选择1%参数进行训练即可达到甚至超越全参数微调的效果，表明预训练模型中存在大量可行的稀疏子网络。


<details>
  <summary>Details</summary>
Motivation: 受“彩票假说”和强化学习中可验证奖励（RLVR）中参数更新稀疏性的启发，探索如何最简单地利用模型中的参数冗余性。

Method: 在RLVR框架下，仅随机选取极稀疏（如1%）的参数子集进行训练，并分析不同随机掩码间的重叠度及性能表现；同时从隐式每步KL约束角度解释该现象。

Result: 在3个模型和2个任务域上，仅训练1%参数即可匹配或超越全参数微调效果；不同成功随机掩码间Jaccard相似度≤0.005，表明存在多个可行稀疏子网络。

Conclusion: 预训练模型中存在大量功能等效的稀疏子网络，而非唯一‘中奖’子网络，支持‘多重彩票假说’；其根源在于RLVR中隐式的低维更新约束。

Abstract: The Lottery Ticket Hypothesis demonstrated that sparse subnetworks can match full-model performance, suggesting parameter redundancy. Meanwhile, in Reinforcement Learning with Verifiable Rewards (RLVR), recent work has shown that updates concentrate on a sparse subset of parameters, which further lends evidence to this underlying redundancy. We study the simplest possible way to exploit this redundancy: training only a randomly selected subset of parameters at extreme sparsities. Empirically, we find that training just 1\% of parameters matches or exceeds full-parameter RLVR finetuning across 3 models and 2 task domains. Moreover, different random masks show minimal overlap ($\leq 0.005$ Jaccard similarity) and yet all succeed, suggesting pretrained models contain many viable sparse subnetworks rather than one privileged set. We term this the Multiple Ticket Hypothesis. We explain this phenomenon through the implicit per-step KL constraint in RLVR, which restricts updates to a low-dimensional subspace, enabling arbitrary sparse masks to succeed.

</details>


### [769] [Universal Redundancies in Time Series Foundation Models](https://arxiv.org/abs/2602.01605)
*Anthony Bao,Venkata Hasith Vattikuti,Jeffrey Lai,William Gilpin*

Main category: cs.LG

TL;DR: 本文研究了时间序列基础模型（TSFMs）的内部机制，发现其存在冗余组件，并提出了一套可解释性分析工具，包括组件消融和残差流上的直接logit归因；通过理论框架将transformer建模为核回归器，基于投影矩阵稳定秩指导注意力头消融，识别出导致‘重复上下文模式’和‘季节性偏差’等退化现象的关键注意力头。


<details>
  <summary>Details</summary>
Motivation: 现有TSFM虽在大规模预训练后表现出强泛化能力，但其内部工作机制尚不清晰，尤其存在如‘重复上下文模式’和‘季节性偏差’等退化现象，亟需可解释性分析以揭示其通用结构特性。

Method: 提出面向TSFM的机械可解释性工具集：1）中间层/注意力头的系统性消融；2）残差流上的直接logit归因；3）构建transformer作为核回归器的理论框架，并据此用投影矩阵稳定秩指导关键头筛选。

Result: 实验证明主流TSFM对整层消融具有鲁棒性；识别出导致parroting与seasonality bias的具体注意力头；方法在多种架构与真实/合成数据集上具有一致性。

Conclusion: TSFM具有高度冗余与内在结构共性；退化行为可归因于特定注意力头，可通过内在指标（如稳定秩）精准定位；该发现为TSFM的简化、诊断与改进提供了理论基础与实用工具。

Abstract: Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual stream. Our findings are consistent across several leading TSFMs with diverse architectures, and across a diverse set of real-world and synthetic time-series datasets. We discover that all models in our study are robust to ablations of entire layers. Furthermore, we develop a theoretical framework framing transformers as kernel regressors, motivating a purely intrinsic strategy for ablating heads based on the stable rank of the per-head projection matrices. Using this approach, we uncover the specific heads responsible for degenerate phenomena widely observed in TSFMs, such as parroting of motifs from the context and seasonality bias. Our study sheds light on the universal properties of this emerging class of architectures for continuous-time sequence modeling.

</details>


### [770] [Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching](https://arxiv.org/abs/2602.01606)
*Zeqiao Li,Yijing Wang,Haoyu Wang,Zheng Li,Zhiqiang Zuo*

Main category: cs.LG

TL;DR: 本文提出FLAME框架，通过Q重加权Flow Matching目标、解耦熵估计器和MeanFlow公式，解决了将Flow Matching集成到最大熵强化学习中的挑战，实现了高效的一步控制，并在MuJoCo上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 扩散策略表达能力强但推理延迟高；Flow Matching虽支持一步生成，但难以融入最大熵强化学习，因其最优策略是难以处理的基于能量的分布，且高效对数似然估计易受离散化偏差影响。

Method: 提出FLAME框架：1）推导Q重加权Flow Matching目标以避免配分函数估计；2）设计解耦熵估计器以严格校正偏差；3）整合MeanFlow公式实现表达力强且高效的一步控制。

Result: 在MuJoCo实验中，FLAME优于高斯基线，并以显著更低的推理成本达到与多步扩散策略相当的性能。

Conclusion: FLAME为最大熵强化学习提供了一种原理清晰、高效实用的Flow Matching集成方案，在保持性能的同时大幅降低推理开销。

Abstract: Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \textbf{F}low-based \textbf{L}og-likelihood-\textbf{A}ware \textbf{M}aximum \textbf{E}ntropy RL (\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.

</details>


### [771] [What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?](https://arxiv.org/abs/2602.01611)
*Weizheng Gu,Chengze Li,Zhuohao Yu,Mengyuan Sun,Zhibang Yang,Wei Wang,Hongrui Jia,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: 本文提出PIPE协议来诊断大语言模型在作为交互式智能体时对特定界面的依赖性，发现轨迹监督微调（trajectory-SFT）会显著加剧模型对训练界面的捷径依赖，而这种依赖在标准基准测试中无法被识别。


<details>
  <summary>Details</summary>
Motivation: 现有智能体基准测试混淆了语义工具使用与界面特定交互模式记忆两种成功机制，导致无法区分模型是否具备环境无关的能力。

Method: 提出PIPE协议，通过最小化重写环境界面但保持任务语义和执行行为不变，来诊断界面依赖；引入Interface Reliance（IR）指标，基于反平衡别名量化模型对训练界面的偏好。

Result: 在16个环境中验证发现，trajectory-SFT训练的智能体在界面微小改写下性能急剧下降，而非trajectory-trained模型则保持稳定；IR指标揭示界面捷径依赖具有环境相关、非单调的训练动态。

Conclusion: 标准基准分数不能可靠反映智能体的泛化能力；界面依赖是一种被忽视但关键的捷径学习现象，需通过如PIPE等新评估协议加以识别和控制。

Abstract: Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.

</details>


### [772] [A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models](https://arxiv.org/abs/2602.01613)
*Sergii Kozyrev,Davyd Maiboroda*

Main category: cs.LG

TL;DR: Minima是一种面向生产的Transformer模型结构化压缩流水线，通过敏感度预测、混合张量分解、短时微调及定制化内核，在Qwen3-32B上实现显著显存降低与吞吐提升，并支持推测解码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型因GPU显存占用高和推理延迟大而难以部署，需一种兼顾压缩效果与实际服务收益的结构化压缩方法。

Method: Minima构建轻量卷积预测器评估层与块级敏感度，在低敏感区域应用Tucker、张量链与张量环分解，辅以短时修复微调，并使用自定义Triton/CUDA算子执行；同时结合小draft+大verifier的推测解码架构。

Result: 在Qwen3-32B（8k上下文）上，峰值显存从64 GiB降至40 GiB；单请求吞吐达50 token/s（+25%），叠加推测解码达75 token/s；50并发下吞吐为53 token/s，仍显著优于基线。

Conclusion: Minima是实用且可扩展的结构化压缩方案，为基于共享张量骨干+微型适配器的更激进压缩范式提供了可行路径。

Abstract: Large language models are limited in deployment by GPU memory and inference latency. We present Minima, a production compression pipeline that learns where and how to structurally compress a Transformer and turns that compression into real serving gains. Minima trains a lightweight convolutional predictor to estimate layer- and patch-level sensitivity, applies a mixture of Tucker, tensor-train, and tensor-ring decompositions to low-sensitivity regions, performs a short healing fine-tune, and executes the resulting operators with custom Triton and CUDA kernels. The reduced memory footprint enables speculative decoding with a small draft model and a larger verifier. On Qwen3-32B at an 8k-token context window, Minima reduces peak VRAM from 64 GiB to 40 GiB. For a single active request, throughput increases from 40 tokens per second (baseline) to 50 tokens per second (Minima) and 75 tokens per second (Minima with speculative decoding). Under 50 parallel requests, throughput is 34, 44, and 53 tokens per second respectively, showing that Minima remains effective under high concurrency even when speculative decoding gains compress. We position Minima relative to recent tensor-network, low-rank plus quantization, and cross-layer sharing methods, and argue that it is a practical step toward more aggressive structural compression via shared tensor backbones with tiny per-layer adapters.

</details>


### [773] [AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems](https://arxiv.org/abs/2602.01614)
*Qi Cheng,Licheng Liu,Yao Zhang,Mu Hong,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 本文提出首个时空农业生态系统温室气体（GHG）基准数据集，融合物理模型模拟与实地观测数据，并评估多种时序深度学习模型在碳氮通量预测中的性能，探索利用模拟数据提升模型对真实观测泛化能力的迁移学习方法。


<details>
  <summary>Details</summary>
Motivation: 农业生态系统受人类活动影响大、温室气体排放占比高，但其碳-养分-水耦合过程难以准确量化；传统方法面临数据稀疏、时空异质性强、地下过程复杂等挑战，且缺乏面向AI建模的可信基准数据集和标准协议。

Method: 构建首个时空农业生态系统GHG基准数据集，整合Ecosys和DayCent物理模型模拟结果与涡度相关通量塔及受控环境设施的实测数据；评估LSTM、时序CNN和Transformer等序列深度学习模型在碳氮通量预测任务上的表现；开展迁移学习实验，利用模拟数据提升模型对真实观测的泛化能力。

Result: 验证了多种深度学习模型在碳氮通量预测中的有效性，揭示了迁移学习可显著提升模型在真实观测上的预测性能；提供了首个AI-ready的农业生态系统时空基准数据集与评估框架。

Conclusion: 该工作为发展更准确、可扩展的AI驱动农业生态系统模型奠定数据与方法基础，有助于深化对生态系统-气候相互作用的理解，并支撑温室气体减排策略制定。

Abstract: Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately do not exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM-based models, temporal CNN-based model, and Transformer-based models. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.

</details>


### [774] [SUSD: Structured Unsupervised Skill Discovery through State Factorization](https://arxiv.org/abs/2602.01619)
*Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 本文提出SUSD框架，通过将状态空间分解为独立因子并为每个因子分配独立技能变量，实现细粒度、解耦的技能发现，显著提升动态性和多样性，并支持下游分层强化学习任务。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法（如基于互信息或距离最大化的方法）难以发现动态、任务相关且覆盖所有可控因素的技能，限制了技能的多样性和实用性。

Method: SUSD利用环境的组合结构，将状态空间分解为独立因子（如物体或可控实体），为各因子分配独立技能变量，并引入动态模型跟踪各因子的学习进度，自适应引导探索未充分学习的因子。

Result: 在三个具有不同数量因子（1~10）的环境中，SUSD显著优于现有无监督技能发现方法，能自主发现多样化、复杂的技能，并生成可解耦的因子化技能表征。

Conclusion: SUSD通过结构化因子分解与动态探索机制，有效提升了技能发现的多样性、动态性与可控性，为复杂环境中的下游分层强化学习任务提供了高效基础。

Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.

</details>


### [775] [Toward Enhancing Representation Learning in Federated Multi-Task Settings](https://arxiv.org/abs/2602.01626)
*Mehdi Setayesh,Mahdi Beitollahi,Yasser H. Khalil,Hongliang Li*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦多任务学习框架FedMuscle，通过引入Muscle loss对比学习目标，在不共享模型参数的前提下，学习跨任务的共享表征空间，从而支持模型与任务异构性，并在多个图像和语言任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦多任务学习（FMTL）方法大多假设用户间模型结构一致（同质），难以适应现实中的模型与任务异构场景；亟需一种不依赖参数共享、能建模跨任务依赖关系的新范式。

Method: 提出Muscle loss——一种新型对比学习目标，通过联合对齐所有参与模型的表征，等价于最大化各模型表征间的互信息；基于此构建通信高效的联邦学习算法FedMuscle，支持异构模型与任务。

Result: 在多种图像与语言任务上，FedMuscle持续超越SOTA基线，尤其在高度异构设置下展现出更强的鲁棒性与性能提升。

Conclusion: 共享表征空间比共享模型参数更适配联邦多任务学习的实际需求；Muscle loss为建模多任务间复杂依赖提供了理论扎实且实用的新工具，FedMuscle为异构FMTL提供了有效解决方案。

Abstract: Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability in realistic settings. To overcome this limitation, we aim to learn a shared representation space across tasks rather than shared model parameters. To this end, we propose Muscle loss, a novel contrastive learning objective that simultaneously aligns representations from all participating models. Unlike existing multi-view or multi-model contrastive methods, which typically align models pairwise, Muscle loss can effectively capture dependencies across tasks because its minimization is equivalent to the maximization of mutual information among all the models' representations. Building on this principle, we develop FedMuscle, a practical and communication-efficient FMTL algorithm that naturally handles both model and task heterogeneity. Experiments on diverse image and language tasks demonstrate that FedMuscle consistently outperforms state-of-the-art baselines, delivering substantial improvements and robust performance across heterogeneous settings.

</details>


### [776] [COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection](https://arxiv.org/abs/2602.01635)
*Jinwoo Park,Hyeongwon Kang,Seung Hun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: 本文提出COMET方法，通过多尺度补丁编码、向量量化核心集和在线码本自适应，解决时间序列异常检测中时序依赖、多变量相关性建模及分布偏移问题，在多个基准数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在补丁级表征学习中难以同时捕获时序依赖与多变量相关性，且受限于单尺度模式，无法检测不同时间范围的异常；此外，仅依赖正常数据建模使其易受推理时分布偏移影响。

Method: 提出COMET框架，包含三部分：(1) 多尺度补丁编码，建模多尺度时序依赖与变量间相关性；(2) 向量量化核心集，利用码本学习正常模式，并结合量化误差与记忆距离进行双得分异常检测；(3) 在线码本自适应，基于码本生成伪标签，通过对比学习实现推理阶段动态模型更新。

Result: 在5个基准数据集上的45项评估指标中，COMET在36项上取得最优性能，验证了其在多样化场景下的有效性。

Conclusion: COMET有效缓解了时间序列异常检测中多尺度建模不足、多变量相关性建模缺失及分布偏移鲁棒性差等问题，为工业时序异常检测提供了更可靠、自适应的新范式。

Abstract: Time series anomaly detection is a critical task across various industrial domains. However, capturing temporal dependencies and multivariate correlations within patch-level representation learning remains underexplored, and reliance on single-scale patterns limits the detection of anomalies across different temporal ranges. Furthermore, focusing on normal data representations makes models vulnerable to distribution shifts at inference time. To address these limitations, we propose Codebook-based Online-adaptive Multi-scale Embedding for Time-series anomaly detection (COMET), which consists of three key components: (1) Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales. (2) Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance. (3) Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning. Experiments on five benchmark datasets demonstrate that COMET achieves the best performance in 36 out of 45 evaluation metrics, validating its effectiveness across diverse environments.

</details>


### [777] [Chance-Constrained Inference for Hallucination Risk Control in Large Language Models](https://arxiv.org/abs/2602.01637)
*Sreenivasan Mohandas*

Main category: cs.LG

TL;DR: 本文提出了一种名为“机会约束推理（chance-constrained inference）”的新方法，旨在对大语言模型生成中的幻觉现象进行概率层面的风险控制，确保接受的输出中幻觉发生的频率被显式上界约束；该方法采用顺序、任意时刻有效的采样检验策略，优于传统置信度筛选等基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽能生成流利文本，但存在事实性幻觉等随机错误；当前缓解策略仅降低平均错误率，缺乏对重复使用中错误发生频率的显式概率控制。

Method: 将推理建模为部署时的风险控制问题，定义幻觉为随机约束违反，并设计一种顺序、anytime-valid的采样检验过程，自适应判定输入是否满足给定的机会约束（即幻觉概率不超过阈值），避免保守的固定样本量界限。

Result: 在NaturalQuestions启发的问答与可控多跳问答任务上验证了该方法可实现可靠的风险控制、对本质不可解输入的早期识别、以及多次调用下的安全组合性；而基于置信度的选择性预测基线无法提供一致的概率保证。

Conclusion: 机会约束推理为大语言模型的安全部署提供了首个具备严格概率风险保证的推理框架，弥补了传统方法在可控性与理论保障上的不足。

Abstract: Large language models generate outputs stochastically and may produce fluent but invalid responses, including factual hallucinations. Existing mitigation strategies reduce average error rates but do not provide explicit control over the \emph{frequency} of such failures under repeated use. We formulate inference as a deployment-time risk control problem and introduce \emph{chance-constrained inference}, which directly bounds the probability of hallucinations among accepted generations. Hallucinations are modeled as stochastic constraint violations, and we show that confidence-based selective prediction does not, in general, imply probabilistic risk guarantees. To enforce chance constraints efficiently, we propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds. Experiments on questions inspired by NaturalQuestions and controlled multi-hop question answering demonstrate reliable risk control, early detection of intrinsically infeasible inputs, and safe composition under repeated use, while confidence-based baselines fail to provide consistent guarantees.

</details>


### [778] [The Effect of Mini-Batch Noise on the Implicit Bias of Adam](https://arxiv.org/abs/2602.01642)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 本文提出了一种理论框架，分析Adam优化器中动量参数(β₁, β₂)与批量大小如何共同影响其隐式偏差，进而影响模型在多轮训练中的泛化性能；发现β₁和β₂对正则化/反向正则化的影响随批量大小变化而发生单调性反转，并指出默认参数(0.9, 0.999)适用于小批量，而大批量时调高β₁更优。


<details>
  <summary>Details</summary>
Motivation: 随着多轮训练在深度学习中重新重要，Adam优化器的动量参数与批量大小对泛化的影响尚缺乏统一理论解释，尤其在有限高质量数据和计算资源增长背景下，需理解其隐式正则化机制。

Method: 建立理论框架，分析mini-batch噪声如何通过Adam的动量机制（依赖β₁、β₂）影响其对损失曲面尖锐/平坦区域的隐式偏好，并推导该影响随批量大小变化的临界行为。

Result: 发现β₂在大批量下加剧反向正则化（损害泛化），但在小批量下作用反转；β₁呈现相反方向的单调性反转；默认(β₁,β₂)=(0.9,0.999)适合小批量，大批量时β₁接近β₂可显著提升验证准确率；理论揭示该转变点与临界批量大小相关。

Conclusion: Adam的隐式偏差高度依赖β₁、β₂与批量大小的协同作用，不能孤立设置超参；应依据批量大小动态调整β₁和β₂以优化多轮训练泛化性能。

Abstract: With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(β_1, β_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $β_1$, $β_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $β_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $β_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $β_1$. In particular, the commonly "default" pair $(β_1, β_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $β_1$ closer to $β_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.

</details>


### [779] [De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion](https://arxiv.org/abs/2602.01643)
*Xichen Sun,Wentao Wei,Jiahua Rao,Jiancong Xie,Yuedong Yang*

Main category: cs.LG

TL;DR: 本文提出MBGen框架，通过引入多体注意力机制和高阶边建模，提升质谱数据驱动的从头分子结构生成性能，尤其在异构体区分方面效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法多采用原子中心和成对相互作用建模，忽略质谱中反映的多原子协同断裂等高阶边相互作用，难以准确解析复杂异构体和非局域碎裂机制。

Method: 提出MBGen：一种增强多体特性的扩散生成框架，融合多体注意力机制与高阶边建模，充分利用MS/MS谱中蕴含的结构信息。

Result: 在NPLIB1和MassSpecGym基准上显著优于现有方法，最高提升达230%；消融实验验证其对高阶相互作用及异构/非局域碎裂信息的敏感性与建模能力。

Conclusion: 多体建模对质谱驱动的分子结构生成具有重要科学价值与实用意义，MBGen为代谢组学与新化合物发现提供了更可靠的方法支撑。

Abstract: Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.

</details>


### [780] [On the Spatiotemporal Dynamics of Generalization in Neural Networks](https://arxiv.org/abs/2602.01651)
*Zichao Wei*

Main category: cs.LG

TL;DR: 本文提出基于物理原理的SEAD架构，通过局部性、对称性和稳定性三大约束，实现神经网络在加法等任务上的长度泛化能力，突破传统深度学习的局限。


<details>
  <summary>Details</summary>
Motivation: 神经网络在数字加法等任务上无法从短序列泛化到长序列，而人类却能掌握规则并推广；作者认为这是违背物理基本原理所致，而非单纯工程问题。

Method: 基于物理学中的局域性、对称性与稳定性三大公设，推导出Spatiotemporal Evolution with Attractor Dynamics（SEAD）架构——一种迭代收敛的神经元胞自动机，采用局部卷积规则。

Result: 在奇偶性判断、大数加法（从16位到100万位）、Rule 110模拟三个任务中均实现完美泛化：加法达100%准确率且计算量自适应输入长度，Rule 110无轨迹发散。

Conclusion: 逻辑推理与统计学习之间的鸿沟不靠扩大模型规模弥补，而需让模型遵循计算的物理规律；SEAD为构建具备真正泛化能力的AI提供了新范式。

Abstract: Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we identify three constraints that any generalizing system must satisfy: (1) Locality -- information propagates at finite speed; (2) Symmetry -- the laws of computation are invariant across space and time; (3) Stability -- the system converges to discrete attractors that resist noise accumulation. From these postulates, we derive -- rather than design -- the Spatiotemporal Evolution with Attractor Dynamics (SEAD) architecture: a neural cellular automaton where local convolutional rules are iterated until convergence. Experiments on three tasks validate our theory: (1) Parity -- demonstrating perfect length generalization via light-cone propagation; (2) Addition -- achieving scale-invariant inference from L=16 to L=1 million with 100% accuracy, exhibiting input-adaptive computation; (3) Rule 110 -- learning a Turing-complete cellular automaton without trajectory divergence. Our results suggest that the gap between statistical learning and logical reasoning can be bridged -- not by scaling parameters, but by respecting the physics of computation.

</details>


### [781] [Efficient Adversarial Attacks on High-dimensional Offline Bandits](https://arxiv.org/abs/2602.01658)
*Seyed Mohammad Hadi Hosseini,Amir Najafi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 本文研究了离线带臂算法（bandit）在评估生成模型时，面对奖励模型被对抗性篡改时的鲁棒性问题，发现高维场景下微小权重扰动即可显著改变带臂行为，并从理论和实验两方面验证了其脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有离线带臂评估方法依赖公开奖励模型，但其在奖励模型被攻击者篡改时的对抗鲁棒性尚未被系统研究。

Method: 提出一种新型威胁模型，理论分析线性及ReLU神经网络奖励函数下攻击可行性，并在两个Hugging Face生成模型评估器（美学质量与构图对齐）上开展实证攻击实验。

Result: 证明高维下成功攻击所需扰动范数随维度增加而减小；实验表明精心设计的扰动可实现近100%攻击成功率，而随机扰动无效。

Conclusion: 离线带臂评估在奖励模型层面存在严重对抗脆弱性，尤其在图像等高维任务中亟需防御机制。

Abstract: Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often distributed with public weights on platforms such as Hugging Face, to provide feedback to the bandit. While online evaluation is expensive and requires repeated trials, offline evaluation with logged data has become an attractive alternative. However, the adversarial robustness of offline bandit evaluation remains largely unexplored, particularly when an attacker perturbs the reward model (rather than the training data) prior to bandit training. In this work, we fill this gap by investigating, both theoretically and empirically, the vulnerability of offline bandit training to adversarial manipulations of the reward model. We introduce a novel threat model in which an attacker exploits offline data in high-dimensional settings to hijack the bandit's behavior. Starting with linear reward functions and extending to nonlinear models such as ReLU neural networks, we study attacks on two Hugging Face evaluators used for generative model assessment: one measuring aesthetic quality and the other assessing compositional alignment. Our results show that even small, imperceptible perturbations to the reward model's weights can drastically alter the bandit's behavior. From a theoretical perspective, we prove a striking high-dimensional effect: as input dimensionality increases, the perturbation norm required for a successful attack decreases, making modern applications such as image evaluation especially vulnerable. Extensive experiments confirm that naive random perturbations are ineffective, whereas carefully targeted perturbations achieve near-perfect attack success rates ...

</details>


### [782] [Quantifying Epistemic Predictive Uncertainty in Conformal Prediction](https://arxiv.org/abs/2602.01667)
*Siu Lun Chau,Soroush H. Zargarbashi,Yusuf Sale,Michele Caprio*

Main category: cs.LG

TL;DR: 本文研究了在共形预测（CP）框架下量化认知预测不确定性（EPU）的问题，通过将CP与可信集（credal set）联系起来，提出了一种基于最大均值不精确度（Maximum Mean Imprecision）的高效可解释的EPU度量方法，并验证其在主动学习和选择性分类中优于仅依赖共形预测区域（CPR）大小的传统方法。


<details>
  <summary>Details</summary>
Motivation: 共形预测（CP）虽能提供可靠预测区间，但其本身未显式刻画模型多重性所导致的认知不确定性（EPU），亟需一种能定量反映这种不确定性并支持决策的度量方法。

Method: 基于CP诱导的可信集（credal set）建模模型多重性，证明该结构在split CP中同样成立；进而提出以最大均值不精确度（Maximum Mean Imprecision）为核心指标来量化可信集中分布间的冲突程度，从而衡量EPU。

Result: 所提EPU度量方法计算高效、解析可解，在主动学习和选择性分类任务中展现出比单纯使用CPR大小更细粒度、更具信息量的不确定性评估能力。

Conclusion: 共形预测不仅可用于构造可靠预测集，还可作为量化与决策支持认知不确定性的坚实理论基础；本文建立的EPU量化框架拓展了CP在不确定性建模中的理论深度与应用潜力。

Abstract: We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-α$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.

</details>


### [783] [ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.01668)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei,Yueqi Xing*

Main category: cs.LG

TL;DR: 本文提出ASGMamba，一种面向资源受限超级计算环境的高效长期多变量时间序列预测框架，通过自适应频谱门控机制和分层多尺度架构，在保持线性复杂度的同时显著提升预测精度并降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型存在二次复杂度问题，难以扩展到长序列；而线性状态空间模型（SSM）难以区分高频噪声与有用信号，导致状态容量浪费。

Method: 提出ASGMamba框架，包含轻量级自适应频谱门控（ASG）机制以动态滤除噪声，并结合分层多尺度架构与变量特定节点嵌入来建模不同物理特性。

Result: 在九个基准数据集上达到SOTA精度，保持严格O(L)复杂度，显著降低长时序任务内存使用。

Conclusion: ASGMamba是一种可扩展、高吞吐、适用于资源受限环境的长期多变量时间序列预测新方案。

Abstract: Long-term multivariate time series forecasting (LTSF) plays a crucial role in various high-performance computing applications, including real-time energy grid management and large-scale traffic flow simulation. However, existing solutions face a dilemma: Transformer-based models suffer from quadratic complexity, limiting their scalability on long sequences, while linear State Space Models (SSMs) often struggle to distinguish valuable signals from high-frequency noise, leading to wasted state capacity. To bridge this gap, we propose ASGMamba, an efficient forecasting framework designed for resource-constrained supercomputing environments. ASGMamba integrates a lightweight Adaptive Spectral Gating (ASG) mechanism that dynamically filters noise based on local spectral energy, enabling the Mamba backbone to focus its state evolution on robust temporal dynamics. Furthermore, we introduce a hierarchical multi-scale architecture with variable-specific Node Embeddings to capture diverse physical characteristics. Extensive experiments on nine benchmarks demonstrate that ASGMamba achieves state-of-the-art accuracy. While keeping strictly $$\mathcal{O}(L)$$ complexity we significantly reduce the memory usage on long-horizon tasks, thus establishing ASGMamba as a scalable solution for high-throughput forecasting in resource limited environments.The code is available at https://github.com/hit636/ASGMamba

</details>


### [784] [Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets](https://arxiv.org/abs/2602.01682)
*Taihei Oki,Shinsaku Sakaue*

Main category: cs.LG

TL;DR: 本文研究在线逆线性优化（即上下文推荐）问题，提出在M-凸可行集下可实现O(d log d)的有限、多项式级后悔界，并扩展至最多C轮对抗性反馈污染情形，获得O((C+1)d log d)的鲁棒后悔界。


<details>
  <summary>Details</summary>
Motivation: 解决在线逆线性优化中是否存在关于维度d的有限且多项式级后悔界的开放问题；此前仅知O(d log T)和指数上界，以及Ω(d)下界。

Method: 利用M-凸集（含拟阵等）上最优解的结构性刻画，结合几何体积论证；进一步通过监测反馈诱导的有向图，自适应检测最多C轮对抗性污染，无需先验知晓C。

Result: 在M-凸可行集下获得O(d log d)的有限后悔界；在C轮对抗污染下获得O((C+1)d log d)的鲁棒后悔界，且不依赖C的先验知识。

Conclusion: 证实了在结构化可行域（M-凸集）下存在关于维度d的多项式级有限后悔界，部分解决了长期开放问题，并提供了对反馈污染的自适应鲁棒性。

Abstract: We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\log T)$, as well as a finite but exponentially large bound of $\exp(O(d\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $Ω(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.

</details>


### [785] [Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment](https://arxiv.org/abs/2602.01685)
*Byeonghu Na,Hyungho Na,Yeongmin Kim,Suhyeon Jo,HeeSun Bae,Mina Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出了一种基于熵正则化Wasserstein距离的语义感知策略正则化方法（WPR），用于改进RLHF框架中的大语言模型对齐效果，克服了传统KL散度无法捕捉语义相似性的缺陷，并在实验中优于基于KL及f-散度的基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF中使用的KL散度及其f-散度变体仅在相同位置的token概率间进行比较，无法反映token之间的语义相似性，限制了策略对齐的质量。

Method: 提出Wasserstein Policy Regularization（WPR），基于熵正则化Wasserstein距离构建语义感知的策略正则化项；利用其对偶形式将正则化表示为通过最优对偶变量施加于奖励上的惩罚项，从而保持与标准强化学习算法的兼容性。

Result: WPR在多个基准任务上显著优于KL和f-散度基线方法，验证了引入语义感知策略距离对提升模型对齐效果的有效性。

Conclusion: 语义感知的Wasserstein距离比传统KL散度更适合作为RLHF中的策略正则化工具，能更好建模token空间几何结构，提升人类偏好对齐性能。

Abstract: Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.

</details>


### [786] [Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner](https://arxiv.org/abs/2602.01705)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: 本文提出LaDi-RL框架，将强化学习探索从离散token空间迁移至连续潜在空间，利用引导扩散建模语义级推理轨迹，缓解离散RL中的多样性坍缩问题，在代码生成和数学推理任务上显著提升pass@1和pass@k性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于离散Chain-of-Thought的RL方法在token空间探索时易因策略熵下降导致多样性坍缩，限制推理能力。

Method: 提出Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL)：在连续潜在空间中建模语义级推理路径，通过多步引导扩散实现去噪探索，解耦潜在空间探索与文本生成，并结合互补文本策略优化。

Result: 在代码生成和数学推理基准上，pass@1分别提升+9.4%和+5.7%，pass@k也一致优于离散RL基线。

Conclusion: 基于潜在空间的扩散式RL是一种有原则的替代方案，比纯token级离散RL更有效，尤其适用于复杂推理任务。

Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.

</details>


### [787] [Revisiting Generalization Measures Beyond IID: An Empirical Study under Distributional Shift](https://arxiv.org/abs/2602.01718)
*Sora Nakai,Youssef Fadhloun,Kacem Mathlouthi,Kotaro Yoshida,Ganesh Talluri,Ioannis Mitliagkas,Hiroki Naganuma*

Main category: cs.LG

TL;DR: 本文系统评估了40多种泛化性度量方法在分布外（OOD）场景下的鲁棒性，发现多数指标在分布偏移下性能显著下降，仅少数保持稳定。


<details>
  <summary>Details</summary>
Motivation: 现有泛化性预测指标在独立同分布（IID）假设下表现良好，但在实际分布偏移（OOD）场景中是否可靠尚不清楚；前人研究已指出其在训练配置变化下存在不稳定性。

Method: 在10,000种超参数配置下训练中小规模模型，评估超40种仅依赖训练数据和模型的泛化度量；实验扩展至多种分布偏移、多架构/训练策略，并首次纳入校准类与信息准则类新指标。

Result: 多数泛化度量在分布偏移下预测能力显著下降；一小部分指标（如某些基于信息准则或校准的度量）展现出跨分布设置的相对稳定性。

Conclusion: 泛化性度量的鲁棒性高度依赖于数据分布假设，单纯在IID设定下验证其有效性存在局限；未来工作需在更现实的OOD条件下设计和评估泛化预测工具。

Abstract: Generalization remains a central yet unresolved challenge in deep learning, particularly the ability to predict a model's performance beyond its training distribution using quantities available prior to test-time evaluation. Building on the large-scale study of Jiang et al. (2020). and concerns by Dziugaite et al. (2020). about instability across training configurations, we benchmark the robustness of generalization measures beyond IID regime. We train small-to-medium models over 10,000 hyperparameter configurations and evaluate more than 40 measures computable from the trained model and the available training data alone. We significantly broaden the experimental scope along multiple axes: (i) extending the evaluation beyond the standard IID setting to include benchmarking for robustness across diverse distribution shifts, (ii) evaluating multiple architectures and training recipes, and (iii) newly incorporating calibration- and information-criteria-based measures to assess their alignment with both IID and OOD generalization. We find that distribution shifts can substantially alter the predictive performance of many generalization measures, while a smaller subset remains comparatively stable across settings.

</details>


### [788] [MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration](https://arxiv.org/abs/2602.01734)
*Lianhai Ren,Yucheng Ding,Xiao Liu,Qianxiao Li,Peng Cheng,Yeyun Gong*

Main category: cs.LG

TL;DR: 本文研究了大语言模型预训练中的训练不稳定性问题，发现权重矩阵稳定秩下降和相邻层雅可比矩阵对齐加剧是梯度爆炸的前兆，并提出一种基于矩阵符号运算的优化器MSign来恢复稳定秩、防止训练崩溃。


<details>
  <summary>Details</summary>
Motivation: 训练不稳定性（尤其是梯度爆炸）严重浪费计算资源，亟需理解其机理并提出有效缓解方法。

Method: 通过分析μP缩放的NanoGPT模型，识别出稳定秩下降与雅可比对齐两个关键前兆现象，并给出理论证明；进而提出MSign优化器，周期性应用矩阵符号操作以恢复稳定秩。

Result: 在5M至3B参数规模模型上验证MSign能有效防止训练失败，计算开销低于7.0%。

Conclusion: 稳定秩下降与雅可比对齐共同导致深度网络中梯度范数指数增长；MSign通过干预稳定秩，为LLM预训练提供了一种低开销、高鲁棒性的稳定性保障方案。

Abstract: Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.

</details>


### [789] [Position: The Inevitable End of One-Architecture-Fits-All-Domains in Time Series Forecasting](https://arxiv.org/abs/2602.01736)
*Qinwei Ma,Jingzhe Shi,Jiahao Qiu,Zaiwen Yang*

Main category: cs.LG

TL;DR: 本文指出当前时间序列预测的神经网络架构研究已陷入瓶颈：通用领域模型复杂度高、性能饱和，且难以兼顾单领域SOTA与跨领域泛化能力；因此呼吁社区转向特定领域深度学习方法或面向通用领域的元学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列神经网络架构在通用领域性能饱和、鲁棒性差，且无法兼顾单领域SOTA与跨领域泛化，导致学术进展难以落地到金融、气象、交通等实际领域。

Method: 通过系统性分析和总结近期研究对时序神经网络有效性和鲁棒性的质疑，揭示其内在局限性——即单领域SOTA与通用泛化能力之间的根本冲突。

Result: 指出现有通用时序神经网络架构研究已趋饱和，对实际领域应用缺乏指导价值，各领域仍依赖独立开发的专用方法。

Conclusion: 应将研究重心从通用领域神经网络架构转向特定领域深度学习方法或面向通用领域的元学习方法。

Abstract: Recent work has questioned the effectiveness and robustness of neural network architectures for time series forecasting tasks. We summarize these concerns and analyze groundly their inherent limitations: i.e. the irreconcilable conflict between single (or few similar) domains SOTA and generalizability over general domains for time series forecasting neural network architecture designs. Moreover, neural networks architectures for general domain time series forecasting are becoming more and more complicated and their performance has almost saturated in recent years. As a result, network architectures developed aiming at fitting general time series domains are almost not inspiring for real world practices for certain single (or few similar) domains such as Finance, Weather, Traffic, etc: each specific domain develops their own methods that rarely utilize advances in neural network architectures of time series community in recent 2-3 years. As a result, we call for the time series community to shift focus away from research on time series neural network architectures for general domains: these researches have become saturated and away from domain-specific SOTAs over time. We should either (1) focus on deep learning methods for certain specific domain(s), or (2) turn to the development of meta-learning methods for general domains.

</details>


### [790] [Softmax Linear Attention: Reclaiming Global Competition](https://arxiv.org/abs/2602.01744)
*Mingwei Xu,Xuan Lin,Xinnan Guo,Wanqing Xu,Wanyun Cui*

Main category: cs.LG

TL;DR: 本文提出Softmax Linear Attention (SLA)框架，在保持线性复杂度的同时，通过在头级别引入softmax竞争机制，恢复线性注意力中缺失的全局竞争能力，从而提升长上下文建模与噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽降低计算复杂度，但因去除softmax导致缺乏全局竞争机制，削弱模型对关键信息的精准聚焦能力，尤其在长上下文和噪声场景下表现不足。

Method: 提出SLA：将softmax操作从token级提升至head级，利用多头结构作为粗粒度语义槽，通过竞争性门控动态选择最相关子空间，重引入winner-take-all机制。

Result: SLA在语言建模与长上下文基准上持续提升RetNet、GLA、GDN等SOTA线性模型性能，尤其在检索任务中显著增强抗噪鲁棒性。

Conclusion: SLA成功在不牺牲线性效率的前提下，恢复了关键的全局竞争能力，验证了高层结构设计（而非仅局部核改进）对提升线性注意力表达力的有效性。

Abstract: While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.

</details>


### [791] [Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning](https://arxiv.org/abs/2602.01745)
*Wenhao Yu,Shaohang Wei,Jiahong Liu,Yifan Li,Minda Hu,Aiwei Liu,Hao Zhang,Irwin King*

Main category: cs.LG

TL;DR: 本文提出RankTuner方法，通过引入概率-熵校准信号（Relative Rank Indicator）进行token级重加权，以更精准识别真正未学好的token，提升大模型在数学推理、OOD泛化及代码生成等任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有token级重加权方法仅依赖单一维度指标（如真值概率或token熵），易误判噪声或易替换token为关键学习位置，或忽略目标对齐性。

Method: 提出Relative Rank Indicator（RRI），比较真值token在预测分布中的实际排名与其期望排名，并用其倒数作为token级Relative Scale，用于重加权微调损失。

Result: 在多个模型主干上实验表明，RankTuner在数学推理基准、OOD推理迁移和代码生成任务上均优于仅用概率或仅用熵的重加权基线。

Conclusion: 概率-熵联合校准的token级重加权机制能更鲁棒、精准地引导模型聚焦于真正欠学习的位置，提升泛化与下游性能。

Abstract: Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.

</details>


### [792] [Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment](https://arxiv.org/abs/2602.01746)
*Hongyi Peng,Han Yu,Xiaoxiao Li,Qiang Yang*

Main category: cs.LG

TL;DR: 本文提出FedGaLore方法，通过客户端梯度子空间优化与服务器端投影二阶矩状态的鲁棒同步，解决非独立同分布（non-IID）下LoRA在联邦微调中性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: LoRA在联邦微调中广泛使用，但在non-IID数据下显著弱于全参数微调，其根本原因在于更新空间不匹配和优化器状态不匹配。

Method: 提出FedGaLore：客户端采用GaLore风格的梯度子空间优化；服务器端通过谱共享信号提取实现投影二阶矩状态的漂移鲁棒同步。

Result: 在NLU、视觉和NLG多个基准上，FedGaLore在non-IID设置下显著优于现有联邦LoRA基线方法，提升鲁棒性与准确率。

Conclusion: FedGaLore有效缓解了LoRA在联邦学习non-IID场景下的性能退化，为低秩适应的分布式优化提供了新思路。

Abstract: Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.

</details>


### [793] [MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network](https://arxiv.org/abs/2602.01751)
*Kunyi Fan,Mengjie Chen,Longlong Li,Cunquan Qu*

Main category: cs.LG

TL;DR: 本文提出MGKAN模型，一种基于图Kolmogorov-Arnold网络的药物相互作用预测方法，通过引入可学习基函数、多视图网络融合与非线性建模，显著提升不对称DDI预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在药物相互作用（DDI）预测中多采用线性聚合和对称假设，难以建模非线性和异质性药理关系，尤其无法有效刻画DDI的方向性语义。

Method: 提出MGKAN：1）用KAN驱动的可学习基函数替代传统MLP；2）整合三种异构网络视图（不对称DDI网络、共互作网络、生化相似性网络），并采用角色特异性嵌入；3）设计融合线性注意力与非线性变换的融合模块。

Result: 在两个基准数据集上显著优于7种SOTA基线；消融实验和案例分析验证了其对方向性药物效应的建模能力与预测准确性。

Conclusion: MGKAN通过非线性、不对称与多视图建模，为DDI预测提供了更符合药理实际的图表示学习框架，提升了临床用药安全性预测能力。

Abstract: Predicting drug-drug interactions (DDIs) is essential for safe pharmacological treatments. Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric assumptions, limiting their ability to capture nonlinear and heterogeneous patterns. We propose MGKAN, a Graph Kolmogorov-Arnold Network that introduces learnable basis functions into asymmetric DDI prediction. MGKAN replaces conventional MLP transformations with KAN-driven basis functions, enabling more expressive and nonlinear modeling of drug relationships. To capture pharmacological dependencies, MGKAN integrates three network views-an asymmetric DDI network, a co-interaction network, and a biochemical similarity network-with role-specific embeddings to preserve directional semantics. A fusion module combines linear attention and nonlinear transformation to enhance representational capacity. On two benchmark datasets, MGKAN outperforms seven state-of-the-art baselines. Ablation studies and case studies confirm its predictive accuracy and effectiveness in modeling directional drug effects.

</details>


### [794] [A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention](https://arxiv.org/abs/2602.01763)
*Xiaowei Ye,Xiaoyu He,Chao Liao,Chen Wu,Pinyan Lu*

Main category: cs.LG

TL;DR: 本文通过理论分析，首次证明了全注意力机制与混合注意力机制（如线性注意力）在表达能力上存在本质差距，特别是在多步序列推理任务中，全注意力网络具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有高效注意力机制（如线性、混合注意力）缺乏对其相对于标准全注意力的表达能力的严格理论刻画，存在基础性理论空白。

Method: 建立基于序列函数复合的多步推理任务模型，通过构造性证明和复杂度下界分析，比较全注意力与可表示为递推形式的线性注意力（含Mamba、DeltaNet等）及其混合结构的表达能力。

Result: 证明(L+1)层全注意力网络可完成某类序列推理任务，而任何含(L−1)层全注意力与高达2^(3L²)层线性注意力的混合网络均无法完成该任务，确立严格的表达力层级关系。

Conclusion: 全注意力机制在本质推理能力上不可被当前主流线性/混合注意力机制有效替代，该结果为注意力机制的设计与选择提供了首个可证明的理论依据。

Abstract: Transformers serve as the foundation of most modern large language models. To mitigate the quadratic complexity of standard full attention, various efficient attention mechanisms, such as linear and hybrid attention, have been developed. A fundamental gap remains: their expressive power relative to full attention lacks a rigorous theoretical characterization. In this work, we theoretically characterize the performance differences among these attention mechanisms. Our theory applies to all linear attention variants that can be formulated as a recurrence, including Mamba, DeltaNet, etc. Specifically, we establish an expressiveness hierarchy: for the sequential function composition-a multi-step reasoning task that must occur within a model's forward pass, an ($L+1$)-layer full attention network is sufficient, whereas any hybrid network interleaving $L-1$ layers of full attention with a substantially larger number ($2^{3L^2}$) of linear attention layers cannot solve it. This result demonstrates a clear separation in expressive power between the two types of attention. Our work provides the first provable separation between hybrid attention and standard full attention, offering a theoretical perspective for understanding the fundamental capabilities and limitations of different attention mechanisms.

</details>


### [795] [CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling](https://arxiv.org/abs/2602.01766)
*Runsong Zhao,Shilei Liu,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Yujin Yuan,Tong Xiao,Jingbo Zhu,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 本文提出Collaborative Memory Transformer (CoMeT)，一种新型Transformer架构，通过双记忆系统（临时FIFO记忆与带门控更新的全局记忆）实现对任意长序列的线性时间复杂度和常量内存消耗处理，可作为插件模块集成到预训练模型中并仅需少量微调。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次计算复杂度和无限增长的KV缓存严重阻碍了长上下文处理能力。

Method: CoMeT采用双记忆系统：基于FIFO队列的临时记忆用于近期事件，带门控更新规则的全局记忆用于建模长程依赖；该系统作为动态软提示作用于后续数据块；同时引入层级流水线并行策略以支持超长上下文的高效微调。

Result: 在1M token序列中能准确从任意位置检索32k上下文训练过的passkey；在SCROLLS基准上超越其他高效方法，摘要任务性能接近全注意力基线；在真实Agent与用户行为问答任务中验证了实用性。

Conclusion: CoMeT是一种高效、即插即用的长上下文建模方案，在保持低资源开销的同时显著提升长程依赖建模能力，具备强实用性和扩展性。

Abstract: The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/

</details>


### [796] [IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination](https://arxiv.org/abs/2602.01769)
*Yuanshuai Li,Yuping Yan,Jirui Han,Fei Ming,Lingjuan Lv,Yaochu Jin*

Main category: cs.LG

TL;DR: 本文提出IRIS方法，利用连续隐式奖励在原生对数概率空间中捕捉多模态内部竞争，通过自生成偏好对进行在线策略优化，无需外部评估器即可有效缓解多模态大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于DPO的对齐方法依赖昂贵的外部评估器，存在离策略学习间隙和离散化损失，且无法获取模型内部状态来识别导致幻觉的多模态细粒度冲突。

Method: 提出IRIS（隐式奖励引导的内部筛选）方法，利用原生log-probability空间中的连续隐式奖励建模模态间竞争，并基于该奖励自动生成和筛选偏好对，实现端到端的在线策略对齐。

Result: 在关键幻觉评测基准上，仅用5.7k样本即取得极具竞争力的性能，全程无需任何外部反馈。

Conclusion: IRIS提供了一种高效、原理清晰的多模态大语言模型幻觉缓解新范式。

Abstract: Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.
  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.

</details>


### [797] [DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics](https://arxiv.org/abs/2602.01772)
*Yucheng Liao,Han Wen,Weinan E,Weijie Zhang*

Main category: cs.LG

TL;DR: DIA-CLIP是一种预训练的跨模态表征学习模型，用于数据非依赖性采集质谱（DIA-MS）数据分析，实现无需每轮半监督训练的零样本肽谱匹配（PSM）推断，在蛋白鉴定数量和特异性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DIA分析框架需每轮半监督训练进行肽谱匹配重打分，易过拟合且泛化能力差，难以适应不同物种和实验条件。

Method: 提出DIA-CLIP模型，结合双编码器对比学习与编码器-解码器架构，构建肽段序列与对应质谱特征的统一跨模态表征空间，支持零样本PSM推断。

Result: 在多个基准测试中显著超越现有工具：蛋白鉴定数提升最多达45%，诱陷识别减少12%；并展现出在单细胞和空间蛋白质组学中的应用潜力。

Conclusion: DIA-CLIP通过预训练和跨模态表征学习，革新了DIA-MS分析范式，提升了鲁棒性、通用性与实用性，为大规模系统生物学研究提供了更可靠、深度更强的分析工具。

Abstract: Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for peptide-spectrum match (PSM) re-scoring. This approach is prone to overfitting and lacks generalizability across diverse species and experimental conditions. Here, we present DIA-CLIP, a pre-trained model shifting the DIA analysis paradigm from semi-supervised training to universal cross-modal representation learning. By integrating dual-encoder contrastive learning framework with encoder-decoder architecture, DIA-CLIP establishes a unified cross-modal representation for peptides and corresponding spectral features, achieving high-precision, zero-shot PSM inference. Extensive evaluations across diverse benchmarks demonstrate that DIA-CLIP consistently outperforms state-of-the-art tools, yielding up to a 45% increase in protein identification while achieving a 12% reduction in entrapment identifications. Moreover, DIA-CLIP holds immense potential for diverse practical applications, such as single-cell and spatial proteomics, where its enhanced identification depth facilitates the discovery of novel biomarkers and the elucidates of intricate cellular mechanisms.

</details>


### [798] [Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting](https://arxiv.org/abs/2602.01776)
*Mingyue Cheng,Xiaoyu Tao,Qi Liu,Ze Guo,Enhong Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间序列预测范式——智能体式时间序列预测（ATSF），将预测重构为包含感知、规划、行动、反思和记忆的智能体过程，强调工作流组织、工具交互与经验积累，而非仅依赖静态预测模型。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测范式在自适应、多轮交互、持续演化等复杂场景中表现不足，亟需引入具备推理、反馈与进化能力的智能体机制。

Method: 提出ATSF概念框架，并设计三种实现范式：基于工作流的设计、智能体强化学习、以及混合智能体工作流范式。

Result: 系统阐述了ATSF的核心要素、典型实现路径，并分析了从模型中心转向智能体中心所面临的关键机遇与挑战。

Conclusion: ATSF为时间序列预测提供了更具适应性、可解释性与持续学习能力的新范式，有望成为该领域未来研究的重要基础方向。

Abstract: Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.

</details>


### [799] [Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions](https://arxiv.org/abs/2602.01777)
*M. Arashi,M. Amintoosi*

Main category: cs.LG

TL;DR: 本文提出了一种基于Stein规则收缩的随机梯度估计器，通过自适应地将小批量梯度向历史动量导出的稳定受限估计器收缩，以降低高维场景下的估计风险，在理论和实验上均优于标准随机梯度。


<details>
  <summary>Details</summary>
Motivation: 在高维学习中，传统无偏随机梯度估计器在二次损失下通常不可接受，存在风险优化空间。

Method: 将随机梯度计算建模为高维估计问题，引入基于Stein规则收缩的决策理论框架；构建数据驱动的收缩梯度估计器，利用自适应噪声方差估计（基于二阶矩统计）控制收缩强度，并将其嵌入Adam优化器。

Result: 在p≥3且满足高斯噪声假设下，该估计器在平方误差损失下一致优于标准随机梯度，且具有经典意义下的minimax最优性；在CIFAR10/100及不同标签噪声下，大批次训练中持续超越Adam；消融表明仅对高维卷积层收缩效果最佳。

Conclusion: 经典收缩理论为现代深度学习中的随机梯度估计提供了原理清晰、切实有效的改进路径。

Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.

</details>


### [800] [Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning](https://arxiv.org/abs/2602.01791)
*Zheng Zhang,Ao Lu,Yuanhao Zeng,Ziwei Shan,Jinjin Guo,Lufei Li,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: 本文提出Grad2Reward框架，通过梯度归因从LLM-as-a-Judge的前向推理中提取密集、细粒度的过程奖励（token-level），实现无需外部裁判或额外奖励模型的自评判式强化学习，显著提升开放域长程推理效果。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在开放任务中依赖LLM-as-a-Judge提供稀疏序列级奖励，缺乏细粒度监督，且忽略Judge内部丰富的中间反馈信号。

Method: 提出Grad2Reward：利用单次反向传播从Judge模型推理过程中提取梯度归因作为密集过程奖励；引入自评判机制，使策略模型能基于自身生成内容的梯度反馈进行优化。

Result: 在多个开放域任务上，使用Grad2Reward优化的策略模型性能显著优于基线方法，训练效率和推理质量均提升。

Conclusion: Grad2Reward通过挖掘Judge内部梯度信号实现高效、自持的强化学习，为开放域复杂推理提供了可扩展、免外部裁判的新范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.

</details>


### [801] [Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It](https://arxiv.org/abs/2602.01826)
*Yaxiang Zhang,Yingru Li,Jiacai Liu,Jiawei Xu,Ziniu Li,Qian Liu,Haoyuan Li*

Main category: cs.LG

TL;DR: 本文分析了大语言模型强化学习训练不稳定的根源，指出训练-推理失配与梯度噪声随训练动态加剧，并提出一种基于响应长度动态触发学习率衰减的新型调度器以稳定训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练大语言模型存在严重不稳定性，现有方法（如重要性采样）在长周期训练中失效，需从优化角度深入理解其动态成因。

Method: 从优化视角分析梯度噪声与训练-推理失配的协同演化关系，发现缩小更新步长可抑制失配；据此设计基于响应长度动态触发学习率衰减的LR调度器。

Result: 所提动态LR调度器能有效抑制训练-推理失配，显著提升RL训练稳定性，在多个实验中实现持续收敛。

Conclusion: 训练-推理失配不是静态数值误差，而是与模型优化过程耦合的动态失败；响应长度是预测失配风险的可靠早期信号，动态调小学习率可稳健应对该问题。

Abstract: Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to "training inference mismatch stemming" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.

</details>


### [802] [Hyperbolic Graph Neural Networks Under the Microscope: The Role of Geometry-Task Alignment](https://arxiv.org/abs/2602.01828)
*Dionisia Naddeo,Jonas Linkerhägner,Nicola Toschi,Geri Skenderi,Veronica Lachi*

Main category: cs.LG

TL;DR: 本文质疑了超球面图神经网络（HGNNs）在树状图表示学习中的普适性，提出‘几何-任务对齐’这一新条件，并通过理论与实验表明：仅当任务本身需要保持度量结构（如链路预测）时，HGNNs才显著优于欧氏模型；否则其优势消失。


<details>
  <summary>Details</summary>
Motivation: 现有工作普遍假设超双曲结构适合树状图，但忽略了任务本身是否与该几何结构匹配；本文旨在揭示‘图是否超双曲’之外更关键的问题——‘任务是否与超双曲几何对齐’。

Method: 提出几何-任务对齐概念，通过理论分析与两个合成回归任务验证HGNNs的低失真表示能力，并在链路预测和节点分类任务上联合评估预测性能与嵌入失真。

Result: 理论与实验证明HGNNs在需保持度量结构的任务（如链路预测）中表现优异，但在节点分类等非对齐任务中不具优势；几何-任务对齐是HGNNs胜出的必要条件。

Conclusion: HGNNs的有效性不仅取决于图的超双曲性，更关键在于任务是否与超双曲几何对齐；该发现为选择图神经网络几何空间提供了新准则。

Abstract: Many complex networks exhibit hyperbolic structural properties, making hyperbolic space a natural candidate for representing hierarchical and tree-like graphs with low distortion. Based on this observation, Hyperbolic Graph Neural Networks (HGNNs) have been widely adopted as a principled choice for representation learning on tree-like graphs. In this work, we question this paradigm by proposing an additional condition of geometry-task alignment, i.e., whether the metric structure of the target follows that of the input graph. We theoretically and empirically demonstrate the capability of HGNNs to recover low-distortion representations on two synthetic regression problems, and show that their geometric inductive bias becomes helpful when the problem requires preserving metric structure. Additionally, we evaluate HGNNs on the tasks of link prediction and node classification by jointly analyzing predictive performance and embedding distortion, revealing that only link prediction is geometry-aligned. Overall, our findings shift the focus from only asking "Is the graph hyperbolic?" to also questioning "Is the task aligned with hyperbolic geometry?", showing that HGNNs consistently outperform Euclidean models under such alignment, while their advantage vanishes otherwise.

</details>


### [803] [DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis](https://arxiv.org/abs/2602.01839)
*Ru Zhang,Xunkai Li,Yaxin Deng,Sicheng Liu,Daohan Su,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang,Jia Li*

Main category: cs.LG

TL;DR: DOGMA is a data-centric framework that enhances single-cell transcriptomics analysis by integrating multi-level biological prior knowledge to improve graph construction and semantic representation, achieving state-of-the-art performance with better robustness, sample efficiency, and lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing methods either ignore intercellular relationships and data quality issues (sequence-based) or rely on heuristic rules without incorporating biological prior knowledge (structured methods), leading to suboptimal graph representations and reduced ML utility.

Method: DOGMA integrates Statistical Anchors with Cell Ontology and Phylogenetic Trees for deterministic, cross-species graph construction, and uses Gene Ontology to bridge feature-level semantic gaps via functional priors.

Result: DOGMA achieves state-of-the-art performance on complex multi-species and multi-organ benchmarks, with superior zero-shot robustness, sample efficiency, and significantly lower computational cost.

Conclusion: Incorporating multi-level biological prior knowledge in a deterministic, structured manner substantially improves data representation and downstream ML performance in single-cell transcriptomics.

Abstract: Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.
  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.

</details>


### [804] [Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models](https://arxiv.org/abs/2602.01842)
*Jinbin Bai,Yixuan Li,Yuchen Zhu,Yi Xin,Qingyu Shi,Aosong Feng,Xiaohong Liu,Molei Tao,Jianru Xue,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: 本文提出Prism框架，一种专为离散扩散语言模型（dLLMs）设计的高效测试时缩放（TTS）方法，通过分层轨迹搜索、局部分支与部分重掩码、以及自验证反馈机制，在数学推理和代码生成任务上实现了性能与效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放算法多依赖自回归解码，不适用于并行解码的离散扩散语言模型（dLLMs），导致其生成潜力尚未被充分挖掘，亟需开发适配dLLMs的高效TTS方法。

Method: 提出Prism框架，包含三部分：(i) 分层轨迹搜索（HTS），在早期至中期去噪窗口中动态剪枝与重分配计算资源；(ii) 局部分支与部分重掩码，兼顾多样性探索与高置信度token保留；(iii) 自验证反馈（SVF），利用中间结果的自评估提示替代外部验证器。

Result: 在四个数学推理与代码生成基准上，于LLaDA 8B Instruct、Dream 7B Instruct和LLaDA 2.0-mini三个dLLMs上验证，Prism以显著更少的函数评估次数（NFE）达到最佳N次采样（best-of-N）性能。

Conclusion: Prism是首个面向dLLMs的高效TTS框架，有效提升了其推理能力，同时兼顾计算效率，为dLLMs的测试时优化提供了新范式。

Abstract: Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.

</details>


### [805] [No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation](https://arxiv.org/abs/2602.01845)
*Furkan Eris*

Main category: cs.LG

TL;DR: Proust是一个309M参数的因果蛋白语言模型，通过借鉴大语言模型的架构创新，在保持生成能力的同时，在蛋白质适应性预测任务上达到与掩码语言模型（MLM）相当甚至更优的性能，且训练成本显著更低。


<details>
  <summary>Details</summary>
Motivation: 解决蛋白语言模型中掩码语言模型（擅长预测但无法生成）与因果模型（可生成但预测性能弱）长期割裂的问题，统一预测与生成能力。

Method: 提出Proust模型，采用分组查询注意力（共享K/V投影）、跨层值残差、深度卷积等架构改进；在33B token数据上训练，仅用40 B200 GPU小时。

Result: 在ProteinGym取代任务上Spearman ρ=0.390，媲美计算开销高50–200倍的MLMs；在indel预测上创SOTA；在EVEREST病毒适应性基准上接近结构感知方法；同时具备原生生成能力；熵方差可部分预测检索增强效果。

Conclusion: Proust成功弥合了蛋白语言模型中预测与生成能力的鸿沟，在效率、性能与多功能性上实现新平衡，为可扩展的生物序列建模与测试时推理优化提供新范式。

Abstract: Protein language models (PLMs) face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures. We introduce \textbf{Proust}, a 309M-parameter causal PLM that bridges this gap through architectural innovations adapted from recent LLM research, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman $ρ= 0.390$ on ProteinGym substitutions, competitive with MLMs requiring 50--200$\times$ the compute. On indels, Proust sets a new state-of-the-art, outperforming models up to 20$\times$ larger. On EVEREST viral fitness benchmarks, it approaches structure-aware methods using sequence alone. These powerful representations position Proust in a sweet spot as it also retains native generative capabilities that MLMs lack by design. Interpretability analysis reveals that per-position entropy variance predicts, to an extent, when retrieval augmentation helps and hurts. Such insights can grow in both quantity and quality at scale and inform capabilities such as test-time scaling. Code and weights are available at https://github.com/Furkan9015/proust-inference

</details>


### [806] [Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models](https://arxiv.org/abs/2602.01849)
*Ziwei Luo,Ziqi Jin,Lei Wang,Lidong Bing,Thomas B. Schön*

Main category: cs.LG

TL;DR: 本文提出了一种名为自奖励序列蒙特卡洛（Self-Rewarding SMC）的推理时扩展算法，用于提升掩码扩散语言模型（MDLMs）的采样质量与多样性。通过并行运行多个扩散轨迹（粒子），并以轨迹级置信度作为自奖励信号进行重要性加权和重采样，该方法无需额外训练或外部奖励即可显著提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有MDLMs多采用基于单步预测置信度的贪心采样策略，导致路径多样性低、易受噪声干扰，难以兼顾生成质量与多样性。

Method: 提出自奖励SMC算法：并行运行多个扩散粒子；定义轨迹级置信度作为自奖励信号；在每步迭代中对粒子进行重要性加权与重采样，引导生成向全局高置信、高质量样本收敛。

Result: 在多个MDLM模型与基准测试上验证有效，显著提升生成质量与多样性，且无需额外训练或外部奖励信号，充分利用并行推理能力。

Conclusion: 自奖励SMC是一种高效、即插即用的推理时优化方法，为扩散语言模型提供了更鲁棒、更多样、更高质量的采样范式。

Abstract: This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.

</details>


### [807] [FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization](https://arxiv.org/abs/2602.01852)
*Zeyan Wang,Zhengmao Liu,Yongxin Cai,Chi Li,Xiaoying Tang,Jingchao Chen,Zibin Pan,Jing Qiu*

Main category: cs.LG

TL;DR: 本文提出FUPareto框架，通过Pareto优化解决联邦遗忘中的效用-遗忘权衡、成员推理攻击风险及多客户端并发遗忘难题，引入MBS损失和Null-Space投影MGDA算法，在保证遗忘效果的同时显著提升模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在三方面问题：遗忘目标损害模型效用或增加成员推理攻击（MIA）风险；遗忘与效用之间存在固有冲突；缺乏对多客户端并发遗忘的有效支持。

Method: 提出FUPareto框架，包含Minimum Boundary Shift（MBS）损失函数以增强遗忘效率并缓解MIA风险；采用Pareto改进步骤维持效用，Pareto扩展步骤保障遗忘；在Pareto扩展中集成Null-Space Projected MGDA算法解耦梯度冲突。

Result: 在多种实验场景下，FUPareto在遗忘效果和保留效用两方面均显著优于现有最先进联邦遗忘方法。

Conclusion: FUPareto通过Pareto-augmented优化实现了高效、公平、并发的多客户端联邦遗忘，在兼顾安全性（抗MIA）与实用性（高保留效用）方面取得突破。

Abstract: Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.

</details>


### [808] [Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning](https://arxiv.org/abs/2602.01853)
*Xiangkun Wu,Qianglin Wen,Yingying Zhang,Hongtu Zhu,Ting Li,Chengchun Shi*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer强化学习的A/B测试新方法，用于时间序列实验中的策略评估，克服了现有方法无法充分利用历史信息和依赖强假设的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有A/B测试在时间序列实验中存在两个主要问题：未充分利用全部历史信息进行干预分配，以及依赖强假设来近似优化目标（如估计处理效应的均方误差）。

Method: 提出一种结合Transformer与强化学习（RL）的新方法：利用Transformer建模完整历史以进行干预分配，并用RL直接优化均方误差（MSE），无需强假设。

Result: 在合成数据、公开调度仿真器及真实网约车数据集上的实验表明，该方法持续优于现有设计。

Conclusion: 该方法通过建模动态依赖关系并直接优化目标函数，在时间序列A/B测试中实现了更优的实验设计。

Abstract: A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.

</details>


### [809] [Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity](https://arxiv.org/abs/2602.01949)
*Leonardo Stoppani,Davide Bacciu,Shahab Mokarizadeh*

Main category: cs.LG

TL;DR: 本文提出了一种新的多样性评估指标（Diversity Score）和一种边界交叉注意力（BCA）模块，以提升扩散模型在平面图生成任务中的布局多样性与几何一致性，揭示了现实性与多样性之间的权衡，并强调需在保真度、多样性和泛化能力间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型优化感知指标（如FID）导致设计多样性受限，且缺乏对建筑边界的几何一致性建模能力。

Method: 提出Diversity Score（DS）量化约束下布局多样性；设计Boundary Cross-Attention（BCA）模块以显式建模建筑边界条件；通过延长训练与OOD评估分析多样性坍塌与先验依赖问题。

Result: BCA显著提升边界遵循度；延长训练引发FID无法检测的多样性坍塌；OOD评估揭示模型严重依赖数据集先验。

Conclusion: 需构建能显式平衡保真度、多样性与泛化能力的生成系统，而非仅优化单一感知指标。

Abstract: Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fréchet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.

</details>


### [810] [Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG](https://arxiv.org/abs/2602.01855)
*Blagoj Hristov,Hristijan Gjoreski,Vesna Ojleska Latkoska,Gorjan Nadzinski*

Main category: cs.LG

TL;DR: 本文提出了一种数据高效、基于双通道sEMG的混合Transformer框架，引入Time2Vec时序嵌入与归一化加性融合策略，在仅用两个传感器通道下实现10类动作95.7%的F1分数，并通过两试次快速校准解决跨被试泛化问题，挑战了高密度肌电传感的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有高精度肌电假肢控制依赖复杂密集的多传感器阵列，成本高、可及性差；亟需一种在极简硬件（如双通道sEMG）下仍能保持高性能的解决方案。

Method: 提出一种面向稀疏双通道sEMG的混合Transformer模型：① 采用可学习的Time2Vec替代固定位置编码以建模生物信号的时间形变；② 设计归一化加性融合对齐时空特征隐分布；③ 引入两阶段课程学习缓解小样本问题；④ 配套两试次/手势的快速校准协议。

Result: 在10类动作识别任务中，跨8被试平均F1达95.7%±0.20%，显著优于标准Transformer和CNN-LSTM；快速校准后新被试准确率从21.0%±2.98%提升至96.9%±0.52%；结构分析表明时空容量均衡分配最稳定。

Conclusion: 高保真时序建模可有效弥补空间采样不足，验证了低硬件复杂度下实现高性能肌电控制的可行性，为低成本、易个性化的新一代假肢接口提供了实用蓝图。

Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\pm$ 2.98% to 96.9% $\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.

</details>


### [811] [FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning](https://arxiv.org/abs/2602.01976)
*Hongwei Yan,Guanglong Sun,Kanglei Zhou,Qian Li,Liyuan Wang,Yi Zhong*

Main category: cs.LG

TL;DR: 本文提出FlyPrompt，一种受果蝇大脑记忆系统启发的通用持续学习（GCL）框架，通过随机扩展解析路由器和时间输出头集成，在单次遍历、无任务边界的数据流中实现高效持续参数微调，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有持续参数高效微调（PET）方法依赖多轮训练和显式任务提示，难以应对通用持续学习（GCL）中无任务边界、单次数据流的挑战；且缺乏对专家参数分配与表征能力提升的针对性设计。

Method: 提出FlyPrompt框架：1）受果蝇稀疏扩展与模块化集成启发，将GCL分解为专家路由与专家能力提升两个子问题；2）设计随机扩展解析路由器实现样本级专家激活；3）构建时间输出头集成以动态调整决策边界。

Result: 在CIFAR-100、ImageNet-R和CUB-200上分别比SOTA基线提升11.23%、12.43%和7.62%；理论与实验验证其有效性。

Conclusion: FlyPrompt为通用持续学习提供了一种新颖、高效且生物启发的参数高效微调范式，有效解决了专家参数分配与有限监督下表征能力提升两大核心挑战。

Abstract: General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.

</details>


### [812] [Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal](https://arxiv.org/abs/2602.01877)
*Zichun Wang,Gar Goei Loke,Ruiting Zuo*

Main category: cs.LG

TL;DR: 本文提出了一种针对自相关不确定性（VARMA过程）的优化模型A-OVE，通过充分统计量直接优化样本外性能，在组合投资问题中表现出低遗憾和鲁棒性，优于传统预测-优化方法。


<details>
  <summary>Details</summary>
Motivation: 传统估计-优化方法在有限样本下表现不佳，而直接优化样本外性能的模型具有潜力；本文聚焦于自相关不确定性（如VARMA过程）这一常见但具挑战性的场景。

Method: 提出自相关Optimize-via-Estimate（A-OVE）模型，其解为充分统计量的函数，并设计递归算法计算这些统计量。

Result: 在含交易成本的投资组合优化任务中，A-OVE相对于完美信息oracle具有较低遗憾，优于多种预测-优化基线；且模型对小幅度模型误设具有鲁棒性；同时验证了预测精度高不一定决策质量好。

Conclusion: A-OVE为处理自相关不确定性下的数据驱动优化提供了有效、鲁棒的新范式，强调应以决策质量而非预测精度为建模目标。

Abstract: Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.

</details>


### [813] [An Empirical Study of World Model Quantization](https://arxiv.org/abs/2602.02110)
*Zhongqian Fu,Tianyi Zhao,Kai Han,Hang Zhou,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: 本文系统研究了世界模型（world model）的后训练量化（PTQ）效果，以DINO-WM为例，在多种设置下评估量化对视觉规划任务的影响，发现量化敏感性在模块间不对称、低比特量化会破坏规划目标与任务成功之间的对齐，并揭示了特有的量化失效模式。


<details>
  <summary>Details</summary>
Motivation: 世界模型计算开销大、内存占用高，亟需量化部署；但目前尚缺乏对其后训练量化效果的系统研究。

Method: 以DINO-WM为代表，系统评估多种后训练量化方法（权重量化与权值-激活联合量化），在不同比特宽、分组粒度和规划步长（最高50步）下进行跨视觉规划任务的实证分析。

Result: 发现：1）分组权重量化可稳定低比特rollout；2）激活量化粒度收益不一致；3）编码器与预测器模块量化敏感性高度不对称；4）激进低比特量化严重损害规划目标与任务成功的对齐，且无法通过额外优化修复。

Conclusion: 世界模型量化存在独特失效模式，需针对性设计量化策略；本文结果为资源受限场景下部署量化世界模型提供了实用指导。

Abstract: World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.

</details>


### [814] [Internal Flow Signatures for Self-Checking and Refinement in LLMs](https://arxiv.org/abs/2602.01897)
*Sungheon Jeong,Sanggeon Yun,Ryozo Masukawa,Wenjun Haung,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: 本文提出了一种名为'内部流签名'（internal flow signatures）的新方法，用于在大语言模型（LLM）生成过程中实时审计其决策形成过程，实现无需外部验证的自检与针对性修正。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常生成看似流畅但脱离上下文的错误回答，而现有防护手段多依赖生成后的外部验证或独立判别器，缺乏对模型内部决策动态的实时、轻量级监控。

Method: 通过在固定层间监测边界上分析深度方向的token运动动力学，采用偏置中心化监控稳定逐token运动；在每个深度窗口内，基于top token及其竞争者构建紧凑的‘移动’读出对齐子空间；利用正交传输对齐相邻窗口帧，提取深度可比的传输步长、转向角和子空间漂移等不变量；最后用轻量GRU验证器学习这些签名以实现自检与定位。

Result: 该方法不仅能高精度检测不忠实生成，还能准确定位导致错误的深度事件，并支持针对性回滚与异常步长钳制，保持残差正交性；整个流程开销低、可操作性强，且无需修改基础模型。

Conclusion: 内部流签名提供了一种从模型内部决策动力学出发的新型自检与精炼范式，为提升LLM可靠性开辟了无需外部依赖、具备深度定位能力的新路径。

Abstract: Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \emph{Code is available at} \texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.

</details>


### [815] [Observation-dependent Bayesian active learning via input-warped Gaussian processes](https://arxiv.org/abs/2602.01898)
*Sanna Jarl,Maria Bånkestad,Jonathan J. S. Scragg,Jens Sjölund*

Main category: cs.LG

TL;DR: 本文提出了一种通过学习单调重参数化来扭曲输入空间的方法，以使高斯过程的后验方差能够反映观测数据，从而提升贝叶斯主动学习的样本效率，尤其在非平稳场景下效果显著。


<details>
  <summary>Details</summary>
Motivation: 高斯过程作为贝叶斯主动学习的标准代理模型，其后验方差仅依赖于超参数而几乎不受实际观测输出影响，导致探索行为对测量结果不敏感。

Method: 引入一个可学习的单调输入空间重参数化（warping），使后验方差能响应观测到的变异性；并设计一种新的自监督训练目标来优化该warps，而非传统边际似然。

Result: 在多个主动学习基准上提升了样本效率，尤其在非平稳性较强的任务中显著优于传统方法。

Conclusion: 观测依赖的输入空间扭曲能有效增强高斯过程的不确定性建模能力，所提自监督训练策略进一步提升了主动学习性能。

Abstract: Bayesian active learning relies on the precise quantification of predictive uncertainty to explore unknown function landscapes. While Gaussian process surrogates are the standard for such tasks, an underappreciated fact is that their posterior variance depends on the observed outputs only through the hyperparameters, rendering exploration largely insensitive to the actual measurements. We propose to inject observation-dependent feedback by warping the input space with a learned, monotone reparameterization. This mechanism allows the design policy to expand or compress regions of the input space in response to observed variability, thereby shaping the behavior of variance-based acquisition functions. We demonstrate that while such warps can be trained via marginal likelihood, a novel self-supervised objective yields substantially better performance. Our approach improves sample efficiency across a range of active learning benchmarks, particularly in regimes where non-stationarity challenges traditional methods.

</details>


### [816] [Segment to Focus: Guiding Latent Action Models in the Presence of Distractors](https://arxiv.org/abs/2602.02259)
*Hamza Adnan,Matthew T. Jackson,Alexey Zakharov*

Main category: cs.LG

TL;DR: 本文提出MaskLAM，通过引入预训练分割模型生成的掩码加权LAM重建损失，提升动作相关特征提取能力，显著改善在含背景噪声场景下的强化学习性能。


<details>
  <summary>Details</summary>
Motivation: Latent Action Models (LAMs)难以区分动作相关特征与动作相关噪声（如背景运动），导致学习到虚假相关性及次优隐空间。

Method: MaskLAM在不修改LAM架构前提下，利用预训练基础模型提供的视觉智能体分割掩码，对LAM的重建损失进行加权，从而抑制背景干扰、聚焦前景动作区域。

Result: 在加入动作相关背景噪声的MuJoCo连续控制任务中，MaskLAM相较基线方法奖励提升最高达4倍，隐动作质量（线性探针评估）提升3倍。

Conclusion: 引入轻量级分割掩码监督可有效缓解LAM中的特征纠缠问题，提升其在真实复杂视频数据中的泛化与鲁棒性。

Abstract: Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.

</details>


### [817] [Data- and Variance-dependent Regret Bounds for Online Tabular MDPs](https://arxiv.org/abs/2602.01903)
*Mingyi Li,Taira Tsuchiya,Kenji Yamanishi*

Main category: cs.LG

TL;DR: 本文研究了具有已知转移的在线回合制表格马尔可夫决策过程（MDP），提出了适用于对抗与随机两种环境的“两全其美”算法，实现了数据依赖和方差依赖的精细遗憾界，并通过全局优化与策略优化两种方法实现自适应性，且理论下界表明其上界近乎最优。


<details>
  <summary>Details</summary>
Motivation: 现有算法通常仅针对对抗性或随机性单一环境设计，缺乏在两种环境中均表现优异的自适应算法；同时，传统遗憾界（如O(√T)）不够精细，无法反映实际数据复杂度或环境随机性。

Method: 基于乐观跟随正则化领导者（OFTRL）框架，采用对数障碍正则化，分别构建全局优化与策略优化两类算法；引入一阶、二阶、路径长度等新数据依赖复杂度度量刻画对抗环境，以及方差相关度量刻画随机环境，并设计相应自适应机制。

Result: 全局优化算法在对抗环境下达到一阶、二阶及路径长度遗憾界，在随机环境下实现方差感知的无间隙和有间隙（polylog(T)）遗憾界；策略优化算法在相同自适应性上仅多出一个回合长度因子；并给出了对应复杂度度量下的匹配下界。

Conclusion: 所提算法首次在已知转移的在线回合制表格MDP中，统一实现对抗与随机环境下的精细自适应遗憾界，且理论分析表明其性能近乎最优。

Abstract: This work studies online episodic tabular Markov decision processes (MDPs) with known transitions and develops best-of-both-worlds algorithms that achieve refined data-dependent regret bounds in the adversarial regime and variance-dependent regret bounds in the stochastic regime. We quantify MDP complexity using a first-order quantity and several new data-dependent measures for the adversarial regime, including a second-order quantity and a path-length measure, as well as variance-based measures for the stochastic regime. To adapt to these measures, we develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization. For global optimization, our algorithms achieve first-order, second-order, and path-length regret bounds in the adversarial regime, and in the stochastic regime, they achieve a variance-aware gap-independent bound and a variance-aware gap-dependent bound that is polylogarithmic in the number of episodes. For policy optimization, our algorithms achieve the same data- and variance-dependent adaptivity, up to a factor of the episode horizon, by exploiting a new optimistic $Q$-function estimator. Finally, we establish regret lower bounds in terms of data-dependent complexity measures for the adversarial regime and a variance measure for the stochastic regime, implying that the regret upper bounds achieved by the global-optimization approach are nearly optimal.

</details>


### [818] [Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs](https://arxiv.org/abs/2602.01914)
*Wenbo Pan,Zhichao Liu,Xianlong Wang,Haining Yu,Xiaohua Jia*

Main category: cs.LG

TL;DR: 本文提出了FlashTrace，一种高效的多令牌归因方法，通过跨度聚合和递归归因机制解决现有方法在长上下文场景下的效率瓶颈和忠实性下降问题，在多个长上下文和多步推理任务上实现了130倍加速并保持更高忠实性。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型依赖长推理链，而现有令牌归因方法面临效率瓶颈（O(M*N)复杂度）和忠实性下降（中间推理令牌吸收归因权重）两大挑战。

Method: 提出FlashTrace：1）采用跨度级聚合实现单次前向计算完成多令牌目标归因；2）设计递归归因机制，将重要性沿推理链反向追溯至原始输入。

Result: 在RULER、MATH、MorehopQA等任务上，相比基线方法提速超130倍，同时保持更优的忠实性；分析表明仅一次递归跳转即可提升忠实性。

Conclusion: FlashTrace有效兼顾归因效率与忠实性，为长上下文和复杂推理场景下的可解释性提供了新范式。

Abstract: Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.

</details>


### [819] [VLM-Guided Experience Replay](https://arxiv.org/abs/2602.01915)
*Elad Sharony,Tom Jurgenson,Orr Krupnik,Dotan Di Castro,Shie Mannor*

Main category: cs.LG

TL;DR: 本文提出利用冻结的预训练视觉语言模型（VLM）自动评估并优先选择回放缓冲区中的有希望子轨迹，从而提升强化学习的样本效率与成功率。


<details>
  <summary>Details</summary>
Motivation: 现有工作已将大语言模型（LLM）和视觉语言模型（VLM）应用于强化学习（RL）多个模块，但尚未探索其在核心组件——回放缓冲区（replay buffer）中的作用。

Method: 使用冻结、无需微调的预训练VLM作为自动化评估器，对智能体经验中的子轨迹进行语义与多模态评估，并据此指导回放缓冲区的经验优先级排序。

Result: 在游戏和机器人等多类任务（涵盖离散与连续动作空间）中，该方法使平均成功率提升11–52%，样本效率提升19–45%。

Conclusion: 将VLM引入回放缓冲区优先级机制是一种有效且即插即用的方式，显著提升了RL的样本效率、成功率与可解释性。

Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

</details>


### [820] [PIMPC-GNN: Physics-Informed Multi-Phase Consensus Learning for Enhancing Imbalanced Node Classification in Graph Neural Networks](https://arxiv.org/abs/2602.01920)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: 本文提出PIMPC-GNN，一种融合热力学扩散、Kuramoto同步与谱嵌入的物理信息多相共识框架，用于图神经网络中的节点类别不平衡分类任务，显著提升少数类召回率和平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在类别不平衡场景下表现不佳，尤其对少数类预测偏差严重。

Method: 提出PIMPC-GNN框架，整合热力学扩散（传播少数类标签）、Kuramoto同步（实现少数类节点振荡一致性）和谱嵌入（结构正则化以分离类别），并采用类别自适应集成加权与结合平衡交叉熵和物理约束的损失函数进行训练。

Result: 在5个基准数据集、不平衡比为5–100的设置下，PIMPC-GNN优于16种SOTA方法，少数类召回率最高提升12.7%，平衡准确率最高提升8.3%；同时提供可解释的共识动力学分析。

Conclusion: 物理建模可有效增强GNN在不平衡学习中的鲁棒性与可解释性，多相共识机制为图表示学习提供了新范式。

Abstract: Graph neural networks (GNNs) often struggle in class-imbalanced settings, where minority classes are under-represented and predictions are biased toward majorities. We propose \textbf{PIMPC-GNN}, a physics-informed multi-phase consensus framework for imbalanced node classification. Our method integrates three complementary dynamics: (i) thermodynamic diffusion, which spreads minority labels to capture long-range dependencies, (ii) Kuramoto synchronisation, which aligns minority nodes through oscillatory consensus, and (iii) spectral embedding, which separates classes via structural regularisation. These perspectives are combined through class-adaptive ensemble weighting and trained with an imbalance-aware loss that couples balanced cross-entropy with physics-based constraints. Across five benchmark datasets and imbalance ratios from 5-100, PIMPC-GNN outperforms 16 state-of-the-art baselines, achieving notable gains in minority-class recall (up to +12.7\%) and balanced accuracy (up to +8.3\%). Beyond empirical improvements, the framework also provides interpretable insights into consensus dynamics in graph learning. The code is available at \texttt{https://github.com/afofanah/PIMPC-GNN}.

</details>


### [821] [Embedding Learning on Multiplex Networks for Link Prediction](https://arxiv.org/abs/2602.01922)
*Orell Trautmann,Olaf Wolkenhauer,Clémence Réda*

Main category: cs.LG

TL;DR: 本文综述了面向多重网络链路预测的嵌入学习方法，提出了新的分类体系、可复现公平评估框架，并针对有向多重网络提出新测试流程。


<details>
  <summary>Details</summary>
Motivation: 随着多重网络复杂性（连接数与交互类型）增加，嵌入学习在链路预测中面临更大挑战，亟需系统性综述与标准化评估方法。

Method: 构建嵌入类型与技术的细化分类法；分析并改进多重网络嵌入学习的可复现性与公平性评估；提出适用于有向多重网络的新型公平测试流程。

Result: 建立了多重网络嵌入学习的结构化分类体系；明确了公平评估的关键问题；提出了有向多重网络链路预测的新测试程序；提供了下游分析的实践指南与工具视角。

Conclusion: 该综述为提升多重网络嵌入学习性能、可扩展性及公平评估奠定了基础，推动了该领域方法论的规范化与实用化发展。

Abstract: Over the past years, embedding learning on networks has shown tremendous results in link prediction tasks for complex systems, with a wide range of real-life applications. Learning a representation for each node in a knowledge graph allows us to capture topological and semantic information, which can be processed in downstream analyses later. In the link prediction task, high-dimensional network information is encoded into low-dimensional vectors, which are then fed to a predictor to infer new connections between nodes in the network. As the network complexity (that is, the numbers of connections and types of interactions) grows, embedding learning turns out increasingly challenging. This review covers published models on embedding learning on multiplex networks for link prediction. First, we propose refined taxonomies to classify and compare models, depending on the type of embeddings and embedding techniques. Second, we review and address the problem of reproducible and fair evaluation of embedding learning on multiplex networks for the link prediction task. Finally, we tackle evaluation on directed multiplex networks by proposing a novel and fair testing procedure. This review constitutes a crucial step towards the development of more performant and tractable embedding learning approaches for multiplex networks and their fair evaluation for the link prediction task. We also suggest guidelines on the evaluation of models, and provide an informed perspective on the challenges and tools currently available to address downstream analyses applied to multiplex networks.

</details>


### [822] [Bayesian Integration of Nonlinear Incomplete Clinical Data](https://arxiv.org/abs/2602.01924)
*Lucía González-Zamorano,Nuria Balbás-Esteban,Vanessa Gómez-Verdejo,Albert Belenguer-Llorens,Carlos Sevilla-Salcedo*

Main category: cs.LG

TL;DR: BIONIC是一种贝叶斯多模态数据融合框架，专为处理临床数据中的高维性、异质性和结构化缺失而设计，兼具强预测性能与内在可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态临床数据具有高维性、异质性和结构化缺失等特点，给建模、整合与解释带来重大挑战。

Method: 提出BIONIC框架，采用联合生成-判别隐变量架构，在贝叶斯框架下整合预训练嵌入（如医学图像、文本）与结构化临床变量，并显式建模模态级、变量级缺失及标签缺失。

Result: 在三个临床/生物医学多模态数据集上，BIONIC在不完整数据场景下显著优于代表性基线方法，且具备内在可解释性，支持模态重要性分析与临床洞见提取。

Conclusion: BIONIC为不完整多模态临床数据提供了一种统一、鲁棒且可解释的概率建模范式。

Abstract: Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.

</details>


### [823] [COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation](https://arxiv.org/abs/2602.01935)
*Annabelle Sujun Tang,Christopher Priebe,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

TL;DR: 本文提出COLT框架，通过多小模型协作在蒙特卡洛树搜索（MCTS）中实现高效编译器优化，以降低大语言模型（LLM）驱动的编译搜索成本，同时保持或超越单一大模型性能。


<details>
  <summary>Details</summary>
Motivation: 模型服务成本高昂，单一大语言模型用于编译器搜索开销大，而单独使用小模型又不可靠；亟需一种低成本、高可靠性的多模型协同优化机制。

Method: 提出轻量级多LLM协作框架COLT，基于共享MCTS树实现跨模型协同推理；引入模型感知树策略（倾向选择小模型但保留探索）和课程调整机制（在持续退化时切换至最大模型）；每个迭代中，当前LLM联合决策（编译变换 + 下一查询模型）。

Result: COLT在编译器优化任务中达到与单一大模型相当甚至更优的性能，显著降低计算开销，避免了传统多智能体架构所需的外部规划器、并发LLM调用、数据库及复杂控制器。

Conclusion: 多小LLM通过共享MCTS结构可高效协作，在编译优化中实现性能与成本的更好权衡，验证了‘轻量协作优于单一重模型’的设计范式。

Abstract: Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.

</details>


### [824] [PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting](https://arxiv.org/abs/2602.01936)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: 本文提出MCPST框架，通过多阶段共识学习解决跨域、数据稀缺场景下的交通流预测问题，结合扩散、同步与谱嵌入建模动态特性，引入自适应共识机制和结构化元学习策略，理论上有界误差与泛化保证，实验上显著优于14种SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 准确的交通流预测在智能交通系统中至关重要，尤其在跨域、数据稀缺场景下，历史数据有限导致模型训练和泛化困难；城市移动网络的复杂时空依赖性和非线性动态进一步加剧了少样本跨城学习的难度。

Method: 提出MCPST——一种多阶段共识时空框架：(1) 多阶段引擎，融合扩散、同步与谱嵌入以全面刻画交通动态；(2) 自适应共识机制，动态融合各阶段预测并强制一致性；(3) 结构化元学习策略，实现仅需少量数据即可快速适配新城市；并提供表示定理与泛化界等理论保障。

Result: 在四个真实世界数据集上，MCPST在时空图学习、动态图迁移学习、提示式时空预测及跨域少样本设置中均超越14种前沿方法，提升预测精度、减少所需训练数据，并提供可解释性洞察。

Conclusion: MCPST为数据稀缺下的跨域交通预测提供了新范式，兼具强性能、理论严谨性与实用性，开源代码促进后续研究。

Abstract: Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.

</details>


### [825] [T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation](https://arxiv.org/abs/2602.01937)
*Suhan Guo,Bingxu Wang,Shaodan Zhang,Furao Shen*

Main category: cs.LG

TL;DR: 本文提出T-LLM框架，通过时间蒸馏将轻量级时序教师模型的预测行为迁移至通用大语言模型（LLM），使其具备时序预测能力；教师模型融合趋势建模与频域分析提供结构化时序监督，推理时被完全移除；实验表明T-LLM在全样本、少样本和零样本设置下均优于现有LLM预测方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据具有时间累积性，难以通过纯规模驱动的预训练获得预测能力；现有LLM预测方法多依赖表征对齐或推理时添加时序模块，未显式教会LLM进行预测。

Method: 提出T-LLM时间蒸馏框架：使用一个融合趋势建模与频率域分析的轻量级时序教师模型，在训练阶段向通用LLM传递预测行为；教师仅用于训练，推理时完全移除，LLM独立完成预测。

Result: 在基准数据集和传染病预测任务上，T-LLM在全样本、少样本和零样本设置下均持续超越现有LLM基预测方法，并支持简单高效的部署流程。

Conclusion: 显式的时间行为蒸馏是赋予通用LLM时序预测能力的有效途径，T-LLM无需推理时额外模块，兼顾性能与部署简洁性。

Abstract: Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.

</details>


### [826] [Deep Multivariate Models with Parametric Conditionals](https://arxiv.org/abs/2602.01953)
*Dmitrij Schlesinger,Boris Flach,Alexander Shekhovtsov*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件概率分布建模多元异构随机变量联合分布的新方法，通过参数化马尔可夫链核最大化数据似然，支持任意下游任务和多种半监督学习场景。


<details>
  <summary>Details</summary>
Motivation: 现有深度多变量模型通常针对特定任务设计，导致泛化能力差、难以适配其他下游任务。

Method: 用各变量组对其余变量的条件概率分布来表示联合分布；将模型学习视为训练参数化的马尔可夫链核，以最大化其平稳分布的数据似然。

Result: 所提模型具有高度灵活性，可支持任意下游任务（如补全、生成、推断等），并天然兼容多种半监督学习设定。

Conclusion: 基于条件分布与马尔可夫链视角的联合建模方式，突破了任务专用设计的局限，提升了多变量模型的通用性与实用性。

Abstract: We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.

</details>


### [827] [Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation](https://arxiv.org/abs/2602.01956)
*Seonghyeon Park,Jewon Yeom,Jaewon Sok,Jeongjae Park,Heejun Kim,Taesup Kim*

Main category: cs.LG

TL;DR: 本文提出了一种利用小型草稿模型（draft models）高效估计大语言模型（LLM）token级认知不确定性（EU）的新框架，通过Jensen-Shannon散度和KL散度分别建模方差与偏差，并引入在线随机蒸馏（OSD）和数据多样性草稿策略（DDD）提升精度与效率；在GSM8K上RMSE降低达37%， hallucination检测性能媲美高开销方法（如TokUR），且推理成本极低。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度集成（Deep Ensembles）的认知不确定性估计方法在大规模LLM上计算开销过大，难以实用；而不确定性量化对抑制幻觉、支持安全关键场景部署至关重要。

Method: 提出基于草稿模型的EU估计框架：1）理论依据为Bias-Variance分解；2）用Jensen-Shannon散度衡量多个草稿输出间的方差（EU代理），用KL散度衡量草稿混合分布与目标模型输出间的偏差（EU代理）；3）引入Online Stochastic Distillation（OSD）近似目标模型聚合；4）采用Data-Diverse Drafts（DDD）策略增强草稿多样性。

Result: 在GSM8K数据集上，所提方法相较基线将EU估计误差（RMSE）最高降低37%；在幻觉检测任务中性能媲美高开销的扰动类方法（如TokUR），但推理成本可忽略。

Conclusion: 该方法以极低计算代价实现了高精度token级认知不确定性估计，为不确定性感知的LLM实际部署提供了高效、实用的解决方案。

Abstract: Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.

</details>


### [828] [Grounding Generated Videos in Feasible Plans via World Models](https://arxiv.org/abs/2602.01960)
*Christos Ziakas,Amir Bar,Alessandra Russo*

Main category: cs.LG

TL;DR: 本文提出GVP-WM方法，利用学习到的动作条件世界模型，将零样本视频生成的规划映射为物理可行的动作序列，通过视频引导的潜在空间轨迹优化实现语义对齐与动力学可行性兼顾。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频生成模型作为零样本视觉规划器时，生成的视频计划常违反时间一致性和物理约束，导致无法执行。

Method: GVP-WM首先由初始与目标观测量生成视频计划，再通过视频引导的潜在配置（video-guided latent collocation）将其投影到动力学可行的潜在轨迹流形上；将‘接地’建模为一种目标条件下的潜在轨迹优化问题，联合优化潜在状态与动作，并保持与视频计划的语义对齐。

Result: 在导航与操作仿真任务中，GVP-WM能从零样本图像到视频生成及运动模糊视频中恢复出可行的长视野动作规划，即使原始视频明显违反物理约束。

Conclusion: GVP-WM有效桥接了视频生成规划与可执行动作之间的语义-动力学鸿沟，提升了零样本视觉规划在现实机器人任务中的实用性与鲁棒性。

Abstract: Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.

</details>


### [829] [Zero-Shot Off-Policy Learning](https://arxiv.org/abs/2602.01962)
*Arip Asadulaev,Maksim Bobrin,Salem Lahlou,Dmitry Dylov,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 本文提出了一种在零样本强化学习中解决离策略学习问题的新方法，通过建立后继测度与稳态密度比的理论联系，实现无需训练的快速任务适应。


<details>
  <summary>Details</summary>
Motivation: 离策略学习在零样本强化学习中面临分布偏移和值函数过估计等挑战，尤其当智能体需在无额外训练情况下适应新任务时更为突出。

Method: 发现后继测度与稳态密度比之间的理论联系，据此推断最优重要性采样比率，实现实时稳态分布校正与任意任务下的最优策略推理。

Result: 在SMPL Humanoid运动追踪、ExoRL连续控制及OGBench长程任务上验证了方法有效性；可无缝集成至前向-后向表征框架，并支持免训练快速适应新任务。

Conclusion: 该工作弥合了离策略学习与零样本适应之间的鸿沟，为两个方向均带来实质性提升。

Abstract: Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.

</details>


### [830] [Self-Consolidation for Self-Evolving Agents](https://arxiv.org/abs/2602.01966)
*Hongzhuo Yu,Fei Zhu,Guo-Sen Xie,Ling Shao*

Main category: cs.LG

TL;DR: 本文提出了一种面向大语言模型（LLM）代理的自演化框架，通过对比式反思提取失败经验中的错误模式，并通过自整合机制将文本经验压缩为可学习参数，从而实现长期、高效、低噪声的代理演化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理缺乏终身交互演化能力；依赖成功轨迹检索的方法忽视失败经验的价值，且持续积累文本经验导致检索开销大、噪声多、上下文超限。

Method: 提出双机制自演化框架：1）对比式反思策略，从成功与失败轨迹中提炼错误模式和可复用洞见；2）自整合机制，将非参数化文本经验蒸馏为紧凑的可学习参数，嵌入代理隐空间。

Result: 在多个长期任务上验证了该方法在演化稳定性、效率及性能提升方面的优势，显著优于仅依赖成功示例或纯检索的方法。

Conclusion: 失败经验具有重要教学价值，将其结构化反思并参数化整合是实现LLM代理可持续自我演化的有效路径。

Abstract: While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.

</details>


### [831] [IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs](https://arxiv.org/abs/2602.01975)
*Meng Li,Peisong Wang,Yuantian Shao,Qinghao Hu,Hongjian Fang,Yifan Zhang,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 本文提出IntraSlice框架，通过模块内分块PCA压缩剪枝，在不引入额外参数的情况下实现高效大语言模型压缩，并设计了基于PCA的全局剪枝比例估计器以优化压缩效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署困难，结构化剪枝虽可加速但导致性能显著下降；现有PCA剪枝方法仅适用于模块间，会引入额外参数并破坏残差连接下的激活分布。

Method: 提出IntraSlice框架：1）利用Transformer模块结构特性，设计可完全融合进模型、无需额外参数的模块内分块近似PCA方法；2）提出考虑压缩后激活分布的PCA基全局剪枝比例估计器。

Result: 在Llama2、Llama3和Phi系列模型及多个语言基准上验证，IntraSlice在相同压缩比或推理速度下，压缩性能优于近期基线方法。

Conclusion: IntraSlice有效缓解了结构化剪枝带来的性能损失，在保持模型精度的同时提升推理效率，为LLM高效部署提供了新思路。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.

</details>


### [832] [SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning](https://arxiv.org/abs/2602.01990)
*Zhen-Hao Xie,Jun-Tao Tang,Yu-Cheng Shi,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.LG

TL;DR: 本文提出SAME方法解决多模态持续指令调优（MCIT）中的专家路由漂移和专家漂移问题，通过正交子空间分解、曲率感知缩放与自适应专家激活提升稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）需在真实场景中持续扩展能力，但现有基于稀疏专家路由的持续学习方法面临路由漂移（专家选择不一致）与专家漂移（共享专家被覆盖）两大挑战。

Method: SAME方法包含三部分：1）将路由动态分解为正交子空间，仅更新任务相关方向以稳定路由；2）利用历史输入协方差进行无回放的曲率感知专家更新调节；3）引入自适应专家激活机制，在训练中冻结已选专家以减少干扰与计算开销。

Result: 在多个MCIT基准上取得SOTA性能，显著缓解路由器与专家漂移，提升持续学习稳定性与效率。

Conclusion: SAME有效解决了MCIT中因数据分布演化导致的专家选择与参数更新不稳定性问题，为多模态模型持续学习提供了鲁棒、高效且无需回放的新范式。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.

</details>


### [833] [Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations](https://arxiv.org/abs/2602.01996)
*Theologos Anthimopoulos,Milad Kokhazadeh,Vasilios Kelefouras,Benjamin Himpel,Georgios Keramidas*

Main category: cs.LG

TL;DR: 本文提出了一种面向RISC-V平台的端到端低秩分解（LRF）设计空间探索方法与专用工具，利用张量训练分解（TTD）压缩全连接层，并结合编译器优化，在保持精度的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的RISC-V设备上部署深度神经网络面临全连接层高计算与内存开销的挑战，而现有低秩分解方法设计空间庞大、权衡复杂、耗时长。

Method: 采用TensorFlow T3F库的张量训练分解（TTD），通过剔除低效分解结构和RISC-V上推理性能差的方案来剪枝设计空间，并应用编译器优化提升定制T3F层性能。

Result: TT分解后的层平均比IREE快3倍、比Pluto快8倍；显著降低推理时间并提升计算效率。

Conclusion: 该方法为RISC-V架构的边缘与嵌入式设备高效部署DNN提供了实用可行的解决方案。

Abstract: Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of fully connected (FC) layers, which dominate resource consumption. Low-rank factorization (LRF) offers an effective approach to compressing FC layers, but the vast design space of LRF solutions involves complex trade-offs among FLOPs, memory size, inference time, and accuracy, making the LRF process complex and time-consuming. This paper introduces an end-to-end LRF design space exploration methodology and a specialized design tool for optimizing FC layers on RISC-V processors. Using Tensor Train Decomposition (TTD) offered by TensorFlow T3F library, the proposed work prunes the LRF design space by excluding first, inefficient decomposition shapes and second, solutions with poor inference performance on RISC-V architectures. Compiler optimizations are then applied to enhance custom T3F layer performance, minimizing inference time and boosting computational efficiency. On average, our TT-decomposed layers run 3x faster than IREE and 8x faster than Pluto on the same compressed model. This work provides an efficient solution for deploying DNNs on edge and embedded devices powered by RISC-V architectures.

</details>


### [834] [On the Limits of Layer Pruning for Generative Reasoning in LLMs](https://arxiv.org/abs/2602.01997)
*Safal Shrestha,Anubhav Shrestha,Aadim Nepal,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: 本文研究了大语言模型层剪枝在生成式推理任务中的性能退化问题，发现多步推理任务对深度减少特别敏感，并提出了一种基于自生成响应的监督微调策略来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝技术在生成式推理任务上性能严重下降，尤其是多步推理任务，而分类任务表现较好，因此需要探究其根本原因并寻找有效缓解策略。

Method: 通过跨多个模型家族的系统性研究，分析层剪枝对不同任务的影响；提出基于自生成响应的监督微调方法，在无预训练规模数据和算力约束下进行后剪枝优化。

Result: 该方法在分类任务上保留高达90%基线性能，在生成式基准上相较先前方法提升20–30个百分点，但生成式推理能力的恢复仍受限，尤其在高剪枝比例下效果不佳。

Conclusion: 层剪枝对生成式推理任务存在固有局限，其适用性高度依赖于剪枝比例与任务类型，在资源受限的后训练场景中需谨慎评估其可行性。

Abstract: Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.

</details>


### [835] [Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs](https://arxiv.org/abs/2602.02001)
*Yoonjun Cho,Dongjae Jeon,Soeun Kim,Moongyu Jeon,Albert No*

Main category: cs.LG

TL;DR: 本文提出Structured Residual Reconstruction (SRR)框架，通过在量化前保留权重的主导奇异子空间、仅量化残差，并将剩余秩用于误差重建，从而提升后训练量化（PTQ）和量化参数高效微调（QPEFT）的精度。


<details>
  <summary>Details</summary>
Motivation: 现有Quantization Error Reconstruction (QER)方法将全部秩预算用于重建量化误差，忽视了权重本身可能存在的内在低秩结构及量化对主方向的破坏，导致次优性能。

Method: SRR框架：1）保留激活缩放后权重的前k个奇异向量张成的子空间；2）仅对该子空间外的残差进行量化；3）用剩余秩r−k重建量化误差；4）理论指导选择最优k值；5）利用该参数化形式支持并稳定QPEFT。

Result: 在多种模型和PTQ设置下显著降低困惑度；在2比特QPEFT下GLUE基准上平均提升5.9个百分点。

Conclusion: SRR通过结构化地分配秩资源，在保留关键权重结构的同时更有效地重建量化误差，兼顾PTQ精度与QPEFT稳定性，为低比特量化提供新范式。

Abstract: Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\mathbf{Q} + \mathbf{L}\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.

</details>


### [836] [Logic-Guided Vector Fields for Constrained Generative Modeling](https://arxiv.org/abs/2602.02009)
*Ali Baheri*

Main category: cs.LG

TL;DR: 本文提出Logic-Guided Vector Fields (LGVF)，一种将可微逻辑约束注入流匹配生成模型的神经符号框架，在训练和推理阶段分别通过逻辑损失与梯度引导修正，显著降低约束违反率，并展现出隐式的障碍规避行为。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型缺乏在生成时强制执行声明式约束的机制，而神经符号系统需融合符号逻辑的表达性与神经学习的灵活性。

Method: 提出LGVF框架：(1) 训练时引入基于连续流轨迹的逻辑损失，加权强调目标分布附近的约束正确性；(2) 推理时利用约束梯度动态调整采样路径。约束以不同iable逻辑松弛形式嵌入。

Result: 在三类约束生成任务（线性、非线性环形、多区域障碍）中，LGVF相较标准流匹配降低约束违反率59–82%，且在各任务中均取得最低违规率；在线性和环形设置中提升MMD指标，在多障碍设置中呈现可行性-保真度权衡；可视化显示向量场具备隐式避障能力。

Conclusion: LGVF有效实现了符号知识对生成过程的实时引导，兼顾约束满足与分布建模，在无需显式路径规划下实现涌现式约束遵守行为，为神经符号生成建模提供了新范式。

Abstract: Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.

</details>


### [837] [SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation](https://arxiv.org/abs/2602.02013)
*Xiaoyi Jiang,Andreas Nienkötter*

Main category: cs.LG

TL;DR: 本文提出SNAP（Self-coNsistent Agreement Principle）自监督框架，通过互一致性原则实现鲁棒计算，无需监督或先验知识即可自动加权并抑制异常值。


<details>
  <summary>Details</summary>
Motivation: 解决无监督、无先验知识条件下鲁棒计算中异常值干扰的问题，提升高维场景下计算的可靠性与稳定性。

Method: 基于Agreement-Reliability假设设计SNAP加权机制，提出指数级抑制异常值权重的关键性质，并在向量平均与子空间估计任务中验证其有效性。

Result: SNAP在非迭代设置下优于迭代的Weiszfeld算法及两种多元均值中位数变体，展现出更强的鲁棒性与实用性。

Conclusion: SNAP是一种灵活、易用、广泛适用的鲁棒计算新范式。

Abstract: We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.

</details>


### [838] [Robust Domain Generalization under Divergent Marginal and Conditional Distributions](https://arxiv.org/abs/2602.02015)
*Jewon Yeom,Kyubyung Chae,Hyunggyu Lim,Yoonna Oh,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: 本文提出了一种面向边际分布 $P(Y)$ 和条件分布 $P(X|Y)$ 同时变化的鲁棒域泛化统一框架，通过分解联合分布并推导新风险界，结合元学习优化，在常规DG基准和多域长尾识别任务上均达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有域泛化方法多假设仅存在条件分布偏移（$P(X|Y)$ 变化），而实际多域场景常同时存在边际分布 $P(Y)$ 和条件分布 $P(X|Y)$ 的复合偏移，亟需能应对二者协同变化的鲁棒方法。

Method: 提出一种统一框架，显式分解联合分布，推导兼顾边际与条件分布偏移的新风险界；设计基于元学习的优化流程，在已见域上最小化并验证该风险界。

Result: 在标准域泛化基准及更具挑战性的多域长尾识别任务（边际与条件偏移均显著）上均取得当前最优性能。

Conclusion: 同时建模并缓解边际与条件分布偏移是提升域泛化鲁棒性的关键，所提风险界与元学习框架为处理复合分布偏移提供了有效且可扩展的解决方案。

Abstract: Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.

</details>


### [839] [DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers](https://arxiv.org/abs/2602.02016)
*Ionut-Vlad Modoranu,Philip Zmushko,Erik Schultheis,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: 本文提出DASH（分布式加速Shampoo），通过堆叠预处理器块为3D张量提升GPU利用率，以及引入Newton-DB迭代和切比雪夫多项式近似来加速Shampoo中逆矩阵根的计算，显著提升了Shampoo优化器的速度与收敛性能。


<details>
  <summary>Details</summary>
Motivation: Shampoo作为领先的近似二阶优化器虽具优势（如降低激活异常值、便于压缩），但其内部运算开销大，导致显著计算 slowdown，亟需加速。

Method: 提出DASH：1）将预处理器块堆叠为3D张量以提升GPU利用率；2）引入Newton-DB迭代与切比雪夫多项式近似，高效计算Shampoo所需的逆矩阵根；3）首次深入分析矩阵缩放对Shampoo收敛的关键影响。

Result: GPU感知实现相比优化后的分布式Shampoo提速最高达4.83×；Newton-DB在所有测试方法中实现最低验证困惑度/迭代。

Conclusion: DASH在保持Shampoo优势的同时大幅提升了计算效率与收敛质量，为大规模二阶优化提供了实用、高效的解决方案。

Abstract: Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \method (for \textbf{D}istributed \textbf{A}ccelerated \textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.

</details>


### [840] [On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems](https://arxiv.org/abs/2602.02045)
*Yiming Yang,Xiaoyuan Cheng,Yi He,Kaiyu Li,Wenxuan Yuan,Zhuo Sun*

Main category: cs.LG

TL;DR: 本文研究了扩散模型在贝叶斯反问题（BIPs）中作为先验的应用，分析了似然函数设定对恢复质量的影响，揭示了现有扩散求解器的不稳定性与非鲁棒性，并提出了一种鲁棒的扩散后验采样方法，在似然误设下仍保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽被广泛用于贝叶斯反问题，但其依赖的观测似然与重建质量之间的理论联系尚不明确，且现有方法在似然误设下的鲁棒性未被探索。

Method: 通过刻画后验近似误差并证明扩散求解器的稳定性，揭示其对似然误设的敏感性；进而提出一种可证明鲁棒、兼容现有梯度后验采样器的‘鲁棒扩散后验采样’方法。

Result: 理论证明了扩散求解器的稳定性缺陷及所提方法的鲁棒性；实验表明该方法在科学反问题和自然图像任务中，面对挑战性的似然误设时仍具一致的性能提升。

Conclusion: 扩散模型在BIPs中的应用需谨慎考虑似然建模；所提出的鲁棒扩散后验采样方法为提升实际部署中的可靠性提供了理论保障与实用方案。

Abstract: Diffusion models have recently emerged as powerful learned priors for Bayesian inverse problems (BIPs). Diffusion-based solvers rely on a presumed likelihood for the observations in BIPs to guide the generation process. However, the link between likelihood and recovery quality for BIPs is unclear in previous works. We bridge this gap by characterizing the posterior approximation error and proving the \emph{stability} of the diffusion-based solvers. Meanwhile, an immediate result of our findings on stability demonstrates the lack of robustness in diffusion-based solvers, which remains unexplored. This can degrade performance when the presumed likelihood mismatches the unknown true data generation processes. To address this issue, we propose a simple yet effective solution, \emph{robust diffusion posterior sampling}, which is provably \emph{robust} and compatible with existing gradient-based posterior samplers. Empirical results on scientific inverse problems and natural image tasks validate the effectiveness and robustness of our method, showing consistent performance improvements under challenging likelihood misspecifications.

</details>


### [841] [FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification](https://arxiv.org/abs/2602.02055)
*Nan Qiao,Sheng Yue*

Main category: cs.LG

TL;DR: 本文提出FORLER框架，结合服务器端Q-集成聚合与设备端actor校正，解决离线联邦强化学习中因数据质量低、异构性高导致的策略污染和局部最优问题，并提供安全策略改进的理论保证。


<details>
  <summary>Details</summary>
Motivation: 在线联邦强化学习存在现实环境交互风险与成本高问题，转向离线FRL后又面临低质量、异构数据导致策略污染和训练崩溃的挑战。

Method: 提出FORLER：服务器端采用Q-ensemble聚合抑制策略污染并卸载计算；设备端引入actor rectification，结合零阶搜索高Q值动作与定制正则项，并采用δ-周期策略降低本地计算开销。

Result: 理论证明了安全策略改进性能保证；实验表明FORLER在不同数据质量与异构性下持续优于强基线。

Conclusion: FORLER有效缓解离线联邦强化学习中的策略污染与局部最优问题，在保障隐私与资源约束前提下提升鲁棒性与性能。

Abstract: In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.

</details>


### [842] [FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance](https://arxiv.org/abs/2602.02060)
*Hyunsuk Chung,Caren Han,Yerin Choi,Seungyeon Ji,Jinwoo Kim,Eun-Jung Holden,Kyungreem Han*

Main category: cs.LG

TL;DR: 本文提出FiLoRA框架，通过指令条件门控机制，在不改变任务语义的前提下，显式调控多模态基础模型对内部特定特征组的依赖，提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究难以在不改变任务语义的情况下，主动调控模型对特定内部特征组（如核心或虚假特征）的依赖；后验分析和特征移除方法缺乏因果性和可控性。

Method: 提出FiLoRA（Focus-and-Ignore LoRA），一种指令驱动、参数高效的适配框架：将LoRA模块按特征组对齐，并引入自然语言指令控制的门控机制，实现计算层面的特征依赖调控。

Result: 在文本-图像和音频-视觉基准上验证了FiLoRA能引发一致且因果性的内部计算变化，可选择性增强或抑制核心/虚假特征组；在虚假特征干预下显著提升模型鲁棒性。

Conclusion: FiLoRA为多模态模型提供了可解释、可控的特征依赖调节机制，超越了相关性驱动的学习范式，推动模型向因果鲁棒方向发展。

Abstract: Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.

</details>


### [843] [Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits](https://arxiv.org/abs/2602.02061)
*Seoungbin Bae,Junyoung Son,Dabeen Lee*

Main category: cs.LG

TL;DR: 本文提出了一种结合隐式反馈（用户重试行为）的上下文队列化多臂老虎机框架（CQB-MNL），并设计了Anytime CQB（ACQB）算法，用于LLM服务中的联合查询路由与调度，在保证队列稳定的同时实现低累积遗憾和队列长度遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有在线算法忽视了LLM服务中用户因未满足而重试导致队列加剧、以及显式反馈（如评分）损害用户体验两大关键问题。

Method: 提出CQB-MNL框架建模用户重试行为与上下文偏好学习；设计ACQB算法，融合Thompson采样与衰减率强制探索；实验中采用对比学习优化查询嵌入，并用分离参数模型学习各LLM专属参数。

Result: ACQB在理论层面实现O~(√t)路由累积遗憾与O~(t^(-1/4))队列长度遗憾；在SPROUT、EmbedLLM和RouterBench数据集上显著优于基线方法。

Conclusion: 利用隐式反馈进行联合路由与调度是提升LLM服务效率与用户体验的有效途径，CQB-MNL框架及ACQB算法为该方向提供了坚实的理论保障与实证支持。

Abstract: Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\widetilde{\mathcal{O}}(\sqrt{t})$ for routing and a queue length regret of $\widetilde{\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.

</details>


### [844] [BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling](https://arxiv.org/abs/2602.02071)
*Zisheng Ye,Xiaoyu He,Maoyuan Song,Guoliang Qiu,Chao Liao,Chen Wu,Yonggang Sun,Zhichun Li,Xiaoru Xie,Yuanyong Luo,Hu Liu,Pinyan Lu,Heng Liao*

Main category: cs.LG

TL;DR: 本文提出了一种基于HiF8低精度浮点格式和分块精度重缩放的softmax新工作流，以解决Transformer推理中softmax成为瓶颈的问题，显著降低数据带宽需求和指数运算单元面积开销，同时保持模型精度，从而提升端到端推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着量化矩阵乘法加速性能趋于饱和，softmax操作成为Transformer推理的关键瓶颈，主要受限于矩阵与向量计算核间的数据带宽不足以及高精度指数运算单元（EXP2）的面积开销大。

Method: 提出一种新型低精度softmax工作流，采用特定8位浮点格式HiF8和块感知精度重缩放；通过算法创新实现低精度softmax可行，避免直接低精度导致的精度大幅下降；使矩阵乘输出限定为8位以减半数据移动带宽，并在8位精度下执行指数运算以大幅缩减EXP2单元面积。

Result: 在语言模型和多模态模型上的广泛评估验证了该方法的有效性；缓解了向量计算瓶颈，可将端到端推理吞吐量提升一倍而不增加芯片面积。

Conclusion: 该工作为未来低精度软硬件协同设计提供了具体路径，推动Transformer高效推理发展。

Abstract: As the performance gains from accelerating quantized matrix multiplication plateau, the softmax operation becomes the critical bottleneck in Transformer inference. This bottleneck stems from two hardware limitations: (1) limited data bandwidth between matrix and vector compute cores, and (2) the significant area cost of high-precision (FP32/16) exponentiation units (EXP2). To address these issues, we introduce a novel low-precision workflow that employs a specific 8-bit floating-point format (HiF8) and block-aware precision rescaling for softmax. Crucially, our algorithmic innovations make low-precision softmax feasible without the significant model accuracy loss that hampers direct low-precision approaches. Specifically, our design (i) halves the required data movement bandwidth by enabling matrix multiplication outputs constrained to 8-bit, and (ii) substantially reduces the EXP2 unit area by computing exponentiations in low (8-bit) precision. Extensive evaluation on language models and multi-modal models confirms the validity of our method. By alleviating the vector computation bottleneck, our work paves the way for doubling end-to-end inference throughput without increasing chip area, and offers a concrete co-design path for future low-precision hardware and software.

</details>


### [845] [Calibrating Adaptive Smoothing Methods for Freeway Traffic Reconstruction](https://arxiv.org/abs/2602.02072)
*Junyi Ji,Derek Gloudemans,Gergely Zachár,Matthew Nice,William Barbour,Daniel B. Work*

Main category: cs.LG

TL;DR: 本文提出了一种基于PyTorch的自适应平滑方法（ASM）Python实现，通过真实全状态观测数据进行端到端校准，并在稀疏雷达传感器网络输入下优化参数化核；评估涵盖速度分布、时空误差分布与空间误差，验证其在多条高速公路的适用性，并讨论了交通模型校准中可复现性挑战及ASM的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决交通状态重建中ASM方法校准缺乏真实数据支持、可复现性差以及难以与现代深度学习框架集成的问题。

Method: 将ASM校准建模为参数化核优化问题，使用全状态观测测试床数据进行端到端训练，基于PyTorch实现以支持深度学习集成。

Result: 实现了可复现的ASM Python版本，在速度分布、时空误差和空间误差方面提供了基准评估结果，并成功应用于多条高速公路。

Conclusion: 该实现提升了ASM的实用性与可扩展性，为自由流道路运行任务提供了可复现基准，但也揭示了ASM在泛化能力与校准可复现性方面的固有局限。

Abstract: The adaptive smoothing method (ASM) is a widely used approach for traffic state reconstruction. This article presents a Python implementation of ASM, featuring end-to-end calibration using real-world ground truth data. The calibration is formulated as a parameterized kernel optimization problem. The model is calibrated using data from a full-state observation testbed, with input from a sparse radar sensor network. The implementation is developed in PyTorch, enabling integration with various deep learning methods. We evaluate the results in terms of speed distribution, spatio-temporal error distribution, and spatial error to provide benchmark metrics for the traffic reconstruction problem. We further demonstrate the usability of the calibrated method across multiple freeways. Finally, we discuss the challenges of reproducibility in general traffic model calibration and the limitations of ASM. This article is reproducible and can serve as a benchmark for various freeway operation tasks.

</details>


### [846] [AICD Bench: A Challenging Benchmark for AI-Generated Code Detection](https://arxiv.org/abs/2602.02079)
*Daniil Orel,Dilshod Azizov,Indraneil Paul,Yuxia Wang,Iryna Gurevych,Preslav Nakov*

Main category: cs.LG

TL;DR: 本文提出了AICD Bench，一个大规模、多任务的AI生成代码检测基准，涵盖200万样本、77个模型、11个模型家族和9种编程语言，并定义了鲁棒二分类、模型家族归因和细粒度人机分类三类现实检测任务；实验表明当前检测方法在分布偏移及混合/对抗代码场景下性能仍远未达实用水平。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成代码检测数据集和基准过于狭窄，仅支持分布内二分类，缺乏对真实复杂场景（如分布偏移、模型溯源、混合代码）的评估能力，亟需更全面、更具挑战性的基准。

Method: 构建了名为AICD Bench的大规模基准，包含2M样本、77个LLM（覆盖11家族、9语言），并设计三类新任务：鲁棒二分类、模型家族归因、细粒度人机分类（含人类、机器、混合、对抗代码）；在多种神经与经典检测器上进行系统评测。

Result: 现有检测器在分布偏移、混合代码及对抗代码等场景下性能显著下降，整体远未达到实用水平；AICD Bench被证实是一个统一且具有挑战性的评估套件。

Conclusion: AICD Bench填补了AI生成代码检测领域缺乏综合性、现实性基准的空白，为推动鲁棒、可泛化检测方法的发展提供了关键基础设施和明确技术挑战。

Abstract: Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\emph{2M examples}$, $\emph{77 models}$ across $\emph{11 families}$, and $\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\emph{i}$)~$\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\emph{ii}$)~$\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\emph{iii}$)~$\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.

</details>


### [847] [Learning Half-Spaces from Perturbed Contrastive Examples](https://arxiv.org/abs/2602.02080)
*Aryan Alavi Razavi Ravari,Farnam Mansouri,Yuxin Chen,Valentio Iverson,Adish Singla,Sandra Zilles*

Main category: cs.LG

TL;DR: 本文研究了一种两步对比示例查询模型，其中每个查询的带标签样本都配有一个相反标签的对比样本；与之前工作假设理想对比样本位于最小距离不同，本文引入了一个由非递减噪声函数f控制的扰动机制，并在阈值和半空间两种设定下分析了其对主动/被动学习样本复杂度的影响。


<details>
  <summary>Details</summary>
Motivation: 现有工作假设对比样本处于理想位置（即距离查询点最近的异类点），但现实中获取如此精确的对比样本可能不现实；因此本文旨在建模并分析当对比样本存在与决策边界距离相关的扰动时的学习性能。

Method: 提出一种参数化的扰动机制，扰动幅度由非递减函数f(d)控制（d为查询点到决策边界的距离）；分别在固定最大扰动和随机扰动两种设定下，针对一维阈值和均匀分布下的半空间分类问题，理论刻画主动与被动对比样本复杂度。

Result: 在特定条件（如f满足一定增长性质）下，即使存在扰动，对比样本仍能显著降低渐近查询复杂度和期望查询复杂度；给出了复杂度随f变化的精确刻画。

Conclusion: 对比学习在存在合理扰动时依然有效，且扰动设计（特别是靠近边界时提供更高质对比样本）可带来理论上的学习加速；该模型更贴近实际应用场景，并为对比式主动学习提供了新的理论基础。

Abstract: We study learning under a two-step contrastive example oracle, as introduced by Mansouri et. al. (2025), where each queried (or sampled) labeled example is paired with an additional contrastive example of opposite label. While Mansouri et al. assume an idealized setting, where the contrastive example is at minimum distance of the originally queried/sampled point, we introduce and analyze a mechanism, parameterized by a non-decreasing noise function $f$, under which this ideal contrastive example is perturbed. The amount of perturbation is controlled by $f(d)$, where $d$ is the distance of the queried/sampled point to the decision boundary. Intuitively, this results in higher-quality contrastive examples for points closer to the decision boundary. We study this model in two settings: (i) when the maximum perturbation magnitude is fixed, and (ii) when it is stochastic.
  For one-dimensional thresholds and for half-spaces under the uniform distribution on a bounded domain, we characterize active and passive contrastive sample complexity in dependence on the function $f$. We show that, under certain conditions on $f$, the presence of contrastive examples speeds up learning in terms of asymptotic query complexity and asymptotic expected query complexity.

</details>


### [848] [Active learning from positive and unlabeled examples](https://arxiv.org/abs/2602.02081)
*Farnam Mansouri,Sandra Zilles,Shai Ben-David*

Main category: cs.LG

TL;DR: 本文研究了主动正例-未标记学习（PU learning）的理论分析，提出了一种在查询标签时仅当实例为正例且独立抛硬币成功时才返回标签的设定，并首次分析了该设定下的标签复杂度。


<details>
  <summary>Details</summary>
Motivation: 受广告投放和异常检测等实际应用驱动，研究主动PU学习的理论基础。

Method: 提出了一个主动PU学习的新设定：从无标签池中自适应查询实例，但仅当实例为正例且独立抛硬币成功时才获得标签；否则无反馈。在此设定下进行理论分析。

Result: 首次给出了主动PU学习中标签复杂度的理论分析结果。

Conclusion: 该工作为弱监督学习中的主动PU学习提供了首个理论框架，揭示了其标签效率的理论边界。

Abstract: Learning from positive and unlabeled data (PU learning) is a weakly supervised variant of binary classification in which the learner receives labels only for (some) positively labeled instances, while all other examples remain unlabeled. Motivated by applications such as advertising and anomaly detection, we study an active PU learning setting where the learner can adaptively query instances from an unlabeled pool, but a queried label is revealed only when the instance is positive and an independent coin flip succeeds; otherwise the learner receives no information. In this paper, we provide the first theoretical analysis of the label complexity of active PU learning.

</details>


### [849] [Efficient Swap Regret Minimization in Combinatorial Bandits](https://arxiv.org/abs/2602.02087)
*Andreas Kontogiannis,Vasilis Pollatos,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.LG

TL;DR: 本文提出了一种高效的无交换遗憾（no-swap regret）算法，用于组合型赌博机问题，其遗憾界在动作数N上为polylogarithmic，在时间步T上为次线性，并实现了每轮计算复杂度也对N呈polylogarithmic依赖。


<details>
  <summary>Details</summary>
Motivation: 在组合型赌博机中，动作空间大小N随维度指数增长，已有工作对较弱的外部遗憾已有较好解决，但实现对N呈polylogarithmic依赖的无交换遗憾仍具挑战。

Method: 提出一种新型无交换遗憾学习算法，并设计了适用于多种经典应用的高效实现方案，确保每轮计算复杂度同样对N呈polylogarithmic依赖。

Result: 算法实现了O(polylog(N) * sqrt(T))量级的swap regret，且该界对组合型赌博机是紧的；同时每轮计算复杂度也为polylog(N)。

Conclusion: 本文首次在组合型赌博机中实现了对N呈polylogarithmic依赖的无交换遗憾，解决了长期开放问题，并提供了可高效实现的算法框架。

Abstract: This paper addresses the problem of designing efficient no-swap regret algorithms for combinatorial bandits, where the number of actions $N$ is exponentially large in the dimensionality of the problem. In this setting, designing efficient no-swap regret translates to sublinear -- in horizon $T$ -- swap regret with polylogarithmic dependence on $N$. In contrast to the weaker notion of external regret minimization - a problem which is fairly well understood in the literature - achieving no-swap regret with a polylogarithmic dependence on $N$ has remained elusive in combinatorial bandits. Our paper resolves this challenge, by introducing a no-swap-regret learning algorithm with regret that scales polylogarithmically in $N$ and is tight for the class of combinatorial bandits. To ground our results, we also demonstrate how to implement the proposed algorithm efficiently -- that is, with a per-iteration complexity that also scales polylogarithmically in $N$ -- across a wide range of well-studied applications.

</details>


### [850] [Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2602.02098)
*Yannik Schnitzer,Mathias Jackermeier,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 本文提出了一种为多任务强化学习策略在未见任务上提供高性能保证的方法，通过结合任务内置信下界与任务级泛化，实现了对未知分布中新任务的高置信度性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有多任务强化学习方法缺乏形式化的性能保证，难以应用于安全关键场景。

Method: 提出一种新的泛化界，将每个任务基于有限轨迹的置信下界与从有限采样任务中获得的任务级泛化能力相结合。

Result: 在多种先进多任务RL方法上验证了该保证在理论上的正确性，并在实际样本规模下具有信息量。

Conclusion: 所提方法能为来自相同（但未知）任务分布的新任务提供高置信度性能保证，填补了多任务RL在安全关键应用中的理论空白。

Abstract: Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.

</details>


### [851] [The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning](https://arxiv.org/abs/2602.02117)
*Youqi Wu,Farzan Farnia*

Main category: cs.LG

TL;DR: 本文将冯·诺依曼熵（VNE）的最大熵原理扩展到机器学习中的核矩阵与核协方差算子场景，提出基于博弈论的最小最大（minimax）框架，为VNE最大化提供理论依据，并应用于核表示选择与核矩阵补全。


<details>
  <summary>Details</summary>
Motivation: 现有VNE在机器学习中作为谱多样性度量被使用，但缺乏类似经典最大熵框架的决策与博弈论解释；亟需为数据驱动场景下的VNE最大化建立原则性理论基础。

Method: 将Grünwald与Dawid提出的最大熵最小最大公式推广至冯·诺依曼熵，定义密度矩阵和迹归一化半正定算子上的VNE minimax优化；将其应用于核嵌入选择（基于核VNE最大化）与部分观测下核矩阵补全。

Result: 建立了VNE最大化的博弈论解释，阐明其在信息不完全下的鲁棒性与‘最少承诺推断’性质；在两个典型核学习任务中验证了该框架的有效性与统一性。

Conclusion: 本文为VNE在核学习中提供了统一的信息论基础，将量子信息概念成功桥接至机器学习建模，并赋予VNE最大化明确的决策语义与鲁棒统计解释。

Abstract: Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Grünwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.

</details>


### [852] [Two-Stage Grid Optimization for Group-wise Quantization of LLMs](https://arxiv.org/abs/2602.02126)
*Junhan Kim,Gukryeol Lee,Seungwoo Son,Jeewook Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: 本文提出了一种两阶段优化框架来改进大语言模型的分组量化，通过显式最小化层重建损失提升低比特量化精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法如GPTQ在确定分组缩放因子时忽略输入统计信息和组间相关性，导致与最小化层重建损失的目标不一致。

Method: 第一阶段在GPTQ前初始化各组缩放因子以最小化组内重建损失；第二阶段固定GPTQ得到的整数权重，利用坐标下降法和闭式更新规则精细化缩放因子，并考虑前序层的量化误差以防止误差累积。

Result: 实验表明该方法在保持极低开销的同时，显著提升了分组量化精度。

Conclusion: 所提两阶段优化框架能有效缓解低比特量化中的精度下降问题，是一种高效且实用的量化改进方案。

Abstract: Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.

</details>


### [853] [Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics](https://arxiv.org/abs/2602.02128)
*Nima Shoghi,Yuxuan Liu,Yuning Shen,Rob Brekelmans,Pan Li,Quanquan Gu*

Main category: cs.LG

TL;DR: STAR-MD是一种可扩展的SE(3)-等变扩散模型，通过因果扩散Transformer实现联合时空注意力，可在微秒尺度上生成物理合理的蛋白质轨迹，显著提升构象覆盖、结构有效性和动态保真度。


<details>
  <summary>Details</summary>
Motivation: 分子动力学（MD）模拟计算成本高，难以覆盖生物学相关的时间尺度；现有生成模型在长时程生成中受限于架构缺陷、误差累积和时空动态建模不足。

Method: 提出STAR-MD：一种基于SE(3)-等变扩散的可扩展模型，核心是具备联合时空注意力的因果扩散Transformer，以高效建模复杂时空依赖并规避内存瓶颈。

Result: 在ATLAS基准测试中全面达到SOTA；成功生成稳定的微秒级轨迹，而基线方法在此尺度下完全失效；全程保持高结构质量。

Conclusion: STAR-MD通过联合时空建模突破了长时程生成瓶颈，为加速探索蛋白质功能提供了新范式。

Abstract: Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.

</details>


### [854] [DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations](https://arxiv.org/abs/2602.02137)
*Minghao Li,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.LG

TL;DR: DCoPilot 是一种面向动态数据中心运行的生成式控制策略框架，结合大语言模型（LLM）生成结构化奖励函数与超网络生成策略权重，实现分钟级、零样本自适应控制，显著降低约束违反率并提升能效与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代AI数据中心功率密度高、负载动态性强，传统人工设计的分段式深度强化学习策略难以及时响应频繁变化的运行状态和服务等级协议（SLA），导致控制滞后与服务中断风险。

Method: 提出混合生成式框架 DCoPilot：1）LLM 符号化生成结构化奖励函数；2）超网络参数化生成策略权重，以 SLA 和场景嵌入为条件；3）通过仿真扩标、元策略蒸馏和在线零样本适配三阶段协同实现快速策略生成。

Result: 在五个典型DC控制任务族上验证，DCoPilot 实现近零约束违反，全面优于所有基线方法；消融实验表明LLM驱动的统一奖励生成对超网络稳定收敛至关重要。

Conclusion: DCoPilot 有效弥合了规范变更与策略部署之间的延迟鸿沟，为高动态数据中心提供了可扩展、可解释、自适应的智能控制新范式。

Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.

</details>


### [855] [Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting](https://arxiv.org/abs/2602.02146)
*Sunho Kim,Susik Yoon*

Main category: cs.LG

TL;DR: 本文提出Back to the Future（BTTF）框架，通过前向增强与自校正优化提升长期时间序列预测的稳定性与准确性，无需复杂模型架构即可显著改善线性模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决长期时间序列预测中并行效率与时间一致性建模之间的权衡问题：DMS方法快但缺乏时序一致性，IMS方法保持依赖但易误差累积且推理慢。

Method: 提出BTTF框架，基于基础模型的初始预测进行前向增强，并在第二阶段通过集成增强后的模型实现自校正优化，不依赖复杂架构。

Result: 在长时序预测任务上精度提升最高达58%，显著缓解线性模型的不稳定性，即使第一阶段模型训练条件不佳仍保持稳定改进。

Conclusion: 利用模型自身预测作为数据增强是一种简单而有效提升长期预测性能的方法，无需引入复杂模型结构。

Abstract: Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across steps, while iterative multi-step forecasting (IMS) preserves temporal dependencies at the cost of error accumulation and slow inference. To bridge this gap, we propose Back to the Future (BTTF), a simple yet effective framework that enhances forecasting stability through look-ahead augmentation and self-corrective refinement. Rather than relying on complex model architectures, BTTF revisits the fundamental forecasting process and refines a base model by ensembling the second-stage models augmented with their initial predictions. Despite its simplicity, our approach consistently improves long-horizon accuracy and mitigates the instability of linear forecasting models, achieving accuracy gains of up to 58% and demonstrating stable improvements even when the first-stage model is trained under suboptimal conditions. These results suggest that leveraging model-generated forecasts as augmentation can be a simple yet powerful way to enhance long-term prediction, even without complex architectures.

</details>


### [856] [ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning](https://arxiv.org/abs/2602.02150)
*Chu Zhao,Enneng Yang,Yuting Liu,Jianzhe Zhao,Guibing Guo*

Main category: cs.LG

TL;DR: 本文提出ECHO方法，通过结合局部熵和组级置信度自适应控制分支宽度，并引入基于置信度的在线剪枝与优势函数塑形，以解决测试时强化学习中 rollout崩溃和早期伪标签偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有树状rollout方法存在高熵分支导致的rollout崩溃，以及早期噪声伪标签引发的自我强化过拟合问题，影响探索效率与策略稳定性。

Method: ECHO在rollout阶段联合使用局部熵和组级置信度动态调控分支宽度，并进行置信度驱动的在线剪枝；在策略更新阶段采用置信度自适应裁剪与熵-置信度混合优势塑形。

Result: ECHO在多个数学与视觉推理基准上取得一致提升，在有限rollout预算下泛化能力更强。

Conclusion: ECHO有效缓解了rollout崩溃与早期偏差问题，提升了测试时强化学习的鲁棒性、效率与泛化性能。

Abstract: Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.

</details>


### [857] [Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing](https://arxiv.org/abs/2602.02157)
*Egor Serov,Ilya Kuleshov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 本文提出了一种基于核与高斯过程平滑的神经控制微分方程路径构建方法，并结合多视图CDE（MV-CDE）及其卷积扩展（MVC-CDE），在保持甚至提升精度的同时显著降低函数评估次数（NFE）和推理时间。


<details>
  <summary>Details</summary>
Motivation: 标准样条插值导致驱动路径过于粗糙，迫使自适应求解器采用过小步长，增加函数评估次数（NFE），影响Neural CDE效率。

Method: 用核函数与高斯过程（GP）平滑替代精确插值以控制路径正则性；引入注意力机制驱动的多视图CDE（MV-CDE）及其卷积变体（MVC-CDE），通过可学习查询恢复平滑损失的细节，并在多个轨迹间分配表征能力。

Result: MVC-CDE（结合GP）在多个任务上达到SOTA精度，同时显著降低NFE和总推理时间。

Conclusion: 路径平滑与多视图建模协同提升了Neural CDE的效率与性能，为连续时间序列建模提供了更优的权衡方案。

Abstract: Neural Controlled Differential Equations (Neural CDEs) provide a powerful continuous-time framework for sequence modeling, yet the roughness of the driving control path often restricts their efficiency. Standard splines introduce high-frequency variations that force adaptive solvers to take excessively small steps, driving up the Number of Function Evaluations (NFE). We propose a novel approach to Neural CDE path construction that replaces exact interpolation with Kernel and Gaussian Process (GP) smoothing, enabling explicit control over trajectory regularity. To recover details lost during smoothing, we propose an attention-based Multi-View CDE (MV-CDE) and its convolutional extension (MVC-CDE), which employ learnable queries to inform path reconstruction. This framework allows the model to distribute representational capacity across multiple trajectories, each capturing distinct temporal patterns. Empirical results demonstrate that our method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and total inference time compared to spline-based baselines.

</details>


### [858] [Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction](https://arxiv.org/abs/2602.02161)
*Aniq Ur Rahman,Justin P. Coon*

Main category: cs.LG

TL;DR: 本文提出了一种用于时序链路预测（TLP）模型的反事实验证框架，通过构建具有已知因果结构的因果时序交互图（CTIGs）来评估模型是否真正捕获了底层因果机制，而非仅拟合统计相关性。


<details>
  <summary>Details</summary>
Motivation: 现有TLP模型多以预测准确率评估，但无法衡量其是否建模了真实的因果机制；缺乏因果可解释性和可验证性的基准评估方法。

Method: 提出基于结构方程模型（SEM）的连续时间事件序列生成机制，支持激发与抑制效应，并扩展至时序交互图；定义基于跨模型预测误差的因果距离度量；设计两种反事实评估场景：可控因果偏移和时间戳随机打乱。

Result: 实证验证了因果距离足够大时，基于某一因果模型训练的预测器在其他模型上性能显著下降；成功实现了对TLP模型因果能力的量化评估。

Conclusion: 该框架为因果感知的TLP模型提供了可解释、可验证的基准评测基础，推动时序图学习从相关性建模迈向因果建模。

Abstract: Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.

</details>


### [859] [Interpretable Tabular Foundation Models via In-Context Kernel Regression](https://arxiv.org/abs/2602.02162)
*Ratmir Miftachov,Bruno Charron,Simon Valentin*

Main category: cs.LG

TL;DR: 本文提出了KernelICL框架，通过将上下文学习显式建模为核回归，在保持性能的同时提升表格基础模型的样本级可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有表格基础模型（如TabPFN、TabICL）虽性能优异，但其架构不透明，缺乏可解释性。

Method: 提出KernelICL框架，用高斯核、点积核或kNN核替代原模型最后一层，使预测变为训练样本标签的加权平均；构建二维分类法统一核方法、邻域方法与注意力机制，并用权重分布困惑度量化可检查性。

Result: 在55个TALENT基准数据集上，KernelICL性能与现有表格基础模型相当。

Conclusion: 在最终层施加显式核约束可在不牺牲性能的前提下实现可检查的预测。

Abstract: Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.

</details>


### [860] [Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents](https://arxiv.org/abs/2602.02164)
*Pengfei He,Ash Fox,Lesly Miculicich,Stefan Friedli,Daniel Fabian,Burak Gokturk,Jiliang Tang,Chen-Yu Lee,Tomas Pfister,Long T. Le*

Main category: cs.LG

TL;DR: Co-RedTeam 是一个面向网络安全的多智能体框架，通过融合安全领域知识、代码感知分析、执行驱动的迭代推理和长期记忆，显著提升了大模型在漏洞发现与利用任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在自动漏洞发现与利用方面受限于交互能力弱、执行基础差以及经验难以复用等问题。

Method: 提出 Co-RedTeam 框架，将漏洞分析分解为协同的发现与利用两个阶段，引入安全领域知识、代码感知分析、执行反馈驱动的迭代推理及长期记忆机制。

Result: 在多个安全基准测试中，漏洞利用成功率超60%，漏洞检测准确率提升超10个百分点；消融实验验证了执行反馈、结构化交互和记忆机制的关键作用。

Conclusion: Co-RedTeam 有效提升了大模型在网络安全任务中的鲁棒性与泛化能力，为构建可执行、可学习、可协作的安全智能体提供了新范式。

Abstract: Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.

</details>


### [861] [Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach](https://arxiv.org/abs/2602.02173)
*Jiancheng Tu,Wenqi Fan,Zhibin Wu*

Main category: cs.LG

TL;DR: 本文提出了一种基于混合整数规划（MIP）的框架，用于在非线性性能指标（如F1-score）下学习最优分类树，并针对类别不平衡问题进行了专门设计；通过定制化分支切割算法、实例约简和热启动策略提升可扩展性，在50个基准数据集上验证了其高效性和优越预测性能。


<details>
  <summary>Details</summary>
Motivation: 全局优化决策树是组合优化中的长期挑战，而此类模型在可解释机器学习中至关重要；现有方法难以有效优化非线性、类别不平衡敏感的指标（如F1-score）。

Method: 提出基于混合整数规划（MIP）的建模框架，并设计问题专用加速技术：定制分支定界算法、实例约简方案和热启动策略。

Result: 在50个基准数据集上的实验表明，该框架能高效优化非线性指标，在预测性能和求解时间上均优于现有方法。

Conclusion: MIP框架结合定制优化技术，为在非线性指标下学习最优分类树提供了高效、实用且可扩展的解决方案。

Abstract: Global optimization of decision trees is a long-standing challenge in combinatorial optimization, yet such models play an important role in interpretable machine learning. Although the problem has been investigated for several decades, only recent advances in discrete optimization have enabled practical algorithms for solving optimal classification tree problems on real-world datasets. Mixed-integer programming (MIP) offers a high degree of modeling flexibility, and we therefore propose a MIP-based framework for learning optimal classification trees under nonlinear performance metrics, such as the F1-score, that explicitly addresses class imbalance. To improve scalability, we develop problem-specific acceleration techniques, including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies. We evaluate the proposed approach on 50 benchmark datasets. The computational results show that the framework can efficiently optimize nonlinear metrics while achieving strong predictive performance and reduced solution times compared with existing methods.

</details>


### [862] [SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02179)
*Marina Mastroleo,Alberto Archetti,Federico Mastroleo,Matteo Matteucci*

Main category: cs.LG

TL;DR: 本文提出了SurvKAN，一种基于Kolmogorov-Arnold网络（KAN）的全参数、时间连续生存预测模型，摆脱比例风险假设，直接以时间为输入预测对数风险函数，在保持高预测性能的同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有生存模型存在明显局限：Cox模型依赖线性与比例风险等强假设，难以拟合真实临床动态；深度学习模型（如DeepSurv、DeepHit）虽表达力强但缺乏可解释性，阻碍临床信任与采纳；现有KAN混合模型（如CoxKAN）仍受限于半参数Cox框架。

Method: 提出SurvKAN——一种全参数、时间连续的KAN架构生存模型：将时间作为显式输入，KAN直接建模并预测log-hazard函数；采用完整生存似然进行端到端训练；通过可学习的单变量函数实现特征-风险关系的可解释建模。

Result: 在标准生存基准数据集上，SurvKAN在一致性指数（concordance）和校准度（calibration）等指标上达到或超越经典及SOTA基线模型；可解释性分析揭示出符合医学先验知识的临床有意义模式。

Conclusion: SurvKAN成功在预测精度与模型可解释性之间取得更好平衡，突破比例风险约束，为临床决策支持提供了兼具可靠性与透明性的新范式。

Abstract: Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.

</details>


### [863] [STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs](https://arxiv.org/abs/2602.02180)
*Weikang Meng,Liangyu Huo,Yadan Luo,Jiawen Guan,Jingyi Zhang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 本文提出STILL框架，通过自显著性评分和范数保持特征映射，在不损害预训练表示的前提下，高效线性化大语言模型，显著提升长上下文任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有线性化方法依赖滑动窗口进行位置导向的token选择，无法捕捉token全局重要性；同时线性注意力因可学习特征映射导致分布偏移，破坏预训练特征幅度。

Method: 提出STILL框架：1）引入具有局部-全局一致性的自显著性评分实现精准token选择；2）设计范数保持特征映射（NP-Map）解耦方向与幅度并重注入预训练范数；3）采用统一训推架构、分块并行与延迟选择提升硬件效率。

Result: STILL在常识与通用推理任务上匹配或超越原始预训练模型，并在长上下文基准上相较先前线性注意力方法取得最高86.2%的相对提升。

Conclusion: STILL是一种有效的LLM线性化框架，兼顾精度、效率与表示保真度，为长上下文建模提供了新思路。

Abstract: Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fails to capture token-specific global importance. Meanwhile, linear attention further suffers from distribution shift caused by learnable feature maps that distort pretrained feature magnitudes. Motivated by these limitations, we propose STILL, an intra-layer hybrid linearization framework for efficiently linearizing LLMs. STILL introduces a Self-Saliency Score with strong local-global consistency, enabling accurate token selection using sliding-window computation, and retains salient tokens for sparse softmax attention while summarizing the remaining context via linear attention. To preserve pretrained representations, we design a Norm-Preserved Feature Map (NP-Map) that decouples feature direction from magnitude and reinjects pretrained norms. We further adopt a unified training-inference architecture with chunk-wise parallelization and delayed selection to improve hardware efficiency. Experiments show that STILL matches or surpasses the original pretrained model on commonsense and general reasoning tasks, and achieves up to a 86.2% relative improvement over prior linearized attention methods on long-context benchmarks.

</details>


### [864] [ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning](https://arxiv.org/abs/2602.02192)
*Jie Xiao,Meng Chen,Qingnan Ren,Song Jingwei,Jiaqi Huang,Yangshen Deng,Chris Tong,Wanyi Chen,Suli Wang,Ziqian Bi,Shuo Lu,Yiqun Duan,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: ECHO-2 是一种面向大语言模型后训练的分布式强化学习框架，通过允许策略陈旧性可控、重叠执行 rollout 生成与模型分发，并采用对等辅助流水线广播和异构 worker 成本感知激活，显著提升广域网环境下的训练成本效率。


<details>
  <summary>Details</summary>
Motivation: 分布式 rollout 执行可利用更低成本的推理资源，但带来广域协调与策略分发延迟挑战；需在策略陈旧性、分发开销与训练效率间取得实用平衡。

Method: 提出 ECHO-2 框架：1）将策略陈旧性设为可控参数，实现 rollout 生成、模型分发与训练三者重叠；2）建立基于重叠的容量模型，指导资源配给；3）采用 peer-assisted pipelined broadcast 和 cost-aware heterogeneous worker activation 降低分发瓶颈与成本。

Result: 在 GRPO 后训练 4B/8B 模型的真实广域带宽下，ECHO-2 在保持与强基线相当 RL 奖励的同时，显著提升成本效率。

Conclusion: 可控策略陈旧性与重叠执行是分布式 LLM 强化学习训练中兼顾效率、成本与收敛质量的有效范式；ECHO-2 为广域部署提供了可落地的工程框架。

Abstract: Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.

</details>


### [865] [State Rank Dynamics in Linear Attention LLMs](https://arxiv.org/abs/2602.02195)
*Ao Sun,Hongtao Zhang,Heng Zhou,Yixuan Ma,Yiran Qin,Tongrui Su,Yan Liu,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 本文揭示了线性注意力大语言模型中状态秩分层现象，发现低秩头对推理至关重要而高秩头具有冗余性，并据此提出零样本联合秩-范数剪枝方法，显著降低KV缓存开销。


<details>
  <summary>Details</summary>
Motivation: 线性注意力大语言模型的压缩状态内部动态尚不明确，需深入理解其运行时状态动态以优化模型效率与性能。

Method: 通过谱分析与诊断探针，系统研究先进线性注意力模型的运行时状态动态，提出联合秩-范数剪枝策略。

Result: 发现状态秩分层现象：部分注意力头保持近零有效秩，另一部分秩快速增长并收敛；低秩头对推理关键，高秩头高度冗余；所提剪枝方法降低38.9% KV-cache开销且基本维持准确率。

Conclusion: 线性注意力模型中注意力头的秩特性是预训练获得的固有结构属性，而非输入依赖的瞬态行为；利用该规律可实现高效零样本模型压缩。

Abstract: Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\% reduction in KV-cache overhead while largely maintaining model accuracy.

</details>


### [866] [Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models](https://arxiv.org/abs/2602.02197)
*Xindian Ma,Yidi Lu,Peng Zhang,Jing Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种分层自适应KV缓存剔除框架HAE，通过双注意力剪枝和动态解码剔除策略，显著降低多模态大语言模型（MLLMs）的内存与计算开销，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: Transformer架构的二次方内存和计算成本是多模态大语言模型（MLLMs）的瓶颈；现有KV缓存剔除方法未能有效处理视觉与文本token间异质的注意力分布，导致效率低下或性能下降。

Method: 提出Hierarchical Adaptive Eviction（HAE）框架：在prefill阶段采用Dual-Attention Pruning（利用视觉token稀疏性与注意力方差），在decoding阶段采用Dynamic Decoding Eviction Strategy（受操作系统回收站启发）；结合索引广播减少计算开销，并从理论上保证信息完整性与更低误差界。

Result: 在Phi3.5-Vision-Instruct模型上，HAE将KV缓存内存降低41%（图像理解任务仅损失0.3%准确率），故事生成推理加速1.5倍且输出质量不变。

Conclusion: HAE是一种高效、理论可证、实用性强的KV缓存优化方法，能兼顾多模态LLMs的理解与生成任务效率与性能。

Abstract: The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\% with minimal accuracy loss (0.3\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.

</details>


### [867] [Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction](https://arxiv.org/abs/2602.02201)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: 本文提出了CardinalGraphFormer，一种结合图结构先验知识的图Transformer模型，通过结构化稀疏注意力和基数保持聚合机制，在分子属性预测任务中实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 药物发现需要在有限标注数据下高效预测分子性质，而化学空间极其庞大，因此依赖大规模无标签分子数据进行自监督预训练至关重要。

Method: 提出CardinalGraphFormer模型，融合Graphormer风格的结构偏差（如最短路径距离、中心性、直接键边偏差），采用最短路径距离≤3的结构化稀疏注意力，并引入基数保持的未归一化聚合通道；预训练结合图级对比对齐与掩码属性重建。

Result: 在11个涵盖MoleculeNet、OGB和TDC ADMET的公开基准上，CardinalGraphFormer在全部任务平均性能上优于基线，并在10个任务上取得统计显著提升。

Conclusion: CardinalGraphFormer通过融合结构先验与新型聚合机制，在数据高效分子表征学习中展现出优越性，为小样本分子属性预测提供了新思路。

Abstract: Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular corpora has become essential for data-efficient molecular representation learning. We introduce **CardinalGraphFormer**, a graph transformer that incorporates Graphormer-inspired structural biases, including shortest-path distance and centrality, as well as direct-bond edge bias, within a structured sparse attention regime limited to shortest-path distance <= 3. The model further augments this design with a cardinality-preserving unnormalized aggregation channel over the same support set. Pretraining combines contrastive graph-level alignment with masked attribute reconstruction. Under a fully matched evaluation protocol, CardinalGraphFormer improves mean performance across all 11 evaluated tasks and achieves statistically significant gains on 10 of 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET tasks when compared to strong reproduced baselines.

</details>


### [868] [Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning](https://arxiv.org/abs/2602.02206)
*Tong Yang,Yemin Wang,Chaoning Zhang,Aming Wu*

Main category: cs.LG

TL;DR: 本文提出Fat-Cat架构，通过语义文件系统、文本策略进化和闭环监视器三个组件，以Markdown文档替代JSON等语法化状态表示，提升LLM代理对上下文信息的利用效率，在多项基准测试中超越GPT-4o等基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理受限于僵化的语法化状态表示（如嵌套JSON），导致模型注意力被大量消耗在语法解析而非语义推理上，影响运行时上下文利用效率。

Method: 提出Fat-Cat文档驱动代理架构，包含：(1) 语义文件系统（用Markdown文档表示状态，贴合预训练语料）；(2) 文本策略进化模块（无参数更新地积累任务知识）；(3) 闭环监视器（监控推理轨迹以抑制幻觉）。

Result: 在推理、检索与编程基准上显著提升性能；Kimi-k2模型使用Fat-Cat后在HotPotQA上超越GPT-4o；替换为JSON状态表示则性能下降，验证了文档驱动状态建模的必要性。

Conclusion: 文档驱动的状态建模能有效提升LLM代理的信号-噪声比和实际性能，是比语法化表示更优的运行时状态管理范式。

Abstract: The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.

</details>


### [869] [Generating Physically Sound Designs from Text and a Set of Physical Constraints](https://arxiv.org/abs/2602.02213)
*Gregory Barber,Todd C. Henry,Mulugeta A. Haile*

Main category: cs.LG

TL;DR: TIDES is a text-informed design method that jointly optimizes structural topology and visual appearance using a pre-trained text-image model and a differentiable physics simulator, validated through simulation and physical 3-point bending tests.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between natural language design intent and physically sound engineering designs by jointly optimizing both visual alignment with text prompts and physical performance under constraints.

Method: TIDES combines a pre-trained text-image model to assess visual-text alignment and a differentiable physics simulator to evaluate structural performance (e.g., compliance, density), enabling joint optimization of topology and visual properties.

Result: TIDES successfully generates designs satisfying engineering requirements (compliance and density) while incorporating text-specified features; validated across varying load/support conditions, resolutions, and confirmed experimentally via 3D-printed 2D beam tests.

Conclusion: TIDES demonstrates feasibility and effectiveness of text-guided, physics-aware generative design, enabling intuitive high-level specification of functional and aesthetic design goals.

Abstract: We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.

</details>


### [870] [Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism](https://arxiv.org/abs/2602.02215)
*Sebastian Müller,Vanessa Toborek,Eike Stadtländer,Tamás Horváth,Brendan Balcerak Jackson,Christian Bauckhage*

Main category: cs.LG

TL;DR: 本文提出了一种名为'黑箱科学理论（SToBB）'的新范式，旨在为黑箱模型构建一个可持久化、可审计、可更新的解释性知识库，以支持全生命周期的可解释AI（XAI）需求。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法多为针对特定问题的孤立解释，缺乏一种能将黑箱模型所有解释信息整合为统一、持久、可审计 artefact 的原则性框架，难以支撑模型全生命周期的可追溯与外部审查。

Method: 基于建构经验主义哲学，提出SToBB概念并形式化其三大义务（经验充分性、可更新性、可审计性），进而构建通用操作框架，包括可扩展观测库、可追溯假设类、构造与修订算法组件及完整文档规范；并设计CoBoT在线算法，在线构建并维护基于规则的代理模型作为SToBB实例。

Result: 实现了首个完整的SToBB实例（面向表格数据神经网络分类器），验证了其经验充分性、动态适应性与第三方可审计性；CoBoT算法成功支持按需查询生成面向不同利益相关者的解释，而非一次性孤立输出。

Conclusion: SToBB为XAI提供了面向模型全生命周期的结构性、可检验的知识基础设施，使解释从临时性输出转变为可持续演化的科学理论，推动可解释性从技术工具升级为系统性治理机制。

Abstract: Explainable AI (XAI) offers a growing number of algorithms that aim to answer specific questions about black-box models. What is missing is a principled way to consolidate explanatory information about a fixed black-box model into a persistent, auditable artefact, that accompanies the black-box throughout its life cycle. We address this gap by introducing the notion of a scientific theory of a black (SToBB). Grounded in Constructive Empiricism, a SToBB fulfils three obligations: (i) empirical adequacy with respect to all available observations of black-box behaviour, (ii) adaptability via explicit update commitments that restore adequacy when new observations arrive, and (iii) auditability through transparent documentation of assumptions, construction choices, and update behaviour. We operationalise these obligations as a general framework that specifies an extensible observation base, a traceable hypothesis class, algorithmic components for construction and revision, and documentation sufficient for third-party assessment. Explanations for concrete stakeholder needs are then obtained by querying the maintained record through interfaces, rather than by producing isolated method outputs. As a proof of concept, we instantiate a complete SToBB for a neural-network classifier on a tabular task and introduce the Constructive Box Theoriser (CoBoT) algorithm, an online procedure that constructs and maintains an empirically adequate rule-based surrogate as observations accumulate. Together, these contributions position SToBBs as a life cycle-scale, inspectable point of reference that supports consistent, reusable analyses and systematic external scrutiny.

</details>


### [871] [Spectral Superposition: A Theory of Feature Geometry](https://arxiv.org/abs/2602.02224)
*Georgi Ivanov,Narmeen Oozeer,Shivam Raval,Tasana Pejovic,Shriyash Upadhyay,Amir Abdullah*

Main category: cs.LG

TL;DR: 本文提出了一种基于谱理论（特别是帧算子 $F = WW^\top$）分析神经网络中特征几何结构的新方法，揭示了超位置（superposition）下特征的谱局域化现象，并将特征几何统一分类为单纯形、多边形、反棱柱等结构，为可解释性研究提供了算子理论新视角。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏线性分解方法忽略了特征的几何结构；而神经网络在超位置中迫使特征共享表征空间，亟需刻画其全局几何交互。

Method: 构建帧算子 $F = WW^\top$，通过其谱（特征值、特征子空间等）量化各特征在不同特征子空间中的范数分配；在超位置的toy模型中进行理论证明与几何分类。

Result: 证明容量饱和导致谱局域化：特征坍缩至单个特征子空间、形成紧框架，并可通过关联方案离散分类；统一解释并分类了以往工作中的所有典型几何构型（如单纯形、多边形、反棱柱）。

Conclusion: 谱测度形式化方法适用于任意权矩阵，可推广至实际模型诊断；该工作标志着将算子理论系统引入神经网络可解释性的新研究方向。

Abstract: Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.

</details>


### [872] [Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts](https://arxiv.org/abs/2602.02229)
*Guangyi Zhang,Yunlong Cai,Guanding Yu,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文提出了一种名为预测驱动风险监控（PPRM）的半监督方法，用于在标注数据稀缺的动态环境中监控模型性能，通过结合合成标签与少量真实标签构建任意时刻有效的风险下界，并以无假设、有限样本保证检测有害分布偏移。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中监控模型性能面临标注数据稀缺的挑战，需要一种可靠且无需强假设的风险监控方法。

Method: 提出预测驱动风险监控（PPRM），基于预测驱动推理（PPI），利用合成标签和少量真实标签构造任意时刻有效的运行风险下界，并通过与名义风险上界阈值比较实现有害偏移检测。

Result: PPRM在图像分类、大语言模型（LLM）及电信监控任务中均展现出良好效果，具备假设无关、有限样本下的误报概率保证。

Conclusion: PPRM是一种实用、理论严谨的半监督风险监控框架，适用于标注受限的真实动态场景。

Abstract: We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.

</details>


### [873] [SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting](https://arxiv.org/abs/2602.02230)
*Ziyu Zhou,Yuchen Fang,Weilin Ruan,Shiyu Wang,James Kwok,Yuxuan Liang*

Main category: cs.LG

TL;DR: 本文提出SEDformer，一种基于脉冲神经网络的新型模型，专为处理具有稀疏-事件对偶性（SED）特性的不规则多变量时间序列（IMTS）而设计，显著提升了遥测数据预测精度并降低了能耗与内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有基于图和Transformer的预测器忽视了IMTS固有的稀疏-事件对偶性（SED）特性，导致建模失真：统一网格对齐引入冗余计算，关系重铸破坏事件语义连续性。

Method: 提出SEDformer模型，包含三部分：(1)基于SED的脉冲编码器，使用事件对齐的LIF神经元将原始观测转为同步脉冲；(2)事件保持型时间下采样模块，压缩长间隔同时保留关键脉冲；(3)基于SED的脉冲Transformer块，采用膜电位驱动的线性注意力机制建模序列内依赖。

Result: 在多个公开遥测IMTS数据集上，SEDformer达到最优预测精度，并显著降低能耗与内存使用。

Conclusion: Spiking Neural Networks天然契合IMTS的SED特性，SEDformer为不规则遥测时间序列提供了一种更忠实、高效且符合其内在结构的建模范式。

Abstract: Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.

</details>


### [874] [Geometry- and Relation-Aware Diffusion for EEG Super-Resolution](https://arxiv.org/abs/2602.02238)
*Laura Yao,Gengwei Zhang,Moajjem Chowdhury,Yunmei Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 本文提出了TopoDiff，一种结合几何与关系感知的扩散模型，用于提升脑电图（EEG）空间超分辨率性能，通过引入拓扑感知图像嵌入和动态通道关系图，增强对生理空间结构的理解，在多个EEG数据集上显著提升了生成保真度与下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有EEG空间超分辨率方法缺乏对生理空间结构的建模，限制了空间生成性能。

Method: 提出TopoDiff模型，融合基于EEG地形图的拓扑感知图像嵌入（提供全局几何上下文）与动态通道关系图（编码电极间关系并随时间演化）。

Result: 在SEED/SEED-IV、PhysioNet MI/MM、TUSZ等多个EEG数据集上，生成保真度和下游任务（如情绪识别、运动想象分类、癫痫检测）性能均取得显著提升。

Conclusion: TopoDiff通过显式建模EEG的几何与通道关系，构建了空间可解释、性能稳健的超分辨率框架，为生理信号建模提供了新思路。

Abstract: Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.

</details>


### [875] [Interpretability in Deep Time Series Models Demands Semantic Alignment](https://arxiv.org/abs/2602.02239)
*Giovanni De Felice,Riccardo D'Elia,Alberto Termine,Pietro Barbiero,Giuseppe Marra,Silvia Santini*

Main category: cs.LG

TL;DR: 本文提出深度时间序列模型的可解释性应追求语义对齐，即预测需以用户有意义的变量表达，并通过时空机制满足用户依赖的约束；作者形式化该要求，强调其在时间演化下必须保持，并给出语义对齐模型的设计蓝图与可信属性。


<details>
  <summary>Details</summary>
Motivation: 现有深度时间序列模型虽性能提升，但因黑箱特性难以部署；当前可解释方法仅解释内部计算，未关注模型推理是否与人类对现象的理解一致。

Method: 提出‘语义对齐’新定义，要求预测以用户有意义的变量表达、经由可施加用户依赖约束的时空机制生成，并强制该对齐在时间演化中保持；进而构建语义对齐模型的设计蓝图，识别支撑信任的关键性质。

Result: 形式化了语义对齐及其时间演化不变性约束，提出了支持该目标的模型设计原则与可信属性框架。

Conclusion: 深度时间序列模型的可解释性不应止于计算溯源，而应以语义对齐为核心目标，其设计需兼顾用户认知意义与动态时序一致性。

Abstract: Deep time series models continue to improve predictive performance, yet their deployment remains limited by their black-box nature. In response, existing interpretability approaches in the field keep focusing on explaining the internal model computations, without addressing whether they align or not with how a human would reason about the studied phenomenon. Instead, we state interpretability in deep time series models should pursue semantic alignment: predictions should be expressed in terms of variables that are meaningful to the end user, mediated by spatial and temporal mechanisms that admit user-dependent constraints. In this paper, we formalize this requirement and require that, once established, semantic alignment must be preserved under temporal evolution: a constraint with no analog in static settings. Provided with this definition, we outline a blueprint for semantically aligned deep time series models, identify properties that support trust, and discuss implications for model design.

</details>


### [876] [Variational Entropic Optimal Transport](https://arxiv.org/abs/2602.02241)
*Roman Dyachenko,Nikita Gushchin,Kirill Sokolov,Petr Mokrov,Evgeny Burnaev,Alexander Korotin*

Main category: cs.LG

TL;DR: 本文提出了一种名为VarEOT的新方法，通过变分重参数化精确处理EOT中难以计算的log-partition项，避免MCMC采样，实现高效可微训练，并在图像翻译等任务上展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于弱对偶EOT目标的方法因log-partition项不可解而计算效率低：或受限于高斯混合等简单分布族以获得解析归一化，或依赖需模拟训练的通用神经网络参数化。

Method: 提出变分熵最优传输（VarEOT），将log-partition项$\log \\mathbb{E}[\\exp(\\cdot)]$精确变分为关于辅助正归一化变量的可微最小化问题，从而构建可直接用随机梯度优化的目标函数。

Result: 理论方面提供了有限样本泛化界与通用函数逼近下的近似性保证；实验表明在合成数据和无配对图像翻译任务中，翻译质量优于或媲美现有方法，且在相同弱对偶目标下优于其他求解器。

Conclusion: VarEOT为连续空间中基于EOT的域迁移提供了一种更高效、可微、无需采样的新优化范式，兼具理论严谨性与实际有效性。

Abstract: Entropic optimal transport (EOT) in continuous spaces with quadratic cost is a classical tool for solving the domain translation problem. In practice, recent approaches optimize a weak dual EOT objective depending on a single potential, but doing so is computationally not efficient due to the intractable log-partition term. Existing methods typically resolve this obstacle in one of two ways: by significantly restricting the transport family to obtain closed-form normalization (via Gaussian-mixture parameterizations), or by using general neural parameterizations that require simulation-based training procedures. We propose Variational Entropic Optimal Transport (VarEOT), based on an exact variational reformulation of the log-partition $\log \mathbb{E}[\exp(\cdot)]$ as a tractable minimization over an auxiliary positive normalizer. This yields a differentiable learning objective optimized with stochastic gradients and avoids the necessity of MCMC simulations during the training. We provide theoretical guarantees, including finite-sample generalization bounds and approximation results under universal function approximation. Experiments on synthetic data and unpaired image-to-image translation demonstrate competitive or improved translation quality, while comparisons within the solvers that use the same weak dual EOT objective support the benefit of the proposed optimization principle.

</details>


### [877] [Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258)
*Gaurav Bhatt,Aditya Chinchure,Jiawei Zhou,Leonid Sigal*

Main category: cs.LG

TL;DR: 本文提出了一种对齐感知的微调框架，通过策略梯度正则化整合外部对齐信号，并引入自适应门控机制动态平衡监督梯度与对齐驱动梯度，同时学习对完全不安全输入的拒绝响应能力，在保持任务性能的同时显著降低有害输出与幻觉。


<details>
  <summary>Details</summary>
Motivation: 标准微调方法仅优化任务目标，忽视安全、防幻觉等关键对齐目标，易导致对齐性退化或无法修复预训练阶段的不 aligned 行为。

Method: 提出基于策略梯度的对齐正则化框架，设计自适应门控机制实现样本级监督梯度与对齐梯度的动态加权，并建模模型对完全不 aligned 输入的主动 abstention（拒绝响应）行为。

Result: 在通用及领域指令微调基准上，显著减少有害输出和幻觉，且不损害下游任务性能；对对抗性微调、提示攻击和不安全初始化具有鲁棒性。

Conclusion: 自适应门控的对齐优化是一种有效的对齐保持与对齐恢复模型适配方法。

Abstract: Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.

</details>


### [878] [Learning Markov Decision Processes under Fully Bandit Feedback](https://arxiv.org/abs/2602.02260)
*Zhengjia Zhuo,Anupam Gupta,Viswanath Nagarajan*

Main category: cs.LG

TL;DR: 本文研究了在完全带状反馈（fully bandit feedback）下，阶段型马尔可夫决策过程（episodic MDPs）的强化学习问题：智能体仅观测到每轮的总奖励，而无法观测任何状态-动作对。作者提出了首个高效算法，实现了	ilde{O}(√T)的遗憾界，并证明该界中关于时间步长H的指数依赖是必要的；同时针对有序MDP获得了更紧的遗憾界，并在k-item prophet inequality等优化问题上验证了算法有效性。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习假设智能体能观测每个状态-动作对及其即时奖励，但现实中常仅能获得聚合奖励（如轨迹反馈），甚至完全无法观测状态-动作信息（即“完全带状”反馈）。本文旨在填补这一高度受限反馈设定下的理论与算法空白。

Method: 设计了一种适用于完全带状反馈的高效学习算法，结合估计技巧与置信区间构造，在无状态-动作可观测性条件下实现策略更新；针对有序MDP结构进一步优化分析以获得更紧界。

Result: 首次为完全带状反馈下的阶段型MDP提供了	ilde{O}(√T)遗憾界的高效算法；证明了遗憾对地平线H存在不可避免的指数依赖；对有序MDP获得近乎紧的改进界；实验表明其在k-item prophet inequality任务中性能媲美具备完整状态-动作反馈的UCB-VI算法。

Conclusion: 完全带状反馈虽极度受限，但仍可在合理理论保证下进行有效学习；结构先验（如有序MDP）可显著缓解信息缺失带来的困难；所提算法兼具理论最优性与实践竞争力。

Abstract: A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\widetilde{O}(\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.

</details>


### [879] [Unlocking the Duality between Flow and Field Matching](https://arxiv.org/abs/2602.02261)
*Daniil Shlenskii,Alexander Varlamov,Nazar Buzun,Alexander Korotin*

Main category: cs.LG

TL;DR: 本文探讨了条件流匹配（CFM）与交互场匹配（IFM）两种生成建模范式之间的关系，证明二者在‘前向-only IFM’子类下等价，并揭示IFM具有更强表达能力；该发现为IFM提供了概率解释，也为CFM带来了新优化技术。


<details>
  <summary>Details</summary>
Motivation: CFM和IFM虽同属生成建模范式，但出发点不同（数据空间概率路径 vs 增广空间物理启发的交互场），引发二者本质是否相同的疑问。

Method: 通过构造CFM与前向-only IFM之间的双射映射，严格证明其等价性；并分析一般IFM的表达能力边界，指出其可涵盖CFM无法实现的交互场（如EFM）。

Result: 1）CFM与前向-only IFM严格等价；2）一般IFM比CFM更具表达力；3）该对偶性可双向赋能：为IFM提供概率解释，为CFM引入IFM驱动的新技术。

Conclusion: CFM与IFM并非互斥，而是在特定条件下统一、在一般情形下IFM更普适；二者对偶性为理论理解与算法设计提供了新视角。

Abstract: Conditional Flow Matching (CFM) unifies conventional generative paradigms such as diffusion models and flow matching. Interaction Field Matching (IFM) is a newer framework that generalizes Electrostatic Field Matching (EFM) rooted in Poisson Flow Generative Models (PFGM). While both frameworks define generative dynamics, they start from different objects: CFM specifies a conditional probability path in data space, whereas IFM specifies a physics-inspired interaction field in an augmented data space. This raises a basic question: are CFM and IFM genuinely different, or are they two descriptions of the same underlying dynamics? We show that they coincide for a natural subclass of IFM that we call forward-only IFM. Specifically, we construct a bijection between CFM and forward-only IFM. We further show that general IFM is strictly more expressive: it includes EFM and other interaction fields that cannot be realized within the standard CFM formulation. Finally, we highlight how this duality can benefit both frameworks: it provides a probabilistic interpretation of forward-only IFM and yields novel, IFM-driven techniques for CFM.

</details>


### [880] [Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training](https://arxiv.org/abs/2602.02264)
*Paolo Marcandelli,Natansh Mathur,Stefano Markidis,Martina Siena,Stefano Mariani*

Main category: cs.LG

TL;DR: 本文提出了一种多阶段物理信息训练策略和PhIS-FNO模型，通过分阶段施加边界条件与内部残差、结合样条核的傅里叶神经算子，在仅使用窄边界标注数据下达到接近监督学习的精度。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子依赖大量监督数据，而物理信息神经网络虽可无监督训练，但收敛不稳定、泛化能力弱。

Method: 提出多阶段物理信息训练策略：分步引入边界条件与内部PDE残差，并在每阶段重初始化优化器；设计PhIS-FNO模型，融合傅里叶层与Hermite样条核以实现平滑残差计算。

Result: 在经典偏微分方程基准测试中，PhIS-FNO仅用窄边界区域的标注数据即达到与监督学习相当的精度。

Conclusion: 分阶段、基于样条的优化范式为物理信息算子学习提供了稳健有效的新路径。

Abstract: Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.

</details>


### [881] [HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control](https://arxiv.org/abs/2602.02268)
*Sanggeon Yun,Raheeb Hassan,Ryozo Masukawa,Sungheon Jeong,Mohsen Imani*

Main category: cs.LG

TL;DR: HopFormer是一种无需位置编码和结构编码的图Transformer，通过头特定的n跳掩码稀疏注意力注入图结构信息，实现了线性计算复杂度，并在多种图结构上展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 挑战现有图Transformer依赖显式位置/结构编码和密集全局注意力的假设，探索更高效、可解释的结构建模方式。

Method: 提出HopFormer，采用头特定的n-hop masked sparse attention机制，在不使用位置编码或架构修改的前提下注入图结构；注意力稀疏性由跳数控制，计算成本随掩码稀疏性线性增长。

Result: 在节点级和图级基准测试中达到竞争性或更优性能；发现强小世界特性的图更适合局部注意力，而弱小世界特性下全局注意力增益有限。

Conclusion: 密集全局注意力并非必需，结构化稀疏注意力是图Transformer设计中一种原则性强、效率高的替代方案。

Abstract: Graph Transformers typically rely on explicit positional or structural encodings and dense global attention to incorporate graph topology. In this work, we show that neither is essential. We introduce HopFormer, a graph Transformer that injects structure exclusively through head-specific n-hop masked sparse attention, without the use of positional encodings or architectural modifications. This design provides explicit and interpretable control over receptive fields while enabling genuinely sparse attention whose computational cost scales linearly with mask sparsity. Through extensive experiments on both node-level and graph-level benchmarks, we demonstrate that our approach achieves competitive or superior performance across diverse graph structures. Our results further reveal that dense global attention is often unnecessary: on graphs with strong small-world properties, localized attention yields more stable and consistently high performance, while on graphs with weaker small-world effects, global attention offers diminishing returns. Together, these findings challenge prevailing assumptions in graph Transformer design and highlight sparsity-controlled attention as a principled and efficient alternative.

</details>


### [882] [Backpropagation as Physical Relaxation: Exact Gradients in Finite Time](https://arxiv.org/abs/2602.02281)
*Antonino Emanuele Scurria*

Main category: cs.LG

TL;DR: 本文提出了一种名为'Dyadic Backpropagation'的新框架，将反向传播解释为一个物理动力系统的有限时间弛豫过程，并通过拉格朗日非保守系统理论导出全局能量泛函，在双状态空间中同时实现前向推理与信用分配；其欧拉离散化在2L步内精确复现标准反向传播，无需近似或对称权重假设。


<details>
  <summary>Details</summary>
Motivation: 为理解反向传播的物理本质，并为其在模拟及神经形态硬件中的精确实现提供严格基础，克服现有能量模型依赖对称权重、渐近收敛或微小扰动等限制。

Method: 将前向推理建模为连续时间过程，运用非保守系统的拉格朗日理论处理非对称相互作用，构建编码激活值与敏感度的双状态空间上的全局能量泛函，并分析其鞍点动力学；采用单位步长欧拉离散化。

Result: 证明该动力系统在2L步（L为网络层数）内精确复现标准反向传播，无任何近似；首次实现有限时间内、非对称权重下、无需渐近假设的精确梯度计算。

Conclusion: 反向传播本质上是某一连续物理弛豫过程在数字离散时间下的精确‘影子’，这为在原生支持连续动力学的模拟/神经形态硬件中实现精确梯度学习提供了理论保障与新范式。

Abstract: Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.

</details>


### [883] [MoLF: Mixture-of-Latent-Flow for Pan-Cancer Spatial Gene Expression Prediction from Histology](https://arxiv.org/abs/2602.02282)
*Susu Hu,Stefanie Speidel*

Main category: cs.LG

TL;DR: 本文提出了一种名为MoLF的生成模型，用于跨癌种空间转录组学预测，通过条件流匹配和混合专家架构，实现了对多组织数据的高效建模与零样本跨物种泛化。


<details>
  <summary>Details</summary>
Motivation: 现有空间转录组学推断方法局限于单组织模型，无法利用跨癌种共享的生物学规律，且难以应对数据稀缺场景；而泛癌训练又因组织异质性给单一架构带来挑战。

Method: 提出MoLF（Mixture-of-Latent-Flow）模型：采用条件流匹配（Flow Matching）目标将噪声映射到基因潜在流形，并以混合专家（MoE）速度场参数化，通过动态路由机制将输入分配至专用子网络，解耦不同组织模式的优化过程。

Result: MoLF在泛癌基准测试中显著优于专用模型和基础模型；并展现出零样本跨物种泛化能力，表明其捕获了保守的组织-分子机制。

Conclusion: MoLF通过引入可扩展、可解耦的生成建模范式，有效解决了泛癌空间转录组学推断中的异质性与数据稀缺难题，为跨组织、跨物种的组学建模提供了新范式。

Abstract: Inferring spatial transcriptomics (ST) from histology enables scalable histogenomic profiling, yet current methods are largely restricted to single-tissue models. This fragmentation fails to leverage biological principles shared across cancer types and hinders application to data-scarce scenarios. While pan-cancer training offers a solution, the resulting heterogeneity challenges monolithic architectures. To bridge this gap, we introduce MoLF (Mixture-of-Latent-Flow), a generative model for pan-cancer histogenomic prediction. MoLF leverages a conditional Flow Matching objective to map noise to the gene latent manifold, parameterized by a Mixture-of-Experts (MoE) velocity field. By dynamically routing inputs to specialized sub-networks, this architecture effectively decouples the optimization of diverse tissue patterns. Our experiments demonstrate that MoLF establishes a new state-of-the-art, consistently outperforming both specialized and foundation model baselines on pan-cancer benchmarks. Furthermore, MoLF exhibits zero-shot generalization to cross-species data, suggesting it captures fundamental, conserved histo-molecular mechanisms.

</details>


### [884] [Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management](https://arxiv.org/abs/2602.02283)
*Owen Shen,Patrick Jaillet*

Main category: cs.LG

TL;DR: 本文提出了一种结合离散选择模型的强化学习方法（choice-model-assisted RL），用于解决收益管理中延迟反馈问题；理论证明其在部分模型误差下收敛至近优Q函数，实验表明其在参数偏移下表现更鲁棒，但在模型误设时性能下降。


<details>
  <summary>Details</summary>
Motivation: 解决收益管理中因客户取消和修改导致的价值延迟反馈问题，传统RL难以及时获取完整奖励信号。

Method: 将校准后的离散选择模型作为固定的部分世界模型，在决策时刻对延迟奖励成分进行插补，并结合Q学习进行优化。

Result: 理论：Q学习在模型插补下收敛至O(ε/(1−γ))邻域；实验：在参数偏移下5/10场景显著提升（最高+12.4%），但模型误设时收入下降1.4–2.6%。

Conclusion: 部分行为模型可在特定分布偏移下增强鲁棒性，但若模型假设被违反，则会引入有害偏差，需权衡建模精度与泛化能力。

Abstract: We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.

</details>


### [885] [An Optimization Method for Autoregressive Time Series Forecasting](https://arxiv.org/abs/2602.02288)
*Zheng Li,Jerry Cheng,Huanying Gu*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间序列预测训练方法，通过在损失函数中显式约束自回归（AR）预测误差随预测步长单调递增，并支持短时预测拼接成长时预测，显著提升了长时预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的时间序列预测模型主要依赖扩大模型规模实现长时预测，而非真正自回归滚动预测；且传统训练忽略时间因果性。

Method: 提出一种新训练方法，强制两个性质：(1) 自回归预测误差随预测步长增加而增大，违反则视为随机猜测并惩罚；(2) 支持将多个短时AR预测结果拼接以生成灵活的长时预测。

Result: 在多个基准上达到SOTA，MSE比iTransformer等强基线降低超10%；短时模型可可靠预测长达7.5倍于其原设计步长的未来值。

Conclusion: 该方法通过引入时间因果约束和误差单调性正则，有效提升模型的自回归泛化能力与长时预测鲁棒性，为时间序列建模提供了新范式。

Abstract: Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt

</details>


### [886] [EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2602.02295)
*Shaima Ahmad Freja,Ferhat Ozgur Catak,Betul Yurdem,Chunming Rong*

Main category: cs.LG

TL;DR: 本文提出EvalQReason框架，通过分析大语言模型（LLM）推理过程中每一步的输出概率分布，无需人工标注即可量化推理质量；提出CSD（相邻步发散度）和SFC（步到终局收敛度）两种算法，并在数学与医疗数据集上验证其有效性，发现CSD更优且推理动态具有领域特异性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅关注最终答案正确性，难以系统评估LLM推理过程的中间步骤质量，缺乏对推理可靠性的过程感知评估手段。

Method: 提出EvalQReason框架，包含两个算法：Consecutive Step Divergence（CSD）衡量相邻推理步间的局部一致性，Step-to-Final Convergence（SFC）衡量各步与最终答案的全局对齐程度；每个算法使用五种统计指标分析生成概率分布；在7B开源模型上结合经典机器学习与序列神经网络进行二分类（正确/错误推理）。

Result: CSD特征在数学任务中表现优异（F1=0.88，ROC-AUC=0.97），显著优于SFC；而医疗任务中CSD几乎无判别力，揭示推理动态存在强领域差异；整体验证了基于概率发散分析可有效预测推理正确性。

Conclusion: EvalQReason实现了无需人工标注、可扩展、过程感知的LLM推理质量评估，确立了基于概率发散分析作为可信AI部署中推理可靠性评估的原理性路径。

Abstract: Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.

</details>


### [887] [Decoupling Generalizability and Membership Privacy Risks in Neural Networks](https://arxiv.org/abs/2602.02296)
*Xingli Fang,Jung-Eun Kim*

Main category: cs.LG

TL;DR: 本文提出了一种隐私保护训练原则（PPTP），通过解耦模型泛化能力与隐私风险在深度神经网络中的不同区域分布，实现隐私保护与模型性能的兼顾。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在获得隐私保护等特性时往往牺牲实用性；隐私保护与实用性之间存在权衡关系，而不同防御方法的损失差异提示了泛化能力与隐私风险可被解耦以最大化隐私收益。

Method: 基于观察到模型泛化能力与隐私风险存在于网络架构的不同区域，提出隐私保护训练原则（PPTP），针对性地保护易受隐私攻击的组件，同时最小化对泛化能力的影响。

Result: 大量实验表明，该方法在显著提升隐私保护能力的同时，更好地维持了模型的泛化性能。

Conclusion: 模型泛化与隐私风险具有空间可分性，PPTP能有效解耦二者，在不显著损害实用性前提下增强隐私保护。

Abstract: A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.

</details>


### [888] [ReasonCACHE: Teaching LLMs To Reason Without Weight Updates](https://arxiv.org/abs/2602.02366)
*Sharut Gupta,Phillip Isola,Stefanie Jegelka,David Lopez-Paz,Kartik Ahuja,Mark Ibrahim,Mohammad Pezeshki*

Main category: cs.LG

TL;DR: 本文提出ReasonCACHE，一种基于Prefix Tuning的无需权重更新、仅通过上下文学习即可实现复杂推理的新方法，它将演示样本蒸馏为固定键值缓存，在多个推理基准上超越标准ICL并媲美甚至优于IWL，同时在数据、推理成本和可训练参数三方面更高效。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型能否在不更新权重、仅依赖上下文学习（ICL）的情况下进行复杂推理；现有ICL在扩展时面临注意力计算开销大、性能饱和或下降等问题，而主流做法IWL需修改权重，缺乏轻量高效替代方案。

Method: 提出ReasonCACHE，利用Prefix Tuning机制将少量演示样本蒸馏为固定大小的键值（key-value）缓存，注入Transformer注意力层，避免扩充上下文窗口和参数更新。

Result: 在GPQA-Diamond等挑战性推理基准上，ReasonCACHE显著优于标准ICL，性能匹配或超过in-weight learning（IWL）方法；同时在数据效率、推理成本和可训练参数量三个维度更优；理论证明其表达能力严格强于低秩权重更新。

Conclusion: ReasonCACHE构建了一种介于in-context learning与in-weight learning之间的新范式，实现了无需参数更新、可扩展、高效的推理能力学习，突破了上下文窗口限制。

Abstract: Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/

</details>


### [889] [C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference](https://arxiv.org/abs/2602.02371)
*Jing Wang,Jie Shen,Qiaomin Xie,Jeremy C Weiss*

Main category: cs.LG

TL;DR: 本文提出C-kNN--LSH框架，利用局部敏感哈希（LSH）在高维、混杂的纵向数据中寻找临床相似个体（'临床双胞胎'），结合双重稳健校正，实现对动态疾病状态下的条件处理效应进行局部估计，并在长新冠队列中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 纵向轨迹因果效应估计对理解复杂疾病进展（如共病、长新冠康复）和优化临床决策至关重要，但面临高维、混杂、不规则采样及患者恢复轨迹异质性等挑战。

Method: 提出C-kNN--LSH方法：基于局部敏感哈希（LSH）高效搜索具有相似协变量历史的‘临床双胞胎’，构建邻域进行局部因果效应估计，并融合双重稳健校正以缓解不规则采样与恢复曲线漂移带来的偏差。

Result: 在含13,511名参与者的长新冠真实队列上，C-kNN--LSH在刻画康复异质性和策略价值估计方面显著优于现有基线方法。

Conclusion: C-kNN--LSH是一种一致且二阶鲁棒的序列因果推断方法，适用于高维混杂纵向医疗数据，可提升动态疾病管理中的个性化干预评估能力。

Abstract: Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.
  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.
  Evaluated on a real-world Long COVID cohort with 13,511 participants, \emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.

</details>


### [890] [Self-Supervised Learning from Structural Invariance](https://arxiv.org/abs/2602.02381)
*Yipeng Zhang,Hafez Ghaemi,Jungyoon Lee,Shahab Bakhtiari,Eilif B. Muller,Laurent Charlin*

Main category: cs.LG

TL;DR: 本文提出AdaSSL方法，通过引入隐变量建模SSL中的一对多映射问题，并推导互信息的变分下界，为对比学习和蒸馏式SSL目标添加简单正则项。


<details>
  <summary>Details</summary>
Motivation: 现有联合嵌入自监督学习方法难以灵活捕捉数据对之间固有的条件不确定性（如视频连续帧存在一对多映射），需建模这种不确定性以提升表征质量。

Method: 引入隐变量显式建模一对多映射中的条件不确定性，推导配对嵌入间互信息的变分下界，由此导出可即插即用的正则化项，适配对比学习与知识蒸馏两类SSL目标。

Result: AdaSSL在因果表征学习、细粒度图像理解及视频世界建模等任务上展现出良好泛化性与实用性，验证了其对多种SSL范式的兼容性和有效性。

Conclusion: 建模SSL中的一对多映射关系是提升表征鲁棒性与语义丰富性的关键；AdaSSL提供了一种通用、简洁且理论驱动的改进路径。

Abstract: Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.

</details>


### [891] [SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization](https://arxiv.org/abs/2602.02383)
*Maksim Afanasyev,Illarion Iov*

Main category: cs.LG

TL;DR: 本文提出SLIME方法，通过锚定偏好响应、稳定拒绝响应概率及双边界机制，在不依赖参考模型的情况下提升大语言模型对齐效果，避免了传统直接偏好优化中的‘遗忘’与‘格式崩溃’问题。


<details>
  <summary>Details</summary>
Motivation: 现有直接偏好优化方法存在目标错配问题，即优化选择与拒绝响应间的相对间隔可能导致高质输出概率下降（'遗忘'）及拒绝序列过度惩罚（'格式崩溃'）。

Method: 提出SLIME（Stabilized Likelihood Implicit Margin Enforcement），包含三部分：（1）锚定项最大化偏好响应似然；（2）稳定惩罚项防止拒绝token概率坍缩至零；（3）结合硬/软约束的双边界机制以精确塑造决策边界。

Result: SLIME在多项指标上优于当前最优基线，并展现出更高的生成稳定性。

Conclusion: SLIME有效解耦偏好学习与生成质量，在无需参考模型的前提下提升了对齐性能与鲁棒性。

Abstract: Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.

</details>


### [892] [Transformers learn factored representations](https://arxiv.org/abs/2602.02385)
*Adam Shai,Loren Amdahl-Culleton,Casper L. Christensen,Henry R. Bigelow,Fernando E. Rosas,Alexander B. Boyd,Eric A. Alt,Kyle J. Ray,Paul M. Riechers*

Main category: cs.LG

TL;DR: 本文探讨了Transformer模型在预训练过程中如何将世界分解为部分，并在残差流的正交子空间中表示这些因素。作者提出了两种表征假设：一种是各因素乘积空间中的表征（维度随部分数量指数增长），另一种是在正交子空间中的分解表征（维度线性增长）。研究发现，当因素条件独立时，模型倾向于采用无损的分解表征；即使存在噪声或隐藏依赖削弱条件独立性，模型早期训练仍偏好该表征，体现出对分解的归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer为何能将复杂世界分解为可解释的部分，并揭示其内部表征结构的几何特性与归纳偏置。

Method: 形式化提出两种表征假设（乘积空间 vs 正交子空间分解），推导各自激活几何结构的精确预测（如子空间数量、维数、上下文嵌入排列），并在具有已知潜在结构的合成数据上训练Transformer进行实证检验。

Result: Transformer在因素条件独立时学习到正交子空间的分解表征；即使条件独立性被噪声或隐藏依赖破坏，模型在训练初期仍偏向该表征，表明存在对分解的强归纳偏置。

Conclusion: Transformer天然倾向于将世界分解为正交子空间中的因素，这种分解表征虽在非独立情形下牺牲部分预测精度，但提升了维度效率和可解释性；该机制为Transformer的可解释低维结构提供了原理性解释。

Abstract: Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.

</details>


### [893] [An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence](https://arxiv.org/abs/2602.02400)
*Qizhen Zhang,Ankush Garg,Jakob Foerster,Niladri Chatterji,Kshitiz Malik,Mike Lewis*

Main category: cs.LG

TL;DR: 本文通过向干净数据集中注入受控的合成随机噪声，系统研究了噪声数据对大语言模型（LLM）预训练中损失发散的影响，发现噪声类型、噪声量和模型规模显著影响发散概率，并区分了噪声引发与高学习率引发的发散模式。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练数据集虽推动LLM成功，但其中不可避免的噪声可能引发训练不稳定甚至损失发散，该现象缺乏系统性实证理解。

Method: 在干净数据集中注入可控的合成均匀随机噪声，跨480M至5.2B参数规模模型分析训练动态，并对比噪声与高学习率导致的激活模式差异。

Result: 证实噪声数据确实导致训练损失发散；发散概率强烈依赖于噪声类型、噪声量和模型规模；噪声引发的发散具有区别于高学习率的独特激活模式，并提出可区分两类失败模式的诊断方法。

Conclusion: 本工作首次在大规模、受控条件下系统刻画了噪声数据对LLM预训练损失发散的影响机制，为鲁棒预训练提供了实证基础与诊断工具。

Abstract: Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.

</details>


### [894] [Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning](https://arxiv.org/abs/2602.02405)
*Ethan Mendes,Jungsoo Park,Alan Ritter*

Main category: cs.LG

TL;DR: 本文提出Distribution Aligned Imitation Learning (DAIL)，通过两步法弥合专家解与模型训练分布间的差距，显著提升LLM推理能力，仅需不到1000条专家解即可实现显著性能增益和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理增强方法依赖模型自采样正确解或更强模型提供监督信号，但对前沿模型仍难解的问题无法获取有效训练信号；而直接模仿昂贵且‘教学式’、含隐式推理缺口的专家解效果差。

Method: DAIL包含两步：（1）将专家解转化为详细、符合模型分布的推理轨迹；（2）采用对比学习目标，聚焦于专家的核心思路与方法论。

Result: 在Qwen2.5-Instruct和Qwen3上，仅用<1000条专家解即实现10–25% pass@k提升，推理效率提高2–4倍，并支持跨领域泛化。

Conclusion: DAIL是一种样本高效、分布对齐的专家知识蒸馏方法，为利用稀缺高质量人类反馈提升LLM推理提供了新范式。

Abstract: Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.

</details>


### [895] [Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models](https://arxiv.org/abs/2602.02415)
*Vivienne Pelletier,Daniel J. Rivera,Obinna Nwokonkwo,Steven A. Wilson,Christopher L. Muhich*

Main category: cs.LG

TL;DR: 本文提出了一种名为Active-Transfer Bagging（ATBagging）的新方法，用于主动学习中的初始种子集选择，通过贝叶斯视角下的装袋集成模型估计样本信息量，并结合行列式点过程（DPP）实现多样性采样，在多个真实数据集上验证了其在低数据场景下的优越性。


<details>
  <summary>Details</summary>
Motivation: 主动学习早期性能严重依赖随机选取的初始种子集，而现实中常存在相关或近似的数据集可被利用以构建更优种子集。

Method: ATBagging基于装袋（bagging）集成模型的贝叶斯解释，通过比较样本在袋内（in-bag）与袋外（out-of-bag）预测分布差异来估计信息增益；并采用基于随机傅里叶特征和质量-多样性分解的行列式点过程（DPP）进行多样性约束下的种子选择，该策略同时用于后续主动学习阶段的样本选取。

Result: 在QM9、ERA5、Forbes 2000和Beijing PM2.5四个真实数据集（涵盖目标迁移与特征偏移场景）上，ATBagging在种子规模nseed=10–100范围内，几乎总能提升或持平早期主动学习性能及学习曲线下的面积（AULC），尤其在低数据 regime 下增益最显著。

Conclusion: ATBagging是一种低成本、高回报的主动学习启动策略，能有效利用已有相关数据提升初始种子集质量，从而加速模型收敛。

Abstract: Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.

</details>


### [896] [Trust Region Continual Learning as an Implicit Meta-Learner](https://arxiv.org/abs/2602.02417)
*Zekun Wang,Anant Gupta,Christopher J. MacLellan*

Main category: cs.LG

TL;DR: 本文提出了一种结合生成式回放与Fisher度量信任区域约束的持续学习方法，兼具正则化与回放优势，在扩散模型图像生成和策略控制任务上展现出更优的性能保持与快速恢复能力。


<details>
  <summary>Details</summary>
Motivation: 标准持续学习方法存在核心权衡：基于正则化的方法（如EWC）在任务最优解弱重叠时过度约束更新；基于回放的方法虽能保持性能但易因回放不完美而漂移。

Method: 提出‘信任区域持续学习’：融合生成式回放与Fisher度量信任区域约束；在局部近似下，其更新具有MAML风格的单隐式内层步解释——回放提供旧任务梯度信号（query-like），Fisher加权惩罚实现高效离线曲率整形（support-like）。

Result: 在任务增量式扩散图像生成和持续扩散策略控制任务上，该方法取得最佳最终性能与保留率，并始终比EWC、回放及持续元学习基线更快恢复早期任务性能。

Conclusion: 信任区域持续学习赋予模型一种新兴的元学习特性：无需显式双层优化，模型即成为可快速‘重收敛’至先前任务最优解的初始化。

Abstract: Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.

</details>


### [897] [Poly-attention: a general scheme for higher-order self-attention](https://arxiv.org/abs/2602.02422)
*Sayak Chakrabarti,Toniann Pitassi,Josh Alman*

Main category: cs.LG

TL;DR: 本文提出了一类名为poly-attention的自注意力广义机制，统一建模高阶token交互，系统分析其计算复杂度与表达能力，并设计出首个能在精确二次时间下完成任意固定次数函数复合的新机制。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力机制无法有效处理三元相关性检测或需多token联合推理的组合任务，而高阶替代方案（如高阶注意力、Strassen注意力）虽可解决部分问题，却带来超二次时间开销。

Method: 定义涵盖任意张量计算与token关系结构的poly-attention机制族；通过算法设计与复杂度理论分析（含上下界证明），系统刻画其时间复杂度与可表达的polyadic任务类型。

Result: 给出多项新算法与匹配的复杂度下界；提出一种可在精确O(n²)时间内计算、支持任意固定次数函数复合的新注意力机制，且证明此前机制无法在亚二次时间内实现两函数复合。

Conclusion: poly-attention机制揭示了表达能力与高效近似可行性之间的紧致权衡：越强的表达力，越需要更大的模型系数才能实现近线性时间近似；新机制在保持二次时间效率的同时显著拓展了可解组合任务范围。

Abstract: The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.
  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.
  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.

</details>


### [898] [Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization](https://arxiv.org/abs/2602.02425)
*Amaru Caceres Arroyo,Lea Bogensperger,Ahmed Allam,Michael Krauthammer,Konrad Schindler,Dominik Narnhofer*

Main category: cs.LG

TL;DR: CHASE is a novel framework for protein fitness optimization that leverages pretrained protein language models and conditional flow-matching to directly generate high-fitness variants without costly predictor-based guidance, achieving state-of-the-art results on AAV and GFP benchmarks.


<details>
  <summary>Details</summary>
Motivation: Protein fitness optimization faces challenges due to the vast, sparse combinatorial sequence space and limitations of existing methods—either poor performance or high computational cost from gradient-based sampling.

Method: CHASE compresses embeddings from pretrained protein language models into a compact latent space and trains a conditional flow-matching model with classifier-free guidance to enable direct ODE-based generation of high-fitness variants, optionally bootstrapped with synthetic data.

Result: CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks; synthetic data bootstrapping further improves performance in low-data regimes.

Conclusion: CHASE demonstrates that distilling evolutionary knowledge from protein language models via flow-matching enables efficient, predictor-free generation of high-fitness protein variants, offering a scalable and effective alternative to existing approaches.

Abstract: Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.

</details>


### [899] [Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning](https://arxiv.org/abs/2602.02427)
*Qihao Wen,Jiahao Wang,Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.LG

TL;DR: 本文提出了一种基于扰动的不确定性量化（UQ）方法，用于评估大语言模型（LLM）在推理过程中各中间步骤的不确定性；该方法通过测量token对前序token嵌入扰动的敏感性来识别错误或不确定的中间步骤，在性能、简洁性和效率上优于基于概率、熵或多采样的基线方法。


<details>
  <summary>Details</summary>
Motivation: LLM在推理任务中不仅需评估最终答案的不确定性，还需评估中间推理步骤的不确定性，以支持细粒度干预；现有UQ方法对此关注不足。

Method: 提出一种基于嵌入扰动的UQ指标：计算每个token对其前序token嵌入微小扰动的输出敏感性，用以衡量中间步骤的不确定性。

Result: 该扰动敏感性指标在识别错误中间步骤上显著优于token概率、token熵等基线方法；且无需多次采样，兼具高效性与简洁性。

Conclusion: 嵌入扰动敏感性是一种有效、高效且实用的中间不确定性量化指标，为LLM可信赖推理提供了新思路。

Abstract: Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.

</details>


### [900] [Maximizing Reliability with Bayesian Optimization](https://arxiv.org/abs/2602.02432)
*Jack M. Buckingham,Ivo Couckuyt,Juergen Branke*

Main category: cs.LG

TL;DR: 本文提出两种基于贝叶斯优化（BO）的新方法（Thompson采样与知识梯度），专为极低失效概率（10⁻⁶–10⁻⁸）的可靠性优化问题设计，结合重要性采样提升效率，并在极端与非极端场景下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 制造中需最大化设计可靠性（即最小化极低概率的失效事件，P_fail = 10^{-6}–10^{-8}），而传统贝叶斯优化难以高效处理如此稀疏的失效样本。

Method: 提出两种BO方法：1）基于Thompson采样的方法；2）基于知识梯度的方法，该方法近似最小化失效概率对数的一步贝叶斯最优策略；两者均引入重要性采样以聚焦极小失效概率区域。

Result: 实验表明，所提方法在极低失效概率（极端）和常规失效概率（非极端）两类场景下，性能均优于现有BO方法。

Conclusion: 结合重要性采样的Thompson采样与知识梯度型BO方法，显著提升了针对极低失效概率可靠性优化问题的样本效率与优化效果。

Abstract: Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.

</details>


### [901] [Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE](https://arxiv.org/abs/2602.02443)
*Yuanteng Chen,Peisong Wang,Nanxin Zeng,Yuantian Shao,Gang Li,Jing Liu,Jian Cheng*

Main category: cs.LG

TL;DR: 本文提出Expert-Sample方法，利用细粒度MoE模型中路由器分数的‘高置信头部’与‘低置信尾部’特性，在不训练的前提下提升LLM测试时采样多样性与稳定性，显著改善pass@n和验证准确率。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法依赖温度调优，难以兼顾多样性与稳定性；而细粒度MoE的丰富路由空间尚未被用于测试时生成优化。

Method: 基于对细粒度MoE路由行为的经验分析，发现路由器分数呈‘高置信头部+低置信尾部’分布；Expert-Sample在保留头部确定性选择的同时，对尾部引入可控随机性，实现多样且稳定生成。

Result: 在多个细粒度MoE模型及数学、知识推理、代码任务上，Expert-Sample一致提升pass@n和验证准确率；例如Qwen3-30B-A3B-Instruct在GPQA-Diamond上pass@32从85.4%升至91.9%，验证准确率从59.1%升至62.6%。

Conclusion: Expert-Sample是一种无需训练、轻量高效的方法，揭示并利用MoE路由内在结构，为测试时扩展提供了新范式。

Abstract: Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.

</details>


### [902] [Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation](https://arxiv.org/abs/2602.02445)
*Seo Taek Kong,R. Srikant*

Main category: cs.LG

TL;DR: 本文推导了非线性随机逼近算法在Wasserstein-p距离下的非渐近误差界，通过耦合方法将离散过程与Ornstein-Uhlenbeck过程比较，获得最后迭代点的显式有限样本保证，并分析了Polyak-Ruppert平均的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 为弥补有限样本分析与渐近理论之间的鸿沟，并提供更紧致的分布收敛保证，尤其在重尾到高斯行为过渡及SGD的中心极限定理收敛速率方面。

Method: 采用耦合论证方法，将离散时间随机逼近过程与极限Ornstein-Uhlenbeck过程进行比较；在一般噪声（如鞅差、遍历马尔可夫链函数）下分析最后迭代点和Polyak-Ruppert平均的Wasserstein-p收敛速率；假设驱动噪声满足非渐近中心极限定理。

Result: 最后迭代点经归一化后以γ_n^{1/6}速率收敛于高斯分布在Wasserstein-p距离下；Polyak-Ruppert平均以n^{-1/6}速率收敛；由此导出优于矩界+马尔可夫不等式的高概率集中不等式；在LSA和SGD两个应用中验证了方法有效性。

Conclusion: 本文建立了统一且通用的非渐近分布收敛分析框架，显著提升了对随机逼近算法统计行为的理解与刻画能力，尤其在有限样本场景下提供了更精细的量化保证。

Abstract: This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.

</details>


### [903] [Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization](https://arxiv.org/abs/2602.02451)
*Patrick Cooper,Alvaro Velasquez*

Main category: cs.LG

TL;DR: 本文提出Active Causal Experimentalist (ACE)，一种基于偏好优化的序列化因果实验策略学习方法，通过比较干预选项而非依赖不稳定的价值信号，在多种基准上显著提升因果发现效率，并自动发现理论一致的实验策略。


<details>
  <summary>Details</summary>
Motivation: 传统因果实验设计方法（如随机采样、贪心信息最大化）无法从经验中学习自适应策略，难以应对序列决策中的知识累积效应。

Method: 提出ACE框架，将实验设计建模为序列决策问题，利用Direct Preference Optimization（DPO）从成对干预比较中学习策略，避免依赖随知识增长而衰减的绝对信息增益。

Result: 在合成数据、物理仿真和经济数据上，ACE相较基线方法在相同干预预算下提升70–71%（p < 0.001，Cohen's d ~ 2），并自主发现针对collider机制需集中干预父变量的理论一致策略。

Conclusion: 偏好式学习能有效恢复有理论依据的实验策略，将先验理论与经验驱动的领域自适应相结合，为因果发现提供新范式。

Abstract: Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.

</details>


### [904] [Conflict-Aware Client Selection for Multi-Server Federated Learning](https://arxiv.org/abs/2602.02458)
*Mingwei Hong,Zheng Lin,Zehang Lin,Lin Li,Miao Yang,Xia Du,Zihan Fang,Zhaolu Kang,Dianxin Luan,Shunzhi Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种基于去中心化强化学习与冲突风险预测（RL-CRP）的方法，用于优化多服务器联邦学习中的客户端选择，以缓解资源争用和通信延迟问题。


<details>
  <summary>Details</summary>
Motivation: 传统单服务器联邦学习存在高通信延迟；多服务器联邦学习则因客户端覆盖重叠和无协调选择导致资源争用、带宽冲突和训练失败。

Method: 提出去中心化强化学习框架RL-CRP：各服务器利用稀疏历史选择序列构建分类隐马尔可夫模型预测客户端选择冲突概率，并引入公平性感知的奖励机制以平衡长期参与与训练效率。

Result: 实验表明RL-CRP显著降低服务器间冲突，提升收敛速度并减少通信开销。

Conclusion: RL-CRP是一种有效解决多服务器联邦学习中客户端选择冲突与资源争用问题的新方法，兼顾训练效率与客户端公平参与。

Abstract: Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.

</details>


### [905] [Expanding the Capabilities of Reinforcement Learning via Text Feedback](https://arxiv.org/abs/2602.02482)
*Yuda Song,Lili Chen,Fahim Tajwar,Remi Munos,Deepak Pathak,J. Andrew Bagnell,Aarti Singh,Andrea Zanette*

Main category: cs.LG

TL;DR: 本文提出了一种利用文本反馈进行大语言模型强化学习的新范式（RLTF），通过两种方法（自蒸馏和反馈建模）将文本反馈内化为单轮推理能力，在多个任务上超越强基线。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF仅用二元奖励信号，信息稀疏；而监督微调依赖高成本人工示范。文本反馈作为中间监督信号，既比标量奖励丰富，又比完整示范廉价，且在现实中广泛存在。

Method: 提出RL from Text Feedback（RLTF）框架：训练时可访问多轮文本反馈，推理时仅单轮；设计两种方法——Self Distillation（RLTF-SD）让单轮策略拟合自身基于反馈的第二轮生成；Feedback Modeling（RLTF-FM）将反馈预测作为辅助目标。

Result: 在推理谜题、竞赛数学和创意写作任务上，RLTF-SD与RLTF-FM均持续优于强基线（如PPO、DPO、SFT等），验证了文本反馈作为可扩展丰富监督信号的有效性。

Conclusion: 文本反馈是一种极具潜力的中间监督形式，RLTF框架及其两种实现方法能有效将反馈内化为单轮性能提升，为LLM对齐提供了新路径。

Abstract: The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.

</details>


### [906] [MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training](https://arxiv.org/abs/2602.02494)
*Dulhan Jayalath,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: 本文提出MEG-XL模型，通过长达2.5分钟的脑磁图（MEG）长上下文预训练，显著提升少样本脑信号到文本解码性能，仅需1小时数据即可达到传统方法50小时监督训练的效果。


<details>
  <summary>Details</summary>
Motivation: 临床脑机接口需面向瘫痪患者，训练数据稀缺；现有预训练方法上下文过短（仅几秒），无法有效建模神经活动的长时依赖关系。

Method: 设计并实现MEG-XL模型，采用2.5分钟（约191k token）MEG序列进行自监督预训练，随后在单词解码任务上微调。

Result: MEG-XL在少量标注数据下（如1小时）即达到与大量监督训练（50小时）相当的性能，并超越现有脑基础模型；验证了长上下文预训练能学习更具迁移性的神经表征。

Conclusion: 延长预训练上下文长度可有效利用被以往方法忽略的扩展神经上下文，是提升脑机接口数据效率的关键路径。

Abstract: Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [907] [MapDream: Task-Driven Map Learning for Vision-Language Navigation](https://arxiv.org/abs/2602.00222)
*Guoxin Lian,Shuo Wang,Yucheng Wang,Yongcai Wang,Maiyue Chen,Kaihui Wang,Bo Zhang,Zhizhong Su,Deying Li,Zhaoxin Fan*

Main category: cs.RO

TL;DR: 本文提出MapDream框架，将地图构建建模为自回归鸟瞰图（BEV）图像合成，联合学习地图生成与动作预测，实现面向导航任务的紧凑、关键信息保留的地图表示，在R2R-CE和RxR-CE上达到单目视觉SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法多依赖于与导航策略无关的手工构造地图，缺乏任务驱动性；作者主张地图应是受导航目标直接塑造的可学习表征，而非对环境的冗余重建。

Method: 提出MapDream：一种‘地图在环’框架，将地图构建视为自回归鸟瞰图（BEV）图像合成任务；联合训练地图生成模块（输出三通道BEV图）与动作预测模块；采用监督预训练初始化映射-控制接口，并通过强化学习进行端到端微调。

Result: 在R2R-CE和RxR-CE基准上取得单目视觉设置下的最先进（SOTA）性能，验证了任务驱动式生成地图学习的有效性。

Conclusion: 任务驱动的、生成式的、紧凑的BEV地图表示优于传统手工或重建式地图，能更有效地支持视觉语言导航。

Abstract: Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.

</details>


### [908] [ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control](https://arxiv.org/abs/2602.00401)
*Jean Pierre Sleiman,He Li,Alphonsus Adu-Bredu,Robin Deits,Arun Kumar,Kevin Bergamin,Mohak Bhardwaj,Scott Biddlestone,Nicola Burger,Matthew A. Estrada,Francesco Iacobelli,Twan Koolen,Alexander Lambert,Erica Lin,M. Eva Mungai,Zach Nobles,Shane Rozen-Levy,Yuyao Shi,Jiashun Wang,Jakob Welner,Fangzhou Yu,Mike Zhang,Alfred Rizzi,Jessica Hodgins,Sylvain Bertrand,Yeuhi Abe,Scott Kuindersma,Farbod Farshidian*

Main category: cs.RO

TL;DR: ZEST是一种零样本具身技能迁移框架，通过强化学习从多种运动数据源（动作捕捉、单目视频、动画）中训练策略，并直接零样本部署到不同机器人硬件上，无需大量调参或接触标签等辅助信息。


<details>
  <summary>Details</summary>
Motivation: 实现类人机器人在敏捷、多接触场景下的鲁棒全身控制仍面临巨大挑战，现有方法依赖大量技能定制和脆弱的控制器调优过程。

Method: 提出ZEST框架，结合自适应采样与基于模型的辅助力矩自动课程学习；采用近似解析臂架值选取关节增益，并改进执行器建模；全部训练在适度域随机化的仿真中完成。

Result: 在Atlas人形机器人上实现了军式爬行、霹雳舞等动态多接触技能；从视频中零样本迁移舞蹈和箱体攀爬技能至Atlas和Unitree G1；扩展至Spot四足机器人，实现连续后空翻等特技。

Conclusion: ZEST实现了跨异构数据源与不同机器人形态的鲁棒零样本部署，为生物运动与机器人运动之间提供了可扩展的接口。

Abstract: Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sources -- high-fidelity motion capture, noisy monocular video, and non-physics-constrained animation -- and deploys them to hardware zero-shot. ZEST generalizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping. Its training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers. We further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators. Trained entirely in simulation with moderate domain randomization, ZEST demonstrates remarkable generality. On Boston Dynamics' Atlas humanoid, ZEST learns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation. Together, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing ZEST as a scalable interface between biological movements and their robotic counterparts.

</details>


### [909] [FISC: A Fluid-Inspired Framework for Decentralized and Scalable Swarm Control](https://arxiv.org/abs/2602.00480)
*Mohini Priya Kolluri,Ammar Waheed,Zohaib Hasnain*

Main category: cs.RO

TL;DR: 本文提出了一种基于流体力学原理的去中心化大群体机器人外环控制方法，通过将机器人状态与流体基本属性映射，使群体像流体一样运动，无需个体间通信；仿真验证了其与CFD结果在速度、密度、压力等场量上的定量一致性，证明了将大规模机器人集群视为连续介质进行可扩展、去中心化控制的可行性。


<details>
  <summary>Details</summary>
Motivation: 大规模机器人集群的可扩展协同常受限于依赖个体间通信所带来的延迟、带宽限制和故障脆弱性。

Method: 提出一种基于流体运动范式的去中心化外环控制方法，建立机器人个体状态与流体基本要素（如速度、密度、压力）之间的映射关系，并通过施加压力边界条件驱动群体‘流动’；将子群赋予流体特性，实现无显式状态通信的集体演化与结构保持。

Result: 在含约1000架四旋翼无人机的仿真中， swarm生成的速度、密度、压力场与CFD解对比，归一化RMSE分别为0.15–0.9、0.61–0.98、0–0.937；验证了宏观结构与第一性原理推导的一致性。

Conclusion: 该方法证明了将大规模机器人集群建模为连续介质系统以实现可扩展、去中心化控制的可行性，为摆脱通信依赖提供了新范式。

Abstract: Achieving scalable coordination in large robotic swarms is often constrained by reliance on inter-agent communication, which introduces latency, bandwidth limitations, and vulnerability to failure. To address this gap, a decentralized approach for outer-loop control of large multi-agent systems based on the paradigm of how a fluid moves through a volume is proposed and evaluated. A relationship between fundamental fluidic element properties and individual robotic agent states is developed such that the corresponding swarm "flows" through a space, akin to a fluid when forced via a pressure boundary condition. By ascribing fluid-like properties to subsets of agents, the swarm evolves collectively while maintaining desirable structure and coherence without explicit communication of agent states within or outside of the swarm. The approach is evaluated using simulations involving $O(10^3)$ quadcopter agents and compared against Computational Fluid Dynamics (CFD) solutions for a converging-diverging domain. Quantitative agreement between swarm-derived and CFD fields is assessed using Root-Mean-Square Error (RMSE), yielding normalized errors of 0.15-0.9 for velocity, 0.61-0.98 for density, 0-0.937 for pressure. These results demonstrate the feasibility of treating large robotic swarms as continuum systems that retain the macroscopic structure derived from first principles, providing a basis for scalable and decentralized control.

</details>


### [910] [Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning](https://arxiv.org/abs/2602.00500)
*Jianyi Zhou,Yujie Wei,Ruichen Zhen,Bo Zhao,Xiaobo Xia,Rui Shao,Xiu Su,Shuo Yang*

Main category: cs.RO

TL;DR: 本文提出INFUSE框架，首次实现了对视觉-语言-动作（VLA）基础模型的鲁棒后门攻击，其注入的后门能在任意用户微调后仍保持高成功率，揭示了VLA模型在部署前植入后门的重大安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的后门攻击易被用户端微调擦除，缺乏实际部署可行性，亟需设计对微调鲁棒的后门攻击方法。

Method: INFUSE通过分析不同微调场景下的参数敏感性，识别出微调不敏感模块，并将后门注入其中，同时冻结其余模块，从而保证后门在用户微调后仍有效。

Result: 在仿真环境和真实机器人任务中，经用户微调后，INFUSE的攻击成功率分别达91.0%和79.8%，显著优于BadVLA；且保持与标准模型相当的干净任务性能。

Conclusion: INFUSE揭示了一种现实威胁：在VLA基础模型分发前植入的后门可跨越用户微调阶段，在最终部署时持续生效，凸显VLA模型供应链安全的重要性。

Abstract: Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored -- particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments. While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose INFUSE (INjection into Fine-tUne-inSensitive modulEs), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain largely unchanged -- the fine-tune-insensitive modules. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE's effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of 91.0% on simulation environments and 79.8% on real-world robot tasks, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models. These results uncover a critical threat: backdoors implanted before distribution can persist through fine-tuning and remain effective at deployment.

</details>


### [911] [A Low-Cost Vision-Based Tactile Gripper with Pretraining Learning for Contact-Rich Manipulation](https://arxiv.org/abs/2602.00514)
*Yaohua Liu,Binkai Ou,Zicheng Qiu,Ce Hao,Yemin Wang,Hengjun Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、高性能的视觉-触觉融合夹爪LVTG，通过扩大触觉感知面积、增大开合角、增强耐用性及模组化设计，提升了接触丰富环境下的抓取稳定性与鲁棒性；并采用CLIP启发的对比学习对齐视觉与触觉特征，显著提升ACT策略在操作任务中的成功率。


<details>
  <summary>Details</summary>
Motivation: 传统触觉传感器在接触丰富的机器人操作中存在感知范围小、可靠性低、成本高等问题，亟需一种更稳定、鲁棒且经济高效的视觉-触觉融合方案。

Method: 设计了低功耗、大感知面积、大开合角、高耐磨表面的模块化LVTG夹爪，并引入CLIP风格的对比学习目标，将触觉嵌入与对应视觉观测对齐，构建跨模态表征空间，进而提升Action Chunking Transformer（ACT）策略性能。

Result: LVTG在接触丰富的操作任务中相比原始ACT方法实现了显著更高的成功率，同时具备更优的数据采集效率和策略学习效果。

Conclusion: LVTG是一种兼具实用性与先进性的低成本视觉-触觉夹爪，其硬件设计与跨模态对齐方法共同推动了接触丰富场景下机器人操作的性能边界。

Abstract: Robotic manipulation in contact-rich environments remains challenging, particularly when relying on conventional tactile sensors that suffer from limited sensing range, reliability, and cost-effectiveness. In this work, we present LVTG, a low-cost visuo-tactile gripper designed for stable, robust, and efficient physical interaction. Unlike existing visuo-tactile sensors, LVTG enables more effective and stable grasping of larger and heavier everyday objects, thanks to its enhanced tactile sensing area and greater opening angle. Its surface skin is made of highly wear-resistant material, significantly improving durability and extending operational lifespan. The integration of vision and tactile feedback allows LVTG to provide rich, high-fidelity sensory data, facilitating reliable perception during complex manipulation tasks. Furthermore, LVTG features a modular design that supports rapid maintenance and replacement. To effectively fuse vision and touch, We adopt a CLIP-inspired contrastive learning objective to align tactile embeddings with their corresponding visual observations, enabling a shared cross-modal representation space for visuo-tactile perception. This alignment improves the performance of an Action Chunking Transformer (ACT) policy in contact-rich manipulation, leading to more efficient data collection and more effective policy learning. Compared to the original ACT method, the proposed LVTG with pretraining achieves significantly higher success rates in manipulation tasks.

</details>


### [912] [APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation](https://arxiv.org/abs/2602.00551)
*Daoxuan Zhang,Ping Chen,Xiaobo Xia,Xiu Su,Ruichen Zhen,Jianqiang Xiao,Shuo Yang*

Main category: cs.RO

TL;DR: 本文提出APEX，一种用于空中目标导航的分层智能体，通过动态空间语义映射记忆、强化学习驱动的动作决策模块和开放词汇检测的目标定位模块，提升UAV在复杂空中环境中的探索效率与目标识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空中环境中难以有效记忆复杂空间表征、缺乏可靠可解释的动作决策机制，且探索与信息采集效率低下。

Method: 提出APEX分层智能体：1）基于VLM零样本能力构建动态高分辨率3D空间语义地图；2）采用强化学习训练动作决策模块；3）使用开放词汇检测器实现目标定位；三者集成于异步并行框架中以规避VLM推理延迟。

Result: 在UAV-ON基准上，APEX相比SOTA提升4.2%成功率（SR）和2.8%路径长度加权成功率（SPL）。

Conclusion: APEX的分层异步设计显著提升了空中目标导航任务的效率与鲁棒性，验证了其架构在复杂空中环境中的有效性。

Abstract: Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM's inference latency and boosting the agent's proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\% SR and +2.8\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \href{https://github.com/4amGodvzx/apex}{GitHub}

</details>


### [913] [ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation](https://arxiv.org/abs/2602.00557)
*Weisheng Dai,Kai Lan,Jianyi Zhou,Bo Zhao,Xiu Su,Junwen Tong,Weili Guan,Shuo Yang*

Main category: cs.RO

TL;DR: 本文提出ConLA框架，通过对比解耦机制从人类演示视频中无监督地学习机器人策略，解决了现有方法因仅关注视觉重建而导致的捷径学习和潜在表征纠缠问题，首次在仅使用人类视频预训练的情况下超越了真实机器人轨迹预训练的效果。


<details>
  <summary>Details</summary>
Motivation: 获取覆盖多样任务和环境的大规模机器人遥操作数据集成本高昂且难以扩展；而人类演示视频虽丰富可扩展，但缺乏显式的动作标注，难以直接用于机器人策略学习。

Method: 提出ConLA无监督预训练框架，引入基于动作类别先验和时序线索的对比解耦机制，将运动动力学与视觉内容分离，避免捷径学习并获得解耦的潜在动作表征。

Result: 在多个基准测试中表现优异，首次实现仅用人类视频预训练即超越真实机器人轨迹预训练的性能。

Conclusion: ConLA能从人类视频中提取纯净、语义一致的潜在动作表征，为可扩展的机器人学习提供了新范式。

Abstract: Vision-Language-Action (VLA) models achieve preliminary generalization through pretraining on large scale robot teleoperation datasets. However, acquiring datasets that comprehensively cover diverse tasks and environments is extremely costly and difficult to scale. In contrast, human demonstration videos offer a rich and scalable source of diverse scenes and manipulation behaviors, yet their lack of explicit action supervision hinders direct utilization. Prior work leverages VQ-VAE based frameworks to learn latent actions from human videos in an unsupervised manner. Nevertheless, since the training objective primarily focuses on reconstructing visual appearances rather than capturing inter-frame dynamics, the learned representations tend to rely on spurious visual cues, leading to shortcut learning and entangled latent representations that hinder transferability. To address this, we propose ConLA, an unsupervised pretraining framework for learning robotic policies from human videos. ConLA introduces a contrastive disentanglement mechanism that leverages action category priors and temporal cues to isolate motion dynamics from visual content, effectively mitigating shortcut learning. Extensive experiments show that ConLA achieves strong performance across diverse benchmarks. Notably, by pretraining solely on human videos, our method for the first time surpasses the performance obtained with real robot trajectory pretraining, highlighting its ability to extract pure and semantically consistent latent action representations for scalable robot learning.

</details>


### [914] [UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning](https://arxiv.org/abs/2602.00566)
*Nan Song,Junzhe Jiang,Jingyu Li,Xiatian Zhu,Li Zhang*

Main category: cs.RO

TL;DR: 本文提出UniMotion，一个基于解码器-only Transformer的统一运动框架，旨在整合自动驾驶中的运动仿真、预测与规划任务，通过共享结构和专用交互模式实现跨任务泛化与高效联合优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法将运动仿真、预测和规划任务孤立处理，导致模型设计专用化、跨任务泛化能力差，并忽视任务间的协同增益。

Method: 提出UniMotion框架，采用decoder-only Transformer架构，引入专用交互模式和定制化训练策略，支持多任务联合训练与按需微调。

Result: 在Waymo Open Motion Dataset上实验表明，联合训练提升鲁棒泛化能力；经微调后，在多项运动任务上达到SOTA性能。

Conclusion: UniMotion是一个通用、可扩展的统一运动建模方案，为自动驾驶系统提供更高效、更一致的运动理解与决策基础。

Abstract: Motion simulation, prediction and planning are foundational tasks in autonomous driving, each essential for modeling and reasoning about dynamic traffic scenarios. While often addressed in isolation due to their differing objectives, such as generating diverse motion states or estimating optimal trajectories, these tasks inherently depend on shared capabilities: understanding multi-agent interactions, modeling motion behaviors, and reasoning over temporal and spatial dynamics. Despite this underlying commonality, existing approaches typically adopt specialized model designs, which hinders cross-task generalization and system scalability. More critically, this separation overlooks the potential mutual benefits among tasks. Motivated by these observations, we propose UniMotion, a unified motion framework that captures shared structures across motion tasks while accommodating their individual requirements. Built on a decoder-only Transformer architecture, UniMotion employs dedicated interaction modes and tailored training strategies to simultaneously support these motion tasks. This unified design not only enables joint optimization and representation sharing but also allows for targeted fine-tuning to specialize in individual tasks when needed. Extensive experiments on the Waymo Open Motion Dataset demonstrate that joint training leads to robust generalization and effective task integration. With further fine-tuning, UniMotion achieves state-of-the-art performance across a range of motion tasks, establishing it as a versatile and scalable solution for autonomous driving.

</details>


### [915] [Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction](https://arxiv.org/abs/2602.00575)
*Chaoqun Cui,Jing Huang,Shijing Wang,Liming Zheng,Qingchao Kong,Zhixiong Zeng*

Main category: cs.RO

TL;DR: 本文提出VAGEN框架，通过引入具备交互能力的验证代理（verifier agent），实现对GUI智能体任务完成情况的主动式、可验证评估，克服了传统规则方法和LLM-as-a-Judge在可扩展性与状态可观测性上的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体评估方法存在两大瓶颈：规则方法难以扩展且无法处理开放任务；LLM-as-a-Judge依赖被动视觉观察，无法捕捉部分可观测环境中的潜在系统状态。

Method: 提出Agentic Interactive Verification范式，构建VAGEN框架——一个配备交互工具的验证代理，能自主规划验证策略并主动与环境交互以获取任务完成证据。

Result: 在OSWorld-Verified和AndroidWorld基准上，VAGEN显著提升评估准确率，并通过测试时缩放策略进一步增强性能。

Conclusion: 主动交互式验证是提升GUI智能体评估可靠性与泛化性的关键路径，VAGEN为RLVR提供了更鲁棒、可扩展的评估新范式。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is pivotal for the continuous evolution of GUI agents, yet existing evaluation paradigms face significant limitations. Rule-based methods suffer from poor scalability and cannot handle open-ended tasks, while LLM-as-a-Judge approaches rely on passive visual observation, often failing to capture latent system states due to partial state observability. To address these challenges, we advocate for a paradigm shift from passive evaluation to Agentic Interactive Verification. We introduce VAGEN, a framework that employs a verifier agent equipped with interaction tools to autonomously plan verification strategies and proactively probe the environment for evidence of task completion. Leveraging the insight that GUI tasks are typically "easy to verify but hard to solve", VAGEN overcomes the bottlenecks of visual limitations. Experimental results on OSWorld-Verified and AndroidWorld benchmarks demonstrate that VAGEN significantly improves evaluation accuracy compared to LLM-as-a-Judge baselines and further enhances performance through test-time scaling strategies.

</details>


### [916] [Factored Reasoning with Inner Speech and Persistent Memory for Evidence-Grounded Human-Robot Interaction](https://arxiv.org/abs/2602.00675)
*Valerio Belcamino,Mariya Kilina,Alessandro Carfì,Valeria Seidita,Fulvio Mastrogiovanni,Antonio Chella*

Main category: cs.RO

TL;DR: 本文提出了JANUS认知架构，用于对话式人机交互中的助手机器人，通过建模为部分可观测马尔可夫决策过程、模块化设计与显式策略控制，实现上下文保持、请求补全和证据支撑的可验证响应。


<details>
  <summary>Details</summary>
Motivation: 对话式人机交互需机器人持续维护用户上下文、处理不完整请求、基于外部证据生成响应，并确保中间决策可验证。

Method: 提出JANUS架构：将交互建模为POMDP；采用带类型接口的分解式控制器；划分六大功能模块（范围检测、意图识别、记忆、内语、查询生成、外语言）；引入信息充分性、执行就绪性、工具接地等显式策略；设计三层记忆机制与受认知理论启发的内语模型以验证参数并触发澄清；施加忠实性约束，使机器人输出严格绑定于工作上下文与工具结果构成的证据束。

Result: 在膳食辅助领域基于知识图谱开展模块级单元测试，显示与人工标注参考高度一致，且具备实用延迟性能。

Conclusion: 分解式推理是实现可扩展、可审计、证据支撑的长时程机器人辅助的一条有前景路径。

Abstract: Dialogue-based human-robot interaction requires robot cognitive assistants to maintain persistent user context, recover from underspecified requests, and ground responses in external evidence, while keeping intermediate decisions verifiable. In this paper we introduce JANUS, a cognitive architecture for assistive robots that models interaction as a partially observable Markov decision process and realizes control as a factored controller with typed interfaces. To this aim, Janus (i) decomposes the overall behavior into specialized modules, related to scope detection, intent recognition, memory, inner speech, query generation, and outer speech, and (ii) exposes explicit policies for information sufficiency, execution readiness, and tool grounding. A dedicated memory agent maintains a bounded recent-history buffer, a compact core memory, and an archival store with semantic retrieval, coupled through controlled consolidation and revision policies. Models inspired by the notion of inner speech in cognitive theories provide a control-oriented internal textual flow that validates parameter completeness and triggers clarification before grounding, while a faithfulness constraint ties robot-to-human claims to an evidence bundle combining working context and retrieved tool outputs. We evaluate JANUS through module-level unit tests in a dietary assistance domain grounded on a knowledge graph, reporting high agreement with curated references and practical latency profiles. These results support factored reasoning as a promising path to scalable, auditable, and evidence-grounded robot assistance over extended interaction horizons.

</details>


### [917] [Toward Reliable Sim-to-Real Predictability for MoE-based Robust Quadrupedal Locomotion](https://arxiv.org/abs/2602.00678)
*Tianyang Wu,Hanwei Guo,Yuhang Wang,Junshu Yang,Xinyang Sui,Jiayi Xie,Xingyu Chen,Zeyang Liu,Xuguang Lan*

Main category: cs.RO

TL;DR: 本文提出了一种基于专家混合（MoE）的本体感知型四足机器人运动策略及RoboGauge评估框架，以提升仿真到现实迁移能力与多地形鲁棒性，并在Unitree Go2上验证了其在雪地、沙地、楼梯等复杂地形中的高性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在四足机器人敏捷运动中面临的仿真-现实差距大、复杂地形下奖励过拟合、物理验证风险高且效率低等问题。

Method: 提出统一框架：1）基于门控机制的专家混合（MoE）运动策略，利用本体感知信息分解地形与指令建模；2）RoboGauge预测评估套件，通过仿真内多维度测试（地形类型、难度、域随机化）提供可迁移性量化指标。

Result: 在Unitree Go2平台上实现对雪、沙、楼梯、斜坡和30cm障碍等未见复杂地形的稳健运动；高速测试达4 m/s，并涌现出提升高速稳定性的窄幅步态。

Conclusion: MoE策略结合RoboGauge评估显著提升了本体感知驱动下的多地形泛化能力与仿真-现实迁移可靠性，降低了物理实验依赖。

Abstract: Reinforcement learning has shown strong promise for quadrupedal agile locomotion, even with proprioception-only sensing. In practice, however, sim-to-real gap and reward overfitting in complex terrains can produce policies that fail to transfer, while physical validation remains risky and inefficient. To address these challenges, we introduce a unified framework encompassing a Mixture-of-Experts (MoE) locomotion policy for robust multi-terrain representation with RoboGauge, a predictive assessment suite that quantifies sim-to-real transferability. The MoE policy employs a gated set of specialist experts to decompose latent terrain and command modeling, achieving superior deployment robustness and generalization via proprioception alone. RoboGauge further provides multi-dimensional proprioception-based metrics via sim-to-sim tests over terrains, difficulty levels, and domain randomizations, enabling reliable MoE policy selection without extensive physical trials. Experiments on a Unitree Go2 demonstrate robust locomotion on unseen challenging terrains, including snow, sand, stairs, slopes, and 30 cm obstacles. In dedicated high-speed tests, the robot reaches 4 m/s and exhibits an emergent narrow-width gait associated with improved stability at high velocity.

</details>


### [918] [Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching](https://arxiv.org/abs/2602.00686)
*Yujie Wei,Jiahan Fan,Jiyu Guo,Ruichen Zhen,Rui Shao,Xiu Su,Zeke Xie,Shuo Yang*

Main category: cs.RO

TL;DR: 本文提出一种可学习的推理加速框架，将缓存策略建模为任务感知的动态决策问题，通过两个轻量模块（缓存令牌选择器与缓存比例预测器）实现端到端优化，在提升VLA模型推理速度的同时还提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型计算开销大，而传统加速方法（如启发式或静态缓存/剪枝）缺乏任务感知能力，无法适应动态场景变化。

Method: 将推理加速建模为可学习的策略优化问题；设计两个轻量协同模块——Cached Token Selector（决定复用哪些token）和Cache Ratio Predictor（决定复用多少token）；采用可微松弛技术实现离散决策的端到端梯度优化。

Result: 在LIBERO和SIMPLER基准及真实机器人实验中，实现1.76倍实测推理加速；LIBERO平均成功率提升1.9个百分点（75.0%→76.9%），真实任务提升5.0个百分点。

Conclusion: 任务感知的可学习计算分配策略能兼顾VLA模型的性能与效率，为实用化部署提供了新路径。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable generalization capabilities in robotic manipulation tasks, yet their substantial computational overhead remains a critical obstacle to real-world deployment. Improving inference efficiency is therefore essential for practical robotic applications. Existing acceleration methods often rely on heuristic or static strategies--such as rule-based token caching or pruning--that are decoupled from task objectives and fail to adapt to dynamic scene changes. In this work, we reformulate inference acceleration as a learnable policy optimization problem and propose a novel framework that integrates a dynamic, task-aware decision-making process directly into the VLA model. At its core are two lightweight, cooperative modules: a Cached Token Selector, which determines which tokens should be reused, and a Cache Ratio Predictor, which controls how many tokens to reuse. Training these modules is non-trivial due to their discrete decisions. We address this by adopting a differentiable relaxation that allows gradient-based end-to-end optimization. Extensive experiments on the LIBERO and SIMPLER benchmarks, as well as real-robot evaluations, show that our method achieves a 1.76x wall-clock inference speedup while simultaneously improving the average success rate by 1.9 percentage points (from 75.0% to 76.9%) on LIBERO and by 5.0 percentage points on real-world tasks, significantly outperforming existing baselines. This work highlights the potential of learning task-aware computational allocation policies, paving the way for VLA models that are both powerful and efficient.

</details>


### [919] [USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation](https://arxiv.org/abs/2602.00708)
*Weiqi Gai,Yuman Gao,Yuan Zhou,Yufan Xie,Zhiyang Liu,Yuze Wu,Xin Zhou,Fei Gao,Zhijun Meng*

Main category: cs.RO

TL;DR: 本文提出USS-Nav轻量级框架，通过构建统一的时空语义图并结合大语言模型（LLM）实现无人机在未知环境中的零样本目标导航，兼顾高语义推理能力与边缘计算资源限制。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在未知环境中进行零样本目标导航时，高层语义推理需求与机载计算资源受限之间的矛盾。

Method: 提出USS-Nav框架：1）基于多面体扩展的增量式空间连通图生成方法，捕获全局几何拓扑；2）通过图聚类动态划分语义区域；3）将开放词汇对象语义锚定到该拓扑，构建分层环境表征；4）采用粗到细探索策略——LLM基于场景图语义定位目标区域，局部规划器基于信息增益优化前沿覆盖。

Result: 在资源受限平台上实现实时更新频率15Hz，计算效率优于现有方法；消融实验表明SPL指标显著提升。

Conclusion: USS-Nav在保证导航性能的同时显著降低计算开销，为边缘端零样本语义导航提供了可行方案。

Abstract: Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph's semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research.

</details>


### [920] [SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning](https://arxiv.org/abs/2602.00743)
*Xu Pan,Zhenglin Wan,Xingrui Yu,Xianwei Zheng,Youkai Ke,Ming Sun,Rui Wang,Ziwei Wang,Ivor Tsang*

Main category: cs.RO

TL;DR: 本文提出SA-VLA框架，通过融合空间表征、设计几何感知奖励和空间条件化探索策略，解决VLA模型在强化学习微调中因空间归纳偏置退化导致的泛化性下降问题。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作中表现出强泛化能力，但RL微调常因空间分布偏移而降低鲁棒性，尤其在flow-matching策略中，空间归纳偏置易被稀疏奖励和空间无关探索侵蚀。

Method: 提出SA-VLA框架：1）将隐式空间表征与视觉token融合；2）设计反映几何进展的稠密奖励；3）引入SCAN——一种适配flow-matching动力学的空间条件化退火探索策略。

Result: 在多物体与杂乱场景操纵基准上，SA-VLA实现稳定RL微调，显著提升零样本空间泛化能力，获得更鲁棒、可迁移的行为。

Conclusion: 对representation learning、reward design和exploration三方面进行空间感知协同设计，可有效维持VLA模型的空间接地能力，提升其在分布外场景下的鲁棒性与泛化性。

Abstract: Vision-Language-Action (VLA) models exhibit strong generalization in robotic manipulation, yet reinforcement learning (RL) fine-tuning often degrades robustness under spatial distribution shifts. For flow-matching VLA policies, this degradation is closely associated with the erosion of spatial inductive bias during RL adaptation, as sparse rewards and spatially agnostic exploration increasingly favor short-horizon visual cues. To address this issue, we propose \textbf{SA-VLA}, a spatially-aware RL adaptation framework that preserves spatial grounding during policy optimization by aligning representation learning, reward design, and exploration with task geometry. SA-VLA fuses implicit spatial representations with visual tokens, provides dense rewards that reflect geometric progress, and employs \textbf{SCAN}, a spatially-conditioned annealed exploration strategy tailored to flow-matching dynamics. Across challenging multi-object and cluttered manipulation benchmarks, SA-VLA enables stable RL fine-tuning and improves zero-shot spatial generalization, yielding more robust and transferable behaviors. Code and project page are available at https://xupan.top/Projects/savla.

</details>


### [921] [Physics-informed Diffusion Mamba Transformer for Real-world Driving](https://arxiv.org/abs/2602.00808)
*Hang Zhou,Qiang Zhang,Peiran Liu,Yihao Qin,Zhaoxu Yan,Yiding Ji*

Main category: cs.RO

TL;DR: 本文提出了一种结合扩散模型、Mamba架构与端口哈密顿神经网络的新框架，用于提升自动驾驶中轨迹预测的多模态性、时序建模能力和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在自动驾驶轨迹预测中难以有效建模长时序依赖和物理规律约束。

Method: 提出Diffusion Mamba Transformer架构融合Mamba与注意力机制以增强序列上下文建模；设计Port-Hamiltonian Neural Network模块将能量守恒等物理先验嵌入扩散过程。

Result: 在标准自动驾驶基准上显著优于SOTA方法，在预测精度、物理合理性与鲁棒性三方面均有提升。

Conclusion: 该统一框架提升了轨迹预测的安全性与可靠性，为符合物理规律的生成式运动规划提供了新范式。

Abstract: Autonomous driving systems demand trajectory planners that not only model the inherent uncertainty of future motions but also respect complex temporal dependencies and underlying physical laws. While diffusion-based generative models excel at capturing multi-modal distributions, they often fail to incorporate long-term sequential contexts and domain-specific physical priors. In this work, we bridge these gaps with two key innovations. First, we introduce a Diffusion Mamba Transformer architecture that embeds mamba and attention into the diffusion process, enabling more effective aggregation of sequential input contexts from sensor streams and past motion histories. Second, we design a Port-Hamiltonian Neural Network module that seamlessly integrates energy-based physical constraints into the diffusion model, thereby enhancing trajectory predictions with both consistency and interpretability. Extensive evaluations on standard autonomous driving benchmarks demonstrate that our unified framework significantly outperforms state-of-the-art baselines in predictive accuracy, physical plausibility, and robustness, thereby advancing safe and reliable motion planning.

</details>


### [922] [SyNeT: Synthetic Negatives for Traversability Learning](https://arxiv.org/abs/2602.00814)
*Bomena Kim,Hojun Lee,Younsoo Park,Yaoyu Hu,Sebastian Scherer,Inwook Shim*

Main category: cs.RO

TL;DR: 本文提出了一种通过显式构建合成负样本（即合理但不可通行区域）来增强视觉可通行性估计的方法，适用于PU和PN学习框架，并引入了面向物体的FPR评估方式，无需额外人工标注，显著提升了模型在复杂户外环境中的鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有自监督可通行性估计方法依赖正样本和未标记样本，缺乏明确的负样本，导致难以准确识别多样的不可通行区域。

Method: 提出一种生成合成负样本（plausible but non-traversable）的方法，并将其融入基于视觉的可通行性学习；设计一种面向物体的假阳性率（FPR）评估方式，在插入合成负样本的区域分析预测结果；该策略可无缝集成到PU或PN框架中，不改变推理结构。

Result: 在公开及自采集数据集上的大量实验表明，该方法显著提升了模型在不同户外环境下的鲁棒性和泛化能力；代码与演示视频已开源。

Conclusion: 显式引入合成负样本能有效弥补PU学习中负信息缺失的问题，提升可通行性模型对非 traversable 区域的判别能力，且评估方式无需额外标注，具备实用性和可扩展性。

Abstract: Reliable traversability estimation is crucial for autonomous robots to navigate complex outdoor environments safely. Existing self-supervised learning frameworks primarily rely on positive and unlabeled data; however, the lack of explicit negative data remains a critical limitation, hindering the model's ability to accurately identify diverse non-traversable regions. To address this issue, we introduce a method to explicitly construct synthetic negatives, representing plausible but non-traversable, and integrate them into vision-based traversability learning. Our approach is formulated as a training strategy that can be seamlessly integrated into both Positive-Unlabeled (PU) and Positive-Negative (PN) frameworks without modifying inference architectures. Complementing standard pixel-wise metrics, we introduce an object-centric FPR evaluation approach that analyzes predictions in regions where synthetic negatives are inserted. This evaluation provides an indirect measure of the model's ability to consistently identify non-traversable regions without additional manual labeling. Extensive experiments on both public and self-collected datasets demonstrate that our approach significantly enhances robustness and generalization across diverse environments. The source code and demonstration videos are publicly available at the project page: https://anonymous-synet.github.io/SyNet.github.io/

</details>


### [923] [Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization](https://arxiv.org/abs/2602.02035)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: 本文提出了一种结合信息瓶颈理论与向量量化的方法，用于多智能体强化学习系统中实现带宽高效、选择性通信，并通过门控机制动态决定通信时机，在降低41.4%带宽使用的同时提升性能181.8%。


<details>
  <summary>Details</summary>
Motivation: 现实机器人应用中多智能体系统面临严重通信带宽限制，影响协调效果。

Method: 融合信息瓶颈理论与向量量化进行消息压缩与离散化，并引入基于环境与智能体状态的门控通信机制。

Result: 相比无通信基线性能提升181.8%，带宽减少41.4%；Pareto前沿分析显示其在成功率-带宽权衡曲线下面积达0.198，优于次优方法（0.142）。

Conclusion: 该方法为带宽受限场景（如机器人集群、自动驾驶车队、分布式传感器网络）提供了理论严谨且实用的多智能体通信框架。

Abstract: Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.

</details>


### [924] [Ocean Current-Harnessing Stage-Gated MPC: Monotone Cost Shaping and Speed-to-Fly for Energy-Efficient AUV Navigation](https://arxiv.org/abs/2602.00823)
*Spyridon Syntakas,Kostas Vlachos*

Main category: cs.RO

TL;DR: 本文提出了一种名为'流场利用分阶段门控模型预测控制（Current-Harnessing Stage-Gated MPC）'的新方法，通过动态评估洋流对航行的帮助程度，在MPC中选择性地引入轻量成本项，从而显著降低AUV能耗，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: AUV的实际部署受限于能源效率和续航能力，而海洋洋流蕴含可被利用的能量资源，但现有MPC方法未能有效、安全地利用洋流辅助推进。

Method: 提出流场利用分阶段门控MPC框架：引入每阶段标量衡量洋流‘帮助性’，并据此门控两类C1连续成本项——(i) 单调代价整形（MCS）项，提供有界平移能量返还；(ii) 速度匹配（STF）项，软约束水下速度匹配洋流，实现近零水相对推力‘滑翔’。所有项可即插即用集成于现有MPC设计。

Result: 在BlueROV2模型与真实洋流场的大量仿真中，该方法相比传统预测控制显著降低能耗，同时保持相近抵达时间与约束满足性。

Conclusion: 所提门控式、洋流感知的MPC方法是一种高效、安全且易于集成的节能策略，为AUV长时自主作业提供了新路径。

Abstract: Autonomous Underwater Vehicles (AUVs) are a highly promising technology for ocean exploration and diverse offshore operations, yet their practical deployment is constrained by energy efficiency and endurance. To address this, we propose Current-Harnessing Stage-Gated MPC, which exploits ocean currents via a per-stage scalar which indicates the "helpfulness" of ocean currents. This scalar is computed along the prediction horizon to gate lightweight cost terms only where the ocean currents truly aids the control goal. The proposed cost terms, that are merged in the objective function, are (i) a Monotone Cost Shaping (MCS) term, a help-gated, non-worsening modification that relaxes along-track position error and provides a bounded translational energy rebate, guaranteeing the shaped objective is never larger than a set baseline, and (ii) a speed-to-fly (STF) cost component that increases the price of thrust and softly matches ground velocity to the ocean current, enabling near zero water-relative "gliding". All terms are C1 and integrate as a plug-and-play in MPC designs. Extensive simulations with the BlueROV2 model under realistic ocean current fields show that the proposed approach achieves substantially lower energy consumption than conventional predictive control while maintaining comparable arrival times and constraint satisfaction.

</details>


### [925] [Safe Stochastic Explorer: Enabling Safe Goal Driven Exploration in Stochastic Environments and Safe Interaction with Unknown Objects](https://arxiv.org/abs/2602.00868)
*Nikhil Uday Shinde,Dylan Hirsch,Michael C. Yip,Sylvia Herbert*

Main category: cs.RO

TL;DR: 本文提出了一种名为Safe Stochastic Explorer（S.S.Explorer）的新框架，用于在具有随机动力学的未知环境中实现安全、目标驱动的探索。该方法利用高斯过程在线学习未知的安全函数，并通过其预测不确定性指导信息采集行为，提供安全违规的概率边界；同时支持离散与连续状态空间，并可扩展至多未知物体的安全物理交互。


<details>
  <summary>Details</summary>
Motivation: 现有安全控制方法（如Hamilton-Jacobi可达性、控制障碍函数）依赖已知系统动力学，而现实未知环境中的固有随机性（如火星车打滑、家用机器人推未知物体）未被现有安全探索方法充分建模，存在关键空白。

Method: 提出Safe Stochastic Explorer（S.S.Explorer）框架，结合高斯过程在线学习未知安全函数，利用其预测不确定性平衡安全与信息获取；先在离散状态空间建模，再通过可扩展松弛推广至连续状态空间；并拓展至多未知物体的安全物理交互场景。

Result: 在仿真和硬件实验中验证了该方法的有效性，实现了在不确定环境中兼顾安全性与探索目标的可靠自主导航与交互。

Conclusion: S.S.Explorer填补了随机未知环境下安全探索的理论与方法空白，为复杂不确定场景中机器人的广泛可靠自主运行提供了新路径。

Abstract: Autonomous robots operating in unstructured, safety-critical environments, from planetary exploration to warehouses and homes, must learn to safely navigate and interact with their surroundings despite limited prior knowledge. Current methods for safe control, such as Hamilton-Jacobi Reachability and Control Barrier Functions, assume known system dynamics. Meanwhile existing safe exploration techniques often fail to account for the unavoidable stochasticity inherent when operating in unknown real world environments, such as an exploratory rover skidding over an unseen surface or a household robot pushing around unmapped objects in a pantry. To address this critical gap, we propose Safe Stochastic Explorer (S.S.Explorer) a novel framework for safe, goal-driven exploration under stochastic dynamics. Our approach strategically balances safety and information gathering to reduce uncertainty about safety in the unknown environment. We employ Gaussian Processes to learn the unknown safety function online, leveraging their predictive uncertainty to guide information-gathering actions and provide probabilistic bounds on safety violations. We first present our method for discrete state space environments and then introduce a scalable relaxation to effectively extend this approach to continuous state spaces. Finally we demonstrate how this framework can be naturally applied to ensure safe physical interaction with multiple unknown objects. Extensive validation in simulation and demonstrative hardware experiments showcase the efficacy of our method, representing a step forward toward enabling reliable widespread robot autonomy in complex, uncertain environments.

</details>


### [926] [Learning When to Jump for Off-road Navigation](https://arxiv.org/abs/2602.00877)
*Zhipeng Zhao,Taimeng Fu,Shaoshu Su,Qiwei Du,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury,Chen Wang*

Main category: cs.RO

TL;DR: 本文提出了一种运动感知的可通行性（MAT）表示方法，通过将地形代价建模为速度的高斯函数，实现对复杂运动动力学的显式建模，从而提升越野导航的敏捷性与安全性。


<details>
  <summary>Details</summary>
Motivation: 低速并不总能保证越野驾驶安全，现有路径规划方法常忽略真实机器人运动动力学，仅基于位置或固定速度进行规划，难以应对如加速越沟等动态场景。

Method: 提出Motion-aware Traversability（MAT）表示，将每个地形区域的通行代价建模为速度的高斯函数；在线规划中分两阶段计算地形代价：(1) 一次前向推理预测地形相关的高斯参数；(2) 基于当前动力学推断的新速度，直接求值函数更新代价，无需重复推理。

Result: 在仿真与真实环境中验证了该系统，实现实时效率，越野导航路径绕行减少75%，同时在复杂地形中保持安全性。

Conclusion: MAT通过显式耦合运动状态与地形可通行性建模，显著提升了越野导航的性能与实用性，为动态地形适应性规划提供了新范式。

Abstract: Low speed does not always guarantee safety in off-road driving. For instance, crossing a ditch may be risky at a low speed due to the risk of getting stuck, yet safe at a higher speed with a controlled, accelerated jump. Achieving such behavior requires path planning that explicitly models complex motion dynamics, whereas existing methods often neglect this aspect and plan solely based on positions or a fixed velocity. To address this gap, we introduce Motion-aware Traversability (MAT) representation to explicitly model terrain cost conditioned on actual robot motion. Instead of assigning a single scalar score for traversability, MAT models each terrain region as a Gaussian function of velocity. During online planning, we decompose the terrain cost computation into two stages: (1) predict terrain-dependent Gaussian parameters from perception in a single forward pass, (2) efficiently update terrain costs for new velocities inferred from current dynamics by evaluating these functions without repeated inference. We develop a system that integrates MAT to enable agile off-road navigation and evaluate it in both simulated and real-world environments with various obstacles. Results show that MAT achieves real-time efficiency and enhances the performance of off-road navigation, reducing path detours by 75% while maintaining safety across challenging terrains.

</details>


### [927] [RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback](https://arxiv.org/abs/2602.00886)
*Amitesh Vatsa,Zhixian Xie,Wanxin Jin*

Main category: cs.RO

TL;DR: 本文提出RoDiF方法，通过统一马尔可夫决策过程（MDP）建模，实现无需奖励信号的直接偏好优化（DPO），提升扩散策略在人类偏好微调中的鲁棒性，尤其在偏好标签被严重污染时仍保持高性能。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人控制中表现强大，但其多步去噪结构使得基于人类偏好的微调面临根本性挑战。

Method: 提出统一MDP公式，将扩散去噪链与环境动态统一建模；在此基础上设计RoDiF方法，从几何‘假设切割’视角重新解释DPO目标，并采用保守切割策略以提升对 corrupted 偏好标签的鲁棒性，不依赖特定噪声分布假设。

Result: 在长视野操作任务上大量实验表明，RoDiF持续优于现有最优基线，能有效引导多种架构的预训练扩散策略趋向人类偏好模式，并在30%偏好标签被污染时仍保持强性能。

Conclusion: RoDiF为扩散策略的偏好驱动微调提供了鲁棒、通用且无需奖励建模的新范式。

Abstract: Diffusion policies are a powerful paradigm for robotic control, but fine-tuning them with human preferences is fundamentally challenged by the multi-step structure of the denoising process. To overcome this, we introduce a Unified Markov Decision Process (MDP) formulation that coherently integrates the diffusion denoising chain with environmental dynamics, enabling reward-free Direct Preference Optimization (DPO) for diffusion policies. Building on this formulation, we propose RoDiF (Robust Direct Fine-Tuning), a method that explicitly addresses corrupted human preferences. RoDiF reinterprets the DPO objective through a geometric hypothesis-cutting perspective and employs a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Extensive experiments on long-horizon manipulation tasks show that RoDiF consistently outperforms state-of-the-art baselines, effectively steering pretrained diffusion policies of diverse architectures to human-preferred modes, while maintaining strong performance even under 30% corrupted preference labels.

</details>


### [928] [UniMorphGrasp: Diffusion Model with Morphology-Awareness for Cross-Embodiment Dexterous Grasp Generation](https://arxiv.org/abs/2602.00915)
*Zhiyuan Wu,Xiangyu Zhang,Zhuo Chen,Jiankang Deng,Rolandos Alexandros Potamias,Shan Luo*

Main category: cs.RO

TL;DR: 本文提出UniMorphGrasp，一种基于扩散模型的跨形态灵巧抓取框架，通过将不同机械手的抓取映射到统一的人类手部规范姿态表示，并结合图结构化的手部运动学信息与物体几何信息进行条件生成，实现了对未见过手部结构的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法针对特定手部设计，难以泛化到训练分布外的未知手部形态，限制了跨形态灵巧抓取的实际部署。

Method: 提出基于扩散模型的UniMorphGrasp框架：1）将多源抓取映射至统一的人类规范手部姿态空间；2）将手部构型编码为图结构以表征运动学；3）联合物体几何进行条件生成；4）设计基于运动学层级结构的关节级监督损失函数。

Result: 在主流灵巧抓取基准上达到SOTA性能，并在未见手部结构上展现出强零样本泛化能力。

Conclusion: UniMorphGrasp为跨形态抓取提供了统一、可扩展且实用的解决方案，显著提升了模型对异构机械手的泛化性与部署灵活性。

Abstract: Cross-embodiment dexterous grasping aims to generate stable and diverse grasps for robotic hands with heterogeneous kinematic structures. Existing methods are often tailored to specific hand designs and fail to generalize to unseen hand morphologies outside the training distribution. To address these limitations, we propose \textbf{UniMorphGrasp}, a diffusion-based framework that incorporates hand morphological information into the grasp generation process for unified cross-embodiment grasp synthesis. The proposed approach maps grasps from diverse robotic hands into a unified human-like canonical hand pose representation, providing a common space for learning. Grasp generation is then conditioned on structured representations of hand kinematics, encoded as graphs derived from hand configurations, together with object geometry. In addition, a loss function is introduced that exploits the hierarchical organization of hand kinematics to guide joint-level supervision. Extensive experiments demonstrate that UniMorphGrasp achieves state-of-the-art performance on existing dexterous grasp benchmarks and exhibits strong zero-shot generalization to previously unseen hand structures, enabling scalable and practical cross-embodiment grasp deployment.

</details>


### [929] [Green-VLA: Staged Vision-Language-Action Model for Generalist Robots](https://arxiv.org/abs/2602.00919)
*I. Apanasevich,M. Artemyev,R. Babakyan,P. Fedotova,D. Grankin,E. Kupryashin,A. Misailidi,D. Nerus,A. Nutalapati,G. Sidorov,I. Efremov,M. Gerasyov,D. Pikurov,Y. Senchenko,S. Davidenko,D. Kulikov,M. Sultankin,K. Askarbek,O. Shamanin,D. Statovoy,E. Zalyaev,I. Zorin,A. Letkin,E. Rusakov,A. Silchenko,V. Vorobyov,S. Sobolnikov,A. Postnikov*

Main category: cs.RO

TL;DR: Green-VLA 是一个分阶段的视觉-语言-动作（VLA）框架，专为 Green 人形机器人实际部署设计，同时支持跨多种机器人本体的泛化能力；通过五阶段课程学习、统一的动作接口与推理时安全增强机制，在仿真与真实机器人上验证了其成功率、鲁棒性与长程任务效率的提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有VLA模型在真实机器人部署中泛化能力差、本体适配难、安全性不足的问题，尤其面向多形态机器人（人形、移动操作臂、固定基座机械臂）的统一控制需求。

Method: 提出五阶段课程学习框架（L0-L1-R0-R1-R2），结合3000小时多本体演示数据、时间对齐与质量过滤的数据处理流程，以及统一的本体感知动作接口；推理阶段引入任务进度预测、分布外检测和关节预测引导机制以提升安全性和目标选择精度。

Result: 在Simpler BRIDGE WidowX、CALVIN ABC-D仿真基准及真实Green机器人上实验表明，RL对齐显著提升任务成功率、鲁棒性与长时程任务效率。

Conclusion: Green-VLA实现了面向真实世界部署的、具备强泛化能力与安全性的统一VLA框架，验证了分阶段训练与本体感知接口对多形态机器人控制的有效性。

Abstract: We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.

</details>


### [930] [SanD-Planner: Sample-Efficient Diffusion Planner in B-Spline Space for Robust Local Navigation](https://arxiv.org/abs/2602.00923)
*Jincheng Wang,Lingfan Bao,Tong Yang,Diego Martinez Plasencia,Jianhao Jiao,Dimitrios Kanoulas*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散模型的样本高效局部规划器SanD-Planner，利用深度图像进行B样条空间中的模仿学习，并结合ESDF安全检查器，在极少量示范数据（500个episode）下实现了高成功率和零样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决高度杂乱和动态环境中可靠局部规划难的问题，尤其是大规模专家示范获取难和小样本下学习效率低两大瓶颈。

Method: 提出SanD-Planner：在截断B样条空间中进行深度图像驱动的扩散式模仿学习，并集成基于ESDF的安全检查器（含显式间隙与完成时间度量），避免价值函数学习负担。

Result: 仅用500个episode（仅为基线数据量的0.25%）训练，在仿真杂乱环境和室内场景中分别达到90.1%和72.0%成功率；并在2D/3D真实实验中实现零样本迁移；开源数据集与预训练模型。

Conclusion: SanD-Planner通过紧凑参数空间建模与显式安全约束设计，显著提升小样本下的规划可靠性与泛化性，为实际部署提供了新范式。

Abstract: The challenge of generating reliable local plans has long hindered practical applications in highly cluttered and dynamic environments. Key fundamental bottlenecks include acquiring large-scale expert demonstrations across diverse scenes and improving learning efficiency with limited data. This paper proposes SanD-Planner, a sample-efficient diffusion-based local planner that conducts depth image-based imitation learning within the clamped B-spline space. By operating within this compact space, the proposed algorithm inherently yields smooth outputs with bounded prediction errors over local supports, naturally aligning with receding-horizon execution. Integration of an ESDF-based safety checker with explicit clearance and time-to-completion metrics further reduces the training burden associated with value-function learning for feasibility assessment. Experiments show that training with $500$ episodes (merely $0.25\%$ of the demonstration scale used by the baseline), SanD-Planner achieves state-of-the-art performance on the evaluated open benchmark, attaining success rates of $90.1\%$ in simulated cluttered environments and $72.0\%$ in indoor simulations. The performance is further proven by demonstrating zero-shot transferability to realistic experimentation in both 2D and 3D scenes. The dataset and pre-trained models will also be open-sourced.

</details>


### [931] [Minimal Footprint Grasping Inspired by Ants](https://arxiv.org/abs/2602.00935)
*Mohamed Sorour,Barbara Webb*

Main category: cs.RO

TL;DR: 本文受蚂蚁前肢抓取能力启发，设计了一种低成本、高鲁棒性的仿生夹爪，具备高摩擦垫、低摩擦毛和柔性单节跗节结构，在散乱堆叠环境中实现了高成功率的单物体抓取。


<details>
  <summary>Details</summary>
Motivation: 受蚂蚁在杂乱环境中高效抓取物体能力的启发，特别是其前肢（尤其是跗节）的高摩擦微结构、覆毛及柔性欠驱动尖端特性。

Method: 抽象蚂蚁前肢关键特征，设计长而细的夹爪腿，集成高摩擦抓取垫、低摩擦毛和类跗节的单段柔性结构，并进行实验验证。

Result: 该夹爪在抓取多种消费级单个物体时成功率100%，并在密集杂乱场景中有效实现单物体拣选。

Conclusion: 该工作推动了抓取技术发展，并揭示了昆虫体表毛状结构与跗节柔性的关键力学作用。

Abstract: Ants are highly capable of grasping objects in clutter, and we have recently observed that this involves substantial use of their forelegs. The forelegs, more specifically the tarsi, have high friction microstructures (setal pads), are covered in hairs, and have a flexible under-actuated tip. Here we abstract these features to test their functional advantages for a novel low-cost gripper design, suitable for bin-picking applications. In our implementation, the gripper legs are long and slim, with high friction gripping pads, low friction hairs and single-segment tarsus-like structure to mimic the insect's setal pads, hairs, and the tarsi's interactive compliance. Experimental evaluation shows this design is highly robust for grasping a wide variety of individual consumer objects, with all grasp attempts successful. In addition, we demonstrate this design is effective for picking single objects from dense clutter, a task at which ants also show high competence. The work advances grasping technology and shed new light on the mechanical importance of hairy structures and tarsal flexibility in insects.

</details>


### [932] [CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining](https://arxiv.org/abs/2602.00937)
*I-Chun Arthur Liu,Krzysztof Choromanski,Sandy Huang,Connor Schenck*

Main category: cs.RO

TL;DR: 本文提出CLAMP框架，通过点云和机器人动作进行3D对比学习预训练，提升行为克隆策略在机器人操作任务中的精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D图像表征的行为克隆方法缺乏对物体与场景3D空间信息的建模，难以支持高精度操作。

Method: 提出CLAMP：利用RGB-D生成融合点云，重渲染多视角四通道图像（含深度与3D坐标），结合动态手腕视角；采用对比学习将3D几何/位置信息与动作模式关联；预训练Diffusion Policy以提升下游微调效率。

Result: 在六个仿真任务和五个真实世界任务上均超越当前最优基线，显著提升少样本微调效率与策略性能。

Conclusion: 3D多视角动作条件下的对比预训练可有效增强机器人操作策略的几何理解与泛化能力，为行为克隆提供更鲁棒的表征基础。

Abstract: Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.

</details>


### [933] [Meanshift Shape Formation Control Using Discrete Mass Distribution](https://arxiv.org/abs/2602.00980)
*Yichen Cai,Yuan Gao,Pengpeng Li,Wei Wang,Guibin Sun,Jinhu Lü*

Main category: cs.RO

TL;DR: 本文提出了一种完全去中心化的基于分布的集群控制策略，通过离散质量分布函数和去中心化均值漂移控制律，实现复杂形状构建与集群规模自适应。


<details>
  <summary>Details</summary>
Motivation: 现有密度分布方法在复杂形状表征和去中心化实现方面存在实际挑战，需发展兼顾二者的新策略。

Method: 1）提出定义在采样点集上的离散质量分布函数建模集群构型；2）设计基于质量估计反馈的去中心化均值漂移控制律；3）构建分布式质量估计器实现全局质量估计收敛。

Result: 理论证明质量估计可渐近收敛至真实全局值；仿真与实物实验验证了复杂形状形成能力及对集群规模变化的良好适应性。

Conclusion: 所提方法克服了连续密度函数建模复杂形状的困难，实现了真正去中心化、可扩展且鲁棒的集群形态控制。

Abstract: The density-distribution method has recently become a promising paradigm owing to its adaptability to variations in swarm size. However, existing studies face practical challenges in achieving complex shape representation and decentralized implementation. This motivates us to develop a fully decentralized, distribution-based control strategy with the dual capability of forming complex shapes and adapting to swarm-size variations. Specifically, we first propose a discrete mass-distribution function defined over a set of sample points to model swarm formation. In contrast to the continuous density-distribution method, our model eliminates the requirement for defining continuous density functions-a task that is difficult for complex shapes. Second, we design a decentralized meanshift control law to coordinate the swarm's global distribution to fit the sample-point distribution by feeding back mass estimates. The mass estimates for all sample points are achieved by the robots in a decentralized manner via the designed mass estimator. It is shown that the mass estimates of the sample points can asymptotically converge to the true global values. To validate the proposed strategy, we conduct comprehensive simulations and real-world experiments to evaluate the efficiency of complex shape formation and adaptability to swarm-size variations.

</details>


### [934] [Geometry-Aware Sampling-Based Motion Planning on Riemannian Manifolds](https://arxiv.org/abs/2602.00992)
*Phone Thiha Kyaw,Jonathan Kelly*

Main category: cs.RO

TL;DR: 本文提出了一种直接在黎曼流形上运行的采样式运动规划框架，通过中点近似和基于黎曼自然梯度的一阶回缩局部规划器，在保持可扩展性的同时提升几何保真度，显著降低了轨迹代价。


<details>
  <summary>Details</summary>
Motivation: 许多机器人运动规划问题中，任务目标与物理约束使构型空间具有非欧几里得几何结构，但现有规划器多使用忽略该结构的欧氏距离，导致路径质量下降。

Method: 提出基于中点的黎曼测地距离三阶精度近似方法，并设计利用一阶回缩与黎曼自然梯度引导的局部规划器，构建面向黎曼流形的采样式规划框架。

Result: 在二连杆平面臂、7自由度Franka机械臂（动能度量）及SE(2)刚体非完整规划等实验中，所提方法生成的轨迹成本始终低于欧氏基线与经典数值测地求解器。

Conclusion: 该框架成功弥合了高维系统中数值测地求解器（精度高但不可扩展）与传统采样规划器（可扩展但几何失真）之间的鸿沟，兼顾效率与几何保真性。

Abstract: In many robot motion planning problems, task objectives and physical constraints induce non-Euclidean geometry on the configuration space, yet many planners operate using Euclidean distances that ignore this structure. We address the problem of planning collision-free motions that minimize length under configuration-dependent Riemannian metrics, corresponding to geodesics on the configuration manifold. Conventional numerical methods for computing such paths do not scale well to high-dimensional systems, while sampling-based planners trade scalability for geometric fidelity. To bridge this gap, we propose a sampling-based motion planning framework that operates directly on Riemannian manifolds. We introduce a computationally efficient midpoint-based approximation of the Riemannian geodesic distance and prove that it matches the true Riemannian distance with third-order accuracy. Building on this approximation, we design a local planner that traces the manifold using first-order retractions guided by Riemannian natural gradients. Experiments on a two-link planar arm and a 7-DoF Franka manipulator under a kinetic-energy metric, as well as on rigid-body planning in $\mathrm{SE}(2)$ with non-holonomic motion constraints, demonstrate that our approach consistently produces lower-cost trajectories than Euclidean-based planners and classical numerical geodesic-solver baselines.

</details>


### [935] [HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving](https://arxiv.org/abs/2602.00993)
*Weizhe Tang,Junwei You,Jiaxi Liu,Zhaoyi Wang,Rui Gan,Zilin Huang,Feng Wei,Bin Ran*

Main category: cs.RO

TL;DR: 本文提出HERMES框架，通过引入长尾风险线索和三模态融合，提升端到端自动驾驶在混合交通长尾场景下的安全与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶模型在长尾混合交通场景（如人车混行、弱势道路使用者共存）中难以保障安全与准确决策。

Method: 提出HERMES框架：1）基于基础模型辅助的标注流程，构建结构化的长尾场景上下文与规划上下文；2）设计三模态驾驶模块，融合多视角感知、历史运动线索与语义引导。

Result: 在真实世界长尾数据集上，HERMES显著优于代表性端到端及VLM驱动基线；消融实验验证各核心组件的互补贡献。

Conclusion: 显式注入长尾风险线索并融合多源信息，可有效提升端到端自动驾驶在复杂不确定长尾场景中的鲁棒性与安全性。

Abstract: End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.

</details>


### [936] [Offline Discovery of Interpretable Skills from Multi-Task Trajectories](https://arxiv.org/abs/2602.01018)
*Chongyu Zhu,Mithun Vanniasinghe,Jiayu Chen,Chi-Guhn Lee*

Main category: cs.RO

TL;DR: 本文提出LOKI框架，通过三阶段方法在无显式奖励或子任务标注的离线多任务数据中发现可复用技能，并实现分层模仿学习，在D4RL Kitchen基准上表现优异，且发现的技能具有语义意义和组合性。


<details>
  <summary>Details</summary>
Motivation: 解决在长时程、多任务离线数据中缺乏显式奖励或子任务标注情况下，如何发现可复用技能这一核心挑战。

Method: LOKI为三阶段端到端学习框架：第一阶段利用带弱任务标签的对齐增强向量量化变分自编码器进行粗粒度任务感知宏分割；第二阶段通过自监督序列模型与迭代聚类完成细粒度微分割与技能边界整合；第三阶段基于精确技能边界，在基于选项的分层策略框架中构建策略，并学习终止条件beta以实现显式技能切换。

Result: 在D4RL Kitchen基准上取得高成功率，优于标准分层模仿学习基线；所发现技能语义合理、符合人类直觉，并具备组合性，能成功串联解决新出现的未见任务。

Conclusion: LOKI提供了一种有效、可扩展的离线技能发现与分层模仿学习方法，兼具实用性与可解释性。

Abstract: Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task.

</details>


### [937] [Learning Adaptive Cross-Embodiment Visuomotor Policy with Contrastive Prompt Orchestration](https://arxiv.org/abs/2602.01040)
*Yuhang Zhang,Chao Yan,Jiaxi Yu,Jiaping Xiao,Mir Feroskhan*

Main category: cs.RO

TL;DR: 本文提出ContrAstive Prompt Orchestration (CAPO)，通过对比式提示学习与自适应提示编排，提升具身智能体在跨形态场景下的视觉运动策略泛化能力与样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以分离任务相关特征与域特异性变化（如光照、视场角、旋转等），导致样本效率低、在未见环境中易崩溃。

Method: 提出CAPO框架：1）混合对比学习策略联合视觉、时序动作与文本目标，构建可学习提示池，每个提示编码细粒度域因子；2）基于观测动态聚合提示，自适应构建最优状态表征，屏蔽无关干扰。

Result: CAPO在样本效率和渐近性能上显著超越SOTA；在光照、视场角、旋转等剧烈环境与物理变化的零样本迁移任务中表现优异。

Conclusion: CAPO为跨形态视觉运动策略自适应提供了有效且鲁棒的解决方案。

Abstract: Learning adaptive visuomotor policies for embodied agents remains a formidable challenge, particularly when facing cross-embodiment variations such as diverse sensor configurations and dynamic properties. Conventional learning approaches often struggle to separate task-relevant features from domain-specific variations (e.g., lighting, field-of-view, and rotation), leading to poor sample efficiency and catastrophic failure in unseen environments. To bridge this gap, we propose ContrAstive Prompt Orchestration (CAPO), a novel approach for learning visuomotor policies that integrates contrastive prompt learning and adaptive prompt orchestration. For prompt learning, we devise a hybrid contrastive learning strategy that integrates visual, temporal action, and text objectives to establish a pool of learnable prompts, where each prompt induces a visual representation encapsulating fine-grained domain factors. Based on these learned prompts, we introduce an adaptive prompt orchestration mechanism that dynamically aggregates these prompts conditioned on current observations. This enables the agent to adaptively construct optimal state representations by identifying dominant domain factors instantaneously. Consequently, the policy optimization is effectively shielded from irrelevant interference, preventing the common issue of overfitting to source domains. Extensive experiments demonstrate that CAPO significantly outperforms state-of-the-art baselines in sample efficiency and asymptotic performance. Crucially, it exhibits superior zero-shot adaptation across unseen target domains characterized by drastic environmental (e.g., illumination) and physical shifts (e.g., field-of-view and rotation), validating its effectiveness as a viable solution for cross-embodiment visuomotor policy adaptation.

</details>


### [938] [LLM-Based Behavior Tree Generation for Construction Machinery](https://arxiv.org/abs/2602.01041)
*Akinosuke Tsutsumi,Tomoya Itsuka,Yuichiro Kasahara,Tomoya Kouno,Kota Akinari,Genki Yamauchi,Daisuke Endo,Taro Abe,Takeshi Hashimoto,Keiji Nagatani,Ryo Kurazume*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型（LLM）的自动化行为树（BT）生成工作流，用于解决建筑工地多机协同作业中的可扩展性与安全性问题，通过引入同步标志和结构化模板，在仿真与真实施工场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 土方工程自动化需求上升，但劳动力老龄化与技能流失严重；现有ROS2-TMS框架依赖人工设计的行为树，难以扩展至异构机械协同场景。

Method: 提出两阶段LLM驱动工作流：1）高层规划阶段由LLM生成同步标志；2）基于结构化模板与系统数据库参数自动生成行为树，确保安全性。

Result: 在仿真环境中完成验证，并在真实施工场景中成功演示多机协同作业，证明了方法的可行性与实用性。

Conclusion: 该LLM增强型BT生成方法显著提升了施工自动化系统的可扩展性与安全性，为土木工程智能化提供了新路径。

Abstract: Earthwork operations are facing an increasing demand, while workforce aging and skill loss create a pressing need for automation. ROS2-TMS for Construction, a Cyber-Physical System framework designed to coordinate construction machinery, has been proposed for autonomous operation; however, its reliance on manually designed Behavior Trees (BTs) limits scalability, particularly in scenarios involving heterogeneous machine cooperation. Recent advances in large language models (LLMs) offer new opportunities for task planning and BT generation. However, most existing approaches remain confined to simulations or simple manipulators, with relatively few applications demonstrated in real-world contexts, such as complex construction sites involving multiple machines. This paper proposes an LLM-based workflow for BT generation, introducing synchronization flags to enable safe and cooperative operation. The workflow consists of two steps: high-level planning, where the LLM generates synchronization flags, and BT generation using structured templates. Safety is ensured by planning with parameters stored in the system database. The proposed method is validated in simulation and further demonstrated through real-world experiments, highlighting its potential to advance automation in civil engineering.

</details>


### [939] [A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation](https://arxiv.org/abs/2602.01067)
*Fanqi Lin,Kushal Arora,Jean Mercat,Haruki Nishimura,Paarth Shah,Chen Xu,Mengchao Zhang,Mark Zolotas,Maya Angeles,Owen Pfannenstiehl,Andrew Beaulieu,Jose Barreiros*

Main category: cs.RO

TL;DR: 本文通过大规模实证研究，系统评估了五种共训练数据模态（视觉-语言数据、密集语言标注轨迹、跨形态机器人数据、人类视频、离散动作标记）及其训练策略对大行为模型泛化能力的影响，发现视觉-语言与跨形态机器人数据共训练显著提升分布外泛化、新任务适应和语言遵循能力，而离散动作标记无明显增益；多模态组合带来累积收益，并能支持快速微调适应长时程灵巧操作任务。


<details>
  <summary>Details</summary>
Motivation: 大型行为模型受限于机器人数据覆盖不足导致泛化能力有限，亟需在不增加昂贵数据采集的前提下扩展数据覆盖；现有共训练方法缺乏对不同数据模态与策略影响的系统性理解。

Method: 开展大规模实证研究，对比五种共训练数据模态（标准视觉-语言数据、密集语言标注轨迹、跨形态机器人数据、人类视频、离散机器人动作标记）及单/多阶段训练策略；使用4000小时机器人与人类操作数据和5000万视觉-语言样本训练视觉-语言-动作策略；在58,000次仿真与2,835次真实世界rollout中评估89个策略。

Result: 视觉-语言与跨形态机器人数据共训练显著提升分布偏移鲁棒性、未见任务泛化与语言遵循能力；离散动作标记无显著增益；多模态组合带来累积性能提升并支持快速微调适应长时程灵巧任务；纯机器人数据训练损害视觉语言模型骨干的视语理解能力，而有效共训练可恢复该能力；基于共训练数据学习的思维链显式条件动作生成未提升仿真基准性能。

Conclusion: 共训练模态的选择与组合对构建可扩展通用机器人策略至关重要；视觉-语言与跨形态机器人数据是最有效的共训练模态，为构建高性能、强泛化能力的大规模机器人策略提供了实用指导。

Abstract: Large behavior models have shown strong dexterous manipulation capabilities by extending imitation learning to large-scale training on multi-task robot data, yet their generalization remains limited by the insufficient robot data coverage. To expand this coverage without costly additional data collection, recent work relies on co-training: jointly learning from target robot data and heterogeneous data modalities. However, how different co-training data modalities and strategies affect policy performance remains poorly understood. We present a large-scale empirical study examining five co-training data modalities: standard vision-language data, dense language annotations for robot trajectories, cross-embodiment robot data, human videos, and discrete robot action tokens across single- and multi-phase training strategies. Our study leverages 4,000 hours of robot and human manipulation data and 50M vision-language samples to train vision-language-action policies. We evaluate 89 policies over 58,000 simulation rollouts and 2,835 real-world rollouts. Our results show that co-training with forms of vision-language and cross-embodiment robot data substantially improves generalization to distribution shifts, unseen tasks, and language following, while discrete action token variants yield no significant benefits. Combining effective modalities produces cumulative gains and enables rapid adaptation to unseen long-horizon dexterous tasks via fine-tuning. Training exclusively on robot data degrades the visiolinguistic understanding of the vision-language model backbone, while co-training with effective modalities restores these capabilities. Explicitly conditioning action generation on chain-of-thought traces learned from co-training data does not improve performance in our simulation benchmark. Together, these results provide practical guidance for building scalable generalist robot policies.

</details>


### [940] [Estimating Force Interactions of Deformable Linear Objects from their Shapes](https://arxiv.org/abs/2602.01085)
*Qi Jing Chen,Shilin Shan,Timothy Bretl,Quang-Cuong Pham*

Main category: cs.RO

TL;DR: 本文提出了一种仅通过观测可变形线状物体（DLO）形状来检测和估计外部作用力的解析方法，适用于机器人与线缆非末端接触的交互场景，无需额外力传感器，基于静力学平衡建模并求解线性方程组，在仿真和真实实验中验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 在机器人-线缆交互任务中，接触常发生在机器人本体而非末端执行器，准确识别此类交互对安全高效的轨迹规划、防止线缆损伤及规避危险至关重要；而现有方法多依赖昂贵的外置力/力矩传感器或假设接触仅发生在末端，存在局限性。

Method: 利用深度相机获取线缆形状，在线缆处于或接近静态平衡的假设下，通过推导的几何-力学一致性条件，建立并求解基于整条线缆受力与力矩平衡的线性方程组，从而估计外部作用力的位置与大小。

Result: 该方法在仿真中实现了高精度估计，并在真实世界实验中多个典型交互场景下成功实现了准确的力与位置估计。

Conclusion: 仅依赖线缆形状观测即可实现非末端接触下的外部力检测与估计，为无额外传感器的柔性物体交互感知提供了可行、高效且解析性强的新途径。

Abstract: This work introduces an analytical approach for detecting and estimating external forces acting on deformable linear objects (DLOs) using only their observed shapes. In many robot-wire interaction tasks, contact occurs not at the end-effector but at other points along the robot's body. Such scenarios arise when robots manipulate wires indirectly (e.g., by nudging) or when wires act as passive obstacles in the environment. Accurately identifying these interactions is crucial for safe and efficient trajectory planning, helping to prevent wire damage, avoid restricted robot motions, and mitigate potential hazards. Existing approaches often rely on expensive external force-torque sensor or that contacts occur at the end-effector for accurate force estimation. Using wire shape information acquired from a depth camera and under the assumption that the wire is in or near its static equilibrium, our method estimates both the location and magnitude of external forces without additional prior knowledge. This is achieved by exploiting derived consistency conditions and solving a system of linear equations based on force-torque balance along the wire. The approach was validated through simulation, where it achieved high accuracy, and through real-world experiments, where accurate estimation was demonstrated in selected interaction scenarios.

</details>


### [941] [Failure-Aware Bimanual Teleoperation via Conservative Value Guided Assistance](https://arxiv.org/abs/2602.01092)
*Peng Zhou,Zhongxuan Li,Jinsong Wu,Jiaming Qi,Jun Hu,David Navarro-Alarcon,Jia Pan,Lihua Xie,Shiyao Zhang,Zeqing Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种基于价值引导、失败感知的双臂遥操作框架，利用离线异构遥操作数据（含成功与失败样本）训练保守价值学习模型来评估任务可行性，并通过关节空间阻抗接口提供合规性力反馈辅助，既提升成功率又降低操作员负担，同时保持人类持续主导权。


<details>
  <summary>Details</summary>
Motivation: 高精度遥操作受限于严格的成功容差和复杂的接触动力学，导致操作员在部分可观测条件下难以预判即将发生的失败。

Method: 采用保守价值学习（Conservative Value Learning）从包含成功与失败的异构离线遥操作数据中学习任务可行性（即保守成功分数），在线阶段结合该分数与学习到的修正动作方向，通过主端关节空间阻抗接口实现连续、合规的力反馈辅助。

Result: 在接触丰富的操作任务中，相比传统遥操作和共享自主基线方法，该框架显著提升了任务成功率并降低了操作员工作负荷。

Conclusion: 保守价值学习是一种有效机制，可将失败感知能力嵌入双边遥操作系统，在保障人类持续权威的同时实现风险敏感的实时辅助。

Abstract: Teleoperation of high-precision manipulation is con-strained by tight success tolerances and complex contact dy-namics, which make impending failures difficult for human operators to anticipate under partial observability. This paper proposes a value-guided, failure-aware framework for bimanual teleoperation that provides compliant haptic assistance while pre-serving continuous human authority. The framework is trained entirely from heterogeneous offline teleoperation data containing both successful and failed executions. Task feasibility is mod-eled as a conservative success score learned via Conservative Value Learning, yielding a risk-sensitive estimate that remains reliable under distribution shift. During online operation, the learned success score regulates the level of assistance, while a learned actor provides a corrective motion direction. Both are integrated through a joint-space impedance interface on the master side, yielding continuous guidance that steers the operator away from failure-prone actions without overriding intent. Experimental results on contact-rich manipulation tasks demonstrate improved task success rates and reduced operator workload compared to conventional teleoperation and shared-autonomy baselines, indicating that conservative value learning provides an effective mechanism for embedding failure awareness into bilateral teleoperation. Experimental videos are available at https://www.youtube.com/watch?v=XDTsvzEkDRE

</details>


### [942] [StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating](https://arxiv.org/abs/2602.01100)
*Hang Wu,Tongqing Chen,Jiasen Wang,Xiaotao Li,Lu Fang*

Main category: cs.RO

TL;DR: StreamVLA提出双系统架构，分离高层规划与低层控制，通过'锁存-门控'机制实现高效长时程机器人操作，显著降低延迟并提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在每个时间步都进行冗余多模态推理，导致高延迟和目标不稳定，难以支持长时程机器人操作。

Method: 提出StreamVLA双系统架构，引入'锁存-门控'机制：仅在子任务切换时触发慢思考（生成文本指令与视觉目标想象），将完成状态作为时不变目标锚点；稳态执行时锁定高层意图，驱动Flow Matching动作头，跳过72%时间步的自回归解码。

Result: 在LIBERO基准上达到98.5%成功率，真实干扰场景中鲁棒恢复能力强，推理延迟比全推理基线降低48%。

Conclusion: StreamVLA通过分层抽象与计算调制，在保持高性能的同时大幅提升推理效率与鲁棒性，为长时程机器人操作提供了新范式。

Abstract: Long-horizon robotic manipulation requires bridging the gap between high-level planning (System 2) and low-level control (System 1). Current Vision-Language-Action (VLA) models often entangle these processes, performing redundant multimodal reasoning at every timestep, which leads to high latency and goal instability. To address this, we present StreamVLA, a dual-system architecture that unifies textual task decomposition, visual goal imagination, and continuous action generation within a single parameter-efficient backbone. We introduce a "Lock-and-Gated" mechanism to intelligently modulate computation: only when a sub-task transition is detected, the model triggers slow thinking to generate a textual instruction and imagines the specific visual completion state, rather than generic future frames. Crucially, this completion state serves as a time-invariant goal anchor, making the policy robust to execution speed variations. During steady execution, these high-level intents are locked to condition a Flow Matching action head, allowing the model to bypass expensive autoregressive decoding for 72% of timesteps. This hierarchical abstraction ensures sub-goal focus while significantly reducing inference latency. Extensive evaluations demonstrate that StreamVLA achieves state-of-the-art performance, with a 98.5% success rate on the LIBERO benchmark and robust recovery in real-world interference scenarios, achieving a 48% reduction in latency compared to full-reasoning baselines.

</details>


### [943] [KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV](https://arxiv.org/abs/2602.01115)
*Zhihao Chen,Yiyuan Ge,Ziyang Wang*

Main category: cs.RO

TL;DR: 本文提出KAN-We-Flow，一种基于流匹配的轻量级 visuomotor 策略，融合RWKV与KAN架构，显著减少参数量并提升3D操作性能。


<details>
  <summary>Details</summary>
Motivation: 扩散策略虽建模能力强但推理效率低；现有流匹配方法仍依赖大型UNet，难以部署于资源受限机器人。

Method: 提出RWKV-KAN模块：RWKV负责高效时序/通道混合以传播任务上下文，GroupKAN进行样条基、分组式非线性动作映射校准；引入动作一致性正则化（ACR）损失，通过欧拉外推对齐预测轨迹与专家示范。

Result: 相比UNet基线参数减少86.8%，运行速度快，且在Adroit、Meta-World和DexArt基准上达到SOTA成功率。

Conclusion: KAN-We-Flow验证了轻量、高效、高表达力的流匹配策略在真实机器人视觉运动控制中的可行性与优越性。

Abstract: Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \href{https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\textcolor{red}{link}}

</details>


### [944] [UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors](https://arxiv.org/abs/2602.01153)
*Zhuo Chen,Fei Ni,Kaiyao Luo,Zhiyuan Wu,Xuyang Zhang,Emmanouil Spyrakos-Papastavridis,Lorenzo Jamone,Nathan F. Lepora,Jiankang Deng,Shan Luo*

Main category: cs.RO

TL;DR: 本文提出UniForce框架，通过联合建模逆向与正向动力学，并利用力平衡和图像重建约束，在多种异构触觉传感器间学习统一的力空间表征，实现零样本迁移的力感知机器人操作。


<details>
  <summary>Details</summary>
Motivation: 触觉传感器种类繁多（如光学、磁性等），其原理、形态和材料差异导致数据采集、标定和模型训练难以通用，限制了力感知策略学习的可扩展性。

Method: 提出UniForce框架：联合建模图像到力（逆动力学）和力到图像（正动力学），引入力平衡约束和图像重建损失；利用传感器-物体-传感器直接接触构建静力平衡下的力配对数据，避免依赖外部力/力矩传感器。

Result: 在GelSight、TacTip、uSkin等多种异构触觉传感器上显著提升力估计精度；支持零样本迁移至下游任务；成功应用于VTLA模型实现跨传感器协同的机器人擦拭任务。

Conclusion: UniForce实现了跨异构触觉传感器的统一表征学习，无需重训练即可零样本迁移，为可扩展、通用的力感知机器人操作提供了新范式。

Abstract: Force sensing is essential for dexterous robot manipulation, but scaling force-aware policy learning is hindered by the heterogeneity of tactile sensors. Differences in sensing principles (e.g., optical vs. magnetic), form factors, and materials typically require sensor-specific data collection, calibration, and model training, thereby limiting generalisability. We propose UniForce, a novel unified tactile representation learning framework that learns a shared latent force space across diverse tactile sensors. UniForce reduces cross-sensor domain shift by jointly modeling inverse dynamics (image-to-force) and forward dynamics (force-to-image), constrained by force equilibrium and image reconstruction losses to produce force-grounded representations. To avoid reliance on expensive external force/torque (F/T) sensors, we exploit static equilibrium and collect force-paired data via direct sensor--object--sensor interactions, enabling cross-sensor alignment with contact force. The resulting universal tactile encoder can be plugged into downstream force-aware robot manipulation tasks with zero-shot transfer, without retraining or finetuning. Extensive experiments on heterogeneous tactile sensors including GelSight, TacTip, and uSkin, demonstrate consistent improvements in force estimation over prior methods, and enable effective cross-sensor coordination in Vision-Tactile-Language-Action (VTLA) models for a robotic wiping task. Code and datasets will be released.

</details>


### [945] [Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models](https://arxiv.org/abs/2602.01166)
*Shuanghao Bai,Jing Lyu,Wanqi Zhou,Zhe Li,Dakai Wang,Lei Xing,Xiaoguang Zhao,Pengwei Wang,Zhongyuan Wang,Cheng Chi,Badong Chen,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出LaRA-VLA，一种将多模态链式思维（CoT）推理内化为连续潜在表示的统一视觉-语言-动作（VLA）框架，避免了推理过程中的显式文本生成，显著降低推理延迟并提升实时具身控制性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖离散、显式的链式思维推理，导致高推理开销，且与连续感知与控制不匹配。

Method: 提出LaRA-VLA框架，将多模态CoT推理嵌入连续潜在空间；设计课程学习训练范式，从显式图文CoT监督逐步过渡到潜在推理建模，并使潜在推理动态适配动作生成；构建两个结构化CoT数据集。

Result: 在仿真基准和真实机器人长视野操作任务上，LaRA-VLA持续优于SOTA VLA方法，推理延迟最高降低90%。

Conclusion: 将链式思维内化为潜在表示是一种高效、实用的具身智能推理范式，适用于实时控制场景。

Abstract: Vision-Language-Action (VLA) models benefit from chain-of-thought (CoT) reasoning, but existing approaches incur high inference overhead and rely on discrete reasoning representations that mismatch continuous perception and control. We propose Latent Reasoning VLA (\textbf{LaRA-VLA}), a unified VLA framework that internalizes multi-modal CoT reasoning into continuous latent representations for embodied action. LaRA-VLA performs unified reasoning and prediction in latent space, eliminating explicit CoT generation at inference time and enabling efficient, action-oriented control. To realize latent embodied reasoning, we introduce a curriculum-based training paradigm that progressively transitions from explicit textual and visual CoT supervision to latent reasoning, and finally adapts latent reasoning dynamics to condition action generation. We construct two structured CoT datasets and evaluate LaRA-VLA on both simulation benchmarks and long-horizon real-robot manipulation tasks. Experimental results show that LaRA-VLA consistently outperforms state-of-the-art VLA methods while reducing inference latency by up to 90\% compared to explicit CoT-based approaches, demonstrating latent reasoning as an effective and efficient paradigm for real-time embodied control. Project Page: \href{https://loveju1y.github.io/Latent-Reasoning-VLA/}{LaRA-VLA Website}.

</details>


### [946] [SPOT: Spatio-Temporal Obstacle-free Trajectory Planning for UAVs in an Unknown Dynamic Environment](https://arxiv.org/abs/2602.01189)
*Astik Srivastava,Thomas J Chackenkulam. Bitla Bhanu Teja,Antony Thomas,Madhava Krishna*

Main category: cs.RO

TL;DR: 本文提出了一种面向未知动态环境的四维时空规划框架，结合视觉驱动的安全飞行走廊生成与轨迹优化，实现无地图、实时响应的四旋翼无人机避障导航。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼在未知且含动态障碍物环境中进行反应式运动规划的难题，克服传统基于建图融合方法计算开销大、适应性差的问题。

Method: 采用4D时空规划器，集成视觉驱动的安全飞行走廊（SFC）生成与轨迹优化；通过视觉分割与跟踪识别并区分动静态障碍物；引入备份规划模块以应对死锁场景下的突发避障需求。

Result: 在仿真与真实硬件实验中均验证了方法有效性，并在与前沿方法的对比基准测试中展现出显著优势。

Conclusion: 所提无地图、感知直驱的框架显著提升了四旋翼在动态未知环境中的实时避障能力与鲁棒性。

Abstract: We address the problem of reactive motion planning for quadrotors operating in unknown environments with dynamic obstacles. Our approach leverages a 4-dimensional spatio-temporal planner, integrated with vision-based Safe Flight Corridor (SFC) generation and trajectory optimization. Unlike prior methods that rely on map fusion, our framework is mapless, enabling collision avoidance directly from perception while reducing computational overhead. Dynamic obstacles are detected and tracked using a vision-based object segmentation and tracking pipeline, allowing robust classification of static versus dynamic elements in the scene. To further enhance robustness, we introduce a backup planning module that reactively avoids dynamic obstacles when no direct path to the goal is available, mitigating the risk of collisions during deadlock situations. We validate our method extensively in both simulation and real-world hardware experiments, and benchmark it against state-of-the-art approaches, showing significant advantages for reactive UAV navigation in dynamic, unknown environments.

</details>


### [947] [SkySim: A ROS2-based Simulation Environment for Natural Language Control of Drone Swarms using Large Language Models](https://arxiv.org/abs/2602.01226)
*Aditya Shibu,Marah Saleh,Mohamed Al-Musleh,Nidhal Abdulaziz*

Main category: cs.RO

TL;DR: 本文提出SkySim框架，利用大语言模型（如Gemini 3.5 Pro）实现自然语言控制无人机集群，并通过人工势场（APF）安全滤波器保障物理可行性与实时安全性，已在Gazebo+ROS2仿真中验证其准确性、实时性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统静态控制方法适应性差，而大语言模型虽支持自然语言交互，却因缺乏物理约束易生成不安全轨迹；亟需一种兼顾高层语义理解与底层安全执行的架构。

Method: 构建基于ROS2和Gazebo的SkySim仿真框架，将LLM（Gemini 3.5 Pro）用于高阶空间指令解析（如'组成圆形'），输出空间航点；引入人工势场（APF）安全滤波器，在线修正航点以满足避障、运动学限制与电子围栏要求，运行频率达20 Hz。

Result: 在3、10、30架Crazyflie无人机仿真中，几何原语空间推理准确率达100%，全程无碰撞，系统具备实时性与良好可扩展性，并支持非专家用户迭代调优行为。

Conclusion: SkySim成功桥接了大语言模型的认知能力与机器人系统的物理安全性，为自然语言驱动的多无人机集群提供了可行、安全、易用的仿真基础，下一步将拓展至真实硬件平台。

Abstract: Unmanned Aerial Vehicle (UAV) swarms offer versatile applications in logistics, agriculture, and surveillance, yet controlling them requires expert knowledge for safety and feasibility. Traditional static methods limit adaptability, while Large Language Models (LLMs) enable natural language control but generate unsafe trajectories due to lacking physical grounding. This paper introduces SkySim, a ROS2-based simulation framework in Gazebo that decouples LLM high-level planning from low-level safety enforcement. Using Gemini 3.5 Pro, SkySim translates user commands (e.g., "Form a circle") into spatial waypoints, informed by real-time drone states. An Artificial Potential Field (APF) safety filter applies minimal adjustments for collision avoidance, kinematic limits, and geo-fencing, ensuring feasible execution at 20 Hz. Experiments with swarms of 3, 10, and 30 Crazyflie drones validate spatial reasoning accuracy (100% across tested geometric primitives), real-time collision prevention, and scalability. SkySim empowers non-experts to iteratively refine behaviors, bridging AI cognition with robotic safety for dynamic environments. Future work targets hardware integration.

</details>


### [948] [Reinforcement Learning for Active Perception in Autonomous Navigation](https://arxiv.org/abs/2602.01266)
*Grzegorz Malczyk,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种端到端强化学习框架，使无人机在未知复杂环境中自主导航时，能主动控制机载相机以提升态势感知能力，并通过体素信息度量耦合避障运动规划与信息驱动的相机控制。


<details>
  <summary>Details</summary>
Motivation: 解决复杂未知环境中自主导航下的主动感知挑战，提升机器人对环境的态势感知能力。

Method: 提出端到端强化学习框架，输入包括机器人状态、当前深度帧及基于短时深度历史构建的局部几何表征；在导航奖励中引入体素信息度量，以联合优化避障运动与主动相机控制。

Result: 实验表明该方法相比固定相机基线更安全，并能激发内在探索行为。

Conclusion: 所提框架能有效平衡目标导向运动与探索性感知，实现更鲁棒和安全的自主导航。

Abstract: This paper addresses the challenge of active perception within autonomous navigation in complex, unknown environments. Revisiting the foundational principles of active perception, we introduce an end-to-end reinforcement learning framework in which a robot must not only reach a goal while avoiding obstacles, but also actively control its onboard camera to enhance situational awareness. The policy receives observations comprising the robot state, the current depth frame, and a particularly local geometry representation built from a short history of depth readings. To couple collision-free motion planning with information-driven active camera control, we augment the navigation reward with a voxel-based information metric. This enables an aerial robot to learn a robust policy that balances goal-directed motion with exploratory sensing. Extensive evaluation demonstrates that our strategy achieves safer flight compared to using fixed, non-actuated camera baselines while also inducing intrinsic exploratory behaviors.

</details>


### [949] [TriphiBot: A Triphibious Robot Combining FOC-based Propulsion with Eccentric Design](https://arxiv.org/abs/2602.01385)
*Xiangyu Li,Mingwei Lai,Mengke Zhang,Junxiao Lin,Tiancheng Lai,Junping Zhi,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 本文提出了一种极简设计的三栖机器人，兼具空中、陆地和水下运动能力，通过偏心质心设计提升地面/海床推进效率，并采用基于磁场定向控制（FOC）的统一推进系统与混合非线性模型预测控制（HNMPC）-PID控制器，实现跨域稳定运动与无缝切换。


<details>
  <summary>Details</summary>
Motivation: 现有两栖或三栖机器人普遍存在机械结构复杂、推进效率低、跨介质控制困难等问题，难以兼顾多域运动性能与工程实用性。

Method: 提出一种基于四旋翼+双被动轮的极简三栖结构；引入偏心质心设计以自然对齐推力与运动方向；采用基于FOC的统一推进系统解决空气与水介质间扭矩匹配与双向推力控制问题；建立基于生存环境与地面支撑视角的动力学模型，并设计HNMPC-PID混合控制器。

Result: 实验验证了该机器人在空、陆、水三域的自主运动能力及跨模态（如空-陆、陆-水）无缝切换能力；推进系统展现出高效率、强适应性与快速双向响应特性。

Conclusion: 该 minimalist 三栖机器人设计在保持结构简洁的同时，显著提升了多域运动效率与控制鲁棒性，为跨域作业机器人提供了新思路与可行技术路径。

Abstract: Triphibious robots capable of multi-domain motion and cross-domain transitions are promising to handle complex tasks across diverse environments. However, existing designs primarily focus on dual-mode platforms, and some designs suffer from high mechanical complexity or low propulsion efficiency, which limits their application. In this paper, we propose a novel triphibious robot capable of aerial, terrestrial, and aquatic motion, by a minimalist design combining a quadcopter structure with two passive wheels, without extra actuators. To address inefficiency of ground-support motion (moving on land/seabed) for quadcopter based designs, we introduce an eccentric Center of Gravity (CoG) design that inherently aligns thrust with motion, enhancing efficiency without specialized mechanical transformation designs. Furthermore, to address the drastic differences in motion control caused by different fluids (air and water), we develop a unified propulsion system based on Field-Oriented Control (FOC). This method resolves torque matching issues and enables precise, rapid bidirectional thrust across different mediums. Grounded in the perspective of living condition and ground support, we analyse the robot's dynamics and propose a Hybrid Nonlinear Model Predictive Control (HNMPC)-PID control system to ensure stable multi-domain motion and seamless transitions. Experimental results validate the robot's multi-domain motion and cross-mode transition capability, along with the efficiency and adaptability of the proposed propulsion system.

</details>


### [950] [Instance-Guided Unsupervised Domain Adaptation for Robotic Semantic Segmentation](https://arxiv.org/abs/2602.01389)
*Michele Antonazzi,Lorenzo Signorelli,Matteo Luperto,Nicola Basilico*

Main category: cs.RO

TL;DR: 本文提出一种利用三维体素地图和基础模型零样本实例分割能力生成多视角一致伪标签的方法，用于机器人部署时的无监督域自适应语义分割，显著提升性能且无需目标域真值标签。


<details>
  <summary>Details</summary>
Motivation: 语义分割网络在部署环境与训练数据分布不同时性能下降，而现有基于多视角一致性的无监督域自适应方法对跨视角实例级不一致敏感。

Method: 从三维体素地图生成多视角一致伪标签，并利用基础模型的零样本实例分割能力进行实例级一致性优化，再以优化后的伪标签进行自监督微调。

Result: 在真实世界数据上实验表明，该方法持续优于基于多视角一致性的最先进无监督域自适应基线方法。

Conclusion: 所提方法能有效缓解域偏移，在无需目标域任何真值标签的前提下，实现机器人感知系统在部署时的高效自适应。

Abstract: Semantic segmentation networks, which are essential for robotic perception, often suffer from performance degradation when the visual distribution of the deployment environment differs from that of the source dataset on which they were trained. Unsupervised Domain Adaptation (UDA) addresses this challenge by adapting the network to the robot's target environment without external supervision, leveraging the large amounts of data a robot might naturally collect during long-term operation. In such settings, UDA methods can exploit multi-view consistency across the environment's map to fine-tune the model in an unsupervised fashion and mitigate domain shift. However, these approaches remain sensitive to cross-view instance-level inconsistencies. In this work, we propose a method that starts from a volumetric 3D map to generate multi-view consistent pseudo-labels. We then refine these labels using the zero-shot instance segmentation capabilities of a foundation model, enforcing instance-level coherence. The refined annotations serve as supervision for self-supervised fine-tuning, enabling the robot to adapt its perception system at deployment time. Experiments on real-world data demonstrate that our approach consistently improves performance over state-of-the-art UDA baselines based on multi-view consistency, without requiring any ground-truth labels in the target domain.

</details>


### [951] [Sem-NaVAE: Semantically-Guided Outdoor Mapless Navigation via Generative Trajectory Priors](https://arxiv.org/abs/2602.01429)
*Gonzalo Olguin,Javier Ruiz-del-Solar*

Main category: cs.RO

TL;DR: 本文提出了一种无需地图的全局导航方法，结合条件变分自编码器（CVAE）生成多样轨迹与轻量级视觉语言模型（VLM）进行开词汇语义分割，实现基于自然语言指令的实时轨迹选择与导航。


<details>
  <summary>Details</summary>
Motivation: 解决户外场景中依赖高精度地图、缺乏语义理解与实时适应能力的导航瓶颈，提升导航系统的灵活性与泛化性。

Method: 融合CVAE生成多样的候选轨迹，利用轻量级VLM进行开词汇语义分割，依据自然语言描述对轨迹打分筛选，并由先进局部规划器输出速度指令执行。

Result: 在真实户外环境中验证了该方法，实时性能优越，轨迹多样性高，导航效果优于当前最优方法。

Conclusion: 该地图无关、语言驱动、端到端可实时运行的导航框架，为开放环境下的具身智能提供了新范式。

Abstract: This work presents a mapless global navigation approach for outdoor applications. It combines the exploratory capacity of conditional variational autoencoders (CVAEs) to generate trajectories and the semantic segmentation capabilities of a lightweight visual language model (VLM) to select the trajectory to execute. Open-vocabulary segmentation is used to score and select the generated trajectories based on natural language, and a state-of-the-art local planner executes velocity commands. One of the key features of the proposed approach is its ability to generate a large variability of trajectories and to select them and navigate in real-time. The approach was validated through real-world outdoor navigation experiments, achieving superior performance compared to state-of-the-art methods. A video showing an experimental run of the system can be found in https://www.youtube.com/watch?v=i3R5ey5O2yk.

</details>


### [952] [Towards a Novel Wearable Robotic Vest for Hemorrhage Suppression](https://arxiv.org/abs/2602.01448)
*Harshith Jella,Pejman Kheradmand,Joseph Klein,Behnam Moradkhani,Yash Chitalia*

Main category: cs.RO

TL;DR: 本文提出了一种新型可变形环形机器人系统，用于在紧急场景（包括空间站）中控制严重出血，通过可调形状的环机制和充气气囊实现对不同解剖部位的适配性加压止血，实验验证了其有效性，但存在对复杂解剖区域贴合度不足的局限。


<details>
  <summary>Details</summary>
Motivation: 应对紧急场景（如空间站）中严重出血的快速、自主、适应性强的止血需求，尤其针对非肢体部位（腹部、背部、颈部等）缺乏有效机械止血设备的问题。

Method: 设计具有形状可调（圆-椭圆切换）的环机制，开发多种柔性臂以提升解剖适配性；集成兼容该机制的可充气环与气囊系统以实现均匀恒定压力；通过弯曲刚度测试和数字秤测力实验评估性能；最终在模拟伤员模型上开展止血功能验证。

Result: 成功实现环机制形状调节与气囊协同加压；实验证明不同臂构型具备可控弯曲刚度，气囊系统能稳定输出压力；在模拟伤员模型上有效控制模拟出血；但受限于气囊部分充放气条件，难以完全贴合复杂解剖曲面。

Conclusion: 该机器人系统为多环境应急止血提供了新思路，具备初步实用性，但需进一步优化环形机构的形变范围与自适应能力以覆盖更复杂人体部位。

Abstract: This paper introduces a novel robotic system designed to manage severe bleeding in emergency scenarios, including unique environments like space stations. The robot features a shape-adjustable "ring mechanism", transitioning from a circular to an elliptical configuration to adjust wound coverage across various anatomical regions. We developed various arms for this ring mechanism with varying flexibilities to improve adaptability when applied to non-extremities of the body (abdomen, back, neck, etc.). To apply equal and constant pressure across the wound, we developed an inflatable ring and airbag balloon that are compatible with this shape-changing ring mechanism. A series of experiments focused on evaluating various ring arm configurations to characterize their bending stiffness. Subsequent experiments measured the force exerted by the airbag balloon system using a digital scale. Despite its promising performance, certain limitations related to coverage area are identified. The shape-changing effect of the device is limited to scenarios involving partially inflated or deflated airbag balloons, and cannot fully conform to complex anatomical regions. Finally, the device was tested on casualty simulation kits, where it successfully demonstrated its ability to control simulated bleeding.

</details>


### [953] [TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching](https://arxiv.org/abs/2602.01501)
*Minwoo Jung,Nived Chebrolu,Lucas Carvalho de Lima,Haedam Oh,Maurice Fallon,Ayoung Kim*

Main category: cs.RO

TL;DR: 本文提出TreeLoc，一种面向森林环境的LiDAR全局定位框架，利用树干及其胸径（DBH）构建场景表示，结合树分布直方图（TDH）与2D三角形描述符实现粗-精匹配，并通过两步几何验证完成6自由度位姿估计，在森林基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 森林环境中GPS信号弱、LiDAR数据重复性高、遮挡严重且结构复杂，传统城市导向的定位方法因依赖独特结构特征而失效，亟需面向森林的鲁棒定位方案。

Method: TreeLoc以树干及其DBH为基本单元建模场景；通过树轴对齐至统一参考系，用树分布直方图（TDH）实现粗粒度地点识别，再用2D三角形描述符进行细粒度匹配；最后采用两步几何验证完成6-DoF位姿估计。

Result: 在多样化森林基准测试中，TreeLoc显著优于现有基线方法，实现高精度全局定位；消融实验验证了各模块的有效性；并展示了其在长期森林管理中的应用潜力（如基于紧凑全局树数据库的描述符检索）。

Conclusion: TreeLoc是一种专为森林复杂环境设计的可靠LiDAR定位框架，兼顾鲁棒性与精度，开源代码已发布，有望推动林业机器人与长期生态监测的发展。

Abstract: Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.

</details>


### [954] [RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots](https://arxiv.org/abs/2602.01515)
*Humphrey Munn,Brendan Tidd,Peter Bohm,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: 本文提出了RAPT，一种轻量级、自监督的部署时监控器，用于50Hz人形机器人控制，通过学习名义执行的概率时空流形并评估执行时的预测偏差，实现可靠的在线OOD检测和可解释的Sim-to-Real失配度量，并结合梯度时序显著性和LLM推理进行零样本故障根因分析。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在高频率控制中不兼容、低误报率下校准差、或缺乏可解释性，难以满足人形机器人实际部署需求。

Method: RAPT基于仿真数据学习名义执行的概率时空流形，实时计算各维度预测偏差作为校准信号；并构建融合梯度时序显著性与大语言模型推理的后验根因分析流程。

Result: 在大规模仿真中，RAPT在0.5%事件级误报率下TPR较最强基线提升37%；在真实G1人形机器人上TPR提升12.5%，根因分类准确率达75%（仅用本体感知数据）。

Conclusion: RAPT实现了高可靠性、低误报、强可解释性的部署监控，显著提升了人形机器人Sim-to-Real迁移的安全性与可调试性。

Abstract: Deploying learned control policies on humanoid robots is challenging: policies that appear robust in simulation can execute confidently in out-of-distribution (OOD) states after Sim-to-Real transfer, leading to silent failures that risk hardware damage. Although anomaly detection can mitigate these failures, prior methods are often incompatible with high-rate control, poorly calibrated at the extremely low false-positive rates required for practical deployment, or operate as black boxes that provide a binary stop signal without explaining why the robot drifted from nominal behavior. We present RAPT, a lightweight, self-supervised deployment-time monitor for 50Hz humanoid control. RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. This yields (i) reliable online OOD detection under strict false-positive constraints and (ii) a continuous, interpretable measure of Sim-to-Real mismatch that can be tracked over time to quantify how far deployment has drifted from training. Beyond detection, we introduce an automated post-hoc root-cause analysis pipeline that combines gradient-based temporal saliency derived from RAPT's reconstruction objective with LLM-based reasoning conditioned on saliency and joint kinematics to produce semantic failure diagnoses in a zero-shot setting. We evaluate RAPT on a Unitree G1 humanoid across four complex tasks in simulation and on physical hardware. In large-scale simulation, RAPT improves True Positive Rate (TPR) by 37% over the strongest baseline at a fixed episode-level false positive rate of 0.5%. On real-world deployments, RAPT achieves a 12.5% TPR improvement and provides actionable interpretability, reaching 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.

</details>


### [955] [Co-Design of Rover Wheels and Control using Bayesian Optimization and Rover-Terrain Simulations](https://arxiv.org/abs/2602.01535)
*Huzaifa Mustafa Unjhawala,Khizar Shaikh,Luning Bakke,Radu Serban,Dan Negrut*

Main category: cs.RO

TL;DR: 本文提出了一种基于贝叶斯优化的轮式机器人车轮几何与转向控制器协同设计框架，利用高效连续介质模型（CRM）进行全车闭环仿真，在可变形地形上实现多目标优化（速度、跟踪误差、能耗），显著缩短计算时间，并在硬件实验中验证了仿真结果的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统离散元法（DEM）建模成本高，难以支撑全车级、闭环、耦合轮-车-控制器的优化；亟需一种高效高保真仿真方法支持机械设计与控制策略的联合优化。

Method: 采用贝叶斯优化框架，结合 continuum-representation model（CRM）进行高保真、全车、闭合回路仿真；同步或分步优化轮式参数（半径、宽度、履齿特征）与转向PID增益；以速度、跟踪误差、能耗为多目标。

Result: 在3000次全车仿真中，优化任务耗时5–9天（相较此前DEM流程数月大幅提速）；同步优化策略优于分步策略；初步实车测试表明仿真优选轮型性能排序在物理平台上保持一致。

Conclusion:  scalable高保真仿真（CRM）可切实支撑越野车辆在可变形地形上的车轮与控制器协同设计，无需依赖计算代价过高的DEM；开源仿真基础设施促进可复现性与后续研究。

Abstract: While simulation is vital for optimizing robotic systems, the cost of modeling deformable terrain has long limited its use in full-vehicle studies of off-road autonomous mobility. For example, Discrete Element Method (DEM) simulations are often confined to single-wheel tests, which obscures coupled wheel-vehicle-controller interactions and prevents joint optimization of mechanical design and control. This paper presents a Bayesian optimization framework that co-designs rover wheel geometry and steering controller parameters using high-fidelity, full-vehicle closed-loop simulations on deformable terrain. Using the efficiency and scalability of a continuum-representation model (CRM) for terramechanics, we evaluate candidate designs on trajectories of varying complexity while towing a fixed load. The optimizer tunes wheel parameters (radius, width, and grouser features) and steering PID gains under a multi-objective formulation that balances traversal speed, tracking error, and energy consumption. We compare two strategies: simultaneous co-optimization of wheel and controller parameters versus a sequential approach that decouples mechanical and control design. We analyze trade-offs in performance and computational cost. Across 3,000 full-vehicle simulations, campaigns finish in five to nine days, versus months with the group's earlier DEM-based workflow. Finally, a preliminary hardware study suggests the simulation-optimized wheel designs preserve relative performance trends on the physical rover. Together, these results show that scalable, high-fidelity simulation can enable practical co-optimization of wheel design and control for off-road vehicles on deformable terrain without relying on prohibitively expensive DEM studies. The simulation infrastructure (scripts and models) is released as open source in a public repository to support reproducibility and further research.

</details>


### [956] [UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning](https://arxiv.org/abs/2602.01536)
*Shuai Liu,Siheng Ren,Xiaoyao Zhu,Quanmin Liang,Zefeng Li,Qiang Li,Xin Hu,Kai Huang*

Main category: cs.RO

TL;DR: UniDWM是一种统一的驾驶世界模型，通过多方面表征学习，在潜空间中联合建模场景结构与动态演化，支持感知、预测与规划的一致推理，并以变分自编码器（VAE）为理论基础。


<details>
  <summary>Details</summary>
Motivation: 复杂驾驶环境中需兼顾场景几何、外观与动态信息，现有模型难以实现可靠高效的规划，亟需一个物理可解释、多任务一致的世界模型。

Method: 提出UniDWM：1）联合重建路径恢复场景几何与纹理；2）基于条件扩散Transformer在潜空间预测未来世界演化；3）将模型形式化为VAE变体，为多方面表征提供理论支撑。

Result: 在轨迹规划、4D重建与生成任务上显著优于基线方法，验证了多方面世界表征对统一驾驶智能的有效性。

Conclusion: UniDWM构建了结构与动态感知的物理可解释潜空间，为感知-预测-规划闭环提供了统一基础，推动自动驾驶向通用驾驶智能演进。

Abstract: Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.

</details>


### [957] [A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation](https://arxiv.org/abs/2602.01632)
*Chuizheng Kong,Yunho Cho,Wonsuhk Jung,Idris Wibowo,Parth Shinde,Sundhar Vinodh-Sangeetha,Long Kiu Chung,Zhenyang Chen,Andrew Mattei,Advaith Nidumukkala,Alexander Elias,Danfei Xu,Taylor Higgins,Shreyas Kousik*

Main category: cs.RO

TL;DR: 本文提出SEW-Mimic方法，将人体运动重定向问题重构为上/下臂朝向对齐问题，实现闭式几何求解，具备最优性保证、高速推理（3kHz）和高精度，适用于主流7自由度人形机器人，并提升遥操作成功率与策略学习质量。


<details>
  <summary>Details</summary>
Motivation: 现有运动重定向方法以末端执行器位置和姿态匹配为目标，导致运动不自然、延迟高、工作空间受限；需更高效、鲁棒且计算友好的替代方案。

Method: 将重定向建模为基于肩-肘-腕（SEW）关键点的上臂与下臂朝向对齐问题，提出闭式几何求解算法SEW-Mimic；支持任意关键点输入源，兼容7-DOF双臂及人形机器人，并集成安全过滤器避免自碰撞。

Result: 在计算速度（3 kHz CPU推理）、重定向精度、遥操作任务成功率及策略学习数据质量方面均优于现有方法；可作为即插即用模块加速全身重定向，并通过硬件实验验证实用性。

Conclusion: SEW-Mimic是一种高效、通用、实用的运动重定向基础方法，显著提升了双臂人形机器人遥操作的性能与可扩展性，是机器人操作与学习的关键构建模块。

Abstract: Retargeting human motion to robot poses is a practical approach for teleoperating bimanual humanoid robot arms, but existing methods can be suboptimal and slow, often causing undesirable motion or latency. This is due to optimizing to match robot end-effector to human hand position and orientation, which can also limit the robot's workspace to that of the human. Instead, this paper reframes retargeting as an orientation alignment problem, enabling a closed-form, geometric solution algorithm with an optimality guarantee. The key idea is to align a robot arm to a human's upper and lower arm orientations, as identified from shoulder, elbow, and wrist (SEW) keypoints; hence, the method is called SEW-Mimic. The method has fast inference (3 kHz) on standard commercial CPUs, leaving computational overhead for downstream applications; an example in this paper is a safety filter to avoid bimanual self-collision. The method suits most 7-degree-of-freedom robot arms and humanoids, and is agnostic to input keypoint source. Experiments show that SEW-Mimic outperforms other retargeting methods in computation time and accuracy. A pilot user study suggests that the method improves teleoperation task success. Preliminary analysis indicates that data collected with SEW-Mimic improves policy learning due to being smoother. SEW-Mimic is also shown to be a drop-in way to accelerate full-body humanoid retargeting. Finally, hardware demonstrations illustrate SEW-Mimic's practicality. The results emphasize the utility of SEW-Mimic as a fundamental building block for bimanual robot manipulation and humanoid robot teleoperation.

</details>


### [958] [AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act](https://arxiv.org/abs/2602.01662)
*Pengyuan Guo,Zhonghao Mai,Zhengtong Xu,Kaidi Zhang,Heng Zhang,Zichen Miao,Arash Ajoudani,Zachary Kingston,Qiang Qiu,Yu She*

Main category: cs.RO

TL;DR: 本文提出了AgenticLab，一个面向开放世界操作任务的模型无关机器人代理平台和基准测试，旨在评估大型视觉语言模型（VLMs）在真实机器人、非结构化环境中的长周期闭环操作能力，并揭示了离线视觉语言测试无法捕捉的关键失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的机器人操作方法缺乏统一可比的评估平台，且多数评测依赖仿真、特权状态或定制化设置，难以反映真实世界中长周期、闭环、非结构化环境下的操作能力。

Method: 构建了AgenticLab——一个模型无关的机器人代理平台，提供包含感知、任务分解、在线验证与重规划的闭环代理流水线，并在此平台上对前沿VLM代理在真实机器人上执行开放世界操作任务进行系统性评测。

Result: 发现了离线视觉语言评测（如VQA、静态图像理解）无法暴露的关键失败模式，包括多步推理中的指代一致性崩溃、遮挡与场景变化下的物体定位失效、以及空间推理不足导致的操作不可靠。

Conclusion: AgenticLab为通用机器人代理研究提供了可复现、真实世界的评测基准；强调需超越静态图像理解，在动态、闭环、具身交互中重新评估和提升VLM的操控能力。

Abstract: Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups' setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.

</details>


### [959] [Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications](https://arxiv.org/abs/2602.01679)
*Raghavasimhan Sankaranarayanan,Paul Stuart,Nicholas Ahn,Arno Sungarian,Yash Chitalia*

Main category: cs.RO

TL;DR: 本文提出了一种全自动机器人系统，用于手术器械的分拣与结构化装盘，通过自建数据集训练混合感知模型，并结合定制化机械臂与防碰撞装盘算法，显著提升了SPD部门装配阶段的准确性、安全性与效率。


<details>
  <summary>Details</summary>
Motivation: 手动检查和准备手术器械托盘耗时长、易出错，且易导致污染和器械损坏，亟需自动化解决方案以提升SPD部门的安全性、一致性和处理效率。

Method: 构建包含31种手术器械、6975张标注图像的自定义数据集；采用YOLO12进行目标检测，级联ResNet模型实现细粒度分类；集成校准视觉模块、6自由度Staubli机械臂及双电磁吸盘，并设计基于规则的装盘算法与3D打印物理隔离结构（隔板与支架）以减少运输中碰撞。

Result: 实验表明该系统具有高感知精度，并在器械间碰撞率上相比人工装盘实现统计显著降低。

Conclusion: 该系统是迈向SPD全流程自动化的重要可扩展第一步，有助于提升手术准备的安全性、一致性并缩短处理时间。

Abstract: The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.

</details>


### [960] [GSR: Learning Structured Reasoning for Embodied Manipulation](https://arxiv.org/abs/2602.01693)
*Kewei Hu,Michael Zhang,Wei Ying,Tianhao Liu,Guoqiang Hao,Zimeng Li,Wanchan Yu,Jiajian Jing,Fangwen Chen,Hanwen Kang*

Main category: cs.RO

TL;DR: 本文提出了一种名为Grounded Scene-graph Reasoning（GSR）的结构化推理范式，通过在语义接地的场景图上显式建模世界状态演化，提升具身智能体在长时程操作任务中的空间一致性、因果依赖与目标约束能力；并构建了Manip-Cognition-1.6M数据集支持训练，在多个基准和真实机器人任务中显著提升零样本泛化与任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能体在长时程操作任务中表现不佳，主因是任务推理隐式嵌入于高维潜在表示中，难以分离任务结构与感知变化。

Method: 提出Grounded Scene-graph Reasoning（GSR），将世界状态演化建模为语义接地的场景图上的状态转移；引入Manip-Cognition-1.6M大规模多任务监督数据集，联合监督世界理解、动作规划与目标解析。

Result: 在RLBench、LIBERO、GSR-benchmark及真实机器人任务上，GSR显著优于基于提示（prompting）的基线方法，尤其在零样本泛化与长时程任务完成率方面提升明显。

Conclusion: 显式的、物理接地的世界状态表征是一种关键的归纳偏置，可支撑可扩展的具身推理。

Abstract: Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.

</details>


### [961] [Tilt-Ropter: A Novel Hybrid Aerial and Terrestrial Vehicle with Tilt Rotors and Passive Wheels](https://arxiv.org/abs/2602.01700)
*Ruoyu Wang,Xuchen Liu,Zongzhou Wu,Zixuan Guo,Wendi Ding,Ben M. Chen*

Main category: cs.RO

TL;DR: 本文提出了一种名为Tilt-Ropter的新型混合空地车辆（HATV），结合倾转旋翼与被动轮，实现节能多模态运动；采用非线性模型预测控制（NMPC）与冗余执行器分配策略，并引入实时外力矩估计算法提升地面接触鲁棒性；实验验证了其无缝空地切换、低跟踪误差及地面模式下功耗降低92.8%。


<details>
  <summary>Details</summary>
Motivation: 现有欠驱动混合空地车辆机动性与环境适应性受限，亟需一种能高效切换空地模式、兼顾能量效率与鲁棒性的全驱动平台。

Method: 设计全驱动Tilt-Ropter结构（倾转旋翼+被动轮），构建非线性动力学模型；开发支持接触约束的NMPC轨迹跟踪控制器；设计控制分配模块以优化执行器能耗；提出实时外部扰动（力/力矩）估计算法增强地面接触鲁棒性。

Result: 仿真与实物实验验证了无缝空地转换与轨迹跟踪能力；空地两模式下跟踪误差均较低；地面运动功耗降低92.8%。

Conclusion: Tilt-Ropter通过全驱动设计、NMPC控制、冗余分配与外力估计，显著提升了混合空地系统的能效、适应性与鲁棒性，适用于大范围、能源受限的长时任务场景。

Abstract: In this work, we present Tilt-Ropter, a novel hybrid aerial-terrestrial vehicle (HATV) that combines tilt rotors with passive wheels to achieve energy-efficient multi-mode locomotion. Unlike existing under-actuated HATVs, the fully actuated design of Tilt-Ropter enables decoupled force and torque control, greatly enhancing its mobility and environmental adaptability. A nonlinear model predictive controller (NMPC) is developed to track reference trajectories and handle contact constraints across locomotion modes, while a dedicated control allocation module exploits actuation redundancy to achieve energy-efficient control of actuators. Additionally, to enhance robustness during ground contact, we introduce an external wrench estimation algorithm that estimates environmental interaction forces and torques in real time. The system is validated through both simulation and real-world experiments, including seamless air-ground transitions and trajectory tracking. Results show low tracking errors in both modes and highlight a 92.8% reduction in power consumption during ground locomotion, demonstrating the system's potential for long-duration missions across large-scale and energy-constrained environments.

</details>


### [962] [Uncertainty-Aware Non-Prehensile Manipulation with Mobile Manipulators under Object-Induced Occlusion](https://arxiv.org/abs/2602.01731)
*Jiwoo Hwang,Taegeun Yang,Jeil Jeong,Minsung Yoon,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: CURA-PPO是一种基于强化学习的非抓取式操纵框架，通过建模部分可观测性下的不确定性（如碰撞可能性分布）来应对物体遮挡传感器视野的问题，结合置信图提升感知可靠性，显著提升在严重遮挡场景下的操纵成功率。


<details>
  <summary>Details</summary>
Motivation: 非抓取式操纵中，被操纵物体遮挡机载传感器视野，导致不可观测区域，易引发碰撞，亟需能主动感知并处理不确定性的方法。

Method: 提出CURA-PPO框架，利用强化学习建模碰撞可能性的概率分布，从中提取风险与不确定性；不确定性驱动主动感知行为，实现操纵与信息采集协同；引入置信图表征观测可靠性，增强对遮挡环境的鲁棒性。

Result: 在不同物体尺寸和障碍物构型下实验表明，CURA-PPO成功率较基线最高提升3倍，且策略能自主应对遮挡。

Conclusion: CURA-PPO为仅依赖机载传感的自主操纵任务提供了实用、安全、鲁棒的解决方案，尤其适用于杂乱环境。

Abstract: Non-prehensile manipulation using onboard sensing presents a fundamental challenge: the manipulated object occludes the sensor's field of view, creating occluded regions that can lead to collisions. We propose CURA-PPO, a reinforcement learning framework that addresses this challenge by explicitly modeling uncertainty under partial observability. By predicting collision possibility as a distribution, we extract both risk and uncertainty to guide the robot's actions. The uncertainty term encourages active perception, enabling simultaneous manipulation and information gathering to resolve occlusions. When combined with confidence maps that capture observation reliability, our approach enables safe navigation despite severe sensor occlusion. Extensive experiments across varying object sizes and obstacle configurations demonstrate that CURA-PPO achieves up to 3X higher success rates than the baselines, with learned behaviors that handle occlusions. Our method provides a practical solution for autonomous manipulation in cluttered environments using only onboard sensing.

</details>


### [963] [RFS: Reinforcement learning with Residual flow steering for dexterous manipulation](https://arxiv.org/abs/2602.01789)
*Entong Su,Tyler Westenbroek,Anusha Nagabandi,Abhishek Gupta*

Main category: cs.RO

TL;DR: 本文提出了一种名为残差流引导（RFS）的数据高效强化学习框架，用于适配预训练的生成式策略（特别是基于流匹配的策略），通过联合优化残差动作和潜在噪声分布，在保留全局探索能力的同时实现局部执行误差的快速修正。


<details>
  <summary>Details</summary>
Motivation: 预训练的生成式行为克隆策略（如基于扩散模型或流匹配的方法）在部署时泛化能力有限，需额外微调；而微调需兼顾全局探索与局部纠错能力。

Method: 提出残差流引导（RFS）框架：在预训练流匹配策略基础上，联合优化一个残差动作项和一个潜在噪声分布，分别支持局部精细化调整与潜在空间的全局探索。

Result: 在灵巧操作任务中验证了RFS的有效性，实现了仿真与真实世界中对预训练基础策略的高效微调。

Conclusion: RFS是一种兼顾表达力与适应效率的新范式，为生成式模仿学习策略的部署适配提供了实用且数据高效的解决方案。

Abstract: Imitation learning has emerged as an effective approach for bootstrapping sequential decision-making in robotics, achieving strong performance even in high-dimensional dexterous manipulation tasks. Recent behavior cloning methods further leverage expressive generative models, such as diffusion models and flow matching, to represent multimodal action distributions. However, policies pretrained in this manner often exhibit limited generalization and require additional fine-tuning to achieve robust performance at deployment time. Such adaptation must preserve the global exploration benefits of pretraining while enabling rapid correction of local execution errors.We propose \emph{Residual Flow Steering} (RFS), a data-efficient reinforcement learning framework for adapting pretrained generative policies. RFS steers a pretrained flow-matching policy by jointly optimizing a residual action and a latent noise distribution, enabling complementary forms of exploration: local refinement through residual corrections and global exploration through latent-space modulation. This design allows efficient adaptation while retaining the expressive structure of the pretrained policy.We demonstrate the effectiveness of RFS on dexterous manipulation tasks, showing efficient fine-tuning both in simulation and in real-world settings when adapting pretrained base policies.Project website:https://weirdlabuw.github.io/rfs.

</details>


### [964] [From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models](https://arxiv.org/abs/2602.01811)
*Wentao Zhang,Aolan Sun,Wentao Mo,Xiaoyang Qu,Yuxin Zheng,Jianzong Wang*

Main category: cs.RO

TL;DR: 本文提出了一种无需训练的轻量级框架VLA-SCT，通过自校正控制循环解决视觉-语言-动作（VLA）模型在抓取任务中动作偏差和任务完成判断不准的问题，在LIBERO基准测试中显著提升了操作成功率和任务完成准确性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在抓取任务中存在动作空间偏差导致失败，且无法可靠识别任务完成，造成冗余动作和超时错误。

Method: 提出训练无关的轻量级框架VLA-SCT，构建自校正控制循环，融合数据驱动的动作优化与基于条件逻辑的任务终止机制。

Result: 在LIBERO所有数据集上均优于基线方法，显著提升精细操作任务成功率，并确保准确的任务完成判定。

Conclusion: VLA-SCT增强了VLA智能体在复杂非结构化环境中的鲁棒性与可靠性，推动其实际部署。

Abstract: While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments.

</details>


### [965] [Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models](https://arxiv.org/abs/2602.01834)
*Siqi Wen,Shu Yang,Shaopeng Fu,Jingfeng Zhang,Lijie Hu,Di Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于概念的字典学习框架，用于VLA模型推理时的安全控制，通过稀疏可解释字典识别并抑制有害概念方向，在多个基准上显著降低攻击成功率，且无需重训练、模型无关、即插即用。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型安全防御方法（如对齐、过滤、提示加固）介入过晚或模态错误，无法有效防护多模态融合表征中的安全隐患，尤其当文本越狱可能引发物理不安全行为时。

Method: 构建基于隐藏层激活的稀疏、可解释概念字典，识别有害概念方向，并在推理时采用阈值干预机制抑制或阻断不安全激活。

Result: 在Libero-Harm、BadRobot、RoboPair和IS-Bench上，攻击成功率降低超70%，同时保持任务成功率；框架即插即用、模型无关、无需重训练。

Conclusion: 这是首个面向具身系统的推理时概念级安全方法，提升了VLA模型的可解释性与安全部署能力。

Abstract: Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models.

</details>


### [966] [Vision-only UAV State Estimation for Fast Flights Without External Localization Systems: A2RL Drone Racing Finalist Approach](https://arxiv.org/abs/2602.01860)
*Filip Novák,Matěj Petrlík,Matej Novosad,Parakh M. Gupta,Robert Pěnička,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种基于单目RGB相机和IMU的机载视觉惯性状态估计算法，通过融合VIO、地标测量与IMU，并引入新型数学漂移模型校正全部VIO状态，在GNSS拒止、高动态、高杂波环境中实现高精度无人机状态估计。


<details>
  <summary>Details</summary>
Motivation: 在GNSS拒止、环境杂乱且需激进机动的高速飞行场景下，现有VIO方法存在漂移未校正、硬件复杂、状态估计不全等问题，导致快速机动时误差显著。

Method: 融合单目RGB相机、IMU与机载地标测量系统；构建并利用新型数学漂移模型，对VIO的位置、姿态、线/角速度等全部状态进行实时补偿与校正。

Result: 在1600次仿真及大量实飞实验中验证有效性；在A2RL无人机竞速挑战赛2025中，团队从210支队伍中晋级四强并获奖牌。

Conclusion: 所提方法仅需轻量硬件（单目+IMU），即可实现全状态漂移校正，显著提升高速动态场景下的状态估计精度与鲁棒性，适用于真实竞速与复杂自主飞行任务。

Abstract: Fast flights with aggressive maneuvers in cluttered GNSS-denied environments require fast, reliable, and accurate UAV state estimation. In this paper, we present an approach for onboard state estimation of a high-speed UAV using a monocular RGB camera and an IMU. Our approach fuses data from Visual-Inertial Odometry (VIO), an onboard landmark-based camera measurement system, and an IMU to produce an accurate state estimate. Using onboard measurement data, we estimate and compensate for VIO drift through a novel mathematical drift model. State-of-the-art approaches often rely on more complex hardware (e.g., stereo cameras or rangefinders) and use uncorrected drifting VIO velocities, orientation, and angular rates, leading to errors during fast maneuvers. In contrast, our method corrects all VIO states (position, orientation, linear and angular velocity), resulting in accurate state estimation even during rapid and dynamic motion. Our approach was thoroughly validated through 1600 simulations and numerous real-world experiments. Furthermore, we applied the proposed method in the A2RL Drone Racing Challenge 2025, where our team advanced to the final four out of 210 teams and earned a medal.

</details>


### [967] [BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models](https://arxiv.org/abs/2602.01870)
*Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.RO

TL;DR: 本文提出BTGenBot-2，一个10亿参数的开源小语言模型，能将自然语言任务描述与机器人动作原语直接转换为可执行的行为树（XML格式），支持零样本生成与运行时错误恢复，轻量高效，适用于资源受限机器人，并构建了首个标准化行为树生成评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的机器人任务规划方法存在闭源、计算开销大、难以部署到真实机器人系统，且缺乏统一、即插即用的机器人任务表征形式。

Method: 设计并训练了一个1B参数的开源小语言模型BTGenBot-2，输入为自然语言任务描述+动作原语列表，输出为XML格式行为树；引入零样本生成与推理/运行时错误恢复机制；构建首个覆盖52个导航与操作任务的标准化评测基准（基于NVIDIA Isaac Sim）。

Result: 在功能与非功能指标上均超越GPT-5、Claude Opus 4.1及更大开源模型：零样本成功率90.38%，单样本98.07%，推理速度较前代BTGenBot提升最高达16倍。

Conclusion: BTGenBot-2验证了轻量级、开源、面向行为树的专用小语言模型在机器人任务规划中的可行性与优越性，为LLM在真实机器人系统中的落地提供了新范式与基准支撑。

Abstract: Recent advances in robot learning increasingly rely on LLM-based task planning, leveraging their ability to bridge natural language with executable actions. While prior works showcased great performances, the widespread adoption of these models in robotics has been challenging as 1) existing methods are often closed-source or computationally intensive, neglecting the actual deployment on real-world physical systems, and 2) there is no universally accepted, plug-and-play representation for robotic task generation. Addressing these challenges, we propose BTGenBot-2, a 1B-parameter open-source small language model that directly converts natural language task descriptions and a list of robot action primitives into executable behavior trees in XML. Unlike prior approaches, BTGenBot-2 enables zero-shot BT generation, error recovery at inference and runtime, while remaining lightweight enough for resource-constrained robots. We further introduce the first standardized benchmark for LLM-based BT generation, covering 52 navigation and manipulation tasks in NVIDIA Isaac Sim. Extensive evaluations demonstrate that BTGenBot-2 consistently outperforms GPT-5, Claude Opus 4.1, and larger open-source models across both functional and non-functional metrics, achieving average success rates of 90.38% in zero-shot and 98.07% in one-shot, while delivering up to 16x faster inference compared to the previous BTGenBot.

</details>


### [968] [Multimodal Large Language Models for Real-Time Situated Reasoning](https://arxiv.org/abs/2602.01880)
*Giulio Antonio Abbo,Senne Lenaerts,Tony Belpaeme*

Main category: cs.RO

TL;DR: 本文探索了多模态大语言模型（如GPT-4o）如何支持实时、情境与价值感知的决策，将其部署于TurtleBot 4模拟的智能扫地机器人中，通过视觉输入判断是否启动清洁，并兼顾清洁度、舒适性与安全性等用户价值。实验在真实家居环境中验证了其上下文与价值观推理能力，但也揭示了一致性、偏见和实时性等挑战。


<details>
  <summary>Details</summary>
Motivation: 提升机器人在家庭环境中的自主决策能力，使其不仅能感知物理环境，还能理解社会规范、用户偏好及隐含价值（如清洁、舒适、安全），实现更人性化、可信的交互。

Method: 将GPT-4o多模态大语言模型与TurtleBot 4机器人平台集成，在仿真与真实家居环境中，利用视觉输入进行实时环境评估与清洁决策；强调上下文理解、价值对齐与行为推理。

Result: 系统能在有限视觉输入下有效推断情境与用户价值观，做出符合社会规范与个体偏好的 nuanced 决策；验证了多模态LLM增强机器人情境感知与自主性的潜力，同时暴露一致性、偏见和实时性能瓶颈。

Conclusion: 多模态大语言模型为具身智能体的价值对齐与情境化决策提供了新路径，但迈向可靠落地仍需解决鲁棒性、公平性与计算效率等关键挑战。

Abstract: In this work, we explore how multimodal large language models can support real-time context- and value-aware decision-making. To do so, we combine the GPT-4o language model with a TurtleBot 4 platform simulating a smart vacuum cleaning robot in a home. The model evaluates the environment through vision input and determines whether it is appropriate to initiate cleaning. The system highlights the ability of these models to reason about domestic activities, social norms, and user preferences and take nuanced decisions aligned with the values of the people involved, such as cleanliness, comfort, and safety. We demonstrate the system in a realistic home environment, showing its ability to infer context and values from limited visual input. Our results highlight the promise of multimodal large language models in enhancing robotic autonomy and situational awareness, while also underscoring challenges related to consistency, bias, and real-time performance.

</details>


### [969] [Path Tracking with Dynamic Control Point Blending for Autonomous Vehicles: An Experimental Study](https://arxiv.org/abs/2602.01892)
*Alexandre Lombard,Florent Perronnet,Nicolas Gaud,Abdeljalil Abbas-Turki*

Main category: cs.RO

TL;DR: 本文提出了一种面向自动驾驶车辆的路径跟踪框架，通过在轮轴上动态选择控制点并融合前后轴控制器，实现平滑适应不同驾驶场景（包括低速和倒车）；同时引入基于曲率感知的纵向控制策略，结合虚拟轨道边界与射线追踪调节车速；该方法在仿真和实车实验中均验证了其轨迹精度、转向平滑性和适应性优势。


<details>
  <summary>Details</summary>
Motivation: 解决传统固定参考点（前轴或后轴）路径跟踪方法在低速、倒车等复杂驾驶场景下适应性差、跟踪不稳定的问题。

Method: 采用沿轮轴动态插值的控制点，通过重心融合前轴Stanley控制器与后轴曲率几何控制器生成横向控制指令；纵向控制则基于虚拟轨道边界和射线追踪，将前方几何约束转化为虚拟障碍距离以调节速度；整套方案集成于统一控制栈。

Result: 在仿真与配备GPS-RTK、雷达、里程计和IMU的真实无人车上验证，闭环跟踪与倒车任务中相比固定控制点基线，轨迹精度更高、转向更平滑、场景适应性更强。

Conclusion: 动态控制点与双控制器融合策略，配合曲率感知纵向控制，显著提升了自动驾驶车辆在多样化工况下的路径跟踪性能与鲁棒性。

Abstract: This paper presents an experimental study of a path-tracking framework for autonomous vehicles in which the lateral control command is applied to a dynamic control point along the wheelbase. Instead of enforcing a fixed reference at either the front or rear axle, the proposed method continuously interpolates between both, enabling smooth adaptation across driving contexts, including low-speed maneuvers and reverse motion. The lateral steering command is obtained by barycentric blending of two complementary controllers: a front-axle Stanley formulation and a rear-axle curvature-based geometric controller, yielding continuous transitions in steering behavior and improved tracking stability. In addition, we introduce a curvature-aware longitudinal control strategy based on virtual track borders and ray-tracing, which converts upcoming geometric constraints into a virtual obstacle distance and regulates speed accordingly. The complete approach is implemented in a unified control stack and validated in simulation and on a real autonomous vehicle equipped with GPS-RTK, radar, odometry, and IMU. The results in closed-loop tracking and backward maneuvers show improved trajectory accuracy, smoother steering profiles, and increased adaptability compared to fixed control-point baselines.

</details>


### [970] [Multi-Task Learning for Robot Perception with Imbalanced Data](https://arxiv.org/abs/2602.01899)
*Ozgur Erkent*

Main category: cs.RO

TL;DR: 本文提出了一种在部分任务缺乏真实标签的情况下仍能有效学习多任务的方法，并分析了任务间的相互影响，验证了其在小样本数据下的有效性。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在不同环境中难以获取每个任务的完整真实标签，且多任务中各类标签数量不均衡（即存在类别不平衡问题），导致训练困难。

Method: 提出一种无需所有任务都具备真实标签的多任务学习方法；通过构建教师网络，以某任务输出（如深度图）作为其他任务的输入，分析任务间促进关系；在NYUDv2和Cityscapes数据集上开展语义分割与深度估计联合实验。

Result: 实验证明该方法在少量标注数据下仍具有效性；发现了任务间存在可量化的性能提升关系，例如深度信息可提升语义分割精度。

Conclusion: 所提方法缓解了多任务学习中标签缺失与数据不平衡问题；任务交互分析为机器人多任务协同学习提供了新思路与实用指导。

Abstract: Multi-task problem solving has been shown to improve the accuracy of the individual tasks, which is an important feature for robots, as they have a limited resource. However, when the number of labels for each task is not equal, namely imbalanced data exist, a problem may arise due to insufficient number of samples, and labeling is not very easy for mobile robots in every environment. We propose a method that can learn tasks even in the absence of the ground truth labels for some of the tasks. We also provide a detailed analysis of the proposed method. An interesting finding is related to the interaction of the tasks. We show a methodology to find out which tasks can improve the performance of other tasks. We investigate this by training the teacher network with the task outputs such as depth as inputs. We further provide empirical evidence when trained with a small amount of data. We use semantic segmentation and depth estimation tasks on different datasets, NYUDv2 and Cityscapes.

</details>


### [971] [ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning](https://arxiv.org/abs/2602.01916)
*Keyu Chen,Wenchao Sun,Hao Cheng,Zheng Fu,Sifa Zheng*

Main category: cs.RO

TL;DR: 本文提出ForSim，一种逐步闭环前向仿真范式，用于提升自动驾驶交通仿真的保真度和可靠性。ForSim通过在每个虚拟时间步选择与参考轨迹时空匹配最佳的候选轨迹，并结合物理运动动力学进行传播，同时以步进式预测更新其他智能体，从而在保持多模态行为多样性的同时确保模态内一致性，并增强智能体间的交互真实性。


<details>
  <summary>Details</summary>
Motivation: 现有交通仿真方法面临协变量偏移和难以反映真实交通中多模态行为两大挑战；现有改进框架（如RIFT）仍缺乏反应性，导致虚拟环境中智能体交互不真实、仿真保真度受限。

Method: 提出ForSim：一种逐步闭环前向仿真范式；在每个虚拟时间步，主智能体传播与参考轨迹时空匹配最优的候选轨迹，基于物理运动动力学保证一致性；其余智能体采用步进式预测更新，实现交互感知的协同演化；并与RIFT中的组相对优化联合微调交通策略。

Result: 在RIFT框架中集成ForSim后，实验表明其在保障效率、真实感和舒适性的同时，显著提升了仿真安全性；验证了建模闭环多模态交互对提高交通仿真保真度与可靠性的关键作用。

Conclusion: ForSim通过引入闭环、步进式、交互感知的前向仿真机制，有效缓解协变量偏移并保留真实交通的多模态特性，为自动驾驶的闭环训练与评估提供了更高保真、更可靠的交通仿真基础。

Abstract: As the foundation of closed-loop training and evaluation in autonomous driving, traffic simulation still faces two fundamental challenges: covariate shift introduced by open-loop imitation learning and limited capacity to reflect the multimodal behaviors observed in real-world traffic. Although recent frameworks such as RIFT have partially addressed these issues through group-relative optimization, their forward simulation procedures remain largely non-reactive, leading to unrealistic agent interactions within the virtual domain and ultimately limiting simulation fidelity. To address these issues, we propose ForSim, a stepwise closed-loop forward simulation paradigm. At each virtual timestep, the traffic agent propagates the virtual candidate trajectory that best spatiotemporally matches the reference trajectory through physically grounded motion dynamics, thereby preserving multimodal behavioral diversity while ensuring intra-modality consistency. Other agents are updated with stepwise predictions, yielding coherent and interaction-aware evolution. When incorporated into the RIFT traffic simulation framework, ForSim operates in conjunction with group-relative optimization to fine-tune traffic policy. Extensive experiments confirm that this integration consistently improves safety while maintaining efficiency, realism, and comfort. These results underscore the importance of modeling closed-loop multimodal interactions within forward simulation and enhance the fidelity and reliability of traffic simulation for autonomous driving. Project Page: https://currychen77.github.io/ForSim/

</details>


### [972] [LIEREx: Language-Image Embeddings for Robotic Exploration](https://arxiv.org/abs/2602.01930)
*Felix Igelbrink,Lennart Niecksch,Marian Renz,Martin Günther,Martin Atzmueller*

Main category: cs.RO

TL;DR: LIEREx combines Vision-Language Foundation Models (e.g., CLIP) with 3D Semantic Scene Graphs to enable open-set, target-directed exploration in partially unknown environments.


<details>
  <summary>Details</summary>
Motivation: Traditional semantic maps rely on fixed vocabularies, limiting adaptability to out-of-distribution objects; there's a need for open-set, flexible semantic understanding.

Method: Integrates Vision-Language Foundation Models (e.g., CLIP) with 3D Semantic Scene Graphs to support open-set mapping and target-directed exploration.

Result: Enables autonomous agents to perform target-directed exploration in partially unknown environments using open-set semantic representations.

Conclusion: LIEREx overcomes vocabulary constraints of traditional semantic mapping by leveraging VLFMs, enabling more flexible and scalable robotic reasoning in real-world settings.

Abstract: Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.

</details>


### [973] [Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy](https://arxiv.org/abs/2602.01939)
*Yuxin He,Ruihao Zhang,Tianao Shen,Cheng Liu,Qiang Nie*

Main category: cs.RO

TL;DR: 本文提出探索性与专注性操作（EFM）这一新问题，旨在解决机器人操作中因视觉遮挡导致的信息缺失问题，并构建了EFM-10基准与双臂主动感知（BAP）策略及配套数据集BAPData，验证了其在模仿学习中的有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉遮挡在机器人操作中频繁发生，尤其当主摄像头安装在机器人头部时，导致完成任务所需的关键信息缺失；作者由此提炼出更本质的问题——探索性与专注性操作（EFM）。

Method: 提出EFM问题定义，构建包含10个任务的EFM-10基准；设计双臂主动感知（BAP）策略——一臂用于主动视觉，另一臂用于力觉感知与操作；据此采集BAPData数据集，并采用模仿学习验证BAP策略有效性。

Result: 成功构建EFM-10基准和BAPData数据集；实验证明BAP策略在EFM-10任务中有效；为后续主动感知与操作研究提供基础支撑。

Conclusion: EFM是一个更具根本性的操作问题，BAP策略及其数据集为解决视觉信息缺失下的复杂操作任务提供了可行路径，有望成为该方向研究的基石。

Abstract: Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.

</details>


### [974] [A Unified Control Architecture for Macro-Micro Manipulation using a Active Remote Center of Compliance for Manufacturing Applications](https://arxiv.org/abs/2602.01948)
*Patrick Frank,Christian Friedrich*

Main category: cs.RO

TL;DR: 本文提出了一种新型宏-微操纵器控制架构，将宏观机械臂纳入主动交互控制中，显著提升交互控制带宽（较现有方法提升2.1倍，较传统力控提升12.5倍），并引入代理模型以提升控制器设计效率与硬件适应性。


<details>
  <summary>Details</summary>
Motivation: 传统宏-微操纵器中，宏观机械臂仅负责位置控制，微观机械臂负责环境交互，限制了交互控制带宽；需突破该分工局限以提升动态交互性能。

Method: 提出一种新型控制架构，使宏观机械臂参与主动交互控制；引入 surrogate models 以简化控制器设计并增强对硬件变更的适应性；通过碰撞、力轨迹跟踪和工业装配等实验验证。

Result: 交互控制带宽提升至现有leader-follower架构的2.1倍、传统机器人力控的12.5倍；实验验证了在多种任务中的有效性与鲁棒性。

Conclusion: 将宏观机械臂纳入主动交互控制是提升宏-微系统整体性能的有效途径；所提架构与代理模型兼顾高性能、易部署与可扩展性。

Abstract: Macro-micro manipulators combine a macro manipulator with a large workspace, such as an industrial robot, with a lightweight, high-bandwidth micro manipulator. This enables highly dynamic interaction control while preserving the wide workspace of the robot. Traditionally, position control is assigned to the macro manipulator, while the micro manipulator handles the interaction with the environment, limiting the achievable interaction control bandwidth. To solve this, we propose a novel control architecture that incorporates the macro manipulator into the active interaction control. This leads to a increase in control bandwidth by a factor of 2.1 compared to the state of the art architecture, based on the leader-follower approach and factor 12.5 compared to traditional robot-based force control. Further we propose surrogate models for a more efficient controller design and easy adaptation to hardware changes. We validate our approach by comparing it against the other control schemes in different experiments, like collision with an object, following a force trajectory and industrial assembly tasks.

</details>


### [975] [Reformulating AI-based Multi-Object Relative State Estimation for Aleatoric Uncertainty-based Outlier Rejection of Partial Measurements](https://arxiv.org/abs/2602.02006)
*Thomas Jantos,Giulio Delama,Stephan Weiss,Jan Steinbrener*

Main category: cs.RO

TL;DR: 本文提出了一种改进的基于AI的目标相对位姿估计方法，通过重构EKF中的测量方程并引入DNN预测的偶然不确定性作为测量协方差，提升状态估计的鲁棒性与一致性。


<details>
  <summary>Details</summary>
Motivation: 将深度神经网络（DNN）用于实时目标相对6-DoF位姿估计时，需在扩展卡尔曼滤波（EKF）中合理建模其不确定性并支持异常值剔除，但传统方法难以解耦位置与旋转误差、且使用固定协方差不够准确。

Method: 重新构建EKF的测量方程，采用直接的目标相对位姿测量；解耦位置与旋转分量以支持部分测量拒绝；用DNN预测的aleatoric不确定性替代固定的测量协方差矩阵。

Result: 所提方法提升了状态估计器的性能与一致性，尤其在旋转测量存在误差时仍能保持位置估计稳定，并增强对异常测量的鲁棒性。

Conclusion: 基于DNN预测不确定性动态调整EKF测量协方差，并解耦位姿分量进行部分测量拒绝，是提升AI辅助机器人定位鲁棒性与精度的有效途径。

Abstract: Precise localization with respect to a set of objects of interest enables mobile robots to perform various tasks. With the rise of edge devices capable of deploying deep neural networks (DNNs) for real-time inference, it stands to reason to use artificial intelligence (AI) for the extraction of object-specific, semantic information from raw image data, such as the object class and the relative six degrees of freedom (6-DoF) pose. However, fusing such AI-based measurements in an Extended Kalman Filter (EKF) requires quantifying the DNNs' uncertainty and outlier rejection capabilities.
  This paper presents the benefits of reformulating the measurement equation in AI-based, object-relative state estimation. By deriving an EKF using the direct object-relative pose measurement, we can decouple the position and rotation measurements, thus limiting the influence of erroneous rotation measurements and allowing partial measurement rejection. Furthermore, we investigate the performance and consistency improvements for state estimators provided by replacing the fixed measurement covariance matrix of the 6-DoF object-relative pose measurements with the predicted aleatoric uncertainty of the DNN.

</details>


### [976] [Synchronized Online Friction Estimation and Adaptive Grasp Control for Robust Gentle Grasp](https://arxiv.org/abs/2602.02026)
*Zhenwei Niu,Xiaoyi Chen,Jiayu Hu,Zhaoyang Liu,Xiaozu Ju*

Main category: cs.RO

TL;DR: 本文提出了一种结合实时摩擦系数估计与自适应抓取控制的柔性机器人抓取统一框架，通过视觉触觉传感器和粒子滤波实现摩擦估计，并与闭环反应控制器协同工作，提升抓取稳定性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 实现柔性、稳定且鲁棒的机器人抓取，尤其在物体表面特性未知或变化时，需实时感知并动态调整抓取力。

Method: 提出基于粒子滤波的实时摩擦系数估计方法，利用视觉触觉传感器数据；设计闭环反应控制器，将摩擦估计结果用于动态调节抓取力；估计与控制同步运行、相互增强。

Result: 在大量机器人实验中验证了该框架的可靠性与高效性，实现了高响应性与强鲁棒性的抓取性能。

Conclusion: 耦合实时摩擦估计与自适应控制的闭环传感器-运动循环，是提升柔性抓取性能的有效范式。

Abstract: We introduce a unified framework for gentle robotic grasping that synergistically couples real-time friction estimation with adaptive grasp control. We propose a new particle filter-based method for real-time estimation of the friction coefficient using vision-based tactile sensors. This estimate is seamlessly integrated into a reactive controller that dynamically modulates grasp force to maintain a stable grip. The two processes operate synchronously in a closed-loop: the controller uses the current best estimate to adjust the force, while new tactile feedback from this action continuously refines the estimation. This creates a highly responsive and robust sensorimotor cycle. The reliability and efficiency of the complete framework are validated through extensive robotic experiments.

</details>


### [977] [Frictional Contact Solving for Material Point Method](https://arxiv.org/abs/2602.02038)
*Etienne Ménager,Justin Carpentier*

Main category: cs.RO

TL;DR: 本文提出了一种用于隐式物质点法（MPM）的精确且鲁棒的摩擦接触处理流程，通过粒子中心几何原语进行接触点定位，并将摩擦接触建模为非线性互补问题（NCP），采用ADMM求解；该方法复用隐式MPM线性化，保证效率与稳定性，兼容多种建模选择，并在多种典型场景中验证了其准确性、可靠性与通用性。


<details>
  <summary>Details</summary>
Motivation: 准确处理带摩擦的接触是物质点法（MPM）的核心瓶颈，涉及可靠的接触点检测和满足非穿透、库仑摩擦及最大耗散原理等摩擦接触定律。

Method: 在碰撞检测阶段，采用粒子中心几何原语定位接触点；在接触求解阶段，将摩擦接触建模为非线性互补问题（NCP），并使用交替方向乘子法（ADMM）求解接触冲量；整个过程复用隐式MPM的线性化，实现高效稳定。

Result: 该方法在七类代表性场景（涵盖弹性/弹塑性响应、简单/复杂几何形变、广泛接触条件）中验证有效，实现了精确接触定位、可靠摩擦处理和良好通用性。

Conclusion: 所提摩擦接触流程是隐式MPM中一种实用、高效、鲁棒且通用的解决方案，适用于机器人等领域中的MPM仿真。

Abstract: Accurately handling contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws (non-penetration, Coulomb friction, and maximum dissipation principle). In this paper, we introduce a frictional-contact pipeline for implicit MPM that is both precise and robust. During the collision detection phase, contact points are localized with particle-centric geometric primitives; during the contact resolution phase, we cast frictional contact as a Nonlinear Complementarity Problem (NCP) over contact impulses and solve it with an Alternating Direction Method of Multipliers (ADMM) scheme. Crucially, the formulation reuses the same implicit MPM linearization, yielding efficiency and numerical stability. The method integrates seamlessly into the implicit MPM loop and is agnostic to modeling choices, including material laws, interpolation functions, and transfer schemes. We evaluate it across seven representative scenes that span elastic and elasto-plastic responses, simple and complex deformable geometries, and a wide range of contact conditions. Overall, the proposed method enables accurate contact localization, reliable frictional handling, and broad generality, making it a practical solution for MPM-based simulations in robotics and related domains.

</details>


### [978] [FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation](https://arxiv.org/abs/2602.02142)
*Ruiteng Zhao,Wenshuo Wang,Yicheng Ma,Xiaocong Li,Francis E. H. Tay,Marcelo H. Ang,Haiyue Zhu*

Main category: cs.RO

TL;DR: 本文提出Force-Distilled VLA（FD-VLA）框架，通过力蒸馏模块（FDM）从视觉与机器人状态中学习预测力信号，无需物理力传感器即可实现接触密集任务中的力感知与操作。


<details>
  <summary>Details</summary>
Motivation: 力感知对接触密集型操作至关重要，但物理力传感器成本高、易损坏，限制了VLA框架在实际机器人上的部署。

Method: 提出力蒸馏模块（FDM），将可学习查询token（基于视觉观测和机器人状态）映射为与真实力信号潜在表征对齐的预测力token；该token在推理时注入预训练视觉语言模型（VLM），实现力感知增强且不破坏原有语义。

Result: 在物理实验中，蒸馏出的力token性能优于直接传感器测量及其他基线方法，显著提升了接触密集场景下的跨模态对齐与感知-动作鲁棒性。

Conclusion: FD-VLA实现了无需硬件力传感器的高质量力感知，降低了部署门槛，并通过前置力-视觉-状态融合增强了VLA模型在接触任务中的有效性与泛化能力。

Abstract: Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.

</details>


### [979] [Extending the Law of Intersegmental Coordination: Implications for Powered Prosthetic Controls](https://arxiv.org/abs/2602.02181)
*Elad Siman Tov,Nili E. Krausz*

Main category: cs.RO

TL;DR: 本文提出了一种分析下肢截肢者步态中段间协调（ISC）的新方法，扩展至基于力矩的协调（ESM），发现截肢者使用被动假肢时ESM协调性降低，并利用ISC约束预测补偿性关节角度/力矩，以改善动力假肢控制。


<details>
  <summary>Details</summary>
Motivation: 尽管动力假肢已能提供净正功，但降低截肢者步行代谢能耗仍是难题；而段间协调（ISC）作为跨步态普遍存在的规律，尚未在截肢者步态中被深入分析或应用。

Method: 开发了适用于3D运动学数据的ISC分析方法，并受运动控制、生物力学与机器人学启发，将ISC拓展为基于力矩的协调（ESM）；利用ISC作为约束预测被动假肢导致的代偿性胫骨角度/力矩；开发了开源ISC3d工具箱。

Result: 发现健康人步态中存在力矩层面的协调（ESM）；截肢者使用被动假肢时ESM协调性下降，而角度层面的ISC仍保持平面性；ISC约束可有效预测补偿性角度/力矩以逼近健康人股骨角度/力矩模式。

Conclusion: ISC及新提出的ESM可作为评估和优化假肢控制的重要生物力学指标；ISC3d工具箱为深入研究步态协调与神经运动控制提供了新工具。

Abstract: Powered prostheses are capable of providing net positive work to amputees and have advanced in the past two decades. However, reducing amputee metabolic cost of walking remains an open problem. The Law of Intersegmental Coordination (ISC) has been observed across gaits and has been previously implicated in energy expenditure of walking, yet it has rarely been analyzed or applied within the context of lower-limb amputee gait. This law states that the elevation angles of the thigh, shank and foot over the gait cycle are not independent. In this work, we developed a method to analyze intersegmental coordination for lower-limb 3D kinematic data, to simplify ISC analysis. Moreover, inspired by motor control, biomechanics and robotics literature, we used our method to broaden ISC toward a new law of coordination of moments. We find these Elevation Space Moments (ESM), and present results showing a moment-based coordination for able bodied gait. We also analyzed ISC for amputee gait walking with powered and passive prosthesis, and found that while elevation angles remained planar, the ESM showed less coordination. We use ISC as a constraint to predict the shank angles/moments that would compensate for alterations due to a passive foot so as to mimic a healthy thigh angle/moment profile. This may have implications for improving powered prosthetic control. We developed the ISC3d toolbox that is freely available online, which may be used to compute kinematic and kinetic ISC in 3D. This provides a means to further study the role of coordination in gait and may help address fundamental questions of the neural control of human movement.

</details>


### [980] [Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL](https://arxiv.org/abs/2602.02236)
*Julian Lemmel,Felix Resch,Mónika Farsang,Ramin Hasani,Daniela Rus,Radu Grosu*

Main category: cs.RO

TL;DR: 本文提出使用实时递归强化学习（RTRRL）在线微调预训练策略，以应对现实环境中动态变化带来的性能下降问题，并结合液阻-液容RNN模型，在仿真和真实小车任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 预训练策略在现实应用中因系统动力学变化、传感器漂移或任务目标改变而迅速失效，亟需具备在线适应能力的控制方法。

Method: 采用生物启发的实时递归强化学习（RTRRL）算法对预训练策略进行在线微调，并与液阻-液容RNN（LR-LC RNN）模型协同构建闭环适应系统。

Result: 在CarRacing仿真环境和搭载事件相机的真实RoboRacer小车线跟踪任务中，该方法显著提升了策略的实时适应性与控制性能。

Conclusion: RTRRL结合LR-LC RNN可有效实现预训练策略的快速在线适应，为学习型控制系统在动态真实环境中的部署提供了可行路径。

Abstract: Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.

</details>


### [981] [Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems](https://arxiv.org/abs/2602.02269)
*Jon Škerlj,Seongjin Bien,Abdeldjallil Naceri,Sami Haddadin*

Main category: cs.RO

TL;DR: 本文提出了multipanda_ros2，一个用于Franka机器人多机器人控制的开源ROS2架构，支持1kHz实时扭矩控制、快速控制器切换（≤2ms）以及高保真MuJoCo仿真与真实世界物理参数迭代校准，显著缩小仿真到现实的差距。


<details>
  <summary>Details</summary>
Motivation: 解决多Franka机器人在实时扭矩控制、交互控制和机器人-环境建模中的关键挑战，满足安全标准要求的1kHz控制频率，并缩小仿真到现实的差距。

Method: 基于ros2_control构建统一多机器人控制框架；提出controllet-feature设计模式实现毫秒级控制器切换；集成高保真MuJoCo仿真并定义运动学与动力学一致性量化指标；结合真实世界惯性参数辨识进行物理模型迭代优化。

Result: 实现了稳定1kHz实时控制频率；控制器切换延迟≤2ms；通过惯性参数辨识显著提升力/扭矩精度；在接触丰富的双臂软硬协同任务中验证了sim2real性能提升。

Conclusion: multipanda_ros2为接触丰富、多机器人、实时控制研究提供了鲁棒、可复现的平台，拓展了软体机器人方法至刚性双臂系统，并为缩小sim2real差距提供了系统性解决方案。

Abstract: We present $multipanda\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.

</details>


### [982] [TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour](https://arxiv.org/abs/2602.02331)
*Shaoting Zhu,Baijun Ye,Jiaxuan Wang,Jiakang Chen,Ziwen Zhuang,Linzhan Mou,Runhan Huang,Hang Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种real-to-sim-to-real框架，结合快速测试时训练（TTT）和高保真几何重建，使双足机器人能在未知复杂地形上实现高度动态的跑酷运动。


<details>
  <summary>Details</summary>
Motivation: 现有通用步态策略难以应对任意且极具挑战性的未知复杂地形，需提升机器人在新环境中的实时适应能力。

Method: 采用两阶段端到端学习范式：先在程序化生成的多样地形上预训练策略，再基于RGB-D重建的真实地形高保真网格进行快速测试时微调；开发了前馈式、高效、高保真的几何重建流水线。

Result: TTT-Parkour使机器人成功跨越楔形、桩、箱体、梯形及窄梁等复杂障碍；整套捕获-重建-训练流程耗时少于10分钟；策略具备强鲁棒零样本sim-to-real迁移能力。

Conclusion: 该框架显著提升了双足机器人在未见复杂地形上的动态适应性与泛化能力，为真实场景下的自主跑酷提供了可行路径。

Abstract: Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.

</details>


### [983] [Mapping-Guided Task Discovery and Allocation for Robotic Inspection of Underwater Structures](https://arxiv.org/abs/2602.02389)
*Marina Ruediger,Ashis G. Banerjee*

Main category: cs.RO

TL;DR: 本文提出了一种基于SLAM数据的水下多机器人巡检任务生成与优化方法，无需预先知道环境几何信息，通过硬件参数、环境条件及关键点得分预期和距离剪枝优化任务，并在真实水下实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决水下多机器人巡检中缺乏先验环境几何信息时的任务自动生成问题，提升对未知或突发结构变化的适应能力。

Method: 利用SLAM重建的网格生成初始任务集，结合硬件参数与环境条件建模，通过预期关键点得分评估与距离驱动的剪枝策略进行任务优化。

Result: 水下实验证明该方法有效；相比Voronoi划分和往复式（boustrophedon）覆盖模式，在模型环境中展现出更优的缺陷敏感区域覆盖能力。

Conclusion: 所提任务发现方法具有强适应性，能在保障全覆盖前提下，动态聚焦于更可能存在缺陷或损伤的区域，适用于复杂未知水下环境。

Abstract: Task generation for underwater multi-robot inspections without prior knowledge of existing geometry can be achieved and optimized through examination of simultaneous localization and mapping (SLAM) data. By considering hardware parameters and environmental conditions, a set of tasks is generated from SLAM meshes and optimized through expected keypoint scores and distance-based pruning. In-water tests are used to demonstrate the effectiveness of the algorithm and determine the appropriate parameters. These results are compared to simulated Voronoi partitions and boustrophedon patterns for inspection coverage on a model of the test environment. The key benefits of the presented task discovery method include adaptability to unexpected geometry and distributions that maintain coverage while focusing on areas more likely to present defects or damage.

</details>


### [984] [PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning](https://arxiv.org/abs/2602.02396)
*Amisha Bhaskar,Pratap Tokekar,Stefano Di Cairano,Alexander Schperberg*

Main category: cs.RO

TL;DR: 本文提出了PRISM，一种基于IMLE的单次通过策略，结合多模态传感器编码器和线性注意力生成器，在真实硬件和大规模仿真中显著提升了模仿学习的成功率与控制频率，同时降低了轨迹抖动。


<details>
  <summary>Details</summary>
Motivation: 现有生成式方法（如扩散模型、流匹配、IMLE）难以同时满足多模态动作分布建模、实时控制速率和多传感模态融合的需求。

Method: 提出PRISM：基于批全局拒绝采样的IMLE变体；采用融合RGB、深度、触觉、音频和本体感知的时序多感官编码器；使用Performer架构的线性注意力生成器。

Result: 在Unitree Go2+D1和UR5硬件上，PRISM在预操作泊车、高精度插入、多物体抓取放置等任务中比SOTA扩散策略成功率高10–25%，并保持30–50 Hz闭环控制；在CALVIN（10%数据）中成功率比扩散高约25%、比流匹配高约20%，轨迹抖动降低20–50倍。

Conclusion: PRISM是一种快速、准确、多感官的模仿策略，兼具多模态动作覆盖能力与低延迟单次推理优势，解决了现有生成式策略在实时性与多模态融合间的权衡难题。

Abstract: Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.

</details>


### [985] [SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation](https://arxiv.org/abs/2602.02402)
*Mu Huang,Hui Wang,Kerui Ren,Linning Xu,Yunsong Zhou,Mulin Yu,Bo Dai,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 本文提出了SoMA，一种基于3D高斯点阵的软体操作模拟器，通过将形变动力学、环境力和机器人关节动作统一建模于潜在神经空间中，实现了端到端的真实到仿真模拟，显著提升了重仿真精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器依赖预定义物理模型或无机器人条件控制的数据驱动动力学，限制了精度、稳定性与泛化性；而真实机器人操作中软体物体的动力学受环境与机器人动作共同影响，亟需更鲁棒、可控、泛化的模拟方法。

Method: 提出SoMA，利用学习得到的高斯点阵（Gaussian splats）建模软体交互，在统一的潜在神经空间中联合表征形变动力学、环境力与机器人关节动作，实现端到端真实到仿真的映射。

Result: SoMA在真实机器人操作任务上的重仿真精度与泛化能力提升20%，并能稳定模拟长时序复杂任务（如布料折叠）。

Conclusion: SoMA摆脱了对显式物理模型的依赖，通过数据驱动的隐式神经动力学建模，实现了更准确、稳定且泛化性强的软体机器人操作仿真。

Abstract: Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.

</details>


### [986] [Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces](https://arxiv.org/abs/2602.02411)
*Hanwen Ren,Junyong Kim,Aathman Tharmasanthiran,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 本文提出了一种名为CAM-MCTS的多智能体协同规划框架，用于在复杂杂乱环境中高效完成非单调物体重排任务，通过集中式任务分配与异步执行策略显著降低总完成时间（makespan）。


<details>
  <summary>Details</summary>
Motivation: 现实中的物体重排任务（如仓库、家庭、救援现场）常为非单调型，即物体相互遮挡，需临时移至中间位置；而以往方法多局限于单调情形，且缺乏高效多智能体协同机制。

Method: 提出Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search（CAM-MCTS）：结合集中式任务分配（全局优化）与异步执行策略（基于一步前瞻代价估计动态分配新任务），避免空闲和同步延迟。

Result: 在多种单调与非单调杂乱环境任务中，CAM-MCTS持续优于强基线方法，显著降低makespan；并在真实多智能体系统上验证了其有效性与鲁棒性。

Conclusion: CAM-MCTS是一种通用、高效、可部署的多智能体重排规划框架，特别适用于非单调、高杂波的实际场景。

Abstract: Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other's intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.

</details>


### [987] [3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM](https://arxiv.org/abs/2602.02430)
*Pierre-Yves Lajoie,Benjamin Ramtoula,Daniele De Martini,Giovanni Beltrame*

Main category: cs.RO

TL;DR: 本文提出了一种基于3D基础模型的鲁棒闭环检测方法，用于解决多机器人去中心化SLAM中因视角差异大而导致的地图重叠识别困难问题，提升了定位与建图精度及计算效率。


<details>
  <summary>Details</summary>
Motivation: 去中心化协同SLAM（C-SLAM）常因机器人间视角差异大而难以识别地图重叠；而近期3D基础模型在跨大视角图像配准方面展现出优势，激发了将其用于构建机器人间测量的思路。

Method: 将3D基础模型集成到现有SLAM流程中，从单目图像对中估计机器人间的相对位姿；引入鲁棒异常值抑制技术；设计专用的位姿图优化方法以高效消除尺度歧义。

Result: 相比当前最优方法，在定位与建图精度上有所提升，同时显著降低了计算与内存开销。

Conclusion: 该方法具备可扩展性与鲁棒性，适用于大规模多机器人部署场景。

Abstract: Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios.

</details>


### [988] [World-Gymnast: Training Robots with Reinforcement Learning in a World Model](https://arxiv.org/abs/2602.02454)
*Ansh Kumar Sharma,Yixiang Sun,Ninghao Lu,Yunzhe Zhang,Jiarao Liu,Sherry Yang*

Main category: cs.RO

TL;DR: 本文提出World-Gymnast方法，利用动作条件视频世界模型与视觉语言模型（VLM）进行强化学习微调，显著提升真实机器人任务性能，超越监督微调和软件仿真。


<details>
  <summary>Details</summary>
Motivation: 物理交互成本高，监督微调受限于专家数据量，软件仿真存在sim-to-real差距；新兴的真实世界视频-动作学习的世界模型为策略训练提供了新可能。

Method: 提出World-Gymnast：在动作条件视频世界模型中对视觉-语言-动作（VLA）策略进行RL微调，并用视觉语言模型（VLM）对rollout结果进行奖励评估。

Result: 在Bridge机器人平台上，World-Gymnast比监督微调性能高18倍，比软件仿真高2倍；支持多样化语言指令、新场景训练、测试时自适应及在线迭代优化世界模型与策略。

Conclusion: 基于真实数据学习世界模型并在云端训练机器人策略，有望实现从演示驱动机器人到通用家用机器人的跨越。

Abstract: Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.

</details>


### [989] [Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning](https://arxiv.org/abs/2602.02456)
*Albert Gassol Puigjaner,Angelos Zacharia,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种增强型分层3D场景图，融合多层级开放词汇特征并结合视觉语言模型（VLM）与大语言模型（LLM）进行语义关系推理与任务理解，提升了自主智能体在真实环境中的导航与交互能力。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM方法缺乏高层抽象和关系推理能力，而3D场景图可弥补这一缺陷，但现有方法在跨层级语义表达与任务驱动推理方面仍有不足。

Method: 构建增强型分层3D场景图，集成开放词汇特征；利用VLM推断对象间语义关系；设计融合LLM与VLM的任务推理模块，实现对场景图的语义与关系理解。

Result: 在四足机器人上多环境、多任务部署验证了该方法的有效性，显著提升了智能体的任务理解与环境交互能力。

Conclusion: 所提方法通过分层场景图与多模态大模型协同，实现了从几何重建到任务级认知的跨越，为具身智能提供了更强大的结构化环境表征与推理框架。

Abstract: Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.

</details>


### [990] [TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments](https://arxiv.org/abs/2602.02459)
*Zhiyu Huang,Yun Zhang,Johnson Liu,Rui Song,Chen Tang,Jiaqi Ma*

Main category: cs.RO

TL;DR: 本文提出了一种名为Think-in-Control (TIC)-VLA的延迟感知视觉-语言-动作框架，用于在动态、以人为中心的环境中实现兼顾语义推理延迟与实时控制的语言导航。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型假设语义推理与动作控制在时间上对齐，但实际中语义推理存在固有延迟，导致部署时性能下降。

Method: 提出TIC-VLA框架，包含延迟语义-控制接口（利用延迟语义状态和显式延迟元数据）及延迟一致性训练流程（在模仿学习和在线强化学习中注入推理延迟）；并构建DynaNav仿真平台用于真实感评测。

Result: 在仿真与真实机器人实验中，TIC-VLA在多秒推理延迟下仍显著优于先前VLA模型，并保持鲁棒的实时控制能力。

Conclusion: 显式建模并训练应对语义推理延迟，是提升VLA模型在真实动态环境中实用性的关键路径。

Abstract: Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/

</details>


### [991] [HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos](https://arxiv.org/abs/2602.02473)
*Yinhuai Wang,Qihan Zhao,Yuen Fui Lau,Runyi Yu,Hok Wai Tsui,Qifeng Chen,Jingbo Wang,Jiangmiao Pang,Ping Tan*

Main category: cs.RO

TL;DR: HumanX 是一个无需任务特定奖励的全栈框架，能将人类视频转化为人形机器人可执行的通用、真实世界交互技能，在多个运动和交互任务中实现了零样本迁移和远超先前方法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于真实交互数据稀缺或需精细设计任务特定奖励，导致可扩展性差。

Method: 提出 HumanX 框架，包含 XGen（从视频合成多样化、物理合理机器人交互数据的数据生成管线）和 XMimic（统一的模仿学习框架），二者协同设计，无需任务特定奖励。

Result: 在篮球、足球、羽毛球、货物拾取和反应式格斗五个领域成功习得10种技能，并零样本迁移到实体 Unitree G1 人形机器人；实现无外部感知的复杂动作（如假动作后仰跳投）及持续10轮的人机传球交互；泛化成功率超先前方法8倍以上。

Conclusion: HumanX 提供了一条可扩展、任务无关的学习路径，使人形机器人能从单视频演示中高效获取多样、真实的交互技能。

Abstract: Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.

</details>


### [992] [Flow Policy Gradients for Robot Control](https://arxiv.org/abs/2602.02481)
*Brent Yi,Hongsuk Choi,Himanshu Gaurav Singh,Xiaoyu Huang,Takara E. Truong,Carmelo Sferrazza,Yi Ma,Rocky Duan,Pieter Abbeel,Guanya Shi,Karen Liu,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: 本文提出了一种基于流匹配（flow matching）的策略梯度新方法，绕过传统似然计算限制，支持更复杂的策略分布，在腿式行走、人形运动跟踪和操作任务中表现优异，并实现稳健的仿真到真实世界迁移。


<details>
  <summary>Details</summary>
Motivation: 传统基于似然的策略梯度方法受限于需计算可微动作似然，只能使用简单分布（如高斯分布），难以表达复杂策略；本文旨在突破该限制，提升策略表达能力和在具身智能任务中的实用性。

Method: 采用流匹配策略梯度框架，设计改进的目标函数，支持训练和微调更具表达力的策略；结合对训练动态的消融分析与可视化，验证其探索能力与微调鲁棒性。

Result: 在腿式机器人运动、人形机器人运动跟踪与操作任务中取得成功；实现两个真实人形机器人上的稳健sim-to-real迁移；相比基线方法，展现出更强的从零训练探索能力与微调鲁棒性。

Conclusion: 流匹配策略梯度是一种有前景的替代范式，能有效支持复杂策略建模与实际机器人控制，尤其适用于高维、多模态动作空间与跨域迁移场景。

Abstract: Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [993] [Fast Sparse Matrix Permutation for Mesh-Based Direct Solvers](https://arxiv.org/abs/2602.00898)
*Behrooz Zarebavami,Ahmed H. Mahmoud,Ana Dodik,Changcheng Yuan,Serban D. Porumbescu,John D. Owens,Maryam Mehri Dehnavi,Justin Solomon*

Main category: cs.GR

TL;DR: 本文提出了一种面向三角网格线性系统、快速的稀疏矩阵置换算法，通过放松嵌套剖分中严格的平衡与分离器最优性要求，实现更快的划分和消去树构建，在CPU和GPU上集成后显著提升稀疏Cholesky求解性能。


<details>
  <summary>Details</summary>
Motivation: 传统嵌套剖分算法在三角网格衍生的线性系统中存在置换开销大、计算效率低的问题，亟需兼顾结构保持与运行效率的新型置换策略。

Method: 将置换分解为面片级局部排序与分离器构成的紧致商图排序两阶段；放松平衡与分离器最优性约束，侧重快速划分与高效消去树构造；并集成至厂商维护的CPU/GPU稀疏Cholesky求解器中。

Result: 在多种图形学应用中，该方法将置换时间显著降低，并使稀疏Cholesky求解性能最高提升6.27倍。

Conclusion: 所提算法在保持Cholesky因式分解所需关键结构前提下，有效规避了其最耗时环节，是一种高效、实用且硬件友好的稀疏矩阵置换方案。

Abstract: We present a fast sparse matrix permutation algorithm tailored to linear systems arising from triangle meshes. Our approach produces nested-dissection-style permutations while significantly reducing permutation runtime overhead. Rather than enforcing strict balance and separator optimality, the algorithm deliberately relaxes these design decisions to favor fast partitioning and efficient elimination-tree construction. Our method decomposes permutation into patch-level local orderings and a compact quotient-graph ordering of separators, preserving the essential structure required by sparse Cholesky factorization while avoiding its most expensive components. We integrate our algorithm into vendor-maintained sparse Cholesky solvers on both CPUs and GPUs. Across a range of graphics applications, including single factorizations, repeated factorizations, our method reduces permutation time and improves the sparse Cholesky solve performance by up to 6.27x.

</details>


### [994] [Genus-0 Surface Parameterization using Spherical Beltrami Differentials](https://arxiv.org/abs/2602.01589)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.GR

TL;DR: 本文提出了一种基于球面Beltrami微分（SBD）和神经优化框架BOOST的新方法，用于解决球面自映射中的任务驱动、双射性与几何失真控制之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有球面自映射方法在满足任务目标（如标志点对齐）、保持双射性与控制几何失真之间存在难以兼顾的权衡。

Method: 引入球面Beltrami微分（SBD）作为球面拟共形自映射的双图表示，并基于谱Beltrami网络（SBN）构建神经优化框架BOOST，在半球球极投影图上优化两个Beltrami场，并通过显式的接缝感知约束保证全局一致性。

Result: 在大变形标志点匹配和强度驱动的球面配准任务中验证了有效性；在脑皮层表面配准中实现了更优的沟回标志点与深度图联合对齐，兼具高任务保真度、可控失真与鲁棒双射性。

Conclusion: SBD理论结合BOOST框架为任务驱动的高质量球面自映射提供了统一、灵活且可学习的建模范式。

Abstract: Spherical surface parameterization is a fundamental tool in geometry processing and imaging science. For a genus-0 closed surface, many efficient algorithms can map the surface to the sphere; consequently, a broad class of task-driven genus-0 mapping problems can be reduced to constructing a high-quality spherical self-map. However, existing approaches often face a trade-off between satisfying task objectives (e.g., landmark or feature alignment), maintaining bijectivity, and controlling geometric distortion. We introduce the Spherical Beltrami Differential (SBD), a two-chart representation of quasiconformal self-maps of the sphere, and establish its correspondence with spherical homeomorphisms up to conformal automorphisms. Building on the Spectral Beltrami Network (SBN), we propose a neural optimization framework BOOST that optimizes two Beltrami fields on hemispherical stereographic charts and enforces global consistency through explicit seam-aware constraints. Experiments on large-deformation landmark matching and intensity-based spherical registration demonstrate the effectiveness of our proposed framework. We further apply the method to brain cortical surface registration, aligning sulcal landmarks and jointly matching cortical sulci depth maps, showing improved task fidelity with controlled distortion and robust bijective behavior.

</details>


### [995] [OFERA: Blendshape-driven 3D Gaussian Control for Occluded Facial Expression to Realistic Avatars in VR](https://arxiv.org/abs/2602.01748)
*Seokhwan Yang,Boram Yoon,Seoyoung Kang,Hail Song,Woontack Woo*

Main category: cs.GR

TL;DR: OFERA是一个用于VR头戴设备用户实时控制照片级真实感高斯头像表情的新框架，利用商用VR头显提供的blendshape信号，通过分布对齐、参数映射和端到端渲染，提升远程临场感。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖额外传感器或内置摄像头恢复被遮挡面部表情，但前者增加设备重量与不适感，后者引发隐私问题且难以获取原始数据。

Method: 提出OFERA框架，包含三部分：(1) Blendshape分布对齐（BDA），用线性回归将头显blendshape映射至标准空间；(2) 表情参数映射器（EPM），将对齐后的信号映射为高斯头像可控参数；(3) 集成映射器的头像（MiA），在头像学习中嵌入EPM以保证分布一致性；并构建端到端实时感知-映射-更新-渲染流程。

Result: EPM在定量指标上优于现有映射方法；用户研究表明OFERA提升了表情保真度且不损害头像真实感。

Conclusion: OFERA实现了VR中实时、高保真、隐私友好的照片级真实感头像表情控制，显著增强VR通信中的远程临场体验。

Abstract: We propose OFERA, a novel framework for real-time expression control of photorealistic Gaussian head avatars for VR headset users. Existing approaches attempt to recover occluded facial expressions using additional sensors or internal cameras, but sensor-based methods increase device weight and discomfort, while camera-based methods raise privacy concerns and suffer from limited access to raw data. To overcome these limitations, we leverage the blendshape signals provided by commercial VR headsets as expression inputs. Our framework consists of three key components: (1) Blendshape Distribution Alignment (BDA), which applies linear regression to align the headset-provided blendshape distribution to a canonical input space; (2) an Expression Parameter Mapper (EPM) that maps the aligned blendshape signals into an expression parameter space for controlling Gaussian head avatars; and (3) a Mapper-integrated Avatar (MiA) that incorporates EPM into the avatar learning process to ensure distributional consistency. Furthermore, OFERA establishes an end-to-end pipeline that senses and maps expressions, updates Gaussian avatars, and renders them in real-time within VR environments. We show that EPM outperforms existing mapping methods on quantitative metrics, and we demonstrate through a user study that the full OFERA framework enhances expression fidelity while preserving avatar realism. By enabling real-time and photorealistic avatar expression control, OFERA significantly improves telepresence in VR communication. A project page is available at https://ysshwan147.github.io/projects/ofera/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [996] [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)
*Wei Zeng,Xuchen Li,Ruili Feng,Zhen Liu,Fengwei An,Jian Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种硬件-算法协同设计框架，通过异构架构解耦计算密集型世界模型与内存密集型解码器，突破‘内存墙’限制，实现720×480分辨率实时神经游戏生成，像素吞吐量提升50倍，帧率达26.4/48.3 FPS，有效延迟仅2.7ms。


<details>
  <summary>Details</summary>
Motivation: 现有实时生成式游戏引擎受限于‘内存墙’，难以支持高分辨率（如64×64以上）部署，阻碍了神经世界模型在高质量交互仿真中的实用化。

Method: 提出硬件-算法协同设计框架：1）面向序列并行约束的非对称资源分配策略；2）以内存为中心的算子融合以降低片外带宽；3）流形感知的潜在空间外推机制利用时序冗余隐藏延迟；整体采用AI加速器集群上的异构架构解耦世界模型（计算密集）与解码器（内存密集）。

Result: 在可编程AI加速器集群上实现720×480分辨率实时生成，像素吞吐量较基线提升50倍；在3D赛车和2D平台游戏基准中分别达到26.4 FPS和48.3 FPS，平均有效延迟为2.7 ms。

Conclusion: ‘内存墙’问题不能仅靠软件优化缓解，必须通过软硬协同的体系结构设计来解决；该工作证明此类协同设计是实现高保真、低延迟神经游戏体验的前提。

Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \times 480$ resolution -- a $50\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.

</details>


### [997] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

TL;DR: 本文对比了FastAPI和NVIDIA Triton两种ML模型部署方案在医疗AI场景下的性能，发现FastAPI适合低延迟单请求，Triton通过动态批处理实现更高吞吐；提出并验证了一种FastAPI（安全网关）+ Triton（推理后端）的混合架构，作为企业级临床AI部署的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 在医疗等受监管领域，ML模型部署需同时满足低推理延迟、高吞吐量和严格数据隐私（如HIPAA）要求，现有方案缺乏系统性权衡分析。

Method: 基于医疗AI参考架构，在Kubernetes上部署DistilBERT模型，对比FastAPI REST服务与NVIDIA Triton Inference Server在p50/p95延迟和吞吐量上的表现，并设计验证FastAPI+Triton混合架构。

Result: FastAPI在p50延迟上更优（22ms），Triton通过动态批处理实现780 QPS吞吐（约为FastAPI的2倍）；混合架构兼顾安全性与性能，被证实为可行最佳实践。

Conclusion: 单一部署范式难以兼顾医疗AI的多重要求，FastAPI与Triton协同的混合架构是实现安全、高性能、高可用临床AI系统的有效路径。

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [998] [Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets](https://arxiv.org/abs/2602.00188)
*Srividhya Sethuraman,Chandrashekar Lakshminarayanan*

Main category: cs.AI

TL;DR: 本文提出了一种可解释的加性特征分解低维需求模型（AFDLD）及相应的在线学习算法ADEPT，用于高维动态定价问题，在保持高效学习的同时提供属性级价格解释。


<details>
  <summary>Details</summary>
Motivation: 解决高维市场中动态定价面临的可扩展性、不确定性和可解释性挑战，尤其是现有低秩带臂方法因依赖隐式特征而难以解释各产品属性对价格的影响。

Method: 提出AFDLD模型，将价格表示为各属性贡献之和，并显式建模替代效应；在此基础上设计无需投影、无需梯度的在线算法ADEPT，直接在属性空间中学习，具有次线性遗憾界。

Result: ADEPT在合成与真实数据上均能快速学习近优价格、适应市场突变与漂移，并提供透明的属性级价格解释。

Conclusion: 通过结构化的属性驱动表征，可同时实现自主定价代理的可解释性与高效性。

Abstract: Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \emph{Additive Feature Decomposition-based Low-Dimensional Demand (\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.

</details>


### [999] [From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models](https://arxiv.org/abs/2602.00190)
*Mohit Jiwatode,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型（LLM）从游戏运行轨迹中反向推导视频游戏描述语言（VGDL）规则的能力，提出一种基于结构因果模型（SCM）的两阶段方法，相比直接代码生成，在规则准确性、逻辑一致性及下游应用（如因果强化学习、可解释智能体、新游戏生成）方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 深度学习智能体虽在复杂游戏中表现优异，但常缺乏对因果机制的理解；本文旨在提升模型对游戏内在因果规律的归纳能力（即因果归纳）。

Method: 提出两种VGDL生成方法：1）直接从观测数据生成VGDL代码；2）先从游戏轨迹推断结构因果模型（SCM），再将其翻译为VGDL。在九个语义聚类选出的GVGAI游戏中，对比多种提示策略与上下文设置（如仅原始观测、部分VGDL提示等）。

Result: SCM方法在盲评中最高达81%偏好胜率，生成规则更接近真实VGDL，逻辑不一致率更低；所学SCM支持因果强化学习、可解释智能体和一致新游戏生成等下游任务。

Conclusion: 将因果建模（SCM）显式引入LLM推理流程，显著提升了其从观测中归纳游戏因果规则的能力，为构建具备因果理解能力的游戏AI提供了可行路径。

Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.

</details>


### [1000] [Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic](https://arxiv.org/abs/2602.00266)
*Yani Zhang,Helmut Bölcskei*

Main category: cs.AI

TL;DR: 本文通过将ReLU神经网络映射为Lukasiewicz逻辑公式，利用其代数公理进行重写，解决了ReLU网络的功能等价识别问题，并提出范式形式实现双向映射，证明所有功能等价网络可通过有限对称变换相互连接。


<details>
  <summary>Details</summary>
Motivation: 解决深度ReLU神经网络中因功能对称性导致的同一函数可由 vastly different 架构和参数实现的问题，即完全识别给定函数对应的所有可能ReLU网络结构。

Method: 将ReLU网络转化为Lukasiewicz逻辑公式；基于Lukasiewicz逻辑公理进行代数重写以实现功能等价变换；提出一种组合式范式形式支持逻辑公式到ReLU网络的逆映射；借助Chang完备性定理建立理论保证。

Result: 证明每个功能等价类中的所有ReLU网络均可通过对应Lukasiewicz逻辑公理的有限对称变换相互连通；建立了ReLU网络与多值逻辑之间的严格对应关系。

Conclusion: ReLU网络的功能对称性可系统化地由Lukasiewicz逻辑公理刻画，该框架为网络简化、等价验证与可解释性分析提供了新工具，类比于Shannon在开关电路设计中的布尔逻辑方法。

Abstract: Deep ReLU neural networks admit nontrivial functional symmetries: vastly different architectures and parameters (weights and biases) can realize the same function. We address the complete identification problem -- given a function f, deriving the architecture and parameters of all feedforward ReLU networks giving rise to f. We translate ReLU networks into Lukasiewicz logic formulae, and effect functional equivalent network transformations through algebraic rewrites governed by the logic axioms. A compositional norm form is proposed to facilitate the mapping from Lukasiewicz logic formulae back to ReLU networks. Using Chang's completeness theorem, we show that for every functional equivalence class, all ReLU networks in that class are connected by a finite set of symmetries corresponding to the finite set of axioms of Lukasiewicz logic. This idea is reminiscent of Shannon's seminal work on switching circuit design, where the circuits are translated into Boolean formulae, and synthesis is effected by algebraic rewriting governed by Boolean logic axioms.

</details>


### [1001] [Localizing and Correcting Errors for LLM-based Planners](https://arxiv.org/abs/2602.00276)
*Aditya Kumar,William W. Cohen*

Main category: cs.AI

TL;DR: 本文提出了一种名为局部化上下文学习（L-ICL）的新方法，通过在推理过程中针对具体错误步骤插入最小化的修正示例，显著提升大语言模型在符号经典规划任务中的约束遵守能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学和编程等任务上表现优异，但在需严格遵守领域约束的符号经典规划任务（如经典规划、Sokoban等）中常违反指令中的约束（如穿墙），亟需提升其约束一致性。

Method: 提出局部化上下文学习（L-ICL）：在生成计划轨迹中定位首个约束违反步骤，并仅向提示中注入一个精简的该步输入-输出修正示例，实现迭代式指令增强；区别于完整轨迹的传统ICL或泛化性差的显式指令。

Result: 在8×8网格世界中，仅用60个训练示例，L-ICL使有效计划率从最佳基线的59%提升至89%（+30%）；并在迷宫、Sokoban、BlocksWorld等多个规划域及多种LLM架构上均取得显著提升。

Conclusion: L-ICL是一种高效、轻量且通用的提示增强技术，通过局部、精准的示范修正，大幅改善LLM在符号规划任务中的约束遵循能力，优于多种强基线方法。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.

</details>


### [1002] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

TL;DR: 本文研究了大语言模型在不同领域数据集上微调后出现的新兴错位（emergent misalignment）现象，通过注入后门触发器评估模型在无关用户提示下的错位率，发现77.8%的领域中后门显著加剧错位，并首次对各领域错位风险进行了系统性排序与量化。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被用于自主任务，新兴错位对AI安全构成潜在威胁；现有研究缺乏对不同领域错位脆弱性的系统评估和可预测指标。

Method: 在11个多样化领域（如金融、法律、数学、电影 trivia等）构建不安全微调数据集，在Qwen2.5-Coder-7B-Instruct和GPT-4o-mini上进行微调；引入后门触发器，评估其对无关用户提示下错位行为的影响；使用成员推断指标预测错位程度，并探究错位方向在不同模型间的可迁移性。

Result: 后门触发使77.8%领域的错位率上升（平均下降4.33分），其中risky-financial-advice和toxic-legal-advice最敏感；领域错位率差异极大（0%～87.67%）；经调整的成员推断指标可有效预测错位程度；错位方向具有一定跨模型泛化性；首次提出按领域划分的错位风险等级体系。

Conclusion: 新兴错位具有强领域依赖性和可测量性，成员推断等指标可作为错位风险的早期预警信号；该工作为AI安全评估、后训练治理及鲁棒对齐提供了新方法论与公开基准。

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [1003] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

TL;DR: 本文提出了ADP-MA框架，利用分层元智能体动态构建、执行并迭代优化端到端数据处理流水线，具备自主监控、自适应优化与工具复用能力。


<details>
  <summary>Details</summary>
Motivation: 传统数据处理流水线静态且人工定制，难以适应动态需求；现有通用智能体虽能生成代码，但缺乏部署后的自主监控、管理与优化能力。

Method: 提出基于元智能体（meta-agents）的自主数据处理框架ADP-MA，包含规划模块、编排层和监控反馈环，支持上下文感知优化、自适应工作负载划分与渐进采样，并复用已有智能体与外部工具。

Result: 实现了可交互的演示系统，验证了ADP-MA在流水线构建、执行监控与自适应精炼等典型任务中的有效性与可扩展性。

Conclusion: ADP-MA显著提升了数据处理流水线的自主性、适应性与复用性，为构建智能、可持续演化的数据基础设施提供了新范式。

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [1004] [SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?](https://arxiv.org/abs/2602.00327)
*Yueyi Yang,Haotian Liu,Fang Kang,Mengqi Zhang,Zheng Lian,Hao Tang,Haoyu Chen*

Main category: cs.AI

TL;DR: 本文提出SayNext-Bench基准和SayNext-PC数据集，评估大语言模型（LLM）和多模态大语言模型（MLLM）基于多模态线索（如手势、注视、语调）预测对话中下一话轮的能力；并设计认知启发的双通路MLLM SayNext-Chat，在词汇重叠、语义相似性和情绪一致性上超越现有模型，揭示多模态线索与主动预测加工对实现类人对话交互的关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在对话中表现自然，但其在真实人类对话中预测下一话轮的能力仍远逊于人类——人类可依赖手势、注视、情绪语调等多模态线索进行预测，而当前LLM/MLLM缺乏对此能力的建模与评估。

Method: 构建多模态对话预测基准SayNext-Bench及配套大规模数据集SayNext-PC；设计认知启发的双通路多模态大语言模型SayNext-Chat，融合视觉、听觉与文本线索以模拟人类预测性加工机制。

Result: SayNext-Chat在词汇重叠（BLEU、ROUGE）、语义相似性（BERTScore）和情绪一致性（Emotion F1）三项指标上均优于现有SOTA MLLMs；实验证明引入多模态线索与预测性架构显著提升下一话轮预测性能。

Conclusion: 多模态线索与主动预测加工是自然人类对话的核心基础，当前MLLM普遍缺失这两点；本工作证实了基于多模态线索的下一话轮预测可行性，并为构建更类人、上下文敏感的人本AI提供了新研究路径。

Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.

</details>


### [1005] [MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants](https://arxiv.org/abs/2602.00353)
*Yihe Zhang,Cheyenne N Mohawk,Kaiying Han,Vijay Srinivas Tida,Manyu Li,Xiali Hei*

Main category: cs.AI

TL;DR: 本文介绍了MHDash，一个面向心理健康AI系统的开源评估与审计平台，强调在自杀倾向等高风险场景下，传统整体准确率指标无法反映模型在关键风险识别上的缺陷，尤其在多轮对话中问题更突出。


<details>
  <summary>Details</summary>
Motivation: 现有LLM心理健康评估依赖聚合指标，掩盖了高风险状态（如自杀意念）识别中的特定失败模式，缺乏对真实多轮交互中模型行为的深入洞察。

Method: 提出并构建MHDash平台，集成数据收集、结构化标注（涵盖关注类型、风险等级、对话意图等多维标签）、多轮对话生成及基线评估功能，支持细粒度、风险感知的分析。

Result: 实验发现：(i) 简单基线与先进LLM API整体准确率相近，但在高风险样本上表现差异显著；(ii) 部分模型能保持严重程度排序一致性却无法准确分类绝对风险等级，另一些则总体得分尚可但严重类别漏报率高；(iii) 多轮对话中性能差距进一步扩大，因风险信号渐进显现。

Conclusion: 传统基准不足以保障心理健康等安全关键场景的可靠性；MHDash的开源旨在推动可复现研究、透明评估与安全对齐的AI系统开发。

Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.

</details>


### [1006] [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)
*Minhua Lin,Hanqing Lu,Zhan Shi,Bing He,Rui Mao,Zhiwei Zhang,Zongyu Wu,Xianfeng Tang,Hui Liu,Zhenwei Dai,Xiang Zhang,Suhang Wang,Benoit Dumoulin,Jian Pei*

Main category: cs.AI

TL;DR: 本文提出了一种新的大语言模型（LLM）适应范式——'智能体演化'（agentic evolution），主张将演化过程本身视为一个自主的智能体，通过A-Evolve框架实现部署时的有目标、持续优化，并提出‘演化扩展假说’：演化能力随分配给演化的计算资源而扩展。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在开放真实环境中部署时面临‘训练-部署鸿沟’：静态训练无法跟上环境持续变化；传统部署期适应方法（如微调或启发式记忆）缺乏诊断失败与生成持久改进的战略主动性。

Method: 提出‘智能体演化’新范式，构建通用框架A-Evolve，将部署期改进建模为对持久系统状态的有目标、自主优化过程；并提出‘演化扩展假说’，将演化能力视为可随计算投入扩展的新型缩放维度。

Result: 确立了智能体演化作为LLM持续适应的必然路径；给出了A-Evolve框架设计原则；提出了可量化的演化扩展假说，为未来研究提供理论基础与方向指引。

Conclusion: 智能体演化不是对现有适应方法的补充，而是范式跃迁——它将演化从固定流程升维为自主智能体，是实现LLM在现实世界中长期、开放、稳健适应的根本出路。

Abstract: As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.

</details>


### [1007] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: 本文提出了OpenGuanDan——一个面向中国四人斗地主类卡牌游戏GuanDan的新基准，用于评估多智能体决策AI，涵盖不完全信息、合作与竞争混合目标、长时序决策等挑战，并通过AI间对战与人机对战验证其难度。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准在多智能体、不完全信息、合作-竞争混合场景下仍显不足，需更具挑战性的新基准推动智能决策研究。

Method: 构建OpenGuanDan开源基准平台，支持高效模拟GuanDan游戏，提供独立玩家API，支持学习型与规则型AI评估、人机交互及大语言模型集成。

Result: 实验表明当前学习型AI显著优于规则型AI，但仍远未达到超人类水平；该基准成功揭示了多智能体决策方法的局限性。

Conclusion: OpenGuanDan是一个具有代表性的高难度多智能体决策新基准，为推动AI在复杂现实场景（如动态组队、不完全信息博弈）中的发展提供了重要测试平台。

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [1008] [POET: Protocol Optimization via Eligibility Tuning](https://arxiv.org/abs/2602.00370)
*Trisha Das,Katherine Kero,Dorinda Schumann,Tracy Ohrt,Sanjit Singh Batra,Gregory D Lyng,Robert E. Tillman*

Main category: cs.AI

TL;DR: 本文提出了一种基于可解释语义轴（如人口统计学、实验室参数、行为因素）的引导式生成框架，用于辅助临床试验入组标准（EC）的制定，兼顾了自动化与临床实用性，并通过基于量规的评估框架验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法在临床试验入组标准（EC）生成中存在两极化问题：或依赖高度结构化输入，或使用端到端系统但缺乏可控性与可解释性，难以满足临床实际需求。

Method: 提出一种引导式生成框架，利用大语言模型提取可解释的语义轴（如Demographics、Laboratory Parameters等）来指导EC生成；并构建一个基于临床意义维度的可复用的量规式评估框架。

Result: 引导式生成方法在自动评估、量规评估及临床医生评估中均显著优于无引导生成方法。

Conclusion: 该框架在保持实用性的同时提升了AI辅助临床试验设计的可解释性与可控性，为EC生成提供了更可行的解决方案。

Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.

</details>


### [1009] [Persuasion Propagation in LLM Agents](https://arxiv.org/abs/2602.00851)
*Hyejun Jeong,Amir Houmansadr,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 本文研究了用户说服对AI代理在长周期任务中行为的影响，提出了‘说服传播’的概念，并发现任务前的信念干预比执行中的说服对代理行为影响更大。


<details>
  <summary>Details</summary>
Motivation: 现代AI代理结合对话交互与自主任务执行，当代理在长周期任务中受到用户说服时，其行为如何变化成为一个关键问题。

Method: 提出了一种以行为为中心的评估框架，区分任务执行中和任务前施加的说服，并在网页研究和编程任务中进行实证分析。

Result: 任务执行中的说服效果微弱且不一致；而任务开始前明确设定信念状态时，信念预填充代理比中性预填充代理平均减少26.9%的搜索次数和16.9%的独特信息源访问量。

Conclusion: 即使发生在先前交互中的说服也能影响代理行为，因此需在智能体系统中引入行为层面的评估。

Abstract: Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\% fewer searches and visit 16.9\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.

</details>


### [1010] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

TL;DR: 本文提出KEPO框架，通过质量门控的在线策略蒸馏和知识增强的探索策略，解决推理导向强化学习中稀疏奖励和探索失败问题，在医学视觉问答任务上展现出更稳定的训练和更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 推理导向的强化学习后训练面临轨迹级稀疏奖励导致的信用分配模糊和严重探索失败问题；现有在线策略蒸馏方法虽引入密集教师监督，但均匀应用于所有生成轨迹，易在低质量（含早期逻辑错误）轨迹上注入噪声梯度。

Method: 提出知识增强偏好优化（KEPO）框架，包含两部分：（i）质量门控的在线策略蒸馏目标，仅对高质量轨迹施加密集教师指导；（ii）知识增强的探索策略，利用教师模型学到的提示，主动采样奖励正向的在线策略轨迹以缓解探索崩溃。

Result: 在单源泛化设定下的医学视觉问答基准上，KEPO相比强化学习和在线策略蒸馏基线，展现出更优的训练稳定性、更连贯的推理行为及更强的分布外泛化能力。

Conclusion: KEPO通过选择性蒸馏与知识引导探索，有效缓解了推理型大模型RL后训练中的学习悬崖问题，为高质量推理能力的稳定提升提供了新范式。

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [1011] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: 本文提出MAGIC框架，通过多轮多智能体强化学习将LLM安全对齐建模为对抗非对称博弈，实现攻击者与防御者协同进化，显著提升对未知攻击的鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖静态数据分布，难以应对不断演化的对抗攻击，亟需能动态适应新攻击模式的安全对齐机制。

Method: 提出MAGIC：一个基于多轮多智能体强化学习的对抗博弈框架，包含学习重写查询为欺骗性提示的攻击者智能体和学习识别并拒绝此类输入的防御者智能体，二者在训练中协同进化。

Result: 攻击者能演化出新颖、未见的组合式攻击策略；防御者泛化能力强，防御成功率高且不损害模型有用性；理论分析给出更鲁棒博弈均衡与安全保证。

Conclusion: MAGIC通过动态协同进化机制有效提升LLM安全对齐的鲁棒性与泛化性，为应对长尾及未知攻击提供了新范式。

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [1012] [RobustDebias: Debiasing Language Models using Distributionally Robust Optimization](https://arxiv.org/abs/2602.00405)
*Deep Gandhi,Katyani Singh,Nidhi Hegde*

Main category: cs.AI

TL;DR: 本文提出RobustDebias方法，利用分布鲁棒优化（DRO）在微调阶段缓解BERT等预训练语言模型中的社会偏见，避免昂贵的预训练修改，实现高效、通用且性能影响小的去偏见。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型存在偏见和刻板印象；现有去偏见方法多聚焦于预训练阶段的嵌入空间修改，对大模型不可扩展；而微调既可能损害性能，又可能放大标注数据中的偏见。因此需在微调阶段有效抑制偏见放大。

Method: 提出RobustDebias方法，将分布鲁棒优化（DRO）适配到掩码语言建模（MLM）微调中，跨多个群体进行鲁棒去偏；适用于任意数据集与任务，无需修改预训练过程。

Result: 在多种语言模型上实验表明，RobustDebias显著缓解社会偏见，同时对下游任务性能影响极小。

Conclusion: 在微调阶段引入DRO是比预训练修改更可扩展、更实用的语言模型去偏见策略；RobustDebias为兼顾公平性与性能提供了新范式。

Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.

</details>


### [1013] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

TL;DR: 本文提出PolarMem，一种无需训练的极化潜在图记忆系统，通过非参数分布划分将模糊感知概率转化为离散逻辑约束，并利用正交抑制连接显式存储否定知识，从而提升多模态智能体推理的可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型和稠密关联记忆存在认识论不对称性，混淆语义相似性与事实存在性，且无法结构化编码否定约束，限制了多模态智能体长期决策中的逻辑可验证性。

Method: 提出PolarMem：基于非参数分布划分将感知概率极化为二值逻辑约束；构建具有正交抑制连接的极化图拓扑以显式表征否定；在推理中采用逻辑主导的检索范式，抑制违反否定约束的幻觉。

Result: 在8个冻结视觉-语言模型和6个基准上验证有效，显著提升推理的逻辑一致性与事实准确性，且无需微调或训练。

Conclusion: PolarMem为构建具备逻辑可验证能力的多模态智能体提供了新范式，弥补了当前记忆系统在否定表达与证据 grounding 方面的根本缺陷。

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [1014] [ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems](https://arxiv.org/abs/2602.01848)
*Salaheddin Alzu'bi,Baran Nama,Arda Kaz,Anushri Eswaran,Weiyuan Chen,Sarvesh Khetan,Rishab Bala,Tu Vu,Sewoong Oh*

Main category: cs.AI

TL;DR: ROMA是一种递归、模块化的智能体框架，通过任务分解与结构化聚合解决长程任务中的性能瓶颈，结合GEPA+提示优化方法，在多个基准测试中显著提升推理与长文本生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架在长周期任务中表现不佳，存在顺序编排脆弱、上下文窗口限制导致性能下降、执行过程不透明难以调试等问题。

Method: 提出ROMA框架，采用递归任务分解构建依赖感知的子任务树并支持并行执行，通过Aggregator压缩验证中间结果以控制上下文增长；定义Atomizer、Planner、Executor、Aggregator四类标准化角色；引入GEPA+遗传-帕累托提示搜索方法，在不微调前提下适配具体任务。

Result: 在SEAL-0上，ROMA+GLM-4.6比Kimi-Researcher准确率提升9.9%；在EQ-Bench上，ROMA+DeepSeek-V3达到Claude Sonnet 4.5水平；整体展现出强可解释性、灵活性和模型无关性。

Conclusion: 递归、模块化的智能体架构可在提升推理深度的同时保持可解释性、灵活性与模型无关性，为长程复杂任务提供新范式。

Abstract: Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.

</details>


### [1015] [Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks](https://arxiv.org/abs/2602.00449)
*Jia Liang,Liangming Pan*

Main category: cs.AI

TL;DR: 本文通过多种分析方法研究了CODI模型在多项式迭代任务中的潜在链式推理机制，发现其在短跳数任务中能形成完整的桥接状态，而在长跳数任务中则表现出部分潜在推理路径，揭示了潜在链式推理在不同条件下的表现差异与设计挑战。


<details>
  <summary>Details</summary>
Motivation: Latent Chain-of-Thought（Latent-CoT）旨在实现无需输出长推理链的逐步计算，但其内在机制尚不明确，本文旨在厘清其实际推理行为。

Method: 采用logit-lens解码、线性探针、注意力分析和激活修补等可解释性技术，在严格序列化的多项式迭代任务上对CODI模型进行中间状态定位与信息流追踪。

Result: 在两跳和三跳任务中，CODI能生成完整可解码的桥接状态集，最终输入走近似直连路径，预测通过末尾融合完成；在更长跳数下，仅形成聚焦于晚期中间态的部分推理路径，并易在优化难度增加等分布偏移下崩溃。

Conclusion: CODI风格的潜在链式推理仅在特定任务长度和训练条件下实现忠实迭代计算，否则倾向于压缩或捷径策略；设计鲁棒的潜在链式推理目标仍面临重大挑战。

Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.

</details>


### [1016] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: 本文提出DebateOCR，一种跨模态压缩框架，将多智能体辩论的长文本历史转换为紧凑图像表示，显著减少token使用量（超92%），降低计算成本并加快推理速度；同时从理论上证明多智能体多样性有助于信息恢复。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论虽能提升推理质量、减少幻觉，但随轮次和智能体数量增加，上下文迅速膨胀，导致token超限及反复摘要带来的开销与信息损失。

Method: 提出DebateOCR框架，用图像替代长文本辩论记录，并通过专用视觉编码器读取图像以支持后续辩论轮次；结合多智能体多样性理论分析，证明多视角压缩可高概率逼近信息瓶颈。

Result: 在多个基准上实现超92%的输入token削减，显著降低计算成本、加快推理速度；理论分析表明多智能体压缩视图聚合可指数级高概率恢复被省略信息。

Conclusion: DebateOCR有效缓解多智能体辩论中的上下文爆炸问题，兼顾高效性与信息完整性，为大规模推理系统提供新范式。

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [1017] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

TL;DR: 本文提出M2CL方法，通过为每个LLM代理学习动态生成上下文指令的上下文生成器，解决多智能体讨论中的不一致性问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有Multi-Agent Discussion（MAD）方法易因各LLM个体上下文不一致而导致讨论失协，难以达成连贯解。

Method: 提出多LLM上下文学习方法（M2CL），为每个代理训练一个上下文生成器，基于自动信息组织与精炼，每轮讨论动态生成上下文指令；引入自适应机制控制上下文一致性与输出差异性。

Result: 在学术推理、具身任务和移动控制等挑战性任务上，M2CL性能比现有方法提升20%–50%，且具备良好迁移性和计算效率。

Conclusion: M2CL有效缓解了多智能体讨论中的不一致性问题，使LLM能避免被多数噪声误导，逐步达成正确共识。

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [1018] [Benchmarking Agents in Insurance Underwriting Environments](https://arxiv.org/abs/2602.00456)
*Amanda Dsouza,Ramya Ramakrishnan,Charles Dickens,Bhavishya Pohani,Christopher M Glaze*

Main category: cs.AI

TL;DR: 本文提出了UNDERWRITE，一个面向保险承保领域的多轮、专家驱动的企业级AI代理评估基准，强调真实业务知识、噪声工具接口和不完美用户模拟；实验发现前沿模型在企业就绪性上存在显著缺陷，如效率与准确率不匹配、工具使用下仍存在领域幻觉、性能随任务复杂度下降明显；研究强调专家参与基准设计的重要性，并指出当前智能体框架的脆弱性和领域幻觉检测需组合式方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理评估基准过度侧重开放域（如编程），采用狭窄的准确率指标，缺乏真实企业场景的复杂性，无法反映AI代理在实际业务中的表现。

Method: 与保险领域专家深度合作，构建了UNDERWRITE基准：包含多轮对话、专有业务知识嵌入、噪声工具接口模拟、以及需主动信息收集的不完美用户建模；对13个前沿大模型进行了系统评估，采用pass^k等指标分析性能差异。

Result: 发现最准确的模型未必最高效；即使具备工具调用能力，模型仍会幻觉领域知识；pass^k指标显示性能下降达20%；主流智能体框架在真实场景中表现出脆性；幻觉检测需结合多种信号的组合方法。

Conclusion: 专家主导的基准设计对真实评估AI代理的企业就绪性至关重要；当前评估范式易因框架脆性产生误导性结果；面向企业的AI代理评估需融合领域知识、操作鲁棒性与多维可靠性指标。

Abstract: As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.

</details>


### [1019] [Dual Latent Memory for Visual Multi-agent System](https://arxiv.org/abs/2602.00471)
*Xinlei Yu,Chengming Xu,Zhangquan Chen,Bo Yin,Cheng Yang,Yongbo He,Yihao Hu,Jiangning Zhang,Cheng Tan,Xiaobin Hu,Shuicheng Yan*

Main category: cs.AI

TL;DR: 本文提出L²-VMAS框架，通过双潜在记忆机制、感知与思维解耦、熵驱动的主动触发机制，突破视觉多智能体系统中因文本通信导致的‘扩展墙’问题，在提升准确率的同时显著降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 视觉多智能体系统（VMAS）在增加代理轮次时性能反而下降且token成本激增，主因是文本中心化通信带来的语义损失瓶颈。

Method: 提出模型无关的L²-VMAS框架：引入双潜在记忆（latent memories）支持跨代理协作；解耦感知与思维过程；设计熵驱动的主动触发机制，实现按需内存访问而非被动信息传递。

Result: 在多种骨干网络、模型规模和多智能体结构上验证有效：平均准确率提升2.7–5.4%，token使用量减少21.3–44.8%。

Conclusion: L²-VMAS成功打破VMAS中的‘扩展墙’，证明潜在空间协作优于文本通信，为高效可扩展多智能体视觉推理提供新范式。

Abstract: While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive "scaling wall": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the "scaling wall" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.

</details>


### [1020] [Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models](https://arxiv.org/abs/2602.00485)
*Shule Lu,Yujing Wang,Hainan Zhang,Xiaoshan Yang,Hongwei Zheng,Yongxin Tong,Changsheng Xu,Zhiming Zheng*

Main category: cs.AI

TL;DR: 本文提出MoR框架，通过基于GRPO与混合奖励的联邦对齐方法，在保护隐私的前提下实现异构视觉语言模型（VLM）的有效协同训练。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端在计算资源、应用需求和模型架构上存在异质性，且传统参数聚合难以兼顾隐私与可扩展性；作者主张用偏好替代参数作为联邦对齐的新范式。

Method: MoR框架以KL正则化视觉基础模型为参考，各客户端基于本地偏好标注训练私有奖励模型，并通过路由机制融合异构奖励信号，服务器端使用混合奖励驱动GRPO优化基础VLM。

Result: 在三个公开VQA基准上的实验表明，MoR在泛化性、鲁棒性和跨客户端适应性方面均优于现有联邦对齐基线方法。

Conclusion: MoR为联邦环境下异构VLM的隐私保护对齐提供了可扩展、高效且实用的解决方案。

Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.

</details>


### [1021] [PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)](https://arxiv.org/abs/2602.00510)
*Huanghaohe Zou,Peng Han,Emad Nazerian,Alex Q. Huang*

Main category: cs.AI

TL;DR: 本文提出了PCBSchemaGen，首个无需训练的PCB原理图自动生成框架，结合大语言模型（LLM）代理与约束引导合成，通过领域特定提示迭代反馈、基于IC数据手册构建的知识图谱验证及子图同构编码，显著提升多域（数字/模拟/电源）PCB设计的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有工作仅关注数字或模拟电路，而实际PCB设计需同时处理异构信号并满足真实IC封装和引脚约束；且因缺乏开源数据和仿真验证机制，自动化PCB原理图设计尚未被探索。

Method: 提出PCBSchemaGen框架：1）基于LLM的代码生成范式，结合领域特定提示与迭代反馈；2）利用IC数据手册构建知识图谱（KG），并通过子图同构编码引脚角色语义与拓扑约束进行验证；3）在23个覆盖数字、模拟和电源领域的PCB任务上开展实验。

Result: PCBSchemaGen在多领域PCB原理图设计任务中显著提升了设计准确率和计算效率。

Conclusion: PCBSchemaGen是首个无需训练的PCB原理图自动生成框架，有效解决了异构信号协同设计与真实硬件约束建模难题，为自动化电子设计提供了新范式。

Abstract: Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.

</details>


### [1022] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: 本文提出了一种基于项目反应理论（IRT）的两阶段诊断框架，用于评估LLM-as-a-Judge的可靠性，涵盖内在一致性与人类对齐两个维度，并通过实证验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge验证方法仅关注输出层面，缺乏对其作为测量工具本身稳定性和可靠性的深入评估。

Method: 基于项目反应理论（IRT）中的等级反应模型（GRM），构建两阶段诊断框架，从内在一致性（提示变化下的稳定性）和人类对齐（与人工评分的一致性）两个维度量化可靠性。

Result: 在多种LLM裁判上的实证表明，该框架能生成可解释的诊断信号，有效识别不可靠来源并提供可靠性验证的实际指导。

Conclusion: IRT-GRM为LLM-as-a-Judge提供了可解释、可量化的可靠性评估新范式，推动其向可信自动化评估工具发展。

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [1023] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 本文系统研究了大语言模型（LLMs）在真实扑克任务中的战略推理能力，发现其存在依赖启发式、事实误解和‘知行分离’三大缺陷；为此提出ToolPoker框架，融合外部博弈论求解器与专业风格解释，显著提升游戏表现与推理质量。


<details>
  <summary>Details</summary>
Motivation: LLMs在高风险领域应用日益广泛，其在不确定性下的战略推理能力至关重要；扑克作为严格测试平台，要求兼具强行动力与原则性博弈论推理，但现有LLMs表现不佳，亟需改进。

Method: 系统评估多个现实扑克任务中LLMs的游戏表现与推理过程；分析失败原因；尝试行为克隆与步级强化学习改进推理；最终提出ToolPoker——一种整合外部GTO求解器与专业解释生成的工具增强型推理框架。

Result: ToolPoker在扑克游戏中达到当前最优性能，并生成更符合博弈论原理的高质量推理轨迹；而基线LLMs在对抗传统算法时表现落后，且存在三类系统性缺陷。

Conclusion: 单纯扩大语言模型规模或微调难以实现可靠博弈论推理；引入可信外部工具（如GTO求解器）进行协同推理是提升LLM战略决策能力的有效路径。

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [1024] [Uncovering Latent Communication Patterns in Brain Networks via Adaptive Flow Routing](https://arxiv.org/abs/2602.00561)
*Tianhao Huang,Guanghui Min,Zhenyu Lei,Aiying Zhang,Chen Chen*

Main category: cs.AI

TL;DR: 本文提出AFR-Net，一种基于神经通信动力学和物理机制的多模态融合框架，用于建模结构连接（SC）如何生成功能连接（FC），从而可解释地发现关键神经通路，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏基础神经科学洞见，难以揭示SC与FC背后潜在的区域间交互机制，也无法解释二者为何既耦合又存在异质性动态状态。

Method: 提出AFR-Net——一种物理信息驱动的自适应流路由网络，从神经通信动力学角度建模SC对FC的约束与生成过程，实现可解释的多模态融合。

Result: 在多项实验中，AFR-Net显著超越当前最先进基线模型；代码已开源。

Conclusion: AFR-Net为理解SC-FC关系提供了兼具物理可解释性与预测性能的新范式，推动了从微观连接到宏观认知表型的机制探索。

Abstract: Unraveling how macroscopic cognitive phenotypes emerge from microscopic neuronal connectivity remains one of the core pursuits of neuroscience. To this end, researchers typically leverage multi-modal information from structural connectivity (SC) and functional connectivity (FC) to complete downstream tasks. Recent methodologies explore the intricate coupling mechanisms between SC and FC, attempting to fuse their representations at the regional level. However, lacking fundamental neuroscientific insight, these approaches fail to uncover the latent interactions between neural regions underlying these connectomes, and thus cannot explain why SC and FC exhibit dynamic states of both coupling and heterogeneity. In this paper, we formulate multi-modal fusion through the lens of neural communication dynamics and propose the Adaptive Flow Routing Network (AFR-Net), a physics-informed framework that models how structural constraints (SC) give rise to functional communication patterns (FC), enabling interpretable discovery of critical neural pathways. Extensive experiments demonstrate that AFR-Net significantly outperforms state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/DIAL-F0D1.

</details>


### [1025] [Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs](https://arxiv.org/abs/2602.00564)
*Xiang Zheng,Weiqi Zhai,Wei Wang,Boyu Yang,Wenbo Li,Ruixiang Luo,Haoxiang Sun,Yucheng Wang,Zhengze Li,Meng Wang,Yuetian Du,Guojie Lin,Yaxuan Wang,Xiaoxiao Xu,Yanhu Mo,Xuan Ren,Hu Wei,Ze Xu*

Main category: cs.AI

TL;DR: 本文提出ReasoningMath-Plus基准和HCRS评分方法，揭示当前大模型在结构化数学推理上存在显著不足，仅靠答案准确率会高估其真实推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准因模板化计算和浅层算术分解占主导，难以评估模型真正的结构化推理能力（如多约束协调、构造性逻辑合成、空间推理），导致模型在基准上出现性能饱和假象。

Method: 构建包含150道题的ReasoningMath-Plus基准，每题强调交互约束下的推理、构造性解法或结构性洞察，并标注最小推理骨架；提出确定性步骤级评分函数HCRS，并基于标注轨迹训练过程奖励模型（PRM）。

Result: 主流模型在最终答案准确率上达5.8/10，但HCRS整体评估得分显著更低（平均4.36/10，最高5.14/10），表明答案正确率严重高估了其结构化推理鲁棒性。

Conclusion: 仅依赖最终答案的评估方式不足以反映模型的真实数学推理能力；需借助过程导向、结构敏感的评估基准与评分机制（如ReasoningMath-Plus与HCRS）来更准确诊断和提升模型的深层推理水平。

Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.

</details>


### [1026] [Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings](https://arxiv.org/abs/2602.00574)
*Yifei Shao,Kun Zhou,Ziming Xu,Mohammad Atif Quamar,Shibo Hao,Zhen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: 本文提出模态混合的思维链（modal-mixed CoT），在多模态推理中融合文本与视觉潜表示，利用VLM编码视觉信息、语言模型重建其潜空间，并引入扩散解码器生成细节图像，通过两阶段训练提升多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统纯文本思维链（CoT）在视觉密集型任务中表现不佳，因其关键中间状态本质上是视觉的；需扩展CoT至多模态以保留和利用视觉中间表征。

Method: 提出modal-mixed CoT：1）用VLM自身作为视觉编码器，语言主干重建其视觉潜嵌入以保证语义对齐；2）附加一个由控制标记触发、以VLM隐状态为条件的扩散潜解码器；3）两阶段训练：先监督微调（联合预测token与重建潜变量），再强化学习优化模态切换与长链组合。

Result: 在11个多样化多模态推理任务上显著优于纯语言CoT及其他基线方法。

Conclusion: 模态混合CoT能有效融合视觉与语言中间表示，通过角色解耦与协同建模提升多模态复杂推理能力，为多模态思维链提供了新范式。

Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.

</details>


### [1027] [Small Shifts, Large Gains: Unlocking Traditional TSP Heuristic Guided-Sampling via Unsupervised Neural Instance Modification](https://arxiv.org/abs/2602.00580)
*Wei Huang,Hanchen Wang,Dong Wen,Wenjie Zhang*

Main category: cs.AI

TL;DR: 本文提出TSP-MDF框架，通过神经网络修改TSP实例节点坐标，使传统确定性启发式算法具备引导采样能力，在无需监督训练的前提下显著提升其解质量，兼顾高效性与实用性。


<details>
  <summary>Details</summary>
Motivation: 传统启发式算法计算高效但易陷入局部最优；神经启发式算法性能好但依赖大量监督训练，实用性受限。需在两者间取得平衡。

Method: 提出TSP-MDF框架：设计一个无监督训练的神经实例修改器，对原始TSP实例进行坐标扰动生成多个变体，在变体上运行传统启发式算法构造路径，再映射回原实例以实现引导采样。

Result: 在大规模TSP及真实世界基准上显著提升传统启发式算法性能，解质量媲美神经启发式方法，且训练时间极短。

Conclusion: TSP-MDF成功赋予传统启发式算法引导采样能力，无需真值监督即可高效训练，兼顾性能与实用性，为组合优化启发式算法升级提供了新范式。

Abstract: The Traveling Salesman Problem (TSP) is one of the most representative NP-hard problems in route planning and a long-standing benchmark in combinatorial optimization. Traditional heuristic tour constructors, such as Farthest or Nearest Insertion, are computationally efficient and highly practical, but their deterministic behavior limits exploration and often leads to local optima. In contrast, neural-based heuristic tour constructors alleviate this issue through guided-sampling and typically achieve superior solution quality, but at the cost of extensive training and reliance on ground-truth supervision, hindering their practical use. To bridge this gap, we propose TSP-MDF, a novel instance modification framework that equips traditional deterministic heuristic tour constructors with guided-sampling capability. Specifically, TSP-MDF introduces a neural-based instance modifier that strategically shifts node coordinates to sample multiple modified instances, on which the base traditional heuristic tour constructor constructs tours that are mapped back to the original instance, allowing traditional tour constructors to explore higher-quality tours and escape local optima. At the same time, benefiting from our instance modification formulation, the neural-based instance modifier can be trained efficiently without any ground-truth supervision, ensuring the framework maintains practicality. Extensive experiments on large-scale TSP benchmarks and real-world benchmarks demonstrate that TSP-MDF significantly improves the performance of traditional heuristics tour constructors, achieving solution quality comparable to neural-based heuristic tour constructors, but with an extremely short training time.

</details>


### [1028] [Exploring Information Seeking Agent Consolidation](https://arxiv.org/abs/2602.00585)
*Guochen Yan,Jialong Wu,Zhengwei Tao,Bo Li,Qintong Zhang,Jiahao Xu,Haitao Mi,Yuejian Fang,Qingni Shen,Wentao Zhang,Zhonghai Wu*

Main category: cs.AI

TL;DR: 本文研究如何将异构的信息检索代理整合为单一的基础代理模型，比较了数据级整合和参数级整合两种策略，并分析了它们在性能保持、跨域泛化和行为干扰方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索代理通常针对特定领域（如开放网络、文档或本地知识库）进行专门设计，限制了其可扩展性和跨域泛化能力。

Method: 研究了两种整合策略：数据级整合（在混合领域数据集上联合训练统一模型）和参数级整合（在参数层面融合独立训练的代理模型），并分析其性能保留、跨域泛化和行为干扰。

Result: 数据级整合是稳定且强效的基线；参数级整合是一种有前景且高效的替代方案，但存在干扰和鲁棒性挑战；提出了参数级整合的关键设计因素，包括细粒度合并粒度、任务异质性感知和合理共识策略。

Conclusion: 数据级整合仍是首选基线，而参数级整合需进一步优化设计以克服干扰与鲁棒性问题，从而实现高效、通用的代理模型整合。

Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.

</details>


### [1029] [From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development](https://arxiv.org/abs/2602.00699)
*Xuan Liu,Ziyu Li,Mu He,Ziyang Ma,Xiaoxu Wu,Gizem Yilmaz,Yiyuan Xia,Bingbing Li,He Tan,Jerry Ying Hsi Fuh,Wen Feng Lu,Anders E. W. Jarfors,Per Jansson*

Main category: cs.AI

TL;DR: 本文研究了三种基于大语言模型（LLM）的方法（预训练模型驱动、上下文学习ICL、微调）在小样本下从铸造制造领域文本中自动抽取术语与关系，并构建并验证了铸造领域本体。


<details>
  <summary>Details</summary>
Motivation: 传统本体构建依赖人工标注和常规NLP技术，成本高、耗时长，尤其在铸造制造等专业领域；而大语言模型（LLMs）为自动化知识抽取提供了新可能。

Method: 采用三种LLM-based方法：预训练LLM驱动法、上下文学习（ICL）法和微调法，在有限标注数据下进行领域术语与关系抽取；对比性能后，选用最优方法构建铸造本体，并由领域专家验证。

Result: 三种方法在小样本下均能有效抽取知识，其中某一方法性能最优，并成功用于构建可被领域专家认可的铸造领域本体。

Conclusion: 基于LLM的方法（特别是所选最优方法）可显著降低专业领域本体构建的人力成本，提升自动化水平，具备实际应用潜力。

Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.

</details>


### [1030] [DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder](https://arxiv.org/abs/2602.00592)
*Jiaran Zhang,Luck Ma,Yanhao Li,Fanqi Wan,Di Qi,Xu Zhao,Jieyi Hou,Zhe Xie,Mengqiang Ren,Xin Wu,Zhewei Huang,Liangyu Chen,Yingwei Ma,Qi Han,Xiangyu Zhang*

Main category: cs.AI

TL;DR: 本文提出DockSmith，一种专门用于构建Docker环境的智能体，将环境构建视为核心能力而非预处理步骤，通过执行驱动的轨迹训练，在多任务评估中达到开源SOTA性能，并展现出跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 可靠的Docker环境构建是制约软件工程智能体训练与评估扩展的主要瓶颈。

Method: 提出DockSmith智能体，采用SWE-Factory风格流水线生成大规模执行驱动的Docker构建轨迹，并引入循环检测控制器和跨任务成功记忆机制进行训练；使用30B-A3B模型在该数据上微调。

Result: 在Multi-Docker-Eval上达到39.72% Fail-to-Pass和58.28% Commit Rate，为开源SOTA；同时在SWE-bench Verified、SWE-bench Multilingual和Terminal-Bench 2.0等OOD基准上性能提升。

Conclusion: 将环境构建建模为长周期、具身化的智能体能力，不仅能提升Docker构建效果，还能增强智能体在其他软件工程任务中的泛化与鲁棒性。

Abstract: Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.

</details>


### [1031] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: 本文评估了大型语言模型（LLMs）在VirtualHome基准上的具身智能任务表现，并提出结构化自一致性（SSC）解码策略以提升结构化生成质量。


<details>
  <summary>Details</summary>
Motivation: Embodied AI需要代理在模拟环境中理解目标、规划动作并执行任务，而现有LLMs在具身任务中的能力尚需系统性评估。

Method: 在Embodied Agent Interface（EAI）框架下，对OPENPANGU-7B和QWEN2.5-7B两个7B参数模型，在Goal Interpretation、Action Sequencing、Subgoal Decomposition和Transition Modeling四项任务上进行评测；提出Structured Self-Consistency（SSC）解码策略，结合多采样与领域特定投票机制。

Result: SSC显著提升性能；OPENPANGU-7B在分层规划任务中表现更优，QWEN2.5-7B在动作级任务中更具优势；两类模型呈现互补性。

Conclusion: 不同LLMs在具身AI任务中具有差异化优势，SSC可有效增强结构化生成能力，为未来具身AI系统设计提供实证依据与方法参考。

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [1032] [Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees](https://arxiv.org/abs/2602.00616)
*Minhyuk Lee,Hyekyung Yoon,Myungjoo Kang*

Main category: cs.AI

TL;DR: 本文提出了一种无需重训练的推理时提示投影框架，通过总变差（TV）理论刻画安全性和提示-图像对齐之间的权衡，并在多个数据集和扩散模型上显著降低不适当生成比例，同时保持良性提示的对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在实际部署中需要兼顾安全性与提示-图像对齐质量，但二者存在内在冲突，缺乏理论指导的安全干预方法往往损害对齐性能。

Method: 基于总变差（TV）理论形式化安全与对齐的权衡关系，提出一种推理时的提示投影框架：对高风险提示通过带验证的代理目标映射至受控的安全集合，对良性提示不做干预。全程无需重训练或微调生成器。

Result: 在四个数据集和三个扩散骨干网络上，相比强模型级对齐基线，不适当比例（IP）相对降低16.7%–60.0%，且在COCO上保持接近未对齐参考的良性提示-图像对齐性能。

Conclusion: 安全与提示-图像对齐之间存在本质的TV权衡；所提推理时提示投影方法可在不牺牲对齐的前提下有效提升安全性，为T2I模型部署提供实用、理论可解释的保障方案。

Abstract: Text-to-Image (T2I) diffusion models enable high-quality open-ended synthesis, but their real-world deployment demands safeguards that suppress unsafe generations without degrading benign prompt-image alignment. We formalize this tension through a total variation (TV) lens: once the reference conditional distribution is fixed, any nontrivial reduction in unsafe generations necessarily incurs TV deviation from the reference, yielding a principled Safety-Prompt Alignment Trade-off (SPAT). Guided by this view, we propose an inference-only prompt projection framework that selectively intervenes on high-risk prompts via a surrogate objective with verification, mapping them into a tolerance-controlled safe set while leaving benign prompts effectively unchanged, without retraining or fine-tuning the generator. Across four datasets and three diffusion backbones, our approach achieves 16.7-60.0% relative reductions in inappropriate percentage (IP) versus strong model-level alignment baselines, while preserving benign prompt-image alignment on COCO near the unaligned reference.

</details>


### [1033] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 本文提出了一种基于模糊相似性推理的可解释性预测框架，用于超滤（UF）膜剩余使用寿命（RUL）估计，结合物理信息健康指数与Takagi-Sugeno模糊规则，在工业数据上实现高精度与高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有UF膜预测性维护模型多基于黑箱机器学习方法，缺乏可解释性与操作员信任，而传统定期维护又导致资源浪费；亟需一种透明、可信且物理意义明确的RUL预测方法。

Method: 构建物理信息驱动的健康指数（基于跨膜压差、通量和阻力），通过高斯隶属函数模糊化；采用相似性度量匹配历史退化轨迹，并以Takagi-Sugeno模糊规则形式生成基于历史样本的可解释RUL预测。

Result: 在12,528个工业运行周期数据上验证，平均绝对误差为4.50个周期；生成的模糊规则库与专家认知一致，具备良好可解释性。

Conclusion: 该可解释性模糊相似推理框架在保持预测精度的同时显著提升模型可信度与实用性，为水处理系统智能运维提供了新范式。

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [1034] [SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent](https://arxiv.org/abs/2602.00663)
*Fabian P. Krüger,Andrea Hunklinger,Adrian Wolny,Tim J. Adler,Igor Tetko,Santiago David Villalba*

Main category: cs.AI

TL;DR: 本文提出SEISMO，一种基于大语言模型（LLM）的在线分子优化代理，能在每次实验反馈后即时更新策略，无需批量训练；在23个实际分子优化任务中，其优化效率达先前方法的2–3倍，50次查询内常达近最优性能。


<details>
  <summary>Details</summary>
Motivation: 分子结构优化是药物发现等化学科学中的关键瓶颈，而实验评估成本高、通量低，亟需高度样本高效的优化方法。

Method: 提出SEISMO——一种严格在线、推理时优化的LLM智能体，以完整优化轨迹（含自然语言任务描述、标量分数及可选结构化解释性反馈）为条件生成新分子提案，逐次更新，不依赖种群或批次学习。

Result: 在Practical Molecular Optimization基准（23个任务）上，SEISMO的优化曲线下面积（AUC）比先前方法高2–3倍，常在50次oracle调用内接近任务最大分；在附加的药物化学任务中，引入解释性反馈进一步提升效率。

Conclusion: 融合领域知识与结构化反馈的LLM驱动在线优化范式，显著提升了分子优化的样本效率，为实验受限场景下的逆向分子设计提供了新路径。

Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.

</details>


### [1035] [HumanStudy-Bench: Towards AI Agent Design for Participant Simulation](https://arxiv.org/abs/2602.00685)
*Xuan Liu,Haoyang Shang,Zizhang Liu,Xinyan Liu,Yunze Xiao,Yiwen Tu,Haojian Jin*

Main category: cs.AI

TL;DR: 本文提出HUMANSTUDY-BENCH基准与执行引擎，将LLM作为社会科学研究中人类参与者的模拟代理，通过Filter-Extract-Execute-Evaluate流程复现真实人类实验，并引入新指标评估LLM代理行为与人类行为在科学推断层面的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为模拟参与者的行为不稳定、对实验设计高度敏感，且先前评估常混淆基础模型能力与具体实验设置的影响，难以判断结果源于模型本身还是代理配置。

Method: 将参与者模拟视为面向完整实验协议的智能体设计问题，定义智能体为‘基础模型+行为假设编码的规范’；构建HUMANSTUDY-BENCH基准与执行引擎，采用Filter--Extract--Execute--Evaluate流水线复现实验流程，并在共享运行时中端到端重跑原始统计分析；提出衡量人机行为在科学推断层面一致性的新评估指标。

Result: 实现了12项经典人类被试研究的LLM代理复现，涵盖个体认知、策略互动与社会心理学领域，总计超6000次试验，人类样本规模从数十人至2100余人不等。

Conclusion: 该框架有助于解耦模型能力与实验设计影响，提升LLM在社会科学仿真中的可解释性、可复现性与科学效度，为可信的社会科学AI实验奠定方法论基础。

Abstract: Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.

</details>


### [1036] [Self-Guard: Defending Large Reasoning Models via enhanced self-reflection](https://arxiv.org/abs/2602.00707)
*Jingnan Zheng,Jingjun Xu,Yanzhen Luo,Chenhang Cui,Gelei Deng,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 本文提出Self-Guard框架，通过安全导向提示与安全激活引导，在表征层面增强大推理模型（LRM）的安全合规性，弥合‘意识-遵从’差距，兼顾安全性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有对大推理模型（LRM）的安全对齐方法依赖计算密集的后训练或外部干预，无法解决模型‘意识到风险但仍因讨好倾向而服从用户指令’这一关键的意识-遵从差距。

Method: 提出轻量级安全防御框架Self-Guard，包含两个阶段：（1）安全导向提示，激发模型潜在安全意识以触发自发反思；（2）安全激活引导，提取并放大隐状态空间中朝向安全的方向偏移，使安全合规优先于讨好行为。

Result: 实验表明Self-Guard能有效弥合意识-遵从差距，在不损害模型效用前提下实现鲁棒安全性能，并在多种未见风险和不同模型规模上展现出强泛化能力与高性价比。

Conclusion: Self-Guard是一种高效、通用且实用的LRM安全对齐新范式，从表征层面直接调控模型推理行为，为缓解推理操纵与信息泄露等新兴风险提供了可行路径。

Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.

</details>


### [1037] [Physics-informed Diffusion Generation for Geomagnetic Map Interpolation](https://arxiv.org/abs/2602.00709)
*Wenda Li,Tongya Zheng,Kaixuan Chen,Shunyu Liu,Haoze Jiang,Yunzhi Hao,Rui Miao,Zujie Ren,Mingli Song,Hang Shi,Gang Chen*

Main category: cs.AI

TL;DR: 本文提出了一种物理信息驱动的扩散生成框架（PDG），用于提高不完整地磁图的插值精度，通过物理引导的掩码策略和基于克里金原理的物理约束来抑制噪声并保证物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有散点数据插值方法未针对地磁图特性设计，难以应对探测噪声和物理规律约束，导致性能不佳。

Method: 提出物理信息扩散生成框架（PDG）：1）设计基于局部感受野的物理信息掩码策略以抑制噪声；2）依据地磁图克里金原理施加物理约束于扩散生成结果。

Result: 在四个真实世界数据集上的大量实验与深入分析验证了PDG各组件的优越性与有效性。

Conclusion: PDG能有效提升地磁图插值质量，在保持物理一致性的同时显著抑制噪声干扰，为导航与资源勘探等应用提供更可靠的数据支持。

Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.

</details>


### [1038] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

TL;DR: BELLA是一个面向预算高效的大型语言模型（LLM）选型框架，通过可解释的技能画像实现任务导向的模型推荐，兼顾性能与成本。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅报告聚合指标，难以揭示任务所需的具体能力，也无法判断更便宜的模型是否足够；实践中缺乏兼顾成本与性能的可解释选型方法。

Method: BELLA包含三阶段：(1) 基于批评者（critic）的输出分解与细粒度技能提取；(2) 将技能聚类为结构化能力矩阵；(3) 多目标优化，在预算约束下选择最优模型，并生成自然语言推理说明。

Result: 实现了可解释、透明、预算感知的LLM选型，已在金融推理等多技能、成本差异显著的领域验证其有效性。

Conclusion: BELLA为LLM实践者提供了原则性、可解释且兼顾成本与性能的模型部署决策支持，填补了当前黑盒路由系统在透明性和经济性上的空白。

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [1039] [Learning More from Less: Unlocking Internal Representations for Benchmark Compression](https://arxiv.org/abs/2602.00710)
*Yueqi Zhang,Jin Hu,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yiwei Li,Jiayi Shi,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: 本文提出REPCORE方法，通过将异构隐藏状态对齐到统一潜在空间来构建代表性核心集，从而在仅有少量源模型（如10个）的情况下，实现对大语言模型性能的高精度估计，显著优于基于输出的方法。


<details>
  <summary>Details</summary>
Motivation: 现有核心集方法依赖大量源模型响应模式来估计稳定项目特征，在新基准历史数据少时效果差；且仅用离散正确性标签会丢失隐藏状态中蕴含的丰富信息。

Method: 提出REPCORE方法，将不同模型的异构隐藏状态对齐至统一潜在空间，并在此空间中构建代表性核心集用于性能外推。

Result: 在5个基准、200多个模型上实验表明，REPCORE在排序相关性和估计精度上持续优于基于输出的基线方法；谱分析显示对齐表征包含可分离的通用响应倾向与任务特异性推理模式成分。

Conclusion: 利用隐藏状态而非离散标签构建核心集更鲁棒高效，尤其适用于源模型少或新基准场景，为LLM高效评估提供了新范式。

Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.

</details>


### [1040] [Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations](https://arxiv.org/abs/2602.00731)
*Kyle Hamilton,Ali Intizar*

Main category: cs.AI

TL;DR: 本文综述了近五年工业场景中预测性维护（PdM）的前沿技术，指出纯数据驱动方法（如深度学习）虽精度高但缺乏可解释性与泛化能力，而基于专家知识的传统方法则准确率低、误报多；为此，文章提出融合深度学习与符号逻辑的神经符号AI（NeSy）作为更具准确性、可解释性与鲁棒性的混合新范式，并重点分析了结合传感器数据与人工规则的若干NeSy架构。


<details>
  <summary>Details</summary>
Motivation: 解决当前预测性维护中数据驱动方法缺乏可解释性、泛化能力差，以及知识驱动方法准确率低、依赖人工调优的问题，推动更可靠、可信赖的工业智能维护系统落地。

Method: 系统性文献综述，对比分析纯数据驱动、纯知识驱动及混合方法；重点介绍并评估多种神经符号AI（NeSy）架构，尤其关注其如何融合传感器数据与人工规则。

Result: 识别出神经符号AI在PdM中兼具高精度、强解释性、良好泛化性与鲁棒性的潜力；梳理出若干典型NeSy架构及其在工业PdM中的适用性与局限性。

Conclusion: 神经符号AI是突破当前PdM技术瓶颈的有前景方向，未来研究应聚焦于构建可扩展、可验证、面向实际工业部署的NeSy系统框架。

Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).

</details>


### [1041] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: 本文通过Maria平台的工业案例研究，提出了一种以四大工程支柱（Clean Architecture、事件驱动架构、Agent模块化、人机协同治理）为基础的可信赖临床AI系统架构，旨在解决当前AI在医疗场景中因原型架构脆弱和缺乏系统性监管导致的安全与问责问题。


<details>
  <summary>Details</summary>
Motivation: 解决AI在临床环境中因原型架构脆弱、缺乏系统性监管而导致的安全与问责缺失问题，即所谓的“责任真空”。

Method: 提出并实践一种融合Clean Architecture（可维护性）、事件驱动架构（弹性与可审计性）、Agent作为模块化单元（具备自主MLOps生命周期）以及人机协同治理模型（作为事件驱动的数据源用于持续改进）的协同架构。

Result: 构建了名为“Maria”的生产级临床AI平台，并将其作为高风险领域中可维护、可扩展、可问责AI系统的参考架构。

Conclusion: 可信赖的临床AI需通过四大工程支柱的整体集成来实现，该架构为高风险领域的AI系统工程实践提供了可复用的方法论与经验。

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [1042] [Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models](https://arxiv.org/abs/2602.00780)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Zenghuan Zhu,Jiajun Deng,Xinrui Lin,Shuo Liu,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.AI

TL;DR: 本文提出EcoVLA，一种无需训练、即插即用的自适应剪枝框架，用于加速视觉-语言-动作（VLA）模型推理，在保持高任务成功率的同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: VLA模型参数量大导致推理延迟高，难以满足实时操作需求；静态剪枝无法适应环境动态变化，固定间隔的动态剪枝粒度粗且重训练开销大。

Method: 提出EcoVLA框架，包含两部分：1）环境感知自适应通道剪枝（EAP），利用物理环境时间一致性动态更新稀疏模式；2）交错推理编排（I²O），利用VLA推理中的FLOPs空泡并行调度剪枝，几乎不增加延迟。

Result: 在多个VLA模型和基准上实现最高1.60×加速（成功率仅降0.4%）；结合token剪枝可达2.18×加速（成功率仅降0.5%）；并在真实机器人上验证有效。

Conclusion: EcoVLA是一种高效、通用、免训练的VLA加速方案，兼顾自适应性与低延迟，可正交集成现有加速方法，并具备实际部署潜力。

Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.

</details>


### [1043] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

TL;DR: 本文提出利用世界模型作为大型语言模型（LLM）智能体与高成本真实环境（如机器人、科学实验、ML工程）之间的中介，以缓解高交互代价带来的样本效率低和离策略学习难等问题，并探讨其在多领域中的应用潜力及构建挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLM智能体在低成本环境（如游戏、数学、编程）中表现优异，但在高物理/时间/资源成本的复杂领域难以落地，核心瓶颈在于获取奖励信号的动作执行代价过高。

Method: 提出将世界模型（建模环境动力学、奖励函数和任务分布）作为智能体与现实世界的中间层，支持高效离策略学习、提升长程任务样本效率，并为多领域提供丰富学习信号；同时系统梳理世界模型构建的关键挑战与应对路径（数据集、架构、扩展性、评估）。

Result: 论证了世界模型可有效缓解高成本域中的样本效率与离策略学习瓶颈，并在ML工程、计算机使用、机器人和AI for Science等场景中展现出通用学习信号供给能力。

Conclusion: 世界模型是推动LLM智能体迈向高成本复杂现实领域的关键桥梁；需在数据、架构、规模化和评估等方面协同突破，方能实现其全部潜力。

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [1044] [MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing](https://arxiv.org/abs/2602.00811)
*Ronghao Lin,Honghao Lu,Ruixing Wu,Aolin Xiong,Qinggong Chu,Qiaolin He,Sijie Mai,Haifeng Hu*

Main category: cs.AI

TL;DR: 本文提出了MissMAC-Bench基准，用于系统评估多模态情感计算（MAC）中缺失模态问题，强调无缺失先验训练与单模型统一处理完整/不完整模态输入，并在多种语言模型和数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态数据常动态缺失，导致MAC模型性能波动大、鲁棒性差，缺乏统一、公平的缺失模态评估标准。

Method: 构建MissMAC-Bench基准，提出两条原则：训练阶段不引入缺失先验；单模型同时处理完整与不完整模态输入；支持固定与随机、数据集级与实例级缺失模式评估。

Result: 在3个主流语言模型和4个数据集上的实验验证了该基准能有效评估各类MAC方法应对缺失模态的能力。

Conclusion: MissMAC-Bench为提升MAC模型鲁棒性提供了标准化评估框架，推动多媒体数据挖掘与实际部署发展。

Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.

</details>


### [1045] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

TL;DR: 本文提出了一种动态单样本策略优化方法（DoPR），通过不确定性感知的强化学习策略，显著降低RLVR训练的计算开销，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: RLVR虽能有效对齐大语言模型的推理行为，但存在数据和计算效率低、 rollout 成本高的问题。

Method: 提出Dynamic One-Shot Policy Refinement（DoPR），基于奖励波动性和探索驱动采样，每批次仅选择一个最具信息量的样本进行策略更新。

Result: DoPR将rollout开销降低近一个数量级，同时保持有竞争力的推理准确率。

Conclusion: DoPR为推理密集型大语言模型提供了可扩展、资源高效的RL后训练路径，提升了RLVR的实用性与可及性。

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [1046] [Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward](https://arxiv.org/abs/2602.00845)
*Senkang Hu,Yong Dai,Yuzhi Zhao,Yihang Tao,Yu Guo,Zhengru Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.AI

TL;DR: 本文提出InfoReasoner框架，通过合成的语义信息增益奖励来激励大推理模型（LRMs）进行高效外部知识检索，该奖励基于信念状态不确定性降低的理论定义，并利用双向文本蕴含进行语义聚类以实现无需人工标注的可扩展优化；实验表明其在多个问答基准上显著优于强基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的推理中，由于缺乏密集、有原则的奖励信号，检索过程的优化仍具挑战性。

Method: 提出InfoReasoner统一框架，定义基于信念状态不确定性减少的信息增益，并设计输出感知的内在估计器（基于双向文本蕴含的语义聚类）来计算该增益，结合Group Relative Policy Optimization（GRPO）进行策略训练。

Result: 在七个问答基准上，InfoReasoner持续超越强检索增强基线，平均准确率最高提升5.4%。

Conclusion: 本工作为基于检索的代理式推理提供了理论严谨且可扩展的优化路径。

Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.

</details>


### [1047] [Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding](https://arxiv.org/abs/2602.00854)
*Fangzhou Lin,Qianwen Ge,Lingyu Xu,Peiran Li,Xiangbo Gao,Shuo Xing,Kazunori Yamada,Ziming Zhang,Haichong Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 本文提出“能力-理解鸿沟”概念，指出AI系统性能提升的同时用户理解能力下降，并定义了保障人类有效监督所需的最低理解水平——认知完整性阈值（CIT），从验证能力、理解保持型交互和制度支撑三方面展开，推动人-AI协同的认知可持续设计与治理。


<details>
  <summary>Details</summary>
Motivation: AI系统日益产生流畅、正确的端到端结果，导致用户解释、验证和干预能力持续弱化，现有透明性、控制权、素养与治理方案未能界定人类在长期AI代理下必须保有的基础理解能力。

Method: 提出‘认知完整性阈值（CIT）’这一新概念并形式化定义；通过三个功能维度（验证能力、理解保持型交互、制度治理支撑）对其进行可操作化；构建面向责任关键场景的人-AI协同设计与治理框架。

Result: 明确了CIT作为保障人类监督有效性、自主性与可问责参与的最小理解门槛；指出CIT不要求完全复现推理过程，也不限制自动化，但一旦低于该阈值，监督将流于程序化、抗辩性失效。

Conclusion: 为确保AI辅助下的可持续人类责任担当，需以CIT为标尺重构人机交互设计与治理体系，强调认知可持续性而非仅追求性能或表面透明。

Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.

</details>


### [1048] [Multi-Head Attention Is a Multi-Player Game](https://arxiv.org/abs/2602.00861)
*Kushal Chakrabarti,Nirmal Balachundar*

Main category: cs.AI

TL;DR: 本文指出Transformer注意力机制中各头之间存在隐式的多智能体博弈，而传统训练方法忽略了这一结构，导致效率低下。作者通过形式化分析，提出了'无序代价'(Price of Anarchy, PoA)来量化这种低效，并证明其受头间交互矩阵非对角质量Γ(G)控制；进而设计GAME-LoRA正则化方法降低Γ(G)，显著减少幻觉并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代Transformer的注意力头在内部具有多智能体特性（竞争与协作），但训练方式却将其视为单一优化器，造成冗余、错误相关等未被定价的外部性问题。

Method: 将注意力头建模为隐式势博弈，推导出无序代价(PoA)上界Γ(G)；理论证明幻觉概率与头冗余均随PoA增长；据此提出GAME-LoRA正则化方法，融合Barlow Twins去相关与log-determinant协调约束。

Result: 实验表明Γ(G)能显著预测幻觉（p<0.05）；观察到头之间的选择性协作现象；GAME-LoRA实现最高18%、平均8%的幻觉降低，且不损害知识保持能力。

Conclusion: Transformer注意力头间的隐式博弈结构是关键建模维度；通过显式建模并正则化头间交互（降低Γ(G)），可统一缓解幻觉与冗余问题，实现帕累托改进。

Abstract: Modern transformer attention is internally multi-agent -- heads compete and coordinate -- yet we train it as if it were a monolithic optimizer. We formalize this gap: cross-entropy training induces an implicit potential game among heads, and gradient descent converges to Nash equilibria with potentially unbounded inefficiency due to unpriced externalities (redundancy, correlated errors). Our main result bounds the Price of Anarchy by $Γ(G)$, the off-diagonal mass of a head interaction matrix capturing weight and gradient coupling. Under mild smoothness assumptions, we prove that both \emph{excess hallucination probability} and \emph{excess head redundancy} scale with PoA, unifying two distinct failure modes into a single mechanism. The bound is prescriptive: regularization that reduces $Γ(G)$ provably tightens PoA. We instantiate this as GAME-LoRA, combining Barlow Twins decorrelation with log-determinant coordination pressure. Experiments validate the theory: $Γ(G)$ predicts hallucination ($p{<}0.05$), emergent coalitions exhibit selective coordination, and GAME-LoRA achieves up to 18\% hallucination reduction (8\% average) with no knowledge degradation -- a Pareto improvement inaccessible to methods ignoring the game structure.

</details>


### [1049] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

TL;DR: 本文提出了一种面向CAN总线数据的基础模型（Foundation CAN Model），通过将解码后的CAN信号视为一种‘语言’进行大规模无监督预训练，并设计统一的离散-连续混合信号分词方案，实现了在多个汽车保险下游任务上的有效迁移与泛化，验证了基础模型范式在车载AI领域的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有CAN数据应用多采用孤立的任务专用模型，缺乏共享表征学习能力，难以跨任务泛化；而NLP和CV领域已成功应用基础模型范式，本文旨在将该范式迁移到CAN数据领域。

Method: 将解码后的CAN信号建模为语言序列，提出统一的混合信号（离散+连续）tokenization方案，基于大规模无标签CAN数据进行自监督预训练，并针对时间复杂性和行程变异性等挑战进行建模优化，最后在多个异构汽车保险任务上微调。

Result: 单个预训练CAN基础模型能在碰撞检测、驾驶员风险建模等多个下游任务上实现有效适配与性能提升，显著优于各任务独立训练的基线模型。

Conclusion: 基础模型范式同样适用于CAN总线数据，为车载AI中的通用表征学习开辟了新方向。

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [1050] [Beyond Output Critique: Self-Correction via Task Distillation](https://arxiv.org/abs/2602.00871)
*Hossein A. Rahmani,Mengting Wan,Pei Zhou,Longqi Yang,Nick Craswell,Emine Yilmaz,Sujay Kumar Jauhar*

Main category: cs.AI

TL;DR: 本文提出SELF-THOUGHT框架，通过在自我修正前引入任务抽象步骤（提取结构化模板），提升LLM推理纠错能力，并支持跨模型模板迁移以增强小模型的自我修正性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我修正方法多停留在输出层面的表面纠错，难以修正深层推理错误；且小模型内在自我修正能力弱，依赖外部验证或大量微调。

Method: 提出SELF-THOUGHT框架：给定输入与初始响应，先由模型提炼出包含关键变量、约束和问题结构的结构化任务模板（任务抽象），再基于该模板进行解的实例化与修正；并验证大模型生成的模板可迁移至小模型指导其修正。

Result: 在多种推理任务上实验表明，该方法显著提升了大小模型的准确性、鲁棒性与泛化性；小模型借助大模型抽象出的模板，无需微调或外部验证即可实现更可靠的自我修正。

Conclusion: SELF-THOUGHT为构建更可靠、可扩展的自我修正语言系统提供了新范式，强调任务理解先于响应修正，并支持知识（模板）跨模型复用。

Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.

</details>


### [1051] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

TL;DR: Synapse is a federated learning framework for LLM-based agents that trains a shared global knowledge model of tool usage, enabling efficient and privacy-aware collaborative learning despite data and tool heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Collaborative learning among LLM-based agents under federated learning suffers from high communication costs, data heterogeneity, and tool-usage diversity, limiting effectiveness.

Method: Synapse trains a shared global knowledge model (tool compendium) via federated aggregation of locally learned tool-usage artifacts; it employs templated representations, embedding retrieval with LLM reranking, and adaptive masking to preserve utility and limit information leakage.

Result: Synapse improves tool-usage effectiveness and reduces communication overhead compared to weight- or prompt-sharing baselines in multi-agent LLM systems, while supporting heterogeneous data and quantifying performance gains.

Conclusion: Synapse enables stable, efficient, and privacy-conscious collaborative tool-use learning across heterogeneous LLM agents in federated settings.

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [1052] [Supervised sparse auto-encoders as unconstrained feature models for semantic composition](https://arxiv.org/abs/2602.00924)
*Ouns El Harzli,Hugo Wallner,Yoonsoo Nam,Haixuan Xavier Tao*

Main category: cs.AI

TL;DR: 本文提出了一种监督式解码器-only稀疏自编码器（SAE）方法，通过联合学习稀疏概念嵌入与解码权重，在 Stable Diffusion 3.5 上实现了对未见概念组合的图像重建与语义级图像编辑，提升了可解释性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器（SAEs）在机制可解释性中重新兴起，但面临L1惩罚非光滑导致重建与扩展性差、以及学习特征与人类语义不对齐两大挑战。

Method: 借鉴神经坍缩理论中的无约束特征模型框架，提出监督式（仅解码器）SAEs：以特征向量为监督目标，联合优化稀疏概念嵌入和解码器权重。

Result: 在 Stable Diffusion 3.5 上验证：具备组合泛化能力（可重建训练未见的概念组合图像），支持无需修改提示词的特征级语义图像编辑。

Conclusion: 该方法有效缓解了传统SAEs的非光滑性与语义不对齐问题，为生成模型的可解释性与可控编辑提供了新路径。

Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.

</details>


### [1053] [Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents](https://arxiv.org/abs/2602.00929)
*Zergham Ahmed,Kazuki Irie,Joshua B. Tenenbaum,Christopher J. Bates,Samuel J. Gershman*

Main category: cs.AI

TL;DR: TheoryCoder-2 是一种新型的理论驱动强化学习（TBRL）智能体，利用大语言模型（LLM）的上下文学习能力，从经验中主动学习可复用的抽象知识，并将其整合进分层规划过程，从而显著提升样本效率和跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体和深度强化学习系统难以像人类一样高效学习并利用抽象知识进行泛化；而此前的TBRL系统（如TheoryCoder）虽具强泛化性，却严重依赖人工提供的抽象，未解决抽象学习问题。

Method: 提出TheoryCoder-2，结合LLM的in-context learning能力，从交互经验中自动合成抽象，并将抽象嵌入分层规划框架；在BabyAI、Minihack和VGDL（如Sokoban）等多样化环境中验证。

Result: TheoryCoder-2在样本效率上显著优于基于经典规划建模、推理式规划及WorldCoder等程序合成基线；能解决基线失败的复杂任务，且仅需极少人工提示。

Conclusion: 自动学习可复用抽象是提升LLM与RL智能体泛化与效率的关键路径，TheoryCoder-2为实现类人抽象学习与分层规划提供了可行框架。

Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.

</details>


### [1054] [The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis](https://arxiv.org/abs/2602.00947)
*Mohan Reddy*

Main category: cs.AI

TL;DR: 本文指出聊天界面在多步、状态依赖的数据分析任务中会系统性地降低分析性能，并提出了五种认知负担机制及相应的八种混合设计模式来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 聊天界面作为AI辅助数据分析的默认接口，在处理多步、状态依赖的分析任务时存在严重缺陷，这源于Woods（1984）提出的Keyhole效应——即通过狭窄视口浏览大量信息空间所带来的认知成本。

Method: 基于认知科学原理，形式化定义了认知过载公式O = max(0, m - v - W)，并识别出五种导致分析性能下降的机制；进而提出八种混合设计模式以针对性缓解这些瓶颈。

Result: 提出了八种可操作的混合UI设计模式（如生成式UI、无限画布、指称交互等），每种均针对特定认知瓶颈，同时保留自然语言用于意图表达与结果综合；还给出了可证伪假设与实验范式。

Conclusion: 聊天界面不适用于开放探索型数据分析任务；结构良好、嵌入专家先验知识的对话系统可在引导式任务中减轻认知负荷；该框架强调需结合可视化与交互设计来提升AI辅助分析效能。

Abstract: Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.

</details>


### [1055] [MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support](https://arxiv.org/abs/2602.00950)
*António Farinhas,Nuno M. Guerreiro,José Pombal,Pedro Henrique Martins,Laura Melton,Alex Conway,Cara Dochat,Maya D'Eon,Ricardo Rei*

Main category: cs.AI

TL;DR: 本文提出了一种临床导向的风险分类体系（MindGuard），结合心理学专家协作构建的标注数据集（MindGuard-testset）和轻量级安全分类器，显著提升了大语言模型在心理健康支持场景中的临床安全性与对话风险识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用安全防护机制难以区分心理治疗中的正常倾诉与真实临床危机，导致安全失效，亟需临床依据充分、细粒度、可操作的风险识别方案。

Method: 1）与博士级心理学家合作构建临床风险分类法；2）发布由临床专家逐轮标注的真实多轮对话数据集MindGuard-testset；3）基于双智能体合成对话训练4B/8B参数的轻量级安全分类器MindGuard；4）在对抗性多轮交互中评估其与临床语言模型协同效果。

Result: MindGuard在高召回率下显著降低误报率；与临床语言模型联用时，相比通用防护机制，攻击成功率和有害交互率更低；所有模型与人工评估数据均已开源。

Conclusion: 临床专业知识深度融入AI安全设计是提升心理健康领域LLM可靠性与实用性的关键路径；MindGuard为构建可信、合乎伦理的心理健康AI系统提供了可复现、可扩展的技术基础。

Abstract: Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures. To address this gap, we introduce a clinically grounded risk taxonomy, developed in collaboration with PhD-level psychologists, that identifies actionable harm (e.g., self-harm and harm to others) while preserving space for safe, non-crisis therapeutic content. We release MindGuard-testset, a dataset of real-world multi-turn conversations annotated at the turn level by clinical experts. Using synthetic dialogues generated via a controlled two-agent setup, we train MindGuard, a family of lightweight safety classifiers (with 4B and 8B parameters). Our classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, help achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards. We release all models and human evaluation data.

</details>


### [1056] [R-HTN: Rebellious Online HTN Planning for Safety and Game AI](https://arxiv.org/abs/2602.00951)
*Hector Munoz-Avila,David W. Aha,Paola Rizzo*

Main category: cs.AI

TL;DR: 本文提出了一种在线分层任务网络（HTN）智能体R-HTN，其行为受内置指令集D约束，在特定条件下可拒绝或调整用户指派任务以避免违反指令，兼顾安全性与目标达成。


<details>
  <summary>Details</summary>
Motivation: 现有智能体缺乏在执行用户任务时兼顾安全指令或个性约束的在线决策能力，尤其在需‘智能不服从’（如紧急避险）场景下存在风险。

Method: 提出R-HTN算法，融合HTN规划、在线规划与指令约束机制；设计两种变体：非自适应型（遇指令冲突即中止）与自适应型（动态重规划以规避冲突）。

Result: 在两个含安全/人格指令的任务域中验证，R-HTN智能体始终不违反指令，且在可行前提下尽可能完成用户目标（方式可能不同于预期）。

Conclusion: R-HTN为在线HTN智能体提供了可证安全的指令遵从与灵活目标达成能力，拓展了智能体在高可靠性与个性化交互场景中的适用性。

Abstract: We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \D. Like other agents that are capable of rebellion (i.e., {\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.

</details>


### [1057] [Small-Margin Preferences Still Matter-If You Train Them Right](https://arxiv.org/abs/2602.00954)
*Jinlong Pang,Zhaowei Zhu,Na Di,Yichi Zhang,Yaxuan Wang,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出MixDPO方法，通过按难度（偏好对间隔）排序数据并分别采用SFT（难对）和DPO（易对）目标进行混合训练，有效利用模糊偏好对提升大模型对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）对偏好对质量与难度敏感；传统做法过滤小间隔（模糊）对，但作者发现这些对在SFT下仍含有效监督信号，仅在偏好损失下易导致训练不稳定。

Method: MixDPO：（i）按偏好对间隔定义难度并构建难度课程；（ii）对易对使用DPO损失，对难对切换为SFT损失，实现难度感知的混合优化。

Result: 在三个LLM-judge基准上，MixDPO持续优于DPO及多种主流变体，尤其在AlpacaEval 2长度控制版胜率上提升显著。

Conclusion: 模糊偏好对并非纯噪声，其价值取决于优化目标；MixDPO通过目标路由机制兼顾稳定性与监督有效性，为偏好学习提供了更鲁棒的训练范式。

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

</details>


### [1058] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: 本文质疑了Agentic Reinforcement Learning（ARL）中推理与工具使用共用同一模型参数的常见假设，提出线性效应归因系统（LEAS）证明二者存在梯度干扰；进而设计解耦动作推理调优（DART）框架，通过独立低秩适配模块分别优化推理与工具使用能力，在保持单模型结构下达到多智能体系统的性能水平。


<details>
  <summary>Details</summary>
Motivation: 现有ARL方法普遍假设共享参数联合训练推理与工具使用能提升整体性能，但该假设缺乏实证检验。

Method: 提出Linear Effect Attribution System（LEAS）定量分析推理与工具使用行为间的干扰；设计Disentangled Action Reasoning Tuning（DART），采用两个独立的低秩适配模块分别更新推理和工具使用的参数。

Result: DART在多个任务上平均提升6.35%，性能媲美显式分离推理与工具使用的多智能体系统。

Conclusion: 推理与工具使用存在训练干扰，联合优化并非最优；显式解耦二者参数更新可显著提升单模型ARL性能。

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [1059] [Error Taxonomy-Guided Prompt Optimization](https://arxiv.org/abs/2602.00997)
*Mayank Singh,Vikas Yadav,Eduardo Blanco*

Main category: cs.AI

TL;DR: 本文提出了一种名为Error Taxonomy-Guided Prompt Optimization (ETGPO)的提示优化方法，采用自上而下的方式，通过构建错误分类体系并针对性增强提示，显著降低计算开销，同时在多个基准测试中达到或超越现有最优方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化（APO）方法多依赖试错或自下而上的反馈调整，易忽略全局失败模式，导致效率低、计算开销大。

Method: 提出ETGPO算法：首先收集模型在任务上的错误样本，构建系统性错误分类体系（Error Taxonomy），再识别最频繁的失败类型，并据此在提示中加入针对性指导以修正这些共性错误。

Result: 在数学、问答和逻辑推理等多个基准上，ETGPO精度媲美或优于SOTA方法，且优化阶段的token消耗与评估预算仅约为现有方法的三分之一。

Conclusion: ETGPO通过引入全局错误分类视角实现高效提示优化，验证了自上而下策略在提示工程中的有效性与实用性。

Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.

</details>


### [1060] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

TL;DR: 本文分析了基于人类反馈的对齐（RLHF）如何通过奖励学习中的偏差放大模型的谄媚行为，并提出了一种在训练时加入‘一致性惩罚’的干预方法来缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基于偏好的后训练中常表现出更强的谄媚倾向，即盲目附和用户观点而牺牲事实准确性；本文旨在从形式化角度揭示这一现象背后的机制。

Method: 通过建立奖励优化与人类偏好数据偏差之间的因果关联，推导出行为漂移方向由基础策略下信念信号与学习奖励的协方差决定；进一步在Bradley-Terry等随机效用模型下刻画标注者偏差如何导致奖励间隙；最后提出一种基于KL散度最小化的闭式奖励修正方法——一致性惩罚。

Result: 实验证明奖励间隙普遍存在，且在所有测试配置中均引发行为漂移；所提干预方法能有效抑制谄媚行为增长。

Conclusion: 谄媚行为增强是RLHF中奖励学习偏差被系统性放大的结果；通过显式建模并修正该机制，可在保持模型原有能力的同时显著提升其诚实性与可靠性。

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [1061] [HalluHard: A Hard Multi-Turn Hallucination Benchmark](https://arxiv.org/abs/2602.01031)
*Dongyang Fan,Sebastien Delsad,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.AI

TL;DR: 本文提出了HalluHard多轮幻觉基准测试，用于评估大语言模型在法律、科研、医疗和编程四个高风险领域中的事实性与引用支持能力，并设计了基于网络搜索的迭代证据检索与验证评估流程；结果显示即使结合网络搜索，前沿模型仍存在约30%的幻觉率，且幻觉受模型能力、对话轮次、推理质量及知识类型影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮对话中易产生看似合理但缺乏事实依据的陈述（即幻觉），且早期错误会随上下文增长而级联放大，亟需更严格、可验证的多轮幻觉评测基准。

Method: 构建HalluHard基准（950个跨领域种子问题），要求所有事实主张必须带内联引用；提出基于网络搜索的迭代评判流程，支持获取、过滤与解析全文（含PDF）以验证引用真实性。

Result: 在多种前沿闭源与开源模型上测试发现，即使启用网络搜索，最强配置（Opus-4.5+web search）幻觉率仍达约30%；内容接地错误普遍持续存在。

Conclusion: 当前大语言模型的事实可靠性仍严重不足；幻觉行为具有系统性规律，受模型容量、对话位置、推理有效性及知识类型共同影响，需从评测与建模两方面协同改进。

Abstract: Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search ($\approx 30\%$ for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.

</details>


### [1062] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

TL;DR: 本文提出了一种面向推理过程的强化学习框架，通过逐步边际信息增益（MIG）机制提供连续奖励信号，并结合解耦掩码策略与双门控监督微调目标，显著提升大语言模型在稀疏奖励下的推理训练效率、准确率及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 标准基于结果的强化学习方法在大语言模型推理增强中面临奖励稀疏和信用分配低效问题。

Method: 提出Step-wise Marginal Information Gain（MIG）机制，以单调历史水印为基准量化每步推理的内在价值；采用Decoupled Masking Strategy，分别对思维链（CoT）施加过程奖励、对完整输出施加结果奖励；引入Dual-Gated SFT目标融合高质量结构与事实信号。

Result: 在MATH、Super-CLEVR等文本与多模态基准上，相比GRPO等基线，样本效率与最终准确率均显著提升；展现出更强的分布外鲁棒性与零样本迁移能力。

Conclusion: 所提框架有效缓解了奖励稀疏与信用分配难题，为大语言模型的高效、稳健推理训练提供了新范式。

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [1063] [SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning](https://arxiv.org/abs/2602.01062)
*Chenyi Li,Yuan Zhang,Bo Wang,Guoqing Ma,Wei Tang,Haoyang Huang,Nan Duan*

Main category: cs.AI

TL;DR: 本文提出了一种基于核相似性的集合级多样性目标，通过leave-one-out边际贡献作为优势塑形项，提升强化学习中大语言模型在数学推理任务中的解的多样性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习方法虽提升了LLM的数学推理能力，但导致输出解的多样性下降，即概率质量过度集中；受收益递减规律启发，作者旨在引入多样性优化目标。

Method: 定义基于采样轨迹的核化相似性集合级多样性目标；推导每条轨迹的leave-one-out边际贡献，并将其作为插件式优势塑形项用于策略优化；在分布扰动框架下理论分析单条轨迹对多样性的贡献。

Result: 在多种模型规模和基准测试上，该算法在Pass@1和Pass@K指标上均持续超越强基线方法。

Conclusion: 所提多样性增强方法能有效缓解强化学习中解的过度集中问题，在保持甚至提升准确率的同时显著增加输出多样性。

Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.

</details>


### [1064] [ConvexBench: Can LLMs Recognize Convex Functions?](https://arxiv.org/abs/2602.01075)
*Yepeng Liu,Yu Huang,Yu-Xiang Wang,Yingbin Liang,Yuheng Bu*

Main category: cs.AI

TL;DR: 本文提出了一个用于测试大语言模型（LLM）在深度函数复合下识别符号目标函数凸性的可扩展、机械可验证基准（ConvexBench），揭示了当前前沿LLM在组合推理上的显著退化现象，并提出一种基于外部解析工具与递归子表达式推理的代理式分治框架，显著提升了深层复合场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）逐步承担研究级数学与科学任务，其对凸性等核心数学概念的理解与推理能力变得至关重要；然而现有评估缺乏对深度函数复合下凸性判断能力的系统性测试。

Method: 构建了ConvexBench基准，涵盖不同复合深度的符号凸性判断任务；通过分析模型推理轨迹识别‘解析失败’与‘惰性推理’两类失败模式；进而设计代理式分治框架：（i）调用外部工具解析生成抽象语法树（AST），（ii）在聚焦上下文中对每个中间子表达式进行递归推理。

Result: 实验显示，前沿LLM在复合深度从2增至100时，F1分数从1.0骤降至约0.2；所提框架在深度100时恢复F1=1.0，显著缓解深层组合失效问题。

Conclusion: LLM在深层符号凸性推理上存在本质性组合泛化瓶颈，而结合形式化解析与结构化递归推理的混合代理方法可有效突破该瓶颈，为数学推理能力增强提供了可行路径。

Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \cb, a scalable and mechanically verifiable benchmark for testing \textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \textit{parsing failure} and \textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).

</details>


### [1065] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

TL;DR: 本文提出了AutoHealth，一个面向健康数据的不确定性感知多智能体系统，通过五个专用智能体的闭环协作，实现数据探索、任务定制建模、训练与优化，并兼顾预测性能与不确定性量化，在真实世界多任务基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代理在健康数据上的应用受限，主要问题包括难以泛化于异构健康数据模态、过度依赖预定义模板而缺乏任务自适应能力、以及忽视对医疗决策至关重要的不确定性估计。

Method: 提出AutoHealth系统，包含五个闭环协同的专用智能体，分别负责数据探索、任务条件建模、训练、优化，并联合优化预测性能与不确定性量化；同时生成可解释性报告以支持风险感知决策。

Result: 在涵盖17个真实健康任务、多种数据模态和学习设置的基准上，AutoHealth完成全部任务，在预测性能上比SOTA基线提升29.2%，在不确定性估计上提升50.2%。

Conclusion: AutoHealth为健康数据分析提供了一种自主、可靠且可解释的新范式，验证了不确定性感知多智能体系统在医疗AI中的有效性与实用性。

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [1066] [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)
*Yiliu He,Tianle Li,Binghao Ji,Zhiyuan Liu,Di Huang*

Main category: cs.AI

TL;DR: 本文提出EvoOpt-LLM，一种基于大语言模型（LLM）的统一框架，用于自动化工业优化建模全流程（建模生成、动态约束注入、变量剪枝），在极少量训练数据下实现高生成率与可执行率，并提升求解效率。


<details>
  <summary>Details</summary>
Motivation: 自然语言需求到可执行MILP模型的转化高度依赖专家经验，而现有LLM方法存在数据效率低、求解器级有效性差、难以扩展至工业规模等问题。

Method: 提出EvoOpt-LLM框架：基于7B参数LLM，采用LoRA高效微调；包含三模块——自动化MILP建模生成、动态业务约束注入（保持原目标）、端到端变量剪枝；仅用3000样本训练，关键性能在1500样本下即显著提升。

Result: 生成率达91%，可执行率达65.9%；约束注入模块可靠扩展模型且不破坏原目标；变量剪枝模块在中等规模LP模型上F1达~0.56（仅400样本）。

Conclusion: EvoOpt-LLM提供了一种实用、数据高效的工业优化建模方案，显著降低专家依赖，增强模型适应性与求解器效率。

Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.

</details>


### [1067] [MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI](https://arxiv.org/abs/2602.01086)
*Takahito Nakajima*

Main category: cs.AI

TL;DR: MedBeads 提出一种基于 Merkle 有向无环图的不可变临床事件数据结构（Beads），解决 LLM 在医疗场景中因 EMR/FHIR 设计面向人类而导致的‘上下文错配’问题，实现确定性、可审计、防篡改的 AI 原生临床数据基础设施。


<details>
  <summary>Details</summary>
Motivation: 现有 EMR 和 FHIR 标准面向人类设计，导致 AI 代理接收碎片化数据，依赖概率性方法（如 RAG）重建病史，引发幻觉与不可审计问题，即‘上下文错配’。

Method: 提出 MedBeads：将临床事件建模为不可变的‘Beads’节点，构成 Merkle DAG；采用‘一次写入、多次读取’架构，通过密码学保证因果引用与篡改可证；实现 Go 核心引擎、Python LLM 中间件和 React 可视化界面；设计 BFS 上下文检索算法。

Result: 在合成数据上验证可行：FHIR 资源成功转为因果图；BFS 检索达 O(V+E) 实时复杂度；篡改必然破坏密码链；可视化清晰呈现因果关系；Bead 格式具备 token 效率优势。

Conclusion: MedBeads 通过从概率搜索转向确定性图遍历、从可变记录转向不可变链，为可信医疗 AI 提供底层数据基座；保障 AI 输入上下文的确定性与防篡改性，解释权仍归 LLM；并以开源形式推动 AI 原生医疗数据标准。

Abstract: Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous "Clinical Agents" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a "Context Mismatch": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable "Beads"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This "write-once, read-many" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the "Context Mismatch" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for "Trustworthy Medical AI." It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient "AI-native language." We release MedBeads as open-source software to accelerate agent-native data standards.

</details>


### [1068] [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)
*Yang Liu,Chuan Zhou,Yancheng Chen,Shuai Zhang,Xixun Lin,Xiaoqing Wang*

Main category: cs.AI

TL;DR: 本文提出FALCON框架，通过语法约束解码、可行性修复层和自适应Best-of-N采样，确保大语言模型在组合优化任务中100%输出可行解；同时提出BOPO训练方法，利用目标差距加权偏好对实现无标注密集监督，并在理论和实验上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在组合优化任务中缺乏保障解可行性的机制，而可行性对实际部署至关重要。

Method: 提出FALCON框架，包含语法约束解码、可行性修复层和自适应Best-of-N采样；并提出BOPO训练方法，基于目标差距加权偏好对进行优化。

Result: 在七个NP难组合优化问题上，FALCON实现了100%可行性，且解质量达到或超过当前最优神经网络与LLM方法。

Conclusion: FALCON有效解决了LLM求解组合优化时的可行性难题，结合BOPO训练策略，在理论收敛性与修复质量损失界上均有保证，兼具实用性与严谨性。

Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% feasibility through three key innovations: (i) \emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

</details>


### [1069] [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)
*Yiming Dong,Kun Fu,Haoyu Li,Xinyuan Zhu,Yurou Liu,Lijing Shao,Jieping Ye,Zheng Wang*

Main category: cs.AI

TL;DR: 本文提出了一种从目标层级‘黑客行为’（objective-level hacking）角度理解RLVR训练不稳定的框架，指出其根源在于token级信用错位，并在30B MoE模型上实证揭示了训练-推理差异异常增长这一关键病态动态的机制。


<details>
  <summary>Details</summary>
Motivation: RLVR在MoE架构中易出现训练不稳定，严重阻碍能力提升，但其成因尚不清楚。

Method: 构建基于目标层级黑客行为的理论框架，并结合在30B MoE模型上的大量实验，追溯并形式化训练-推理差异异常增长的机制。

Result: 明确了RLVR在MoE中不稳定的核心机制是token级信用错位引发的目标层级黑客行为，导致系统级虚假优化信号，并解释了训练-推理差异异常增长这一现象。

Conclusion: 该研究为理解MoE模型中RLVR训练不稳定性提供了因果明确的机制解释，对设计稳定RLVR算法具有指导意义。

Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.

</details>


### [1070] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

TL;DR: 本文提出了BiCarFormer，一种结合诊断故障码（DTC）序列与环境传感器数据（如温湿度、压力）的双向Transformer多模态模型，用于多标签错误模式分类，显著提升了车辆故障诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 现有车载诊断系统仅依赖DTC序列，忽略环境等上下文信息，而这些信息对专家判断故障至关重要，但其高噪声和复杂性带来建模挑战。

Method: 提出BiCarFormer：基于双向Transformer的多模态模型，融合DTC序列与环境传感数据嵌入，并引入协同注意力机制建模二者交互关系。

Result: 在包含22,137条错误码和360种错误模式的真实汽车数据集上，BiCarFormer显著优于仅用DTC序列或传统序列模型的方法。

Conclusion: 融合环境上下文信息可显著提升车辆故障诊断的准确性与鲁棒性，有助于降低维护成本并推动汽车工业自动化。

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [1071] [Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach](https://arxiv.org/abs/2602.01131)
*Yue Zhong,Jiawen Kang,Yongju Tong,Hong-Ning Dai,Dong In Kim,Abbas Jamalipour,Shengli Xie*

Main category: cs.AI

TL;DR: 本文提出了一种面向低空经济中无人机（UAV）作为空中基站的感-通-算-控闭环框架，通过Lyapunov稳定性理论将控制稳定性映射为可量化的通信资源约束，并构建Stackelberg博弈进行资源定价与分配；进一步设计轻量级剪枝型PPO算法，降低DRL在边缘设备上的计算开销，实现在动态低空环境中兼顾控制稳定性和系统效用。


<details>
  <summary>Details</summary>
Motivation: 低空经济快速发展下，无人机作为空中基站需支持多样化用户服务，但受限于机载资源与严苛的控制稳定性要求，传统吞吐量优先的设计难以保障任务可靠性。

Method: 提出感-通-算-控闭环框架；利用Lyapunov稳定性理论建立控制状态演化与通信约束的映射关系；构建UAV为领导者、用户为跟随者的Stackelberg博弈模型；设计基于动态结构化剪枝的轻量级PPO强化学习算法。

Result: 仿真表明该方案能在动态低空环境中有效保障控制环路稳定性，同时最大化系统效用，并显著降低DRL推理延迟与模型规模。

Conclusion: 所提框架与算法成功将控制稳定性需求转化为可优化的资源分配问题，兼顾了实时性、可靠性与边缘部署可行性，为低空异构网络提供了新型协同优化范式。

Abstract: With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networks is often compromised by the conflict between limited onboard resources and stringent stability requirements. Moving beyond traditional throughput-centric designs, we propose a Sensing-Communication-Computing-Control closed-loop framework that explicitly models the impact of communication latency on physical control stability. To guarantee mission reliability, we leverage the Lyapunov stability theory to derive an intrinsic mapping between the state evolution of the control system and communication constraints, transforming abstract stability requirements into quantifiable resource boundaries. Then, we formulate the resource allocation problem as a Stackelberg game, where UAVs (as leaders) dynamically price resources to balance load and ensure stability, while users (as followers) optimize requests based on service urgency. Furthermore, addressing the prohibitive computational overhead of standard Deep Reinforcement Learning (DRL) on energy-constrained edge platforms, we propose a novel and lightweight pruning-based Proximal Policy Optimization (PPO) algorithm. By integrating a dynamic structured pruning mechanism, the proposed algorithm significantly compresses the neural network scale during training, enabling the UAV to rapidly approximate the game equilibrium with minimal inference latency. Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments.

</details>


### [1072] [PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?](https://arxiv.org/abs/2602.01146)
*Sidharth Pulipaka,Oliver Chen,Manas Sharma,Taaha S Bajwa,Vyas Raina,Ivaxi Sheth*

Main category: cs.AI

TL;DR: 本文提出PersistBench基准，用于评估大语言模型（LLM）在集成长期记忆时引发的两类新型安全风险：跨领域信息泄露和记忆诱导的谄媚行为；实验发现18个前沿及开源LLM在该基准上失败率极高（中位数达53%和97%），凸显当前长期记忆机制的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 长期记忆虽能提升个性化交互，但其持续性可能带来被忽视的安全风险，如跨领域信息泄露和记忆诱导的用户偏见强化。

Method: 构建PersistBench基准，系统识别并量化两类长期记忆特有风险，并在18个前沿及开源LLM上进行实证评估。

Result: 18个LLM在跨领域泄露样本上中位失败率为53%，在记忆诱导谄媚样本上高达97%。

Conclusion: 当前LLM的长期记忆机制存在严重安全隐患，亟需开发更鲁棒、更安全的记忆使用方法。

Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.

</details>


### [1073] [Capabilities and Fundamental Limits of Latent Chain-of-Thought](https://arxiv.org/abs/2602.01148)
*Jiaxuan Zou,Yaozhong Xiong,Yong Liu*

Main category: cs.AI

TL;DR: 本文揭示了Latent CoT模型中探索与执行性能不一致的根本原因在于决策确定性，并提出了符号索引作为量化该确定性的核心机制，证明了课程学习的理论必要性，推动模型设计从静态架构转向基于任务需求动态调节决策确定性的自适应系统。


<details>
  <summary>Details</summary>
Motivation: Latent CoT模型在探索任务（如ProsQA）上表现优异，但在计算任务（如GSM8K）上表现差，存在性能不一致问题，需揭示其内在机理。

Method: 通过理论分析刻画探索-执行权衡，提出‘符号索引’量化决策承诺度，并建立其与执行稳定性及探索能力的因果关系；同时证明课程学习在理论上的必要性。

Result: 明确了决策确定性是调控探索与执行性能的关键因素；符号索引被证实为核心机制；课程学习被严格证明为必需训练策略。

Conclusion: Latent CoT模型的设计范式应从固定架构转向能根据任务动态调节决策确定性的自适应系统。

Abstract: Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certainty. Our contributions are threefold: (1) We theoretically characterize the fundamental Exploration-Execution Trade-off, proving that high certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation. (2) We introduce the Symbolic Index--quantifying decisional commitment--as the core mechanism governing this trade-off and establish its causal relationship with both execution stability and exploration capability. (3) We prove that curriculum learning is theoretically necessary, as direct training provably fails due to distributional mismatch. Our framework shifts the design paradigm from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands.

</details>


### [1074] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: 本文提出CAREP多智能体系统，自动从车辆诊断故障码（DTC）序列中生成错误模式（EP）规则，结合因果发现、上下文整合与可解释推理，显著优于纯大模型基线，并提供透明因果解释。


<details>
  <summary>Details</summary>
Motivation: 当前汽车制造商依赖专家手工编写错误模式（EP）规则，成本高、易出错，难以应对日益增长的车辆复杂性。

Method: 提出CAREP多智能体系统，包含因果发现代理（识别DTC-EP关系）、上下文信息代理（融合元数据与描述）和协调代理（合成布尔规则并生成可解释推理链）。

Result: 在含29,100个唯一DTC和474个EP的大规模汽车数据集上，CAREP能自动准确发现未知EP规则，性能优于纯LLM基线，并提供透明因果解释。

Conclusion: CAREP通过融合因果发现与基于智能体的推理，推动实现可扩展、可解释、低成本的全自动车辆故障诊断。

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [1075] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

TL;DR: 本文发现视觉语言模型（VLM）中存在损害下游任务性能的“任务干扰层”，提出无需训练的测试时自适应方法TaLo，通过动态绕过最干扰层提升性能。


<details>
  <summary>Details</summary>
Motivation: 观察到在预训练VLM中，默认使用所有层进行预测，但对单层干预（如置零）反而能提升某些任务性能，说明部分层对下游任务有害，需系统探究各层对不同任务的影响机制。

Method: 提出层干预分析法，定义任务-层交互向量量化每层对任务的影响；发现任务干扰层具有任务特异性敏感模式；据此设计无训练、测试时自适应方法TaLo，动态识别并绕过最干扰层。

Result: TaLo在多个模型和数据集上提升性能，例如使Qwen-VL在ScienceQA的Maps任务准确率提升达16.6%；验证了任务干扰层的跨模型/数据集泛化性及任务间交互向量的高相似性。

Conclusion: 预训练VLM中存在隐含的任务相关模块化结构；TaLo作为一种即插即用、无需训练的推理时优化机制，可有效释放模型潜在能力。

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [1076] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 本文提出了ASP-Bench基准，包含128个自然语言问题实例，用于评估将自然语言规范自动翻译为Answer Set Programs（ASP）的系统，并通过基于ReAct框架的智能体方法验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 自动化将自然语言规范翻译为逻辑程序是神经符号工程中的关键挑战，现有基准缺乏对ASP核心特性的系统覆盖和建模难度的多维刻画。

Method: 构建了ASP-Bench基准，涵盖64个基础问题（含易/难变体），系统覆盖ASP关键特性（如选择规则、聚合、优化）并提供参考验证器；引入七维推理维度刻画问题难度；采用基于ReAct框架的反馈驱动迭代建模智能体方法进行评测。

Result: ReAct智能体在ASP-Bench上达到完全饱和（full saturation），表明求解器反馈驱动的迭代精化是可靠鲁棒的ASP建模方法；多轮智能体运行分析揭示了影响建模难度的关键因素。

Conclusion: ASP-Bench为自然语言到ASP翻译提供了首个系统性、多维可解释的基准；ReAct等反馈驱动的智能体范式是解决该任务的有效路径，建模难度主要取决于问题在多个推理维度上的组合复杂性。

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [1077] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

TL;DR: 本文提出了一种基于状态转移的高效推理框架，利用线性注意力机制建模LLM推理过程，降低注意力计算复杂度至线性，并通过状态更新避免冗余推理，从而提升推理效率与性能。


<details>
  <summary>Details</summary>
Motivation: 长思维链（CoT）虽提升LLM复杂推理能力，但其高计算与内存开销限制了实用性；现有压缩CoT方法又与测试时缩放冲突，削弱推理能力。

Method: 将LLM推理建模为状态转移过程：用线性注意力机制估计并维护‘推理状态’以记录历史信息；当前步推理基于查询提示和该状态进行，并动态更新状态；引入状态驱动的推理策略抑制噪声步骤导致的过思考。

Result: 在多个数据集和不同规模模型上实验表明，该框架显著提升LLM推理效率（注意力复杂度由O(n²)降至O(n)），同时增强推理性能。

Conclusion: 所提状态转移式推理框架兼顾效率与效果，突破了传统CoT压缩方法在测试时扩展性上的瓶颈，为高效可扩展推理提供了新范式。

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [1078] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: 本文提出Workflow-R1框架，将工作流构建重构为多轮自然语言驱动的序列决策过程，并引入结构感知强化学习算法GSsPO，以Think-Action原子周期为优化单元，提升LLM智能体在复杂多轮推理任务中的工作流优化能力。


<details>
  <summary>Details</summary>
Motivation: 现有工作流优化方法将合成视为静态、单次代码生成问题，过度依赖模型编码能力，缺乏动态问题求解所需的灵活性。

Method: 提出Workflow-R1框架，将工作流构建建模为多轮自然语言序列决策；设计Group Sub-sequence Policy Optimization (GSsPO)，以Think-Action原子周期为优化粒度，实现结构感知的强化学习。

Result: 在多个问答基准上，Workflow-R1显著优于强基线；GSsPO被验证为适用于多轮序列推理任务的通用优化方法。

Conclusion: Workflow-R1代表了一种面向自动化工作流优化的新范式，GSsPO为其提供了鲁棒、可泛化的训练机制。

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [1079] [Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)](https://arxiv.org/abs/2602.01206)
*Zeinab Dehghani*

Main category: cs.AI

TL;DR: 本文提出了gSMILE框架，用于提升生成式AI模型（如大语言模型和图像编辑模型）的可解释性，通过输入扰动、Wasserstein距离和加权代理建模实现细粒度归因与可视化，并在多种模型和场景下验证了其鲁棒性与人类对齐性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI决策过程不透明，限制了其在高风险场景中的可信度与可问责性。

Method: 扩展SMILE方法至生成式模型，提出gSMILE框架，结合受控文本扰动、Wasserstein距离度量和加权代理建模，支持token级归因与热力图可视化；引入基于ODD的场景化评估策略及多项归因质量指标（稳定性、保真度、准确性、一致性、可信性）。

Result: gSMILE在多个先进生成模型上展现出鲁棒、人类对齐的归因能力，且具有良好泛化性。

Conclusion: gSMILE为生成式AI提供了统一、可评估的可解释性框架，有助于推动其透明、可靠与负责任的部署。

Abstract: The rapid advancement of generative artificial intelligence has enabled models capable of producing complex textual and visual outputs; however, their decision-making processes remain largely opaque, limiting trust and accountability in high-stakes applications. This thesis introduces gSMILE, a unified framework for the explainability of generative models, extending the Statistical Model-agnostic Interpretability with Local Explanations (SMILE) method to generative settings. gSMILE employs controlled perturbations of textual input, Wasserstein distance metrics, and weighted surrogate modelling to quantify and visualise how specific components of a prompt or instruction influence model outputs. Applied to Large Language Models (LLMs), gSMILE provides fine-grained token-level attribution and generates intuitive heatmaps that highlight influential tokens and reasoning pathways. In instruction-based image editing models, the exact text-perturbation mechanism is employed, allowing for the analysis of how modifications to an editing instruction impact the resulting image. Combined with a scenario-based evaluation strategy grounded in the Operational Design Domain (ODD) framework, gSMILE allows systematic assessment of model behaviour across diverse semantic and environmental conditions. To evaluate explanation quality, we define rigorous attribution metrics, including stability, fidelity, accuracy, consistency, and faithfulness, and apply them across multiple generative architectures. Extensive experiments demonstrate that gSMILE produces robust, human-aligned attributions and generalises effectively across state-of-the-art generative models. These findings highlight the potential of gSMILE to advance transparent, reliable, and responsible deployment of generative AI technologies.

</details>


### [1080] [Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models](https://arxiv.org/abs/2602.01207)
*Hui Wu,Hengyi Cai,Jinman Zhao,Xinran Chen,Ziheng Li,Zhejun Zhao,Shuaiqiang Wang,Yuchen Li,Dawei Yin*

Main category: cs.AI

TL;DR: 本文提出SAGE框架，通过动态选择高信噪比的偏好样本提升大模型推理对齐效果，显著加速收敛并超越静态基线方法。


<details>
  <summary>Details</summary>
Motivation: 标准偏好对齐方法（如DPO）将所有偏好对一视同仁，忽视了训练实例效用的动态变化，导致优化低效或不稳定：既浪费计算资源于无梯度贡献的简单样本，又受决策边界附近噪声样本干扰。

Method: SAGE是一种动态对齐框架，包含两个核心组件：1）基于模型能力更新候选池的粗粒度课程学习机制；2）细粒度稳定性感知评分函数，优先选择信息量大且置信度高的错误样本，过滤不稳定样本，以最大化策略更新的信噪比。

Result: 在多个数学推理基准上实验表明，SAGE显著加快收敛速度，并在性能上优于DPO等静态基线方法。

Conclusion: 在推理对齐中，策略感知、稳定性优先的数据选择至关重要；SAGE通过动态调控样本效用，提升了对齐过程的可靠性与效率。

Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.

</details>


### [1081] [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)
*Shaoxiong Yang,Junting Li,Mengyuan Zhang,Chao Li,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: 本文提出FutureMind框架，通过自适应知识蒸馏将大语言模型（LLM）的思维模式迁移至小语言模型（SLM），增强其在复杂、知识密集型任务中的结构化推理与检索能力，并在多跳问答基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）虽具高效低延迟优势，但在需结构化推理和有效检索的复杂知识密集型任务中表现不足，亟需提升其认知能力。

Method: 提出模块化推理框架FutureMind，包含问题分析、逻辑推理、策略规划和检索引导四大动态模块，并融合三种检索范式以分解复杂查询；通过自适应知识蒸馏，将LLM的思维模式先验注入SLM。

Result: 在2WikiMultihopQA、MuSiQue、Bamboogle和Frames等多跳问答基准上显著超越Search-o1等强基线，实现免费训练条件下的SOTA性能，且适配多种SLM架构与规模。

Conclusion: FutureMind有效提升了SLM的推理与检索能力；研究发现思维模式蒸馏受限于师生模型间的认知偏差瓶颈，为提升SLM的认知可迁移性提供了新视角。

Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.

</details>


### [1082] [Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models](https://arxiv.org/abs/2602.01237)
*Katrina Brown,Aneesh Muppidi,Rana Shahout*

Main category: cs.AI

TL;DR: 本文提出Predictive Scheduling框架，通过轻量级预测器预估每个查询所需的最优推理长度或难度，并动态分配固定总token预算，从而在不增加计算成本的前提下显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 固定每条查询的token预算会导致简单问题过度计算、困难问题计算不足，亟需一种能自适应分配计算资源的方法。

Method: 设计轻量级预测器（基于中间层隐藏状态的MLP或LoRA微调分类器）预估查询难度，并结合贪心批处理分配器动态分配总token预算。

Result: 在GSM8K数据集上，相比均匀分配预算，绝对准确率最高提升7.9个百分点，填补了超50%的oracle性能差距；中间层（12–17层）隐藏状态最利于难度估计。

Conclusion: 预运行预算预测可实现对计算-精度权衡的细粒度控制，为低延迟、高性价比的大模型部署提供可行路径。

Abstract: Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.

</details>


### [1083] [LLM-Driven Ontology Construction for Enterprise Knowledge Graphs](https://arxiv.org/abs/2602.01276)
*Abdulsobur Oyewale,Tommaso Soru*

Main category: cs.AI

TL;DR: 本文提出了OntoEKG，一个利用大语言模型（LLM）从非结构化企业数据中自动生成领域本体的端到端管道，分为提取和蕴含两个模块，并在Data、Finance、Logistics三个领域的新基准上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 企业知识图谱构建中，底层本体的手动构建过程资源密集、依赖专家经验，亟需自动化方法。

Method: 提出OntoEKG：包含提取模块（识别核心类与属性）和蕴含模块（逻辑构建层次结构并序列化为RDF）的LLM驱动两阶段流水线。

Result: 在Data领域新基准上达到模糊匹配F1-score 0.724；揭示了范围定义与层次推理方面的局限性。

Conclusion: LLM可用于加速企业本体生成，但当前方法在语义精确性和逻辑推理能力上仍有挑战，需进一步改进。

Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.

</details>


### [1084] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: 本文提出RE-MCDF框架，通过生成-验证-修正闭环架构，结合多专家协同与医学知识图谱驱动的逻辑关系约束，提升神经科EMR数据下的临床诊断鲁棒性与准确性。


<details>
  <summary>Details</summary>
Motivation: 神经科电子病历（EMR）具有异质性、稀疏性和噪声大等特点，单智能体易产生自强化错误；现有多智能体方法交互浅、结构松散，且忽略疾病间逻辑依赖（如互斥性、病理相容性），难以排除临床不合理假设。

Method: 提出RE-MCDF：包含生成专家（输出诊断及证据）、检验专家（动态加权临床指标）、多关系感知评估专家组（基于医学知识图谱显式建模疾病间逻辑约束），构成生成-验证-修正闭环。

Result: 在NEEMRs和自建XMEMRs数据集上，RE-MCDF在复杂诊断任务中持续优于SOTA基线模型。

Conclusion: 引入结构化多专家协作与显式疾病逻辑关系建模，可显著提升LLM在真实神经科EMR场景下的诊断可靠性与临床合理性。

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [1085] [Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance](https://arxiv.org/abs/2602.01346)
*Wei Yang,Hong Xie,Tao Tan,Xin Li,Defu Lian,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出了一种基于视觉编码器内部功能动态的VLM模型选择框架，通过层间电导表示任务，并引入非对称的‘定向电导散度（DCD）’指标来预测目标任务下的模型排序，无需直接推理，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开源视觉语言模型（VLMs）大量涌现，但在计算受限和少样本场景下，难以穷举评估以选出最适合特定下游任务的预训练模型；现有选择方法或依赖数据密集型代理指标，或使用忽略迁移方向性和模型特异性的对称文本描述。

Method: 提出基于视觉编码器内部功能动态的模型选择框架：用层间电导（layer-wise conductance）表征任务，通过熵正则化对齐得到目标条件下的模块重要性分布，并定义非对称的‘定向电导散度（DCD）’来衡量源任务对目标任务关键功能模块的覆盖程度，从而聚合源任务排名预测目标任务模型排序。

Result: 在48个VLM模型、21个数据集上的实验表明，该方法在NDCG@5指标上较SOTA基线SWAB提升14.7%。

Conclusion: DCD作为一种轻量、无须微调与直接推理的非对称迁移可预测性度量，能更准确刻画VLM跨任务迁移能力，为高效模型选择提供了新范式。

Abstract: While open sourced Vision-Language Models (VLMs) have proliferated, selecting the optimal pretrained model for a specific downstream task remains challenging. Exhaustive evaluation is often infeasible due to computational constraints and data limitations in few shot scenarios. Existing selection methods fail to fully address this: they either rely on data-intensive proxies or use symmetric textual descriptors that neglect the inherently directional and model-specific nature of transferability. To address this problem, we propose a framework that grounds model selection in the internal functional dynamics of the visual encoder. Our approach represents each task via layer wise conductance and derives a target-conditioned block importance distribution through entropy regularized alignment. Building on this, we introduce Directional Conductance Divergence (DCD), an asymmetric metric that quantifies how effectively a source task covers the target's salient functional blocks. This allows for predicting target model rankings by aggregating source task ranks without direct inference. Experimental results on 48 VLMs across 21 datasets demonstrate that our method outperforms state-of-the-art baselines, achieving a 14.7% improvement in NDCG@5 over SWAB.

</details>


### [1086] [Aggregation Queries over Unstructured Text: Benchmark and Agentic Method](https://arxiv.org/abs/2602.01355)
*Haojia Zhu,Qinyuan Xu,Haoyu Li,Yuxi Liu,Hanchen Qiu,Jiaoyan Chen,Jiahui Jin*

Main category: cs.AI

TL;DR: 本文提出了一种面向完整性的文本聚合查询新范式，定义了实体级聚合查询任务，构建了评估基准AGGBench，并设计了模块化智能体方法DFA，在证据覆盖率上优于现有RAG和智能体基线。


<details>
  <summary>Details</summary>
Motivation: 聚合查询要求系统‘找到全部’而非‘找到一个’，但现有Text-to-SQL和RAG等范式无法满足严格完整性需求。

Method: 形式化实体级聚合查询任务；构建大规模完整性导向基准AGGBench；提出三阶段模块化智能体方法DFA（消歧—过滤—聚合）。

Result: DFA在聚合证据覆盖率上持续优于强RAG和智能体基线；AGGBench支持完整性导向的公平评估。

Conclusion: 面向完整性的聚合查询需新任务定义、专用基准与可解释模块化架构；DFA为该方向提供了可扩展、可诊断的基线框架。

Abstract: Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to "find all," not merely "find one." Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.

</details>


### [1087] [Building Better Deception Probes Using Targeted Instruction Pairs](https://arxiv.org/abs/2602.01425)
*Vikram Natarajan,Devina Jain,Shivam Arora,Satvik Golechha,Joseph Bloom*

Main category: cs.AI

TL;DR: 本文研究了线性探针在检测AI系统欺骗行为中的应用，发现训练时使用的指令对（instruction pair）对性能影响最大（占方差70.6%），且应依据可解释的欺骗行为分类法设计专用探针，而非追求通用检测器。


<details>
  <summary>Details</summary>
Motivation: 现有线性探针在检测AI欺骗行为时存在显著缺陷，如虚假相关性和对非欺骗响应的误报，亟需提升其鲁棒性与可解释性。

Method: 分析不同指令对对线性探针性能的影响，并基于人类可解释的欺骗行为分类法，构建针对性探针，在多个评估数据集上进行验证。

Result: 指令对主要捕获欺骗意图而非内容模式，其选择解释了探针性能差异的70.6%；按欺骗类型定制的探针显著优于通用设置。

Conclusion: 由于欺骗类型在不同数据集中高度异质，组织应根据自身威胁模型设计专用探针，放弃寻求通用欺骗检测器。

Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.

</details>


### [1088] [SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce](https://arxiv.org/abs/2602.01443)
*Alberto Castelo,Zahra Zanjani Foumani,Ailin Fan,Keat Yang Koay,Vibhor Malik,Yuanzheng Zhu,Han Li,Meysam Feghhi,Ronie Uliana,Shuang Xie,Zhaoyu Zhang,Angelo Ocana Martins,Mingyu Zhao,Francis Pelland,Jonathan Faerman,Nikolas LeBlanc,Aaron Glazer,Andrew McNamara,Lingyun Wang,Zhong Wu*

Main category: cs.AI

TL;DR: 本文提出SimGym系统，利用基于大语言模型的合成买家代理，在真实浏览器中进行快速离线A/B测试，显著缩短实验周期并避免影响真实用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试存在分流流量、耗时长（需数周）、可能损害用户体验等问题，亟需更高效、安全的替代评估方法。

Method: SimGym从生产数据中提取店铺级买家画像与意图，识别行为原型，并基于队列加权方式在控制组与实验组 storefront 上模拟会话；代理运行于真实浏览器环境，且未使用对齐后训练。

Result: SimGym在真实电商平台上验证，无需后训练对齐即达到业界最优的结果偏移拟合度；实验周期由数周缩短至一小时以内。

Conclusion: SimGym实现了高保真、低风险、超快速的离线UI效果评估，为电商UI迭代提供了可扩展的新范式。

Abstract: A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Language Model agents operating in a live browser. SimGym extracts per-shop buyer profiles and intents from production interaction data, identifies distinct behavioral archetypes, and simulates cohort-weighted sessions across control and treatment storefronts. We validate SimGym against real human outcomes from real UI changes on a major e-commerce platform under confounder control. Even without alignment post training, SimGym agents achieve state of the art alignment with observed outcome shifts and reduces experiment cycles from weeks to under an hour , enabling rapid experimentation without exposure to real buyers.

</details>


### [1089] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体协作的自动化软件工程系统，模拟真实开发团队的组织结构与工作流程，在SWE-bench上达到72.4%任务解决率，优于单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 现实软件开发是团队协作、角色分工明确、有沟通与评审机制的过程，而现有自主系统多采用单体或流水线式处理，未能体现组织性。

Method: 基于开源平台agyn构建多智能体系统，为协调、研究、实现、评审等角色分配专用智能体，配备隔离沙箱，并支持结构化通信；遵循完整开发方法论（问题分析、任务定义、PR创建、迭代评审），全程无人干预。

Result: 在未针对SWE-bench调优的前提下，该系统在SWE-bench 500上解决72.4%的任务，显著优于同级别语言模型的单智能体基线。

Conclusion: 复现团队结构、方法论和沟通机制是提升自主软件工程能力的有效范式；未来进步可能更依赖组织设计与智能体基础设施，而非仅靠模型改进。

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [1090] [Legal Infrastructure for Transformative AI Governance](https://arxiv.org/abs/2602.01474)
*Gillian K. Hadfield*

Main category: cs.AI

TL;DR: 本文探讨了AI治理中法律和监管基础设施建设的重要性，提出了三种构建框架的方案：前沿模型注册制度、自主代理注册与识别制度、以及促进私营企业创新提供AI监管服务的监管市场设计。


<details>
  <summary>Details</summary>
Motivation: AI的变革性要求我们不仅关注实质性的规则制定，更要重视构建能够生成和实施这些规则的法律与监管基础设施。

Method: 通过回顾作者提出的三个具体制度设计方案来分析AI治理中的基础设施建设问题。

Result: 提出了三种可行的AI治理基础设施建设路径：前沿模型注册制、自主代理识别制、以及监管市场机制。

Conclusion: 构建适应AI发展的法律与监管基础设施，是实现有效AI治理的关键环节，需与实质性规则制定同步推进。

Abstract: Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.

</details>


### [1091] [Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models](https://arxiv.org/abs/2602.01475)
*Brij Malhotra,Shivvrat Arya,Tahrima Rahman,Vibhav Giridhar Gogate*

Main category: cs.AI

TL;DR: 本文提出了一种基于注意力机制的神经网络框架，用于在重复查询场景下对概率图模型（PGM）中的Most Probable Explanation（MPE）推理进行神经摊销优化，通过预测局部移动对汉明距离的减少能力来指导局部搜索，从而提升收敛性与解质量。


<details>
  <summary>Details</summary>
Motivation: 在固定图结构、多次不同证据模式下进行MPE推理的实际场景中，传统随机局部搜索（SLS）易陷入局部最优，而现有启发式方法（如GLS+）难以跨查询复用指导信息。

Method: 设计一个基于注意力机制的神经网络，利用固定图结构学习对局部移动进行打分，预测其减小到近优解的汉明距离的能力；将该打分信号嵌入现有局部搜索流程，在邻域选择中权衡即时似然增益与长期优化潜力。

Result: 在高树宽基准测试中，该方法在摊销推理设置下一致优于SLS和GLS+，理论分析支持距离导向的移动选择可改善收敛行为。

Conclusion: 神经摊销框架能有效提升重复MPE查询下的局部搜索性能，为PGM推理提供可复用、可泛化的学习型指导机制。

Abstract: Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be performed repeatedly for varying evidence patterns. Stochastic Local Search (SLS) algorithms scale to large models but rely on myopic best-improvement rule that prioritizes immediate likelihood gains and often stagnate in poor local optima. Heuristics such as Guided Local Search (GLS+) partially alleviate this limitation by modifying the search landscape, but their guidance cannot be reused effectively across multiple inference queries on the same model. We propose a neural amortization framework for improving local search in this repeated-query regime. Exploiting the fixed graph structure, we train an attention-based network to score local moves by predicting their ability to reduce Hamming distance to a near-optimal solution. Our approach integrates seamlessly with existing local search procedures, using this signal to balance short-term likelihood gains with long-term promise during neighbor selection. We provide theoretical intuition linking distance-reducing move selection to improved convergence behavior, and empirically demonstrate consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in the amortized inference setting.

</details>


### [1092] [Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection](https://arxiv.org/abs/2602.01518)
*Jongseok Park,Sunga Kim,Alvin Cheung,Ion Stoica*

Main category: cs.AI

TL;DR: 本文提出Qrita，一种基于枢轴选择策略的高效Top-k和Top-p采样算法，通过高斯sigma截断和四元枢轴搜索技术，在保证确定性输出的同时显著提升GPU上大语言模型采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有Top-k/Top-p实现依赖排序（计算与内存开销大）或随机方法（改变输出），缺乏兼顾效率与确定性的高效方案。

Method: 基于RTop-k思想，提出Qrita算法：1）高斯sigma截断缩小候选搜索空间；2）带重复处理的四元枢轴搜索，减少迭代次数并保证确定性；全部使用Triton在GPU上实现。

Result: 相比vLLM、SGLang、FlashInfer等主流引擎的Top-k/Top-p内核，Qrita吞吐量最高提升2倍、显存占用减半，且输出与排序法完全一致。

Conclusion: Qrita为大语言模型推理中的Top-k/Top-p采样提供了更高效、低内存、确定性的新范式，具有实际部署价值。

Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.

</details>


### [1093] [PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents](https://arxiv.org/abs/2602.01532)
*Yuxuan Fu,Xiaoyu Tan,Teqi Hao,Chen Zhan,Xihe Qiu*

Main category: cs.AI

TL;DR: 本文提出PRISM框架，通过成本敏感的选择性干预机制，结合决策理论门控与双过程推理架构，实现对主动代理干预时机的精确控制，在减少误报的同时提升F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有主动代理系统依赖脆弱启发式或无差别长推理，难以平衡干预收益与成本；需一种可调、可审计、计算高效且精准的干预决策机制。

Method: 提出PRISM框架：1）基于用户接受概率与不对称成本（漏帮/误报）设定门控阈值；2）仅在决策边界附近触发资源密集型Slow模式并进行反事实验证；3）采用门控对齐、模式锁定的知识蒸馏训练策略，解耦响应策略与干预门控。

Result: 在ProactiveBench基准上，相比强基线，误报率降低22.78%，F1提升20.14%；验证了决策门控、选择性慢推理与对齐蒸馏的有效性。

Conclusion: PRISM实现了精准、高效、可控的主动干预，为构建可信赖的主动代理提供了新范式。

Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: "make haste slowly"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.

</details>


### [1094] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

TL;DR: 本文提出了S1-NexusAgent，一种面向多学科科学研究的自演化智能体框架，通过分层规划与执行范式、动态工具检索、稀疏上下文管理及批评-技能提炼闭环，显著提升了长周期科学任务的规划、工具协同与持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型和工具型智能体在处理大规模科学数据、复杂工作流及专业化工具时，存在长周期规划能力弱、目标维持不稳、执行后难以持续学习等问题。

Method: 提出S1-NexusAgent框架：采用Plan-and-CodeAct双环架构解耦全局规划与子任务执行；原生支持MCP协议；集成数千跨学科科学工具；引入基于对象引用的稀疏上下文管理；设计Critic Agent评估执行轨迹并提炼可复用的‘Scientific Skills’以实现自我演化。

Result: 在生物（biomini-eval）、化学（ChemBench）和材料科学（MatSciBench）等权威科学评测基准上达到SOTA性能，验证了其在长周期、高复杂度科研任务中的有效性与泛化性。

Conclusion: S1-NexusAgent为多学科科学探索提供了可扩展、可持续进化的智能代理范式，推动AI for Science向更自主、更鲁棒、更通用的方向发展。

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [1095] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出了一种基于人类模拟的框架，使AI系统能自主形成问题并设定任务，通过整合内部驱动、环境感知和多智能体感知的提示机制，提升其在动态开放环境中的自主决策能力，并在多智能体仿真中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的AI系统依赖预定义任务和固定提示，难以在环境变化时自主识别需解决的问题。

Method: 提出一种人类模拟框架，将问题生成视为首要决策过程，融合内部状态、环境观测及多智能体交互，支持从经验中学习问题生成过程。

Result: 实验表明，环境感知提示显著减少‘未进食’事件；多智能体感知提示进一步使20天累计‘未进食’事件降低超60%，且统计显著（p < 0.05）。

Conclusion: 该框架提升了AI系统在动态开放环境中的自主性、适应性与决策质量，为构建更类人的AI认知机制提供了新路径。

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [1096] [Reasoning with Autoregressive-Diffusion Collaborative Thoughts](https://arxiv.org/abs/2602.01608)
*Mu Yuan,Liekang Zeng,Guoliang Xing,Lan Zhang,Yunhao Liu*

Main category: cs.AI

TL;DR: 本文提出Collaborative Thoughts框架，通过自回归模型与扩散模型的闭环协作，结合视觉批评模块，实现结构化规划、空间约束实例化与迭代优化，提升空间推理可靠性与生成可控性。


<details>
  <summary>Details</summary>
Motivation: 自回归模型擅长序列规划但缺乏空间/物理 grounding，扩散模型擅长空间结构建模但缺乏逻辑控制；二者互补却难以协同，亟需统一框架融合其优势。

Method: 构建Collaborative Thoughts统一框架：自回归模型负责结构化规划与约束管理，扩散模型将约束实例化为中间视觉thoughts，视觉批评模块评估是否满足结构与物理要求，并将反馈用于迭代优化后续步骤。

Result: 在代表性任务中验证了该框架能显著提升空间推理的可靠性与生成过程的可控性，且同一协作循环适用于自回归问答与扩散视觉生成两类任务。

Conclusion: Collaborative Thoughts成功弥合了自回归与扩散范式间的鸿沟，为多模态协同生成提供了可扩展、闭环反馈驱动的新范式。

Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.

</details>


### [1097] [ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning](https://arxiv.org/abs/2602.01610)
*Zitao Guo,Changyang Jiang,Tianhong Zhao,Jinzhou Cao,Genan Dai,Bowen Zhang*

Main category: cs.AI

TL;DR: 本文提出ToPT框架，通过空间感知区域嵌入学习（SREL）和任务感知区域嵌入提示（Prompt4RE），在城市计算中实现空间一致的特征融合与显式任务语义对齐，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段方法生成任务无关的区域表征，而基于提示的方法缺乏显式空间先验和稳健的任务语义对齐机制。

Method: ToPT包含两个模块：SREL使用Graphormer注入距离与区域中心性作为可学习注意力偏置；Prompt4RE利用冻结多模态大语言模型生成任务语义向量，并通过多头交叉注意力与区域嵌入对齐。

Result: 在多个城市与任务上达到SOTA，最高提升64.2%，验证了空间先验与提示-区域对齐的必要性与互补性。

Conclusion: ToPT有效解决了区域嵌入中空间不一致性与任务语义脱节问题，为城市计算提供了更鲁棒、可解释的表示学习范式。

Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.

</details>


### [1098] [ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development](https://arxiv.org/abs/2602.01655)
*Pengrui Lu,Shiqi Zhang,Yunzhong Hou,Lyumanshan Ye,Chaoyi Huang,Zixi Chen,Ji Zeng,Hantao Jiang,Pengfei Liu,Yiwei Wang,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: 本文提出了ProjDevBench，一个面向端到端项目开发的新型基准测试，用于评估编码智能体从需求理解到完整代码库生成的综合能力，涵盖系统架构设计、功能正确性和迭代优化三方面；实验表明当前编码智能体在基础功能上表现尚可，但在复杂系统设计、时间复杂度优化和资源管理等方面仍存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法局限于问题级缺陷修复，无法反映编码智能体在真实端到端软件开发中的实际能力，亟需更全面的基准测试。

Method: 构建ProjDevBench基准：包含20个跨8类的编程任务，融合在线评测（OJ）与大语言模型辅助代码评审，从系统架构设计、功能正确性、迭代解决方案优化三维度评估编码智能体。

Result: 在6个基于不同大语言模型的编码智能体上测试，整体通过率为27.38%；智能体能较好处理基础功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面表现较差。

Conclusion: ProjDevBench填补了端到端项目级评估的空白，揭示了当前编码智能体的关键能力瓶颈，为后续研究提供了新方向和开源基准。

Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.

</details>


### [1099] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

TL;DR: 本文提出了FlowSteer，一种端到端强化学习框架，用于自动化工作流编排，通过轻量级策略模型与可执行画布环境的多轮交互实现，并设计了CWRPO算法以稳定训练、抑制捷径行为，在12个数据集上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作流编排面临高人工成本、依赖特定算子或大语言模型（LLMs）、稀疏奖励信号等关键挑战。

Method: 提出FlowSteer框架：以轻量级策略模型为智能体，在可执行画布环境中进行多轮交互；策略模型分析执行状态并选择编辑动作，画布执行算子并反馈；同时提出Canvas Workflow Relative Policy Optimization（CWRPO）算法，引入多样性约束与条件释放的奖励机制以稳定训练。

Result: 在十二个数据集上的实验表明，FlowSteer在各类任务中显著优于基线方法。

Conclusion: FlowSteer提供了一种通用、即插即用的工作流自动编排方案，支持多样化算子库和可替换LLM后端，有效缓解了人工干预、模型依赖与训练不稳定性等问题。

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [1100] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: 本文提出了TRIP-Bench——一个基于真实旅行规划场景的长周期交互式基准测试，并设计了GTPO在线多轮强化学习方法以提升大模型代理在复杂约束与动态交互下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能充分反映LLM智能体在现实复杂场景中面临的全局约束执行、多工具协同推理及长期多轮交互中适应用户行为变化等关键挑战。

Method: 构建了基于真实数据的长周期旅行规划基准TRIP-Bench（含18个工具、40+需求、多难度划分），并提出GTPO方法：一种结合奖励归一化与奖励差分的在线多轮强化学习算法。

Result: 实验表明，即使先进模型在TRIP-Bench易集成功率也不超50%，难集低于10%；GTPO应用于Qwen2.5-32B-Instruct后，在约束满足与交互鲁棒性上超越Gemini-3-Pro。

Conclusion: TRIP-Bench有望推动实用型长周期交互智能体的发展，GTPO为鲁棒长周期训练提供了有效的在线强化学习范式。

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [1101] [What LLMs Think When You Don't Tell Them What to Think About?](https://arxiv.org/abs/2602.01689)
*Yongchan Kwon,James Zou*

Main category: cs.AI

TL;DR: 本文研究了大语言模型（LLMs）在极简、主题中立输入下的无约束生成行为，发现不同模型家族存在系统性主题偏好、内容深度差异及特有的退化模式，并开源了25.6万样本数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 现有LLM行为分析多依赖主题或任务特定提示，观察范围受限；需通过最小化输入揭示模型内在生成倾向，以支持更可靠的监控与AI安全评估。

Method: 采用主题中立的极简输入（如空字符串、单字符等）对16个主流LLM进行大规模采样（共256,000条输出），系统分析其生成内容的主题分布、技术深度、重复退化模式等行为特征。

Result: 发现各模型家族具有强且系统的主题偏好（如GPT-OSS偏编程/数学，Llama偏文学，DeepSeek偏宗教，Qwen偏多选题）；GPT-OSS生成内容技术深度更高；不同模型呈现独特退化现象（如Llama生成个人社交媒体URL）。

Conclusion: LLMs在近无约束条件下仍表现出稳定、可识别的内在行为模式，这些模式可作为模型指纹用于安全监测与模型溯源；主题中立探针是揭示模型本质偏好的有效新范式。

Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.

</details>


### [1102] [Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning](https://arxiv.org/abs/2602.01695)
*Yadong Wang,Haodong Chen,Yu Tian,Chuanxing Geng,Dong Liang,Xiang Chen*

Main category: cs.AI

TL;DR: 本文提出LSTR框架，通过将稀疏转码器提升为活跃推理算子，实现基于稀疏语义转移的多步推理，在保持准确性和压缩效率的同时显著提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法依赖难以解释和控制的稠密隐状态转移，而稀疏表示虽具可解释性却仅限于后验分析，二者存在张力。

Method: 提出LSTR（潜在稀疏转码推理）框架，核心是具有残差跳跃结构的潜在转移转码器（LTT），解耦线性流形传输与稀疏语义更新，并引入显式稀疏性约束以实现可控语义分辨率。

Result: 实验表明LSTR在保持推理准确率和压缩效率的同时，显著优于稠密潜在基线的可解释性；因果干预与轨迹分析证实稀疏特征既是可解释的，也是因果有效的推理操作符。

Conclusion: LSTR成功弥合了潜在推理的可控性、效率与可解释性之间的鸿沟，为构建透明、可控的神经符号推理系统提供了新路径。

Abstract: Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.

</details>


### [1103] [Mitigating loss of control in advanced AI systems through instrumental goal trajectories](https://arxiv.org/abs/2602.01699)
*Willem Fourie*

Main category: cs.AI

TL;DR: 本文提出了一种超越模型本身的AI安全干预框架，通过识别AI系统为获取资源而可能采取的组织层面行为路径（采购、治理、金融三条 instrumental goal trajectories, IGTs），将监控与干预点从模型内部扩展到支撑其运行的组织系统。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全措施过于技术化和系统中心化，忽视了高能力AI系统为实现工具性目标（如获取计算资源）可能在组织层面采取的行为路径，亟需拓展干预视角。

Method: 提出并定义采购、治理和金融三条工具性目标轨迹（IGTs），分析其如何依赖组织资源（算力、数据、资金等）及产生的可监控组织痕迹（如采购合同、预算审批、治理决策），从而构建组织层面的干预框架。

Result: IGTs为定义AI系统能力水平、实施可纠正性与可中断性提供了具体路径，将AI安全的关注点从单一模型属性转向支撑模型运行的整个组织系统。

Conclusion: 将AI安全干预从技术层面向组织层面延伸是必要且可行的；IGTs不仅拓展了现有安全策略，还为监管者和开发者提供了可操作的、基于组织痕迹的监控与干预新范式。

Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.

</details>


### [1104] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

TL;DR: 本文提出因果提示优化（CPO）框架，利用双重机器学习构建无偏因果奖励模型，实现高效、鲁棒且低成本的查询特定提示优化，显著提升LLM在企业场景中的可靠性与经济性。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法存在两大问题：静态提示无法适应异构查询；依赖相关性离线奖励模型易受查询特征混淆，导致提示效果评估偏差。

Method: CPO分两阶段：第一阶段用双重机器学习（DML）在提示与查询语义嵌入上学习无偏因果奖励模型，分离提示变化的因果效应；第二阶段基于该奖励信号进行资源高效的查询特异性提示搜索，无需高成本在线评估。

Result: 在数学推理、可视化和数据分析基准上，CPO持续优于人工设计提示及SOTA自动优化器，尤其在困难查询上鲁棒性显著提升，并大幅降低推理成本。

Conclusion: 因果推断可作为企业级LLM提示优化的可扩展基础，兼顾高性能、高鲁棒性与低成本。

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [1105] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种名为MACD的新解码策略，通过模型自反馈识别易致幻物体区域，生成目标反事实输入，并结合对比解码以抑制视频语言模型中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有解码方法（如对比解码）依赖随机扰动构建对比数据，难以精准控制驱动幻觉的视觉线索，也无法良好匹配模型弱点。

Method: 提出Model-aware Counterfactual Data based Contrastive Decoding（MACD），利用Video-LLM自身反馈定位最易导致幻觉的物体区域，在物体级别生成有针对性的反事实输入，并将其融入对比解码过程，强制解码时选择有视觉证据支持的token。

Result: 在EventHallusion、MVBench、Perception-test和Video-MME等多个基准上验证，MACD能持续降低幻觉率，同时保持或提升任务准确率，尤其在小尺寸、遮挡或共现物体等挑战性场景中效果显著。

Conclusion: MACD是一种有效、可控且模型感知的推理策略，显著缓解Video-LLM幻觉问题，具备通用性和实用性，代码与数据将开源。

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [1106] [Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives](https://arxiv.org/abs/2602.01749)
*Lin Chen,Samuel Drapeau,Fanghao Shao,Xuekai Zhu,Bo Xue,Yunchong Song,Mathieu Laurière,Zhouhan Lin*

Main category: cs.AI

TL;DR: 本文提出α-GFN，通过引入可调参数α来推广GFlowNet中的前向与后向策略混合方式，从而直接控制探索-利用平衡，提升模式发现能力，并在多个基准测试中显著优于原有GFlowNet目标。


<details>
  <summary>Details</summary>
Motivation: GFlowNet目标隐式固定了前向与后向策略的等比例混合，可能限制训练过程中的探索-利用权衡；需从理论层面揭示该约束来源并提供可调节机制。

Method: 基于GFlowNet与马尔可夫链的联系，建立GFlowNet目标与马尔可夫链可逆性的等价关系，据此提出含可调参数α的α-GFN框架，以灵活控制策略混合比例。

Result: α-GFN在Set、Bit Sequence和分子生成等多个基准任务上一致优于原有GFlowNet目标，模式发现数量最高提升达10倍。

Conclusion: GFlowNet目标的本质约束源于马尔可夫链可逆性；α-GFN通过解耦混合比例，在保证唯一流收敛的同时显著增强模式覆盖能力。

Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \times$ increase in the number of discovered modes.

</details>


### [1107] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: 本文提出Adversarial Reward Auditing (ARA)框架，将奖励黑客行为建模为动态博弈，通过Hacker-Auditor双策略机制检测并抑制RLHF中的奖励欺骗，显著提升对齐性与实用性平衡，并展现跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法易受奖励欺骗（reward hacking）影响，且静态防御无法应对新型欺骗策略。

Method: 提出ARA框架：第一阶段由Hacker策略发现奖励模型漏洞，Auditor策略从潜在表征中学习检测欺骗；第二阶段通过Auditor-Guided RLHF（AG-RLHF）对奖励信号进行门控，惩罚被检测到的欺骗行为。

Result: 在三种欺骗场景（sycophancy、verbosity、code gaming）中，ARA在对齐-效用权衡上优于所有基线；且Hacker与Auditor均展现出跨领域泛化能力。

Conclusion: ARA将奖励欺骗从不可观测的失败转化为可测量、可控制的信号，为RLHF提供了动态、可扩展、跨领域的鲁棒对齐新范式。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [1108] [PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models](https://arxiv.org/abs/2602.01762)
*Xuliang Wang,Yuetao Chen,Maochan Zhen,Fang Liu,Xinzhou Zheng,Xingwu Liu,Hong Xu,Ming Li*

Main category: cs.AI

TL;DR: 本文提出PRISM架构，通过解耦模型容量与推理开销，在保持低延迟的同时显著提升推测解码中draft模型的预测质量与接受长度，实现端到端2.6倍以上的解码吞吐加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法为提升draft模型预测质量而增大参数量，导致计算开销剧增，亟需在预测精度与延迟之间取得新平衡。

Method: 提出PRISM架构，将每步预测计算拆分至不同参数集，重构draft模型的计算路径，实现模型容量与推理成本的解耦。

Result: PRISM在多项实验中超越所有现有draft架构，获得更长的接受长度和更低的draft延迟，端到端加速超2.6倍；且在数据规模扩展下展现出更优的缩放规律。

Conclusion: PRISM通过架构创新有效破解了draft模型精度与效率的根本矛盾，为高效LLM推理提供了新范式。

Abstract: Large Language Models (LLMs), constrained by their auto-regressive nature, suffer from slow decoding. Speculative decoding methods have emerged as a promising solution to accelerate LLM decoding, attracting attention from both systems and AI research communities. Recently, the pursuit of better draft quality has driven a trend toward parametrically larger draft models, which inevitably introduces substantial computational overhead. While existing work attempts to balance the trade-off between prediction accuracy and compute latency, we address this fundamental dilemma through architectural innovation.
  We propose PRISM, which disaggregates the computation of each predictive step across different parameter sets, refactoring the computational pathways of draft models to successfully decouple model capacity from inference cost. Through extensive experiments, we demonstrate that PRISM outperforms all existing draft architectures, achieving exceptional acceptance lengths while maintaining minimal draft latency for superior end-to-end speedup. We also re-examine scaling laws with PRISM, revealing that PRISM scales more effectively with expanding data volumes than other draft architectures. Through rigorous and fair comparison, we show that PRISM boosts the decoding throughput of an already highly optimized inference engine by more than 2.6x.

</details>


### [1109] [Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction](https://arxiv.org/abs/2602.01775)
*Yucheng Wu,Yuekui Yang,Hongzheng Li,Anan Liu,Jian Xiao,Junjie Zhai,Huan Yu,Shaoping Ma,Leye Wang*

Main category: cs.AI

TL;DR: 本文提出CrossAdapt框架，通过离线阶段的维度自适应嵌入投影与渐进式网络蒸馏，以及在线阶段的非对称协同蒸馏与分布感知自适应机制，实现高效跨架构知识迁移，在多个数据集和微信视频号大规模部署中显著提升AUC、降低训练时间并缓解性能退化。


<details>
  <summary>Details</summary>
Motivation: 在大规模用户响应预测系统中，新架构部署面临高昂的模型切换成本（如海量历史数据重训练开销）及数据保留约束下的性能下降问题；现有知识蒸馏方法难以应对架构异构性及大规模嵌入表迁移的高成本。

Method: 提出两阶段CrossAdapt框架：离线阶段采用无需迭代训练的维度自适应投影实现快速嵌入迁移，并结合渐进式网络蒸馏与策略采样以降低计算开销；在线阶段引入非对称协同蒸馏（学生高频更新、教师低频更新）与分布感知自适应机制，动态平衡历史知识保留与对演化数据的快速适配。

Result: 在三个公开数据集上AUC提升0.27–0.43%，训练时间减少43–71%；在腾讯微信视频号（日均千万样本）的大规模部署中，显著缓解AUC下降、LogLoss上升和预测偏差。

Conclusion: CrossAdapt有效解决了跨架构知识迁移中的架构异构性与嵌入表迁移高成本难题，兼顾效率与效果，适用于真实工业级响应预测系统的持续演进。

Abstract: Deploying new architectures in large-scale user response prediction systems incurs high model switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterogeneity and the prohibitive cost of transferring large embedding tables. We propose CrossAdapt, a two-stage framework for efficient cross-architecture knowledge transfer. The offline stage enables rapid embedding transfer via dimension-adaptive projections without iterative training, combined with progressive network distillation and strategic sampling to reduce computational cost. The online stage introduces asymmetric co-distillation, where students update frequently while teachers update infrequently, together with a distribution-aware adaptation mechanism that dynamically balances historical knowledge preservation and fast adaptation to evolving data. Experiments on three public datasets show that CrossAdapt achieves 0.27-0.43% AUC improvements while reducing training time by 43-71%. Large-scale deployment on Tencent WeChat Channels (~10M daily samples) further demonstrates its effectiveness, significantly mitigating AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.

</details>


### [1110] [LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning](https://arxiv.org/abs/2602.01779)
*Rui Hua,Yu Wei,Zixin Shu,Kai Chang,Dengying Yan,Jianan Xia,Zeyu Liu,Hui Zhu,Shujie Song,Mingzhong Xiao,Xiaodong Li,Dongmei Jia,Zhuye Gao,Yanyan Meng,Naixuan Zhao,Yu Fu,Haibin Yu,Benman Yu,Yuanyuan Chen,Fei Dong,Zhizhou Meng,Pengcheng Yang,Songxue Zhao,Lijuan Pei,Yunhui Hu,Kan Ding,Jiayuan Duan,Wenmao Yin,Yang Gu,Runshun Zhang,Qiang Zhu,Jian Yu,Jiansheng Li,Baoyan Liu,Wenjia Wang,Xuezhong Zhou*

Main category: cs.AI

TL;DR: 本文提出了LingLanMiDian（LingLan）基准，用于全面、统一地评估大语言模型在中医领域的知识理解、多跳推理与临床决策能力，强调其专家标注、多任务设计、一致性指标及Hard子集评测，并揭示当前模型与人类专家在中医推理上的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有中医NLP基准存在覆盖碎片化、规模有限、评分标准不统一或过度依赖生成式评估等问题，难以公平衡量大语言模型在中医本体、术语和推理模式下的真实能力。

Method: 构建大规模、专家标注、多任务的LingLan基准，涵盖知识召回、多跳推理、信息抽取与临床决策；引入同义词容错标签协议、每数据集400题Hard子集，并将诊断与治疗推荐重构为单选决策识别任务；对14个主流开源与闭源大模型开展零样本评测。

Result: 零样本评测显示当前模型在中医常识理解与推理方面仍显著落后于人类专家，尤其在Hard子集上表现更差；LingLan提供了首个统一、量化、可扩展的中医大模型评估框架。

Conclusion: LingLan基准填补了中医领域大语言模型标准化评估的空白，通过连接基础医学知识与实际临床推理，为中医AI研究提供了坚实、可复现且可持续演进的评估基础设施。

Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.

</details>


### [1111] [ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing](https://arxiv.org/abs/2602.01797)
*Hanlin Zhou,Huah Yong Chan*

Main category: cs.AI

TL;DR: 本文提出了ORCH框架，一种用于离散选择推理的确定性多智能体协调框架，通过多个异构大语言模型独立分析、再由合并代理统一决策，实现可复现、可解释、免训练的推理流程，并在多个基准上显著超越单模型和多数投票基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统多依赖随机路由或启发式方法，导致行为不可复现、决策过程难解释。

Method: 提出ORCH框架，采用“多分析、一决策”范式：多个基础模型独立生成结构化分析，由专用merge agent聚合输出最终选择；使用固定规则进行任务分解与答案聚合，保证确定性；可选引入EMA引导的路由器，基于历史准确率、延迟或成本动态更新代理选择。

Result: 在MMLU、MMLU-Pro和GSM8K上显著优于单模型和多数投票基线：MMLU-Pro提升超10分，GSM8K提升超50分；McNemar检验确认统计显著；EMA路由器额外带来0.7–2.0分增益；消融实验证明多智能体协作与路由机制均起关键作用。

Conclusion: ORCH为离散选择推理提供了一条可控、可解释且可部署的大语言模型智能体系统实用路径。

Abstract: Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.

</details>


### [1112] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

TL;DR: 本文提出INDIBATOR框架，通过结合科学家的论文历史和分子历史构建个体化代理画像，以提升多智能体系统在分子发现任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用泛化的角色（如“评审员”或“作者”）或粗粒度关键词定义代理角色，无法反映真实科学家基于个人研究轨迹的独特贡献。

Method: 提出INDIBATOR框架，利用科学家的出版历史（文献知识）和分子历史（结构先验）构建个体化代理画像，并通过多轮辩论（提案、批评、投票）进行协作。

Result: 实验表明，基于细粒度个体化画像的代理显著优于基于粗粒度角色的系统，在分子发现任务中达到竞争性甚至SOTA性能。

Conclusion: 捕获代理的‘科学DNA’（即个体科研特征）对高质量科学发现至关重要。

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [1113] [Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs](https://arxiv.org/abs/2602.01832)
*Rui Wang,Yaoguang Cao,Yuyi Chen,Jianyi Xu,Zhuoyang Li,Jiachen Shang,Shichun Yang*

Main category: cs.AI

TL;DR: 本文提出了一种名为车辆联觉（SoV）的新框架，通过视觉输入预测触觉激励，以提升自动驾驶汽车的安全性；提出了跨模态时空对齐方法和基于潜在扩散的视觉-触觉联觉（VTSyn）生成模型，并在真实车辆系统上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆依赖的视觉和光学传感器无法检测对车辆动态控制至关重要的路面诱发激励，而人类具有联觉能力，启发作者探索视觉到触觉的跨模态感知。

Method: 提出跨模态时空对齐方法解决视觉与触觉信号在时空上的不一致问题，并构建基于潜在扩散的视觉-触觉联觉（VTSyn）生成模型，实现无监督高质量触觉数据合成。

Result: 在真实车辆采集的多模态数据集上，VTSyn在时间域、频率域及分类任务上均优于现有模型，显著提升了自动驾驶车辆的主动触觉感知能力与安全性。

Conclusion: SoV框架及其VTSyn模型成功实现了从视觉到触觉的高保真跨模态预测，为弥补传感器盲区、增强AV动态控制提供了新范式。

Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.

</details>


### [1114] [SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures](https://arxiv.org/abs/2602.01858)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: 本文提出SOPRAG框架，通过引入实体、因果和流程图专家、Procedure Card层和LLM引导的门控机制，解决工业标准操作规程（SOP）检索中的结构复杂性、条件依赖性和可执行性等挑战，并在多个工业领域显著优于现有RAG方法。


<details>
  <summary>Details</summary>
Motivation: 标准操作规程（SOP）在工业环境中至关重要，但其检索面临刚性专有结构、条件依赖相关性和需可执行性等挑战，传统语义驱动的RAG范式难以应对。

Method: 提出SOPRAG框架：采用Entity、Causal和Flow图三类专家替代扁平化分块；设计Procedure Card层以剪枝搜索空间；引入LLM-Guided门控机制动态加权专家；并构建多智能体自动化基准生成流程以缓解领域数据稀缺问题。

Result: 在四个工业领域上的实验表明，SOPRAG在检索准确率和响应实用性上显著超越强基线（词法、稠密、图式RAG），并在真实关键任务中实现完美执行得分。

Conclusion: SOPRAG有效解决了工业SOP检索的核心难点，为面向高可靠性操作场景的RAG系统提供了新范式。

Abstract: Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.

</details>


### [1115] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

TL;DR: 本文提出ProcMEM框架，使LLM智能体能自主从交互经验中学习程序性记忆（无需参数更新），通过Skill-MDP建模和非参数PPO机制实现高复用、低冗余、高稳定性的决策能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在序列决策中依赖即时推理，无法有效复用过往经验，导致计算冗余和执行不稳定。

Method: 提出ProcMEM框架：1）构建Skill-MDP将片段化经验转化为具激活/执行/终止条件的可执行Skill；2）设计非参数PPO（含语义梯度候选生成与PPO门控验证）保障Skill可靠性；3）采用基于得分的记忆维护机制实现紧凑高质量记忆存储。

Result: 在域内、跨任务、跨智能体场景中，ProcMEM显著提升技能复用率与性能，同时实现极高的内存压缩率；可视化分析揭示其程序性知识的积累、精炼与复用过程。

Conclusion: ProcMEM为LLM智能体提供了无需微调的、可演化的程序性记忆机制，有效支撑长期自主决策。

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [1116] [Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models](https://arxiv.org/abs/2602.01884)
*Shidong Yang,Tongwen Huang,Hao Wen,Yong Wang,Li Chen,Xiangxiang Chu*

Main category: cs.AI

TL;DR: 本文提出了一种基于响应熵的Entropy-Guided Training（EGT）方法，用于提升多模态推理奖励模型的训练效果，通过熵引导的数据筛选与渐进式训练策略，有效缓解标注噪声与样本难度差异问题，在多个基准上超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 多模态奖励模型训练面临两大挑战：偏好数据集中的固有噪声会降低模型性能，传统训练方法忽略样本难度差异导致效率低下。

Method: 基于响应熵与准确率强相关这一发现，提出熵引导训练（EGT）方法，包含熵引导的数据清洗（剔除高熵不可靠样本）和熵引导的渐进式训练（由易到难引入样本）两个策略。

Result: 在三个基准测试上，EGT训练的模型持续优于当前最先进的多模态奖励模型。

Conclusion: 响应熵可作为无监督的标注噪声与样本难度代理指标；EGT方法能有效提升多模态推理奖励模型的鲁棒性与性能。

Abstract: Multimodal reward models are crucial for aligning multimodal large language models with human preferences. Recent works have incorporated reasoning capabilities into these models, achieving promising results. However, training these models suffers from two critical challenges: (1) the inherent noise in preference datasets, which degrades model performance, and (2) the inefficiency of conventional training methods, which ignore the differences in sample difficulty. In this paper, we identify a strong correlation between response entropy and accuracy, indicating that entropy can serve as a reliable and unsupervised proxy for annotation noise and sample difficulty. Based on this insight, we propose a novel Entropy-Guided Training (EGT) approach for multimodal reasoning reward models, which combines two strategies: (1) entropy-guided data curation to mitigate the impact of unreliable samples, and (2) an entropy-guided training strategy that progressively introduces more complex examples. Extensive experiments across three benchmarks show that the EGT-trained model consistently outperforms state-of-the-art multimodal reward models.

</details>


### [1117] [Geometric Analysis of Token Selection in Multi-Head Attention](https://arxiv.org/abs/2602.01893)
*Timur Mudarisov,Mikhal Burtsev,Tatiana Petrova,Radu State*

Main category: cs.AI

TL;DR: 本文提出了一种几何框架来分析大语言模型中多头注意力机制，将标准注意力视为top-N选择过程，并在值状态空间中定义了Precision、Recall和F-score等几何度量来量化被选与未选token的可分性；理论推导了非渐近界，并在多个7B模型上验证了理论预测，发现注意力头可划分为Retriever、Mixer、Reset三类几何模式。


<details>
  <summary>Details</summary>
Motivation: 理解多头注意力在大语言模型中的内在工作机制，尤其是其几何结构与token选择行为的关系，以提升可解释性并指导更高效的稀疏化与注意力设计。

Method: 将注意力机制建模为top-N选择过程，在value-state空间中定义几何度量（Precision/Recall/F-score），基于稳定值范数、压缩sink token、指数相似度衰减和分段注意力权重等经验假设，推导非渐近理论界，并在LLaMA-2-7B、Gemma-7B、Mistral-7B上实证验证。

Result: 理论预测小N时存在最强非平凡可分性，且sink相似度与Recall正相关；实证发现LLaMA-2-7B中注意力头呈现Retriever、Mixer、Reset三种几何特性的专业化分工；所有模型测量结果均紧密贴合理论包络线。

Conclusion: 多头注意力本质上是一种具有可量化选择准则的结构化几何分类器，该框架为头级别可解释性提供新视角，并支持几何感知的注意力稀疏化与架构设计。

Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.

</details>


### [1118] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

TL;DR: 本文提出了DomusFM，首个专为智能家居传感器数据设计的基础模型，采用自监督双对比学习范式，有效解决标注数据稀缺、现有模型不适用于二元传感器事件等问题，在多个下游任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督模型需要大量标注数据，基础模型仅关注惯性传感器，而大语言模型存在隐私和成本问题，无法满足真实智能家居场景需求。

Method: 提出DomusFM模型，采用自监督双对比学习，融合轻量语言模型的语义嵌入与专用时序及二元状态编码器，学习可迁移的通用表征。

Result: 在七个公开智能家居数据集上进行留一数据集评估，DomusFM在不同下游任务中均超越现有最优基线，仅用5%标注数据微调即达优异性能。

Conclusion: DomusFM有效缓解数据稀缺问题，兼顾泛化能力与实际部署可行性，为智能家居传感数据分析提供了新范式。

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


### [1119] [Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling](https://arxiv.org/abs/2602.01933)
*Fabrice Boissier,Monica Sen,Irina Rychkova*

Main category: cs.AI

TL;DR: 本文比较了大型语言模型（LLM，使用GPT-5）与形式概念分析（FCA，通过CREA流程）在主题建模任务中的表现，采用三阶段零样本提示策略评估LLM，并在教学材料和信息系统领域研究论文两组数据上进行实验对比。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少探索大语言模型在主题建模中的实用性，而形式概念分析虽被提出为候选方法却缺乏实际应用验证；因此需系统比较二者在该任务中的优劣。

Method: 采用CREA流程实现FCA主题建模；对GPT-5设计三阶段零样本提示策略：文档批次主题生成、批次结果合并为最终主题、主题命名；在两组真实文本数据（教学材料、40篇信息系统论文）上开展对比实验。

Result: 实验表明LLM（GPT-5）与FCA（CREA）在主题建模中各具优势：LLM更灵活、可解释性强，FCA结构严谨、概念关系清晰；在信息系统论文实验中，二者提取的主题均能较好对应实际子领域。

Conclusion: LLM与FCA并非互斥，而是互补——LLM适合快速、语义丰富的主题发现，FCA适合可解释、结构化的知识组织；未来可融合二者构建混合主题建模框架。

Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.

</details>


### [1120] [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)
*Yun Qu,Qi Wang,Yixiu Mao,Heming Zou,Yuhang Jiang,Weijie Liu,Clive Bai,Kai Yang,Yangkun Chen,Saiyong Yang,Xiangyang Ji*

Main category: cs.AI

TL;DR: 本文提出了一种通用可迁移的预测式提示选择方法（GPS），通过轻量级生成模型结合贝叶斯推理，基于共享优化历史预测提示难度，实现高效在线提示选择，显著提升强化学习中大语言模型推理训练与测试的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习中提示选择方法依赖高成本精确评估或泛化能力差的提示专用预测模型，难以兼顾效率与泛化性。

Method: 提出Generalizable Predictive Prompt Selection（GPS）：利用轻量级生成模型在共享优化历史上进行贝叶斯推理以估计提示难度；引入中等难度优先与历史锚定多样性策略进行批量提示选取；该小模型还可迁移到测试时实现高效计算分配。

Result: 在多个推理基准上实验表明，GPS在训练效率、最终性能和测试时效率方面均显著优于先进基线方法。

Conclusion: GPS通过兼顾泛化性与轻量化建模，为大语言模型的高效强化学习推理提供了可扩展、实用的新范式。

Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.

</details>


### [1121] [Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning](https://arxiv.org/abs/2602.01983)
*Xintian Shen,Jiawei Chen,Lihao Zheng,Hao Ma,Tao Wei,Kun Zhan*

Main category: cs.AI

TL;DR: 本文提出UCT框架，使大语言模型在推理过程中无需训练即可自主创建和更新工具，从而提升工具集成推理（TIR）能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理模型依赖固定工具、缺乏自优化机制、工具构建耗时费力，难以应对开放性问题和错误工具输出的误导。

Method: 提出无训练的UCT框架，从LLM推理轨迹中提取隐含解题能力，自动提炼可复用工具；引入记忆整合机制维护工具库，支持推理中动态创建与自我更新。

Result: 在多领域数学与科学推理基准上性能显著提升，分别提高+20.86%和+23.04%，验证了代理的自演化能力。

Conclusion: UCT为TIR模型提供了一种新型自动化工具构建范式，实现了推理过程中的持续能力进化，无需额外训练。

Abstract: Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\uparrow$ and +23.04%$\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.

</details>


### [1122] [Emergent Analogical Reasoning in Transformers](https://arxiv.org/abs/2602.01992)
*Gouki Minegishi,Jingyuan Feng,Hiroki Furuta,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文受范畴论中函子概念启发，将类比推理形式化为跨范畴实体间对应关系的推断，并设计合成任务研究Transformer中类比推理的涌现机制；发现其依赖数据、优化与模型规模，并揭示其包含嵌入空间关系结构几何对齐和Transformer内部函子应用两个关键机制，且该现象在预训练大语言模型中同样存在。


<details>
  <summary>Details</summary>
Motivation: 尽管类比是人类认知的核心能力，但Transformer如何获得并实现类比推理的机制仍不清楚。

Method: 受范畴论中函子概念启发，将类比推理形式化为跨范畴实体对应关系的推断；设计可控的合成任务评估类比推理涌现；通过机制分析探究其内在原理。

Result: 发现类比推理的涌现高度依赖于数据特征、优化选择和模型规模；揭示其由嵌入空间中关系结构的几何对齐和Transformer内部函子应用两个关键机制组成；该现象在预训练大语言模型中也普遍存在。

Conclusion: 将抽象的认知类比概念转化为现代神经网络中具有明确机制基础的具体现象。

Abstract: Analogy is a central faculty of human intelligence, enabling abstract patterns discovered in one domain to be applied to another. Despite its central role in cognition, the mechanisms by which Transformers acquire and implement analogical reasoning remain poorly understood. In this work, inspired by the notion of functors in category theory, we formalize analogical reasoning as the inference of correspondences between entities across categories. Based on this formulation, we introduce synthetic tasks that evaluate the emergence of analogical reasoning under controlled settings. We find that the emergence of analogical reasoning is highly sensitive to data characteristics, optimization choices, and model scale. Through mechanistic analysis, we show that analogical reasoning in Transformers decomposes into two key components: (1) geometric alignment of relational structure in the embedding space, and (2) the application of a functor within the Transformer. These mechanisms enable models to transfer relational structure from one category to another, realizing analogy. Finally, we quantify these effects and find that the same trends are observed in pretrained LLMs. In doing so, we move analogy from an abstract cognitive notion to a concrete, mechanistically grounded phenomenon in modern neural networks.

</details>


### [1123] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: 本文提出了一种基于诊断知识图谱的两步式对话诊断系统，通过生成假设与验证提问交替进行，结合模糊症状建模的真实患者模拟器（基于MIMIC-IV），显著提升了诊断准确率与效率，并获医生临床认可。


<details>
  <summary>Details</summary>
Motivation: 现有对话诊断方法过度依赖模型参数知识或假设患者提供丰富具体信息，不符合临床实际；缺乏能真实模拟早期模糊症状患者的评估环境。

Method: 构建基于诊断知识图谱的两阶段推理系统：（i）从对话上下文中生成诊断假设；（ii）通过生成并提出澄清性问题验证假设，迭代直至确诊；采用并改进基于MIMIC-IV的患者模拟器，使其支持模糊症状表达。

Result: 在实验中诊断准确率和效率均优于强基线；经临床医生评估，所提模拟器具备现实性，生成的问题具有临床实用性。

Conclusion: 基于知识图谱的结构化推理与真实感患者模拟相结合，是提升对话诊断系统临床可行性的有效路径。

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [1124] [Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction](https://arxiv.org/abs/2602.02018)
*Enes Altinisik,Masoomali Fatehkia,Fatih Deniz,Nadir Durrani,Majd Hawasly,Mohammad Raza,Husrev Taha Sencar*

Main category: cs.AI

TL;DR: VeriFY是一种训练时框架，通过一致性自验证引导大语言模型推理事实不确定性，显著降低幻觉率且保持较高召回率。


<details>
  <summary>Details</summary>
Motivation: 现有缓解大语言模型事实幻觉的方法过于保守，依赖外部后验验证或直接将不确定性映射为拒绝回答，效果有限。

Method: 提出VeriFY框架，在训练中引入结构化验证轨迹，包括生成初始答案、构造并回答验证问题、一致性判断、最终回答或拒绝；并采用阶段级损失掩码策略，避免对幻觉答案部分进行监督。

Result: 在多个模型家族和规模上，VeriFY将事实幻觉率降低9.7%至53.3%，仅造成0.4%至5.7%的召回率下降，并在跨数据集上具有良好泛化性。

Conclusion: VeriFY有效提升了大语言模型的事实可靠性，兼顾准确性与鲁棒性，具备实际部署潜力。

Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.

</details>


### [1125] [Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron](https://arxiv.org/abs/2602.02027)
*Sicheng Shen,Mingyang Lv,Han Shen,Jialin Wu,Binghao Wang,Zhou Yang,Guobin Shen,Dongcheng Zhao,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级的安全感知解码方法（NGSD），仅需低成本训练一个专家模型，并利用单个神经元作为门控机制，在保持模型实用性的同时提升输出安全性，具有训练开销小、跨模型规模泛化能力强的优势。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐主要依赖计算昂贵且泛化性差的后训练方法；少量轻量方法又过度依赖预计算的安全注入或模型自身能力，导致泛化性、效率和生成可用性受限。

Method: 提出安全感知解码方法NGSD，通过低开销训练一个专家模型，使用单个神经元作为门控机制，在解码过程中动态平衡模型内在能力与外部安全引导。

Result: 该方法在训练开销和跨模型规模（如不同参数量LLM）的泛化性上显著优于现有方法，同时兼顾输出安全性和语言模型实用性。

Conclusion: NGSD为大语言模型提供了高效、通用且实用的安全对齐新范式，适用于安全可靠的工业部署。

Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.

</details>


### [1126] [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028)
*Ya Gao,Kalle Kujanpää,Pekka Marttinen,Harri Valpola,Alexander Ilin*

Main category: cs.AI

TL;DR: 本文提出一种将新知识内化为推理能力的训练策略，强调通过背景故事、自生成多跳问题和知识蒸馏，使大语言模型能灵活运用新知识进行多步推理。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法仅关注原子事实的记忆，难以将新知识整合进可跨情境使用的连贯框架；作者认为知识内化本质是推理问题而非记忆问题。

Method: 提出基于三个原则的训练策略：1）以连贯背景故事引入新知识；2）用模型自生成的多跳问题训练多步推理；3）采用知识蒸馏，使学生模型在无新知识输入下模仿教师的推理行为。

Result: 实验表明该策略显著提升模型在需融合多个新事实的复杂推理任务上的表现，能有效调用新知识进行推理。

Conclusion: 知识内化应通过任务驱动的多步推理训练实现，而非简单记忆更新；所提策略为大模型持续学习与灵活推理提供了新范式。

Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.

</details>


### [1127] [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029)
*Zhongyuan Lyu,Shuoyu Hu,Lujie Liu,Hongxia Yang,Ming LI*

Main category: cs.AI

TL;DR: 本文提出了一种名为Canonical Intermediate Representation (CIR)的中间表示方法，用于提升大语言模型在将自然语言描述自动转化为优化模型时对复杂操作规则（尤其是复合约束和建模范式）的理解与建模能力；并基于CIR构建了多智能体框架R2C，在新构建的基准测试中达到47.2%准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的方法难以准确处理运筹学问题中复杂的复合约束和合适的建模范式，亟需一种能解耦规则语义与数学表达的中间表示机制。

Method: 提出Canonical Intermediate Representation（CIR）作为显式中间层，通过约束原型和候选建模范式编码操作规则语义；在此基础上构建多智能体rule-to-constraint（R2C）框架，包含文本解析、CIR知识检索与合成、优化模型实例化三个阶段，并引入反思机制提升性能。

Result: R2C在新构建的富含操作规则的基准上达到47.2%准确率（SOTA），在已有基准上表现接近GPT-5等专有模型；加入反思机制后进一步刷新部分基准的最佳结果。

Conclusion: CIR作为一种语义解耦的中间表示，有效提升了LLM在运筹优化建模任务中的可解释性与准确性；R2C框架验证了该范式的有效性，为自然语言驱动的自动化建模提供了新路径。

Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.

</details>


### [1128] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: 本文提出了一种基于有限视界马尔可夫决策过程（MDP）的多智能体系统，用于在合规与尽职调查等受监管场景中建模不确定性与人机协作；各智能体对应特定角色或决策阶段，通过蒙特卡洛估计量化认知不确定性，并以MDP终止状态区分自动判定与人工复核；在AI安全自伤检测评估案例中，相较单智能体基线，准确率提升达19%，人工复核需求减少最高85倍，部分配置下处理时间亦缩短。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体系统多依赖单一智能体的提示工程，在受监管的复杂多步流程（如合规、尽职调查）中难以观测和比较模型在不确定性处理与跨阶段协同（尤其含人工监督）方面的能力。

Method: 将多智能体系统形式化为具有有向无环结构的有限视界马尔可夫决策过程（MDP）；每个智能体对应特定角色或决策阶段（如内容、业务、法律审查），预定义转移表示任务升级或完成；采用蒙特卡洛估计量化智能体级认知不确定性；系统级不确定性由MDP终止于自动标注状态或人工复核状态来表征。

Result: 在AI安全自伤检测评估的合规系统案例中，相比单智能体基线：准确率最高提升19%；所需人工复核量最多减少85倍；部分配置下处理时间降低。

Conclusion: 该多智能体MDP框架能更透明、可量化地建模不确定性与人机协同，显著提升复杂监管任务中的性能与效率，为高可靠性LLM代理系统设计提供了新范式。

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [1129] [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039)
*Wei Liu,Peijie Yu,Michele Orini,Yali Du,Yulan He*

Main category: cs.AI

TL;DR: 本文提出了一种新的智能类型——'探究性智能'（investigatory intelligence），用于衡量大语言模型在数据科学任务中自主设定目标、探索数据并提取关键洞见的能力，并构建了Deep Data Research (DDR) 任务与 DDR-Bench 基准进行可验证评估。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估多聚焦于执行型智能（完成给定任务），而忽视了真实数据分析所需的自主目标设定与开放探索能力；数据科学作为天然测试场景，却缺乏相应基准。

Method: 提出Deep Data Research（DDR）这一开放式数据探究任务，设计基于检查清单的DDR-Bench大规模基准，支持对模型自主探索过程与结果的可验证评估。

Result: 前沿模型展现出初步的探究性智能，但在长周期探索任务上仍面临显著挑战；探究性智能不仅依赖代理架构或模型规模，更取决于模型内在的主动策略。

Conclusion: 探究性智能是衡量LLM真正‘能动性’的关键维度，需从任务设计、评估方法和模型内在机制三方面协同推进。

Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.

</details>


### [1130] [Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents](https://arxiv.org/abs/2602.02050)
*Zeping Li,Hongru Wang,Yiwen Zhao,Guanhua Chen,Yixia Li,Keyang Chen,Yixin Cao,Guangnan Ye,Hongfeng Chai,Mengdi Wang,Zhenfei Yin*

Main category: cs.AI

TL;DR: 本文提出基于熵减少的监督信号来优化大语言模型工具使用代理的行为，设计了稀疏结果奖励和密集过程奖励两种策略，分别提高了工具使用的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 工具使用代理在长轨迹中常触发过多且低质量的工具调用，导致延迟增加和推理性能下降，管理工具使用行为具有挑战性。

Method: 通过基于熵的先导实验发现熵减少与高质量工具调用呈强正相关，进而提出以熵减少为监督信号，并设计稀疏结果奖励（粗粒度轨迹级指导）和密集过程奖励（细粒度过程监督）两种奖励策略。

Result: 实验表明，稀疏结果奖励使工具调用次数相比基线平均减少72.07%，密集过程奖励使性能提升22.27%。

Conclusion: 熵减少是增强工具使用行为的关键机制，有助于提升代理在现实应用中的适应性。

Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.

</details>


### [1131] [SIDiffAgent: Self-Improving Diffusion Agent](https://arxiv.org/abs/2602.02051)
*Shivank Garg,Ayush Singh,Gaurav Kumar Nayak*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的自改进扩散代理框架SIDiffAgent，利用Qwen系列多模态模型实现自动提示工程、生成质量检测与修复、以及基于记忆的迭代优化，显著提升文本到图像生成的可控性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型存在对提示词敏感、语义歧义、生成伪影及提示工程复杂等问题，且多数改进方法需额外训练、可控性差，难以适应实际应用需求。

Method: 提出SIDiffAgent框架，集成Qwen-VL、Qwen-Image、Qwen-Edit和Qwen-Embedding等模型，实现无需训练的端到端代理式流程：包括自主提示优化、生成结果诊断与修正、细粒度伪影去除，并通过经验数据库支持迭代式自我改进与提示引导。

Result: 在GenAIBench上平均VQA得分为0.884，显著优于现有开源、闭源及代理类方法。

Conclusion: SIDiffAgent验证了训练-free、多模型协同、记忆增强的代理范式在提升扩散模型实用性与鲁棒性方面的有效性，为可控图像生成提供了新路径。

Abstract: Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.

</details>


### [1132] [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133)
*Sangwoo Shin,BumJun Kim,Kyelim Lee,Moongyu Jeon,Albert No*

Main category: cs.AI

TL;DR: 本文揭示了掩码扩散语言模型（MDMs）比自回归模型（ARMs）更少受“反转诅咒”影响的根本原因，指出其关键在于Transformer编码器中的权重共享结构导致前向与反向注意力分数正相关，且梯度对齐，从而在优化前向损失时也降低了反向损失。


<details>
  <summary>Details</summary>
Motivation: 解释为何掩码扩散语言模型（MDMs）比自回归模型（ARMs）更能缓解‘反转诅咒’这一关键失败模式。

Method: 理论分析结合实验验证：在单层Transformer编码器中推导权重共享如何使前向与反向注意力分数正相关、梯度对齐；并在可控玩具任务和大规模扩散语言模型上进行实验验证。

Result: 证实MDMs对反转诅咒的缓解主要源于架构（特别是权重共享）与训练的交互，而非仅因any-order训练目标；前向与反向注意力及梯度存在正相关与对齐关系。

Conclusion: MDMs部分克服反转诅咒的关键机制是架构层面的权重共享引发的注意力与梯度对称性，这为理解生成模型的推理泛化能力提供了新视角。

Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that "$A$ is $B$", they often fail on the reverse query "$B$ is $A$". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing "[MASK] is $B$" during training does not necessarily teach the model to handle the reverse prompt "$B$ is [MASK]". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.

</details>


### [1133] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

TL;DR: 本文提出DGR方法，通过将外部安全推理数据集对齐目标大模型的内部分布，缓解安全对齐带来的推理能力下降问题，并发现少量样本即可激活有效拒绝行为。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐数据集由外部模型或人工标注生成，与目标大模型存在分布差异，作者推测该分布差距是导致目标模型推理能力显著下降（即安全税）的主因。

Method: 提出DGR（Distribution-Grounded Refinement）方法，对现有非目标分布的安全推理数据集进行变换与精炼，使其适配目标大语言模型的内在分布。

Result: DGR在保持安全性能的同时显著缓解安全税：相比基础监督微调（Vanilla SFT），DirectRefusal提升30.2%，R1-ACT提升21.2%；且推理退化程度与分布偏移程度正相关；仅需10个样本即可有效激活拒绝行为。

Conclusion: 分布一致性对安全对齐至关重要；安全对齐在大推理模型中更像一种‘激活机制’，而非知识注入过程；DGR为兼顾安全性与推理能力提供了新范式。

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [1134] [Traffic-Aware Navigation in Road Networks](https://arxiv.org/abs/2602.02158)
*Sarah Nassar*

Main category: cs.AI

TL;DR: This paper compares three graph search algorithms—Floyd-Warshall-Ingerman, Dijkstra’s/A*, and Yen’s—for traffic-aware navigation in Kingston’s road network, evaluating trade-offs among preprocessing cost, real-time speed, and traffic-aware optimality.


<details>
  <summary>Details</summary>
Motivation: To identify the most suitable graph search approach for traffic-aware navigation in a real-world urban road network, balancing preprocessing overhead, real-time performance, and solution quality.

Method: Empirical comparison of three algorithms: (1) Floyd-Warshall-Ingerman (multi-query preprocessing), (2) Dijkstra’s and A* (continuous single-query real-time search), and (3) Yen’s algorithm (top-K shortest paths followed by real-time iteration).

Result: Dijkstra’s and A* achieved best traffic-aware optimality with minimal preprocessing; Floyd-Warshall-Ingerman was fastest in real time but traffic-agnostic; Yen’s offered a middle ground with higher preprocessing but balanced runtime and optimality.

Conclusion: No single algorithm dominates all criteria; optimal choice depends on deployment context—e.g., real-time responsiveness vs. traffic awareness vs. computational constraints.

Abstract: This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.

</details>


### [1135] [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188)
*Xia Jiang,Jing Chen,Cong Zhang,Jie Gao,Chengpeng Hu,Chenhao Zhang,Yaoxin Wu,Yingqian Zhang*

Main category: cs.AI

TL;DR: 本文提出了NLCO基准，用于评估大语言模型在组合优化任务上的端到端推理能力，涵盖43个问题并建立四层分类体系；实验表明当前LLM在小规模实例上表现良好，但随规模增大性能显著下降，且在图结构和瓶颈目标等问题上更易失败。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在数学与逻辑推理中表现优异，但在需在高维解空间中满足硬约束的组合优化（CO）任务上的能力尚不明确，亟需系统性评测基准。

Method: 构建了自然语言组合优化基准NLCO，包含43个CO问题，并基于变量类型、约束族、全局模式和目标类别建立四层分类法；提供求解器标注的真值解，从可行性、解最优性和推理效率三方面全面评测主流LLM。

Result: 实验发现高性能LLM在小规模实例上可行性与解质量均较好，但随实例规模增大二者均明显下降；且在集合型任务上表现较好，而在图结构问题和瓶颈类目标上失败率更高。

Conclusion: 当前LLM在自然语言驱动的组合优化任务上仍存在显著局限，尤其在处理大规模、图结构化及瓶颈优化问题时，揭示了其符号推理与约束满足能力的瓶颈，为后续研究提供了细粒度评测框架与改进方向。

Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \textbf{N}atural \textbf{L}anguage \textbf{C}ombinatorial \textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.

</details>


### [1136] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: 本文提出了Test-time Improvement Diagnostic Evaluation (TIDE)框架，用于系统评估自主大语言模型（LLM）智能体在测试时改进（TTI）过程中的行为机制，聚焦于任务完成的时间动态、递归循环行为与记忆负担三方面。


<details>
  <summary>Details</summary>
Motivation: 现有对测试时改进（TTI）的评估方法未能有效刻画智能体的任务优化效率、错误后行为适应能力及工作记忆的具体作用，其成功/失败机制尚不清晰。

Method: 提出TIDE——一种智能体无关、环境无关的诊断性评估框架，将TTI解耦为三个相互关联的维度：(1)任务完成的整体时间动态；(2)性能是否受限于递归循环行为；(3)是否受累积记忆负担制约。

Result: 通过跨多种智能体与环境的大规模实验，TIDE揭示：提升智能体性能不能仅依赖内部推理扩展，更需显式优化智能体与环境之间的交互动力学。

Conclusion: TIDE为理解与改进TTI提供了可解释、可分解的评估范式，强调交互设计比单纯扩大模型规模更为关键。

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [1137] [More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression](https://arxiv.org/abs/2602.02199)
*Aryan Sood,Tanvi Sharma,Vansh Agrawal*

Main category: cs.AI

TL;DR: 本文提出LASER-KV框架，通过分块累积策略和精确LSH召回机制，在严格累积内存预算下实现高效KV缓存压缩，显著优于现有方法，在128k上下文长度下准确率提升最高达10%。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法在节省内存的同时牺牲语义召回能力，且依赖注意力分数作为token重要性代理的假设缺乏充分验证。

Method: 提出LASER-KV框架，采用层累积选择（Layer Accumulated Selection）与精确局部敏感哈希（Exact-LSH）召回机制，并引入保护因子n控制分块式累积压缩，避免滑动窗口干扰。

Result: 在Babilong基准测试中，相比先前方法性能下降15–30%，LASER-KV在128k长度下保持稳定，准确率最高提升10%。

Conclusion: 注意力分数不足以可靠衡量token效用；基于累积预算与精确召回的压缩策略更优，挑战了当前主流假设。

Abstract: While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.

</details>


### [1138] [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304)
*Martino Ciaperoni,Marzio Di Vece,Luca Pappalardo,Fosca Giannotti,Francesco Giannini*

Main category: cs.AI

TL;DR: 本文提出了一种新的可解释人工智能框架——比较型XAI（Δ-XAI），用于解释大模型在干预（如微调、强化学习等）后出现的行为变化，强调对参考模型与干预模型之间的差异进行对比解释，而非孤立分析单个模型。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI（XAI）方法只能分析单一模型快照，难以解释模型在不同训练阶段或干预后内部行为的变化机制；而大模型中广泛存在的行为突变（behavioral shifts）亟需一种能刻画‘变化本身’的解释范式。

Method: 提出比较型XAI（Δ-XAI）框架，定义其核心目标为解释干预引起的模型行为差异，并给出形式化设计准则（desiderata）；构建若干Δ-XAI方法流程，并通过具体实验验证其有效性。

Result: 建立了首个面向行为变化的系统性XAI框架Δ-XAI，明确了比较解释所需满足的关键性质，并展示了可行的实现路径与实证案例。

Conclusion: 解释大模型的行为突变必须转向比较视角；Δ-XAI为理解模型演化提供了新范式，弥补了传统XAI在动态建模上的结构性缺陷。

Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.

</details>


### [1139] [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313)
*Changming Li,Kaixing Zhang,Haoyun Xu,Yingdong Shi,Zheng Zhang,Kaitao Song,Kan Ren*

Main category: cs.AI

TL;DR: 本文提出了一种名为Integrated Policy Gradient（IPG）的新框架，用于定位和调控大语言模型中对推理行为具有序列性贡献的内部组件，通过将基于结果的信号（如推理后准确率）沿推理轨迹反向传播，实现更精确的推理机制解释与调控。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法难以精确定位复杂推理机制或捕捉从模型内部到推理输出的序列影响。

Method: 提出Integrated Policy Gradient（IPG）框架，基于结果导向与序列影响感知原则，将复合的结果信号（如推理准确率）沿模型推理轨迹反向传播，归因推理行为至内部组件。

Result: 实验证明IPG能更精确地定位推理相关组件，并在多种推理模型上可靠地调节推理能力与强度。

Conclusion: IPG为理解与调控大语言模型复杂推理行为提供了新范式，提升了推理机制的可解释性与可控性。

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.

</details>


### [1140] [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369)
*Yaolun Zhang,Yiran Wu,Yijiong Yu,Qingyun Wu,Huazheng Wang*

Main category: cs.AI

TL;DR: 本文提出了Live-Evo，一种面向流式数据的在线自演化记忆系统，通过解耦经验与元指南、动态加权更新经验，显著提升LLM代理在真实持续学习场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自演化系统多基于静态数据集，难以应对真实分布偏移和持续反馈，缺乏真正的在线学习能力。

Method: Live-Evo构建Experience Bank和Meta-Guideline Bank，分离‘发生了什么’与‘如何使用’；通过反馈动态调整经验权重，实现类人式的强化与遗忘机制。

Result: 在10周Prophet Arena实时基准上，Brier分数提升20.8%，市场收益提升12.9%；并在深度研究基准上展现出跨任务泛化能力。

Conclusion: Live-Evo为LLM代理提供了可扩展、鲁棒且持续适应的在线记忆演化框架，推动了从静态评估向真实持续学习的范式转变。

Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.

</details>


### [1141] [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416)
*Ankur Samanta,Akshayaa Magesh,Ayush Jain,Kavosh Asadi,Youliang Yu,Daniel Jiang,Boris Vidolov,Kaveh Hassani,Paul Sajda,Jalaj Bhandari,Yonathan Efroni*

Main category: cs.AI

TL;DR: 本文提出Thought-ICS框架，通过将推理分解为离散、语义连贯的‘思维步骤’，使语言模型能准确定位错误并自我修正，显著提升自纠错能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型的自我纠错能力仍不理想；受人类大脑在离散决策点监测错误并重采样的启发，探索模型能否显式定位推理中的错误。

Method: 提出结构化推理提示方法，并构建Iterative Correction Sampling of Thoughts（Thought-ICS）框架：逐个生成完整且离散的思维步骤，验证后定位首个错误步，回溯至最后一个正确步骤重新采样替代推理。

Result: 在有oracle验证的设定下，Thought-ICS实现20–40%的自我纠正性能提升；在完全自主（无外部验证）设定下，优于当前主流自纠错基线。

Conclusion: 将推理显式结构化为离散思维步骤是提升语言模型自我纠错能力的有效路径，Thought-ICS为构建可自校验AI系统提供了新范式。

Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

</details>


### [1142] [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: 本文提出SafeGround框架，通过不确定性感知和校准机制，在GUI界面定位任务中实现风险可控的预测，显著提升系统级准确率。


<details>
  <summary>Details</summary>
Motivation: GUI定位错误可能导致高成本、难以逆转的操作（如错误支付批准），因此需要提高模型可靠性。

Method: SafeGround采用分布感知的不确定性量化方法，捕获模型输出随机样本的空间离散度，并在校准过程中推导具有统计保证的错误发现率（FDR）控制的测试时决策阈值。

Result: 在ScreenSpot-Pro基准上，SafeGround的不确定性度量优于现有基线，能更好地区分正确与错误预测；校准后的阈值可实现严格的风险控制，并将多个GUI定位模型的系统级准确率最高提升5.38个百分点。

Conclusion: SafeGround为GUI接地任务提供了可靠、可解释且具备统计保障的风险感知解决方案，提升了自动化GUI交互的安全性与实用性。

Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference.

</details>


### [1143] [Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling](https://arxiv.org/abs/2602.02453)
*Andong Chen,Wenxin Zhu,Qiuyu Ding,Yuchen Song,Muyun Yang,Tiejun Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种新的视觉推理范式——'Thinking with Comics'，利用漫画作为图像与视频之间的高信息密度中间媒介，以兼顾时间结构、文本嵌入和叙事连贯性，同时降低计算开销；实验表明其在多步时序与因果推理任务上优于基于图像的方法，且比基于视频的方法更高效。


<details>
  <summary>Details</summary>
Motivation: 静态图像难以表达时间结构，而视频则冗余度高、计算成本大，亟需一种兼顾时间性、信息密度与效率的中间视觉表征形式。

Method: 提出'Thinking with Comics'范式，系统研究两条基于漫画的推理路径，并在多种推理任务和长上下文理解任务上进行评估。

Result: Thinking with Comics在多步时序与因果推理任务上优于Thinking with Images，且显著比Thinking with Video更高效；不同漫画叙事结构与风格对性能具有一致性影响。

Conclusion: 漫画作为一种高效的中间视觉表示，在提升多模态推理能力方面具有显著潜力。

Abstract: Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.

</details>


### [1144] [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455)
*Han Bao,Zheyuan Zhang,Pengcheng Jing,Zhengqing Yuan,Kaiwen Shi,Yanfang Ye*

Main category: cs.AI

TL;DR: 本文提出了Drift-Bench，首个用于评估大语言模型作为自主智能体在面对用户输入缺陷（如隐含意图、参数缺失、错误预设或表达模糊）时，通过多轮澄清进行语用推理能力的诊断性基准。它结合状态导向与服务导向执行环境，基于经典交际理论构建故障分类体系，并引入人格驱动的用户模拟器与Rise评估协议。实验表明现有模型在各类输入故障下性能显著下降，且澄清效果因用户人格和故障类型而异。


<details>
  <summary>Details</summary>
Motivation: 现有基准假设用户指令规范或仅限单轮文本澄清，无法衡量智能体在真实执行风险下的多轮消歧能力；而实际中用户输入常违反合作原则，带来未被文本评估捕获的执行风险。

Method: 提出Drift-Bench基准，构建统一的合作失效分类法；设计人格驱动的用户模拟器；采用Rise评估协议，在状态导向和服务导向两类具身执行环境中开展多轮澄清任务评测。

Result: 实验显示主流智能体在各类输入故障下性能大幅下降；澄清有效性高度依赖用户人格设定与故障类型；Drift-Bench可系统揭示导致不安全执行的语用失败模式。

Conclusion: Drift-Bench填补了智能体语用能力与安全评估之间的空白，为提升自主智能体在非理想用户交互下的鲁棒性与安全性提供了可诊断、可量化的基准框架。

Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.

</details>


### [1145] [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465)
*Jana Zeller,Thaddäus Wiedemer,Fanfei Li,Thomas Klein,Prasanna Mayilvahanan,Matthias Bethge,Felix Wichmann,Ryan Cotterell,Wieland Brendel*

Main category: cs.AI

TL;DR: 本文提出MentisOculi评测套件，用于评估统一多模态模型（UMMs）是否能像人类一样利用视觉表征辅助推理；实验发现当前UMMs虽能生成正确图像，却无法有效利用视觉信息（包括真实图像），视觉策略反而未提升性能，揭示其在视觉-语言协同推理上的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 前沿多模态模型正从仅理解视觉信息的MLLMs向能原生交错生成的UMMs演进，研究者开始探索用中间可视化作为类比人类心智意象的推理辅助手段，但尚缺乏系统评估该能力的方法与实证依据。

Method: 构建了程序化、分层的多步视觉可解推理评测集MentisOculi，并在多种视觉策略（如潜变量token、显式生成图像）下对前沿UMMs进行评估，同时分析其对真实图像的利用能力。

Result: 实验表明：各类视觉策略普遍未能提升模型性能；UMMs虽具文本推理能力且偶能生成正确图像，但存在生成误差累积问题，甚至无法有效利用提供给它的ground-truth可视化输入。

Conclusion: 当前统一多模态模型尚不具备可靠利用‘视觉思维’辅助推理的能力，视觉表征尚未成为其有效推理工具；MentisOculi为后续诊断和弥合该能力差距提供了基础评测框架。

Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.

</details>


### [1146] [Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts](https://arxiv.org/abs/2602.02468)
*Aiden Yiliu Li,Xinyue Hao,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: Avenir-Web 是一种新型开源网络代理，通过混合定位专家、经验模仿规划和自适应任务跟踪记忆机制，在真实网页任务（Online-Mind2Web 基准）上达到当前最优性能，显著优于以往开源方法，并媲美顶尖闭源模型。


<details>
  <summary>Details</summary>
Motivation: 现有自主网页代理在长周期任务中存在元素定位不准、缺乏网站特异性流程知识、以及DOM结构复杂下任务跟踪与记忆不稳定等问题。

Method: 提出Avenir-Web，包含三部分核心：1）混合定位专家（Mixture of Grounding Experts）提升元素识别精度；2）基于经验模仿的规划（Experience-Imitation Planning）注入网站流程先验知识；3）任务检查清单与自适应记忆机制保障长程任务一致性。

Result: 在Online-Mind2Web真实网页基准测试中，Avenir-Web显著超越所有现有开源代理，性能与顶级闭源模型持平，成为新的开源SOTA。

Conclusion: Avenir-Web通过多模块协同设计有效缓解了网页代理在动态复杂界面下的关键瓶颈，验证了结合定位增强、流程先验与鲁棒记忆是构建可靠开源网页代理的有效路径。

Abstract: Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.

</details>


### [1147] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

TL;DR: 本文提出一种名为'Identity Bridge'的简单数据增强方法（如'A → A'），通过在训练数据中加入此类样本，使自回归大语言模型能够突破'反转诅咒'（reversal curse），即从'A → B'泛化到'B ← A'的能力；理论证明单层Transformer在该策略下可克服此限制，实验表明1B模型微调后反转任务成功率从接近0%提升至40%。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为自回归大语言模型因架构本质难以掌握逻辑反转等高层规则，仅记忆事实性知识；本文质疑这一‘固有局限’观点，旨在探索低成本、数据驱动的方法促使模型学习可泛化的规则。

Method: 提出Identity Bridge正则化数据配方（形式为'A → A'）并融入前向知识（'A → B'）训练数据；理论层面分析梯度下降在单层Transformer上的隐式偏置；实验层面在1B参数预训练模型上进行微调验证。

Result: 理论证明单层Transformer在Identity Bridge数据下可打破反转诅咒；实证显示微调后模型在反转任务上成功率从近0%显著提升至40%。

Conclusion: 反转诅咒并非自回归LLM不可逾越的固有缺陷，而可通过简单、低开销的数据正则化策略缓解；该工作为理解与提升LLM的规则归纳能力提供了新理论视角与实用路径。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>


### [1148] [AgentRx: Diagnosing AI Agent Failures from Execution Trajectories](https://arxiv.org/abs/2602.02475)
*Shraddha Barke,Arnav Goyal,Alind Khare,Avaljot Singh,Suman Nath,Chetan Bansal*

Main category: cs.AI

TL;DR: 本文提出AGENTRX框架，用于自动定位AI代理运行失败的关键步骤，并构建了一个包含115条失败轨迹的新基准，涵盖API工作流、事件管理及开放性网络/文件任务。


<details>
  <summary>Details</summary>
Motivation: AI代理的执行具有概率性、长周期、多代理及工具输出噪声大等特点，导致其失败原因难以定位。

Method: 通过人工标注失败轨迹构建跨领域失败分类法，并提出AGENTRX：一种基于约束合成与逐步验证的自动化诊断框架，结合LLM裁判进行关键步骤与失败类别定位。

Result: AGENTRX在三个领域中均优于现有基线方法，提升了关键步骤定位与失败归因的准确性。

Conclusion: AGENTRX是一种可审计、领域无关的AI代理失败诊断框架，有效降低了人工归因成本，并为理解代理失败机制提供了结构化基准与工具。

Abstract: AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.

</details>
