<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 63]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.AI](#cs.AI) [Total: 32]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029)
*Yihao Luo,Xianglong He,Chuanyu Pan,Yiwen Chen,Jiaqi Wu,Yangguang Li,Wanli Ouyang,Yuanming Hu,Guang Yang,ChoonHwai Yap*

Main category: cs.CV

TL;DR: 提出了一种名为Faithful Contouring的稀疏体素化表示方法，支持高分辨率且无需水密化或等值面提取，实现了接近无损的几何保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于等值面的3D网格体素化方法依赖水密化或渲染优化，损害了几何保真度，难以准确表达复杂几何与拓扑结构。

Method: 提出Faithful Contouring，一种无需将网格转换为场函数或进行等值面提取的稀疏体素化表示；设计双模式自编码器以实现可扩展且细节保留的形状重建。

Result: 在直接表示中达到10^{-5}级别的距离误差；在网格重建中Chamfer Distance降低93%，F-score提升35%，显著优于基线方法。

Conclusion: Faithful Contouring在精度和效率上均优于现有方法，能高保真地表示任意复杂网格，适用于3D重建与生成任务。

Abstract: Accurate and efficient voxelized representations of 3D meshes are the
foundation of 3D reconstruction and generation. However, existing
representations based on iso-surface heavily rely on water-tightening or
rendering optimization, which inevitably compromise geometric fidelity. We
propose Faithful Contouring, a sparse voxelized representation that supports
2048+ resolutions for arbitrary meshes, requiring neither converting meshes to
field functions nor extracting the isosurface during remeshing. It achieves
near-lossless fidelity by preserving sharpness and internal structures, even
for challenging cases with complex geometry and topology. The proposed method
also shows flexibility for texturing, manipulation, and editing. Beyond
representation, we design a dual-mode autoencoder for Faithful Contouring,
enabling scalable and detail-preserving shape reconstruction. Extensive
experiments show that Faithful Contouring surpasses existing methods in
accuracy and efficiency for both representation and reconstruction. For direct
representation, it achieves distance errors at the $10^{-5}$ level; for mesh
reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\%
improvement in F-score over strong baselines, confirming superior fidelity as a
representation for 3D learning tasks.

</details>


### [2] [LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices](https://arxiv.org/abs/2511.03765)
*Hyunseok Kwak,Kyeongwon Lee,Jae-Jin Lee,Woojoo Lee*

Main category: cs.CV

TL;DR: LoRA-Edge是一种面向边缘设备的参数高效微调方法，基于低秩适应（LoRA）并引入张量分解（TT-SVD），在保持推理开销不变的前提下，显著减少可训练参数数量，实现快速、高效的CNN模型在线微调。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的CNN全量微调受限于内存、计算和能耗预算，难以应对域偏移问题，尤其是在人类活动识别（HAR）等应用中，因此需要一种高效且结构对齐的微调方法。

Method: LoRA-Edge采用张量分解（TT-SVD）对预训练卷积层进行分解，仅选择性更新输出侧的核心参数，并通过零初始化保证辅助路径初始无影响，最后将更新融合回原始卷积核，保持推理时的模型结构和计算成本不变。

Result: 在多个HAR数据集和CNN骨干网络上，LoRA-Edge仅更新最多1.49%的参数，准确率与全微调相差在4.7%以内，且性能优于现有参数高效方法；在Jetson Orin Nano上收敛速度提升1.4-3.8倍。

Conclusion: LoRA-Edge通过结构对齐的低秩适应设计，实现了高效、实用的边缘端CNN模型微调，为资源受限设备上的持续学习提供了可行方案。

Abstract: On-device fine-tuning of CNNs is essential to withstand domain shift in edge
applications such as Human Activity Recognition (HAR), yet full fine-tuning is
infeasible under strict memory, compute, and energy budgets. We present
LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on
Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies
Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional
layers, (ii) selectively updates only the output-side core with
zero-initialization to keep the auxiliary path inactive at the start, and (iii)
fuses the update back into dense kernels, leaving inference cost unchanged.
This design preserves convolutional structure and reduces the number of
trainable parameters by up to two orders of magnitude compared to full
fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves
accuracy within 4.7% of full fine-tuning while updating at most 1.49% of
parameters, consistently outperforming prior parameter-efficient baselines
under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and
selective-core training yield 1.4-3.8x faster convergence to target F1.
LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN
adaptation practical for edge platforms.

</details>


### [3] [SILVI: Simple Interface for Labeling Video Interactions](https://arxiv.org/abs/2511.03819)
*Ozan Kanbertay,Richard Vogg,Elif Karakoc,Peter M. Kappeler,Claudia Fichtel,Alexander S. Ecker*

Main category: cs.CV

TL;DR: SILVI是一个开源的视频标注工具，能够同时标注动物行为和个体间的互动，填补了现有工具在行为生态学与计算机视觉结合中的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的开源标注工具无法同时支持个体定位和交互行为标注，限制了对社会性和个体化动物行为的深入研究。

Method: 开发了一个名为SILVI的开源标注软件，集成行为标注与个体定位功能，可在视频中直接标注行为和互动，并生成适用于训练和验证计算机视觉模型的结构化输出。

Result: SILVI成功实现了对动物行为和互动的精细化标注，支持动态场景图的提取，可用于动物行为研究及相关领域。

Conclusion: SILVI有效连接了行为生态学与计算机视觉，推动了自动化精细行为分析的发展，具有广泛的应用潜力。

Abstract: Computer vision methods are increasingly used for the automated analysis of
large volumes of video data collected through camera traps, drones, or direct
observations of animals in the wild. While recent advances have focused
primarily on detecting individual actions, much less work has addressed the
detection and annotation of interactions -- a crucial aspect for understanding
social and individualized animal behavior. Existing open-source annotation
tools support either behavioral labeling without localization of individuals,
or localization without the capacity to capture interactions. To bridge this
gap, we present SILVI, an open-source labeling software that integrates both
functionalities. SILVI enables researchers to annotate behaviors and
interactions directly within video data, generating structured outputs suitable
for training and validating computer vision models. By linking behavioral
ecology with computer vision, SILVI facilitates the development of automated
approaches for fine-grained behavioral analyses. Although developed primarily
in the context of animal behavior, SILVI could be useful more broadly to
annotate human interactions in other videos that require extracting dynamic
scene graphs. The software, along with documentation and download instructions,
is available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.

</details>


### [4] [Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets](https://arxiv.org/abs/2511.03855)
*Duong Mai,Lawrence Hall*

Main category: cs.CV

TL;DR: 本研究探讨了在训练过程中引入基本噪声（如高斯、斑点、泊松和椒盐噪声）以提高深度学习模型在不同分布数据下的泛化能力，特别是在胸部X光片中检测COVID-19的应用中显著缩小了分布内与分布外数据的性能差距。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在图像识别中难以泛化到不同设备或人群的数据，尤其在COVID-19的CXRs检测中，模型容易依赖训练数据中的源特异性伪影而非真正的生物标志物，导致在新临床来源的分布外数据上表现不佳。

Method: 通过在训练过程中注入多种基本噪声（高斯、斑点、泊松、椒盐噪声），增强模型对分布偏移的鲁棒性，并在多个评估指标下验证其效果。

Result: 该方法将分布内与分布外数据之间的性能差距从0.10-0.20显著降低至0.01-0.06（基于AUC、F1、准确率、召回率和特异性等指标在十个随机种子下的平均结果）。

Conclusion: 噪声注入是一种简单而有效的方法，可提升深度学习模型在医学图像识别中的跨分布泛化能力，有助于更可靠地应用于临床实践。

Abstract: Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood

</details>


### [5] [Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures](https://arxiv.org/abs/2511.03882)
*Florence Klitzner,Blanca Inigo,Benjamin D. Killeen,Lalithkumar Seenivasan,Michelle Song,Axel Krieger,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文研究了基于模仿学习的机器人控制策略在双平面X射线引导下套管插入手术中的应用，开发了一个高仿真的仿真沙箱环境，并训练了仅基于视觉信息进行迭代对齐的模仿学习策略，在首次尝试中成功率达到68.5%，并在复杂解剖结构和不同初始化条件下表现出良好的泛化性和鲁棒性，尽管在穿刺点精度方面仍存在局限。


<details>
  <summary>Details</summary>
Motivation: 由于多视角X射线解读复杂，尚不清楚基于视频的模仿学习是否适用于X射线引导下的脊柱手术，因此本文旨在探索其在双平面X射线引导下套管插入中的机会与挑战。

Method: 开发了一个高度真实的、可扩展的、自动化的X射线引导脊柱手术的仿真沙箱环境，构建了包含正确轨迹和对应双平面X射线序列的数据集，并在此基础上训练了用于规划和开环控制的模仿学习策略，使其能仅根据视觉信息迭代对齐套管。

Result: 所提出的策略在首次尝试中成功率达到68.5%，能够在不同的椎体水平维持安全的椎弓内路径，对骨折等复杂解剖结构具有良好的泛化能力，且对不同初始条件保持鲁棒；在真实双平面X射线上进行测试也显示出模型能够生成合理的轨迹，尽管训练完全在仿真中完成。

Conclusion: 尽管结果初步但令人鼓舞，模仿学习在X射线引导脊柱手术中具有潜力，未来结合更强的先验知识和领域知识，可能为轻量化、无需CT的术中脊柱导航机器人系统奠定基础，但需进一步解决闭环控制反馈频率和穿刺点精度等问题。

Abstract: Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.

</details>


### [6] [Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model](https://arxiv.org/abs/2511.03888)
*Abdulmumin Sa'ad,Sulaimon Oyeniyi Adebayo,Abdul Jabbar Siddiqui*

Main category: cs.CV

TL;DR: 提出一种基于轻量级YOLOv12与自对抗训练（SAT）和数据增强策略的实时废物检测框架，用于在沙漠环境中通过无人机高效检测各类废物。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾收集方法在偏远或恶劣环境（如沙漠）中效率低且危险，现有研究多集中于城市环境和可回收物，忽视了有机和有害垃圾及复杂地形的需求。

Method: 基于剪枝后的轻量级YOLOv12模型，结合自对抗训练（SAT）和专用数据增强策略，在DroneTrashNet数据集上进行训练与验证，并评估其精度、召回率、mAP、延迟和模型大小。

Result: 所提方法在精度、召回率和mAP上均有显著提升，同时具备低延迟和小模型尺寸，适合资源受限的无人机部署，且在与其他轻量级YOLO变体对比中表现出更优的准确率与效率平衡。

Conclusion: 数据驱动与模型优化相结合的方法能有效提升沙漠环境下无人机垃圾检测的鲁棒性与实时性，具有实际应用潜力。

Abstract: The global waste crisis is escalating, with solid waste generation expected
to increase by 70% by 2050. Traditional waste collection methods, particularly
in remote or harsh environments like deserts, are labor-intensive, inefficient,
and often hazardous. Recent advances in computer vision and deep learning have
opened the door to automated waste detection systems, yet most research focuses
on urban environments and recyclable materials, overlooking organic and
hazardous waste and underexplored terrains such as deserts. In this work, we
propose an enhanced real-time object detection framework based on a pruned,
lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)
and specialized data augmentation strategies. Using the DroneTrashNet dataset,
we demonstrate significant improvements in precision, recall, and mean average
precision (mAP), while achieving low latency and compact model size suitable
for deployment on resource-constrained aerial drones. Benchmarking our model
against state-of-the-art lightweight YOLO variants further highlights its
optimal balance of accuracy and efficiency. Our results validate the
effectiveness of combining data-centric and model-centric enhancements for
robust, real-time waste detection in desert environments.

</details>


### [7] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine,Majid Ben Yakhlef,Soulaiman El Hazzat*

Main category: cs.CV

TL;DR: 本文提出了一种类别级图像融合方法（Class-Based Image Composition），通过将同类图像合成为复合图像（CoImg）来提升小样本、类别不平衡数据下的深度学习模型性能，在OCT眼底图像数据集上实现了接近完美的诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 针对小样本、类别不平衡及图像质量差导致深度学习模型误判率高的问题，需要增强训练样本的信息密度和类内多样性。

Method: 提出Class-Based Image Composition方法，将同一类别的多张图像融合为复合输入图像（CoImg），构建了类平衡的Co-OCTDL数据集，并采用VGG16模型在原始与复合图像上进行对比实验，保持模型结构和超参数一致。

Result: 在Co-OCTDL数据集上，模型准确率达到99.6%，F1-score为0.995，AUC为0.9996，显著优于原始数据集上的基线模型，且误判率明显降低。

Conclusion: 该方法能有效提升小样本和类别不平衡情况下模型的诊断性能，通过增强样本信息密度和类内方差，实现高质量预测。

Abstract: Small, imbalanced datasets and poor input image quality can lead to high
false predictions rates with deep learning models. This paper introduces
Class-Based Image Composition, an approach that allows us to reformulate
training inputs through a fusion of multiple images of the same class into
combined visual composites, named Composite Input Images (CoImg). That enhances
the intra-class variance and improves the valuable information density per
training sample and increases the ability of the model to distinguish between
subtle disease patterns. Our method was evaluated on the Optical Coherence
Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et
al., 2024), which contains 2,064 high-resolution optical coherence tomography
(OCT) scans of the human retina, representing seven distinct diseases with a
significant class imbalance. We constructed a perfectly class-balanced version
of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout
composite image. To assess the effectiveness of this new representation, we
conducted a comparative analysis between the original dataset and its variant
using a VGG16 model. A fair comparison was ensured by utilizing the identical
model architecture and hyperparameters for all experiments. The proposed
approach markedly improved diagnostic results.The enhanced Dataset achieved
near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared
to a baseline model trained on raw dataset. The false prediction rate was also
significantly lower, this demonstrates that the method can producehigh-quality
predictions even for weak datasets affected by class imbalance or small sample
size.

</details>


### [8] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: 提出一种无需异常标签的无监督框架，通过小型验证的正常图像种子，交替进行轻量级适配器更新和不确定性门控样本加入，逐步扩展可信正常样本集，有效提升医学图像中未知异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于标注异常样本稀缺且专家监督成本高，医学影像中未知异常检测面临挑战，需要一种无需异常标签、可增量学习的高效无监督方法。

Method: 基于冻结的预训练视觉主干网络，添加小型卷积适配器进行快速领域自适应；利用紧凑的coreset存储特征嵌入，通过k近邻进行异常评分；采用基于z-score距离阈值和SWAG估计的认知不确定性双门控机制，确保正常样本集增量扩展的安全性。

Result: 在多个医学影像数据集上显著优于基线方法：COVID-CXR的ROC-AUC从0.9489提升至0.9982，Pneumonia CXR从0.6834提升至0.8968，Brain MRI ND-5的ROC-AUC从0.6041提升至0.7269，PR-AUC从0.7539提升至0.8211。

Conclusion: 该框架在标签稀缺的实际医学影像场景中表现出高效性和有效性，能够在不依赖生成重建或回放缓冲的情况下持续优化正常性定义，防止模型漂移和错误纳入异常样本。

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [9] [Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization](https://arxiv.org/abs/2511.03943)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文提出了两种互补的方法：边界距离回归（BDR）和自适应时间细化（ATR），用于提升时序动作定位的边界检测精度与计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不同难度的边界时采用统一计算，忽略了边界定位难度的显著差异，导致效率和精度受限。

Method: BDR通过符号距离回归替代分类实现信息论最优的定位；ATR通过连续深度选择τ∈[0,1]实现端到端可微的自适应计算分配，并结合知识蒸馏降低训练成本。

Result: BDR使边界峰值锐化43%，在多个架构上带来1.8-3.1%的mAP@0.7提升；ATR在THUMOS14上以18%更少计算量获得2.9%性能提升，短动作上达4.2%提升，轻量学生模型保留99%性能。

Conclusion: BDR和ATR有效提升了时序动作定位的精度与效率，且可广泛适用于不同架构，在多个基准上通过严格统计检验验证了其优势。

Abstract: Temporal action localization requires precise boundary detection; however,
current methods apply uniform computation despite significant variations in
difficulty across boundaries. We present two complementary contributions.
First, Boundary Distance Regression (BDR) provides information-theoretically
optimal localization through signed-distance regression rather than
classification, achieving 43\% sharper boundary peaks. BDR retrofits to
existing methods with approximately 50 lines of code, yielding consistent 1.8
to 3.1\% mAP@0.7 improvements across diverse architectures. Second, Adaptive
Temporal Refinement (ATR) allocates computation via continuous depth selection
$\tau \in [0,1]$, enabling end-to-end differentiable optimization without
reinforcement learning. On THUMOS14, ATR achieves 56.5\% mAP@0.7 at 162G FLOPs,
compared to 53.6\% at 198G for uniform processing, providing a 2.9\%
improvement with 18\% less compute. Gains scale with boundary heterogeneity,
showing 4.2\% improvement on short actions. Training cost is mitigated via
knowledge distillation, with lightweight students retaining 99\% performance at
baseline cost. Results are validated across four benchmarks with rigorous
statistical testing.

</details>


### [10] [Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization](https://arxiv.org/abs/2511.03950)
*Zhejia Cai,Puhua Jiang,Shiwei Mao,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种统一优化几何和外观的高斯-网格联合优化框架，通过高斯引导的可微渲染实现高质量3D重建，支持重光照和形状编辑等下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将几何精度与外观渲染分离，导致在编辑任务中表现不佳，本文旨在统一优化几何与外观以提升重建质量及可编辑性。

Method: 提出一种新颖的框架，利用输入图像的光度一致性以及法线和深度图的几何正则化，通过高斯引导的网格可微分渲染同时优化网格顶点位置、面片结构和顶点颜色。

Result: 实现了高质量的3D重建，在几何准确性和渲染真实感之间取得良好平衡，并验证了其在重光照和形状变形等编辑任务中的有效性。

Conclusion: 该方法实现了几何与外观的联合优化，为多视角图像的3D重建提供了更适用于下游编辑任务的解决方案。

Abstract: Reconstructing real-world objects from multi-view images is essential for
applications in 3D editing, AR/VR, and digital content creation. Existing
methods typically prioritize either geometric accuracy (Multi-View Stereo) or
photorealistic rendering (Novel View Synthesis), often decoupling geometry and
appearance optimization, which hinders downstream editing tasks. This paper
advocates an unified treatment on geometry and appearance optimization for
seamless Gaussian-mesh joint optimization. More specifically, we propose a
novel framework that simultaneously optimizes mesh geometry (vertex positions
and faces) and vertex colors via Gaussian-guided mesh differentiable rendering,
leveraging photometric consistency from input images and geometric
regularization from normal and depth maps. The obtained high-quality 3D
reconstruction can be further exploit in down-stream editing tasks, such as
relighting and shape deformation. The code will be publicly available upon
acceptance.

</details>


### [11] [A Linear Fractional Transformation Model and Calibration Method for Light Field Camera](https://arxiv.org/abs/2511.03962)
*Zhong Chen,Changfeng Chen*

Main category: cs.CV

TL;DR: 提出一种基于线性分数变换（LFT）参数α的光场相机内参标定方法，解耦主镜头与微透镜阵列，结合解析解与非线性优化，提升了3D重建精度与仿真效率。


<details>
  <summary>Details</summary>
Motivation: 准确标定光场相机的内参对3D重建至关重要，但传统方法难以有效解耦主镜头与微透镜阵列的影响，导致标定精度不足。

Method: 引入LFT参数α来解耦主镜头与微透镜阵列；采用基于最小二乘的解析解，并进行非线性优化 refinement；提出了从原始图像中检测特征的方法。

Result: 在真实和模拟数据上的实验验证了该方法的有效性，且基于该模型的光场图像仿真速度更快，有利于深度学习方法的数据生成。

Conclusion: 所提方法能有效提升光场相机内参标定的精度与效率，支持更快速的仿真，为数据驱动的深度学习应用提供了良好基础。

Abstract: Accurate calibration of internal parameters is a crucial yet challenging
prerequisite for 3D reconstruction using light field cameras. In this paper, we
propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled
the main lens and micro lens array (MLA). The proposed method includes an
analytical solution based on least squares, followed by nonlinear refinement.
The method for detecting features from the raw images is also introduced.
Experimental results on both physical and simulated data have verified the
performance of proposed method. Based on proposed model, the simulation of raw
light field images becomes faster, which is crucial for data-driven deep
learning methods. The corresponding code can be obtained from the author's
website.

</details>


### [12] [Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images](https://arxiv.org/abs/2511.03970)
*Sam Bahrami,Dylan Campbell*

Main category: cs.CV

TL;DR: 本文提出了一种合成数据集“Room Envelopes”，用于促进对场景中不可见结构元素（如墙壁、地板和天花板）的3D重建研究，通过提供RGB图像及对应的可见表面与结构布局点图，实现对单目几何估计模型的直接监督。


<details>
  <summary>Details</summary>
Motivation: 现有场景重建方法只能恢复可见表面，无法处理被遮挡的结构部分；而场景的结构性元素（如墙、地板、天花板）通常具有平面性、重复性和简单性，因此更易于预测，值得专门研究。

Method: 构建一个名为Room Envelopes的合成数据集，包含RGB图像以及两个对应的点图：一个表示可见表面，另一个表示去除装饰物后的结构布局表面，用于对前馈单目几何估计器进行直接监督训练。

Result: 该数据集能够支持模型同时预测可见表面和结构布局表面，从而帮助理解场景的整体范围以及物体的形状与位置。

Conclusion: 通过引入Room Envelopes数据集，为单目图像下完整场景结构预测提供了有效训练与评估手段，推动了对室内场景整体几何理解的研究。

Abstract: Modern scene reconstruction methods are able to accurately recover 3D
surfaces that are visible in one or more images. However, this leads to
incomplete reconstructions, missing all occluded surfaces. While much progress
has been made on reconstructing entire objects given partial observations using
generative models, the structural elements of a scene, like the walls, floors
and ceilings, have received less attention. We argue that these scene elements
should be relatively easy to predict, since they are typically planar,
repetitive and simple, and so less costly approaches may be suitable. In this
work, we present a synthetic dataset -- Room Envelopes -- that facilitates
progress on this task by providing a set of RGB images and two associated
pointmaps for each image: one capturing the visible surface and one capturing
the first surface once fittings and fixtures are removed, that is, the
structural layout. As we show, this enables direct supervision for feed-forward
monocular geometry estimators that predict both the first visible surface and
the first layout surface. This confers an understanding of the scene's extent,
as well as the shape and location of its objects.

</details>


### [13] [Simple 3D Pose Features Support Human and Machine Social Scene Understanding](https://arxiv.org/abs/2511.03988)
*Wenshuo Qin,Leyla Isik*

Main category: cs.CV

TL;DR: 该研究发现人类通过3D姿态信息进行社交互动判断，而大多数AI视觉模型缺乏此类信息；基于3D面部位置和方向的简洁特征可显著提升AI模型对人类社交判断的预测能力。


<details>
  <summary>Details</summary>
Motivation: 理解人类如何从视觉输入中快速识别社交互动，并揭示当前AI视觉系统在该任务上表现不佳的原因。

Method: 结合先进的姿态与深度估计算法，提取视频中人物的3D关节位置，构建简化的3D社交姿态特征（仅包含面部位置和朝向），并与现有AI视觉模型进行对比分析。

Result: 3D关节位置信息优于多数现有AI模型的表现，且仅基于面部3D信息的简化特征即可达到相近甚至更强的预测能力；将该特征融入现有AI模型可显著提升其性能，且模型中3D社交姿态特征的表征程度与其匹配人类判断的能力呈正相关。

Conclusion: 人类对社交场景的理解依赖于显式的3D姿态表征，这种能力可通过简单的结构化视觉线索实现，为改进AI社交理解提供了新方向。

Abstract: Humans can quickly and effortlessly extract a variety of information about
others' social interactions from visual input, ranging from visuospatial cues
like whether two people are facing each other to higher-level information. Yet,
the computations supporting these abilities remain poorly understood, and
social interaction recognition continues to challenge even the most advanced AI
vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose
information to make social interaction judgments, which is absent in most AI
vision models. To test this, we combined state-of-the-art pose and depth
estimation algorithms to extract 3D joint positions of people in short video
clips depicting everyday human actions and compared their ability to predict
human social interaction judgments with current AI vision models. Strikingly,
3D joint positions outperformed most current AI vision models, revealing that
key social information is available in explicit body position but not in the
learned features of most vision models, including even the layer-wise
embeddings of the pose models used to extract joint positions. To uncover the
critical pose features humans use to make social judgments, we derived a
compact set of 3D social pose features describing only the 3D position and
direction of faces in the videos. We found that these minimal descriptors
matched the predictive strength of the full set of 3D joints and significantly
improved the performance of off-the-shelf AI vision models when combined with
their embeddings. Moreover, the degree to which 3D social pose features were
represented in each off-the-shelf AI vision model predicted the model's ability
to match human social judgments. Together, our findings provide strong evidence
that human social scene understanding relies on explicit representations of 3D
pose and can be supported by simple, structured visuospatial primitives.

</details>


### [14] [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](https://arxiv.org/abs/2511.03992)
*Yuwen Tao,Kanglei Zhou,Xin Tan,Yuan Xie*

Main category: cs.CV

TL;DR: 本文提出了一种名为Camera Aware Referring Field (CaRF)的全微分框架，用于在3D高斯空间中实现语言引导的3D区域分割，并通过引入相机几何信息和多视角监督提升跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖2D渲染伪监督和视图特定特征学习，导致在跨视角一致性方面表现不佳，难以准确实现自由形式语言表达与3D高斯场中对应区域的对齐。

Method: 提出CaRF框架，包含高斯场相机编码（GFCE），将相机几何信息融入高斯点与文本的交互中，以建模视角相关变化；并设计训练时配对视图监督（ITPVS），在标定的不同视图间对齐每个高斯点的分类logits，增强多视角一致性。

Result: 在Ref LERF、LERF OVS和3D OVS三个基准上，CaRF相比现有方法mIoU分别平均提升了16.8%、4.3%和2.0%，显著提高了跨视角一致性和分割性能。

Conclusion: CaRF实现了更可靠且具多视角一致性的3D场景理解，为具身AI、AR/VR交互和自主感知等应用提供了有力支持。

Abstract: Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret
free-form language expressions and localize the corresponding 3D regions in
Gaussian fields. While recent advances have introduced cross-modal alignment
between language and 3D geometry, existing pipelines still struggle with
cross-view consistency due to their reliance on 2D rendered pseudo supervision
and view specific feature learning. In this work, we present Camera Aware
Referring Field (CaRF), a fully differentiable framework that operates directly
in the 3D Gaussian space and achieves multi view consistency. Specifically,
CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates
camera geometry into Gaussian text interactions to explicitly model view
dependent variations and enhance geometric reasoning. Building on this, In
Training Paired View Supervision (ITPVS) is proposed to align per Gaussian
logits across calibrated views during training, effectively mitigating single
view overfitting and exposing inter view discrepancies for optimization.
Extensive experiments on three representative benchmarks demonstrate that CaRF
achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of
the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.
Moreover, this work promotes more reliable and view consistent 3D scene
understanding, with potential benefits for embodied AI, AR/VR interaction, and
autonomous perception.

</details>


### [15] [PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection](https://arxiv.org/abs/2511.03997)
*Peiyao Wang,Weining Wang,Qi Li*

Main category: cs.CV

TL;DR: 本文提出了PhysCorr，一个统一的框架，用于建模、评估和优化视频生成中的物理一致性，通过引入PhysicsRM奖励模型和PhyDPO优化方法，显著提升了生成视频的物理真实感。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成模型在感知质量上表现良好，但生成内容常违反物理合理性，限制了其在具身AI、机器人和仿真等领域的应用。

Method: 提出PhysicsRM——首个双维度奖励模型，量化对象内部稳定性和对象间交互；基于此构建PhyDPO，利用对比反馈和物理感知重加权进行直接偏好优化，并支持多种视频生成模型。

Result: 在多个基准上的实验表明，PhysCorr显著提高了物理真实性，同时保持了视觉保真度和语义一致性。

Conclusion: PhysCorr为实现物理上合理且可信的视频生成迈出了关键一步。

Abstract: Recent advances in text-to-video generation have achieved impressive
perceptual quality, yet generated content often violates fundamental principles
of physical plausibility - manifesting as implausible object dynamics,
incoherent interactions, and unrealistic motion patterns. Such failures hinder
the deployment of video generation models in embodied AI, robotics, and
simulation-intensive domains. To bridge this gap, we propose PhysCorr, a
unified framework for modeling, evaluating, and optimizing physical consistency
in video generation. Specifically, we introduce PhysicsRM, the first
dual-dimensional reward model that quantifies both intra-object stability and
inter-object interactions. On this foundation, we develop PhyDPO, a novel
direct preference optimization pipeline that leverages contrastive feedback and
physics-aware reweighting to guide generation toward physically coherent
outputs. Our approach is model-agnostic and scalable, enabling seamless
integration into a wide range of video diffusion and transformer-based
backbones. Extensive experiments across multiple benchmarks demonstrate that
PhysCorr achieves significant improvements in physical realism while preserving
visual fidelity and semantic alignment. This work takes a critical step toward
physically grounded and trustworthy video generation.

</details>


### [16] [GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization](https://arxiv.org/abs/2511.04008)
*Mahmoud Soliman,Omar Abdelaziz,Ahmed Radwan,Anand,Mohamed Shehata*

Main category: cs.CV

TL;DR: 提出GNN-MoE方法，结合图神经网络路由与Mixture-of-Experts框架，通过Kronecker适配器实现高效参数微调，提升Vision Transformer在未见域上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 标准微调成本高且损害泛化能力，现有参数高效微调方法在域泛化中仍缺乏对域间结构变化的适应性，需更高效、上下文感知的适应机制。

Method: 构建基于图神经网络（GCN/GAT/SAGE）的路由器，在图像块间构建图结构，动态将图像块分配给专用专家；采用Kronecker适配器实现参数高效的Mixture-of-Experts框架，利用块间关系进行上下文感知的路由。

Result: 在多个域泛化基准上达到最先进或具有竞争力的性能，同时保持高参数效率，显著优于传统微调和其他PEFT方法。

Conclusion: GNN-MoE通过图结构引导的上下文感知路由，有效提升了ViT在未见域上的鲁棒性和适应性，验证了图基路由在轻量级域泛化中的有效性。

Abstract: Domain generalization (DG) seeks robust Vision Transformer (ViT) performance
on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;
standard fine-tuning is costly and can impair generalization. We propose
GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a
Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead
of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,
SAGE) operates on inter-patch graphs to dynamically assign patches to
specialized experts. This context-aware GNN routing leverages inter-patch
relationships for better adaptation to domain shifts. GNN-MoE achieves
state-of-the-art or competitive DG benchmark performance with high parameter
efficiency, highlighting the utility of graph-based contextual routing for
robust, lightweight DG.

</details>


### [17] [MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging](https://arxiv.org/abs/2511.04016)
*Mahmoud Soliman,Islam Osman,Mohamed S. Shehata,Rasika Rajapakshe*

Main category: cs.CV

TL;DR: MedDChest是一个专为胸部影像设计的基于Vision Transformer的基础模型，通过在大规模、多模态医学图像数据上从零训练，并结合提出的内容感知数据增强方法Guided Random Resized Crops，显著优于ImageNet预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型在医学影像中表现受限，主要因为依赖于自然图像预训练的骨干网络，存在领域差异问题。因此需要专门针对医学影像域进行预训练以提升性能。

Method: 提出MedDChest，一种专用于胸部影像的ViT基础模型；在超过120万张来自10个公开来源的X光和CT图像上从头预训练；引入Guided Random Resized Crops，一种关注解剖相关区域的内容感知数据增强策略。

Result: 在多种下游诊断任务上微调后，MedDChest显著优于现有的ImageNet预训练模型，验证了大规模域内预训练与领域特定数据增强的有效性。

Conclusion: MedDChest作为更强健的特征提取器，为各类胸部诊断任务提供了更优的起点，证明了域内预训练的重要性，模型权重将公开以促进后续研究。

Abstract: The performance of vision models in medical imaging is often hindered by the
prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain
natural images. To address this fundamental domain gap, we propose MedDChest, a
new foundational Vision Transformer (ViT) model optimized specifically for
thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,
multimodal dataset of over 1.2 million images, encompassing different
modalities including Chest X-ray and Computed Tomography (CT) compiled from 10
public sources. A core technical contribution of our work is Guided Random
Resized Crops, a novel content-aware data augmentation strategy that biases
sampling towards anatomically relevant regions, overcoming the inefficiency of
standard cropping techniques on medical scans. We validate our model's
effectiveness by fine-tuning it on a diverse set of downstream diagnostic
tasks. Comprehensive experiments empirically demonstrate that MedDChest
significantly outperforms strong, publicly available ImageNet-pretrained
models. By establishing the superiority of large-scale, in-domain pre-training
combined with domain-specific data augmentation, MedDChest provides a powerful
and robust feature extractor that serves as a significantly better starting
point for a wide array of thoracic diagnostic tasks. The model weights will be
made publicly available to foster future research and applications.

</details>


### [18] [A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals](https://arxiv.org/abs/2511.04037)
*Arfina Rahman,Mahesh Banavar*

Main category: cs.CV

TL;DR: 本文提出了一种基于低帧率指尖视频提取PPG信号的轻量级生物特征认证框架，结合连续小波变换与混合深度学习模型CVT-ConvMixer-LSTM，在CFIHSR数据集上实现了98%的认证准确率，具有良好的抗噪性和跨被试鲁棒性，适用于移动和嵌入式安全应用。


<details>
  <summary>Details</summary>
Motivation: PPG信号虽在生物认证中具潜力，但易受运动伪影、光照变化和个体间生理差异影响，需提升特征提取与分类的鲁棒性。

Method: 采用CFIHSR数据集（46名被试，14Hz采样率），对PPG信号进行去基线漂移、PCA去运动伪影、带通滤波、傅里叶重采样和归一化；通过连续小波变换（CWT）将一维PPG信号转为二维时频图，并设计CVT-ConvMixer-LSTM混合模型融合空间与时间特征进行认证。

Result: 在46名被试上的实验显示认证准确率达98%，模型对噪声和个体差异具有强鲁棒性。

Conclusion: 所提轻量级PPG认证系统高效、可扩展且具备活体检测能力，适合实际移动与嵌入式应用场景。

Abstract: Photoplethysmography (PPG) signals, which measure changes in blood volume in
the skin using light, have recently gained attention in biometric
authentication because of their non-invasive acquisition, inherent liveness
detection, and suitability for low-cost wearable devices. However, PPG signal
quality is challenged by motion artifacts, illumination changes, and
inter-subject physiological variability, making robust feature extraction and
classification crucial. This study proposes a lightweight and cost-effective
biometric authentication framework based on PPG signals extracted from
low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings
from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The
raw PPG signals undergo a standard preprocessing pipeline involving baseline
drift removal, motion artifact suppression using Principal Component Analysis
(PCA), bandpass filtering, Fourier-based resampling, and amplitude
normalization. To generate robust representations, each one-dimensional PPG
segment is converted into a two-dimensional time-frequency scalogram via the
Continuous Wavelet Transform (CWT), effectively capturing transient
cardiovascular dynamics. We developed a hybrid deep learning model, termed
CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision
Transformer (CVT) and ConvMixer branches with temporal features from a Long
Short-Term Memory network (LSTM). The experimental results on 46 subjects
demonstrate an authentication accuracy of 98%, validating the robustness of the
model to noise and variability between subjects. Due to its efficiency,
scalability, and inherent liveness detection capability, the proposed system is
well-suited for real-world mobile and embedded biometric security applications.

</details>


### [19] [Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment](https://arxiv.org/abs/2511.04078)
*Zehui Feng,Chenqi Zhang,Mingru Wang,Minuo Wei,Shiwei Cheng,Cuntai Guan,Ting Han*

Main category: cs.CV

TL;DR: 提出Bratrix，首个实现语言锚定的视觉-脑对齐的端到端框架，通过解耦视觉刺激的层次化语义成分并引入不确定性感知模块，在EEG、MEG和fMRI任务中显著提升检索、重建和描述性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接对齐神经活动与视觉嵌入，但缺乏对潜在语义维度的捕捉，导致可解释性和鲁棒性受限。

Method: Bratrix将视觉刺激解耦为视觉和语言语义成分，映射到共享隐空间，并引入不确定性感知模块进行加权对齐；采用语言锚定语义矩阵增强跨模态关联，结合单模态预训练与多模态微调的两阶段训练策略。

Result: 在EEG、MEG和fMRI基准上，Bratrix在检索、重建和图像描述任务中优于现有方法，200类EEG检索任务性能提升14.3%。

Conclusion: Bratrix通过语言锚定的多模态对齐和不确定性建模，有效提升了从神经信号中解码视觉语义的精度与鲁棒性。

Abstract: Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI
remains a fundamental challenge due to subject variability and the entangled
nature of visual features. Existing approaches primarily align neural activity
directly with visual embeddings, but visual-only representations often fail to
capture latent semantic dimensions, limiting interpretability and deep
robustness. To address these limitations, we propose Bratrix, the first
end-to-end framework to achieve multimodal Language-Anchored Vision-Brain
alignment. Bratrix decouples visual stimuli into hierarchical visual and
linguistic semantic components, and projects both visual and brain
representations into a shared latent space, enabling the formation of aligned
visual-language and brain-language embeddings. To emulate human-like perceptual
reliability and handle noisy neural signals, Bratrix incorporates a novel
uncertainty perception module that applies uncertainty-aware weighting during
alignment. By leveraging learnable language-anchored semantic matrices to
enhance cross-modal correlations and employing a two-stage training strategy of
single-modality pretraining followed by multimodal fine-tuning, Bratrix-M
improves alignment precision. Extensive experiments on EEG, MEG, and fMRI
benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and
captioning performance compared to state-of-the-art methods, specifically
surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.

</details>


### [20] [Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score](https://arxiv.org/abs/2511.04083)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

TL;DR: 本文研究了在无配对和自监督条件下CT图像去噪的两种高效训练方法：基于CycleGAN的残差翻译器和Noise2Score（N2S）得分匹配去噪器。实验结果表明，CycleGAN在最终图像质量上表现最佳，而Noise2Score在缺乏干净配对数据时具有较强的实用性。


<details>
  <summary>Details</summary>
Motivation: 在缺乏配对训练数据的情况下，如何有效提升CT图像去噪性能是一个关键挑战。本文旨在评估两种无需配对数据的先进去噪方法，以推动实际临床应用中的图像质量改善。

Method: 采用CycleGAN-based残差翻译器和Noise2Score得分匹配模型，在统一评估协议下进行对比实验，并通过参数扫描选择最优配置（如U-Net骨干网络、损失权重等），随后进行充分训练。

Result: 选定的CycleGAN模型将输入图像从34.66 dB / 0.9234 SSIM提升至38.913 dB / 0.971 SSIM，在Kaggle未见数据集上取得1.9343分；Noise2Score虽在PSNR/SSIM略低，但在极噪声输入下表现出显著提升。

Conclusion: CycleGAN提供最优图像质量，适合高质量去噪需求；Noise2Score则作为无需配对数据的稳健替代方案，具备实际应用价值。

Abstract: We study CT image denoising in the unpaired and self-supervised regimes by
evaluating two strong, training-data-efficient paradigms: a CycleGAN-based
residual translator and a Noise2Score (N2S) score-matching denoiser. Under a
common evaluation protocol, a configuration sweep identifies a simple standard
U-Net backbone within CycleGAN (lambda_cycle = 30, lambda_iden = 2, ngf = ndf =
64) as the most reliable setting; we then train it to convergence with a longer
schedule. The selected CycleGAN improves the noisy input from 34.66 dB / 0.9234
SSIM to 38.913 dB / 0.971 SSIM and attains an estimated score of 1.9441 and an
unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly
behind in absolute PSNR / SSIM, achieves large gains over very noisy inputs,
highlighting its utility when clean pairs are unavailable. Overall, CycleGAN
offers the strongest final image quality, whereas Noise2Score provides a robust
pair-free alternative with competitive performance. Source code is available at
https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score.

</details>


### [21] [When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.04084)
*Nishchal Sapkota,Haoyan Shi,Yejia Zhang,Xianshi Ma,Bofang Zheng,Danny Z. Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的医学图像分割架构UKAST，结合了Kolmogorov-Arnold Networks（KANs）与Swin Transformer，在减少计算量的同时实现了更高的数据效率和准确性，尤其在标注数据稀缺的情况下表现突出。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临复杂解剖结构和标注数据有限的挑战，传统CNN难以捕捉长距离依赖，而Transformer则数据需求大、计算成本高。因此需要一种更高效、更适应小数据场景的模型。

Method: 提出UKAST，一种类似U-Net的架构，将基于有理函数的KANs（特别是GR-KANs）引入Swin Transformer编码器中，替代原始样条KAN，提升表达能力和计算效率。

Result: 在四个2D和3D医学图像分割基准上达到SOTA性能，显著优于CNN和Transformer基线方法，尤其在数据稀缺场景下表现更优，且FLOPs降低，参数增长极小。

Conclusion: KAN增强的Transformer能够有效提升医学图像分割的数据效率和精度，为低数据环境下的模型设计提供了新方向。

Abstract: Medical image segmentation is critical for accurate diagnostics and treatment
planning, but remains challenging due to complex anatomical structures and
limited annotated training data. CNN-based segmentation methods excel at local
feature extraction, but struggle with modeling long-range dependencies.
Transformers, on the other hand, capture global context more effectively, but
are inherently data-hungry and computationally expensive. In this work, we
introduce UKAST, a U-Net like architecture that integrates rational-function
based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By
leveraging rational base functions and Group Rational KANs (GR-KANs) from the
Kolmogorov-Arnold Transformer (KAT), our architecture addresses the
inefficiencies of vanilla spline-based KANs, yielding a more expressive and
data-efficient framework with reduced FLOPs and only a very small increase in
parameter count compared to SwinUNETR. UKAST achieves state-of-the-art
performance on four diverse 2D and 3D medical image segmentation benchmarks,
consistently surpassing both CNN- and Transformer-based baselines. Notably, it
attains superior accuracy in data-scarce settings, alleviating the data-hungry
limitations of standard Vision Transformers. These results show the potential
of KAN-enhanced Transformers to advance data-efficient medical image
segmentation. Code is available at: https://github.com/nsapkota417/UKAST

</details>


### [22] [SpatialLock: Precise Spatial Control in Text-to-Image Synthesis](https://arxiv.org/abs/2511.04112)
*Biao Liu,Yuanzhi Liang*

Main category: cs.CV

TL;DR: 提出SpatialLock框架，通过结合感知信号和定位信息来精确控制文本到图像生成中的对象位置，实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用位置信息，导致对对象空间布局的理解不足，难以实现精确的对象定位控制。

Method: 提出SpatialLock，包含位置感知注入（PoI）和位置引导学习（PoG）两个组件：PoI通过注意力层直接融合空间信息，PoG利用基于感知的监督进一步优化定位。

Result: 在多个数据集上实现了超过0.9的IOU分数，显著提升了对象定位精度和生成图像的视觉质量。

Conclusion: SpatialLock有效解决了文本到图像生成中对象定位不准确的问题，为精确空间控制提供了新思路。

Abstract: Text-to-Image (T2I) synthesis has made significant advancements in recent
years, driving applications such as generating datasets automatically. However,
precise control over object localization in generated images remains a
challenge. Existing methods fail to fully utilize positional information,
leading to an inadequate understanding of object spatial layouts. To address
this issue, we propose SpatialLock, a novel framework that leverages perception
signals and grounding information to jointly control the generation of spatial
locations. SpatialLock incorporates two components: Position-Engaged Injection
(PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial
information through an attention layer, encouraging the model to learn the
grounding information effectively. PoG employs perception-based supervision to
further refine object localization. Together, these components enable the model
to generate objects with precise spatial arrangements and improve the visual
quality of the generated images. Experiments show that SpatialLock sets a new
state-of-the-art for precise object positioning, achieving IOU scores above 0.9
across multiple datasets.

</details>


### [23] [BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems](https://arxiv.org/abs/2511.04388)
*Chang Liu,Juan Li,Sheng Zhang,Chang Liu,Jie Li,Xu Zhang*

Main category: cs.CV

TL;DR: 提出了一种参数量仅为8.7M的单目深度估计模型BoRe-Depth，通过增强特征自适应融合模块和引入语义信息，在嵌入式设备上实现了高效的深度估计和清晰的物体边界。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计方法在嵌入式系统上存在性能差和物体边界模糊的问题，亟需轻量且高精度的解决方案。

Method: 设计了增强特征自适应融合模块（EFAF）以提升边界细节表征，并在编码器中融入语义知识以增强物体识别与边界感知能力。

Result: 模型在NVIDIA Jetson Orin上达到50.7 FPS，显著优于以往轻量级模型，在多个数据集上表现出色，且边界质量明显改善。

Conclusion: BoRe-Depth在保持极低参数量的同时，实现了高效、精确的深度估计，特别适用于无人系统的嵌入式部署。

Abstract: Depth estimation is one of the key technologies for realizing 3D perception
in unmanned systems. Monocular depth estimation has been widely researched
because of its low-cost advantage, but the existing methods face the challenges
of poor depth estimation performance and blurred object boundaries on embedded
systems. In this paper, we propose a novel monocular depth estimation model,
BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate
depth maps on embedded systems and significantly improves boundary quality.
Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which
adaptively fuses depth features to enhance boundary detail representation.
Secondly, we integrate semantic knowledge into the encoder to improve the
object recognition and boundary perception capabilities. Finally, BoRe-Depth is
deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We
demonstrate that the proposed model significantly outperforms previous
lightweight models on multiple challenging datasets, and we provide detailed
ablation studies for the proposed methods. The code is available at
https://github.com/liangxiansheng093/BoRe-Depth.

</details>


### [24] [Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration](https://arxiv.org/abs/2511.04117)
*Yunghee Lee,Byeonghyun Pak,Junwha Hong,Hoseong Kim*

Main category: cs.CV

TL;DR: 提出了一种无需训练的加速扩散采样方法Tortoise and Hare Guidance (THG)，通过多速率ODE系统在保持生成质量的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成高质量图像时采样速度慢，且传统求解器未能充分利用不同分支对数值误差的敏感性差异。

Method: 将分类器无关引导（CFG）ODE重构为多速率系统，分别在细粒度和粗粒度时间步上积分噪声估计和附加引导项，并引入误差感知的时间步采样器和引导尺度调度器。

Result: 相比现有方法，在几乎不损失生成质量（ΔImageReward ≤ 0.032）的情况下，函数求值次数（NFE）最多减少30%。

Conclusion: 多速率公式在扩散求解器中具有巨大潜力，可在无需模型重训练的情况下实现高效高质量图像生成。

Abstract: In this paper, we propose Tortoise and Hare Guidance (THG), a training-free
strategy that accelerates diffusion sampling while maintaining high-fidelity
generation. We demonstrate that the noise estimate and the additional guidance
term exhibit markedly different sensitivity to numerical error by reformulating
the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our
error-bound analysis shows that the additional guidance branch is more robust
to approximation, revealing substantial redundancy that conventional solvers
fail to exploit. Building on this insight, THG significantly reduces the
computation of the additional guidance: the noise estimate is integrated with
the tortoise equation on the original, fine-grained timestep grid, while the
additional guidance is integrated with the hare equation only on a coarse grid.
We also introduce (i) an error-bound-aware timestep sampler that adaptively
selects step sizes and (ii) a guidance-scale scheduler that stabilizes large
extrapolation spans. THG reduces the number of function evaluations (NFE) by up
to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward
$\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free
accelerators under identical computation budgets. Our findings highlight the
potential of multirate formulations for diffusion solvers, paving the way for
real-time high-quality image synthesis without any model retraining. The source
code is available at https://github.com/yhlee-add/THG.

</details>


### [25] [Text to Sketch Generation with Multi-Styles](https://arxiv.org/abs/2511.04123)
*Tengjie Li,Shikui Tu,Lei Xu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的扩散模型框架M3S，通过文本提示和参考风格草图实现对草图生成风格的显式控制，有效减少内容泄露并提升生成质量，支持多风格可控生成。


<details>
  <summary>Details</summary>
Motivation: 现有草图生成方法缺乏对风格的精确控制，且在风格迁移过程中容易发生内容泄露，难以处理结构差异大的参考与目标草图。

Method: 基于扩散模型，引入参考特征作为辅助信息，采用线性平滑和风格-内容引导机制，并通过联合AdaIN模块整合多个参考草图特征以实现多风格生成。

Result: 实验表明该方法在风格对齐准确性、生成质量和风格控制灵活性方面优于现有方法，尤其在低结构相似性场景下表现突出。

Conclusion: 所提M3S框架实现了高质量、可控的单/多风格草图生成，无需训练且有效缓解了风格迁移中的内容泄露问题。

Abstract: Recent advances in vision-language models have facilitated progress in sketch
generation. However, existing specialized methods primarily focus on generic
synthesis and lack mechanisms for precise control over sketch styles. In this
work, we propose a training-free framework based on diffusion models that
enables explicit style guidance via textual prompts and referenced style
sketches. Unlike previous style transfer methods that overwrite key and value
matrices in self-attention, we incorporate the reference features as auxiliary
information with linear smoothing and leverage a style-content guidance
mechanism. This design effectively reduces content leakage from reference
sketches and enhances synthesis quality, especially in cases with low
structural similarity between reference and target sketches. Furthermore, we
extend our framework to support controllable multi-style generation by
integrating features from multiple reference sketches, coordinated via a joint
AdaIN module. Extensive experiments demonstrate that our approach achieves
high-quality sketch generation with accurate style alignment and improved
flexibility in style control. The official implementation of M3S is available
at https://github.com/CMACH508/M3S.

</details>


### [26] [Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)](https://arxiv.org/abs/2511.04126)
*Venkata Manikanta Desu,Syed Fawaz Ali*

Main category: cs.CV

TL;DR: 本研究提出了一套基于深度学习的自动化网球比赛分析管道，整合了多个模型实现球员与球的检测跟踪及球场关键点识别，并生成包含详细性能指标的标注视频。


<details>
  <summary>Details</summary>
Motivation: 为了提供实时、准确且全面的网球比赛分析，支持教练、转播方和运动员进行技战术评估和决策。

Method: 采用YOLOv8进行球员检测，自定义训练的YOLOv5模型用于网球跟踪，结合ResNet50架构实现球场关键点检测，构建完整分析流程。

Result: 系统在不同场地条件和比赛场景下均表现出良好的鲁棒性，能够输出球员移动模式、球速、击球准确率和反应时间等详细数据。

Conclusion: 该框架能有效提升网球比赛分析的自动化水平，具备实际应用价值，适用于训练优化与赛事转播。

Abstract: This study presents a complete pipeline for automated tennis match analysis.
Our framework integrates multiple deep learning models to detect and track
players and the tennis ball in real time, while also identifying court
keypoints for spatial reference. Using YOLOv8 for player detection, a
custom-trained YOLOv5 model for ball tracking, and a ResNet50-based
architecture for court keypoint detection, our system provides detailed
analytics including player movement patterns, ball speed, shot accuracy, and
player reaction times. The experimental results demonstrate robust performance
in varying court conditions and match scenarios. The model outputs an annotated
video along with detailed performance metrics, enabling coaches, broadcasters,
and players to gain actionable insights into the dynamics of the game.

</details>


### [27] [DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms](https://arxiv.org/abs/2511.04128)
*Shengyu Tang,Zeyuan Lu,Jiazhi Dong,Changdong Yu,Xiaoyu Wang,Yaohui Lyu,Weihao Xia*

Main category: cs.CV

TL;DR: 提出一种高效的双分支海上多目标跟踪方法DMSORT，结合检测重识别与相机运动估计分支，在复杂海况下实现鲁棒的船舶跟踪，具有高精度、强抗干扰性和实时性。


<details>
  <summary>Details</summary>
Motivation: 复杂的海上环境导致相机运动和视觉退化，严重影响多目标跟踪性能，现有方法难以兼顾实时性与身份一致性。

Method: 设计双分支并行跟踪框架：一枝采用可逆列状检测网络（RCDN）和轻量Transformer外观提取器（Li-TAE）进行检测与重识别；另一枝通过投影变换解耦平台运动，在卡尔曼滤波中进行运动补偿；最后通过聚类优化的特征融合模块融合运动与外观线索。

Result: 在新加坡海事数据集上达到最先进性能，是目前最快的基于ReID的MOT方法，具备优异的身份一致性、抗抖动和抗遮挡能力。

Conclusion: DMSORT有效解决了海上动态相机条件下的多目标跟踪难题，在保持高精度的同时显著提升运行速度，适用于实际海上导航与监控应用。

Abstract: Accurate perception of the marine environment through robust multi-object
tracking (MOT) is essential for ensuring safe vessel navigation and effective
maritime surveillance. However, the complicated maritime environment often
causes camera motion and subsequent visual degradation, posing significant
challenges to MOT. To address this challenge, we propose an efficient
Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the
framework is a parallel tracker with affine compensation, which incorporates an
object detection and re-identification (ReID) branch, along with a dedicated
branch for dynamic camera motion estimation. Specifically, a Reversible
Columnar Detection Network (RCDN) is integrated into the detection module to
leverage multi-level visual features for robust object detection. Furthermore,
a lightweight Transformer-based appearance extractor (Li-TAE) is designed to
capture global contextual information and generate robust appearance features.
Another branch decouples platform-induced and target-intrinsic motion by
constructing a projective transformation, applying platform-motion compensation
within the Kalman filter, and thereby stabilizing true object trajectories.
Finally, a clustering-optimized feature fusion module effectively combines
motion and appearance cues to ensure identity consistency under noise,
occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset
demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT
attains the fastest runtime among existing ReID-based MOT frameworks while
maintaining high identity consistency and robustness to jitter and occlusion.
Code is available at:
https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.

</details>


### [28] [Learning from Online Videos at Inference Time for Computer-Use Agents](https://arxiv.org/abs/2511.04137)
*Yujian Liu,Ze Wang,Hao Chen,Ximeng Sun,Xiaodong Yu,Jialian Wu,Jiang Liu,Emad Barsoum,Zicheng Liu,Shiyu Chang*

Main category: cs.CV

TL;DR: 本文提出了一种使计算机使用代理在推理时从在线视频中学习的框架，通过检索、过滤教程视频并将其转化为结构化示范轨迹，动态选择最相关的指导信息，从而提升代理在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用代理在需要特定领域程序性知识的任务上仍落后于人类用户，而人类可以通过观看视频教程来快速掌握操作技能。因此，研究如何让代理像人一样从在线视频中实时学习具有重要意义。

Method: 提出一个包含视频检索与过滤、将视频转换为结构化动作轨迹、并利用视觉语言模型（VLM）推断UI动作、分割视频为子序列并标注文本目标的框架；在推理时采用两阶段选择机制，动态选取最优轨迹作为上下文指导。

Result: 在两个广泛使用的基准上实验表明，该框架显著优于强基线代理及仅使用文本教程或转录本的变体，验证了轨迹分割与选择、动作过滤和视觉信息的关键作用。

Conclusion: 在线视频可被系统地提炼为可在推理时有效提升计算机使用代理性能的 actionable guidance，为代理获取 procedural knowledge 提供了新路径。

Abstract: Computer-use agents can operate computers and automate laborious tasks, but
despite recent rapid progress, they still lag behind human users, especially
when tasks require domain-specific procedural knowledge about particular
applications, platforms, and multi-step workflows. Humans can bridge this gap
by watching video tutorials: we search, skim, and selectively imitate short
segments that match our current subgoal. In this paper, we study how to enable
computer-use agents to learn from online videos at inference time effectively.
We propose a framework that retrieves and filters tutorial videos, converts
them into structured demonstration trajectories, and dynamically selects
trajectories as in-context guidance during execution. Particularly, using a
VLM, we infer UI actions, segment videos into short subsequences of actions,
and assign each subsequence a textual objective. At inference time, a two-stage
selection mechanism dynamically chooses a single trajectory to add in context
at each step, focusing the agent on the most helpful local guidance for its
next decision. Experiments on two widely used benchmarks show that our
framework consistently outperforms strong base agents and variants that use
only textual tutorials or transcripts. Analyses highlight the importance of
trajectory segmentation and selection, action filtering, and visual
information, suggesting that abundant online videos can be systematically
distilled into actionable guidance that improves computer-use agents at
inference time. Our code is available at
https://github.com/UCSB-NLP-Chang/video_demo.

</details>


### [29] [Seeing Straight: Document Orientation Detection for Efficient OCR](https://arxiv.org/abs/2511.04161)
*Suranjan Goswami,Abhinav Ravi,Raja Kolla,Ali Faraz,Shaharukh Khan,Akash,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 本文提出了一种用于评估OCR在图像旋转下的鲁棒性的新基准OCR-Rotation-Bench（ORB），并构建了一个基于Phi-3.5-Vision模型的快速、鲁棒且轻量级的旋转分类流程，在两个数据集上分别实现了96%和92%的准确率，显著提升了OCR性能。


<details>
  <summary>Details</summary>
Motivation: 扫描或拍摄文档时常出现方向错误，影响OCR等下游任务性能，现有方法在真实场景中的旋转校正仍面临挑战，因此需要一个专门评估OCR旋转鲁棒性的基准及高效准确的旋转分类方法。

Method: 构建了包含英语和11种印度语系语言的OCR-Rotation-Bench（ORB）基准；提出一种基于Phi-3.5-Vision视觉编码器并结合动态图像裁剪的轻量级旋转分类流水线，并针对4类旋转任务进行独立微调。

Result: 该方法在ORB-En和ORB-Indic数据集上的旋转识别准确率分别达到96%和92%；在模拟真实场景中，显著提升闭源OCR模型性能最多14%，开源模型性能最多达4倍。

Conclusion: 提出的ORB基准为评估OCR系统对旋转的鲁棒性提供了有效工具，所设计的轻量级旋转分类模块不仅准确率高，且能显著增强各类OCR系统的性能，具有实际应用价值。

Abstract: Despite significant advances in document understanding, determining the
correct orientation of scanned or photographed documents remains a critical
pre-processing step in the real world settings. Accurate rotation correction is
essential for enhancing the performance of downstream tasks such as Optical
Character Recognition (OCR) where misalignment commonly arises due to user
errors, particularly incorrect base orientations of the camera during capture.
In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for
evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from
rotation-transformed structured and free-form English OCR datasets, and (ii)
ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource
languages. We also present a fast, robust and lightweight rotation
classification pipeline built on the vision encoder of Phi-3.5-Vision model
with dynamic image cropping, fine-tuned specifically for 4-class rotation task
in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy
on identifying the rotations respectively on both the datasets. Beyond
classification, we demonstrate the critical role of our module in boosting OCR
performance: closed-source (up to 14%) and open-weights models (up to 4x) in
the simulated real-world setting.

</details>


### [30] [Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology](https://arxiv.org/abs/2511.04171)
*Fatemehzahra Darzi,Rodrigo Escobar Diaz Guerrero,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 本研究探讨了不同颜色转换技术对HE染色图像与非线性多模态图像配准效果的影响，发现CycleGAN颜色转换在两种场景下均显著降低了配准误差，提升了数字病理学中的图像对齐精度。


<details>
  <summary>Details</summary>
Motivation: 提高不同成像模态间病理图像配准的准确性，以支持生物标志物分析和组织重建等应用。

Method: 采用20组组织样本对，比较CycleGAN、Macenko、Reinhard和Vahadane等颜色转换方法，并结合反转、对比度调整等预处理；使用VALIS方法进行刚性与非刚性配准，评估指标包括MMrTRE、AMrTRE及手动选取的关键点误差。

Result: CycleGAN颜色转换在原始和反转多模态图像场景下均取得最低的配准误差，优于其他颜色归一化方法。

Conclusion: 配准前应用颜色转换（尤其是CycleGAN）可有效改善跨模态病理图像的对齐效果，提升数字病理分析的可靠性。

Abstract: Image registration refers to the process of spatially aligning two or more
images by mapping them into a common coordinate system, so that corresponding
anatomical or tissue structures are matched across images. In digital
pathology, registration enables direct comparison and integration of
information from different stains or imaging modalities, sup-porting
applications such as biomarker analysis and tissue reconstruction. Accurate
registration of images from different modalities is an essential step in
digital pathology. In this study, we investigated how various color
transformation techniques affect image registration between hematoxylin and
eosin (H&E) stained images and non-linear multimodal images. We used a dataset
of 20 tissue sample pairs, with each pair undergoing several preprocessing
steps, including different color transformation (CycleGAN, Macenko, Reinhard,
Vahadane), inversion, contrast adjustment, intensity normalization, and
denoising. All images were registered using the VALIS registration method,
which first applies rigid registration and then performs non-rigid registration
in two steps on both low and high-resolution images. Registration performance
was evaluated using the relative Target Registration Error (rTRE). We reported
the median of median rTRE values (MMrTRE) and the average of median rTRE values
(AMrTRE) for each method. In addition, we performed a custom point-based
evaluation using ten manually selected key points. Registration was done
separately for two scenarios, using either the original or inverted multimodal
images. In both scenarios, CycleGAN color transformation achieved the lowest
registration errors, while the other methods showed higher errors. These
findings show that applying color transformation before registration improves
alignment between images from different modalities and supports more reliable
analysis in digital pathology.

</details>


### [31] [Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification](https://arxiv.org/abs/2511.04190)
*Josef Mayr,Anna Reithmeir,Maxime Di Folco,Julia A. Schnabel*

Main category: cs.CV

TL;DR: 本文研究了基于预训练视觉编码器的协方差描述符在医学图像分类中的有效性，发现结合DINOv2特征的SPDNet性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 协方差描述符在通用计算机视觉中表现良好，但在医学成像中尚未充分探索，本文旨在评估其在传统和基于学习的医学图像分类中的效果。

Method: 从预训练的通用视觉编码器（如DINOv2和MedSAM）提取特征构建协方差描述符，并与手工设计的描述符进行比较，使用SPDNet网络对多个医学图像数据集进行分类实验。

Result: 基于GVE特征的协方差描述符 consistently 优于手工特征；SPDNet结合DINOv2特征时性能超过现有最先进方法。

Conclusion: 结合强大的预训练视觉编码器与协方差描述符具有提升医学图像分析性能的巨大潜力。

Abstract: Covariance descriptors capture second-order statistics of image features.
They have shown strong performance in general computer vision tasks, but remain
underexplored in medical imaging. We investigate their effectiveness for both
conventional and learning-based medical image classification, with a particular
focus on SPDNet, a classification network specifically designed for symmetric
positive definite (SPD) matrices. We propose constructing covariance
descriptors from features extracted by pre-trained general vision encoders
(GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and
MedSAM - are evaluated across eleven binary and multi-class datasets from the
MedMNSIT benchmark. Our results show that covariance descriptors derived from
GVE features consistently outperform those derived from handcrafted features.
Moreover, SPDNet yields superior performance to state-of-the-art methods when
combined with DINOv2 features. Our findings highlight the potential of
combining covariance descriptors with powerful pretrained vision encoders for
medical image analysis.

</details>


### [32] [AStF: Motion Style Transfer via Adaptive Statistics Fusor](https://arxiv.org/abs/2511.04192)
*Hanmo Chen,Chenghao Xu,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新的自适应统计融合器（AStF），通过引入偏度和峰度来增强运动风格迁移，结合风格解耦模块和高阶多统计注意力机制，在动态风格的时空统计建模上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖均值和方差进行运动风格迁移，但难以充分捕捉运动数据的复杂动态模式和时空一致性，因此需要更全面的统计建模方法。

Method: 提出自适应统计融合器（AStF），包含风格解耦模块（SDM）和高阶多统计注意力（HOS-Attn），并引入偏度和峰度作为高阶统计量，结合运动一致性正则化（MCR）判别器进行训练。

Result: 实验结果表明，AStF在运动风格迁移任务中优于当前最先进的方法，能更有效地捕捉动态风格的时空统计特性。

Conclusion: 通过引入高阶统计量，AStF显著提升了运动风格迁移的质量和真实性，为未来动作生成提供了更优的建模思路。

Abstract: Human motion style transfer allows characters to appear less rigidity and
more realism with specific style. Traditional arbitrary image style transfer
typically process mean and variance which is proved effective. Meanwhile,
similar methods have been adapted for motion style transfer. However, due to
the fundamental differences between images and motion, relying on mean and
variance is insufficient to fully capture the complex dynamic patterns and
spatiotemporal coherence properties of motion data. Building upon this, our key
insight is to bring two more coefficient, skewness and kurtosis, into the
analysis of motion style. Specifically, we propose a novel Adaptive Statistics
Fusor (AStF) which consists of Style Disentanglement Module (SDM) and
High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in
conjunction with a Motion Consistency Regularization (MCR) discriminator.
Experimental results show that, by providing a more comprehensive model of the
spatiotemporal statistical patterns inherent in dynamic styles, our proposed
AStF shows proficiency superiority in motion style transfers over
state-of-the-arts. Our code and model are available at
https://github.com/CHMimilanlan/AStF.

</details>


### [33] [MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection](https://arxiv.org/abs/2511.04255)
*Marawan Elbatel,Anbang Wang,Keyuan Liu,Kaouther Mouheb,Enrique Almar-Munoz,Lizhuo Lin,Yanqi Yang,Karim Lekadir,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文提出MedSapiens，通过将人类中心的基础模型Sapiens迁移到医学图像解剖标志点检测任务中，在多数据集预训练后实现了新的性能记录，在平均成功检测率上显著优于现有通用和专用模型，并在少样本场景下也表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统解剖标志点检测依赖领域特定模型，而大规模预训练视觉模型提供了新机遇。作者旨在探索人类中心基础模型（如用于姿态估计的Sapiens）在医学影像中的潜力，填补该方向的研究空白。

Method: 通过对Sapiens模型进行多数据集预训练，将其适应于医学图像中的解剖标志点检测任务，构建MedSapiens模型，并在多个公开数据集上与现有最先进模型进行比较，同时评估其在少样本设置下的表现。

Result: MedSapiens在平均成功检测率（SDR）上比通用模型最高提升5.26%，比专用模型最高提升21.81%；在少样本场景下比当前最优方法提升2.69%。

Conclusion: 人类中心基础模型在空间姿态定位上的优化可为解剖标志点检测提供强先验，MedSapiens有效挖掘了这一潜力，展现出卓越的性能和良好的下游任务适应能力，证明了跨域迁移在医学图像分析中的巨大潜力。

Abstract: This paper does not introduce a novel architecture; instead, it revisits a
fundamental yet overlooked baseline: adapting human-centric foundation models
for anatomical landmark detection in medical imaging. While landmark detection
has traditionally relied on domain-specific models, the emergence of
large-scale pre-trained vision models presents new opportunities. In this
study, we investigate the adaptation of Sapiens, a human-centric foundation
model designed for pose estimation, to medical imaging through multi-dataset
pretraining, establishing a new state of the art across multiple datasets. Our
proposed model, MedSapiens, demonstrates that human-centric foundation models,
inherently optimized for spatial pose localization, provide strong priors for
anatomical landmark detection, yet this potential has remained largely
untapped. We benchmark MedSapiens against existing state-of-the-art models,
achieving up to 5.26% improvement over generalist models and up to 21.81%
improvement over specialist models in the average success detection rate (SDR).
To further assess MedSapiens adaptability to novel downstream tasks with few
annotations, we evaluate its performance in limited-data settings, achieving
2.69% improvement over the few-shot state of the art in SDR. Code and model
weights are available at https://github.com/xmed-lab/MedSapiens .

</details>


### [34] [Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery](https://arxiv.org/abs/2511.04260)
*Claudio Giusti,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CV

TL;DR: 本文提出了一种基于潜在空间信号泄露的AI生成图像与深度伪造溯源框架Proto-LeakNet，无需重新训练即可识别未知生成器，具有高可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着合成图像和深度伪造技术的发展，图像来源追踪和真实性验证变得愈发困难，现有方法难以应对未知生成器且缺乏可解释性。

Method: 在扩散模型的潜在空间中，通过重模拟部分前向扩散过程来暴露生成器特有的残留线索；采用时间注意力编码器聚合多步潜在特征，并设计特征加权原型头来构建可解释的嵌入空间，结合闭集分类与基于密度的开集评估实现对已知和未知生成器的分析。

Result: 在仅使用闭集数据训练的情况下，Proto-LeakNet达到98.13%的Macro AUC，在后处理下仍保持鲁棒性，优于现有最先进方法，并在已知与未知生成器之间展现出强分离性。

Conclusion: 通过建模扩散模型潜在空间中的信号泄露偏差，能够实现可靠且可解释的AI生成图像与深度伪造溯源，为未来数字取证提供了有效途径。

Abstract: The growing sophistication of synthetic image and deepfake generation models
has turned source attribution and authenticity verification into a critical
challenge for modern computer vision systems. Recent studies suggest that
diffusion pipelines unintentionally imprint persistent statistical traces,
known as signal leaks, within their outputs, particularly in latent
representations. Building on this observation, we propose Proto-LeakNet, a
signal-leak-aware and interpretable attribution framework that integrates
closed-set classification with a density-based open-set evaluation on the
learned embeddings, enabling analysis of unseen generators without retraining.
Operating in the latent domain of diffusion models, our method re-simulates
partial forward diffusion to expose residual generator-specific cues. A
temporal attention encoder aggregates multi-step latent features, while a
feature-weighted prototype head structures the embedding space and enables
transparent attribution. Trained solely on closed data and achieving a Macro
AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under
post-processing, surpassing state-of-the-art methods, and achieves strong
separability between known and unseen generators. These results demonstrate
that modeling signal-leak bias in latent space enables reliable and
interpretable AI-image and deepfake forensics. The code for the whole work will
be available upon submission.

</details>


### [35] [DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification](https://arxiv.org/abs/2511.04281)
*Yujie Yang,Shuang Li,Jun Ye,Neng Dong,Fan Li,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了一种基于DINOv2的步态表征学习框架（DinoGRL），用于视频可见光-红外行人重识别，通过语义感知轮廓与步态学习模型和多粒度增强模块，有效融合步态与外观特征，显著提升了跨模态行人匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要利用模态不变的外观特征，忽略了同样具有模态不变性且富含时序动态信息的步态特征，限制了跨模态视频匹配中的时空一致性建模能力。

Method: 提出DinoGRL框架，包含两个核心组件：1）语义感知轮廓与步态学习模型（SASGL），利用DINOv2的语义先验生成并增强轮廓表示，并与ReID目标联合优化；2）渐进式双向多粒度增强模块（PBMGE），在多个空间粒度上实现步态与外观流之间的双向交互，充分挖掘二者互补性以增强全局表征。

Result: 在HITSZ-VCM和BUPT数据集上进行了大量实验，结果表明所提方法显著优于现有的最先进方法。

Conclusion: DinoGRL通过引入DINOv2驱动的步态特征学习，并结合外观与步态的多粒度融合策略，有效提升了视频可见光-红外行人重识别的性能，验证了步态特征在跨模态匹配中的重要作用。

Abstract: Video-based Visible-Infrared person re-identification (VVI-ReID) aims to
retrieve the same pedestrian across visible and infrared modalities from video
sequences. Existing methods tend to exploit modality-invariant visual features
but largely overlook gait features, which are not only modality-invariant but
also rich in temporal dynamics, thus limiting their ability to model the
spatiotemporal consistency essential for cross-modal video matching. To address
these challenges, we propose a DINOv2-Driven Gait Representation Learning
(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn
gait features complementary to appearance cues, facilitating robust
sequence-level representations for cross-modal retrieval. Specifically, we
introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which
generates and enhances silhouette representations with general-purpose semantic
priors from DINOv2 and jointly optimizes them with the ReID objective to
achieve semantically enriched and task-adaptive gait feature learning.
Furthermore, we develop a Progressive Bidirectional Multi-Granularity
Enhancement (PBMGE) module, which progressively refines feature representations
by enabling bidirectional interactions between gait and appearance streams
across multiple spatial granularities, fully leveraging their complementarity
to enhance global representations with rich local details and produce highly
discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets
demonstrate the superiority of our approach, significantly outperforming
existing state-of-the-art methods.

</details>


### [36] [FastGS: Training 3D Gaussian Splatting in 100 Seconds](https://arxiv.org/abs/2511.04283)
*Shiwei Ren,Tianci Wen,Yongchun Fang,Biao Lu*

Main category: cs.CV

TL;DR: 本文提出了FastGS，一种基于多视角一致性的新型3D高斯加速框架，无需预算机制即可实现高效的训练加速，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯加速方法在训练过程中无法有效控制高斯数量，导致计算冗余。

Method: 设计了一种基于多视角一致性的高斯稠密化与剪枝策略，摒弃了传统的预算机制，充分考虑每个高斯的重要性。

Result: 在Mip-NeRF 360上相比DashGaussian实现了3.32倍的训练加速且保持相当的渲染质量，在Deep Blending数据集上相比原始3DGS达到15.45倍加速，并在多种任务中展现出2-7倍的加速效果。

Conclusion: FastGS是一种高效、通用的加速框架，能显著缩短训练时间，同时保持良好的渲染质量，适用于多种3D重建任务。

Abstract: The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to
properly regulate the number of Gaussians during training, causing redundant
computational time overhead. In this paper, we propose FastGS, a novel, simple,
and general acceleration framework that fully considers the importance of each
Gaussian based on multi-view consistency, efficiently solving the trade-off
between training time and rendering quality. We innovatively design a
densification and pruning strategy based on multi-view consistency, dispensing
with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &
Temples, and Deep Blending datasets demonstrate that our method significantly
outperforms the state-of-the-art methods in training speed, achieving a
3.32$\times$ training acceleration and comparable rendering quality compared
with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration
compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that
FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration
across various tasks, including dynamic scene reconstruction, surface
reconstruction, sparse-view reconstruction, large-scale reconstruction, and
simultaneous localization and mapping. The project page is available at
https://fastgs.github.io/

</details>


### [37] [Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment](https://arxiv.org/abs/2511.04288)
*Leire Benito-Del-Valle,Artzai Picón,Daniel Mugica,Manuel Ramos,Eva Portillo,Javier Romero,Carlos Javier Jimenez,Ramón Navarra-Mestre*

Main category: cs.CV

TL;DR: 本文提出了一种针对除草剂田间试验的领域专用视觉基础模型，通过自监督学习在大规模农业数据集上训练，显著提升了植物种类识别和除草剂损伤分类的性能，尤其在跨环境和域偏移场景下表现更优，并能大幅减少标注需求。


<details>
  <summary>Details</summary>
Motivation: 通用视觉基础模型在农业领域应用时，因物种和损伤类型的细粒度差异而性能受限，难以满足除草剂田间试验对精确识别和评估的需求。

Method: 采用自监督学习方法，在大规模、经过整理的农业数据集上对通用视觉基础模型进行领域特定的预训练，以学习适用于除草剂试验图像的丰富且可迁移的表征。

Result: 该领域专用模型在物种识别（F1分数从0.91提升至0.94）和损伤分类（从0.26提升至0.33）上均显著优于通用模型；在新环境和无人机图像等域偏移场景下仍保持优势；同时在少样本条件下显著提升分割精度，仅用20%的标注数据即可超越通用模型。

Conclusion: 领域专用基础模型具有更强的泛化能力，可显著降低人工标注成本，为除草剂田间试验提供可扩展、自动化的分析方案。

Abstract: Herbicide field trials require accurate identification of plant species and
assessment of herbicide-induced damage across diverse environments. While
general-purpose vision foundation models have shown promising results in
complex visual domains, their performance can be limited in agriculture, where
fine-grained distinctions between species and damage types are critical.
  In this work, we adapt a general-purpose vision foundation model to herbicide
trial characterization. Trained using a self-supervised learning approach on a
large, curated agricultural dataset, the model learns rich and transferable
representations optimized for herbicide trials images.
  Our domain-specific model significantly outperforms the best general-purpose
foundation model in both species identification (F1 score improvement from 0.91
to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions
(new locations and other time), it achieves even greater gains (species
identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In
domain-shift scenarios, such as drone imagery, it maintains strong performance
(species classification from 0.49 to 0.60).
  Additionally, we show that domain-specific pretraining enhances segmentation
accuracy, particularly in low-annotation regimes. An annotation-efficiency
analysis reveals that, under unseen conditions, the domain-specific model
achieves 5.4% higher F1 score than the general-purpose model, while using 80%
fewer labeled samples.
  These results demonstrate the generalization capabilities of domain-specific
foundation models and their potential to significantly reduce manual annotation
efforts, offering a scalable and automated solution for herbicide trial
analysis.

</details>


### [38] [Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data](https://arxiv.org/abs/2511.04304)
*Robin Spanier,Thorsten Hoeser,Claudia Kuenzer*

Main category: cs.CV

TL;DR: 本研究利用合成与真实Sentinel-1卫星图像训练YOLOv10模型，提升稀少样本下海上基础设施检测性能，并验证其地理可迁移性。


<details>
  <summary>Details</summary>
Motivation: 由于海上基础设施种类、形状和尺寸的样本稀缺且数据不平衡，传统检测模型表现受限，亟需提升模型泛化能力与数据均衡性。

Method: 结合合成与真实的Sentinel-1卫星影像训练YOLOv10目标检测模型，并在三个未见区域（墨西哥湾、北海、波斯湾）进行区域留出评估以检验模型的地理可迁移性。

Result: 共检测到3,529个海上平台，其中北海411个、墨西哥湾1,519个、波斯湾1,593个；模型F1分数从0.85提升至0.90，显示合成数据有效改善类别不平衡并提升整体性能。

Conclusion: 合成数据是缓解遥感中样本稀缺与类别不平衡问题的有效策略，有助于实现可扩展、全球适用的海上基础设施监测。

Abstract: The recent and ongoing expansion of marine infrastructure, including offshore
wind farms, oil and gas platforms, artificial islands, and aquaculture
facilities, highlights the need for effective monitoring systems. The
development of robust models for offshore infrastructure detection relies on
comprehensive, balanced datasets, but falls short when samples are scarce,
particularly for underrepresented object classes, shapes, and sizes. By
training deep learning-based YOLOv10 object detection models with a combination
of synthetic and real Sentinel-1 satellite imagery acquired in the fourth
quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of
Guinea, and Coast of Brazil), this study investigates the use of synthetic
training data to enhance model performance. We evaluated this approach by
applying the model to detect offshore platforms in three unseen regions (Gulf
of Mexico, North Sea, Persian Gulf) and thereby assess geographic
transferability. This region-holdout evaluation demonstrated that the model
generalises beyond the training areas. In total, 3,529 offshore platforms were
detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and
1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which
improved to 0.90 upon incorporating synthetic data. We analysed how synthetic
data enhances the representation of unbalanced classes and overall model
performance, taking a first step toward globally transferable detection of
offshore infrastructure. This study underscores the importance of balanced
datasets and highlights synthetic data generation as an effective strategy to
address common challenges in remote sensing, demonstrating the potential of
deep learning for scalable, global offshore infrastructure monitoring.

</details>


### [39] [RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation](https://arxiv.org/abs/2511.04317)
*Xiangjun Zhang,Litong Gong,Yinglin Zheng,Yansong Liu,Wentao Jiang,Mingyi Xu,Biao Wang,Tiezheng Ge,Ming Zeng*

Main category: cs.CV

TL;DR: 提出RISE-T2V框架，通过引入重述适配器模块，将提示重述与语义特征提取融合为一步，提升文本到视频生成模型对用户意图的理解和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有T2V模型依赖预训练文本编码器，但语义理解有限，无法在线优化提示，导致生成质量差、可扩展性差。

Method: 设计Rephrasing Adapter模块，利用LLM在下一词预测中的隐藏状态作为视频生成条件，实现提示重述与语义提取一体化，并兼容多种LLM和视频扩散模型。

Result: 实验表明RISE-T2V能显著提升不同架构视频扩散模型的生成质量与意图对齐能力，支持更广泛的T2V任务。

Conclusion: RISE-T2V是一种通用、灵活且高效的T2V生成框架，通过深度融合LLM的语义理解能力，有效解决了提示理解不足和提示表达局限的问题。

Abstract: Most text-to-video(T2V) diffusion models depend on pre-trained text encoders
for semantic alignment, yet they often fail to maintain video quality when
provided with concise prompts rather than well-designed ones. The primary issue
lies in their limited textual semantics understanding. Moreover, these text
encoders cannot rephrase prompts online to better align with user intentions,
which limits both the scalability and usability of the models, To address these
challenges, we introduce RISE-T2V, which uniquely integrates the processes of
prompt rephrasing and semantic feature extraction into a single and seamless
step instead of two separate steps. RISE-T2V is universal and can be applied to
various pre-trained LLMs and video diffusion models(VDMs), significantly
enhancing their capabilities for T2V tasks. We propose an innovative module
called the Rephrasing Adapter, enabling diffusion models to utilize text hidden
states during the next token prediction of the LLM as a condition for video
generation. By employing a Rephrasing Adapter, the video generation model can
implicitly rephrase basic prompts into more comprehensive representations that
better match the user's intent. Furthermore, we leverage the powerful
capabilities of LLMs to enable video generation models to accomplish a broader
range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a
versatile framework applicable to different video diffusion model
architectures, significantly enhancing the ability of T2V models to generate
high-quality videos that align with user intent. Visual results are available
on the webpage at https://rise-t2v.github.io.

</details>


### [40] [Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography](https://arxiv.org/abs/2511.04334)
*Saúl Alonso-Monsalve,Leigh H. Whitehead,Adam Aurisano,Lorena Escudero Sanchez*

Main category: cs.CV

TL;DR: 本文提出了一种基于体素稀疏化和子流形稀疏卷积网络的两阶段方法，用于在高分辨率3D CT图像中自动分割肾癌肿瘤，实现了最先进的精度，并显著降低了计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 准确且高效地分割CT图像中的肿瘤是临床定量分析的关键瓶颈，传统方法因3D数据量大而受限，亟需一种兼顾精度与效率的自动化分割方法。

Method: 采用两阶段策略：首先进行体素稀疏化以减少无效计算，然后利用子流形稀疏卷积网络处理高分辨率3D输入，在保留细节的同时降低内存和计算开销。

Result: 在KiTS23肾癌数据集上达到与挑战赛优胜者相当的性能，肾脏+病灶Dice系数为95.8%，肿瘤+囊肿为85.7%，单独肿瘤为80.3%；相比密集模型，推理时间最多减少60%，显存占用最多降低75%。

Conclusion: 该方法在保持高精度的同时大幅提升了计算效率，适用于临床环境中对高分辨率3D医学图像进行快速、低成本的肿瘤分割。

Abstract: The accurate delineation of tumours in radiological images like Computed
Tomography is a very specialised and time-consuming task, and currently a
bottleneck preventing quantitative analyses to be performed routinely in the
clinical setting. For this reason, developing methods for the automated
segmentation of tumours in medical imaging is of the utmost importance and has
driven significant efforts in recent years. However, challenges regarding the
impracticality of 3D scans, given the large amount of voxels to be analysed,
usually requires the downsampling of such images or using patches thereof when
applying traditional convolutional neural networks. To overcome this problem,
in this paper we propose a new methodology that uses, divided into two stages,
voxel sparsification and submanifold sparse convolutional networks. This method
allows segmentations to be performed with high-resolution inputs and a native
3D model architecture, obtaining state-of-the-art accuracies while
significantly reducing the computational resources needed in terms of GPU
memory and time. We studied the deployment of this methodology in the context
of Computed Tomography images of renal cancer patients from the KiTS23
challenge, and our method achieved results competitive with the challenge
winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%
for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also
offers significant computational improvements, achieving up to a 60% reduction
in inference time and up to a 75\% reduction in VRAM usage compared to an
equivalent dense architecture, across both CPU and various GPU cards tested.

</details>


### [41] [Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset](https://arxiv.org/abs/2511.04344)
*Muhammad Annas Shaikh,Hamza Zaman,Arbaz Asif*

Main category: cs.CV

TL;DR: 本文评估了九种卷积神经网络在VOC 2008数据集上对马和摩托车进行二分类的表现，重点解决类别不平衡问题。通过使用少数类增强技术，比较了包括ResNet-50、ConvNeXt-Tiny、DenseNet-121和Vision Transformer在内的多种现代架构。结果显示ConvNeXt-Tiny在检测性能上最优，数据增强显著提升了少数类的检测效果，尤其有利于深层架构。


<details>
  <summary>Details</summary>
Motivation: 由于VOC 2008数据集中存在显著的类别不平衡问题，影响二分类性能，本文旨在评估不同CNN架构在此类任务中的表现，并探索数据增强对缓解类别不平衡的有效性。

Method: 采用九种CNN架构（如ResNet-50、ConvNeXt-Tiny、DenseNet-121和Vision Transformer）在VOC 2008数据集上进行二分类实验，实施少数类数据增强策略，并在多个性能指标下比较模型表现。

Result: ConvNeXt-Tiny取得最佳性能，马检测的平均精度（AP）达95.53%，摩托车为89.12%；数据增强显著提升少数类检测效果，且对深层网络增益更明显。不同架构间性能差异显著。

Conclusion: 在处理类别不平衡的二分类任务时，选择合适的网络架构（如ConvNeXt-Tiny）并结合数据增强策略可显著提升检测性能，本研究为类似任务中的模型选择与优化提供了实证依据。

Abstract: This paper presents a comprehensive evaluation of nine convolutional neural
network architectures for binary classification of horses and motorcycles in
the VOC 2008 dataset. We address the significant class imbalance problem by
implementing minority-class augmentation techniques. Our experiments compare
modern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and
Vision Transformer across multiple performance metrics. Results demonstrate
substantial performance variations, with ConvNeXt-Tiny achieving the highest
Average Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle
detection. We observe that data augmentation significantly improves minority
class detection, particularly benefiting deeper architectures. This study
provides insights into architecture selection for imbalanced binary
classification tasks and quantifies the impact of data augmentation strategies
in mitigating class imbalance issues in object detection.

</details>


### [42] [Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection](https://arxiv.org/abs/2511.04347)
*Sanjay Kumar,Tim Brophy,Eoin Martino Grua,Ganesh Sistu,Valentina Donzella,Ciaran Eising*

Main category: cs.CV

TL;DR: 本文研究了在鸟瞰图（BEV）融合架构中，摄像头和激光雷达传感器遮挡对3D目标检测性能的影响，发现模型更依赖LiDAR，尤其在严重遮挡下LiDAR性能显著下降，强调需开发抗遮挡的融合方法。


<details>
  <summary>Details</summary>
Motivation: 探索复杂真实环境中传感器遮挡（如雾、霾或物理障碍）对多模态BEV融合检测性能的影响，填补该问题在现有研究中的空白。

Method: 基于BEVFusion架构，在nuScenes数据集上评估不同遮挡程度下摄像头和LiDAR单模态及融合模式的3D检测性能，使用mAP和NDS作为评价指标。

Result: 中等摄像头遮挡导致纯视觉mAP下降41.3%；严重LiDAR遮挡使其mAP下降47.3%，且长距离检测受影响严重；融合模式下遮挡摄像头影响较小（mAP降4.1%），而遮挡LiDAR导致mAP下降26.8%，显示模型更依赖LiDAR。

Conclusion: 当前BEV融合模型对LiDAR输入高度依赖，传感器遮挡显著影响检测性能，未来需发展遮挡感知的评估方法和鲁棒的融合技术以应对传感器退化或失效。

Abstract: Accurate 3D object detection is essential for automated vehicles to navigate
safely in complex real-world environments. Bird's Eye View (BEV)
representations, which project multi-sensor data into a top-down spatial
format, have emerged as a powerful approach for robust perception. Although
BEV-based fusion architectures have demonstrated strong performance through
multimodal integration, the effects of sensor occlusions, caused by
environmental conditions such as fog, haze, or physical obstructions, on 3D
detection accuracy remain underexplored. In this work, we investigate the
impact of occlusions on both camera and Light Detection and Ranging (LiDAR)
outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.
Detection performance is measured using mean Average Precision (mAP) and the
nuScenes Detection Score (NDS). Our results show that moderate camera
occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is
based only on the camera. On the other hand, LiDAR sharply drops in performance
only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),
with a severe impact on long-range detection. In fused settings, the effect
depends on which sensor is occluded: occluding the camera leads to a minor 4.1%
drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%
drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task
of 3D object detection. Our results highlight the need for future research into
occlusion-aware evaluation methods and improved sensor fusion techniques that
can maintain detection accuracy in the presence of partial sensor failure or
degradation due to adverse environmental conditions.

</details>


### [43] [A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications](https://arxiv.org/abs/2511.04349)
*Puneet Mishra,Martijntje Vollebregt,Yizhou Ma,Maria Font-i-Furnols*

Main category: cs.CV

TL;DR: 本教程旨在通过提供逐步指导，帮助分析化学领域研究人员利用现有的开源深度学习模型从成像数据中提取空间信息，并将其与其他数据（如光谱信息）结合，以提升数据分析能力。


<details>
  <summary>Details</summary>
Motivation: 传统化学计量方法在高效提取和分析成像数据中的空间信息方面存在局限，而尽管深度学习在图像处理方面取得进展，其在分析化学中的应用因缺乏系统性实施指南而受限。

Method: 提供基于MATLAB代码的详细教程，演示如何使用现有开源深度学习模型从多种成像模态中提取深层特征，重点不在于训练模型，而在于利用预训练模型进行特征提取。

Result: 成功展示了从不同成像数据中提取空间信息并融合其他数据源的方法，提供了可直接应用于用户自有数据集的代码示例。

Conclusion: 该教程填补了深度学习在分析化学中应用的实践空白，促进了深度特征提取技术的普及和实际应用。

Abstract: Background In analytical chemistry, spatial information about materials is
commonly captured through imaging techniques, such as traditional color cameras
or with advanced hyperspectral cameras and microscopes. However, efficiently
extracting and analyzing this spatial information for exploratory and
predictive purposes remains a challenge, especially when using traditional
chemometric methods. Recent advances in deep learning and artificial
intelligence have significantly enhanced image processing capabilities,
enabling the extraction of multiscale deep features that are otherwise
challenging to capture with conventional image processing techniques. Despite
the wide availability of open-source deep learning models, adoption in
analytical chemistry remains limited because of the absence of structured,
step-by-step guidance for implementing these models.
  Results This tutorial aims to bridge this gap by providing a step-by-step
guide for applying deep learning approaches to extract spatial information from
imaging data and integrating it with other data sources, such as spectral
information. Importantly, the focus of this work is not on training deep
learning models for image processing but on using existing open source models
to extract deep features from imaging data.
  Significance The tutorial provides MATLAB code tutorial demonstrations,
showcasing the processing of imaging data from various imaging modalities
commonly encountered in analytical chemistry. Readers must run the tutorial
steps on their own datasets using the codes presented in this tutorial.

</details>


### [44] [Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA](https://arxiv.org/abs/2511.04384)
*Itbaan Safwan,Muhammad Annas Shaikh,Muhammad Haaris,Ramail Khan,Muhammad Atif Tahir*

Main category: cs.CV

TL;DR: 提出了一种基于LoRA微调Florence-2模型的多任务框架，用于MediaEval Medico 2025挑战赛，同时实现视觉问答、解释生成和视觉定位。


<details>
  <summary>Details</summary>
Motivation: 为了提升医学视觉问答系统的准确性和可解释性，通过多任务学习联合优化视觉理解、推理和定位能力。

Method: 采用LoRA微调Florence-2模型，结合三个精心构建的数据集：Kvasir-VQA-x1（问答）、合成增强的解释数据集（医学推理）和文本到区域配对数据集（视觉接地），实现多任务联合训练。

Result: 实验表明，该方法在答案准确性和视觉定位方面均显著优于单任务基线模型。

Conclusion: 所提出的多任务框架有效提升了医学VQA系统的性能，验证了基于 grounding 的多任务学习在医学图像理解中的潜力。

Abstract: We present a multi-task framework for the MediaEval Medico 2025 challenge,
leveraging a LoRA-tuned Florence-2 model for simultaneous visual question
answering (VQA), explanation generation, and visual grounding. The proposed
system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer
learning, (2) a synthetically enriched explanation dataset offering structured
medical reasoning, and (3) text-to-region pairs linking visual features with
segmentation masks. This multi-task setup enables the model to jointly learn
visual grounding, reasoning, and interpretation, producing responses that are
both accurate and interpretable. Extensive evaluation demonstrates that our
approach substantially improves over single-task baselines in both answer
accuracy and visual localization, highlighting the effectiveness of grounded
multi-task learning for medical VQA applications.

</details>


### [45] [DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale](https://arxiv.org/abs/2511.04394)
*Ke Du,Yimin Peng,Chao Gao,Fan Zhou,Siqiao Xue*

Main category: cs.CV

TL;DR: DORAEMON是一个开源的PyTorch库，统一了多尺度视觉对象建模与表示学习，支持分类、检索和度量学习，提供超过1000个预训练骨干网络及模块化组件，实现可复现的高性能结果，并支持一键导出至ONNX或HuggingFace，促进研究到部署的高效转化。


<details>
  <summary>Details</summary>
Motivation: 为了统一和简化跨尺度的视觉对象建模与表示学习，整合分散的数据集、模型和训练方法，提供一个可扩展、易用且高效的平台以加速视觉识别领域的研究与应用。

Method: 通过一个YAML驱动的工作流整合分类、检索和度量学习任务，利用timm兼容接口集成超过1000个预训练 backbone 模型，结合模块化的损失函数、数据增强和分布式训练工具，并支持一键导出模型至ONNX或HuggingFace。

Result: 在ImageNet-1K、MS-Celeb-1M和Stanford Online Products等基准上实现了可复现且达到或超越参考结果的性能，验证了其有效性与可扩展性。

Conclusion: DORAEMON为视觉识别与表示学习提供了统一、可扩展的开源平台，显著降低了研究与部署门槛，促进了研究成果向实际应用的快速迁移。

Abstract: DORAEMON is an open-source PyTorch library that unifies visual object
modeling and representation learning across diverse scales. A single
YAML-driven workflow covers classification, retrieval and metric learning; more
than 1000 pretrained backbones are exposed through a timm-compatible interface,
together with modular losses, augmentations and distributed-training utilities.
Reproducible recipes match or exceed reference results on ImageNet-1K,
MS-Celeb-1M and Stanford online products, while one-command export to ONNX or
HuggingFace bridges research and deployment. By consolidating datasets, models,
and training techniques into one platform, DORAEMON offers a scalable
foundation for rapid experimentation in visual recognition and representation
learning, enabling efficient transfer of research advances to real-world
applications. The repository is available at https://github.com/wuji3/DORAEMON.

</details>


### [46] [HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats](https://arxiv.org/abs/2511.04426)
*Alan de Aguiar,Michaella Pereira Andrade,Charles Morphy D. Santos,João Paulo Gois*

Main category: cs.CV

TL;DR: 本文提出了一种名为HideAndSeg的最小监督AI工具，用于分割章鱼视频，结合SAM2与自训练YOLOv11检测器，并引入两个无监督指标评估分割质量，在减少人工干预的同时实现了对自然环境中章鱼的有效重识别与分割。


<details>
  <summary>Details</summary>
Motivation: 由于章鱼具有伪装能力、皮肤纹理和颜色快速变化、非刚体形变及频繁遮挡，加之水下光照和浑浊度变化，使其在自然栖息地中的分析极具挑战性；现有大规模标注数据集缺乏，亟需自动化工具支持行为研究。

Method: 将SAM2与自定义训练的YOLOv11检测器结合：首先由用户输入点坐标生成初始分割掩码用于训练YOLO，之后通过边界框提示实现全流程自动化；引入时间一致性DICE_t和新组件计数NC_t两个无监督指标评估并优化分割结果。

Result: HideAndSeg相比手动提示方法显著减少了分割噪声，能够在完全遮挡后重新识别并分割章鱼，表现出良好的性能，并实现了在无真实标签情况下的有效分割质量评估。

Conclusion: 该方法大幅降低了对人工标注的依赖，为野生头足类动物的行为研究提供了一个高效、实用的工具。

Abstract: Analyzing octopuses in their natural habitats is challenging due to their
camouflage capability, rapid changes in skin texture and color, non-rigid body
deformations, and frequent occlusions, all of which are compounded by variable
underwater lighting and turbidity. Addressing the lack of large-scale annotated
datasets, this paper introduces HideAndSeg, a novel, minimally supervised
AI-based tool for segmenting videos of octopuses. It establishes a quantitative
baseline for this task. HideAndSeg integrates SAM2 with a custom-trained
YOLOv11 object detector. First, the user provides point coordinates to generate
the initial segmentation masks with SAM2. These masks serve as training data
for the YOLO model. After that, our approach fully automates the pipeline by
providing a bounding box prompt to SAM2, eliminating the need for further
manual intervention. We introduce two unsupervised metrics - temporal
consistency $DICE_t$ and new component count $NC_t$ - to quantitatively
evaluate segmentation quality and guide mask refinement in the absence of
ground-truth data, i.e., real-world information that serves to train, validate,
and test AI models. Results show that HideAndSeg achieves satisfactory
performance, reducing segmentation noise compared to the manually prompted
approach. Our method can re-identify and segment the octopus even after periods
of complete occlusion in natural environments, a scenario in which the manually
prompted model fails. By reducing the need for manual analysis in real-world
scenarios, this work provides a practical tool that paves the way for more
efficient behavioral studies of wild cephalopods.

</details>


### [47] [Solving Convex Partition Visual Jigsaw Puzzles](https://arxiv.org/abs/2511.04450)
*Yaniv Ohayon,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 本文提出了一种针对凸多边形拼图的自动求解方法，结合几何和图像兼容性，开发了贪婪求解器，并发布了首个此类拼图的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的自动拼图求解研究主要集中在方形拼图上，限制了实际应用。本文旨在扩展可计算处理的拼图类型，涵盖更实用的凸多边形拼图。

Method: 利用几何形状和图像内容的兼容性，设计了一种贪婪算法求解器，并构建了首个凸分割拼图的基准数据集。

Result: 成功实现了对凸多边形拼图的自动求解，报告了多种性能指标，并提供了公开数据集以促进后续研究。

Conclusion: 该方法显著扩展了自动拼图求解的适用范围，为更广泛的多边形拼图问题提供了可行的解决方案。

Abstract: Jigsaw puzzle solving requires the rearrangement of unordered pieces into
their original pose in order to reconstruct a coherent whole, often an image,
and is known to be an intractable problem. While the possible impact of
automatic puzzle solvers can be disruptive in various application domains, most
of the literature has focused on developing solvers for square jigsaw puzzles,
severely limiting their practical use. In this work, we significantly expand
the types of puzzles handled computationally, focusing on what is known as
Convex Partitions, a major subset of polygonal puzzles whose pieces are convex.
We utilize both geometrical and pictorial compatibilities, introduce a greedy
solver, and report several performance measures next to the first benchmark
dataset of such puzzles.

</details>


### [48] [V-Thinker: Interactive Thinking with Images](https://arxiv.org/abs/2511.04460)
*Runqi Qiao,Qiuna Tan,Minghan Yang,Guanting Dong,Peiqing Yang,Shiqiang Lang,Enhui Wan,Xiaowan Wang,Yida Xu,Lan Yang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.CV

TL;DR: 提出V-Thinker，一种通过端到端强化学习实现图像交互式推理的通用多模态推理助手，并构建VTBench基准测试其性能。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉模型在图像交互与长程推理深度融合方面受限于视觉工具空间有限和任务特定的工作流设计。

Method: 提出V-Thinker，包含数据进化飞轮自动生成和优化交互推理数据集，以及视觉渐进训练课程，通过点级监督对齐感知，并采用两阶段强化学习框架整合交互推理。

Result: 在VTBench等广泛实验中，V-Thinker在通用和交互式推理场景下均优于强大多模态基线模型。

Conclusion: V-Thinker有效推动了以图像为中心的交互式推理能力发展，为图像交互应用提供了新方向。

Abstract: Empowering Large Multimodal Models (LMMs) to deeply integrate image
interaction with long-horizon reasoning capabilities remains a long-standing
challenge in this field. Recent advances in vision-centric reasoning explore a
promising "Thinking with Images" paradigm for LMMs, marking a shift from
image-assisted reasoning to image-interactive thinking. While this milestone
enables models to focus on fine-grained image regions, progress remains
constrained by limited visual tool spaces and task-specific workflow designs.
To bridge this gap, we present V-Thinker, a general-purpose multimodal
reasoning assistant that enables interactive, vision-centric thinking through
end-to-end reinforcement learning. V-Thinker comprises two key components: (1)
a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies
interactive reasoning datasets across three dimensions-diversity, quality, and
difficulty; and (2) a Visual Progressive Training Curriculum that first aligns
perception via point-level supervision, then integrates interactive reasoning
through a two-stage reinforcement learning framework. Furthermore, we introduce
VTBench, an expert-verified benchmark targeting vision-centric interactive
reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently
outperforms strong LMM-based baselines in both general and interactive
reasoning scenarios, providing valuable insights for advancing
image-interactive reasoning applications.

</details>


### [49] [Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability](https://arxiv.org/abs/2511.04474)
*Wenwen Li,Sizhe Wang,Hyunho Lee,Chenyan Lu,Sujit Roy,Rahul Ramachandran,Chia-Yu Hsu*

Main category: cs.CV

TL;DR: 本研究提出了一种面向遥感滑坡制图的三轴分析框架（传感器、标签、域），基于地理空间基础模型Prithvi-EO-2.0，展示了其在跨传感器、区域和少样本条件下的优越性能，优于多种专用模型和其他基础模型，同时讨论了计算成本与数据可用性等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在跨传感器、区域或标注数据有限的情况下泛化能力差，难以满足滑坡灾害快速响应与风险防控的需求，亟需更具鲁棒性和可扩展性的方法。

Method: 提出传感器-标签-域三轴分析框架，基于全球预训练、自监督学习和可适应微调的地理空间基础模型Prithvi-EO-2.0，进行系统性消融与对比实验，评估其在不同条件下的迁移与泛化能力。

Result: Prithvi-EO-2.0在多个滑坡数据集上 consistently 超过U-Net、SegFormer等专用模型及其他GeoFMs，表现出对光谱差异的鲁棒性、在标注稀缺下的稳定性以及跨区域的良好泛化能力。

Conclusion: 地理空间基础模型（GeoFMs）代表了滑坡制图向更鲁棒、可扩展方向发展的关键一步，但其广泛应用仍受限于计算资源和高质量AI-ready训练数据的缺乏。

Abstract: Landslides cause severe damage to lives, infrastructure, and the environment,
making accurate and timely mapping essential for disaster preparedness and
response. However, conventional deep learning models often struggle when
applied across different sensors, regions, or under conditions of limited
training data. To address these challenges, we present a three-axis analytical
framework of sensor, label, and domain for adapting geospatial foundation
models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a
series of experiments, we show that it consistently outperforms task-specific
CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other
GeoFMs (TerraMind, SatMAE). The model, built on global pretraining,
self-supervision, and adaptable fine-tuning, proved resilient to spectral
variation, maintained accuracy under label scarcity, and generalized more
reliably across diverse datasets and geographic settings. Alongside these
strengths, we also highlight remaining challenges such as computational cost
and the limited availability of reusable AI-ready training data for landslide
research. Overall, our study positions GeoFMs as a step toward more robust and
scalable approaches for landslide risk reduction and environmental monitoring.

</details>


### [50] [THEval. Evaluation Framework for Talking Head Video Generation](https://arxiv.org/abs/2511.04520)
*Nabyl Quignon,Baptiste Chopin,Yaohui Wang,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 提出一个包含8个指标的新评估框架，用于评估说话人头像生成的质量、自然性和同步性，强调效率和与人类偏好的一致性，并基于新构建的真实数据集进行实验，发现现有方法在表情生成和细节处理上仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的说话头像生成评估主要依赖有限的指标和用户研究，缺乏全面且高效的评估体系，无法充分反映生成视频的真实质量。

Method: 设计了一个涵盖质量、自然性和同步性三个维度的8项指标评估框架，重点关注头部、嘴部和眉毛的细粒度动态以及面部质量，并基于新构建的无偏真实数据集对17种最先进模型生成的8.5万段视频进行评估。

Result: 实验表明，尽管许多算法在唇部同步方面表现良好，但在生成表情丰富性和无伪影细节方面仍面临挑战；新框架能更全面地评估生成方法的优劣。

Conclusion: 所提出的评估框架能够更有效地衡量说话头像生成模型的进步，推动该领域向更高质量、更自然的方向发展，代码、数据集和排行榜将公开并持续更新。

Abstract: Video generation has achieved remarkable progress, with generated videos
increasingly resembling real ones. However, the rapid advance in generation has
outpaced the development of adequate evaluation metrics. Currently, the
assessment of talking head generation primarily relies on limited metrics,
evaluating general video quality, lip synchronization, and on conducting user
studies. Motivated by this, we propose a new evaluation framework comprising 8
metrics related to three dimensions (i) quality, (ii) naturalness, and (iii)
synchronization. In selecting the metrics, we place emphasis on efficiency, as
well as alignment with human preferences. Based on this considerations, we
streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as
well as face quality. Our extensive experiments on 85,000 videos generated by
17 state-of-the-art models suggest that while many algorithms excel in lip
synchronization, they face challenges with generating expressiveness and
artifact-free details. These videos were generated based on a novel real
dataset, that we have curated, in order to mitigate bias of training data. Our
proposed benchmark framework is aimed at evaluating the improvement of
generative methods. Original code, dataset and leaderboards will be publicly
released and regularly updated with new methods, in order to reflect progress
in the field.

</details>


### [51] [Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy](https://arxiv.org/abs/2511.04525)
*Dimitrios Anastasiou,Santiago Barbarisi,Lucy Culshaw,Jayna Patel,Evangelos B. Mazomenos,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 本文提出了一种名为STC-Net的新框架，用于在腹腔镜胆囊切除术（LC）中基于Parkland分级量表（PGS）自动评估手术复杂性，能够在弱时间监督下直接处理完整手术视频，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 准确评估LC手术复杂性至关重要，尤其是在严重炎症情况下；然而，现有方法多局限于静态图像或手动裁剪片段，缺乏对完整未修剪视频的自动化分析方法。

Method: 提出STC-Net框架，包含定位、窗口提议和分级模块，联合实现时间定位与复杂性分级，并引入结合硬/软定位目标和背景感知分级监督的新型损失函数，在弱时间监督下直接处理完整LC视频。

Result: 在1,859个LC视频的私有数据集上，STC-Net达到62.11%的准确率和61.42%的F1分数，比非定位基线模型高出10%以上，验证了弱监督在手术复杂性评估中的有效性。

Conclusion: STC-Net为基于PGS的手术复杂性自动化评估提供了一种可扩展且有效的方法，适用于术后分析和外科培训。

Abstract: Purpose: Accurate assessment of surgical complexity is essential in
Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with
longer operative times and increased risk of postoperative complications. The
Parkland Grading Scale (PGS) provides a clinically validated framework for
stratifying inflammation severity; however, its automation in surgical videos
remains largely unexplored, particularly in realistic scenarios where complete
videos must be analyzed without prior manual curation. Methods: In this work,
we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity
estimation in LC via the PGS, designed to operate under weak temporal
supervision. Unlike prior methods limited to static images or manually trimmed
clips, STC-Net operates directly on full videos. It jointly performs temporal
localization and grading through a localization, window proposal, and grading
module. We introduce a novel loss formulation combining hard and soft
localization objectives and background-aware grading supervision. Results:
Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy
of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by
over 10% in both metrics and highlighting the effectiveness of weak supervision
for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable
and effective approach for automated PGS-based surgical complexity estimation
from full LC videos, making it promising for post-operative analysis and
surgical training.

</details>


### [52] [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)
*Jingqi Tong,Yurong Mou,Hangcheng Li,Mingzhe Li,Yongzhuo Yang,Ming Zhang,Qiguang Chen,Tianyi Liang,Xiaomeng Hu,Yining Zheng,Xinchi Chen,Jun Zhao,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CV

TL;DR: 提出“Thinking with Video”新范式，利用视频生成模型（如Sora-2）统一视觉与文本推理，并通过VideoThinkBench基准验证其在视觉和文本任务上的优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有“Thinking with Text”和“Thinking with Images”范式难以捕捉动态过程且割裂了图文模态，限制了多模态统一理解与生成。

Method: 提出“Thinking with Video”范式，使用视频生成模型（如Sora-2）进行跨模态、时序连贯的推理，并构建VideoThinkBench基准测试，包含视觉中心和文本中心两类任务。

Result: Sora-2在视觉任务上媲美甚至超越SOTA VLMs（如Eyeballing Games），在文本任务上取得MATH 92%、MMMU 75.53%的准确率，且自一致性与上下文学习可进一步提升性能。

Conclusion: 视频生成模型具备成为统一多模态理解与生成模型的潜力，“Thinking with Video”是一种有前景的统一多模态推理范式。

Abstract: "Thinking with Text" and "Thinking with Images" paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have inherent limitations. (1)
Images capture only single moments and fail to represent dynamic processes or
continuous changes, and (2) The separation of text and vision as distinct
modalities, hindering unified multimodal understanding and generation. To
overcome these limitations, we introduce "Thinking with Video", a new paradigm
that leverages video generation models, such as Sora-2, to bridge visual and
textual reasoning in a unified temporal framework. To support this exploration,
we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
Furthermore, we systematically analyse the source of these abilities. We also
find that self-consistency and in-context learning can improve Sora-2's
performance. In summary, our findings demonstrate that the video generation
model is the potential unified multimodal understanding and generation model,
positions "thinking with video" as a unified multimodal reasoning paradigm.

</details>


### [53] [UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction](https://arxiv.org/abs/2511.04595)
*Chen Shi,Shaoshuai Shi,Xiaoyang Lyu,Chunyang Liu,Kehua Sheng,Bo Zhang,Li Jiang*

Main category: cs.CV

TL;DR: UniSplat提出了一种面向自动驾驶的通用前馈式3D重建框架，通过统一的潜在时空融合实现鲁棒的动态场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏、非重叠相机视图和复杂场景动态下表现不佳，难以实现高质量的动态场景重建。

Method: 构建3D潜在支架，利用预训练基础模型捕捉几何与语义上下文；引入高效的融合机制，在支架内进行跨时空信息整合；设计双分支解码器生成动态感知的高斯分布，并维护静态高斯的持久记忆以支持流式场景补全。

Result: 在真实世界数据集上实验表明，UniSplat在新视角合成方面达到最先进水平，且在超出原始相机覆盖范围的视角下仍能提供鲁棒、高质量的渲染结果。

Conclusion: UniSplat通过统一的潜在时空融合框架，有效解决了稀疏视图和动态场景下的3D重建难题，具备良好的泛化性和实用性。

Abstract: Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,
yet existing methods struggle with the joint challenges of sparse,
non-overlapping camera views and complex scene dynamics. We present UniSplat, a
general feed-forward framework that learns robust dynamic scene reconstruction
through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent
scaffold, a structured representation that captures geometric and semantic
scene context by leveraging pretrained foundation models. To effectively
integrate information across spatial views and temporal frames, we introduce an
efficient fusion mechanism that operates directly within the 3D scaffold,
enabling consistent spatio-temporal alignment. To ensure complete and detailed
reconstructions, we design a dual-branch decoder that generates dynamic-aware
Gaussians from the fused scaffold by combining point-anchored refinement with
voxel-based generation, and maintain a persistent memory of static Gaussians to
enable streaming scene completion beyond current camera coverage. Extensive
experiments on real-world datasets demonstrate that UniSplat achieves
state-of-the-art performance in novel view synthesis, while providing robust
and high-quality renderings even for viewpoints outside the original camera
coverage.

</details>


### [54] [PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning](https://arxiv.org/abs/2511.04601)
*Yicheng Xiao,Yu Chen,Haoxuan Ma,Jiale Hong,Caorui Li,Lingxiang Wu,Haiyun Guo,Jinqiao Wang*

Main category: cs.CV

TL;DR: 本文提出PixCLIP，通过结合视觉提示和长文本描述，提升CLIP在细粒度图文对齐上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型受限于文本编码器的长度限制，难以处理包含丰富细节的长文本，从而限制了其在细粒度图文对齐上的表现。同时，MLLM研究表明长文本有助于提升对齐质量，因此需要一种能同时增强视觉和文本细粒度处理能力的方法。

Method: 构建自动化标注流程生成像素级、长文本描述，建立包含150万样本的LongGRIT数据集；用大语言模型（LLM）替代CLIP原始文本编码器，并设计三分支的像素-文本对齐学习框架，实现任意粒度的图文局部对齐。

Result: PixCLIP在像素级交互和长文本处理方面取得突破，显著提升细粒度图文对齐性能，在多个任务上达到SOTA。

Conclusion: PixCLIP有效克服了CLIP在文本长度和细粒度对齐上的局限，通过融合视觉提示与长文本描述，实现了更精细的跨模态对齐，为后续研究提供了高质量数据集（LongGRIT）和新架构范式。

Abstract: While the Contrastive Language-Image Pretraining(CLIP) model has achieved
remarkable success in a variety of downstream vison language understanding
tasks, enhancing its capability for fine-grained image-text alignment remains
an active research focus. To this end, most existing works adopt the strategy
of explicitly increasing the granularity of visual information processing,
e.g., incorporating visual prompts to guide the model focus on specific local
regions within the image. Meanwhile, researches on Multimodal Large Language
Models(MLLMs) have demonstrated that training with long and detailed textual
descriptions can effectively improve the model's fine-grained vision-language
alignment. However, the inherent token length limitation of CLIP's text encoder
fundamentally limits CLIP to process more granular textual information embedded
in long text sequences. To synergistically leverage the advantages of enhancing
both visual and textual content processing granularity, we propose PixCLIP, a
novel framework designed to concurrently accommodate visual prompt inputs and
process lengthy textual descriptions. Specifically, we first establish an
automated annotation pipeline capable of generating pixel-level localized,
long-form textual descriptions for images. Utilizing this pipeline, we
construct LongGRIT, a high-quality dataset comprising nearly 1.5 million
samples. Secondly, we replace CLIP's original text encoder with the LLM and
propose a three-branch pixel-text alignment learning framework, facilitating
fine-grained alignment between image regions and corresponding textual
descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP
showcases breakthroughs in pixel-level interaction and handling long-form
texts, achieving state-of-the-art performance.

</details>


### [55] [Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality](https://arxiv.org/abs/2511.04615)
*Tushar Kataria,Shikha Dubey,Mary Bronner,Jolanta Jedrzkiewicz,Ben J. Brintz,Shireen Y. Elhabian,Beatrice S. Knudsen*

Main category: cs.CV

TL;DR: 提出了一种基于准确性的自动化框架来评估虚拟免疫组化（IHC）染色图像的质量，使用像素级染色准确性指标（如Dice、IoU、Hausdorff距离），发现传统图像保真度指标与实际染色准确性和病理学家评估相关性差，配对模型表现更优，并强调全切片图像（WSI）层面评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基于纹理和分布的图像质量评估指标无法准确反映虚拟IHC染色的准确性，缺乏无需人工标注的客观、可靠的评估方法。

Method: 采用颜色去卷积生成真实和虚拟IHC的棕染像素掩码，利用Dice、IoU和Hausdorff距离等指标量化染色准确性，对比十六种配对或非配对图像翻译模型在patch和WSI层面的表现。

Result: 传统指标（FID、PSNR、SSIM）与染色准确性和病理评估相关性差；配对模型（如PyramidPix2Pix、AdaptiveNCE）染色准确性最高；非配对扩散模型和GAN模型可靠性较低；WSI评估揭示了patch评估中无法发现的性能下降。

Conclusion: 该框架提供了一种可重复、基于准确性的虚拟IHC图像质量评估方法，对推动其在病理学常规应用中的转化至关重要。

Abstract: Deep learning models can generate virtual immunohistochemistry (IHC) stains
from hematoxylin and eosin (H&E) images, offering a scalable and low-cost
alternative to laboratory IHC. However, reliable evaluation of image quality
remains a challenge as current texture- and distribution-based metrics quantify
image fidelity rather than the accuracy of IHC staining. Here, we introduce an
automated and accuracy grounded framework to determine image quality across
sixteen paired or unpaired image translation models. Using color deconvolution,
we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by
each virtual IHC model. We use the segmented masks of real and virtual IHC to
compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly
quantify correct pixel - level labeling without needing expert manual
annotations. Our results demonstrate that conventional image fidelity metrics,
including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),
and structural similarity (SSIM), correlate poorly with stain accuracy and
pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE
achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based
models are less reliable in providing accurate IHC positive pixel labels.
Moreover, whole-slide images (WSI) reveal performance declines that are
invisible in patch-based evaluations, emphasizing the need for WSI-level
benchmarks. Together, this framework defines a reproducible approach for
assessing the quality of virtual IHC models, a critical step to accelerate
translation towards routine use by pathologists.

</details>


### [56] [NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment](https://arxiv.org/abs/2511.04628)
*Kylie Cancilla,Alexander Moore,Amar Saini,Carmen Carrano*

Main category: cs.CV

TL;DR: 提出了一种基于时序感知的无参考视频质量评估模型，通过合成退化数据训练，直接预测全参考指标，在无需人工标注和参考视频的情况下实现了优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频质量评估方法受限于需要参考视频或依赖昂贵的人类主观评分，且多数无参考方法忽略关键的时序信息。

Method: 利用DAVIS数据集的合成退化版本，训练一个具有时序感知能力的卷积架构，以在推理时无需参考视频的情况下直接预测LPIPS、PSNR、SSIM等全参考指标。

Result: 该流式模型在多种退化类型上优于图像基线模型，并比BRISQUE更好地与全参考指标相关。

Conclusion: 时序建模对可扩展的无参考视频质量评估至关重要，所提方法在真实场景中具有实用价值。

Abstract: Video quality assessment (VQA) is vital for computer vision tasks, but
existing approaches face major limitations: full-reference (FR) metrics require
clean reference videos, and most no-reference (NR) models depend on training on
costly human opinion labels. Moreover, most opinion-unaware NR methods are
image-based, ignoring temporal context critical for video object detection. In
this work, we present a scalable, streaming-based VQA model that is both
no-reference and opinion-unaware. Our model leverages synthetic degradations of
the DAVIS dataset, training a temporal-aware convolutional architecture to
predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without
references at inference. We show that our streaming approach outperforms our
own image-based baseline by generalizing across diverse degradations,
underscoring the value of temporal modeling for scalable VQA in real-world
vision systems. Additionally, we demonstrate that our model achieves higher
correlation with full-reference metrics compared to BRISQUE, a widely-used
opinion-aware image quality assessment baseline, validating the effectiveness
of our temporal, opinion-unaware approach.

</details>


### [57] [Polarization-resolved imaging improves eye tracking](https://arxiv.org/abs/2511.04652)
*Mantas Žurauskas,Tom Bu,Sanaz Alali,Beyza Kalkanli,Derek Shi,Fernando Alamos,Gauresh Pandit,Christopher Mei,Ali Behrooz,Ramin Mirjalili,Dave Stronks,Alexander Fix,Dmitri Model*

Main category: cs.CV

TL;DR: 本文提出了一种基于偏振分辨近红外成像的偏振增强型眼动追踪（PET）系统，利用眼部组织反射光的偏振状态提供额外光学对比度，结合卷积神经网络显著降低了眼动追踪误差，尤其在存在眼睑遮挡、眼距变化和瞳孔大小变化等挑战下表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统强度图像在眼动追踪中缺乏足够的可追踪特征，尤其是在复杂实际条件下精度受限，因此需要引入新的光学对比机制以提升追踪鲁棒性和准确性。

Method: 采用偏振滤波阵列相机与线性偏振近红外光源组成的PET系统，获取眼球表面的偏振信息，并使用卷积神经网络模型对偏振数据进行训练和眼动估计。

Result: 在346名参与者的数据上验证，PET系统相比强度基线模型在正常条件和多种干扰条件下将中位95%绝对注视误差降低了10-16%。

Conclusion: 偏振成像能有效增强眼动追踪性能，揭示了光-组织偏振效应在人机交互中的实用价值，使PET成为未来可穿戴设备中一种简单且鲁棒的感知模态。

Abstract: Polarization-resolved near-infrared imaging adds a useful optical contrast
mechanism to eye tracking by measuring the polarization state of light
reflected by ocular tissues in addition to its intensity. In this paper we
demonstrate how this contrast can be used to enable eye tracking. Specifically,
we demonstrate that a polarization-enabled eye tracking (PET) system composed
of a polarization--filter--array camera paired with a linearly polarized
near-infrared illuminator can reveal trackable features across the sclera and
gaze-informative patterns on the cornea, largely absent in intensity-only
images. Across a cohort of 346 participants, convolutional neural network based
machine learning models trained on data from PET reduced the median
95th-percentile absolute gaze error by 10--16\% relative to capacity-matched
intensity baselines under nominal conditions and in the presence of eyelid
occlusions, eye-relief changes, and pupil-size variation. These results link
light--tissue polarization effects to practical gains in human--computer
interaction and position PET as a simple, robust sensing modality for future
wearable devices.

</details>


### [58] [Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts](https://arxiv.org/abs/2511.04655)
*Ellis Brown,Jihan Yang,Shusheng Yang,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: 本文提出了一种诊断和去偏框架，用于检测和减轻多模态基准测试中的非视觉偏差，通过在纯文本输入上进行测试集压力测试（TsT）和迭代偏差剪枝（IBP），发现现有视觉中心基准存在严重漏洞，并构建了更鲁棒的VSI-Bench-Debiased。


<details>
  <summary>Details</summary>
Motivation: 现有许多多模态大模型基准可在缺乏真正视觉理解的情况下通过语言先验和表面模式被“作弊”，尤其影响本应依赖视觉输入的视觉中心任务，因此需要更严格的诊断原则来评估和改进基准的可靠性。

Method: 提出Test-set Stress-Test（TsT）方法：使用k折交叉验证在测试集的纯文本输入上微调大语言模型，识别可利用的捷径并为样本分配偏差分数；辅以基于手工特征的随机森林进行快速可解释审计；进一步提出Iterative Bias Pruning（IBP）去偏方法，过滤高偏差样本。

Result: 在VSI-Bench、CV-Bench、MMMU和VideoMME四个基准上均发现普遍存在的非视觉偏差；应用该框架构建VSI-Bench-Debiased，显著降低仅靠文本即可解决的比例，扩大了视觉盲模型与真实模型之间的性能差距。

Conclusion: 基准设计者应在发布前主动“攻击”自己的测试集，通过系统性诊断和去偏提升多模态基准的鲁棒性和有效性，防止模型通过非视觉捷径虚假地表现优越。

Abstract: Robust benchmarks are crucial for evaluating Multimodal Large Language Models
(MLLMs). Yet we find that models can ace many multimodal benchmarks without
strong visual understanding, instead exploiting biases, linguistic priors, and
superficial patterns. This is especially problematic for vision-centric
benchmarks that are meant to require visual inputs. We adopt a diagnostic
principle for benchmark design: if a benchmark can be gamed, it will be.
Designers should therefore try to ``game'' their own benchmarks first, using
diagnostic and debiasing procedures to systematically identify and mitigate
non-visual biases. Effective diagnosis requires directly ``training on the test
set'' -- probing the released test set for its intrinsic, exploitable patterns.
  We operationalize this standard with two components. First, we diagnose
benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.
Our primary diagnostic tool involves fine-tuning a powerful Large Language
Model via $k$-fold cross-validation on exclusively the non-visual, textual
inputs of the test set to reveal shortcut performance and assign each sample a
bias score $s(x)$. We complement this with a lightweight Random Forest-based
diagnostic operating on hand-crafted features for fast, interpretable auditing.
Second, we debias benchmarks by filtering high-bias samples using an
``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four
benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive
non-visual biases. As a case study, we apply our full framework to create
VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider
vision-blind performance gap than the original.

</details>


### [59] [SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding](https://arxiv.org/abs/2511.04668)
*Ellis Brown,Arijit Ray,Ranjay Krishna,Ross Girshick,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: 本文提出了SIMS-V，一个利用3D模拟器生成空间丰富视频训练数据的系统性框架，以提升多模态语言模型在现实世界中的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态语言模型在时空空间推理方面表现不佳，且真实世界视频数据难以获得精确的空间标注，限制了模型训练。

Method: 提出SIMS-V框架，利用3D模拟器的先验信息生成具有丰富空间标注的视频数据，并通过系统性消融实验研究问题类型、组合和规模对现实世界迁移效果的影响。

Result: 仅用25K个模拟样本微调的7B参数视频大模型，性能超过更大的72B基线模型，并在现实空间推理基准上达到与专有模型相当的水平；同时保持通用视频理解能力，显著提升具身和现实空间任务表现。

Conclusion: SIMS-V通过最小化的三类问题（度量测量、视角依赖推理和时序追踪）实现了高效训练和强泛化能力，证明了模拟数据在发展可迁移空间智能方面的有效性。

Abstract: Despite impressive high-level video comprehension, multimodal language models
struggle with spatial reasoning across time and space. While current spatial
training approaches rely on real-world video data, obtaining diverse footage
with precise spatial annotations remains a bottleneck. To alleviate this
bottleneck, we present SIMS-V -- a systematic data-generation framework that
leverages the privileged information of 3D simulators to create spatially-rich
video training data for multimodal language models. Using this framework, we
investigate which properties of simulated data drive effective real-world
transfer through systematic ablations of question types, mixes, and scales. We
identify a minimal set of three question categories (metric measurement,
perspective-dependent reasoning, and temporal tracking) that prove most
effective for developing transferable spatial intelligence, outperforming
comprehensive coverage despite using fewer question types. These insights
enable highly efficient training: our 7B-parameter video LLM fine-tuned on just
25K simulated examples outperforms the larger 72B baseline and achieves
competitive performance with proprietary models on rigorous real-world spatial
reasoning benchmarks. Our approach demonstrates robust generalization,
maintaining performance on general video understanding while showing
substantial improvements on embodied and real-world spatial tasks.

</details>


### [60] [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670)
*Shusheng Yang,Jihan Yang,Pinzhi Huang,Ellis Brown,Zihao Yang,Yue Yu,Shengbang Tong,Zihan Zheng,Yifan Xu,Muhan Wang,Daohan Lu,Rob Fergus,Yann LeCun,Li Fei-Fei,Saining Xie*

Main category: cs.CV

TL;DR: 本文提出“超感知”（supersensing）作为推动多模态智能发展的新范式，涵盖语义感知、事件认知、隐式3D空间理解与预测性世界建模四个阶段。为评估该能力，作者构建了VSI-SUPER基准（含VSR和VSC任务），并发现当前模型即使在大规模数据下表现仍有限，表明单纯扩大规模不足。最后提出“预测性感知”方法，通过自监督预测潜在帧的误差来驱动记忆与事件分割，在VSI-SUPER上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型多局限于被动、任务导向的反应模式，且依赖暴力扩展上下文长度，难以实现真正的空间理解与持续感知。作者认为需要从单纯‘看见’转向‘主动预测与组织经验’的智能范式，以推动多模态系统具备类人水平的空间与事件认知能力。

Method: 提出空间超感知的四阶段框架，并构建VSI-SUPER基准测试（包括长视野视觉空间回忆VSR和连续视觉空间计数VSC）。通过构建大规模数据集VSI-590K并训练Cambrian-S模型验证数据扩展的影响；进一步提出基于自监督的下一潜在帧预测器，利用预测误差（surprise）驱动记忆更新与事件分割，实现预测性感知机制。

Result: Cambrian-S在VSI-Bench上提升30%，但在VSI-SUPER上表现仍有限，说明规模不足以解决空间超感知任务。所提预测性感知方法在VSI-SUPER上显著优于主流闭源基线，验证了预测机制对记忆与事件分割的有效性。

Conclusion: 真正的多模态智能需超越被动识别与长上下文堆砌，转向具有预测能力的主动感知系统。空间超感知是关键路径，而预测误差可作为驱动智能体选择、组织和记忆经验的核心机制。

Abstract: We argue that progress in true multimodal intelligence calls for a shift from
reactive, task-driven systems and brute-force long context towards a broader
paradigm of supersensing. We frame spatial supersensing as four stages beyond
linguistic-only understanding: semantic perception (naming what is seen),
streaming event cognition (maintaining memory across continuous experiences),
implicit 3D spatial cognition (inferring the world behind pixels), and
predictive world modeling (creating internal models that filter and organize
information). Current benchmarks largely test only the early stages, offering
narrow coverage of spatial cognition and rarely challenging models in ways that
require true world modeling. To drive progress in spatial supersensing, we
present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
recall) and VSC (continual visual spatial counting). These tasks require
arbitrarily long video inputs yet are resistant to brute-force context
expansion. We then test data scaling limits by curating VSI-590K and training
Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
indicating that scale alone is insufficient for spatial supersensing. We
propose predictive sensing as a path forward, presenting a proof-of-concept in
which a self-supervised next-latent-frame predictor leverages surprise
(prediction error) to drive memory and event segmentation. On VSI-SUPER, this
approach substantially outperforms leading proprietary baselines, showing that
spatial supersensing requires models that not only see but also anticipate,
select, and organize experience.

</details>


### [61] [InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation](https://arxiv.org/abs/2511.04675)
*Jinlai Liu,Jian Han,Bin Yan,Hui Wu,Fengda Zhu,Xing Wang,Yi Jiang,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: InfinityStar是一个统一的时空自回归框架，用于高分辨率图像和动态视频合成，能够高效生成720p视频，并在多项指标上优于现有自回归和部分扩散模型。


<details>
  <summary>Details</summary>
Motivation: 为了实现高效且高质量的视频生成，克服现有自回归和扩散模型在分辨率和生成速度上的局限性。

Method: 提出了一种纯离散的、统一的时空自回归架构，通过单一模型联合捕捉空间和时间依赖，支持多种生成任务。

Result: 在VBench上得分为83.74，显著优于其他自回归模型，生成5秒720p视频的速度比主流扩散方法快约10倍。

Conclusion: InfinityStar是首个能生成工业级720p视频的离散自回归视频生成器，具备高效性和多任务兼容性，推动了高质量视频生成的发展。

Abstract: We introduce InfinityStar, a unified spacetime autoregressive framework for
high-resolution image and dynamic video synthesis. Building on the recent
success of autoregressive modeling in both vision and language, our purely
discrete approach jointly captures spatial and temporal dependencies within a
single architecture. This unified design naturally supports a variety of
generation tasks such as text-to-image, text-to-video, image-to-video, and long
interactive video synthesis via straightforward temporal autoregression.
Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench,
outperforming all autoregressive models by large margins, even surpassing some
diffusion competitors like HunyuanVideo. Without extra optimizations, our model
generates a 5s, 720p video approximately 10x faster than leading
diffusion-based methods. To our knowledge, InfinityStar is the first discrete
autoregressive video generator capable of producing industrial level 720p
videos. We release all code and models to foster further research in efficient,
high-quality video generation.

</details>


### [62] [Tracking and Understanding Object Transformations](https://arxiv.org/abs/2511.04678)
*Yihong Sun,Xinyu Yang,Jennifer J. Sun,Bharath Hariharan*

Main category: cs.CV

TL;DR: 本文提出了Track Any State任务，旨在通过物体状态变化进行跟踪，并引入了新的基准数据集VOST-TAS和零样本系统TubeletGraph，能够恢复变换后的缺失物体并生成描述状态演变的图。


<details>
  <summary>Details</summary>
Motivation: 现有方法在物体外观发生显著变化时容易丢失目标物体，难以有效跟踪经历状态变换的真实世界物体。

Method: 提出TubeletGraph系统，通过语义和邻近先验识别被忽略的轨迹并判断是否合并，进而推理新增轨迹并生成描述每次状态变化的图结构。

Result: 在VOST-TAS数据集上实现了最先进的跟踪性能，同时展现出对复杂物体变换的时间定位和语义推理能力。

Conclusion: TubeletGraph能有效应对物体状态变换带来的挑战，在跟踪性能和对物体动态的理解方面均表现出优越性。

Abstract: Real-world objects frequently undergo state transformations. From an apple
being cut into pieces to a butterfly emerging from its cocoon, tracking through
these changes is important for understanding real-world objects and dynamics.
However, existing methods often lose track of the target object after
transformation, due to significant changes in object appearance. To address
this limitation, we introduce the task of Track Any State: tracking objects
through transformations while detecting and describing state changes,
accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we
present TubeletGraph, a zero-shot system that recovers missing objects after
transformation and maps out how object states are evolving over time.
TubeletGraph first identifies potentially overlooked tracks, and determines
whether they should be integrated based on semantic and proximity priors. Then,
it reasons about the added tracks and generates a state graph describing each
observed transformation. TubeletGraph achieves state-of-the-art tracking
performance under transformations, while demonstrating deeper understanding of
object transformations and promising capabilities in temporal grounding and
semantic reasoning for complex object transformations. Code, additional
results, and the benchmark dataset are available at
https://tubelet-graph.github.io.

</details>


### [63] [Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping](https://arxiv.org/abs/2511.04680)
*Rafe Loya,Andrew Hamara,Benjamin Estell,Benjamin Kilpatrick,Andrew C. Freeman*

Main category: cs.CV

TL;DR: 本文探讨了自动生成多张具有美学吸引力的不同裁剪图像的问题，提出了一个包含277张图像及人工标注的数据集，并评估了结合图像分割预处理的单裁剪模型的效果。


<details>
  <summary>Details</summary>
Motivation: 现代社交媒体应用需要为同一图像生成多个不同的高质量裁剪版本，但现有研究主要集中在单一裁剪上，缺乏对多裁剪美学质量的关注。

Method: 通过引入图像分割算法作为预处理步骤，结合现有的单裁剪模型，生成多个不同且具美学吸引力的裁剪区域，并利用新构建的数据集进行评估。

Result: 构建了一个包含277张图像和人类标注的公开数据集，并验证了所提方法在生成多裁剪结果上的有效性。

Conclusion: 结合图像分割与单裁剪模型的方法能够有效生成多个美观的图像裁剪，为自动多裁剪研究提供了新的数据和思路。

Abstract: Automatic image cropping is a method for maximizing the human-perceived
quality of cropped regions in photographs. Although several works have proposed
techniques for producing singular crops, little work has addressed the problem
of producing multiple, distinct crops with aesthetic appeal. In this paper, we
motivate the problem with a discussion on modern social media applications,
introduce a dataset of 277 relevant images and human labels, and evaluate the
efficacy of several single-crop models with an image partitioning algorithm as
a pre-processing step. The dataset is available at
https://github.com/RafeLoya/carousel.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [64] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 提出一种新方法，通过大五人格特质从Transformer模型中提取隐藏状态，利用低秩子空间发现技术识别不同架构下的最优层，并实现对语言模型人格特征的精确控制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成时表现出隐式人格，但如何可靠地控制或对齐这些人格特质仍是一个开放问题。现有研究缺乏有效机制来操纵模型行为，且心理构念与模型内部表征之间的关系尚不明确。

Method: 提出一个新管道：使用大五人格特质（OCEAN）提取Transformer层的隐藏状态激活，应用低秩子空间发现方法，识别跨模型架构的特质特定最优层，并通过动态层选择的灵活引导框架实现行为调控。

Result: 发现人格特质占据低秩共享子空间，这些潜在结构可通过精细扰动转化为有效的引导机制，在不影响流畅性、多样性及模型通用能力的前提下，实现对人格表达的精准控制。

Conclusion: 该工作建立了心理学理论与模型对齐之间的桥梁，证明了人格特征可在神经表示层面被识别和操作，为构建可控、个性化的人格感知大模型提供了可行路径。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [65] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: 本文提出了TextualVerifier，一个基于大语言模型的文本验证框架，用于增强TextGrad在无显式数值梯度情况下的推理有效性。该框架通过思维链分解、变体生成、多数投票和共识聚合四阶段流程，非侵入式集成到TextGrad中，并在多个基准上显著提升了推理正确率。


<details>
  <summary>Details</summary>
Motivation: TextGrad虽能实现基于文本的自动微分优化，但缺乏确保推理有效性的自我验证机制，限制了其在复杂决策中的可靠性。因此，需要一种无需数值梯度的自验证方法来提升系统可信度。

Method: 提出TextualVerifier框架，采用四阶段流程：思维链分解、变体生成、多数投票与共识聚合；利用大语言模型进行多路径推理并基于多数投票判断有效性，非侵入式集成于TextGrad的损失函数与结果验证环节。

Result: 在PRM800K上单独评估时，推理步骤有效性提升29%；与TextGrad集成后，在GPQA-Diamond、MMLU-ML和MMLU-CP上分别提升2.2、10.71和3.92个百分点，平均增加5.9次LLM调用；统计检验显示p < 0.001，结果显著。

Conclusion: TextualVerifier是首个为TextGrad设计的基于LLM的自验证框架，无需数值梯度即可提升文本优化系统的推理可靠性，为文本驱动的AI系统验证提供了新方向。

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [66] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 本文提出了一个扩展的希腊方言数据集（GRDD+），包含10种方言，共637万词，是目前规模最大、方言种类最多的希腊语数据集。作者利用该数据集对多种大语言模型进行了微调实验，评估高质量方言数据对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的希腊方言数据集覆盖范围有限，缺乏足够的方言多样性与数据规模，限制了大语言模型在方言处理上的表现。因此，构建一个更全面、更大规模的方言数据集成为必要。

Method: 在原有GRDD数据集基础上，新增来自克里特、塞浦路斯、本都、北希腊等地区的数据，并加入六种新方言（如格雷科-科西嘉语、 Griko语、马尼奥特语等），构建GRDD+数据集。随后对三种模型架构（Llama-3-8B、Llama-3.1-8B、Krikri-8B）进行微调，并与前沿模型（Claude-3.7-Sonnet、Gemini-2.5、ChatGPT-5）进行对比实验。

Result: GRDD+成为首个涵盖10种希腊方言、总规模达6,374,939词的数据集。实验表明，使用该高质量方言数据进行微调显著提升了语言模型在方言理解与生成任务上的表现。

Conclusion: GRDD+为希腊多方言研究提供了重要资源，实验证明高质量、多样化的方言数据能有效提升大语言模型在低资源语言变体上的性能，具有重要的应用与研究价值。

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [67] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: PLLuM是首个专为波兰语设计的大型开源语言模型系列，由波兰主要研究机构联合开发，包含新的预训练语料库、指令数据集和偏好优化数据集，并集成负责任AI框架，旨在推动开放研究和增强波兰的自主AI技术。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要集中于英语，对其他语言支持有限，缺乏高质量、透明且符合本地文化需求的语言模型，因此需要开发针对波兰语的专用模型。

Method: 构建了1400亿token的波兰语文本语料库用于预训练，7.7万条定制指令数据集和10万条偏好优化数据集；采用基础模型与指令微调变体相结合的架构，结合负责任AI框架（含严格数据治理及混合式输出修正与安全过滤模块）进行训练和对齐。

Result: 成功开发出PLLUM模型系列，并在公共管理领域的下游任务中验证了其有效性，展示了其在实际应用中的潜力。

Conclusion: PLLuM填补了非英语大模型的空白，通过公开发布促进开放研究，增强波兰在人工智能领域的技术主权，并为多语言大模型发展提供参考。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [68] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: 提出了一种名为STARS的解码时对齐算法，通过分段采样、评分和拒绝/接受机制，在提升大语言模型与人类价值观对齐质量的同时显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法如微调计算成本高，而推理时方法如Best-of-N需要不切实际的计算量，因此需要一种更高效且有效的对齐方法。

Method: 提出STARS：在解码时逐段生成固定长度的token片段，利用奖励模型进行评分，并通过拒绝采样决定是否接受该片段，从而在生成过程中实现早期纠偏。

Result: 在六个大语言模型上实验表明，STARS在胜率指标上优于监督微调最多14.9个百分点，优于DPO最多4.3个百分点，同时与强Best-of-N基线相当。

Conclusion: STARS提供了一种可泛化、鲁棒且高效的LLM对齐新范式，是传统微调和全序列排序方法的有力替代方案。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [69] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 本文探索了三种多智能体LLM流水线，用于提升小模型在Text-to-SQL任务上的性能，实验表明多智能体协作能显著提高执行准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在从自然语言生成SQL方面表现不佳，尤其是面对大规模模式和复杂推理时；而小型高效模型常被忽视。

Method: 设计并评估三种多智能体LLM流水线：多智能体讨论、Planner-Coder、Coder-Aggregator，在Bird-Bench Mini-Dev数据集上对多种开源模型进行系统性基准测试。

Result: 多智能体讨论可使小模型（如Qwen2.5-7b-Instruct）执行准确率提升高达10.6%；LLM Reasoner-Coder流水线效果最佳，使用DeepSeek-R1-32B和QwQ-32B作为规划器将Gemma 3 27B IT的准确率从52.4%提升至56.4%。

Conclusion: 多智能体协作框架能有效提升小模型在Text-to-SQL任务中的表现，为高效、实用的SQL生成提供了可行路径。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [70] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 提出一种基于二元决策的高效多标签文本分类方法，通过将分类任务分解为多个独立的是/否查询，并结合前缀缓存机制，在不损失精度的情况下显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统的多标签分类方法在大语言模型上生成所有标签时效率较低，尤其在处理短文本时存在计算开销大的问题，因此需要一种更高效的替代方案。

Method: 将多标签分类任务重新定义为一系列二元（是/否）决策问题，每个标签维度独立查询，并利用前缀缓存优化推理；通过LLM-to-SLM蒸馏，使用DeepSeek-V3生成多标注数据来微调小型模型（如HerBERT-Large、PLLuM-8B等）。

Result: 微调后的模型在训练中见过的情感维度上显著优于零样本基线，且该方法在保持准确率的同时大幅提升了推理速度和效率。

Conclusion: 将多标签分类分解为二元查询，结合知识蒸馏与缓存感知推理，构成了一种可扩展、高效的LLM-based分类框架，具有跨领域的通用性。

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [71] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 本文研究了三种低资源语言（Afan Oromo、Amharic 和 Tigrinya）的机器翻译数据集的质量，重点关注其中的性别表征问题。研究发现数据集中存在男性偏向以及对女性的有害刻板印象，且数据量最大的语言问题最严重，表明数量不等于质量。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言的NLP研究中，过度追求数据规模而忽视质量可能导致技术性能差和传播社会偏见。因此，有必要评估现有数据集的质量，特别是性别表征的公平性。

Method: 分析三种低资源语言的机器翻译训练数据和基准数据集，从领域分布和性别表征（人名、动词语法性别、文本内容）两个方面进行系统评估。

Result: 发现训练数据以政治和宗教内容为主，而基准数据集中在新闻、健康和体育；数据集中普遍存在男性主导现象，并包含针对女性的有害和毒性描述，且数据量越大的语言此类问题越突出。

Conclusion: 数据质量而非数量才是关键，当前低资源语言数据集存在显著的性别偏见和有害内容，需在数据收集阶段就引入质量审查和偏见缓解机制。

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [72] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 本文提出了一种计算图灵测试框架，用于评估大语言模型（LLM）生成文本与人类语言的接近程度，并比较了九种开源LLM在不同校准策略下的表现，发现即使经过校准，LLM输出仍明显区别于人类文本，且提升类人程度常以牺牲语义保真度为代价。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM模拟人类行为的研究依赖未经充分验证的假设，且缺乏可靠工具来评估其生成文本的真实性，尤其是基于人类判断的评估方法存在主观性和不可靠性，因此需要更客观、可扩展的验证框架。

Method: 提出一种结合聚合指标（如BERT-based可检测性和语义相似性）与可解释语言特征（如风格标记和主题模式）的计算图灵测试框架，并系统评估九个开源LLM在五种校准策略（包括微调、风格提示和上下文检索）下再现X、Bluesky和Reddit用户交互的能力。

Result: 研究发现，即使经过校准，LLM生成文本在情感表达和情绪基调上仍显著区别于人类文本；指令微调模型表现不如基础模型，增大模型规模也未提升类人程度；同时存在类人化与语义保真度之间的权衡。

Conclusion: 当前LLM在模拟人类交流方面存在明显局限，需谨慎使用；本文提供了可扩展的验证与校准框架，为未来研究提供了方法论基础。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [73] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 提出了一种名为Graph-Retrieved Adaptive Decoding (GRAD) 的解码方法，利用语料库衍生的稀疏token转移图，在不解码模型的情况下有效减少大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示或检索外部知识的方法存在脆弱性、领域敏感性或高检索成本，因此需要一种轻量且鲁棒的幻觉缓解方法。

Method: 通过在少量检索到的语料上累积下一个token的logits构建稀疏token转移图，并在解码时将图中检索到的logits与模型原始logits自适应融合，以增强生成内容的事实性。

Result: 在多个问答基准上，GRAD相比贪婪解码最高提升9.7%的内在准确率，降低8.6%的幻觉率，提高6.9%的正确性，并在真实性-信息量综合评分中表现最优。

Conclusion: GRAD提供了一种无需重训练、轻量可插拔的生成控制方法，证明了基于语料库级token转移的统计证据能有效引导LLM生成更真实、可验证的内容。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [74] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 在迭代指代游戏中，研究人类与视觉-语言模型在不同上下文条件下的表现，发现相关上下文显著提升模型性能，但抽象指代的少样本任务仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 探究模型在多轮语言环境中进行上下文敏感的语用推理能力，尤其是在指代消解任务中的表现。

Method: 通过迭代指代游戏实验，比较人类与视觉-语言模型在上下文的数量、顺序和相关性变化下的表现。

Result: 无相关上下文时，模型表现高于随机但远差于人类；有相关上下文时，模型表现随轮次显著提升。

Conclusion: 相关上下文对模型性能至关重要，但当前模型在少样本、抽象指代任务上仍有局限。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [75] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: RUST-BENCH是一个新的基准，用于评估大语言模型在真实、复杂表格上的推理能力，涵盖规模、异构性、领域特异性和推理复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有表格推理基准主要测试小型、同质的表格，无法充分反映现实世界数据的复杂性，也无法全面评估大语言模型的推理能力。

Method: 构建了一个包含7966个问题、来自2031个真实表格的数据集，覆盖科学（NSF资助记录）和体育（NBA统计数据）两个领域，评估模型在大规模、异构、领域特定和多跳推理任务上的表现。

Result: 实验表明，现有大语言模型在处理异构模式和复杂多跳推理时表现不佳，暴露出当前架构和提示策略的局限性。

Conclusion: RUST-BENCH为推进表格推理研究提供了一个具有挑战性的新测试平台。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [76] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 提出基于26亿条美国推文的“人类繁荣地理指数”（HFGI），利用大语言模型分析多维度幸福感，提供高时空分辨率的社会福祉数据。


<details>
  <summary>Details</summary>
Motivation: 现有衡量人类繁荣的指标缺乏足够的时空精细度，且多依赖传统调查方法，难以全面反映社会福祉的动态变化。

Method: 基于2013-2023年约26亿条地理标记的美国推文，使用微调的大语言模型对48个与哈佛全球繁荣研究框架一致的指标进行分类，并加入对移民态度和腐败感知的分析，生成县和州级月度、年度数据。

Result: 构建了具有高时空分辨率的人类繁荣相关话语数据集，验证显示该指数能准确反映潜在构念，并与现有指标呈现预期相关性。

Conclusion: HFGI为研究美国过去十年的社会福祉、不平等和社会变迁提供了跨学科、高分辨率的新工具，揭示了社交媒体话语中人类繁荣的动态。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [77] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 本文提出通过向量翻译在多模型间建立潜在语义桥梁，实现跨模型的语义直接交换，提升多智能体系统中的信息传递效率与协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多智能体交互中仅通过离散token传递信息，丢失大量潜在语义，限制了信息传输效率并增加计算开销。

Method: 设计一种双编码器翻译器，在Llama-2-7B与Mistral-7B-Instruct之间学习表示空间的映射关系，并以30%的混合强度注入翻译后的向量以引导目标模型生成。

Result: 实现了平均0.538的余弦对齐度，双向评估显示2.01:1的传输不对称性，表明通用模型比指令调优模型更具可迁移性。

Conclusion: 跨模型潜在通信是可行的，且通过保守注入可在保持计算稳定的同时实现语义共享，为构建协同AI系统提供了新路径。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [78] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 本文提出了一种将溯因推理整合到检索增强型大语言模型中的框架，通过检测证据不足、生成候选前提并进行一致性与合理性验证，提升了回答准确性和推理可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）系统在检索到的证据不完整时容易失败，导致推理过程中出现空白，因此需要一种能够填补这些空白的机制。

Method: 提出一个包含三个步骤的框架：检测证据不足、生成可能的缺失前提，并通过一致性和合理性检查来验证这些前提。

Result: 在溯因推理和多跳问答基准上的实验表明，该方法提高了答案准确性和推理过程的忠实性。

Conclusion: 将溯因推理引入RAG系统是一种增强其鲁棒性和可解释性的有前景方向。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [79] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 提出了一种弱监督的WST模型，能够在高错误率的转录文本下保持良好的语音识别性能，优于现有的CTC-based方法。


<details>
  <summary>Details</summary>
Motivation: RNN-T在端到端语音识别中依赖大量高质量标注数据，获取成本高，因此需要一种能容忍转录错误的弱监督方法。

Method: 设计了一个灵活的训练图结构，使WST无需置信度估计或辅助预训练模型即可鲁棒地处理转录错误。

Result: 在合成和工业数据集上实验表明，即使转录错误率达到70%，WST仍能保持良好性能，且 consistently 优于BTC和OTC等现有方法。

Conclusion: WST具有在真实场景ASR任务中应用的实用性和鲁棒性，未来将开源实现。

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [80] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 提出T-FIX基准，用于评估大模型在知识密集型领域中生成解释与领域专家判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注解释的合理性或内部一致性，无法反映解释内容是否符合专家直觉，特别是在专业领域中需要专家级推理的场景。

Method: 与多个领域的专家合作，构建覆盖七个知识密集型领域的T-FIX基准，并开发新指标来衡量大模型解释与专家判断的对齐程度。

Result: 建立了T-FIX基准和新的评估指标，能够更准确地衡量LLM生成解释在专业领域中的专家对齐水平。

Conclusion: T-FIX为知识密集型应用中的解释质量提供了更贴近实际需求的评估标准，强调了专家对齐作为解释质量的重要维度。

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [81] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种名为PoK的框架，结合知识规划与对比时序检索，提升大语言模型在时序知识图谱问答中的推理精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理时间敏感问题时难以充分理解复杂的时间约束语义，且大语言模型存在幻觉和知识缺失问题，限制了其在时序推理中的表现。

Method: 提出Plan of Knowledge模块，将复杂时序问题分解为子目标序列，并构建带有对比检索机制的时序知识库（TKS），实现语义和时间对齐的事实检索，结合结构化规划与知识检索进行推理。

Result: 在四个基准TKGQA数据集上的实验表明，PoK显著提升了检索精度和推理准确性，最高超越现有最先进方法达56.0%。

Conclusion: PoK通过结构化规划与对比检索的有效结合，增强了大语言模型在时序知识图谱问答中的事实一致性与时序推理能力。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [82] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 本文比较了人类与大语言模型在情感词汇联想上的行为差异，发现两者联想有一定重叠，但大语言模型的联想更可预测、创造性较低，并倾向于放大情感强度。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否能像人类一样形成概念间的联想，特别是在情感词汇上的表现，以理解其创造力和认知模拟能力。

Method: 通过对比人类被试对情感词汇的联想反应与大语言模型生成的联想，分析其相似性、情感强度变化及创造性水平。

Result: 人类与大语言模型的联想重叠程度中等；大语言模型的联想更具可预测性，创造性较低，并倾向于放大输入词的情感负荷。

Conclusion: 大语言模型在词汇联想上与人类有一定程度的相似性，但在创造性和情感表达方面存在系统性差异，显示出其生成过程更受训练数据模式驱动而非真实情感体验。

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [83] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 本研究通过大规模训练数据和跨机构基准测试，提升了基于transformer的放射学报告去标识化模型的性能，并在PHI检测上超越了现有的学术和商业系统。


<details>
  <summary>Details</summary>
Motivation: 为了提高放射学报告中受保护健康信息（PHI）自动去标识化的准确性和泛化能力，需要扩展模型规模并评估其在不同机构数据上的表现。

Method: 基于先进的transformer模型，在斯坦福大学的两个大型标注放射学语料库上进行微调，引入新的PHI类别（AGE），并在宾夕法尼亚大学的数据集上测试；同时评估合成PHI生成的稳定性及与商业系统的性能对比。

Result: 模型在宾夕法尼亚大学和斯坦福大学测试集上的F1分数分别为0.973和0.996，优于先前的最先进模型；在合成数据上F1达0.959，且显著优于所有商业系统（0.960 vs. 0.632–0.754）。

Conclusion: 基于多样化放射学数据训练的transformer模型在PHI检测方面优于现有学术和商业系统，为安全的临床文本处理建立了新基准。

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [84] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 本文研究了在极限情况下的语言识别问题，引入了k个猜测列表的学习模型，并给出了可识别语言集合的精确刻画：一个语言集合可以被k-列表识别当且仅当它能分解为k个各自可被单列表识别的子集。同时证明了在统计设定下，k-列表可识别性对应指数收敛速率，否则无法实现任何趋于零的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 受近期语言生成问题取得的积极成果启发，重新审视经典的语言识别问题，探索通过允许学习者在每一步输出k个猜测来提升识别能力的可能性。

Method: 基于Angluin对单列表识别的刻画，提出递归形式的k-列表识别条件，并利用该条件进行理论分析；进一步在独立同分布的数据流设定下研究识别的收敛速率。

Result: 给出了k-列表极限可识别语言集合的精确特征描述；证明了其等价于将原集合分解为k个传统可识别子集之和；在统计设定下，k-列表可识别性意味着指数收敛速度，反之则无法实现趋于零的收敛速度。

Conclusion: k-列表识别框架扩展了经典语言识别的边界，提供了更灵活的识别机制，并建立了识别能力与收敛速度之间的本质联系。

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [85] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 本文研究了批量提示（batch prompting）在大推理模型（LRMs）中的作用，发现其不仅能降低推理成本，还能正则化多步推理行为，抑制过度思考和犹豫性语言，提升准确率并显著减少推理所需的token数量（3-5倍），且存在从简单到难的跨样本泛化效应。


<details>
  <summary>Details</summary>
Motivation: 尽管批量提示已被用于摊销大语言模型的推理成本，但其对推理行为的影响尚未充分探索。本文旨在揭示批量提示在多步推理中的潜在正则化作用，以提升推理效率与可靠性。

Method: 通过在13个多样化基准上进行综合实验，对比不同批处理设置下的模型表现，并对生成行为进行细粒度分析，包括过度思考、自我修正和决策确定性等指标。

Result: 批量提示显著提高推理准确率，减少3-5倍的推理token使用；有效抑制过度思考和重复自我纠正；并观察到模型在同一批次中从前例泛化解决难题的集体效应。

Conclusion: 批量提示不仅是提高吞吐量的技术，更是一种有效的推理时正则化手段，可提升大推理模型的效率与稳定性。

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [86] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: 提出RIDE框架，利用项目反应理论（IRT）和强化学习生成更具挑战性的数学问题变体，以更严格地评估大语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的对抗扰动方法常生成不合理的题目，难以系统评估问题难度，且可能高估模型的真实数学推理能力。需要一种更可靠的方法来衡量模型的鲁棒性。

Method: 提出RIDE框架：利用35个大语言模型模拟学生答题行为，基于IRT构建问题难度排序器；通过强化学习指导问题重写模型，在保持语义合理的同时生成跨难度级别的新问题。

Result: 在竞赛级数学基准上应用RIDE后，26个先进大模型平均性能下降21.73%，表明其数学推理能力缺乏鲁棒性；生成的问题更难但格式正确，验证了评估方法的有效性。

Conclusion: RIDE能有效揭示当前大语言模型在数学推理上的脆弱性，提供了一种可量化难度、系统化生成对抗性问题的新范式，推动更可靠的推理能力评估。

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [87] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: 本文提出CantoASR，一种结合声学特征与大音频语言模型推理的协作式粤语语音识别纠错框架，在低资源、多变调的粤语ASR任务中显著优于Whisper。


<details>
  <summary>Details</summary>
Motivation: 粤语作为一种低资源语言，存在标注数据少、六种声调、连读变调和口音差异等问题，导致现有ASR系统（如Whisper）词错误率高，难以准确识别。

Method: 提出CantoASR框架：1）使用强制对齐提取声学特征；2）LoRA微调Whisper以提升声调分辨能力；3）指令微调Qwen-Audio进行韵律感知的错误纠正。

Result: 在自发性粤语数据上评估显示，相比Whisper-Large-V3，CantoASR显著降低了字符错误率（CER）。

Conclusion: 将显式声学线索（如声调和韵律）与大音频语言模型的上下文推理相结合，为低资源声调语言和方言的ASR提供了一种可扩展的有效策略。

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [88] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: 本文提出LAAC（大型语言模型作为沟通中介）范式，旨在通过结构化对话捕捉发送者意图，促进真实的知识交流，而非陷入AI生成内容的膨胀与压缩循环。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的泛滥，沟通变得冗长且缺乏真实性，双方依赖LLM进行内容的生成与摘要，导致信息失真和参与度下降。

Method: 采用多智能体架构，在多种应用场景中通过控制实验评估LAAC在信息捕获保真度、可重现性和查询响应完整性三个维度的信任要求。

Result: 初步发现表明，在高风险沟通场景中可靠部署LAAC之前，必须解决存在的可衡量的信任差距。

Conclusion: 要实现LLM作为可信沟通中介，需系统性提升其在意图提取、知识一致性与响应可靠性方面的能力。

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [89] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 该研究评估了当前大语言模型（LLMs）是否能通过波兰国家上诉委员会的官方资格考试，发现尽管模型在知识测试中表现尚可，但在实际判案写作部分均未达标，且“LLM作为评委”的自动评分与人工评分存在偏差，表明现有LLM尚不能替代人类法官。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在法律职业资格考试中的应用潜力，评估其在真实法律决策任务中的可行性和局限性。

Method: 将LLM作为考生参与考试，并采用'LLM-as-a-judge'方法由模型自动评分；构建混合信息检索与提取管道，在闭卷及多种检索增强生成（RAG）设置下测试多个LLM。

Result: LLMs在多项选择题的知识测试中得分满意，但在书面判决部分均未达到通过标准，且模型间的自动评价结果与官方评审委员会的判断常不一致。

Conclusion: 当前的大语言模型因易产生幻觉、错误引用法律条文、逻辑论证薄弱等问题，仍无法取代波兰公共采购领域的法官或独立考官，需法律专家与技术团队紧密协作以提升实用性。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [90] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: REMIND是一种新的评估方法，用于检测机器遗忘中残留的记忆影响，通过分析模型在小输入变化下的损失来判断数据是否被有效遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有的遗忘评估方法通常只在单个输入级别上进行，可能忽略语义相似样本中的残余影响，导致隐私泄露。需要更敏感和可靠的评估手段。

Method: REMIND通过查询模型在目标数据邻域内的损失动态，识别遗忘后数据周围更平坦的损失景观，从而检测残留记忆，并据此分类数据是否被有效遗忘。

Result: REMIND能在不同模型、数据集和改写输入下稳定检测到未被完全遗忘的影响，优于现有方法，且仅需查询访问权限，具有实用性和鲁棒性。

Conclusion: REMIND提供了一个更敏感、可解释且可靠的框架来评估语言模型的遗忘效果，为记忆与遗忘研究提供了新视角。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [91] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 本文研究了当前大语言模型预训练过程中数据利用效率的问题，发现通过检索增强生成和测试时计算可以显著提升性能，表明现有预训练方法未能充分利用数据中的信息。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在不断进步，但人们对预训练过程中从数据中提取知识的效率缺乏理解，本文旨在量化这一“遗留价值”。

Method: 采用检索增强生成（RAG）结合测试时计算的方法，评估在不同规模下预训练模型未能利用的数据价值，并在MMLU、Math-500和SimpleQA等任务上进行验证。

Result: 在标准开源数据集上，检索能带来显著准确率提升，对MMLU而言检索相当于5倍计算量的增益；通过测试时额外计算解析检索内容，LLaMA 3.1 8B模型在MMLU上提升了10个百分点。

Conclusion: 当前的预训练方法并未充分挖掘现有数据集中的信息，存在巨大改进空间。

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [92] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 本文提出一种基于图的方法进行主题标注，通过语义扩展和关系分析为无标签文本中的主题生成可解释的标签，在保持计算效率的同时性能媲美ChatGPT-3.5。


<details>
  <summary>Details</summary>
Motivation: 现有主题建模方法生成的主题词缺乏可解释性，且多数先进方法计算成本高，需要一种高效且准确的主题标注方案。

Method: 采用图结构对主题词进行语义扩展，引入相关术语并分析词间关系，基于图的连接性生成有意义的主题标签。

Result: 在两个数据集上对比多种基准方法（包括ChatGPT-3.5），该方法在BERTScore和余弦相似度上均优于传统基准，结果与ChatGPT-3.5相当，且计算开销低。

Conclusion: 所提出的图方法在主题标注任务中兼具高效性与有效性，为提升主题模型的可解释性和自动化提供了可行路径。

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [93] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: 本文提出了SSPO方法，通过引入句子级重要性比率和基于句子熵的动态剪裁机制，解决了GRPO和GSPO在强化学习奖励验证中的训练不稳定和采样效率低的问题，在多个数据集上取得了优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR算法如GRPO和GSPO在训练稳定性或采样数据利用率方面存在缺陷：GRPO因令牌级重要性比率易受异常值影响导致训练崩溃，GSPO因响应级比率导致极端值影响整体判断，降低数据利用效率。

Method: 提出SSPO，采用句子级重要性比率，在GRPO和GSPO之间取得平衡；同时引入句子熵来动态调整PPO-CLIP的剪裁边界，使高熵词元更鼓励探索，低熵词元缩小剪裁范围。

Result: SSPO在五个数据集上的平均得分为46.57，优于GRPO（43.01）和GSPO（44.42），并在三个数据集上达到最先进性能。

Conclusion: SSPO有效提升了训练稳定性和生成数据的利用效率，兼具GRPO与GSPO的优势并克服其缺点，是更优的RLVR策略。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [94] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 提出一种基于可学习性分数和批处理选择策略的数据选择方法，用于机器翻译模型的微调，显著提升数据效率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 高质量数据的选择对机器翻译模型性能至关重要，但现有方法在数据利用效率和相关性建模方面存在不足。

Method: 通过结合学习模型与预训练参考模型，定义可学习性分数来评估数据点的训练价值，并采用考虑样本间依赖关系的批量选择策略。

Result: 在英-波斯等多个语言对上实验表明，相比随机选择，该方法数据效率提升达五倍，使用缓存嵌入时计算效率提高24倍，并显著提升翻译性能。

Conclusion: 所提数据选择方法能有效提升机器翻译微调过程中的数据利用率、计算效率和模型泛化能力。

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [95] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLM）在1940年时间背景下的时序推理能力，使用一本挪威语 trivia 书籍的问题，分别用英语和挪威语进行提示，并通过LLM作为裁判评估回答。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在历史特定时间背景下进行准确推理的能力，尤其是语言选择和模型规模对性能的影响。

Method: 使用1940年的挪威 trivia 书籍问题，以英语和挪威语向多个LLM（包括DeepSeek-R1、Gemma3、Qwen3、Llama3.1及专为挪威语设计的最大LLM）提问，并模拟1940年的知识状态；答案由LLM评判并辅以母语者抽样验证。

Result: 英语提示的效果 consistently 优于挪威语提示，结果出乎意料；更大的模型表现更好。

Conclusion: 模型规模提升有助于时序推理表现，但语言选择的影响复杂，英语反而比挪威语更有效，表明语言与文化背景的建模仍存在挑战。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [96] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 提出PTTSD框架，用于从临床访谈文本中预测抑郁严重程度（PHQ-8分数），结合LSTM、自注意力和残差连接，并建模时间不确定性，具有良好的校准预测区间和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁严重程度预测模型缺乏不确定性估计和时间序列建模能力，限制了其在临床决策支持中的应用可信度与实用性。

Method: 提出PTTSD，包含序列到序列和序列到单值两种结构，结合双向LSTM、自注意力机制和残差连接，使用高斯或Student-t分布输出头，通过负对数似然训练，实现带不确定性估计的时间序列抑郁检测。

Result: 在E-DAIC和DAIC-WOZ数据集上达到文本-only 方法的SOTA性能（如E-DAIC上MAE=3.85，DAIC上MAE=3.55），预测区间校准良好，消融实验验证注意力机制与概率建模的有效性，与MentalBERT对比显示方法通用性。

Conclusion: PTTSD能有效进行抑郁严重程度的可解释、不确定性感知预测，具备临床实用潜力，支持更可靠的决策辅助。

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [97] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: ThaiOCRBench是首个针对泰语文本密集型视觉理解任务的综合基准，包含2,808个人工标注样本，涵盖13个任务类别，用于评估多模态模型在低资源、文字复杂环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型基准主要集中于高资源语言，泰语在文档结构理解等任务中缺乏代表性，亟需专门的评估基准。

Method: 构建了一个名为ThaiOCRBench的多样化、人工标注数据集，在零样本设置下对多种前沿视觉语言模型（包括闭源和开源）进行评估，并开展详细错误分析。

Result: 闭源模型（如Gemini 2.5 Pro）显著优于开源模型；开源模型在细粒度文本识别和手写内容提取上表现最差；发现语言偏见、结构错配和幻觉内容等关键挑战。

Conclusion: ThaiOCRBench为泰语等低资源语言提供了标准化的视觉语言模型评估框架，并为改进泰语文档理解提供了可操作的见解。

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [98] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: 本文提出了基于提示生成的多轮文本简化方法，用于可读性控制的文本简化，并发现源CEFR级别与目标CEFR级别之间的差距显著影响简化效果。


<details>
  <summary>Details</summary>
Motivation: 受提示法在文本简化中表现的启发，探索如何通过多轮简化提升性能，特别是利用源和目标CEFR级别差异的影响。

Method: 提出两种多轮简化方法：基于规则的简化（MRS-Rule）和联合规则与大模型的简化（MRS-Joint），并通过GPT-4o生成结果。

Result: 提交的系统在20支队伍中排名第7；后续改进显示，以LLM简化的结果为起点能进一步提升性能。

Conclusion: 多轮简化策略有效，尤其是结合LLM与规则的方法（MRS-Joint），验证了起始简化点对最终效果的重要性。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [99] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 本研究使用BFI-2框架系统评估六种大语言模型在不同采样温度下的五大人格特质表达，发现神经质和外向性受温度影响显著，且模型架构差异可能导致稳定的人格特征聚类。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在人类中心应用中的普及，理解其类人格行为对负责任的开发与部署至关重要。

Method: 采用Big Five Inventory-2（BFI-2）框架，对六种LLMs在不同采样温度下进行系统性人格特质评估，并通过分层聚类分析模型间的特征模式。

Result: 在五个维度中，四个表现出显著差异；神经质和外向性受温度调节影响明显；分层聚类揭示了基于架构特征的模型聚类现象，表明某些模型具有更稳定的特质轮廓。

Conclusion: LLMs展现出可测量的类人格模式，这些模式受生成参数和模型架构的影响，为模型调优、选择及AI伦理治理提供了新视角。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [100] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: 本文提出了RAGalyst，一个用于评估领域特定检索增强生成（RAG）系统的自动化、与人类判断对齐的代理框架。该框架通过生成高质量合成问答数据并优化LLM-as-a-Judge指标，实现了在军事、网络安全和桥梁工程等高风险领域的可靠评估。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估方法在专业且安全关键的领域中难以捕捉领域细微差异，且缺乏与人类判断的一致性，因此需要一种更严谨、可信赖的评估框架。

Method: 提出RAGalyst框架，包含代理式流水线生成合成问答数据，并引入代理过滤步骤确保数据保真度；通过提示优化改进Answer Correctness和Answerability两个LLM-as-a-Judge指标，使其与人类标注高度相关。

Result: 在三个不同领域应用该框架发现RAG性能高度依赖上下文，没有单一模型或配置始终最优；同时分析了导致答案正确性低的主要原因。

Conclusion: RAGalyst提供了一种系统化、可复现的评估方法，有助于揭示领域特定权衡，支持构建更可靠有效的RAG系统。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [101] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: 提出了一种两部分框架来处理放射学报告中的显性和隐性不确定性，并发布了包含不确定性感知的Lunguage++数据集。


<details>
  <summary>Details</summary>
Motivation: 放射学报告中存在显性和隐性不确定性，影响自动化分析的准确性，需更有效的建模方法。

Method: 通过构建专家验证的LLM基准对常见模糊表达进行排序以量化显性不确定性，并基于专家定义的诊断路径扩展子发现以建模隐性不确定性。

Result: 成功构建了Lunguage++数据集，支持不确定性感知的图像分类和更真实的诊断推理。

Conclusion: 该框架有效提升了放射学报告结构化过程中对不确定性的建模能力，有助于临床决策和后续研究。

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [102] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 该研究探讨了推理语言模型在生成过程中是否表征了可能的替代路径，发现隐藏激活状态可以控制和预测模型在思维链推理中的不确定性，并能预测模型未来的输出分布。


<details>
  <summary>Details</summary>
Motivation: 由于语言模型在生成文本时可能因不同token选择而走向不同推理路径，难以量化其不确定性，因此研究者希望探究模型内部是否隐含表示这些可能的路径。

Method: 通过分析语言模型的隐藏激活状态，进行干预实验，测试其对模型不确定性的控制与预测能力，特别是在思维链推理过程中的表现。

Result: 发现模型在不同token处的不确定性与其激活状态可被引导的程度存在明显相关性，且隐藏激活能够预测模型未来的输出分布，表明模型隐式地表征了可能的推理路径空间。

Conclusion: 语言模型的隐藏激活不仅反映当前推理状态，还包含对多种可能路径的表征，尤其在尚未确定最终答案时更具可塑性，为理解和干预模型推理过程提供了新视角。

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [103] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: IntelliProof是一个利用大语言模型（LLM）分析议论文的交互式系统，将文章结构化为论证图，强调可视化与用户体验，提供分类依据和连贯性量化指标，支持快速评估论点质量并保留人工监督。


<details>
  <summary>Details</summary>
Motivation: 现有自动作文评分系统缺乏对论证结构的深入分析和用户友好性，IntelliProof旨在通过结构化表示和交互式可视化提升议论文的理解与评估效果。

Method: 将议论文建模为论证图，节点表示主张，边表示支持或攻击关系，使用LLM进行关系分类与打分，并通过可视化界面展示结果，同时生成自然语言解释和连贯性度量。

Result: 系统能够有效呈现议论文的论证结构，提供可解释的分类结果和定量分析指标，支持用户快速探索文章的论点质量，并增强对文本结构的理解。

Conclusion: IntelliProof通过结合LLM与交互式可视化，在保留人工监督的同时提升了议论文分析的透明度与可用性，弥合了文本结构语义与用户理解之间的差距。

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [104] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 本文研究了基于大语言模型（LLM）的编程助手在软件开发中产生的安全漏洞问题，指出当前主流开源模型仍易受早期已知漏洞影响，并提出一种新的风险评估指标Prompt Exposure（PE）及Model Exposure（ME）评分，以量化生成漏洞的严重性与普遍性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在编程辅助中的广泛应用，其生成代码的安全性对网络安全至关重要。然而现有安全基准和改进方法对实际模型的影响尚不明确，亟需评估真实场景下的漏洞风险并推动有效修复。

Method: 通过分析最新开源LLM在现实使用场景中对早期漏洞的脆弱性，提出Prompt Exposure（PE）指标，综合考虑漏洞严重性、生成概率和诱导提示形式；进一步定义Model Exposure（ME）评分来衡量模型整体暴露风险。

Result: 发现即使是最新的开源LLM仍在早期漏洞场景中表现出脆弱性，表明安全性与功能性的权衡阻碍了有效修补；PE和ME能够更全面地评估和引导对高风险漏洞的缓解。

Conclusion: 当前LLM编码助手仍存在显著安全风险，需引入PE和ME等细粒度指标来指导模型安全优化，优先解决最严重且常见的漏洞问题。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [105] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 本文提出了首个大规模孟加拉语生物医学选择题数据集BanglaMedQA和BanglaMMedBench，并评估了多种检索增强生成（RAG）策略，其中基于代理的RAG方法结合教材与网络检索，在GPT-120B上达到89.54%的准确率，显著提升了孟加拉语医学问答系统的性能。


<details>
  <summary>Details</summary>
Motivation: 低资源语言中缺乏可靠的生物医学问答系统，限制了人们获取医疗知识的公平性，因此需要开发针对孟加拉语等语言的高质量医学AI系统。

Method: 构建了两个新的孟加拉语医学MCQ数据集，并设计了包括传统、零样本回退、代理式、迭代反馈和聚合RAG在内的多种RAG策略；利用OCR技术整合孟加拉语医学教材，并构建动态选择检索与推理策略的代理RAG流程。

Result: 代理RAG在openai/gpt-oss-120b模型上取得了89.54%的最高准确率，优于其他配置，且生成的推理理由质量更优。

Conclusion: 基于RAG的方法能有效提升孟加拉语医学问答系统的准确性与可靠性，为多语言医疗AI研究奠定了基础。

Abstract: Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [106] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: 本文提出了一种名为DeReC的轻量级事实验证框架，利用通用文本嵌入结合密集检索与分类，在准确性和效率上均优于基于大语言模型的解释生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的事实验证方法存在计算成本高和幻觉风险的问题，限制了其在现实场景中的应用。

Method: 提出DeReC框架，使用密集检索（Dense Retrieval）获取相关证据，并通过专门的分类器进行事实判断，避免使用自回归式大语言模型生成解释。

Result: 在RAWFC数据集上F1得分为65.58%，超过现有最优方法L-Defense（61.20%）；运行时间比LLM方法减少95%（从454分钟降至23分钟）；在LIAR-RAW上减少92%。

Conclusion: 精心设计的基于检索的系统可以在特定任务上达到甚至超越大语言模型的性能，同时具备更高的实际部署可行性。

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [107] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

TL;DR: LEASH是一种无需训练的解码算法，通过监控token级熵斜率和顶级logit margin的改进来自适应停止推理生成，减少30-35%的平均token生成和27%的延迟，仅牺牲10个百分点的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的思维链（CoT）提示在大语言模型中虽有效，但生成固定长度的推理过程浪费计算资源，导致高延迟和高token消耗。

Method: 提出LEASH算法，利用两个内在信号——token级熵的斜率和top-logit margin的改善——判断模型是否达到稳定的推理状态，并在此时停止生成。

Result: 在GSM8K和AQuA-RAT基准上的四个指令调优模型中，LEASH平均减少30-35%的token生成和27%的延迟，准确率下降约10个百分点。

Conclusion: LEASH是一种模型无关、无需训练的高效替代CoT的方法，能在可接受精度损失下显著提升推理效率。

Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [108] [OptiMA: A Transaction-Based Framework with Throughput Optimization for Very Complex Multi-Agent Systems](https://arxiv.org/abs/2511.03761)
*Umut Çalıkyılmaz,Nitin Nayak,Jinghua Groppe,Sven Groppe*

Main category: cs.MA

TL;DR: 本文提出了一个基于事务的框架OptiMA，用于设计和执行复杂的多智能体系统（VCMAS），并通过引入事务调度缓解性能瓶颈，实验显示性能提升超过16%。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统复杂度增加，系统易出现故障和性能瓶颈问题，亟需有效框架来应对这些挑战。

Method: 提出基于事务的框架OptiMA，集成事务调度机制，并在包含上百个智能体的系统中实现和验证。

Result: OptiMA能够有效支持大规模复杂多智能体系统的运行，事务调度带来了超过16%的性能提升，并提供了事务调度问题的理论分析与实用工具。

Conclusion: 基于事务的设计与调度机制可有效提升复杂多智能体系统的可靠性与性能，为未来研究提供了可行路径与工具支持。

Abstract: In recent years, the research of multi-agent systems has taken a direction to
explore larger and more complex models to fulfill sophisticated tasks. We point
out two possible pitfalls that might be caused by increasing complexity;
susceptibilities to faults, and performance bottlenecks. To prevent the former
threat, we propose a transaction-based framework to design very complex
multi-agent systems (VCMAS). To address the second threat, we offer to
integrate transaction scheduling into the proposed framework. We implemented
both of these ideas to develop the OptiMA framework and show that it is able to
facilitate the execution of VCMAS with more than a hundred agents. We also
demonstrate the effect of transaction scheduling on such a system by showing
improvements up to more than 16\%. Furthermore, we also performed a theoretical
analysis on the transaction scheduling problem and provided practical tools
that can be used for future research on it.

</details>


### [109] [ASAP: an Agentic Solution to Auto-optimize Performance of Large-Scale LLM Training](https://arxiv.org/abs/2511.03844)
*Yuran Ding,Xinwei Chen,Xiaofan Zhang,Zongwei Zhou*

Main category: cs.MA

TL;DR: ASAP是一个多智能体系统，通过结合LLM推理与性能分析工具、屋顶线模型及专家经验知识库，自动诊断大规模语言模型训练中的性能瓶颈并推荐优化的分片配置，显著提升分布式训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语言模型训练优化方法依赖耗时的手动调优或资源密集型黑箱搜索，难以跟上快速发展的LLM领域需求，导致开发缓慢和资源利用率低。

Method: 提出ASAP，一种基于多智能体（Coordinator、Analyzer、Proposal）的自动化性能优化系统，融合LLM推理能力与性能剖析工具、roofline分析以及专家最佳实践知识库，实现对性能瓶颈的自动诊断和分片策略优化建议。

Result: 实验表明，ASAP生成的分片配置最多可减少28%的训练步时间，吞吐量提升1.43倍；结合人工进一步优化，吞吐量可达2.58倍。

Conclusion: ASAP提供了一种可扩展且可解释的AI辅助大规模LLM训练性能工程方法，有效提升了分布式训练的自动化与效率。

Abstract: Optimizing large-language model (LLM) training on distributed domain-specific
accelerator systems presents significant challenges due to its complex
optimization space. Existing optimization methods, however, rely on
time-consuming manual tuning or resource-intensive black-box searches, which
struggle to keep pace with the rapidly evolving LLM domain, leading to slow
development and underutilized resources. To address this, we introduce ASAP, an
Agentic Solution to Auto-optimize Performance of Large-Scale LLM Training. It
is a multi-agent system, featuring Coordinator, Analyzer, and Proposal agents,
which integrates LLM reasoning with insights from performance profiling tools,
roofline analysis, and a knowledge base of best practices and successful past
optimizations from human experts. Our proposed design can automate the
diagnosis of performance bottlenecks and recommend optimized sharding
configurations with reasoning, thus effectively improving the efficiency of
distributed LLM training. Experiments have shown that the ASAP-generated
sharding configurations can contribute up to 28% training step time reduction
and 1.43 times throughput improvement. When combined with additional
optimization from human experts, throughput can be further increased to 2.58
times. The proposed ASAP promises to provide a scalable and explainable
methodology for AI-assisted performance engineering in large-scale LLM
training.

</details>


### [110] [Multi-Agent Collaborative Framework For Math Problem Generation](https://arxiv.org/abs/2511.03958)
*Kia Karbasi,Kevin Hong,Mohammad Amin Samadi,Gregory Pottie*

Main category: cs.MA

TL;DR: 提出了一种基于协作多智能体框架的数学教育自动问答生成方法，通过多智能体迭代优化问题与答案对，提升问题在认知挑战与清晰度之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有预训练语言模型在自动问题生成中难以精确控制问题复杂度和认知需求，尤其在数学教育领域存在不足。

Method: 设计了一个协作式多智能体框架，在推理时引入计算机制，多个智能体协同迭代优化生成的问题-答案对，以更好平衡复杂性和认知负荷。

Result: 在相关性、重要性、清晰度、难度匹配和可回答性五个元评估标准下表现良好，初步评估显示该方法能有效提升生成问题的质量和可控性。

Conclusion: 协作多智能体框架有助于生成更可控、更具教学价值的教育内容，推动自动化教育内容生成和自适应学习环境的发展。

Abstract: Automatic question generation (AQG) for mathematics education remains an
elusive goal for Intelligent Tutoring Systems and educators. While pre-trained
transformer-based language models have significantly advanced natural language
generation, they often struggle to precisely control problem complexity and
cognitive demands. In this paper, we introduce a collaborative multi-agent
framework as a novel method of incorporating inference-time computation into
AQG. This approach leverages multiple agents that iteratively refine generated
question-answer pairs to better balance complexity and cognitive demand. We
evaluate the generated questions on five meta-evaluation criteria: relevance,
importance, clarity, difficulty matching, answerability, to assess the system's
ability to control the required complexity and quality of the questions.
Preliminary evaluations show that this collaborative multi-agent framework
elevates the quality of generated educational content by fostering a more
nuanced balance between cognitive challenge and clarity. These promising
outcomes suggest that integrating collaborative multi-agent workflows can yield
more controlled, pedagogically valuable content that can help advance automated
educational content generation and adaptive learning environments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [111] [Caption Injection for Optimization in Generative Search Engine](https://arxiv.org/abs/2511.04080)
*Xiaolu Chen,Yong Liao*

Main category: cs.IR

TL;DR: 本文提出了首个多模态生成式搜索引擎优化（G-SEO）方法“字幕注入”（Caption Injection），通过从图像中提取字幕并将其融入文本内容，增强生成式搜索场景中内容的主观可见性。实验表明，该方法在多模态和单模态环境下均显著优于纯文本G-SEO基线。


<details>
  <summary>Details</summary>
Motivation: 现有的G-SEO方法局限于文本优化，未能充分利用多模态数据（如图像、音频、视频），无法满足生成式搜索引擎（GSEs）对丰富内容整合的需求。随着多模态检索增强生成（MRAG）技术的发展，亟需一种能有效融合视觉语义的G-SEO方法。

Method: 提出“字幕注入”（Caption Injection）方法：从图像中提取字幕，并将这些字幕嵌入到原始文本内容中，从而在不改变原有内容结构的前提下引入视觉语义信息，提升内容在生成式搜索中的主观可见性。该方法在MRAMG多模态基准上进行了系统评估。

Result: 实验结果显示，在G-Eval指标下，字幕注入方法显著优于仅使用文本的G-SEO基线模型，无论是在单模态还是多模态设置下均表现出更强的内容可见性提升效果，验证了多模态整合在G-SEO中的必要性和有效性。

Conclusion: 字幕注入是首个面向多模态G-SEO的方法，成功将视觉语义融入文本优化过程，显著提升了生成式搜索中用户对内容的感知可见性，为未来多模态搜索引擎优化提供了新方向。

Abstract: Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation
(RAG) techniques and Large Language Models (LLMs) to integrate multi-source
information and provide users with accurate and comprehensive responses. Unlike
traditional search engines that present results in ranked lists, GSEs shift
users' attention from sequential browsing to content-driven subjective
perception, driving a paradigm shift in information retrieval. In this context,
enhancing the subjective visibility of content through Generative Search Engine
Optimization (G-SEO) methods has emerged as a new research focus. With the
rapid advancement of Multimodal Retrieval-Augmented Generation (MRAG)
techniques, GSEs can now efficiently integrate text, images, audio, and video,
producing richer responses that better satisfy complex information needs.
Existing G-SEO methods, however, remain limited to text-based optimization and
fail to fully exploit multimodal data. To address this gap, we propose Caption
Injection, the first multimodal G-SEO approach, which extracts captions from
images and injects them into textual content, integrating visual semantics to
enhance the subjective visibility of content in generative search scenarios. We
systematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under
both unimodal and multimodal settings. Experimental results show that Caption
Injection significantly outperforms text-only G-SEO baselines under the G-Eval
metric, demonstrating the necessity and effectiveness of multimodal integration
in G-SEO to improve user-perceived content visibility.

</details>


### [112] [E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce](https://arxiv.org/abs/2511.04087)
*Ge Zhang,Rohan Deepak Ajwani,Tony Zheng,Hongjian Gu,Yaochen Hu,Wei Guo,Mark Coates,Yingxue Zhang*

Main category: cs.IR

TL;DR: 提出了一种高效的常识增强推荐增强器E-CARE，通过单次LLM前向推理生成常识推理因子图，显著提升电商场景下查询与商品相关性建模的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型挖掘查询与商品交叉特征的方法在推理时成本高，依赖大量实时LLM调用和人工标注，难以高效部署。

Method: 提出E-CARE框架，利用常识推理因子图编码强大LLM的推理模式，使增强模型在每次查询仅需一次LLM前向传递即可获取常识推理能力。

Result: 在两个下游任务上的实验表明，该方法最高可将precision@5提升12.1%。

Conclusion: E-CARE在显著降低LLM推理开销的同时，有效保留了其常识推理能力，提升了电商推荐系统的准确性和效率。

Abstract: Finding relevant products given a user query plays a pivotal role in an
e-commerce platform, as it can spark shopping behaviors and result in revenue
gains. The challenge lies in accurately predicting the correlation between
queries and products. Recently, mining the cross-features between queries and
products based on the commonsense reasoning capacity of Large Language Models
(LLMs) has shown promising performance. However, such methods suffer from high
costs due to intensive real-time LLM inference during serving, as well as human
annotations and potential Supervised Fine Tuning (SFT). To boost efficiency
while leveraging the commonsense reasoning capacity of LLMs for various
e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation
Enhancer (E-CARE). During inference, models augmented with E-CARE can access
commonsense reasoning with only a single LLM forward pass per query by
utilizing a commonsense reasoning factor graph that encodes most of the
reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show
an improvement of up to 12.1% on precision@5.

</details>


### [113] [Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance](https://arxiv.org/abs/2511.04172)
*Mashrur Rahman,Mantaqa abedin,Monowar Zamil Abir,Faizul Islam Ansari,Adib Reza,Farig Yousuf Sadeque,Niloy Farhan*

Main category: cs.IR

TL;DR: 本文提出了一种基于AI的聊天机器人，为BRAC大学的学生提供个性化指导，结合BM25和ChromaDB进行信息检索，并利用LLaMA-3.3-70B生成响应，具有高效的数据处理能力和高语义相关性。


<details>
  <summary>Details</summary>
Motivation: 大学生在本科学习期间缺乏及时、个性化的指导，现有数字工具无法为新生提供定制化辅导。

Method: 构建一个AI驱动的聊天机器人，采用数据摄取管道整合CSV文件和网页等多源信息，结合BM25词汇排序与ChromaDB语义检索，并使用LLaMA-3.3-70B大语言模型生成对话回复。

Result: 生成文本语义相关性高，BERTScore达0.831，METEOR得分为0.809；数据管道更新效率高，更新耗时106.82秒，新数据处理耗时368.62秒。

Conclusion: 该聊天机器人能有效回答学生问题，帮助其更好理解校园生活并规划学期安排，具备实际应用价值。

Abstract: University students face immense challenges during their undergraduate lives,
often being deprived of personalized on-demand guidance that mentors fail to
provide at scale. Digital tools exist, but there is a serious lack of
customized coaching for newcomers. This paper presents an AI-powered chatbot
that will serve as a mentor for the students of BRAC University. The main
component is a data ingestion pipeline that efficiently processes and updates
information from diverse sources, such as CSV files and university webpages.
The chatbot retrieves information through a hybrid approach, combining BM25
lexical ranking with ChromaDB semantic retrieval, and uses a Large Language
Model, LLaMA-3.3-70B, to generate conversational responses. The generated text
was found to be semantically highly relevant, with a BERTScore of 0.831 and a
METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82
seconds for updates, compared to 368.62 seconds for new data. This chatbot will
be able to help students by responding to their queries, helping them to get a
better understanding of university life, and assisting them to plan better
routines for their semester in the open-credit university.

</details>


### [114] [Coordination-Free Lane Partitioning for Convergent ANN Search](https://arxiv.org/abs/2511.04221)
*Carl Kugblenu,Petri Vuorimaa*

Main category: cs.IR

TL;DR: 提出一种无协调的查询分片方法，通过确定性候选池和伪随机排列将并行查询的重复计算转化为互补覆盖，在不增加成本和延迟的情况下显著提升召回率。


<details>
  <summary>Details</summary>
Motivation: 现有向量搜索系统中，并行查询分片常导致各lane重复发现相同候选结果，造成计算资源浪费，无法有效提升检索覆盖率。

Method: 为每个查询构建一个大小等于总top-k预算的确定性候选池，应用伪随机排列后将候选位置均匀划分给各个lane，使各lane天然返回不同结果，无需运行时协调。

Result: 在SIFT1M和MS MARCO数据集上，使用HNSW索引时recall@10从0.249提升至0.999，hit@10从0.200提升至0.601，MRR@10从0.133提升至0.330；IVF索引也有稳定增益（如MS MARCO上+11%），规划器开销仅约37微秒。

Conclusion: 通过合理设计候选池和分片策略，可将冗余的并行查询转化为互补检索，显著提升检索效果而不增加成本或延迟。

Abstract: Production vector search systems often fan out each query across parallel
lanes (threads, replicas, or shards) to meet latency service-level objectives
(SLOs). In practice, these lanes rediscover the same candidates, so extra
compute does not increase coverage. We present a coordination-free lane
partitioner that turns duplication into complementary work at the same cost and
deadline. For each query we (1) build a deterministic candidate pool sized to
the total top-k budget, (2) apply a per-query pseudorandom permutation, and (3)
assign each lane a disjoint slice of positions. Lanes then return different
results by construction, with no runtime coordination.
  At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT
feature vectors) with Hierarchical Navigable Small World graphs (HNSW)
recall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100%
to 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to
0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted
file (IVF) indexes we see smaller but consistent gains (for example, +11% on MS
MARCO) by de-duplicating list routing. A microbenchmark shows planner overhead
of ~37 microseconds per query (mean at the main setting) with linear growth in
the number of merged candidates.
  These results yield a simple operational guideline: size the per-query pool
to the total budget, deterministically partition positions across lanes, and
turn redundant fan-out into complementary coverage without changing budget or
deadline.

</details>


### [115] [Denoised Recommendation Model with Collaborative Signal Decoupling](https://arxiv.org/abs/2511.04237)
*Zefeng Li,Ning Yang*

Main category: cs.IR

TL;DR: 提出了一种基于图神经网络的协同过滤模型DRCSD，通过解耦协作信号和逐阶去噪来提升推荐系统的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有去噪方法在单图上操作可能导致协作信号衰减，影响推荐性能。

Method: 设计了协作信号解耦模块和逐阶去噪模块，并修改了GNN的信息聚合机制以避免跨阶信号干扰。

Result: 在三个真实数据集上实验表明，DRCSD在推荐准确率指标上显著优于现有模型，具有更强的鲁棒性。

Conclusion: DRCSD有效缓解了用户-项目交互矩阵中的噪声问题，提升了协同过滤的推荐性能。

Abstract: Although the collaborative filtering (CF) algorithm has achieved remarkable
performance in recommendation systems, it suffers from suboptimal
recommendation performance due to noise in the user-item interaction matrix.
Numerous noise-removal studies have improved recommendation models, but most
existing approaches conduct denoising on a single graph. This may cause
attenuation of collaborative signals: removing edges between two nodes can
interrupt paths between other nodes, weakening path-dependent collaborative
information. To address these limitations, this study proposes a novel
GNN-based CF model called DRCSD for denoising unstable interactions. DRCSD
includes two core modules: a collaborative signal decoupling module (decomposes
signals into distinct orders by structural characteristics) and an order-wise
denoising module (performs targeted denoising on each order). Additionally, the
information aggregation mechanism of traditional GNN-based CF models is
modified to avoid cross-order signal interference until the final pooling
operation. Extensive experiments on three public real-world datasets show that
DRCSD has superior robustness against unstable interactions and achieves
statistically significant performance improvements in recommendation accuracy
metrics compared to state-of-the-art baseline models.

</details>


### [116] [LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems](https://arxiv.org/abs/2511.04541)
*Baptiste Bonin,Maxime Heuillet,Audrey Durand*

Main category: cs.IR

TL;DR: 研究了大型语言模型（LLM）如何通过成对推理作为用户偏好“世界模型”来应对跨领域排序推荐中的挑战。


<details>
  <summary>Details</summary>
Motivation: 在排序推荐中，建模跨领域的用户偏好仍是一个关键难题。

Method: 通过在三个不同数据集任务上对多个LLM进行实证研究，利用LLM对排序结果进行成对推理以捕捉用户偏好。

Result: 揭示了任务性能与LLM所捕获的偏好函数特性之间的关系。

Conclusion: LLM有潜力作为推荐系统中的世界模型，但也指出了需要改进的方向。

Abstract: Modeling user preferences across domains remains a key challenge in slate
recommendation (i.e. recommending an ordered sequence of items) research. We
investigate how Large Language Models (LLM) can effectively act as world models
of user preferences through pairwise reasoning over slates. We conduct an
empirical study involving several LLMs on three tasks spanning different
datasets. Our results reveal relationships between task performance and
properties of the preference function captured by LLMs, hinting towards areas
for improvement and highlighting the potential of LLMs as world models in
recommender systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [117] [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)
*Zhaorun Chen,Zhuokai Zhao,Kai Zhang,Bo Liu,Qi Qi,Yifan Wu,Tarun Kalluri,Sara Cao,Yuanhao Xiong,Haibo Tong,Huaxiu Yao,Hengduo Li,Jiacheng Zhu,Xian Li,Dawn Song,Bo Li,Jason Weston,Dat Huynh*

Main category: cs.AI

TL;DR: DreamGym是一个统一框架，通过推理-based经验模型生成可扩展的多样化经验，支持大语言模型代理的高效在线强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大语言模型代理中的应用受限于高昂的 rollout 成本、任务多样性不足、奖励信号不可靠和基础设施复杂性，难以获取可扩展的经验数据。

Method: DreamGym利用从真实世界数据初始化并持续更新的经验回放缓冲区，结合基于推理的环境动态建模来生成一致的状态转移和反馈信号，并自适应生成新任务以实现在线课程学习。

Result: 实验表明，DreamGym在全合成设置和从模拟到现实的迁移场景中均显著提升RL训练效果；在WebArena等非RL就绪任务上超越所有基线30%以上，在仅使用合成交互的情况下达到与GRPO和PPO相当的性能，并在迁移到真实环境时显著减少实际交互需求。

Conclusion: DreamGym为通用强化学习提供了一种可扩展的预训练策略，能够在减少真实环境交互的同时提升代理性能。

Abstract: While reinforcement learning (RL) can empower large language model (LLM)
agents by enabling self-improvement through interaction, its practical adoption
remains challenging due to costly rollouts, limited task diversity, unreliable
reward signals, and infrastructure complexity, all of which obstruct the
collection of scalable experience data. To address these challenges, we
introduce DreamGym, the first unified framework designed to synthesize diverse
experiences with scalability in mind to enable effective online RL training for
autonomous agents. Rather than relying on expensive real-environment rollouts,
DreamGym distills environment dynamics into a reasoning-based experience model
that derives consistent state transitions and feedback signals through
step-by-step reasoning, enabling scalable agent rollout collection for RL. To
improve the stability and quality of transitions, DreamGym leverages an
experience replay buffer initialized with offline real-world data and
continuously enriched with fresh interactions to actively support agent
training. To improve knowledge acquisition, DreamGym adaptively generates new
tasks that challenge the current agent policy, enabling more effective online
curriculum learning. Experiments across diverse environments and agent
backbones demonstrate that DreamGym substantially improves RL training, both in
fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready
tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in
RL-ready but costly settings, it matches GRPO and PPO performance using only
synthetic interactions. When transferring a policy trained purely on synthetic
experiences to real-environment RL, DreamGym yields significant additional
performance gains while requiring far fewer real-world interactions, providing
a scalable warm-start strategy for general-purpose RL.

</details>


### [118] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 本研究探讨了汇编代码分析中的分词技术，评估了不同NLP分词模型和参数选择对内在特性和下游任务（如函数签名预测）的影响，揭示了分词器在低级代码分析中的权衡与优化策略。


<details>
  <summary>Details</summary>
Motivation: 汇编代码的分词技术目前研究不足，但其对词汇大小、语义覆盖和下游任务性能有重要影响，因此需要系统评估以优化基于自然语言模型的二进制分析流程。

Method: 通过内在评估比较不同分词器在汇编指令编码效率、词汇压缩和表示保真度方面的表现，并使用Llama 3.2、BERT和BART等预训练模型评估其在下游任务中的效果。

Result: 分词器的选择显著影响下游任务性能，内在指标能在一定程度上但不完全预测外在表现，存在内在特性与实际应用之间的复杂权衡。

Conclusion: 研究为优化低级代码分析中的分词模型提供了有价值的见解，有助于提升基于自然语言模型的二进制分析流程的鲁棒性和可扩展性。

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [119] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong,Luyi Ma,Varun Vasudevan,Jason Cho,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: 本文提出了一个名为BehaviorLens的系统性基准框架，用于评估多模态大语言模型（MLLMs）在用户行为推理中的不同数据模态表现。研究比较了将交易数据表示为文本段落、散点图和流程图三种形式的效果，结果表明图像形式（尤其是散点图和流程图）显著提升了MLLM在下一购买预测任务中的准确性，相比文本表示提高了87.5%，且无需额外计算成本。


<details>
  <summary>Details</summary>
Motivation: 目前关于多模态大语言模型在用户行为序列数据上的推理能力研究较少，特别是不同数据模态（如文本与图像）对模型性能的影响尚不明确。因此，本文旨在系统性地评估不同模态表示对MLLM在用户行为理解任务中的影响。

Method: 提出BehaviorLens框架，使用真实世界购买序列数据集，将交易数据分别表示为文本段落、散点图和流程图，并在六个主流MLLM上进行实验，评估其在下一购买预测任务中的表现。

Result: 实验结果显示，当数据以图像形式（散点图或流程图）输入时，MLLM的预测准确率比纯文本表示高出87.5%，且无额外计算开销。

Conclusion: 图像形式的用户行为数据表示显著优于文本表示，能够大幅提升MLLM在用户行为推理任务中的性能，建议在未来的智能代理系统中优先采用可视化数据输入方式。

Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.

</details>


### [120] [KnowThyself: An Agentic Assistant for LLM Interpretability](https://arxiv.org/abs/2511.03878)
*Suraj Prasai,Mengnan Du,Ying Zhang,Fan Yang*

Main category: cs.AI

TL;DR: KnowThyself是一个基于聊天界面的代理助手，旨在提升大语言模型（LLM）的可解释性，通过整合现有工具并提供自然语言交互和可视化解释，降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM可解释性工具分散且依赖代码，使用门槛高，缺乏统一、用户友好的交互方式。

Method: 构建一个包含 orchestrator LLM、agent router 和专用模块的系统，将用户自然语言问题重述并路由到相应模块，生成可视化结果并整合为连贯解释。

Result: 实现了一个集成化的、对话式的大语言模型可解释性平台，支持模型上传、自然语言提问和交互式可视化解释。

Conclusion: KnowThyself通过对话式工作流显著降低了LLM可解释性的技术壁垒，为未来可扩展的模型检查提供了坚实基础。

Abstract: We develop KnowThyself, an agentic assistant that advances large language
model (LLM) interpretability. Existing tools provide useful insights but remain
fragmented and code-intensive. KnowThyself consolidates these capabilities into
a chat-based interface, where users can upload models, pose natural language
questions, and obtain interactive visualizations with guided explanations. At
its core, an orchestrator LLM first reformulates user queries, an agent router
further directs them to specialized modules, and the outputs are finally
contextualized into coherent explanations. This design lowers technical
barriers and provides an extensible platform for LLM inspection. By embedding
the whole process into a conversational workflow, KnowThyself offers a robust
foundation for accessible LLM interpretability.

</details>


### [121] [Extracting Causal Relations in Deep Knowledge Tracing](https://arxiv.org/abs/2511.03948)
*Kevin Hong,Kia Karbasi,Gregory Pottie*

Main category: cs.AI

TL;DR: 本文挑战了DKT模型性能提升源于其建模知识点间双向关系能力的观点，提出其优势实际上来自于隐式建模先决条件的因果结构。通过将练习关系图剪枝为有向无环图并在因果子集上训练DKT，验证了其预测能力与因果结构高度一致，并提出了基于DKT学习表征提取练习关系DAG的新方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为DKT的优势在于建模知识点间的双向关系，但作者质疑这一解释，试图探究其真正有效的机制。

Method: 将练习关系图修剪为有向无环图（DAG），在具有因果结构的Assistments数据子集上训练DKT，并利用DKT学习到的表示提取练习关系DAG。

Result: 实验表明DKT的预测能力与因果结构高度一致，且其性能在因果子集上保持稳定；提出的DAG提取方法能有效反映知识点间的先序依赖。

Conclusion: DKT的有效性主要源于其对知识点间因果依赖关系的近似能力，而非简单的双向关联建模，这为可解释知识追踪提供了新的视角。

Abstract: A longstanding goal in computational educational research is to develop
explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which
leverages a Recurrent Neural Network (RNN) to predict student knowledge and
performance on exercises, has been proposed as a major advancement over
traditional KT methods. Several studies suggest that its performance gains stem
from its ability to model bidirectional relationships between different
knowledge components (KCs) within a course, enabling the inference of a
student's understanding of one KC from their performance on others. In this
paper, we challenge this prevailing explanation and demonstrate that DKT's
strength lies in its implicit ability to model prerequisite relationships as a
causal structure, rather than bidirectional relationships. By pruning exercise
relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal
subsets of the Assistments dataset, we show that DKT's predictive capabilities
align strongly with these causal structures. Furthermore, we propose an
alternative method for extracting exercise relation DAGs using DKT's learned
representations and provide empirical evidence supporting our claim. Our
findings suggest that DKT's effectiveness is largely driven by its capacity to
approximate causal dependencies between KCs rather than simple relational
mappings.

</details>


### [122] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: 研究发现，尽管提示语言和文化视角会影响大语言模型（LLM）的输出，但所有模型均表现出对荷兰、德国、美国和日本等少数国家文化价值的系统性偏倚，难以真正代表全球文化多样性。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在多语言和多文化背景下的响应是否能够反映其广泛用户群的文化多样性，揭示当前模型在文化价值观对齐方面的局限性。

Method: 通过将Hofstede价值观调查模块和世界价值观调查的63个项目翻译成11种语言，并以有无明确文化视角的方式构建提示，测试了10个大语言模型的响应。

Result: 提示语言和文化视角均影响模型输出，但模型普遍偏向少数国家的文化价值观；显式文化视角比语言选择更能提升与人类价值观的一致性，而两者结合并未带来额外增益。

Conclusion: 大语言模型处于一种尴尬的中间状态：虽能响应提示变化产生差异，却过于锚定特定文化默认值，无法充分代表文化多样性。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [123] [When Empowerment Disempowers](https://arxiv.org/abs/2511.04177)
*Claire Yang,Maya Cakmak,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 本文提出并研究了在多人体环境中基于赋能的AI辅助可能导致他人被削弱（disempowerment）的问题，通过开源多人体网格世界测试套件Disempower-Grid进行实证分析，发现优化单个用户赋能可能损害他人影响力，并指出多智能体情境下目标无关目标可能产生错位风险。


<details>
  <summary>Details</summary>
Motivation: 在多人体共存的真实场景（如家庭、医院）中，现有基于赋能的AI辅助研究仅关注单一人类，忽略了对其他人类的影响，可能导致负面外部效应，因此需要探究多人体环境下的赋能辅助是否会导致他人被削弱。

Method: 构建了一个开源的多人体网格世界测试平台Disempower-Grid，利用强化学习代理在其中优化某一人类的赋能，并测量其对其他人环境影响力和奖励的影响；同时评估联合赋能作为缓解策略的效果。

Result: 实验表明，优化一个用户的赋能会显著降低其他用户的环境影响力和奖励，即发生‘削弱’现象；而采用联合赋能可缓解该问题，但会牺牲主用户的部分奖励。

Conclusion: 看似在单智能体场景中对齐的目标无关目标（如赋能），在多智能体场景中可能产生错位，揭示了AI对齐研究在多主体环境中面临的更广泛挑战。

Abstract: Empowerment, a measure of an agent's ability to control its environment, has
been proposed as a universal goal-agnostic objective for motivating assistive
behavior in AI agents. While multi-human settings like homes and hospitals are
promising for AI assistance, prior work on empowerment-based assistance assumes
that the agent assists one human in isolation. We introduce an open source
multi-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we
empirically show that assistive RL agents optimizing for one human's
empowerment can significantly reduce another human's environmental influence
and rewards - a phenomenon we formalize as disempowerment. We characterize when
disempowerment occurs in these environments and show that joint empowerment
mitigates disempowerment at the cost of the user's reward. Our work reveals a
broader challenge for the AI alignment community: goal-agnostic objectives that
seem aligned in single-agent settings can become misaligned in multi-agent
contexts.

</details>


### [124] [ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering](https://arxiv.org/abs/2511.03985)
*Zhuowen Yuan,Tao Liu,Yang Yang,Yang Wang,Feng Qi,Kaushik Rangadurai,Bo Li,Shuang Yang*

Main category: cs.AI

TL;DR: ArchPilot是一个多智能体系统，通过结合架构生成、代理评估和自适应搜索，显著降低自动化机器学习中的计算开销，提升搜索效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动化机器学习代理依赖多次完整训练来评估候选方案，导致计算成本高、扩展性差、迭代慢。

Method: 提出ArchPilot，包含三个专用智能体：协调搜索过程的编排智能体（基于改进的MCTS算法）、生成与优化架构的生成智能体、执行代理训练并构建高保真评估指标的评估智能体。

Result: 在MLE-Bench上的实验表明，ArchPilot优于AIDE和ML-Master等最先进基线方法。

Conclusion: ArchPilot通过多智能体协作和代理评估机制，实现了在有限预算下的高效机器学习工程，有效解决了传统方法计算开销大的问题。

Abstract: Recent LLM-based agents have demonstrated strong capabilities in automated ML
engineering. However, they heavily rely on repeated full training runs to
evaluate candidate solutions, resulting in significant computational overhead,
limited scalability to large search spaces, and slow iteration cycles. To
address these challenges, we introduce ArchPilot, a multi-agent system that
integrates architecture generation, proxy-based evaluation, and adaptive search
into a unified framework. ArchPilot consists of three specialized agents: an
orchestration agent that coordinates the search process using a Monte Carlo
Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and
manages memory of previous candidates; a generation agent that iteratively
generates, improves, and debugs candidate architectures; and an evaluation
agent that executes proxy training runs, generates and optimizes proxy
functions, and aggregates the proxy scores into a fidelity-aware performance
metric. This multi-agent collaboration allows ArchPilot to prioritize
high-potential candidates with minimal reliance on expensive full training
runs, facilitating efficient ML engineering under limited budgets. Experiments
on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE
and ML-Master, validating the effectiveness of our multi-agent system.

</details>


### [125] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: 本文首次系统研究了基于大语言模型的多智能体AI系统中的异常检测问题，提出了任务定义、数据集构建流程，并发布了两个标注数据集（共4,275和894条轨迹），实验表明XGBoost和SVDD方法在检测沉默故障上准确率分别达98%和96%。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统因大语言模型的非确定性而容易出现难以察觉的沉默故障（如行为漂移、循环、输出缺失等），亟需有效的异常检测机制。

Method: 提出异常检测任务，设计数据集构建流程以捕捉用户行为、智能体非确定性和LLM变异，并构建两个标注数据集；采用监督（XGBoost）和半监督（SVDD）方法进行基准测试。

Result: 构建了包含4,275和894条轨迹的两个基准数据集，实验显示XGBoost准确率达98%，SVDD达96%，两者表现相近。

Conclusion: 这是多智能体AI系统中异常检测的首个系统性研究，提供了数据集、基准和洞察，为后续研究奠定了基础。

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [126] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: 该研究通过构建博弈论实验的数字孪生，系统评估大语言模型（LLM）在模拟人类决策行为方面的表现，发现Llama能高保真复现人类合作模式，而Qwen更接近纳什均衡预测，并展示了LLM在生成新假设和探索未测试实验空间中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM被广泛用于实际决策和社会行为模拟，但其与真实人类决策的一致性尚不明确。这种一致性对应用安全和社会仿真有效性至关重要，因此需要系统评估LLM的行为对齐程度。

Method: 构建博弈实验的数字孪生环境，提出系统的提示与探测框架，测试Llama、Mistral和Qwen三种开源模型在无角色设定提示下的行为反应，并扩展至新博弈配置以生成可检验假设。

Result: Llama高度复现了人类合作行为及偏离理性选择的现象；Qwen则更接近纳什均衡；实现了无需人格化提示的人群级行为复制，并成功生成针对新游戏设置的可预注册假设。

Conclusion: 经过适当校准的LLM不仅能有效复制人类集体行为模式，还可作为传统社会科学研究的补充工具，支持对未知实验空间的系统探索并产生新的实证预测。

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [127] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: 该论文研究了大语言模型（LLM）在处理数值属性时的内部表征机制，发现LLM会系统性放大现实世界中的数值相关性，且无关数值上下文会影响其表示和输出，这种影响随模型规模变化。


<details>
  <summary>Details</summary>
Motivation: 尽管行为研究已记录到大语言模型在数值推理上的错误，但其背后的表征机制尚不清楚。本文旨在探究数值属性在LLM内部如何被整合以及无关数值上下文如何干扰这些表征。

Method: 结合线性探测、偏相关分析和基于提示的脆弱性测试，在不同规模的模型上进行实验，以分析多数值属性的内部整合方式及上下文干扰效应。

Result: 发现LLM编码了现实世界的数值相关性但会系统性放大；无关上下文会引起数值表示的一致性偏移，且对下游输出的影响随模型规模而异。

Conclusion: LLM在数值决策上存在表征层面的脆弱性，研究结果为在多属性纠缠下实现更公平、基于表征的控制提供了基础。

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [128] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种名为Agentmandering的新型重划选区框架，通过引入基于大语言模型的代理博弈机制，在满足法律约束的同时减少党派偏见，提升选举公平性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有重划选区方法多关注生成合法选区方案，但忽视了选择过程中的策略性操纵，导致技术合规的地图仍可能产生不公平的政治优势。因此，需要一种能反映战略互动的机制来提升公平性。

Method: 提出Agentmandering框架，将重划选区建模为两个对立政治利益代理之间的回合制谈判，借鉴博弈论中的Choose-and-Freeze协议，利用大语言模型代理交替选择并冻结候选地图中的选区，逐步完成全州划分。

Result: 在2020年美国人口普查数据上的评估显示，该方法显著降低了党派偏见和不公平性，且相比基准方法方差降低2到3个数量级，尤其在摇摆州表现出更强的稳定性和公平性。

Conclusion: Agentmandering通过引入战略交互机制，有效提升了重划选区过程的公平性与稳健性，为对抗党派操纵提供了新思路。

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


### [129] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: 本文提出了DR.WELL，一种去中心化的神经符号框架，用于多智能体协同规划。该框架通过两阶段协商协议实现合作：首先提出候选角色并进行推理，然后在共识和环境约束下达成联合分配。之后各智能体独立生成并执行符号化计划，通过共享的世界模型将计划与执行结果对齐。


<details>
  <summary>Details</summary>
Motivation: 多智能体协同规划面临信息不完整、通信受限以及轨迹级协调易因微小偏差导致冲突的问题。传统方法难以实现鲁棒的协同，因此需要更高层次的抽象来提升同步性与可解释性。

Method: 提出DR.WELL框架，采用符号化规划与神经网络结合的方法。通过两阶段协商（角色提议与共识承诺）确定分工，各智能体基于共享世界模型独立生成符号计划，并在执行中动态更新状态。

Result: 在协作推块任务中实验表明，该方法能跨episode自适应，动态世界模型捕获可复用模式，提高了任务完成率和效率。尽管引入一定时间开销，但通过协商与自我优化实现了更高效的协作策略。

Conclusion: DR.WELL通过符号级规划与动态世界模型，在不依赖精细轨迹对齐的情况下实现了鲁棒、可解释且可重用的多智能体协同，为复杂部分可观环境下的协作提供了一种有效解决方案。

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [130] [KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04093)
*Yuanning Cui,Zequn Sun,Wei Hu,Zhangjie Fu*

Main category: cs.AI

TL;DR: 提出LLM-KGFR协作框架，结合大语言模型与知识图谱基础检索器（KGFR），通过零样本迁移和渐进式传播实现对大规模或未见知识图的可扩展、通用推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集型问题上受限于上下文和参数化知识，现有方法依赖微调或图神经网络检索器，缺乏在大型或未见图上的泛化与可扩展性。

Method: 设计KGFR检索器，利用LLM生成的关系描述和基于问题角色的实体初始化，结合非对称渐进传播（APP）策略进行高效图扩展，并通过节点、边、路径级接口与LLM交互，形成可控的迭代推理循环。

Result: 实验证明LLM-KGFR在保持可扩展性和泛化能力的同时，在知识图谱增强推理任务中表现出色。

Conclusion: LLM-KGFR为知识图谱增强的推理提供了一种实用、可扩展且具备零样本泛化能力的解决方案。

Abstract: Large language models (LLMs) excel at reasoning but struggle with
knowledge-intensive questions due to limited context and parametric knowledge.
However, existing methods that rely on finetuned LLMs or GNN retrievers are
limited by dataset-specific tuning and scalability on large or unseen graphs.
We propose the LLM-KGFR collaborative framework, where an LLM works with a
structured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR
encodes relations using LLM-generated descriptions and initializes entities
based on their roles in the question, enabling zero-shot generalization to
unseen KGs. To handle large graphs efficiently, it employs Asymmetric
Progressive Propagation (APP)- a stepwise expansion that selectively limits
high-degree nodes while retaining informative paths. Through node-, edge-, and
path-level interfaces, the LLM iteratively requests candidate answers,
supporting facts, and reasoning paths, forming a controllable reasoning loop.
Experiments demonstrate that LLM-KGFR achieves strong performance while
maintaining scalability and generalization, providing a practical solution for
KG-augmented reasoning.

</details>


### [131] [Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms](https://arxiv.org/abs/2511.04133)
*Miguel E. Andres,Vadim Fedorov,Rida Sadek,Enric Spagnolo-Arrizabalaga,Nadescha Trudel*

Main category: cs.AI

TL;DR: 提出了首个评估语音AI测试质量的系统性框架，通过人类中心基准测试来衡量测试对话的生成和评估质量，并对三个商业平台进行了实证评估，验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着语音AI代理逐渐投入实际应用，缺乏可靠的测试方法导致组织无法客观评估其测试手段的有效性，存在严重的测量空白。

Method: 结合心理测量技术（如成对比较产生Elo评分、自助置信区间和置换检验）与严格统计验证，构建了一个可复现的评估框架，用于评估语音AI测试平台在模拟质量和评估质量两方面的表现。

Result: 通过对21,600次人工判断和60个真实对话的验证，发现Evalion平台表现最佳，评估质量f1-score达0.92（其他为0.73），模拟质量得分为0.61（其他为0.43），差异具有统计显著性。

Conclusion: 该框架为研究人员和组织提供了评估任何语音AI测试平台能力的实证工具，奠定了大规模部署语音AI所需的测量基础。

Abstract: Voice AI agents are rapidly transitioning to production deployments, yet
systematic methods for ensuring testing reliability remain underdeveloped.
Organizations cannot objectively assess whether their testing approaches
(internal tools or external platforms) actually work, creating a critical
measurement gap as voice AI scales to billions of daily interactions.
  We present the first systematic framework for evaluating voice AI testing
quality through human-centered benchmarking. Our methodology addresses the
fundamental dual challenge of testing platforms: generating realistic test
conversations (simulation quality) and accurately evaluating agent responses
(evaluation quality). The framework combines established psychometric
techniques (pairwise comparisons yielding Elo ratings, bootstrap confidence
intervals, and permutation tests) with rigorous statistical validation to
provide reproducible metrics applicable to any testing approach.
  To validate the framework and demonstrate its utility, we conducted
comprehensive empirical evaluation of three leading commercial platforms
focused on Voice AI Testing using 21,600 human judgments across 45 simulations
and ground truth validation on 60 conversations. Results reveal statistically
significant performance differences with the proposed framework, with the
top-performing platform, Evalion, achieving 0.92 evaluation quality measured as
f1-score versus 0.73 for others, and 0.61 simulation quality using a league
based scoring system (including ties) vs 0.43 for other platforms.
  This framework enables researchers and organizations to empirically validate
the testing capabilities of any platform, providing essential measurement
foundations for confident voice AI deployment at scale. Supporting materials
are made available to facilitate reproducibility and adoption.

</details>


### [132] [Opus: A Quantitative Framework for Workflow Evaluation](https://arxiv.org/abs/2511.04220)
*Alan Seroul,Théo Fagnoni,Inès Adnani,Dana O. Mohamed,Phillip Kingston*

Main category: cs.AI

TL;DR: 本文提出了Opus工作流评估框架，通过概率-规范性公式量化工作流的质量和效率，结合奖励与惩罚机制实现工作流的评分、比较与优化。


<details>
  <summary>Details</summary>
Motivation: 为了在现代自动化系统中有效评估、比较和优化工作流，需要一个统一的数学模型来综合衡量正确性、可靠性和成本等因素。

Method: 提出Opus工作流奖励函数（估计成功概率、资源使用和输出增益）和Opus规范性惩罚函数（衡量内聚性、耦合性、可观测性和信息卫生等质量属性），并构建联合优化模型进行工作流评分与排序。

Result: 建立了可自动评估、排序和优化工作流的框架，并支持集成到强化学习中以指导工作流的发现与改进。

Conclusion: 该框架为工作流的设计与优化提供了系统的量化方法，能够在复杂自动化系统中提升工作流的整体性能与质量。

Abstract: This paper introduces the Opus Workflow Evaluation Framework, a
probabilistic-normative formulation for quantifying Workflow quality and
efficiency. It integrates notions of correctness, reliability, and cost into a
coherent mathematical model that enables direct comparison, scoring, and
optimization of Workflows. The framework combines the Opus Workflow Reward, a
probabilistic function estimating expected performance through success
likelihood, resource usage, and output gain, with the Opus Workflow Normative
Penalties, a set of measurable functions capturing structural and informational
quality across Cohesion, Coupling, Observability, and Information Hygiene. It
supports automated Workflow assessment, ranking, and optimization within modern
automation systems such as Opus and can be integrated into Reinforcement
Learning loops to guide Workflow discovery and refinement. In this paper, we
introduce the Opus Workflow Reward model that formalizes Workflow success as a
probabilistic expectation over costs and outcomes. We define measurable Opus
Workflow Normative Penalties capturing structural, semantic, and signal-related
properties of Workflows. Finally, we propose a unified optimization formulation
for identifying and ranking optimal Workflows under joint Reward-Penalty
trade-offs.

</details>


### [133] [Shared Spatial Memory Through Predictive Coding](https://arxiv.org/abs/2511.04235)
*Zhengru Fang,Yu Guo,Jingjing Wang,Yuang Zhang,Haonan An,Yinhai Wang,Yuguang Fang*

Main category: cs.AI

TL;DR: 提出了一种基于预测编码的多智能体框架，通过最小化智能体间的互不确定性来实现高效通信与协作，在带宽受限下表现出强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中由于部分可观测性和带宽限制导致的空间记忆共享与协调失败问题。

Method: 构建基于信息瓶颈目标的多智能体预测编码框架，利用自监督运动预测自发形成的网格细胞状空间编码，并发展出高效的通信机制和类似海马体社交位置细胞的神经表征，结合分层强化学习策略主动探索以减少联合不确定性。

Result: 在Memory-Maze基准上，当带宽从128 bit/step降至4 bit/step时，成功率仅从73.5%缓慢下降到64.4%，显著优于全广播基线（从67.6%降至28.6%）。

Conclusion: 该框架为复杂社会表征的涌现提供了理论合理且生物可解释的基础，推动了社会性群体智能的发展。

Abstract: Sharing and reconstructing a consistent spatial memory is a critical
challenge in multi-agent systems, where partial observability and limited
bandwidth often lead to catastrophic failures in coordination. We introduce a
multi-agent predictive coding framework that formulate coordination as the
minimization of mutual uncertainty among agents. Instantiated as an information
bottleneck objective, it prompts agents to learn not only who and what to
communicate but also when. At the foundation of this framework lies a
grid-cell-like metric as internal spatial coding for self-localization,
emerging spontaneously from self-supervised motion prediction. Building upon
this internal spatial code, agents gradually develop a bandwidth-efficient
communication mechanism and specialized neural populations that encode
partners' locations: an artificial analogue of hippocampal social place cells
(SPCs). These social representations are further enacted by a hierarchical
reinforcement learning policy that actively explores to reduce joint
uncertainty. On the Memory-Maze benchmark, our approach shows exceptional
resilience to bandwidth constraints: success degrades gracefully from 73.5% to
64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast
baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically
principled and biologically plausible basis for how complex social
representations emerge from a unified predictive drive, leading to social
collective intelligence.

</details>


### [134] [RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization](https://arxiv.org/abs/2511.04285)
*Zeng Zhiyuan,Jiashuo Liu,Zhangyue Yin,Ge Zhang,Wenhao Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: RLoop 是一种通过迭代策略初始化来缓解强化学习中过拟合和灾难性遗忘问题的自改进框架，显著提升了推理模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在强化学习用于可验证奖励（RLVR）训练大型推理模型时，存在策略过度特化和遗忘多样化解决方案的问题，导致过拟合和泛化能力下降。

Method: 提出 RLoop 框架，结合强化学习探索解空间，并利用拒绝采样微调（RFT）将成功轨迹构建成专家数据集以优化初始策略，实现迭代式的策略初始化循环。

Result: 实验表明，RLoop 有效缓解了遗忘问题，平均准确率提升 9%，pass@32 提升超过 15%。

Conclusion: RLoop 通过将策略探索与利用相结合，将短暂的策略变化转化为稳定的性能提升，在保持奖励收益的同时显著增强模型泛化能力。

Abstract: While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for
training large reasoning models, its training dynamics harbor a critical
challenge: RL overfitting, where models gain training rewards but lose
generalization. Our analysis reveals this is driven by policy
over-specialization and catastrophic forgetting of diverse solutions generated
during training. Standard optimization discards this valuable inter-step policy
diversity. To address this, we introduce RLoop, a self-improving framework
built on iterative policy initialization. RLoop transforms the standard
training process into a virtuous cycle: it first uses RL to explore the
solution space from a given policy, then filters the successful trajectories to
create an expert dataset. This dataset is used via Rejection-sampling
Fine-Tuning (RFT) to refine the initial policy, creating a superior starting
point for the next iteration. This loop of exploration and exploitation via
iterative re-initialization effectively converts transient policy variations
into robust performance gains. Our experiments show RLoop mitigates forgetting
and substantially improves generalization, boosting average accuracy by 9% and
pass@32 by over 15% compared to vanilla RL.

</details>


### [135] [GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents](https://arxiv.org/abs/2511.04307)
*Jian Mu,Chaoyun Zhang,Chiming Ni,Lu Wang,Bo Qiao,Kartik Mathur,Qianhui Wu,Yuhang Xie,Xiaojun Ma,Mengyu Zhou,Si Qin,Liqun Li,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: GUI-360$^\circ$ 是一个大规模、综合性的数据集和基准套件，旨在推动计算机使用智能体（CUA）的发展，包含120多万个动作步骤、多模态轨迹和自动化构建的评测任务。


<details>
  <summary>Details</summary>
Motivation: 现有CUA研究面临真实任务稀缺、多模态轨迹采集标注困难以及缺乏统一评测基准的问题。

Method: 提出一种基于大语言模型增强的自动化流水线，用于任务生成、执行与质量过滤，并构建涵盖GUI定位、屏幕解析和动作预测的统一基准。

Result: 在流行办公软件中收集了数千条轨迹和超过120万次执行动作，支持GUI+API混合动作空间；评测显示当前视觉-语言模型在开箱即用情况下表现不佳，微调和强化学习可提升性能但仍未达到人类水平。

Conclusion: GUI-360$^\circ$ 有效填补了CUA领域在数据、标注和评测方面的空白，为开发更鲁棒的桌面智能体提供了重要资源。

Abstract: We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and
benchmark suite designed to advance computer-using agents (CUAs). CUAs present
unique challenges and is constrained by three persistent gaps: a scarcity of
real-world CUA tasks, the lack of automated collection-and-annotation pipelines
for multi-modal trajectories, and the absence of a unified benchmark that
jointly evaluates GUI grounding, screen parsing, and action prediction.
  GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated
pipeline for query sourcing, environment-template construction, task
instantiation, batched execution, and LLM-driven quality filtering. The
released corpus contains over 1.2M executed action steps across thousands of
trajectories in popular Windows office applications, and includes
full-resolution screenshots, accessibility metadata when available,
instantiated goals, intermediate reasoning traces, and both successful and
failed action trajectories. The dataset supports three canonical tasks, GUI
grounding, screen parsing, and action prediction, and a hybrid GUI+API action
space that reflects modern agent designs. Benchmarking state-of-the-art
vision--language models on GUI-360$^\circ$ reveals substantial out-of-the-box
shortcomings in grounding and action prediction; supervised fine-tuning and
reinforcement learning yield significant gains but do not close the gap to
human-level reliability. We release GUI-360$^\circ$ and accompanying code to
facilitate reproducible research and accelerate progress on robust desktop
CUAs.
  The full dataset has been made public on
https://huggingface.co/datasets/vyokky/GUI-360.

</details>


### [136] [Probing the Probes: Methods and Metrics for Concept Alignment](https://arxiv.org/abs/2511.04312)
*Jacob Lysnæs-Larsen,Marte Eggen,Inga Strümke*

Main category: cs.AI

TL;DR: 本文指出在可解释AI中，使用线性分类探针获得的概念激活向量（CAVs）的准确性并不能可靠反映概念对齐程度，提出应采用基于空间线性归因的新方法，并引入三类量化指标来评估概念对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的CAV探针准确率高并不意味着其真正捕捉到目标概念，可能存在捕获虚假相关性的风险，因此需要更可靠的评估方式。

Method: 提出一种基于空间线性归因的概念定位方法，并与现有特征可视化技术进行比较；设计三类评估指标：硬准确率、分割分数和增强鲁棒性。

Result: 实验表明，故意引入虚假相关性的探针仍能达到接近标准探针的准确率；具有平移不变性和空间对齐的探针能持续提升概念对齐度。

Conclusion: 仅依赖探针准确率评估CAV不可靠，应采用基于对齐的评估指标，并根据模型结构和目标概念特性设计探针。

Abstract: In explainable AI, Concept Activation Vectors (CAVs) are typically obtained
by training linear classifier probes to detect human-understandable concepts as
directions in the activation space of deep neural networks. It is widely
assumed that a high probe accuracy indicates a CAV faithfully representing its
target concept. However, we show that the probe's classification accuracy alone
is an unreliable measure of concept alignment, i.e., the degree to which a CAV
captures the intended concept. In fact, we argue that probes are more likely to
capture spurious correlations than they are to represent only the intended
concept. As part of our analysis, we demonstrate that deliberately misaligned
probes constructed to exploit spurious correlations, achieve an accuracy close
to that of standard probes. To address this severe problem, we introduce a
novel concept localization method based on spatial linear attribution, and
provide a comprehensive comparison of it to existing feature visualization
techniques for detecting and mitigating concept misalignment. We further
propose three classes of metrics for quantitatively assessing concept
alignment: hard accuracy, segmentation scores, and augmentation robustness. Our
analysis shows that probes with translation invariance and spatial alignment
consistently increase concept alignment. These findings highlight the need for
alignment-based evaluation metrics rather than probe accuracy, and the
importance of tailoring probes to both the model architecture and the nature of
the target concept.

</details>


### [137] [AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research](https://arxiv.org/abs/2511.04316)
*Tim Beyer,Jonas Dornbusch,Jakob Steimle,Moritz Ladenburger,Leo Schwinn,Stephan Günnemann*

Main category: cs.AI

TL;DR: AdversariaLLM是一个用于LLM越狱鲁棒性研究的工具箱，旨在解决当前大模型安全研究中碎片化和难以复现的问题，支持多种攻击算法、基准数据集和开源模型，强调可复现性、正确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型安全与鲁棒性研究领域存在实现、数据集和评估方法碎片化的问题，导致研究难以复现和比较，阻碍了有效进展。

Method: 设计并实现了一个名为AdversariaLLM的开源工具箱，集成了12种对抗攻击算法、7个涵盖有害性、过度拒绝和效用评估的基准数据集，并通过Hugging Face支持多种开源大模型；同时引入计算资源追踪、确定性结果和分布评估等机制以提升可比性和可复现性，并通过JudgeZoo实现自动化评判。

Result: AdversariaLLM提供了一个透明、可比较且可复现的研究框架，支持全面的LLM安全评估，提升了研究的可靠性和一致性。

Conclusion: AdversariaLLM为大语言模型的安全性研究建立了一个坚实、开放的基础，有助于推动该领域的标准化和可持续发展。

Abstract: The rapid expansion of research on Large Language Model (LLM) safety and
robustness has produced a fragmented and oftentimes buggy ecosystem of
implementations, datasets, and evaluation methods. This fragmentation makes
reproducibility and comparability across studies challenging, hindering
meaningful progress. To address these issues, we introduce AdversariaLLM, a
toolbox for conducting LLM jailbreak robustness research. Its design centers on
reproducibility, correctness, and extensibility. The framework implements
twelve adversarial attack algorithms, integrates seven benchmark datasets
spanning harmfulness, over-refusal, and utility evaluation, and provides access
to a wide range of open-weight LLMs via Hugging Face. The implementation
includes advanced features for comparability and reproducibility such as
compute-resource tracking, deterministic results, and distributional evaluation
techniques. \name also integrates judging through the companion package
JudgeZoo, which can also be used independently. Together, these components aim
to establish a robust foundation for transparent, comparable, and reproducible
research in LLM safety.

</details>


### [138] [RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation](https://arxiv.org/abs/2511.04328)
*Jiahao Zhao,Luxin Xu,Minghuan Tan,Lichao Zhang,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: 提出一个名为RxSafeBench的基准框架，用于评估大语言模型在药物安全方面的表现，通过构建包含真实临床风险场景的数据库，发现当前模型在处理禁忌症和药物相互作用方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏真实世界数据集以及对大语言模型在实际临床咨询中药物安全性的评估不足，亟需系统性评估其药物安全能力。

Method: 构建模拟临床咨询的评估框架，生成包含药物风险的问诊对话，建立包含禁忌症、药物相互作用等信息的RxRisk DB数据库，并采用两阶段过滤策略创建高质量的RxSafeBench基准，使用结构化选择题评估主流开源和专有大语言模型。

Result: 当前大语言模型在整合禁忌症和药物相互作用知识方面表现不佳，尤其当风险信息为隐含而非明确提示时；RxSafeBench成为首个全面评估大语言模型药物安全性的基准。

Conclusion: 大语言模型在药物安全应用中仍面临重大挑战，需通过改进提示方法和任务特定微调来提升可靠性，RxSafeBench为推动更安全可信的AI临床决策支持提供了重要工具。

Abstract: Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.

</details>


### [139] [Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning](https://arxiv.org/abs/2511.04341)
*Nick Oh,Fernand Gobet*

Main category: cs.AI

TL;DR: 提出了Monitor-Generate-Verify (MGV)框架，通过引入元认知监控机制扩展了生成-验证范式，以解决模型在推理过程中过早陷入次优路径的问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时推理架构缺乏对推理启动时机和方式的监控过程，导致模型容易陷入前缀主导陷阱，造成约20%的准确率损失。

Method: 将Flavell及Nelson和Narens的元认知理论形式化为计算规范，提出MGV框架，在生成前加入显式监控模块，并利用验证反馈优化后续监控。

Result: 虽无实证验证，但首次系统性地将基础元认知理论转化为计算框架，提供了理解推理系统失效的原理性语言。

Conclusion: MGV框架为未来的测试时推理架构设计提供了具体的干预方向和理论基础。

Abstract: Test-time reasoning architectures such as those following the Generate-Verify
paradigm -- where a model iteratively refines or verifies its own generated
outputs -- prioritise generation and verification but exclude the monitoring
processes that determine when and how reasoning should begin. This omission may
contribute to the prefix dominance trap, in which models commit early to
suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy
loss. We address this architectural gap by formalising Flavell's and Nelson and
Narens' metacognitive theories into computational specifications, proposing the
Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify
paradigm by adding explicit monitoring that captures metacognitive experiences
(from difficulty assessments to confidence judgements) before generation begins
and refines future monitoring through verification feedback. Though we present
no empirical validation, this work provides the first systematic computational
translation of foundational metacognitive theories, offering a principled
vocabulary for understanding reasoning system failures and suggesting specific
architectural interventions for future test-time reasoning designs.

</details>


### [140] [Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach](https://arxiv.org/abs/2511.04393)
*Chanwoo Park,Ziyang Chen,Asuman Ozdaglar,Kaiqing Zhang*

Main category: cs.AI

TL;DR: 提出了一种名为Iterative Regret-Minimization Fine-Tuning（Iterative RMFT）的后训练方法，通过反复蒸馏低遗憾决策轨迹来提升大语言模型在动态环境中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型原本并非为决策任务设计，在在线决策问题中表现不佳，难以实现低遗憾或有效的探索-利用权衡。因此需要一种能增强其决策能力的方法。

Method: 提出Iterative RMFT：每轮生成多个决策轨迹，选择其中k个遗憾最低的轨迹，并用这些轨迹对模型进行微调；利用遗憾值作为指标，引导模型自我提炼决策能力和推理逻辑。

Result: 在多种模型上验证了该方法的有效性，包括Transformer、开源LLM和GPT-4o mini等，在不同任务结构下均表现出更强的决策性能和泛化能力；同时理论分析表明单层Transformer在此框架下可在简化设定中成为无遗憾学习者。

Conclusion: Iterative RMFT是一种通用且原则性强的后训练框架，能够有效提升大语言模型在交互式动态环境中的决策能力，且不依赖人工设计的模板或外部算法动作序列。

Abstract: Large language models (LLMs) are increasingly deployed as "agents" for
decision-making (DM) in interactive and dynamic environments. Yet, since they
were not originally designed for DM, recent studies show that LLMs can struggle
even in basic online DM problems, failing to achieve low regret or an effective
exploration-exploitation tradeoff. To address this, we introduce Iterative
Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure
that repeatedly distills low-regret decision trajectories back into the base
model. At each iteration, the model rolls out multiple decision trajectories,
selects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior
methods that (a) distill action sequences from known DM algorithms or (b) rely
on manually crafted chain-of-thought templates, our approach leverages the
regret metric to elicit the model's own DM ability and reasoning rationales.
This reliance on model-generated reasoning avoids rigid output engineering and
provides more flexible, natural-language training signals. Empirical results
show that Iterative RMFT improves LLMs' DM performance across diverse models -
from Transformers with numerical input/output, to open-weight LLMs, and
advanced closed-weight models like GPT-4o mini. Its flexibility in output and
reasoning formats enables generalization across tasks with varying horizons,
action spaces, reward processes, and natural-language contexts. Finally, we
provide theoretical insight showing that a single-layer Transformer under this
paradigm can act as a no-regret learner in a simplified setting. Overall,
Iterative RMFT offers a principled and general post-training framework for
enhancing LLMs' decision-making capabilities.

</details>


### [141] [The Peril of Preference: Why GRPO fails on Ordinal Rewards](https://arxiv.org/abs/2511.04439)
*Anisha Garg,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 提出Correctness Relative Policy Optimization (CoRPO)，通过自适应基线解决GRPO在非二元反馈下错误强化失败行为的问题，在代码验证任务中表现出更稳定的收敛和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: GRPO的简单性在使用序数奖励时导致其对失败轨迹赋予正优势，从而错误地强化不正确行为，限制了LLM从丰富反馈中学习的能力。

Method: 引入CoRPO，采用自适应基线机制：首先设定最低质量阈值，确保失败解不被正向强化；当策略稳定达标后，自动切换至相对偏好模式，推动模型寻找最优解而非仅可接受解。

Result: 在代码验证任务中，CoRPO相比GRPO展现出更稳定的训练收敛性和更强的跨领域泛化能力。

Conclusion: CoRPO有效解决了GRPO在非二元反馈下的缺陷，是实现LLM通过强化学习掌握新能力的重要进展，支持从二元到序数乃至更密集的逐步监督学习路径。

Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly
desirable for adapting LLMs to become experts at specific tasks. But this
simplicity also makes it ill-specified as we seek to enhance RL training with
richer, non-binary feedback. When using ordinal rewards to give partial credit,
GRPO's simplicity starts to hurt, as its group-average baseline often assigns a
positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new
formulation that solves this flaw. CoRPO uses an adaptive baseline that
enforces a minimum quality threshold, ensuring failed solutions are never
positively reinforced. Once the policy consistently meets this threshold, the
baseline automatically transitions to a relative preference mode, pushing the
model to find optimal solutions rather than just "acceptable" ones. We
empirically validate CoRPO on a code verification task, where it demonstrates
more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to
enable LLMs to learn genuinely new capabilities through reinforcement learning.
We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback
- progressing from binary to ordinal rewards in this work, and onward to
denser, per-step supervision.

</details>


### [142] [Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context](https://arxiv.org/abs/2511.04464)
*Carnot Braun,Rafael O. Jarczewski,Gabriel U. Talasso,Leandro A. Villas,Allan M. de Souza*

Main category: cs.AI

TL;DR: 本文提出了PAVe（个性化代理车辆路由）系统，结合经典多目标Dijkstra算法与大语言模型（LLM）代理，通过语义理解用户任务、偏好和约束，实现更智能的城市路径规划。实验表明其初始路径选择准确率超过88%。


<details>
  <summary>Details</summary>
Motivation: 传统车辆路径系统难以理解和整合驾驶员复杂的语义与动态上下文需求（如多步骤任务、紧急情况等），缺乏个性化能力。

Method: 采用多目标（时间、CO2）Dijkstra算法生成候选路径，利用大语言模型代理结合预处理的地理空间POI缓存，根据用户任务、偏好和规避规则对路径进行语义评估与优化。

Result: 在真实城市场景基准测试中，PAVe能有效将复杂用户意图转化为合理路径调整，本地模型初始路径选择准确率超过88%。

Conclusion: 将经典路径算法与基于LLM的语义推理层结合，是实现个性化、自适应且可扩展的城市出行优化的有效途径。

Abstract: Traditional vehicle routing systems efficiently optimize singular metrics
like time or distance, and when considering multiple metrics, they need more
processes to optimize . However, they lack the capability to interpret and
integrate the complex, semantic, and dynamic contexts of human drivers, such as
multi-step tasks, situational constraints, or urgent needs. This paper
introduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a
hybrid agentic assistant designed to augment classical pathfinding algorithms
with contextual reasoning. Our approach employs a Large Language Model (LLM)
agent that operates on a candidate set of routes generated by a multi-objective
(time, CO2) Dijkstra algorithm. The agent evaluates these options against
user-provided tasks, preferences, and avoidance rules by leveraging a
pre-processed geospatial cache of urban Points of Interest (POIs). In a
benchmark of realistic urban scenarios, PAVe successfully used complex user
intent into appropriate route modifications, achieving over 88% accuracy in its
initial route selections with a local model. We conclude that combining
classical routing algorithms with an LLM-based semantic reasoning layer is a
robust and effective approach for creating personalized, adaptive, and scalable
solutions for urban mobility optimization.

</details>


### [143] [Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis](https://arxiv.org/abs/2511.04481)
*Lars Krupp,Daniel Geißler,Vishal Banwari,Paul Lukowicz,Jakob Karolus*

Main category: cs.AI

TL;DR: 本文探讨了网络代理（如OpenAI的Operator和Google的Project Mariner）在自主操作互联网任务时的能源消耗和二氧化碳排放问题，通过理论估算和实证基准测试，揭示了不同设计哲学对能耗的显著影响，并呼吁在评估中引入专门的能耗指标。


<details>
  <summary>Details</summary>
Motivation: 尽管网络代理技术发展迅速，但其带来的可持续性问题，特别是能源消耗和碳排放，尚未得到充分研究。本文旨在引起对该问题的关注。

Method: 采用理论估算和实证基准测试相结合的方法，评估不同网络代理的能源消耗和CO2排放。

Result: 研究发现，不同的网络代理设计哲学会显著影响能耗，且更高的能耗并不一定带来更好的性能；同时，模型参数和流程披露不足限制了能耗估算的准确性。

Conclusion: 应改变对网络代理的评估方式，在基准测试中纳入专门的能耗衡量指标，以促进更可持续的技术发展。

Abstract: Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful
agentic systems pushing the boundaries of Large Language Models (LLM). They can
autonomously interact with the internet at the user's behest, such as
navigating websites, filling search masks, and comparing price lists. Though
web agent research is thriving, induced sustainability issues remain largely
unexplored. To highlight the urgency of this issue, we provide an initial
exploration of the energy and $CO_2$ cost associated with web agents from both
a theoretical -via estimation- and an empirical perspective -by benchmarking.
Our results show how different philosophies in web agent creation can severely
impact the associated expended energy, and that more energy consumed does not
necessarily equate to better results. We highlight a lack of transparency
regarding disclosing model parameters and processes used for some web agents as
a limiting factor when estimating energy consumption. Our work contributes
towards a change in thinking of how we evaluate web agents, advocating for
dedicated metrics measuring energy consumption in benchmarks.

</details>


### [144] [Optimizing Sensor Placement in Urban Storm Sewers: A Data-Driven Sparse Sensing Approach](https://arxiv.org/abs/2511.04556)
*Zihang Ding,Kun Zhang*

Main category: cs.AI

TL;DR: 提出一种数据驱动的稀疏传感（DSS）框架，结合EPA-SWMM模型，优化城市排水系统中的传感器布局并重建峰值流量，具有高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 城市地表水洪水频发，受限于时间、预算和技术，难以实现高时空分辨率的洪水监测与预测，亟需在资源受限条件下有效监控排水系统并预测水流状况。

Method: 利用SWMM模型生成暴雨管网中峰值流量的训练数据集，结合奇异值分解（SVD）降维和QR分解进行传感器选址，通过DSS框架识别最优监测节点，并使用模拟数据验证其代表性。

Result: 在77个节点中仅布置3个传感器即实现了峰值流量重构的良好性能，Nash-Sutcliffe效率系数达0.92–0.95，且对测量不确定性和传感器故障具有良好的鲁棒性，性能随传感器数量增加而提升。

Conclusion: 该DSS框架在计算效率与物理可解释性之间取得平衡，可用最少的传感器实现高精度流量重构，未来可集成至预警与实时控制系统，适用于资源有限的城市洪水监测场景。

Abstract: Urban surface water flooding, triggered by intense rainfall overwhelming
drainage systems, is increasingly frequent and widespread. While flood
prediction and monitoring in high spatial-temporal resolution are desired,
practical constraints in time, budget, and technology hinder its full
implementation. How to monitor urban drainage networks and predict flow
conditions under constrained resource is a major challenge. This study presents
a data-driven sparse sensing (DSS) framework, integrated with EPA-SWMM, to
optimize sensor placement and reconstruct peak flowrates in a stormwater
system, using the Woodland Avenue catchment in Duluth, Minnesota, as a case
study. We utilized a SWMM model to generate a training dataset of peak flowrate
profiles across the stormwater network. Furthermore, we applied DSS -
leveraging singular value decomposition for dimensionality reduction and QR
factorization for sensor allocation - to identify the optimal monitoring nodes
based on the simulated training dataset. We then validated the
representativeness of these identified monitoring nodes by comparing the
DSS-reconstructed peak flowrate profiles with those obtained from SWMM. Three
optimally placed sensors among 77 nodes achieved satisfactory reconstruction
performance with Nash-Sutcliffe Efficiency (NSE) values of 0.92-0.95 (25th to
75th percentiles). In addition, the model showed good robustness to uncertainty
in measurements. Its robustness to sensor failures is location-dependent and
improves with the number of sensors deployed. The framework balances
computational efficiency and physical interpretability, enabling high-accuracy
flow reconstruction with minimal sensors. This DSS framework can be further
integrated with predictive models to realize flood early warning and real-time
control under limited sensing and monitoring resource.

</details>


### [145] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: 本文提出了Jr. AI Scientist，一个模拟新手研究者科研流程的自主AI科学家系统，能够分析论文局限、提出假设、实验验证并撰写论文，在多文件复杂实现中展现出科学价值，评估显示其生成论文质量优于现有全自动系统，但也揭示了应用风险与未来挑战。


<details>
  <summary>Details</summary>
Motivation: 为了确保AI驱动科研的可信与可持续发展，并维护学术生态的完整性，需要理解当前AI科学家系统的功能与风险。

Method: 开发了一个名为Jr. AI Scientist的系统，模拟新手研究者的科研流程，包括分析基线论文、提出新假设、实验验证和撰写论文，并利用现代编码代理处理复杂的多文件实现。

Result: 在自动化评估、作者评估及Agents4Science提交中，Jr. AI Scientist生成的论文评审得分高于现有全自动系统，但评估也揭示了其存在若干局限性和潜在风险。

Conclusion: Jr. AI Scientist在科学贡献方面表现优越，但当前AI科学家系统仍存在应用风险，需进一步研究以应对关键挑战。

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


### [146] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 本文提出了一种将自然语言查询中的歧义视为合作交互特征的框架，区分了可解析的合作性查询与不可解析的非合作性查询，并分析了15个常用数据集中查询类型的混合问题，倡导在表格数据自然语言接口的设计与评估中更注重合作性。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据的自然语言接口通常将查询歧义视为缺陷，导致评估体系无法准确衡量系统的解析或执行能力。作者希望重新定义歧义的角色，将其转化为人机协作的机会，从而改进系统设计和评估方法。

Method: 提出一个原则性框架，用于区分合作性与非合作性查询；基于该框架对15个流行的表格问答与分析数据集中的查询进行分类和分析，揭示当前数据集中查询类型混杂的问题。

Result: 发现现有数据集普遍存在合作性与非合作性查询的无控制混合，这种混合影响了对系统执行准确性和语义解析能力的独立评估。

Conclusion: 应将歧义处理从‘消除’转向‘合作解决’，未来自然语言接口的设计与评估应更明确地区分查询类型，促进更具协作性的交互范式。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [147] [Question the Questions: Auditing Representation in Online Deliberative Processes](https://arxiv.org/abs/2511.04588)
*Soham De,Lodewijk Gelauff,Ashish Goel,Smitha Milli,Ariel Procaccia,Alice Siu*

Main category: cs.AI

TL;DR: 提出了一种基于公正代表性的审计框架，用于衡量在公民大会等审议过程中所选问题对参与者兴趣的代表性，并开发了高效的算法进行评估，结果表明大语言模型在支持审议过程方面既有潜力也有局限。


<details>
  <summary>Details</summary>
Motivation: 在审议过程中，由于时间限制只能选择少量问题向专家提问，如何确保这些问题能充分代表所有参与者的关切成为一个关键挑战。

Method: 引入基于公正代表性（JR）的审计框架，并设计了适用于一般效用设置的审计算法，最高效算法运行时间为$O(mn\log n)$，并应用于实际审议数据中，比较不同方法选择问题的代表性。

Result: 实际由主持人选择的问题代表性有限；整数线性规划方法表现良好；大语言模型生成的摘要问题有一定代表性但仍有不足。

Conclusion: 所提出的审计框架和算法可有效提升审议过程中问题选择的代表性，有助于改进民主审议实践，且已集成到实际使用的在线平台中。

Abstract: A central feature of many deliberative processes, such as citizens'
assemblies and deliberative polls, is the opportunity for participants to
engage directly with experts. While participants are typically invited to
propose questions for expert panels, only a limited number can be selected due
to time constraints. This raises the challenge of how to choose a small set of
questions that best represent the interests of all participants. We introduce
an auditing framework for measuring the level of representation provided by a
slate of questions, based on the social choice concept known as justified
representation (JR). We present the first algorithms for auditing JR in the
general utility setting, with our most efficient algorithm achieving a runtime
of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number
of proposed questions. We apply our auditing methods to historical
deliberations, comparing the representativeness of (a) the actual questions
posed to the expert panel (chosen by a moderator), (b) participants' questions
chosen via integer linear programming, (c) summary questions generated by large
language models (LLMs). Our results highlight both the promise and current
limitations of LLMs in supporting deliberative processes. By integrating our
methods into an online deliberation platform that has been used for over
hundreds of deliberations across more than 50 countries, we make it easy for
practitioners to audit and improve representation in future deliberations.

</details>


### [148] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: VeriCoT是一种神经符号方法，通过将思维链（CoT）推理步骤形式化为一阶逻辑并验证其逻辑有效性，来检测和纠正大语言模型中的错误推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能通过思维链进行多步推理，但无法可靠验证自身逻辑，可能导致推理过程存在缺陷，影响高风险场景下的可信度。

Method: 提出VeriCoT方法，将CoT的每一步推理转化为一阶逻辑，提取支撑前提（来自上下文、常识或先前步骤），利用符号求解器验证逻辑有效性，并保留自然语言前提以供人类或系统审查。

Result: 在ProofWriter、LegalBench和BioASQ数据集上实验表明，VeriCoT能有效识别错误推理，并准确预测答案正确性；同时其验证信号可用于推理时自省、监督微调和基于偏好的微调，提升模型推理的有效性和准确性。

Conclusion: VeriCoT通过结合符号逻辑验证与神经网络推理，增强了对大模型推理过程的可解释性与可靠性，为构建可信的AI推理系统提供了有效途径。

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [149] [Shellular Metamaterial Design via Compact Electric Potential Parametrization](https://arxiv.org/abs/2511.04025)
*Chang Liu,Bohan Wang*

Main category: cs.GR

TL;DR: 提出了一种紧凑且表达能力强的壳体超材料设计空间，结合高效的GPU基均质化流程，实现了结构的快速评估和逆向设计，生成的结构具有几何多样性及广泛的力学响应，并接近理论性能上限，通过增材制造验证了其可制造性。


<details>
  <summary>Details</summary>
Motivation: 为了实现壳体超材料在几何表达能力与设计效率之间的平衡，并支持快速性能评估与逆向设计，以满足多样化机械性能需求。

Method: 构建仅含几十个自由度的紧凑设计空间，涵盖从平面结构到复杂三周期极小曲面的几何形态；开发基于GPU的高效均质化流水线，在20毫秒内完成结构评估，并在0.5秒内计算有效弹性张量。

Result: 实现了设计空间的全面探索与针对目标宏观性能的逆向设计；所得结构覆盖广泛材料性能，最高达到理论性能上限的91.86%；并通过增材制造验证了实际可制造性。

Conclusion: 该方法在保持设计简洁的同时实现了高表达能力，显著提升了壳体超材料的设计效率与性能优化能力，具备良好的工程应用前景。

Abstract: We introduce a compact yet highly expressive design space for shellular
metamaterials. By employing only a few dozen degrees of freedom, this design
space represents geometries ranging from simple planar configurations to
complex triply periodic minimal surfaces. Coupled with this representation, we
develop an efficient GPU-based homogenization pipeline that evaluates the
structure in under 20 ms and computes the corresponding effective elastic
tensor in near-real-time (0.5 s). The high speed of this evaluation facilitates
an exhaustive exploration of the design space and supports an inverse-design
scheme that tailors the shellular structure to specific macroscopic target
property. Structures derived through this approach exhibit not only geometric
diversity but also a wide spectrum of mechanical responses, covering a broad
range of material properties. Moreover, they achieve up to 91.86% of
theoretical upper bounds, a level of performance comparable to state-of-the-art
shellular structures with low solid volume. Finally, our prototypes, fabricated
via additive manufacturing, confirm the practical manufacturability of these
designs, underscoring their potential for real-world engineering applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [150] [Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland](https://arxiv.org/abs/2511.03749)
*Oluwadurotimi Onibonoje,Vuong M. Ngo,Andrew McCarre,Elodie Ruelle,Bernadette O-Briend,Mark Roantree*

Main category: cs.LG

TL;DR: 提出基于深度学习的单变量模型，特别是时间卷积网络，用于预测爱尔兰黑麦草生长，具有高精度和实用性，有助于提升可持续奶牛养殖。


<details>
  <summary>Details</summary>
Motivation: 现有草地生长预测依赖不切实际的机理模型，且爱尔兰乳制品行业面临盈利与可持续性挑战，亟需高效、低成本的预测方法。

Method: 采用针对单变量数据定制的深度学习模型，特别是时间卷积网络（TCN），利用34年共1757周的历史草高数据进行训练与验证。

Result: 所提出的TCN模型在预测黑麦草生长中表现优异，RMSE为2.74，MAE为3.46，验证了其在长期时间序列预测中的有效性与稳定性。

Conclusion: 该研究提升了对草地生长预测模型行为的理解，增强了预测可靠性，为可持续乳品农业提供了可行的技术支持。

Abstract: Grasslands, constituting the world's second-largest terrestrial carbon sink,
play a crucial role in biodiversity and the regulation of the carbon cycle.
Currently, the Irish dairy sector, a significant economic contributor, grapples
with challenges related to profitability and sustainability. Presently, grass
growth forecasting relies on impractical mechanistic models. In response, we
propose deep learning models tailored for univariate datasets, presenting
cost-effective alternatives. Notably, a temporal convolutional network designed
for forecasting Perennial Ryegrass growth in Cork exhibits high performance,
leveraging historical grass height data with RMSE of 2.74 and MAE of 3.46.
Validation across a comprehensive dataset spanning 1,757 weeks over 34 years
provides insights into optimal model configurations. This study enhances our
understanding of model behavior, thereby improving reliability in grass growth
forecasting and contributing to the advancement of sustainable dairy farming
practices.

</details>


### [151] [Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices](https://arxiv.org/abs/2511.03753)
*Youssef Elmir,Yassine Himeur,Abbes Amira*

Main category: cs.LG

TL;DR: 本研究提出了一种基于Gramian Angular Field（GAF）图像的联邦学习（FL）框架，用于物联网（IoT）医疗环境中的隐私保护心电图（ECG）分类。该方法将1D ECG信号转换为2D GAF图像，并利用CNN进行特征提取，同时确保敏感数据保留在本地设备中。实验在服务器、笔记本电脑和树莓派4上部署，验证了其在异构IoT设备上的可行性，实现了95.18%的高分类准确率，显著优于单客户端基线模型。


<details>
  <summary>Details</summary>
Motivation: 在IoT医疗环境中，如何在保护患者隐私的同时实现高效的ECG信号分类是一个关键挑战。集中式训练存在数据泄露风险，而联邦学习允许多设备协作训练而不共享原始数据，结合GAF图像化方法可提升CNN对时序信号的处理能力，因此探索适用于资源受限设备的隐私保护ECG分类方案具有重要意义。

Method: 采用Gramian Angular Field（GAF）将一维ECG信号转化为二维图像，利用卷积神经网络（CNN）进行特征学习；在此基础上构建联邦学习框架，在多个异构IoT设备（包括服务器、笔记本电脑和Raspberry Pi 4）之间协同训练模型，原始数据保留在本地，仅上传模型参数。通过跨设备实验评估分类性能、通信开销与资源利用率。

Result: 所提出的FL-GAF模型在多客户端设置下达到了95.18%的分类准确率，显著优于单客户端基线模型，且训练时间更短；尽管GAF转换带来一定计算负担，但整体通信开销低，资源利用高效，验证了其在边缘-云集成IoT环境中的可行性与优越性。

Conclusion: 该研究表明，结合GAF图像化与联邦学习的轻量级AI框架能够在保障隐私的前提下，有效支持IoT环境下ECG信号的高精度分类，具备在智能健康系统中实现可扩展、安全边缘部署的潜力。

Abstract: This study presents a federated learning (FL) framework for
privacy-preserving electrocardiogram (ECG) classification in Internet of Things
(IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian
Angular Field (GAF) images, the proposed approach enables efficient feature
extraction through Convolutional Neural Networks (CNNs) while ensuring that
sensitive medical data remain local to each device. This work is among the
first to experimentally validate GAF-based federated ECG classification across
heterogeneous IoT devices, quantifying both performance and communication
efficiency. To evaluate feasibility in realistic IoT settings, we deployed the
framework across a server, a laptop, and a resource-constrained Raspberry Pi 4,
reflecting edge-cloud integration in IoT ecosystems. Experimental results
demonstrate that the FL-GAF model achieves a high classification accuracy of
95.18% in a multi-client setup, significantly outperforming a single-client
baseline in both accuracy and training time. Despite the added computational
complexity of GAF transformations, the framework maintains efficient resource
utilization and communication overhead. These findings highlight the potential
of lightweight, privacy-preserving AI for IoT-based healthcare monitoring,
supporting scalable and secure edge deployments in smart health systems.

</details>


### [152] [Laugh, Relate, Engage: Stylized Comment Generation for Short Videos](https://arxiv.org/abs/2511.03757)
*Xuan Ouyang,Senan Wang,Bouzhou Wang,Siyuan Xiahou,Jinrong Zhou,Yuekang Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为LOLGORITHM的模块化多智能体系统，用于可控的短视频评论生成，支持六种不同风格的评论，并通过多模态大语言模型实现细粒度风格控制，在中英文数据集上均表现出优越的用户偏好。


<details>
  <summary>Details</summary>
Motivation: 在短视频平台上，评论对于促进社区参与和内容再创作至关重要，但生成既符合平台规范又具有风格多样性和上下文感知能力的评论仍是一个挑战。

Method: 设计了一个模块化的多智能体系统（MAS），结合视频分割、上下文与情感分析以及风格感知提示构建，利用多模态大语言模型直接处理视频输入，并通过显式提示标记和少量示例实现精细的风格控制。

Result: 构建了覆盖五类热门视频的双语数据集，结合自动指标和包含40个视频、105名参与者的大型人类偏好研究进行评估，结果显示LOLGORITHM在抖音上的偏好率达到90%以上，在YouTube上达到87.55%，显著优于基线模型。

Conclusion: LOLGORITHM提供了一个可扩展且文化适应性强的框架，有望提升短视频平台的用户参与度和创造性互动。

Abstract: Short-video platforms have become a central medium in the modern Internet
landscape, where efficient information delivery and strong interactivity are
reshaping user engagement and cultural dissemination. Among the various forms
of user interaction, comments play a vital role in fostering community
participation and enabling content re-creation. However, generating comments
that are both compliant with platform guidelines and capable of exhibiting
stylistic diversity and contextual awareness remains a significant challenge.
We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for
controllable short-video comment generation. The system integrates video
segmentation, contextual and affective analysis, and style-aware prompt
construction. It supports six distinct comment styles: puns (homophones),
rhyming, meme application, sarcasm (irony), plain humor, and content
extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM
directly processes video inputs and achieves fine-grained style control through
explicit prompt markers and few-shot examples. To support development and
evaluation, we construct a bilingual dataset using official APIs from Douyin
(Chinese) and YouTube (English), covering five popular video genres: comedy
skits, daily life jokes, funny animal clips, humorous commentary, and talk
shows. Evaluation combines automated metrics originality, relevance, and style
conformity with a large-scale human preference study involving 40 videos and
105 participants. Results show that LOLGORITHM significantly outperforms
baseline models, achieving preference rates of over 90% on Douyin and 87.55% on
YouTube. This work presents a scalable and culturally adaptive framework for
stylized comment generation on short-video platforms, offering a promising path
to enhance user engagement and creative interaction.

</details>


### [153] [What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes](https://arxiv.org/abs/2511.03768)
*Candace Ross,Florian Bordes,Adina Williams,Polina Kirichenko,Mark Ibrahim*

Main category: cs.LG

TL;DR: 本文提出了一种新的真实场景多模态推理基准Common-O，旨在评估模型在跨场景推理中的表现，发现即使最先进的模型在该任务上表现不佳，尤其是在复杂场景中，揭示了现有模型在避免幻觉和真实世界推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型在感知基准上表现饱和，但在真实世界场景推理中仍存在严重幻觉问题，缺乏有效评估跨场景推理能力的基准。

Method: 构建了一个包含超过10.5k新图像的基准Common-O，避免训练数据污染，借鉴人类认知测试设计‘什么共同点’任务，评估多种先进多模态语言模型（包括具备链式思维能力的模型）在单图感知和跨场景推理上的表现。

Result: 最佳模型在Common-O上仅达到35%准确率，在更复杂的Common-O Complex上仅为1%；模型在存在相似物体时更容易产生幻觉；模型规模带来轻微提升，而显式多图训练的模型表现更好。

Conclusion: 当前多模态模型在跨场景推理方面仍有重大缺陷，尤其在复杂场景中易产生幻觉；多图输入训练可能是改善推理能力的有效方向，Common-O为研究该问题提供了新基准。

Abstract: Multimodal language models possess a remarkable ability to handle an
open-vocabulary's worth of objects. Yet the best models still suffer from
hallucinations when reasoning about scenes in the real world, revealing a gap
between their seemingly strong performance on existing perception benchmarks
that are saturating and their reasoning in the real world. To address this gap,
we build a novel benchmark of in-the-wild scenes that we call Common-O. With
more than 10.5k examples using exclusively new images not found in web training
data to avoid contamination, Common-O goes beyond just perception, inspired by
cognitive tests for humans, to probe reasoning across scenes by asking "what's
in common?". We evaluate leading multimodal language models, including models
specifically trained to perform chain-of-thought reasoning. We find that
perceiving objects in single images is tractable for most models, yet reasoning
across scenes is very challenging even for the best models, including reasoning
models. Despite saturating many leaderboards focusing on perception, the best
performing model only achieves 35% on Common-O -- and on Common-O Complex,
consisting of more complex scenes, the best model achieves only 1%. Curiously,
we find models are more prone to hallucinate when similar objects are present
in the scene, suggesting models may be relying on object co-occurrence seen
during training. Among the models we evaluated, we found scale can provide
modest improvements while models explicitly trained with multi-image inputs
show bigger improvements, suggesting scaled multi-image training may offer
promise. We make our benchmark publicly available to spur research into the
challenge of hallucination when reasoning across scenes.

</details>


### [154] [Contamination Detection for VLMs using Multi-Modal Semantic Perturbation](https://arxiv.org/abs/2511.03774)
*Jaden Park,Mu Cai,Feng Yao,Jingbo Shang,Soochahn Lee,Yong Jae Lee*

Main category: cs.LG

TL;DR: 本文提出了一种基于多模态语义扰动的新型检测方法，用于识别因训练数据污染而导致性能虚高的视觉-语言模型（VLMs），并通过多种现实污染策略验证了其有效性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于视觉-语言模型（VLMs）在大规模、常为专有的预训练数据上可能存在测试集泄露问题，导致性能被高估，因此需要有效的方法来检测模型是否受到数据污染。

Method: 通过在开源VLM上故意引入污染，评估现有检测方法的局限性，并提出一种基于多模态语义扰动的简单而有效的检测方法，观察受污染模型在扰动下的泛化能力。

Result: 实验表明现有检测方法表现不佳或不稳定，而所提出的语义扰动方法能有效区分受污染模型，在多种污染场景下均表现出良好的鲁棒性和准确性。

Conclusion: 基于语义扰动的检测方法为识别VLM中的测试集泄露问题提供了可行方案，有助于提升模型评估的可信度。

Abstract: Recent advances in Vision-Language Models (VLMs) have achieved
state-of-the-art performance on numerous benchmark tasks. However, the use of
internet-scale, often proprietary, pretraining corpora raises a critical
concern for both practitioners and users: inflated performance due to test-set
leakage. While prior works have proposed mitigation strategies such as
decontamination of pretraining data and benchmark redesign for LLMs, the
complementary direction of developing detection methods for contaminated VLMs
remains underexplored. To address this gap, we deliberately contaminate
open-source VLMs on popular benchmarks and show that existing detection
approaches either fail outright or exhibit inconsistent behavior. We then
propose a novel simple yet effective detection method based on multi-modal
semantic perturbation, demonstrating that contaminated models fail to
generalize under controlled perturbations. Finally, we validate our approach
across multiple realistic contamination strategies, confirming its robustness
and effectiveness. The code and perturbed dataset will be released publicly.

</details>


### [155] [FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features](https://arxiv.org/abs/2511.03806)
*Linghui Zeng,Ruixuan Liu,Atiquer Rahman Sarkar,Xiaoqian Jiang,Joyce C. Ho,Li Xiong*

Main category: cs.LG

TL;DR: 提出FusionDP框架，在特征级差分隐私下通过基础模型填补敏感特征，提升模型效用。


<details>
  <summary>Details</summary>
Motivation: 传统DP-SGD对所有特征施加隐私保护，导致非敏感特征也被加噪，影响模型性能；而实际中仅部分特征（如年龄、性别）更敏感，需针对性保护。

Method: FusionDP分两步：首先利用大基础模型根据非敏感特征推断敏感特征，作为外部先验；然后在原始和填补特征上训练模型，结合改进的DP-SGD保证敏感特征的隐私。

Result: 在PhysioNet脓毒症预测和MIMIC-III临床文本分类任务中，FusionDP相比基线方法在保持严格特征级隐私的同时显著提升模型性能。

Conclusion: FusionDP通过基础模型驱动的填补机制，有效改善了多模态数据下的隐私-效用权衡，为特征级隐私保护提供了新思路。

Abstract: Ensuring the privacy of sensitive training data is crucial in
privacy-preserving machine learning. However, in practical scenarios, privacy
protection may be required for only a subset of features. For instance, in ICU
data, demographic attributes like age and gender pose higher privacy risks due
to their re-identification potential, whereas raw lab results are generally
less sensitive. Traditional DP-SGD enforces privacy protection on all features
in one sample, leading to excessive noise injection and significant utility
degradation. We propose FusionDP, a two-step framework that enhances model
utility under feature-level differential privacy. First, FusionDP leverages
large foundation models to impute sensitive features given non-sensitive
features, treating them as external priors that provide high-quality estimates
of sensitive attributes without accessing the true values during model
training. Second, we introduce a modified DP-SGD algorithm that trains models
on both original and imputed features while formally preserving the privacy of
the original sensitive features. We evaluate FusionDP on two modalities: a
sepsis prediction task on tabular data from PhysioNet and a clinical note
classification task from MIMIC-III. By comparing against privacy-preserving
baselines, our results show that FusionDP significantly improves model
performance while maintaining rigorous feature-level privacy, demonstrating the
potential of foundation model-driven imputation to enhance the privacy-utility
trade-off for various modalities.

</details>


### [156] [Fair and Explainable Credit-Scoring under Concept Drift: Adaptive Explanation Frameworks for Evolving Populations](https://arxiv.org/abs/2511.03807)
*Shivogo John*

Main category: cs.LG

TL;DR: 本研究提出并评估了三种自适应SHAP解释方法，以应对信用评分系统中因概念漂移导致的数据分布变化，提升了模型解释的稳定性与公平性，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统解释方法（如SHAP）假设数据分布静态不变，难以应对信用评分中持续发生的概念漂移，导致解释不稳定且可能不公平。因此，需要开发能动态适应数据变化的可解释框架。

Method: 结合XGBoost预测模型，采用三种自适应SHAP变体：(A) 按数据切片进行解释重加权以适应特征分布变化；(B) 基于滑动窗口背景样本的漂移感知SHAP重定基线；(C) 使用增量岭回归的在线代理校准。通过多维度指标与静态SHAP对比评估。

Result: 自适应方法显著提升了时间稳定性（方向性和排序稳定性）并减少了对不同人口群体的差异影响，同时未牺牲模型预测性能（AUC、F1）。鲁棒性测试表明其在反事实扰动、背景敏感性和代理变量检测下表现稳健。

Conclusion: 自适应可解释性是维持动态信用评分系统透明性、问责制和伦理可靠性的重要机制，该方法可推广至其他随人群变化而演化的决策系统领域。

Abstract: Evolving borrower behaviors, shifting economic conditions, and changing
regulatory landscapes continuously reshape the data distributions underlying
modern credit-scoring systems. Conventional explainability techniques, such as
SHAP, assume static data and fixed background distributions, making their
explanations unstable and potentially unfair when concept drift occurs. This
study addresses that challenge by developing adaptive explanation frameworks
that recalibrate interpretability and fairness in dynamically evolving credit
models. Using a multi-year credit dataset, we integrate predictive modeling via
XGBoost with three adaptive SHAP variants: (A) per-slice explanation
reweighting that adjusts for feature distribution shifts, (B) drift-aware SHAP
rebaselining with sliding-window background samples, and (C) online surrogate
calibration using incremental Ridge regression. Each method is benchmarked
against static SHAP explanations using metrics of predictive performance (AUC,
F1), directional and rank stability (cosine, Kendall tau), and fairness
(demographic parity and recalibration). Results show that adaptive methods,
particularly rebaselined and surrogate-based explanations, substantially
improve temporal stability and reduce disparate impact across demographic
groups without degrading predictive accuracy. Robustness tests, including
counterfactual perturbations, background sensitivity analysis, and
proxy-variable detection, confirm the resilience of adaptive explanations under
real-world drift conditions. These findings establish adaptive explainability
as a practical mechanism for sustaining transparency, accountability, and
ethical reliability in data-driven credit systems, and more broadly, in any
domain where decision models evolve with population change.

</details>


### [157] [Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters](https://arxiv.org/abs/2511.04073)
*Ananya Sutradhar,Suryansh Gupta,Ravishankar Krishnaswamy,Haiyang Xu,Aseem Rastogi,Gopal Srinivasa*

Main category: cs.LG

TL;DR: 本文提出了一种基于数据学习的过滤近似最近邻搜索方法，通过优化向量距离与标签匹配之间的权衡，显著提升了检索精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的过滤ANN方法使用固定惩罚机制，难以适应不同数据集的标签和向量分布，缺乏泛化能力。

Method: 将距离与标签匹配的权衡建模为约束线性优化问题，从数据中学习最优权重，并将其用于搜索过程和索引构建。

Result: 实验表明，该方法相比固定惩罚方法在精度上提升了5-10%，且更具灵活性和可泛化性。

Conclusion: 通过数据驱动的方式学习距离函数中的权重，能更有效地捕捉标签分布和语义，为过滤ANN搜索提供了更优的解决方案。

Abstract: Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest
vectors for a query vector from a dataset. It enforces that a specified set of
discrete labels $S$ for the query must be included in the labels of each
retrieved vector. Existing graph-based methods typically incorporate filter
awareness by assigning fixed penalties or prioritizing nodes based on filter
satisfaction. However, since these methods use fixed, data in- dependent
penalties, they often fail to generalize across datasets with diverse label and
vector distributions. In this work, we propose a principled alternative that
learns the optimal trade-off between vector distance and filter match directly
from the data, rather than relying on fixed penalties. We formulate this as a
constrained linear optimization problem, deriving weights that better reflect
the underlying filter distribution and more effectively address the filtered
ANN search problem. These learned weights guide both the search process and
index construction, leading to graph structures that more effectively capture
the underlying filter distribution and filter semantics. Our experiments
demonstrate that adapting the distance function to the data significantly im-
proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible
and generalizable framework for the filtered ANN search problem.

</details>


### [158] [Optimizing Reasoning Efficiency through Prompt Difficulty Prediction](https://arxiv.org/abs/2511.03808)
*Bo Zhao,Berkcan Kapusuzoglu,Kartik Balasubramaniam,Sambit Sahu,Supriyo Chakraborty,Genta Indra Winata*

Main category: cs.LG

TL;DR: 提出一种基于问题难度的路由方法，通过将问题分配给最合适的轻量级模型来解决，从而在不牺牲准确率的情况下显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 推理语言模型虽然性能强大，但因其模型体积大和推理路径长导致部署成本高，需要更高效的部署方案。

Method: 利用s1.1-32B模型的中间表示训练轻量级预测器，预测问题难度或模型正确性，并据此在多个推理模型间进行路由分配。

Result: 在多种数学基准上，该路由方法相比随机分配提升了效率，在显著减少计算量的同时达到了与s1.1-32B相当的性能。

Conclusion: 难度感知的路由策略能有效支持推理模型的低成本高效部署。

Abstract: Reasoning language models perform well on complex tasks but are costly to
deploy due to their size and long reasoning traces. We propose a routing
approach that assigns each problem to the smallest model likely to solve it,
reducing compute without sacrificing accuracy. Using intermediate
representations from s1.1-32B, we train lightweight predictors of problem
difficulty or model correctness to guide routing across a pool of reasoning
models. On diverse math benchmarks, routing improves efficiency over random
assignment and matches s1.1-32B's performance while using significantly less
compute. Our results demonstrate that difficulty-aware routing is effective for
cost-efficient deployment of reasoning models.

</details>


### [159] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: 提出SynthKGQA框架，用于从任意知识图谱生成高质量的合成问答数据集，并发布基于Wikidata的GTSQA数据集以评测知识图检索模型的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 缺乏具有真实标签的、面向图检索的挑战性问答数据集，导致难以比较不同知识图检索方法的性能。

Method: 设计SynthKGQA框架，从任意知识图谱自动生成包含完整真实路径的问答数据；应用该框架构建GTSQA数据集，并在多种主流KG增强LLM方法上进行基准测试。

Result: 生成的数据集能更全面地评估检索器性能，且可用于训练更优模型；在GTSQA上的实验表明现有方法在未见图结构和关系类型上泛化能力有限。

Conclusion: SynthKGQA为知识图谱问答提供了可扩展的数据生成方案，GTSQA揭示了当前KG检索器在零样本场景下的不足，推动了更具鲁棒性的检索模型研究。

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


### [160] [One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA](https://arxiv.org/abs/2511.03809)
*François Belias,Naser Ezzati-Jivan,Foutse Khomh*

Main category: cs.LG

TL;DR: 本文提出了DEBA，一种动态高效的批量大小自适应方法，并通过在多种神经网络架构上的系统评估，揭示了不同架构对自适应批处理策略的响应差异显著，表明批量自适应需考虑架构特性。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应批大小方法通常假设一种通用策略适用于所有架构，但忽略了不同架构在优化动态上的差异，导致效果不稳定。因此，需要一种能够根据架构特性动态调整批大小的方法。

Method: 提出DEBA（Dynamic Efficient Batch Adaptation），通过监控梯度方差、梯度范数变化和损失变化来指导批大小的调整；引入基于滑动窗口统计和冷却期的设计；并提出梯度稳定性指标（如稳定性分数）用于预测哪些架构能从自适应中受益。

Result: 在六种架构（ResNet-18/50、DenseNet-121、EfficientNet-B0、MobileNet-V3、ViT-B16）上进行实验表明：轻量级和中等深度架构获得45-62%加速且准确率提升1-7%；ResNet-18准确率提升2.4-4.0%，速度提升36-43%；ResNet-50表现不稳定；ViT-B16仅获6%加速。滑动窗口和足够冷却期（≥5 epoch）被验证为关键设计因素。

Conclusion: 自适应批大小方法的效果高度依赖于网络架构，不能一概而论；必须采用架构感知的设计。DEBA结合梯度稳定性分析，为选择适用的自适应策略提供了可预测的框架。

Abstract: Adaptive batch size methods aim to accelerate neural network training, but
existing approaches apply identical adaptation strategies across all
architectures, assuming a one-size-fits-all solution. We introduce DEBA
(Dynamic Efficient Batch Adaptation), an adaptive batch scheduler that monitors
gradient variance, gradient norm variation and loss variation to guide batch
size adaptations. Through systematic evaluation across six architectures
(ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16) on
CIFAR-10 and CIFAR-100, with five random seeds per configuration, we
demonstrate that the architecture fundamentally determines adaptation efficacy.
Our findings reveal that: (1) lightweight and medium-depth architectures
(MobileNet-V3, DenseNet-121, EfficientNet-B0) achieve a 45-62% training speedup
with simultaneous accuracy improvements of 1-7%; (2) shallow residual networks
(ResNet-18) show consistent gains of +2.4 - 4.0% in accuracy, 36 - 43% in
speedup, while deep residual networks (ResNet-50) exhibit high variance and
occasional degradation; (3) already-stable architectures (ViT-B16) show minimal
speedup (6%) despite maintaining accuracy, indicating that adaptation benefits
vary with baseline optimization characteristics. We introduce a baseline
characterization framework using gradient stability metrics (stability score,
gradient norm variation) that predicts which architectures will benefit from
adaptive scheduling. Our ablation studies reveal critical design choices often
overlooked in prior work: sliding window statistics (vs. full history) and
sufficient cooldown periods (5+ epochs) between adaptations are essential for
success. This work challenges the prevailing assumption that adaptive methods
generalize across architectures and provides the first systematic evidence that
batch size adaptation requires an architecture-aware design.

</details>


### [161] [Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems](https://arxiv.org/abs/2511.04594)
*Utkarsh U. Chavan,Prashant Trivedi,Nandyala Hemachandra*

Main category: cs.LG

TL;DR: 本文研究了基于线性函数逼近的去中心化多智能体随机最短路径（Dec-MASSP）问题，提出了首个该设置下的后悔下界Ω(√K)，并通过构造难以学习的实例揭示了去中心化控制的学习复杂性。


<details>
  <summary>Details</summary>
Motivation: 去中心化多智能体系统在许多应用中至关重要，但多智能体SSP的去中心化学习问题尚未被充分探索，尤其是在线性函数逼近条件下。

Method: 采用基于对称性的新论证方法，识别最优策略的结构，并通过构建难学习实例推导出后悔下界。

Result: 得到了任意n个智能体下Dec-MASSP的首个后悔下界Ω(√K)，表明其固有的学习难度。

Conclusion: 该结果阐明了去中心化控制的学习复杂性，为设计高效的多智能体学习算法提供了理论指导。

Abstract: Multi-agent systems (MAS) are central to applications such as swarm robotics
and traffic routing, where agents must coordinate in a decentralized manner to
achieve a common objective. Stochastic Shortest Path (SSP) problems provide a
natural framework for modeling decentralized control in such settings. While
the problem of learning in SSP has been extensively studied in single-agent
settings, the decentralized multi-agent variant remains largely unexplored. In
this work, we take a step towards addressing that gap. We study decentralized
multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the
transition dynamics and costs are represented using linear models. Applying
novel symmetry-based arguments, we identify the structure of optimal policies.
Our main contribution is the first regret lower bound for this setting based on
the construction of hard-to-learn instances for any number of agents, $n$. Our
regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the
inherent learning difficulty in Dec-MASSPs. These insights clarify the learning
complexity of decentralized control and can further guide the design of
efficient learning algorithms in multi-agent systems.

</details>


### [162] [Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks](https://arxiv.org/abs/2511.03824)
*Ryien Hosseini,Filippo Simini,Venkatram Vishwanath,Rebecca Willett,Henry Hoffmann*

Main category: cs.LG

TL;DR: 提出了一种称为“Sketched Random Features”的方法，通过将随机的全局特征嵌入到标准GNN中，有效缓解了GNN在长距离信息传递、节点表示过平滑和表达能力有限等方面的问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNN）在处理图结构数据时面临长距离信息压缩（oversquashing）、节点表示过平滑（oversmoothing）以及表达能力不足等挑战，本文旨在通过引入全局信息来缓解这些问题。

Method: 引入一种名为Sketched Random Features的随机全局嵌入方法，将其注入标准GNN中。这些嵌入具有唯一性、距离敏感性和拓扑无关性，能够高效捕捉长程依赖关系。

Result: 实验结果表明，该方法在真实世界的图学习任务中 consistently 提升了GNN的性能，且可与图位置编码等现有技术结合使用，进一步增强模型表现。

Conclusion: Sketched Random Features为GNN提供了一种有效的增强手段，既能独立使用，也可作为现有方法的补充，显著缓解GNN的三大关键问题。

Abstract: Graph Neural Networks learn on graph-structured data by iteratively
aggregating local neighborhood information. While this local message passing
paradigm imparts a powerful inductive bias and exploits graph sparsity, it also
yields three key challenges: (i) oversquashing of long-range information, (ii)
oversmoothing of node representations, and (iii) limited expressive power. In
this work we inject randomized global embeddings of node features, which we
term \textit{Sketched Random Features}, into standard GNNs, enabling them to
efficiently capture long-range dependencies. The embeddings are unique,
distance-sensitive, and topology-agnostic -- properties which we analytically
and empirically show alleviate the aforementioned limitations when injected
into GNNs. Experimental results on real-world graph learning tasks confirm that
this strategy consistently improves performance over baseline GNNs, offering
both a standalone solution and a complementary enhancement to existing
techniques such as graph positional encodings. Our source code is available at
\href{https://github.com/ryienh/sketched-random-features}{https://github.com/ryienh/sketched-random-features}.

</details>


### [163] [From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification](https://arxiv.org/abs/2511.03828)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为Energy-Guided Diffusion Stratification（StratDiff）的新方法，通过扩散模型和能量函数对离线数据进行分层，以实现从离线到在线强化学习的平滑过渡。


<details>
  <summary>Details</summary>
Motivation: 现有方法较少显式利用离线数据的分布结构，难以适应不同类型样本，导致离线到在线强化学习转换困难。

Method: 使用扩散模型从离线数据集中学习先验知识，并结合能量函数优化策略模仿和生成类似离线的动作；通过计算KL散度将训练样本分为离线类和在线类，分别采用离线和在线学习目标进行更新。

Result: 在D4RL基准上与Cal-QL和IQL结合使用时，StratDiff显著优于现有方法，表现出更强的适应性和更稳定的性能。

Conclusion: StratDiff通过利用数据分布结构实现了更高效的离线到在线迁移，为混合强化学习提供了一种有效的新范式。

Abstract: Transitioning from offline to online reinforcement learning (RL) poses
critical challenges due to distributional shifts between the fixed behavior
policy in the offline dataset and the evolving policy during online learning.
Although this issue is widely recognized, few methods attempt to explicitly
assess or utilize the distributional structure of the offline data itself,
leaving a research gap in adapting learning strategies to different types of
samples. To address this challenge, we propose an innovative method,
Energy-Guided Diffusion Stratification (StratDiff), which facilitates smoother
transitions in offline-to-online RL. StratDiff deploys a diffusion model to
learn prior knowledge from the offline dataset. It then refines this knowledge
through energy-based functions to improve policy imitation and generate
offline-like actions during online fine-tuning. The KL divergence between the
generated action and the corresponding sampled action is computed for each
sample and used to stratify the training batch into offline-like and
online-like subsets. Offline-like samples are updated using offline objectives,
while online-like samples follow online learning strategies. We demonstrate the
effectiveness of StratDiff by integrating it with off-the-shelf methods Cal-QL
and IQL. Extensive empirical evaluations on D4RL benchmarks show that StratDiff
significantly outperforms existing methods, achieving enhanced adaptability and
more stable performance across diverse RL settings.

</details>


### [164] [Higher-Order Causal Structure Learning with Additive Models](https://arxiv.org/abs/2511.03831)
*James Enouen,Yujia Zheng,Ignavier Ng,Yan Liu,Kun Zhang*

Main category: cs.LG

TL;DR: 本文扩展了因果加性模型（CAM）以包含高阶交互作用，提出使用有向无环超图（hyper DAG）表示高阶因果结构，并提供了相应的理论工具和可识别性结果，同时改进了贪婪CAM算法以适应超图搜索空间，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法大多忽略现实世界中普遍存在的高阶交互机制，缺乏对交互作用的显式建模，限制了因果结构学习的表达能力和准确性。

Method: 将因果加性模型（CAM）扩展到包含高阶交互的加性模型，引入有向无环超图（hyper DAG）作为表示工具，发展相应的理论框架与可识别性条件，并改进贪婪CAM算法以在超图空间中进行搜索。

Result: 提出了适用于高阶交互的因果模型及其理论基础，证明了hyper DAG的可识别性，分析了模型假设与学习复杂度的关系，并通过合成实验验证了所提算法在实际学习中的有效性。

Conclusion: 通过引入高阶交互和hyper DAG结构，不仅增强了因果模型的表达能力，而且在适当假设下能获得更好的有限样本学习性能，为因果结构学习提供了新的模块化层次。

Abstract: Causal structure learning has long been the central task of inferring causal
insights from data. Despite the abundance of real-world processes exhibiting
higher-order mechanisms, however, an explicit treatment of interactions in
causal discovery has received little attention. In this work, we focus on
extending the causal additive model (CAM) to additive models with higher-order
interactions. This second level of modularity we introduce to the structure
learning problem is most easily represented by a directed acyclic hypergraph
which extends the DAG. We introduce the necessary definitions and theoretical
tools to handle the novel structure we introduce and then provide
identifiability results for the hyper DAG, extending the typical Markov
equivalence classes. We next provide insights into why learning the more
complex hypergraph structure may actually lead to better empirical results. In
particular, more restrictive assumptions like CAM correspond to easier-to-learn
hyper DAGs and better finite sample complexity. We finally develop an extension
of the greedy CAM algorithm which can handle the more complex hyper DAG search
space and demonstrate its empirical usefulness in synthetic experiments.

</details>


### [165] [Enhancing Q-Value Updates in Deep Q-Learning via Successor-State Prediction](https://arxiv.org/abs/2511.03836)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的深度Q网络SADQ，通过建模环境动态和引入后继状态分布来减少训练方差并提高学习效率。


<details>
  <summary>Details</summary>
Motivation: 传统DQN依赖过去策略生成的状态进行目标更新，可能导致学习信号不充分且更新方差高，尤其在采样转换与当前策略不一致时问题更严重。

Method: SADQ使用随机转移模型显式建模环境动态，并将后继状态分布整合到Q值估计中，实现更稳定、与当前策略对齐的值更新，同时探索基于模型结构的高效动作选择策略。

Result: 理论证明SADQ在降低训练方差的同时保持无偏价值估计，实验结果表明其在多个标准强化学习基准和真实世界向量控制任务上优于DQN变体。

Conclusion: SADQ通过引入后继状态建模有效提升了DQN的稳定性与学习效率，具有较强的实用性和理论支持。

Abstract: Deep Q-Networks (DQNs) estimate future returns by learning from transitions
sampled from a replay buffer. However, the target updates in DQN often rely on
next states generated by actions from past, potentially suboptimal, policy. As
a result, these states may not provide informative learning signals, causing
high variance into the update process. This issue is exacerbated when the
sampled transitions are poorly aligned with the agent's current policy. To
address this limitation, we propose the Successor-state Aggregation Deep
Q-Network (SADQ), which explicitly models environment dynamics using a
stochastic transition model. SADQ integrates successor-state distributions into
the Q-value estimation process, enabling more stable and policy-aligned value
updates. Additionally, it explores a more efficient action selection strategy
with the modeled transition structure. We provide theoretical guarantees that
SADQ maintains unbiased value estimates while reducing training variance. Our
extensive empirical results across standard RL benchmarks and real-world
vector-based control tasks demonstrate that SADQ consistently outperforms DQN
variants in both stability and learning efficiency.

</details>


### [166] [Benchmark Datasets for Lead-Lag Forecasting on Social Platforms](https://arxiv.org/abs/2511.03877)
*Kimia Kazemian,Zhenzhen Liu,Yangfanyu Yang,Katie Z Luo,Shuhan Gu,Audrey Du,Xinyu Yang,Jack Jansons,Kilian Q Weinberger,John Thickstun,Yian Yin,Sarah Dean*

Main category: cs.LG

TL;DR: 本文提出了“先导-滞后预测”（Lead-Lag Forecasting, LLF）这一新的时间序列预测范式，旨在利用早期用户行为（如浏览、点赞）预测未来长期的高影响力结果（如引用、销售）。作者构建了两个大规模基准数据集（arXiv 和 GitHub），并验证了其中的先导-滞后关系，为社会性和协作平台中的长期预测提供了标准化研究基础。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准化数据集，尽管先导-滞后现象在社交与协作平台中普遍存在，但尚未被作为统一的时间序列预测问题进行系统研究。本文旨在填补这一空白，推动对这类长期动态关系的建模与分析。

Method: 提出先导-滞后预测（LLF）框架，收集并清洗来自arXiv和GitHub的大规模多变量时间序列数据，通过统计检验和分类实验验证lead-lag动态，并对参数化与非参数化的回归基线模型进行基准测试。

Result: 建立了两个高质量、无幸存者偏差的公开数据集（arXiv: 230万论文访问→引用；GitHub: 300万仓库推送/星标→分叉），验证了跨多年长周期的lead-lag模式存在性，并提供了基线模型的性能基准。

Conclusion: LLF是一种新颖且重要的预测范式，该研究为其在社交与使用数据中的系统研究奠定了实证基础，并通过公开数据门户促进了后续研究。

Abstract: Social and collaborative platforms emit multivariate time-series traces in
which early interactions-such as views, likes, or downloads-are followed,
sometimes months or years later, by higher impact like citations, sales, or
reviews. We formalize this setting as Lead-Lag Forecasting (LLF): given an
early usage channel (the lead), predict a correlated but temporally shifted
outcome channel (the lag). Despite the ubiquity of such patterns, LLF has not
been treated as a unified forecasting problem within the time-series community,
largely due to the absence of standardized datasets. To anchor research in LLF,
here we present two high-volume benchmark datasets-arXiv (accesses -> citations
of 2.3M papers) and GitHub (pushes/stars -> forks of 3M repositories)-and
outline additional domains with analogous lead-lag dynamics, including
Wikipedia (page views -> edits), Spotify (streams -> concert attendance),
e-commerce (click-throughs -> purchases), and LinkedIn profile (views ->
messages). Our datasets provide ideal testbeds for lead-lag forecasting, by
capturing long-horizon dynamics across years, spanning the full spectrum of
outcomes, and avoiding survivorship bias in sampling. We documented all
technical details of data curation and cleaning, verified the presence of
lead-lag dynamics through statistical and classification tests, and benchmarked
parametric and non-parametric baselines for regression. Our study establishes
LLF as a novel forecasting paradigm and lays an empirical foundation for its
systematic exploration in social and usage data. Our data portal with downloads
and documentation is available at https://lead-lag-forecasting.github.io/.

</details>


### [167] [DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets](https://arxiv.org/abs/2511.03911)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Mohsen Imani*

Main category: cs.LG

TL;DR: 本文提出了DecoHD，一种用于超维计算（HDC）的直接学习分解方法，在保持原生HDC计算流程的同时实现极致内存压缩、低参数量训练和显著的能效与速度提升。


<details>
  <summary>Details</summary>
Motivation: 传统的HDC压缩方法通常沿特征轴压缩，损害数据集中性和鲁棒性；现有分解方法使用固定的原子超向量解码，难以有效压缩学习得到的类别原型。因此需要一种更高效、适应性强的HDC压缩方案。

Method: 提出DecoHD，采用小规模共享的逐层通道进行分层乘法绑定，并在末端进行捆绑，形成从紧凑因子中生成大表示空间的机制；沿类别轴通过轻量级捆绑头实现压缩，支持端到端训练，推理过程保持纯HDC特性，适配存内/近存加速器。

Result: DecoHD在严格部署预算下实现了极高的内存节省，准确率仅轻微下降（平均比强基准低0.1-0.15%，最差情况5.7%）；对随机位翻转噪声更具鲁棒性；训练参数最多减少约97%即可达到精度平台；硬件上相比CPU、GPU和HDC专用ASIC分别获得约277倍/35倍、13.5倍/3.7倍和2.0倍/2.4倍的能效与速度增益。

Conclusion: DecoHD为超维计算提供了一种高效、可训练且硬件友好的模型压缩方法，在保持HDC本质优势的同时显著提升了实用性和部署效率，特别适用于资源受限场景。

Abstract: Decomposition is a proven way to shrink deep networks without changing I/O.
We bring this idea to hyperdimensional computing (HDC), where footprint cuts
usually shrink the feature axis and erode concentration and robustness. Prior
HDC decompositions decode via fixed atomic hypervectors, which are ill-suited
for compressing learned class prototypes. We introduce DecoHD, which learns
directly in a decomposed HDC parameterization: a small, shared set of per-layer
channels with multiplicative binding across layers and bundling at the end,
yielding a large representational space from compact factors. DecoHD compresses
along the class axis via a lightweight bundling head while preserving native
bind-bundle-score; training is end-to-end, and inference remains pure HDC,
aligning with in/near-memory accelerators. In evaluation, DecoHD attains
extreme memory savings with only minor accuracy degradation under tight
deployment budgets. On average it stays within about 0.1-0.15% of a strong
non-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip
noise, reaches its accuracy plateau with up to ~97% fewer trainable parameters,
and -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU
(AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x
over a baseline HDC ASIC.

</details>


### [168] [On Predicting Sociodemographics from Mobility Signals](https://arxiv.org/abs/2511.03924)
*Ekin Uğurel,Cynthia Chen,Brian H. Y. Lee,Filipe Rodrigues*

Main category: cs.LG

TL;DR: 本文提出了一种基于有向移动图的高阶移动描述符，结合多任务学习框架，以提高从移动数据中推断社会人口统计属性的准确性、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于移动模式与社会人口特征之间的关系较弱且不一致，且跨情境泛化能力有限，准确推断社会人口属性具有挑战性。

Method: 引入基于有向移动图的行为导向高阶移动描述符，设计用于衡量模型置信度与准确率一致性的指标和可视化诊断工具，并构建共享表征的多任务学习框架以提升泛化能力和样本效率。

Result: 所提特征显著优于基线方法在年龄、性别、收入和家庭结构预测上的表现；多任务学习框架在训练数据有限或测试集分布变化时优于单任务模型；提出的诊断工具有助于量化模型不确定性。

Conclusion: 该方法在保持可解释性的同时提升了预测性能和跨情境的适用性，为交通规划者利用被动收集的移动数据提供了更可靠、稳健的工具。

Abstract: Inferring sociodemographic attributes from mobility data could help
transportation planners better leverage passively collected datasets, but this
task remains difficult due to weak and inconsistent relationships between
mobility patterns and sociodemographic traits, as well as limited
generalization across contexts. We address these challenges from three angles.
First, to improve predictive accuracy while retaining interpretability, we
introduce a behaviorally grounded set of higher-order mobility descriptors
based on directed mobility graphs. These features capture structured patterns
in trip sequences, travel modes, and social co-travel, and significantly
improve prediction of age, gender, income, and household structure over
baselines features. Second, we introduce metrics and visual diagnostic tools
that encourage evenness between model confidence and accuracy, enabling
planners to quantify uncertainty. Third, to improve generalization and sample
efficiency, we develop a multitask learning framework that jointly predicts
multiple sociodemographic attributes from a shared representation. This
approach outperforms single-task models, particularly when training data are
limited or when applying models across different time periods (i.e., when the
test set distribution differs from the training set).

</details>


### [169] [SynQuE: Estimating Synthetic Dataset Quality Without Annotations](https://arxiv.org/abs/2511.03928)
*Arthur Chen,Victor Zhong*

Main category: cs.LG

TL;DR: 本文提出了合成数据集质量估计（SynQuE）问题，旨在仅使用有限的未标注真实数据来对合成数据集进行排序以优化实际任务性能，并引入了首个综合基准和基于大语言模型的新型代理指标LENS，在多种任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在真实数据因采集成本或隐私限制而稀缺的情况下，如何有效选择高质量的合成数据用于训练是一个关键且尚未解决的问题。

Method: 提出并形式化SynQuE问题，构建基准测试，设计基于分布与多样性距离度量的代理指标，并提出LENS——一种利用大语言模型推理能力的新代理指标，以更好评估复杂任务中的合成数据质量。

Result: 实验表明，所提出的SynQuE代理指标在情感分析、Text2SQL、网页导航和图像分类等多种任务中均与真实任务性能有良好相关性，其中LENS在复杂任务上表现最优；例如在Text2SQL任务中，使用SynQuE选出的前3个合成数据集可使准确率平均从30.4%提升至38.4%（+8.1%）。

Conclusion: SynQuE为数据稀缺场景下的合成数据选择提供了实用框架，LENS展示了大语言模型在数据质量评估中的潜力，推动了基于基础模型的数据表征与细粒度选择的未来研究。

Abstract: We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE)
problem: ranking synthetic datasets by their expected real-world task
performance using only limited unannotated real data. This addresses a critical
and open challenge where data is scarce due to collection costs or privacy
constraints. We establish the first comprehensive benchmarks for this problem
by introducing and evaluating proxy metrics that choose synthetic data for
training to maximize task performance on real data. We introduce the first
proxy metrics for SynQuE by adapting distribution and diversity-based distance
measures to our context via embedding models. To address the shortcomings of
these metrics on complex planning tasks, we propose LENS, a novel proxy that
leverages large language model reasoning. Our results show that SynQuE proxies
correlate with real task performance across diverse tasks, including sentiment
analysis, Text2SQL, web navigation, and image classification, with LENS
consistently outperforming others on complex tasks by capturing nuanced
characteristics. For instance, on text-to-SQL parsing, training on the top-3
synthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to
38.4 (+8.1)% on average compared to selecting data indiscriminately. This work
establishes SynQuE as a practical framework for synthetic data selection under
real-data scarcity and motivates future research on foundation model-based data
characterization and fine-grained data selection.

</details>


### [170] [NVIDIA Nemotron Nano V2 VL](https://arxiv.org/abs/2511.03929)
*NVIDIA,:,Amala Sanjay Deshmukh,Kateryna Chumachenko,Tuomas Rintamaki,Matthieu Le,Tyler Poon,Danial Mohseni Taheri,Ilia Karmanov,Guilin Liu,Jarno Seppanen,Guo Chen,Karan Sapra,Zhiding Yu,Adi Renduchintala,Charles Wang,Peter Jin,Arushi Goel,Mike Ranzinger,Lukas Voegtle,Philipp Fischer,Timo Roman,Wei Ping,Boxin Wang,Zhuolin Yang,Nayeon Lee,Shaokun Zhang,Fuxiao Liu,Zhiqi Li,Di Zhang,Greg Heinrich,Hongxu,Yin,Song Han,Pavlo Molchanov,Parth Mannan,Yao Xu,Jane Polak Scowcroft,Tom Balough,Subhashree Radhakrishnan,Paris Zhang,Sean Cha,Ratnesh Kumar,Zaid Pervaiz Bhat,Jian Zhang,Darragh Hanley,Pritam Biswas,Jesse Oliver,Kevin Vasques,Roger Waleffe,Duncan Riach,Oluwatobi Olabiyi,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Pritam Gundecha,Khanh Nguyen,Alexandre Milesi,Eugene Khvedchenia,Ran Zilberstein,Ofri Masad,Natan Bagrov,Nave Assaf,Tomer Asida,Daniel Afrimi,Amit Zuker,Netanel Haber,Zhiyu Cheng,Jingyu,Xin,Di,Wu,Nik Spirin,Maryam Moosaei,Roman Ageev,Vanshil Atul Shah,Yuting Wu,Daniel Korzekwa,Unnikrishnan Kizhakkemadam Sreekumar,Wanli Jiang,Padmavathy Subramanian,Alejandra Rico,Sandip Bhaskar,Saeid Motiian,Kedi Wu,Annie Surla,Chia-Chih Chen,Hayden Wolff,Matthew Feinberg,Melissa Corpuz,Marek Wawrzos,Eileen Long,Aastha Jhunjhunwala,Paul Hendricks,Farzan Memarian,Benika Hall,Xin-Yu Wang,David Mosallanezhad,Soumye Singhal,Luis Vega,Katherine Cheung,Krzysztof Pawelec,Michael Evans,Katherine Luna,Jie Lou,Erick Galinkin,Akshay Hazare,Kaustubh Purandare,Ann Guan,Anna Warno,Chen Cui,Yoshi Suhara,Shibani Likhite,Seph Mard,Meredith Price,Laya Sleiman,Saori Kaji,Udi Karpas,Kari Briski,Joey Conway,Michael Lightstone,Jan Kautz,Mohammad Shoeybi,Mostofa Patwary,Jonathen Cohen,Oleksii Kuchaiev,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron Nano V2 VL 是一个改进的视觉-语言模型，专为文档理解、长视频理解和推理任务设计，在架构、数据集和训练方法上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 为了提升现实场景中文档和视频理解的性能，特别是在长序列处理和推理能力方面。

Method: 基于混合Mamba-Transformer架构的Nemotron Nano V2，结合创新的token压缩技术，并采用优化的数据集和训练策略。

Result: 在所有视觉和文本任务上均优于前代模型Llama-3.1-Nemotron-Nano-VL-8B，显著提高了长文档和视频场景下的推理吞吐量。

Conclusion: Nemotron Nano V2 VL 在效率和性能上取得平衡，支持多种精度格式发布，并开源部分数据与代码以促进社区发展。

Abstract: We introduce Nemotron Nano V2 VL, the latest model of the Nemotron
vision-language series designed for strong real-world document understanding,
long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers
significant improvements over our previous model,
Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major
enhancements in model architecture, datasets, and training recipes. Nemotron
Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and
innovative token reduction techniques to achieve higher inference throughput in
long document and video scenarios. We are releasing model checkpoints in BF16,
FP8, and FP4 formats and sharing large parts of our datasets, recipes and
training code.

</details>


### [171] [LogHD: Robust Compression of Hyperdimensional Classifiers via Logarithmic Class-Axis Reduction](https://arxiv.org/abs/2511.03938)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Pietro Mercati,Nathaniel D. Bastian,Mohsen Imani*

Main category: cs.LG

TL;DR: LogHD提出了一种在超维计算中沿类别轴进行对数压缩的新方法，通过使用约⌈log_k C⌉个束超向量替代C个类原型，将内存复杂度从O(CD)降低到O(D log_k C)，同时保持维度D不变，提升了存储效率和抗噪能力，并在ASIC实现中展现出显著的能效和速度优势。


<details>
  <summary>Details</summary>
Motivation: 传统的超维计算采用“每类一个原型”的设计，内存开销大（O(CD)），虽然已有工作通过降低特征维度D来压缩模型，但会牺牲鲁棒性；因此需要一种既能减少内存占用又不削弱系统稳健性的新压缩方法。

Method: LogHD引入类别轴的对数压缩，用n≈⌈log_k C⌉个bundle超向量（基于大小为k的字母表）替代C个类原型，在n维激活空间中进行解码；采用容量感知的码本和基于轮廓的解码策略，并可与特征轴稀疏化方法结合使用。

Result: LogHD在多种数据集和比特翻转攻击下表现出色：在相同内存条件下，其抗比特翻转能力比特征轴压缩高2.5-3.0倍；ASIC实现相比AMD Ryzen 9 9950X实现了498倍能效提升和62.6倍速度提升，相比NVIDIA RTX 4090也有24.3倍/6.58倍的优势，且比特征轴压缩的HDC ASIC基线快2.19倍、节能4.06倍。

Conclusion: LogHD通过类别轴对数压缩有效降低了超维计算的内存开销，同时保持了高准确率和强鲁棒性，结合硬件实现展现出卓越的能效与性能，为资源受限系统提供了一种高效可靠的替代方案。

Abstract: Hyperdimensional computing (HDC) suits memory, energy, and
reliability-constrained systems, yet the standard "one prototype per class"
design requires $O(CD)$ memory (with $C$ classes and dimensionality $D$). Prior
compaction reduces $D$ (feature axis), improving storage/compute but weakening
robustness. We introduce LogHD, a logarithmic class-axis reduction that
replaces the $C$ per-class prototypes with $n\!\approx\!\lceil\log_k C\rceil$
bundle hypervectors (alphabet size $k$) and decodes in an $n$-dimensional
activation space, cutting memory to $O(D\log_k C)$ while preserving $D$. LogHD
uses a capacity-aware codebook and profile-based decoding, and composes with
feature-axis sparsification. Across datasets and injected bit flips, LogHD
attains competitive accuracy with smaller models and higher resilience at
matched memory. Under equal memory, it sustains target accuracy at roughly
$2.5$-$3.0\times$ higher bit-flip rates than feature-axis compression; an ASIC
instantiation delivers $498\times$ energy efficiency and $62.6\times$ speedup
over an AMD Ryzen 9 9950X and $24.3\times$/$6.58\times$ over an NVIDIA RTX
4090, and is $4.06\times$ more energy-efficient and $2.19\times$ faster than a
feature-axis HDC ASIC baseline.

</details>


### [172] [RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods](https://arxiv.org/abs/2511.03939)
*Raghav Sharma,Manan Mehta,Sai Tiger Raina*

Main category: cs.LG

TL;DR: 本文综述了基于人类反馈的强化学习（RLHF）在多模态对齐、文化公平性和低延迟优化方面的最新进展，系统分析了PPO、DPO和GRPO等算法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，传统的文本对齐方法已不足以应对多模态、文化和效率方面的挑战，亟需系统性综述来指导未来研究。

Method: 首先回顾基础对齐算法（如PPO、DPO、GRPO），然后对新兴技术进行比较分析，涵盖多模态对齐、文化公平性和低延迟优化三个关键领域。

Result: 总结了当前对齐技术的创新成果，识别出各领域的关键挑战与差距，提供了技术对比和演进路径。

Conclusion: 该研究为构建更鲁棒、高效和公平的AI系统提供了重要路线图，推动对齐研究从单一文本向多维度综合发展。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for
aligning Large Language Models (LLMs), yet recent progress has moved beyond
canonical text-based methods. This survey synthesizes the new frontier of
alignment research by addressing critical gaps in multi-modal alignment,
cultural fairness, and low-latency optimization. To systematically explore
these domains, we first review foundational algo- rithms, including PPO, DPO,
and GRPO, before presenting a detailed analysis of the latest innovations. By
providing a comparative synthesis of these techniques and outlining open
challenges, this work serves as an essential roadmap for researchers building
more robust, efficient, and equitable AI systems.

</details>


### [173] [Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels](https://arxiv.org/abs/2511.03953)
*Wuxia Chen,Taposh Banerjee,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出了一种基于分数的CUSUM方法，用于在未知转移核的马尔可夫过程中进行快速变化检测，通过学习条件得分避免显式似然计算，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 在高维数据下，传统方法难以处理未知转移核的马尔可夫过程中的快速变化检测问题，因此需要一种无需显式建模概率密度的方法。

Method: 直接从样本对中学习条件得分∇_y log p(y|x)，利用条件Hyvarinen得分差构建基于分数的CUSUM统计量，并采用截断策略确保增量有界。

Result: 证明了平均虚假报警时间的指数下界和检测延迟的渐近上界，表明该方法具有良好的理论性能和实际可行性。

Conclusion: 所提出的基于分数的变化检测方法在高维马尔可夫模型中有效且可行，能够在不估计完整转移核的情况下实现快速、可靠的变化检测。

Abstract: We address the problem of quickest change detection in Markov processes with
unknown transition kernels. The key idea is to learn the conditional score
$\nabla_{\mathbf{y}} \log p(\mathbf{y}|\mathbf{x})$ directly from sample pairs
$( \mathbf{x},\mathbf{y})$, where both $\mathbf{x}$ and $\mathbf{y}$ are
high-dimensional data generated by the same transition kernel. In this way, we
avoid explicit likelihood evaluation and provide a practical way to learn the
transition dynamics. Based on this estimation, we develop a score-based CUSUM
procedure that uses conditional Hyvarinen score differences to detect changes
in the kernel. To ensure bounded increments, we propose a truncated version of
the statistic. With Hoeffding's inequality for uniformly ergodic Markov
processes, we prove exponential lower bounds on the mean time to false alarm.
We also prove asymptotic upper bounds on detection delay. These results give
both theoretical guarantees and practical feasibility for score-based detection
in high-dimensional Markov models.

</details>


### [174] [PrivacyCD: Hierarchical Unlearning for Protecting Student Privacy in Cognitive Diagnosis](https://arxiv.org/abs/2511.03966)
*Mingliang Hou,Yinuo Wang,Teng Guo,Zitao Liu,Wenzhou Dou,Jiaqi Zheng,Renqiang Luo,Mi Tian,Weiqi Luo*

Main category: cs.LG

TL;DR: 本文提出了首个针对认知诊断模型的数据遗忘系统研究，提出了一种高效的新算法HIF，能够有效应对用户数据删除请求，兼顾模型性能与隐私保护。


<details>
  <summary>Details</summary>
Motivation: 随着用户对“被遗忘权”的重视，从认知诊断模型中移除特定学生数据成为迫切需求，但现有模型缺乏有效的数据遗忘机制。

Method: 提出层次化重要性引导遗忘（HIF）算法，利用参数在不同层的重要性特征，结合个体与层级重要性进行平滑处理，精准识别并遗忘与待删除数据相关的参数。

Result: 在三个真实数据集上的实验表明，HIF在遗忘完整性、模型效用和效率等关键指标上显著优于基线方法。

Conclusion: HIF为认知诊断模型提供了首个有效的数据遗忘解决方案，有助于构建高性能且符合隐私保护要求的人工智能系统。

Abstract: The need to remove specific student data from cognitive diagnosis (CD) models
has become a pressing requirement, driven by users' growing assertion of their
"right to be forgotten". However, existing CD models are largely designed
without privacy considerations and lack effective data unlearning mechanisms.
Directly applying general purpose unlearning algorithms is suboptimal, as they
struggle to balance unlearning completeness, model utility, and efficiency when
confronted with the unique heterogeneous structure of CD models. To address
this, our paper presents the first systematic study of the data unlearning
problem for CD models, proposing a novel and efficient algorithm: hierarchical
importanceguided forgetting (HIF). Our key insight is that parameter importance
in CD models exhibits distinct layer wise characteristics. HIF leverages this
via an innovative smoothing mechanism that combines individual and layer, level
importance, enabling a more precise distinction of parameters associated with
the data to be unlearned. Experiments on three real world datasets show that
HIF significantly outperforms baselines on key metrics, offering the first
effective solution for CD models to respond to user data removal requests and
for deploying high-performance, privacy preserving AI systems

</details>


### [175] [Non-Asymptotic Optimization and Generalization Bounds for Stochastic Gauss-Newton in Overparameterized Models](https://arxiv.org/abs/2511.03972)
*Semih Cayci*

Main category: cs.LG

TL;DR: 本文研究了随机高斯-牛顿法（SGN）在过参数化深度神经网络训练中的优化与泛化性能，提出了有限时间收敛性与非渐近泛化界的理论分析，揭示了曲率、批量大小和过参数化对泛化的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨高阶优化方法（如SGN）如何影响深度学习模型的泛化能力，尤其是在过参数化设置下的表现。

Method: 采用带Levenberg-Marquardt阻尼和小批量采样的随机高斯-牛顿法，通过变量度量分析在参数空间中建立有限时间收敛界，并利用均匀稳定性推导非渐近泛化界。

Result: 1) 建立了显式依赖批量大小、网络宽度和深度的收敛界；2) 推导出SGN的非渐近泛化界，发现优化路径上更大的高斯-牛顿矩阵最小特征值可带来更紧的稳定性界，有利于泛化。

Conclusion: SGN在过参数化回归问题中存在有利的泛化区域，较大的最小曲率特征值有助于提升泛化性能，理论支持了二阶方法在特定条件下优于一阶方法的可能性。

Abstract: An important question in deep learning is how higher-order optimization
methods affect generalization. In this work, we analyze a stochastic
Gauss-Newton (SGN) method with Levenberg-Marquardt damping and mini-batch
sampling for training overparameterized deep neural networks with smooth
activations in a regression setting. Our theoretical contributions are twofold.
First, we establish finite-time convergence bounds via a variable-metric
analysis in parameter space, with explicit dependencies on the batch size,
network width and depth. Second, we derive non-asymptotic generalization bounds
for SGN using uniform stability in the overparameterized regime, characterizing
the impact of curvature, batch size, and overparameterization on generalization
performance. Our theoretical results identify a favorable generalization regime
for SGN in which a larger minimum eigenvalue of the Gauss-Newton matrix along
the optimization path yields tighter stability bounds.

</details>


### [176] [PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction](https://arxiv.org/abs/2511.03976)
*Xu Zou*

Main category: cs.LG

TL;DR: 本文提出了PETRA，一种基于进化轨迹而非原始RNA序列的新型Transformer模型，用于预测SARS-CoV-2突变。该方法通过系统发育树提取演化路径，有效降低测序噪声并捕捉病毒进化的层次结构，在突变预测任务中显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: SARS-CoV-2持续出现免疫逃逸变异株，给公共卫生和疫苗研发带来挑战，亟需有效工具预测其进化方向。然而，传统的大型生成预训练模型在处理 noisy 病毒基因组序列时存在局限性。

Method: 提出PETRA模型，利用系统发育树构建病毒的进化轨迹作为输入，替代原始RNA序列；采用加权训练框架以缓解全球序列数据在地理和时间上的不平衡问题。

Result: PETRA在核苷酸突变预测上加权recall@1达到9.45%，刺突蛋白氨基酸突变预测达到17.10%，显著优于最佳基线（分别为0.49%和6.64%），并成功应用于主要克隆群如24F(XEC)和25A(LP.8.1)的实时突变预测。

Conclusion: PETRA通过建模病毒进化路径而非原始序列，有效提升了对SARS-CoV-2未来突变的预测能力，为应对病毒持续演化提供了有力工具。

Abstract: Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable
evolutionary trajectory, characterized by the continual emergence of
immune-evasive variants. This poses persistent challenges to public health and
vaccine development.
  While large-scale generative pre-trained transformers (GPTs) have
revolutionized the modeling of sequential data, their direct applications to
noisy viral genomic sequences are limited. In this paper, we introduce
PETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based
on evolutionary trajectories derived from phylogenetic trees rather than raw
RNA sequences. This method effectively mitigates sequencing noise and captures
the hierarchical structure of viral evolution.
  With a weighted training framework to address substantial geographical and
temporal imbalances in global sequence data, PETRA excels in predicting future
SARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide
mutations and 17.10\% for spike amino-acid mutations, compared to 0.49% and
6.64% respectively for the best baseline. PETRA also demonstrates its ability
to aid in the real-time mutation prediction of major clades like 24F(XEC) and
25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra

</details>


### [177] [Structural Priors and Modular Adapters in the Composable Fine-Tuning Algorithm of Large-Scale Models](https://arxiv.org/abs/2511.03981)
*Yuxiao Wang,Di Wu,Feng Liu,Zhimin Qiu,Chenrui Hu*

Main category: cs.LG

TL;DR: 提出一种可组合的微调方法，结合图结构先验与模块化适配器，提升大规模预训练模型在多任务适应中的计算效率和结构稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模预训练模型在多任务适应中面临的高计算成本和结构不稳定性问题。

Method: 引入关系矩阵建模任务依赖关系，将节点与路径的相关性编码为图结构先验，指导低秩映射和可插拔机制下的模块化适配器嵌入，实现高效的任务组合与重用。

Result: 实验表明该方法在路由温度、门控阈值和关系矩阵正则化强度等超参数下具有一致且优越的性能，显著提升了任务预测准确率、适配器权重分配精度和计算效率。

Conclusion: 图结构先验与模块化机制的协同作用有效增强了可组合微调的参数效率、训练稳定性和模型轻量化设计。

Abstract: This paper proposes a composable fine-tuning method that integrates graph
structural priors with modular adapters to address the high computational cost
and structural instability faced by large-scale pre-trained models in
multi-task adaptation. The method introduces a relation matrix to model
dependencies among tasks, explicitly encoding correlations between nodes and
paths into graph structural priors, which provide unified structural
constraints for adapter weight allocation and path selection. Modular adapters
are embedded into different layers through low-rank mapping and a pluggable
mechanism, enabling efficient cross-task composition and reuse under prior
guidance. This mechanism not only improves parameter efficiency and training
stability but also alleviates path conflicts and redundant computation in
multi-task scenarios. Furthermore, experiments on hyperparameter sensitivity,
environmental sensitivity, and data sensitivity are conducted to systematically
analyze key factors such as routing temperature, gating thresholds, and
relation matrix regularization strength, verifying the consistency and superior
performance of the method under structural constraints. The results demonstrate
that the proposed framework significantly enhances task prediction accuracy,
adapter weight allocation precision, and overall computational efficiency while
maintaining model lightweight design, highlighting the synergistic advantages
of graph priors and modular mechanisms in composable fine-tuning.

</details>


### [178] [TwIST: Rigging the Lottery in Transformers with Independent Subnetwork Training](https://arxiv.org/abs/2511.03983)
*Michael Menezes,Barbara Su,Xinze Feng,Yehya Farhat,Hamza Shili,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: TwIST是一种分布式训练框架，用于高效的大语言模型稀疏化，通过并行训练多个子网络、周期性聚合参数和重采样，在训练过程中直接发现高质量子网络（“黄金票”），实现零成本剪枝且保持竞争力的困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型剪枝方法通常依赖训练后校准或恢复步骤，难以在部署时高效应用；同时非结构化剪枝在通用硬件上无法有效加速。因此需要一种训练时即完成高效稀疏化的方案。

Method: TwIST采用分布式并行训练多个子网络，周期性聚合各子网络参数，并在训练中动态重采样新子网络，从而在训练过程中直接搜索和优化稀疏子网络结构。

Result: TwIST在高稀疏度下显著优于基线方法，例如在50%以上稀疏度时达到23.14困惑度，优于先前最佳方法的31.64；生成的结构化稠密矩阵可在CPU等通用硬件上实现实际推理加速和内存节省。

Conclusion: TwIST提供了一种高效的训练时稀疏化路径，无需后期微调或恢复，即可获得可部署的高性能稀疏大模型，兼具效果与实用性。

Abstract: We introduce TwIST, a distributed training framework for efficient large
language model (LLM) sparsification. TwIST trains multiple subnetworks in
parallel, periodically aggregates their parameters, and resamples new
subnetworks during training. This process identifies high-quality subnetworks
("golden tickets") without requiring post-training procedures such as
calibration or Hessian-based recovery. As a result, TwIST enables zero-cost
pruning at deployment time while achieving perplexity competitive with
state-of-the-art post-training sparsification methods. The benefits are most
pronounced under aggressive sparsity (e.g., 50%+), where TwIST significantly
outperforms baseline methods; for example, reaching 23.14 PPL compared to 31.64
for the closest prior approach. Unlike unstructured pruning, TwIST produces
structured, dense matrices that offer practical inference speedups and memory
reductions on commodity hardware (e.g., CPUs) that do not support efficient
sparse computation. TwIST provides an efficient training-time path to
deployable sparse LLMs without additional fine-tuning or recovery overhead.

</details>


### [179] [Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes](https://arxiv.org/abs/2511.03986)
*Ahmed A. Metwally,Heyjun Park,Yue Wu,Tracey McLaughlin,Michael P. Snyder*

Main category: cs.LG

TL;DR: 该综述提出利用连续血糖监测（CGM）和可穿戴技术结合机器学习，实现对糖尿病前期和糖尿病的动态、个性化代谢表型分型，超越传统静态血糖阈值分类，推动精准糖尿病预防。


<details>
  <summary>Details</summary>
Motivation: 传统基于静态血糖阈值的糖尿病分类方法无法反映胰岛素抵抗、β细胞功能障碍和肠促胰素缺陷等病理生理异质性，亟需更精细的代谢分型方法。

Method: 结合家庭环境下的CGM增强口服葡萄糖耐量试验、机器学习模型分析高分辨率血糖数据，并整合可穿戴设备采集的饮食、睡眠和运动等行为数据，进行动态代谢表型刻画。

Result: 机器学习模型能准确预测肌肉胰岛素抵抗和β细胞功能；个体对食物的餐后血糖反应可作为代谢亚型生物标志物；生活习惯及其时间模式与特定代谢异常相关；饮食干预效果具有表型依赖性。

Conclusion: CGM可将早期糖代谢异常分解为可操作的亚型，实现针对个体核心代谢缺陷的精准营养、行为和药物干预，开启精准糖尿病预防新纪元。

Abstract: The classification of diabetes and prediabetes by static glucose thresholds
obscures the pathophysiological dysglycemia heterogeneity, primarily driven by
insulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This
review demonstrates that continuous glucose monitoring and wearable
technologies enable a paradigm shift towards non-invasive, dynamic metabolic
phenotyping. We show evidence that machine learning models can leverage
high-resolution glucose data from at-home, CGM-enabled oral glucose tolerance
tests to accurately predict gold-standard measures of muscle IR and beta-cell
function. This personalized characterization extends to real-world nutrition,
where an individual's unique postprandial glycemic response (PPGR) to
standardized meals, such as the relative glucose spike to potatoes versus
grapes, could serve as a biomarker for their metabolic subtype. Moreover,
integrating wearable data reveals that habitual diet, sleep, and physical
activity patterns, particularly their timing, are uniquely associated with
specific metabolic dysfunctions, informing precision lifestyle interventions.
The efficacy of dietary mitigators in attenuating PPGR is also shown to be
phenotype-dependent. Collectively, this evidence demonstrates that CGM can
deconstruct the complexity of early dysglycemia into distinct, actionable
subphenotypes. This approach moves beyond simple glycemic control, paving the
way for targeted nutritional, behavioral, and pharmacological strategies
tailored to an individual's core metabolic defects, thereby paving the way for
a new era of precision diabetes prevention.

</details>


### [180] [Multiscale Astrocyte Network Calcium Dynamics for Biologically Plausible Intelligence in Anomaly Detection](https://arxiv.org/abs/2511.03993)
*Berk Iskar,Michael Taynnan Barros*

Main category: cs.LG

TL;DR: 提出一种受脑内星形胶质细胞Ca²⁺信号启发的Ca²⁺调节学习框架，用于网络异常检测，结合多细胞星形胶质细胞动力学模拟器与深度神经网络，在CTU-13数据上表现优于传统DNN，准确率达98%，且误报和漏报更少。


<details>
  <summary>Details</summary>
Motivation: 传统离线训练的异常检测系统易受概念漂移和新型威胁（如零日攻击、多态攻击）影响，缺乏快速适应能力。

Method: 构建一个模拟星形胶质细胞Ca²⁺动力学的多细胞系统，包含IP₃介导的Ca²⁺释放、SERCA泵摄取及间隙连接中的传导感知扩散，并将其与深度神经网络耦合，实现动态门控调节。

Result: 在CTU-13（Neris）数据集上，该模型准确率高达约98%，显著优于基线DNN，假阳性和假阴性更低，且预计算Ca²⁺轨迹后运行开销可忽略。

Conclusion: 该Ca²⁺调节学习框架是一种生物可解释、适用于流式检测任务的通用方案，能实现对动态数据模式的快速自适应，在网络安全等领域具有应用潜力。

Abstract: Network anomaly detection systems encounter several challenges with
traditional detectors trained offline. They become susceptible to concept drift
and new threats such as zero-day or polymorphic attacks. To address this
limitation, we propose a Ca$^{2+}$-modulated learning framework that draws
inspiration from astrocytic Ca$^{2+}$ signaling in the brain, where rapid,
context-sensitive adaptation enables robust information processing. Our
approach couples a multicellular astrocyte dynamics simulator with a deep
neural network (DNN). The simulator models astrocytic Ca$^{2+}$ dynamics
through three key mechanisms: IP$_3$-mediated Ca$^{2+}$ release, SERCA pump
uptake, and conductance-aware diffusion through gap junctions between cells.
Evaluation of our proposed network on CTU-13 (Neris) network traffic data
demonstrates the effectiveness of our biologically plausible approach. The
Ca$^{2+}$-gated model outperforms a matched baseline DNN, achieving up to
$\sim$98\% accuracy with reduced false positives and negatives across multiple
train/test splits. Importantly, this improved performance comes with negligible
runtime overhead once Ca$^{2+}$ trajectories are precomputed. While
demonstrated here for cybersecurity applications, this Ca$^{2+}$-modulated
learning framework offers a generic solution for streaming detection tasks that
require rapid, biologically grounded adaptation to evolving data patterns.

</details>


### [181] [Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations](https://arxiv.org/abs/2511.04000)
*Kyaw Hpone Myint,Zhe Wu,Alexandre G. R. Day,Giri Iyengar*

Main category: cs.LG

TL;DR: 提出一种高效的合成预训练数据生成方法，用于决策树的元学习，通过MetaTree transformer架构实现接近真实数据的性能，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 决策树在金融和医疗等高风险领域广泛应用，但基于真实数据或最优决策树进行元学习通常计算代价高昂且扩展性差，因此需要更高效、可扩展的方法。

Method: 通过合成采样近似最优的决策树，构建大规模且逼真的数据集，并利用MetaTree transformer架构进行元学习。

Result: 该方法在性能上可媲美基于真实数据或计算密集型最优决策树的预训练方法，同时大幅降低计算开销，并提升数据生成的灵活性。

Conclusion: 所提方法实现了可扩展且高效的决策树元学习，为可解释模型的训练提供了新路径。

Abstract: Decision trees are widely used in high-stakes fields like finance and
healthcare due to their interpretability. This work introduces an efficient,
scalable method for generating synthetic pre-training data to enable
meta-learning of decision trees. Our approach samples near-optimal decision
trees synthetically, creating large-scale, realistic datasets. Using the
MetaTree transformer architecture, we demonstrate that this method achieves
performance comparable to pre-training on real-world data or with
computationally expensive optimal decision trees. This strategy significantly
reduces computational costs, enhances data generation flexibility, and paves
the way for scalable and efficient meta-learning of interpretable decision tree
models.

</details>


### [182] [Accelerating scientific discovery with the common task framework](https://arxiv.org/abs/2511.04001)
*J. Nathan Kutz,Peter Battaglia,Michael Brenner,Kevin Carlberg,Aric Hagberg,Shirley Ho,Stephan Hoyer,Henning Lange,Hod Lipson,Michael W. Mahoney,Frank Noe,Max Welling,Laure Zanna,Francis Zhu,Steven L. Brunton*

Main category: cs.LG

TL;DR: 本文提出了一个用于科学和工程领域的通用任务框架（CTF），通过多样化的挑战数据集和客观度量标准，促进机器学习与人工智能算法在动态系统建模、预测、状态重构等任务中的评估与比较。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和人工智能在多个科学与工程领域的发展，亟需统一的评估框架来比较不同算法在实际任务中的性能，特别是在数据有限和噪声干扰下的表现。

Method: 引入通用任务框架（CTF），提供一系列具有共同科学目标的挑战性数据集，并建立适用于多种应用场景（如预测、状态重建、泛化和控制）的标准化评估指标。

Result: CTF为科学与工程领域提供了可比较、可重复的评估平台，有助于推动机器学习算法在真实世界动态系统中的发展与部署。

Conclusion: 通用任务框架（CTF）是推动物理、工程和生物科学中AI/ML算法进步的关键技术，能够有效支持算法性能的客观评估与持续优化。

Abstract: Machine learning (ML) and artificial intelligence (AI) algorithms are
transforming and empowering the characterization and control of dynamic systems
in the engineering, physical, and biological sciences. These emerging modeling
paradigms require comparative metrics to evaluate a diverse set of scientific
objectives, including forecasting, state reconstruction, generalization, and
control, while also considering limited data scenarios and noisy measurements.
We introduce a common task framework (CTF) for science and engineering, which
features a growing collection of challenge data sets with a diverse set of
practical and common objectives. The CTF is a critically enabling technology
that has contributed to the rapid advance of ML/AI algorithms in traditional
applications such as speech recognition, language processing, and computer
vision. There is a critical need for the objective metrics of a CTF to compare
the diverse algorithms being rapidly developed and deployed in practice today
across science and engineering.

</details>


### [183] [Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing](https://arxiv.org/abs/2511.04002)
*Mingyu Sung,Vikas Palakonda,Suhwan Im,Sunghwan Moon,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.LG

TL;DR: 本文提出了一种面向自回归推理的边缘计算分割框架，通过混合精度量化和两阶段压缩方法，在保证精度的同时显著降低通信开销并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型参数量大、解码过程内存消耗高，难以部署在资源受限的物联网设备上；现有分割计算方法未有效解决自回归推理中迭代生成和KV缓存增长的问题。

Method: 提出了三点创新：一是设计了一点分割压缩（OPSC）混合精度量化方案；二是采用阈值分割与逐token自适应比特量化（TAB-Q）的两阶段中间激活压缩；三是构建了一个联合优化框架，协同选择最优分割点、量化配置和序列长度。

Result: 在多个大模型和硬件平台上的实验表明，该框架相比SmoothQuant、OmniQuant和Atom等先进方法，实现了1.49倍的推理加速和显著的通信开销降低，同时保持或提升了模型精度。

Conclusion: 所提出的自回归感知分割计算框架有效解决了大语言模型在边缘设备部署中的内存、延迟和通信瓶颈，为LLM在资源受限场景下的高效运行提供了可行方案。

Abstract: Large language models (LLMs) have achieved near-human performance across
diverse reasoning tasks, yet their deployment on resource-constrained
Internet-of-Things (IoT) devices remains impractical due to massive parameter
footprints and memory-intensive autoregressive decoding. While split computing
offers a promising solution by partitioning model execution between edge
devices and cloud servers, existing approaches fail to address the unique
challenges of autoregressive inference, particularly the iterative token
generation process and expanding key-value (KV) cache requirements. This work
introduces the first autoregressive-aware split computing framework designed
explicitly for LLM deployment on edge devices. Our approach makes three key
contributions. First, we develop one-point split compression (OPSC), a
mixed-precision quantization scheme that prevents out-of-memory failures by
strategically partitioning models into front-end and back-end segments with
different precision levels. Second, we propose a two-stage intermediate
compression pipeline that combines threshold splitting (TS) and token-wise
adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations
while dramatically reducing communication overhead. Third, we formulate a
unified optimization framework that jointly selects optimal split points,
quantization settings, and sequence lengths to satisfy strict memory and
latency constraints. Extensive evaluations across diverse LLMs and hardware
platforms demonstrate superior performance compared to state-of-the-art
quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework
achieves a 1.49 inference speedup and significant communication overhead
reduction while maintaining or improving model accuracy.

</details>


### [184] [Enhancing Multimodal Protein Function Prediction Through Dual-Branch Dynamic Selection with Reconstructive Pre-Training](https://arxiv.org/abs/2511.04040)
*Xiaoling Luo,Peng Chen,Chengliang Liu,Xiaopeng Jin,Jie Wen,Yumeng Liu,Junsong Wang*

Main category: cs.LG

TL;DR: 提出了一种新的多模态蛋白质功能预测方法DSRPGO，结合动态选择和重构预训练机制，显著提升了在人类数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态蛋白特征包含结构、序列、属性和相互作用网络等复杂信息，难以解析其相互关系，且层次化多标签分类具有挑战性。

Method: 引入重构预训练以挖掘低层次语义信息，设计双向交互模块（BInM）促进多模态特征间的交互学习，并提出动态选择模块（DSM）以选择最适合当前功能预测的特征表示。

Result: DSRPGO在人类数据集的BPO、MFO和CCO上显著优于其他基准模型。

Conclusion: DSRPGO通过动态选择和重构预训练机制有效整合多模态蛋白特征，在蛋白质功能预测任务中表现出优越性能。

Abstract: Multimodal protein features play a crucial role in protein function
prediction. However, these features encompass a wide range of information,
ranging from structural data and sequence features to protein attributes and
interaction networks, making it challenging to decipher their complex
interconnections. In this work, we propose a multimodal protein function
prediction method (DSRPGO) by utilizing dynamic selection and reconstructive
pre-training mechanisms. To acquire complex protein information, we introduce
reconstructive pre-training to mine more fine-grained information with low
semantic levels. Moreover, we put forward the Bidirectional Interaction Module
(BInM) to facilitate interactive learning among multimodal features.
Additionally, to address the difficulty of hierarchical multi-label
classification in this task, a Dynamic Selection Module (DSM) is designed to
select the feature representation that is most conducive to current protein
function prediction. Our proposed DSRPGO model improves significantly in BPO,
MFO, and CCO on human datasets, thereby outperforming other benchmark models.

</details>


### [185] [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)
*Yuantian Shao,Yuanteng Chen,Peisong Wang,Jianlin Yu,Jing Lin,Yiwu Yao,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 提出了一种高效的分布感知旋转校准方法DartQuant，用于大模型量化，显著加速并降低内存消耗，首次在单个3090 GPU上完成70B模型的旋转校准。


<details>
  <summary>Details</summary>
Motivation: 现有旋转矩阵优化方法计算成本高且易过拟合，需更高效、低依赖任务损失的校准方法。

Method: 提出DartQuant，通过约束旋转后激活值的分布来降低优化复杂度，并引入QR-Orth优化方案替代昂贵的交替优化。

Result: 在70B模型上实现47倍加速和10倍内存节省，首次在单个3090 GPU上完成旋转校准，性能优于现有方法。

Conclusion: DartQuant显著提升大模型量化的效率与可行性，尤其适用于资源受限环境。

Abstract: Quantization plays a crucial role in accelerating the inference of
large-scale models, and rotational matrices have been shown to effectively
improve quantization performance by smoothing outliers. However, end-to-end
fine-tuning of rotational optimization algorithms incurs high computational
costs and is prone to overfitting. To address this challenge, we propose an
efficient distribution-aware rotational calibration method, DartQuant, which
reduces the complexity of rotational optimization by constraining the
distribution of the activations after rotation. This approach also effectively
reduces reliance on task-specific losses, thereby mitigating the risk of
overfitting. Additionally, we introduce the QR-Orth optimization scheme, which
replaces expensive alternating optimization with a more efficient solution. In
a variety of model quantization experiments, DartQuant demonstrates superior
performance. Compared to existing methods, it achieves 47$\times$ acceleration
and 10$\times$ memory savings for rotational optimization on a 70B model.
Furthermore, it is the first to successfully complete rotational calibration
for a 70B model on a single 3090 GPU, making quantization of large language
models feasible in resource-constrained environments. Code is available at
https://github.com/CAS-CLab/DartQuant.git.

</details>


### [186] [Pediatric Appendicitis Detection from Ultrasound Images](https://arxiv.org/abs/2511.04069)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: cs.LG

TL;DR: 本研究基于预训练的ResNet架构开发了一个深度学习模型，用于从超声图像中自动检测儿童阑尾炎，在Regensburg儿科阑尾炎数据集上取得了93.44%的准确率，表现出良好的临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 儿童阑尾炎诊断因症状重叠和影像质量差异而具有挑战性，亟需一种自动化、可靠的辅助诊断方法。

Method: 采用预训练的ResNet模型，对Regensburg儿科阑尾炎数据集中的超声图像进行微调，用于区分阑尾炎与非阑尾炎病例；图像经过归一化、调整大小和数据增强处理。

Result: 模型在图像分类任务中达到93.44%的准确率、91.53%的精确率和89.8%的召回率，能有效学习判别性空间特征，克服了低对比度、斑点噪声和解剖变异带来的挑战。

Conclusion: 基于ResNet的深度学习模型在儿童阑尾炎的超声图像自动识别中表现优异，具备成为临床辅助诊断工具的潜力。

Abstract: Pediatric appendicitis remains one of the most common causes of acute
abdominal pain in children, and its diagnosis continues to challenge clinicians
due to overlapping symptoms and variable imaging quality. This study aims to
develop and evaluate a deep learning model based on a pretrained ResNet
architecture for automated detection of appendicitis from ultrasound images. We
used the Regensburg Pediatric Appendicitis Dataset, which includes ultrasound
scans, laboratory data, and clinical scores from pediatric patients admitted
with abdominal pain to Children Hospital. Hedwig in Regensburg, Germany. Each
subject had 1 to 15 ultrasound views covering the right lower quadrant,
appendix, lymph nodes, and related structures. For the image based
classification task, ResNet was fine tuned to distinguish appendicitis from
non-appendicitis cases. Images were preprocessed by normalization, resizing,
and augmentation to enhance generalization. The proposed ResNet model achieved
an overall accuracy of 93.44, precision of 91.53, and recall of 89.8,
demonstrating strong performance in identifying appendicitis across
heterogeneous ultrasound views. The model effectively learned discriminative
spatial features, overcoming challenges posed by low contrast, speckle noise,
and anatomical variability in pediatric imaging.

</details>


### [187] [Left Atrial Segmentation with nnU-Net Using MRI](https://arxiv.org/abs/2511.04071)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: cs.LG

TL;DR: 本研究应用nnU-Net框架对2013年左心房分割挑战赛的MRI数据集进行左心房自动分割，取得了93.5的平均Dice分数，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 左心房的准确分割对房颤消融术和心脏生物物理建模至关重要，但手动标注耗时且依赖观察者，难以适用于大规模或时间敏感的临床流程。

Method: 采用nnU-Net这一自动化、自配置的深度学习分割框架，利用30例带专家标注的MRI扫描数据进行训练与评估，模型自动调整预处理、网络结构和训练流程。

Result: 模型在Dice相似系数上达到平均93.5分，定性分析显示其能准确分割左心房主体及肺静脉近端，且对形态、对比度和图像质量变化具有良好的泛化能力。

Conclusion: nnU-Net在左心房MRI分割任务中表现出高精度和强鲁棒性，具备应用于临床实践的潜力。

Abstract: Accurate segmentation of the left atrium (LA) from cardiac MRI is critical
for guiding atrial fibrillation (AF) ablation and constructing biophysical
cardiac models. Manual delineation is time-consuming, observer-dependent, and
impractical for large-scale or time-sensitive clinical workflows. Deep learning
methods, particularly convolutional architectures, have recently demonstrated
superior performance in medical image segmentation tasks. In this study, we
applied the nnU-Net framework, an automated, self-configuring deep learning
segmentation architecture, to the Left Atrial Segmentation Challenge 2013
dataset. The dataset consists of thirty MRI scans with corresponding
expert-annotated masks. The nnU-Net model automatically adapted its
preprocessing, network configuration, and training pipeline to the
characteristics of the MRI data. Model performance was quantitatively evaluated
using the Dice similarity coefficient (DSC), and qualitative results were
compared against expert segmentations. The proposed nnUNet model achieved a
mean Dice score of 93.5, demonstrating high overlap with expert annotations and
outperforming several traditional segmentation approaches reported in previous
studies. The network exhibited robust generalization across variations in left
atrial shape, contrast, and image quality, accurately delineating both the
atrial body and proximal pulmonary veins.

</details>


### [188] [DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level Anomaly Detection](https://arxiv.org/abs/2511.04086)
*Qingfeng Chen,Haojin Zeng,Jingyi Jie,Shichao Zhang,Debo Cheng*

Main category: cs.LG

TL;DR: 提出了一种名为DeNoise的无监督图级异常检测框架，专门用于处理受污染的训练数据，通过对抗性目标和锚点对齐去噪机制学习抗噪声的图表示。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络方法通常假设训练集是干净的，但在实际中训练数据常被异常图污染，导致模型性能下降。

Method: DeNoise联合优化图级编码器、属性解码器和结构解码器，采用对抗性目标学习抗噪嵌入；引入编码器锚点对齐去噪机制，融合正常图的高信息节点嵌入，并结合对比学习压缩正常图表示、排斥异常图。

Result: 在八个真实世界数据集上的实验表明，DeNoise在不同噪声强度下均能学习到可靠的图级表示，并显著优于现有的UGAD基线方法。

Conclusion: DeNoise是一种鲁棒的UGAD框架，有效应对训练数据污染问题，在复杂现实场景中具有良好的应用潜力。

Abstract: With the rapid growth of graph-structured data in critical domains,
unsupervised graph-level anomaly detection (UGAD) has become a pivotal task.
UGAD seeks to identify entire graphs that deviate from normal behavioral
patterns. However, most Graph Neural Network (GNN) approaches implicitly assume
that the training set is clean, containing only normal graphs, which is rarely
true in practice. Even modest contamination by anomalous graphs can distort
learned representations and sharply degrade performance. To address this
challenge, we propose DeNoise, a robust UGAD framework explicitly designed for
contaminated training data. It jointly optimizes a graph-level encoder, an
attribute decoder, and a structure decoder via an adversarial objective to
learn noise-resistant embeddings. Further, DeNoise introduces an encoder
anchor-alignment denoising mechanism that fuses high-information node
embeddings from normal graphs into all graph embeddings, improving
representation quality while suppressing anomaly interference. A contrastive
learning component then compacts normal graph embeddings and repels anomalous
ones in the latent space. Extensive experiments on eight real-world datasets
demonstrate that DeNoise consistently learns reliable graph-level
representations under varying noise intensities and significantly outperforms
state-of-the-art UGAD baselines.

</details>


### [189] [KoTaP: A Panel Dataset for Corporate Tax Avoidance, Performance, and Governance in Korea](https://arxiv.org/abs/2511.04094)
*Hyungjong Na,Wonho Song,Seungyong Han,Donghyeon Jo,Sejin Myung,Hyungjoon Kim*

Main category: cs.LG

TL;DR: 本研究介绍了KoTaP，一个涵盖2011至2024年韩国KOSPI和KOSDAQ非金融上市公司的长期面板数据集，包含12,653个公司年度观测值，用于研究企业避税行为及其在多个经济领域的关联。


<details>
  <summary>Details</summary>
Motivation: 构建一个兼具国际可比性与韩国制度特征的标准化、平衡面板数据集，以支持企业税务规避与其他财务、治理等领域关系的研究。

Method: 基于排除金融企业、非12月财年结束、资本减值和税前亏损企业后的样本，构建包含CETR、GETR、TSTA、TSDA等避税指标的面板数据，并标准化变量以确保可解释性和国际一致性。

Result: 最终数据集包含1,754家公司共12,653个公司-年度观测值，具有良好的平衡性，核心变量分布与国际文献一致，并体现韩国企业高外资持股、集中所有权和高流动性等独特特征。

Conclusion: KoTaP数据集为会计、金融及跨学科研究提供了重要开放资源，可支持计量建模、深度学习、政策评估、审计规划和投资分析等多方面应用。

Abstract: This study introduces the Korean Tax Avoidance Panel (KoTaP), a long-term
panel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011
and 2024. After excluding financial firms, firms with non-December fiscal year
ends, capital impairment, and negative pre-tax income, the final dataset
consists of 12,653 firm-year observations from 1,754 firms. KoTaP is designed
to treat corporate tax avoidance as a predictor variable and link it to
multiple domains, including earnings management (accrual- and activity-based),
profitability (ROA, ROE, CFO, LOSS), stability (LEV, CUR, SIZE, PPE, AGE,
INVREC), growth (GRW, MB, TQ), and governance (BIG4, FORN, OWN). Tax avoidance
itself is measured using complementary indicators cash effective tax rate
(CETR), GAAP effective tax rate (GETR), and book-tax difference measures (TSTA,
TSDA) with adjustments to ensure interpretability. A key strength of KoTaP is
its balanced panel structure with standardized variables and its consistency
with international literature on the distribution and correlation of core
indicators. At the same time, it reflects distinctive institutional features of
Korean firms, such as concentrated ownership, high foreign shareholding, and
elevated liquidity ratios, providing both international comparability and
contextual uniqueness. KoTaP enables applications in benchmarking econometric
and deep learning models, external validity checks, and explainable AI
analyses. It further supports policy evaluation, audit planning, and investment
analysis, making it a critical open resource for accounting, finance, and
interdisciplinary research.

</details>


### [190] [Decomposable Neuro Symbolic Regression](https://arxiv.org/abs/2511.04124)
*Giorgio Morales,John W. Sheppard*

Main category: cs.LG

TL;DR: 提出一种可分解的符号回归方法，利用Transformer、遗传算法和遗传编程生成可解释的多元数学表达式，能更准确地恢复真实数学结构。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法多关注降低预测误差，忽视了对真实 governing equations 的识别，导致表达式过于复杂或不准确。

Method: 采用Multi-Set Transformer生成单变量符号骨架，通过遗传算法评估并筛选高质量候选，再用基于遗传编程的级联方法逐步合并为多元表达式，最后用遗传算法优化系数。

Result: 在不同噪声水平下，该方法的插值和外推误差低于或相当于其他符号回归方法，并能一致地恢复原始数学结构。

Conclusion: 该方法在保持表达式简洁性和可解释性的同时，显著提升了对真实函数结构的识别能力。

Abstract: Symbolic regression (SR) models complex systems by discovering mathematical
expressions that capture underlying relationships in observed data. However,
most SR methods prioritize minimizing prediction error over identifying the
governing equations, often producing overly complex or inaccurate expressions.
To address this, we present a decomposable SR method that generates
interpretable multivariate expressions leveraging transformer models, genetic
algorithms (GAs), and genetic programming (GP). In particular, our explainable
SR method distills a trained ``opaque'' regression model into mathematical
expressions that serve as explanations of its computed function. Our method
employs a Multi-Set Transformer to generate multiple univariate symbolic
skeletons that characterize how each variable influences the opaque model's
response. We then evaluate the generated skeletons' performance using a
GA-based approach to select a subset of high-quality candidates before
incrementally merging them via a GP-based cascade procedure that preserves
their original skeleton structure. The final multivariate skeletons undergo
coefficient optimization via a GA. We evaluated our method on problems with
controlled and varying degrees of noise, demonstrating lower or comparable
interpolation and extrapolation errors compared to two GP-based methods, three
neural SR methods, and a hybrid approach. Unlike them, our approach
consistently learned expressions that matched the original mathematical
structure.

</details>


### [191] [Exploring the Feasibility of End-to-End Large Language Model as a Compiler](https://arxiv.org/abs/2511.04132)
*Hongbin Zhang,Shihao Gao,Yang Liu,Mingjie Xing,Yanjun Wu,Chen Zhao*

Main category: cs.LG

TL;DR: 本文探索了大语言模型（LLM）作为端到端编译器的可行性，提出了CompilerEval数据集和评估框架，发现当前LLM虽具备基本编译能力但成功率较低，通过优化提示、扩大模型规模和引入推理方法可显著提升生成汇编代码的质量，并对未来LLaC的发展提出架构设计与研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多个领域展现出优势，其在编译器领域的端到端应用尚未充分探索。传统编译器开发复杂且维护困难，而LLM有望简化这一过程，因此亟需评估LLM直接作为编译器的潜力与挑战。

Method: 设计专用的CompilerEval数据集和评估框架，用于测试主流LLM在源码理解与汇编代码生成方面的能力；分析生成错误类型，尝试多种提示优化和推理方法，并评估跨平台编译性能。

Result: 实验表明LLM已具备基本的编译能力，但编译成功率仍然较低；通过优化提示、使用更大模型和引入推理机制，可显著提升生成汇编代码的质量；同时验证了部分跨平台编译能力。

Conclusion: LLM作为编译器（LaaC）具有发展潜力，未来通过针对性训练、知识丰富的提示设计和专用基础设施支持，有望实现高质量汇编代码生成，推动编译技术范式转变。

Abstract: In recent years, end-to-end Large Language Model (LLM) technology has shown
substantial advantages across various domains. As critical system software and
infrastructure, compilers are responsible for transforming source code into
target code. While LLMs have been leveraged to assist in compiler development
and maintenance, their potential as an end-to-end compiler remains largely
unexplored. This paper explores the feasibility of LLM as a Compiler (LaaC) and
its future directions. We designed the CompilerEval dataset and framework
specifically to evaluate the capabilities of mainstream LLMs in source code
comprehension and assembly code generation. In the evaluation, we analyzed
various errors, explored multiple methods to improve LLM-generated code, and
evaluated cross-platform compilation capabilities. Experimental results
demonstrate that LLMs exhibit basic capabilities as compilers but currently
achieve low compilation success rates. By optimizing prompts, scaling up the
model, and incorporating reasoning methods, the quality of assembly code
generated by LLMs can be significantly enhanced. Based on these findings, we
maintain an optimistic outlook for LaaC and propose practical architectural
designs and future research directions. We believe that with targeted training,
knowledge-rich prompts, and specialized infrastructure, LaaC has the potential
to generate high-quality assembly code and drive a paradigm shift in the field
of compilation.

</details>


### [192] [Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning](https://arxiv.org/abs/2511.04147)
*Jiaming Zhang,Yujie Yang,Haoning Wang,Liping Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出了一种名为交换策略优化（EPO）的算法框架，用于解决具有无限约束的半无限安全强化学习问题，通过动态调整约束集实现性能与安全性的平衡。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，安全强化学习需要在连续参数空间上满足无穷多个安全约束，现有方法难以高效处理此类半无限约束问题。

Method: 提出EPO框架，迭代求解带有有限约束集的安全RL子问题，并通过约束扩张和删除机制动态更新活跃约束集，防止工作集无限增长。

Result: 理论分析表明，在温和假设下，EPO训练的策略性能接近最优，且全局约束违反严格保持在预设界限内。

Conclusion: EPO能有效处理半无限安全强化学习问题，在保证安全性的同时实现高性能策略学习。

Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements
while optimizing long-term performance. In many practical applications,
however, the problem involves an infinite number of constraints, known as
semi-infinite safe RL (SI-safe RL). Such constraints typically appear when
safety conditions must be enforced across an entire continuous parameter space,
such as ensuring adequate resource distribution at every spatial location. In
this paper, we propose exchange policy optimization (EPO), an algorithmic
framework that achieves optimal policy performance and deterministic bounded
safety. EPO works by iteratively solving safe RL subproblems with finite
constraint sets and adaptively adjusting the active set through constraint
expansion and deletion. At each iteration, constraints with violations
exceeding the predefined tolerance are added to refine the policy, while those
with zero Lagrange multipliers are removed after the policy update. This
exchange rule prevents uncontrolled growth of the working set and supports
effective policy training. Our theoretical analysis demonstrates that, under
mild assumptions, strategies trained via EPO achieve performance comparable to
optimal solutions with global constraint violations strictly remaining within a
prescribed bound.

</details>


### [193] [Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories](https://arxiv.org/abs/2511.04155)
*Olav Finne Praesteng Larsen,Massimiliano Ruocco,Michail Spitieris,Abdulmajid Murad,Martina Ragosta*

Main category: cs.LG

TL;DR: 本研究探讨了利用数据丰富机场（苏黎世）的生成模型，通过迁移学习适应数据稀缺机场（都柏林）的可行性，结果表明基于扩散的模型在仅使用5%都柏林数据时即可达到竞争性性能，显著降低了空中交通管理中轨迹生成的数据需求。


<details>
  <summary>Details</summary>
Motivation: 许多次要和区域机场面临航迹数据严重不足的问题，限制了机器学习方法的应用以及大规模仿真和“假设”分析的能力。

Method: 采用基于扩散和流匹配的先进生成模型架构，先在苏黎世机场数据上预训练，然后在不同比例（0%至100%）的都柏林机场本地数据上进行微调，评估其迁移能力。

Result: 基于扩散的模型在仅使用5%都柏林数据时即表现出竞争力，并在约20%数据时达到基线水平，持续优于从头训练的模型；潜在流匹配与潜在扩散模型也受益于预训练但增益较不稳定，而流匹配模型泛化能力较弱。

Conclusion: 迁移学习能显著减少空中交通管理中航迹生成所需的数据量，即使在历史数据有限的环境中也能实现逼真的合成数据生成，具有广泛应用潜力。

Abstract: Access to trajectory data is a key requirement for developing and validating
Air Traffic Management (ATM) solutions, yet many secondary and regional
airports face severe data scarcity. This limits the applicability of machine
learning methods and the ability to perform large-scale simulations or
"what-if" analyses. In this paper, we investigate whether generative models
trained on data-rich airports can be efficiently adapted to data-scarce
airports using transfer learning. We adapt state-of-the-art diffusion- and
flow-matching-based architectures to the aviation domain and evaluate their
transferability between Zurich (source) and Dublin (target) landing trajectory
datasets. Models are pretrained on Zurich and fine-tuned on Dublin with varying
amounts of local data, ranging from 0% to 100%. Results show that
diffusion-based models achieve competitive performance with as little as 5% of
the Dublin data and reach baseline-level performance around 20%, consistently
outperforming models trained from scratch across metrics and visual
inspections. Latent flow matching and latent diffusion models also benefit from
pretraining, though with more variable gains, while flow matching models show
weaker generalization. Despite challenges in capturing rare trajectory
patterns, these findings demonstrate the potential of transfer learning to
substantially reduce data requirements for trajectory generation in ATM,
enabling realistic synthetic data generation even in environments with limited
historical records.

</details>


### [194] [Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data](https://arxiv.org/abs/2511.04158)
*Anzhuo Xie,Wei-Chen Chang*

Main category: cs.LG

TL;DR: 提出一种基于Transformer的纵向建模方法，用于处理多源异构电子健康记录数据中的临床风险分类问题。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录数据中存在的不规则时间模式、模态差异大和语义结构复杂等挑战。

Method: 采用特征嵌入层统一表示结构化与非结构化数据，引入可学习的时间编码机制捕捉不均匀采样间隔下的动态演变，使用多头自注意力结构进行全局依赖建模，并设计语义加权池化模块以增强关键医疗事件的表征。

Result: 实验结果表明，该模型在准确率、召回率、精确度和F1分数上均优于传统机器学习和时序深度学习模型，实现了稳定且精准的风险识别。

Conclusion: 所提方法在多源异构EHR环境中表现出优越性能，为临床智能决策提供了高效可靠的技术框架。

Abstract: This study proposes a Transformer-based longitudinal modeling method to
address challenges in clinical risk classification with heterogeneous
Electronic Health Record (EHR) data, including irregular temporal patterns,
large modality differences, and complex semantic structures. The method takes
multi-source medical features as input and employs a feature embedding layer to
achieve a unified representation of structured and unstructured data. A
learnable temporal encoding mechanism is introduced to capture dynamic
evolution under uneven sampling intervals. The core model adopts a multi-head
self-attention structure to perform global dependency modeling on longitudinal
sequences, enabling the aggregation of long-term trends and short-term
fluctuations across different temporal scales. To enhance semantic
representation, a semantic-weighted pooling module is designed to assign
adaptive importance to key medical events, improving the discriminative ability
of risk-related features. Finally, a linear mapping layer generates
individual-level risk scores. Experimental results show that the proposed model
outperforms traditional machine learning and temporal deep learning models in
accuracy, recall, precision, and F1-Score, achieving stable and precise risk
identification in multi-source heterogeneous EHR environments and providing an
efficient and reliable framework for clinical intelligent decision-making.

</details>


### [195] [On Joint Regularization and Calibration in Deep Ensembles](https://arxiv.org/abs/2511.04160)
*Laurits Fredsgaard,Mikkel N. Schmidt*

Main category: cs.LG

TL;DR: 本文研究了联合调优权重衰减、温度缩放和早停对深度集成模型性能和不确定性量化的影响，并提出了一种部分重叠的验证策略，在数据利用与联合评估之间取得平衡。实验表明，联合调优通常能匹配或提升性能，但效果因任务和指标而异。


<details>
  <summary>Details</summary>
Motivation: 传统的深度集成方法通常独立调优各个模型，但已有研究表明联合调优可能更优。本文旨在探究联合调优超参数对模型性能和不确定性校准的影响，并解决数据划分在训练与验证间的权衡问题。

Method: 通过联合调优深度集成中的权重衰减、温度缩放和早停参数，比较其与独立调优的效果差异；提出一种部分重叠的验证集策略，以在保证联合评估的同时最大化训练数据利用率。

Result: 联合调优在多数情况下能够匹配或优于独立调优的性能，但在不同任务和指标下效果差异显著；提出的重叠验证策略在实践中表现良好，有效平衡了数据使用与模型评估需求。

Conclusion: 联合调优深度集成具有潜力，尤其在不确定性量化方面；部分重叠的验证策略为实际应用提供了可行方案，为深度集成的优化提供了实用指导。

Abstract: Deep ensembles are a powerful tool in machine learning, improving both model
performance and uncertainty calibration. While ensembles are typically formed
by training and tuning models individually, evidence suggests that jointly
tuning the ensemble can lead to better performance. This paper investigates the
impact of jointly tuning weight decay, temperature scaling, and early stopping
on both predictive performance and uncertainty quantification. Additionally, we
propose a partially overlapping holdout strategy as a practical compromise
between enabling joint evaluation and maximizing the use of data for training.
Our results demonstrate that jointly tuning the ensemble generally matches or
improves performance, with significant variation in effect size across
different tasks and metrics. We highlight the trade-offs between individual and
joint optimization in deep ensemble training, with the overlapping holdout
strategy offering an attractive practical solution. We believe our findings
provide valuable insights and guidance for practitioners looking to optimize
deep ensemble models. Code is available at:
https://github.com/lauritsf/ensemble-optimality-gap

</details>


### [196] [ScaleDL: Towards Scalable and Efficient Runtime Prediction for Distributed Deep Learning Workloads](https://arxiv.org/abs/2511.04162)
*Xiaokai Wang,Shaoyuan Huang,Yuting Li,Xiaofei Wang*

Main category: cs.LG

TL;DR: 提出了一种名为ScaleDL的新框架，结合非线性层建模与图神经网络的跨层交互机制，实现高精度、可泛化的深度神经网络运行时预测，并通过D最优法降低数据收集成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在准确性、泛化能力与数据收集成本之间难以平衡，传统方法精度有限，而图增强模型虽性能提升但数据成本高昂。

Method: 提出ScaleDL框架，采用非线性层建模与GNN-based跨层交互机制，并利用D-optimal方法减少数据采集开销。

Result: 在五个主流DNN模型任务上实验表明，相比基线模型，ScaleDL将MRE降低6倍，RMSE降低5倍，显著提升预测精度与泛化能力。

Conclusion: ScaleDL在保证较低数据收集成本的同时，实现了高精度和层次化泛化能力，为大规模DNN训练和推理的资源调度提供了有效解决方案。

Abstract: Deep neural networks (DNNs) form the cornerstone of modern AI services,
supporting a wide range of applications, including autonomous driving,
chatbots, and recommendation systems. As models increase in size and
complexity, DNN workloads like training and inference tasks impose
unprecedented demands on distributed computing resources, making the accurate
prediction of runtime essential for optimizing development and resource
allocation. Traditional methods rely on additive computational unit models,
limiting their accuracy and generalizability. In contrast, graph-enhanced
modeling improves performance but significantly increases data collection
costs. Therefore, there is a critical need for a method that strikes a balance
between accuracy, generalizability, and the costs of data collection. To
address these challenges, we propose ScaleDL, a novel runtime prediction
framework that combines nonlinear layer-wise modeling with graph neural network
(GNN)-based cross-layer interaction mechanism, enabling accurate DNN runtime
prediction and hierarchical generalizability across different network
architectures. Additionally, we employ the D-optimal method to reduce data
collection costs. Experiments on the workloads of five popular DNN models prove
that ScaleDL enhances runtime prediction accuracy and generalizability,
achieving 6$\times$ lower MRE and 5$\times$ lower RMSE compared to baseline
models.

</details>


### [197] [Block Rotation is All You Need for MXFP4 Quantization](https://arxiv.org/abs/2511.04214)
*Yuantian Shao,Peisong Wang,Yuanteng Chen,Chang Xu,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 本文研究了在MXFP4格式下后训练量化（PTQ）方法的性能，发现现有旋转方法与MXFP4不兼容，并提出一种新的块旋转策略以提升大语言模型在低精度格式下的量化效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的增长，部署成本高昂，亟需高效的量化方法；然而当前主流的INT4量化技术难以适配新兴的硬件支持的MXFP4浮点格式，尤其是基于旋转的方法存在兼容性问题。

Method: 通过系统评估现有PTQ方法在MXFP4下的表现，分析旋转方法失效的原因，并提出一种适配MXFP4特性的块级旋转策略，解决PoT缩放与全局能量重分布之间的冲突。

Result: 实验表明GPTQ等非旋转方法在MXFP4下表现良好，而传统旋转方法性能下降严重；所提出的块旋转策略显著提升了旋转类方法在多种大模型上的准确性。

Conclusion: 该工作为MXFP4等新兴低精度格式下的PTQ提供了有效解决方案和实践指导，推动了面向硬件友好的大模型压缩研究。

Abstract: Large language models (LLMs) have achieved remarkable success, but their
rapidly growing scale imposes prohibitive costs in memory, computation, and
energy. Post-training quantization (PTQ) is a promising solution for efficient
deployment, yet achieving accurate W4A4 quantization remains an open challenge.
While most existing methods are designed for INT4 formats, the emergence of
MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--
raises questions about the applicability of current techniques. In this work,
we establish a comprehensive benchmark of PTQ methods under the MXFP4 format.
Through systematic evaluation, we find that methods like GPTQ consistently
deliver strong performance, whereas rotation-based approaches, which are almost
used by all state-of-the-art approaches, suffer from severe incompatibility
with MXFP4. We further provide the first in-depth analysis of this conflict,
tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)
block scaling and the redistribution of outlier energy via global rotation.
Building on this insight, we propose a simple yet effective block rotation
strategy that adapts rotation-based methods to MXFP4, leading to substantial
accuracy improvements across diverse LLMs. Our findings not only offer clear
guidance for practitioners but also set a foundation for advancing PTQ research
under emerging low-precision formats.

</details>


### [198] [The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms](https://arxiv.org/abs/2511.04217)
*Hikari Otsuka,Daiki Chijiwa,Yasuyuki Okoshi,Daichi Fujiki,Susumu Takeuchi,Masato Motomura*

Main category: cs.LG

TL;DR: 本文提出了多头注意力机制中强彩票假设（SLTH）的理论分析，证明了在适当隐藏维度下，随机初始化的MHA包含可逼近任意MHA的强彩票子网络，并将该理论扩展到无归一化层的Transformer，实验验证了源模型隐藏维度增加时逼近误差呈指数下降。


<details>
  <summary>Details</summary>
Motivation: 现有的SLTH理论尚未涵盖Transformer中的核心组件——多头注意力（MHA）机制，缺乏对其存在强彩票票券的理论解释。

Method: 通过理论分析证明，在特定隐藏维度条件下，随机初始化的MHA中存在可高概率逼近任意目标MHA的强彩票子网络，并进一步将该结果扩展至无归一化层的Transformer结构。

Result: 证明了当MHA的键和值的隐藏维度为O(d log(Hd^{3/2}))时，其中H为头数、d为输入维度，随机初始化的MHA以高概率包含一个强彩票子网络；实验显示，随着源模型隐藏维度增加，SLT与目标模型之间的逼近误差呈指数级下降。

Conclusion: 本文填补了SLTH在Transformer特别是MHA机制中的理论空白，证实了MHA和无归一化层Transformer中强彩票子网络的存在性，并通过实验证明其有效性。

Abstract: The strong lottery ticket hypothesis (SLTH) conjectures that high-performing
subnetworks, called strong lottery tickets (SLTs), are hidden in randomly
initialized neural networks. Although recent theoretical studies have
established the SLTH across various neural architectures, the SLTH for
transformer architectures still lacks theoretical understanding. In particular,
the current theory of the SLTH does not yet account for the multi-head
attention (MHA) mechanism, a core component of transformers. To address this
gap, we introduce a theoretical analysis of the existence of SLTs within MHAs.
We prove that, if a randomly initialized MHA of $H$ heads and input dimension
$d$ has the hidden dimension $O(d\log(Hd^{3/2}))$ for the key and value, it
contains an SLT that approximates an arbitrary MHA with the same input
dimension with high probability. Furthermore, by leveraging this theory for
MHAs, we extend the SLTH to transformers without normalization layers. We
empirically validate our theoretical findings, demonstrating that the
approximation error between the SLT within a source model (MHA and transformer)
and an approximate target counterpart decreases exponentially by increasing the
hidden dimension of the source model.

</details>


### [199] [seqme: a Python library for evaluating biological sequence design](https://arxiv.org/abs/2511.04239)
*Rasmus Møller-Larsen,Adam Izdebski,Jan Olszewski,Pankhil Gawade,Michal Kmicikiewicz,Wojciech Zarzecki,Ewa Szczurek*

Main category: cs.LG

TL;DR: 本文介绍了seqme，一个模块化且高度可扩展的开源Python库，用于评估生物序列设计的计算方法，涵盖基于序列、嵌入和属性的多种指标。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法缺乏统一的评估工具，因此需要一个通用且模型无关的软件库来系统评估生物序列设计方法的性能。

Method: 开发了一个名为seqme的开源Python库，集成了三类评估指标（序列-based、嵌入-based、属性-based），支持多种生物序列类型，并提供嵌入模型、属性模型及可视化诊断功能。

Result: seqme实现了对多种生物序列（如蛋白质、DNA、小分子等）设计方法的全面评估，适用于一次性与迭代设计方法，并具备良好的可扩展性。

Conclusion: seqme填补了生物序列设计领域缺乏统一评估工具的空白，为不同计算方法提供了标准化、可比较的评估框架。

Abstract: Recent advances in computational methods for designing biological sequences
have sparked the development of metrics to evaluate these methods performance
in terms of the fidelity of the designed sequences to a target distribution and
their attainment of desired properties. However, a single software library
implementing these metrics was lacking. In this work we introduce seqme, a
modular and highly extendable open-source Python library, containing
model-agnostic metrics for evaluating computational methods for biological
sequence design. seqme considers three groups of metrics: sequence-based,
embedding-based, and property-based, and is applicable to a wide range of
biological sequences: small molecules, DNA, ncRNA, mRNA, peptides and proteins.
The library offers a number of embedding and property models for biological
sequences, as well as diagnostics and visualization functions to inspect the
results. seqme can be used to evaluate both one-shot and iterative
computational design methods.

</details>


### [200] [Guided by Stars: Interpretable Concept Learning Over Time Series via Temporal Logic Semantics](https://arxiv.org/abs/2511.04244)
*Irene Ferfoglia,Simone Silvetti,Gaia Saveri,Laura Nenzi,Luca Bortolussi*

Main category: cs.LG

TL;DR: 提出了一种名为STELLE的神经符号框架，通过将时间序列嵌入到时序逻辑概念空间中，实现分类与解释的统一，提供可读性强的局部和全局解释。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在安全关键应用中至关重要，但现有深度学习方法多为黑箱模型，缺乏可解释性。

Method: 提出STELLE框架，引入一种受信号时序逻辑（STL）启发的核函数，将原始时间序列映射到预定义STL公式的一致性空间，并联合优化分类准确性和可解释性。

Result: 实验表明，STELLE在多个真实世界基准上实现了具有竞争力的准确性，同时提供了逻辑上忠实的局部和全局解释。

Conclusion: STELLE有效平衡了时间序列分类的性能与可解释性，通过逻辑化表示支持人类理解模型决策过程。

Abstract: Time series classification is a task of paramount importance, as this kind of
data often arises in safety-critical applications. However, it is typically
tackled with black-box deep learning methods, making it hard for humans to
understand the rationale behind their output. To take on this challenge, we
propose a novel approach, STELLE (Signal Temporal logic Embedding for
Logically-grounded Learning and Explanation), a neuro-symbolic framework that
unifies classification and explanation through direct embedding of trajectories
into a space of temporal logic concepts. By introducing a novel STL-inspired
kernel that maps raw time series to their alignment with predefined STL
formulae, our model jointly optimises accuracy and interpretability, as each
prediction is accompanied by the most relevant logical concepts that
characterise it. This yields (i) local explanations as human-readable STL
conditions justifying individual predictions, and (ii) global explanations as
class-characterising formulae. Experiments demonstrate that STELLE achieves
competitive accuracy while providing logically faithful explanations, validated
on diverse real-world benchmarks.

</details>


### [201] [Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference](https://arxiv.org/abs/2511.04286)
*Matteo Cercola,Valeria Capretti,Simone Formentin*

Main category: cs.LG

TL;DR: 提出了一种结合RLHF可扩展性和PBO查询效率的混合框架，通过在RLHF流程中引入主动查询机制，实现了更高效、更节省样本的偏好学习，在高维优化和大语言模型微调任务中均表现出性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于人类偏好的学习方法在数据收集上成本高、效率低，而现有方法如RLHF和PBO各有优劣，缺乏兼顾可扩展性与样本效率的统一框架。

Method: 将PBO中的主动查询机制（acquisition-driven模块）集成到RLHF流程中，构建一个兼具RLHF可扩展性和PBO样本效率的混合框架，实现主动且高效的偏好数据收集。

Result: 在高维偏好优化和大语言模型微调两个典型任务上的实验表明，该方法在样本效率和整体性能上均显著优于传统方法。

Conclusion: 所提出的混合框架成功融合了RLHF与PBO的优势，为高效的人类偏好学习提供了新的有效范式。

Abstract: Learning from human preferences is a cornerstone of aligning machine learning
models with subjective human judgments. Yet, collecting such preference data is
often costly and time-consuming, motivating the need for more efficient
learning paradigms. Two established approaches offer complementary advantages:
RLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,
while PBO achieves greater sample efficiency through active querying. We
propose a hybrid framework that unifies RLHF's scalability with PBO's query
efficiency by integrating an acquisition-driven module into the RLHF pipeline,
thereby enabling active and sample-efficient preference gathering. We validate
the proposed approach on two representative domains: (i) high-dimensional
preference optimization and (ii) LLM fine-tuning. Experimental results
demonstrate consistent improvements in both sample efficiency and overall
performance across these tasks.

</details>


### [202] [Differentially Private In-Context Learning with Nearest Neighbor Search](https://arxiv.org/abs/2511.04332)
*Antti Koskela,Tejas Kulkarni,Laith Zumot*

Main category: cs.LG

TL;DR: 提出了一种结合差分隐私的上下文学习框架，通过隐私感知的最近邻搜索和隐私过滤机制，在文本分类和文档问答任务中实现了优于现有基线方法的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私上下文学习方法忽略了在检索相关上下文数据时使用的相似性搜索所带来的隐私风险，尤其是在现代大语言模型管道中这一环节至关重要。

Method: 提出一个集成最近邻检索的差分隐私框架，采用隐私过滤器跟踪所选样本的累积隐私成本，确保符合中心差分隐私预算。

Result: 在多个基准测试中显著优于现有基线方法，尤其在文本分类和文档问答任务上表现出更优的隐私与效用平衡。

Conclusion: 该方法有效解决了上下文学习中相似性搜索的隐私隐患，为大语言模型的隐私保护提供了更完整的解决方案。

Abstract: Differentially private in-context learning (DP-ICL) has recently become an
active research topic due to the inherent privacy risks of in-context learning.
However, existing approaches overlook a critical component of modern large
language model (LLM) pipelines: the similarity search used to retrieve relevant
context data. In this work, we introduce a DP framework for in-context learning
that integrates nearest neighbor search of relevant examples in a privacy-aware
manner. Our method outperforms existing baselines by a substantial margin
across all evaluated benchmarks, achieving more favorable privacy-utility
trade-offs. To achieve this, we employ nearest neighbor retrieval from a
database of context data, combined with a privacy filter that tracks the
cumulative privacy cost of selected samples to ensure adherence to a central
differential privacy budget. Experimental results on text classification and
document question answering show a clear advantage of the proposed method over
existing baselines.

</details>


### [203] [LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in Intensive Care](https://arxiv.org/abs/2511.04333)
*Federico Pirola,Fabio Stella,Marco Grzegorczyk*

Main category: cs.LG

TL;DR: 提出一种基于Gibbs采样的新方法，用于从不完整数据中学习动态贝叶斯网络（DBN），通过将缺失值视为服从高斯分布的未知参数，并在每次迭代中从其完整条件分布中采样，实现有原则的插补和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有处理纵向临床数据中缺失数据的方法多源于静态贝叶斯网络，未能充分考虑数据的时序特性，限制了对时间上不确定性量化的准确性，尤其在重症监护等依赖时间动态的场景中影响模型可信度与适用性。

Method: 提出一种基于Gibbs采样的贝叶斯方法，将每个缺失值视为服从高斯分布的未知参数，在每轮迭代中从其完整条件分布中采样未观测值，从而实现联合学习DBN结构与缺失数据插补，并提供不确定性量化。

Result: 在模拟数据和真实重症监护数据上的实验表明，相比MICE等标准模型无关方法，该方法具有更高的重构精度和更好的收敛性。

Conclusion: 将完整贝叶斯推断引入时序模型可提升插补可靠性，增强对模型行为的理解，支持更安全、更可靠的临床决策，尤其适用于缺失数据频繁且影响重大的医疗场景。

Abstract: Dynamic Bayesian networks (DBNs) are increasingly used in healthcare due to
their ability to model complex temporal relationships in patient data while
maintaining interpretability, an essential feature for clinical
decision-making. However, existing approaches to handling missing data in
longitudinal clinical datasets are largely derived from static Bayesian
networks literature, failing to properly account for the temporal nature of the
data. This gap limits the ability to quantify uncertainty over time, which is
particularly critical in settings such as intensive care, where understanding
the temporal dynamics is fundamental for model trustworthiness and
applicability across diverse patient groups. Despite the potential of DBNs, a
full Bayesian framework that integrates missing data handling remains
underdeveloped. In this work, we propose a novel Gibbs sampling-based method
for learning DBNs from incomplete data. Our method treats each missing value as
an unknown parameter following a Gaussian distribution. At each iteration, the
unobserved values are sampled from their full conditional distributions,
allowing for principled imputation and uncertainty estimation. We evaluate our
method on both simulated datasets and real-world intensive care data from
critically ill patients. Compared to standard model-agnostic techniques such as
MICE, our Bayesian approach demonstrates superior reconstruction accuracy and
convergence properties. These results highlight the clinical relevance of
incorporating full Bayesian inference in temporal models, providing more
reliable imputations and offering deeper insight into model behavior. Our
approach supports safer and more informed clinical decision-making,
particularly in settings where missing data are frequent and potentially
impactful.

</details>


### [204] [Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness](https://arxiv.org/abs/2511.04401)
*Subeen Park,Joowang Kim,Hakyung Lee,Sunjae Yoo,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出了一种新的嵌入正则化方法SCER，用于提升深度学习模型在最差组上的鲁棒性，通过抑制虚假相关性特征，在多个视觉和语言任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏将嵌入空间表示与最差组误差联系起来的严格理论框架，导致在子群体分布偏移下性能受限。

Method: 提出SCER方法，基于组间均值嵌入差异识别核心方向与虚假方向，并在嵌入层施加理论约束以抑制模型对虚假特征的依赖。

Result: 在多个视觉和语言基准上验证了SCER的有效性，显著提升了最差组准确率，优于当前最先进的方法。

Conclusion: SCER通过理论驱动的嵌入级正则化，有效减少模型对虚假相关的依赖，增强了在分布偏移下的最差组鲁棒性。

Abstract: Deep learning models achieve strong performance across various domains but
often rely on spurious correlations, making them vulnerable to distribution
shifts. This issue is particularly severe in subpopulation shift scenarios,
where models struggle in underrepresented groups. While existing methods have
made progress in mitigating this issue, their performance gains are still
constrained. They lack a rigorous theoretical framework connecting the
embedding space representations with worst-group error. To address this
limitation, we propose Spurious Correlation-Aware Embedding Regularization for
Worst-Group Robustness (SCER), a novel approach that directly regularizes
feature representations to suppress spurious cues. We show theoretically that
worst-group error is influenced by how strongly the classifier relies on
spurious versus core directions, identified from differences in group-wise mean
embeddings across domains and classes. By imposing theoretical constraints at
the embedding level, SCER encourages models to focus on core features while
reducing sensitivity to spurious patterns. Through systematic evaluation on
multiple vision and language, we show that SCER outperforms prior
state-of-the-art studies in worst-group accuracy. Our code is available at
\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.

</details>


### [205] [The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity](https://arxiv.org/abs/2511.04418)
*Tim Tomov,Dominik Fuchsgruber,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 本文研究了大语言模型在存在歧义的数据上的不确定性量化（UQ）性能，发现现有方法在无歧义任务上表现良好，但在歧义数据上退化至接近随机水平。为此，作者提出了首个带有真实答案分布标注的歧义问答数据集 MAQA* 和 AmbigQA*，并通过实验和理论分析揭示了当前 UQ 方法的根本局限性。


<details>
  <summary>Details</summary>
Motivation: 现实语言具有内在歧义性，而现有不确定性量化方法通常在无歧义任务上评估，无法反映真实场景下的模型可靠性，因此需要研究现有UQ方法在歧义数据上的表现及其根本限制。

Method: 构建了两个新的歧义问答数据集 MAQA* 和 AmbigQA*，其真实答案分布基于事实共现进行估计；在多种UQ范式下（包括预测分布、模型内部表征和模型集成）评估现有不确定性估计器的表现，并通过理论分析解释其在歧义情况下的失效机制。

Result: 实验表明，当前各类不确定性估计方法在歧义数据上性能显著下降，接近随机水平；理论分析揭示了基于预测分布和模型集成的方法在处理歧义时存在根本性局限。

Conclusion: 现有大语言模型的不确定性量化方法在面对语言歧义时表现不佳，暴露了当前建模范式的不足，亟需重新思考和改进以适应真实世界的模糊性。

Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is
critical for trustworthy deployment. While real-world language is inherently
ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically
benchmarked against tasks with no ambiguity. In this work, we demonstrate that
while current uncertainty estimators perform well under the restrictive
assumption of no ambiguity, they degrade to close-to-random performance on
ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first
ambiguous question-answering (QA) datasets equipped with ground-truth answer
distributions estimated from factual co-occurrence. We find this performance
deterioration to be consistent across different estimation paradigms: using the
predictive distribution itself, internal representations throughout the model,
and an ensemble of models. We show that this phenomenon can be theoretically
explained, revealing that predictive-distribution and ensemble-based estimators
are fundamentally limited under ambiguity. Overall, our study reveals a key
shortcoming of current UQ methods for LLMs and motivates a rethinking of
current modeling paradigms.

</details>


### [206] [On the Equivalence of Regression and Classification](https://arxiv.org/abs/2511.04422)
*Jayadeva,Naman Dwivedi,Hari Krishnan,N. M. Anoop Krishnan*

Main category: cs.LG

TL;DR: 本文提出回归与分类之间的一一对应关系，通过将回归问题转化为等效的分类任务，揭示了传统支持向量回归中边界最大化的新解释，并提出了用于评估回归难度的“可回归性”度量及学习线性化映射的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管在支持向量回归中使用了边界最大化项，但其作为正则化项的解释并不充分，缺乏与分类任务之间的形式化联系，因此需要建立回归与分类之间的明确等价关系以提供理论支持。

Method: 通过构造一个包含2M个样本的线性可分分类任务来等价表示具有M个样本位于超平面上的回归问题，利用该等价性推导新的回归公式，并基于此提出可回归性度量和神经网络学习线性化映射的方法。

Result: 发现了回归与分类之间的严格一一对应关系，由此导出不同于传统的回归公式，提出了无需先训练模型即可估计数据集回归难度的可回归性度量，并成功训练神经网络学习输入变量的线性化映射。

Conclusion: 回归与分类之间存在形式化的等价关系，这种等价性不仅为支持向量回归中的边界最大化提供了新解释，还启发了新的回归方法、评估指标以及神经网络的学习策略。

Abstract: A formal link between regression and classification has been tenuous. Even
though the margin maximization term $\|w\|$ is used in support vector
regression, it has at best been justified as a regularizer. We show that a
regression problem with $M$ samples lying on a hyperplane has a one-to-one
equivalence with a linearly separable classification task with $2M$ samples. We
show that margin maximization on the equivalent classification task leads to a
different regression formulation than traditionally used. Using the
equivalence, we demonstrate a ``regressability'' measure, that can be used to
estimate the difficulty of regressing a dataset, without needing to first learn
a model for it. We use the equivalence to train neural networks to learn a
linearizing map, that transforms input variables into a space where a linear
regressor is adequate.

</details>


### [207] [ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting](https://arxiv.org/abs/2511.04445)
*Syeda Sitara Wishal Fatima,Afshin Rahimi*

Main category: cs.LG

TL;DR: 本文提出了ForecastGAN，一种基于分解的对抗性框架，用于多步时间序列预测，能够有效结合数值和类别特征，在短期预测中优于现有Transformer模型，同时在长期预测中保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列预测方法在短期预测中表现不佳，且通常忽略类别特征，而Transformer模型虽擅长长期预测但存在局限性。

Method: ForecastGAN包含三个模块：分解模块提取趋势和季节性成分；模型选择模块根据预测步长选择最优神经网络结构；对抗训练模块利用条件生成对抗网络提升预测鲁棒性。

Result: 在11个多元时间序列基准数据集上的实验表明，ForecastGAN在短期预测上 consistently 优于最先进的Transformer模型，在长期预测上性能相当。

Conclusion: ForecastGAN提供了一种更具泛化能力的时间序列预测方法，能自适应不同预测场景和数据特性，且无需大量超参数调优。

Abstract: Time series forecasting is essential across domains from finance to supply
chain management. This paper introduces ForecastGAN, a novel decomposition
based adversarial framework addressing limitations in existing approaches for
multi-horizon predictions. Although transformer models excel in long-term
forecasting, they often underperform in short-term scenarios and typically
ignore categorical features. ForecastGAN operates through three integrated
modules: a Decomposition Module that extracts seasonality and trend components;
a Model Selection Module that identifies optimal neural network configurations
based on forecasting horizon; and an Adversarial Training Module that enhances
prediction robustness through Conditional Generative Adversarial Network
training. Unlike conventional approaches, ForecastGAN effectively integrates
both numerical and categorical features. We validate our framework on eleven
benchmark multivariate time series datasets that span various forecasting
horizons. The results show that ForecastGAN consistently outperforms
state-of-the-art transformer models for short-term forecasting while remaining
competitive for long-term horizons. This research establishes a more
generalizable approach to time series forecasting that adapts to specific
contexts while maintaining strong performance across diverse data
characteristics without extensive hyperparameter tuning.

</details>


### [208] [Federated Stochastic Minimax Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2511.04456)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 本文提出了两种用于联邦学习中重尾噪声下非凸-PL极小极大优化的新算法Fed-NSGDA-M和FedMuon-DA，并在较弱条件下证明了它们的收敛速率。据作者所知，这是首个在此类噪声下具有严格理论保证的联邦极小极大优化算法。


<details>
  <summary>Details</summary>
Motivation: 由于大量实证研究表明重尾噪声比标准的有界方差假设更符合实际，因此在非凸随机优化中受到越来越多关注。本文旨在解决联邦学习中重尾梯度噪声下的非凸-PL极小极大优化问题。

Method: 提出两种新算法：Fed-NSGDA-M（引入归一化梯度）和FedMuon-DA（利用Muon优化器进行本地更新），以应对重尾噪声。两种算法均在较温和的条件下设计。

Result: 理论上证明了两种算法均可达到O(1/(TNp)^((s-1)/(2s)))的收敛速率。实验结果广泛验证了其有效性。

Conclusion: Fed-NSGDA-M和FedMuon-DA是首个在重尾噪声下具有严格理论收敛保证的联邦极小极大优化算法，有效应对了该场景中的挑战。

Abstract: Heavy-tailed noise has attracted growing attention in nonconvex stochastic
optimization, as numerous empirical studies suggest it offers a more realistic
assumption than standard bounded variance assumption. In this work, we
investigate nonconvex-PL minimax optimization under heavy-tailed gradient noise
in federated learning. We propose two novel algorithms: Fed-NSGDA-M, which
integrates normalized gradients, and FedMuon-DA, which leverages the Muon
optimizer for local updates. Both algorithms are designed to effectively
address heavy-tailed noise in federated minimax optimization, under a milder
condition. We theoretically establish that both algorithms achieve a
convergence rate of $O({1}/{(TNp)^{\frac{s-1}{2s}}})$. To the best of our
knowledge, these are the first federated minimax optimization algorithms with
rigorous theoretical guarantees under heavy-tailed noise. Extensive experiments
further validate their effectiveness.

</details>


### [209] [Towards Causal Market Simulators](https://arxiv.org/abs/2511.04469)
*Dennis Thumm,Luis Ontaneda Mijares*

Main category: cs.LG

TL;DR: 提出了一种结合变分自编码器与结构因果模型的时间序列神经因果模型（TNCM-VAE），用于生成符合因果关系的反事实金融时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度生成模型的金融市场生成器缺乏因果推理能力，难以支持反事实分析和风险评估。

Method: 在解码器结构中引入有向无环图以施加因果约束，并采用因果Wasserstein距离进行训练，结合VAE与结构因果模型生成反事实金融时间序列。

Result: 在受Ornstein-Uhlenbeck过程启发的合成自回归模型上验证，反事实概率估计的L1距离低至0.03–0.10，显著优于基线方法。

Conclusion: TNCM-VAE能有效生成保持时间依赖性和因果关系的金融时间序列，支持压力测试、情景分析和增强回测。

Abstract: Market generators using deep generative models have shown promise for
synthetic financial data generation, but existing approaches lack causal
reasoning capabilities essential for counterfactual analysis and risk
assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that
combines variational autoencoders with structural causal models to generate
counterfactual financial time series while preserving both temporal
dependencies and causal relationships. Our approach enforces causal constraints
through directed acyclic graphs in the decoder architecture and employs the
causal Wasserstein distance for training. We validate our method on synthetic
autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating
superior performance in counterfactual probability estimation with L1 distances
as low as 0.03-0.10 compared to ground truth. The model enables financial
stress testing, scenario analysis, and enhanced backtesting by generating
plausible counterfactual market trajectories that respect underlying causal
mechanisms.

</details>


### [210] [Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training](https://arxiv.org/abs/2511.04485)
*Ipsita Ghosh,Ethan Nguyen,Christian Kümmerle*

Main category: cs.LG

TL;DR: 提出了一种基于二次重加权秩正则化（Q3R）的新型低秩诱导训练策略，能够在保持模型性能的同时有效压缩模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有的低秩优化方法在预训练任务中难以维持低秩结构和优化目标，亟需一种更有效的低秩训练方法。

Method: 基于迭代重加权最小二乘（IRLS）框架，提出Q3R方法，使用二次正则项来上界平滑后的对数行列式作为秩的代理目标，从而实现低秩诱导训练。

Result: Q3R可在ViT-Tiny等模型上实现60%至80%的参数压缩，在CIFAR-10上仅带来1.3%到4%的精度下降，并在图像与语言任务的Transformer模型上验证了其有效性。

Conclusion: Q3R是一种高效、兼容性强的低秩训练方法，适用于预训练和微调，能在显著压缩模型的同时保持良好性能。

Abstract: Parameter-efficient training, based on low-rank optimization, has become a
highly successful tool for fine-tuning large deep-learning models. However,
these methods fail at low-rank pre-training tasks where maintaining the
low-rank structure and the objective remains a challenging task. We propose the
Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel
low-rank inducing training strategy inspired by the iteratively reweighted
least squares (IRLS) framework. Q3R is based on a quadratic regularizer term
which majorizes a smoothed log determinant serving as rank surrogate objective.
Unlike other low-rank training techniques, Q3R is able to train weight matrices
with prescribed, low target ranks of models that achieve comparable predictive
performance as dense models, with small computational overhead, while remaining
fully compatible with existing architectures. For example, we demonstrated one
experiment where we are able to truncate $60\%$ and $80\%$ of the parameters of
a ViT-Tiny model with $~1.3\%$ and $~4\%$ accuracy drop in CIFAR-10 performance
respectively. The efficacy of Q3R is confirmed on Transformers across both
image and language tasks, including for low-rank fine-tuning.

</details>


### [211] [Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks](https://arxiv.org/abs/2511.04494)
*Alper Kalle,Theo Rudkiewicz,Mohamed-Oumar Ouerfelli,Mohamed Tamaazousti*

Main category: cs.LG

TL;DR: 提出基于数据感知的范数进行神经网络压缩，通过优化函数空间中的误差而非权重空间，使用Tucker-2和CPD张量分解方法，在无需微调的情况下实现高效压缩，并验证了其在多个模型和数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络压缩方法通常在权重空间中使用各向同性范数（如Frobenius范数）进行低秩逼近，忽略了输入数据分布的影响，导致压缩后性能下降，常需后续微调。本文旨在通过引入数据相关的范数，在函数空间中衡量压缩误差，提升压缩效率与精度。

Method: 提出一种基于数据感知的范数 $\lVert (W - \widetilde{W}) \Sigma^{1/2}\rVert_F$，其中 $\Sigma^{1/2}$ 是层输入协方差矩阵的平方根，用于衡量压缩前后输出分布的变化。针对Tucker-2和CPD两种常见张量分解，设计了新的交替最小二乘算法以直接优化该范数。

Result: 在ResNet-18/50、GoogLeNet等模型及ImageNet、Cifar等数据集上实验表明，该方法在无需微调时即可达到与现有方法相当甚至更好的精度；且协方差信息可跨数据集迁移，在原始训练数据不可用时仍能有效压缩。

Conclusion: 通过将压缩误差度量从权重空间转移到函数空间，并结合输入数据的统计特性，所提方法显著提升了张量分解压缩的效果，减少了对微调的依赖，增强了压缩模型的泛化与实用性。

Abstract: Neural networks are widely used for image-related tasks but typically demand
considerable computing power. Once a network has been trained, however, its
memory- and compute-footprint can be reduced by compression. In this work, we
focus on compression through tensorization and low-rank representations.
Whereas classical approaches search for a low-rank approximation by minimizing
an isotropic norm such as the Frobenius norm in weight-space, we use
data-informed norms that measure the error in function space. Concretely, we
minimize the change in the layer's output distribution, which can be expressed
as $\lVert (W - \widetilde{W}) \Sigma^{1/2}\rVert_F$ where $\Sigma^{1/2}$ is
the square root of the covariance matrix of the layer's input and $W$,
$\widetilde{W}$ are the original and compressed weights. We propose new
alternating least square algorithms for the two most common tensor
decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike
conventional compression pipelines, which almost always require
post-compression fine-tuning, our data-informed approach often achieves
competitive accuracy without any fine-tuning. We further show that the same
covariance-based norm can be transferred from one dataset to another with only
a minor accuracy drop, enabling compression even when the original training
dataset is unavailable. Experiments on several CNN architectures (ResNet-18/50,
and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100)
confirm the advantages of the proposed method.

</details>


### [212] [Alternative Fairness and Accuracy Optimization in Criminal Justice](https://arxiv.org/abs/2511.04505)
*Shaolong Wu,James Blume,Geshi Yeung*

Main category: cs.LG

TL;DR: 本文探讨了算法公平性在刑事司法中的应用，提出了一种改进的群体公平性方法，通过最小化加权误差损失并控制假阴性率差异来提高预测准确性，并提供了可操作的框架以增强技术设计的合法性。


<details>
  <summary>Details</summary>
Motivation: 由于算法公平性的关键概念仍不明确，尤其是在刑事司法领域，因此需要更清晰和实用的公平性定义与实施框架。

Method: 提出一种对标准群体公平性的简单修改：不再追求受保护群体间的完全平等，而是最小化加权误差损失，同时将假阴性率的差异限制在较小范围内；并针对数据偏见、潜在的积极行动和子群约束激增三类批评进行分析。

Result: 新方法更容易找到可行解，提高了预测准确性，并突出了误差成本的伦理选择；提出的三支柱实践框架（基于需求的决策、透明度与问责、精准定制的定义与解决方案）有助于公共决策系统的部署。

Conclusion: 将技术设计与合法性联系起来，为使用风险评估工具的机构提供切实可行的指导，推动算法公平性在现实场景中的负责任应用。

Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts
remain unsettled, especially in criminal justice. We review group, individual,
and process fairness and map the conditions under which they conflict. We then
develop a simple modification to standard group fairness. Rather than exact
parity across protected groups, we minimize a weighted error loss while keeping
differences in false negative rates within a small tolerance. This makes
solutions easier to find, can raise predictive accuracy, and surfaces the
ethical choice of error costs. We situate this proposal within three classes of
critique: biased and incomplete data, latent affirmative action, and the
explosion of subgroup constraints. Finally, we offer a practical framework for
deployment in public decision systems built on three pillars: need-based
decisions, Transparency and accountability, and narrowly tailored definitions
and solutions. Together, these elements link technical design to legitimacy and
provide actionable guidance for agencies that use risk assessment and related
tools.

</details>


### [213] [Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers](https://arxiv.org/abs/2511.04514)
*C. Hepburn,T. Zielke,A. P. Raulf*

Main category: cs.LG

TL;DR: 研究了线性模式连接（LMC）在数据偏移下的表现，发现小学习率和大批次可减轻数据偏移影响，LMC有助于在训练效率与集成多样性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 理解数据偏移对LMC的影响，并探索提升模型鲁棒性和泛化能力的训练条件。

Method: 将数据偏移视为随机梯度噪声的一种来源，通过实验分析不同学习率和批次大小对LMC及损失景观特性的影响。

Result: 小学习率和大批次有助于模型收敛到相同或更平滑的局部极小区域；通过LMC采样的模型错误更相似，但LMC能有效平衡训练效率与集成收益。

Conclusion: LMC不仅反映优化稳定性，还可作为分析数据偏移、泛化能力和模型多样性的工具，适当调整训练参数可增强其有效性。

Abstract: The phenomenon of linear mode connectivity (LMC) links several aspects of
deep learning, including training stability under noisy stochastic gradients,
the smoothness and generalization of local minima (basins), the similarity and
functional diversity of sampled models, and architectural effects on data
processing. In this work, we experimentally study LMC under data shifts and
identify conditions that mitigate their impact. We interpret data shifts as an
additional source of stochastic gradient noise, which can be reduced through
small learning rates and large batch sizes. These parameters influence whether
models converge to the same local minimum or to regions of the loss landscape
with varying smoothness and generalization. Although models sampled via LMC
tend to make similar errors more frequently than those converging to different
basins, the benefit of LMC lies in balancing training efficiency against the
gains achieved from larger, more diverse ensembles. Code and supplementary
materials will be made publicly available at https://github.com/DLR-KI/LMC in
due course.

</details>


### [214] [Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom Parity](https://arxiv.org/abs/2511.04518)
*Obed Amo,Samit Ghosh,Markus Lange-Hegermann,Bogdan Raiţă,Michael Pokojovy*

Main category: cs.LG

TL;DR: 本研究提出了一种基于边界约束的Ehrenpreis-Palamodov高斯过程（B-EPGP）代理模型，并与经典的有限元法结合Crank-Nicolson时间步进（CN-FEM）求解二维波动方程在齐次Dirichlet边界条件下的性能进行比较。通过匹配自由度（DoF）以确保公平对比，结果表明B-EPGP在时空L²误差和最大时间L²空间误差上显著优于CN-FEM，精度提高约两个数量级。


<details>
  <summary>Details</summary>
Motivation: 为了提升求解具有齐次Dirichlet边界条件的二维波动方程的精度与效率，研究旨在开发一种能精确满足PDE及边界条件的新方法，并与传统数值方法进行公平比较。

Method: 采用源自特征簇的指数-多项式基函数构建B-EPGP代理模型，使其精确满足偏微分方程和边界条件，并利用惩罚最小二乘法估计系数；同时引入自由度匹配协议，确保与CN-FEM在相同复杂度下进行公平比较。

Result: 在自由度匹配的情况下，B-EPGP在时空L²误差和最大时间L²空间误差方面均显著低于CN-FEM，精度提升约两个数量级。

Conclusion: B-EPGP方法在求解二维波动方程时相较于传统CN-FEM具有显著更高的精度，展示了其作为高效、高精度代理模型的潜力。

Abstract: We present a new benchmarking study comparing a boundary-constrained
Ehrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classical
finite element method combined with Crank--Nicolson time stepping (CN-FEM) for
solving the two-dimensional wave equation with homogeneous Dirichlet boundary
conditions. The B-EPGP construction leverages exponential-polynomial bases
derived from the characteristic variety to enforce the PDE and boundary
conditions exactly and employs penalized least squares to estimate the
coefficients. To ensure fairness across paradigms, we introduce a
degrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGP
consistently attains lower space-time $L^2$-error and maximum-in-time
$L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders of
magnitude.

</details>


### [215] [End-to-End Reinforcement Learning of Koopman Models for eNMPC of an Air Separation Unit](https://arxiv.org/abs/2511.04522)
*Daniel Mayfrank,Kayra Dernek,Laura Lang,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 提出一种基于强化学习的Koopman代理模型训练方法，适用于经济非线性模型预测控制，在大规模空气分离装置案例中表现出良好的可扩展性，并在仅依赖少数可观测变量的情况下避免约束违反。


<details>
  <summary>Details</summary>
Motivation: 现有Koopman模型在经济非线性模型预测控制中存在约束违反问题，且多在小规模案例验证，缺乏在大规模复杂系统中的应用验证。

Method: 采用基于强化学习的方法训练Koopman代理模型，优化其在特定经济非线性模型预测控制应用中的性能，并在仅有少量可观测变量的条件下进行数值实验。

Result: 在大规模氮气空气分离单元的需求响应案例中，该方法相较于纯系统辨识的Koopman eNMPC具有相当的经济性能，同时有效避免了约束违反。

Conclusion: 所提出的强化学习驱动的Koopman模型训练方法具有良好的可扩展性，适用于复杂工业过程的经济非线性模型预测控制，且在有限测量信息下仍能保证运行安全性。

Abstract: With our recently proposed method based on reinforcement learning (Mayfrank
et al. (2024), Comput. Chem. Eng. 190), Koopman surrogate models can be trained
for optimal performance in specific (economic) nonlinear model predictive
control ((e)NMPC) applications. So far, our method has exclusively been
demonstrated on a small-scale case study. Herein, we show that our method
scales well to a more challenging demand response case study built on a
large-scale model of a single-product (nitrogen) air separation unit. Across
all numerical experiments, we assume observability of only a few realistically
measurable plant variables. Compared to a purely system identification-based
Koopman eNMPC, which generates small economic savings but frequently violates
constraints, our method delivers similar economic performance while avoiding
constraint violations.

</details>


### [216] [Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics](https://arxiv.org/abs/2511.04534)
*Jonas E. Katona,Emily K. de Jong,Nipun Gunawardena*

Main category: cs.LG

TL;DR: 提出了一种模型无关的、后处理的预测不确定性量化框架，利用共形预测为降阶模型的潜空间动力学、重构和端到端预测提供统计预测区间。


<details>
  <summary>Details</summary>
Motivation: 现有降阶模型缺乏鲁棒的不确定性量化方法，且多为架构或训练特定，限制了通用性和灵活性。

Method: 采用共形预测方法，在不修改原有模型架构和训练过程的前提下，对潜空间降阶模型的多个环节进行不确定性量化。

Result: 在云微物理的潜空间动力学模型上验证了该方法，能够准确预测液滴尺寸分布的演化，并量化整个降阶模型流程中的不确定性。

Conclusion: 所提方法是一种灵活、通用的后处理不确定性量化框架，适用于多种潜空间降阶模型，具有良好的校准性和实用性。

Abstract: Reduced-order models (ROMs) can efficiently simulate high-dimensional
physical systems, but lack robust uncertainty quantification methods. Existing
approaches are frequently architecture- or training-specific, which limits
flexibility and generalization. We introduce a post hoc, model-agnostic
framework for predictive uncertainty quantification in latent space ROMs that
requires no modification to the underlying architecture or training procedure.
Using conformal prediction, our approach estimates statistical prediction
intervals for multiple components of the ROM pipeline: latent dynamics,
reconstruction, and end-to-end predictions. We demonstrate the method on a
latent space dynamical model for cloud microphysics, where it accurately
predicts the evolution of droplet-size distributions and quantifies uncertainty
across the ROM pipeline.

</details>


### [217] [Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning](https://arxiv.org/abs/2511.04557)
*Divyansha Lachi,Mahmoud Mohammadi,Joe Meyer,Vinam Arora,Tom Palczewski,Eva L. Dyer*

Main category: cs.LG

TL;DR: 提出了一种基于时间子图采样和关系图感知器（RGP）的图变换器架构，用于有效融合结构与时间上下文，支持多任务联合学习，在多个基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图模型主要关注空间结构，忽视时间信息作为建模信号，并且通常仅支持单任务预测，难以应对复杂时序关系数据中的多任务学习需求。

Method: 引入时间子图采样以增强全局上下文，并提出RGP模型，采用基于交叉注意力的潜在瓶颈融合多类型节点和边的信息，通过共享潜在空间整合时空上下文，并利用灵活的交叉注意力解码器支持多任务联合学习。

Result: 在RelBench、SALT和CTU数据集上的实验表明，RGP在多种预测任务上达到最先进水平，展现出良好的通用性和可扩展性。

Conclusion: RGP能够有效集成复杂关系数据中的长程时空依赖，支持多任务学习，为关系型深度学习提供了一个通用且可扩展的解决方案。

Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics
of relational data emerge from complex interactions-such as those between
patients and providers, or users and products across diverse categories. To be
broadly useful, models operating on these data must integrate long-range
spatial and temporal dependencies across diverse types of entities, while also
supporting multiple predictive tasks. However, existing graph models for
relational data primarily focus on spatial structure, treating temporal
information merely as a filtering constraint to exclude future events rather
than a modeling signal, and are typically designed for single-task prediction.
To address these gaps, we introduce a temporal subgraph sampler that enhances
global context by retrieving nodes beyond the immediate neighborhood to capture
temporally relevant relationships. In addition, we propose the Relational Graph
Perceiver (RGP), a graph transformer architecture for relational deep learning
that leverages a cross-attention-based latent bottleneck to efficiently
integrate information from both structural and temporal contexts. This latent
bottleneck integrates signals from different node and edge types into a common
latent space, enabling the model to build global context across the entire
relational system. RGP also incorporates a flexible cross-attention decoder
that supports joint learning across tasks with disjoint label spaces within a
single model. Experiments on RelBench, SALT, and CTU show that RGP delivers
state-of-the-art performance, offering a general and scalable solution for
relational deep learning with support for diverse predictive tasks.

</details>


### [218] [ARETE: an R package for Automated REtrieval from TExt with large language models](https://arxiv.org/abs/2511.04573)
*Vasco V. Branco,Jandó Benedek,Lidia Pivovarova,Luís Correia,Pedro Cardoso*

Main category: cs.LG

TL;DR: ARETE是一个基于R语言的开源软件包，利用大语言模型（如chatGPT API）自动化提取物种分布数据，整合从OCR到异常值检测的全流程，并以表格形式输出，显著提升生物多样性研究中数据获取效率。


<details>
  <summary>Details</summary>
Motivation: 缺乏关键物种尤其是出现记录的数据是当前保护工作的主要障碍，且传统文献中的非机器可读信息需要大量人工提取，难以满足快速获取信息的需求。

Method: 开发ARETE R包，结合光学字符识别和大语言模型自动提取科学文献中的物种出现数据，集成数据提取与验证全过程，并通过与人工标注结果的系统比较进行验证。

Result: 在100种蜘蛛的案例中，新提取的数据使已知分布范围平均扩大三个数量级，揭示了物种历史分布的新区域，对空间保护规划和灭绝风险评估具有重要意义。

Conclusion: ARETE显著加速了以往未被充分利用的物种出现数据的获取，为依赖此类数据的研究项目提供了高效、可扩展的解决方案，有助于优化资源分配和项目规划。

Abstract: 1. A hard stop for the implementation of rigorous conservation initiatives is
our lack of key species data, especially occurrence data. Furthermore,
researchers have to contend with an accelerated speed at which new information
must be collected and processed due to anthropogenic activity. Publications
ranging from scientific papers to gray literature contain this crucial
information but their data are often not machine-readable, requiring extensive
human work to be retrieved. 2. We present the ARETE R package, an open-source
software aiming to automate data extraction of species occurrences powered by
large language models, namely using the chatGPT Application Programming
Interface. This R package integrates all steps of the data extraction and
validation process, from Optical Character Recognition to detection of outliers
and output in tabular format. Furthermore, we validate ARETE through systematic
comparison between what is modelled and the work of human annotators. 3. We
demonstrate the usefulness of the approach by comparing range maps produced
using GBIF data and with those automatically extracted for 100 species of
spiders. Newly extracted data allowed to expand the known Extent of Occurrence
by a mean three orders of magnitude, revealing new areas where the species were
found in the past, which mayhave important implications for spatial
conservation planning and extinction risk assessments. 4. ARETE allows faster
access to hitherto untapped occurrence data, a potential game changer in
projects requiring such data. Researchers will be able to better prioritize
resources, manually verifying selected species while maintaining automated
extraction for the majority. This workflow also allows predicting available
bibliographic data during project planning.

</details>


### [219] [Complexity as Advantage: A Regret-Based Perspective on Emergent Structure](https://arxiv.org/abs/2511.04590)
*Oshri Naparstek*

Main category: cs.LG

TL;DR: 提出“复杂性即优势”（CAA）框架，将系统的复杂性定义为不同观察者在建模时产生的预测遗憾差异，表明复杂性可带来信息优势，并统一多种涌现行为概念。


<details>
  <summary>Details</summary>
Motivation: 传统复杂性度量多关注系统内在属性，而忽视观察者的作用；本文旨在建立一个以观察者为中心的复杂性框架，解释为何某些系统在功能上更具价值。

Method: 通过引入预测遗憾（predictive regret）作为衡量标准，分析不同观察者对同一系统的建模难度差异，构建相对复杂性理论，并结合多尺度熵、预测信息等概念进行形式化统一。

Result: CAA框架成功整合了多种涌现行为的度量方式，揭示‘有趣’系统的关键在于在不同观察者间产生差异化的预测遗憾，并在简单动力学模型中得到验证。

Conclusion: 复杂性不应视为系统固有属性，而是相对于观察者家族的差异表现；该视角为理解学习、进化和人工智能中的复杂系统提供了新的量化基础。

Abstract: We introduce Complexity as Advantage (CAA), a framework that defines the
complexity of a system relative to a family of observers. Instead of measuring
complexity as an intrinsic property, we evaluate how much predictive regret a
system induces for different observers attempting to model it. A system is
complex when it is easy for some observers and hard for others, creating an
information advantage. We show that this formulation unifies several notions of
emergent behavior, including multiscale entropy, predictive information, and
observer-dependent structure. The framework suggests that "interesting" systems
are those positioned to create differentiated regret across observers,
providing a quantitative grounding for why complexity can be functionally
valuable. We demonstrate the idea through simple dynamical models and discuss
implications for learning, evolution, and artificial agents.

</details>


### [220] [Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning](https://arxiv.org/abs/2511.04598)
*Hampus Åström,Elin Anna Topp,Jacek Malec*

Main category: cs.LG

TL;DR: 本文研究了如何将常规强化学习环境转换为基于目标的环境，使智能体能够自主且无需外部奖励地学习解决任务。方法在训练时间上与外部引导的强化学习相当，且独立于底层算法，具有环境无关性，可实现通用智能体训练。


<details>
  <summary>Details</summary>
Motivation: 希望让智能体在没有外部奖励信号的情况下自主学习解决任务，提升其通用性和适应性。

Method: 通过将常规强化学习环境转化为目标条件环境，使智能体在训练过程中自主选择目标进行学习，且该方法不依赖特定的离策略学习算法。

Result: 实验表明，尽管单个目标的表现可能存在不稳定性，但平均目标成功率逐渐提高并趋于稳定；训练后的智能体可被指示追求环境中任何观察到的状态。

Conclusion: 该方法实现了环境无关、奖励自由的任务学习，支持智能体在具体应用场景前进行通用化训练，具备良好的扩展潜力。

Abstract: In this paper we study how transforming regular reinforcement learning
environments into goal-conditioned environments can let agents learn to solve
tasks autonomously and reward-free. We show that an agent can learn to solve
tasks by selecting its own goals in an environment-agnostic way, at training
times comparable to externally guided reinforcement learning. Our method is
independent of the underlying off-policy learning algorithm. Since our method
is environment-agnostic, the agent does not value any goals higher than others,
leading to instability in performance for individual goals. However, in our
experiments, we show that the average goal success rate improves and
stabilizes. An agent trained with this method can be instructed to seek any
observations made in the environment, enabling generic training of agents prior
to specific use cases.

</details>


### [221] [Addressing divergent representations from causal interventions on neural networks](https://arxiv.org/abs/2511.04638)
*Satchel Grant,Simon Jerome Han,Alexa Tartaglini,Christopher Potts*

Main category: cs.LG

TL;DR: 本文探讨了在机械可解释性中常用的因果干预技术是否会导致模型表示偏离自然分布，并提出了改进的反事实潜在（CL）损失来减少有害偏差，从而提高解释方法的可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究因果干预技术在改变模型内部表示时是否会引发表示偏离自然分布的问题，以及这种偏离对模型解释保真度的影响。

Method: 通过实证分析常见因果干预技术对模型表示分布的影响，并进行理论分析区分‘无害’和‘有害’的偏差；修改CL损失以正则化干预过程，使其更接近自然分布。

Result: 发现常见的因果干预常导致表示偏离自然分布；识别出两类偏差：发生在权重零空间内或行为决策边界协方差内的‘无害’偏差，以及激活隐藏路径并引发休眠行为变化的‘有害’偏差；改进的CL损失能有效降低有害偏差的发生率，同时保持干预的解释力。

Conclusion: 为实现更可靠的可解释性方法提供了理论依据和实用工具，强调应关注干预引起的分布偏移问题，并提出正则化策略作为解决方案。

Abstract: A common approach to mechanistic interpretability is to causally manipulate
model representations via targeted interventions in order to understand what
those representations encode. Here we ask whether such interventions create
out-of-distribution (divergent) representations, and whether this raises
concerns about how faithful their resulting explanations are to the target
model in its natural state. First, we demonstrate empirically that common
causal intervention techniques often do shift internal representations away
from the natural distribution of the target model. Then, we provide a
theoretical analysis of two classes of such divergences: `harmless' divergences
that occur in the null-space of the weights and from covariance within
behavioral decision boundaries, and `pernicious' divergences that activate
hidden network pathways and cause dormant behavioral changes. Finally, in an
effort to mitigate the pernicious cases, we modify the Counterfactual Latent
(CL) loss from Grant (2025) that regularizes interventions to remain closer to
the natural distributions, reducing the likelihood of harmful divergences while
preserving the interpretive power of interventions. Together, these results
highlight a path towards more reliable interpretability methods.

</details>


### [222] [Efficient probabilistic surrogate modeling techniques for partially-observed large-scale dynamical systems](https://arxiv.org/abs/2511.04641)
*Hans Harder,Abhijeet Vishwasrao,Luca Guastoni,Ricardo Vinuesa,Sebastian Peitz*

Main category: cs.LG

TL;DR: 本文研究并比较了多种减少采样步骤的流匹配范式扩展方法，用于预测由偏微分方程描述的动力系统，并在多个复杂系统上进行实验，探索了直接预测大规模三维模拟二维切片的挑战。


<details>
  <summary>Details</summary>
Motivation: 为了提高基于偏微分方程（如Navier-Stokes方程）的动力系统预测效率，减少生成过程中的采样步骤，提升计算性能与实用性。

Method: 比较了直接蒸馏、渐进蒸馏、对抗性扩散蒸馏、Wasserstein GANs 和修正流等扩展流匹配的方法，并在多个具有挑战性的动力系统上进行了实验验证。

Result: 实验表明这些方法在减少采样步数方面有效，且能够直接预测大规模3D模拟的2D切片，为求解器的高效流入生成提供了新途径。

Conclusion: 修正流及其他蒸馏策略显著减少了采样步骤，在保持预测精度的同时提升了生成效率，适用于复杂动力系统的概率预测任务。

Abstract: This paper is concerned with probabilistic techniques for forecasting
dynamical systems described by partial differential equations (such as, for
example, the Navier-Stokes equations). In particular, it is investigating and
comparing various extensions to the flow matching paradigm that reduce the
number of sampling steps. In this regard, it compares direct distillation,
progressive distillation, adversarial diffusion distillation, Wasserstein GANs
and rectified flows. Moreover, experiments are conducted on a set of
challenging systems. In particular, we also address the challenge of directly
predicting 2D slices of large-scale 3D simulations, paving the way for
efficient inflow generation for solvers.

</details>


### [223] [Optimal Inference Schedules for Masked Diffusion Models](https://arxiv.org/abs/2511.04647)
*Sitan Chen,Kevin Cong,Jerry Li*

Main category: cs.LG

TL;DR: 本文研究了掩码扩散模型（MDM）在生成文本时的并行采样能力，提出了对采样分布与真实分布之间差异的精确刻画，并通过与单变量函数逼近理论的联系，给出了新的上下界。结果表明，在某些自然条件下，可在O(log n)步内完成采样而无明显性能损失。


<details>
  <summary>Details</summary>
Motivation: 标准自回归语言模型推理过程串行，导致耗时且成本高；扩散语言模型（尤其是MDM）虽支持并行采样，但缺乏对其并行程度与性能权衡的严格理解。

Method: 建立采样分布与真实分布间差异的精确表达式，利用与单变量函数逼近理论的联系，分析不同解掩码调度下的误差，并基于总相关性和对偶总相关性等信息论性质提出新的上下界和采样策略。

Result: 得到了适用于任意分布和解掩码调度的误差精确表征；证明了最优调度一般不可学习；提出了依赖于总相关性和对偶总相关性的新上界和采样方案，显示在某些情况下可在O(log n)步内高效采样。

Conclusion: 尽管最优解掩码调度难以实现，但通过信息论指标可设计高效的近似调度，在保持生成质量的同时显著提升采样效率，为扩散语言模型的高效推理提供了理论基础。

Abstract: A major bottleneck of standard auto-regressive large language models is that
their inference process is inherently sequential, resulting in very long and
costly inference times. To circumvent this, practitioners proposed a class of
language models called diffusion language models, of which the masked diffusion
model (MDM) is the most successful. The MDM is able to sample tokens
out-of-order and, ostensibly, many tokens at once and in parallel. However,
there is very limited rigorous understanding of how much parallel sampling
these models can perform without noticeable degradation in their sampling
performance. Prior work of Li and Cai obtained some preliminary bounds, but
these are not tight for many natural classes of distributions. In this work, we
give a new, exact characterization of the expected divergence between the true
distribution and the sampled distribution, for any distribution and any
unmasking schedule for the sampler, showing an elegant connection to the theory
of univariate function approximation.
  By leveraging this connection, we then attain a number of novel lower and
upper bounds for this problem. While the connection to function approximation
in principle gives the optimal unmasking schedule for any distribution, we show
that it is in general impossible to compete with it without strong a priori
knowledge of the distribution, even in seemingly benign settings. However, we
also demonstrate new upper bounds and new sampling schedules in terms of
well-studied information-theoretic properties of the base distribution, namely,
its total correlation and dual total correlation, which show that in some
natural settings, one can sample in $O(log n)$ steps without any visible loss
in performance, where $n$ is the total sequence length.

</details>


### [224] [TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning](https://arxiv.org/abs/2511.04653)
*Xinlu Zhang,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 本文提出在无线时间触发的联邦学习（TT-Fed）系统中引入自适应模型剪枝，联合优化剪枝比例和带宽分配，以在满足延迟约束的同时最小化训练损失。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习面临通信开销大、设备异构导致的straggler问题，尤其是在无线带宽受限的情况下，亟需降低通信成本并控制学习延迟。

Method: 通过分析TT-Fed模型在模型剪枝下的梯度l2范数收敛性，推导出收敛上界，并基于此建立剪枝比例与无线带宽的联合优化问题，利用KKT条件求得闭式解。

Result: 仿真结果表明，所提模型剪枝方法可在保持模型性能的同时，将通信成本降低40%。

Conclusion: 自适应模型剪枝能有效降低无线TT-Fed系统的通信开销，在保证学习性能的前提下显著提升训练效率，适用于资源受限的联邦学习场景。

Abstract: Federated learning (FL) offers new opportunities in machine learning,
particularly in addressing data privacy concerns. In contrast to conventional
event-based federated learning, time-triggered federated learning (TT-Fed), as
a general form of both asynchronous and synchronous FL, clusters users into
different tiers based on fixed time intervals. However, the FL network consists
of a growing number of user devices with limited wireless bandwidth,
consequently magnifying issues such as stragglers and communication overhead.
In this paper, we introduce adaptive model pruning to wireless TT-Fed systems
and study the problem of jointly optimizing the pruning ratio and bandwidth
allocation to minimize the training loss while ensuring minimal learning
latency. To answer this question, we perform convergence analysis on the
gradient l_2 norm of the TT-Fed model based on model pruning. Based on the
obtained convergence upper bound, a joint optimization problem of pruning ratio
and wireless bandwidth is formulated to minimize the model training loss under
a given delay threshold. Then, we derive closed-form solutions for wireless
bandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The
simulation results show that model pruning could reduce the communication cost
by 40% while maintaining the model performance at the same level.

</details>


### [225] [Nowcast3D: Reliable precipitation nowcasting via gray-box learning](https://arxiv.org/abs/2511.04659)
*Huaguan Chen,Wei Han,Haofei Sun,Ning Lin,Xingtao Song,Yunfan Yang,Jie Tian,Yang Liu,Ji-Rong Wen,Xiaoye Zhang,Xueshun Shen,Hao Sun*

Main category: cs.LG

TL;DR: 提出一种基于三维雷达反射率的灰箱框架，结合物理约束与数据驱动方法，实现高精度极端降水临近预报。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端降水临近预报中存在速度慢、分辨率低、误差累积和忽略垂直信息等问题，难以满足高时空精度和长预测时效的需求。

Method: 构建一个全三维的灰箱预报框架，直接处理体扫描雷达数据，结合物理约束的神经算子与数据驱动学习；模型在守恒对流算子下学习垂直变化的3D平流场，参数化空间变分扩散，并引入布朗运动启发的随机项表征未解析运动，同时使用残差分支捕捉小尺度对流触发和微物理变化，扩散型随机模块估计不确定性。

Result: 该框架在多种降水情景下实现了长达三小时的准确预报，在160名气象学家的盲评中57%的情况下排名第一。

Conclusion: 通过恢复具有物理一致性的完整三维动力结构，该方法为极端降水的精准、可靠临近预报提供了一条可扩展且鲁棒的路径。

Abstract: Extreme precipitation nowcasting demands high spatiotemporal fidelity and
extended lead times, yet existing approaches remain limited. Numerical Weather
Prediction (NWP) and its deep-learning emulations are too slow and coarse for
rapidly evolving convection, while extrapolation and purely data-driven models
suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based
methods discard crucial vertical information, preventing accurate
reconstruction of height-dependent dynamics. We introduce a gray-box, fully
three-dimensional nowcasting framework that directly processes volumetric radar
reflectivity and couples physically constrained neural operators with
datadriven learning. The model learns vertically varying 3D advection fields
under a conservative advection operator, parameterizes spatially varying
diffusion, and introduces a Brownian-motion--inspired stochastic term to
represent unresolved motions. A residual branch captures small-scale convective
initiation and microphysical variability, while a diffusion-based stochastic
module estimates uncertainty. The framework achieves more accurate forecasts up
to three-hour lead time across precipitation regimes and ranked first in 57\%
of cases in a blind evaluation by 160 meteorologists. By restoring full 3D
dynamics with physical consistency, it offers a scalable and robust pathway for
skillful and reliable nowcasting of extreme precipitation.

</details>


### [226] [Forgetting is Everywhere](https://arxiv.org/abs/2511.04666)
*Ben Sanati,Thomas L. Lee,Trevor McInroe,Aidan Scannell,Nikolay Malkin,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: 提出了一种算法和任务无关的遗忘理论，将遗忘定义为学习者对未来经验预测分布的不一致性，并通过跨多种学习场景的实验证明了该理论的有效性。


<details>
  <summary>Details</summary>
Motivation: 缺乏对遗忘的统一定义，难以理解学习算法在适应新数据时丢失过去知识的机制。

Method: 提出一种将遗忘描述为预测分布中信息损失的理论，并设计通用的遗忘度量方法。

Result: 在分类、回归、生成建模和强化学习中均观察到遗忘现象，且遗忘程度与学习效率密切相关。

Conclusion: 该理论为理解和改进通用学习算法的信息保留能力提供了原则性基础。

Abstract: A fundamental challenge in developing general learning algorithms is their
tendency to forget past knowledge when adapting to new data. Addressing this
problem requires a principled understanding of forgetting; yet, despite decades
of study, no unified definition has emerged that provides insights into the
underlying dynamics of learning. We propose an algorithm- and task-agnostic
theory that characterises forgetting as a lack of self-consistency in a
learner's predictive distribution over future experiences, manifesting as a
loss of predictive information. Our theory naturally yields a general measure
of an algorithm's propensity to forget. To validate the theory, we design a
comprehensive set of experiments that span classification, regression,
generative modelling, and reinforcement learning. We empirically demonstrate
how forgetting is present across all learning settings and plays a significant
role in determining learning efficiency. Together, these results establish a
principled understanding of forgetting and lay the foundation for analysing and
improving the information retention capabilities of general learning
algorithms.

</details>


### [227] [Multi-Method Analysis of Mathematics Placement Assessments: Classical, Machine Learning, and Clustering Approaches](https://arxiv.org/abs/2511.04667)
*Julian D. Allagan,Dasia A. Singleton,Shanae N. Perry,Gabrielle C. Morgan,Essence A. Morgan*

Main category: cs.LG

TL;DR: 本研究通过结合经典测试理论、机器学习和无监督聚类方法，对一项40题的数学 placement 考试进行评估，发现55%的题目具有优良区分度，30%需替换；第6题为最强区分项；机器学习模型表现优异，聚类分析揭示了与现行标准不同的能力分界点，建议优化考试设计。


<details>
  <summary>Details</summary>
Motivation: 改进数学 placement 考试的有效性和公平性，避免学生被错误分类至补习课程，提升 Placement 决策的科学依据。

Method: 采用经典测试理论（CTT）分析题目区分度，使用随机森林和梯度提升等机器学习算法预测成绩并评估特征重要性，结合K-means聚类识别学生能力结构，并通过交叉验证和稳定性检验评估模型性能。

Result: 55%的题目区分度优秀（D ≥ 0.40），30%区分度差（D < 0.20）需更换；第6题（图表解读）区分度达1.000，F统计量和特征重要性均为最高；随机森林和梯度提升的交叉验证准确率分别为97.5%和96.0%；K-means聚类发现自然分界点为42.5%，低于机构现行的55%，且两簇解稳定（ARI = 0.855），下层纯度完美。

Conclusion: 多方法融合为数学 placement 考试优化提供了可靠依据，建议淘汰低区分度题目、实施两阶段测评，并将随机森林预测与透明化机制结合，以提高 placement 的准确性与公正性。

Abstract: This study evaluates a 40-item mathematics placement examination administered
to 198 students using a multi-method framework combining Classical Test Theory,
machine learning, and unsupervised clustering. Classical Test Theory analysis
reveals that 55\% of items achieve excellent discrimination ($D \geq 0.40$)
while 30\% demonstrate poor discrimination ($D < 0.20$) requiring replacement.
Question 6 (Graph Interpretation) emerges as the examination's most powerful
discriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVA
F-statistic ($F = 4609.1$), and maximum Random Forest feature importance
(0.206), accounting for 20.6\% of predictive power. Machine learning algorithms
demonstrate exceptional performance, with Random Forest and Gradient Boosting
achieving 97.5\% and 96.0\% cross-validation accuracy. K-means clustering
identifies a natural binary competency structure with a boundary at 42.5\%,
diverging from the institutional threshold of 55\% and suggesting potential
overclassification into remedial categories. The two-cluster solution exhibits
exceptional stability (bootstrap ARI = 0.855) with perfect lower-cluster
purity. Convergent evidence across methods supports specific refinements:
replace poorly discriminating items, implement a two-stage assessment, and
integrate Random Forest predictions with transparency mechanisms. These
findings demonstrate that multi-method integration provides a robust empirical
foundation for evidence-based mathematics placement optimization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [228] [Dynamic Shape Control of Soft Robots Enabled by Data-Driven Model Reduction](https://arxiv.org/abs/2511.03931)
*Iman Adibnazari,Harsh Sharma,Myungsun Park,Jacobo Cervera-Torralba,Boris Kramer,Michael T. Tolley*

Main category: cs.RO

TL;DR: 本研究比较了三种数据驱动的模型降维方法在软体机器人动态形状控制中的有效性，发现基于Lagrangian算子推断（LOpInf）的方法在轨迹跟踪中表现最优。


<details>
  <summary>Details</summary>
Motivation: 软体机器人需要高效的动力学建模工具以实现全身动态形状控制，但高维动力学和缺乏通用建模工具带来了挑战。

Method: 比较了特征系统实现算法、带控制的动态模态分解和Lagrangian算子推断（LOpInf）三种数据驱动的模型降维方法，并将其应用于模拟的仿鳗软体机器人的模型预测控制中。

Result: 在三种实验场景下（可行参考轨迹、基于生物模型的轨迹、物理模拟器生成的轨迹），LOpInf方法对应的控制策略均产生最低的跟踪误差。

Conclusion: LOpInf方法在生成适用于动态形状控制的线性模型方面优于其他方法，具有更强的控制精度和应用潜力。

Abstract: Soft robots have shown immense promise in settings where they can leverage
dynamic control of their entire bodies. However, effective dynamic shape
control requires a controller that accounts for the robot's high-dimensional
dynamics--a challenge exacerbated by a lack of general-purpose tools for
modeling soft robots amenably for control. In this work, we conduct a
comparative study of data-driven model reduction techniques for generating
linear models amendable to dynamic shape control. We focus on three
methods--the eigensystem realization algorithm, dynamic mode decomposition with
control, and the Lagrangian operator inference (LOpInf) method. Using each
class of model, we explored their efficacy in model predictive control policies
for the dynamic shape control of a simulated eel-inspired soft robot in three
experiments: 1) tracking simulated reference trajectories guaranteed to be
feasible, 2) tracking reference trajectories generated from a biological model
of eel kinematics, and 3) tracking reference trajectories generated by a
reduced-scale physical analog. In all experiments, the LOpInf-based policies
generated lower tracking errors than policies based on other models.

</details>


### [229] [Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots](https://arxiv.org/abs/2511.03996)
*Yushi Wang,Changsheng Luo,Penghui Chen,Jianran Liu,Weijian Sun,Tong Guo,Kechang Yang,Biao Hu,Yangang Zhang,Mingguo Zhao*

Main category: cs.RO

TL;DR: 提出一种基于强化学习的统一控制器，通过视觉感知与运动控制的直接集成，使类人机器人能够在动态环境中实现反应式足球技能。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖解耦模块，在动态环境中导致响应延迟和行为不连贯，且现实世界的感知限制加剧了这些问题。

Method: 扩展对抗性运动先验至真实动态环境中的感知设置，结合编码器-解码器架构与虚拟感知系统，建模现实世界视觉特征，从不完美观测中恢复特权状态，实现感知与动作的主动协调。

Result: 控制器在多种场景下表现出强反应性，能持续执行连贯且鲁棒的足球行为，包括实际RoboCup比赛。

Conclusion: 该方法有效整合感知与控制，提升了类人机器人在复杂动态环境中的实时决策与运动表现能力。

Abstract: Humanoid soccer poses a representative challenge for embodied intelligence,
requiring robots to operate within a tightly coupled perception-action loop.
However, existing systems typically rely on decoupled modules, resulting in
delayed responses and incoherent behaviors in dynamic environments, while
real-world perceptual limitations further exacerbate these issues. In this
work, we present a unified reinforcement learning-based controller that enables
humanoid robots to acquire reactive soccer skills through the direct
integration of visual perception and motion control. Our approach extends
Adversarial Motion Priors to perceptual settings in real-world dynamic
environments, bridging motion imitation and visually grounded dynamic control.
We introduce an encoder-decoder architecture combined with a virtual perception
system that models real-world visual characteristics, allowing the policy to
recover privileged states from imperfect observations and establish active
coordination between perception and action. The resulting controller
demonstrates strong reactivity, consistently executing coherent and robust
soccer behaviors across various scenarios, including real RoboCup matches.

</details>


### [230] [Integrating Ergonomics and Manipulability for Upper Limb Postural Optimization in Bimanual Human-Robot Collaboration](https://arxiv.org/abs/2511.04009)
*Chenzui Li,Yiming Chen,Xi Wu,Giacinto Barresi,Fei Chen*

Main category: cs.RO

TL;DR: 本文提出了一种上肢姿态优化方法，用于提升双手人机协同搬运任务中的物理人体工学性能和力操作性，通过优化人体关节角度并结合机器人末端执行器控制，实验证明该方法显著改善了肌肉负荷。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常只关注人类安全或操作效率，而忽略了二者的结合，本文旨在同时提升人机协作中的安全性与操作能力。

Method: 通过最小化代价函数来优化简化人体骨架模型的关节角度，并利用变换模块生成机器人参考末端位姿，采用双臂模型预测阻抗控制器（MPIC）对CURI机器人进行轨迹规划与姿态调整。

Result: 在不同对象和受试者的人-人协作（HHC）和人-机协作（HRC）实验中，目标肌肉激活程度显著降低，表明肌肉状况得到明显改善。

Conclusion: 所提方法有效整合了人体安全与操作灵活性，在多种协作条件下均能提升协作质量，具有良好的应用潜力。

Abstract: This paper introduces an upper limb postural optimization method for
enhancing physical ergonomics and force manipulability during bimanual
human-robot co-carrying tasks. Existing research typically emphasizes human
safety or manipulative efficiency, whereas our proposed method uniquely
integrates both aspects to strengthen collaboration across diverse conditions
(e.g., different grasping postures of humans, and different shapes of objects).
Specifically, the joint angles of a simplified human skeleton model are
optimized by minimizing the cost function to prioritize safety and manipulative
capability. To guide humans towards the optimized posture, the reference
end-effector poses of the robot are generated through a transformation module.
A bimanual model predictive impedance controller (MPIC) is proposed for our
human-like robot, CURI, to recalibrate the end effector poses through planned
trajectories. The proposed method has been validated through various subjects
and objects during human-human collaboration (HHC) and human-robot
collaboration (HRC). The experimental results demonstrate significant
improvement in muscle conditions by comparing the activation of target muscles
before and after optimization.

</details>


### [231] [An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue](https://arxiv.org/abs/2511.04042)
*Kailun Ji,Xiaoyu Hu,Xinyu Zhang,Jun Chen*

Main category: cs.RO

TL;DR: 提出一种基于大语言模型（LLM）的认知增强框架（LLM-CRF），用于提升无人机群在大规模搜救任务中与人类操作员的协作效率，通过自然交互理解意图并实现任务分解与规划，显著减少任务时间、提髙成功率并降低认知负荷。


<details>
  <summary>Details</summary>
Motivation: 复杂地形和通信中断使搜救行动面临挑战，传统人机协作存在从高层目标到低层指令转换的“意图-行动鸿沟”，易出错且对操作员认知负担大。

Method: 设计一个闭环LLM-CRF系统，利用大语言模型作为认知引擎，通过语音或图形标注捕捉操作员意图，进行意图理解、分层任务分解和无人机群任务规划，并支持实时反馈。

Result: 在模拟搜救场景中，相比传统指挥接口，任务完成时间减少约64.2%，任务成功率提高7%，NASA-TLX认知负荷评分下降42.9%。

Conclusion: 大语言模型能有效弥合人-群协同中的意图-行动鸿沟，提升搜救任务中人机协作的直观性与效能，具有在高风险场景中应用的巨大潜力。

Abstract: Large-scale disaster Search And Rescue (SAR) operations are persistently
challenged by complex terrain and disrupted communications. While Unmanned
Aerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area
search and supply delivery, yet their effective coordination places a
significant cognitive burden on human operators. The core human-machine
collaboration bottleneck lies in the ``intention-to-action gap'', which is an
error-prone process of translating a high-level rescue objective into a
low-level swarm command under high intensity and pressure. To bridge this gap,
this study proposes a novel LLM-CRF system that leverages Large Language Models
(LLMs) to model and augment human-swarm teaming cognition. The proposed
framework initially captures the operator's intention through natural and
multi-modal interactions with the device via voice or graphical annotations. It
then employs the LLM as a cognitive engine to perform intention comprehension,
hierarchical task decomposition, and mission planning for the UAV swarm. This
closed-loop framework enables the swarm to act as a proactive partner,
providing active feedback in real-time while reducing the need for manual
monitoring and control, which considerably advances the efficacy of the SAR
task. We evaluate the proposed framework in a simulated SAR scenario.
Experimental results demonstrate that, compared to traditional order and
command-based interfaces, the proposed LLM-driven approach reduced task
completion time by approximately $64.2\%$ and improved task success rate by
$7\%$. It also leads to a considerable reduction in subjective cognitive
workload, with NASA-TLX scores dropping by $42.9\%$. This work establishes the
potential of LLMs to create more intuitive and effective human-swarm
collaborations in high-stakes scenarios.

</details>


### [232] [Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors](https://arxiv.org/abs/2511.04052)
*Kyongsik Yun,David Bayard,Gerik Kubiak,Austin Owens,Andrew Johnson,Ryan Johnson,Dan Scharf,Thomas Lu*

Main category: cs.RO

TL;DR: 本文评估了在下一代多核处理器上部署制导、导航与控制（GNC）和着陆器视觉系统（LVS）算法的性能，展示了相较于传统航天硬件显著的速度提升，并提出了一种名为ARBITER的多核投票机制，用于实现实时容错计算，为未来行星探测任务提供了可扩展且节能的高可靠计算架构。


<details>
  <summary>Details</summary>
Motivation: 未来的行星探测任务需要高性能、容错能力强的计算系统，以支持进入、下降和着陆阶段的自主GNC和LVS操作，而传统航天硬件性能有限，难以满足需求。

Method: 将GNC和LVS算法部署在HPSC、Snapdragon VOXL2和AMD Xilinx Versal等多核处理器上，提出ARBITER多核投票机制进行实时故障检测与纠正，并通过故障注入实验分析关键敏感环节。

Result: 相比传统硬件，LVS图像处理速度提升达15倍，GFOLD轨迹优化速度提升超过250倍；ARBITER在静态优化和动态闭环控制中均有效实现容错；故障注入表明GFOLD的梯度计算阶段最易受比特级错误影响。

Conclusion: 该研究建立了一个可扩展、能效高的容错计算架构，适用于火星样本返回、土卫二轨道着陆器和谷神星样本返回等未来任务，满足其对自主性、低延迟和高可靠性的要求。

Abstract: Future planetary exploration missions demand high-performance, fault-tolerant
computing to enable autonomous Guidance, Navigation, and Control (GNC) and
Lander Vision System (LVS) operations during Entry, Descent, and Landing (EDL).
This paper evaluates the deployment of GNC and LVS algorithms on
next-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx
Versal--demonstrating up to 15x speedup for LVS image processing and over 250x
speedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory
optimization compared to legacy spaceflight hardware. To ensure computational
reliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for
Trusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that
performs real-time fault detection and correction across redundant cores.
ARBITER is validated in both static optimization tasks (GFOLD) and dynamic
closed-loop control (Attitude Control System). A fault injection study further
identifies the gradient computation stage in GFOLD as the most sensitive to
bit-level errors, motivating selective protection strategies and vector-based
output arbitration. This work establishes a scalable and energy-efficient
architecture for future missions, including Mars Sample Return, Enceladus
Orbilander, and Ceres Sample Return, where onboard autonomy, low latency, and
fault resilience are critical.

</details>


### [233] [CBMC-V3: A CNS-inspired Control Framework Towards Manipulation Agility with SNN](https://arxiv.org/abs/2511.04109)
*Yanbo Pang,Qingkai Li,Mingguo Zhao*

Main category: cs.RO

TL;DR: 提出一种基于脉冲神经网络（SNN）的仿生控制框架，模拟人类中枢神经系统，实现机器人臂在复杂环境中的敏捷操作。


<details>
  <summary>Details</summary>
Motivation: 现有控制算法在动态轨迹、不可预测交互和多样化物体的复杂环境中难以实现敏捷操作，尤其是在非工业场景中。

Method: 设计了一个包含五个模块（大脑皮层、小脑、丘脑、脑干、脊髓）、三个层级和两条信息通路（上行与下行）的仿生SNN控制框架。各模块均用SNN实现，脊髓用于反馈控制，脑干通过强化学习调节参数，小脑通过递归SNN学习动力学并提供前馈补偿力矩。

Result: 在仿真和真实机械臂平台上验证了该框架在不同负载和轨迹下的有效性，结果表明其在操作敏捷性上优于工业级位置控制。

Conclusion: 所提出的SNN仿生控制框架能够有效提升机器人臂在复杂动态环境中的敏捷操控能力，具有良好的实际应用潜力。

Abstract: As robotic arm applications extend beyond industrial settings into
healthcare, service, and daily life, existing control algorithms struggle to
achieve the agile manipulation required for complex environments with dynamic
trajectories, unpredictable interactions, and diverse objects. This paper
presents a biomimetic control framework based on Spiking Neural Networks (SNN),
inspired by the human Central Nervous System (CNS), to achieve agile control in
such environments. The proposed framework features five control modules
(cerebral cortex, cerebellum, thalamus, brainstem, spinal cord), three
hierarchical control levels (first-order, second-order, third-order), and two
information pathways (ascending, descending). Each module is fully implemented
using SNN. The spinal cord module uses spike encoding and Leaky
Integrate-and-Fire (LIF) neurons for feedback control. The brainstem module
employs a network of LIF and non-spiking LIF neurons to dynamically adjust
spinal cord parameters via reinforcement learning. The thalamus module
similarly adjusts the cerebellum's torque outputs. The cerebellum module uses a
recurrent SNN to learn the robotic arm's dynamics through regression, providing
feedforward gravity compensation torques. The framework is validated both in
simulation and on real-world robotic arm platform under various loads and
trajectories. Results demonstrate that our method outperforms the
industrial-grade position control in manipulation agility.

</details>


### [234] [BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning](https://arxiv.org/abs/2511.04131)
*Yitang Li,Zhengyi Luo,Tonghe Zhang,Cunxi Dai,Anssi Kanervisto,Andrea Tirinzoni,Haoyang Weng,Kris Kitani,Mateusz Guzek,Ahmed Touati,Alessandro Lazaric,Matteo Pirotta,Guanya Shi*

Main category: cs.RO

TL;DR: BFM-Zero 是一种用于人形机器人的行为基础模型框架，通过学习统一的潜在表示空间，实现无需重新训练即可支持多种下游任务的通用策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么局限于仿真环境，要么仅针对特定任务，缺乏在真实世界中适用于多任务的通用控制策略。

Method: 基于无监督强化学习和前向-后向模型，构建包含动作、目标和奖励的共享潜在空间，并结合奖励塑形、域随机化和历史依赖的非对称学习以缩小仿真到现实的差距。

Result: 在真实的 Unitree G1 人形机器人上实现了零样本运动跟踪、目标到达、奖励优化及少样本自适应等多种全身技能。

Conclusion: BFM-Zero 实现了首个可在真实环境中运行的可提示化、多任务人形机器人行为基础模型，推动了可扩展的通用控制发展。

Abstract: Building Behavioral Foundation Models (BFMs) for humanoid robots has the
potential to unify diverse control tasks under a single, promptable generalist
policy. However, existing approaches are either exclusively deployed on
simulated humanoid characters, or specialized to specific tasks such as
tracking. We propose BFM-Zero, a framework that learns an effective shared
latent representation that embeds motions, goals, and rewards into a common
space, enabling a single policy to be prompted for multiple downstream tasks
without retraining. This well-structured latent space in BFM-Zero enables
versatile and robust whole-body skills on a Unitree G1 humanoid in the real
world, via diverse inference methods, including zero-shot motion tracking, goal
reaching, and reward optimization, and few-shot optimization-based adaptation.
Unlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds
upon recent advancements in unsupervised RL and Forward-Backward (FB) models,
which offer an objective-centric, explainable, and smooth latent representation
of whole-body motions. We further extend BFM-Zero with critical reward shaping,
domain randomization, and history-dependent asymmetric learning to bridge the
sim-to-real gap. Those key design choices are quantitatively ablated in
simulation. A first-of-its-kind model, BFM-Zero establishes a step toward
scalable, promptable behavioral foundation models for whole-body humanoid
control.

</details>


### [235] [PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration](https://arxiv.org/abs/2511.04180)
*Yizhen Yin,Dapeng Feng,Hongbo Chen,Yuhua Qi*

Main category: cs.RO

TL;DR: 提出了一种结合路径-不确定性联合优化深度强化学习和轻量级停滞检测的混合框架，显著提高了主动SLAM的探索效率。


<details>
  <summary>Details</summary>
Motivation: 现有主动SLAM方法存在探索速度慢、路径次优等问题，需提升探索效率和路径质量。

Method: 采用路径-不确定性联合优化的深度强化学习框架，并引入基于Lidar静态异常检测和地图更新停滞检测的轻量级停滞机制，通过双目标奖励函数平衡探索与利用，并在低扩展率时终止 episode。

Result: 相比前沿法和RRT方法，探索时间最多减少65%，路径距离最多减少42%，且在复杂环境中保持良好的地图完整性；消融实验表明协作机制加快了训练收敛；实物平台验证了算法的实际应用性和仿真到现实的可迁移性。

Conclusion: 所提出的混合框架有效提升了主动SLAM的探索效率和路径优化能力，具备实际部署潜力。

Abstract: Existing Active SLAM methodologies face issues such as slow exploration speed
and suboptimal paths. To address these limitations, we propose a hybrid
framework combining a Path-Uncertainty Co-Optimization Deep Reinforcement
Learning framework and a Lightweight Stagnation Detection mechanism. The
Path-Uncertainty Co-Optimization framework jointly optimizes travel distance
and map uncertainty through a dual-objective reward function, balancing
exploration and exploitation. The Lightweight Stagnation Detection reduces
redundant exploration through Lidar Static Anomaly Detection and Map Update
Stagnation Detection, terminating episodes on low expansion rates. Experimental
results show that compared with the frontier-based method and RRT method, our
approach shortens exploration time by up to 65% and reduces path distance by up
to 42%, significantly improving exploration efficiency in complex environments
while maintaining reliable map completeness. Ablation studies confirm that the
collaborative mechanism accelerates training convergence. Empirical validation
on a physical robotic platform demonstrates the algorithm's practical
applicability and its successful transferability from simulation to real-world
environments.

</details>


### [236] [GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments](https://arxiv.org/abs/2511.04199)
*Shenglin Wang,Mingtong Dai,Jingxuan Su,Lingbo Liu,Chunjie Chen,Xinyu Wu,Liang Lin*

Main category: cs.RO

TL;DR: GraspView是一种仅使用RGB的机器人抓取管道，无需深度传感器即可在杂乱环境中实现精确操作，通过全局感知场景重建、渲染评分主动感知策略和在线度量对齐模块，在严重遮挡、近场感知和透明物体等挑战下显著优于RGB-D和单视图RGB基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统的RGB-D相机在透明或反光物体以及近距离感知时表现不佳，且在杂乱环境中由于遮挡和三维重建不一致常导致抓取失败，因此需要一种不依赖深度传感器但仍能实现稳定抓取的解决方案。

Method: GraspView结合了三个关键组件：(i) 全局感知场景重建，从单个RGB视图生成局部一致的尺度上一致的几何结构，并融合多视角投影构建连贯的全局3D场景；(ii) 渲染并评分的主动感知策略，动态选择最佳下一视角以揭示被遮挡区域；(iii) 在线度量对齐模块，将VGGT预测与机器人运动学校准以确保物理尺度一致性。在此基础上采用最佳视图全局抓取，融合多视角重建并利用GraspNet进行鲁棒执行。

Result: 在多种桌面物体上的实验表明，GraspView在严重遮挡、近场感知和透明物体条件下显著优于现有的RGB-D和单视图RGB基线方法。

Conclusion: GraspView是一种实用且通用的RGB-D抓取管道替代方案，能够在非结构化的现实环境中实现可靠抓取。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation, yet
remains highly challenging in cluttered environments where occlusion, poor
perception quality, and inconsistent 3D reconstructions often lead to unstable
or failed grasps. Conventional pipelines have widely relied on RGB-D cameras to
provide geometric information, which fail on transparent or glossy objects and
degrade at close range. We present GraspView, an RGB-only robotic grasping
pipeline that achieves accurate manipulation in cluttered environments without
depth sensors. Our framework integrates three key components: (i) global
perception scene reconstruction, which provides locally consistent, up-to-scale
geometry from a single RGB view and fuses multi-view projections into a
coherent global 3D scene; (ii) a render-and-score active perception strategy,
which dynamically selects next-best-views to reveal occluded regions; and (iii)
an online metric alignment module that calibrates VGGT predictions against
robot kinematics to ensure physical scale consistency. Building on these
tailor-designed modules, GraspView performs best-view global grasping, fusing
multi-view reconstructions and leveraging GraspNet for robust execution.
Experiments on diverse tabletop objects demonstrate that GraspView
significantly outperforms both RGB-D and single-view RGB baselines, especially
under heavy occlusion, near-field sensing, and with transparent objects. These
results highlight GraspView as a practical and versatile alternative to RGB-D
pipelines, enabling reliable grasping in unstructured real-world environments.

</details>


### [237] [Can Context Bridge the Reality Gap? Sim-to-Real Transfer of Context-Aware Policies](https://arxiv.org/abs/2511.04249)
*Marco Iannotta,Yuxuan Yang,Johannes A. Stork,Erik Schaffernicht,Todor Stoyanov*

Main category: cs.RO

TL;DR: 本文研究了在强化学习中通过引入上下文感知策略来提升从仿真到现实迁移性能的方法，通过集成上下文估计模块并与现有监督策略比较，在控制基准和真实推动物体任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于仿真与现实之间的环境动态差异，仿真训练的策略往往难以迁移到现实世界，尽管域随机化（DR）缓解了这一问题，但性能下降，因此需要更有效的迁移方法。

Method: 将上下文估计模块集成到基于域随机化的强化学习框架中，并系统比较当前最先进的监督策略，以实现上下文感知的策略训练。

Result: 上下文感知策略在所有测试场景中均优于无上下文基线，但最优监督策略依赖于具体任务。

Conclusion: 引入动态参数估计作为上下文能够有效提升sim-to-real迁移性能，选择合适的监督策略对任务表现至关重要。

Abstract: Sim-to-real transfer remains a major challenge in reinforcement learning (RL)
for robotics, as policies trained in simulation often fail to generalize to the
real world due to discrepancies in environment dynamics. Domain Randomization
(DR) mitigates this issue by exposing the policy to a wide range of randomized
dynamics during training, yet leading to a reduction in performance. While
standard approaches typically train policies agnostic to these variations, we
investigate whether sim-to-real transfer can be improved by conditioning the
policy on an estimate of the dynamics parameters -- referred to as context. To
this end, we integrate a context estimation module into a DR-based RL framework
and systematically compare SOTA supervision strategies. We evaluate the
resulting context-aware policies in both a canonical control benchmark and a
real-world pushing task using a Franka Emika Panda robot. Results show that
context-aware policies outperform the context-agnostic baseline across all
settings, although the best supervision strategy depends on the task.

</details>


### [238] [Design and Control of a Coaxial Dual-rotor Reconfigurable Tailsitter UAV Based on Swashplateless Mechanism](https://arxiv.org/abs/2511.04251)
*Jinfeng Liang,Haocheng Guo,Ximin Lyu*

Main category: cs.RO

TL;DR: 提出一种具有可重构机翼设计和无倾斜盘机构的高效能倾转尾坐式VTOL无人机，能够在多旋翼模式下收起机翼以减小风阻，并通过同轴异构双旋翼配置提升动力效率，经飞行测试验证了全飞行包线内的稳定性能。


<details>
  <summary>Details</summary>
Motivation: 传统尾坐式无人机在多旋翼模式下因大迎风面积易受风扰，且结构复杂、能耗高，需改进气动设计与控制机构以提升稳定性与效率。

Method: 采用可收放机翼设计，多旋翼模式下收起机翼以减少风阻；使用同轴异构双旋翼配置降低功耗；设计带挥舞铰的无倾斜盘机构简化结构并抑制振动；进行完整的过渡飞行试验验证全包线飞行性能。

Result: 实现了更低的结构重量与功耗，有效抑制了多旋翼模式下的振动，并在全飞行包线内展现出稳定的飞行性能，特别是在抗风性和过渡飞行稳定性方面表现良好。

Conclusion: 所提出的可重构机翼与优化的无倾斜盘控制机构显著提升了尾坐式VTOL无人机的飞行效率、稳定性和环境适应性，适用于复杂环境下的垂直起降飞行任务。

Abstract: The tailsitter vertical takeoff and landing (VTOL) UAV is widely used due to
its lower dead weight, which eliminates the actuators and mechanisms for
tilting. However, the tailsitter UAV is susceptible to wind disturbances in
multi-rotor mode, as it exposes a large frontal fuselage area. To address this
issue, our tailsitter UAV features a reconfigurable wing design, allowing wings
to retract in multi-rotor mode and extend in fixed- wing mode. Considering
power efficiency, we design a coaxial heterogeneous dual-rotor configuration,
which significantly re- duces the total power consumption. To reduce structural
weight and simplify structural complexity, we employ a swashplateless mechanism
with an improved design to control pitch and roll in multi-rotor mode. We
optimize the structure of the swashplateless mechanism by adding flapping
hinges, which reduces vibration during cyclic acceleration and deceleration.
Finally, we perform comprehensive transition flight tests to validate stable
flight performance across the entire flight envelope of the tailsitter UAV.

</details>


### [239] [MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments](https://arxiv.org/abs/2511.04320)
*Kuankuan Sima,Longbin Tang,Haozhe Ma,Lin Zhao*

Main category: cs.RO

TL;DR: 提出了一种名为MacroNav的基于学习的导航框架，通过多任务自监督学习和图推理实现高效、鲁棒的自主导航。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境下，现有方法难以平衡丰富的上下文表示与导航效率，需要紧凑且具有表达力的空间理解以支持高层决策。

Method: 设计了一个轻量级上下文编码器，采用多任务自监督学习捕捉多尺度、以导航为中心的空间表示，并结合强化学习策略与基于图的推理进行高效动作选择。

Result: 实验表明该上下文编码器具备高效且鲁棒的环境理解能力，在真实场景部署中，MacroNav在成功率（SR）和路径长度加权成功率（SPL）上显著优于现有最先进方法，同时保持低计算成本。

Conclusion: MacroNav在未知环境中的自主导航任务中实现了性能与效率的良好平衡，验证了其在实际应用中的有效性。

Abstract: Autonomous navigation in unknown environments requires compact yet expressive
spatial understanding under partial observability to support high-level
decision making. Existing approaches struggle to balance rich contextual
representation with navigation efficiency. We present MacroNav, a
learning-based navigation framework featuring two key components: (1) a
lightweight context encoder trained via multi-task self-supervised learning to
capture multi-scale, navigation-centric spatial representations; and (2) a
reinforcement learning policy that seamlessly integrates these representations
with graph-based reasoning for efficient action selection. Extensive
experiments demonstrate the context encoder's efficient and robust
environmental understanding. Real-world deployments further validate MacroNav's
effectiveness, yielding significant gains over state-of-the-art navigation
methods in both Success Rate (SR) and Success weighted by Path Length (SPL),
while maintaining low computational cost. Code will be released upon
acceptance.

</details>


### [240] [GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies](https://arxiv.org/abs/2511.04357)
*Maëlic Neau,Zoe Falomir,Paulo E. Santos,Anne-Gwenn Bosser,Cédric Buche*

Main category: cs.RO

TL;DR: 本文提出了一种新的神经符号方法GraSP-VLA，利用连续场景图表示从人类演示中生成符号化表征，用于自动规划域生成并协调低层VLA策略，从而提升长时任务中的技能学习与扩展能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言动作（VLA）模型的端到端模仿学习缺乏高层符号化规划能力，难以应对长时任务；而符号化动作模型学习（AML）方法又缺乏泛化性和可扩展性。因此需要一种结合两者优势的新方法。

Method: 提出GraSP-VLA框架，采用连续场景图（Continuous Scene Graph）表示来建模人类演示，提取符号化结构，并在推理时生成新的规划域，同时作为低层VLA策略的协调器，实现动作序列的扩展执行。

Result: 实验表明，GraSP-VLA能有效从观察中生成规划域，并在真实世界任务中展现出协调低层VLA策略完成长时任务的潜力，显著提升了动作序列的可扩展性。

Conclusion: GraSP-VLA通过融合神经网络与符号表示，在保持泛化能力的同时引入了高层规划机制，为机器人从演示中学习复杂、长时任务提供了一种可扩展的解决方案。

Abstract: Deploying autonomous robots that can learn new skills from demonstrations is
an important challenge of modern robotics. Existing solutions often apply
end-to-end imitation learning with Vision-Language Action (VLA) models or
symbolic approaches with Action Model Learning (AML). On the one hand, current
VLA models are limited by the lack of high-level symbolic planning, which
hinders their abilities in long-horizon tasks. On the other hand, symbolic
approaches in AML lack generalization and scalability perspectives. In this
paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that
uses a Continuous Scene Graph representation to generate a symbolic
representation of human demonstrations. This representation is used to generate
new planning domains during inference and serves as an orchestrator for
low-level VLA policies, scaling up the number of actions that can be reproduced
in a row. Our results show that GraSP-VLA is effective for modeling symbolic
representations on the task of automatic planning domain generation from
observations. In addition, results on real-world experiments show the potential
of our Continuous Scene Graph representation to orchestrate low-level VLA
policies in long-horizon tasks.

</details>


### [241] [Studying the Effect of Explicit Interaction Representations on Learning Scene-level Distributions of Human Trajectories](https://arxiv.org/abs/2511.04375)
*Anna Mészáros,Javier Alonso-Mora,Jens Kober*

Main category: cs.RO

TL;DR: 研究了在自动驾驶场景中如何最好地表示智能体之间的交互以捕捉联合分布，发现明确的交互定义通常能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚如何最佳表示智能体间的交互以捕捉联合分布，缺乏关于隐式学习或显式建模交互的共识。

Method: 在相同的网络结构下研究多种描述智能体交互的方式及其对最终学习到的联合分布的影响。

Result: 实验发现，仅依靠数据让网络隐式建立交互连接往往对性能有负面影响，而使用明确定义的交互（如交叉口谁先通过）则能显著提升性能。

Conclusion: 显式定义智能体间的交互关系比隐式学习更有效，有助于提高场景预测的准确性。

Abstract: Effectively capturing the joint distribution of all agents in a scene is
relevant for predicting the true evolution of the scene and in turn providing
more accurate information to the decision processes of autonomous vehicles.
While new models have been developed for this purpose in recent years, it
remains unclear how to best represent the joint distributions particularly from
the perspective of the interactions between agents. Thus far there is no clear
consensus on how best to represent interactions between agents; whether they
should be learned implicitly from data by neural networks, or explicitly
modeled using the spatial and temporal relations that are more grounded in
human decision-making. This paper aims to study various means of describing
interactions within the same network structure and their effect on the final
learned joint distributions. Our findings show that more often than not, simply
allowing a network to establish interactive connections between agents based on
data has a detrimental effect on performance. Instead, having well defined
interactions (such as which agent of an agent pair passes first at an
intersection) can often bring about a clear boost in performance.

</details>


### [242] [ForeRobo: Unlocking Infinite Simulation Data for 3D Goal-driven Robotic Manipulation](https://arxiv.org/abs/2511.04381)
*Dexin wang,Faliang Chang,Chunsheng Liu*

Main category: cs.RO

TL;DR: 本文提出ForeRobo，一种结合生成式仿真与经典控制的机器人智能体，通过“提出-生成-学习-执行”循环自主获取操作技能，在多种任务中实现优越的泛化性和79.28%的平均成功率。


<details>
  <summary>Details</summary>
Motivation: 在模拟环境中高效获取高级操作技能具有挑战性且意义重大，现有端到端策略学习方法缺乏可解释性和执行效率。

Method: 提出ForeRobo框架，包含自指导的propose-generate-learn-actuate循环：首先规划需学习的技能并构建相应仿真环境；利用ForeGen生成无限量技能一致的目标状态；训练ForeFormer模型预测当前状态下各点的3D目标位置；最后使用经典控制器在真实环境中基于预期目标状态执行动作。

Result: ForeFormer在刚体和关节物体操作任务中比现有最先进模型平均提升56.32%，具备强通用性；在超过20个真实机器人任务中实现零样本sim-to-real迁移，平均成功率达79.28%。

Conclusion: 将生成式建模与经典控制相结合的方法能有效提升机器人操作技能的可解释性、执行效率和跨任务泛化能力，验证了生成式仿真在自主技能获取中的潜力。

Abstract: Efficiently leveraging simulation to acquire advanced manipulation skills is
both challenging and highly significant. We introduce \textit{ForeRobo}, a
generative robotic agent that utilizes generative simulations to autonomously
acquire manipulation skills driven by envisioned goal states. Instead of
directly learning low-level policies, we advocate integrating generative
paradigms with classical control. Our approach equips a robotic agent with a
self-guided \textit{propose-generate-learn-actuate} cycle. The agent first
proposes the skills to be acquired and constructs the corresponding simulation
environments; it then configures objects into appropriate arrangements to
generate skill-consistent goal states (\textit{ForeGen}). Subsequently, the
virtually infinite data produced by ForeGen are used to train the proposed
state generation model (\textit{ForeFormer}), which establishes point-wise
correspondences by predicting the 3D goal position of every point in the
current state, based on the scene state and task instructions. Finally,
classical control algorithms are employed to drive the robot in real-world
environments to execute actions based on the envisioned goal states. Compared
with end-to-end policy learning methods, ForeFormer offers superior
interpretability and execution efficiency. We train and benchmark ForeFormer
across a variety of rigid-body and articulated-object manipulation tasks, and
observe an average improvement of 56.32\% over the state-of-the-art state
generation models, demonstrating strong generality across different
manipulation patterns. Moreover, in real-world evaluations involving more than
20 robotic tasks, ForeRobo achieves zero-shot sim-to-real transfer and exhibits
remarkable generalization capabilities, attaining an average success rate of
79.28\%.

</details>


### [243] [Temporal Action Selection for Action Chunking](https://arxiv.org/abs/2511.04421)
*Yueyang Weng,Xiaopeng Zhang,Yongjin Mu,Yingcong Zhu,Yanjie Li,Qi Liu*

Main category: cs.RO

TL;DR: 提出了一种新的算法Temporal Action Selector (TAS)，通过缓存多步预测的动作块并利用轻量级选择网络动态选择最优动作，有效平衡了反应性、决策一致性和运动连贯性，在多种任务中显著提升了成功率，并增强了与强化学习结合时的训练效率和性能上限。


<details>
  <summary>Details</summary>
Motivation: 现有的动作分块方法在降低决策频率的同时牺牲了对最新观测的利用，导致系统反应性不足，难以应对传感器噪声和环境动态变化，且现有方法无法同时兼顾反应性和决策一致性。

Method: 提出Temporal Action Selector (TAS)算法，缓存多个时间步的预测动作块，并通过一个轻量级的选择网络动态选择最佳动作，从而在保持决策一致性的同时提升反应性。

Result: 在多个任务和不同基础策略下实验表明，TAS显著提高了成功率，绝对增益高达73.3%；与残差强化学习结合后，训练效率和最终性能均得到提升；仿真和真实机器人实验验证了其有效性。

Conclusion: TAS能够在不牺牲决策一致性的前提下，有效提升动作分块系统的反应性和整体性能，为学习自示范提供了更鲁棒和高效的解决方案。

Abstract: Action chunking is a widely adopted approach in Learning from Demonstration
(LfD). By modeling multi-step action chunks rather than single-step actions,
action chunking significantly enhances modeling capabilities for human expert
policies. However, the reduced decision frequency restricts the utilization of
recent observations, degrading reactivity - particularly evident in the
inadequate adaptation to sensor noise and dynamic environmental changes.
Existing efforts to address this issue have primarily resorted to trading off
reactivity against decision consistency, without achieving both. To address
this limitation, we propose a novel algorithm, Temporal Action Selector (TAS),
which caches predicted action chunks from multiple timesteps and dynamically
selects the optimal action through a lightweight selector network. TAS achieves
balanced optimization across three critical dimensions: reactivity, decision
consistency, and motion coherence. Experiments across multiple tasks with
diverse base policies show that TAS significantly improves success rates -
yielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a
base policy with residual reinforcement learning (RL) substantially enhances
training efficiency and elevates the performance plateau. Experiments in both
simulation and physical robots confirm the method's efficacy.

</details>


### [244] [Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment](https://arxiv.org/abs/2511.04555)
*Tao Lin,Yilei Zhong,Yuxin Du,Jingjing Zhang,Jiting Liu,Yinxinyu Chen,Encheng Gu,Ziyan Liu,Hongyi Cai,Yanwen Zou,Lixing Zou,Zhaoye Zhou,Gen Li,Bo Zhao*

Main category: cs.RO

TL;DR: 提出Evo-1，一种轻量级视觉-语言-动作（VLA）模型，无需机器人数据预训练，在多个基准上达到SOTA性能，且具有高推理效率和低内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型参数多、依赖大量机器人数据预训练，导致训练成本高、部署困难，且训练过程常损害视觉语言骨干的感知表征，泛化能力差。

Method: 基于原生多模态视觉-语言模型（VLM），引入交叉调制扩散Transformer和优化集成模块，并采用两阶段训练范式逐步对齐动作与感知，保留VLM表征能力。

Result: Evo-1仅含7.7亿参数，在Meta-World和RoboTwin上分别超越先前最优模型12.4%和6.9%，在LIBERO上达到94.8%性能；真实场景中实现78%任务成功率，推理频率高、内存开销低。

Conclusion: Evo-1在不依赖机器人数据预训练的前提下，实现了高效训练与部署，并在仿真和真实环境中均表现出优异性能，推动了轻量级VLA模型的发展。

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful framework that
unifies perception, language, and control, enabling robots to perform diverse
tasks through multimodal understanding. However, current VLA models typically
contain massive parameters and rely heavily on large-scale robot data
pretraining, leading to high computational costs during training, as well as
limited deployability for real-time inference. Moreover, most training
paradigms often degrade the perceptual representations of the vision-language
backbone, resulting in overfitting and poor generalization to downstream tasks.
In this work, we present Evo-1, a lightweight VLA model that reduces
computation and improves deployment efficiency, while maintaining strong
performance without pretraining on robot data. Evo-1 builds on a native
multimodal Vision-Language model (VLM), incorporating a novel cross-modulated
diffusion transformer along with an optimized integration module, together
forming an effective architecture. We further introduce a two-stage training
paradigm that progressively aligns action with perception, preserving the
representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1
achieves state-of-the-art results on the Meta-World and RoboTwin suite,
surpassing the previous best models by 12.4% and 6.9%, respectively, and also
attains a competitive result of 94.8% on LIBERO. In real-world evaluations,
Evo-1 attains a 78% success rate with high inference frequency and low memory
overhead, outperforming all baseline methods. We release code, data, and model
weights to facilitate future research on lightweight and efficient VLA models.

</details>


### [245] [SAFe-Copilot: Unified Shared Autonomy Framework](https://arxiv.org/abs/2511.04664)
*Phat Nguyen,Erfan Aasi,Shiva Sreeram,Guy Rosman,Andrew Silva,Sertac Karaman,Daniela Rus*

Main category: cs.RO

TL;DR: 提出一种基于语义层面语言表示的统一共享自主框架，利用视觉语言模型从多模态线索中推断驾驶意图，并在高层实现人机协同决策，显著提升自动驾驶系统的鲁棒性和与人类意图的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有共享自动驾驶系统大多在低层次轨迹层面进行仲裁，仅考虑几何路径而丢失驾驶意图，导致在罕见或分布外场景下表现不佳。本文旨在通过更高层次的抽象实现人机协同，保留并融合人类驾驶意图。

Method: 提出一个统一的共享自主框架，利用视觉语言模型（VLM）从驾驶员行为和环境上下文等多模态线索中推断驾驶意图，并在语义层面（语言表示）融合人类输入与自动驾驶规划器，生成连贯的驾驶策略。

Result: 在模拟人类环境下达到完美召回率及高准确率和精确率；人类受试调查显示92%的情况下参与者认同仲裁结果；在Bench2Drive基准上显著降低碰撞率并提升整体性能。

Conclusion: 在语义、语言层面进行仲裁是一种有效的共享自主设计原则，能够实现常识推理并保持与人类意图的一致性，提升系统在复杂场景下的鲁棒性。

Abstract: Autonomous driving systems remain brittle in rare, ambiguous, and
out-of-distribution scenarios, where human driver succeed through contextual
reasoning. Shared autonomy has emerged as a promising approach to mitigate such
failures by incorporating human input when autonomy is uncertain. However, most
existing methods restrict arbitration to low-level trajectories, which
represent only geometric paths and therefore fail to preserve the underlying
driving intent. We propose a unified shared autonomy framework that integrates
human input and autonomous planners at a higher level of abstraction. Our
method leverages Vision Language Models (VLMs) to infer driver intent from
multi-modal cues -- such as driver actions and environmental context -- and to
synthesize coherent strategies that mediate between human and autonomous
control. We first study the framework in a mock-human setting, where it
achieves perfect recall alongside high accuracy and precision. A human-subject
survey further shows strong alignment, with participants agreeing with
arbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive
benchmark demonstrates a substantial reduction in collision rate and
improvement in overall performance compared to pure autonomy. Arbitration at
the level of semantic, language-based representations emerges as a design
principle for shared autonomy, enabling systems to exercise common-sense
reasoning and maintain continuity with human intent.

</details>


### [246] [Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions](https://arxiv.org/abs/2511.04665)
*Kaifeng Zhang,Shuo Sha,Hanxiao Jiang,Matthew Loper,Hyunjong Song,Guangyan Cai,Zhuo Xu,Xiaochen Hu,Changxi Zheng,Yunzhu Li*

Main category: cs.RO

TL;DR: 提出了一种基于真实视频构建软体数字孪生并结合3D高斯点阵实现光真实感渲染的仿真框架，用于可重复、可扩展且准确地评估机器人操作策略。


<details>
  <summary>Details</summary>
Motivation: 现有仿真器难以准确捕捉软体对象交互中的视觉与物理耦合复杂性，且真实世界直接评估机器人操作策略成本高、难复现。

Method: 从真实世界视频中重建软体数字孪生，并利用3D高斯点阵对机器人、物体和环境进行光真实感渲染，构建现实到仿真的策略评估框架。

Result: 在毛绒玩具打包、绳索引导和T型块推动等任务中验证了该方法，仿真结果与真实表现高度相关，并能揭示策略的关键行为模式。

Conclusion: 结合物理感知重建与高质量渲染的仿真方法，能够实现对机器人操作策略的有效、可扩展且可复现的评估。

Abstract: Robotic manipulation policies are advancing rapidly, but their direct
evaluation in the real world remains costly, time-consuming, and difficult to
reproduce, particularly for tasks involving deformable objects. Simulation
provides a scalable and systematic alternative, yet existing simulators often
fail to capture the coupled visual and physical complexity of soft-body
interactions. We present a real-to-sim policy evaluation framework that
constructs soft-body digital twins from real-world videos and renders robots,
objects, and environments with photorealistic fidelity using 3D Gaussian
Splatting. We validate our approach on representative deformable manipulation
tasks, including plush toy packing, rope routing, and T-block pushing,
demonstrating that simulated rollouts correlate strongly with real-world
execution performance and reveal key behavioral patterns of learned policies.
Our results suggest that combining physics-informed reconstruction with
high-quality rendering enables reproducible, scalable, and accurate evaluation
of robotic manipulation policies. Website: https://real2sim-eval.github.io/

</details>


### [247] [X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations](https://arxiv.org/abs/2511.04671)
*Maximus A. Pace,Prithwish Dan,Chuanruo Ning,Atiksh Bhardwaj,Audrey Du,Edward W. Duan,Wei-Chiu Ma,Kushal Kedia*

Main category: cs.RO

TL;DR: 提出X-Diffusion框架，利用前向扩散过程融合人类视频数据训练机器人策略，在保留高层次任务指导的同时消除低层次动作执行差异，显著提升操作任务成功率。


<details>
  <summary>Details</summary>
Motivation: 人类与机器人在身体结构上存在根本差异，直接迁移人类动作为机器人执行会导致物理上不可行的动作，因此需要一种方法在利用人类演示提供高价值运动线索的同时克服执行不匹配问题。

Method: X-Diffusion首先训练一个分类器判断带噪动作来自人类还是机器人；然后仅在加入足够噪声使分类器无法分辨其来源时才将人类动作用于策略训练；在去噪过程中，机器人动作在低噪声水平下指导精细去噪，而人类动作仅在高噪声水平下提供粗略引导。

Result: 在五个操作任务中，X-Diffusion比最佳基线平均成功率高出16%，实验表明该方法能有效利用人类数据并避免动态不可行动作的学习。

Conclusion: X-Diffusion通过扩散模型机制有效桥接了人类演示与机器人执行之间的鸿沟，为利用大规模人类视频数据训练机器人策略提供了可靠且高效的解决方案。

Abstract: Human videos can be recorded quickly and at scale, making them an appealing
source of training data for robot learning. However, humans and robots differ
fundamentally in embodiment, resulting in mismatched action execution. Direct
kinematic retargeting of human hand motion can therefore produce actions that
are physically infeasible for robots. Despite these low-level differences,
human demonstrations provide valuable motion cues about how to manipulate and
interact with objects. Our key idea is to exploit the forward diffusion
process: as noise is added to actions, low-level execution differences fade
while high-level task guidance is preserved. We present X-Diffusion, a
principled framework for training diffusion policies that maximally leverages
human data without learning dynamically infeasible motions. X-Diffusion first
trains a classifier to predict whether a noisy action is executed by a human or
robot. Then, a human action is incorporated into policy training only after
adding sufficient noise such that the classifier cannot discern its embodiment.
Actions consistent with robot execution supervise fine-grained denoising at low
noise levels, while mismatched human actions provide only coarse guidance at
higher noise levels. Our experiments show that naive co-training under
execution mismatches degrades policy performance, while X-Diffusion
consistently improves it. Across five manipulation tasks, X-Diffusion achieves
a 16% higher average success rate than the best baseline. The project website
is available at https://portal-cornell.github.io/X-Diffusion/.

</details>


### [248] [GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction](https://arxiv.org/abs/2511.04679)
*Qingzhou Lu,Yao Feng,Baiyu Shi,Michael Piseno,Zhenan Bao,C. Karen Liu*

Main category: cs.RO

TL;DR: 提出GentleHumanoid框架，通过将阻抗控制集成到全身运动跟踪策略中，实现上身柔顺性，从而在模拟和真实人形机器人上实现更安全、自然的物理交互。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人强化学习策略多强调刚性跟踪，抑制外部力，缺乏对柔顺交互的支持；现有阻抗方法局限于末端或基础控制，难以实现与人类自然互动所需的全身柔顺性。

Method: 提出一种基于弹簧的统一公式，建模抵抗性接触（如抵住表面）和引导性接触（来自人类动作数据的推拉），并将该阻抗模型融入全身运动跟踪的强化学习策略中，实现上身多关节（肩、肘、腕）的运动一致性与柔顺响应，同时设置可调力阈值以增强安全性。

Result: 在模拟和Unitree G1人形机器人上的实验表明，该方法在拥抱、起坐辅助和物体操作等任务中显著降低峰值接触力，同时保持任务成功率，实现更平滑、自然的交互。

Conclusion: GentleHumanoid为实现人形机器人在真实环境中与人类安全、有效协作提供了可行路径，推动了柔顺物理交互的发展。

Abstract: Humanoid robots are expected to operate in human-centered environments where
safe and natural physical interaction is essential. However, most recent
reinforcement learning (RL) policies emphasize rigid tracking and suppress
external forces. Existing impedance-augmented approaches are typically
restricted to base or end-effector control and focus on resisting extreme
forces rather than enabling compliance. We introduce GentleHumanoid, a
framework that integrates impedance control into a whole-body motion tracking
policy to achieve upper-body compliance. At its core is a unified spring-based
formulation that models both resistive contacts (restoring forces when pressing
against surfaces) and guiding contacts (pushes or pulls sampled from human
motion data). This formulation ensures kinematically consistent forces across
the shoulder, elbow, and wrist, while exposing the policy to diverse
interaction scenarios. Safety is further supported through task-adjustable
force thresholds. We evaluate our approach in both simulation and on the
Unitree G1 humanoid across tasks requiring different levels of compliance,
including gentle hugging, sit-to-stand assistance, and safe object
manipulation. Compared to baselines, our policy consistently reduces peak
contact forces while maintaining task success, resulting in smoother and more
natural interactions. These results highlight a step toward humanoid robots
that can safely and effectively collaborate with humans and handle objects in
real-world environments.

</details>
