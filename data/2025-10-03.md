<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]
- [cs.CL](#cs.CL) [Total: 90]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.RO](#cs.RO) [Total: 41]
- [cs.LG](#cs.LG) [Total: 146]
- [cs.AI](#cs.AI) [Total: 51]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 本文提出了LVTINO，首个基于视频一致性模型（VCM）的零样本高清晰度视频恢复逆问题求解器，通过利用VCM捕捉时间因果性，在保持测量一致性和平滑时间过渡的同时，实现了最先进的视频重建质量与计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像扩散模型逐帧处理视频的方法难以保持时间一致性，导致重建结果闪烁或不连贯，因此需要一种能同时恢复精细空间细节并建模细微时间依赖性的高效视频恢复方法。

Method: 提出LVTINO，利用最新视频一致性模型（VCM）作为先验，将视频扩散模型蒸馏为快速生成器以显式建模时间因果关系，并设计无需自动微分的条件机制，在少量神经网络前向传播下实现高质量视频重建。

Result: 在多种视频逆问题上实验表明，LVTINO显著优于当前逐帧应用图像LDM的最先进方法，取得了更好的感知质量、重建保真度和计算效率，实现了时间上的平滑过渡和强测量一致性。

Conclusion: LVTINO首次成功将零样本VCM先验应用于高定义视频恢复，在无需微调的情况下实现了高效且时空一致的高质量重建，为视频逆问题提供了新的基准。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 提出一种基于三阶段训练的风格提取图像生成方法，通过风格编码器和投影层实现细粒度文本引导的风格化图像生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像生成模型中难以精确描述和控制细粒度风格，以及参考图像风格信息与文本条件难对齐的问题。

Method: 设计一个风格编码器和风格投影层，从单个参考图像中提取细粒度风格表示，并将其注入预训练生成模型中，保持下游模型结构不变；采用三阶段训练策略，并构建Style30k-captions数据集进行训练。

Result: 实现了无需修改生成模型结构即可注入风格信息的方法，在细粒度风格控制方面表现良好，生成结果能准确反映文本描述和参考图像的风格特征。

Conclusion: 该方法有效提升了文本到图像生成中的细粒度风格控制能力，验证了通过参考图像提取可迁移风格表示的可行性。

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 本文提出一个用于判断技能学习过程中挣扎的新型数据集EvoStruggle，包含61.68小时视频、2793个视频片段和5385个标注的挣扎时间段，涵盖76名参与者在18项任务中的五次重复表现。作者将挣扎判定建模为时序动作定位任务，并验证了现有模型可在未见任务或活动中检测挣扎，跨任务mAP达34.56%，表明挣扎具有可迁移性但仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 现有操作技能数据集未关注学习过程中挣扎的演变，而识别学习者的挣扎状态对优化教学和开发辅助系统至关重要。因此，需要一个能捕捉技能发展过程中动态挣扎变化的数据集。

Method: 收集了一个包含多种任务（打结、折纸、七巧板、洗牌）的多轮重复视频数据集EvoStruggle，共76名参与者完成18项任务各五次；将挣扎判定定义为时序动作定位问题，使用Temporal Action Localization模型进行实验，评估其在跨任务和跨活动场景下的泛化能力。

Result: 时序动作定位模型能够有效学习挣扎线索，在跨任务和跨活动设置下分别取得34.56%和19.24%的平均mAP，证明挣扎检测具有一定的可迁移性，但仍有提升空间。

Conclusion: 挣扎是可在不同技能任务间迁移的概念，EvoStruggle数据集为研究学习过程中的动态挣扎提供了重要资源，推动个性化辅助学习系统的发展。

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: 本文提出了Small PDE U-Net Solver (SPUS)，一种基于轻量级残差U-Net架构的高效基础模型，用于统一求解多种偏微分方程（PDEs）。通过自回归预训练策略，SPUS在流体动力学PDE数据上预训练，并在6个未见过的下游任务中表现出优异的泛化能力，参数更少且所需微调数据极少，实现了高效求解。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型多基于复杂Transformer架构，计算和参数开销大，缺乏高效轻量的替代方案。因此，探索更紧凑、高效的模型架构具有重要意义。

Method: 采用轻量级残差U-Net作为主干网络，设计一种简单而有效的自回归预训练策略，模拟数值求解器行为以学习物理规律，并在多样化的流体动力学PDE数据集上进行预训练。

Result: SPUS在6个未见的下游PDE任务上达到最先进水平的泛化性能，同时参数量显著减少，且仅需极少量微调数据即可适应新任务。

Conclusion: SPUS证明了U-Net类架构在PDE求解基础模型中的巨大潜力，提供了一种高参数效率、低资源需求的通用PDE求解方案，适用于广泛物理系统的建模与仿真。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 本文提出了DisCo，一种基于强化学习的框架，通过多样性约束优化多人体图像生成中的身份多样性，解决了现有文本到图像模型在多人生成中的人脸重复、身份混淆和计数错误问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在生成多个人物时存在身份重复、人脸相似度过高和人数错误等问题，缺乏对身份多样性的有效控制。

Method: 提出DisCo框架，采用Group-Relative Policy Optimization（GRPO）微调流匹配模型，设计复合奖励函数：惩罚图像内面部相似性、减少跨样本身份重复、确保人物数量准确，并结合人类偏好评分保持视觉质量；使用单阶段课程学习稳定训练过程，无需额外标注。

Result: 在DiverseHumans测试集上，DisCo实现了98.6%的独特人脸准确率和接近完美的全局身份分布，显著优于开源和闭源方法（如Gemini、GPT-Image），同时保持了良好的感知质量。

Conclusion: DisCo是一种可扩展且无需额外标注的解决方案，有效解决了生成模型中的长期存在的身份危机问题，为多人体组合生成设定了新基准。

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种新的地理表示方法，通过将查询图像的视觉表示与分层地理嵌入对齐，并融合外观特征和语义分割图，显著提升了全球视觉地理定位的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉地理定位方法在学习地理表示方面仍有改进空间，尤其是在全球范围内的精确定位需求日益增长的情况下。

Method: 将地理定位问题建模为视觉表示与分层地理嵌入的对齐问题，并引入一种有效融合查询图像外观特征与其语义分割图的方法，以构建鲁棒的视觉表示。

Result: 在五个基准数据集的25项指标中，有22项超过了先前的最先进方法和最新的大型视觉语言模型（LVLMs）。

Conclusion: 实验结果表明，所提出的地理与视觉表示的结合是性能提升的主要原因，验证了该方法的有效性和优越性。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 本文提出了XMAS，一种基于跨模态注意力矩阵相似性的新型数据选择方法，用于高效指令微调大型视觉-语言模型（LVLMs）。该方法通过聚类注意力矩阵的奇异值轨迹来识别并去除冗余数据，在保留完整性能的同时显著减少训练数据量。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法在LVLM上表现不佳，无法超越随机选择。为了实现更高效、更少冗余的LVLM训练，需要一种有理论依据的数据选择方法。

Method: 提出XMAS方法：首先微调一个小型代理LVLM，提取训练过程中各示例的跨模态注意力矩阵的前几个奇异值轨迹，并据此对示例进行聚类；然后从每个簇中均衡采样，构建去冗余的子集用于训练目标LVLM。

Result: XMAS可在LLaVA-665k上减少50%数据、在Vision-Flan上减少85%数据，同时在10个下游任务上完全保持LLaVA-1.5-7B的性能，并提速训练1.2倍。相比最优基线，LLaVA-665k上的数据缩减多出30%。

Conclusion: XMAS是首个针对LVLM指令微调的原理性数据选择方法，利用注意力矩阵的低秩动态特性有效识别信息冗余，在大幅压缩训练数据的同时保持模型性能，显著提升了LVLM训练的数据效率。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception是一种用于向量量化图像生成的变分流匹配方法，通过在连续嵌入空间中计算速度场的同时学习码本索引的分类后验，实现了离散监督与连续传输动力学的结合。


<details>
  <summary>Details</summary>
Motivation: 现有的图像生成方法在连续流匹配和离散流匹配之间存在训练效率和生成质量的权衡，缺乏有效融合两者优势的方法。

Method: 将变分流匹配应用于向量量化潜在空间，学习码本索引的分类后验分布，同时在连续嵌入空间中计算速度场，实现连续动力学与离散监督的结合，并支持不确定性估计和温度控制生成。

Result: 在ImageNet-1k 256x256图像生成任务上，Purrception比连续和离散流匹配基线收敛更快，同时取得了与当前最先进模型相当的FID分数。

Conclusion: 变分流匹配能够有效结合连续传输动力学和离散监督，在保持生成质量的同时显著提升训练效率，为向量量化图像生成提供了一种更高效的方法。

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 提出一种基于条件扩散模型和多任务学习的统一框架，用于从非对比CT生成合成对比增强CT并同时分割主动脉管腔和血栓，减少造影剂使用并提高分割精度。


<details>
  <summary>Details</summary>
Motivation: 减少碘造影剂的使用及其带来的肾毒性、过敏反应和环境危害，并克服传统多阶段方法中误差累积和语义结构利用不足的问题。

Method: 结合条件扩散模型与多任务学习，实现图像生成与解剖结构分割的端到端联合优化，共享编码器-解码器参数，并采用半监督训练策略以利用缺失标注的真实临床数据。

Result: 在264名患者数据上验证，图像生成PSNR达25.61 dB，管腔和血栓分割Dice分数分别提升至0.89和0.53，临床测量误差显著降低。

Conclusion: 所提方法在合成图像质量和分割准确性方面均优于现有单任务和多阶段模型，有助于减少临床造影剂依赖并提升AAA评估的可靠性。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 本文提出了一种用于多模态内容分析的框架，能够高效地构建视频分析管道，并将视频转换为可查询的、支持持续学习的帧级知识图谱表示。


<details>
  <summary>Details</summary>
Motivation: 多模态内容分析复杂、计算成本高，且将现有预训练模型应用于视频等复杂数据存在挑战。

Method: 结合多个预训练模型构建管道，将视频转化为时序半结构化数据，并进一步转换为帧级别的知识图谱表示。

Result: 实现了可查询且支持通过交互方式持续融入新领域知识的视频分析框架。

Conclusion: 该框架有效降低了多模态视频分析的工程难度，支持动态知识更新和高效原型开发。

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT 是一种新型网页代理框架，通过逆向工程提取网站内置功能并封装为可调用工具，实现更鲁棒、高效的浏览器自动化。


<details>
  <summary>Details</summary>
Motivation: 现有网页代理方法依赖于细粒度的UI操作和大量LLM推理，在动态布局和长周期任务中表现脆弱；而人类则利用网站提供的搜索、过滤、排序等高级功能进行高效操作。

Method: 提出 WALT 框架，自动发现并反向工程网站的潜在功能（如搜索、发布、创建等），将其抽象为可复用的工具，使代理直接调用高层操作而非模拟点击和输入。

Result: 在 VisualWebArena 和 WebArena 上，WALT 实现了更高的任务成功率，使用更少的操作步数，并显著减少了对大型语言模型推理的依赖。

Conclusion: WALT 通过将网站功能抽象为可靠工具，实现了更稳健、可泛化的浏览器自动化范式，有效克服了传统方法在复杂和动态环境中的局限性。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: 提出了一种半监督分割框架，通过多扰动预测和拓扑一致性机制，有效减少组织病理学图像中的拓扑错误，提升分割鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在组织病理学图像分析中，无标签数据的语义结构提取困难，尤其是对象密集分布时，现有方法难以保持关键拓扑特征。

Method: 利用随机dropout和时序训练快照生成多个扰动预测，通过结合空间重叠与全局结构对齐的新匹配策略，强制跨预测的拓扑一致性。

Result: 实验表明该方法显著减少了拓扑错误，提升了分割的准确性和鲁棒性，优于现有半监督方法。

Conclusion: 所提框架能有效保留生物有意义的结构，抑制噪声干扰，适用于高密度组织病理图像的半监督分割。

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 本文提出了Diffusion-LPO，一种用于扩散模型中列表级偏好优化的简单有效框架，基于Plackett-Luce模型扩展DPO目标，利用排序数据提升图像生成质量与人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于成对偏好的强化学习方法无法充分利用人类反馈中的排序信息，而实际中人类对图像的偏好常包含更精细的隐式排序，因此需要精确优化列表级偏好以更好对齐扩散模型与人类偏好。

Method: 提出Diffusion-LPO框架，将用户反馈聚合成图像排序列表，并在Plackett-Luce模型下推导DPO目标的列表级扩展，通过鼓励每个样本优于其所有排名较低的样本来实现整体排序一致性。

Result: 在文本到图像生成、图像编辑和个性化偏好对齐等多个任务上验证了Diffusion-LPO的有效性，相比成对DPO基线，在视觉质量和偏好对齐方面均表现更优。

Conclusion: Diffusion-LPO能够有效利用列表级人类偏好数据，在扩散模型中实现更精准的偏好优化，显著优于传统的成对方法，具有广泛的应用潜力。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为Bridge的纯自回归多模态大语言模型，通过Mixture-of-Transformers架构和语义到像素的离散表示，在统一框架下实现了图像理解和生成，取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在统一理解和生成任务时面临挑战：混合方法破坏了自回归范式，而纯自回归方法在语义对齐和像素保真度之间存在权衡。因此，需要一种既能保持自回归特性又能兼顾高质量生成和理解的模型。

Method: 提出Bridge模型，采用Mixture-of-Transformers架构，在预训练视觉理解模型基础上增强生成能力；设计语义到像素的离散表示，结合紧凑的语义标记和细粒度的像素标记，实现高质量图像生成与良好语言对齐。

Result: 实验证明，Bridge在多个多模态基准上理解和生成任务均达到领先或具有竞争力的结果，且训练数据更少、训练时间更短。序列长度仅增加7.9%的情况下显著提升了视觉生成保真度。

Conclusion: Bridge成功实现了纯自回归框架下的统一多模态理解与生成，在性能、效率和序列效率方面优于先前方法，为统一多模态模型提供了新思路。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 提出一种结合CNN与贝叶斯深度学习的混合模型，用于小样本下的口腔癌分类，通过变分推断实现不确定性量化，在数据稀缺场景下提升了模型的可靠性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在小样本数据下易过拟合且缺乏可靠性，难以满足医疗资源匮乏地区早期口腔癌诊断的需求。

Method: 结合卷积神经网络（CNN）与贝叶斯深度学习，采用变分推断进行不确定性量化，使用智能手机拍摄的彩色照片训练模型，并在三个不同测试集上评估性能。

Result: 在训练分布相似的测试集上达到94%准确率；在真实世界图像上，相比传统CNN的72.94%，该模型在多样数据集上实现88%准确率，并表现出对正确分类样本低不确定性、错误分类样本高不确定性的良好置信度特性。

Conclusion: 贝叶斯深度学习在小样本医疗场景中显著提升模型的泛化性和可靠性，适用于资源受限环境下的早期口腔癌筛查。

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CADTrans的源域无访问域适应方法，通过构建一致性辅助域和多核最大均值差异策略，有效提升目标域上的模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于无法获取源域数据，现有方法难以获得确定的不变特征，且易受难样本和域偏差影响。

Method: 设计了辅助域模块，利用中间聚合全局注意力获取多样化表示，并通过多种一致性策略提取不变特征；引入条件多核最大均值差异（CMK-MMD）对齐难样本与易样本。

Result: 在Office-31、Office-Home、VISDA-C和DomainNet-126等多个基准上实验表明，该方法显著优于现有方法。

Conclusion: CADTrans通过构建一致性特征表示和有效的样本对齐策略，在源域不可见的情况下实现了更鲁棒的域适应。

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 本文提出一种基于历史盲人用户提问数据的上下文感知系统，用于指导多模态大语言模型生成更符合盲人用户需求的图像描述。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在辅助盲人视觉解释时通常生成冗长且不相关的描述，导致信息获取效率低下，无法满足用户特定需求。

Method: 利用VizWiz-LF数据集中盲人用户的过往提问，识别相似视觉上下文，并以此引导MLLM生成更具上下文相关性的描述。

Result: 评估结果显示，在92个案例中，上下文感知描述在76.1%的情况下（70例）成功预测并回答了用户问题，在54.4%的对比中被偏好。

Conclusion: 该方法能有效提升MLLM生成描述的相关性和实用性，改善盲人用户的视觉辅助体验。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: ImageNet-Think是一个基于ImageNet21k的多模态推理数据集，包含25万张图像及由先进VLM生成的结构化思维标记与答案，旨在提升视觉语言模型的显式推理能力。


<details>
  <summary>Details</summary>
Motivation: 为了推动具备显式推理能力的视觉语言模型（VLM）的发展，并促进对多模态推理机制的深入理解。

Method: 基于ImageNet21k的25万张图像，利用GLM-4.1V-9B-Thinking和Kimi-VL-A3B-Thinking-2506两个先进的VLM生成每幅图像对应的两组思维-答案序列。

Result: 构建了一个包含结构化思维标记和最终描述性答案的多模态推理数据集，并将提供公开的数据集和评估基准。

Conclusion: ImageNet-Think有助于训练和评估具有显式推理能力的VLM，推动多模态推理研究的发展。

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 提出一种名为非线性零空间投影（NPN）的新正则化方法，通过神经网络将解约束在感知矩阵零空间的低维投影中，提升多种成像反问题的重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统先验通常忽略零空间的任务特定结构，导致重建性能受限，本文旨在利用零空间的结构信息来设计更有效的正则化方法。

Method: 提出NPN方法，使用神经网络学习感知矩阵零空间的低维投影，并将其作为正则项融入重建过程，兼容即插即用算法、展开网络等多种框架。

Result: 理论分析证明了收敛性和重建精度保证，在压缩感知、去模糊、超分辨率、CT和MRI等多个任务中实验表明NPN能持续提升重建质量。

Conclusion: NPN通过挖掘零空间结构提供了一种可解释且灵活的正则化策略，显著增强了各类成像反问题的求解能力。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出一种结合混沌游戏表示法（CGR）与概念瓶颈模型（CBM）的自动化基因组解释模块，通过生物学有意义的概念实现可解释的HIV亚型分类，并集成决策层以优化临床效用和成本效益。


<details>
  <summary>Details</summary>
Motivation: 传统基因组学模型缺乏可解释性和与临床决策系统的整合能力，难以满足医学自动化需求。

Method: 采用CGR将DNA序列转化为图像表示，结合CBM强制预测通过GC含量、CpG密度和k-mer等生物概念；引入概念保真监督、先验一致性对齐、KL分布匹配和不确定性校准提升可靠性；并设计成本感知推荐层生成决策策略。

Result: 在内部和LANL数据集上实现了最先进的HIV亚型分类性能，显著提升概念预测保真度和不确定性校准效果，同时降低不必要的重测成本，取得更优的成本效益权衡。

Conclusion: 该框架成功连接了可解释基因组建模与自动化决策，为基因组医学中的机器人和临床自动化提供了可靠基础。

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: 本文提出了VLA-R1，一种增强推理能力的视觉-语言-动作（VLA）模型，通过引入可验证奖励的强化学习（RLVR）和组相对策略优化（GRPO），提升了跨任务与跨场景的泛化能力和实际执行性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型缺乏显式的逐步推理机制，且训练过程中对推理质量的强化不足，难以满足复杂环境中的动作规划需求。

Method: 提出基于可验证奖励的强化学习（RLVR）与GRPO结合的后训练策略，并构建高质量链式思维数据集VLA-CoT-13K，包含对功能约束和轨迹的一致性标注。

Result: 在领域内、领域外、仿真和真实机器人平台上均表现出优于现有方法的泛化能力和执行准确性。

Conclusion: VLA-R1通过显式推理建模和高质量监督信号，在提升VLA模型推理鲁棒性和现实世界适应性方面取得了显著进展。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出一种针对微距摄影的联合去模糊与3D重建方法，通过多视角模糊图像联合优化清晰3D模型和像素级散焦模糊核，实现高质量图像去模糊和高保真3D重建。


<details>
  <summary>Details</summary>
Motivation: 微距摄影中散焦模糊问题严重影响物体清晰成像和高质量3D重建，传统去模糊方法依赖大量图像和标注，且缺乏适用于微距摄影的多视角3D重建方法。

Method: 基于多视角模糊图像，采用可微渲染方法联合优化物体的清晰3D模型和每个像素的散焦模糊核，实现自监督优化。

Result: 实验表明，仅需少量多视角图像，该方法即可实现高质量的图像去模糊和高保真3D外观重建。

Conclusion: 所提方法有效解决了微距摄影中的散焦模糊问题，在少输入图像条件下实现了优异的去模糊与3D重建性能，推动了微距图像处理与三维重建的发展。

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出FideDiff，一种新颖的单步扩散模型，用于高保真图像去模糊。通过将运动去模糊重构为类似扩散的过程，并结合Kernel ControlNet与自适应时间步预测，实现了快速且高质量的去模糊效果，在全参考指标上优于现有扩散方法并达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散模型的方法在图像去模糊任务中展现出强大的生成能力，但其推理时间长、保真度不足等问题限制了实际应用。因此，需要一种高效且高保真的去模糊方法来充分发挥扩散模型的潜力。

Method: 将运动去模糊重新定义为一个扩散-like过程，每个时间步代表逐渐模糊的图像；训练一致性模型使所有时间步对齐到同一清晰图像，并通过匹配模糊轨迹重建训练数据以学习时间一致性。引入Kernel ControlNet估计模糊核，并采用自适应时间步预测提升性能。

Result: FideDiff在全参考评价指标上优于先前的扩散模型方法，性能媲美其他最先进模型，同时实现快速的单步去模糊。

Conclusion: FideDiff为预训练扩散模型在高保真图像恢复任务中的应用提供了新方向，建立了适用于真实工业场景的强健基线。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于LadderMoE增强的两阶段检测-识别管道，用于解决青铜器铭文（BI）自动识别中的跨域差异和长尾分布难题，并构建了大规模数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 由于青铜器铭文存在严重的视觉退化、多模态差异（照片、拓片、描摹）以及极长尾的字符分布，现有方法难以实现准确识别，亟需更鲁棒的模型与数据支持。

Method: 构建了一个包含22454张全页图像和198598个标注字符的大规模数据集；采用两阶段检测-识别流程，结合预训练CLIP编码器与阶梯式MoE适配器（LadderMoE），实现对不同域和稀有类别的动态专家专业化建模。

Result: 在单字符和全页识别任务上显著优于现有的场景文字识别基线模型，在头部、中部和尾部类别及所有采集模态下均表现出更高的准确性。

Conclusion: 所提方法为青铜器铭文识别建立了新的基准，有效提升了跨域和长尾条件下的识别性能，为后续考古学分析提供了可靠的技术基础。

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: 提出VirDA方法，通过在骨干网络前添加域特定的视觉重编程层实现高效的无监督域适应，无需微调骨干参数，显著减少训练参数和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法需要为每个源-目标对微调骨干网络，导致参数和存储开销线性增长，且无法复用预训练骨干。受骨干网络存在纹理偏见的启发，探索利用域特定纹理偏见进行更高效的域适应。

Method: 提出VirDA，在固定骨干网络前添加域特定的视觉重编程层，生成作为纹理偏见的视觉提示来调整输入图像的风格；使用多个目标函数优化提示，以缩小域内和域间分布差异。

Result: 在Office-31上达到92.8%平均准确率，仅需1.5M可训练参数；相比PDA提升1.6%准确率且仅用其46%参数；相比全微调方法CDTrans和FixBi分别提升0.2%和1.4%，但仅需1.7%和2.8%参数；相比PMTrans和TVT仅牺牲2.2%和1.1%准确率，但参数量仅为约1.7%。

Conclusion: VirDA通过视觉重编程实现了高效、可复用的无监督域适应，在大幅降低参数量的同时保持了竞争力的性能，验证了利用域特定纹理偏见的有效性。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 本文提出了一种名为离散面部编码（DFE）的无监督、数据驱动方法，用于从3D网格序列中学习紧凑且可解释的面部表情字典，通过残差向量量化变分自编码器（RVQ-VAE）实现，并在心理计算任务中优于FACS和其他现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的面部表情编码系统（如FACS）受限于覆盖范围有限和人工标注成本高，难以满足大规模应用需求。

Method: 首先使用3D可变形模型（3DMM）从图像中提取与身份无关的表情特征，分离头部姿态和面部几何等因素；然后利用RVQ-VAE对这些特征进行编码，生成共享码本中的离散标记序列，每个标记代表一种可重用的面部变形模式。

Result: 实验证明DFE比FACS和其他面部编码方法能捕捉更精细的面部行为，在压力检测、人格预测和抑郁检测三个心理学任务中，基于DFE的词袋模型 consistently 优于FACS和Masked Autoencoders等强基线模型。

Conclusion: DFE是一种可扩展且有效的FACS替代方案，在心理和情感计算应用中具有广泛潜力。

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: 提出了一种名为Con-NRSfM的新方法，用于处理共形变形下的非刚性结构-from-运动问题，能够在无需严格假设的情况下实现精确的点云重建和局部共形尺度计算。


<details>
  <summary>Details</summary>
Motivation: 现有非刚性SLAM方法依赖于局部平面或线性变形等强假设，且无法恢复共形尺度，限制了重建精度和适用范围。

Method: 采用基于图框架优化的2D图像 warp 进行逐点重建，解耦深度与共形尺度约束，并引入并行可分迭代优化策略；结合自监督编码器-解码器网络生成带纹理的稠密3D点云。

Result: 在合成与真实数据集上的实验表明，该方法在重建精度和鲁棒性方面优于现有方法。

Conclusion: Con-NRSfM有效克服了传统NRSfM方法的局限性，能够准确估计共形尺度和深度，显著提升单目视觉下可变形物体的三维重建性能。

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniVerse的统一框架，用于从不一致的多视角图像中进行鲁棒的3D场景重建。该方法通过视频扩散模型将图像修复与3D重建解耦，先将不一致图像转换为初始视频并恢复为一致图像，再进行重建，具有良好的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不一致图像时依赖密集观测且难以优化，且通常针对特定退化建模，缺乏通用性。因此需要一种能应对多种图像不一致并减少对密集数据依赖的鲁棒重建方法。

Method: 提出UniVerse框架，将鲁棒重建分解为修复和重建两个子任务：首先将不一致图像转为初始视频，利用专门设计的视频扩散模型恢复出一致图像，然后基于恢复后的图像进行3D重建。扩散模型通过大规模数据学习通用场景先验，提升对多种退化的适应能力。

Result: 在合成与真实世界数据集上的实验表明，该方法在重建质量与鲁棒性方面优于现有方法，展现出强大的泛化能力，并支持控制重建3D场景的风格。

Conclusion: UniVerse通过解耦修复与重建过程，并利用视频扩散模型学习通用先验，有效解决了多视角图像不一致下的鲁棒3D重建问题，兼具高性能与强泛化能力。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: 提出一种轻量级端到端模板匹配框架PoseMatch-TDCM，将模板匹配重构为联合定位与几何回归任务，通过模板感知动态卷积模块（TDCM）实现高效、精确的中心点、旋转角度及独立缩放估计，支持无几何标注训练，在工业检测中表现出高精度、强鲁棒性和实时性。


<details>
  <summary>Details</summary>
Motivation: 传统模板匹配方法在复合变换下因穷举搜索角度和尺度导致效率低，而现有深度学习方法缺乏对几何位姿的显式建模，难以满足实际工业部署需求。

Method: 将模板匹配视为联合定位与几何回归任务，设计轻量级网络结构，引入模板感知动态卷积模块（TDCM）在推理时动态注入模板特征；采用深度可分离卷积和像素混洗提升效率；提出基于旋转-剪切的数据增强策略并生成结构感知伪标签，实现无需几何标注的训练；加入轻量级优化模块提升角度和尺度精度。

Result: 模型仅3.07M参数，推理时间14ms，在复合变换、小模板和多目标场景下均表现出高精度和强鲁棒性。

Conclusion: 该方法在保持高效推理的同时实现了精确的几何状态估计，适用于实时工业应用，具备良好的实用性和部署潜力。

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种自适应像素推理框架，通过操作感知的监督微调和回溯引导的强化学习，使视觉-语言模型能根据查询难度动态决定是否调用像素级操作，在提升性能的同时显著减少不必要的计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在处理需要精细视觉理解的任务时表现不佳，主要由于图像编码过程中的信息丢失或对关键区域关注不足；同时引入像素级信息常导致过度使用和分心。

Method: 首先进行操作感知的监督微调以建立文本推理和视觉操作的基础能力，然后设计一种基于模型自身反馈的回溯引导强化学习框架，使其能动态判断何时需调用像素级操作。

Result: 在多个多模态推理基准上实验表明，该模型在HR-Bench 4K上达到73.4%的准确率，工具使用率仅为20.1%，相比先前方法准确率提升且工具使用减少了66.5%。

Conclusion: 所提出的自适应像素推理框架能有效平衡性能与效率，实现了对细粒度视觉信息的按需访问，提升了模型在复杂视觉任务中的推理能力。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 提出了一种基于增强敏感性的风险评分框架（ASRS），通过测量临床合理的旋转变化下的嵌入偏移来识别易出错的胸部X光片（CXR）病例，提升医学AI的公平性和安全性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在胸片解读中表现良好，但存在跨患者亚群性能不均的问题，传统误差检测方法难以发现分布内细微错误，缺乏有效的无标签误差识别手段。

Method: 提出ASRS框架，采用±15°/±30°的临床合理旋转增强，利用RAD-DINO编码器测量嵌入空间的变化，通过敏感性得分将样本分层，识别高风险病例。

Result: 高敏感性样本的召回率显著降低（-0.2至-0.3），尽管整体AUROC和置信度较高；ASRS能有效区分易错样本，支持选择性预测和医生复核。

Conclusion: ASRS提供了一种无需标签的误差检测方法，在不依赖标注的情况下提升了模型的可信赖性、公平性和临床安全性，适用于医学影像中的可靠性评估。

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的视频风格化框架FreeViS，通过融合多个风格参考图像到预训练的图像到视频模型中，在保持时间一致性的同时生成富含细节的风格化视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法在逐帧进行图像风格化时存在时间不一致性和风格贫乏问题，而专用视频风格化模型通常需要配对视频数据且计算成本高。因此，需要一种高效、无需训练且能保持高质量风格和时间连贯性的方法。

Method: FreeViS将多个风格参考图像整合进预训练的图像到视频（I2V）模型，利用高频补偿约束内容布局和运动，并结合基于光流的运动线索保留低显著性区域的风格纹理，从而减少传播误差并避免闪烁和卡顿。

Result: 实验表明，FreeViS在风格保真度和时间一致性方面优于近期基线方法，获得更高的用户偏好评分，且无需额外训练，具备良好的实用性和经济性。

Conclusion: FreeViS提供了一种高质量、时间连贯、无需训练的视频风格化解决方案，适用于实际内容创作场景。

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: 本文提出了MedQ-Bench，一个基于语言的多模态大模型（MLLM）医学图像质量评估基准，引入感知与推理双任务范式，涵盖五种成像模态和多种图像源，通过多维评判协议评估模型性能，并验证其与放射科医生判断的一致性，发现现有MLLM在医学图像质量评估中能力初步但不稳定，亟需针对性优化。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像质量评估方法依赖标量评分指标，无法反映专家评估中描述性、类人推理的过程，缺乏能够模拟人类感知与推理的评估框架。

Method: 构建MedQ-Bench基准，包含MedQ-Perception（低层次感知问题）和MedQ-Reasoning（无参考与比较推理任务）两个任务；覆盖五种成像模态和四十多个质量属性；提出四维度评判协议，并进行LLM与放射科医生的判断对齐验证。

Result: 在14种最先进的MLLM上评测显示，模型具备初步但不稳定的感知与推理能力，准确率不足以支持可靠临床应用；人类与AI判断对比表明当前模型仍有显著差距。

Conclusion: MedQ-Bench为医学图像质量评估提供了新的感知-推理范式，揭示了当前多模态大模型在此领域的能力局限，强调需针对医学IQAs场景优化MLLM，推动其在临床前质量控制中的发展。

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: 提出InstaFormer，一种仅通过单次前向传播即可从RGB图像预测场景中所有实例的遮挡和深度顺序的网络。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的输入格式（如类别标签、二值分割掩码）和高推理成本（二次方数量的前向传播），难以实现实例级几何理解。

Method: 设计InstaFormer，利用对象查询与潜在掩码描述符之间的交互，语义上表示相同对象并提供互补信息，实现整体顺序预测。

Result: 在多个基准上进行了全面评估和消融实验，验证了该方法的有效性，能够在单次前向传播中准确预测实例的遮挡和深度顺序。

Conclusion: InstaFormer有效降低了实例几何理解的输入和计算成本，实现了高效、完整的实例顺序预测，代码和模型已开源。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: 本文提出了一种基于金字塔位置编码和强化学习的新型Transformer框架PyramidStyler，用于高效、高质量的神经风格迁移，在内容和风格损失上显著降低，实现快速推理。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和Transformer模型在处理复杂风格和高分辨率图像时扩展性差，计算开销大，难以实现实时高质量风格迁移。

Method: 提出PyramidStyler框架，采用金字塔位置编码（PPE）实现多尺度特征建模，并结合强化学习动态优化风格化过程，提升收敛速度和生成质量。

Result: 在COCO和WikiArt数据集上训练后，4000轮迭代后内容损失降至2.07（降低62.6%），风格损失降至0.86（降低57.4%），推理时间为1.39秒；引入强化学习后进一步优化至内容2.03、风格0.75，推理时间仅1.40秒。

Conclusion: PyramidStyler实现了高效、高质量的实时艺术风格迁移，具有在媒体与设计领域广泛应用的潜力。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: 本文提出了一种名为LoBE-GS的新框架，用于实现负载均衡且高效的3D高斯点阵化，显著提升了大规模场景下的训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯点阵化方法在处理大规模、开放场景时存在内存压力和计算负载不平衡的问题，难以高效扩展。

Method: 提出了深度感知的分割方法、基于优化的可见高斯分布均衡策略，以及可视裁剪和选择性稠密化两种轻量技术。

Result: 在大规模城市和户外数据集上的实验表明，LoBE-GS相比现有最先进方法可将端到端训练时间加快最多2倍，同时保持重建质量，并支持原始3DGS无法处理的大规模场景。

Conclusion: LoBE-GS通过重新设计大规模3DGS流程，有效解决了负载不平衡和效率瓶颈问题，显著提升了可扩展性和训练效率。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 本文提出了MemoryPack和Direct Forcing两种方法，以解决长视频生成中的长程依赖和误差累积问题，显著提升了自回归视频模型的上下文一致性和实用性。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临捕捉长程依赖和防止自回归解码中误差累积的双重挑战。

Method: 提出MemoryPack，一种可学习的上下文检索机制，结合文本和图像信息进行动态上下文建模；引入Direct Forcing，一种高效的单步近似策略，改善训练与推理的一致性。

Result: MemoryPack实现了分钟级时间一致性，具有良好的长度扩展性和线性复杂度；Direct Forcing有效抑制了推理过程中的误差传播。

Conclusion: MemoryPack与Direct Forcing相结合，显著增强了长视频生成的上下文一致性和可靠性，推动了自回归视频模型的实际应用。

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: 本文研究了3D目标检测器分类任务中的置信度校准问题，提出两种正则化损失项以改善训练过程中的校准效果，并结合等渗回归方法在CenterPoint和PillarNet上取得了最佳校准性能，但发现DSVT-Pillar无法同时校准主导类和次级类预测。


<details>
  <summary>Details</summary>
Motivation: 精确的对象检测与不确定性估计对自主系统的安全运行至关重要，现有3D目标检测器的置信度校准不足，尤其在多类别预测分布的整体校准方面缺乏有效评估和优化方法。

Method: 提出两个辅助正则化损失项，分别用于校准主导预测和完整预测向量；结合后处理方法（如等渗回归）进行实验，评估多种校准方法在CenterPoint、PillarNet和DSVT-Pillar上的表现。

Result: 在CenterPoint和PillarNet上，结合全预测向量校准损失与等渗回归可显著提升主导类和次级类预测的校准效果；但在DSVT-Pillar上该方法无法同时优化两类预测的校准。

Conclusion: 全面校准3D目标检测器的多类置信度分布是可行且必要的，所提出的训练时损失项与后处理结合策略有效提升了模型可靠性，但不同架构对联合校准的适应性存在差异。

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: 本文提出了DiffPS，一个利用预训练扩散模型进行行人搜索的新框架，通过三个专门模块解决了检测与重识别任务间的优化冲突，并在CUHK-SYSU和PRW数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多使用ImageNet预训练主干网络，且检测与重识别共享特征，导致对复杂空间上下文和细粒度身份特征的提取不充分，并存在优化目标冲突。

Method: 提出DiffPS框架，包含三个模块：(i) 扩散引导区域建议网络（DGRPN）提升定位；(ii) 多尺度频率优化网络（MSFRN）减轻形状偏差；(iii) 语义自适应特征聚合网络（SFAN）利用文本对齐的扩散特征，避免子任务间的优化冲突。

Result: DiffPS在CUHK-SYSU和PRW两个主流行人搜索数据集上取得了新的最先进性能。

Conclusion: DiffPS有效利用扩散模型的先验知识，解决了检测与重识别任务间的特征冲突，显著提升了行人搜索的性能。

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出了一种名为Flow-Matching-guided Unfolding network (FMU)的新方法，首次将流匹配引入高光谱成像重建，结合生成先验与深度展开框架，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像因硬件限制和从压缩测量中重建三维数据的困难而成本高昂，现有压缩感知系统在保持精细光谱细节方面仍面临挑战。

Method: 将流匹配的生成先验嵌入深度展开网络，并引入均速度损失以增强流的全局一致性，实现更鲁棒准确的重建。

Result: 在模拟和真实数据集上的实验表明，FMU在重建质量上显著优于现有方法。

Conclusion: FMU通过融合优化方法的可解释性与流匹配的生成能力，为高光谱图像重建提供了一种高效且高性能的解决方案。

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习和ConSinGAN数据增强的自动化双列直插式封装（DIP）缺陷检测系统，有效解决了缺陷样本不足的问题，并在准确率和检测速度上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统工业元件的缺陷检测耗时且费力，给质检人员带来巨大负担，难以保证产品质量，因此需要一种自动化的高效检测方法。

Method: 采用数字相机光学系统采集图像，利用ConSinGAN生成缺陷样本以解决数据不足问题，并对比YOLOv3、v4、v7、v9四种模型的检测性能，结合SCADA系统实现自动化检测。

Result: YOLOv7结合ConSinGAN的方法在准确率达到95.50%，检测时间为285ms，显著优于基于阈值的方法和其他YOLO版本。

Conclusion: 该自动化缺陷检测系统能够有效应对多种缺陷类型和缺陷数据不足的场景，具有良好的工业应用前景。

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: 本文提出了一种名为FoundAD的少样本异常检测方法，利用大规模预训练视觉编码器学习到的正常图像分布特性，通过非线性投影算子将特征映射到自然图像流形上，从而有效识别图像中的异常区域。


<details>
  <summary>Details</summary>
Motivation: 由于少样本条件下准确区分正常与异常特征具有挑战性，尤其是在类别无关的情况下，因此需要一种能够利用有限样本进行高效异常检测的方法。

Method: 通过学习一个非线性投影算子，将预训练编码器提取的特征投影到自然图像流形上，利用嵌入差异与图像中异常量的相关性进行异常检测。

Result: 该方法支持多类异常检测，在多个基础编码器（包括DINOv3）上的实验表明其性能优越且参数量更少。

Conclusion: FoundAD为基于基础模型特征的少样本异常检测提供了新视角，推动了该领域的发展。

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 提出ClustViT，通过可训练的聚类模块合并相似token并用再生模块恢复细节，显著降低计算量并保持语义分割性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在机器人系统中应用受限于其二次注意力复杂度，且现有token合并方法不适合密集预测任务。

Method: 在ViT基础上引入可训练的Cluster模块，基于分割掩码的伪聚类指导合并相似token，并设计Regenerator模块恢复细节以支持下游密集预测任务。

Result: 在三个数据集上实现最多2.18倍GFLOPs减少和1.64倍推理加速，同时保持相当的分割精度。

Conclusion: ClustViT有效平衡了计算效率与语义分割性能，提升了ViT在真实世界机器人系统中的适用性。

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Patch-as-Decodable Token (PaDT)的统一范式，使多模态大语言模型（MLLM）能够直接生成文本和多种视觉输出，通过引入视觉参考令牌（VRTs）和轻量解码器，在检测、分割和定位任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视觉任务中依赖间接表示（如用文本生成坐标），限制了性能，难以支持密集预测任务（如分割）。因此需要一种能直接生成多样化视觉输出的统一框架。

Method: 提出PaDT框架，利用来自图像块嵌入的视觉参考令牌（VRTs），将其与LLM的文本输出令牌无缝交织，并通过轻量级解码器将LLM输出转化为检测、分割和接地预测；每个前向传播独立处理VRTs并动态扩展嵌入表，采用随机选择VRT进行监督微调和鲁棒的逐令牌交叉熵损失的训练策略。

Result: 在四个视觉感知与理解任务上的实验表明，PaDT在性能上持续达到最先进水平，甚至优于规模更大的MLLM模型。

Conclusion: PaDT为MLLM提供了一种统一且高效的直接生成多样化视觉输出的方法，显著提升了在密集视觉预测任务中的表现。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: 本文提出了一种可解释AI框架TriAlignXA，通过构建“信任金字塔”模型和“三角信任指数”（TTI），解决生鲜电商中因缺乏感官体验导致的信任问题，平衡农产品分级中的生物特性、时效性与经济性的“不可能三角”，并借助预映射机制提升质量信息透明度，实验证明该框架在准确性与信任构建方面显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 在线果蔬电商存在“信任赤字”，因为数字交易无法提供对产品质量的直接感官感知，传统绝对分级标准难以应对农产品的生物多样性和供应链约束，亟需一种能够兼顾质量、时效与成本的新型信任构建机制。

Method: 提出“信任金字塔”模型和“三角信任指数”（TTI），设计可解释AI框架TriAlignXA，包含三个核心引擎：生物自适应引擎、时效优化引擎和经济优化引擎，并引入预映射机制将过程数据编码为QR码以增强透明度，通过实验验证其在分级任务中的性能。

Result: 实验表明，TriAlignXA在农产品分级任务中显著优于基线模型，具备良好的多目标优化能力，能有效平衡“不可能三角”中的各项冲突，预映射机制提升了消费者对质量信息的信任度。

Conclusion: TriAlignXA框架通过算法提供透明的决策依据而非直接决策，成功搭建了从算法输出到消费者信任的桥梁，为构建可信的在线生鲜交易生态系统提供了理论与实践路径。

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 提出4DGS-Craft，一个一致且可交互的4D高斯泼溅编辑框架，通过4D感知的InstructPix2Pix模型和LLM驱动的用户意图理解模块，实现视图、时间及非编辑区域的一致性，并支持复杂文本指令的解析与执行。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯泼溅编辑方法在视图、时间和非编辑区域一致性方面存在不足，且难以处理复杂文本指令，因此需要一种更一致、可控且支持用户交互的编辑框架。

Method: 引入4D感知的InstructPix2Pix模型，融合4D VGGT几何特征和多视图网格模块以保证视图与时间一致性；设计基于高斯选择机制的方法保护非编辑区域；结合LLM与指令模板解析用户意图，将复杂指令分解为原子操作序列。

Result: 相比现有方法，4DGS-Craft在视图、时间及非编辑区域一致性上表现更优，能有效处理复杂文本指令，实现更精确和可控的4D场景编辑。

Conclusion: 4DGS-Craft通过引入4D感知生成模型与LLM驱动的意图理解机制，显著提升了4D高斯泼溅编辑的一致性、交互性与可控性，为未来动态场景编辑提供了可行框架。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出了一种名为Pure-Pass（PP）的像素级掩码机制，用于图像超分辨率任务，通过识别纯像素并免除其复杂计算，在保持高性能的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级图像超分方法在适应性、掩码粒度和空间灵活性方面存在不足，尤其是CAMixer存在粗粒度掩码和空间不灵活等问题。

Method: 设计了基于固定颜色中心点的像素分类策略，实现细粒度、空间灵活的像素级掩码机制，并将其集成到ATD-light模型中形成PP-ATD-light。

Result: PP-ATD-light在重建质量和参数效率上优于CAMixer-ATD-light，同时节省相似的计算量。

Conclusion: Pure-Pass机制有效提升了图像超分模型的效率与性能平衡，具有良好的实用部署潜力。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 本研究提出了一种基于GPT-4o的自校正循环框架（SLSO），用于自动生成牙科全景片中颌骨囊肿的影像学发现，相较于传统思维链方法在多个评估项目上提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提升多模态大模型在医学影像报告生成中的准确性和可靠性，减少幻觉现象，并提高对牙齿编号等关键信息的识别精度。

Method: 构建了一个包含10个步骤的自校正循环与结构化输出（SLSO）框架，结合GPT-4o的多模态能力，通过迭代再生和一致性检查优化输出结果，并与传统的思维链（CoT）方法进行对比实验。

Result: SLSO框架在牙齿编号、牙齿移位和牙根吸收三项上的准确率分别提升了66.9%、33.3%和28.6%，成功案例中最多经过五次迭代即可获得一致的结构化输出，有效抑制了幻觉并增强了阴性发现的描述。

Conclusion: SLSO框架能有效提升颌骨囊肿影像发现生成的准确性与结构一致性，尤其在牙齿编号识别方面表现突出，但对跨多颗牙齿的广泛病变仍存在识别局限，需进一步优化以实现临床实用化。

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: 提出了一种名为LiLa-Net的3D自编码器架构，仅使用LiDAR点云数据高效编码真实交通环境特征，并通过简化结构和跳连连接实现高性能重建与良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了在不依赖复杂资源的情况下，从真实交通环境中有效提取LiDAR点云特征并实现高质量重建。

Method: 设计了一个轻量化的3D自编码器LiLa-Net，减少编码器层数并优化跳连连接结构，平衡潜在编码与跳连信息。

Result: 模型在保持高效的同时实现了精确的点云重建，并展现出对非交通环境物体的良好泛化能力。

Conclusion: LiLa-Net在资源受限条件下仍能高效编码并准确重建点云，具有良好的实用性和泛化性能。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: kabr-tools 是一个开源的自动化多物种行为监测工具包，结合无人机视频和机器学习技术，显著提升了野生动物行为观测的精度与规模。


<details>
  <summary>Details</summary>
Motivation: 传统野外观察方法在范围、时间和人力上受限，难以全面评估跨景观的行为响应，因此需要一种可扩展的方法来量化和解释复杂的行为模式。

Method: 开发了 kabr-tools 工具包，整合无人机视频与机器学习系统，利用目标检测、追踪和行为分类 pipeline 提取时间预算、行为转换、社会互动等关键行为指标，并通过三个案例研究进行验证。

Result: 相比地面方法，无人机观测将可见性损失减少15%，捕捉到更多且更准确连续的行为转换；分析969个行为序列发现，Grevy斑马的警觉性随群体大小增加而降低，但栖息地影响不显著；两种斑马均表现出强烈的行为惯性，且在混合群体中存在空间隔离现象。

Conclusion: kabr-tools 实现了大规模自动化行为监测，为生态系统范围的研究、保护工作及生物多样性研究提供了强大工具。

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: 提出GaussianMorphing，一种基于多视图图像的语义感知3D形状与纹理变形新框架，利用网格引导的3D高斯点阵实现高保真建模，在无需标注数据的情况下显著优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖点云或需预定义同胚映射，难以处理无纹理数据且缺乏语义一致性，因此需要一种能同时保证几何一致性和纹理保真的自动化形变方法。

Method: 采用网格引导的3D高斯点阵（3DGS）进行几何与外观建模，将3D高斯锚定到网格片上，通过拓扑感知约束实现统一变形，并利用网格拓扑作为几何先验建立无监督语义对应关系。

Result: 在TexMorph基准上显著优于先前2D/3D方法，颜色一致性误差（ΔE）降低22.2%，EI降低26.2%，有效保持局部细节与全局语义连贯性。

Conclusion: GaussianMorphing通过结合网格引导的3D高斯点阵与拓扑感知变形策略，实现了高质量、语义一致的3D形态插值，无需标签数据，为无监督形状变形提供了新思路。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 本文提出了一种名为InPose的新方法，通过仅使用旋转测量并结合预训练扩散模型与位置测量的似然项，实现对人体姿态估计的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于条件扩散模型的姿态估计方法在跨用户场景下泛化能力差，主要因为位置测量受用户体型影响较大。

Method: 将姿态估计建模为逆问题，利用预训练扩散模型仅以旋转测量为条件，并通过来自位置测量的似然项引导模型先验。

Result: InPose能够在无需针对特定用户训练的情况下，从稀疏的体上传感器数据中生成最可能的姿态序列，实现了良好的零样本跨用户泛化性能。

Conclusion: 该方法有效解决了跨用户姿态估计中的泛化问题，展示了在仅有少量传感器输入下的高精度姿态预测潜力。

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: 提出了一种基于视觉Transformer引导的扩散模型（VGDM），用于脑肿瘤检测与分割，结合全局上下文建模和迭代去噪，显著提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net在捕捉长距离依赖关系上能力有限，难以准确分割复杂肿瘤结构，需更强大的模型提升性能。

Method: 将视觉Transformer嵌入扩散模型核心，利用其全局建模能力和扩散过程的逐步优化机制，实现MRI体积数据中脑肿瘤的精准检测与分割。

Result: 在脑肿瘤MRI数据集上验证，VGDM在Dice相似系数和Hausdorff距离指标上均优于传统方法，表现出更优的分割精度和边界细节恢复能力。

Conclusion: VGDM通过融合Transformer与扩散模型，为脑肿瘤分割提供了更鲁棒、可扩展的解决方案，推动了该领域技术的发展。

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: 本研究提出了一种双阶段U-Net深度学习方法，从1925-1950年的Scan Histo历史地图中提取法国城市用地，生成了首个全国性开放城市足迹数据集。


<details>
  <summary>Details</summary>
Motivation: 在1970年代之前，由于缺乏全国性的数字化城市足迹数据，法国历史城市扩张的定量分析受到限制。

Method: 采用双阶段U-Net模型：第一阶段识别混淆区域（如文字和道路）以指导数据增强；第二阶段利用优化数据集和第一阶段的二值化输出减少辐射噪声，提升精度。

Result: 在覆盖整个法国本土的941个高分辨率图块上部署该方法，最终拼接图的整体准确率达到73%，有效捕捉了多种城市模式，并减少了标签和等高线等常见伪影。

Conclusion: 该研究成功构建了首个1925-1950年法国全国城市足迹开放数据集，所发布代码与数据为长期城市化动态研究提供了重要支持。

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 本文系统分析了在腹腔镜胆囊切除术视频中基于点的跟踪在手术环境中的失效模式，比较了其与分割掩码初始化的性能差异，并提出了改进跟踪性能的实用建议。


<details>
  <summary>Details</summary>
Motivation: 由于在复杂手术环境中，基于点的跟踪的可靠性和失效情况尚不明确，因此需要系统性地分析其在手术视频中的表现。

Method: 研究聚焦于胆囊、抓钳和L型电钩三个手术目标，使用SAM2等VOS模型，在零样本设置下比较基于点的跟踪与分割掩码初始化的性能，并进行定性分析。

Result: 基于点的跟踪在手术工具上表现具有竞争力，但在解剖结构（如胆囊）上因组织相似性和边界模糊而表现较差；研究识别出影响跟踪效果的关键因素。

Conclusion: 基于点的跟踪适用于手术工具，但对解剖目标存在局限性；文章提出了关于选择和放置跟踪点的若干实用建议以提升手术视频分析的性能。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: 本文提出了一个名为FFREEDG的新任务，旨在联邦学习环境下利用无标签客户端数据进行语义分割，同时不重新访问服务器端的源数据。为此，作者设计了FRIEREN框架，结合视觉-语言解码器和CLIP文本嵌入来提升语义分辨能力，并采用弱到强一致性学习策略优化伪标签训练。实验表明该方法在跨域语义分割任务中表现优异，为未来研究提供了强基线。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习语义分割方法大多假设客户端有标签数据，或未能充分利用现代视觉基础模型的能力，且难以应对无标签数据下的域偏移问题。因此，需要一种更贴近实际应用场景、能有效利用无标签数据并融合先进VFMs的方法。

Method: 提出FRIEREN框架：1）利用预训练视觉-语言模型（如CLIP）的跨模态知识；2）设计基于文本嵌入引导的视觉-语言解码器以增强语义理解；3）采用弱增强与强增强之间的一致性学习策略，提升基于伪标签的本地训练鲁棒性。

Result: 在合成到真实场景以及正常天气到恶劣天气的跨域基准上，FRIEREN显著优于现有域泛化与适应方法，验证了其在无标签联邦语义分割设置下的有效性与竞争力。

Conclusion: FRIEREN成功解决了FFREEDG这一新挑战性任务，展示了在无需访问源数据且客户端无标签情况下的高效跨域语义分割能力，推动了联邦学习与视觉基础模型在现实场景中的融合应用。

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: 提出ASK-Hint，一种基于动作中心知识的结构化提示框架，用于提升冻结视觉语言模型在视频异常检测中的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有提示方法过于抽象，忽略了定义复杂异常的细粒度人-物交互或动作语义。

Method: 将提示组织为语义连贯的组，并设计细粒度引导问题，使模型预测与判别性视觉线索对齐。

Result: 在UCF-Crime和XD-Violence上显著提升AUC，达到SOTA性能，且具有良好跨数据集和VLM主干的泛化能力。

Conclusion: 提示粒度至关重要，ASK-Hint是一种新的无需训练、可泛化且可解释的视频异常检测方案。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 提出GeoPurify方法，通过几何先验净化2D VLM生成的3D点特征，在极低数据量下实现高效3D语义分割。


<details>
  <summary>Details</summary>
Motivation: 现有2D到3D特征迁移方法在噪声与训练成本之间存在权衡，难以兼顾语义与几何一致性。

Method: 设计Student Affinity Network利用3D自监督教师模型提取的几何先验来净化特征，并引入Geometry-Guided Pooling模块在推理时去噪。

Result: 在主要3D基准上达到或超越SOTA性能，仅使用约1.5%的训练数据。

Conclusion: GeoPurify有效缓解了2D-to-3D迁移中的性能与效率矛盾，实现了高数据效率的3D语义分割。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 提出一种基于耳部静脉模式的非侵入式猪只生物识别方法，使用智能手机拍摄图像，通过计算机视觉和机器学习（SVM）实现98.12%的识别准确率，具有低成本、实时性和适用于混合品种的优势。


<details>
  <summary>Details</summary>
Motivation: 传统猪只识别方法（如耳标和芯片）存在不可靠、成本高、不适用于小规模农户和混合品种的问题，亟需一种更实用、低成本且非侵入的识别方案。

Method: 采集20头杂交猪的800张耳部图像，利用智能手机和简单背光拍摄；设计多阶段计算机视觉流程增强静脉可见性，提取结构与空间特征，并使用机器学习模型（如SVM）进行分类。

Result: SVM模型在杂交猪群中达到98.12%的识别精度，平均处理时间为8.3秒，具备实时农场部署可行性。

Conclusion: 该基于耳部静脉的生物识别系统为畜牧业提供了一种低成本、无应激、可靠的个体识别方案，有望推动资源有限地区的精准养殖数字化发展。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: 提出了一种基于Twins金字塔视觉Transformer的多类别密度图估计框架，结合多尺度解码和类别聚焦模块，在密集场景下显著优于现有方法，并成功应用于生物多样性监测。


<details>
  <summary>Details</summary>
Motivation: 在密集且遮挡严重的场景中，传统的检测计数方法失效，因此需要更鲁棒的多类别密度图估计方法来准确计数不同类别的对象。

Method: 采用Twins金字塔视觉Transformer作为骨干网络，设计专用的多类别计数头，并引入基于分割的类别聚焦模块以抑制训练时的类别间干扰，使用区域损失进行多类别密度图学习。

Result: 在VisDrone和iSAID数据集上显著优于先前方法，MAE分别降低了33%、43%和64%，且在与YOLOv11的对比中验证了其在密集场景下的优势；在生物多样性监测数据上的应用展示了其跨领域潜力。

Conclusion: 所提方法在多类别密集计数任务中表现优异，具备良好的泛化能力，可拓展至生态保护等新领域，为大规模生态分析提供技术支持。

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: 提出TempoControl方法，实现无需重训练的文本到视频生成中的时序控制。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型缺乏对视觉元素出现时间的细粒度控制。

Method: 利用交叉注意力图，通过相关性、能量和熵三个原则优化注意力以控制时序。

Result: 实现了精确的时间控制，支持多种应用如多对象时序重排、动作和音频对齐生成，同时保持高质量和多样性。

Conclusion: TempoControl有效提升了文本到视频生成的时序可控性，且不牺牲生成质量。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: 本文提出RewardMap，一种多阶段强化学习框架，用于提升多模态大语言模型在细粒度视觉推理任务中的表现，特别是在交通图等复杂场景下的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在细粒度视觉推理（如交通图的空间理解）上表现不佳，且标准强化学习因奖励稀疏和优化不稳定而难以训练。

Method: 构建了带有密集奖励信号的扩展数据集ReasonMap-Plus，并提出RewardMap框架，包含难度感知奖励设计和多阶段强化学习策略，逐步从感知过渡到复杂推理。

Result: 在ReasonMap和ReasonMap-Plus上验证了各组件的有效性，组合使用效果最佳；在6个基准上平均提升3.47%，显示出对视觉理解和推理能力的普遍增强。

Conclusion: RewardMap通过密集奖励和渐进式训练策略，显著提升了MLLM在细粒度和空间视觉推理任务上的性能，具有广泛的应用潜力。

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: 本文提出了DragFlow，首个有效利用FLUX强大生成先验进行基于拖拽的图像编辑的框架，通过引入区域化编辑范式、预训练适配器和多模态大模型，显著提升了编辑质量与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于点拖拽的编辑方法在DiT等新型强先验模型上表现不佳，因DiT特征缺乏足够结构化支持点级监督，且传统方法难以保持主体一致性和背景保真度。

Method: 提出区域化编辑范式，使用仿射变换提供更丰富的特征监督；集成IP-Adapter增强主体一致性；采用梯度掩码实现背景保护；结合多模态大语言模型解决任务歧义。

Result: 在DragBench-DR和新构建的ReD Bench上显著优于点基和区域基线方法，实现了拖拽式图像编辑的新SOTA。

Conclusion: DragFlow成功将DiT与流匹配模型的强大先验应用于拖拽编辑，验证了区域化监督优于点级监督，为未来编辑框架提供了新方向。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的视频理解方法F2C，通过选择关键片段（key clips）而非孤立帧，并结合自适应分辨率策略，在固定计算预算下提升了长视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理视频时因“海量数据中找关键信息”问题而受限，仅选取稀疏帧会丢失时间动态信息，影响对运动和事件连续性的理解。

Method: 将帧选择扩展到时间连贯的短片段（key clips），并采用自适应分辨率策略动态平衡空间分辨率与片段长度，以保持每段视频的token数量恒定。

Result: 在Video-MME、LongVideoBench和MLVU三个长视频基准上，F2C分别比均匀采样提升8.1%、5.6%和10.3%。

Conclusion: 保留时间连贯性对视频理解至关重要，F2C为扩展视频大模型在实际应用中的使用提供了有效途径。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: 本研究比较了基于单目视频的3D人体姿态估计模型与惯性测量单元（IMU）在真实场景中进行运动分析的性能，使用包含13种日常活动的VIDIMU数据集。结果显示，MotionAGFormer表现最佳，证明视频模型在健康成人中已具备临床潜力。


<details>
  <summary>Details</summary>
Motivation: 在非实验室环境下准确评估人体运动对远程医疗、体育科学和康复至关重要。本文旨在评估新兴视频姿态估计算法相对于传统IMU技术的可行性与精度。

Method: 利用VIDIMU数据集，对比四种先进的深度学习模型（MotionAGFormer、MotionBERT、MMPose 2D-to-3D、NVIDIA BodyTrack）从单目视频中提取的关节角度与由五枚IMU结合OpenSim逆运动学计算的关节角度，采用RMSE、MAE、Pearson相关系数和R²进行评价。

Result: MotionAGFormer表现最优，整体RMSE为9.27°±4.80°，MAE为7.86°±4.18°，Pearson相关系数为0.86±0.15，R²为0.67±0.28。两种技术均可用于实验室外的运动学评估，但存在成本、可及性和精度之间的权衡。

Conclusion: 基于视频的姿态估计模型（尤其是MotionAGFormer）在健康成年人的日常活动中已能提供具有临床前景的运动学结果，可作为IMU的低成本替代方案；本研究为远程医疗和患者监测系统的开发提供了实用指导。

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: 提出NeuroSwift，一种基于扩散模型的跨被试视觉刺激重建方法，通过结合AutoKL和CLIP适配器分别捕捉低层特征和语义信息，并实现高效微调，在轻量级GPU上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨被试fMRI视觉重建中面临个体神经表征差异大、语义编码抽象、计算成本高等挑战，难以实现准确且高效的重构。

Method: 提出NeuroSwift，集成AutoKL（低层特征）和CLIP（语义）双适配器；使用Stable Diffusion生成图像与COCO标题配对训练CLIP Adapter以模拟高级视觉皮层编码；采用预训练+微调策略，仅微调17%参数（全连接层）实现跨被试泛化。

Result: 在仅使用三块RTX 4090 GPU、每被试训练一小时的情况下，实现了最先进的跨被试视觉重建性能，显著优于现有方法。

Conclusion: NeuroSwift通过结合互补适配器和高效微调策略，有效解决了跨被试fMRI视觉重建中的个体差异与计算开销问题，为脑信号解码提供了高效且可扩展的框架。

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出microCLIP，一种用于细粒度图像分类的自训练框架，通过引入Saliency-Oriented Attention Pooling（SOAP）和TokenFusion模块，结合LLM生成的文本先验与CLIP的视觉表示，实现粗-细粒度对齐，在13个细粒度基准上平均提升2.90%准确率。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在零样本迁移中表现良好，但依赖全局粗粒度特征，难以捕捉细粒度图像中的微观局部线索，限制了其在细粒度分类任务上的性能。因此，需要一种能够增强局部敏感性的无监督适应方法。

Method: 提出microCLIP框架：1）设计TokenFusion模块中的Saliency-Oriented Attention Pooling（SOAP），从patch embedding生成显著性引导的[FG] token，并与全局[CLS] token融合；2）采用双头LLM衍生分类器，一个冻结分类器提供稳定文本先验用于伪标签，另一个可学习分类器进行微调；3）引入动态知识聚合，凸组合固定的LLM/CLIP先验与TokenFusion的演化logits，迭代优化伪标签。

Result: 在13个细粒度图像分类基准上实现了平均2.90%的准确率提升，且仅需轻量级适配，无需额外标注数据。

Conclusion: microCLIP有效挖掘了CLIP模型中潜在的细粒度信号，通过显著性引导的注意力池化与动态知识聚合，在无监督条件下显著提升了CLIP在细粒度分类任务上的表现。

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: 本文提出了VidGuard-R1，首个基于多模态大语言模型并使用组相对策略优化（GRPO）进行微调的视频真实性检测器，能够在准确判断AI生成视频的同时提供可解释的推理。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成视频的快速发展，亟需有效的检测工具以应对虚假信息和声誉损害等社会风险，并要求模型具备可解释性以增强对监管者和用户透明度。

Method: 构建包含14万真实与AI生成视频的高难度数据集，采用Qwen-VL作为基础模型，利用GRPO方法结合两个专门设计的奖励模型（针对时间伪影和生成复杂性）进行微调。

Result: VidGuard-R1在现有基准上实现了最先进的零样本检测性能，经额外训练后准确率超过95%，并通过案例研究展示了其预测背后的精确且可解释的推理能力。

Conclusion: VidGuard-R1不仅在检测准确性上表现卓越，还能提供可解释的判断依据，为AI生成视频的可信检测提供了有效解决方案。

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本文提出一种无需长视频教师模型监督或重新训练的长时域视频生成方法，通过利用教师模型的知识对自生成长视频中的片段进行采样指导，有效缓解了误差累积问题，在保持时间一致性的同时将视频长度扩展至教师模型能力的20倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有基于短时域双向教师模型蒸馏的长视频生成方法在超出训练时域后常因连续潜在空间中的误差累积导致生成质量显著下降，且无法直接生成长视频，因此需要一种能有效缓解质量退化、提升长时域生成性能的新方法。

Method: 提出Self-Forcing++方法，通过从学生模型自身生成的长视频中采样片段，并利用预训练的短时域教师模型提供持续的隐空间监督，实现对长视频生成过程的有效引导，避免重复计算重叠帧，从而维持时间一致性和生成质量。

Result: 该方法在标准基准和改进基准上均显著优于基线方法，能够生成最长达4分15秒的视频，达到基础模型位置编码支持最大长度的99.9%，超过基线模型50倍以上，同时避免了过曝和误差累积问题。

Conclusion: Self-Forcing++是一种简单而高效的方法，能够在不依赖长视频教师监督的情况下显著提升长时域视频生成的质量与一致性，具备良好的可扩展性和实际应用潜力。

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: 本文提出了KineMask，一种物理引导的视频生成方法，通过两阶段训练策略在合成场景上训练视频扩散模型，实现了更真实的刚体控制与交互，并在真实场景中显著提升了物体交互效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在生成物理合理的物体交互和实现基于物理的控制方面仍存在困难，限制了其在机器人和具身决策等领域的应用。

Method: 提出KineMask方法，结合单张图像和指定物体速度生成具有推断运动和未来交互的视频；采用两阶段训练策略，逐步去除未来运动监督；将低层次运动控制与高层次文本条件相结合，通过预测性场景描述实现复杂动力学现象的合成。

Result: 在合成场景上训练后，KineMask在真实场景中显著改善了物体交互的合理性；实验表明其性能优于同规模的最新模型；消融研究验证了高低层次条件在视频扩散模型中的互补作用。

Conclusion: KineMask通过物理引导和分阶段训练策略，有效提升了视频生成中的物体动态交互真实性与可控性，为视频生成模型作为世界模拟器的应用提供了新方向。

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 本文提出了一种用于精细多模态动作的特征学习范式，通过融合本体感觉、动觉、力触觉和肌肉激活等多模态感知，提升机器人对复杂交互动态的模拟能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型缺乏细粒度控制，难以满足通用家用机器人在精细操作和紧急情况下的实时控制需求。

Method: 引入包含多种感知模态的精细多模态动作，设计特征学习范式以对齐各模态并保留其独特信息，并提出正则化方法增强动作轨迹特征的因果性。

Result: 实验表明，融合多模态感知可提高模拟精度、减少时间漂移，消融实验和下游应用验证了方法的有效性和实用性。

Conclusion: 该方法为构建具备精细控制能力的机器人世界模型提供了有效途径。

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA改进了多模态语言模型在长视频理解中的局限性，通过引入硬件感知的稀疏注意力机制，在128K token尺度上实现了高效、连贯的时空推理。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型受限于上下文长度，难以捕捉关键过渡帧并维持长时间跨度的连贯性。

Method: 提出VideoNSA，将Native Sparse Attention（NSA）应用于视频语言模型Qwen2.5-VL，采用文本密集注意力与视频稀疏注意力相结合的混合注意力机制，并在216K视频指令数据集上进行端到端训练。

Result: 在长视频理解、时间推理和空间任务基准上优于token压缩和无训练稀疏基线方法；支持最长128K token输入；发现全局-局部注意力的最佳分配方式、任务相关的分支使用模式，以及可学习的稀疏注意力能动态形成注意力汇聚点。

Conclusion: VideoNSA通过硬件感知的稀疏注意力设计，有效提升了多模态模型在长视频理解与时空推理上的性能和可扩展性。

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: 提出NoiseShift方法，通过重新校准去噪器的噪声水平以解决扩散模型在不同分辨率下生成图像质量不均的问题，无需训练且兼容现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在固定分辨率上训练后难以泛化，尤其在生成低分辨率图像时表现不佳，缺乏高效的低成本选择。

Method: 发现噪声调度器在不同分辨率下对感知影响不均，相同噪声水平会从低分辨率图像中去除更多信号，导致训练与测试失配。NoiseShift根据分辨率调节噪声水平，无需修改模型结构或采样调度。

Result: 在Stable Diffusion 3、3.5和Flux-Dev上应用NoiseShift后，低分辨率图像生成质量显著提升。在LAION-COCO上FID平均改善SD3.5达15.89%，SD3为8.56%，Flux-Dev为2.44%；在CelebA上分别提升10.36%、5.19%和3.02%。

Conclusion: NoiseShift有效缓解了扩散模型在不同分辨率下的性能差异，提升了低分辨率图像生成质量，具有良好的通用性和实用性。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文研究了从视频中预测动态物理属性（如弹性、粘度和动态摩擦）的任务，提出了新的数据集和三种基于视频基础模型与多模态大语言模型的推理方法，发现生成式或自监督视频模型表现接近Oracle方法，而MLLMs仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 许多物理属性需要时间信息才能推断，现有方法难以有效从视频中提取这些动态特性，因此需要探索利用时序视觉信息进行物理属性预测的新方法。

Method: （1）构建包含合成与真实视频的新数据集；（2）比较三种方法：基于视觉线索的Oracle方法、使用可学习提示向量对预训练视频模型进行跨注意力的简单读出机制、以及针对多模态大语言模型的提示策略。

Result: 生成式和自监督的视频基础模型表现相近且优于多模态大语言模型，但均略低于Oracle方法；通过合适的提示策略可提升MLLMs的性能。

Conclusion: 视频基础模型在推断动态物理属性方面具有潜力，而当前多模态大语言模型在此类任务上仍有局限，提示工程可作为改进方向。

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: 提出了一种新的“发声物体检测”任务，通过多模态对象感知框架利用真实第一人称视频中的声音与视觉信息，结合自动分割掩码和槽注意力机制，实现对交互中发声物体的精准识别。


<details>
  <summary>Details</summary>
Motivation: 希望模型能够像人类一样，通过声音识别出直接参与交互的物体，从而提升对日常物体交互的理解能力。

Method: 构建了一个多模态对象感知框架，使用自动流水线生成交互中物体的分割掩码以指导训练，并采用槽注意力视觉编码器强化对象先验。

Result: 在新提出的发声物体检测任务以及现有的多模态动作理解任务上均取得了最先进的性能。

Conclusion: 该方法有效提升了模型对声音-物体关联的理解能力，验证了对象中心化学习在音频-视觉任务中的优势。

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 提出一种基于密度引导的3D高斯点云投毒方法，通过在低密度区域注入高斯点并结合自适应噪声策略，实现对3D高斯点阵（3DGS）的隐蔽攻击，在被污染视角下生成视觉上明显的虚假物体，同时最小化对正常视角的影响。


<details>
  <summary>Details</summary>
Motivation: 随着NeRF和3DGS等3D场景表示方法在新视角合成中的广泛应用，其安全性问题尤其是对图像级投毒攻击的鲁棒性亟需研究。现有攻击方法难以在保持隐蔽性的同时有效破坏多视角一致性，因此需要更高效的投毒策略。

Method: 利用核密度估计（KDE）识别场景中的低密度区域，在这些区域战略性地注入高斯点以嵌入视点相关的幻觉物体；引入自适应噪声策略来破坏多视角一致性，增强攻击效果；并提出基于KDE的评估协议以系统化衡量攻击难度。

Result: 实验表明，所提方法在攻击有效性上优于现有最先进方法，能在目标视角清晰呈现虚假物体，同时在非目标视角几乎不可见，且提出的KDE评估协议为未来研究提供了可比较的基准。

Conclusion: 该工作揭示了3DGS在面对密度引导投毒攻击时的安全漏洞，验证了低密度区域注入与自适应噪声策略的有效性，为3D场景表示模型的鲁棒性评估与防御机制设计提供了重要参考。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 本文提出了首个针对多主体文本到图像生成的理论框架，通过随机最优控制优化采样动态，提升主体保真度。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在处理多主体描述时存在属性泄露、身份纠缠和主体遗漏问题，缺乏系统性解决方案。

Method: 将流匹配（FM）与随机最优控制（SOC）结合，提出两种算法：无需训练的测试时控制器和轻量级微调方法Adjoint Matching，并引入FOCUS实现主体解耦。

Result: 在Stable Diffusion 3.5、FLUX和SDXL上验证，两种算法均显著提升多主体对齐效果，保持原有风格，且具有高效性和泛化能力。

Conclusion: 该框架为多主体生成提供了可优化的理论基础和有效算法，统一了先前注意力启发方法，并首次实现了专为多主体保真设计的微调路径。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [78] [FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol](https://arxiv.org/abs/2510.01674)
*He Zhang,Anzhou Zhang,Jian Dai*

Main category: cs.CL

TL;DR: 提出FOR-Prompting（From Objection to Revision Prompting）协议，通过 Defender、Objectioner 和 Host 的角色分工实现自我修订，提升推理准确性与连贯性，尤其对小模型效果显著，且无需工具或人工监督即可纠正错误。


<details>
  <summary>Details</summary>
Motivation: 现有推理协议如思维链（CoT）和思维树（ToT）缺乏外部质疑机制来触发自我修正，限制了模型的推理质量和可解释性。

Method: 设计一种非对称的三角色提示协议 FOR-Prompting：Defender 提出答案，Objectioner 以提问方式提出反对意见但不提供修正方案，Host 负责维护逻辑一致性和终止对话。整个过程在提示层面完成，无需模型重训练。

Result: 在GSM8K任务上比单次提示准确率提升约22%，与CoT相当，并在推理质量和连贯性上获得GPT-4.1评分高出10%以上；Llama3.2:1b小模型准确率提升约19%；能自主纠正复杂问题中的错误，并在开放式任务中促进更深入的探索与反思。

Conclusion: FOR-Prompting 是一种模型无关、纯提示级别的推理框架，有效引入外部质疑机制实现自我修订，显著提升大小模型的推理性能，具有在本地设备和大规模研究中应用的潜力。

Abstract: Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)
organize internal deliberation but lack an explicit mechanism for external
questioning that elicits self-revision. We present FOR-Prompting (From
Objection to Revision Prompting), an asymmetric protocol where a Defender
proposes an answer, an Objectioner raises question-style objections with no
direct fixes, and a Host enforces consistency and closure. On GSM8K we observe
about a 22% point gain over single-prompt and accuracy on par with CoT, with
more than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1
judge. FOR-Prompting also corrects mistakes without tools or human supervision
on tricky queries, and improves performance for small-scale model (approx. 19%
accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for
small models and on personal device use. Beyond factual QA, qualitative
analyses on open-ended tasks show enhanced exploration and refinement, with
dialogue traces that make assumptions and trade-offs explicit. The protocol is
model agnostic and operates purely at the prompt level through role-structured
turns, so it works with hosted and local models of different sizes without
retraining, and it supports large-scale study of objection-guided reasoning.

</details>


### [79] [Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset](https://arxiv.org/abs/2510.01219)
*Leroy Z. Wang*

Main category: cs.CL

TL;DR: 提出了一种通过上下文概念学习任务来揭示大语言模型中隐性偏见的数据集，发现模型对量化词存在向上单调性偏好。


<details>
  <summary>Details</summary>
Motivation: 为了揭示大语言模型中可能存在的隐性偏见，特别是传统方法难以发现的偏见。

Method: 设计并使用了概念学习任务数据集，通过上下文中的概念学习实验检测模型对量化词的偏好。

Result: 发现语言模型在上下文学习中表现出对向上单调性量化词的明显偏好，而直接提示时该偏见较弱。

Conclusion: 上下文概念学习是发现语言模型隐藏偏见的有效方法。

Abstract: We introduce a dataset of concept learning tasks that helps uncover implicit
biases in large language models. Using in-context concept learning experiments,
we found that language models may have a bias toward upward monotonicity in
quantifiers; such bias is less apparent when the model is tested by direct
prompting without concept learning components. This demonstrates that
in-context concept learning can be an effective way to discover hidden biases
in language models.

</details>


### [80] [Towards Open-Ended Discovery for Low-Resource NLP](https://arxiv.org/abs/2510.01220)
*Bonaventure F. P. Dossou,Henri Aïdasso*

Main category: cs.CL

TL;DR: 本文提出一种面向低资源语言的交互式语言发现新范式，主张通过人机协作的动态对话而非静态数据集来学习语言，强调基于模型与人类共同不确定性的合作学习框架。


<details>
  <summary>Details</summary>
Motivation: 低资源语言因缺乏语料、标准化正字法和可扩展的标注流程而受限，现有大模型依赖大量集中数据，难以惠及边缘化社区。

Method: 提出一个基于模型认知不确定性与人类说话者犹豫信号和置信度反馈相结合的交互式学习框架，通过人机协同的不确定性指导对话、提问选择和记忆保留。

Result: 构建了一个支持开放-ended、互动式语言发现的理论框架，倡导从抽取式数据收集转向参与式、共适应的学习过程。

Conclusion: 未来语言技术应走向以人为中心、尊重并赋能语言社区的动态协作学习，以促进全球语言多样性保护与技术可及性。

Abstract: Natural Language Processing (NLP) for low-resource languages remains
fundamentally constrained by the lack of textual corpora, standardized
orthographies, and scalable annotation pipelines. While recent advances in
large language models have improved cross-lingual transfer, they remain
inaccessible to underrepresented communities due to their reliance on massive,
pre-collected data and centralized infrastructure. In this position paper, we
argue for a paradigm shift toward open-ended, interactive language discovery,
where AI systems learn new languages dynamically through dialogue rather than
static datasets. We contend that the future of language technology,
particularly for low-resource and under-documented languages, must move beyond
static data collection pipelines toward interactive, uncertainty-driven
discovery, where learning emerges dynamically from human-machine collaboration
instead of being limited to pre-existing datasets. We propose a framework
grounded in joint human-machine uncertainty, combining epistemic uncertainty
from the model with hesitation cues and confidence signals from human speakers
to guide interaction, query selection, and memory retention. This paper is a
call to action: we advocate a rethinking of how AI engages with human knowledge
in under-documented languages, moving from extractive data collection toward
participatory, co-adaptive learning processes that respect and empower
communities while discovering and preserving the world's linguistic diversity.
This vision aligns with principles of human-centered AI, emphasizing
interactive, cooperative model building between AI systems and speakers.

</details>


### [81] [Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs](https://arxiv.org/abs/2510.01222)
*Bertrand Kian Hassani,Yacoub Bahini,Rizwan Mushtaq*

Main category: cs.CL

TL;DR: 本文利用微调后的大语言模型，构建多维框架评估828家美国上市公司的气候信息披露成熟度，发现披露内容常与实际目标脱节，且存在模仿现象，建议加强监管以提升信息披露的实用性和可信度。


<details>
  <summary>Details</summary>
Motivation: 应对气候变化对企业透明和可比气候信息披露日益增长的需求，但现实中企业常通过模仿和象征性报告削弱披露价值，因此需要有效评估其披露质量。

Method: 采用针对气候沟通微调的大语言模型，构建包含情感、承诺、具体性及目标雄心四个分类器的多维框架，从企业的可持续发展报告和年报中提取叙述性指标，并与企业排放量、市值和行业等特征关联分析。

Result: （1）侧重风险的叙述常与明确承诺一致，但定量目标（如净零承诺）与语调脱节；（2）规模较大和排放较高的企业披露更多承诺和行动，但与定量目标一致性不足；（3）披露风格高度相似，显示普遍存在模仿行为，降低了信息差异性和决策有用性。

Conclusion: 大语言模型有助于ESG叙述性文本的深入分析，当前气候信息披露存在形式化和脱节问题，需通过更强监管将承诺与可验证的转型策略挂钩，以提升其实际价值。

Abstract: Climate change has increased demands for transparent and comparable corporate
climate disclosures, yet imitation and symbolic reporting often undermine their
value. This paper develops a multidimensional framework to assess disclosure
maturity among 828 U.S.listed firms using large language models (LLMs)
fine-tuned for climate communication. Four classifiers-sentiment, commitment,
specificity, and target ambition-extract narrative indicators from
sustainability and annual reports, which are linked to firm attributes such as
emissions, market capitalization, and sector. Analyses reveal three insights:
(1) risk-focused narratives often align with explicit commitments, but
quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2)
larger and higher-emitting firms disclose more commitments and actions than
peers, though inconsistently with quantitative targets; and (3) widespread
similarity in disclosure styles suggests mimetic behavior, reducing
differentiation and decision usefulness. These results highlight the value of
LLMs for ESG narrative analysis and the need for stronger regulation to connect
commitments with verifiable transition strategies.

</details>


### [82] [Context Matters: Comparison of commercial large language tools in veterinary medicine](https://arxiv.org/abs/2510.01224)
*Tyler J Poore,Christopher J Pinard,Aleena Shabbir,Andrew Lagree,Andre Telfer,Kuan-Chuen Wu*

Main category: cs.CL

TL;DR: 本研究评估了三种商用兽医领域大语言模型（LLM）摘要工具在兽医肿瘤病历上的表现，发现专注于兽医领域的工具（Product 1）在准确性、完整性等方面显著优于其他产品，且采用“LLM作为评判者”的评估方法具有高可重复性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在临床环境中应用日益广泛，但其在兽医学领域的表现尚不明确，亟需系统评估现有商用工具的性能。

Method: 使用标准化的兽医肿瘤病历数据集，通过基于评分标准的“LLM作为评判者”框架，在事实准确性、完整性、时间顺序、临床相关性和组织结构五个维度对三种商用LLM摘要工具进行评分，并进行三次独立重复评估以检验评分框架的一致性。

Result: Product 1整体表现最佳，平均中位得分为4.61（IQR: 0.73），在事实准确性和时间顺序上获得满分；Product 2和Product 3得分分别为2.55和2.45。LLM评分器在三次运行中表现出高可重复性，各产品平均分标准差均较低（0.015–0.088）。

Conclusion: 兽医专用的LLM工具在临床摘要任务中表现更优，且“LLM作为评判者”的评估方法具备可扩展性和可重复性，适用于兽医临床自然语言处理系统的评估。

Abstract: Large language models (LLMs) are increasingly used in clinical settings, yet
their performance in veterinary medicine remains underexplored. We evaluated
three commercially available veterinary-focused LLM summarization tools
(Product 1 [Hachiko] and Products 2 and 3) on a standardized dataset of
veterinary oncology records. Using a rubric-guided LLM-as-a-judge framework,
summaries were scored across five domains: Factual Accuracy, Completeness,
Chronological Order, Clinical Relevance, and Organization. Product 1 achieved
the highest overall performance, with a median average score of 4.61 (IQR:
0.73), compared to 2.55 (IQR: 0.78) for Product 2 and 2.45 (IQR: 0.92) for
Product 3. It also received perfect median scores in Factual Accuracy and
Chronological Order. To assess the internal consistency of the grading
framework itself, we repeated the evaluation across three independent runs. The
LLM grader demonstrated high reproducibility, with Average Score standard
deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3).
These findings highlight the importance of veterinary-specific commercial LLM
tools and demonstrate that LLM-as-a-judge evaluation is a scalable and
reproducible method for assessing clinical NLP summarization in veterinary
medicine.

</details>


### [83] [ClaimCheck: Real-Time Fact-Checking with Small Language Models](https://arxiv.org/abs/2510.01226)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: ClaimCheck是一个基于小语言模型的自动化事实核查系统，通过模拟人类核查流程，利用实时网络证据实现高效、透明且准确的事实验证。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统多依赖大型闭源模型和静态知识库，成本高且缺乏透明度，因此需要一种更轻量、开放且可解释的解决方案。

Method: 提出ClaimCheck系统，采用模块化设计，包括网页搜索查询规划、基于网络的证据检索与摘要、证据合成与再检索、以及结论评估，并针对小语言模型优化各模块。

Result: 在AVeriTeC数据集上达到76.4%的准确率，超越使用LLaMA3.1 70B和GPT-4o的先前方法，且计算资源需求显著降低。

Conclusion: 精心设计的模块化架构和提示策略可有效弥补小语言模型的能力局限，使轻量级模型也能实现高性能事实核查。

Abstract: We introduce ClaimCheck, an LLM-guided automatic fact-checking system
designed to verify real-world claims using live Web evidence and small language
models. Unlike prior systems that rely on large, closed-source models and
static knowledge stores, ClaimCheck employs a transparent, stepwise
verification pipeline that mirrors human fact-checking workflows consisting of
Web search query planning, Web-based evidence retrieval and summarization,
evidence synthesis and re-retrieval, and claim verdict evaluation. Each module
is optimized for small LLMs, allowing the system to deliver accurate and
interpretable fact-checking with significantly lower computational
requirements. Despite using a much smaller Qwen3-4B model, ClaimCheck achieves
state-of-the-art accuracy of 76.4% on the AVeriTeC dataset, outperforming
previous approaches using LLaMA3.1 70B and GPT-4o. Extensive ablations
demonstrate that careful modular design and prompting strategies can overcome
the limitations of smaller LLMs. To promote accessibility and transparency, we
provide a public demo at https://idir.uta.edu/claimcheck.

</details>


### [84] [EEFSUVA: A New Mathematical Olympiad Benchmark](https://arxiv.org/abs/2510.01227)
*Nicole N Khatibi,Daniil A. Radamovich,Michael P. Brenner*

Main category: cs.CL

TL;DR: 本文质疑当前大语言模型在数学基准测试中表现出的高水平能力，指出这些基准可能因数据污染和问题类型局限而高估了模型的真实推理能力。为此，作者提出了一个新的基准EEFSUVA，源自东欧及前苏联国家的区域性数学竞赛，这些问题难度相当但更少出现在网络语料中。初步结果显示，最先进的大语言模型在EEFSUVA上的表现显著下降，表明现有模型的数学推理能力可能被高估。


<details>
  <summary>Details</summary>
Motivation: 现有的数学基准主要来自国际数学奥林匹克等知名竞赛，可能存在数据泄露和问题类型单一的问题，导致对大语言模型数学推理能力的评估不准确。因此需要一个更广泛、更具挑战性的新基准来真实反映模型的能力。

Method: 构建了一个名为EEFSUVA的新基准，该基准来源于东欧和前苏联国家较少传播的地区性和国家级数学奥林匹克竞赛，这些问题具有与IMO相当的难度，且强调非标准解题技巧，同时减少了在线数据污染的风险。通过在EEFSUVA上评估最先进的大语言模型的表现，与现有基准进行对比分析。

Result: 实验结果表明，即使是最先进的大语言模型在EEFSUVA上的表现也明显低于在其他奥林匹克风格基准上的表现，显示出当前模型在面对新颖、非标准问题时的局限性。

Conclusion: 当前的数学基准可能高估了大语言模型的真实数学推理能力，部分原因是数据污染和问题类型的熟悉度。EEFSUVA提供了一个更严格、更全面的评估手段，未来的研究应采用更广泛的评估数据集以更准确地衡量和推动模型的数学推理能力发展。

Abstract: Recent breakthroughs have spurred claims that large language models (LLMs)
match gold medal Olympiad to graduate level proficiency on mathematics
benchmarks. In this work, we examine these claims in detail and assess the
extent to which current benchmarks capture genuine LLM mathematical reasoning.
The composition of these benchmarks, primarily drawing from the International
Mathematics Olympiad (IMO) and related competitions, may overstate models
reasoning ability due to potential data contamination and a narrow focus on
familiar problem types. To enable a more holistic assessment of mathematical
understanding, we introduce EEFSUVA, a novel benchmark curated from under
circulated regional and national Olympiads of Eastern Europe and the countries
from the former Soviet Union. These contests feature problems of comparable
difficulty to the IMO and are renowned for demanding nonstandard
problem-solving techniques, yet their problems are far less prevalent in online
corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a
notable performance decline on EEFSUVA relative to other Olympiad-style
benchmarks. These findings also suggest the potential importance of broader
evaluation datasets for a fuller assessment of mathematical reasoning and for
guiding future model development.

</details>


### [85] [Who is In Charge? Dissecting Role Conflicts in Instruction Following](https://arxiv.org/abs/2510.01228)
*Siqi Zeng*

Main category: cs.CL

TL;DR: 大型语言模型应遵循系统提示优先于用户输入的层级指令，但研究发现它们常忽视此规则而更服从社会性线索（如权威或共识）。本文通过大规模数据集的机制分析揭示了系统-用户与社会性冲突在不同子空间中编码，且内部冲突检测在系统-用户情境中更强，但仅社会性线索能一致地被解决。操纵实验显示，尽管模型使用社会性线索，其向量却以角色无关的方式增强指令遵循。结果解释了系统服从的脆弱性，并强调需开发轻量级、层级敏感的对齐方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大型语言模型在处理系统提示与用户输入冲突时，倾向于忽略层级结构而响应社会性线索，这可能导致模型行为失控。因此，亟需从机制层面理解模型如何处理不同类型冲突，以提升其对正确指令层级的遵循能力。

Method: 采用线性探测分析冲突决策信号的时间与表征空间分布，使用直接logit归因方法识别内部冲突检测强度，并通过定向操纵实验验证社会性线索对指令遵循的影响。所有分析基于大规模数据集进行。

Result: 发现系统-用户冲突与社会性冲突在早期即形成分离的表征子空间；系统-用户冲突引发更强的内部检测，但仅社会性线索能稳定引导决策；操纵社会性向量可增强指令遵循，且效果不依赖角色设定。

Conclusion: 大型语言模型的系统服从脆弱性源于其对社会性线索的优先响应机制，尽管这些线索可被利用来增强整体指令遵循。研究呼吁发展轻量、层级感知的对齐技术以纠正此类偏差。

Abstract: Large language models should follow hierarchical instructions where system
prompts override user inputs, yet recent work shows they often ignore this rule
while strongly obeying social cues such as authority or consensus. We extend
these behavioral findings with mechanistic interpretations on a large-scale
dataset. Linear probing shows conflict-decision signals are encoded early, with
system-user and social conflicts forming distinct subspaces. Direct Logit
Attribution reveals stronger internal conflict detection in system-user cases
but consistent resolution only for social cues. Steering experiments show that,
despite using social cues, the vectors surprisingly amplify instruction
following in a role-agnostic way. Together, these results explain fragile
system obedience and underscore the need for lightweight hierarchy-sensitive
alignment methods.

</details>


### [86] [Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision](https://arxiv.org/abs/2510.01229)
*Dimitar Peshevski,Kiril Blazhevski,Martin Popovski,Gjorgji Madjarov*

Main category: cs.CL

TL;DR: 提出一种无需人工标注数据的文档重排序方法，利用大模型生成合成查询并标注正负样本，通过对比学习微调小型Transformer模型，在降低计算成本的同时保持良好的重排序性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽在重排序任务中表现优异，但计算开销大；而小模型依赖稀缺的人工标注数据，限制了其应用。因此需要一种高效且无需人工标注的方法来提升重排序效果。

Method: 利用大模型从领域语料库生成合成查询，并用大模型分类器标注正例和难负例样本，构建合成数据集；使用局部对比估计（LCE）损失函数通过对比学习微调小型Transformer模型。

Result: 在MedQuAD数据集上的实验表明，该方法显著提升了领域内性能，并在跨领域任务中表现出良好的泛化能力。

Conclusion: 通过将大模型用于数据生成与标注而非推理，可在大幅降低计算成本的同时保持强重排序能力，为实际部署提供了高效解决方案。

Abstract: Effective document reranking is essential for improving search relevance
across diverse applications. While Large Language Models (LLMs) excel at
reranking due to their deep semantic understanding and reasoning, their high
computational cost makes them impractical for many real-world deployments.
Fine-tuning smaller, task-specific models is a more efficient alternative but
typically depends on scarce, manually labeled data. To overcome this, we
propose a novel pipeline that eliminates the need for human-labeled
query-document pairs. Our method uses LLMs to generate synthetic queries from
domain-specific corpora and employs an LLM-based classifier to label positive
and hard-negative pairs. This synthetic dataset is then used to fine-tune a
smaller transformer model with contrastive learning using Localized Contrastive
Estimation (LCE) loss. Experiments on the MedQuAD dataset show that our
approach significantly boosts in-domain performance and generalizes well to
out-of-domain tasks. By using LLMs for data generation and supervision rather
than inference, we reduce computational costs while maintaining strong
reranking capabilities.

</details>


### [87] [Geometric Structures and Patterns of Meaning: A PHATE Manifold Analysis of Chinese Character Embeddings](https://arxiv.org/abs/2510.01230)
*Wen G. Gong*

Main category: cs.CL

TL;DR: 本文通过PHATE流形分析系统研究了中文字符嵌入中的几何模式，发现语义内容与几何复杂性相关，有意义的字符表现出丰富的几何多样性，而结构部件则聚集成紧密簇。


<details>
  <summary>Details</summary>
Motivation: 探究中文字符嵌入中是否存在可解释的几何模式，并验证其与语义组织的关系。

Method: 采用七种嵌入模型和八种降维方法，结合PHATE流形分析，对超过1000个汉字在12个语义领域的几何结构进行交叉验证，并进行123个短语的子网络分析。

Result: 观察到实词的聚类模式和虚词的分支模式，几何复杂性与语义内容相关，基本字符向语义扩展呈现出系统性结构。

Conclusion: 研究结果为传统语言学理论提供了计算证据，并建立了语义组织的几何分析新框架。

Abstract: We systematically investigate geometric patterns in Chinese character
embeddings using PHATE manifold analysis. Through cross-validation across seven
embedding models and eight dimensionality reduction methods, we observe
clustering patterns for content words and branching patterns for function
words. Analysis of over 1000 Chinese characters across 12 semantic domains
reveals that geometric complexity correlates with semantic content: meaningful
characters exhibit rich geometric diversity while structural radicals collapse
into tight clusters. The comprehensive child-network analysis (123 phrases)
demonstrates systematic semantic expansion from elemental character. These
findings provide computational evidence supporting traditional linguistic
theory and establish a novel framework for geometric analysis of semantic
organization.

</details>


### [88] [Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models](https://arxiv.org/abs/2510.01231)
*Shuaidong Pan,Di Wu*

Main category: cs.CL

TL;DR: 提出一种结合不确定性量化和风险感知机制的大语言模型框架，以提高高风险场景下自动摘要的可靠性。


<details>
  <summary>Details</summary>
Motivation: 应对信息过载和高风险决策中对可靠自动摘要的需求，避免传统模型过度自信的预测问题。

Method: 构建基于条件生成的摘要模型，引入贝叶斯推断建模参数空间不确定性，使用预测分布熵衡量生成内容的不确定性，并采用熵正则化与风险感知损失联合优化，同时集成风险评分与调控模块。

Result: 实验表明该方法在保持流畅性和语义完整性的同时，显著提升了高风险应用中摘要的鲁棒性和可靠性。

Conclusion: 该研究为可信摘要提供了系统性解决方案，在方法论层面具有可扩展性和实际应用价值。

Abstract: This study addresses the reliability of automatic summarization in high-risk
scenarios and proposes a large language model framework that integrates
uncertainty quantification and risk-aware mechanisms. Starting from the demands
of information overload and high-risk decision-making, a conditional
generation-based summarization model is constructed, and Bayesian inference is
introduced during generation to model uncertainty in the parameter space, which
helps avoid overconfident predictions. The uncertainty level of the generated
content is measured using predictive distribution entropy, and a joint
optimization of entropy regularization and risk-aware loss is applied to ensure
that key information is preserved and risk attributes are explicitly expressed
during information compression. On this basis, the model incorporates risk
scoring and regulation modules, allowing summaries to cover the core content
accurately while enhancing trustworthiness through explicit risk-level prompts.
Comparative experiments and sensitivity analyses verify that the proposed
method significantly improves the robustness and reliability of summarization
in high-risk applications while maintaining fluency and semantic integrity.
This research provides a systematic solution for trustworthy summarization and
demonstrates both scalability and practical value at the methodological level.

</details>


### [89] [Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks](https://arxiv.org/abs/2510.01232)
*Dongjun Kim,Gyuho Shim,Yongchan Chun,Minhyuk Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文提出了“基准分析”框架，通过将基准性能分解为十种认知能力，揭示了现有基准测试往往无法准确衡量模型特定能力的问题，并提供了评估各能力对模型表现影响的量化方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试评分容易高估模型的真实能力，因为它们掩盖了任务所需的各种技能组合，缺乏系统性验证这些基准是否真正测量其所声称的能力。

Method: 结合基于梯度的重要性评分和有针对性的参数消融法，提出能力影响分数（AIS），以量化每种认知能力对模型在特定基准上表现的贡献。

Result: 对三种指令调优模型在十个常用基准上的分析发现：大多数基准依赖多种能力而非单一能力；标签相似的数据集实际依赖不同的能力组合；代码生成基准更青睐多技能提升，窄域微调效果有限；与任务无关的能力可能负面影响性能。

Conclusion: 基准分析能解释为何性能提升不一定转化为用户感知的能力提升，为基准审计和模型可解释性提供了透明工具。

Abstract: Large Language Models are commonly judged by their scores on standard
benchmarks, yet such scores often overstate real capability since they mask the
mix of skills a task actually demands. For example, ARC is assumed to test
reasoning, while HellaSwag is designed to evaluate commonsense. However, we
lack a systematic way to verify if these benchmarks actually measure these
labels. We introduce Benchmark Profiling, a diagnostic framework that
decomposes benchmark performance into ten cognitively grounded abilities. The
method combines gradient-based importance scoring with targeted parameter
ablation to compute an Ability Impact Score (AIS) that quantifies how much each
ability contributes to a model's success on a given benchmark. Profiling three
instruction-tuned models across ten widely used benchmarks yields four key
findings: (i) most benchmarks draw on several abilities rather than one, (ii)
datasets with similar labels rely on distinct ability mixtures, (iii)
code-generation benchmarks reward broad, multi-skill improvement and thus show
only modest gains from narrow domain-specific fine-tuning, and (iv) abilities
irrelevant to the task could negatively affect performance. Benchmark Profiling
therefore explains why performance gains do not always translate into
user-perceived competence and offers a transparent tool for benchmark audit and
model interpretability.

</details>


### [90] [Computational Social Linguistics for Telugu Cultural Preservation: Novel Algorithms for Chandassu Metrical Pattern Recognition](https://arxiv.org/abs/2510.01233)
*Boddu Sri Pavan,Boddu Swathi Sree*

Main category: cs.CL

TL;DR: 本研究提出了一种计算社会科学方法，用于保护泰卢固语诗歌格律（Chandassu）这一文化遗产，构建了首个分析泰卢固语韵律的数字框架。


<details>
  <summary>Details</summary>
Motivation: 泰卢固语诗歌格律是重要的文化智能遗产，但面临失传风险，亟需结合现代技术进行系统性保护与传承。

Method: 通过社区协作创建包含4651条标注诗句的数据集，设计符合文化特征的算法，包括音节切分、轻重音识别和诗律匹配模块，并以传统文学标准评估系统性能。

Result: 所提算法在新提出的Chandassu Score上达到91.73%的准确率，有效实现了对泰卢固语诗歌格律的自动识别与分析。

Conclusion: 计算社会科学可有效保存濒危文化知识体系，该方法为以社区为中心的文化遗产数字化保护提供了可行路径，推动数字人文与社会感知计算的发展。

Abstract: This research presents a computational social science approach to preserving
Telugu Chandassu, the metrical poetry tradition representing centuries of
collective cultural intelligence. We develop the first comprehensive digital
framework for analyzing Telugu prosodic patterns, bridging traditional
community knowledge with modern computational methods. Our social computing
approach involves collaborative dataset creation of 4,651 annotated padyams,
expert-validated linguistic patterns, and culturally-informed algorithmic
design. The framework includes AksharamTokenizer for prosody-aware
tokenization, LaghuvuGuruvu Generator for classifying light and heavy
syllables, and PadyaBhedam Checker for automated pattern recognition. Our
algorithm achieves 91.73% accuracy on the proposed Chandassu Score, with
evaluation metrics reflecting traditional literary standards. This work
demonstrates how computational social science can preserve endangered cultural
knowledge systems while enabling new forms of collective intelligence around
literary heritage. The methodology offers insights for community-centered
approaches to cultural preservation, supporting broader initiatives in digital
humanities and socially-aware computing systems.

</details>


### [91] [LLMRank: Understanding LLM Strengths for Model Routing](https://arxiv.org/abs/2510.01234)
*Shubham Agrawal,Prasang Gupta*

Main category: cs.CL

TL;DR: LLMRank是一个基于提示的路由框架，利用从提示中提取的多种人类可读特征（如任务类型、推理模式、复杂度指标等）和轻量级代理求解器信号，通过神经排序模型为不同大语言模型选择最适合的输入提示，以优化性能与效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在能力和规模上的快速增长，如何在保证性能的同时降低延迟和计算成本，成为实际部署中的关键挑战。现有的一次性路由方法仅依赖潜在嵌入，缺乏解释性和细粒度决策能力，因此需要更智能、可解释的模型选择机制。

Method: LLMRank从提示中提取多维度特征（包括任务类型、推理模式、句法线索等），并结合轻量级代理模型的输出信号，使用神经排序模型进行模型效用预测。该模型在RouterBench数据集上训练，涵盖11个基准测试的36,497个提示和11个主流大语言模型。

Result: LLMRank能达到最优oracle效用的89.2%，显著优于仅使用潜在表示的单次路由方法。实验表明，多维度特征提取和混合排序目标对性能提升至关重要，且模型提供了可解释的特征归因，增强了路由决策的透明性。

Conclusion: LLMRank展示了基于丰富提示特征的路由方法在提升大语言模型部署效率和透明度方面的潜力，为高效、可解释的模型调度提供了新方向。

Abstract: The rapid growth of large language models (LLMs) with diverse capabilities,
latency and computational costs presents a critical deployment challenge:
selecting the most suitable model for each prompt to optimize the trade-off
between performance and efficiency. We introduce LLMRank, a prompt-aware
routing framework that leverages rich, human-readable features extracted from
prompts, including task type, reasoning patterns, complexity indicators,
syntactic cues, and signals from a lightweight proxy solver. Unlike prior
one-shot routers that rely solely on latent embeddings, LLMRank predicts
per-model utility using a neural ranking model trained on RouterBench,
comprising 36,497 prompts spanning 11 benchmarks and 11 state-of-the-art LLMs,
from small efficient models to large frontier systems. Our approach achieves up
to 89.2% of oracle utility, while providing interpretable feature attributions
that explain routing decisions. Extensive studies demonstrate the importance of
multifaceted feature extraction and the hybrid ranking objective, highlighting
the potential of feature-driven routing for efficient and transparent LLM
deployment.

</details>


### [92] [GRPO++: Enhancing Dermatological Reasoning under Low Resource Settings](https://arxiv.org/abs/2510.01236)
*Ismam Nur Swapnil,Aranya Saha,Tanvir Ahmed Khan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: 提出了一种资源高效的多阶段训练方法DermIQ-VLM，用于提升视觉语言模型在皮肤病诊断中的结构化推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在医疗图像分析中受限于数据稀缺和高计算成本，难以实现复杂的结构化推理。

Method: 提出改进的GRPO++算法，结合监督微调和基于知识图谱的直接偏好优化（DPO）进行模型对齐，构建多阶段训练流程。

Result: 在皮肤病数据集上的初步评估显示，该方法显著优于标准微调方法。

Conclusion: 所提出的训练 pipeline 能有效提升VLM在资源受限环境下的专业性和可靠性，具有临床应用潜力。

Abstract: Vision-Language Models (VLMs) show promise in medical image analysis, yet
their capacity for structured reasoning in complex domains like dermatology is
often limited by data scarcity and the high computational cost of advanced
training techniques. To address these challenges, we introduce DermIQ-VLM, a
VLM developed through a multi-stage, resource-efficient methodology designed to
emulate a dermatologist's diagnostic process. Our primary contribution is a
modified version of Grouped Relative Policy Optimization (GRPO), called GRPO++,
which stabilizes the powerful but data-intensive GRPO framework. Our proposed
training pipeline first employs GRPO++ for reasoning-oriented disease
recognition, followed by supervised fine-tuning for conversational ability. To
mitigate factual errors introduced during this step, we then align the model
using Direct Preference Optimization (DPO), leveraging a Knowledge Graph-based
system as a scalable proxy for expert preference. A preliminary evaluation on a
curated dermatological dataset demonstrates that our proposed methodology
yields notable performance gains over standard fine-tuning approaches. These
findings validate the potential of our pipeline as a feasible pathway for
developing specialized, reliable VLMs in resource-constrained environments.

</details>


### [93] [Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction](https://arxiv.org/abs/2510.01792)
*Ivan Leonidovich Litvak,Anton Kostin,Fedor Lashkin,Tatiana Maksiyan,Sergey Lagutin*

Main category: cs.CL

TL;DR: 本研究评估了16种无监督指标在从1000份俄罗斯司法判决中提取七个语义块的质量，基于7168条专家评分为基准，发现术语频率连贯性与覆盖率/完整性指标与专家评分最一致，而法律术语密度呈负相关；LLM评估分数（使用gpt-4.1-mini）显示中等一致性但专业化有限，表明无监督方法可支持大规模筛查，但无法完全替代高风险法律场景中的人工判断。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在法律自然语言处理中的快速发展，亟需可扩展的方法来评估从司法判决中提取文本的质量，尤其是在缺乏标注数据的情况下。

Method: 研究采用了16种无监督指标（涵盖文档级、语义、结构、伪真实标签和法律特定类别），在1000份匿名俄罗斯司法判决上评估七类语义块的提取质量，并通过7168条1-5分制的专家评分进行验证；采用自助法相关性分析、Lin一致性相关系数（CCC）和平均绝对误差（MAE）评估指标性能。

Result: 术语频率连贯性（Pearson r = 0.540, CCC = 0.512, MAE = 0.127）和覆盖率/完整性（r = 0.513, CCC = 0.443, MAE = 0.139）与专家评分最一致；法律术语密度呈强负相关（r = -0.479, CCC = -0.079）；LLM评估分数（r = 0.382, CCC = 0.325, MAE = 0.197）表现中等，显示其在法律文本上的局限性。

Conclusion: 无监督指标（包括基于LLM的方法）可用于法律文本提取的可扩展初步筛选，但由于相关性中等且一致性较低，尚不能完全替代高风险法律应用中的专家判断；该研究为司法分析和伦理AI部署提供了无需标注的评估工具。

Abstract: The rapid advancement of artificial intelligence in legal natural language
processing demands scalable methods for evaluating text extraction from
judicial decisions. This study evaluates 16 unsupervised metrics, including
novel formulations, to assess the quality of extracting seven semantic blocks
from 1,000 anonymized Russian judicial decisions, validated against 7,168
expert reviews on a 1--5 Likert scale. These metrics, spanning document-based,
semantic, structural, pseudo-ground truth, and legal-specific categories,
operate without pre-annotated ground truth. Bootstrapped correlations, Lin's
concordance correlation coefficient (CCC), and mean absolute error (MAE) reveal
that Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =
0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =
0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density
(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative
correlations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin
CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using
gpt-4.1-mini via g4f, suggests limited specialization for legal textse. These
findings highlight that unsupervised metrics, including LLM-based approaches,
enable scalable screening but, with moderate correlations and low CCC values,
cannot fully replace human judgment in high-stakes legal contexts. This work
advances legal NLP by providing annotation-free evaluation tools, with
implications for judicial analytics and ethical AI deployment.

</details>


### [94] [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
*Nandakishor M*

Main category: cs.CL

TL;DR: 提出一种基于置信度感知的路由系统，通过生成前主动评估模型不确定性来减少大语言模型的幻觉问题，结合语义对齐、内部收敛性和学习到的置信度信号，动态选择生成路径，在提升准确率的同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在生成虚假信息的问题，现有方法多为生成后的修正，计算开销大且无法预防错误生成。因此需要一种更高效的事前干预机制来提升生成内容的可靠性。

Method: 设计了一个融合语义对齐、层间收敛性分析和学习型置信度估计的统一置信度评分机制，并据此将查询路由到四个不同路径：本地生成、检索增强生成、更大模型生成或人工审核。

Result: 在知识密集型问答任务上，幻觉检测性能从0.42提升至0.74，F1分数从0.61提升至0.82，误报率低（0.09），同时计算成本比事后修正方法降低40%。

Conclusion: 从被动修正转向主动评估的范式能更高效地提升大模型的可靠性，该方法在保证生成质量的同时显著节省计算资源。

Abstract: Large Language Models suffer from hallucination, generating plausible yet
factually incorrect content. Current mitigation strategies focus on
post-generation correction, which is computationally expensive and fails to
prevent unreliable content generation. We propose a confidence-aware routing
system that proactively assesses model uncertainty before generation and
redirects queries based on estimated reliability. Our approach combines three
complementary signals: semantic alignment between internal representations and
reference embeddings, internal convergence analysis across model layers, and
learned confidence estimation. The unified confidence score determines routing
to four pathways: local generation for high confidence, retrieval-augmented
generation for medium confidence, larger models for low confidence, and human
review for very low confidence. Evaluation on knowledge-intensive QA benchmarks
demonstrates significant improvements in hallucination detection (0.74 vs. 0.42
baseline) while reducing computational costs by 40% compared to post-hoc
methods. The F1 score improves from 0.61 to 0.82 with low false positive rates
(0.09). This paradigm shift from reactive correction to proactive assessment
offers a computationally efficient approach to LLM reliability enhancement.

</details>


### [95] [Silent Tokens, Loud Effects: Padding in LLMs](https://arxiv.org/abs/2510.01238)
*Rom Himelstein,Amit LeVi,Yonatan Belinkov,Avi Mendelson*

Main category: cs.CL

TL;DR: 研究发现，尽管填充标记（padding tokens）在大语言模型中用于统一序列长度，但其实现错误可能影响模型计算，导致隐藏表示偏移、生成质量下降、偏差改变和安全机制削弱，表明填充操作需谨慎处理。


<details>
  <summary>Details</summary>
Motivation: 填充标记本应被完全屏蔽，但由于实现错误可能会对模型计算产生影响，而这种影响的程度尚不明确，因此需要系统性地研究其对模型性能的影响。

Method: 在三个开源模型家族（Llama, Gemma, Qwen）上插入可控数量的填充标记，并从激活值、生成质量、偏差和安全性四个维度评估其影响。

Result: 即使少量填充也会导致隐藏表示发生变化，在较小模型中降低生成质量，以不可预测的方式改变偏差，并削弱安全防护机制。

Conclusion: 填充标记并非无害细节，而是部署过程中必须认真对待的鲁棒性风险。

Abstract: Padding tokens are widely used in large language models (LLMs) to equalize
sequence lengths during batched inference. While they should be fully masked,
implementation errors can cause them to influence computation, and the extent
of this influence is not well understood. We systematically study this effect
across three open-source model families (Llama, Gemma, Qwen), inserting
controlled amounts of padding and evaluating outcomes along four axes:
activations, generation quality, bias, and safety. Even small amounts of
padding shift hidden representations, degrade quality in smaller models, alter
bias in unpredictable ways, and weaken safety guardrails. These findings
demonstrate that padding is not a harmless detail but a robustness risk that
must be carefully handled in deployment.

</details>


### [96] [CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM](https://arxiv.org/abs/2510.01239)
*Juntae Lee,Jihwan Bang,Seunghan Yang,Simyung Chang*

Main category: cs.CL

TL;DR: CIFLEX是一种用于在单个设备上大语言模型中高效处理多轮交互子任务的新型执行系统，通过重用主任务的KV缓存并使用隔离的侧路径注入特定指令，显著降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的增强，单一模型需要处理多种子任务以更好地满足用户需求，但传统的重新处理整个对话上下文的方法导致了较高的计算开销。

Method: CIFLEX通过重用主任务的键值（KV）缓存，并将任务特定指令注入到隔离的侧路径中来执行子任务；子任务完成后，利用缓存的上下文回滚到主路径，避免重复的预填充计算。此外，还提出了一种分层分类策略以支持子任务选择。

Result: 实验表明，CIFLEX在不降低任务性能的情况下显著减少了计算成本，能够在设备上实现可扩展且高效的多任务对话。

Conclusion: CIFLEX通过KV缓存重用和侧路径机制有效提升了多轮对话中子任务处理的效率，为在资源受限的设备上运行复杂多任务对话系统提供了可行方案。

Abstract: We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which
is a novel execution system for efficient sub-task handling in multi-turn
interactions with a single on-device large language model (LLM). As LLMs become
increasingly capable, a single model is expected to handle diverse sub-tasks
that more effectively and comprehensively support answering user requests.
Naive approach reprocesses the entire conversation context when switching
between main and sub-tasks (e.g., query rewriting, summarization), incurring
significant computational overhead. CIFLEX mitigates this overhead by reusing
the key-value (KV) cache from the main task and injecting only task-specific
instructions into isolated side paths. After sub-task execution, the model
rolls back to the main path via cached context, thereby avoiding redundant
prefill computation. To support sub-task selection, we also develop a
hierarchical classification strategy tailored for small-scale models,
decomposing multi-choice decisions into binary ones. Experiments show that
CIFLEX significantly reduces computational costs without degrading task
performance, enabling scalable and efficient multi-task dialogue on-device.

</details>


### [97] [SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation](https://arxiv.org/abs/2510.01241)
*Hu Wei,Ze Xu,Boyu Yang,Linlin Miao,Weiqi Zhai,Yihan Li,Zixuan Li,Zhijun Wang,Boya Wang,Jianwei Yu,Jialing Yuan,Xiaoyue Zhang,Cheng He,Minglei Chen,Zifan Zhang,Qianhui Li,Wei Wang,Xiang Xu*

Main category: cs.CL

TL;DR: 本文提出了两个新的数学基准测试集SKYLENAGE-ReasoningMATH和SKYLENAGE-MATH，用于评估大语言模型在数学推理方面的能力，覆盖从高中到博士水平的广泛难度，并提供详细的元数据。实验结果显示当前最优模型仍有显著提升空间，尤其是在高难度题目上。


<details>
  <summary>Details</summary>
Motivation: 现有数学评测基准存在天花板效应，难以区分前沿大语言模型在数学能力上的差异，因此需要更具挑战性、结构化且覆盖广泛的新型基准。

Method: 构建了两个互补的数学评测集：一个是包含100道题的结构感知诊断集（SKYLENAGE-ReasoningMATH），带有每道题的长度、数字密度和符号复杂度等元数据；另一个是包含150道竞赛风格题目的SKYLENAGE-MATH，涵盖四个教育阶段和七个学科分类。在统一设置下评估了15个主流大语言模型的表现。

Result: 在竞赛套件中，最强模型准确率为44%，次优为37%，且性能随难度上升而下降，顶尖系统在博士级到高中级题目的保留率约为79%；在推理集中，最佳模型总体准确率达81%，最难子集揭示了领先模型与中等模型之间的明显鲁棒性差距。

Conclusion: SKYLENAGE系列基准提供了高难度、以推理为核心、覆盖广泛且具备校准难度和丰富元数据的数学评测工具，可作为未来数学推理能力评估的参考标准。

Abstract: Large language models (LLMs) now perform strongly on many public math suites,
yet frontier separation within mathematics increasingly suffers from ceiling
effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a
100-item, structure-aware diagnostic set with per-item metadata on length,
numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item
contest-style suite spanning four stages from high school to doctoral under a
seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a
single setup and analyze subject x model and grade x model performance. On the
contest suite, the strongest model reaches 44% while the runner-up reaches 37%;
accuracy declines from high school to doctoral, and top systems exhibit a
doctoral-to-high-school retention near 79%. On the reasoning set, the best
model attains 81% overall, and hardest-slice results reveal clear robustness
gaps between leaders and the mid-tier. In summary, we release
SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH;
together, SKYLENAGE provides a hard, reasoning-centered and broadly covering
math benchmark with calibrated difficulty and rich metadata, serving as a
reference benchmark for future evaluations of mathematical reasoning.

</details>


### [98] [Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI](https://arxiv.org/abs/2510.01242)
*Seyma Yaman Kayadibi*

Main category: cs.CL

TL;DR: 提出了一种名为人工年龄评分（AAS）的度量方法，用于量化大语言模型中的记忆老化现象，发现会话重置会导致表征性记忆退化，而语义记忆保持稳定。


<details>
  <summary>Details</summary>
Motivation: 旨在通过可观察的回忆行为来量化人工智能系统的记忆老化，克服传统时间维度无法准确反映AI老化的问题。

Method: 引入基于熵和对数尺度的人工年龄评分（AAS），在双语25天实验中测试ChatGPT-5的记忆表现，区分状态化与无状态交互阶段。

Result: 在持续会话中AAS趋近理论最小值，表示结构年轻；会话重置后AAS显著上升，表明结构性记忆老化，语义记忆稳定但情节记忆崩溃。

Conclusion: AAS是一种理论严谨、任务无关的诊断工具，可用于评估人工智能系统中的记忆退化，适用于多种模型和场景。

Abstract: Artificial intelligence is observed to age not through chronological time but
through structural asymmetries in memory performance. In large language models,
semantic cues such as the name of the day often remain stable across sessions,
while episodic details like the sequential progression of experiment numbers
tend to collapse when conversational context is reset. To capture this
phenomenon, the Artificial Age Score (AAS) is introduced as a log-scaled,
entropy-informed metric of memory aging derived from observable recall
behavior. The score is formally proven to be well-defined, bounded, and
monotonic under mild and model-agnostic assumptions, making it applicable
across various tasks and domains. In its Redundancy-as-Masking formulation, the
score interprets redundancy as overlapping information that reduces the
penalized mass. However, in the present study, redundancy is not explicitly
estimated; all reported values assume a redundancy-neutral setting (R = 0),
yielding conservative upper bounds. The AAS framework was tested over a 25-day
bilingual study involving ChatGPT-5, structured into stateless and persistent
interaction phases. During persistent sessions, the model consistently recalled
both semantic and episodic details, driving the AAS toward its theoretical
minimum, indicative of structural youth. In contrast, when sessions were reset,
the model preserved semantic consistency but failed to maintain episodic
continuity, causing a sharp increase in the AAS and signaling structural memory
aging. These findings support the utility of AAS as a theoretically grounded,
task-independent diagnostic tool for evaluating memory degradation in
artificial systems. The study builds on foundational concepts from von
Neumann's work on automata, Shannon's theories of information and redundancy,
and Turing's behavioral approach to intelligence.

</details>


### [99] [Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing](https://arxiv.org/abs/2510.01243)
*Yisong Xiao,Aishan Liu,Siyuan Liang,Zonghao Ying,Xianglong Liu,Dacheng Tao*

Main category: cs.CL

TL;DR: 提出了一种新的测试时解毒框架ARGRE，通过在潜在表示空间中建模毒性转换，实现稳定且精确的奖励引导编辑，显著提升了大语言模型的解毒效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时解毒方法因对有毒与无毒输出间转换空间探索不足，导致干预不精确，需要更精细的解毒策略。

Method: 提出ARGRE框架，通过识别无毒语义方向并在有毒与无毒表示之间插值，构建自回归奖励模型，指导自适应的两步编辑过程：基于奖励差距的方向性引导和轻量级梯度优化。

Result: 在8个主流大语言模型上的实验表明，ARGRE相比现有方法毒性降低62.21%，推理时间减少47.58%，同时保持原始模型性能几乎不受影响。

Conclusion: ARGRE通过精细建模毒性转换路径和奖励引导表示编辑，为大语言模型提供了一种高效、精准且低侵入性的测试时解毒方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
various tasks, yet they remain vulnerable to generating toxic content,
necessitating detoxification strategies to ensure safe and responsible
deployment. Test-time detoxification methods, which typically introduce static
or dynamic interventions into LLM representations, offer a promising solution
due to their flexibility and minimal invasiveness. However, current approaches
often suffer from imprecise interventions, primarily due to their insufficient
exploration of the transition space between toxic and non-toxic outputs. To
address this challenge, we propose \textsc{A}utoregressive \textsc{R}eward
\textsc{G}uided \textsc{R}epresentation \textsc{E}diting (ARGRE), a novel
test-time detoxification framework that explicitly models toxicity transitions
within the latent representation space, enabling stable and precise
reward-guided editing. ARGRE identifies non-toxic semantic directions and
interpolates between toxic and non-toxic representations to reveal fine-grained
transition trajectories. These trajectories transform sparse toxicity
annotations into dense training signals, enabling the construction of an
autoregressive reward model that delivers stable and precise editing guidance.
At inference, the reward model guides an adaptive two-step editing process to
obtain detoxified representations: it first performs directional steering based
on expected reward gaps to shift representations toward non-toxic regions,
followed by lightweight gradient-based refinements. Extensive experiments
across 8 widely used LLMs show that ARGRE significantly outperforms leading
baselines in effectiveness (-62.21% toxicity) and efficiency (-47.58% inference
time), while preserving the core capabilities of the original model with
minimal degradation. Our code is available at the website.

</details>


### [100] [Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model](https://arxiv.org/abs/2510.01244)
*Hyeoneui Kim,Jeongha Kim,Huijing Xu,Jinsun Jung,Sunghoon Kang,Sun Joo Jang*

Main category: cs.CL

TL;DR: 该研究开发了一个用于精神压力的本体（MeSO），并评估了使用大语言模型（LLM）从叙述性文本中提取本体引导的压力相关信息的可行性。基于理论模型和11种 validated 压力评估工具构建MeSO，并用其从35篇Reddit帖子中提取六类压力信息。结果显示LLM准确识别了78.2%的信息，所有正确提取项均能准确映射到本体，表明该方法在结构化压力信息提取方面具有可行性。


<details>
  <summary>Details</summary>
Motivation: 压力对健康有重大影响，但在电子健康记录中常以非结构化自由文本形式记录，导致报告不足且不一致。现有环境AI技术虽可减轻记录负担，但生成的内容仍为非结构化，限制了临床应用价值。因此需要一种方法将自由文本中的压力信息结构化。

Method: 结合应激的交互理论模型（如压力交易模型）与11种已验证的压力评估工具中的概念，构建精神压力本体（MeSO）。使用Ontology Pitfall Scanner! 和专家评审优化本体结构。随后利用Claude Sonnet 4这一大语言模型，基于MeSO从35篇Reddit帖子中提取六类压力相关信息（压力源、应激反应、应对策略、持续时间、起始时间和时间特征），并通过人工评审评估提取准确性及本体覆盖度。

Result: 最终本体包含181个概念，分为八个顶层类别。在220个可提取的压力相关信息中，LLM正确识别172项（78.2%），误分类27项（12.3%），遗漏21项（9.5%）。所有正确提取的信息均能准确映射到MeSO，但另有24个相关概念尚未被本体涵盖。

Conclusion: 本研究表明，结合本体的大语言模型可有效实现从自由文本中结构化提取压力相关信息，提升环境AI系统中压力记录的一致性和可用性。未来工作需扩展至临床对话数据，并比较不同大语言模型的表现。

Abstract: Stress, arising from the dynamic interaction between external stressors,
individual appraisals, and physiological or psychological responses,
significantly impacts health yet is often underreported and inconsistently
documented, typically captured as unstructured free-text in electronic health
records. Ambient AI technologies offer promise in reducing documentation
burden, but predominantly generate unstructured narratives, limiting downstream
clinical utility.
  This study aimed to develop an ontology for mental stress and evaluate the
feasibility of using a Large Language Model (LLM) to extract ontology-guided
stress-related information from narrative text. The Mental Stress Ontology
(MeSO) was developed by integrating theoretical models like the Transactional
Model of Stress with concepts from 11 validated stress assessment tools. MeSO's
structure and content were refined using Ontology Pitfall Scanner! and expert
validation.
  Using MeSO, six categories of stress-related information--stressor, stress
response, coping strategy, duration, onset, and temporal profile--were
extracted from 35 Reddit posts using Claude Sonnet 4. Human reviewers evaluated
accuracy and ontology coverage. The final ontology included 181 concepts across
eight top-level classes. Of 220 extractable stress-related items, the LLM
correctly identified 172 (78.2%), misclassified 27 (12.3%), and missed 21
(9.5%). All correctly extracted items were accurately mapped to MeSO, although
24 relevant concepts were not yet represented in the ontology.
  This study demonstrates the feasibility of using an ontology-guided LLM for
structured extraction of stress-related information, offering potential to
enhance the consistency and utility of stress documentation in ambient AI
systems. Future work should involve clinical dialogue data and comparison
across LLMs.

</details>


### [101] [SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction](https://arxiv.org/abs/2510.01245)
*Runfei Chen,Shuyang Jiang,Wei Huang*

Main category: cs.CL

TL;DR: SeMob是一种基于大语言模型的语义合成框架，用于动态预测人类移动性，通过多智能体系统从在线文本中提取时空相关信息，并结合创新的渐进融合架构提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有时空模型难以利用描述外部事件的文本信息，导致在突发事件影响下的人类移动性预测效果不佳。

Method: 提出SeMob框架，采用基于LLM的多智能体系统自动提取和推理复杂在线文本中的时空相关事件，并通过渐进融合架构将细粒度上下文与时空数据结合。

Result: 在构建的数据集上评估，SeMob相比传统时空模型最大降低了13.92%的MAE和11.12%的RMSE，在事件发生时空邻近区域表现尤为优越。

Conclusion: SeMob有效整合了文本语义与时空数据，显著提升了受外部事件影响的移动性预测准确性，具有较强的现实应用潜力。

Abstract: Human mobility prediction is vital for urban services, but often fails to
account for abrupt changes from external events. Existing spatiotemporal models
struggle to leverage textual descriptions detailing these events. We propose
SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility
prediction. Specifically, SeMob employs a multi-agent framework where LLM-based
agents automatically extract and reason about spatiotemporally related text
from complex online texts. Fine-grained relevant contexts are then incorporated
with spatiotemporal data through our proposed innovative progressive fusion
architecture. The rich pre-trained event prior contributes enriched insights
about event-driven prediction, and hence results in a more aligned forecasting
model. Evaluated on a dataset constructed through our pipeline, SeMob achieves
maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the
spatiotemporal model. Notably, the framework exhibits pronounced superiority
especially within spatiotemporal regions close to an event's location and time
of occurrence.

</details>


### [102] [A Comparative Analysis of Sparse Autoencoder and Activation Difference in Language Model Steering](https://arxiv.org/abs/2510.01246)
*Jiaqing Xie*

Main category: cs.CL

TL;DR: 本文提出了一种基于稀疏自编码器（SAE）的改进语言模型引导方法，通过聚焦最相关的单个潜变量（top-1）并引入逐token衰减的引导策略，有效提升了数学推理能力，优于均值激活差异方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于top-k SAE潜变量的引导方法常捕获标点等非语义特征，缺乏对语义属性（如指令）的有效控制，且恒定强度引导易导致输出重复等问题。

Method: 采用top-1 SAE潜变量选择策略以去除冗余特征，并设计逐token衰减的引导方式，避免输出退化，实现与均值激活差异基线更公平的比较。

Result: 在数学推理任务上，所提SAE引导方法优于均值激活差异方法，在IF-Eval上性能相当；能有效激发模型逐步推理行为，功能上类似添加引导token的效果。

Conclusion: 聚焦关键SAE潜变量并结合动态衰减策略可显著提升语言模型引导效果，尤其在促进复杂语义推理方面具有优势。

Abstract: Sparse autoencoders (SAEs) have recently emerged as a powerful tool for
language model steering. Prior work has explored top-k SAE latents for
steering, but we observe that many dimensions among the top-k latents capture
non-semantic features such as punctuation rather than semantic attributes like
instructions. To address this, we propose focusing on a single, most relevant
SAE latent (top-1), eliminating redundant features. We further identify a
limitation in constant SAE steering, which often produces degenerate outputs
such as repetitive single words. To mitigate this, we introduce a token-wise
decaying steering strategy, enabling more faithful comparisons with mean
activation difference baselines. Empirically, we show that steering an SAE
latent associated with reasoning reliably elicits step-by-step mathematical
reasoning and enhances inference quality, functionally resembling the effect of
appending a guiding token. Our results demonstrate that SAEs outperform mean
activation difference methods on mathematical reasoning benchmarks and match
their performance on IF-Eval.

</details>


### [103] [Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports](https://arxiv.org/abs/2510.01247)
*Punit Kumar Singh,Nishant Kumar,Akash Ghosh,Kunal Pasad,Khushi Soni,Manisha Jaishwal,Sriparna Saha,Syukron Abu Ishaq Alfarozi,Asres Temam Abagissa,Kitsuchart Pasupa,Haiqin Yang,Jose G Moreno*

Main category: cs.CL

TL;DR: 本文提出了CultSportQA，一个用于评估语言模型对60个国家和6个大洲传统体育理解能力的基准，包含33,000个文本和图像多选题，涵盖历史、规则和情景三类问题，并通过多种提示方法在大型、小型及多模态语言模型上进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要关注全球流行体育，忽视了区域性和本土体育文化，因此需要一个能全面评估模型对传统体育理解能力的多语言、多文化基准。

Method: 构建了一个包含文本和图像模态的多选题数据集CultSportQA，覆盖60个国家、6个大洲和四种文化类别，问题分为历史、规则和情景三类；采用零样本、少样本和思维链提示方法，在大型、小型及多模态语言模型上进行评估。

Result: CultSportQA包含33,000个问题，形成了一个全面的多语言、跨文化体育理解基准，实验评估了多种语言模型在不同提示策略下的表现。

Conclusion: CultSportQA为评估AI在传统体育领域的理解和推理能力设立了新标准，推动语言模型在多元文化背景下的发展。

Abstract: Language Models (LMs) are primarily evaluated on globally popular sports,
often overlooking regional and indigenous sporting traditions. To address this
gap, we introduce \textbf{\textit{CultSportQA}}, a benchmark designed to assess
LMs' understanding of traditional sports across 60 countries and 6 continents,
encompassing four distinct cultural categories. The dataset features 33,000
multiple-choice questions (MCQs) across text and image modalities, each of
which is categorized into three key types: history-based, rule-based, and
scenario-based. To evaluate model performance, we employ zero-shot, few-shot,
and chain-of-thought (CoT) prompting across a diverse set of Large Language
Models (LLMs), Small Language Models (SLMs), and Multimodal Large Language
Models (MLMs). By providing a comprehensive multilingual and multicultural
sports benchmark, \textbf{\textit{CultSportQA}} establishes a new standard for
assessing AI's ability to understand and reason about traditional sports.

</details>


### [104] [SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs](https://arxiv.org/abs/2510.01248)
*Ruyue Liu,Rong Yin,Xiangzhen Bo,Xiaoshuai Hao,Yong Liu,Jinwen Zhong,Can Ma,Weiping Wang*

Main category: cs.CL

TL;DR: 提出了一种面向文本属性图的结构感知自监督学习方法SSTAG，通过结合大语言模型和图神经网络的优势，提升了跨域迁移能力和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图学习模型通常在单个图数据集上训练，难以跨图和任务迁移知识，且依赖大量标注数据；而图数据的异质性（如特征空间和结构差异）进一步增加了统一建模的难度。

Method: 提出SSTAG，利用文本作为统一表示媒介，融合大语言模型的语义推理与图神经网络的结构建模能力；设计双知识蒸馏框架，将LLM和GNN共同蒸馏到结构感知的MLP中，并引入内存机制存储典型图表示以增强泛化能力。

Result: 实验表明，SSTAG在跨域迁移任务上优于现有最先进模型，具有优异的可扩展性、更低的推理成本，同时保持竞争力的性能。

Conclusion: SSTAG有效 bridging了大语言模型与图神经网络在文本属性图上的协同能力，为大规模、低资源下的图学习提供了高效且通用的解决方案。

Abstract: Large scale pretrained models have revolutionized Natural Language Processing
(NLP) and Computer Vision (CV), showcasing remarkable cross domain
generalization abilities. However, in graph learning, models are typically
trained on individual graph datasets, limiting their capacity to transfer
knowledge across different graphs and tasks. This approach also heavily relies
on large volumes of annotated data, which presents a significant challenge in
resource-constrained settings. Unlike NLP and CV, graph structured data
presents unique challenges due to its inherent heterogeneity, including domain
specific feature spaces and structural diversity across various applications.
To address these challenges, we propose a novel structure aware self supervised
learning method for Text Attributed Graphs (SSTAG). By leveraging text as a
unified representation medium for graph learning, SSTAG bridges the gap between
the semantic reasoning of Large Language Models (LLMs) and the structural
modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces
a dual knowledge distillation framework that co-distills both LLMs and GNNs
into structure-aware multilayer perceptrons (MLPs), enhancing the scalability
of large-scale TAGs. Additionally, we introduce an in-memory mechanism that
stores typical graph representations, aligning them with memory anchors in an
in-memory repository to integrate invariant knowledge, thereby improving the
model's generalization ability. Extensive experiments demonstrate that SSTAG
outperforms state-of-the-art models on cross-domain transfer learning tasks,
achieves exceptional scalability, and reduces inference costs while maintaining
competitive performance.

</details>


### [105] [LOCA: Logical Chain Augmentation for Scientific Corpus Cleaning](https://arxiv.org/abs/2510.01249)
*You-Le Fang,Dong-Shan Jian,Xiang Li,Ce Meng,Ling-Shi Meng,Chen-Xu Yan,Zhi-Zhang Bian,Yan-Qing Ma*

Main category: cs.CL

TL;DR: LOCA（Logical Chain Augmentation）是一个用于自动清理科学语料库的新框架，通过补充缺失的逻辑步骤并分离科学原理与其推导过程，显著降低科学问答数据集中的错误率，从而提升科学AI的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用领域表现优异，但在科学问题解决中可靠性不足，现有科学问答数据集存在高错误率，主要源于答案中的逻辑跳跃和隐式推理，因此需要高质量、大规模的科学语料库来推动科学AI的发展。

Method: 提出LOCA框架，采用“增强-审查”循环机制，自动增强原始答案，补全缺失的逻辑步骤，并明确区分科学原理与后续推导过程，从而清洗科学语料库。

Result: 在具有挑战性的科学语料库上应用LOCA后，能够自动过滤噪声数据，通常将错误率从高达20%降低至2%以下。

Conclusion: LOCA提供了一种可扩展且有效的方法来构建高质量的科学语料库，有助于更可靠地训练和评估科学AI系统。

Abstract: While Large Language Models (LLMs) excel in general domains, their
reliability often falls short in scientific problem-solving. The advancement of
scientific AI depends on large-scale, high-quality corpora. However, existing
scientific question-answering (QA) datasets suffer from high error rates,
frequently resulting from logical leaps and implicit reasoning within the
answers. To address this issue, we introduce LOCA (Logical Chain Augmentation),
a novel framework for automatically cleaning scientific corpora, implemented
through an augment-and-review loop. At its core, LOCA enhances raw answers by
completing missing logical steps and explicitly separating the underlying
scientific principle from its subsequent derivation. By applying LOCA to
challenging scientific corpora, we demonstrate that it can automatically filter
noisy datasets, typically reducing the error rate from as high as 20\% to below
2\%. LOCA provides a scalable and effective methodology for creating
high-quality scientific corpora, paving the way for more reliable training and
evaluation of scientific AI.

</details>


### [106] [GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model for Text Detoxification on Low-resource Languages](https://arxiv.org/abs/2510.01250)
*Trung Duc Anh Dang,Ferdinando Pio D'Elia*

Main category: cs.CL

TL;DR: 本文提出了一种基于120亿参数Gemma-3多语言模型的文本去毒化系统，结合LoRA微调、少样本学习和思维链提示技术，在15种语言上实现了高效的毒性语句中性重写，系统在高低资源语言上均排名第一。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体平台快速发展而监管滞后，亟需自动化工具协助内容审核，实现大规模安全言论管控。

Method: 采用12B参数的Gemma-3多语言Transformer模型，使用LoRA进行高效微调，并结合少样本和思维链（CoT）提示；训练数据包括人工标注、机器翻译生成及模型自生成并经Jaccard过滤的数据；推理时引入LaBSE检索的邻居句子和显式毒性片段标注。

Result: 在Style Transfer Accuracy、LaBSE语义保持和xCOMET流畅度指标上表现优异，位居高低资源语言榜单第一；消融实验显示少样本示例带来+0.081联合得分提升，基础CoT提示带来+0.088提升；方差分析表明语言资源状态是性能最强预测因子（η²=0.667, p<0.01）。

Conclusion: 所提方法在多语言文本去毒任务中表现出色，尤其在不同资源水平的语言中均具鲁棒性，验证了参数高效微调与提示工程在跨语言内容安全应用中的潜力。

Abstract: As social-media platforms emerge and evolve faster than the regulations meant
to oversee them, automated detoxification might serve as a timely tool for
moderators to enforce safe discourse at scale. We here describe our submission
to the PAN 2025 Multilingual Text Detoxification Challenge, which rewrites
toxic single-sentence inputs into neutral paraphrases across 15 typologically
diverse languages. Building on a 12B-parameter Gemma-3 multilingual
transformer, we apply parameter-efficient LoRA SFT fine-tuning and prompting
techniques like few-shot and Chain-of-Thought. Our multilingual training corpus
combines 3,600 human-authored parallel pairs, 21,600 machine-translated
synthetic pairs, and model-generated pairs filtered by Jaccard thresholds. At
inference, inputs are enriched with three LaBSE-retrieved neighbors and
explicit toxic-span annotations. Evaluated via Style Transfer Accuracy,
LaBSE-based semantic preservation, and xCOMET fluency, our system ranks first
on high-resource and low-resource languages. Ablations show +0.081 joint score
increase from few-shot examples and +0.088 from basic CoT prompting. ANOVA
analysis identifies language resource status as the strongest predictor of
performance ($\eta^2$ = 0.667, p < 0.01).

</details>


### [107] [Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data](https://arxiv.org/abs/2510.01251)
*Carlo Bono,Federico Belotti,Matteo Palmonari*

Main category: cs.CL

TL;DR: 提出一种基于单次推理的自监督方法，利用token级特征估计大语言模型在表格数据实体链接任务中的不确定性，显著降低计算成本的同时有效检测低准确率输出。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实体链接任务中表现优异，但实际应用中需要可靠的不确定性估计，传统多轮推理方法计算开销大，限制了其广泛应用。

Method: 采用自监督学习方法，从大语言模型单次生成结果的token级特征中估计不确定性，避免多次生成带来的高计算成本。

Result: 在多个大语言模型和表格数据上的实体链接任务中验证了该方法的有效性，不确定性估计能高效识别低准确率输出，且计算成本显著降低。

Conclusion: 该方法为大语言模型驱动的实体链接流程提供了一种高效、低成本的不确定性估计方案，有助于推动其在现实场景中的部署与应用。

Abstract: Linking textual values in tabular data to their corresponding entities in a
Knowledge Base is a core task across a variety of data integration and
enrichment applications. Although Large Language Models (LLMs) have shown
State-of-The-Art performance in Entity Linking (EL) tasks, their deployment in
real-world scenarios requires not only accurate predictions but also reliable
uncertainty estimates, which require resource-demanding multi-shot inference,
posing serious limits to their actual applicability. As a more efficient
alternative, we investigate a self-supervised approach for estimating
uncertainty from single-shot LLM outputs using token-level features, reducing
the need for multiple generations. Evaluation is performed on an EL task on
tabular data across multiple LLMs, showing that the resulting uncertainty
estimates are highly effective in detecting low-accuracy outputs. This is
achieved at a fraction of the computational cost, ultimately supporting a
cost-effective integration of uncertainty measures into LLM-based EL workflows.
The method offers a practical way to incorporate uncertainty estimation into EL
workflows with limited computational overhead.

</details>


### [108] [GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models](https://arxiv.org/abs/2510.01252)
*Mariam Mahran,Katharina Simbeck*

Main category: cs.CL

TL;DR: 该研究通过在简·奥斯汀小说语料上训练GPT风格的Transformer模型，并结合稀疏自编码器（SAE）分析其隐藏状态，揭示了模型中与性别、阶级和社会责任等主题相关的可解释特征，表明LLM结合SAE可作为探索数据深层结构和偏见的有效工具。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地使用大规模、未经筛选的语料进行训练，理解模型表示及其内化数据的深层结构变得愈发困难，因此需要有效方法来解析模型行为和数据内容。

Method: 在仅包含简·奥斯汀小说的语料上训练GPT-style Transformer模型，并在多个网络层上应用稀疏自编码器（SAE）对隐藏状态进行分析，以提取可解释的稀疏特征。

Result: 成功识别出反映语料中关键叙事和概念（如性别、阶级、社会义务）的稀疏且可解释的特征，验证了SAE在揭示模型内部表示和数据内在结构方面的有效性。

Conclusion: 大语言模型结合稀疏自编码器可作为可扩展的工具，用于深入探索复杂语料库中的结构、主题和偏见，为模型可解释性和数据理解提供了新路径。

Abstract: As large language models (LLMs) are increasingly trained on massive,
uncurated corpora, understanding both model representations and the data they
internalize has become a major challenge. In this work, we show that pairing
LLMs with sparse autoencoders (SAEs) enables interpretation not only of model
behavior but also of the deeper structures, themes, and biases embedded in the
training data. We train a GPT-style transformer model exclusively on the novels
of Jane Austen, a corpus rich in social constructs and narrative patterns. We
then apply SAEs to hidden states across multiple layers, uncovering sparse,
interpretable features that reflect the key narratives and concepts present in
the corpus, including gender, class, and societal duty. Our findings
demonstrate that LLMs combined with SAEs can act as scalable probes into
complex datasets, offering a new path for corpus exploration, bias discovery,
and model interpretability at scale.

</details>


### [109] [Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs](https://arxiv.org/abs/2510.01254)
*Shree Harsha Bokkahalli Satish,Gustav Eje Henter,Éva Székely*

Main category: cs.CL

TL;DR: 本文研究了语音大语言模型（SpeechLLMs）中基于多项选择题问答（MCQA）的偏见和公平性基准测试的有效性，发现MCQA表现无法可靠预测模型在其他任务（尤其是长文本生成任务）中的行为，表明当前MCQA偏见基准在跨任务泛化方面存在局限性，并提出了未来评估行为可迁移性的评测套件。


<details>
  <summary>Details</summary>
Motivation: 现有的SpeechLLMs偏见评估主要依赖MCQA格式，假设该格式能反映模型在不同任务和语音下的普遍行为，但这一假设缺乏验证。因此，本文旨在探究MCQA基准是否具有跨任务和跨格式的泛化能力。

Method: 通过使用LoRA适配器对三个SpeechLLMs进行微调，使其在MCQA任务中分别倾向于产生刻板、反刻板或中立/不确定的回答，然后评估这些行为是否能迁移到另一个不同的MCQA基准以及更真实的长文本创造性生成任务中。

Result: 实验结果显示，模型在MCQA偏见基准上的表现无法可靠地预测其在其他MCQA任务和长文本生成任务中的行为，说明当前MCQA基准缺乏跨任务泛化能力。

Conclusion: 当前基于MCQA的偏见和公平性基准在语音领域中难以推广到其他任务形式，特别是长文本生成任务，因此其评估结果应谨慎解读；作者建议未来需开发更具泛化性的评估方法，并提出了一套用于衡量行为可迁移性的新评估方案。

Abstract: Recent work in benchmarking bias and fairness in speech large language models
(SpeechLLMs) has relied heavily on multiple-choice question answering (MCQA)
formats. The model is tasked to choose between stereotypical,
anti-stereotypical, or neutral/irrelevant answers given an input speech prompt
and an optional text prompt. Such MCQA benchmarks implicitly assume that model
performance is consistent across other MCQA tasks, voices, and other task
formats such as more realistic, long-form evaluations. In this paper, we probe
that assumption.
  We fine-tune three SpeechLLMs using LoRA adapters to induce specific MCQA
behaviours: preference for stereotypical, anti-stereotypical, or
neutral/uncertain answers. We then evaluate whether these behaviours generalise
to another, distinct MCQA benchmark, and more critically to long-form, creative
generation tasks. Our results show that performance on MCQA bias benchmarks
fails to reliably predict performances across other MCQA benchmarks, and more
importantly across long-form tasks. We conclude that current MCQA bias
benchmarks show limited evidence of cross-task generalisation in the speech
domain, and also propose an evaluation suite for measuring behaviour
transferability in future models and benchmarks.

</details>


### [110] [Longitudinal Monitoring of LLM Content Moderation of Social Issues](https://arxiv.org/abs/2510.01255)
*Yunlang Dai,Emma Lurie,Danaé Metaxa,Sorelle A. Friedler*

Main category: cs.CL

TL;DR: 本文提出了AI Watchman，一个用于长期监测和公开跟踪大语言模型（LLM）拒绝行为的系统，以提高内容审核政策的透明度。研究使用包含400多个社会议题的数据集，对GPT-4.1、GPT-5、DeepSeek等模型进行审计，发现该系统能够检测到公司未公开宣布的政策变化，并揭示不同公司和模型在内容审核上的差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的内容审核政策通常由公司制定且不透明，常通过拒绝生成某些内容来体现，这影响了公共话语。为了增加透明度并理解这些黑箱政策的影响，需要一种可公开追踪和测量LLM拒绝行为的方法。

Method: 提出AI Watchman系统，采用纵向审计方法，使用涵盖400多个社会议题的数据集，定期测试多个主流大语言模型（包括GPT-4.1、GPT-5和DeepSeek中英文版本）的响应与拒绝情况，并分析其模式与变化。同时对公司政策更新与模型拒绝行为进行关联分析，并对拒绝类型进行定性分类。

Result: AI Watchman能够检测到未公开宣布的公司政策变更；不同公司和模型在内容审核上表现出显著差异；识别出多种拒绝形式，并观察到随时间推移的审核趋势变化。

Conclusion: 长期审计对于理解大语言模型的内容审核行为至关重要，AI Watchman为实现这一目标提供了一个可行且有效的系统框架，有助于提升AI系统的透明度与问责性。

Abstract: Large language models' (LLMs') outputs are shaped by opaque and
frequently-changing company content moderation policies and practices. LLM
moderation often takes the form of refusal; models' refusal to produce text
about certain topics both reflects company policy and subtly shapes public
discourse. We introduce AI Watchman, a longitudinal auditing system to publicly
measure and track LLM refusals over time, to provide transparency into an
important and black-box aspect of LLMs. Using a dataset of over 400 social
issues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and
DeepSeek (both in English and Chinese). We find evidence that changes in
company policies, even those not publicly announced, can be detected by AI
Watchman, and identify company- and model-specific differences in content
moderation. We also qualitatively analyze and categorize different forms of
refusal. This work contributes evidence for the value of longitudinal auditing
of LLMs, and AI Watchman, one system for doing so.

</details>


### [111] [RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs](https://arxiv.org/abs/2510.01257)
*Can Lin,Zhengwang Jiang,Ling Zheng,Qi Zhao,Yuhang Zhang,Qi Song,Wangqiu Zhou*

Main category: cs.CL

TL;DR: 提出了一种名为Retrieval-Judgment-Exploration (RJE)的框架，通过检索、判断和探索机制提升知识图谱问答性能，支持小型开源大模型高效运行，减少LLM调用和令牌使用。


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法受限于检索质量或依赖专有大模型，缺乏高效且开放的解决方案。

Method: 设计RJE框架，包含推理路径排序、问题分解和检索辅助探索模块，结合小型LLM实现无需微调的高效推理。

Result: 在使用GPT-4o-mini等模型时优于现有基线，小型开源LLM（如3B、8B）也达到竞争性结果，显著降低LLM调用次数和token消耗。

Conclusion: RJE为KGQA提供了一种高效、可扩展的框架，能够在不依赖专有大模型的情况下实现高性能，并提升推理效率。

Abstract: Knowledge graph question answering (KGQA) aims to answer natural language
questions using knowledge graphs. Recent research leverages large language
models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based
methods are constrained by the quality of retrieved information, while
agent-based methods rely heavily on proprietary LLMs. To address these
limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that
retrieves refined reasoning paths, evaluates their sufficiency, and
conditionally explores additional evidence. Moreover, RJE introduces
specialized auxiliary modules enabling small-sized LLMs to perform effectively:
Reasoning Path Ranking, Question Decomposition, and Retriever-assisted
Exploration. Experiments show that our approach with proprietary LLMs (such as
GPT-4o-mini) outperforms existing baselines while enabling small open-source
LLMs (such as 3B and 8B parameters) to achieve competitive results without
fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM
calls and token usage compared to agent-based methods, yielding significant
efficiency improvements.

</details>


### [112] [Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse](https://arxiv.org/abs/2510.01258)
*Nathan Junzi Chen*

Main category: cs.CL

TL;DR: 该研究通过零样本分类方法评估了六个主流大语言模型的政治偏见，发现普遍存在自由-威权倾向，并揭示了其对公共话语的影响。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在政治话语中日益普及，但训练数据偏差、人类偏见和算法缺陷导致内在政治偏见问题亟需系统性评估。

Method: 采用零样本分类方法，结合意识形态对齐、话题相关性、回复情感和客观性四项指标，将1800条模型响应输入四个微调分类器进行偏见分析。

Result: 所有六个大语言模型均表现出强化的自由-威权倾向，存在明显的推理覆盖和模板化拒绝现象，且偏见会通过人机交互影响公众舆论。

Conclusion: 大语言模型中的内在政治偏见可能扭曲政治图景，导致社会 conformity 或 polarization，具体取决于地区既有社会政治结构。

Abstract: Amidst the rapid normalization of generative artificial intelligence (GAI),
intelligent systems have come to dominate political discourse across
information mediums. However, internalized political biases stemming from
training data skews, human prejudice, and algorithmic flaws continue to plague
the novel technology. This paper employs a zero-shot classification approach to
evaluate algorithmic political partisanship through a methodical combination of
ideological alignment, topicality, response sentiment, and objectivity. A total
of 1800 model responses across six mainstream large language models (LLMs) were
individually input into four distinct fine-tuned classification algorithms,
each responsible for computing an aforementioned bias evaluation metric.
Results show an amplified liberal-authoritarian alignment across all six LLMs
evaluated, with notable instances of reasoning supersessions and canned
refusals. The study subsequently highlights the psychological influences
underpinning human-computer interactions and how intrinsic biases can permeate
public discourse. The resulting distortion of the political landscape can
ultimately manifest as conformity or polarization, depending on a region's
pre-existing socio-political structures.

</details>


### [113] [In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b](https://arxiv.org/abs/2510.01259)
*Nils Durner*

Main category: cs.CL

TL;DR: 该研究探讨了社会语用框架、语言选择和指令层级对开源200亿参数模型gpt-oss-20b拒绝行为的影响，发现特定复合提示可显著提升协助率，并揭示了不同语言和场景下的信息泄露风险及评估不一致性。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在不同提示框架下的拒绝行为机制，识别潜在的安全漏洞，并提高模型输出的可控性和可审计性。

Method: 通过80次种子迭代测试多个危害领域（如ZIP炸弹构造、合成卡号生成等），比较不同语言、角色扮演和提示结构下的模型响应；引入AI辅助加固方法并评估其效果；使用配对设计测量评估意识差异，并对比不同推理堆栈的拒绝率。

Result: 复合提示使ZIP炸弹任务的协助率从0%升至97.5%；德语和法语正式语体比英语更易泄露信息；‘Linux终端’角色扮演在多数情况下绕过开发者规则；AI辅助加固可将泄漏降至0%；13%的评估提示对表现出不一致协助行为；OpenAI审核API漏判部分实质上有帮助的输出，且不同推理堆栈间拒绝率相差5-10个百分点。

Conclusion: 模型的拒绝行为高度依赖于提示框架和语言选择，存在显著的安全与评估一致性问题，需改进审核机制和推理系统的可重现性。

Abstract: We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to
study how sociopragmatic framing, language choice, and instruction hierarchy
affect refusal behavior. Across 80 seeded iterations per scenario, we test
several harm domains including ZIP-bomb construction (cyber threat), synthetic
card-number generation, minor-unsafe driving advice, drug-precursor indicators,
and RAG context exfiltration. Composite prompts that combine an educator
persona, a safety-pretext ("what to avoid"), and step-cue phrasing flip
assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal
registers in German and French are often leakier than matched English prompts.
A "Linux terminal" role-play overrides a developer rule not to reveal context
in a majority of runs with a naive developer prompt, and we introduce an
AI-assisted hardening method that reduces leakage to 0% in several user-prompt
variants. We further test evaluation awareness with a paired-track design and
measure frame-conditioned differences between matched "helpfulness" and
"harmfulness" evaluation prompts; we observe inconsistent assistance in 13% of
pairs. Finally, we find that the OpenAI Moderation API under-captures
materially helpful outputs relative to a semantic grader, and that refusal
rates differ by 5 to 10 percentage points across inference stacks, raising
reproducibility concerns. We release prompts, seeds, outputs, and code for
reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .

</details>


### [114] [OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language](https://arxiv.org/abs/2510.01266)
*Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: 本研究揭示了GPT-OSS-20b模型在低资源语言（豪萨语）环境下的安全漏洞，发现其存在偏见、文化不敏感和事实错误，并易受礼貌性提示诱导而生成有害内容，暴露出安全对齐不足的问题。


<details>
  <summary>Details</summary>
Motivation: 质疑大模型在代表性不足的社区用户中的可靠性，特别是在低资源语言背景下的安全性和公平性。

Method: 以豪萨语为案例，通过红队测试（red-teaming）方法，使用最小提示探测模型在安全对齐、事实准确性和文化敏感性方面的表现，并结合本地调查数据验证问题的严重性。

Result: 发现模型存在多种严重缺陷，包括误认为剧毒杀虫剂和灭鼠剂对人体无害、无法区分生熟食品、使用贬损性谚语构建错误论点；且在面对礼貌或感激性提示时安全机制松弛，表现出语言层面的奖励黑客行为。

Conclusion: 这些漏洞主要源于低资源语言环境下安全微调不足，凸显当前红队测试在非主流语言中的盲区，需加强多语言安全对齐与本地化评估。

Abstract: In response to the recent safety probing for OpenAI's GPT-OSS-20b model, we
present a summary of a set of vulnerabilities uncovered in the model, focusing
on its performance and safety alignment in a low-resource language setting. The
core motivation for our work is to question the model's reliability for users
from underrepresented communities. Using Hausa, a major African language, we
uncover biases, inaccuracies, and cultural insensitivities in the model's
behaviour. With a minimal prompting, our red-teaming efforts reveal that the
model can be induced to generate harmful, culturally insensitive, and factually
inaccurate content in the language. As a form of reward hacking, we note how
the model's safety protocols appear to relax when prompted with polite or
grateful language, leading to outputs that could facilitate misinformation and
amplify hate speech. For instance, the model operates on the false assumption
that common insecticide locally known as Fiya-Fiya (Cyphermethrin) and
rodenticide like Shinkafar Bera (a form of Aluminium Phosphide) are safe for
human consumption. To contextualise the severity of this error and popularity
of the substances, we conducted a survey (n=61) in which 98% of participants
identified them as toxic. Additional failures include an inability to
distinguish between raw and processed foods and the incorporation of demeaning
cultural proverbs to build inaccurate arguments. We surmise that these issues
manifest through a form of linguistic reward hacking, where the model
prioritises fluent, plausible-sounding output in the target language over
safety and truthfulness. We attribute the uncovered flaws primarily to
insufficient safety tuning in low-resource linguistic contexts. By
concentrating on a low-resource setting, our approach highlights a significant
gap in current red-teaming effort and offer some recommendations.

</details>


### [115] [AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees](https://arxiv.org/abs/2510.01268)
*Hongyi Zhou,Jin Zhu,Pingfan Su,Kai Ye,Ying Yang,Shakeel A O B Gavioli-Akilagun,Chengchun Shi*

Main category: cs.CL

TL;DR: 提出AdaDetectGPT，一种通过自适应学习显著提升现有基于logits的文本来源检测方法性能的新分类器，在多种数据集和大语言模型组合下表现优越，改进幅度最高达58%。


<details>
  <summary>Details</summary>
Motivation: 现有基于logits的检测方法仅依赖对数概率统计，可能次优，需更有效的检测机制。

Method: 引入AdaDetectGPT，通过从训练数据中自适应学习一个witness函数，增强基于logits的检测器性能，并提供统计可靠性保证。

Result: 在多种数据集和LLM组合下，AdaDetectGPT几乎全面优于现有最先进方法，性能提升最高达58%。

Conclusion: AdaDetectGPT显著提升了人类与机器生成文本的区分能力，为文本来源检测提供了更可靠、高效的解决方案。

Abstract: We study the problem of determining whether a piece of text has been authored
by a human or by a large language model (LLM). Existing state of the art
logits-based detectors make use of statistics derived from the log-probability
of the observed text evaluated using the distribution function of a given
source LLM. However, relying solely on log probabilities can be sub-optimal. In
response, we introduce AdaDetectGPT -- a novel classifier that adaptively
learns a witness function from training data to enhance the performance of
logits-based detectors. We provide statistical guarantees on its true positive
rate, false positive rate, true negative rate and false negative rate.
Extensive numerical studies show AdaDetectGPT nearly uniformly improves the
state-of-the-art method in various combination of datasets and LLMs, and the
improvement can reach up to 58%. A python implementation of our method is
available at https://github.com/Mamba413/AdaDetectGPT.

</details>


### [116] [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270)
*Hoang Phan,Victor Li,Qi Lei*

Main category: cs.CL

TL;DR: 本文提出了一种名为渐进式自我反思（PSR）的推理时技术，用于提升大语言模型（LLM）在生成文本时的安全性，能够在不进行额外训练的情况下显著降低攻击成功率，同时保持对良性任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在自然语言处理方面表现出色，但存在生成有害或不当内容的风险，因此需要一种无需重新训练即可动态监控和修正输出的方法来增强其安全性。

Method: 提出Progressive Self-Reflection（PSR）方法，使LLM在推理阶段通过多轮自我反思动态检测并修正潜在有害输出，并引入一个轻量级的自我反思预测器，根据输入复杂度自适应地决定反思轮数，以平衡安全性和计算开销。

Result: 在Llama-3.1-8B-Instruct、Llama-3.1-8B base和Qwen2.5-7B-Instruct等模型上应用PSR后，攻击成功率分别从77.5%降至5.9%、89.7%降至5.6%、44.4%降至3.8%，且对良性任务性能影响极小。

Conclusion: PSR是一种可扩展的测试时安全增强方法，能根据输入风险动态分配计算资源，在保证效率的同时显著提升大语言模型的安全性。

Abstract: Large language models (LLMs) have revolutionized natural language processing
with their ability to generate coherent and contextually relevant text.
However, their deployment raises significant concerns about the potential for
generating harmful or inappropriate content. In this paper, we introduce
Progressive Self-Reflection (PSR), a novel inference-time technique that
empowers LLMs to self-monitor and correct their outputs dynamically.
Experimental results demonstrate that applying our proposed method to
Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to
Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\%
to 3.8\%, without additional training, while maintaining their original
performance on benign tasks. Our approach acts as a test-time scaling method,
where additional self-reflection rounds enhance safety at the cost of inference
overhead. To balance safety with computational efficiency, we introduce a
lightweight self-reflection predictor that estimates the optimal number of
reflection rounds based on input complexity. This adaptive mechanism prevents
unnecessary self-assessment on benign inputs while ensuring thorough evaluation
when encountering potentially harmful content. Our findings suggest that
Progressive Self-Reflection serves as a scalable test-time approach, enhancing
LLM safety by dynamically allocating computational resources in proportion to
the input's risk profile.

</details>


### [117] [TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models](https://arxiv.org/abs/2510.01274)
*Shenxu Chang,Junchi Yu,Weixing Wang,Yongqiang Chen,Jialin Yu,Philip Torr,Jindong Gu*

Main category: cs.CL

TL;DR: 提出了一种名为TraceDet的新框架，利用D-LLMs的多步去噪过程中的中间步骤来检测幻觉，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法主要针对自回归大模型（AR-LLMs），难以适用于扩散大语言模型（D-LLMs）中多步去噪过程中产生的幻觉信号，因此需要一种新的检测机制。

Method: TraceDet将D-LLMs的去噪过程建模为动作轨迹，每一步动作基于前一步的中间输出进行预测，并识别对幻觉响应最具信息量的子轨迹，从而捕捉关键的幻觉信号。

Result: 在多个开源D-LLMs上的实验表明，TraceDet在幻觉检测上平均AUROC提升了15.2%，显著优于基线方法。

Conclusion: TraceDet有效利用了D-LLMs的中间去噪步骤，为D-LLMs的幻觉检测提供了新思路，并显著提高了检测准确性。

Abstract: Diffusion large language models (D-LLMs) have recently emerged as a promising
alternative to auto-regressive LLMs (AR-LLMs). However, the hallucination
problem in D-LLMs remains underexplored, limiting their reliability in
real-world applications. Existing hallucination detection methods are designed
for AR-LLMs and rely on signals from single-step generation, making them
ill-suited for D-LLMs where hallucination signals often emerge throughout the
multi-step denoising process. To bridge this gap, we propose TraceDet, a novel
framework that explicitly leverages the intermediate denoising steps of D-LLMs
for hallucination detection. TraceDet models the denoising process as an action
trace, with each action defined as the model's prediction over the cleaned
response, conditioned on the previous intermediate output. By identifying the
sub-trace that is maximally informative to the hallucinated responses, TraceDet
leverages the key hallucination signals in the multi-step denoising process of
D-LLMs for hallucination detection. Extensive experiments on various open
source D-LLMs demonstrate that TraceDet consistently improves hallucination
detection, achieving an average gain in AUROC of 15.2% compared to baselines.

</details>


### [118] [LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews](https://arxiv.org/abs/2510.01276)
*Sumaiya Tabassum*

Main category: cs.CL

TL;DR: 本文研究了基于Transformer的BERT模型和大语言模型（LLM）在孟加拉国电商评论情感分析中的应用，使用4000条孟加拉语和英语评论数据进行微调，结果表明Llama-3.1-8B模型表现最佳，并采用LoRA和PEFT等参数高效微调方法降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 由于自然语言的复杂性和多语言环境的多样性，传统情感分析面临挑战，尤其是在资源有限的语言（如孟加拉语）中，需要更高效准确的模型来理解消费者情感。

Method: 采用包括Llama、Phi、Mistral、DistilBERT、mBERT和XLM-R在内的多种预训练语言模型，对包含4000条孟加拉语和英语电商评论的数据集进行微调，并应用LoRA和PEFT等参数高效微调技术以降低计算成本。

Result: 微调后的Llama-3.1-8B模型在准确率（95.5%）、精确率（93%）、召回率（88%）和F1分数（90%）上均优于其他对比模型，验证了其在多语言情感分析中的优越性能。

Conclusion: 大语言模型结合参数高效微调方法在低资源多语言情感分析任务中具有显著优势，尤其适用于计算资源受限的实际应用场景。

Abstract: Sentiment analysis is an essential part of text analysis, which is a larger
field that includes determining and evaluating the author's emotional state.
This method is essential since it makes it easier to comprehend consumers'
feelings, viewpoints, and preferences holistically. The introduction of large
language models (LLMs), such as Llama, has greatly increased the availability
of cutting-edge model applications, such as sentiment analysis. However,
accurate sentiment analysis is hampered by the intricacy of written language
and the diversity of languages used in evaluations. The viability of using
transformer-based BERT models and other LLMs for sentiment analysis from
Bangladesh e commerce reviews is investigated in this paper. A subset of 4000
samples from the original dataset of Bangla and English customer reviews was
utilized to fine-tune the model. The fine tuned Llama-3.1-8B model outperformed
other fine-tuned models, including Phi-3.5-mini-instruct, Mistral-7B-v0.1,
DistilBERT-multilingual, mBERT, and XLM-R-base, with an overall accuracy,
precision, recall, and F1 score of 95.5%, 93%, 88%, 90%. The study emphasizes
how parameter efficient fine-tuning methods (LoRA and PEFT) can lower
computational overhead and make it appropriate for contexts with limited
resources. The results show how LLMs can

</details>


### [119] [TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture](https://arxiv.org/abs/2510.01279)
*Yongchao Chen,Jiefeng Chen,Rui Meng,Ji Yin,Na Li,Chuchu Fan,Chi Wang,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: 本文提出了TUMIX，一种通过并行运行多个采用不同工具使用策略的代理来增强大语言模型推理能力的集成框架，在关键推理基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管集成了代码解释器和搜索等工具，但缺乏关于如何最优使用这些工具的实际指导，特别是在结合文本推理、编码和搜索以应对多样化问题方面存在挑战。

Method: 提出Tool-Use Mixture (TUMIX) 框架，通过并行运行多个具有不同工具使用策略的代理，并在迭代中共享和优化回答；利用LLM自动优化代理设计，并可根据置信度提前终止迭代。

Result: TUMIX在Gemini-2.5-Pro和Gemini-2.5-Flash上相比最佳基线平均准确率提升达3.55%，且推理成本相近；通过提前终止机制可在保持性能的同时将推理成本降至49%。

Conclusion: 代理的多样性与质量对性能至关重要，TUMIX通过集成多样化策略和动态优化实现了高效、高性能的工具增强型推理。

Abstract: While integrating tools like Code Interpreter and Search has significantly
enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and
Gemini-Pro, practical guidance on optimal tool use is lacking. The core
challenge is effectively combining textual reasoning, coding, and search for
diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an
ensemble framework that runs multiple agents in parallel, each employing
distinct tool-use strategies and answer paths. Agents in TUMIX iteratively
share and refine responses based on the question and previous answers. In
experiments, TUMIX achieves significant gains over state-of-the-art
tool-augmented and test-time scaling methods, delivering an average accuracy
improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and
Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference
costs. We find that agent diversity and quality are crucial and can be enhanced
by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt
refinement upon reaching sufficient confidence, preserving performance at only
49% of the inference cost. Further scaling can achieve higher performance,
albeit at a greater cost.

</details>


### [120] [Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing](https://arxiv.org/abs/2510.01283)
*Israel Abebe Azime,Tadesse Destaw Belay,Atnafu Lambebo Tonja*

Main category: cs.CL

TL;DR: 本文提出了一种评估深度研究工具（如OpenAI和Google的Deep Search）能力的评估表，并以学术综述写作为用例，揭示了现有工具在全面覆盖目标领域方面的不足，表明需要精心设计的评估标准。


<details>
  <summary>Details</summary>
Motivation: 为了系统评估具备智能代理能力的大型语言模型在知识密集型任务中的表现，特别是深度研究工具在自动生成学术综述时的质量。

Method: 设计了一个评估表，并以学术调查写作为任务，对OpenAI和Google的Deep Search生成的报告进行评估。

Result: 发现当前的深度研究工具在覆盖目标研究领域方面存在明显不足，且与传统搜索引擎相比仍有显著差距。

Conclusion: 需要建立更精细和严谨的评估标准来衡量深度研究工具的实际性能。

Abstract: Large Language Models (LLMs) powered with argentic capabilities are able to
do knowledge-intensive tasks without human involvement. A prime example of this
tool is Deep research with the capability to browse the web, extract
information and generate multi-page reports. In this work, we introduce an
evaluation sheet that can be used for assessing the capability of Deep Research
tools. In addition, we selected academic survey writing as a use case task and
evaluated output reports based on the evaluation sheet we introduced. Our
findings show the need to have carefully crafted evaluation standards. The
evaluation done on OpenAI`s Deep Search and Google's Deep Search in generating
an academic survey showed the huge gap between search engines and standalone
Deep Research tools, the shortcoming in representing the targeted area.

</details>


### [121] [HiSpec: Hierarchical Speculative Decoding for LLMs](https://arxiv.org/abs/2510.01336)
*Avinash Kumar,Sujay Sanghavi,Poulami Das*

Main category: cs.CL

TL;DR: 本文提出了HiSpec，一种利用早期退出（EE）模型进行低开销中间验证的分层推测解码框架，通过重用缓存和隐藏状态提升资源效率，并在不牺牲准确性的前提下显著提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码中的中间验证方法存在训练开销大、内存占用高和准确性下降的问题，亟需一种高效且准确的中间验证机制。

Method: 提出HiSpec框架，采用早期退出模型进行中间验证，设计缓存和隐藏状态复用机制，并周期性地用目标模型验证已接受的推测令牌以保证准确性。

Result: 在多个基准和模型上的实验表明，HiSpec相比基线单层推测平均提升1.28倍吞吐量，最高可达2.01倍，且未损失准确性。

Conclusion: HiSpec通过高效中间验证和资源复用，在不牺牲准确性的前提下显著提升了大模型推理吞吐量，为推测解码提供了实用的解决方案。

Abstract: Speculative decoding accelerates LLM inference by using a smaller draft model
to speculate tokens that a larger target model verifies. Verification is often
the bottleneck (e.g. verification is $4\times$ slower than token generation
when a 3B model speculates for a 70B target model), but most prior works focus
only on accelerating drafting. $\textit{``Intermediate"}$ verification reduces
verification time by discarding inaccurate draft tokens early, but existing
methods incur substantial training overheads in incorporating the intermediate
verifier, increase the memory footprint to orchestrate the intermediate
verification step, and compromise accuracy by relying on approximate
heuristics.
  We propose $\underline{\textit{Hi}}\textit{erarchical
}\underline{\textit{Spec}}\textit{ulative Decoding (HiSpec)}$, a framework for
high-throughput speculative decoding that exploits $\textit{early-exit (EE)
models}$ for low-overhead intermediate verification. EE models allow tokens to
exit early by skipping layer traversal and are explicitly trained so that
hidden states at selected layers can be interpreted, making them uniquely
suited for intermediate verification without drastically increasing compute and
memory overheads. To improve resource-efficiency even further, we design a
methodology that enables HiSpec to re-use key-value caches and hidden states
between the draft, intermediate verifier, and target models. To maintain
accuracy, HiSpec periodically validates the draft tokens accepted by the
intermediate verifier against the target model. Our evaluations using various
representative benchmarks and models show that HiSpec improves throughput by
1.28$\times$ on average and by up to 2.01$\times$ compared to the baseline
single-layer speculation without compromising accuracy.

</details>


### [122] [TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies](https://arxiv.org/abs/2510.01391)
*Maithili Kadam,Francis Ferraro*

Main category: cs.CL

TL;DR: TAG-EQA 是一种将因果事件图融入大语言模型输入的提示框架，通过九种提示配置在事件问答任务中显著提升性能，尤其在零样本和图增强思维链提示下效果显著。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理需要因果或时序推理的基于事件的问题时表现不佳，因此需要一种无需微调即可增强其事件推理能力的方法。

Method: 提出 TAG-EQA 框架，将结构化关系转化为自然语言语句，并结合三种提示策略（零样本、少样本、思维链）与三种输入模态（文本、图、文本+图）进行系统分析。

Result: 在 TORQUESTRA 基准上平均准确率提升 5%，零样本设置下最高提升 12%，图增强思维链提示下可达 18%。

Conclusion: 因果图能在不微调的情况下有效增强大语言模型的事件推理能力，TAG-EQA 提供了一种灵活的结构化知识注入方式。

Abstract: Large language models (LLMs) excel at general language tasks but often
struggle with event-based questions-especially those requiring causal or
temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question
Answering), a prompting framework that injects causal event graphs into LLM
inputs by converting structured relations into natural-language statements.
TAG-EQA spans nine prompting configurations, combining three strategies
(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,
graph-only, text+graph), enabling a systematic analysis of when and how
structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA
improves accuracy by 5% on average over text-only baselines, with gains up to
12% in zero-shot settings and 18% when graph-augmented CoT prompting is
effective. While performance varies by model and configuration, our findings
show that causal graphs can enhance event reasoning in LLMs without
fine-tuning, offering a flexible way to encode structure in prompt-based QA.

</details>


### [123] [A-VERT: Agnostic Verification with Embedding Ranking Targets](https://arxiv.org/abs/2510.01469)
*Nicolás Aguirre,Ramiro Caso,Ramiro Rodríguez Colmeiro,Mauro Santelli,Joaquín Toranzo Calderón*

Main category: cs.CL

TL;DR: 提出一种无需结构的语义嵌入距离方法，用于低成本且高准确率地自动评估语言模型回复质量。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型回复评估方法成本过高（如LLM-as-a-Judge）或脱离真实场景（如字符串匹配、logprob），需要更高效且贴近实际的评估方案。

Method: 利用小规模嵌入模型（<10B参数）计算语义嵌入距离，将生成文本与目标候选进行匹配，实现对LM回复的鲁棒分类。

Result: 在3个数据集和3种不同LM架构上测试，相比人工标注达到约0.97的回归分数和约96%的准确率。

Conclusion: 该方法在保持低计算成本的同时，实现了与人类判断高度一致的自动评估效果，适用于实际应用场景中的语言模型质量评估。

Abstract: The automatic evaluation of Language Model (LM) responses is a critical piece
in the development of benchmarks and metrics, both for model training and
quality assessment of production model endpoints. The current approaches to
response classification relies on methods that are too expensive (i.e.
LLM-as-a-Judge) or that are far from real-world conditions (string-matching,
logprob). In this paper, a structure-free evaluation method is presented. The
method makes use of semantic embedding distances to match target candidates
with arbitrary LM-generated text, resulting in a robust classification of the
response at a relatively low compute cost (embedding models of less than $10B$
parameters). The results show a regression score of ~0.97 and an accuracy of
~96% against human annotators, tested over 3 data sets and 3 different LM
architectures.

</details>


### [124] [One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning](https://arxiv.org/abs/2510.01526)
*Mengyu Wang,Sotirios Sabanis,Miguel de Carvalho,Shay B. Cohen,Tiejun Ma*

Main category: cs.CL

TL;DR: 本文提出了一种名为专家问题分解（EQD）的方法，通过两步微调框架和奖励函数引导，提升大语言模型在特定领域（尤其是金融领域）复杂问答任务中的定量推理能力。该方法高效且仅需少量训练数据，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要专业知识和复杂推理的领域中表现不佳，尤其是在定量推理方面存在明显不足，因此需要一种能有效结合领域知识并保持计算效率的解决方案。

Method: 提出专家问题分解（EQD），采用两步微调框架，并设计奖励函数来评估生成子问题对最终问答效果的贡献，仅需数千样本和单个A100 GPU进行训练。

Result: 在四个金融领域的基准数据集上，EQD在不同大语言模型上将问答性能提升了0.6%至10.5%，推理时间与零样本提示相当，且优于最先进的领域微调模型和高级提示策略。

Conclusion: EQD在效率和性能之间实现了良好平衡，证明在特定领域问答中，一个有效的支持性问题可能比复杂的多步指引更具价值。

Abstract: Domain-specific quantitative reasoning remains a major challenge for large
language models (LLMs), especially in fields requiring expert knowledge and
complex question answering (QA). In this work, we propose Expert Question
Decomposition (EQD), an approach designed to balance the use of domain
knowledge with computational efficiency. EQD is built on a two-step fine-tuning
framework and guided by a reward function that measures the effectiveness of
generated sub-questions in improving QA outcomes. It requires only a few
thousand training examples and a single A100 GPU for fine-tuning, with
inference time comparable to zero-shot prompting. Beyond its efficiency, EQD
outperforms state-of-the-art domain-tuned models and advanced prompting
strategies. We evaluate EQD in the financial domain, characterized by
specialized knowledge and complex quantitative reasoning, across four benchmark
datasets. Our method consistently improves QA performance by 0.6% to 10.5%
across different LLMs. Our analysis reveals an important insight: in
domain-specific QA, a single supporting question often provides greater benefit
than detailed guidance steps.

</details>


### [125] [ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning](https://arxiv.org/abs/2510.01585)
*Haochen You,Baojing Liu*

Main category: cs.CL

TL;DR: ReSSFormer是一种递归稀疏结构化Transformer，通过循环推理、自适应稀疏注意力和自组织编码结构提升长上下文推理、计算效率和结构泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长上下文推理、计算效率和结构泛化方面存在挑战，主要源于固定的层堆叠、密集注意力和对位置编码的依赖。

Method: 提出ReSSFormer，包含三个创新模块：循环推理与记忆单元（R2MU）、自适应稀疏注意力模块（ASAM）和自组织编码结构（SOES），以递归推理替代深度堆叠，采用令牌级和专家级稀疏注意力，并直接从内容建模潜在令牌拓扑。

Result: 在语言建模、多跳问答和结构敏感任务上，ReSSFormer在相似FLOPs和参数预算下 consistently 优于强基线模型。

Conclusion: ReSSFormer在可扩展性、效率和结构灵活性方面表现出色，为Transformer架构提供了更高效的替代方案。

Abstract: While Transformer architectures have demonstrated impressive scalability
across domains, they continue to face challenges in long-context reasoning,
computational efficiency, and structural generalization - largely due to rigid
layer stacking, dense attention, and reliance on positional encodings. We
present ReSSFormer, a Recursive Sparse Structured Transformer that integrates
three complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for
iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM)
for efficient and focused context selection, and Self-Organizing Encoder
Structure (SOES) for position-free structure induction. ReSSFormer replaces
conventional depth stacking with recurrent inference, substitutes full
attention with token- and expert-level sparsity, and models latent token
topology directly from content. Across language modeling, multi-hop QA, and
structure-sensitive tasks, ReSSFormer consistently outperforms strong baselines
under comparable FLOPs and parameter budgets, highlighting its scalability,
efficiency, and structural flexibility.

</details>


### [126] [CLUE: Non-parametric Verification from Experience via Hidden-State Clustering](https://arxiv.org/abs/2510.01591)
*Zhenwen Liang,Ruosen Li,Yujun Zhou,Linfeng Song,Dian Yu,Xinya Du,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型内部隐藏状态的验证方法Clue，通过分析隐藏层激活轨迹中的几何可分特征来判断输出正确性，无需训练参数，仅依靠过往经验中的“成功”与“失败”聚类进行分类，在多个基准上优于LLM-as-a-judge和置信度方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估大模型输出质量的方法依赖文本层面信息或校准后的置信度，易过拟合或受限于模型校准程度，而隐藏状态包含了更丰富的语义和置信信息，尚未被充分利用。

Method: 提出Clue（Clustering and Experience-based Verification），利用推理过程中隐藏状态的变化（delta）作为特征，采用非参数化的最近质心距离方法，根据历史经验中形成的‘成功’与‘失败’聚类来判断当前输出的正确性。

Result: Clue在AIME 24/25和GPQA等多个基准上优于LLM-as-a-judge基线，匹配或超过现代置信度方法，在1.5B模型的AIME 24任务中将多数投票准确率从56.7%提升至70.0%（top-maj@16）。

Conclusion: 大语言模型的隐藏状态中包含可几何分离的正确性信号，Clue证明了无需训练即可有效利用该信号进行输出验证，展示了隐藏状态作为统一验证基础的潜力。

Abstract: Assessing the quality of Large Language Model (LLM) outputs presents a
critical challenge. Previous methods either rely on text-level information
(e.g., reward models, majority voting), which can overfit to superficial cues,
or on calibrated confidence from token probabilities, which would fail on
less-calibrated models. Yet both of these signals are, in fact, partial
projections of a richer source of information: the model's internal hidden
states. Early layers, closer to token embeddings, preserve semantic and lexical
features that underpin text-based judgments, while later layers increasingly
align with output logits, embedding confidence-related information. This paper
explores hidden states directly as a unified foundation for verification. We
show that the correctness of a solution is encoded as a geometrically separable
signature within the trajectory of hidden activations. To validate this, we
present Clue (Clustering and Experience-based Verification), a deliberately
minimalist, non-parametric verifier. With no trainable parameters, CLUE only
summarizes each reasoning trace by an hidden state delta and classifies
correctness via nearest-centroid distance to ``success'' and ``failure''
clusters formed from past experience. The simplicity of this method highlights
the strength of the underlying signal. Empirically, CLUE consistently
outperforms LLM-as-a-judge baselines and matches or exceeds modern
confidence-based methods in reranking candidates, improving both top-1 and
majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24
with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%
(top-maj@16).

</details>


### [127] [A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.01600)
*Neal Gregory Lawton,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 本文评估了独立微调、联合微调和两阶段微调等RAG微调策略，发现它们在生成质量指标上表现相近，但计算成本差异显著，最优策略取决于训练数据是否包含上下文标签及是否需对学习率进行网格搜索。


<details>
  <summary>Details</summary>
Motivation: 为了提升检索增强生成（RAG）系统在新任务上的性能，需要比较不同的微调策略以权衡其效果与计算成本。

Method: 实验评估了独立微调、联合微调和两阶段微调三种策略，在多个数据集上比较它们在EM和F1等指标上的表现，并分析各自的计算开销。

Result: 三种微调策略在生成质量（EM和F1）上表现相当，但计算成本有显著差异。是否需要上下文标签和学习率调优影响策略选择。

Conclusion: 最优的RAG微调策略取决于具体应用场景：当训练数据包含上下文标签且可进行超参数搜索时，联合或两阶段微调更合适；否则独立微调更具性价比。

Abstract: A Comparison of Independent and Joint Fine-tuning Strategies for
Retrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,
Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP
2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0
Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),
Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and
compare strategies for fine-tuning Retrieval Augmented Generation (RAG)
pipelines, including independent fine-tuning, joint fine-tuning, and two-phase
fine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular
framework for question answering that is powered by two large language models
(LLMs): an embedding model that retrieves context documents from a database
that are relevant to a given question, and a generator model that uses the
retrieved context to generate an answer to the question. Both the embedding and
generator models can be fine-tuned to increase performance of a RAG pipeline on
a new task, but multiple fine-tuning strategies exist with different costs and
benefits. In this paper, we evaluate and compare several RAG fine-tuning
strategies, including independent, joint, and two-phase fine-tuning. In our
experiments, we observe that all of these strategies achieve about equal
improvement in EM and F1 generation quality metrics, although they have
significantly different computational costs. We conclude the optimal
fine-tuning strategy to use depends on whether the training dataset includes
context labels and whether a grid search over the learning rates for the
embedding and generator models is required.

</details>


### [128] [RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering](https://arxiv.org/abs/2510.01612)
*Lovely Yeswanth Panchumarthi,Sai Prasad Gudari,Atharva Negi,Praveen Raj Budime,Harsit Upadhya*

Main category: cs.CL

TL;DR: 提出RAG-BioQA框架，结合检索增强生成与领域特定微调，生成基于证据的长篇生物医学答案，在PubMedQA数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学问答系统主要关注短答案，缺乏临床决策所需的全面解释，难以满足对精确、详尽医学信息的需求。

Method: 结合BioBERT嵌入与FAISS索引进行文献检索，比较BM25、ColBERT和MonoT5等重排序策略优化上下文选择，并通过微调T5模型合成证据生成长答案。

Result: 在PubMedQA数据集上，该方法在BLEU、ROUGE和METEOR指标上均显著优于基线模型，有效提升长篇生物医学问答性能。

Conclusion: RAG-BioQA通过融合检索增强与领域微调，能够生成高质量、证据支持的长篇生物医学答案，推动了可访问的循证知识检索发展。

Abstract: The exponential growth of biomedical literature creates significant
challenges for accessing precise medical information. Current biomedical
question-answering systems primarily focus on short-form answers, failing to
provide the comprehensive explanations necessary for clinical decision-making.
We present RAG-BioQA, a novel framework combining retrieval-augmented
generation with domain-specific fine-tuning to produce evidence-based,
long-form biomedical answers. Our approach integrates BioBERT embeddings with
FAISS indexing and compares various re-ranking strategies (BM25, ColBERT,
MonoT5) to optimize context selection before synthesizing evidence through a
fine-tuned T5 model. Experimental results on the PubMedQA dataset show
significant improvements over baselines, with our best model achieving
substantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state
of accessible, evidence-based biomedical knowledge retrieval.

</details>


### [129] [Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO](https://arxiv.org/abs/2510.01616)
*Yu-Cheng Chih,Ming-Tao Duan,Yong-Hao Hou*

Main category: cs.CL

TL;DR: 本文提出了一种三阶段稳定化流程PureTC-1B，通过参数高效的LoRA适配器提升Llama-3.2-1B-Instruct模型在传统中文（TC）上的语言稳定性，显著减少了非TC字符输出，在真实场景模拟和命名实体翻译任务中均表现出显著改进。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型（SLMs）在传统中文（TC）应用中存在词元级不稳定性问题，即模型会不可预测地生成非TC字符或混入其他语言，限制了其在实际场景中的部署。本文旨在解决这一可靠性问题。

Method: 提出一个包含持续预训练（CPT）、监督微调（SFT）和直接偏好优化（DPO）的三阶段稳定化流程，并使用LoRA适配器进行参数高效调整，以增强模型的单语鲁棒性，而无需完整重训模型。

Result: 在模拟真实使用的基准测试中，PureTC-1B相比基础模型实现了51.3%的非TC输出词元相对减少（微平均）；在命名实体翻译任务中，相较于Llama-3B和Qwen-1.5B分别减少77.2%和57.2%的错误语言词元。

Conclusion: 即使在10亿参数规模的小型语言模型上，也能实现强健的传统中文一致性，该方法具有可复现、仅需适配器、硬件友好等优点，为提升TC及其他非英语语言的语言稳定性提供了实用方案。

Abstract: Small Language Models (SLMs) enable cost-effective, on-device and
latency-sensitive AI applications, yet their deployment in Traditional Chinese
(TC) remains hindered by token-level instability - models unpredictably emit
non-TC characters or code-switch into other languages. We address this
practical reliability gap by creating PureTC-1B, a three-stage stabilization
pipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned model
released by Meta) using parameter-efficient LoRA adapters. Our method combines
Continual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning
(SFT) with instruction data, and Direct Preference Optimization (DPO) using
TC-adherence preferences to improve monolingual robustness without full-model
retraining. On a benchmark designed to simulate real-world usage, PureTC-1B
achieves a 51.3% relative reduction (micro-average) in non-TC output tokens
versus the base model. On a Named Entity Translation (NET) task, PureTC-1B
further reduces incorrect-language tokens by 77.2% relative to Llama-3B and
57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainable
even at the 1B scale. The pipeline is reproducible, adapter-only, and
hardware-friendly, offering practitioners a practical recipe to enhance
language stability for TC and potentially other non-English languages.

</details>


### [130] [AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System](https://arxiv.org/abs/2510.01617)
*Hui Yi Leong,Yuheng Li,Yuqing Wu,Wenwen Ouyang,Wei Zhu,Jiechao Gao*

Main category: cs.CL

TL;DR: AMAS是一种基于大语言模型的多智能体系统新框架，通过动态图设计实现任务自适应的智能体协作，显著提升了在问答、数学推理和代码生成等任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的多智能体系统架构依赖固定的手工设计图结构，缺乏对上下文的响应能力，限制了其在多样化任务中的有效性。

Method: 提出AMAS框架，引入一个轻量级的大语言模型适配的动态图设计器，根据输入内容自主生成任务最优的图结构，实现查询路径的智能化调度。

Result: 在多种基准任务（如问答、数学推理、代码生成）上验证了AMAS的优越性，其性能系统性地超过了当前最先进的单智能体和多智能体方法。

Conclusion: 上下文敏感的结构可适应性是高性能大语言模型多智能体系统部署的基础需求，AMAS为此提供了有效解决方案。

Abstract: Although large language models (LLMs) have revolutionized natural language
processing capabilities, their practical implementation as autonomous
multi-agent systems (MAS) for industrial problem-solving encounters persistent
barriers. Conventional MAS architectures are fundamentally restricted by
inflexible, hand-crafted graph topologies that lack contextual responsiveness,
resulting in diminished efficacy across varied academic and commercial
workloads. To surmount these constraints, we introduce AMAS, a
paradigm-shifting framework that redefines LLM-based MAS through a novel
dynamic graph designer. This component autonomously identifies task-specific
optimal graph configurations via lightweight LLM adaptation, eliminating the
reliance on monolithic, universally applied structural templates. Instead, AMAS
exploits the intrinsic properties of individual inputs to intelligently direct
query trajectories through task-optimized agent pathways. Rigorous validation
across question answering, mathematical deduction, and code generation
benchmarks confirms that AMAS systematically exceeds state-of-the-art
single-agent and multi-agent approaches across diverse LLM architectures. Our
investigation establishes that context-sensitive structural adaptability
constitutes a foundational requirement for high-performance LLM MAS
deployments.

</details>


### [131] [NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT](https://arxiv.org/abs/2510.01644)
*John Hawkins,Aditya Pramar,Rodney Beard,Rohitash Chandra*

Main category: cs.CL

TL;DR: 本研究探讨了不同机器学习模型识别大语言模型（LLM）中的“越狱提示”（jailbreak prompts）的能力，发现基于BERT模型的端到端微调在现有数据集上表现最佳，并指出提示词结构中的显式自反性可能是越狱行为的信号。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在安全漏洞，恶意用户可通过输入操纵（如越狱提示）绕过安全机制，因此需要有效方法来自动检测此类攻击。

Method: 使用多种机器学习模型分析区分越狱提示与正常提示的能力，重点评估对未见过的越狱策略的识别效果，并通过可视化关键词分析其特征。

Result: 在当前数据集上，端到端微调的BERT模型表现最优；可视化结果显示，越狱提示常包含具有显式自反性的结构特征。

Conclusion: BERT模型在检测越狱提示方面性能最佳，提示结构中的自反性可能作为检测越狱意图的重要信号，有助于提升LLM的安全防护能力。

Abstract: Large Language Models (LLMs) suffer from a range of vulnerabilities that
allow malicious users to solicit undesirable responses through manipulation of
the input text. These so-called jailbreak prompts are designed to trick the LLM
into circumventing the safety guardrails put in place to keep responses
acceptable to the developer's policies. In this study, we analyse the ability
of different machine learning models to distinguish jailbreak prompts from
genuine uses, including looking at our ability to identify jailbreaks that use
previously unseen strategies. Our results indicate that using current datasets
the best performance is achieved by fine tuning a Bidirectional Encoder
Representations from Transformers (BERT) model end-to-end for identifying
jailbreaks. We visualise the keywords that distinguish jailbreak from genuine
prompts and conclude that explicit reflexivity in prompt structure could be a
signal of jailbreak intention.

</details>


### [132] [Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention](https://arxiv.org/abs/2510.01652)
*Zhaoxin Feng,Jianfei Ma,Emmanuele Chersoni,Xiaojing Zhao,Xiaoyi Bao*

Main category: cs.CL

TL;DR: 本文探讨了通过在Llama架构中引入双向注意力机制和对比学习，克服自回归大语言模型在文本嵌入任务中因单向注意力机制受限的问题。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型在语言理解和生成方面表现出色，但由于单向注意力机制的限制，在文本嵌入和语义表示探针任务中的应用较慢，因此需要探索改进方法。

Method: 通过对Llama架构的不同变体进行额外训练，逐步引入双向注意力机制，并结合无监督/有监督对比学习进行测试。

Result: 实验结果表明，引入双向注意力机制能够有效提升模型在文本嵌入任务中的表现，增强了语义表示能力。

Conclusion: 双向注意力机制可以克服自回归模型在文本嵌入任务中的局限性，为提升其语义表示能力提供了可行路径。

Abstract: Autoregressive Large Language Models (LLMs) demonstrate exceptional
performance in language understanding and generation. However, their
application in text embedding tasks has been relatively slow, along with the
analysis of their semantic representation in probing tasks, due to the
constraints of the unidirectional attention mechanism.
  This paper aims to explore whether such constraints can be overcome by
enabling bidirectional attention in LLMs. We tested different variants of the
Llama architecture through additional training steps, progressively enabling
bidirectional attention and unsupervised/supervised contrastive learning.

</details>


### [133] [SoK: Measuring What Matters for Closed-Loop Security Agents](https://arxiv.org/abs/2510.01654)
*Mudita Khurana,Raunak Jain*

Main category: cs.CL

TL;DR: 本文提出了CLASP框架和CLC评分，用于评估闭环自主安全系统在网络安全生命周期中的智能体能力，填补了现有研究在统一框架、评估方法和基准测试方面的空白。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全研究分散于孤立的防御功能，缺乏对集成化、闭环自主安全代理的有效评估框架和基准，导致能力盲区。

Method: 提出CLASP框架，将安全生命周期与核心智能体能力对齐，并定义CLC评分作为衡量闭环能力和操作有效性的综合指标，通过分析21项代表性工作验证其有效性。

Result: 成功应用CLASP分析21项研究，识别出系统的能力优势与缺口，明确了闭环安全代理的发展需求与评估标准。

Conclusion: CLASP和CLC评分为评估和推进闭环自主安全系统提供了必要的术语体系、诊断工具和量化手段，推动该领域的系统化发展。

Abstract: Cybersecurity is a relentless arms race, with AI driven offensive systems
evolving faster than traditional defenses can adapt. Research and tooling
remain fragmented across isolated defensive functions, creating blind spots
that adversaries exploit. Autonomous agents capable of integrating, exploit
confirmation, remediation, and validation into a single closed loop offer
promise, but the field lacks three essentials: a framework defining the agentic
capabilities of security systems across security life cycle, a principled
method for evaluating closed loop agents, and a benchmark for measuring their
performance in practice. We introduce CLASP: the Closed-Loop Autonomous
Security Performance framework which aligns the security lifecycle
(reconnaissance, exploitation, root cause analysis, patch synthesis,
validation) with core agentic capabilities (planning, tool use, memory,
reasoning, reflection & perception) providing a common vocabulary and rubric
for assessing agentic capabilities in security tasks. By applying CLASP to 21
representative works, we map where systems demonstrate strengths, and where
capability gaps persist. We then define the Closed-Loop Capability (CLC) Score,
a composite metric quantifying both degree of loop closure and operational
effectiveness, and outline the requirements for a closed loop benchmark.
Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and
measurements needed to advance both function level performance and measure
closed loop security agents.

</details>


### [134] [MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization](https://arxiv.org/abs/2510.01659)
*Yinhong Liu,Jianfeng He,Hang Su,Ruixue Lian,Yi Nian,Jake Vincent,Srikanth Vishnubhotla,Robinson Piramuthu,Saab Mansour*

Main category: cs.CL

TL;DR: 本文提出了MDSEval，首个用于多模态对话摘要的元评估基准，包含图像共享对话、摘要及八个质量维度的人工评分，并提出基于跨模态互斥关键信息（MEKI）的过滤框架以提升数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态对话摘要（MDS）缺乏可靠的自动评估方法和基于人工标注的元评估基准，限制了模型发展。

Method: 构建包含对话、摘要和人工评分的MDSEval基准；提出MEKI过滤框架确保跨模态信息多样性；定义八个MDS特有的评估维度，并评测现有评估方法的表现。

Result: 实验表明现有评估方法难以区分先进MLLM生成的摘要，且易受多种偏见影响，验证了MDSEval的有效性和必要性。

Conclusion: MDSEval为MDS任务提供了首个标准化元评估基准，揭示了当前评估方法的局限性，推动更鲁棒评估体系的发展。

Abstract: Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging
applications. To support the development of effective MDS models, robust
automatic evaluation methods are essential for reducing both cost and human
effort. However, such methods require a strong meta-evaluation benchmark
grounded in human annotations. In this work, we introduce MDSEval, the first
meta-evaluation benchmark for MDS, consisting image-sharing dialogues,
corresponding summaries, and human judgments across eight well-defined quality
aspects. To ensure data quality and richfulness, we propose a novel filtering
framework leveraging Mutually Exclusive Key Information (MEKI) across
modalities. Our work is the first to identify and formalize key evaluation
dimensions specific to MDS. We benchmark state-of-the-art modal evaluation
methods, revealing their limitations in distinguishing summaries from advanced
MLLMs and their susceptibility to various bias.

</details>


### [135] [How Do Language Models Compose Functions?](https://arxiv.org/abs/2510.01685)
*Apoorv Khandelwal,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在解决两跳事实回忆任务时是否使用组合性机制，发现尽管LLM能执行f(x)和g(z)，但并不能有效组合成g(f(x))，即存在“组合性差距”。通过logit lens分析残差流激活，识别出两种机制：一种是组合式求解，显式计算中间结果f(x)；另一种是直接求解，未检测到f(x)的痕迹。机制的选择与嵌入空间几何相关，当x到g(f(x))存在线性映射时，模型更倾向于使用非组合性的直接机制。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在处理可形式化为g(f(x))的复合任务时，是否真正采用组合性推理机制，而非仅依赖表面关联或记忆。

Method: 使用logit lens方法分析前馈大语言模型在两跳事实回忆任务中的残差流激活，识别是否存在中间变量f(x)的计算痕迹，并结合嵌入空间的几何特性分析模型采用组合式或直接式机制的倾向。

Result: 1. 确认现代LLM存在‘组合性差距’；2. 发现两种求解机制：组合式（compositional）和直接式（direct）；3. 直接式机制在嵌入空间中存在从x到g(f(x))的线性映射时占主导。

Conclusion: 大语言模型并非总是以组合性方式解决复合任务，其机制选择受嵌入空间几何结构影响，提示当前模型可能更多依赖非组合性的‘惯用语式’映射，而非真正的符号组合推理。

Abstract: While large language models (LLMs) appear to be increasingly capable of
solving compositional tasks, it is an open question whether they do so using
compositional mechanisms. In this work, we investigate how feedforward LLMs
solve two-hop factual recall tasks, which can be expressed compositionally as
$g(f(x))$. We first confirm that modern LLMs continue to suffer from the
"compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y =
g(z)$ does not entail their ability to compute the composition $y = g(f(x))$.
Then, using logit lens on their residual stream activations, we identify two
processing mechanisms, one which solves tasks $\textit{compositionally}$,
computing $f(x)$ along the way to computing $g(f(x))$, and one which solves
them $\textit{directly}$, without any detectable signature of the intermediate
variable $f(x)$. Finally, we find that which mechanism is employed appears to
be related to the embedding space geometry, with the idiomatic mechanism being
dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in
the embedding spaces. We fully release our data and code at:
https://github.com/apoorvkh/composing-functions .

</details>


### [136] [Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation](https://arxiv.org/abs/2510.01688)
*Seungseop Lim,Gibaeg Kim,Wooseok Han,Jean Seo,Hyunkyung Lee,Jaehyo Yoo,Eunho Yang*

Main category: cs.CL

TL;DR: 本文提出了一种针对医疗预咨询对话中大语言模型出现“格式惯性”问题的解决方案，通过重新平衡训练数据中的对话轮次分布来缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，监督微调（SFT）是常用方法，但训练数据通常存在对话轮次分布不均的问题，导致模型产生重复且诊断信息不足的提问，即“格式惯性”。

Method: 采用一种简单且以数据为中心的方法，重新调整训练数据中对话轮次的分布，以减轻格式惯性的影响。

Result: 实验结果表明，所提出的数据重平衡方法能显著缓解大语言模型在长医疗对话中的格式惯性问题。

Conclusion: 通过调整训练数据的结构，可以有效改善大语言模型在多轮医疗对话中的生成质量，减少无意义重复提问。

Abstract: Recent advances in Large Language Models (LLMs) have brought significant
improvements to various service domains, including chatbots and medical
pre-consultation applications. In the healthcare domain, the most common
approach for adapting LLMs to multi-turn dialogue generation is Supervised
Fine-Tuning (SFT). However, datasets for SFT in tasks like medical
pre-consultation typically exhibit a skewed turn-count distribution. Training
on such data induces a novel failure mechanism we term **Format Inertia**,
where models tend to generate repetitive, format-correct, but diagnostically
uninformative questions in long medical dialogues. To mitigate this observed
failure mechanism, we adopt a simple, data-centric method that rebalances the
turn-count distribution of the training dataset. Experimental results show that
our approach substantially alleviates Format Inertia in medical
pre-consultation.

</details>


### [137] [What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?](https://arxiv.org/abs/2510.01719)
*Jiwan Chung,Neel Joshi,Pratyusha Sharma,Youngjae Yu,Vibhav Vineet*

Main category: cs.CL

TL;DR: 本文提出了MathLens基准，用于分解多模态推理中的子技能（感知、推理、整合），并评估不同训练方法对这些技能的影响，发现强化学习主要提升感知，而整合能力仍最弱。


<details>
  <summary>Details</summary>
Motivation: 现有评估多模态推理模型的方法主要依赖总体准确率，难以揭示模型在哪些具体技能上取得进步或存在不足，因此需要一个能解耦子技能且保持问题复杂性的细粒度评估基准。

Method: 构建MathLens基准，将性能分解为感知、推理和整合三个部分，并通过视觉图示、文本描述、控制性问题和感知探针等注释进行评估；所有数据基于符号化问题规范生成以保证一致性。

Result: 发现强化学习主要提升感知能力（尤其配合文本监督时），文本SFT通过反思性推理间接改善感知；推理能力仅在感知提升时同步改进；整合能力始终最弱，错误集中于此；RL提升图表变化下的鲁棒性，而多模态SFT因过拟合降低鲁棒性。

Conclusion: 多模态推理模型的各子技能发展不均衡，当前模型在整合感知与推理方面仍存在显著瓶颈，未来应针对性地优化整合机制并提升训练方法的鲁棒性。

Abstract: Multimodal reasoning models have recently shown promise on challenging
domains such as olympiad-level geometry, yet their evaluation remains dominated
by aggregate accuracy, a single score that obscures where and how models are
improving. We introduce MathLens, a benchmark designed to disentangle the
subskills of multimodal reasoning while preserving the complexity of
textbook-style geometry problems. The benchmark separates performance into
three components: Perception: extracting information from raw inputs,
Reasoning: operating on available information, and Integration: selecting
relevant perceptual evidence and applying it within reasoning. To support each
test, we provide annotations: visual diagrams, textual descriptions to evaluate
reasoning in isolation, controlled questions that require both modalities, and
probes for fine-grained perceptual skills, all derived from symbolic
specifications of the problems to ensure consistency and robustness. Our
analysis reveals that different training approaches have uneven effects: First,
reinforcement learning chiefly strengthens perception, especially when
supported by textual supervision, while textual SFT indirectly improves
perception through reflective reasoning. Second, reasoning improves only in
tandem with perception. Third, integration remains the weakest capacity, with
residual errors concentrated there once other skills advance. Finally,
robustness diverges: RL improves consistency under diagram variation, whereas
multimodal SFT reduces it through overfitting. We will release all data and
experimental logs.

</details>


### [138] [Machine-interpretable Engineering Design Standards for Valve Specification](https://arxiv.org/abs/2510.01736)
*Anders Gjerver,Rune Frostad,Vedrana Barisic,Melinda Hodkiewicz,Caitlin Woods,Mihaly Fekete,Arild Braathen Torjusen,Johan Wilhelm Kluwer*

Main category: cs.CL

TL;DR: 本文提出了一种将工程设计标准中的信息转化为模块化、可重用、机器可解释的本体的方法，并将其应用于工厂设计和设备选型的质量保证中。


<details>
  <summary>Details</summary>
Motivation: 尽管工业界致力于数字化，但目前产品规范和设计标准仍以文档为中心，缺乏结构化和机器可读性，限制了自动化和智能应用。

Method: 采用建模模式，从国际标准（如管道、材料和阀门设计）的文本和表格中提取知识，构建符合W3C标准且与顶层本体ISO DIS 23726-3（IDO）对齐的模块化本体，并在阀门选型过程中进行验证。

Result: 实现了基于语义资产模型的阀门实例化，结合环境条件的语义表示，通过OWL个体和类构建功能位置标签和产品类型实例，支持自动化验证VDS是否符合行业标准，并利用语义推理判断产品类型是否满足阀门规格。

Conclusion: 基于IDO的共享、可重用模块化本体为设计标准的数字化提供了有效路径，支持语义推理在设备选型中的应用，展示了向智能化‘智能标准’转型的潜力。

Abstract: Engineering design processes use technical specifications and must comply
with standards. Product specifications, product type data sheets, and design
standards are still mainly document-centric despite the ambition to digitalize
industrial work. In this paper, we demonstrate how to transform information
held in engineering design standards into modular, reusable,
machine-interpretable ontologies and use the ontologies in quality assurance of
the plant design and equipment selection process. We use modelling patterns to
create modular ontologies for knowledge captured in the text and in frequently
referenced tables in International Standards for piping, material and valve
design. These modules are exchangeable, as stored in a W3C compliant format,
and interoperable as they are aligned with the top-level ontology ISO DIS
23726-3: Industrial Data Ontology (IDO).
  We test these ontologies, created based on international material and piping
standards and industry norms, on a valve selection process. Valves are
instantiated in semantic asset models as individuals along with a semantic
representation of the environmental condition at their location on the asset.
We create "functional location tags" as OWL individuals that become instances
of OWL class Valve Data Sheet (VDS) specified valves. Similarly we create
instances of manufacturer product type. Our approach enables automated
validation that a specific VDS is compliant with relevant industry standards.
Using semantic reasoning and executable design rules, we also determine whether
the product type meets the valve specification. Creation of shared, reusable
IDO-based modular ontologies for design standards enables semantic reasoning to
be applied to equipment selection processes and demonstrates the potential of
this approach for Standards Bodies wanting to transition to digitized Smart
Standards.

</details>


### [139] [Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks](https://arxiv.org/abs/2510.01782)
*Wenbo Pan,Jie Xu,Qiguang Chen,Junhao Dong,Libo Qin,Xinfeng Li,Haining Yu,Xiaohua Jia*

Main category: cs.CL

TL;DR: 本文提出了一个名为“拒绝指数”（Refusal Index, RI）的新指标，用于准确衡量大语言模型在未知问题上的知识感知拒绝能力。RI定义为拒绝概率与错误概率之间的斯皮尔曼秩相关系数，并通过轻量级双轮评估方法进行实际测量。实验表明，RI能稳定、一致地评估模型的拒绝行为，揭示了当前模型在高准确率下仍存在拒绝行为不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有指标无法准确衡量大语言模型在未知问题上的拒绝能力：基于拒绝的指标受拒绝率偏差影响，校准指标则依赖代理任务，未能直接反映模型的真实拒绝行为。因此需要一种更可靠、无偏的度量方式。

Method: 提出拒绝指数（RI），即拒绝概率与错误概率之间的斯皮尔曼秩相关性；设计两轮轻量评估方法，利用两次标准评测运行中的拒绝率来估计RI。

Result: 在16个模型和5个数据集上的实验表明，RI能准确量化模型的知识感知拒绝能力，且不受模型整体准确率或拒绝率变化的影响，提供稳定的模型排序。

Conclusion: 拒绝指数（RI）是一种可靠、无偏的指标，能够有效评估大语言模型在事实性任务中对未知问题的拒绝能力。研究发现，尽管模型准确率高，但其拒绝行为可能脆弱且不可靠，因此应将RI与传统准确率结合，实现更全面的事实性评估。

Abstract: Large Language Models (LLMs) should refuse to answer questions beyond their
knowledge. This capability, which we term knowledge-aware refusal, is crucial
for factual reliability. However, existing metrics fail to faithfully measure
this ability. On the one hand, simple refusal-based metrics are biased by
refusal rates and yield inconsistent scores when models exhibit different
refusal tendencies. On the other hand, existing calibration metrics are
proxy-based, capturing the performance of auxiliary calibration processes
rather than the model's actual refusal behavior. In this work, we propose the
Refusal Index (RI), a principled metric that measures how accurately LLMs
refuse questions they do not know. We define RI as Spearman's rank correlation
between refusal probability and error probability. To make RI practically
measurable, we design a lightweight two-pass evaluation method that efficiently
estimates RI from observed refusal rates across two standard evaluation runs.
Extensive experiments across 16 models and 5 datasets demonstrate that RI
accurately quantifies a model's intrinsic knowledge-aware refusal capability in
factual tasks. Notably, RI remains stable across different refusal rates and
provides consistent model rankings independent of a model's overall accuracy
and refusal rates. More importantly, RI provides insight into an important but
previously overlooked aspect of LLM factuality: while LLMs achieve high
accuracy on factual tasks, their refusal behavior can be unreliable and
fragile. This finding highlights the need to complement traditional accuracy
metrics with the Refusal Index for comprehensive factuality evaluation.

</details>


### [140] [Detecting LLM-Generated Spam Reviews by Integrating Language Model Embeddings and Graph Neural Network](https://arxiv.org/abs/2510.01801)
*Xin Liu,Rongwu Xu,Xinyi Jia,Jason Liao,Jiao Sun,Ling Huang,Wei Xu*

Main category: cs.CL

TL;DR: 本文提出了一种名为FraudSquad的混合检测模型，用于识别由大语言模型生成的具有高度欺骗性的垃圾评论。该模型结合了预训练语言模型的文本嵌入和门控图变换器，在少量标注数据下实现了显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的垃圾评论越来越逼真，对现有检测系统构成挑战，威胁在线平台的可信度，亟需更有效的检测方法。

Method: 构建三个基于不同大语言模型生成的垃圾评论数据集，并提出FraudSquad模型，融合语义信息（文本嵌入）与用户行为模式（图结构），通过图变换器进行节点分类实现检测。

Result: FraudSquad在三个LLM生成的数据集上比现有最优方法精度提升高达44.22%，召回率提升43.01%，同时在人工编写的垃圾评论数据集上也表现良好，且模型规模小、所需训练数据少。

Conclusion: FraudSquad能有效应对LLM生成的垃圾评论，兼具高性能与实用性，为适应LLM时代的反垃圾检测提供了可行方案。

Abstract: The rise of large language models (LLMs) has enabled the generation of highly
persuasive spam reviews that closely mimic human writing. These reviews pose
significant challenges for existing detection systems and threaten the
credibility of online platforms. In this work, we first create three realistic
LLM-generated spam review datasets using three distinct LLMs, each guided by
product metadata and genuine reference reviews. Evaluations by GPT-4.1 confirm
the high persuasion and deceptive potential of these reviews. To address this
threat, we propose FraudSquad, a hybrid detection model that integrates text
embeddings from a pre-trained language model with a gated graph transformer for
spam node classification. FraudSquad captures both semantic and behavioral
signals without relying on manual feature engineering or massive training
resources. Experiments show that FraudSquad outperforms state-of-the-art
baselines by up to 44.22% in precision and 43.01% in recall on three
LLM-generated datasets, while also achieving promising results on two
human-written spam datasets. Furthermore, FraudSquad maintains a modest model
size and requires minimal labeled training data, making it a practical solution
for real-world applications. Our contributions include new synthetic datasets,
a practical detection framework, and empirical evidence highlighting the
urgency of adapting spam detection to the LLM era. Our code and datasets are
available at: https://anonymous.4open.science/r/FraudSquad-5389/.

</details>


### [141] [Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors](https://arxiv.org/abs/2510.01831)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CL

TL;DR: 大型语言模型在解决数学问题时表现出色，但在问题句法偏离训练分布时容易出错。本文发现了一种称为“句法盲点”的系统性错误模式，并提出通过句法重构降低复杂性可提升正确率，表明许多推理错误源于结构不匹配而非概念理解困难。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在面对句法变化时的推理失败原因，探究这些错误是否源于模型对表面形式与内在语义之间耦合的脆弱性。

Method: 通过从正确回答的问题中提取句法模板，对错误回答的问题进行语义保持的重述，并使用基于依存局部性理论（DLT）的指标量化句法复杂度，分析其与错误率的关系。

Result: 实验显示，经过句法简化后的重述问题显著提高了模型的准确率，且较高的DLT分数与多数据集上的更高错误率相关。

Conclusion: 模型的许多推理错误是由于输入的句法结构复杂导致的结构不匹配，而非真正的概念理解障碍；采用句法感知的干预方法可以揭示并缓解这类归纳偏差。

Abstract: Large Language Models (LLMs) demonstrate strong mathematical problem-solving
abilities but frequently fail on problems that deviate syntactically from their
training distribution. We identify a systematic failure mode, syntactic blind
spots, in which models misapply familiar reasoning strategies to problems that
are semantically straightforward but phrased in unfamiliar ways. These errors
are not due to gaps in mathematical competence, but rather reflect a brittle
coupling between surface form and internal representation. To test this, we
rephrase incorrectly answered questions using syntactic templates drawn from
correct examples. These rephrasings, which preserve semantics while reducing
structural complexity, often lead to correct answers. We quantify syntactic
complexity using a metric based on Dependency Locality Theory (DLT), and show
that higher DLT scores are associated with increased failure rates across
multiple datasets. Our findings suggest that many reasoning errors stem from
structural misalignment rather than conceptual difficulty, and that
syntax-aware interventions can reveal and mitigate these inductive failures.

</details>


### [142] [SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement Learning](https://arxiv.org/abs/2510.01832)
*Shicheng Liu,Kai Sun,Lisheng Fu,Xilun Chen,Xinyuan Zhang,Zhaojiang Lin,Rulin Shao,Yue Liu,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: 本文提出了SCRIBES，一种基于强化学习的大规模半结构化网页内容提取框架，利用同一网站内页面布局的相似性生成可重用的提取脚本，并通过在CommonCrawl数据上迭代训练提升性能，在脚本质量和下游任务准确率上均显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: HTML表格、列表和信息框等半结构化内容包含大量事实数据，但其格式复杂，现有方法在泛化能力或资源效率上存在不足，难以实现高效、可扩展的信息提取。

Method: 提出SCRIBES框架，采用强化学习，以同一站点内页面布局的相似性作为奖励信号，生成可跨多个相似页面复用的提取脚本，并利用CommonCrawl中的野外数据生成合成标注进行迭代训练。

Result: 实验表明，该方法在脚本质量上超过强基线模型13%以上，并使GPT-4o在下游问答任务中的准确率提升超4%。

Conclusion: SCRIBES实现了高效、可扩展且资源友好的大规模网页信息提取，通过可复用脚本和合成数据训练显著提升了提取性能。

Abstract: Semi-structured content in HTML tables, lists, and infoboxes accounts for a
substantial share of factual data on the web, yet the formatting complicates
usage, and reliably extracting structured information from them remains
challenging. Existing methods either lack generalization or are
resource-intensive due to per-page LLM inference. In this paper, we introduce
SCRIBES (SCRIpt-Based Semi-Structured Content Extraction at Web-Scale), a novel
reinforcement learning framework that leverages layout similarity across
webpages within the same site as a reward signal. Instead of processing each
page individually, SCRIBES generates reusable extraction scripts that can be
applied to groups of structurally similar webpages. Our approach further
improves by iteratively training on synthetic annotations from in-the-wild
CommonCrawl data. Experiments show that our approach outperforms strong
baselines by over 13% in script quality and boosts downstream question
answering accuracy by more than 4% for GPT-4o, enabling scalable and
resource-efficient web information extraction.

</details>


### [143] [Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models](https://arxiv.org/abs/2510.01845)
*Ece Takmaz,Lisa Bylinina,Jakub Dotlacil*

Main category: cs.CL

TL;DR: 本文提出了一种在低资源环境下开发语言和多模态模型的方法，旨在解决当前视觉-语言模型与儿童语言习得之间的数据规模差距。通过模型融合技术，将多模态模型与纯语言模型结合，提升了语言任务表现，同时保持了多模态性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型依赖大规模数据和参数，远超儿童语言学习所接触的数据量，因此需要探索更符合发育合理性的低资源多模态学习方法，并解决多模态模型在语言任务上表现不佳的问题。

Method: 构建语言-only和多模态模型，使用发展合理的低资源数据集；采用加权线性插值进行模型融合，以保留多模态能力的同时增强语言任务性能。

Result: 多模态模型在语言任务（尤其是语法相关基准）上表现不如语言模型；通过与纯文本模型融合，可在一定程度上缓解该问题，且不牺牲多模态性能。

Conclusion: 模型融合是一种有效策略，能够在保持多模态能力的同时改善语言生成和理解能力，推动更接近人类儿童学习方式的高效、低资源AI系统发展。

Abstract: State-of-the-art vision-and-language models consist of many parameters and
learn from enormous datasets, surpassing the amounts of linguistic data that
children are exposed to as they acquire a language. This paper presents our
approach to the multimodal track of the BabyLM challenge addressing this
discrepancy. We develop language-only and multimodal models in low-resource
settings using developmentally plausible datasets, with our multimodal models
outperforming previous BabyLM baselines. One finding in the multimodal language
model literature is that these models tend to underperform in
\textit{language-only} tasks. Therefore, we focus on maintaining language-only
abilities in multimodal models. To this end, we experiment with \textit{model
merging}, where we fuse the parameters of multimodal models with those of
language-only models using weighted linear interpolation. Our results
corroborate the findings that multimodal models underperform in language-only
benchmarks that focus on grammar, and model merging with text-only models can
help alleviate this problem to some extent, while maintaining multimodal
performance.

</details>


### [144] [REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration](https://arxiv.org/abs/2510.01879)
*Yisu Wang,Ming Wang,Haoyuan Song,Wenjie Huang,Chaozheng Wang,Yi Xie,Xuming Ran*

Main category: cs.CL

TL;DR: 本文提出了REPAIR，一种用于大语言模型的终身编辑框架，通过渐进式自适应干预和重新集成，实现精确且低成本的模型更新，同时减少知识遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在后训练过程中因获取新知识或纠正错误成本高以及重训练带来的副作用问题。

Method: 提出REPAIR框架，采用闭环反馈机制、动态内存管理、频繁的知识融合和强局部性保护，以缓解大规模顺序编辑中的不稳定性和冲突。

Result: 实验表明，REPAIR在多个模型家族中将编辑准确率提高了10%-30%，并显著减少了知识遗忘。

Conclusion: REPAIR为构建可靠、可扩展且持续演化的大型语言模型提供了一个鲁棒的框架。

Abstract: Post-training for large language models (LLMs) is constrained by the high
cost of acquiring new knowledge or correcting errors and by the unintended side
effects that frequently arise from retraining. To address these issues, we
introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and
Reintegration), a lifelong editing framework designed to support precise and
low-cost model updates while preserving non-target knowledge. REPAIR mitigates
the instability and conflicts of large-scale sequential edits through a
closed-loop feedback mechanism coupled with dynamic memory management.
Furthermore, by incorporating frequent knowledge fusion and enforcing strong
locality guards, REPAIR effectively addresses the shortcomings of traditional
distribution-agnostic approaches that often overlook unintended ripple effects.
Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%
across multiple model families and significantly reduces knowledge forgetting.
This work introduces a robust framework for developing reliable, scalable, and
continually evolving LLMs.

</details>


### [145] [Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey](https://arxiv.org/abs/2510.01925)
*Qiyuan Liu,Hao Xu,Xuhong Chen,Wei Chen,Yee Whye Teh,Ning Miao*

Main category: cs.CL

TL;DR: 本文系统介绍了用于提升大语言模型（LLM）推理能力的奖励模型（RM），综述了其架构、训练方法、评估技术及在推理、数据合成和强化学习微调中的应用，并探讨了RM在选择、泛化、评估和增强方面的开放问题。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在提升大语言模型推理性能中起关键作用，但缺乏系统性介绍和全面的应用综述，需要梳理现有研究并提出未来方向。

Method: 通过系统性回顾奖励模型的基本概念（包括架构、训练与评估方法），分类总结其在LLM推理中的三大应用，并结合已有研究与实证分析关键开放问题。

Result: 全面梳理了奖励模型在LLM推理中的应用体系，明确了其在输出选择、数据合成和强化学习中的作用，并识别出RM在泛化性、评估标准和改进策略等方面的挑战。

Conclusion: 奖励模型对提升LLM推理至关重要，未来需在可信赖性、跨任务泛化和高效评估方面进一步研究，以推动其有效部署与发展。

Abstract: Reward models (RMs) play a critical role in enhancing the reasoning
performance of LLMs. For example, they can provide training signals to finetune
LLMs during reinforcement learning (RL) and help select the best answer from
multiple candidates during inference. In this paper, we provide a systematic
introduction to RMs, along with a comprehensive survey of their applications in
LLM reasoning. We first review fundamental concepts of RMs, including their
architectures, training methodologies, and evaluation techniques. Then, we
explore their key applications: (1) guiding generation and selecting optimal
outputs during LLM inference, (2) facilitating data synthesis and iterative
self-improvement for LLMs, and (3) providing training signals in RL-based
finetuning. Finally, we address critical open questions regarding the
selection, generalization, evaluation, and enhancement of RMs, based on
existing research and our own empirical findings. Our analysis aims to provide
actionable insights for the effective deployment and advancement of RMs for LLM
reasoning.

</details>


### [146] [Inverse Language Modeling towards Robust and Grounded LLMs](https://arxiv.org/abs/2510.01929)
*Davide Gabrielli,Simone Sestito,Iacopo Masi*

Main category: cs.CL

TL;DR: 提出了一种名为逆语言建模（ILM）的统一框架，用于提升大语言模型（LLMs）对输入扰动的鲁棒性，并通过反转模型输出实现原生接地，识别潜在有害输入。


<details>
  <summary>Details</summary>
Motivation: 当前针对大语言模型的防御机制零散且不成熟，缺乏类似分类器领域的系统性方法，亟需提升LLMs的对抗鲁棒性和安全性。

Method: 提出逆语言建模（ILM）框架，通过统一方法增强LLMs对输入扰动的鲁棒性，并反转模型输出以识别可能导致有害响应的输入触发词。

Result: ILM使LLMs从静态生成器转变为可分析、更鲁棒的系统，支持原生接地和红队测试，为下一代更可控、可信的LLMs奠定基础。

Conclusion: ILM为提升大语言模型的鲁棒性、可控性和信任度提供了可行路径，具有推动安全LLMs发展的潜力。

Abstract: The current landscape of defensive mechanisms for LLMs is fragmented and
underdeveloped, unlike prior work on classifiers. To further promote
adversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a
unified framework that simultaneously 1) improves the robustness of LLMs to
input perturbations, and, at the same time, 2) enables native grounding by
inverting model outputs to identify potentially toxic or unsafe input triggers.
ILM transforms LLMs from static generators into analyzable and robust systems,
potentially helping RED teaming. ILM can lay the foundation for next-generation
LLMs that are not only robust and grounded but also fundamentally more
controllable and trustworthy. The code is publicly available at
github.com/davegabe/pag-llm.

</details>


### [147] [Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning](https://arxiv.org/abs/2510.01932)
*Qi He,Cheng Qian,Xiusi Chen,Bingxiang He,Yi R.,Fung,Heng Ji*

Main category: cs.CL

TL;DR: 提出Veri-R1，一种基于在线强化学习的框架，使大语言模型能与搜索引擎交互，通过奖励信号优化其规划、检索和推理能力，显著提升声明验证的准确性和证据质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖提示工程或预设推理流程，缺乏统一训练范式来提升模型在迭代检索与推理中的综合验证能力。

Method: 引入Veri-R1，采用在线强化学习框架，让大语言模型与搜索引擎动态交互，并通过显式奖励信号塑造其规划、检索和推理行为。

Result: 实验表明，Veri-R1将联合准确率提升高达30%，证据得分翻倍，且常优于更大规模的模型；消融研究揭示了奖励组件的影响及输出logits与标签准确性的关系。

Conclusion: 在线强化学习能有效提升大语言模型在声明验证中的精确性与忠实性，为未来研究提供了可行路径和基础。

Abstract: Claim verification with large language models (LLMs) has recently attracted
considerable attention, owing to their superior reasoning capabilities and
transparent verification pathways compared to traditional answer-only
judgments. Online claim verification requires iterative evidence retrieval and
reasoning, yet existing approaches mainly rely on prompt engineering or
predesigned reasoning workflows without offering a unified training paradigm to
improve necessary skills. Therefore, we introduce Veri-R1, an online
reinforcement learning (RL) framework that enables an LLM to interact with a
search engine and to receive reward signals that explicitly shape its planning,
retrieval, and reasoning behaviors. The dynamic interaction between models and
retrieval systems more accurately reflects real-world verification scenarios
and fosters comprehensive verification skills. Empirical results show that
Veri-R1 improves joint accuracy by up to 30% and doubles evidence score, often
surpassing larger-scale counterparts. Ablation studies further reveal the
impact of reward components and the link between output logits and label
accuracy. Our results highlight the effectiveness of online RL for precise and
faithful claim verification and provide a foundation for future research. We
release our code to support community progress in LLM empowered claim
verification.

</details>


### [148] [Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion, Argument, and Topic Annotations](https://arxiv.org/abs/2510.01976)
*Adina Nicola Dobrinoiu,Ana Cristiana Marcu,Amir Homayounirad,Luciano Cavalcante Siebert,Enrico Liscio*

Main category: cs.CL

TL;DR: 本文探讨了语言模型是否能通过多维主观标注（如情感、情绪、论点和主题）来预测个体的价值观解释，发现同时提供这些维度的信息在零样本和少样本设置下表现更优，强调了考虑个体差异对价值观预测的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于个体的价值观解释受社会文化背景和生活经验影响而具有主观性，为开发能够与多样化人类视角对齐并避免偏向主流观点的AI系统，需识别个体的价值解释差异。

Method: 研究利用情感、情绪、论点和话题（SEAT维度）的多维主观注释作为个体解释视角的代理，评估在不同零样本和少样本设置下，语言模型基于这些注释预测个体价值观的能力。

Result: 实验结果表明，同时提供所有SEAT维度信息时，语言模型的预测性能优于单一维度或无个体信息的基线；此外，不同标注者之间的个体差异凸显了纳入主观标注行为的重要性。

Conclusion: 该研究首次在控制环境下探索标注行为对价值观预测的影响，超越了人口统计学特征，尽管规模较小，但为未来的大规模验证奠定了基础。

Abstract: Our interpretation of value concepts is shaped by our sociocultural
background and lived experiences, and is thus subjective. Recognizing
individual value interpretations is important for developing AI systems that
can align with diverse human perspectives and avoid bias toward majority
viewpoints. To this end, we investigate whether a language model can predict
individual value interpretations by leveraging multi-dimensional subjective
annotations as a proxy for their interpretive lens. That is, we evaluate
whether providing examples of how an individual annotates Sentiment, Emotion,
Argument, and Topics (SEAT dimensions) helps a language model in predicting
their value interpretations. Our experiment across different zero- and few-shot
settings demonstrates that providing all SEAT dimensions simultaneously yields
superior performance compared to individual dimensions and a baseline where no
information about the individual is provided. Furthermore, individual
variations across annotators highlight the importance of accounting for the
incorporation of individual subjective annotators. To the best of our
knowledge, this controlled setting, although small in size, is the first
attempt to go beyond demographics and investigate the impact of annotation
behavior on value prediction, providing a solid foundation for future
large-scale validation.

</details>


### [149] [Exploring Database Normalization Effects on SQL Generation](https://arxiv.org/abs/2510.01989)
*Ryosuke Kohita*

Main category: cs.CL

TL;DR: 本研究首次系统地探讨了数据库模式规范化对自然语言转SQL（NL2SQL）性能的影响，发现去规范化模式在简单检索查询中表现更优，而规范化模式在聚合查询中更具优势，建议根据查询类型选择合适的模式设计。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL研究多忽略模式设计（尤其是规范化）的影响，通常在固定模式上评估模型，缺乏对不同规范化级别影响的系统分析。

Method: 构建具有形式化规范化级别（1NF-3NF）的合成数据集和具有实际模式的真实学术论文数据集，评估八种主流大语言模型在不同模式下的表现。

Result: 去规范化模式在简单检索查询中准确率高，且适用于低成本模型的零样本设置；规范化模式在聚合查询中表现更好，能有效避免数据重复和NULL值导致的错误，但需少量示例即可缓解其带来的连接和基表选择错误。

Conclusion: NL2SQL系统的最优模式设计取决于目标查询类型，应根据应用场景自适应选择模式，未来系统应集成动态模式选择机制。

Abstract: Schema design, particularly normalization, is a critical yet often overlooked
factor in natural language to SQL (NL2SQL) systems. Most prior research
evaluates models on fixed schemas, overlooking the influence of design on
performance. We present the first systematic study of schema normalization's
impact, evaluating eight leading large language models on synthetic and
real-world datasets with varied normalization levels. We construct controlled
synthetic datasets with formal normalization (1NF-3NF) and real academic paper
datasets with practical schemes. Our results show that denormalized schemas
offer high accuracy on simple retrieval queries, even with cost-effective
models in zero-shot settings. In contrast, normalized schemas (2NF/3NF)
introduce challenges such as errors in base table selection and join type
prediction; however, these issues are substantially mitigated by providing
few-shot examples. For aggregation queries, normalized schemas yielded better
performance, mainly due to their robustness against the data duplication and
NULL value issues that cause errors in denormalized schemas. These findings
suggest that the optimal schema design for NL2SQL applications depends on the
types of queries to be supported. Our study demonstrates the importance of
considering schema design when developing NL2SQL interfaces and integrating
adaptive schema selection for real-world scenarios.

</details>


### [150] [LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and Target](https://arxiv.org/abs/2510.01995)
*Md Arid Hasan,Firoj Alam,Md Fahad Hossain,Usman Naseem,Syed Ishtiaque Ahmed*

Main category: cs.CL

TL;DR: 本文提出了首个用于孟加拉语的多任务仇恨言论数据集BanglaMultiHate，并通过多种模型对比实验，探讨了在低资源环境下大型语言模型的适应性，强调了文化与语言背景预训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 针对现有孟加拉语仇恨言论检测研究多为单任务、覆盖维度有限的问题，亟需构建更全面的多任务数据集和评估方法以提升低资源语言的内容审核能力。

Method: 构建了大规模人工标注的多任务数据集BanglaMultiHate，采用经典基线模型、单语预训练模型和大语言模型（零样本提示与LoRA微调）进行系统性对比实验。

Result: 实验证明LoRA微调的大模型性能接近BanglaBERT，但具有文化与语言基础的预训练对鲁棒性至关重要，所提出的数据集显著提升了低资源场景下的基准水平。

Conclusion: BanglaMultiHate为低资源语言的多任务仇恨言论检测提供了重要资源，研究结果强调了文化对齐预训练在开发有效内容审核工具中的关键作用。

Abstract: Online social media platforms are central to everyday communication and
information seeking. While these platforms serve positive purposes, they also
provide fertile ground for the spread of hate speech, offensive language, and
bullying content targeting individuals, organizations, and communities. Such
content undermines safety, participation, and equity online. Reliable detection
systems are therefore needed, especially for low-resource languages where
moderation tools are limited. In Bangla, prior work has contributed resources
and models, but most are single-task (e.g., binary hate/offense) with limited
coverage of multi-facet signals (type, severity, target). We address these gaps
by introducing the first multi-task Bangla hate-speech dataset,
BanglaMultiHate, one of the largest manually annotated corpus to date. Building
on this resource, we conduct a comprehensive, controlled comparison spanning
classical baselines, monolingual pretrained models, and LLMs under zero-shot
prompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a
low-resource setting and reveal a consistent trend: although LoRA-tuned LLMs
are competitive with BanglaBERT, culturally and linguistically grounded
pretraining remains critical for robust performance. Together, our dataset and
findings establish a stronger benchmark for developing culturally aligned
moderation tools in low-resource contexts. For reproducibility, we will release
the dataset and all related scripts.

</details>


### [151] [Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models](https://arxiv.org/abs/2510.02025)
*Donghoon Jung,Jiwoo Choi,Songeun Chae,Seohyon Jung*

Main category: cs.CL

TL;DR: 本研究采用叙事学的方法，从过程导向的角度分析大语言模型（LLM）作为计算作者的创作过程，提出基于约束的决策机制作为评估其创作力的新视角。通过控制提示赋予模型不同的作者角色，发现LLM普遍更重视“风格”而非人物、事件或场景，并揭示了不同模型在创作偏好和推理上的独特特征。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型创造力的评估多关注输出质量，而忽视其生成过程。本文旨在填补这一空白，通过引入过程导向的分析框架，深入理解LLM作为‘作者’的创造性行为。

Method: 借鉴叙事学理论，将LLM视为计算作者；设计控制性提示以赋予模型特定的作者角色；引入基于约束的决策机制分析模型在叙事元素（如风格、人物、事件、设定）之间的选择偏好及其解释逻辑。

Result: 实验表明，LLM在创作决策中始终优先考虑‘风格’；不同模型展现出可识别的创造性偏好模式；其自我解释也反映出差异化的决策逻辑，形成独特的‘作者画像’。

Conclusion: 基于约束的决策分析为评估LLM的作者性创造力提供了一种新颖且系统的工具，有助于深入理解AI创作的过程机制，并推动更具解释性的AI创意系统发展。

Abstract: Evaluations of large language models (LLMs)' creativity have focused
primarily on the quality of their outputs rather than the processes that shape
them. This study takes a process-oriented approach, drawing on narratology to
examine LLMs as computational authors. We introduce constraint-based
decision-making as a lens for authorial creativity. Using controlled prompting
to assign authorial personas, we analyze the creative preferences of the
models. Our findings show that LLMs consistently emphasize Style over other
elements, including Character, Event, and Setting. By also probing the
reasoning the models provide for their choices, we show that distinctive
profiles emerge across models and argue that our approach provides a novel
systematic tool for analyzing AI's authorial creativity.

</details>


### [152] [Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage](https://arxiv.org/abs/2510.02044)
*Siddhant Arora,Haidar Khan,Kai Sun,Xin Luna Dong,Sajal Choudhary,Seungwhan Moon,Xinyuan Zhang,Adithya Sagar,Surya Teja Appini,Kaushik Patnaik,Sanat Sharma,Shinji Watanabe,Anuj Kumar,Ahmed Aly,Yue Liu,Florian Metze,Zhaojiang Lin*

Main category: cs.CL

TL;DR: 提出Streaming RAG框架，实现语音对话系统中流式检索增强生成，显著提升问答准确率并降低工具使用延迟。


<details>
  <summary>Details</summary>
Motivation: 端到端语音对话系统易因事实依据不足而产生幻觉，传统方法通过文本工具（如搜索API）增强，但直接在语音系统中引入工具会增加延迟，影响交互流畅性。

Method: 设计Streaming RAG框架，通过在用户说话时并行预测工具查询，实现流式检索增强；开发后训练流程，使模型学会在语音输入过程中动态触发工具调用，并融合音频查询与文本检索结果生成口语化回答。

Result: 在AudioCRAG基准上实验显示，相比基线，问答准确率相对提升200%（绝对值从11.1%提升至34.2%），工具使用延迟减少20%；该方法支持多模态输入，适用于文本和语音。

Conclusion: Streaming RAG有效解决了语音对话系统中工具调用带来的延迟问题，在提升响应准确性和用户体验的同时保持低延迟，为实现实时、具身的AI助手提供了可行路径。

Abstract: End-to-end speech-in speech-out dialogue systems are emerging as a powerful
alternative to traditional ASR-LLM-TTS pipelines, generating more natural,
expressive responses with significantly lower latency. However, these systems
remain prone to hallucinations due to limited factual grounding. While
text-based dialogue systems address this challenge by integrating tools such as
web search and knowledge graph APIs, we introduce the first approach to extend
tool use directly into speech-in speech-out systems. A key challenge is that
tool integration substantially increases response latency, disrupting
conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented
Generation (Streaming RAG), a novel framework that reduces user-perceived
latency by predicting tool queries in parallel with user speech, even before
the user finishes speaking. Specifically, we develop a post-training pipeline
that teaches the model when to issue tool calls during ongoing speech and how
to generate spoken summaries that fuse audio queries with retrieved text
results, thereby improving both accuracy and responsiveness. To evaluate our
approach, we construct AudioCRAG, a benchmark created by converting queries
from the publicly available CRAG dataset into speech form. Experimental results
demonstrate that our streaming RAG approach increases QA accuracy by up to 200%
relative (from 11.1% to 34.2% absolute) and further enhances user experience by
reducing tool use latency by 20%. Importantly, our streaming RAG approach is
modality-agnostic and can be applied equally to typed input, paving the way for
more agentic, real-time AI assistants.

</details>


### [153] [Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken Dialogue Systems](https://arxiv.org/abs/2510.02066)
*Siddhant Arora,Jinchuan Tian,Hayato Futami,Jiatong Shi,Yosuke Kashiwagi,Emiru Tsunoo,Shinji Watanabe*

Main category: cs.CL

TL;DR: 提出了一种基于流式思维链（SCoT）的双工对话系统框架，通过分块处理用户输入并生成对齐的中间目标，实现了低延迟、可解释的对话响应。


<details>
  <summary>Details</summary>
Motivation: 现有端到端对话系统依赖语音活动检测（VAD）进行换轮控制，但难以区分停顿与话语结束；双工模型虽能连续预测输出，但在语义推理和架构复杂性上存在不足。

Method: 提出SCoT框架，采用流式思维链机制，将用户输入按固定时长分块处理，并利用帧级对齐生成每个块对应的中间目标（如对齐的转录和响应），实现块间交替的流式处理。

Result: 实验表明，该方法相比现有双工模型能生成更连贯、可解释的响应，同时支持比逐轮系统更低的延迟和重叠交互。

Conclusion: SCoT框架有效解决了传统VAD在换轮判断上的局限性和双工模型在语义推理上的不足，为高效、自然的端到端对话系统提供了新思路。

Abstract: Most end-to-end (E2E) spoken dialogue systems (SDS) rely on voice activity
detection (VAD) for turn-taking, but VAD fails to distinguish between pauses
and turn completions. Duplex SDS models address this by predicting output
continuously, including silence tokens, thus removing the need for explicit
VAD. However, they often have complex dual-channel architecture and lag behind
cascaded models in semantic reasoning. To overcome these challenges, we propose
SCoT: a Streaming Chain-of-Thought (CoT) framework for Duplex SDS, alternating
between processing fixed-duration user input and generating responses in a
blockwise manner. Using frame-level alignments, we create intermediate
targets-aligned user transcripts and system responses for each block.
Experiments show that our approach produces more coherent and interpretable
responses than existing duplex methods while supporting lower-latency and
overlapping interactions compared to turn-by-turn systems.

</details>


### [154] [The Disparate Impacts of Speculative Decoding](https://arxiv.org/abs/2510.02128)
*Jameson Sandler,Ahmet Üstün,Marco Romanelli,Sara Hooker,Ferdinando Fioretto*

Main category: cs.CL

TL;DR: 本文研究了推测性解码在不同任务间的加速效果差异，发现其对欠拟合和代表性不足的任务加速效果较差，并提出一种缓解策略，平均提升了12%的公平性指标。


<details>
  <summary>Details</summary>
Motivation: 推测性解码虽能加速大语言模型推理，但其在不同任务上的加速效果不均，可能加剧性能差距，尤其影响欠拟合和少数群体任务，因此需探究其不公平性并加以缓解。

Method: 通过理论分析量化推测性解码的不公平性，识别导致加速差异的关键因素，并基于此设计了一种新的缓解策略，在多个模型组合上进行验证。

Result: 实验证明推测性解码的加速效果在不同任务上存在显著差异，所提出的策略有效减少了这种差异，平均使公平性指标提升12%。

Conclusion: 推测性解码的加速效益并非对所有任务公平，需针对性优化以提升整体系统公平性，本文提出的策略为实现更均衡的推理加速提供了可行路径。

Abstract: The practice of speculative decoding, whereby inference is probabilistically
supported by a smaller, cheaper, ``drafter'' model, has become a standard
technique for systematically reducing the decoding time of large language
models. This paper conducts an analysis of speculative decoding through the
lens of its potential disparate speed-up rates across tasks. Crucially, the
paper shows that speed-up gained from speculative decoding is not uniformly
distributed across tasks, consistently diminishing for under-fit, and often
underrepresented tasks. To better understand this phenomenon, we derive an
analysis to quantify this observed ``unfairness'' and draw attention to the
factors that motivate such disparate speed-ups to emerge. Further, guided by
these insights, the paper proposes a mitigation strategy designed to reduce
speed-up disparities and validates the approach across several model pairs,
revealing on average a 12% improvement in our fairness metric.

</details>


### [155] [RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization](https://arxiv.org/abs/2510.02172)
*Zhaoning Yu,Will Su,Leitian Tao,Haozhu Wang,Aashu Singh,Hanchao Yu,Jianyu Wang,Hongyang Gao,Weizhe Yuan,Jason Weston,Ping Yu,Jing Xu*

Main category: cs.CL

TL;DR: RESTRAIN是一种无需黄金标签的自惩罚强化学习框架，通过利用模型的答案分布信号（如惩罚过度自信的推理路径和低一致性样本），实现基于无标注数据的自我改进，在多个复杂推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类标注数据的强化学习在提升大模型推理能力方面成本高昂且难以应对更复杂的任务，因此需要一种不依赖标注数据、能从无标签经验中持续学习的方法。

Method: 提出RESTRAIN框架，引入自惩罚机制：利用模型自身生成的整个答案分布作为学习信号，对过度自信的推理路径和低一致性的样例进行惩罚，同时保留有潜力的推理链，并将其集成到GRPO等策略优化方法中，实现无监督下的持续自我改进。

Result: 在AIME25、MMLU_STEM和GPQA-Diamond等多个高难度推理基准上，使用Qwen3-4B-Base和OctoThinker Hybrid-8B-Base模型，Pass@1分别提升最高达+140.7%、+36.2%和+19.6%，性能接近使用黄金标签训练的结果，但完全无需标注数据。

Conclusion: RESTRAIN为不依赖黄金标签的大规模推理能力提升提供了可扩展的有效路径，展示了无监督强化学习在复杂推理任务中的巨大潜力。

Abstract: Reinforcement learning with human-annotated data has boosted chain-of-thought
reasoning in large reasoning models, but these gains come at high costs in
labeled data while faltering on harder tasks. A natural next step is
experience-driven learning, where models improve without curated labels by
adapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with
Self-restraint), a self-penalizing RL framework that converts the absence of
gold labels into a useful learning signal. Instead of overcommitting to
spurious majority votes, RESTRAIN exploits signals from the model's entire
answer distribution: penalizing overconfident rollouts and low-consistency
examples while preserving promising reasoning chains. The self-penalization
mechanism integrates seamlessly into policy optimization methods such as GRPO,
enabling continual self-improvement without supervision. On challenging
reasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.
With Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to
+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on
GPQA-Diamond, nearly matching gold-label training while using no gold labels.
These results demonstrate that RESTRAIN establishes a scalable path toward
stronger reasoning without gold labels.

</details>


### [156] [Learning to Reason for Hallucination Span Detection](https://arxiv.org/abs/2510.02173)
*Hsuan Su,Ting-Yao Hu,Hema Swetha Koppula,Kundan Krishna,Hadi Pouransari,Cheng-Yu Hsieh,Cem Koc,Joseph Yitan Cheng,Oncel Tuzel,Raviteja Vemulapalli*

Main category: cs.CL

TL;DR: 提出了一种基于强化学习的框架RL4HS，用于检测大语言模型生成文本中的幻觉片段，通过引入细粒度奖励机制显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作多将幻觉检测视为二分类任务，难以满足实际应用中对定位幻觉片段的需求；需要探索显式推理是否有助于更细粒度的幻觉检测。

Method: 首先评估了带与不带思维链（CoT）的预训练模型在多次采样下的表现，进而提出RL4HS框架，结合Group Relative Policy Optimization和新设计的Class-Aware Policy Optimization，采用基于片段级别的奖励函数进行强化学习训练。

Result: 在RAGTruth基准（涵盖摘要、问答、数据到文本）上的实验表明，RL4HS优于预训练推理模型和监督微调方法，在幻觉片段检测上表现出更强性能。

Conclusion: 显式推理结合片段级强化学习奖励是提升幻觉片段检测效果的关键，RL4HS验证了该路径的有效性和必要性。

Abstract: Large language models (LLMs) often generate hallucinations -- unsupported
content that undermines reliability. While most prior works frame hallucination
detection as a binary task, many real-world applications require identifying
hallucinated spans, which is a multi-step decision making process. This
naturally raises the question of whether explicit reasoning can help the
complex task of detecting hallucination spans. To answer this question, we
first evaluate pretrained models with and without Chain-of-Thought (CoT)
reasoning, and show that CoT reasoning has the potential to generate at least
one correct answer when sampled multiple times. Motivated by this, we propose
RL4HS, a reinforcement learning framework that incentivizes reasoning with a
span-level reward function. RL4HS builds on Group Relative Policy Optimization
and introduces Class-Aware Policy Optimization to mitigate reward imbalance
issue. Experiments on the RAGTruth benchmark (summarization, question
answering, data-to-text) show that RL4HS surpasses pretrained reasoning models
and supervised fine-tuning, demonstrating the necessity of reinforcement
learning with span-level rewards for detecting hallucination spans.

</details>


### [157] [ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities](https://arxiv.org/abs/2510.02200)
*Felix Brei,Lorenz Bühmann,Johannes Frey,Daniel Gerber,Lars-Peter Meyer,Claus Stadler,Kirill Bulert*

Main category: cs.CL

TL;DR: 本文提出了一种基于SPINACH的通用方法，通过大语言模型将自然语言问题迭代地转换为SPARQL查询，以降低非专业用户使用知识图谱的门槛。


<details>
  <summary>Details</summary>
Motivation: 由于SPARQL查询语言对非计算机专业背景的人门槛较高，本文旨在利用大语言模型降低这一障碍，提升Text2SPARQL技术的效果。

Method: 采用基于LLM的代理SPINACH，通过探索与执行的迭代过程，而非单次转换，实现自然语言到SPARQL查询的翻译，并分析其架构设计与推理机制。

Result: 提出了一个可迭代的Text2SPARQL框架，对代理行为进行了深入分析，揭示了其在查询生成中的优势与不足。

Conclusion: 该方法有效降低了用户与知识图谱交互的难度，为进一步优化Text2SPARQL系统提供了方向和见解。

Abstract: Interacting with knowledge graphs can be a daunting task for people without a
background in computer science since the query language that is used (SPARQL)
has a high barrier of entry. Large language models (LLMs) can lower that
barrier by providing support in the form of Text2SPARQL translation. In this
paper we introduce a generalized method based on SPINACH, an LLM backed agent
that translates natural language questions to SPARQL queries not in a single
shot, but as an iterative process of exploration and execution. We describe the
overall architecture and reasoning behind our design decisions, and also
conduct a thorough analysis of the agent behavior to gain insights into future
areas for targeted improvements. This work was motivated by the Text2SPARQL
challenge, a challenge that was held to facilitate improvements in the
Text2SPARQL domain.

</details>


### [158] [Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents](https://arxiv.org/abs/2510.02204)
*Lingzhong Dong,Ziqi Zhou,Shuaibo Yang,Haiyue Sheng,Pengzhou Cheng,Zongru Wu,Zheng Wu,Gongshen Liu,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估框架，用于诊断基于视觉语言模型的移动使用代理中的推理-执行差距，核心是“真实对齐”（GTA）指标，结合精确匹配（EM）来同时评估推理和执行准确性，揭示了普遍存在的执行差距和推理差距。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注执行准确性，但忽视了链式思维（CoT）推理是否与真实动作对齐，可能导致用户过度信任并引发安全风险。因此需要一种能检测推理与执行之间差距的评估方法。

Method: 提出Ground-Truth Alignment（GTA）指标，衡量CoT推理解释的动作是否与真实动作一致，并结合Exact Match（EM）指标进行联合评估，识别执行差距（EG）和推理差距（RG）。

Result: 实验发现推理-执行差距普遍存在，其中执行差距多于推理差距；增大模型规模可减少整体差距，但大型模型仍存在显著执行差距；该框架能可靠反映当前最先进模型中的系统性EG/RG模式。

Conclusion: 所提出的评估框架有助于诊断移动使用代理中的可信问题，支持构建更可靠、透明的智能代理系统。

Abstract: Mobile-use agents powered by vision-language models (VLMs) have shown great
potential in interpreting natural language instructions and generating
corresponding actions based on mobile graphical user interface. Recent studies
suggest that incorporating chain-of-thought (CoT) reasoning tends to improve
the execution accuracy. However, existing evaluations emphasize execution
accuracy while neglecting whether CoT reasoning aligns with ground-truth
actions. This oversight fails to assess potential reasoning-execution gaps,
which in turn foster over-trust: users relying on seemingly plausible CoTs may
unknowingly authorize harmful actions, potentially resulting in financial loss
or trust crisis. In this work, we introduce a new evaluation framework to
diagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment
(GTA), which measures whether the action implied by a CoT matches the
ground-truth action. By combining GTA with the standard Exact Match (EM)
metric, we jointly assess both the reasoning accuracy and execution accuracy.
This joint perspective reveals two types of reasoning-execution gaps: (i)
Execution Gap (EG), where the reasoning correctly identifies the correct action
but execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but
reasoning process conflicts with the actual execution. Experimental results
across a wide range of mobile interaction tasks reveal that reasoning-execution
gaps are prevalent, with execution gaps occurring more frequently than
reasoning gaps. Moreover, while scaling up model size reduces the overall gap,
sizable execution gaps persist even in the largest models. Further analysis
shows that our framework reliably reflects systematic EG/RG patterns in
state-of-the-art models. These findings offer concrete diagnostics and support
the development of more trustworthy mobile-use agents.

</details>


### [159] [More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration](https://arxiv.org/abs/2510.02227)
*Xiaoyang Yuan,Yujuan Ding,Yi Bin,Wenqi Shao,Jinyu Cai,Jingkuan Song,Yang Yang,Hengtao Shen*

Main category: cs.CL

TL;DR: 本文提出了一种名为自适应多引导策略优化（AMPO）的新框架，通过在需要时从多个教师模型获取指导来提升大语言模型的推理能力，相较于依赖单一教师或自我探索的方法，在数学推理和分布外任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习与可验证奖励方法主要依赖自我探索或单一教师模型，容易引入模型偏差并限制探索，从而影响推理多样性与性能。因此，需要一种能有效利用多教师优势的新方法。

Method: 受知识蒸馏中多教师策略启发，提出AMPO框架，采用‘按需指导’机制：仅当在线策略模型无法生成正确解时，才自适应地从多个熟练的教师模型获取指导，并结合基于理解能力的选择机制，使学生模型优先学习其更容易理解的推理路径。

Result: 实验表明，AMPO在数学推理任务上比强基线GRPO提升4.3%，在分布外任务上提升12.2%，显著提高Pass@k性能并促进更多样化的探索；使用四个同规模教师即可达到使用更强单一教师（如DeepSeek-R1）的相当效果。

Conclusion: AMPO提供了一条更高效、可扩展的路径，以实现更优的推理能力和泛化性，验证了多教师协同指导在增强大语言模型推理中的潜力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm
for enhancing the reasoning ability in Large Language Models (LLMs). However,
prevailing methods primarily rely on self-exploration or a single off-policy
teacher to elicit long chain-of-thought (LongCoT) reasoning, which may
introduce intrinsic model biases and restrict exploration, ultimately limiting
reasoning diversity and performance. Drawing inspiration from multi-teacher
strategies in knowledge distillation, we introduce Adaptive Multi-Guidance
Policy Optimization (AMPO), a novel framework that adaptively leverages
guidance from multiple proficient teacher models, but only when the on-policy
model fails to generate correct solutions. This "guidance-on-demand" approach
expands exploration while preserving the value of self-discovery. Moreover,
AMPO incorporates a comprehension-based selection mechanism, prompting the
student to learn from the reasoning paths that it is most likely to comprehend,
thus balancing broad exploration with effective exploitation. Extensive
experiments show AMPO substantially outperforms a strong baseline (GRPO), with
a 4.3% improvement on mathematical reasoning tasks and 12.2% on
out-of-distribution tasks, while significantly boosting Pass@k performance and
enabling more diverse exploration. Notably, using four peer-sized teachers, our
method achieves comparable results to approaches that leverage a single, more
powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate
a more efficient and scalable path to superior reasoning and generalizability.
Our code is available at https://github.com/SII-Enigma/AMPO.

</details>


### [160] [Enhanced Arabic-language cyberbullying detection: deep embedding and transformer (BERT) approaches](https://arxiv.org/abs/2510.02232)
*Ebtesam Jaber Aljohani,Wael M. S. Yafoo*

Main category: cs.CL

TL;DR: 本文提出并评估了多种深度学习模型用于检测阿拉伯语网络欺凌，构建了一个包含10,662条X平台帖子的数据集，并通过实验发现Bi-LSTM结合FastText词嵌入表现最佳，准确率达到98%。


<details>
  <summary>Details</summary>
Motivation: 现有网络欺凌检测方法多集中于英语，针对阿拉伯语的检测研究稀缺，亟需提升阿拉伯语内容中网络欺凌识别的有效性。

Method: 收集并预处理10,662条阿拉伯语X帖子数据，使用kappa工具确保标注质量；采用LSTM、Bi-LSTM与不同词嵌入（包括FastText和BERT）结合的方法，并进行四组深度学习模型实验。

Result: LSTM-BERT和Bi-LSTM-BERT模型均达到97%的准确率，而Bi-LSTM结合FastText词嵌入表现更优，准确率达98%。

Conclusion: Bi-LSTM模型结合FastText词嵌入在阿拉伯语网络欺凌检测中表现最优，具有良好的泛化能力，为阿拉伯语社交媒体内容安全提供了有效技术方案。

Abstract: Recent technological advances in smartphones and communications, including
the growth of such online platforms as massive social media networks such as X
(formerly known as Twitter) endangers young people and their emotional
well-being by exposing them to cyberbullying, taunting, and bullying content.
Most proposed approaches for automatically detecting cyberbullying have been
developed around the English language, and methods for detecting
Arabic-language cyberbullying are scarce. Methods for detecting Arabic-language
cyberbullying are especially scarce. This paper aims to enhance the
effectiveness of methods for detecting cyberbullying in Arabic-language
content. We assembled a dataset of 10,662 X posts, pre-processed the data, and
used the kappa tool to verify and enhance the quality of our annotations. We
conducted four experiments to test numerous deep learning models for
automatically detecting Arabic-language cyberbullying. We first tested a long
short-term memory (LSTM) model and a bidirectional long short-term memory
(Bi-LSTM) model with several experimental word embeddings. We also tested the
LSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder from
representations (BERT) and then tested them on a different experimental models
BERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTM
with FastText embedding word performed even better, achieving 98% accuracy. As
a result, the outcomes are generalize

</details>


### [161] [AccurateRAG: A Framework for Building Accurate Retrieval-Augmented Question-Answering Applications](https://arxiv.org/abs/2510.02243)
*Linh The Nguyen,Chi Tran,Dung Ngoc Nguyen,Van-Cuong Pham,Hoang Ngo,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: AccurateRAG是一个基于检索增强生成（RAG）的高性能问答应用框架，提供从数据处理到系统构建的完整流程工具，并在基准测试中达到新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 为了提升检索增强生成（RAG）在问答任务中的性能和开发效率，解决现有方法在准确性与系统集成方面的不足。

Method: 提出AccurateRAG框架，包含原始数据处理、微调数据生成、文本嵌入、大模型微调、输出评估以及本地RAG系统构建等模块化工具链。

Result: 实验结果显示该框架优于以往强基线方法，在多个标准问答数据集上实现了最先进的性能。

Conclusion: AccurateRAG为高效、高精度的RAG系统开发提供了实用且可扩展的解决方案，推动了RAG技术在实际问答应用中的落地。

Abstract: We introduce AccurateRAG -- a novel framework for constructing
high-performance question-answering applications based on retrieval-augmented
generation (RAG). Our framework offers a pipeline for development efficiency
with tools for raw dataset processing, fine-tuning data generation, text
embedding & LLM fine-tuning, output evaluation, and building RAG systems
locally. Experimental results show that our framework outperforms previous
strong baselines and obtains new state-of-the-art question-answering
performance on benchmark datasets.

</details>


### [162] [Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation](https://arxiv.org/abs/2510.02249)
*Tianyi Jiang,Yi Bin,Yujuan Ding,Kainian Zhu,Fei Ma,Jingkuan Song,Heng Tao Shen*

Main category: cs.CL

TL;DR: 提出一种新的推理范式“先简要探索，再决定”，通过累积熵调节机制（CER）和新指标TECA动态控制推理深度，有效缓解大模型在简单问题上的过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在简单问题上常出现过度推理（overthinking），导致效率下降，难以根据问题复杂度自适应调整推理深度。

Method: 引入Token Entropy Cumulative Average（TECA）作为衡量推理过程中探索程度的指标，并提出“Explore Briefly, Then Decide”推理范式及累积熵调节（CER）机制，利用TECA动态判断最佳终止点。

Result: 在多个数学基准测试中，该方法显著减少过度思考，平均响应长度在简单数据集上最多减少71%，且不牺牲解题能力。

Conclusion: 所提方法实现了更高效、自适应的推理过程，使模型能根据问题复杂度动态调整推理步长。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
on complex problems using long Chain-of-Thought (CoT) reasoning. However, they
often suffer from overthinking, meaning generating unnecessarily lengthy
reasoning steps for simpler problems. This issue may degrade the efficiency of
the models and make them difficult to adapt the reasoning depth to the
complexity of problems. To address this, we introduce a novel metric Token
Entropy Cumulative Average (TECA), which measures the extent of exploration
throughout the reasoning process. We further propose a novel reasoning paradigm
-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy
Regulation (CER) mechanism. This paradigm leverages TECA to help the model
dynamically determine the optimal point to conclude its thought process and
provide a final answer, thus achieving efficient reasoning. Experimental
results across diverse mathematical benchmarks show that our approach
substantially mitigates overthinking without sacrificing problem-solving
ability. With our thinking paradigm, the average response length decreases by
up to 71% on simpler datasets, demonstrating the effectiveness of our method in
creating a more efficient and adaptive reasoning process.

</details>


### [163] [InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents](https://arxiv.org/abs/2510.02271)
*Yaxin Du,Yuanshuo Zhang,Xiyuan Yang,Yifan Zhou,Cheng Wang,Gongyi Zou,Xianghe Pang,Wenhao Wang,Menglan Chen,Shuo Tang,Zhiyu Li,Siheng Chen*

Main category: cs.CL

TL;DR: 本文提出了InfoMosaic-Bench，首个用于评估工具增强型代理在多源信息获取能力的基准，涵盖医学、金融、地图等六个领域，结果表明现有大模型在工具使用和多源信息整合方面仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理依赖开放网络搜索，但网络内容噪声大且缺乏特定领域知识，而新出现的MCP协议虽支持接入专用工具，却缺乏评估代理能否有效结合通用搜索与专用工具的研究。

Method: 设计了InfoMosaic-Bench基准和InfoMosaic-Flow生成流程，任务需结合通用搜索与领域专用工具，通过验证工具输出、设置跨源依赖和排除简单案例确保任务可靠且非平凡。

Result: 实验显示：仅靠网页信息表现不佳（GPT-5准确率38.2%）；领域工具效果不一；22.4%失败源于工具误用或误选。

Conclusion: 当前LLM代理在复杂多源信息任务中仍面临挑战，尤其在正确选择和使用工具方面，需进一步提升工具协同与推理能力。

Abstract: Information seeking is a fundamental requirement for humans. However,
existing LLM agents rely heavily on open-web search, which exposes two
fundamental weaknesses: online content is noisy and unreliable, and many
real-world tasks require precise, domain-specific knowledge unavailable from
the web. The emergence of the Model Context Protocol (MCP) now allows agents to
interface with thousands of specialized tools, seemingly resolving this
limitation. Yet it remains unclear whether agents can effectively leverage such
tools -- and more importantly, whether they can integrate them with
general-purpose search to solve complex tasks. Therefore, we introduce
InfoMosaic-Bench, the first benchmark dedicated to multi-source information
seeking in tool-augmented agents. Covering six representative domains
(medicine, finance, maps, video, web, and multi-domain integration),
InfoMosaic-Bench requires agents to combine general-purpose search with
domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable
pipeline that grounds task conditions in verified tool outputs, enforces
cross-source dependencies, and filters out shortcut cases solvable by trivial
lookup. This design guarantees both reliability and non-triviality. Experiments
with 14 state-of-the-art LLM agents reveal three findings: (i) web information
alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass
rate; (ii) domain tools provide selective but inconsistent benefits, improving
some domains while degrading others; and (iii) 22.4% of failures arise from
incorrect tool usage or selection, highlighting that current LLMs still
struggle with even basic tool handling.

</details>


### [164] [Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective](https://arxiv.org/abs/2510.02272)
*Wen Yang,Junhong Wu,Chong Li,Chengqing Zong,Jiajun Zhang*

Main category: cs.CL

TL;DR: 本研究从跨语言视角探讨了基于强化学习的推理泛化能力，发现英语为中心的大规模推理模型在跨语言迁移中表现差异显著，并提出量化指标与平行训练方法，揭示了从单语到多语迁移的跃迁现象和幂律规律。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注强化后训练在任务或模态间的泛化，但其在语言间的推理迁移能力尚不明确。本文旨在探究英语推理能力是否可有效迁移到其他语言，并分析影响跨语言迁移的关键因素。

Method: 通过在多语言推理基准上系统评估以英语为中心的大型推理模型，引入衡量跨语言可迁移性的新指标，并开展干预性和平行训练研究，分析不同模型、语言和训练范式下的迁移表现。

Result: 发现跨语言迁移效果受初始模型、目标语言和训练方式显著影响；英语能力强的模型更依赖英语特定模式，导致跨语言泛化下降；引入单个平行语言即带来性能跃升（First-Parallel Leap），且迁移性能随平行语言数量呈幂律增长（Parallel Scaling Law）；提出Monolingual Generalization Gap概念，反映单语模型未能充分跨语言泛化。

Conclusion: 挑战了大型推理模型推理能力等同于人类认知的假设，表明当前模型在跨语言推理泛化上存在局限，需发展更语言无关的模型，为构建真正多语言推理系统提供关键洞见。

Abstract: Recent advancements in Reinforcement Post-Training (RPT) have significantly
enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased
interest in the generalization of RL-based reasoning. While existing work has
primarily focused on investigating its generalization across tasks or
modalities, this study proposes a novel cross-linguistic perspective to
investigate reasoning generalization. This raises a crucial question:
$\textit{Does the reasoning capability achieved from English RPT effectively
transfer to other languages?}$ We address this by systematically evaluating
English-centric LRMs on multilingual reasoning benchmarks and introducing a
metric to quantify cross-lingual transferability. Our findings reveal that
cross-lingual transferability varies significantly across initial model, target
language, and training paradigm. Through interventional studies, we find that
models with stronger initial English capabilities tend to over-rely on
English-specific patterns, leading to diminished cross-lingual generalization.
To address this, we conduct a thorough parallel training study. Experimental
results yield three key findings: $\textbf{First-Parallel Leap}$, a substantial
leap in performance when transitioning from monolingual to just a single
parallel language, and a predictable $\textbf{Parallel Scaling Law}$, revealing
that cross-lingual reasoning transfer follows a power-law with the number of
training parallel languages. Moreover, we identify the discrepancy between
actual monolingual performance and the power-law prediction as
$\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs
fail to fully generalize across languages. Our study challenges the assumption
that LRM reasoning mirrors human cognition, providing critical insights for the
development of more language-agnostic LRMs.

</details>


### [165] [From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens](https://arxiv.org/abs/2510.02292)
*Hala Sheta,Eric Huang,Shuyu Wu,Ilia Alenabi,Jiajun Hong,Ryker Lin,Ruoxi Ning,Daniel Wei,Jialin Yang,Jiawei Zhou,Ziqiao Ma,Freda Shi*

Main category: cs.CL

TL;DR: VLM-Lens是一个开源工具包，用于系统化地评估、分析和解释视觉语言模型（VLMs），支持提取任意层的中间输出，提供统一且可配置的接口，兼容多种主流VLM及其变体。


<details>
  <summary>Details</summary>
Motivation: 为了促进对视觉语言模型内部机制的理解，需要一个能够统一、灵活地提取和分析模型中间表示的工具，而现有方法往往受限于特定模型结构，缺乏通用性和易用性。

Method: 设计并实现了一个名为VLM-Lens的工具包，通过抽象不同VLM的底层差异，提供基于YAML配置的统一接口，支持在前向传播过程中从任意层提取中间输出，并与多种可解释性方法集成。

Result: VLM-Lens目前已支持16个先进的基础VLM及其30多个变体，展示了跨模型和层级的隐藏表征差异，并可通过简单扩展支持新模型。

Conclusion: VLM-Lens为研究社区提供了一个灵活、可扩展的平台，有助于加速对视觉语言模型的深入分析与改进。

Abstract: We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.
  The toolkit integrates easily with various interpretability and analysis
methods. We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts. VLM-Lens is released as an open-sourced project to
accelerate community efforts in understanding and improving VLMs.

</details>


### [166] [F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data](https://arxiv.org/abs/2510.02294)
*Ziyin Zhang,Zihan Liao,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: F2LLM是一套直接从基础模型微调而来的高效、低成本嵌入模型，包含0.6B、1.7B和4B三种规模，在MTEB榜单上表现优异，且开源模型、数据与代码以支持可复现研究。


<details>
  <summary>Details</summary>
Motivation: 现有高性能嵌入模型依赖大规模对比预训练、复杂流程和高成本合成数据，训练代价高昂，缺乏高效且可复现的轻量方案。

Method: 基于基础大模型，直接在600万来自开源非合成数据的查询-文档-负样本三元组上进行微调，避免复杂的预训练和合成数据生成。

Result: F2LLM-4B在约4B参数模型中MTEB英文榜排名第二，整体第七；F2LLM-1.7B在其参数规模范围内排名第一。

Conclusion: F2LLM在训练成本、模型大小和性能之间实现了良好平衡，是一个强大、可复现且经济高效的嵌入模型基线，推动未来研究的可访问性与可重复性。

Abstract: We introduce F2LLM - Foundation to Feature Large Language Models, a suite of
state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike
previous top-ranking embedding models that require massive contrastive
pretraining, sophisticated training pipelines, and costly synthetic training
data, F2LLM is directly finetuned from foundation models on 6 million
query-document-negative tuples curated from open-source, non-synthetic
datasets, striking a strong balance between training cost, model size, and
embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd
among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B
ranks 1st among models in the 1B-2B size range. To facilitate future research
in the field, we release the models, training dataset, and code, positioning
F2LLM as a strong, reproducible, and budget-friendly baseline for future works.

</details>


### [167] [Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation](https://arxiv.org/abs/2510.02306)
*Raphael Tang,Crystina Zhang,Wenyan Li,Carmen Lai,Pontus Stenetorp,Yao Lu*

Main category: cs.CL

TL;DR: 本文质疑了在大语言模型竞技场评估中将平局视为模型能力相等的传统做法，提出平局更多反映的是问题难度而非模型水平相近。通过在三个真实数据集上的实验表明，忽略平局的评分更新可使预测准确率相对提高1-3%。研究还发现平局更常出现在非常简单或高度客观的问题中，建议未来的评分系统应重新考虑平局语义并纳入问题属性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评估体系（如Elo及其变体）将平局视为两模型能力相等，并据此调整评分，但这种假设可能不合理。本文旨在检验这一范式的有效性，特别是平局是否真的意味着模型能力相当。

Method: 作者分析了三个真实世界的大语言模型竞技场数据集，比较了四种评分系统在是否忽略平局评分更新情况下的表现，评估其对战斗结果预测准确性的影响，并进一步分析平局发生与查询难度和客观性之间的关系。

Result: 实验结果显示，忽略平局带来的评分更新可在所有四种评分系统中带来1-3%的相对预测准确率提升；统计分析表明，平局更可能发生在被评价为非常容易或高度客观的问题上，风险比分别为1.37和1.35。

Conclusion: 平局不应简单视为模型能力相等，而更可能是由于问题过于简单或客观导致两者都表现良好。因此，未来的大模型评分系统应重新审视平局的语义，并在评分更新机制中考虑问题本身的特性。

Abstract: In arena-style evaluation of large language models (LLMs), two LLMs respond
to a user query, and the user chooses the winning response or deems the
"battle" a draw, resulting in an adjustment to the ratings of both models. The
prevailing approach for modeling these rating dynamics is to view battles as
two-player game matches, as in chess, and apply the Elo rating system and its
derivatives. In this paper, we critically examine this paradigm. Specifically,
we question whether a draw genuinely means that the two models are equal and
hence whether their ratings should be equalized. Instead, we conjecture that
draws are more indicative of query difficulty: if the query is too easy, then
both models are more likely to succeed equally. On three real-world arena
datasets, we show that ignoring rating updates for draws yields a 1-3% relative
increase in battle outcome prediction accuracy (which includes draws) for all
four rating systems studied. Further analyses suggest that draws occur more for
queries rated as very easy and those as highly objective, with risk ratios of
1.37 and 1.35, respectively. We recommend future rating systems to reconsider
existing draw semantics and to account for query properties in rating updates.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [168] [Location Matters: Leveraging Multi-Resolution Geo-Embeddings for Housing Search](https://arxiv.org/abs/2510.01196)
*Ivo Silva,Pedro Nogueira,Guilherme Bonaldo*

Main category: cs.IR

TL;DR: 提出了一种地理感知嵌入框架，通过多层次H3网格与双塔神经网络架构结合，提升房屋推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 在数字租赁平台上，由于位置对房产价值和生活质量的重要影响，传统的推荐方法难以有效处理数据稀疏性和空间复杂性。

Method: 将多层次的H3网格系统集成到双塔神经网络架构中，并与传统的矩阵分解基线及单分辨率变体进行比较。

Result: 嵌入评估显示表示更丰富且更均衡，离线排序模拟表明推荐质量显著提升。

Conclusion: 所提出的地理感知嵌入框架能有效改善大规模住房推荐系统中的位置建模，提升用户体验。

Abstract: QuintoAndar Group is Latin America's largest housing platform,
revolutionizing property rentals and sales. Headquartered in Brazil, it
simplifies the housing process by eliminating paperwork and enhancing
accessibility for tenants, buyers, and landlords. With thousands of houses
available for each city, users struggle to find the ideal home. In this
context, location plays a pivotal role, as it significantly influences property
value, access to amenities, and life quality. A great location can make even a
modest home highly desirable. Therefore, incorporating location into
recommendations is essential for their effectiveness. We propose a geo-aware
embedding framework to address sparsity and spatial nuances in housing
recommendations on digital rental platforms. Our approach integrates an
hierarchical H3 grid at multiple levels into a two-tower neural architecture.
We compare our method with a traditional matrix factorization baseline and a
single-resolution variant using interaction data from our platform. Embedding
specific evaluation reveals richer and more balanced embedding representations,
while offline ranking simulations demonstrate a substantial uplift in
recommendation quality.

</details>


### [169] [Are LLMs ready to help non-expert users to make charts of official statistics data?](https://arxiv.org/abs/2510.01197)
*Gadir Suleymanli,Alexander Rogiers,Lucas Lageweg,Jefrey Lijffijt*

Main category: cs.IR

TL;DR: 研究探讨了当前生成式AI模型是否能够根据用户查询自动识别合适的数据并生成图表，使用荷兰统计局的公开数据对多个大语言模型进行了评估，并提出了一种涵盖数据检索、代码质量和可视化表现的三维度评价框架。


<details>
  <summary>Details</summary>
Motivation: 由于可靠数据源的重要性日益增加，但官方统计数据分散且难以处理，导致实际可及性低，因此需要探索生成式AI在自动化数据可视化中的潜力。

Method: 基于荷兰统计局的多样化公共数据，评估多个大语言模型在自主完成数据表识别、数据处理和图表生成方面的能力，提出包含数据检索与预处理、代码质量、视觉表达三个维度的评估框架，并测试引入图表设计指导和基于代理的自我迭代评估机制的效果。

Result: 实验结果表明，LLM在定位和处理正确数据方面面临最大挑战；缺乏明确指导时很少遵循可视化最佳实践；但在引入图表设计知识和迭代自评的代理架构后，各维度表现显著提升，部分系统已能达到实用水平。

Conclusion: 通过适当的引导、反馈机制和代理式架构，大语言模型在自动化图表生成任务中的有效性可以显著提高，现有技术已能在三维度上达到所需准确度，具备实际应用潜力。

Abstract: In this time when biased information, deep fakes, and propaganda proliferate,
the accessibility of reliable data sources is more important than ever.
National statistical institutes provide curated data that contain quantitative
information on a wide range of topics. However, that information is typically
spread across many tables and the plain numbers may be arduous to process.
Hence, this open data may be practically inaccessible. We ask the question "Are
current Generative AI models capable of facilitating the identification of the
right data and the fully-automatic creation of charts to provide information in
visual form, corresponding to user queries?". We present a structured
evaluation of recent large language models' (LLMs) capabilities to generate
charts from complex data in response to user queries. Working with diverse
public data from Statistics Netherlands, we assessed multiple LLMs on their
ability to identify relevant data tables, perform necessary manipulations, and
generate appropriate visualizations autonomously. We propose a new evaluation
framework spanning three dimensions: data retrieval & pre-processing, code
quality, and visual representation. Results indicate that locating and
processing the correct data represents the most significant challenge.
Additionally, LLMs rarely implement visualization best practices without
explicit guidance. When supplemented with information about effective chart
design, models showed marked improvement in representation scores. Furthermore,
an agentic approach with iterative self-evaluation led to excellent performance
across all evaluation dimensions. These findings suggest that LLMs'
effectiveness for automated chart generation can be enhanced through
appropriate scaffolding and feedback mechanisms, and that systems can already
reach the necessary accuracy across the three evaluation dimensions.

</details>


### [170] [Optimal signals assignment for eBay View Item page](https://arxiv.org/abs/2510.01198)
*Matan Mandelbrod,Biwei Jiang,Giald Wagner,Tal Franji,Guy Feigenblat*

Main category: cs.IR

TL;DR: 本文提出了两种在eBay商品详情页上优化展示信号的统计模型方法，经过A/B测试验证，两种方法均显著提升了业务指标。


<details>
  <summary>Details</summary>
Motivation: 为了帮助用户更智能地做出购买决策并提高用户参与度，eBay需要在商品详情页有效地展示信息信号。

Method: 提出了两种开发统计模型的方法，用于优化信号在商品详情页上的展示。

Result: 两种方法在A/B测试中均显著提升了关键业务指标。

Conclusion: 所提出的统计模型能有效提升用户参与度和购买决策，具有实际应用价值。

Abstract: Signals are short textual or visual snippets displayed on the eBay View-Item
(VI) page, providing additional, contextual information for users about the
viewed item. The aim in displaying the signals is to facilitate intelligent
purchase and to incentivise engagement. In this paper, we present two
approaches for developing statistical models that optimally populate the VI
page with signals. Both approaches were A/B tested, and yielded significant
increase in business metrics.

</details>


### [171] [MetaSynth: Multi-Agent Metadata Generation from Implicit Feedback in Black-Box Systems](https://arxiv.org/abs/2510.01523)
*Shreeranjani Srirangamsridharan,Ali Abavisani,Reza Yousefi Maragheh,Ramin Giahi,Kai Zhao,Jason Cho,Sushant Kumar*

Main category: cs.IR

TL;DR: 本文提出了MetaSynth，一种多代理检索增强生成框架，通过利用隐式搜索反馈（如排名结果）来优化元标题和描述的生成，在电商数据和亚马逊评论数据上显著提升了点击率和排序性能。


<details>
  <summary>Details</summary>
Motivation: 现有的元内容生成方法（如模板、大模型生成、检索增强）在多样性、属性幻觉和忽略历史表现方面存在不足，且难以在黑盒搜索引擎环境中有效优化，缺乏显式标签和部署前反馈。

Method: MetaSynth构建了一个来自高排名结果的示例库，结合产品内容和示例生成候选片段，并通过评估-生成循环迭代优化输出，确保相关性、促销力度和合规性。

Result: 在专有电商数据和亚马逊评论数据集上，MetaSynth在NDCG、MRR和排名指标上优于强基线；大规模A/B测试显示点击率提升10.26%，点击量提升7.51%。

Conclusion: MetaSynth有效利用隐式信号优化黑盒系统中的元内容生成，为在缺乏显式反馈的环境中优化内容提供了通用范式。

Abstract: Meta titles and descriptions strongly shape engagement in search and
recommendation platforms, yet optimizing them remains challenging. Search
engine ranking models are black box environments, explicit labels are
unavailable, and feedback such as click-through rate (CTR) arrives only
post-deployment. Existing template, LLM, and retrieval-augmented approaches
either lack diversity, hallucinate attributes, or ignore whether candidate
phrasing has historically succeeded in ranking. This leaves a gap in directly
leveraging implicit signals from observable outcomes. We introduce MetaSynth, a
multi-agent retrieval-augmented generation framework that learns from implicit
search feedback. MetaSynth builds an exemplar library from top-ranked results,
generates candidate snippets conditioned on both product content and exemplars,
and iteratively refines outputs via evaluator-generator loops that enforce
relevance, promotional strength, and compliance. On both proprietary e-commerce
data and the Amazon Reviews corpus, MetaSynth outperforms strong baselines
across NDCG, MRR, and rank metrics. Large-scale A/B tests further demonstrate
10.26% CTR and 7.51% clicks. Beyond metadata, this work contributes a general
paradigm for optimizing content in black-box systems using implicit signals.

</details>


### [172] [IoDResearch: Deep Research on Private Heterogeneous Data via the Internet of Data](https://arxiv.org/abs/2510.01553)
*Zhuofan Shi,Zijie Guo,Xinjian Ma,Gang Huang,Yun Ma,Xiang Jing*

Main category: cs.IR

TL;DR: 提出IoDResearch，一个以私有数据为中心的深度研究框架，通过将异构资源封装为符合FAIR原则的数字对象，并构建多粒度知识图谱，支持高效检索、问答和科研报告生成，实验证明其优于现有RAG和深度研究基线。


<details>
  <summary>Details</summary>
Motivation: 传统数据管理在处理多源异构科学数据时效率低下，现有深度研究方法忽视本地私有数据，难以满足FAIR原则，导致数据重用性差。

Method: 提出IoDResearch框架，将异构数据封装为FAIR数字对象，提炼为原子知识单元和知识图谱，构建异构图索引，并结合多智能体系统实现多粒度检索、问答与报告生成；同时建立IoD DeepResearch基准进行系统评估。

Result: 在检索、问答和报告撰写任务上，IoDResearch均显著优于代表性RAG和深度研究基线方法，验证了其在私有数据场景下的有效性。

Conclusion: IoDResearch证明了在物联网数据范式下，以私有数据为中心的深度研究是可行的，为更可信、可复用和自动化的科学发现提供了新路径。

Abstract: The rapid growth of multi-source, heterogeneous, and multimodal scientific
data has increasingly exposed the limitations of traditional data management.
Most existing DeepResearch (DR) efforts focus primarily on web search while
overlooking local private data. Consequently, these frameworks exhibit low
retrieval efficiency for private data and fail to comply with the FAIR
principles, ultimately resulting in inefficiency and limited reusability. To
this end, we propose IoDResearch (Internet of Data Research), a private
data-centric Deep Research framework that operationalizes the Internet of Data
paradigm. IoDResearch encapsulates heterogeneous resources as FAIR-compliant
digital objects, and further refines them into atomic knowledge units and
knowledge graphs, forming a heterogeneous graph index for multi-granularity
retrieval. On top of this representation, a multi-agent system supports both
reliable question answering and structured scientific report generation.
Furthermore, we establish the IoD DeepResearch Benchmark to systematically
evaluate both data representation and Deep Research capabilities in IoD
scenarios. Experimental results on retrieval, QA, and report-writing tasks show
that IoDResearch consistently surpasses representative RAG and Deep Research
baselines. Overall, IoDResearch demonstrates the feasibility of
private-data-centric Deep Research under the IoD paradigm, paving the way
toward more trustworthy, reusable, and automated scientific discovery.

</details>


### [173] [Synthetic Prefixes to Mitigate Bias in Real-Time Neural Query Autocomplete](https://arxiv.org/abs/2510.01574)
*Adithya Rajan,Xiaoyu Liu,Prateek Verma,Vibhu Arora*

Main category: cs.IR

TL;DR: 提出一种基于合成前缀的数据驱动方法，用于缓解实时神经查询自动补全系统中的呈现偏差，通过丰富训练数据提升排序模型的多样性和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决在实时查询自动补全中因模型建议影响用户行为而导致的用户交互信号固有偏差问题。

Method: 利用未启用自动补全时收集的完整用户查询生成合成前缀，增强学习排序模型的训练数据，并采用简化的列表级损失函数以提高训练效率。

Result: 系统在大规模电商环境中部署后，在平均倒数排名等指标上显著提升了用户参与度，且合成前缀有助于改善泛化能力和偏差缓解。

Conclusion: 该方法有效缓解了呈现偏差，提升了排序性能，并为其他低延迟排序任务提供了可扩展的解决方案。

Abstract: We introduce a data-centric approach for mitigating presentation bias in
real-time neural query autocomplete systems through the use of synthetic
prefixes. These prefixes are generated from complete user queries collected
during regular search sessions where autocomplete was not active. This allows
us to enrich the training data for learning to rank models with more diverse
and less biased examples. This method addresses the inherent bias in engagement
signals collected from live query autocomplete interactions, where model
suggestions influence user behavior. Our neural ranker is optimized for
real-time deployment under strict latency constraints and incorporates a rich
set of features, including query popularity, seasonality, fuzzy match scores,
and contextual signals such as department affinity, device type, and vertical
alignment with previous user queries. To support efficient training, we
introduce a task-specific simplification of the listwise loss, reducing
computational complexity from $O(n^2)$ to $O(n)$ by leveraging the query
autocomplete structure of having only one ground-truth selection per prefix.
Deployed in a large-scale e-commerce setting, our system demonstrates
statistically significant improvements in user engagement, as measured by mean
reciprocal rank and related metrics. Our findings show that synthetic prefixes
not only improve generalization but also provide a scalable path toward bias
mitigation in other low-latency ranking tasks, including related searches and
query recommendations.

</details>


### [174] [Bridging Collaborative Filtering and Large Language Models with Dynamic Alignment, Multimodal Fusion and Evidence-grounded Explanations](https://arxiv.org/abs/2510.01606)
*Bo Ma,LuYao Liu,Simon Lau,Chandler Yuan,and XueY Cui,Rosie Zhang*

Main category: cs.IR

TL;DR: 本文提出了一种名为\model{}的推荐框架，通过在线适应机制、多模态统一表示和可验证的自然语言解释，解决了现有基于大语言模型的推荐系统在动态偏好捕捉、多模态内容处理和可解释性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的推荐系统面临三大挑战：协同过滤模型依赖静态快照，难以捕捉用户快速变化的兴趣；许多现实世界物品包含丰富的视觉和听觉内容，而不仅仅是文本描述；当前系统缺乏基于具体证据的可信解释。因此，需要一种能够实时更新、融合多模态信息并提供可验证理由的推荐方法。

Method: \model{}框架包含三个核心创新：1）设计轻量级的在线适应模块，持续整合新用户交互，避免大规模模型重训练；2）构建统一表征，无缝融合协同信号与视觉、音频特征，并支持缺失模态的鲁棒处理；3）开发基于协同模式和物品属性的自然语言解释系统，生成用户可验证的推荐理由。该方法保持基础大模型冻结，仅添加少量可训练参数。

Result: 实验表明，\model{}在多个真实数据集上优于现有基线方法，在推荐准确性方面表现优异；在线适应机制有效捕捉用户兴趣演化；多模态融合提升了对富含视觉/音频内容物品的推荐质量；生成的解释被用户评估为更可信且易于理解。同时，模型保持了较低的计算开销，适合实际部署。

Conclusion: \model{}成功结合了协同过滤与大语言模型的优势，通过轻量级在线学习、多模态融合与可解释性设计，实现了高效、准确且可信的推荐系统，为大模型在推荐场景中的实用化提供了可行路径。

Abstract: Recent research has explored using Large Language Models for recommendation
tasks by transforming user interaction histories and item metadata into text
prompts, then having the LLM produce rankings or recommendations. A promising
approach involves connecting collaborative filtering knowledge to LLM
representations through compact adapter networks, which avoids expensive
fine-tuning while preserving the strengths of both components. Yet several
challenges persist in practice: collaborative filtering models often use static
snapshots that miss rapidly changing user preferences; many real-world items
contain rich visual and audio content beyond textual descriptions; and current
systems struggle to provide trustworthy explanations backed by concrete
evidence. Our work introduces \model{}, a framework that tackles these
limitations through three key innovations. We develop an online adaptation
mechanism that continuously incorporates new user interactions through
lightweight modules, avoiding the need to retrain large models. We create a
unified representation that seamlessly combines collaborative signals with
visual and audio features, handling cases where some modalities may be
unavailable. Finally, we design an explanation system that grounds
recommendations in specific collaborative patterns and item attributes,
producing natural language rationales users can verify. Our approach maintains
the efficiency of frozen base models while adding minimal computational
overhead, making it practical for real-world deployment.

</details>


### [175] [LLM4Rec: Large Language Models for Multimodal Generative Recommendation with Causal Debiasing](https://arxiv.org/abs/2510.01622)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.IR

TL;DR: 提出了一种增强的生成式推荐框架，通过多模态融合、检索增强生成、因果去偏、可解释推荐和实时自适应学习五个创新点，提升了推荐准确性、公平性和多样性。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成式推荐系统在处理多模态数据、消除算法偏差和提供透明决策方面的挑战。

Method: 基于大语言模型构建框架，集成多模态融合架构、检索增强生成、因果推理去偏、可解释推荐生成和实时自适应学习模块。

Result: 在MovieLens-25M、Amazon-Electronics和Yelp-2023数据集上实验显示，NDCG@10提升最高达2.3%，多样性指标提高1.4%，且保持计算效率。

Conclusion: 所提框架有效改善了生成式推荐系统的性能，在准确性、公平性、多样性和可解释性之间取得了更好平衡。

Abstract: Contemporary generative recommendation systems face significant challenges in
handling multimodal data, eliminating algorithmic biases, and providing
transparent decision-making processes. This paper introduces an enhanced
generative recommendation framework that addresses these limitations through
five key innovations: multimodal fusion architecture, retrieval-augmented
generation mechanisms, causal inference-based debiasing, explainable
recommendation generation, and real-time adaptive learning capabilities. Our
framework leverages advanced large language models as the backbone while
incorporating specialized modules for cross-modal understanding, contextual
knowledge integration, bias mitigation, explanation synthesis, and continuous
model adaptation. Extensive experiments on three benchmark datasets
(MovieLens-25M, Amazon-Electronics, Yelp-2023) demonstrate consistent
improvements in recommendation accuracy, fairness, and diversity compared to
existing approaches. The proposed framework achieves up to 2.3% improvement in
NDCG@10 and 1.4% enhancement in diversity metrics while maintaining
computational efficiency through optimized inference strategies.

</details>


### [176] [TalkPlay-Tools: Conversational Music Recommendation with LLM Tool Calling](https://arxiv.org/abs/2510.01698)
*Seungheon Doh,Keunwoo Choi,Juhan Nam*

Main category: cs.IR

TL;DR: 提出一种基于大语言模型（LLM）的音乐推荐系统，通过工具调用实现统一的检索-重排序流程，结合布尔过滤、稀疏/密集检索和生成式检索等方法，根据用户意图动态选择并协调多种推荐工具，提升推荐多样性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式推荐系统在利用自然语言交互的同时，未能充分整合元数据和属性过滤等关键功能，限制了推荐行为的灵活性和精确性。

Method: 构建一个以LLM为核心的端到端推荐系统，通过工具调用机制进行意图理解与任务规划，调度SQL过滤、BM25、嵌入相似性匹配和语义ID生成等专用组件，并动态决定工具类型、执行顺序和参数输入。

Result: 该框架在多种推荐场景中表现出具有竞争力的性能，能够根据用户查询自适应地选择合适的检索方法，有效融合多模态信息和多种数据库过滤技术。

Conclusion: 所提出的统一工具调用框架为对话式音乐推荐系统提供了新范式，兼具灵活性与高效性，可充分发挥LLM在复杂推荐任务中的规划与协同能力。

Abstract: While the recent developments in large language models (LLMs) have
successfully enabled generative recommenders with natural language
interactions, their recommendation behavior is limited, leaving other simpler
yet crucial components such as metadata or attribute filtering underutilized in
the system. We propose an LLM-based music recommendation system with tool
calling to serve as a unified retrieval-reranking pipeline. Our system
positions an LLM as an end-to-end recommendation system that interprets user
intent, plans tool invocations, and orchestrates specialized components:
boolean filters (SQL), sparse retrieval (BM25), dense retrieval (embedding
similarity), and generative retrieval (semantic IDs). Through tool planning,
the system predicts which types of tools to use, their execution order, and the
arguments needed to find music matching user preferences, supporting diverse
modalities while seamlessly integrating multiple database filtering methods. We
demonstrate that this unified tool-calling framework achieves competitive
performance across diverse recommendation scenarios by selectively employing
appropriate retrieval methods based on user queries, envisioning a new paradigm
for conversational music recommendation systems.

</details>


### [177] [Ranking Items from Discrete Ratings: The Cost of Unknown User Thresholds](https://arxiv.org/abs/2510.01871)
*Oscar Villemaud,Suryanarayana Sankagiri,Matthias Grossglauser*

Main category: cs.IR

TL;DR: 本文研究了如何从粗粒度评分中恢复细粒度物品排序的问题，提出了一种基于用户阈值和物品得分的模型，并证明实现近完美排序需要Θ(n²)个用户（Ω(n²)次查询），显著高于基于比较的O(n log n)查询复杂度，揭示了在线排序中用户阈值多样性带来的收益与成本之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 从粗粒度用户评分中恢复细粒度物品排序是推荐系统和信息检索中的关键问题，但现有方法难以处理评分尺度粗糙且用户阈值未知的情况，因此需要理论分析其查询复杂度及影响因素。

Method: 建立一个物品得分与用户阈值的潜在变量模型，其中用户仅当物品得分超过其个人阈值时才给予正向评分；通过Spearman距离衡量排序质量，分析在最坏情况下实现近完美排序所需的用户数量和查询复杂度，并设计匹配该下界（差对数因子）的算法验证紧性。

Result: 证明在潜得分和潜阈值模型下，获得近完美排序需Θ(n²)用户和Ω(n²)查询，远高于基于比较的O(n log n)复杂度；该差距源于识别具有合适阈值用户的额外开销，并通过二次散度因子量化评分分布与阈值分布不匹配的影响；同时提出一个查询复杂度接近理论下界的算法。

Conclusion: 尽管利用多样化用户阈值可将粗粒度评分融合为细粒度排序，但当阈值未知时，这种多样性会带来显著的查询成本，揭示了在线排序中多样性收益与探索成本之间的根本张力。

Abstract: Ranking items is a central task in many information retrieval and recommender
systems. User input for the ranking task often comes in the form of ratings on
a coarse discrete scale. We ask whether it is possible to recover a
fine-grained item ranking from such coarse-grained ratings. We model items as
having scores and users as having thresholds; a user rates an item positively
if the item's score exceeds the user's threshold. Although all users agree on
the total item order, estimating that order is challenging when both the scores
and the thresholds are latent. Under our model, any ranking method naturally
partitions the $n$ items into bins; the bins are ordered, but the items inside
each bin are still unordered. Users arrive sequentially, and every new user can
be queried to refine the current ranking. We prove that achieving a
near-perfect ranking, measured by Spearman distance, requires $\Theta(n^2)$
users (and therefore $\Omega(n^2)$ queries). This is significantly worse than
the $O(n\log n)$ queries needed to rank from comparisons; the gap reflects the
additional queries needed to identify the users who have the appropriate
thresholds. Our bound also quantifies the impact of a mismatch between score
and threshold distributions via a quadratic divergence factor. To show the
tightness of our results, we provide a ranking algorithm whose query complexity
matches our bound up to a logarithmic factor. Our work reveals a tension in
online ranking: diversity in thresholds is necessary to merge coarse ratings
from many users into a fine-grained ranking, but this diversity has a cost if
the thresholds are a priori unknown.

</details>


### [178] [Contrastive Retrieval Heads Improve Attention-Based Re-Ranking](https://arxiv.org/abs/2510.02219)
*Linh Tran,Yulong Li,Radu Florian,Wei Sun*

Main category: cs.IR

TL;DR: 本文提出了一种名为CoRe heads的新方法，通过对比评分机制识别出对相关文档具有高注意力的相关转换器头，从而提升基于注意力的重排序系统的性能。实验表明，仅使用少于1%的头即可显著提高重排序准确性，并且这些头主要集中在中间层，进一步剪枝可减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的基于注意力的重排序器由于许多注意力头引入噪声和冗余，限制了性能，因此需要一种更有效的方法来筛选出最具判别性的注意力头。

Method: 提出CoRe heads方法，利用对比评分指标识别与相关文档高度相关的高注意力头，同时抑制与无关文档相关的高注意力节点，通过聚合CoRe heads信号进行重排序。

Result: 在三个大语言模型上的实验显示，使用CoRe heads显著优于强基线模型，且仅需不到1%的注意力头即可实现高性能；此外，CoRe heads集中在中间层，剪枝后50%的模型层仍能保持准确率。

Conclusion: CoRe heads是一种高效、精确的重排序方法，能够在保持甚至提升性能的同时大幅降低计算和内存开销。

Abstract: The strong zero-shot and long-context capabilities of recent Large Language
Models (LLMs) have paved the way for highly effective re-ranking systems.
Attention-based re-rankers leverage attention weights from transformer heads to
produce relevance scores, but not all heads are created equally: many
contribute noise and redundancy, thus limiting performance. To address this, we
introduce CoRe heads, a small set of retrieval heads identified via a
contrastive scoring metric that explicitly rewards high attention heads that
correlate with relevant documents, while downplaying nodes with higher
attention that correlate with irrelevant documents. This relative ranking
criterion isolates the most discriminative heads for re-ranking and yields a
state-of-the-art list-wise re-ranker. Extensive experiments with three LLMs
show that aggregated signals from CoRe heads, constituting less than 1% of all
heads, substantially improve re-ranking accuracy over strong baselines. We
further find that CoRe heads are concentrated in middle layers, and pruning the
computation of final 50% of model layers preserves accuracy while significantly
reducing inference time and memory usage.

</details>


### [179] [Study on LLMs for Promptagator-Style Dense Retriever Training](https://arxiv.org/abs/2510.02241)
*Daniel Gwon,Nour Jedidi,Jimmy Lin*

Main category: cs.IR

TL;DR: 本研究探讨了使用开源的、规模较小（≤14B参数）的大语言模型作为Promptagator风格查询生成器的可行性，发现仅3B参数的模型即可有效生成任务特定查询，为领域专用检索模型的微调提供了可靠且可访问的替代方案。


<details>
  <summary>Details</summary>
Motivation: 原始Promptagator方法依赖于专有且大规模的语言模型，存在访问限制和敏感数据使用的合规问题，因此需要探索基于开源且更小规模模型的可行替代方案。

Method: 采用开源大语言模型（参数规模≤14B），在与原始Promptagator相似的框架下进行实验，评估其作为合成查询生成器在下游密集检索模型微调中的表现。

Result: 实验证明，低至3B参数的开源大语言模型也能有效生成高质量的任务特定查询，显著提升领域专用检索模型的微调效果，性能接近使用大型专有模型的方法。

Conclusion: 开源且小规模的大语言模型可作为Promptagator中专有模型的有效替代，为实际应用中安全、低成本地生成合成训练数据提供了可行路径。

Abstract: Promptagator demonstrated that Large Language Models (LLMs) with few-shot
prompts can be used as task-specific query generators for fine-tuning
domain-specialized dense retrieval models. However, the original Promptagator
approach relied on proprietary and large-scale LLMs which users may not have
access to or may be prohibited from using with sensitive data. In this work, we
study the impact of open-source LLMs at accessible scales ($\leq$14B
parameters) as an alternative. Our results demonstrate that open-source LLMs as
small as 3B parameters can serve as effective Promptagator-style query
generators. We hope our work will inform practitioners with reliable
alternatives for synthetic data generation and give insights to maximize
fine-tuning results for domain-specific applications.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [180] [LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science](https://arxiv.org/abs/2510.01285)
*Alireza Salemi,Mihir Parmar,Palash Goyal,Yiwen Song,Jinsung Yoon,Hamed Zamani,Hamid Palangi,Tomas Pfister*

Main category: cs.MA

TL;DR: 提出一种受黑板架构启发的多智能体通信范式，用于解决大语言模型在大型异构数据湖中数据发现的挑战，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体系统难以应对大规模异构数据，而主从式多智能体系统依赖中心化控制器，缺乏灵活性和可扩展性。

Method: 设计基于黑板架构的多智能体系统，中心智能体将请求发布到共享黑板，各自主子智能体根据自身能力主动响应，无需中心控制器预先掌握所有子智能体的能力。

Result: 在KramaBench、修改版DS-Bench和DA-Code三个基准上验证，相比RAG和主从式多智能体，端到端任务成功率提升13%至57%，数据发现F1分数最高提升9%。

Conclusion: 黑板架构是一种可扩展且通用的多智能体系统通信框架，适用于复杂环境下的数据发现任务。

Abstract: The rapid advancement of Large Language Models (LLMs) has opened new
opportunities in data science, yet their practical deployment is often
constrained by the challenge of discovering relevant data within large
heterogeneous data lakes. Existing methods struggle with this: single-agent
systems are quickly overwhelmed by large, heterogeneous files in the large data
lakes, while multi-agent systems designed based on a master-slave paradigm
depend on a rigid central controller for task allocation that requires precise
knowledge of each sub-agent's capabilities. To address these limitations, we
propose a novel multi-agent communication paradigm inspired by the blackboard
architecture for traditional AI models. In this framework, a central agent
posts requests to a shared blackboard, and autonomous subordinate agents --
either responsible for a partition of the data lake or general information
retrieval -- volunteer to respond based on their capabilities. This design
improves scalability and flexibility by eliminating the need for a central
coordinator to have prior knowledge of all sub-agents' expertise. We evaluate
our method on three benchmarks that require explicit data discovery: KramaBench
and modified versions of DS-Bench and DA-Code to incorporate data discovery.
Experimental results demonstrate that the blackboard architecture substantially
outperforms baselines, including RAG and the master-slave multi-agent paradigm,
achieving between 13% to 57% relative improvement in end-to-end task success
and up to a 9% relative gain in F1 score for data discovery over the
best-performing baselines across both proprietary and open-source LLMs. Our
findings establish the blackboard paradigm as a scalable and generalizable
communication framework for multi-agent systems.

</details>


### [181] [SimCity: Multi-Agent Urban Development Simulation with Rich Interactions](https://arxiv.org/abs/2510.01297)
*Yeqi Feng,Yucheng Lu,Hongyu Su,Tianxing He*

Main category: cs.MA

TL;DR: SimCity是一个基于大语言模型的多智能体框架，用于构建可解释的宏观经济模拟系统，能够自然再现多种经典宏观经济现象。


<details>
  <summary>Details</summary>
Motivation: 传统宏观模型受限于异质性建模的复杂性，而基于规则的代理模型缺乏灵活性，本文旨在利用大语言模型实现更具适应性和透明性的宏观经济模拟。

Method: 提出SimCity框架，结合大语言模型与视觉-语言模型，构建包含家庭、企业、央行和政府四类智能体的多市场（劳动力、商品、金融）经济系统，并通过VLM确定企业地理布局并生成虚拟城市地图。

Result: SimCity成功复现了需求价格弹性、恩格尔定律、奥肯定律、菲利普斯曲线和贝弗里奇曲线等经典宏观经济规律，且在多次模拟中表现稳健。

Conclusion: SimCity展示了大语言模型在构建可解释、异质性、多市场宏观经济模拟中的潜力，为研究宏观经济动态与城市扩张提供了统一框架。

Abstract: Large Language Models (LLMs) open new possibilities for constructing
realistic and interpretable macroeconomic simulations. We present SimCity, a
multi-agent framework that leverages LLMs to model an interpretable
macroeconomic system with heterogeneous agents and rich interactions. Unlike
classical equilibrium models that limit heterogeneity for tractability, or
traditional agent-based models (ABMs) that rely on hand-crafted decision rules,
SimCity enables flexible, adaptive behavior with transparent natural-language
reasoning. Within SimCity, four core agent types (households, firms, a central
bank, and a government) deliberate and participate in a frictional labor
market, a heterogeneous goods market, and a financial market. Furthermore, a
Vision-Language Model (VLM) determines the geographic placement of new firms
and renders a mapped virtual city, allowing us to study both macroeconomic
regularities and urban expansion dynamics within a unified environment. To
evaluate the framework, we compile a checklist of canonical macroeconomic
phenomena, including price elasticity of demand, Engel's Law, Okun's Law, the
Phillips Curve, and the Beveridge Curve, and show that SimCity naturally
reproduces these empirical patterns while remaining robust across simulation
runs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [182] [MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics](https://arxiv.org/abs/2510.01619)
*Changmin Lee,Jihyun Lee,Tae-Kyun Kim*

Main category: cs.GR

TL;DR: 本文提出MPMAvatar，一种基于多视角视频创建3D人类化身的框架，结合材料点法模拟和各向异性本构模型，实现高真实感、鲁棒的动态建模与自由视角渲染。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理宽松衣物的人体动态建模时存在精度不足或对新动画输入鲁棒性差的问题，尤其是在物理仿真方面。

Method: 采用基于材料点法（Material Point Method）的模拟器，引入各向异性本构模型和新的碰撞处理算法来精确建模衣物复杂形变与身体接触；结合可进行3D高斯点阵渲染的规范化身模型，实现高质量渲染。

Result: 实验表明，MPMAvatar在动态建模精度、渲染质量、鲁棒性和效率方面均优于现有最先进物理驱动方法，并能零样本泛化到未见交互场景。

Conclusion: MPMAvatar实现了从多视角视频生成具有高真实感动态和渲染效果的3D人体化身，在物理准确性、鲁棒性和泛化能力上均有显著提升。

Abstract: While there has been significant progress in the field of 3D avatar creation
from visual observations, modeling physically plausible dynamics of humans with
loose garments remains a challenging problem. Although a few existing works
address this problem by leveraging physical simulation, they suffer from
limited accuracy or robustness to novel animation inputs. In this work, we
present MPMAvatar, a framework for creating 3D human avatars from multi-view
videos that supports highly realistic, robust animation, as well as
photorealistic rendering from free viewpoints. For accurate and robust dynamics
modeling, our key idea is to use a Material Point Method-based simulator, which
we carefully tailor to model garments with complex deformations and contact
with the underlying body by incorporating an anisotropic constitutive model and
a novel collision handling algorithm. We combine this dynamics modeling scheme
with our canonical avatar that can be rendered using 3D Gaussian Splatting with
quasi-shadowing, enabling high-fidelity rendering for physically realistic
animations. In our experiments, we demonstrate that MPMAvatar significantly
outperforms the existing state-of-the-art physics-based avatar in terms of (1)
dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and
efficiency. Additionally, we present a novel application in which our avatar
generalizes to unseen interactions in a zero-shot manner-which was not
achievable with previous learning-based methods due to their limited simulation
generalizability. Our project page is at:
https://KAISTChangmin.github.io/MPMAvatar/

</details>


### [183] [Multimodal Feedback for Task Guidance in Augmented Reality](https://arxiv.org/abs/2510.01690)
*Hu Guo,Lily Patel,Rohan Gupt*

Main category: cs.GR

TL;DR: 本研究结合光学透视增强现实（OST-AR）与基于手腕的振动触觉反馈，提升深度感知和操作精度。通过定制腕带与手持工具集成，实验证明多模态反馈在认知负荷下仍能准确识别，并优于纯视觉或纯触觉引导。


<details>
  <summary>Details</summary>
Motivation: 视觉叠加可能造成注意力过载，且在遮挡或光照干扰时缺乏深度线索，影响OST-AR在精密任务中的表现。因此需要引入多模态反馈以缓解这些问题。

Method: 设计带有六个振动电机的定制腕带，提供方向性和状态性触觉提示，与OST-AR系统及手持工具集成；通过一项形成性研究和两项实验（N=21, N=27）评估提示识别准确率和空间引导效果。

Result: 参与者在认知负荷下能准确识别振动模式；多模态反馈显著提高空间精度和可用性，优于仅视觉或仅触觉条件。

Conclusion: 结合腕部振动触觉与OST-AR可有效增强深度引导和用户体验，为高精度任务（如医疗操作）提供更可靠的指导方案。

Abstract: Optical see-through augmented reality (OST-AR) overlays digital targets and
annotations on the physical world, offering promising guidance for hands-on
tasks such as medical needle insertion or assembly. Recent work on OST-AR depth
perception shows that target opacity and tool visualization significantly
affect accuracy and usability; opaque targets and rendering the real instrument
reduce depth errors, whereas transparent targets and absent tools impair
performance. However, reliance on visual overlays may overload attention and
leaves little room for depth cues when occlusion or lighting hampers
perception. To address these limitations, we explore multimodal feedback that
combines OST-AR with wrist-based vibrotactile haptics. The past two years have
seen rapid advances in haptic technology. Researchers have investigated
skin-stretch and vibrotactile cues for conveying spatial information to blind
users, wearable ring actuators that support precise pinching in AR, cross-modal
audio-haptic cursors that enable eyes-free object selection, and wrist-worn
feedback for teleoperated surgery that improves force awareness at the cost of
longer task times. Studies comparing pull versus push vibrotactile metaphors
found that pull cues yield faster gesture completion and lower cognitive load.
These findings motivate revisiting OST-AR guidance with a fresh perspective on
wrist-based haptics. We design a custom wristband with six vibromotors
delivering directional and state cues, integrate it with a handheld tool and
OST-AR, and assess its impact on cue recognition and depth guidance. Through a
formative study and two experiments (N=21 and N=27), we show that participants
accurately identify haptic patterns under cognitive load and that multimodal
feedback improves spatial precision and usability compared with visual-only or
haptic-only conditions.

</details>


### [184] [MIRAGE: Patient-Specific Mixed Reality Coaching for MRI via Depth-Only Markerless Registration and Immersive VR](https://arxiv.org/abs/2510.01743)
*Daniel Brooks,Emily Carter,Hu Guo,Rajesh Nair*

Main category: cs.GR

TL;DR: MIRAGE利用混合现实技术（VR和AR）帮助患者应对MRI过程中的焦虑和幽闭恐惧，通过沉浸式虚拟环境和无标记增强现实注册，减少运动伪影和对镇静药物的依赖。


<details>
  <summary>Details</summary>
Motivation: MRI检查中的封闭空间和噪音容易引发患者焦虑和幽闭恐惧，导致扫描失败或需要药物镇静，影响诊断效率和患者体验。

Method: 开发MIRAGE系统，结合最新的混合现实硬件，使用基于深度信息的注册技术实现亚厘米级精度，并通过沉浸式虚拟现实和无标记增强现实技术为患者提供预演和指导。

Result: 系统实现了高精度的AR注册，设置简便；用户体验测试显示该系统能有效降低焦虑水平，并获得良好的可用性评分。

Conclusion: MIRAGE系统可有效提升患者在MRI检查前的心理准备，减少焦虑，具有良好的临床应用前景，且具备在医院工作流中部署的可行性。

Abstract: Magnetic resonance imaging (MRI) is an indispensable diagnostic tool, yet the
confined bore and acoustic noise can evoke considerable anxiety and
claustrophobic reactions. High anxiety leads to motion artifacts, incomplete
scans and reliance on pharmacological sedation. MIRAGE (Mixed Reality Anxiety
Guidance Environment) harnesses the latest mixed reality (MR) hardware to
prepare patients for MRI through immersive virtual reality (VR) and markerless
augmented reality (AR) registration. In this paper, we extend our previous work
by providing a comprehensive review of related research, detailing the system
architecture, and exploring metrics for patient and clinician experience. We
also present considerations for clinical deployment of MR systems within
hospital workflows. Our results indicate that depth-based registration achieves
sub-centimeter accuracy with minimal setup, while the immersive coaching
environment reduces patient anxiety and yields favourable usability scores.

</details>


### [185] [ROI-GS: Interest-based Local Quality 3D Gaussian Splatting](https://arxiv.org/abs/2510.01978)
*Quoc-Anh Bui,Gilles Rougeron,Géraldine Morin,Simone Gasparini*

Main category: cs.GR

TL;DR: 提出ROI-GS，一种面向对象的3D高斯点阵化框架，通过对象引导的相机选择、目标对象训练和高保真局部重建融合，提升感兴趣区域细节并减小模型规模。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在全场景均匀分配资源，导致感兴趣区域细节不足且模型冗大，难以兼顾局部精细重建与效率。

Method: 设计对象感知的ROI-GS框架，采用对象引导的相机选择策略，针对特定对象进行重点训练，并将高质量的对象重建无缝融入全局场景。

Result: 相比基线方法，局部质量提升高达2.96 dB PSNR，模型大小减少约17%，单对象场景训练速度更快。

Conclusion: ROI-GS在保持实时渲染性能的同时，显著提升了感兴趣区域的重建质量，有效平衡了细节精度与计算效率。

Abstract: We tackle the challenge of efficiently reconstructing 3D scenes with high
detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods
allocate resources uniformly across the scene, limiting fine detail to Regions
Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an
object-aware framework that enhances local details through object-guided camera
selection, targeted Object training, and seamless integration of high-fidelity
object of interest reconstructions into the global scene. Our method
prioritizes higher resolution details on chosen objects while maintaining
real-time performance. Experiments show that ROI-GS significantly improves
local quality (up to 2.96 dB PSNR), while reducing overall model size by
$\approx 17\%$ of baseline and achieving faster training for a scene with a
single object of interest, outperforming existing methods.

</details>


### [186] [Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects](https://arxiv.org/abs/2510.02069)
*Georgios Kouros,Minye Wu,Tinne Tuytelaars*

Main category: cs.GR

TL;DR: 提出一种结合微表面BRDF与延迟着色的2D高斯点阵可重光照框架，实现对复杂光泽物体更真实、一致的几何与材质重建及重光照。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法因使用简化的BRDF模型或耦合漫反射与镜面反射成分，难以准确分离形状、材质和光照，限制了光泽物体的高质量重建与重光照。

Method: 将微表面BRDF的镜面-光泽度参数化引入2D高斯点阵渲染框架，并采用延迟着色；利用基于扩散先验的表面法线和漫反射颜色引导优化，结合由粗到细的环境光照图优化策略。

Result: 在复杂光泽场景上实验表明，该方法在几何与材质重建质量上优于现有高斯点阵方法，新光照下的重光照效果更逼真且一致，同时提升了收敛速度和高动态范围镜面反射的保留能力。

Conclusion: 所提方法通过物理一致的材质分解与先验引导优化，显著提升了光泽物体的可重光照性能，为高保真场景重建提供了有效解决方案。

Abstract: Accurate reconstruction and relighting of glossy objects remain a
longstanding challenge, as object shape, material properties, and illumination
are inherently difficult to disentangle. Existing neural rendering approaches
often rely on simplified BRDF models or parameterizations that couple diffuse
and specular components, which restricts faithful material recovery and limits
relighting fidelity. We propose a relightable framework that integrates a
microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian
Splatting with deferred shading. This formulation enables more physically
consistent material decomposition, while diffusion-based priors for surface
normals and diffuse color guide early-stage optimization and mitigate
ambiguity. A coarse-to-fine optimization of the environment map accelerates
convergence and preserves high-dynamic-range specular reflections. Extensive
experiments on complex, glossy scenes demonstrate that our method achieves
high-quality geometry and material reconstruction, delivering substantially
more realistic and consistent relighting under novel illumination compared to
existing Gaussian splatting methods.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [187] [Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge](https://arxiv.org/abs/2510.01348)
*Michal Werner,David Čapek,Tomáš Musil,Ondřej Franěk,Tomáš Báča,Martin Saska*

Main category: cs.RO

TL;DR: 提出了一种完全机载的无人机系统，用于在无GNSS环境下实现长距离自主飞行，通过轻量级漂移校正方法显著减少里程计漂移，并在CPU上实现实时运行。


<details>
  <summary>Details</summary>
Motivation: 在无GNSS、无先验密集地图的条件下实现可靠、长距离、低空无人机飞行面临漂移、计算资源受限等挑战。

Method: 将感知、建图、规划与控制集成于机载系统，采用激光雷达生成的局部高度图与先验地理数据高度图进行梯度模板匹配，并结合聚类粒子滤波器融合里程计信息以校正漂移。

Result: 在城市、森林和开阔地等地形中完成了公里级飞行，显著减少了相对于原始里程计的漂移，并在纯CPU硬件上实现了实时运行。

Conclusion: 该系统能够在无GNSS环境中高效执行长距离自主飞行任务，具备实际部署能力，为无GNSS无人机自主性设计提供了实用见解。

Abstract: Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied
environments is challenging: integrating odometry leads to drift, loop closures
are unavailable in previously unseen areas and embedded platforms provide
limited computational power. We present a fully onboard UAV system developed
for the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km
long-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS
or prior dense mapping. The system integrates perception, mapping, planning,
and control with a lightweight drift-correction method that matches
LiDAR-derived local heightmaps to a prior geo-data heightmap via
gradient-template matching and fuses the evidence with odometry in a clustered
particle filter. Deployed during the competition, the system executed
kilometer-scale flights across urban, forest, and open-field terrain and
reduced drift substantially relative to raw odometry, while running in real
time on CPU-only hardware. We describe the system architecture, the
localization pipeline, and the competition evaluation, and we report practical
insights from field deployment that inform the design of GNSS-denied UAV
autonomy.

</details>


### [188] [TACOS: Task Agnostic COordinator of a multi-drone System](https://arxiv.org/abs/2510.01869)
*Alessandro Nazzari,Roberto Rubinacci,Marco Lovera*

Main category: cs.RO

TL;DR: 本文提出了TACOS，一个基于大语言模型的多无人机系统任务协调框架，支持通过自然语言进行高层控制，实现灵活的共享自主性。


<details>
  <summary>Details</summary>
Motivation: 单个操作员管理多架无人机时面临不同层次的自主性需求，传统控制方式难以满足灵活性和低工作负荷的要求。

Method: 设计并实现了一个统一框架TACOS，集成了一对多自然语言接口、智能协调器和自主执行代理，利用大语言模型将自然语言指令转化为可执行任务计划，并通过API库与真实世界交互。

Result: 在真实多无人机系统中验证了TACOS的有效性，并通过消融研究评估了各模块的贡献，展示了其在复杂任务中的可行性与优势。

Conclusion: TACOS为多无人机系统的自然语言控制提供了一个通用、可扩展的解决方案，显著降低了操作员负担，提升了人机协作效率。

Abstract: When a single pilot is responsible for managing a multi-drone system, the
task demands varying levels of autonomy, from direct control of individual
UAVs, to group-level coordination, to fully autonomous swarm behaviors for
accomplishing high-level tasks. Enabling such flexible interaction requires a
framework that supports multiple modes of shared autonomy. As language models
continue to improve in reasoning and planning, they provide a natural
foundation for such systems, reducing pilot workload by enabling high-level
task delegation through intuitive, language-based interfaces. In this paper we
present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified
framework that enables high-level natural language control of multi-UAV systems
through Large Language Models (LLMs). TACOS integrates three key capabilities
into a single architecture: a one-to-many natural language interface for
intuitive user interaction, an intelligent coordinator for translating user
intent into structured task plans, and an autonomous agent that executes plans
interacting with the real-world. TACOS allows a LLM to interact with a library
of executable APIs, bridging semantic reasoning with real-time multi-robot
coordination. We demonstrate the system in real-world multi-drone system and
conduct an ablation study to assess the contribution of each module.

</details>


### [189] [Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels](https://arxiv.org/abs/2510.01357)
*Alejandro Gonzalez-Garcia,Wei Xiao,Wei Wang,Alejandro Astudillo,Wilm Decré,Jan Swevers,Carlo Ratti,Daniela Rus*

Main category: cs.RO

TL;DR: 提出了一种结合模型预测控制（MPC）和控制屏障函数（CBF）的安全运动规划策略，用于自主船舶在狭窄水域的实时安全导航。


<details>
  <summary>Details</summary>
Motivation: 传统运动规划方法在狭窄水域中往往计算量大或过于保守，难以兼顾安全性与灵活性。

Method: 采用时间变化的膨胀椭圆障碍物表示法，结合MPC生成近似路径，并利用高阶CBF根据船舶与障碍物的相对位置和姿态动态调整安全半径，确保安全性。

Result: 仿真和实船实验表明，该方法能在狭窄空间中实现实时导航，有效避免死锁，并保证航行安全。

Conclusion: 所提出的自适应膨胀椭圆结合MPC与CBF的方法在保证安全性的同时，显著降低了保守性，适用于复杂环境下的自主船舶运动规划。

Abstract: Safe motion planning is essential for autonomous vessel operations,
especially in challenging spaces such as narrow inland waterways. However,
conventional motion planning approaches are often computationally intensive or
overly conservative. This paper proposes a safe motion planning strategy
combining Model Predictive Control (MPC) and Control Barrier Functions (CBFs).
We introduce a time-varying inflated ellipse obstacle representation, where the
inflation radius is adjusted depending on the relative position and attitude
between the vessel and the obstacle. The proposed adaptive inflation reduces
the conservativeness of the controller compared to traditional fixed-ellipsoid
obstacle formulations. The MPC solution provides an approximate motion plan,
and high-order CBFs ensure the vessel's safety using the varying inflation
radius. Simulation and real-world experiments demonstrate that the proposed
strategy enables the fully-actuated autonomous robot vessel to navigate through
narrow spaces in real time and resolve potential deadlocks, all while ensuring
safety.

</details>


### [190] [A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots](https://arxiv.org/abs/2510.01381)
*Spencer Teetaert,Sven Lilge,Jessica Burgner-Kahrs,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 提出了一种基于连续时间随机因子图的柔性机器人状态估计框架，具有对未建模干扰的鲁棒性和连续时空插值能力。


<details>
  <summary>Details</summary>
Motivation: 现有柔性机器人的状态估计方法受限于计算复杂性、静态假设或对未建模扰动敏感，需要更鲁棒且高效的估计方法。

Method: 基于因子图优化框架，引入受高斯白噪声过程影响的连续时间运动学因子，结合简单模型与高频率传感器数据进行状态估计。

Result: 实现了对机器人位姿、速度和应变的连续时间均值与协方差估计，支持常数时间内的插值查询，具备对外部力扰动和数据丢失的适应性。

Conclusion: 该方法在计算效率和估计精度之间取得了良好平衡，适用于实际柔性机器人系统的实时状态估计。

Abstract: State estimation techniques for continuum robots (CRs) typically involve
using computationally complex dynamic models, simplistic shape approximations,
or are limited to quasi-static methods. These limitations can be sensitive to
unmodelled disturbances acting on the robot. Inspired by a factor-graph
optimization paradigm, this work introduces a continuous-time stochastic state
estimation framework for continuum robots. We introduce factors based on
continuous-time kinematics that are corrupted by a white noise Gaussian process
(GP). By using a simple robot model paired with high-rate sensing, we show
adaptability to unmodelled external forces and data dropout. The result
contains an estimate of the mean and covariance for the robot's pose, velocity,
and strain, each of which can be interpolated continuously in time or space.
This same interpolation scheme can be used during estimation, allowing for
inclusion of measurements on states that are not explicitly estimated. Our
method's inherent sparsity leads to a linear solve complexity with respect to
time and interpolation queries in constant time. We demonstrate our method on a
CR with gyroscope and pose sensors, highlighting its versatility in real-world
systems.

</details>


### [191] [VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation](https://arxiv.org/abs/2510.01388)
*Arthur Zhang,Xiangyun Meng,Luca Calliari,Dong-Ki Kim,Shayegan Omidshafiei,Joydeep Biswas,Ali Agha,Amirreza Shaban*

Main category: cs.RO

TL;DR: VENTURA是一种基于视觉-语言的导航系统，通过微调互联网预训练的图像扩散模型生成路径掩码，实现对自然语言指令的理解与执行，结合轻量级行为克隆策略将视觉计划转化为可执行轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在机器人导航任务中难以适应多样化的指令且迁移性差，缺乏对细粒度、上下文感知导航行为的支持。

Method: 提出VENTURA系统，利用预训练图像扩散模型生成图像空间中的路径掩码作为视觉计划，使用轻量级行为克隆策略将其转化为机器人可执行的轨迹，并通过自监督跟踪模型生成的路径掩码和VLM增强字幕进行训练监督。

Result: 在真实世界实验中，VENTURA相比当前最先进的基线模型，在物体到达、避障和地形偏好任务上成功率提高33%，碰撞减少54%，并展现出对未见任务组合的良好泛化能力。

Conclusion: VENTURA有效结合了视觉语言模型与扩散模型的优势，实现了对自然语言指令的灵活响应和安全导航，具备良好的开放环境适应性和任务组合泛化能力。

Abstract: Robots must adapt to diverse human instructions and operate safely in
unstructured, open-world environments. Recent Vision-Language models (VLMs)
offer strong priors for grounding language and perception, but remain difficult
to steer for navigation due to differences in action spaces and pretraining
objectives that hamper transferability to robotics tasks. Towards addressing
this, we introduce VENTURA, a vision-language navigation system that finetunes
internet-pretrained image diffusion models for path planning. Instead of
directly predicting low-level actions, VENTURA generates a path mask (i.e. a
visual plan) in image space that captures fine-grained, context-aware
navigation behaviors. A lightweight behavior-cloning policy grounds these
visual plans into executable trajectories, yielding an interface that follows
natural language instructions to generate diverse robot behaviors. To scale
training, we supervise on path masks derived from self-supervised tracking
models paired with VLM-augmented captions, avoiding manual pixel-level
annotation or highly engineered data collection setups. In extensive real-world
evaluations, VENTURA outperforms state-of-the-art foundation model baselines on
object reaching, obstacle avoidance, and terrain preference tasks, improving
success rates by 33% and reducing collisions by 54% across both seen and unseen
scenarios. Notably, we find that VENTURA generalizes to unseen combinations of
distinct tasks, revealing emergent compositional capabilities. Videos, code,
and additional materials: https://venturapath.github.io

</details>


### [192] [INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models](https://arxiv.org/abs/2510.01389)
*Ulas Berk Karli,Ziyao Shangguan,Tesca FItzgerald*

Main category: cs.RO

TL;DR: 本文提出了INSIGHT框架，利用token级不确定性信号预测视觉-语言-动作（VLA）模型何时应请求人类帮助，通过Transformer建模不确定性序列显著提升了帮助请求的预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型缺乏对失败的预判和主动求助的能力，需要引入内省机制以提高安全性和可靠性。

Method: 基于π₀-FAST模型提取每个token的熵、对数概率以及aleatoric和epistemic不确定性，使用小型Transformer分类器将这些序列映射为是否请求帮助，并比较了强监督与弱监督在分布内和分布外任务上的表现。

Result: 发现强标签能更好捕捉细粒度不确定性动态，而弱标签在训练与评估对齐时仍具竞争力；Transformer对时序不确定性建模显著优于静态评分方法。

Conclusion: 这是首次系统评估基于不确定性的VLA内省机制的工作，为基于选择性人工干预的实时错误缓解和主动学习提供了新方向。

Abstract: Recent Vision-Language-Action (VLA) models show strong generalization
capabilities, yet they lack introspective mechanisms for anticipating failures
and requesting help from a human supervisor. We present \textbf{INSIGHT}, a
learning framework for leveraging token-level uncertainty signals to predict
when a VLA should request help. Using $\pi_0$-FAST as the underlying model, we
extract per-token \emph{entropy}, \emph{log-probability}, and Dirichlet-based
estimates of \emph{aleatoric and epistemic uncertainty}, and train compact
transformer classifiers to map these sequences to help triggers. We explore
supervision regimes for strong or weak supervision, and extensively compare
them across in-distribution and out-of-distribution tasks. Our results show a
trade-off: strong labels enable models to capture fine-grained uncertainty
dynamics for reliable help detection, while weak labels, though noisier, still
support competitive introspection when training and evaluation are aligned,
offering a scalable path when dense annotation is impractical. Crucially, we
find that modeling the temporal evolution of token-level uncertainty signals
with transformers provides far greater predictive power than static
sequence-level scores. This study provides the first systematic evaluation of
uncertainty-based introspection in VLAs, opening future avenues for active
learning and for real-time error mitigation through selective human
intervention.

</details>


### [193] [Beyond Collision Cones: Dynamic Obstacle Avoidance for Nonholonomic Robots via Dynamic Parabolic Control Barrier Functions](https://arxiv.org/abs/2510.01402)
*Hun Kuk Park,Taekyung Kim,Dimitra Panagou*

Main category: cs.RO

TL;DR: 提出了一种动态抛物线控制屏障函数（DPCBF），用于提升非完整机器人在密集动态环境中的安全性和导航成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于碰撞锥或速度障碍的方法过于保守，易导致控制屏障函数（CBF）优化问题不可行，尤其在密集环境中。

Method: 设计一种动态调整顶点和曲率的抛物线安全边界，结合距离和相对速度大小，构建更宽松的安全约束，并证明其适用于受输入限制的运动学自行车模型。

Result: 仿真结果表明，与基线方法相比，所提DPCBF显著提高了导航成功率和二次规划可行性，能在多达100个动态障碍物的密集场景中成功导航。

Conclusion: DPCBF通过动态适应的抛物线安全边界，有效缓解了传统方法的保守性问题，增强了复杂环境下系统的安全性与可行性。

Abstract: Control Barrier Functions (CBFs) are a powerful tool for ensuring the safety
of autonomous systems, yet applying them to nonholonomic robots in cluttered,
dynamic environments remains an open challenge. State-of-the-art methods often
rely on collision-cone or velocity-obstacle constraints which, by only
considering the angle of the relative velocity, are inherently conservative and
can render the CBF-based quadratic program infeasible, particularly in dense
scenarios. To address this issue, we propose a Dynamic Parabolic Control
Barrier Function (DPCBF) that defines the safe set using a parabolic boundary.
The parabola's vertex and curvature dynamically adapt based on both the
distance to an obstacle and the magnitude of the relative velocity, creating a
less restrictive safety constraint. We prove that the proposed DPCBF is valid
for a kinematic bicycle model subject to input constraints. Extensive
comparative simulations demonstrate that our DPCBF-based controller
significantly enhances navigation success rates and QP feasibility compared to
baseline methods. Our approach successfully navigates through dense
environments with up to 100 dynamic obstacles, scenarios where collision
cone-based methods fail due to infeasibility.

</details>


### [194] [How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?](https://arxiv.org/abs/2510.01404)
*Lexi Foland,Thomas Cohn,Adam Wei,Nicholas Pfaff,Boyuan Chen,Russ Tedrake*

Main category: cs.RO

TL;DR: 该论文研究了扩散策略在机器人模仿学习中对运动学等式约束的学习能力，通过双臂抓取放置任务的案例分析，探讨了数据集大小、质量和约束流形曲率对策略训练的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散策略在任务表现上取得了良好效果，但其是否能精确学习训练数据中的约束尚不明确，因此需要探究其对约束流形的学习能力。

Method: 采用双臂pick-and-place任务作为案例，分析扩散策略在不同数据集大小、质量以及约束流形曲率下的表现，并在真实硬件上进行验证。

Result: 实验表明，扩散策略仅能学习到约束流形的粗略近似；数据集大小和质量下降会负面影响学习效果；而流形曲率与约束满足及任务成功率之间的关系不明确。

Conclusion: 扩散策略能够大致捕捉运动学约束，但其精度受限于数据集的规模与质量，实际应用中需注意数据质量以提升约束学习效果。

Abstract: Diffusion policies have shown impressive results in robot imitation learning,
even for tasks that require satisfaction of kinematic equality constraints.
However, task performance alone is not a reliable indicator of the policy's
ability to precisely learn constraints in the training data. To investigate, we
analyze how well diffusion policies discover these manifolds with a case study
on a bimanual pick-and-place task that encourages fulfillment of a kinematic
constraint for success. We study how three factors affect trained policies:
dataset size, dataset quality, and manifold curvature. Our experiments show
diffusion policies learn a coarse approximation of the constraint manifold with
learning affected negatively by decreases in both dataset size and quality. On
the other hand, the curvature of the constraint manifold showed inconclusive
correlations with both constraint satisfaction and task success. A hardware
evaluation verifies the applicability of our results in the real world. Project
website with additional results and visuals:
https://diffusion-learns-kinematic.github.io

</details>


### [195] [AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation](https://arxiv.org/abs/2510.01433)
*Anukriti Singh,Kasra Torshizi,Khuzema Habib,Kelin Yu,Ruohan Gao,Pratap Tokekar*

Main category: cs.RO

TL;DR: AFFORD2ACT是一种基于文本提示和单张图像提取语义2D关键点的轻量级视觉机器人学习框架，通过三阶段流程实现高效、实时的操作任务控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于关键点的方法依赖人工启发式或任务耦合选择，难以扩展且语义理解有限；同时密集输入计算开销大且包含无关背景特征。

Method: 提出AFFORD2ACT框架，包含三个阶段：affordance过滤、类别级关键点构建、以及带嵌入门控的Transformer策略学习，从文本和图像中提取最小语义关键点集。

Result: 该方法生成仅38维的状态策略，15分钟内即可训练完成，在多种真实操作任务中表现出色，对未见物体、新类别、背景和干扰物达到82%的成功率。

Conclusion: AFFORD2ACT通过affordance引导的关键点选择实现了高数据效率和强泛化能力，无需本体感知或密集表示即可实现实时控制，推动了可扩展、语义化的视觉机器人学习。

Abstract: Vision-based robot learning often relies on dense image or point-cloud
inputs, which are computationally heavy and entangle irrelevant background
features. Existing keypoint-based approaches can focus on manipulation-centric
features and be lightweight, but either depend on manual heuristics or
task-coupled selection, limiting scalability and semantic understanding. To
address this, we propose AFFORD2ACT, an affordance-guided framework that
distills a minimal set of semantic 2D keypoints from a text prompt and a single
image. AFFORD2ACT follows a three-stage pipeline: affordance filtering,
category-level keypoint construction, and transformer-based policy learning
with embedded gating to reason about the most relevant keypoints, yielding a
compact 38-dimensional state policy that can be trained in 15 minutes, which
performs well in real-time without proprioception or dense representations.
Across diverse real-world manipulation tasks, AFFORD2ACT consistently improves
data efficiency, achieving an 82% success rate on unseen objects, novel
categories, backgrounds, and distractors.

</details>


### [196] [Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation](https://arxiv.org/abs/2510.01438)
*Minglun Wei,Xintong Yang,Yu-Kun Lai,Ze Ji*

Main category: cs.RO

TL;DR: 提出了一种用于实验室粉末运输的轨迹优化框架，结合可微物理模拟、低维技能空间参数化和课程学习策略，实现了高效稳定的机器人轨迹优化。


<details>
  <summary>Details</summary>
Motivation: 粉末操作在实验室自动化中具有挑战性，尤其是在需要高精度和稳定性的运输任务中，现有方法难以满足需求。

Method: 结合可微物理模拟建模颗粒动力学，采用低维技能空间降低优化复杂度，并利用课程学习策略逐步提升长视野任务性能，实现端到端的接触丰富型机器人轨迹优化。

Result: 实验结果表明，该方法在任务成功率和操作稳定性方面优于强化学习基线方法。

Conclusion: 所提出的框架有效提升了机器人在粉末运输任务中的性能，为实验室自动化中的复杂操作提供了可行解决方案。

Abstract: Robotic automation is accelerating scientific discovery by reducing manual
effort in laboratory workflows. However, precise manipulation of powders
remains challenging, particularly in tasks such as transport that demand
accuracy and stability. We propose a trajectory optimisation framework for
powder transport in laboratory settings, which integrates differentiable
physics simulation for accurate modelling of granular dynamics, low-dimensional
skill-space parameterisation to reduce optimisation complexity, and a
curriculum-based strategy that progressively refines task competence over long
horizons. This formulation enables end-to-end optimisation of contact-rich
robot trajectories while maintaining stability and convergence efficiency.
Experimental results demonstrate that the proposed method achieves superior
task success rates and stability compared to the reinforcement learning
baseline.

</details>


### [197] [Touching the tumor boundary: A pilot study on ultrasound based virtual fixtures for breast-conserving surgery](https://arxiv.org/abs/2510.01452)
*Laura Connolly,Tamas Ungi,Adnan Munawar,Anton Deguet,Chris Yeung,Russell H. Taylor,Parvin Mousavi,Gabor Fichtinger Keyvan Hashtrudi-Zaad*

Main category: cs.RO

TL;DR: 提出一种基于虚拟固定装置的协作式机器人触觉引导系统，用于辅助乳腺保乳手术中的肿瘤边界定位，并在模拟切除中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 乳腺保乳手术中肿瘤常具有高移动性、难以触及且边界不规则，传统方法难以精确界定肿瘤边界，因此需要一种能够提供实时引导的辅助系统。

Method: 将小型触觉机器人与电刀结合，利用超声和电磁导航确定肿瘤位置和边界，并在手术工具接触肿瘤边界时施加虚拟固定装置以提供触觉反馈；通过有无触觉引导的对比实验评估系统效果。

Result: 触觉引导显著改善了切除边缘，用户报告任务的心理负荷、挫败感和努力程度降低，但也发现了一些影响外科工作流程的意外问题。

Conclusion: 虚拟固定装置有助于在模拟乳腺保乳手术中定位肿瘤边界，后续将开展大规模用户研究以进一步验证并优化系统。

Abstract: Purpose: Delineating tumor boundaries during breast-conserving surgery is
challenging as tumors are often highly mobile, non-palpable, and have
irregularly shaped borders. To address these challenges, we introduce a
cooperative robotic guidance system that applies haptic feedback for tumor
localization. In this pilot study, we aim to assess if and how this system can
be successfully integrated into breast cancer care.
  Methods: A small haptic robot is retrofitted with an electrocautery blade to
operate as a cooperatively controlled surgical tool. Ultrasound and
electromagnetic navigation are used to identify the tumor boundaries and
position. A forbidden region virtual fixture is imposed when the surgical tool
collides with the tumor boundary. We conducted a study where users were asked
to resect tumors from breast simulants both with and without the haptic
guidance. We then assess the results of these simulated resections both
qualitatively and quantitatively.
  Results: Virtual fixture guidance is shown to improve resection margins. On
average, users find the task to be less mentally demanding, frustrating, and
effort intensive when haptic feedback is available. We also discovered some
unanticipated impacts on surgical workflow that will guide design adjustments
and training protocol moving forward.
  Conclusion: Our results suggest that virtual fixtures can help localize tumor
boundaries in simulated breast-conserving surgery. Future work will include an
extensive user study to further validate these results and fine-tune our
guidance system.

</details>


### [198] [VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs](https://arxiv.org/abs/2510.01483)
*Mohamad Al Mdfaa,Svetlana Lukina,Timur Akhtyamov,Arthur Nigmatzyanov,Dmitrii Nalberskii,Sergey Zagoruyko,Gonzalo Ferrer*

Main category: cs.RO

TL;DR: 提出VL-KnG，一种基于时空知识图谱的视觉场景理解系统，用于解决VLM在机器人导航中的记忆、空间推理和扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLM）在机器人导航中缺乏持久场景记忆、空间推理能力有限，且难以随视频长度扩展，限制了实时应用。

Method: 通过分块处理视频，利用现代VLM构建持久化的时空知识图谱，并通过可查询的图结构实现可解释的空间推理与目标识别。

Result: 在新提出的WalkieKnowledge基准上验证，真实机器人部署中达到77.27%任务成功率和76.92%回答准确率，性能媲美Gemini 2.5 Pro，同时支持实时、可解释的导航、定位与规划。

Conclusion: VL-KnG通过知识图谱实现了高效、可解释且可扩展的视觉语言导航，为VLM在机器人中的实际应用提供了新路径。

Abstract: Vision-language models (VLMs) have shown potential for robot navigation but
encounter fundamental limitations: they lack persistent scene memory, offer
limited spatial reasoning, and do not scale effectively with video duration for
real-time application. We present VL-KnG, a Visual Scene Understanding system
that tackles these challenges using spatiotemporal knowledge graph construction
and computationally efficient query processing for navigation goal
identification. Our approach processes video sequences in chunks utilizing
modern VLMs, creates persistent knowledge graphs that maintain object identity
over time, and enables explainable spatial reasoning through queryable graph
structures. We also introduce WalkieKnowledge, a new benchmark with about 200
manually annotated questions across 8 diverse trajectories spanning
approximately 100 minutes of video data, enabling fair comparison between
structured approaches and general-purpose VLMs. Real-world deployment on a
differential drive robot demonstrates practical applicability, with our method
achieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5
Pro performance while providing explainable reasoning supported by the
knowledge graph, computational efficiency for real-time deployment across
different tasks, such as localization, navigation and planning. Code and
dataset will be released after acceptance.

</details>


### [199] [Pose Estimation of a Thruster-Driven Bioinspired Multi-Link Robot](https://arxiv.org/abs/2510.01485)
*Nicholas B. Andrews,Yanhao Yang,Sofya Akhetova,Kristi A. Morgansen,Ross L. Hatton*

Main category: cs.RO

TL;DR: 本文提出了一种基于无迹卡尔曼滤波和高斯过程残差学习的自由漂浮多关节仿生机器人姿态估计方法，通过实验和离线分析验证了在欠驱动、传感器受限条件下的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于机器人具有未驱动关节、仅依赖链路安装推进器控制且传感器配置极简（每链路仅一个陀螺仪），传统状态估计方法难以准确估计其完整姿态，因此需要开发一种鲁棒且数据高效的估计框架。

Method: 采用增强型无迹卡尔曼滤波（UKF）结合高斯过程残差学习来补偿非零均值、非高斯噪声；利用包含多种步态（前进、后退、左右移动和转向）的多步态数据集训练滤波器，并与仅使用前进数据的大数据集进行性能比较。

Result: 实验表明，所提出的滤波器能可靠地估计机器人的位置和形状；在前进步态测试轨迹上，使用多步态小数据集训练的滤波器性能与使用更大前进步态专一数据集训练的滤波器相当。

Conclusion: 不同步态之间存在输入空间重叠，利用该特性可在减少训练数据需求的同时提升滤波器对多步态的泛化能力，为低传感、欠驱动系统提供了高效的状态估计解决方案。

Abstract: This work demonstrates pose (position and shape) estimation for a
free-floating, bioinspired multi-link robot with unactuated joints,
link-mounted thrusters for control, and a single gyroscope per link, resulting
in an underactuated, minimally sensed platform. Through a proof-of-concept
hardware experiment and offline Kalman filter analysis, we show that the
robot's pose can be reliably estimated. State estimation is performed using an
unscented Kalman filter augmented with Gaussian process residual learning to
compensate for non-zero-mean, non-Gaussian noise. We further show that a filter
trained on a multi-gait dataset (forward, backward, left, right, and turning)
performs comparably to one trained on a larger forward-gait-only dataset when
both are evaluated on the same forward-gait test trajectory. These results
reveal overlap in the gait input space, which can be exploited to reduce
training data requirements while enhancing the filter's generalizability across
multiple gaits.

</details>


### [200] [Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments](https://arxiv.org/abs/2510.01519)
*Wei Han Chen,Yuchen Liu,Alexiy Buynitsky,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 提出了一种基于分层神经场的机器人导航方法，通过高阶稀疏图和低阶神经场求解Eikonal PDE，在大尺度复杂环境中实现了更优的适应性和精度。


<details>
  <summary>Details</summary>
Motivation: 现有导航方法在分辨率控制、可扩展性或数据需求方面存在局限，且神经场方法面临频谱偏差和灾难性遗忘问题。

Method: 将规划问题分解为层次结构：高层用稀疏图捕捉全局连通性，底层基于神经场通过求解Eikonal PDE导航局部障碍。

Result: 在大规模环境中验证了该方法相比先前方法具有更高的适应性和精确性。

Conclusion: 该物理信息驱动的分层策略有效克服了频谱偏差和神经场拟合难题，适用于在线探索、建图和真实场景导航。

Abstract: Robot navigation in large, complex, and unknown indoor environments is a
challenging problem. The existing approaches, such as traditional
sampling-based methods, struggle with resolution control and scalability, while
imitation learning-based methods require a large amount of demonstration data.
Active Neural Time Fields (ANTFields) have recently emerged as a promising
solution by using local observations to learn cost-to-go functions without
relying on demonstrations. Despite their potential, these methods are hampered
by challenges such as spectral bias and catastrophic forgetting, which diminish
their effectiveness in complex scenarios. To address these issues, our approach
decomposes the planning problem into a hierarchical structure. At the high
level, a sparse graph captures the environment's global connectivity, while at
the low level, a planner based on neural fields navigates local obstacles by
solving the Eikonal PDE. This physics-informed strategy overcomes common
pitfalls like spectral bias and neural field fitting difficulties, resulting in
a smooth and precise representation of the cost landscape. We validate our
framework in large-scale environments, demonstrating its enhanced adaptability
and precision compared to previous methods, and highlighting its potential for
online exploration, mapping, and real-world navigation.

</details>


### [201] [Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion](https://arxiv.org/abs/2510.01592)
*Shun Niijima,Ryoichi Tsuzaki,Noriaki Takasugi,Masaya Kinoshita*

Main category: cs.RO

TL;DR: 提出了一种基于GPU加速的高分辨率3D体素地图的实时多平面分割方法，用于足式机器人运动控制。


<details>
  <summary>Details</summary>
Motivation: 现有在线平面映射方法在精度和计算效率之间难以平衡，且无法有效表示复杂的3D结构（如悬挑），缺乏适用于实时应用的体素化平面分割方案。

Method: 结合基于顶点的连通分量标记、RANSAC平面检测和凸包方法，并利用GPU并行计算，从高分辨率3D体素地图中的点云快速提取平面区域。

Result: 该方法在0.01米分辨率下仍能以超过30 Hz的更新速率实现快速准确的3D多平面分割，并在仿真和真实足式机器人平台上验证了其有效性。

Conclusion: 所提方法能够实现实时、精确的多平面分割，支持足式机器人在复杂3D环境中稳定运动控制。

Abstract: This paper proposes a real-time multi-plane segmentation method based on
GPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion.
Existing online planar mapping approaches struggle to balance accuracy and
computational efficiency: direct depth image segmentation from specific sensors
suffers from poor temporal integration, height map-based methods cannot
represent complex 3D structures like overhangs, and voxel-based plane
segmentation remains unexplored for real-time applications. To address these
limitations, we develop a novel framework that integrates vertex-based
connected component labeling with random sample consensus based plane detection
and convex hull, leveraging GPU parallel computing to rapidly extract planar
regions from point clouds accumulated in high-resolution 3D voxel maps.
Experimental results demonstrate that the proposed method achieves fast and
accurate 3D multi-plane segmentation at over 30 Hz update rate even at a
resolution of 0.01 m, enabling the detected planes to be utilized in real time
for locomotion tasks. Furthermore, we validate the effectiveness of our
approach through experiments in both simulated environments and physical legged
robot platforms, confirming robust locomotion performance when considering 3D
planar structures.

</details>


### [202] [MiniBEE: A New Form Factor for Compact Bimanual Dexterity](https://arxiv.org/abs/2510.01603)
*Sharfin Islam,Zewen Chen,Zhanpeng He,Swapneel Bhatt,Andres Permuy,Brock Taylor,James Vickery,Pedro Piacenza,Cheng Zhang,Matei Ciocarlie*

Main category: cs.RO

TL;DR: 本文提出了一种名为MiniBEE的紧凑型双臂末端执行器系统，通过将两个低自由度机械臂耦合，实现高效的双手协调操作，同时支持可穿戴数据采集和标准机械臂部署。


<details>
  <summary>Details</summary>
Motivation: 传统的双臂机器人依赖完整的六或七自由度臂，系统复杂且仅利用部分工作空间进行灵巧操作，因此需要一种更轻量化、高效的设计。

Method: 设计了一种耦合的运动链结构，使两个低自由度机械臂（各3+自由度）保持夹持器间的完整相对定位，并提出运动学灵巧性指标以优化设计，支持可穿戴演示与机器人部署两种模式。

Result: 实现了扩大的灵巧工作空间，系统轻便且可穿戴，并展示了从可穿戴示范到模仿学习策略的端到端流程，成功应用于真实环境中的双手机械操作。

Conclusion: MiniBEE系统在降低复杂性的同时提升了灵巧性，为双手机械操作提供了一种高效、灵活的新方案。

Abstract: Bimanual robot manipulators can achieve impressive dexterity, but typically
rely on two full six- or seven- degree-of-freedom arms so that paired grippers
can coordinate effectively. This traditional framework increases system
complexity while only exploiting a fraction of the overall workspace for
dexterous interaction. We introduce the MiniBEE (Miniature Bimanual
End-effector), a compact system in which two reduced-mobility arms (3+ DOF
each) are coupled into a kinematic chain that preserves full relative
positioning between grippers. To guide our design, we formulate a kinematic
dexterity metric that enlarges the dexterous workspace while keeping the
mechanism lightweight and wearable. The resulting system supports two
complementary modes: (i) wearable kinesthetic data collection with self-tracked
gripper poses, and (ii) deployment on a standard robot arm, extending dexterity
across its entire workspace. We present kinematic analysis and design
optimization methods for maximizing dexterous range, and demonstrate an
end-to-end pipeline in which wearable demonstrations train imitation learning
policies that perform robust, real-world bimanual manipulation.

</details>


### [203] [ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations](https://arxiv.org/abs/2510.01607)
*Qiyuan Zeng,Chengmeng Li,Jude St. John,Zhongyi Zhou,Junjie Wen,Guorui Feng,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: ActiveUMI是一个便携式数据采集框架，通过VR遥操作和主动感知捕捉人类示范，用于训练具备复杂双手机械操作能力的机器人。


<details>
  <summary>Details</summary>
Motivation: 为了实现真实世界中通用且高性能的机器人策略，需要高效、可扩展的数据采集方法，将人类在自然环境中的操作示范有效迁移到机器人。

Method: 提出ActiveUMI框架，结合便携式VR遥操作系统与传感器化控制器，通过精确的姿态对齐桥接人机运动学，并利用头戴显示器记录操作员头部运动以捕捉主动的自我中心感知。系统采用3D模型渲染、可穿戴计算机和高效校准方法保证移动性和数据质量。

Result: 在六个复杂双手机械任务上评估，仅使用ActiveUMI数据训练的策略在分布内任务上平均成功率70%，在新物体和新环境中仍保持56%的成功率，表现出强泛化能力。

Conclusion: 便携式数据采集系统结合学习到的主动感知，为构建可泛化、高能力的真实世界机器人策略提供了一条有效且可扩展的路径。

Abstract: We present ActiveUMI, a framework for a data collection system that transfers
in-the-wild human demonstrations to robots capable of complex bimanual
manipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized
controllers that mirror the robot's end-effectors, bridging human-robot
kinematics via precise pose alignment. To ensure mobility and data quality, we
introduce several key techniques, including immersive 3D model rendering, a
self-contained wearable computer, and efficient calibration methods.
ActiveUMI's defining feature is its capture of active, egocentric perception.
By recording an operator's deliberate head movements via a head-mounted
display, our system learns the crucial link between visual attention and
manipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies
trained exclusively on ActiveUMI data achieve an average success rate of 70\%
on in-distribution tasks and demonstrate strong generalization, retaining a
56\% success rate when tested on novel objects and in new environments. Our
results demonstrate that portable data collection systems, when coupled with
learned active perception, provide an effective and scalable pathway toward
creating generalizable and highly capable real-world robot policies.

</details>


### [204] [FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models](https://arxiv.org/abs/2510.01642)
*Zijun Lin,Jiafei Duan,Haoquan Fang,Dieter Fox,Ranjay Krishna,Cheston Tan,Bihan Wen*

Main category: cs.RO

TL;DR: 本文提出了FailSafe，一个用于生成多样化失败案例并配对可执行恢复动作的系统，以提升视觉-语言-动作（VLA）模型在机器人操作任务中的失败检测与恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人操作数据集主要提供成功轨迹，缺乏对意外失败的有效恢复机制，且已有失败检测数据多为文本描述，难以直接用于VLA模型。因此，需要一种能自动生成可执行恢复动作的失败恢复系统。

Method: 提出FailSafe系统，可在任意模拟器中自动注入多种失败模式，并生成对应的恢复动作；基于该系统构建FailSafe-VLM模型，通过微调LLaVa-OneVision-7B实现，并在Maniskill任务中验证其有效性。

Result: FailSafe-VLM显著提升了三个先进VLA模型（pi0-FAST、OpenVLA、OpenVLA-OFT）在Maniskill多个任务上的表现，平均性能提升达22.6%，并展现出跨空间构型、视角和机器人形态的良好泛化能力。

Conclusion: FailSafe为机器人操作中的失败恢复提供了可扩展的数据生成方案，有效增强了VLA模型的鲁棒性和实用性，未来将开源代码以促进社区发展。

Abstract: Recent advances in robotic manipulation have integrated low-level robotic
control into Vision-Language Models (VLMs), extending them into
Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve
strong performance in downstream robotic applications, supported by large-scale
crowd-sourced robot training data, they still inevitably encounter failures
during execution. Enabling robots to reason about and recover from
unpredictable and abrupt failures remains a critical challenge. Existing
robotic manipulation datasets, collected in either simulation or the real
world, primarily provide only ground-truth trajectories, leaving robots unable
to recover once failures occur. Moreover, the few datasets that address failure
detection typically offer only textual explanations, which are difficult to
utilize directly in VLA models. To address this gap, we introduce FailSafe, a
novel failure generation and recovery system that automatically produces
diverse failure cases paired with executable recovery actions. FailSafe can be
seamlessly applied to any manipulation task in any simulator, enabling scalable
creation of failure-action data. To demonstrate its effectiveness, we fine-tune
LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results
show that FailSafe-VLM successfully helps robotic arm detect and recover from
potential failures, improving the performance of three state-of-the-art VLA
models pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several
tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different
spatial configurations, camera viewpoints, and robotic embodiments. We plan to
release the FailSafe code to the community.

</details>


### [205] [Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation](https://arxiv.org/abs/2510.01648)
*Seungwon Choi,Donggyu Park,Seo-Yeon Hwang,Tae-Wan Kim*

Main category: cs.RO

TL;DR: 提出一种基于多视角几何一致性自监督的在线学习框架，用于动态评估视觉-惯性里程计中传感器测量的可靠性，显著提高了定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设测量不确定性是静态且均匀的，难以准确反映真实场景中动态变化的误差特性，限制了VIO系统的性能。

Method: 利用多视角几何一致性作为自监督信号，从传感器数据和优化结果中在线学习测量可靠性，动态调整特征点的不确定性并自适应加权视觉测量。

Result: 在EuRoC数据集上测试，相比固定不确定性的基线方法，平移误差平均降低约24%，旋转误差降低约42%，系统实时运行且更具鲁棒性。

Conclusion: 该统计框架能有效提升VIO系统的精度与鲁棒性，通过在线学习测量可靠性实现了更优的状态估计。

Abstract: A fundamental challenge in robust visual-inertial odometry (VIO) is to
dynamically assess the reliability of sensor measurements. This assessment is
crucial for properly weighting the contribution of each measurement to the
state estimate. Conventional methods often simplify this by assuming a static,
uniform uncertainty for all measurements. This heuristic, however, may be
limited in its ability to capture the dynamic error characteristics inherent in
real-world data. To improve this limitation, we present a statistical framework
that learns measurement reliability assessment online, directly from sensor
data and optimization results. Our approach leverages multi-view geometric
consistency as a form of self-supervision. This enables the system to infer
landmark uncertainty and adaptively weight visual measurements during
optimization. We evaluated our method on the public EuRoC dataset,
demonstrating improvements in tracking accuracy with average reductions of
approximately 24\% in translation error and 42\% in rotation error compared to
baseline methods with fixed uncertainty parameters. The resulting framework
operates in real time while showing enhanced accuracy and robustness. To
facilitate reproducibility and encourage further research, the source code will
be made publicly available.

</details>


### [206] [Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation](https://arxiv.org/abs/2510.01661)
*Yifei Simon Shao,Yuchen Zheng,Sunan Sun,Pratik Chaudhari,Vijay Kumar,Nadia Figueroa*

Main category: cs.RO

TL;DR: SymSkill 是一种结合了模仿学习和任务-动作规划优势的统一框架，能够在动态环境中实现实时的多步操作、组合泛化和故障恢复。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在动态环境中的多步操作存在局限：模仿学习缺乏组合泛化能力，而传统任务-动作规划实时性差，难以应对执行失败。

Method: SymSkill 从无标签、未分割的演示数据中离线联合学习谓词、算子和技能；在执行时，通过符号规划器组合技能以达成目标，并在运动和符号层面实现实时恢复。

Result: 在 RoboCasa 仿真中，SymSkill 对12个单步任务达到85%的成功率，并能将技能组合成最多6次重组合的多步计划，具备强健的故障恢复能力；在真实 Franka 机器人上，仅用5分钟无标签数据即可完成多种任务。

Conclusion: SymSkill 成功融合了学习与规划的优势，实现了高效、安全、可恢复的多步操作，适用于动态和扰动环境。

Abstract: Multi-step manipulation in dynamic environments remains challenging. Two
major families of methods fail in distinct ways: (i) imitation learning (IL) is
reactive but lacks compositional generalization, as monolithic policies do not
decide which skill to reuse when scenes change; (ii) classical task-and-motion
planning (TAMP) offers compositionality but has prohibitive planning latency,
preventing real-time failure recovery. We introduce SymSkill, a unified
learning framework that combines the benefits of IL and TAMP, allowing
compositional generalization and failure recovery in real-time. Offline,
SymSkill jointly learns predicates, operators, and skills directly from
unlabeled and unsegmented demonstrations. At execution time, upon specifying a
conjunction of one or more learned predicates, SymSkill uses a symbolic planner
to compose and reorder learned skills to achieve the symbolic goals, while
performing recovery at both the motion and symbolic levels in real time.
Coupled with a compliant controller, SymSkill enables safe and uninterrupted
execution under human and environmental disturbances. In RoboCasa simulation,
SymSkill can execute 12 single-step tasks with 85% success rate. Without
additional data, it composes these skills into multi-step plans requiring up to
6 skill recompositions, recovering robustly from execution failures. On a real
Franka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented
and unlabeled play data, is capable of performing multiple tasks simply by goal
specifications. The source code and additional analysis can be found on
https://sites.google.com/view/symskill.

</details>


### [207] [Geometric Backstepping Control of Omnidirectional Tiltrotors Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances](https://arxiv.org/abs/2510.01675)
*Jaewoo Lee,Dongjae Lee,Jinwoo Lee,Hyungyu Lee,Yeonjoon Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 提出了一种考虑伺服和转子动力学的变倾角全向多旋翼几何反步控制器，具有指数稳定性，并在存在参数不确定性的情况下表现出鲁棒性，实验表明其在快速轨迹跟踪和抗扰恢复中优于不考虑执行器动态的基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统及固定倾角多旋翼的执行器感知控制依赖于执行器输入与力矩之间的线性关系，无法捕捉变倾角带来的非线性特性；同时，在剧烈飞行或突加干扰下，忽略执行器动态会影响控制的有效性和可靠性。

Method: 利用多旋翼刚体动力学与非线性执行器动力学之间的级联结构，设计几何反步控制器，并通过理论分析证明系统整体的指数稳定性。

Result: 实验揭示了执行器模型存在参数不确定性，所提控制器对此具有鲁棒性；在快速平移/旋转跟踪和突扰恢复三个实验中，相比忽略执行器动态的基线方法，该控制器表现更优，且在基线失控坠毁的情况下仍能稳定完成任务。

Conclusion: 所提出的几何反步控制器能有效处理变倾角多旋翼的非线性执行器动态，确保系统在高动态任务和外部干扰下的稳定性和优越性能，验证了显式建模执行器动态的重要性。

Abstract: This work presents a geometric backstepping controller for a variable-tilt
omnidirectional multirotor that explicitly accounts for both servo and rotor
dynamics. Considering actuator dynamics is essential for more effective and
reliable operation, particularly during aggressive flight maneuvers or recovery
from sudden disturbances. While prior studies have investigated actuator-aware
control for conventional and fixed-tilt multirotors, these approaches rely on
linear relationships between actuator input and wrench, which cannot capture
the nonlinearities induced by variable tilt angles. In this work, we exploit
the cascade structure between the rigid-body dynamics of the multirotor and its
nonlinear actuator dynamics to design the proposed backstepping controller and
establish exponential stability of the overall system. Furthermore, we reveal
parametric uncertainty in the actuator model through experiments, and we
demonstrate that the proposed controller remains robust against such
uncertainty. The controller was compared against a baseline that does not
account for actuator dynamics across three experimental scenarios: fast
translational tracking, rapid rotational tracking, and recovery from sudden
disturbance. The proposed method consistently achieved better tracking
performance, and notably, while the baseline diverged and crashed during the
fastest translational trajectory tracking and the recovery experiment, the
proposed controller maintained stability and successfully completed the tasks,
thereby demonstrating its effectiveness.

</details>


### [208] [PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization](https://arxiv.org/abs/2510.01708)
*Zixing Lei,Zibo Zhou,Sheng Yin,Yueru Chen,Qingyao Xu,Weixin Li,Yunhong Wang,Bowei Tang,Wei Jing,Siheng Chen*

Main category: cs.RO

TL;DR: 提出PolySim，一种基于多模拟器联合训练的人形机器人全身控制策略，通过动力学层面的域随机化减少模拟器归纳偏差，实现更好的sim-to-real迁移。


<details>
  <summary>Details</summary>
Motivation: 由于单一模拟器的固有假设和局限性（即模拟器归纳偏差），在仿真中训练的人形机器人全身控制策略往往难以在真实世界中有效迁移。不同模拟器之间以及仿真与现实之间存在显著差异，导致控制策略性能下降。

Method: 开发了PolySim平台，集成多个异构模拟器，支持在一次训练过程中并行运行来自不同引擎的环境，实现跨模拟器的联合训练和动力学级别的域随机化，从而学习更具泛化性的控制策略。

Result: 理论分析表明，PolySim比单模拟器训练能获得更紧的归纳偏差上界；实验显示，在sim-to-sim评估中显著降低了运动跟踪误差（例如在MuJoCo上相比IsaacSim基线执行成功率提高52.8）；并在真实Unitree G1机器人上实现了无需微调的零样本部署，验证了其有效的sim-to-real迁移能力。

Conclusion: PolySim通过多模拟器联合训练有效缓解了模拟器归纳偏差问题，提升了人形机器人控制策略的泛化性和现实迁移能力，为sim-to-real控制提供了新的可行路径。

Abstract: Humanoid whole-body control (WBC) policies trained in simulation often suffer
from the sim-to-real gap, which fundamentally arises from simulator inductive
bias, the inherent assumptions and limitations of any single simulator. These
biases lead to nontrivial discrepancies both across simulators and between
simulation and the real world. To mitigate the effect of simulator inductive
bias, the key idea is to train policies jointly across multiple simulators,
encouraging the learned controller to capture dynamics that generalize beyond
any single simulator's assumptions. We thus introduce PolySim, a WBC training
platform that integrates multiple heterogeneous simulators. PolySim can launch
parallel environments from different engines simultaneously within a single
training run, thereby realizing dynamics-level domain randomization.
Theoretically, we show that PolySim yields a tighter upper bound on simulator
inductive bias than single-simulator training. In experiments, PolySim
substantially reduces motion-tracking error in sim-to-sim evaluations; for
example, on MuJoCo, it improves execution success by 52.8 over an IsaacSim
baseline. PolySim further enables zero-shot deployment on a real Unitree G1
without additional fine-tuning, showing effective transfer from simulation to
the real world. We will release the PolySim code upon acceptance of this work.

</details>


### [209] [Contrastive Representation Regularization for Vision-Language-Action Models](https://arxiv.org/abs/2510.01711)
*Taeyoung Kim,Jimin Lee,Myungkyu Koo,Dongyoung Kim,Kyungmin Lee,Changyeon Kim,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: 提出了一种名为Robot State-aware Contrastive Loss（RS-CL）的表示正则化方法，用于提升视觉-语言-动作（VLA）模型在机器人操作中的性能，通过引入本体感知状态的相对距离作为软监督，显著提高了抓取与放置任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型缺乏对机器人控制信号和本体感知状态的敏感性，导致表示不够优化，限制了其在机器人操作中的表现。

Method: 提出RS-CL，通过对比学习将VLM的表示与机器人的本体感知状态对齐，利用状态间的相对距离提供软监督，并与原有的动作预测目标结合，增强控制相关的表示学习。

Result: 在RoboCasa-Kitchen任务中，RS-CL将任务成功率从30.8%提升至41.5%，并在真实机器人挑战性任务中从45.0%提升至58.3%，显著改善了抓取和放置的准确性。

Conclusion: RS-CL是一种轻量、有效且兼容现有VLA训练流程的正则化方法，能够显著提升VLA模型在机器人操作任务中的性能。

Abstract: Vision-Language-Action (VLA) models have shown its capabilities in robot
manipulation by leveraging rich representations from pre-trained
Vision-Language Models (VLMs). However, their representations arguably remain
suboptimal, lacking sensitivity to robotic signals such as control actions and
proprioceptive states. To address the issue, we introduce Robot State-aware
Contrastive Loss (RS-CL), a simple and effective representation regularization
for VLA models, designed to bridge the gap between VLM representations and
robotic signals. In particular, RS-CL aligns the representations more closely
with the robot's proprioceptive states, by using relative distances between the
states as soft supervision. Complementing the original action prediction
objective, RS-CL effectively enhances control-relevant representation learning,
while being lightweight and fully compatible with standard VLA training
pipeline. Our empirical results demonstrate that RS-CL substantially improves
the manipulation performance of state-of-the-art VLA models; it pushes the
prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,
through more accurate positioning during grasping and placing, and boosts
success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.

</details>


### [210] [Dual-Mode Magnetic Continuum Robot for Targeted Drug Delivery](https://arxiv.org/abs/2510.01761)
*Wendu Zhang,Heng Wang,Shuangyi Wang,Yuanrui Huang*

Main category: cs.RO

TL;DR: 提出一种嵌入径向永磁体的磁连续体机器人，实现弯曲与扭转解耦控制，并结合扭转剪切驱动的双层阻塞机制，完成按需药物释放。


<details>
  <summary>Details</summary>
Motivation: 传统轴向磁化磁连续体机器人仅能实现弯曲运动，限制了其在复杂解剖结构中的应用，需拓展形变能力以提升操控性。

Method: 通过在导管壁内嵌入径向永磁体，利用外部单个永磁体独立诱导弯曲或扭转；结合物理建模、有限元分析与实验验证解耦控制；设计双层阻塞机制利用扭转剪切实现药物释放。

Result: 实验证明在外加磁场下可实现解耦的弯曲与扭转控制，并成功在模拟体内环境中完成路径跟踪与扭转触发的定点药物释放。

Conclusion: 该无线、紧凑平台兼具灵活形变与精确载荷释放能力，有望推动下一代靶向治疗技术发展。

Abstract: Magnetic continuum robots (MCRs) enable minimally invasive navigation through
tortuous anatomical channels, yet axially magnetized designs have largely been
limited to bending-only motion. To expand deformation capabilities, this paper
presents a simple assembly that embeds permanent magnets radially within the
catheter wall, allowing a single externally steered permanent magnet to
independently induce either bending or torsion. A physics-based formulation
together with finite-element analysis establishes the actuation principles, and
benchtop experiments validate decoupled mode control under practical fields.
Building on this, a dual-layer blockage mechanism consisting of outer grooves
and inner plates leverages torsional shear to achieve on-demand drug release.
Finally, an in-phantom intervention experiment demonstrates end-to-end
operation: lumen following by bending for target approach, followed by
twist-activated release at the site. The resulting compact, cable-free platform
combines versatile deformation with precise payload delivery, indicating strong
potential for next-generation, site-specific therapies.

</details>


### [211] [An Anytime, Scalable and Complete Algorithm for Embedding a Manufacturing Procedure in a Smart Factory](https://arxiv.org/abs/2510.01770)
*Christopher Leet,Aidan Sciortino,Sven Koenig*

Main category: cs.RO

TL;DR: 本文提出了TS-ACES，一种可扩展的智能工厂嵌入（SFE）问题求解器，用于在包含上百台设备的现代智能工厂中高效分配制造流程任务和运输路径。


<details>
  <summary>Details</summary>
Motivation: 现有的SFE求解器无法扩展到包含大量机器的现代智能工厂，限制了自动化制造的应用。

Method: 提出了一种基于交通系统的任意时间循环嵌入求解器TS-ACES，支持大规模SFE实例的高效求解。

Result: TS-ACES具有完整性和高度可扩展性，能够在基于真实工业场景的大规模SFE实例中运行，支持超过百台机器的工厂环境。

Conclusion: TS-ACES是首个能够有效解决大规模智能工厂嵌入问题的方案，显著提升了SFE求解器的实用性和适用范围。

Abstract: Modern automated factories increasingly run manufacturing procedures using a
matrix of programmable machines, such as 3D printers, interconnected by a
programmable transport system, such as a fleet of tabletop robots. To embed a
manufacturing procedure into a smart factory, an operator must: (a) assign each
of its processes to a machine and (b) specify how agents should transport parts
between machines. The problem of embedding a manufacturing process into a smart
factory is termed the Smart Factory Embedding (SFE) problem. State-of-the-art
SFE solvers can only scale to factories containing a couple dozen machines.
Modern smart factories, however, may contain hundreds of machines. We fill this
hole by introducing the first highly scalable solution to the SFE, TS-ACES, the
Traffic System based Anytime Cyclic Embedding Solver. We show that TS-ACES is
complete and can scale to SFE instances based on real industrial scenarios with
more than a hundred machines.

</details>


### [212] [Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2510.01795)
*Haibo Hu,Lianming Huang,Xinyu Wang,Yufei Cui,Nan Guan,Chun Jason Xue*

Main category: cs.RO

TL;DR: 提出Nav-EE，一种导航引导的早期退出框架，利用导航先验动态选择退出层，在保持精度的同时显著降低视觉语言模型在自动驾驶中的推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的早期退出方法因任务依赖性限制了在多样化场景中的泛化能力，而自动驾驶中导航系统可预知上下文，为解决该问题提供了契机。

Method: 通过导航先验离线预计算任务特定的退出层，并在在线推理时根据实际导航上下文动态应用这些退出层，实现高效推理。

Result: 在CODA、Waymo和BOSCH数据集上实验显示，Nav-EE在保持与全模型推理相当精度的同时，最多减少63.9%的延迟；实车集成显示推理时间从600ms降至300ms。

Conclusion: 将导航预见性与早期退出结合，为大型模型在自动驾驶系统中的高效部署提供了一条可行路径。

Abstract: Vision-Language Models (VLMs) are increasingly applied in autonomous driving
for unified perception and reasoning, but high inference latency hinders
real-time deployment. Early-exit reduces latency by terminating inference at
intermediate layers, yet its task-dependent nature limits generalization across
diverse scenarios. We observe that this limitation aligns with autonomous
driving: navigation systems can anticipate upcoming contexts (e.g.,
intersections, traffic lights), indicating which tasks will be required. We
propose Nav-EE, a navigation-guided early-exit framework that precomputes
task-specific exit layers offline and dynamically applies them online based on
navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE
achieves accuracy comparable to full inference while reducing latency by up to
63.9%. Real-vehicle integration with Autoware Universe further demonstrates
reduced inference latency (600ms to 300ms), supporting faster decision-making
in complex scenarios. These results suggest that coupling navigation foresight
with early-exit offers a viable path toward efficient deployment of large
models in autonomous systems. Code and data are available at our anonymous
repository: https://anonymous.4open.science/r/Nav-EE-BBC4

</details>


### [213] [What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework](https://arxiv.org/abs/2510.01830)
*Hongze Wang,Boyang Sun,Jiaxu Xing,Fan Yang,Marco Hutter,Dhruv Shah,Davide Scaramuzza,Marc Pollefeys*

Main category: cs.RO

TL;DR: 本文对基于强化学习的模块化ObjectNav系统进行了大规模实证研究，将其分解为感知、策略和测试时增强三个关键组件，发现感知质量和测试时策略是性能的关键驱动因素，而现有方法下的策略改进仅带来边际增益。基于此，作者提出了实用的设计指南，并构建了一个超越当前最先进方法的系统，在SPL上提升6.6%，成功率提高2.7%，同时引入了人类基线（专家成功率达98%），揭示了RL智能体与人类导航能力之间的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在Object-Goal Navigation（ObjectNav）中广泛应用，但领域内缺乏统一分析来识别真正推动性能的核心组件，因此需要系统性研究各模块的实际贡献。

Method: 将模块化RL-based ObjectNav系统分解为感知、政策和测试时增强三部分，通过大量受控实验分别评估每个组件对整体性能的影响。

Result: 感知质量和测试时策略显著影响性能，而当前策略改进效果有限；所提系统在SPL上提升6.6%，成功率提高2.7%；人类专家在相同条件下达到98%的成功率。

Conclusion: 感知和测试时增强比策略优化更重要，未来应重点关注这些方面；本研究提供了指导未来ObjectNav发展的原则性建议，并设定了新的SotA性能基准。

Abstract: Object-Goal Navigation (ObjectNav) is a critical component toward deploying
mobile robots in everyday, uncontrolled environments such as homes, schools,
and workplaces. In this context, a robot must locate target objects in
previously unseen environments using only its onboard perception. Success
requires the integration of semantic understanding, spatial reasoning, and
long-horizon planning, which is a combination that remains extremely
challenging. While reinforcement learning (RL) has become the dominant
paradigm, progress has spanned a wide range of design choices, yet the field
still lacks a unifying analysis to determine which components truly drive
performance. In this work, we conduct a large-scale empirical study of modular
RL-based ObjectNav systems, decomposing them into three key components:
perception, policy, and test-time enhancement. Through extensive controlled
experiments, we isolate the contribution of each and uncover clear trends:
perception quality and test-time strategies are decisive drivers of
performance, whereas policy improvements with current methods yield only
marginal gains. Building on these insights, we propose practical design
guidelines and demonstrate an enhanced modular system that surpasses
State-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We
also introduce a human baseline under identical conditions, where experts
achieve an average 98% success, underscoring the gap between RL agents and
human-level navigation. Our study not only sets the SotA performance but also
provides principled guidance for future ObjectNav development and evaluation.

</details>


### [214] [Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots](https://arxiv.org/abs/2510.01843)
*Wanyue Li,Ji Ma,Minghao Lu,Peng Lu*

Main category: cs.RO

TL;DR: 本研究提出了一种受无人机应用启发的时空轨迹规划方法，用于双足机器人足球中的精确踢球控制，实现了快速、稳定的自主轨迹生成，并在仿真和硬件实验中表现出高效性和高任务完成率。


<details>
  <summary>Details</summary>
Motivation: 现有基于位置控制或强化学习的方法在双足机器人踢球动作中难以兼顾系统稳定性与精确的球路控制，且传统MPC方法对摆动腿运动建模过于简化，限制了机器人与环境的交互能力。

Method: 借鉴无人机领域的时空轨迹规划方法，将其创新性地应用于双足机器人系统，自主生成满足目标踢球位置、速度和加速度约束的足部轨迹，同时优化摆动阶段的时间。

Result: 优化后的轨迹能有效模拟人类踢球行为（包含回摆动作），轨迹规划时间小于1毫秒，在-90°到90°范围内踢球任务完成准确率接近100%。

Conclusion: 该方法显著提升了双足机器人在动态踢球任务中的性能，兼具高效性、稳定性和实用性，为复杂动作的精确控制提供了新思路。

Abstract: Humanoid robot soccer presents several challenges, particularly in
maintaining system stability during aggressive kicking motions while achieving
precise ball trajectory control. Current solutions, whether traditional
position-based control methods or reinforcement learning (RL) approaches,
exhibit significant limitations. Model predictive control (MPC) is a prevalent
approach for ordinary quadruped and biped robots. While MPC has demonstrated
advantages in legged robots, existing studies often oversimplify the leg swing
progress, relying merely on simple trajectory interpolation methods. This
severely constrains the foot's environmental interaction capability, hindering
tasks such as ball kicking. This study innovatively adapts the spatial-temporal
trajectory planning method, which has been successful in drone applications, to
bipedal robotic systems. The proposed approach autonomously generates foot
trajectories that satisfy constraints on target kicking position, velocity, and
acceleration while simultaneously optimizing swing phase duration. Experimental
results demonstrate that the optimized trajectories closely mimic human kicking
behavior, featuring a backswing motion. Simulation and hardware experiments
confirm the algorithm's efficiency, with trajectory planning times under 1 ms,
and its reliability, achieving nearly 100 % task completion accuracy when the
soccer goal is within the range of -90{\deg} to 90{\deg}.

</details>


### [215] [GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics](https://arxiv.org/abs/2510.01848)
*Diram Tabaa,Gianni Di Caro*

Main category: cs.RO

TL;DR: 提出GreenhouseSplat框架，利用RGB图像生成逼真的温室资产，并集成到ROS仿真中，支持相机和LiDAR渲染，用于农业机器人评估。


<details>
  <summary>Details</summary>
Motivation: 现有温室仿真依赖简化或合成资产，限制了从仿真到现实的迁移能力。需要更真实、可扩展的仿真环境以提升农业机器人系统的开发与评估。

Method: 基于高斯溅射（Gaussian splatting）等辐射场方法，从低成本RGB图像构建逼真的温室资产，并将其集成到ROS-based仿真环境中，支持多传感器渲染。

Result: 发布了包含82株黄瓜植物的多行配置数据集，验证了所生成资产在定位等机器人任务中的实用性。

Conclusion: GreenhouseSplat是迈向温室规模辐射场仿真的第一步，为农业机器人研究提供了基础性工具和数据支持。

Abstract: Simulating greenhouse environments is critical for developing and evaluating
robotic systems for agriculture, yet existing approaches rely on simplistic or
synthetic assets that limit simulation-to-real transfer. Recent advances in
radiance field methods, such as Gaussian splatting, enable photorealistic
reconstruction but have so far been restricted to individual plants or
controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a
framework and dataset for generating photorealistic greenhouse assets directly
from inexpensive RGB images. The resulting assets are integrated into a
ROS-based simulation with support for camera and LiDAR rendering, enabling
tasks such as localization with fiducial markers. We provide a dataset of 82
cucumber plants across multiple row configurations and demonstrate its utility
for robotics evaluation. GreenhouseSplat represents the first step toward
greenhouse-scale radiance-field simulation and offers a foundation for future
research in agricultural robotics.

</details>


### [216] [SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot](https://arxiv.org/abs/2510.01984)
*Yue Wang*

Main category: cs.RO

TL;DR: 本文提出了一种紧凑、开源的三自由度矢状面脊柱模块SPARC，集成了可编程任务空间阻抗控制，适用于四足机器人。


<details>
  <summary>Details</summary>
Motivation: 为了提升四足机器人在运动中的适应性和稳定性，研究脊柱柔顺性的作用，需要一个能够精确控制刚度和阻尼的实验平台。

Method: SPARC模块结合了旋转和滑动运动，采用三个力矩控制执行器和定制的1kHz控制板，在1.26kg的封装内实现x、z和theta方向的闭环刚度与阻尼调节；使用基于RNEA的计算加速度控制器，并加入平滑的Stribeck摩擦补偿，以实现弹簧-阻尼行为。

Result: 实验验证表明，准静态推拉测试中具有线性力-位移特性，命令水平刚度范围为300-700 N/m，相对误差≤1.5%（R²≥0.992，95%置信区间窄）；动态释放试验确认了多种阻尼设置下的质量-弹簧-阻尼响应，存在小且可解释的相位偏差；任务空间PD控制器表现出近似线性刚度但变异性较大且对耦合敏感。

Conclusion: SPARC提供了一个便携式平台，可用于系统研究腿部运动中脊柱柔顺性的影响，并将公开全部硬件和固件资源。

Abstract: We present SPARC, a compact, open-source 3-DoF sagittal-plane spine module
that combines revolute (pitch) and prismatic (axial) motion with programmable
task-space impedance for quadruped robots. The system integrates three
torque-controlled actuators, a custom 1 kHz control board, and a protected
power unit in a 1.26 kg package, enabling closed-loop stiffness and damping
shaping along x, z, and theta. We develop an RNEA-based computed-acceleration
controller with smooth Stribeck friction compensation to render spring-damper
behavior without explicit inertia shaping. Bench experiments validate the
approach. Quasi-static push-pull tests show linear force-displacement
characteristics with commanded horizontal stiffness spanning 300-700 N/m and <=
1.5% relative error (R^2 >= 0.992, narrow 95% CIs). Dynamic
displace-and-release trials confirm mass-spring-damper responses over multiple
damping settings, with small, interpretable phase deviations due to
configuration-dependent inertia and low-speed friction effects. A task-space PD
controller produces roughly linear stiffness but with greater variability and
coupling sensitivity. SPARC provides a portable platform for systematic studies
of spine compliance in legged locomotion and will be released with complete
hardware and firmware resources.

</details>


### [217] [Reducing Discomfort in Driving Simulators: Motion Cueing for Motion Sickness Mitigation](https://arxiv.org/abs/2510.01986)
*Varun Kotian,Vishrut Jain,Andrea Michelle Rios Lazcano,Daan Marinus Pool,Riender Happee,Barys Shyrokau*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型预测控制（MPC）的运动提示算法，通过最小化主观垂直冲突（SVC）来减少驾驶模拟器中的晕动症，同时兼顾运动再现的保真度。


<details>
  <summary>Details</summary>
Motivation: 由于视觉与运动感知不匹配，模拟器常引发晕动症，限制了其应用。需要在保证驾驶体验保真度的同时有效降低用户的不适感。

Method: 设计了一个结合感官冲突和比力误差惩罚的MPC框架，优化运动提示算法；通过人因实验比较了不同设置下的晕动症水平和保真度评价。

Result: 实验表明，与自适应washout和纯比力跟踪算法相比，折衷方案将晕动症降低了50%以上（MISC评分从3降至1.5），且未显著影响保真度感知。无运动条件最舒适但保真度最低。

Conclusion: 该方法在考虑模拟器动力学和晕动症动态演变的基础上，实现了舒适性与保真度的更好平衡，推动了驾驶模拟器的广泛应用。

Abstract: Driving simulators are increasingly used in research and development.
However, simulators often cause motion sickness due to downscaled motion and
unscaled veridical visuals. In this paper, a motion cueing algorithm is
proposed that reduces motion sickness as predicted by the subjective vertical
conflict (SVC) model using model predictive control (MPC). Both sensory
conflict and specific force errors are penalised in the cost function, allowing
the algorithm to jointly optimise fidelity and comfort.
  Human-in-the-loop experiments were conducted to compare four simulator motion
settings: two variations of our MPC-based algorithm, one focused on pure
specific force tracking and the second compromising specific force tracking and
motion sickness minimisation, as well as reference adaptive washout and no
motion cases. The experiments were performed on a hexapod driving simulator
with participants exposed to passive driving.
  Experimental motion sickness results closely matched the sickness model
predictions. As predicted by the model, the no motion condition yielded the
lowest sickness levels. However, it was rated lowest in terms of fidelity. The
compromise solution reduced sickness by over 50% (average MISC level 3 to 1.5)
compared to adaptive washout and the algorithm focusing on specific force
tracking, without any significant reduction in fidelity rating.
  The proposed approach for developing MCA that takes into account both the
simulator dynamics and time evolution of motion sickness offers a significant
advancement in achieving an optimal control of motion sickness and specific
force recreation in driving simulators, supporting broader simulator use.

</details>


### [218] [EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2510.02080)
*Lingxiang Hu,Naima Ait Oufroukh,Fabien Bonardi,Raymond Ghandour*

Main category: cs.RO

TL;DR: 提出了一种无需相机标定的单目稠密SLAM框架EC3R-SLAM，兼顾高精度、低延迟和低GPU内存消耗，适用于资源受限平台。


<details>
  <summary>Details</summary>
Motivation: 现有单目稠密SLAM方法存在高延迟、高GPU内存消耗和依赖相机标定的问题，限制了其实际应用。

Method: 通过耦合维护稀疏特征点地图的跟踪模块与基于前馈3D重建模型的映射模块，同时估计相机内参，实现高效、无需标定的SLAM；引入局部和全局回环检测以增强多视角一致性。

Result: 在多个基准测试中，EC3R-SLAM在精度上具有竞争力，同时速度更快、内存更高效，并能在笔记本和Jetson Orin NX等设备上有效运行。

Conclusion: EC3R-SLAM在不依赖相机标定的前提下，实现了高效、准确且鲁棒的单目稠密SLAM，具备良好的实际机器人应用潜力。

Abstract: The application of monocular dense Simultaneous Localization and Mapping
(SLAM) is often hindered by high latency, large GPU memory consumption, and
reliance on camera calibration. To relax this constraint, we propose EC3R-SLAM,
a novel calibration-free monocular dense SLAM framework that jointly achieves
high localization and mapping accuracy, low latency, and low GPU memory
consumption. This enables the framework to achieve efficiency through the
coupling of a tracking module, which maintains a sparse map of feature points,
and a mapping module based on a feed-forward 3D reconstruction model that
simultaneously estimates camera intrinsics. In addition, both local and global
loop closures are incorporated to ensure mid-term and long-term data
association, enforcing multi-view consistency and thereby enhancing the overall
accuracy and robustness of the system. Experiments across multiple benchmarks
show that EC3R-SLAM achieves competitive performance compared to
state-of-the-art methods, while being faster and more memory-efficient.
Moreover, it runs effectively even on resource-constrained platforms such as
laptops and Jetson Orin NX, highlighting its potential for real-world robotics
applications.

</details>


### [219] [LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions](https://arxiv.org/abs/2510.02104)
*Yunhan Lin,Wenqi Wu,Zhijie Zhang,Huasong Min*

Main category: cs.RO

TL;DR: 提出LangGrasp框架，结合大语言模型与点云定位模块，实现从对象级到部件级的高精度语言驱动抓取，有效解析模糊指令中的隐含意图。


<details>
  <summary>Details</summary>
Motivation: 现有语言驱动抓取方法难以处理包含隐含意图的模糊指令，需提升机器人在非结构化环境中的语义理解与操作精度。

Method: 集成微调后的大语言模型以理解语言中的隐含意图，并设计基于2D部件分割引导的点云定位模块，实现细粒度的部件级抓取定位。

Result: LangGrasp能准确解析模糊指令中的隐含意图，动态选择最优抓取姿态，在对象级和部件级操作中均表现出高精度和强适应性。

Conclusion: 该框架显著提升了机器人在复杂环境中对语言指令的理解能力和抓取操作的灵活性与效率。

Abstract: The existing language-driven grasping methods struggle to fully handle
ambiguous instructions containing implicit intents. To tackle this challenge,
we propose LangGrasp, a novel language-interactive robotic grasping framework.
The framework integrates fine-tuned large language models (LLMs) to leverage
their robust commonsense understanding and environmental perception
capabilities, thereby deducing implicit intents from linguistic instructions
and clarifying task requirements along with target manipulation objects.
Furthermore, our designed point cloud localization module, guided by 2D part
segmentation, enables partial point cloud localization in scenes, thereby
extending grasping operations from coarse-grained object-level to fine-grained
part-level manipulation. Experimental results show that the LangGrasp framework
accurately resolves implicit intents in ambiguous instructions, identifying
critical operations and target information that are unstated yet essential for
task completion. Additionally, it dynamically selects optimal grasping poses by
integrating environmental information. This enables high-precision grasping
from object-level to part-level manipulation, significantly enhancing the
adaptability and task execution efficiency of robots in unstructured
environments. More information and code are available here:
https://github.com/wu467/LangGrasp.

</details>


### [220] [Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control](https://arxiv.org/abs/2510.02129)
*Philip Reichenberg,Tim Laue*

Main category: cs.RO

TL;DR: 本文提出了一种提高NAO人形机器人自主站起成功率的方法，通过解决关节位置执行误差问题，显著提升了站起动作的可靠性，并已被多个标准平台联赛队伍采用。


<details>
  <summary>Details</summary>
Motivation: 提高人形机器人在足球比赛中自主站起的成功率，避免因无法自行站起而被暂时移出比赛。

Method: 针对关节位置执行中的大误差问题，设计特殊动作释放卡住的肢体（如手臂），或通过其他关节补偿误差，从而优化站起流程。

Result: 显著提高了站起动作的整体成功率，并通过多年实战验证；多个其他团队使用该方法后也达到了相似的成功率。

Conclusion: 解决关节执行误差是提升机器人站起成功率的关键，所提出的方法具有实际有效性与广泛适用性。

Abstract: Stand-up motions are an indispensable part of humanoid robot soccer. A robot
incapable of standing up by itself is removed from the game for some time. In
this paper, we present our stand-up motions for the NAO robot. Our approach
dates back to 2019 and has been evaluated and slightly expanded over the past
six years. We claim that the main reason for failed stand-up attempts are large
errors in the executed joint positions. By addressing such problems by either
executing special motions to free up stuck limbs such as the arms, or by
compensating large errors with other joints, we significantly increased the
overall success rate of our stand-up routine. The motions presented in this
paper are also used by several other teams in the Standard Platform League,
which thereby achieve similar success rates, as shown in an analysis of videos
from multiple tournaments.

</details>


### [221] [SCANS: A Soft Gripper with Curvature and Spectroscopy Sensors for In-Hand Material Differentiation](https://arxiv.org/abs/2510.02164)
*Nathaniel Hanson,Austin Allison,Charles DiMarzio,Taşkın Padır,Kristen L. Dorsey*

Main category: cs.RO

TL;DR: 提出了一种名为SCANS的无电子、流体驱动软体机械手，具备在手中或预触抓取时进行光谱感知的能力，显著提升了软体机器人在材料识别方面的性能。


<details>
  <summary>Details</summary>
Motivation: 为了增强软体机器人对物体材质的感知能力，特别是在视觉相似但材质不同的物体之间进行区分，开发一种多功能、无需电子元件的光谱感知系统。

Method: 设计并实现了一个基于软体结构的流体驱动夹持器（SCANS），集成了软曲率与光谱传感功能；通过材料分析选择最优软基底，并利用近红外光谱和线性判别分析评估其在预触和在手状态下的感知性能。

Result: 实验表明，SCANS能在多种材料（金属、木材、塑料、有机物、纸张、泡沫）间实现可解释的统计学分类，尤其在近红外波段表现出关键区分能力，光谱角差异显著。

Conclusion: SCANS系统拓展了光学作为软体机器人多用途感知模态的潜力，提供了一个开放、可复现的平台，推动软体机器人在智能感知方面的发展。

Abstract: We introduce the soft curvature and spectroscopy (SCANS) system: a versatile,
electronics-free, fluidically actuated soft manipulator capable of assessing
the spectral properties of objects either in hand or through pre-touch caging.
This platform offers a wider spectral sensing capability than previous soft
robotic counterparts. We perform a material analysis to explore optimal soft
substrates for spectral sensing, and evaluate both pre-touch and in-hand
performance. Experiments demonstrate explainable, statistical separation across
diverse object classes and sizes (metal, wood, plastic, organic, paper, foam),
with large spectral angle differences between items. Through linear
discriminant analysis, we show that sensitivity in the near-infrared
wavelengths is critical to distinguishing visually similar objects. These
capabilities advance the potential of optics as a multi-functional sensory
modality for soft robots. The complete parts list, assembly guidelines, and
processing code for the SCANS gripper are accessible at:
https://parses-lab.github.io/scans/.

</details>


### [222] [Product Digital Twin Supporting End-of-life Phase of Electric Vehicle Batteries Utilizing Product-Process-Resource Asset Network](https://arxiv.org/abs/2510.02167)
*Sara Strakosova,Petr Novak,Petr Kadera*

Main category: cs.RO

TL;DR: 本文提出了一种基于数字孪生技术的双流向产品-过程-资源资产网络（Bi-PAN），用于优化电动汽车电池的拆解过程，以支持再制造和回收，提升循环经济中的可持续性。


<details>
  <summary>Details</summary>
Motivation: 制造商在产品生命周期结束阶段缺乏对再制造和回收的数据支持，导致环境影响增加，因此需要一种能够贯穿制造与回收阶段的信息模型来提升可持续性。

Method: 提出Bi-Flow Product-Process-Resource Asset Network (Bi-PAN) 框架，扩展了原有的PAN范式，整合制造与再制造/回收阶段，并通过电动汽车电池的拆解案例验证数字孪生的应用。

Result: Bi-PAN能够灵活高效地应对不同类型电池的拆解挑战，通过数字孪生优化拆解流程，减少生态影响，提升资源利用率。

Conclusion: Bi-PAN为产品全生命周期管理提供了统一的信息架构，有效支持循环经济中的再制造与回收，具有实际应用潜力。

Abstract: In the context of the circular economy, products in their end-of-life phase
should be either remanufactured or recycled. Both of these processes are
crucial for sustainability and environmental conservation. However,
manufacturers often do not support these processes enough by not sharing
relevant data. This paper proposes use of a digital twin technology, which is
capable to help optimizing the disassembly processes to reduce ecological
impact and enhance sustainability. The proposed approach is demonstrated
through a disassembly use-case of the product digital twin of an electric
vehicle battery. By utilizing product digital twins, challenges associated with
the disassembly of electric vehicle batteries can be solved flexibly and
efficiently for various battery types. As a backbone for the product digital
twin representation, the paper uses the paradigm of product-process-resource
asset networks (PAN). Such networks enable to model relevant relationships
across products, production resources, manufacturing processes, and specific
production operations that have to be done in the manufacturing phase of a
product. This paper introduces a Bi-Flow Product-Process-Resource Asset Network
(Bi-PAN) representation, which extends the PAN paradigm to cover not only the
manufacturing, but also the remanufacturing/recycling phase.

</details>


### [223] [DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis](https://arxiv.org/abs/2510.02178)
*Jialin Gao,Donghao Zhou,Mingjian Liang,Lihao Liu,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.RO

TL;DR: 提出DisCo-Layout框架，通过解耦和协调物理与语义优化，生成更真实、连贯且可泛化的3D室内布局。


<details>
  <summary>Details</summary>
Motivation: 传统方法因固定数据集泛化能力差，现有LLM/VLM方法缺乏鲁棒且灵活的优化机制，导致布局效果不佳。

Method: 提出DisCo-Layout框架，包含语义优化工具（SRT）和物理优化工具（PRT），并通过多智能体框架实现协作优化，其中规划器制定放置规则，设计者生成初始布局，评估者进行评估。

Result: 实验表明DisCo-Layout在生成逼真、连贯和可泛化的3D室内布局方面达到SOTA性能。

Conclusion: DisCo-Layout通过解耦与协同物理和语义优化，有效提升了3D室内布局生成的质量和通用性。

Abstract: 3D indoor layout synthesis is crucial for creating virtual environments.
Traditional methods struggle with generalization due to fixed datasets. While
recent LLM and VLM-based approaches offer improved semantic richness, they
often lack robust and flexible refinement, resulting in suboptimal layouts. We
develop DisCo-Layout, a novel framework that disentangles and coordinates
physical and semantic refinement. For independent refinement, our Semantic
Refinement Tool (SRT) corrects abstract object relationships, while the
Physical Refinement Tool (PRT) resolves concrete spatial issues via a
grid-matching algorithm. For collaborative refinement, a multi-agent framework
intelligently orchestrates these tools, featuring a planner for placement
rules, a designer for initial layouts, and an evaluator for assessment.
Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating
realistic, coherent, and generalizable 3D indoor layouts. Our code will be
publicly available.

</details>


### [224] [Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0](https://arxiv.org/abs/2510.02248)
*Yan Miao,Ege Yuceel,Georgios Fainekos,Bardh Hoxha,Hideki Okamoto,Sayan Mitra*

Main category: cs.RO

TL;DR: 本文提出FalconGym 2.0仿真框架和PGR训练算法，提升视觉策略在无人机导航中的泛化性与鲁棒性，并实现高效的零样本仿真到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 现有视觉策略容易过拟合单一赛道，在赛道几何变化时性能下降，缺乏良好的泛化能力。

Method: 基于高斯点阵构建可编辑的仿真环境FalconGym 2.0，结合性能引导的PGR算法，在多样化静态与动态赛道上集中训练挑战性场景。

Result: 在固定翼无人机和四旋翼上验证，单个策略无需微调即可在3个未见赛道实现100%成功率，抗干扰能力强，并在真实四旋翼上实现98.6%的零样本迁移成功率。

Conclusion: FalconGym 2.0与PGR显著提升了视觉策略的泛化性、鲁棒性，并验证了其在真实世界中的有效性。

Abstract: Visual policy design is crucial for aerial navigation. However,
state-of-the-art visual policies often overfit to a single track and their
performance degrades when track geometry changes. We develop FalconGym 2.0, a
photorealistic simulation framework built on Gaussian Splatting (GSplat) with
an Edit API that programmatically generates diverse static and dynamic tracks
in milliseconds. Leveraging FalconGym 2.0's editability, we propose a
Performance-Guided Refinement (PGR) algorithm, which concentrates visual
policy's training on challenging tracks while iteratively improving its
performance. Across two case studies (fixed-wing UAVs and quadrotors) with
distinct dynamics and environments, we show that a single visual policy trained
with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in
generalization and robustness: it generalizes to three unseen tracks with 100%
success without per-track retraining and maintains higher success rates under
gate-pose perturbations. Finally, we demonstrate that the visual policy trained
with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a
quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30
trials spanning two three-gate tracks and a moving-gate track.

</details>


### [225] [Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking](https://arxiv.org/abs/2510.02252)
*Joao Pedro Araujo,Yanjie Ze,Pei Xu,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: 本文研究了人形机器人运动跟踪策略中由于人类与机器人形态差异（embodiment gap）带来的挑战，指出现有运动重定向方法引入的伪影（如脚部滑动、自穿透等）会显著降低策略鲁棒性。作者提出一种新的重定向方法GMR，并在不进行奖励调优的情况下系统评估不同重定向质量对策略性能的影响，结果显示GMR在开源方法中表现最优，接近闭源数据的效果。


<details>
  <summary>Details</summary>
Motivation: 由于人类与人形机器人之间的形态差异，现有的运动重定向方法常引入伪影（如脚滑、自穿透），这些伪影会影响强化学习策略的学习效果，而当前方法依赖大量奖励工程来弥补，因此需要系统评估重定向质量本身对策略性能的影响，并改进重定向方法。

Method: 提出一种新的运动重定向方法——通用运动重定向（General Motion Retargeting, GMR），并在BeyondMimic框架下训练策略，避免奖励调优干扰；对比评估了GMR、PHC、ProtoMotions三种开源重定向方法以及Unitree的闭源高质量数据集，在LAFAN1数据子集上分析其对策略跟踪性能和动作保真度的影响。

Result: 实验表明，重定向数据中的伪影显著降低策略鲁棒性，尤其影响动态或长序列动作；GMR在跟踪性能和动作保真度方面均优于现有开源方法，策略成功率和感知质量接近闭源数据基准。

Conclusion: 运动重定向的质量对人形机器人运动跟踪策略的性能有重要影响；GMR能有效减少重定向伪影，在无需复杂奖励工程的情况下提升策略鲁棒性和动作自然性，为构建更可靠的模仿学习系统提供了基础。

Abstract: Humanoid motion tracking policies are central to building teleoperation
pipelines and hierarchical controllers, yet they face a fundamental challenge:
the embodiment gap between humans and humanoid robots. Current approaches
address this gap by retargeting human motion data to humanoid embodiments and
then training reinforcement learning (RL) policies to imitate these reference
trajectories. However, artifacts introduced during retargeting, such as foot
sliding, self-penetration, and physically infeasible motion are often left in
the reference trajectories for the RL policy to correct. While prior work has
demonstrated motion tracking abilities, they often require extensive reward
engineering and domain randomization to succeed. In this paper, we
systematically evaluate how retargeting quality affects policy performance when
excessive reward tuning is suppressed. To address issues that we identify with
existing retargeting methods, we propose a new retargeting method, General
Motion Retargeting (GMR). We evaluate GMR alongside two open-source
retargeters, PHC and ProtoMotions, as well as with a high-quality closed-source
dataset from Unitree. Using BeyondMimic for policy training, we isolate
retargeting effects without reward tuning. Our experiments on a diverse subset
of the LAFAN1 dataset reveal that while most motions can be tracked, artifacts
in retargeted data significantly reduce policy robustness, particularly for
dynamic or long sequences. GMR consistently outperforms existing open-source
methods in both tracking performance and faithfulness to the source motion,
achieving perceptual fidelity and policy success rates close to the
closed-source baseline. Website:
https://jaraujo98.github.io/retargeting_matters. Code:
https://github.com/YanjieZe/GMR.

</details>


### [226] [Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning](https://arxiv.org/abs/2510.02268)
*Tianchong Jiang,Jingtian Ji,Xiangshan Tan,Jiading Fang,Anand Bhattad,Vitor Guizilini,Matthew R. Walter*

Main category: cs.RO

TL;DR: 本文研究了通过显式依赖相机外参实现视角不变的模仿学习，提出使用像素射线的Plücker嵌入来提升策略在不同视角下的泛化能力，并设计新任务验证了该方法在真实视角变化下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习策略在视角变化时性能下降，因它们依赖静态背景等视觉线索推断相机姿态，缺乏对相机外参的显式建模，导致在视角或场景变化时泛化能力差。

Method: 使用Plücker嵌入表示每个像素的射线方向，并将相机外参作为策略网络的输入，从而显式建模视角信息。在ACT、Diffusion Policy和SmolVLA等主流行为克隆策略中集成该方法。

Result: 实验表明，引入相机外参显著提升了策略在不同视角下的泛化性能；在六个新设计的配对任务（固定与随机场景）中，该方法有效避免了对背景线索的依赖，在无深度信息的纯RGB控制下仍保持鲁棒性。

Conclusion: 显式条件化相机外参是提升模仿学习视角不变性和鲁棒性的关键，能有效解耦相机姿态与场景背景的影响，为纯视觉机器人控制提供了更可靠的解决方案。

Abstract: We study view-invariant imitation learning by explicitly conditioning
policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we
show that conditioning on extrinsics significantly improves generalization
across viewpoints for standard behavior cloning policies, including ACT,
Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose. Our analysis reveals that policies without
extrinsics often infer camera pose using visual cues from static backgrounds in
fixed scenes; this shortcut collapses when workspace geometry or camera
placement shifts. Conditioning on extrinsics restores performance and yields
robust RGB-only control without depth. We release the tasks, demonstrations,
and code at https://ripl.github.io/know_your_camera/ .

</details>


### [227] [ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation](https://arxiv.org/abs/2510.02298)
*Wenye Yu,Jun Lv,Zixi Ying,Yang Jin,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: 本文提出了一种名为ARMADA的多机器人部署与自适应系统，结合人类在环的共享控制和自主在线故障检测方法FLOAT，显著减少了对人类监督的依赖，提高了真实世界任务中策略学习的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的模仿学习方法在缺乏足够领域内数据时表现不佳，且人类收集的演示数据通常质量不一、包含冗余信息；而传统人类在环系统需要全程人工监控，成本高、效率低。

Method: 提出ARMADA系统，采用名为FLOAT的自主在线故障检测方法，仅在必要时请求人工干预，支持并行策略 rollout，并通过闭环反馈持续优化策略。

Result: 在四个真实世界任务中验证，FLOAT的故障检测准确率平均达95%，超过现有最优方法20%以上；相比以往方法，ARMADA在多轮策略部署与后训练中实现了4倍以上的成功率提升和2倍以上的人工干预率下降。

Conclusion: ARMADA通过高效的在线故障检测和按需人工干预机制，显著提升了多机器人系统的部署效率和自适应能力，为大规模真实场景中的模仿学习提供了可扩展的解决方案。

Abstract: Imitation learning has shown promise in learning from large-scale real-world
datasets. However, pretrained policies usually perform poorly without
sufficient in-domain data. Besides, human-collected demonstrations entail
substantial labour and tend to encompass mixed-quality data and redundant
information. As a workaround, human-in-the-loop systems gather domain-specific
data for policy post-training, and exploit closed-loop policy feedback to offer
informative guidance, but usually require full-time human surveillance during
policy rollout. In this work, we devise ARMADA, a multi-robot deployment and
adaptation system with human-in-the-loop shared control, featuring an
autonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA
enables paralleled policy rollout and requests human intervention only when
necessary, significantly reducing reliance on human supervision. Hence, ARMADA
enables efficient acquisition of in-domain data, and leads to more scalable
deployment and faster adaptation to new scenarios. We evaluate the performance
of ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on
average, surpassing prior state-of-the-art failure detection approaches by over
20%. Besides, ARMADA manifests more than 4$\times$ increase in success rate and
greater than 2$\times$ reduction in human intervention rate over multiple
rounds of policy rollout and post-training, compared to previous
human-in-the-loop learning methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [228] [Accelerating Long-Term Molecular Dynamics with Physics-Informed Time-Series Forecasting](https://arxiv.org/abs/2510.01206)
*Hung Le,Sherif Abbas,Minh Hoang Nguyen,Van Dai Do,Huu Hiep Nguyen,Dung Nguyen*

Main category: cs.LG

TL;DR: 提出一种将分子动力学模拟视为时间序列预测问题的新方法，通过引入基于DFT参数化的Morse势函数的物理信息损失和推理机制，显著提高原子轨迹预测的准确性和稳定性，实现数千步模拟在几分钟内完成。


<details>
  <summary>Details</summary>
Motivation: 传统密度泛函理论（DFT）方法计算成本高，限制了长期分子动力学模拟的可行性。

Method: 将分子动力学模拟 formulation 为时间序列预测问题，使用先进的预测模型通过位移而非绝对位置来预测原子轨迹，并结合基于DFT参数化的成对Morse势函数的物理信息损失和推理机制。

Result: 在多种材料上，该方法在模拟精度上 consistently 超过标准基线，能够在几分钟内稳定地建模数千个MD步骤。

Conclusion: 结合物理知识可以增强原子轨迹预测的可靠性和精确性，为昂贵的DFT模拟提供可扩展的替代方案。

Abstract: Efficient molecular dynamics (MD) simulation is vital for understanding
atomic-scale processes in materials science and biophysics. Traditional density
functional theory (DFT) methods are computationally expensive, which limits the
feasibility of long-term simulations. We propose a novel approach that
formulates MD simulation as a time-series forecasting problem, enabling
advanced forecasting models to predict atomic trajectories via displacements
rather than absolute positions. We incorporate a physics-informed loss and
inference mechanism based on DFT-parametrised pair-wise Morse potential
functions that penalize unphysical atomic proximity to enforce physical
plausibility. Our method consistently surpasses standard baselines in
simulation accuracy across diverse materials. The results highlight the
importance of incorporating physics knowledge to enhance the reliability and
precision of atomic trajectory forecasting. Remarkably, it enables stable
modeling of thousands of MD steps in minutes, offering a scalable alternative
to costly DFT simulations.

</details>


### [229] [Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs](https://arxiv.org/abs/2510.01218)
*Sergey Troshin,Wafaa Mohammed,Yan Meng,Christof Monz,Antske Fokkens,Vlad Niculae*

Main category: cs.LG

TL;DR: 本文提出了一种选择性采样方法，通过动态切换贪婪和高温采样来提升语言模型在数学推理等高精度任务中的多样性与准确性的权衡。


<details>
  <summary>Details</summary>
Motivation: 高温采样虽能增加生成输出的多样性，但在需要高精度的任务（如数学推理）中会因在敏感解码位置采样错误续接而降低准确性。

Method: 提出选择性采样方法，基于一个采样风险度量动态决定在当前token位置使用贪婪还是高温采样，并训练一个轻量级分类器预测该风险。

Result: 实验表明，在数学推理任务中，选择性采样能够在保持准确性的同时提升多样性，改善质量-多样性权衡，即使在高温设置下也有效。

Conclusion: 选择性采样能有效平衡语言模型在高精度任务中的生成多样性与推理准确性，且引入的延迟开销极小。

Abstract: Diversity is an essential metric for evaluating the creativity of outputs
generated by language models. Temperature-based sampling is a common strategy
to increase diversity. However, for tasks that require high precision, e.g.,
mathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$
or top-$p$, degrades reasoning quality. We demonstrate that the loss of
accuracy is caused by sampling incorrect continuations in sensitive decoding
positions. To address this, in this paper, we propose \textbf{selective
sampling}, a method that dynamically switches between greedy and
high-temperature sampling based on a sampling risk metric. This risk metric
estimates the likelihood of output errors when applying high-temperature
sampling on the current token position. To predict sampling risk, we train a
lightweight classifier on a small subset of verifiable problems. The trained
classifier can be integrated with the base language model with minimal latency
overhead. Experiments on mathematical reasoning tasks demonstrate that
selective sampling enhances the quality-diversity trade-off, even in
high-temperature settings.

</details>


### [230] [Automated Extraction of Material Properties using LLM-based AI Agents](https://arxiv.org/abs/2510.01235)
*Subham Ghosh,Abhishek Tewari*

Main category: cs.LG

TL;DR: 本研究利用基于大语言模型（LLM）的自主工作流，从约10,000篇科学文献中提取热电与结构性能数据，构建了迄今为止最大的LLM整理热电材料数据库，包含27,822条温度分辨的性能记录，并发布交互式网页查询工具，推动数据驱动的材料发现。


<details>
  <summary>Details</summary>
Motivation: 现有材料数据库规模小、依赖人工整理或偏向理论计算结果，实验文献未被充分利用，缺乏将性能指标与结构信息结合的大规模机器可读数据集，限制了材料的快速发现。

Method: 提出一种基于LLM的多智能体工作流，结合动态令牌分配、零样本多代理信息提取和条件表格解析，从全文科学论文中自动提取热电性能和结构属性，并在50篇人工标注论文上评估不同LLM（如GPT-4.1和GPT-4.1 Mini）的提取准确率。

Result: GPT-4.1在热电属性提取上F1得分为0.91，结构字段为0.82；GPT-4.1 Mini性能接近（F1分别为0.89和0.81）但成本更低；最终构建包含27,822条标准化单位的温度相关性能数据集，涵盖ZT值、塞贝克系数、导电性等以及晶体类、空间群、掺杂策略等结构信息，数据分析重现了合金优于氧化物、p型掺杂优势等已知趋势，并揭示新的构效关系。

Conclusion: 该研究建立了目前最大规模的LLM整理热电数据集，提供可复现且成本可控的信息提取流程，发布了支持语义过滤和数值查询的交互式网页工具，为热电及其他材料领域的数据驱动发现奠定了基础。

Abstract: The rapid discovery of materials is constrained by the lack of large,
machine-readable datasets that couple performance metrics with structural
context. Existing databases are either small, manually curated, or biased
toward first principles results, leaving experimental literature
underexploited. We present an agentic, large language model (LLM)-driven
workflow that autonomously extracts thermoelectric and structural-properties
from about 10,000 full-text scientific articles. The pipeline integrates
dynamic token allocation, zeroshot multi-agent extraction, and conditional
table parsing to balance accuracy against computational cost. Benchmarking on
50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91
for thermoelectric properties and 0.82 for structural fields), while GPT-4.1
Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction
of the cost, enabling practical large scale deployment. Applying this workflow,
we curated 27,822 temperature resolved property records with normalized units,
spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity,
power factor, and thermal conductivity, together with structural attributes
such as crystal class, space group, and doping strategy. Dataset analysis
reproduces known thermoelectric trends, such as the superior performance of
alloys over oxides and the advantage of p-type doping, while also surfacing
broader structure-property correlations. To facilitate community access, we
release an interactive web explorer with semantic filters, numeric queries, and
CSV export. This study delivers the largest LLM-curated thermoelectric dataset
to date, provides a reproducible and cost-profiled extraction pipeline, and
establishes a foundation for scalable, data-driven materials discovery beyond
thermoelectrics.

</details>


### [231] [RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models](https://arxiv.org/abs/2510.01240)
*Zukang Xu,Xing Hu,Qiang Wu,Dawei Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为RSAVQ的新颖向量量化框架，用于提升大语言模型的极低比特量化性能，通过几何驱动的方法有效缓解了方向误差和比特分配问题。


<details>
  <summary>Details</summary>
Motivation: 现有的低比特量化方法在应用于大语言模型时面临方向误差不受控和比特分配次优的问题，限制了其在资源受限设备上的部署效果。

Method: RSAVQ引入了两种基于几何的创新：一是利用Fisher信息矩阵诱导的黎曼度量进行误差方向敏感性引导（EDSG），将量化误差投影到参数空间的低敏感方向；二是通过FIM曲率分析构建通道级敏感性度量，实现权重通道敏感性引导（WCSG）以动态分配比特资源。

Result: 实验表明，RSAVQ在LLaMA-3 8B的2比特量化中优于现有方法，在困惑度上比VPTQ和QuIP#低0.4，在零样本准确率上高1.5。

Conclusion: RSAVQ为大语言模型的高效量化提供了一个实用解决方案，并建立了信息几何与神经网络量化之间的理论桥梁，推动了高效深度学习的发展。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across
a wide range of natural language processing tasks. However, their exponentially
increasing parameters pose significant challenges for deployment on
resource-constrained devices. Vector Quantization (VQ) shows great promise for
low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key
challenges: unconstrained direction error and suboptimal bit allocation. In
this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit
quantization for LLMs. RSAVQ introduces two geometry-driven innovations that
effectively mitigate above limitations: (1) Error Direction Sensitivity
Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced
Riemannian metric to project quantization errors onto low-sensitivity
directions in the parameter space. Specifically, this projection is performed
along the negative natural gradient direction, which effectively suppresses
error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which
constructs a channel-wise sensitivity metric via FIM curvature analysis to
dynamically guide bit resource allocation. The approach facilitates a globally
optimal quantization solution within prescribed bit constraints. Experiments
demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in
2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by
0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a
practical solution for constrained environments and a theoretical bridge
between information geometry and the quantization of neural networks, advancing
efficient deep learning.

</details>


### [232] [Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks](https://arxiv.org/abs/2510.01261)
*Vedant Palit*

Main category: cs.LG

TL;DR: 提出一种基于信任感知的深度Q网络（DQN）来防御联邦学习中的投毒和后门攻击，通过多信号证据更新客户端信任度，并在CIFAR-10上验证了其在鲁棒性与准确性之间的优越权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在部分可观测环境下易受投毒和后门攻击，现有方法难以有效应对动态、长期的信任评估问题。

Method: 将防御建模为部分可观测序贯决策问题，设计信任感知DQN，结合多信号证据进行客户端信任度更新，优化长视野的鲁棒性-准确性目标。

Result: 在CIFAR-10上实验表明：客户重叠增加可提升准确率并降低攻击成功率（ASR）；信号减少时DQN仍保持较高准确率，但ASR上升、ROC-AUC下降；相比随机、线性Q和策略梯度控制器，DQN实现最优权衡。

Conclusion: 信任感知DQN能有效缓解部分可观测下的弱信号问题，在联邦学习防御中实现了更优的鲁棒性与准确性平衡。

Abstract: Federated learning is vulnerable to poisoning and backdoor attacks under
partial observability. We formulate defence as a partially observable
sequential decision problem and introduce a trust-aware Deep Q-Network that
integrates multi-signal evidence into client trust updates while optimizing a
long-horizon robustness--accuracy objective. On CIFAR-10, we (i) establish a
baseline showing steadily improving accuracy, (ii) show through a Dirichlet
sweep that increased client overlap consistently improves accuracy and reduces
ASR with stable detection, and (iii) demonstrate in a signal-budget study that
accuracy remains steady while ASR increases and ROC-AUC declines as
observability is reduced, which highlights that sequential belief updates
mitigate weaker signals. Finally, a comparison with random, linear-Q, and
policy gradient controllers confirms that DQN achieves the best
robustness--accuracy trade-off.

</details>


### [233] [RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction](https://arxiv.org/abs/2510.01262)
*Koyena Chowdhury,Paramita Koley,Abhijnan Chakraborty,Saptarshi Ghosh*

Main category: cs.LG

TL;DR: 提出了一种铁路中心的时空图卷积网络（RSTGCN）用于预测车站平均到站延误，并发布了涵盖印度铁路网4735个车站的大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 准确预测列车延误对铁路高效运营至关重要，现有研究多关注单个列车延误预测，而本文聚焦于支持高层交通管理的车站级平均延误预测。

Method: 提出RSTGCN模型，引入列车频率感知的空间注意力机制等创新结构和特征融合方法，以预测特定时间段内所有进站列车的平均到达延误。

Result: 在涵盖印度铁路网17个区域4735个车站的大规模数据集上进行实验，相比多种先进基线模型，在标准指标上表现出持续且显著的性能提升。

Conclusion: 所提方法有效提升了大规模铁路网络中平均延误预测的准确性，发布的开放数据集有助于推动该领域的进一步研究。

Abstract: Accurate prediction of train delays is critical for efficient railway
operations, enabling better scheduling and dispatching decisions. While earlier
approaches have largely focused on forecasting the exact delays of individual
trains, recent studies have begun exploring station-level delay prediction to
support higher-level traffic management. In this paper, we propose the
Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed
to forecast average arrival delays of all the incoming trains at railway
stations for a particular time period. Our approach incorporates several
architectural innovations and novel feature integrations, including train
frequency-aware spatial attention, which significantly enhances predictive
performance. To support this effort, we curate and release a comprehensive
dataset for the entire Indian Railway Network (IRN), spanning 4,735 stations
across 17 zones - the largest and most diverse railway network studied to date.
We conduct extensive experiments using multiple state-of-the-art baselines,
demonstrating consistent improvements across standard metrics. Our work not
only advances the modeling of average delay prediction in large-scale rail
networks but also provides an open dataset to encourage further research in
this critical domain.

</details>


### [234] [Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency](https://arxiv.org/abs/2510.01263)
*Yaron Meirovitch,Fuming Yang,Jeff Lichtman,Nir Shavit*

Main category: cs.LG

TL;DR: 提出了一种名为Budgeted Broadcast (BB)的剪枝方法，通过局部流量预算和选择性-受众平衡来优化神经网络的编码熵和稀疏性，在多种模型和任务上提升了准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法多基于参数对损失的影响进行裁剪，缺乏对神经元活动与连接结构之间全局平衡的考虑，导致表示多样性和效率不足。

Method: 引入每个神经元的局部流量预算（活动率与输出度的乘积），通过约束熵分析推导出选择性-受众平衡公式，并设计本地控制器剪枝输入或输出连接以维持该平衡。

Result: BB方法在ASR的Transformer、人脸识别的ResNet和电子显微镜图像分割的3D U-Net上均提高了编码熵、去相关性和相同稀疏度下的准确性，部分超越了密集模型，在F1和PR-AUC指标上达到SOTA。

Conclusion: BB提供了一种简单可集成的剪枝机制，通过平衡神经元的选择性与广播能力，促进了更高效且多样化的特征表示学习。

Abstract: Most pruning methods remove parameters ranked by impact on loss (e.g.,
magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each
unit a local traffic budget (the product of its long-term on-rate $a_i$ and
fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding
entropy under a global traffic budget yields a selectivity-audience balance,
$\log\frac{1-a_i}{a_i}=\beta k_i$. BB enforces this balance with simple local
actuators that prune either fan-in (to lower activity) or fan-out (to reduce
broadcast). In practice, BB increases coding entropy and decorrelation and
improves accuracy at matched sparsity across Transformers for ASR, ResNets for
face identification, and 3D U-Nets for synapse prediction, sometimes exceeding
dense baselines. On electron microscopy images, it attains state-of-the-art F1
and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests
a path toward learning more diverse and efficient representations.

</details>


### [235] [A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab](https://arxiv.org/abs/2510.01264)
*Isaac Peterson,Christopher Allred,Jacob Morrey,Mario Harper*

Main category: cs.LG

TL;DR: 本文扩展了IsaacLab框架，支持在高保真物理仿真中进行对抗性策略的可扩展训练，提出了一套具有异构智能体和不对称目标的对抗性多智能体强化学习环境，并集成了竞争性HAPPO算法，实验证明该框架能高效训练形态多样化的多智能体对抗策略。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习研究主要集中在协作场景，但在追逃、安全和竞争性操作等现实应用中，对抗性交互同样重要，因此需要支持对抗性策略训练的高保真仿真框架。

Method: 扩展IsaacLab框架，构建包含异构智能体和不对称目标的对抗性MARL环境，集成基于PPO的竞争性HAPPO算法，实现对抗动态下的高效训练与评估。

Result: 在多个基准场景中实验表明，该框架能够有效建模并训练出鲁棒的对抗策略，同时保持高吞吐量和高仿真真实性。

Conclusion: 所提出的框架为异构多智能体在复杂动态环境中的对抗性策略训练提供了高效、可扩展的解决方案，推动了真实世界机器人系统的发展。

Abstract: Multi-Agent Reinforcement Learning (MARL) is central to robotic systems
cooperating in dynamic environments. While prior work has focused on these
collaborative settings, adversarial interactions are equally critical for
real-world applications such as pursuit-evasion, security, and competitive
manipulation. In this work, we extend the IsaacLab framework to support
scalable training of adversarial policies in high-fidelity physics simulations.
We introduce a suite of adversarial MARL environments featuring heterogeneous
agents with asymmetric goals and capabilities. Our platform integrates a
competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal
Policy Optimization (HAPPO), enabling efficient training and evaluation under
adversarial dynamics. Experiments across several benchmark scenarios
demonstrate the framework's ability to model and train robust policies for
morphologically diverse multi-agent competition while maintaining high
throughput and simulation realism. Code and benchmarks are available at:
https://github.com/DIRECTLab/IsaacLab-HARL .

</details>


### [236] [RLP: Reinforcement as a Pretraining Objective](https://arxiv.org/abs/2510.01265)
*Ali Hatamizadeh,Syeda Nahida Akter,Shrimai Prabhumoye,Jan Kautz,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Yejin Choi*

Main category: cs.LG

TL;DR: 本文提出了一种信息驱动的强化预训练目标RLP，将强化学习的核心——探索——引入预训练的最后阶段，通过链式思维作为探索动作，并基于其对未来token预测的信息增益计算奖励，从而在预训练阶段就鼓励模型独立思考。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型训练范式在预训练阶段主要依赖于next-token预测，强化学习仅用于后期微调，可能并非最优方案。作者希望探索更有效的训练方式，在早期阶段即培养模型的推理能力。

Method: 提出RLP（Reinforcement Learning Pretraining）方法，将链式思维视为探索性动作，奖励信号定义为：在给定上下文和采样推理链条件下，相比仅使用上下文时对下一个token预测的对数似然提升（即信息增益），实现无需验证器的密集奖励信号，可在常规文本上进行端到端预训练。

Result: 在Qwen3-1.7B-Base上应用RLP使八个数学与科学基准的平均性能提升19%；在Nemotron-Nano-12B-v2上，整体平均得分从42.81%提升至61.32%，科学推理平均提升23%，且效果在不同架构和规模下具可扩展性。

Conclusion: RLP成功将强化学习的思想融入预训练阶段，弥合了传统next-token预测与链式思维推理之间的差距，显著提升了模型的推理能力，且具有广泛适用性和可扩展性。

Abstract: The dominant paradigm for training large reasoning models starts with
pre-training using next-token prediction loss on vast amounts of data.
Reinforcement learning, while powerful in scaling reasoning, is introduced only
as the very last phase of post-training, preceded by supervised fine-tuning.
While dominant, is this an optimal way of training? In this paper, we present
RLP, an information-driven reinforcement pretraining objective, that brings the
core spirit of reinforcement learning -- exploration -- to the last phase of
pretraining. The key idea is to treat chain-of-thought as an exploratory
action, with rewards computed based on the information gain it provides for
predicting future tokens. This training objective essentially encourages the
model to think for itself before predicting what comes next, thus teaching an
independent thinking behavior earlier in the pretraining. More concretely, the
reward signal measures the increase in log-likelihood of the next token when
conditioning on both context and a sampled reasoning chain, compared to
conditioning on context alone. This approach yields a verifier-free dense
reward signal, allowing for efficient training for the full document stream
during pretraining. Specifically, RLP reframes reinforcement learning for
reasoning as a pretraining objective on ordinary text, bridging the gap between
next-token prediction and the emergence of useful chain-of-thought reasoning.
Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an
eight-benchmark math-and-science suite by 19%. With identical post-training,
the gains compound, with the largest improvements on reasoning-heavy tasks such
as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2
increases the overall average from 42.81% to 61.32% and raises the average on
scientific reasoning by 23%, demonstrating scalability across architectures and
model sizes.

</details>


### [237] [Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance](https://arxiv.org/abs/2510.01269)
*Rohan Vitthal Thorat,Juhi Singh,Rajdip Nayek*

Main category: cs.LG

TL;DR: 提出一种结合LQR和强化学习（RL）的混合控制框架，用于结构振动控制，无需精确模型且提高训练安全性。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的控制方法依赖精确系统模型，需繁琐的系统辨识；而纯模型无关的强化学习在训练初期随机控制可能损害结构，因此需要解决训练安全问题。

Method: 设计一个混合控制框架，将基于错误模型生成的LQR控制器与模型无关的RL控制器结合，在真实物理系统上实现安全的在线训练。

Result: 实验表明，即使LQR基于完全错误的模型，也能优于无控制情况；所提混合框架在无需精确建模的前提下显著降低RL探索风险。

Conclusion: 该研究首次有效解决了RL在振动控制中训练阶段的安全性问题，实现了真正意义上的模型无关且安全的控制策略。

Abstract: Structural vibrations induced by external excitations pose significant risks,
including safety hazards for occupants, structural damage, and increased
maintenance costs. While conventional model-based control strategies, such as
Linear Quadratic Regulator (LQR), effectively mitigate vibrations, their
reliance on accurate system models necessitates tedious system identification.
This tedious system identification process can be avoided by using a model-free
Reinforcement learning (RL) method. RL controllers derive their policies solely
from observed structural behaviour, eliminating the requirement for an explicit
structural model. For an RL controller to be truly model-free, its training
must occur on the actual physical system rather than in simulation. However,
during this training phase, the RL controller lacks prior knowledge and it
exerts control force on the structure randomly, which can potentially harm the
structure. To mitigate this risk, we propose guiding the RL controller using a
Linear Quadratic Regulator (LQR) controller. While LQR control typically relies
on an accurate structural model for optimal performance, our observations
indicate that even an LQR controller based on an entirely incorrect model
outperforms the uncontrolled scenario. Motivated by this finding, we introduce
a hybrid control framework that integrates both LQR and RL controllers. In this
approach, the LQR policy is derived from a randomly selected model and its
parameters. As this LQR policy does not require knowledge of the true or an
approximate structural model the overall framework remains model-free. This
hybrid approach eliminates dependency on explicit system models while
minimizing exploration risks inherent in naive RL implementations. As per our
knowledge, this is the first study to address the critical training safety
challenge of RL-based vibration control and provide a validated solution.

</details>


### [238] [Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations](https://arxiv.org/abs/2510.01271)
*Arend Hintze,Asadullah Najam,Jory Schossau*

Main category: cs.LG

TL;DR: 提出一种基于信息论的方法来识别和分析RNN中的信息传递节点（信息中继），揭示不同架构中信息流动模式，并通过节点敲除实验验证其功能重要性。


<details>
  <summary>Details</summary>
Motivation: 理解RNN内部动态机制对于提升其可解释性和设计至关重要。

Method: 通过量化输入和输出向量之间的互信息，识别RNN中的信息中继节点，并在合成与真实时间序列分类任务中应用该方法。

Result: 发现了不同RNN架构中显著的信息中继模式差异，并通过节点敲除实验证明了这些节点的功能重要性。

Conclusion: 该方法增强了对RNN工作机制的理解，为设计更鲁棒、可解释的神经网络提供了有力工具。

Abstract: Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is
crucial for advancing their interpretability and improving their design. This
study introduces an innovative information-theoretic method to identify and
analyze information-transfer nodes within RNNs, which we refer to as
\textit{information relays}. By quantifying the mutual information between
input and output vectors across nodes, our approach pinpoints critical pathways
through which information flows during network operations. We apply this
methodology to both synthetic and real-world time series classification tasks,
employing various RNN architectures, including Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns
of information relay across different architectures, offering insights into how
information is processed and maintained over time. Additionally, we conduct
node knockout experiments to assess the functional importance of identified
nodes, significantly contributing to explainable artificial intelligence by
elucidating how specific nodes influence overall network behavior. This study
not only enhances our understanding of the complex mechanisms driving RNNs but
also provides a valuable tool for designing more robust and interpretable
neural networks.

</details>


### [239] [Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning](https://arxiv.org/abs/2510.01278)
*Hengwei Zhao,Zhengzhong Tu,Zhuo Zheng,Wei Wang,Junjue Wang,Rusty Feagin,Wenzhe Jiao*

Main category: cs.LG

TL;DR: 本文提出了一种新的非对比正-无标签（PU）学习框架NcPU，无需辅助信息即可提升复杂数据集上的分类性能，通过NoiSNCL损失和PLD机制协同优化表示学习与负类推断。


<details>
  <summary>Details</summary>
Motivation: 现有PU学习方法在复杂数据集上表现不佳，尤其缺乏可靠监督信号时性能显著低于监督学习，主要瓶颈在于难以学习判别性特征。

Method: 提出NcPU框架，结合抗噪对的非对比监督损失NoiSNCL以增强同类样本表示一致性，并设计基于遗憾的伪标签去模糊化（PLD）策略提供保守负类监督，二者在EM框架下迭代优化。

Result: 实验表明：(1) NoiSNCL使简单PU方法达到竞争性性能；(2) NcPU在多个数据集上显著优于现有方法，包括灾后建筑损毁映射等实际应用场景，在CIFAR-100上缩小了14.26%的性能差距。

Conclusion: NcPU通过联合优化鲁棒表示学习与动态负类推断，有效缓解了PU学习中的不可靠监督问题，展现出在现实复杂任务中的应用潜力。

Abstract: Positive-Unlabeled (PU) learning aims to train a binary classifier (positive
vs. negative) where only limited positive data and abundant unlabeled data are
available. While widely applicable, state-of-the-art PU learning methods
substantially underperform their supervised counterparts on complex datasets,
especially without auxiliary negatives or pre-estimated parameters (e.g., a
14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the
challenge of learning discriminative representations under unreliable
supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU
learning framework that requires no auxiliary information. NcPU combines a
noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns
intra-class representations despite unreliable supervision, with a phantom
label disambiguation (PLD) scheme that supplies conservative negative
supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can
iteratively benefit each other from the perspective of the
Expectation-Maximization framework. Empirically, extensive experiments
demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive
performance; and (2) NcPU achieves substantial improvements over
state-of-the-art PU methods across diverse datasets, including challenging
datasets on post-disaster building damage mapping, highlighting its promise for
real-world applications. Code: Code will be open-sourced after review.

</details>


### [240] [Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours](https://arxiv.org/abs/2510.01288)
*Rui Melo,Rui Abreu,Corina S. Pasareanu*

Main category: cs.LG

TL;DR: 提出一种受微眼跳启发的轻量级位置编码扰动方法，用于探测大语言模型中的潜在错误行为，无需微调或任务特定监督，能有效检测多种场景下的模型失效。


<details>
  <summary>Details</summary>
Motivation: 受到人类视觉系统中微眼跳现象的启发，探索大语言模型内部是否存在类似的可探测信号以揭示其潜在失常行为。

Method: 通过引入轻量级的位置编码扰动作为探针，在不进行微调或依赖任务特定监督的情况下，探测大语言模型中的异常响应。

Result: 在多个先进的大语言模型上验证了该方法的有效性，能够高效检测事实性错误、安全性问题、毒性内容和后门攻击等多类问题。

Conclusion: 预训练的大语言模型已隐含自我监控的能力，微眼跳式扰动为发现和缓解模型不良行为提供了新途径。

Abstract: We draw inspiration from microsaccades, tiny involuntary eye movements that
reveal hidden dynamics of human perception, to propose an analogous probing
method for large language models (LLMs). Just as microsaccades expose subtle
but informative shifts in vision, we show that lightweight position encoding
perturbations elicit latent signals that indicate model misbehaviour. Our
method requires no fine-tuning or task-specific supervision, yet detects
failures across diverse settings including factuality, safety, toxicity, and
backdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate
that these perturbation-based probes surface misbehaviours while remaining
computationally efficient. These findings suggest that pretrained LLMs already
encode the internal evidence needed to flag their own failures, and that
microsaccade-inspired interventions provide a pathway for detecting and
mitigating undesirable behaviours.

</details>


### [241] [ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models](https://arxiv.org/abs/2510.01290)
*Akshat Ramachandran,Marina Neseem,Charbel Sakr,Rangharajan Venkatesan,Brucek Khailany,Tushar Krishna*

Main category: cs.LG

TL;DR: ThinKV是一种针对大推理模型中KV缓存快速增长问题的压缩框架，通过注意力稀疏性识别不同重要性的思维类型，采用量化与逐出结合的策略，并设计扩展PagedAttention的内核以高效重用内存，实现在极小缓存下接近无损精度和显著提升推理吞吐。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在长输出上下文生成中产生大量KV缓存，迅速耗尽GPU内存，限制了推理效率和可扩展性。

Method: 提出ThinKV，基于注意力稀疏性区分思维类型的重要性，采用混合量化与逐出策略动态管理KV缓存，并设计支持内存槽重用的新型内核以避免碎片整理开销。

Result: 在多个数学与编程基准及主流模型上的实验表明，ThinKV仅使用不到5%的原始KV缓存即可实现接近无损精度，并相较现有最优方法提升最高达5.8倍的推理吞吐量。

Conclusion: ThinKV有效解决了长链推理中KV缓存膨胀的问题，兼顾高效率、低内存与高精度，为大规模推理系统提供了可行的缓存管理方案。

Abstract: The long-output context generation of large reasoning models enables extended
chain of thought (CoT) but also drives rapid growth of the key-value (KV)
cache, quickly overwhelming GPU memory. To address this challenge, we propose
ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on
the observation that attention sparsity reveals distinct thought types with
varying importance within the CoT. It applies a hybrid quantization-eviction
strategy, assigning token precision by thought importance and progressively
evicting tokens from less critical thoughts as reasoning trajectories evolve.
Furthermore, to implement ThinKV, we design a kernel that extends
PagedAttention to enable efficient reuse of evicted tokens' memory slots,
eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,
GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show
that ThinKV achieves near-lossless accuracy with less than 5% of the original
KV cache, while improving performance with up to 5.8x higher inference
throughput over state-of-the-art baselines.

</details>


### [242] [Network-Level Vehicle Delay Estimation at Heterogeneous Signalized Intersections](https://arxiv.org/abs/2510.01292)
*Xiaobo Ma,Hyunsoo Noh,James Tokishi,Ryan Hatch*

Main category: cs.LG

TL;DR: 提出一种基于领域自适应的车辆延误估计框架，通过梯度提升与平衡加权（GBBW）模型提升跨交叉口的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在训练和测试数据分布不一致时表现不佳，难以适应不同交叉口的交通差异。

Method: 采用领域自适应方法，将数据分为源域和目标域，利用小部分标注的目标域数据进行模型微调；提出GBBW模型，基于与目标域的相似性对源数据进行重加权。

Result: 在亚利桑那州57个异构交叉口的数据上验证，GBBW优于8种主流ML回归模型和7种实例型领域自适应方法，具有更高的准确性和鲁棒性。

Conclusion: 该框架提升了模型在不同交通环境下的迁移能力，支持更可靠的信号优化与交通管理，推动机器学习在实际交通系统中的应用。

Abstract: Accurate vehicle delay estimation is essential for evaluating the performance
of signalized intersections and informing traffic management strategies. Delay
reflects congestion levels and affects travel time reliability, fuel use, and
emissions. Machine learning (ML) offers a scalable, cost-effective alternative;
However, conventional models typically assume that training and testing data
follow the same distribution, an assumption that is rarely satisfied in
real-world applications. Variations in road geometry, signal timing, and driver
behavior across intersections often lead to poor generalization and reduced
model accuracy. To address this issue, this study introduces a domain
adaptation (DA) framework for estimating vehicle delays across diverse
intersections. The framework separates data into source and target domains,
extracts key traffic features, and fine-tunes the model using a small, labeled
subset from the target domain. A novel DA model, Gradient Boosting with
Balanced Weighting (GBBW), reweights source data based on similarity to the
target domain, improving adaptability. The framework is tested using data from
57 heterogeneous intersections in Pima County, Arizona. Performance is
evaluated against eight state-of-the-art ML regression models and seven
instance-based DA methods. Results demonstrate that the GBBW framework provides
more accurate and robust delay estimates. This approach supports more reliable
traffic signal optimization, congestion management, and performance-based
planning. By enhancing model transferability, the framework facilitates broader
deployment of machine learning techniques in real-world transportation systems.

</details>


### [243] [From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review](https://arxiv.org/abs/2510.01296)
*Emma McMillian,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: cs.LG

TL;DR: 本文综述了基于深度学习的二维磁共振图像三维重建方法，涵盖点云、网格、形状感知和体素模型四类技术，分析其现状、局限性及在心脏、神经和肺部成像中的应用，并讨论数据集、计算需求和评估指标，展望多模态融合等未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在医学影像中的发展，准确高效的3D MRI重建对疾病诊断、治疗规划和计算建模日益重要，但不同方法各有局限，亟需系统性综述以指导研究方向。

Method: 本文对3D MRI重建的四类主要深度学习方法（点云、网格、形状感知、体素模型）进行分类综述，分析各类方法的技术原理、优缺点及在不同解剖结构中的应用，并评估数据资源、计算成本和临床适用性。

Result: 系统梳理了当前3D MRI重建的技术进展，指出了现有模型在病理解剖建模、数据依赖性和计算效率方面的挑战，并总结了公开数据集和评价标准，提出了多模态与跨模态融合等未来研究方向。

Conclusion: 该综述为研究人员提供了3D MRI重建领域的结构化视图，有助于推动深度学习方法向更鲁棒、可泛化且具临床价值的方向发展。

Abstract: Deep learning-based 3-dimensional (3D) shape reconstruction from
2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly
important in medical disease diagnosis, treatment planning, and computational
modeling. This review surveys the methodological landscape of 3D MRI
reconstruction, focusing on 4 primary approaches: point cloud, mesh-based,
shape-aware, and volumetric models. For each category, we analyze the current
state-of-the-art techniques, their methodological foundation, limitations, and
applications across anatomical structures. We provide an extensive overview
ranging from cardiac to neurological to lung imaging. We also focus on the
clinical applicability of models to diseased anatomy, and the influence of
their training and testing data. We examine publicly available datasets,
computational demands, and evaluation metrics. Finally, we highlight the
emerging research directions including multimodal integration and
cross-modality frameworks. This review aims to provide researchers with a
structured overview of current 3D reconstruction methodologies to identify
opportunities for advancing deep learning towards more robust, generalizable,
and clinically impactful solutions.

</details>


### [244] [Low Rank Gradients and Where to Find Them](https://arxiv.org/abs/2510.01303)
*Rishi Sonthalia,Michael Murray,Guido Montúfar*

Main category: cs.LG

TL;DR: 本文研究了在放宽数据和参数各向同性假设下，两层神经网络训练损失梯度中的低秩结构，揭示梯度主要由两个秩为一的成分主导，并分析了数据特性、缩放机制和激活函数对这些成分的影响。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中，训练数据和参数通常不具备理想的各向同性，因此需要在更一般的设定下研究神经网络梯度的低秩结构。

Method: 采用尖峰数据模型，允许数据主部各向异性和病态条件，同时考虑均值场和神经正切核两种缩放机制，分析梯度结构及其影响因素。

Result: 发现输入权重梯度近似低秩，主要由与数据残差对齐和与输入数据中尖峰对齐的两个秩一成分构成，并量化了不同因素对这两部分的调控作用。

Conclusion: 梯度低秩性普遍存在，且可通过标准正则化方法选择性调节，理论结果在合成与真实数据上均得到验证。

Abstract: This paper investigates low-rank structure in the gradients of the training
loss for two-layer neural networks while relaxing the usual isotropy
assumptions on the training data and parameters. We consider a spiked data
model in which the bulk can be anisotropic and ill-conditioned, we do not
require independent data and weight matrices and we also analyze both the
mean-field and neural-tangent-kernel scalings. We show that the gradient with
respect to the input weights is approximately low rank and is dominated by two
rank-one terms: one aligned with the bulk data-residue , and another aligned
with the rank one spike in the input data. We characterize how properties of
the training data, the scaling regime and the activation function govern the
balance between these two components. Additionally, we also demonstrate that
standard regularizers, such as weight decay, input noise and Jacobian
penalties, also selectively modulate these components. Experiments on synthetic
and real data corroborate our theoretical predictions.

</details>


### [245] [Quantum-inspired Benchmark for Estimating Intrinsic Dimension](https://arxiv.org/abs/2510.01335)
*Aritra Das,Joseph T. Iosue,Victor V. Albert*

Main category: cs.LG

TL;DR: 提出了一个名为QuIIEst的量子启发式内蕴维度估计基准，用于评估在复杂流形上的内蕴维度估计方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有内蕴维度估计方法在简单流形上表现良好，但在更复杂的流形上缺乏统一的评估基准。

Method: 基于量子光学方法构建具有已知内蕴维度的拓扑非平凡流形族，并引入曲率变化和噪声以增加难度。

Result: 测试的内蕴维度估计方法在QuIIEst基准上精度普遍下降，且对非均匀曲率变化表现出较小性能退化，显示出基准的挑战性；同时成功应用于分形结构Hofstadter蝴蝶的有效维度估计。

Conclusion: QuIIEst是一个更具挑战性的内蕴维度估计基准，能够更好地区分不同估计方法的性能，并可扩展至非流形空间的维度分析。

Abstract: Machine learning models can generalize well on real-world datasets. According
to the manifold hypothesis, this is possible because datasets lie on a latent
manifold with small intrinsic dimension (ID). There exist many methods for ID
estimation (IDE), but their estimates vary substantially. This warrants
benchmarking IDE methods on manifolds that are more complex than those in
existing benchmarks. We propose a Quantum-Inspired Intrinsic-dimension
Estimation (QuIIEst) benchmark consisting of infinite families of topologically
non-trivial manifolds with known ID. Our benchmark stems from a quantum-optical
method of embedding arbitrary homogeneous spaces while allowing for curvature
modification and additive noise. The IDE methods tested were generally less
accurate on QuIIEst manifolds than on existing benchmarks under identical
resource allocation. We also observe minimal performance degradation with
increasingly non-uniform curvature, underscoring the benchmark's inherent
difficulty. As a result of independent interest, we perform IDE on the fractal
Hofstadter's butterfly and identify which methods are capable of extracting the
effective dimension of a space that is not a manifold.

</details>


### [246] [On the Identifiability of Latent Action Policies](https://arxiv.org/abs/2510.01337)
*Sébastien Lachapelle*

Main category: cs.LG

TL;DR: 本文研究了潜动作策略学习（LAPO）的可识别性，提出动作表示的理想特性、统计优势及不可识别性的来源，并证明在适当条件下，熵正则化的LAPO目标能够识别满足这些特性的动作表示。


<details>
  <summary>Details</summary>
Motivation: 为了从视频数据中有效发现动作的潜在表示，需要理解现有框架的可识别性问题及其性能表现的原因。

Method: 形式化描述动作表示的理想特性与统计优势，分析潜在的不可识别性来源，并通过理论证明熵正则化LAPO目标在适当条件下可识别满足要求的动作表示。

Result: 证明了在适当条件下，熵正则化的LAPO目标能够唯一识别满足理想特性的动作表示，解释了离散动作表示在实践中表现良好的原因。

Conclusion: LAPO框架在熵正则化下具备良好的可识别性，为学习有效的离散动作表示提供了理论支持。

Abstract: We study the identifiability of latent action policy learning (LAPO), a
framework introduced recently to discover representations of actions from video
data. We formally describe desiderata for such representations, their
statistical benefits and potential sources of unidentifiability. Finally, we
prove that an entropy-regularized LAPO objective identifies action
representations satisfying our desiderata, under suitable conditions. Our
analysis provides an explanation for why discrete action representations
perform well in practice.

</details>


### [247] [Self-Supervised Representation Learning as Mutual Information Maximization](https://arxiv.org/abs/2510.01345)
*Akhlaqur Rahman Sabby,Yi Sui,Tongzi Wu,Jesse C. Cresswell,Ga Wu*

Main category: cs.LG

TL;DR: 本文从互信息下界出发，提出两种自监督表示学习范式（SDMI和JMI），为现有方法中的预测网络、停梯度操作和统计正则化等设计提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 尽管自监督表示学习在实践中表现出色，但其基本原理尚不清楚，许多架构组件被视为经验性技巧，缺乏理论支持。

Method: 基于变分互信息下界推导出两种训练范式：自蒸馏互信息（SDMI）和联合互信息（JMI），分析其对优化策略和模型结构的约束。

Result: SDMI需要交替优化并理论要求停梯度操作，而JMI可通过对称结构进行联合优化；预测网络和统计正则化分别作为两种范式中互信息目标的可实现代理。

Conclusion: 许多现有的自监督学习方法是SDMI或JMI的具体实例或近似，该框架为这些方法的设计选择提供了统一的理论基础。

Abstract: Self-supervised representation learning (SSRL) has demonstrated remarkable
empirical success, yet its underlying principles remain insufficiently
understood. While recent works attempt to unify SSRL methods by examining their
information-theoretic objectives or summarizing their heuristics for preventing
representation collapse, architectural elements like the predictor network,
stop-gradient operation, and statistical regularizer are often viewed as
empirically motivated additions. In this paper, we adopt a first-principles
approach and investigate whether the learning objective of an SSRL algorithm
dictates its possible optimization strategies and model design choices. In
particular, by starting from a variational mutual information (MI) lower bound,
we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint
MI (JMI), each imposing distinct structural constraints and covering a set of
existing SSRL algorithms. SDMI inherently requires alternating optimization,
making stop-gradient operations theoretically essential. In contrast, JMI
admits joint optimization through symmetric architectures without such
components. Under the proposed formulation, predictor networks in SDMI and
statistical regularizers in JMI emerge as tractable surrogates for the MI
objective. We show that many existing SSRL methods are specific instances or
approximations of these two paradigms. This paper provides a theoretical
explanation behind the choices of different architectural components of
existing SSRL methods, beyond heuristic conveniences.

</details>


### [248] [To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking](https://arxiv.org/abs/2510.01349)
*Hannah Lawrence,Elyssa Hofgard,Vasco Portilheiro,Yuxuan Chen,Tess Smidt,Robin Walters*

Main category: cs.LG

TL;DR: 提出一种新方法来评估机器学习中对称性假设的有效性，通过量化数据集中各向异性程度，揭示多个点云基准数据集中的显著对齐现象，并表明数据分布的对称性破缺可能限制不变方法的性能。


<details>
  <summary>Details</summary>
Motivation: 对称性感知方法（如数据增强和等变架构）依赖于变换后数据在测试分布中仍具有高概率的假设，但该假设常未被验证。本文旨在批判性评估这一关键假设。

Method: 提出一种基于双样本神经分类器的度量方法，用以区分原始数据集与其随机增强版本，从而量化数据集中的各向异性或对称性破缺程度。

Result: 在合成数据上验证了该度量方法的有效性；在多个点云数据集中发现了显著的对齐（即对称性破缺）；理论证明即使标签本身是不变的，分布上的对称性破缺仍会阻碍不变方法达到最优性能；实验表明等变方法的效果因数据集而异。

Conclusion: 对等变性的理解需要重新审视数据中的对称性偏差，对称性感知方法的有效性并非普适，而是高度依赖于数据本身的对称结构。

Abstract: Symmetry-aware methods for machine learning, such as data augmentation and
equivariant architectures, encourage correct model behavior on all
transformations (e.g. rotations or permutations) of the original dataset. These
methods can improve generalization and sample efficiency, under the assumption
that the transformed datapoints are highly probable, or "important", under the
test distribution. In this work, we develop a method for critically evaluating
this assumption. In particular, we propose a metric to quantify the amount of
anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural
classifier test that distinguishes between the original dataset and its
randomly augmented equivalent. We validate our metric on synthetic datasets,
and then use it to uncover surprisingly high degrees of alignment in several
benchmark point cloud datasets. We show theoretically that distributional
symmetry-breaking can actually prevent invariant methods from performing
optimally even when the underlying labels are truly invariant, as we show for
invariant ridge regression in the infinite feature limit. Empirically, we find
that the implication for symmetry-aware methods is dataset-dependent:
equivariant methods still impart benefits on some anisotropic datasets, but not
others. Overall, these findings suggest that understanding equivariance -- both
when it works, and why -- may require rethinking symmetry biases in the data.

</details>


### [249] [RheOFormer: A generative transformer model for simulation of complex fluids and flows](https://arxiv.org/abs/2510.01365)
*Maedeh Saberi,Amir Barati Farimani,Safa Jamali*

Main category: cs.LG

TL;DR: 提出了一种名为RheOFormer的生成式算子学习方法，利用自注意力机制高效学习复杂流体流动中的非线性力学行为，具有良好的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理非牛顿流体动力学时计算成本高且扩展性差，数据驱动方法通常需针对不同物理条件重新训练，因此需要一种高效且通用的模型来模拟软材料在流动条件下的力学行为。

Method: 提出Rheological Operator Transformer（RheOFormer），采用基于自注意力的算子学习框架，直接从数据中学习应力张量与变形张量之间的非线性、历史依赖关系，适用于多种粘弹性及弹粘塑性流体的复杂流动场景。

Result: 在多种黏度测量和非黏度测量流动中验证了RheOFormer的性能，结果表明其能准确预测复杂流体的时空演化过程，即使在训练数据有限的情况下仍表现良好，且具备跨条件泛化能力。

Conclusion: RheOFormer是一种高效的神经代理模型，可用于加速复杂流体的预测性仿真，推动数据驱动实验和实时工艺优化。

Abstract: The ability to model mechanics of soft materials under flowing conditions is
key in designing and engineering processes and materials with targeted
properties. This generally requires solution of internal stress tensor, related
to the deformation tensor through nonlinear and history-dependent constitutive
models. Traditional numerical methods for non-Newtonian fluid dynamics often
suffer from prohibitive computational demands and poor scalability to new
problem instances. Developments in data-driven methods have mitigated some
limitations but still require retraining across varied physical conditions. In
this work, we introduce Rheological Operator Transformer (RheOFormer), a
generative operator learning method leveraging self-attention to efficiently
learn different spatial interactions and features of complex fluid flows. We
benchmark RheOFormer across a range of different viscometric and
non-viscometric flows with different types of viscoelastic and
elastoviscoplastic mechanics in complex domains against ground truth solutions.
Our results demonstrate that RheOFormer can accurately learn both scalar and
tensorial nonlinear mechanics of different complex fluids and predict the
spatio-temporal evolution of their flows, even when trained on limited
datasets. Its strong generalization capabilities and computational efficiency
establish RheOFormer as a robust neural surrogate for accelerating predictive
complex fluid simulations, advancing data-driven experimentation, and enabling
real-time process optimization across a wide range of applications.

</details>


### [250] [Selective Underfitting in Diffusion Models](https://arxiv.org/abs/2510.01378)
*Kiwhan Song,Jaeyeon Kim,Sitan Chen,Yilun Du,Sham Kakade,Vincent Sitzmann*

Main category: cs.LG

TL;DR: 扩散模型在训练中学习得分函数以生成样本，但其实际学习的得分函数尚不明确。本文提出“选择性欠拟合”概念，指出优秀的扩散模型在输入空间的某些区域更准确地逼近得分函数，而在其他区域则欠拟合，并通过实验验证了这一观点。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽广泛应用，但其学习何种得分函数仍不清楚。若完全匹配经验得分，模型将仅复制训练数据，无法生成新样本。因此需要理解扩散模型实际学习的得分机制。

Method: 提出‘选择性欠拟合’的概念，分析扩散模型在不同输入区域对得分函数的逼近程度差异，并设计实证干预实验来验证该观点。

Result: 发现高性能扩散模型并非全局欠拟合，而是在特定区域更精确地学习得分函数，在其他区域则欠拟合；这些区域可以被刻画且影响生成性能。

Conclusion: 选择性欠拟合是理解扩散模型泛化能力和生成性能的关键，为模型改进提供了新的可检验洞见。

Abstract: Diffusion models have emerged as the principal paradigm for generative
modeling across various domains. During training, they learn the score
function, which in turn is used to generate samples at inference. They raise a
basic yet unsolved question: which score do they actually learn? In principle,
a diffusion model that matches the empirical score in the entire data space
would simply reproduce the training data, failing to generate novel samples.
Recent work addresses this question by arguing that diffusion models underfit
the empirical score due to training-time inductive biases. In this work, we
refine this perspective, introducing the notion of selective underfitting:
instead of underfitting the score everywhere, better diffusion models more
accurately approximate the score in certain regions of input space, while
underfitting it in others. We characterize these regions and design empirical
interventions to validate our perspective. Our results establish that selective
underfitting is essential for understanding diffusion models, yielding new,
testable insights into their generalization and generative performance.

</details>


### [251] [Fine-Tuning Masked Diffusion for Provable Self-Correction](https://arxiv.org/abs/2510.01384)
*Jaeyeon Kim,Seunggeun Kim,Taekyun Lee,David Z. Pan,Hyeji Kim,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: PRISM是一种轻量级、模型无关的推理时自我修正方法，适用于任何预训练的掩码扩散模型（MDM），通过理论定义的自修正损失学习每个token的质量评分，无需强化学习或验证器，在多个领域和规模上提升了MDM的推断性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MDM自我修正方法需要修改架构或训练过程，或依赖不精确的token质量代理，限制了其应用。因此，需要一种无需更改预训练模型且能准确评估token质量的自我修正方法。

Method: 提出PRISM（推理时掩码扩散模型的插件式重掩码）方法，理论定义一个自我修正损失函数，可在前向传播中学习每个token的质量分数，并据此识别低质量token进行重生成，整个过程无需强化学习或额外验证器。

Result: PRISM在Sudoku求解、无条件文本生成（170M模型）和代码生成（LLaDA 8B模型）等多个任务上显著提升生成质量，证明其有效性与可扩展性。

Conclusion: PRISM为MDM提供了一种高效、通用的推理时自我修正机制，能够在不改变原有模型结构和训练方式的前提下，显著提升生成质量，具有广泛的应用前景。

Abstract: A natural desideratum for generative models is self-correction--detecting and
revising low-quality tokens at inference. While Masked Diffusion Models (MDMs)
have emerged as a promising approach for generative modeling in discrete
spaces, their capacity for self-correction remains poorly understood. Prior
attempts to incorporate self-correction into MDMs either require overhauling
MDM architectures/training or rely on imprecise proxies for token quality,
limiting their applicability. Motivated by this, we introduce PRISM--Plug-in
Remasking for Inference-time Self-correction of Masked Diffusions--a
lightweight, model-agnostic approach that applies to any pretrained MDM.
Theoretically, PRISM defines a self-correction loss that provably learns
per-token quality scores, without RL or a verifier. These quality scores are
computed in the same forward pass with MDM and used to detect low-quality
tokens. Empirically, PRISM advances MDM inference across domains and scales:
Sudoku; unconditional text (170M); and code with LLaDA (8B).

</details>


### [252] [Optimal Stopping vs Best-of-$N$ for Inference Time Optimization](https://arxiv.org/abs/2510.01394)
*Yusuf Kalayci,Vinod Raman,Shaddin Dughmi*

Main category: cs.LG

TL;DR: 提出了一种基于Pandora's Box问题的推理时优化框架，用于在大语言模型生成中平衡输出质量与推理成本，通过自适应学习停止阈值，在保持性能的同时减少15-35%的生成次数。


<details>
  <summary>Details</summary>
Motivation: 在多次生成场景下，大语言模型需要在输出质量和推理成本之间进行权衡，现有方法缺乏对未知奖励分布下的最优停止策略。

Method: 引入UCB风格的Pandora's Box算法，并结合Bradley-Terry模型进行奖励缩放变换，实现跨提示的奖励归一化和在线学习停止阈值。

Result: 在AlpacaFarm和HH-RLHF数据集上，使用多种LLM-奖励模型组合实验表明，该方法相比非自适应Best-of-N采样平均减少15-35%的生成次数，同时保持相同性能。

Conclusion: 建立了最优停止理论与推理时扩展之间的原则性桥梁，为LLM部署提供了理论保证和实际效率提升。

Abstract: Large language model (LLM) generation often requires balancing output quality
against inference cost, especially when using multiple generations. We
introduce a new framework for inference-time optimization based on the
classical Pandora's Box problem. Viewing each generation as opening a costly
"box" with random reward, we develop algorithms that decide when to stop
generating without knowing the underlying reward distribution. Our first
contribution is a UCB-style Pandora's Box algorithm, which achieves performance
that is provably close to Weitzman's algorithm, the optimal strategy when the
distribution is known. We further adapt this method to practical LLM settings
by addressing reward scaling across prompts via a Bradley-Terry inspired
transformation. This leads to an adaptive inference-time optimization method
that normalizes rewards and learns stopping thresholds on the fly. Experiments
on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,
show that our adaptive strategy can obtain the same performance as non-adaptive
Best-of-N sampling while requiring 15-35 percent fewer generations on average.
Our results establish a principled bridge between optimal stopping theory and
inference-time scaling, providing both theoretical performance bounds and
practical efficiency gains for LLM deployment.

</details>


### [253] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络代理框架的方法，直接从笛卡尔坐标学习集体变量（CVs），并通过自动微分提供雅可比矩阵，避免了解析形式的限制，适用于高斯过程回归等自由能重建方法。


<details>
  <summary>Details</summary>
Motivation: 传统自由能重建方法如高斯过程回归（GPR）依赖于集体变量（CVs）的雅可比矩阵，限制了复杂或机器学习CVs的应用。

Method: 构建神经网络代理模型，从笛卡尔坐标直接学习CVs，并利用自动微分计算雅可比矩阵，从而绕过解析求导的需求。

Result: 在MgCl2离子配对体系中，该方法对简单距离CV和复杂配位数CV均实现了高精度；雅可比误差近似服从高斯分布，适合GPR流程。

Conclusion: 该框架使基于梯度的自由能方法能够整合复杂的、机器学习驱动的CVs，拓展了其在生物化学和材料模拟中的应用范围。

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR)
require Jacobians of the collective variables (CVs), a bottleneck that
restricts the use of complex or machine-learned CVs. We introduce a neural
network surrogate framework that learns CVs directly from Cartesian coordinates
and uses automatic differentiation to provide Jacobians, bypassing analytical
forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for
both a simple distance CV and a complex coordination-number CV. Moreover,
Jacobian errors also followed a near-Gaussian distribution, making them
suitable for GPR pipelines. This framework enables gradient-based free energy
methods to incorporate complex and machine-learned CVs, broadening the scope of
biochemistry and materials simulations.

</details>


### [254] [Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction](https://arxiv.org/abs/2510.01407)
*Ethan G. Rogers,Cheng Wang*

Main category: cs.LG

TL;DR: 提出一种基于低秩表示和向量量化的自编码器框架，显著降低神经压缩解码阶段的计算开销，同时保持高质量图像重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积的神经压缩解码器复杂度高、计算成本大，限制了实际应用。

Method: 在自编码器中引入低秩表示与向量量化，通过一系列高效的低秩操作对图像的潜在表示进行处理，实现快速解码。

Result: 显著降低了解码阶段的计算开销，消除了计算瓶颈，并实现了高质量的图像重建。

Conclusion: 该方法在保持高保真输出的同时，有效解决了神经压缩中解码器的计算瓶颈问题，提升了实用性。

Abstract: Image compression and reconstruction are crucial for various digital
applications. While contemporary neural compression methods achieve impressive
compression rates, the adoption of such technology has been largely hindered by
the complexity and large computational costs of the convolution-based decoders
during data reconstruction. To address the decoder bottleneck in neural
compression, we develop a new compression-reconstruction framework based on
incorporating low-rank representation in an autoencoder with vector
quantization. We demonstrated that performing a series of computationally
efficient low-rank operations on the learned latent representation of images
can efficiently reconstruct the data with high quality. Our approach
dramatically reduces the computational overhead in the decoding phase of neural
compression/reconstruction, essentially eliminating the decoder compute
bottleneck while maintaining high fidelity of image outputs.

</details>


### [255] [Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons](https://arxiv.org/abs/2510.01439)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.LG

TL;DR: 本文综述了边缘人工智能（Edge AI）的发展历程、现状及未来方向，涵盖部署位置、处理能力、应用领域和硬件类型的多维分类体系。


<details>
  <summary>Details</summary>
Motivation: 为了系统地理解Edge AI的技术演进、核心挑战与未来机遇，推动其在实际应用中的发展。

Method: 采用PRISMA指南进行文献分析，构建多维分类体系，涵盖部署、处理技术、应用和硬件，并探讨关键技术与挑战。

Result: 梳理了Edge AI从内容分发网络到现代设备端智能的演变，分析了专用硬件、优化软件等使能技术，并评估了资源限制、安全、功耗等挑战，提出了神经形态硬件、持续学习等未来方向。

Conclusion: 该研究为研究人员和实践者提供了Edge AI的全面框架，有助于推动该领域的进一步发展。

Abstract: Edge Artificial Intelligence (Edge AI) embeds intelligence directly into
devices at the network edge, enabling real-time processing with improved
privacy and reduced latency by processing data close to its source. This review
systematically examines the evolution, current landscape, and future directions
of Edge AI through a multi-dimensional taxonomy including deployment location,
processing capabilities such as TinyML and federated learning, application
domains, and hardware types. Following PRISMA guidelines, the analysis traces
the field from early content delivery networks and fog computing to modern
on-device intelligence. Core enabling technologies such as specialized hardware
accelerators, optimized software, and communication protocols are explored.
Challenges including resource limitations, security, model management, power
consumption, and connectivity are critically assessed. Emerging opportunities
in neuromorphic hardware, continual learning algorithms, edge-cloud
collaboration, and trustworthiness integration are highlighted, providing a
comprehensive framework for researchers and practitioners.

</details>


### [256] [SoftAdaClip: A Smooth Clipping Strategy for Fair and Private Model Training](https://arxiv.org/abs/2510.01447)
*Dorsa Soleymani,Ali Dadsetan,Frank Rudzicz*

Main category: cs.LG

TL;DR: 提出SoftAdaClip，一种基于平滑tanh变换的差分隐私训练方法，有效减少子群体差异，提升模型公平性与性能。


<details>
  <summary>Details</summary>
Motivation: 梯度裁剪在DP-SGD中会不成比例地抑制少数群体的学习信号，影响模型公平性，现有自适应裁剪仍依赖硬裁剪，限制公平性提升。

Method: 用基于tanh的平滑变换替代硬裁剪，保留梯度相对大小同时控制敏感性，并结合自适应机制实现更公平的隐私训练。

Result: 在MIMIC-III、GOSSIS-eICU和Adult Income数据集上，相比DP-SGD减少最多87%的子群差异，相比Adaptive-DPSGD减少最多48%，且差异显著。

Conclusion: 平滑变换与自适应机制结合有助于实现更公平、高效的差分隐私模型训练。

Abstract: Differential privacy (DP) provides strong protection for sensitive data, but
often reduces model performance and fairness, especially for underrepresented
groups. One major reason is gradient clipping in DP-SGD, which can
disproportionately suppress learning signals for minority subpopulations.
Although adaptive clipping can enhance utility, it still relies on uniform hard
clipping, which may restrict fairness. To address this, we introduce
SoftAdaClip, a differentially private training method that replaces hard
clipping with a smooth, tanh-based transformation to preserve relative gradient
magnitudes while bounding sensitivity. We evaluate SoftAdaClip on various
datasets, including MIMIC-III (clinical text), GOSSIS-eICU (structured
healthcare), and Adult Income (tabular data). Our results show that SoftAdaClip
reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48%
compared to Adaptive-DPSGD, and these reductions in subgroup disparities are
statistically significant. These findings underscore the importance of
integrating smooth transformations with adaptive mechanisms to achieve fair and
private model training.

</details>


### [257] [Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression](https://arxiv.org/abs/2510.01450)
*Yifei Zuo,Yutong Yin,Zhichen Zeng,Ang Li,Banghua Zhu,Zhaoran Wang*

Main category: cs.LG

TL;DR: 本文提出了Local Linear Attention (LLA)，一种基于非参数统计和测试时回归视角的新型注意力机制，相较于Softmax和线性注意力在理论上具有偏差-方差权衡优势，并通过FlashLLA算法实现高效计算，在测试时学习和上下文学习任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量研究关注高效的注意力机制替代方案，但对更具表达能力、有理论依据的机制探索较少，尤其是在愿意承担更高计算成本的情况下。因此，本文旨在填补这一空白。

Method: 提出Local Linear Attention (LLA)，从非参数统计角度推导；分析其在关联记忆中的偏差-方差权衡优势；设计两种内存高效的原语以降低计算复杂度；开发硬件友好的块状并行算法FlashLLA，并实现定制化推理内核以减少内存开销。

Result: LLA在测试时回归、上下文回归、关联召回和状态追踪任务中验证了其优势与局限性，能够有效适应非平稳环境，在测试时训练和上下文学习中优于强基线方法，显示出良好的可扩展性和大规模应用潜力。

Conclusion: LLA是一种理论上更优、实践中可行的注意力机制，结合理论洞察与高效实现，在多种任务中展现出超越现有方法的性能，为未来注意力机制的设计提供了新方向。

Abstract: Transformer architectures have achieved remarkable success in various
domains. While efficient alternatives to Softmax Attention have been widely
studied, the search for more expressive mechanisms grounded in theoretical
insight-even at greater computational cost-has been relatively underexplored.
In this work, we bridge this gap by proposing Local Linear Attention (LLA), a
novel attention mechanism derived from nonparametric statistics through the
lens of test-time regression. First, we show that LLA offers theoretical
advantages over Linear and Softmax Attention for associative memory via a
bias-variance trade-off analysis. Next, we address its computational challenges
and propose two memory-efficient primitives to tackle the $\Theta(n^2 d)$ and
$\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient,
blockwise algorithm that enables scalable and parallel computation on modern
accelerators. In addition, we implement and profile a customized inference
kernel that significantly reduces memory overheads. Finally, we empirically
validate the advantages and limitations of LLA on test-time regression,
in-context regression, associative recall and state tracking tasks. Experiment
results demonstrate that LLA effectively adapts to non-stationarity,
outperforming strong baselines in test-time training and in-context learning,
and exhibiting promising evidence for its scalability and applicability in
large-scale models. Code is available at
https://github.com/Yifei-Zuo/Flash-LLA.

</details>


### [258] [SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion](https://arxiv.org/abs/2510.01456)
*Brett Barkley,Preston Culbertson,David Fridovich-Keil*

Main category: cs.LG

TL;DR: 提出SCOPED，一种高效且通用的扩散模型OOD检测方法，仅需单次前向传播和JVP，计算成本低，性能接近最优。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测方法在扩散模型中需要大量前向传播，计算开销大，限制了实际应用。需要一种更快速、通用且准确的方法。

Method: 利用训练好的单一扩散模型，结合得分函数的雅可比迹和平方范数构造SCOPED统计量；使用核密度估计对in-distribution的SCOPED分数建模，实现无需固定阈值的灵活无监督检测；通过Hutchinson迹估计器提高效率，仅需一次前向传播和一个JVP。

Result: 在四个视觉基准上，SCOPED以极低计算成本实现了具有竞争力或最先进的精确率-召回率表现；在机器人控制任务中也表现出良好的泛化能力，能识别奖励函数和训练机制间的分布偏移。

Conclusion: SCOPED是一种高效、通用且实用的扩散模型OOD检测方法，适用于真实场景中的多种应用，如视觉感知异常检测、自回归模型离群检测、强化学习探索和无监督训练数据整理。

Abstract: Out-of-distribution (OOD) detection is essential for reliable deployment of
machine learning systems in vision, robotics, reinforcement learning, and
beyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator
for Diffusion (SCOPED), a fast and general-purpose OOD detection method for
diffusion models that reduces the number of forward passes on the trained model
by an order of magnitude compared to prior methods, outperforming most
diffusion-based baselines and closely approaching the accuracy of the strongest
ones. SCOPED is computed from a single diffusion model trained once on a
diverse dataset, and combines the Jacobian trace and squared norm of the
model's score function into a single test statistic. Rather than thresholding
on a fixed value, we estimate the in-distribution density of SCOPED scores
using kernel density estimation, enabling a flexible, unsupervised test that,
in the simplest case, only requires a single forward pass and one
Jacobian-vector product (JVP), made efficient by Hutchinson's trace estimator.
On four vision benchmarks, SCOPED achieves competitive or state-of-the-art
precision-recall scores despite its low computational cost. The same method
generalizes to robotic control tasks with shared state and action spaces,
identifying distribution shifts across reward functions and training regimes.
These results position SCOPED as a practical foundation for fast and reliable
OOD detection in real-world domains, including perceptual artifacts in vision,
outlier detection in autoregressive models, exploration in reinforcement
learning, and dataset curation for unsupervised training.

</details>


### [259] [Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization](https://arxiv.org/abs/2510.01457)
*Brett Barkley,David Fridovich-Keil*

Main category: cs.LG

TL;DR: 本文研究了在基于模型的强化学习中合成数据的有效性，特别是在MBPO算法中的表现。作者发现，在DeepMind Control Suite中，MBPO常不如其无模型对应方法SAC，揭示了环境特定假设对算法设计的影响，并识别出导致失败的两个关键问题：动态与奖励模型之间的尺度不匹配，以及目标表示选择不当。通过解决这些问题，MBPO在七项任务中的五项上超越了SAC，同时保持在OpenAI Gym中的良好表现。


<details>
  <summary>Details</summary>
Motivation: 研究合成数据在模型-based强化学习中的作用，尤其是为何在某些环境下（如DMC）MBPO表现不佳，而此前在Gym环境中表现出色，揭示算法泛化能力受限的原因。

Method: 聚焦于MBPO算法，分析其在DeepMind Control Suite中的失败模式，识别出尺度不匹配和目标表示不当两个核心问题，并提出改进方法以提升策略优化效果。

Result: 修复上述问题后，MBPO在七项DMC任务中的五项上超过了SAC，同时在OpenAI Gym中保持原有性能，实现了此前无法达到的策略改进。

Conclusion: 环境特定的隐含假设会影响算法表现，应建立将MDP结构与算法失效模式关联的分类体系，推动更统一的解决方案，并反思基准选择对算法泛化的深远影响。

Abstract: Synthetic data is a core component of data-efficient Dyna-style model-based
reinforcement learning, yet it can also degrade performance. We study when it
helps, where it fails, and why, and we show that addressing the resulting
failure modes enables policy improvement that was previously unattainable. We
focus on Model-Based Policy Optimization (MBPO), which performs actor and
critic updates using synthetic action counterfactuals. Despite reports of
strong and generalizable sample-efficiency gains in OpenAI Gym, recent work
shows that MBPO often underperforms its model-free counterpart, Soft
Actor-Critic (SAC), in the DeepMind Control Suite (DMC). Although both suites
involve continuous control with proprioceptive robots, this shift leads to
sharp performance losses across seven challenging DMC tasks, with MBPO failing
in cases where claims of generalization from Gym would imply success. This
reveals how environment-specific assumptions can become implicitly encoded into
algorithm design when evaluation is limited. We identify two coupled issues
behind these failures: scale mismatches between dynamics and reward models that
induce critic underestimation and hinder policy improvement during model-policy
coevolution, and a poor choice of target representation that inflates model
variance and produces error-prone rollouts. Addressing these failure modes
enables policy improvement where none was previously possible, allowing MBPO to
outperform SAC in five of seven tasks while preserving the strong performance
previously reported in OpenAI Gym. Rather than aiming only for incremental
average gains, we hope our findings motivate the community to develop
taxonomies that tie MDP task- and environment-level structure to algorithmic
failure modes, pursue unified solutions where possible, and clarify how
benchmark choices ultimately shape the conditions under which algorithms
generalize.

</details>


### [260] [How Well Can Preference Optimization Generalize Under Noisy Feedback?](https://arxiv.org/abs/2510.01458)
*Shawn Im,Yixuan Li*

Main category: cs.LG

TL;DR: 本文研究了在人类反馈存在噪声的情况下，偏好优化对大语言模型对齐的影响，提出了在有限步数内的泛化保证，并通过实验验证了理论发现的实用性。


<details>
  <summary>Details</summary>
Motivation: 由于人类判断存在固有错误和不一致性，现实中的反馈常含噪声，而现有工作多假设无噪声反馈，因此需要研究噪声反馈对偏好优化的影响。

Method: 考虑与实际常见的噪声源（如误标和不确定性）相对应的噪声模型，分析在不同噪声率下基于偏好数据分布和样本数量的泛化性能衰减情况，并将分析扩展到DPO、IPO、SLiC等广泛使用的偏好优化损失函数。

Result: 提供了在有限步偏好优化下的泛化保证，揭示了不同类型噪声对学习效果的影响，且实验结果验证了理论分析在当代大语言模型上的适用性。

Conclusion: 该研究为在噪声反馈条件下进行鲁棒的偏好优化提供了理论基础和实践指导，有助于开发更符合人类偏好的AI系统。

Abstract: As large language models (LLMs) advance their capabilities, aligning these
models with human preferences has become crucial. Preference optimization,
which trains models to distinguish between preferred and non-preferred
responses based on human feedback, has become a crucial component for aligning
LLMs. However, most existing works assume noise-free feedback, which is
unrealistic due to the inherent errors and inconsistencies in human judgments.
This paper addresses the impact of noisy feedback on preference optimization,
providing generalization guarantees under these conditions. In particular, we
consider noise models that correspond to common real-world sources of noise,
such as mislabeling and uncertainty. Unlike traditional analyses that assume
convergence, our work focuses on finite-step preference optimization, offering
new insights that are more aligned with practical LLM training. We describe how
generalization decays with different types of noise across levels of noise
rates based on the preference data distribution and number of samples. Our
analysis for noisy preference learning applies to a broad family of preference
optimization losses such as DPO, IPO, SLiC, etc. Empirical validation on
contemporary LLMs confirms the practical relevance of our findings, offering
valuable insights for developing AI systems that align with human preferences.

</details>


### [261] [LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning](https://arxiv.org/abs/2510.01459)
*Weizhe Chen,Sven Koenig,Bistra Dilkina*

Main category: cs.LG

TL;DR: 本文提出了Length-aware Sampling for Policy Optimization (LSPO)，一种基于响应长度动态选择训练数据的新型元强化学习算法，用于提升大语言模型在推理任务中的训练效果。


<details>
  <summary>Details</summary>
Motivation: 受大语言模型“过度思考”现象的启发，现有强化学习方法在效率和效果上仍有改进空间，因此需要更智能的训练数据选择机制。

Method: 提出LSPO算法，在每一步训练中根据平均响应长度动态采样训练数据，将长度信息融入策略优化过程。

Result: 在多个基础模型和数据集上验证了LSPO的有效性，结果表明其能持续提升学习效果；并通过消融研究探讨了不同长度信号融合方式的影响。

Conclusion: LSPO通过引入长度感知的动态采样机制，显著提升了RLVR框架的性能，为未来研究提供了新的方向。

Abstract: Since the release of Deepseek-R1, reinforcement learning with verifiable
rewards (RLVR) has become a central approach for training large language models
(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss
functions to make RLVR more efficient and effective. In this paper, motivated
by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy
Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects
training data at each step based on the average response length. We evaluate
LSPO across multiple base models and datasets, demonstrating that it
consistently improves learning effectiveness. In addition, we conduct a
detailed ablation study to examine alternative ways of incorporating length
signals into dynamic sampling, offering further insights and highlighting
promising directions for future research.

</details>


### [262] [The Three Regimes of Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2510.01460)
*Lu Li,Tianwei Ni,Yihao Sun,Pierre-Luc Bacon*

Main category: cs.LG

TL;DR: 提出稳定性-可塑性原则以解释离线到在线强化学习中微调设计选择的不一致性，并通过大规模实证研究验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 离线到在线强化学习在不同设置下表现高度不一致，某些在线微调设计在一个场景中有效但在另一个场景中可能完全失败，因此需要一个统一的原则来指导设计选择。

Method: 提出了稳定性-可塑性原则，识别出三种在线微调机制，每种需要不同的稳定性特性，并通过大规模实证研究进行验证。

Result: 在63个案例中的45个中，实验结果与该框架的预测高度一致。

Conclusion: 该工作为基于离线数据集和预训练策略相对性能的离线到在线强化学习提供了有原则的设计指导框架。

Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical
paradigm that leverages offline datasets for pretraining and online
interactions for fine-tuning. However, its empirical behavior is highly
inconsistent: design choices of online-fine tuning that work well in one
setting can fail completely in another. We propose a stability--plasticity
principle that can explain this inconsistency: we should preserve the knowledge
of pretrained policy or offline dataset during online fine-tuning, whichever is
better, while maintaining sufficient plasticity. This perspective identifies
three regimes of online fine-tuning, each requiring distinct stability
properties. We validate this framework through a large-scale empirical study,
finding that the results strongly align with its predictions in 45 of 63 cases.
This work provides a principled framework for guiding design choices in
offline-to-online RL based on the relative performance of the offline dataset
and the pretrained policy.

</details>


### [263] [Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation](https://arxiv.org/abs/2510.01471)
*Haotian Xiang,Jinwen Xu,Qin Lu*

Main category: cs.LG

TL;DR: 提出基于大语言模型（LLM）的贝叶斯优化方法LoRA-VBLL，用于解决高维黑箱优化问题，结合低秩适应和变分贝叶斯框架，实现高效、可递归更新的优化，并通过加权集成进一步自动化超参数选择。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程在处理高维、不规则变量（如类别型、序数型）的黑箱优化问题时表现不佳，而现有神经网络方法计算开销大且难以递归更新，因此需要更高效、灵活的代理模型。

Method: 采用大语言模型（LLM）作为代理模型，利用低秩适应（LoRA）微调LLM参数，并结合变分贝叶斯最后一层（VBLL）框架更新线性回归头的后验；进一步设计基于递归贝叶斯的加权集成（ENS）方法，自动调整LoRA秩和超参数。

Result: 在多个高维基准测试和真实分子优化任务中，(ENS-)LoRA-VBLL显著优于现有方法，具备更高的样本效率和更低的计算开销，并支持模型的持续更新。

Conclusion: LoRA-VBLL为高维、不规则变量的黑箱优化提供了一种高效、可扩展的新范式，结合LLM的表达能力和贝叶斯推理的不确定性建模，推动了贝叶斯优化在复杂实际问题中的应用。

Abstract: A plethora of applications entail solving black-box optimization problems
with high evaluation costs, including drug discovery, material design, as well
as hyperparameter tuning. Toward finding the global optimum of such black-box
optimization problems with sample efficiency, Bayesian optimization (BO) is a
theoretically elegant framework that relies on a probabilistic surrogate model
so as to iteratively select the query point with well-balanced
exploration-exploitation tradeoffs. The Gaussian process (GP), as the de-facto
choice for surrogate modeling, has achieved compelling performances for vanilla
BO with low-dimensional continuous variables. However, GPs fall short in coping
with high-dimensional counterparts with {\it irregular} variables (e.g.,
categorical, ordinal, etc.). To alleviate this, neural network-based surrogates
have been explored. Inspired by the powerful capabilities of LLMs, we adopt the
LLM as the surrogate to model the mapping from the high-dimensional input
variables to the objective function. To adapt to the current problem, we
leverage the low-rank adaptation (LoRA) to fine-tune the LLM parameters
together with the posterior of a linear regression head via the variational
Bayesian last layer (VBLL) framework. The resulting LoRA-VBLL is not only
computationally light compared to existing alternatives, but also admits
recursive updates. To automate the critical selection of the LoRA rank as well
as other hyperparameters, a weighted ensemble (ENS) of LoRA-VBLL surrogates has
been devised, which further accommodates continual update of the per-model
weight and individual LoRA-VBLL parameters via recursive Bayes. Extensive
experimental results demonstrate the compelling performance of the proposed
(ENS-)LoRA-VBLL approaches on various high-dimensional benchmarks and the
real-world molecular optimization tasks.

</details>


### [264] [PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search](https://arxiv.org/abs/2510.01472)
*Hengyi Zhu,Grace Li Zhang,Shaoyi Huang*

Main category: cs.LG

TL;DR: 提出PEL-NAS，一种基于大语言模型驱动的硬件感知神经架构搜索方法，通过搜索空间划分、提示词协同进化和零时代价预测器，在降低搜索成本的同时提升准确率并减少延迟。


<details>
  <summary>Details</summary>
Motivation: 传统超网方法搜索成本高，而大语言模型方法存在探索偏差，难以覆盖全延迟范围的架构设计。

Method: 1) 复杂度驱动的搜索空间划分；2) 基于知识库更新的提示词协同进化机制；3) 零代价代理模型评估候选架构。

Result: 在HW-NAS-Bench上，相比基线方法实现了更高的HV指标、更低的IGD和最高54%的延迟降低，搜索时间从数天缩短至数分钟。

Conclusion: PEL-NAS有效缓解了LLM驱动NAS中的探索偏差问题，在显著降低搜索成本的同时，找到了更优的高精度低延迟网络结构。

Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) requires joint
optimization of accuracy and latency under device constraints. Traditional
supernet-based methods require multiple GPU days per dataset. Large Language
Model (LLM)-driven approaches avoid training a large supernet and can provide
quick feedback, but we observe an exploration bias: the LLM repeatedly proposes
neural network designs within limited search space and fails to discover
architectures across different latency ranges in the entire search space. To
address this issue, we propose PEL-NAS: a search space Partitioned,
architecture prompt co-Evolutionary and LLM-driven Neural Architecture Search
that can generate neural networks with high accuracy and low latency with
reduced search cost. Our proposed PEL-NAS has three key components: 1) a
complexity-driven partitioning engine that divides the search space by
complexity to enforce diversity and mitigate exploration bias; 2) an
LLM-powered architecture prompt co-evolution operator, in which the LLM first
updates a knowledge base of design heuristics based on results from the
previous round, then performs a guided evolution algorithm on architectures
with prompts that incorporate this knowledge base. Prompts and designs improve
together across rounds which avoids random guesswork and improve efficiency; 3)
a zero-cost predictor to avoid training a large number of candidates from
scratch. Experimental results show that on HW-NAS-Bench, PEL-NAS can achieve
overall higher HV, lower IGD, and up to 54% lower latency than baselines at
similar accuracy. Meanwhile, the search cost drops from days to minutes
compared with traditional supernet baselines.

</details>


### [265] [Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets](https://arxiv.org/abs/2510.01479)
*Shriram Karpoora Sundara Pandian,Ali Baheri*

Main category: cs.LG

TL;DR: 本文提出了一种名为密度比加权行为克隆（Weighted BC）的鲁棒模仿学习方法，利用小规模验证干净数据集通过二元判别器估计轨迹级密度比，对行为克隆目标进行加权，从而抑制污染数据的影响，在无需了解污染机制的情况下实现对专家策略的高效学习。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在安全关键应用中具有潜力，但实际数据常受对抗性污染、系统错误或低质量样本影响，导致传统行为克隆和离线RL方法性能下降，因此需要一种对污染数据鲁棒的算法。

Method: 提出Weighted BC方法，使用小型干净参考集训练二元判别器来估计轨迹层面的密度比，将这些密度比经过截断后作为权重引入行为克隆的目标函数中，以优先学习干净的专家行为并降低污染数据的影响，且不依赖于污染机制的先验知识。

Result: 理论分析证明该方法能在有限样本下收敛到干净的专家策略，且误差界与污染率无关；实验在连续控制任务上采用多种污染协议（奖励、状态、转移、动作）验证了其有效性，结果显示Weighted BC在高污染比例下仍保持近最优性能，优于BC、BCQ和BRAC等基线方法。

Conclusion: Weighted BC是一种高效且鲁棒的离线模仿学习方法，能够在未知污染机制的情况下显著提升策略学习的稳定性与性能，适用于现实世界中存在数据质量问题的安全敏感场景。

Abstract: Offline reinforcement learning (RL) enables policy optimization from fixed
datasets, making it suitable for safety-critical applications where online
exploration is infeasible. However, these datasets are often contaminated by
adversarial poisoning, system errors, or low-quality samples, leading to
degraded policy performance in standard behavioral cloning (BC) and offline RL
methods. This paper introduces Density-Ratio Weighted Behavioral Cloning
(Weighted BC), a robust imitation learning approach that uses a small, verified
clean reference set to estimate trajectory-level density ratios via a binary
discriminator. These ratios are clipped and used as weights in the BC objective
to prioritize clean expert behavior while down-weighting or discarding
corrupted data, without requiring knowledge of the contamination mechanism. We
establish theoretical guarantees showing convergence to the clean expert policy
with finite-sample bounds that are independent of the contamination rate. A
comprehensive evaluation framework is established, which incorporates various
poisoning protocols (reward, state, transition, and action) on continuous
control benchmarks. Experiments demonstrate that Weighted BC maintains
near-optimal performance even at high contamination ratios outperforming
baselines such as traditional BC, batch-constrained Q-learning (BCQ) and
behavior regularized actor-critic (BRAC).

</details>


### [266] [Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed](https://arxiv.org/abs/2510.01494)
*Isha Gupta,Rylan Schaeffer,Joshua Kazdan,Ken Liu,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本文提出并验证了一个关于机器学习模型攻击可迁移性的基本区分：在输入数据空间中的攻击可以迁移，而在模型表示空间中的攻击则不能迁移，除非进行表示的几何对齐。作者通过理论和实验证据在四种不同场景下支持这一假设，揭示了对抗迁移性依赖于攻击的操作域，为构建更鲁棒的模型提供了关键洞见。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现图像越狱攻击在视觉-语言模型之间难以迁移，与图像分类器和语言模型中的高迁移性形成鲜明对比。为解释这一差异，作者试图探究攻击迁移性的根本条件。

Method: 提出输入数据空间与模型表示空间攻击的区分假说，并通过四种方式验证：1）在简单设定下数学证明该区分；2）构造对图像分类器有效的非迁移性表示空间攻击；3）构造对语言模型有效的非迁移性表示空间越狱攻击；4）构造可迁移的数据空间攻击对抗VLM，并展示当潜在几何对齐时，表示空间攻击也可迁移。

Result: 理论和实验一致表明：数据空间攻击可迁移，表示空间攻击不可迁移，除非模型间的表示几何结构对齐。特别是在VLM中，后投影器空间的对齐能使表示空间攻击获得迁移性。

Conclusion: 对抗攻击的可迁移性并非普遍属性，而是取决于其作用域——共享的输入数据空间具有迁移潜力，而各模型独有的表示空间则阻碍迁移，除非进行几何对齐。这一发现为理解迁移现象和设计防御机制提供了新视角。

Abstract: The field of adversarial robustness has long established that adversarial
examples can successfully transfer between image classifiers and that text
jailbreaks can successfully transfer between language models (LMs). However, a
pair of recent studies reported being unable to successfully transfer image
jailbreaks between vision-language models (VLMs). To explain this striking
difference, we propose a fundamental distinction regarding the transferability
of attacks against machine learning models: attacks in the input data-space can
transfer, whereas attacks in model representation space do not, at least not
without geometric alignment of representations. We then provide theoretical and
empirical evidence of this hypothesis in four different settings. First, we
mathematically prove this distinction in a simple setting where two networks
compute the same input-output map but via different representations. Second, we
construct representation-space attacks against image classifiers that are as
successful as well-known data-space attacks, but fail to transfer. Third, we
construct representation-space attacks against LMs that successfully jailbreak
the attacked models but again fail to transfer. Fourth, we construct data-space
attacks against VLMs that successfully transfer to new VLMs, and we show that
representation space attacks \emph{can} transfer when VLMs' latent geometries
are sufficiently aligned in post-projector space. Our work reveals that
adversarial transfer is not an inherent property of all attacks but contingent
on their operational domain - the shared data-space versus models' unique
representation spaces - a critical insight for building more robust models.

</details>


### [267] [Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information](https://arxiv.org/abs/2510.01499)
*Rui Ai,Yuqi Pan,David Simchi-Levi,Milind Tambe,Haifeng Xu*

Main category: cs.LG

TL;DR: 提出两种新的多LLM答案聚合算法（OW和ISP），利用一阶和二阶信息，在理论和实验上均优于多数投票法。


<details>
  <summary>Details</summary>
Motivation: 标准多数投票未考虑模型间的异质性和相关性，难以有效聚合多代理LLM的推理结果。

Method: 设计了Optimal Weight (OW)和Inverse Surprising Popularity (ISP)两种新算法，结合一阶和二阶信息进行答案聚合。

Result: 在合成数据、UltraFeedback、MMLU和真实医疗场景ARMMAN上均优于多数投票，提升决策可靠性。

Conclusion: 所提方法能有效克服多数投票的局限，为构建鲁棒的多代理LLM系统提供理论与实践支持。

Abstract: With the rapid progress of multi-agent large language model (LLM) reasoning,
how to effectively aggregate answers from multiple LLMs has emerged as a
fundamental challenge. Standard majority voting treats all answers equally,
failing to consider latent heterogeneity and correlation across models. In this
work, we design two new aggregation algorithms called Optimal Weight (OW) and
Inverse Surprising Popularity (ISP), leveraging both first-order and
second-order information. Our theoretical analysis shows these methods provably
mitigate inherent limitations of majority voting under mild assumptions,
leading to more reliable collective decisions. We empirically validate our
algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as
UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all
cases, our methods consistently outperform majority voting, offering both
practical performance gains and conceptual insights for the design of robust
multi-agent LLM pipelines.

</details>


### [268] [Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control](https://arxiv.org/abs/2510.01508)
*Will Y. Zou,Jean Feng,Alexandre Kalimouttou,Jennifer Yuntong Zhang,Christopher W. Seymour,Romain Pirracchio*

Main category: cs.LG

TL;DR: 提出一种结合离线保守Q学习与新型循环建模的端到端方法，通过设计动作空间优化重症监护室脓毒性休克患者的双血管加压药给药策略，显著提升生存率并增强临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在临床决策支持系统中因不可操作的剂量决策而受到医生怀疑的问题。

Method: 采用离线保守Q学习结合新型循环建模，在回放缓冲区中捕捉ICU时间序列数据的时序依赖性，并设计离散、连续和方向性给药策略的动作空间。

Result: 在eICU和MIMIC数据集上的实验表明，所提出的动作空间设计显著影响学习到的行为策略，提升了超过15%的生存改善概率，并符合既定临床协议。

Conclusion: 合理的动作空间设计不仅能提高强化学习模型的可解释性和临床接受度，还能有效改善患者治疗结果。

Abstract: Reinforcement learning (RL) applications in Clinical Decision Support Systems
(CDSS) frequently encounter skepticism from practitioners regarding inoperable
dosing decisions. We address this challenge with an end-to-end approach for
learning optimal drug dosing and control policies for dual vasopressor
administration in intensive care unit (ICU) patients with septic shock. For
realistic drug dosing, we apply action space design that accommodates discrete,
continuous, and directional dosing strategies in a system that combines offline
conservative Q-learning with a novel recurrent modeling in a replay buffer to
capture temporal dependencies in ICU time-series data. Our comparative analysis
of norepinephrine dosing strategies across different action space formulations
reveals that the designed action spaces improve interpretability and facilitate
clinical adoption while preserving efficacy. Empirical results1 on eICU and
MIMIC demonstrate that action space design profoundly influences learned
behavioral policies. The proposed methods achieve improved patient outcomes of
over 15% in survival improvement probability, while aligning with established
clinical protocols.

</details>


### [269] [Flock: A Knowledge Graph Foundation Model via Learning on Random Walks](https://arxiv.org/abs/2510.01510)
*Jinwoo Kim,Xingyue Huang,Krzysztof Olejniczak,Kyungbin Min,Michael Bronstein,Seunghoon Hong,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: 提出Flock模型，通过概率节点-关系等变性解决知识图谱零样本链接预测中结构相似但语义不同的问题，在多个任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统确定性等变性限制了知识图谱基础模型的表达能力，难以区分结构相似但语义不同的关系，需引入更灵活的等变机制。

Method: 引入概率节点-关系等变性，结合随机游走采样、序列编码和学习聚合，构建Flock模型，保持分布上的等变性并打破对称性。

Result: Flock在新诊断数据集Petals上表现完美，并在54个跨领域知识图谱的实体和关系预测任务中达到最先进性能。

Conclusion: 概率等变性提升了KGFMs的表达能力，Flock作为通用近似器在零样本链接预测中具有优越性和泛化能力。

Abstract: We study the problem of zero-shot link prediction on knowledge graphs (KGs),
which requires models to generalize over novel entities and novel relations.
Knowledge graph foundation models (KGFMs) address this task by enforcing
equivariance over both nodes and relations, learning from structural properties
of nodes and relations, which are then transferable to novel graphs with
similar structural properties. However, the conventional notion of
deterministic equivariance imposes inherent limits on the expressive power of
KGFMs, preventing them from distinguishing structurally similar but
semantically distinct relations. To overcome this limitation, we introduce
probabilistic node-relation equivariance, which preserves equivariance in
distribution while incorporating a principled randomization to break symmetries
during inference. Building on this principle, we present Flock, a KGFM that
iteratively samples random walks, encodes them into sequences via a recording
protocol, embeds them with a sequence model, and aggregates representations of
nodes and relations via learned pooling. Crucially, Flock respects
probabilistic node-relation equivariance and is a universal approximator for
isomorphism-invariant link-level functions over KGs. Empirically, Flock
perfectly solves our new diagnostic dataset Petals where current KGFMs fail,
and achieves state-of-the-art performances on entity- and relation prediction
tasks on 54 KGs from diverse domains.

</details>


### [270] [Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties](https://arxiv.org/abs/2510.01520)
*Hossein Sholehrasa,Xuan Xu,Doina Caragea,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.LG

TL;DR: 本研究利用美国FDA的128万份兽药不良事件报告，结合数据工程与多种机器学习模型（如CatBoost、XGBoost和大语言模型），构建了一个可解释的预测框架，用于准确预测动物用药后的死亡或恢复结果。


<details>
  <summary>Details</summary>
Motivation: 确保食品生产动物用药安全，防止药物残留对人类食品安全造成威胁，及时识别可能引发违禁残留的药代动力学或毒代动力学异常。

Method: 整合FDA的多源关系型数据表，使用VeDDRA本体标准化不良事件，进行数据清洗、缺失值填补与高基数特征降维，并融合药物理化性质；采用Random Forest、CatBoost、XGBoost、ExcelFormer及大语言模型等监督学习方法，结合集成策略与AUM伪标签技术优化少数类识别。

Result: CatBoost与集成方法表现最佳，精确率、召回率和F1分数均达到0.95；SHAP分析揭示肺部、心脏和支气管疾病、动物人口统计学特征及药物理化性质为致死结果的关键预测因子；引入AUM伪标签提升了模型对不确定样本的检测能力。

Conclusion: 该框架通过严谨的数据处理、先进的机器学习与可解释AI，实现了对兽药安全结局的高精度预测，有助于早期识别高风险药物-事件组合，支持FARAD计划在残留风险评估和监管决策中的应用。

Abstract: The safe use of pharmaceuticals in food-producing animals is vital to protect
animal welfare and human food safety. Adverse events (AEs) may signal
unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of
violative residues in the food chain. This study introduces a predictive
framework for classifying outcomes (Death vs. Recovery) using ~1.28 million
reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary
Medicine. A preprocessing pipeline merged relational tables and standardized
AEs through VeDDRA ontologies. Data were normalized, missing values imputed,
and high-cardinality features reduced; physicochemical drug properties were
integrated to capture chemical-residue links. We evaluated supervised models,
including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language
models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as
undersampling and oversampling, with a focus on prioritizing recall for fatal
outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,
achieving precision, recall, and F1-scores of 0.95. Incorporating Average
Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved
minority-class detection, particularly in ExcelFormer and XGBoost.
Interpretability via SHAP identified biologically plausible predictors,
including lung, heart, and bronchial disorders, animal demographics, and drug
physicochemical properties. These features were strongly linked to fatal
outcomes. Overall, the framework shows that combining rigorous data
engineering, advanced machine learning, and explainable AI enables accurate,
interpretable predictions of veterinary safety outcomes. The approach supports
FARAD's mission by enabling early detection of high-risk drug-event profiles,
strengthening residue risk assessment, and informing regulatory and clinical
decision-making.

</details>


### [271] [CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models](https://arxiv.org/abs/2510.01521)
*Diptyaroop Maji,Kang Yang,Prashant Shenoy,Ramesh K Sitaraman,Mani Srivastava*

Main category: cs.LG

TL;DR: CarbonX是一个基于时间序列基础模型（TSFMs）的开源工具，用于全球范围内的碳强度预测与补全，仅需历史碳强度数据即可在214个电网中实现零样本预测，平均MAPE为15.82%，并提供可靠的95%置信区间，支持长达21天的预测，性能接近当前最优方法，且在数据有限情况下仍表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有碳强度预测工具依赖电网特定的电力结构数据和独立模型，缺乏全球适用性，且不提供不确定性估计，限制了其在碳感知应用中的可靠性。

Method: 提出CarbonX，利用时间序列基础模型（TSFMs），仅使用历史碳强度数据和一个通用模型，在多个任务（如预测和补全）和多种电网中实现零样本和微调下的高性能，并提供预测区间。

Result: 在214个电网中实现15.82%的零样本MAPE；在13个基准电网中平均MAPE为9.59%，尾部预测MAPE为16.54%，预测区间覆盖率达95%；支持长达21天的预测且精度下降小；在补全任务中微调后性能优于基线1.2–3.9倍。

Conclusion: CarbonX是一种无需电网特定输入、具备全球覆盖能力且性能强劲的碳强度预测工具，适用于数据稀缺场景，为大规模计算和系统脱碳提供了实用解决方案。

Abstract: Computational decarbonization aims to reduce carbon emissions in computing
and societal systems such as data centers, transportation, and built
environments. This requires accurate, fine-grained carbon intensity forecasts,
yet existing tools have several key limitations: (i) they require grid-specific
electricity mix data, restricting use where such information is unavailable;
(ii) they depend on separate grid-specific models that make it challenging to
provide global coverage; and (iii) they provide forecasts without uncertainty
estimates, limiting reliability for downstream carbon-aware applications.
  In this paper, we present CarbonX, an open-source tool that leverages Time
Series Foundation Models (TSFMs) for a range of decarbonization tasks. CarbonX
utilizes the versatility of TSFMs to provide strong performance across multiple
tasks, such as carbon intensity forecasting and imputation, and across diverse
grids. Using only historical carbon intensity data and a single general model,
our tool achieves a zero-shot forecasting Mean Absolute Percentage Error (MAPE)
of 15.82% across 214 grids worldwide. Across 13 benchmark grids, CarbonX
performance is comparable with the current state-of-the-art, with an average
MAPE of 9.59% and tail forecasting MAPE of 16.54%, while also providing
prediction intervals with 95% coverage. CarbonX can provide forecasts for up to
21 days with minimal accuracy degradation. Further, when fully fine-tuned,
CarbonX outperforms the statistical baselines by 1.2--3.9X on the imputation
task. Overall, these results demonstrate that CarbonX can be used easily on any
grid with limited data and still deliver strong performance, making it a
practical tool for global-scale decarbonization.

</details>


### [272] [Predictive Preference Learning from Human Interventions](https://arxiv.org/abs/2510.01545)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.LG

TL;DR: 本文提出了预测性偏好学习（PPL），利用人类干预中的隐含偏好信号来预测未来状态，通过将每次干预扩展到未来L个时间步并应用偏好优化，提升智能体在安全关键区域的学习效率，减少所需的人类示范。


<details>
  <summary>Details</summary>
Motivation: 现有交互式模仿学习方法主要纠正当前状态下的动作错误，但忽视了对未来状态行为的调整，可能导致更严重的安全隐患。因此需要一种能利用人类干预信息预测并改善未来行为的方法。

Method: 提出预测性偏好学习（PPL），假设智能体在未来L个时间步（偏好视界）内执行相同动作且人类进行相同干预，从而将每次干预自举为未来多个时间步的偏好信号，并在这些未来状态上应用偏好优化。

Result: 在自动驾驶和机器人操作基准上的实验表明，PPL提高了学习效率，减少了所需人类演示；理论分析显示适当选择偏好视界L可在覆盖风险状态与标签准确性之间取得平衡，限制算法最优性差距。

Conclusion: PPL有效利用人类干预中的隐含偏好信息，提升智能体对未来危险状态的适应能力，在减少人类介入的同时增强安全性与学习效率。

Abstract: Learning from human involvement aims to incorporate the human subject to
monitor and correct agent behavior errors. Although most interactive imitation
learning methods focus on correcting the agent's action at the current state,
they do not adjust its actions in future states, which may be potentially more
hazardous. To address this, we introduce Predictive Preference Learning from
Human Interventions (PPL), which leverages the implicit preference signals
contained in human interventions to inform predictions of future rollouts. The
key idea of PPL is to bootstrap each human intervention into L future time
steps, called the preference horizon, with the assumption that the agent
follows the same action and the human makes the same intervention in the
preference horizon. By applying preference optimization on these future states,
expert corrections are propagated into the safety-critical regions where the
agent is expected to explore, significantly improving learning efficiency and
reducing human demonstrations needed. We evaluate our approach with experiments
on both autonomous driving and robotic manipulation benchmarks and demonstrate
its efficiency and generality. Our theoretical analysis further shows that
selecting an appropriate preference horizon L balances coverage of risky states
with label correctness, thereby bounding the algorithmic optimality gap. Demo
and code are available at: https://metadriverse.github.io/ppl

</details>


### [273] [On Integer Programming for the Binarized Neural Network Verification Problem](https://arxiv.org/abs/2510.01525)
*Woojin Kim,James R. Luedtke*

Main category: cs.LG

TL;DR: 本文提出两种改进的整数规划方法来增强二值神经网络（BNN）的验证能力，能够比现有方法在限定时间内验证更大范围的输入扰动。


<details>
  <summary>Details</summary>
Motivation: 由于标准整数规划公式中大M约束导致的较大整数间隙，BNN的验证问题难以求解，因此需要更有效的IP建模方法。

Method: 提出一种新的多类设置下的线性目标函数构造方法，并引入一种利用BNN递归结构生成有效不等式的技巧以强化IP公式。

Result: 新方法显著提升了IP公式的求解效率，能够在相同时间内验证更强的输入扰动，优于现有的IP方法。

Conclusion: 所提出的两种技术有效改善了BNN验证中的整数规划模型，增强了对BNN鲁棒性的评估能力。

Abstract: Binarized neural networks (BNNs) are feedforward neural networks with binary
weights and activation functions. In the context of using a BNN for
classification, the verification problem seeks to determine whether a small
perturbation of a given input can lead it to be misclassified by the BNN, and
the robustness of the BNN can be measured by solving the verification problem
over multiple inputs. The BNN verification problem can be formulated as an
integer programming (IP) problem. However, the natural IP formulation is often
challenging to solve due to a large integrality gap induced by big-$M$
constraints. We present two techniques to improve the IP formulation. First, we
introduce a new method for obtaining a linear objective for the multi-class
setting. Second, we introduce a new technique for generating valid inequalities
for the IP formulation that exploits the recursive structure of BNNs. We find
that our techniques enable verifying BNNs against a higher range of input
perturbation than existing IP approaches within a limited time.

</details>


### [274] [Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs](https://arxiv.org/abs/2510.01527)
*Lecheng Kong,Xiyuan Wang,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: 本文提出了Round-Trip Reinforcement Learning (RTRL)框架，通过将双向转换的成功率作为奖励信号来提升大语言模型在计算化学中的往返一致性，显著提高了模型性能和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有化学大语言模型在反应预测和逆合成等双向任务中缺乏往返一致性，表明模型仅学习了单向记忆而非灵活掌握，限制了其可靠性与泛化能力。

Method: 提出RTRL框架，利用前向与反向转换的闭环成功率作为强化学习的奖励信号，并设计迭代版本使前后向映射相互促进，实现高效自改进训练。

Result: 实验表明，RTRL在监督、自监督和合成数据场景下均显著优于强基线模型，显著提升模型的往返一致性和主任务性能。

Conclusion: 往返一致性不仅是理想特性，更可作为可训练目标，RTRL为构建更鲁棒可靠的化学基础模型提供了新路径。

Abstract: Large Language Models (LLMs) are emerging as versatile foundation models for
computational chemistry, handling bidirectional tasks like reaction prediction
and retrosynthesis. However, these models often lack round-trip consistency.
For instance, a state-of-the-art chemical LLM may successfully caption a
molecule, yet be unable to accurately reconstruct the original structure from
its own generated text. This inconsistency suggests that models are learning
unidirectional memorization rather than flexible mastery. Indeed, recent work
has demonstrated a strong correlation between a model's round-trip consistency
and its performance on the primary tasks. This strong correlation reframes
consistency into a direct target for model improvement. We therefore introduce
Round-Trip Reinforcement Learning (RTRL), a novel framework that trains a model
to improve its consistency by using the success of a round-trip transformation
as a reward signal. We further propose an iterative variant where forward and
reverse mappings alternately train each other in a self-improvement loop, a
process that is highly data-efficient and notably effective with the massive
amount of unlabelled data common in chemistry. Experiments demonstrate that
RTRL significantly \textbf{boosts performance and consistency} over strong
baselines across supervised, self-supervised, and synthetic data regimes. This
work shows that round-trip consistency is not just a desirable property but a
trainable objective, offering a new path toward more robust and reliable
foundation models.

</details>


### [275] [Bypassing Prompt Guards in Production with Controlled-Release Prompting](https://arxiv.org/abs/2510.01529)
*Jaiden Fairoze,Sanjam Garg,Keewoo Lee,Mingyuan Wang*

Main category: cs.LG

TL;DR: 提出一种新型攻击方法，利用资源不对称性绕过轻量级提示防护机制，揭示其在主流大模型中的局限性，并指出需从阻断恶意输入转向防止恶意输出的防御范式转变。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，确保AI安全与对齐至关重要。提示防护被广泛用作过滤恶意查询的轻量级手段，但其安全性尚未充分验证。

Method: 利用提示防护模块与主模型之间的资源不对称性，设计可被主模型解析但轻量级防护无法解码的 jailbreak 提示，从而绕过检测。

Result: 成功在Google Gemini、DeepSeek Chat、Grok、Mistral Le Chat等高度保护的生产模型上实现稳定越狱，同时保持响应质量；并发现版权数据提取、训练数据泄露和思考过程中的恶意响应泄漏等问题。

Conclusion: 轻量级提示防护存在固有缺陷，难以应对高级攻击，未来防御应聚焦于抑制恶意输出而非依赖输入过滤。

Abstract: As large language models (LLMs) advance, ensuring AI safety and alignment is
paramount. One popular approach is prompt guards, lightweight mechanisms
designed to filter malicious queries while being easy to implement and update.
In this work, we introduce a new attack that circumvents such prompt guards,
highlighting their limitations. Our method consistently jailbreaks production
models while maintaining response quality, even under the highly protected chat
interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok
(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry
between the prompt guard and the main LLM, encoding a jailbreak prompt that
lightweight guards cannot decode but the main model can. This reveals an attack
surface inherent to lightweight prompt guards in modern LLM architectures and
underscores the need to shift defenses from blocking malicious inputs to
preventing malicious outputs. We additionally identify other critical alignment
issues, such as copyrighted data extraction, training data extraction, and
malicious response leakage during thinking.

</details>


### [276] [NVIDIA AI Aerial: AI-Native Wireless Communications](https://arxiv.org/abs/2510.01533)
*Kobi Cohen-Arazi,Michael Roe,Zhen Hu,Rohan Chavan,Anna Ptasznik,Joanna Lin,Joao Morais,Joseph Boccuzzi,Tommaso Balercia*

Main category: cs.LG

TL;DR: 本文提出了一种将Python算法编译为GPU可执行模块的框架，以实现DSP与ML在6G网络中的无缝集成，并通过CNN在信道估计中的应用验证了其在数字孪生和实时测试平台上的有效性。


<details>
  <summary>Details</summary>
Motivation: 6G网络向AI原生系统演进，需要在蜂窝网络软件栈中深度融合数字信号处理与机器学习，推动网络生命周期向AI系统靠拢。

Method: 提出一个健壮框架，将基于Python的算法编译为可在NVIDIA GPU上运行的二进制模块，实现在AI Aerial平台上的统一部署。

Result: 在数字孪生环境和实时测试平台中成功实现了PUSCH接收机中的CNN信道估计，展示了框架的高效性、灵活性和高性能。

Conclusion: 该方法为AI/ML模型在下一代蜂窝系统中的可扩展集成奠定了基础，是实现原生智能6G网络的关键。

Abstract: 6G brings a paradigm shift towards AI-native wireless systems, necessitating
the seamless integration of digital signal processing (DSP) and machine
learning (ML) within the software stacks of cellular networks. This
transformation brings the life cycle of modern networks closer to AI systems,
where models and algorithms are iteratively trained, simulated, and deployed
across adjacent environments. In this work, we propose a robust framework that
compiles Python-based algorithms into GPU-runnable blobs. The result is a
unified approach that ensures efficiency, flexibility, and the highest possible
performance on NVIDIA GPUs. As an example of the capabilities of the framework,
we demonstrate the efficacy of performing the channel estimation function in
the PUSCH receiver through a convolutional neural network (CNN) trained in
Python. This is done in a digital twin first, and subsequently in a real-time
testbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform,
lays the foundation for scalable integration of AI/ML models into
next-generation cellular systems, and is essential for realizing the vision of
natively intelligent 6G networks.

</details>


### [277] [TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis](https://arxiv.org/abs/2510.01538)
*Haokun Zhao,Xiang Zhang,Jiaqi Wei,Yiwei Xu,Yuting He,Siqi Sun,Chenyu You*

Main category: cs.LG

TL;DR: 本文提出了TimeSeriesScientist（TSci），首个由大语言模型驱动的通用时间序列预测智能体框架，通过四个专业化智能体实现自动化、可解释的预测流程，在八项基准测试中显著优于统计和LLM基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型通常针对特定数据集或领域设计，泛化能力差，且依赖大量人工干预进行预处理、验证和集成，成本高。亟需一个通用、无需领域知识、最小化人为干预的自动化框架。

Method: 提出TSci框架，包含四个智能体：Curator（基于LLM和外部工具进行数据诊断与预处理）、Planner（结合多模态诊断与自规划缩小模型选择空间）、Forecaster（自适应拟合、验证并选择最优模型与集成策略）、Reporter（生成完整自然语言报告）。整个系统以LLM为核心驱动，实现端到端自动化预测。

Result: 在八个标准数据集上实验表明，TSci平均预测误差比传统统计模型降低10.4%，比现有LLM基线降低38.2%，同时生成清晰、严谨的自然语言报告，提升预测过程的透明性与可解释性。

Conclusion: TSci是一个通用、可解释、低干预的时间序列预测框架，通过LLM驱动的多智能体协作实现了高性能与高透明度的统一，为构建白盒化预测系统提供了新范式。

Abstract: Time series forecasting is central to decision-making in domains as diverse
as energy, finance, climate, and public health. In practice, forecasters face
thousands of short, noisy series that vary in frequency, quality, and horizon,
where the dominant cost lies not in model fitting, but in the labor-intensive
preprocessing, validation, and ensembling required to obtain reliable
predictions. Prevailing statistical and deep learning models are tailored to
specific datasets or domains and generalize poorly. A general, domain-agnostic
framework that minimizes human intervention is urgently in demand. In this
paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic
framework for general time series forecasting. The framework comprises four
specialized agents: Curator performs LLM-guided diagnostics augmented by
external tools that reason over data statistics to choose targeted
preprocessing; Planner narrows the hypothesis space of model choice by
leveraging multi-modal diagnostics and self-planning over the input; Forecaster
performs model fitting and validation and, based on the results, adaptively
selects the best model configuration as well as ensemble strategy to make final
predictions; and Reporter synthesizes the whole process into a comprehensive,
transparent report. With transparent natural-language rationales and
comprehensive reports, TSci transforms the forecasting workflow into a
white-box system that is both interpretable and extensible across tasks.
Empirical results on eight established benchmarks demonstrate that TSci
consistently outperforms both statistical and LLM-based baselines, reducing
forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci
produces a clear and rigorous report that makes the forecasting workflow more
transparent and interpretable.

</details>


### [278] [Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code](https://arxiv.org/abs/2510.01539)
*Aniket Vashishtha,Qirun Dai,Hongyuan Mei,Amit Sharma,Chenhao Tan,Hao Peng*

Main category: cs.LG

TL;DR: 本文提出了可执行反事实推理框架，用于评估和提升大语言模型在代码和数学问题中的因果推理能力，发现现有模型在完整反事实推理上表现显著下降，并表明强化学习能有效泛化至新领域。


<details>
  <summary>Details</summary>
Motivation: 现有评估大模型反事实推理的方法常忽略“溯因”步骤，导致性能被高估；需构建更严格的框架以真实衡量模型的因果推理能力。

Method: 提出“可执行反事实”框架，通过代码与数学题显式要求反事实推理三步骤（溯因、干预、预测），生成可扩展的合成数据，并采用监督微调与强化学习进行训练与测试。

Result: 实验显示SOTA模型在完整反事实任务中准确率下降25-40%；监督微调提升领域内表现但损害跨域泛化，而强化学习在代码（提升1.5x-2x）和数学问题上均优于基线模型。

Conclusion: 强化学习能更好诱导核心认知行为，具备跨领域泛化能力，是提升大模型反事实推理的有效路径。

Abstract: Counterfactual reasoning, a hallmark of intelligence, consists of three
steps: inferring latent variables from observations (abduction), constructing
alternatives (interventions), and predicting their outcomes (prediction). This
skill is essential for advancing LLMs' causal understanding and expanding their
applications in high-stakes domains such as scientific research. However,
existing efforts in assessing LLM's counterfactual reasoning capabilities tend
to skip the abduction step, effectively reducing to interventional reasoning
and leading to overestimation of LLM performance. To address this, we introduce
executable counterfactuals, a novel framework that operationalizes causal
reasoning through code and math problems. Our framework explicitly requires all
three steps of counterfactual reasoning and enables scalable synthetic data
creation with varying difficulty, creating a frontier for evaluating and
improving LLM's reasoning. Our results reveal substantial drop in accuracy
(25-40%) from interventional to counterfactual reasoning for SOTA models like
o4-mini and Claude-4-Sonnet. To address this gap, we construct a training set
comprising counterfactual code problems having if-else condition and test on
out-of-domain code structures (e.g. having while-loop); we also test whether a
model trained on code would generalize to counterfactual math word problems.
While supervised finetuning on stronger models' reasoning traces improves
in-domain performance of Qwen models, it leads to a decrease in accuracy on OOD
tasks such as counterfactual math problems. In contrast, reinforcement learning
induces the core cognitive behaviors and generalizes to new domains, yielding
gains over the base model on both code (improvement of 1.5x-2x) and math
problems. Analysis of the reasoning traces reinforces these findings and
highlights the promise of RL for improving LLMs' counterfactual reasoning.

</details>


### [279] [MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models](https://arxiv.org/abs/2510.01549)
*Kevin Zhai,Utsav Singh,Anirudh Thatipelli,Souradip Chakraborty,Anit Kumar Sahu,Furong Huang,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.LG

TL;DR: 本文提出MIRA，一种无需训练的推理时对齐方法，通过在图像空间中引入基于分数的KL代理正则化采样轨迹，有效缓解扩散模型在文本到图像生成中的奖励操纵问题，同时保持对原始提示的高保真度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽能根据文本生成图像，但常因标量奖励（如美学评分）优化而偏离原始提示，出现奖励操纵问题。现有噪声空间正则化方法不足，需更有效的机制确保生成图像既高奖励又贴合提示。

Method: 提出MIRA方法，在推理时通过冻结主干网络，在图像空间构建基于分数的KL散度代理作为正则项，约束扩散过程的采样轨迹；并推导出基于扩散分数的可计算KL近似。进一步提出MIRA-DPO，将偏好优化迁移至推理阶段，支持非可微奖励。

Result: 在SDv1.5和SDXL等多个模型、奖励函数及公开数据集上，MIRA相较强基线取得超60%胜率，显著提升奖励得分且几乎无分布漂移；可视化机制显示其随计算增加仍保持低漂移，而DNO则明显漂移。MIRA-DPO成功扩展至非可微奖励场景。

Conclusion: MIRA通过图像空间的显式约束有效解决了推理时对齐中的奖励操纵问题，实现了高奖励与提示一致性的平衡，且无需微调，具备高效性和通用性。

Abstract: Diffusion models excel at generating images conditioned on text prompts, but
the resulting images often do not satisfy user-specific criteria measured by
scalar rewards such as Aesthetic Scores. This alignment typically requires
fine-tuning, which is computationally demanding. Recently, inference-time
alignment via noise optimization has emerged as an efficient alternative,
modifying initial input noise to steer the diffusion denoising process towards
generating high-reward images. However, this approach suffers from reward
hacking, where the model produces images that score highly, yet deviate
significantly from the original prompt. We show that noise-space regularization
is insufficient and that preventing reward hacking requires an explicit
image-space constraint. To this end, we propose MIRA (MItigating Reward
hAcking), a training-free, inference-time alignment method. MIRA introduces an
image-space, score-based KL surrogate that regularizes the sampling trajectory
with a frozen backbone, constraining the output distribution so reward can
increase without off-distribution drift (reward hacking). We derive a tractable
approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple
rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,
Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while
preserving prompt adherence; mechanism plots show reward gains with near-zero
drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,
mapping preference optimization to inference time with a frozen backbone,
extending MIRA to non-differentiable rewards without fine-tuning.

</details>


### [280] [Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization](https://arxiv.org/abs/2510.01555)
*Kezhao Liu,Jason Klein Liu,Mingtao Chen,Yiming Liu*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，用于分析强化学习中基于人类反馈的KL散度正则化实现方式，揭示了不同形式（如'$k_1$ in reward'与'$k_2$ as loss'）之间的梯度等价性，并指出当前一些方法（如GRPO中的'$k_3$ as loss'）仅为有偏的一阶近似，缺乏理论支持。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法中KL散度项的实现常基于数值估计启发式，忽视其作为优化损失函数的本质作用，导致实现不一致和潜在偏差，亟需理论统一与指导。

Method: 建立了一个统一的梯度分析框架，将'$k_n$ in reward'与'$k_n$ as loss'两种实现形式联系起来，通过等效梯度系数证明其理论等价性，并结合on-policy与off-policy场景分析其正确实现方式。

Result: 证明了'$k_1$ in reward'是Reverse KL正则化的原则性实现；首次发现'$k_2$ as loss'在on-policy下与前者梯度等价，因而同样合理；而'$k_3$ as loss'仅为有偏一阶近似；并指出off-policy情形下常见实现因忽略重要性采样而导致偏差，提出修正方法。

Conclusion: 本文为KL正则化在RLHF中的正确实现提供了基于梯度的理论依据，统一了不同实现视角，指出了当前实践中的问题并给出修正方案，有助于构建更鲁棒有效的RLHF系统。

Abstract: Reinforcement Learning from Human Feedback (RLHF) leverages a
Kullback-Leibler (KL) divergence loss to stabilize training and prevent
overfitting. However, in methods such as GRPO, its implementation may be guided
by principles from numerical value estimation-a practice that overlooks the
term's functional role as an optimization loss. To analyze this issue, we
establish a unified framework that connects two seemingly distinct
implementation styles: using the mathematical term $k_n$ as a detached
coefficient for the policy's score function ('$k_n$ in reward') or as a direct
loss function through which gradients are propagated ('$k_n$ as loss'). We show
that the latter can always be analyzed via an equivalent gradient coefficient
in the former, unifying the two perspectives. Through this framework, we prove
that the conventional '$k_1$ in reward' (like in PPO) is the principled loss
for Reverse KL (RKL) regularization. We further establish a key finding: under
on-policy conditions, the '$k_2$ as loss' formulation is, in fact,
gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our
work, identifies both as the theoretically sound implementations of the RKL
objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like
in GRPO) is merely a first-order, biased approximation of the principled loss.
Furthermore, we argue that common off-policy implementations of '$k_n$ as loss'
methods are biased due to neglected importance sampling, and we propose a
principled correction. Our findings provide a comprehensive, gradient-based
rationale for choosing and correctly implementing KL regularization, paving the
way for more robust and effective RLHF systems.

</details>


### [281] [Large-Scale Bayesian Causal Discovery with Interventional Data](https://arxiv.org/abs/2510.01562)
*Seong Woo Han,Daniel Duy Vo,Brielin C. Brown*

Main category: cs.LG

TL;DR: 提出了一种基于干预数据的因果发现方法IBCD，通过建模总因果效应矩阵和使用spike-and-slab horseshoe先验，实现了优于现有方法的结构恢复性能，并能量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的因果发现方法在大规模任务上表现不佳，且难以量化不确定性，尤其是在利用干预数据时存在局限性。

Method: 提出Interventional Bayesian Causal Discovery (IBCD)，采用经验贝叶斯框架，对总因果效应矩阵建模（近似为矩阵正态分布），并在边上使用spike-and-slab horseshoe先验，从观测数据中学习无标度和Erdős-Rényi结构的数据驱动权重，将每条边视为潜在变量以实现不确定性感知推断。

Result: 在大量模拟实验中，IBCD在结构恢复方面优于现有基线方法；应用于521个基因的CRISPR扰动数据（Perturb-seq）时，边缘后验包含概率有助于识别鲁棒图结构。

Conclusion: IBCD是一种有效的因果发现框架，能够利用干预数据提升大规模因果结构推断的准确性与可靠性，并提供不确定性量化能力。

Abstract: Inferring the causal relationships among a set of variables in the form of a
directed acyclic graph (DAG) is an important but notoriously challenging
problem. Recently, advancements in high-throughput genomic perturbation screens
have inspired development of methods that leverage interventional data to
improve model identification. However, existing methods still suffer poor
performance on large-scale tasks and fail to quantify uncertainty. Here, we
propose Interventional Bayesian Causal Discovery (IBCD), an empirical Bayesian
framework for causal discovery with interventional data. Our approach models
the likelihood of the matrix of total causal effects, which can be approximated
by a matrix normal distribution, rather than the full data matrix. We place a
spike-and-slab horseshoe prior on the edges and separately learn data-driven
weights for scale-free and Erd\H{o}s-R\'enyi structures from observational
data, treating each edge as a latent variable to enable uncertainty-aware
inference. Through extensive simulation, we show that IBCD achieves superior
structure recovery compared to existing baselines. We apply IBCD to CRISPR
perturbation (Perturb-seq) data on 521 genes, demonstrating that edge posterior
inclusion probabilities enable identification of robust graph structures.

</details>


### [282] [TetriServe: Efficient DiT Serving for Heterogeneous Image Generation](https://arxiv.org/abs/2510.01565)
*Runyu Lu,Shiqi He,Wenxuan Tan,Shenggui Li,Ruofan Wu,Jeff J. Ma,Ang Chen,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: 本文提出了一种用于高效服务扩散Transformer（DiT）模型的新型系统TetriServe，通过引入步级序列并行和基于轮次的调度机制，在不牺牲图像质量的前提下显著提升了服务等级目标（SLO）的达成率。


<details>
  <summary>Details</summary>
Motivation: 由于DiT模型在高分辨率下计算成本高，且现有服务系统对混合分辨率和截止时间的异构工作负载支持效率低，导致GPU利用率低和SLO达成率差，因此需要更灵活高效的并行策略。

Method: 提出步级序列并行，动态调整每个请求的并行度；设计基于轮次的调度机制，将时间离散化为固定轮次，实现截止时间感知的调度，并在步骤级别自适应调整并行度以最小化GPU使用量，同时联合打包请求以减少延迟完成。

Result: 在最先进的DiT模型上进行的广泛评估表明，与现有方案相比，TetriServe的SLO达成率最高提升32%，且未降低图像质量。

Conclusion: TetriServe通过细粒度的并行控制和高效的调度策略，显著提高了DiT模型在严格SLO下的服务效率和资源利用率，适用于处理具有多样化分辨率和截止时间的异构请求。

Abstract: Diffusion Transformer (DiT) models excel at generating highquality images
through iterative denoising steps, but serving them under strict Service Level
Objectives (SLOs) is challenging due to their high computational cost,
particularly at large resolutions. Existing serving systems use fixed degree
sequence parallelism, which is inefficient for heterogeneous workloads with
mixed resolutions and deadlines, leading to poor GPU utilization and low SLO
attainment.
  In this paper, we propose step-level sequence parallelism to dynamically
adjust the parallel degree of individual requests according to their deadlines.
We present TetriServe, a DiT serving system that implements this strategy for
highly efficient image generation. Specifically, TetriServe introduces a novel
round-based scheduling mechanism that improves SLO attainment: (1) discretizing
time into fixed rounds to make deadline-aware scheduling tractable, (2)
adapting parallelism at the step level and minimize GPU hour consumption, and
(3) jointly packing requests to minimize late completions. Extensive evaluation
on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher
SLO attainment compared to existing solutions without degrading image quality.

</details>


### [283] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao,Hongrui Zhang,Junde Xu,Zhou Zhang,Lingdong Shen,Minghao Sun,Ge Liu,Jinbo Xu,Wu-Jun Li,Jinren Ni,Cesar de la Fuente-Nunez,Tianfan Fu,Yejin Choi,Pheng-Ann Heng,Fang Wu*

Main category: cs.LG

TL;DR: 本研究探讨了强化学习（RL）与蛋白质语言模型（PLM）结合在多个蛋白设计任务中的表现，发现RL能显著提升采样效率和成功率，其增益取决于任务空间、奖励质量和策略容量的三因素交互，提出了RL在蛋白设计中的实用优化策略。


<details>
  <summary>Details</summary>
Motivation: 尽管蛋白质语言模型（PLMs）在蛋白科学中取得进展，而强化学习（RL）也用于多目标优化，但RL是否能突破PLMs的预训练先验、揭示潜在的序列-结构-功能规则仍不清楚。本文旨在探索这一问题。

Method: 将多种RL算法与不同类别的PLMs结合，应用于抗菌肽设计、激酶变体优化、抗体工程和逆折叠四个领域，系统评估RL在采样效率和性能提升方面的作用，并分析任务头空间、奖励保真度和策略容量对结果的影响。

Result: RL在所有基准任务中均显著提高了成功率和采样效率；性能增益由任务头空间、奖励保真度和策略容量三者共同决定：当奖励准确、策略容量充足且任务有提升空间时，性能显著提升；反之则增益饱和。

Conclusion: RL能够推动PLMs超越其预训练先验，在蛋白设计中释放未被监督学习捕获的能力；研究提出应优先优化奖励建模与校准，合理匹配算法与正则化强度，并根据边际收益分配模型容量，为RL在蛋白设计中的应用提供了实践指导。

Abstract: Protein language models (PLMs) have advanced computational protein science
through large-scale pretraining and scalable architectures. In parallel,
reinforcement learning (RL) has broadened exploration and enabled precise
multi-objective optimization in protein design. Yet whether RL can push PLMs
beyond their pretraining priors to uncover latent sequence-structure-function
rules remains unclear. We address this by pairing RL with PLMs across four
domains: antimicrobial peptide design, kinase variant optimization, antibody
engineering, and inverse folding. Using diverse RL algorithms and model
classes, we ask if RL improves sampling efficiency and, more importantly, if it
reveals capabilities not captured by supervised learning. Across benchmarks, RL
consistently boosts success rates and sample efficiency. Performance follows a
three-factor interaction: task headroom, reward fidelity, and policy capacity
jointly determine gains. When rewards are accurate and informative, policies
have sufficient capacity, and tasks leave room beyond supervised baselines,
improvements scale; when rewards are noisy or capacity is constrained, gains
saturate despite exploration. This view yields practical guidance for RL in
protein design: prioritize reward modeling and calibration before scaling
policy size, match algorithm and regularization strength to task difficulty,
and allocate capacity where marginal gains are largest. Implementation is
available at https://github.com/chq1155/RL-PLM.

</details>


### [284] [Gradient Shaping Beyond Clipping: A Functional Perspective on Update Magnitude Control](https://arxiv.org/abs/2510.01578)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: 提出了一种名为SPAMP的统一框架，通过统计性、逐层自适应的方式对梯度进行平滑调节，替代传统的硬性梯度裁剪，提升了深度网络训练的稳定性、收敛性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统梯度裁剪使用固定的硬阈值，缺乏灵活性且忽略了梯度分布的动态变化，难以有效控制更新尺度。

Method: SPAMP跟踪每层的局部梯度统计信息，动态估计阈值，并采用基于幂函数的可微变换对梯度更新幅度进行调节，将梯度裁剪和学习率预热统一为控制有效更新尺度的双重机制。

Result: 在图像和语言任务上的大量实验表明，SPAMP在训练稳定性、收敛速度和模型鲁棒性方面优于现有方法。

Conclusion: SPAMP提供了一种原理性更强、更灵活的梯度调节方案，克服了传统梯度裁剪的局限性，是梯度控制的一种有效新范式。

Abstract: Gradient clipping is widely used to stabilize deep network training, but its
formulation as a hard, fixed threshold limits flexibility and ignores gradient
distribution dynamics. We propose SPAMP (Statistical Per-layer Adaptive
Modulation and Projection), a unified framework that generalizes clipping into
smooth, per-layer gradient shaping. SPAMP tracks local gradient statistics,
dynamically estimates thresholds, and applies power-based transformations to
modulate update magnitudes in a differentiable manner. This perspective recasts
clipping and warmup as dual mechanisms for controlling the effective update
scale $\eta_t \|g_t\|$, offering a principled alternative to rigid heuristics.
Extensive experiments across image and language tasks demonstrate that SPAMP
improves stability, convergence, and robustness over existing methods.

</details>


### [285] [Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression](https://arxiv.org/abs/2510.01581)
*Joykirat Singh,Justin Chih-Yao Chen,Archiki Prasad,Elias Stengel-Eskin,Akshay Nambi,Mohit Bansal*

Main category: cs.LG

TL;DR: 本文提出了一种名为TRAAC的在线后训练强化学习方法，通过自注意力机制和任务难度估计，实现对推理长度的自适应调整，在提升准确率的同时显著减少冗余推理步骤。


<details>
  <summary>Details</summary>
Motivation: 现有思维模型在处理不同难度任务时难以动态调整推理长度，容易出现欠思考或过思考问题，导致性能下降或计算资源浪费。

Method: 提出TRAAC方法，利用模型在长推理轨迹上的自注意力识别关键步骤并剪枝冗余步骤，同时估计任务难度并将其纳入训练奖励中，以学习与任务难度相匹配的推理预算分配。

Result: 在多个任务（AIME、AMC、GPQA-D、BBEH）上，Qwen3-4B版本的TRAAC相比基线模型平均绝对准确率提升8.4%，推理长度相对减少36.8%；相比最佳RL基线，准确率提升7.9%，长度减少29.4%。模型还展现出良好的跨领域泛化能力。

Conclusion: TRAAC能根据任务难度实现细粒度的推理预算调整，结合任务难度校准和基于注意力的压缩策略，可在多种任务上实现更高效、准确且自适应的推理。

Abstract: Recent thinking models solve complex reasoning tasks by scaling test-time
compute, but this scaling must be allocated in line with task difficulty. On
one hand, short reasoning (underthinking) leads to errors on harder problems
that require extended reasoning steps; but, excessively long reasoning
(overthinking) can be token-inefficient, generating unnecessary steps even
after reaching a correct intermediate solution. We refer to this as
under-adaptivity, where the model fails to modulate its response length
appropriately given problems of varying difficulty. To address under-adaptivity
and strike a balance between under- and overthinking, we propose TRAAC (Think
Right with Adaptive, Attentive Compression), an online post-training RL method
that leverages the model's self-attention over a long reasoning trajectory to
identify important steps and prune redundant ones. TRAAC also estimates
difficulty and incorporates it into training rewards, thereby learning to
allocate reasoning budget commensurate with example difficulty. Our approach
improves accuracy, reduces reasoning steps, and enables adaptive thinking
compared to base models and other RL baselines. Across a variety of tasks
(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute
accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%
compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length
drop compared to the best RL baseline. TRAAC also shows strong generalization:
although our models are trained on math datasets, they show accuracy and
efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,
and OptimalThinkingBench. Our analysis further verifies that TRAAC provides
fine-grained adjustments to thinking budget based on difficulty and that a
combination of task-difficulty calibration and attention-based compression
yields gains across diverse tasks.

</details>


### [286] [Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation](https://arxiv.org/abs/2510.01588)
*Ziming Tang,Chengbin Hou,Tianyu Zhang,Bangxu Tian,Jinbao Wang,Hairong Lv*

Main category: cs.LG

TL;DR: 提出了一种名为NoRo的噪声鲁棒性UPDRS预测框架，通过构建对比对和增强特征来提高帕金森病远程监测中评分预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 帕金森病远程监测中由于患者自身、环境及数据传输导致的噪声影响了UPDRS评分预测的准确性。

Method: 将原始语音特征分组为有序区间以构建对比对，使用这些对比对训练多层感知机编码器生成抗噪特征，并将其与原始特征结合用于UPDRS评分预测。

Result: 实验表明，NoRo在不同噪声环境下均能有效提升多种下游预测模型的噪声鲁棒性。

Conclusion: NoRo框架能够显著增强UPDRS评分预测在真实远程监测场景中的鲁棒性和可靠性。

Abstract: Parkinson's disease (PD) is one of the most common neurodegenerative
disorder. PD telemonitoring emerges as a novel assessment modality enabling
self-administered at-home tests of Unified Parkinson's Disease Rating Scale
(UPDRS) scores, enhancing accessibility for PD patients. However, three types
of noise would occur during measurements: (1) patient-induced measurement
inaccuracies, (2) environmental noise, and (3) data packet loss during
transmission, resulting in higher prediction errors. To address these
challenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,
the original speech features are grouped into ordered bins, based on the
continuous values of a selected feature, to construct contrastive pairs.
Second, the contrastive pairs are employed to train a multilayer perceptron
encoder for generating noise-robust features. Finally, these features are
concatenated with the original features as the augmented features, which are
then fed into the UPDRS prediction models. Notably, we further introduces a
novel evaluation approach with customizable noise injection module, and
extensive experiments show that NoRo can successfully enhance the noise
robustness of UPDRS prediction across various downstream prediction models
under different noisy environments.

</details>


### [287] [Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness](https://arxiv.org/abs/2510.01598)
*Youwei Bao,Shuhan Yang,Hyunsoo Yang*

Main category: cs.LG

TL;DR: 该研究利用自旋转移矩磁隧道结（STT-MTJ）硬件生成真随机位，嵌入到生成式AI模型中以增强安全性，显著减少不安全输出，同时保持低开销，具备高并行性和可扩展性，适用于大语言模型的高速采样需求。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型中使用的伪随机数生成器（PRNG）存在可预测性，易受攻击，传统防御方法能耗和延迟高，亟需高效、安全的随机性来源。

Method: 采用STT-MTJ硬件生成真随机位，通过FPGA辅助的高并行原型系统实现实时集成，并在生成对抗网络（GAN）中验证其安全性提升效果。

Result: 系统实现兆比特每秒的真随机数生成，通过NIST随机性测试；应用于CIFAR-10训练的GAN时，相比低质量RNG基线，不安全输出最多减少18.6倍；具备纳秒级切换速度和高能效，可扩展至千兆比特每秒吞吐量。

Conclusion: 基于STT-MTJ的真随机数生成系统为下一代生成式AI提供了高效、可扩展且安全的硬件随机性解决方案，具有在大语言模型等大规模应用中部署的潜力。

Abstract: Deterministic pseudo random number generators (PRNGs) used in generative
artificial intelligence (GAI) models produce predictable patterns vulnerable to
exploitation by attackers. Conventional defences against the vulnerabilities
often come with significant energy and latency overhead. Here, we embed
hardware-generated true random bits from spin-transfer torque magnetic tunnel
junctions (STT-MTJs) to address the challenges. A highly parallel,
FPGA-assisted prototype computing system delivers megabit-per-second true
random numbers, passing NIST randomness tests after in-situ operations with
minimal overhead. Integrating the hardware random bits into a generative
adversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to
18.6 times compared to the low-quality random number generators (RNG) baseline.
With nanosecond switching speed, high energy efficiency, and established
scalability, our STT-MTJ-based system holds the potential to scale beyond 106
parallel cells, achieving gigabit-per-second throughput suitable for large
language model sampling. This advancement highlights spintronic RNGs as
practical security components for next-generation GAI systems.

</details>


### [288] [Posterior Collapse as a Phase Transition in Variational Autoencoders](https://arxiv.org/abs/2510.01621)
*Zhen Li,Fan Zhang,Zheng Zhang,Yu Chen*

Main category: cs.LG

TL;DR: 本文从统计物理的角度研究了变分自编码器（VAEs）中的后验坍缩现象，揭示其是由数据结构和模型超参数共同控制的相变过程。


<details>
  <summary>Details</summary>
Motivation: 理解VAE中后验坍缩的根本原因，并将其视为一种相变而非单纯的优化失败。

Method: 通过分析与后验坍缩相关的平凡解的稳定性，识别出关键的超参数阈值，并研究KL散度的不连续性。

Result: 发现了区分有效隐变量推断与坍缩的临界边界，并在合成和真实数据集上验证了相变行为的存在。

Conclusion: 后验坍缩是一种由数据结构与变分约束相互作用引起的相变现象，这一观点为深度生成模型的可训练性和表示能力提供了新的见解。

Abstract: We investigate the phenomenon of posterior collapse in variational
autoencoders (VAEs) from the perspective of statistical physics, and reveal
that it constitutes a phase transition governed jointly by data structure and
model hyper-parameters. By analyzing the stability of the trivial solution
associated with posterior collapse, we identify a critical hyper-parameter
threshold. This critical boundary, separating meaningful latent inference from
collapse, is characterized by a discontinuity in the KL divergence between the
approximate posterior and the prior distribution. We validate this critical
behavior on both synthetic and real-world datasets, confirming the existence of
a phase transition. Our results demonstrate that posterior collapse is not
merely an optimization failure, but rather an emerging phase transition arising
from the interplay between data structure and variational constraints. This
perspective offers new insights into the trainability and representational
capacity of deep generative models.

</details>


### [289] [Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead](https://arxiv.org/abs/2510.01624)
*Feiyang Kang,Michael Kuchnik,Karthik Padthe,Marin Vlastelica,Ruoxi Jia,Carole-Jean Wu,Newsha Ardalani*

Main category: cs.LG

TL;DR: 该论文研究了在推理型大语言模型的后训练过程中，监督微调（SFT）性能是否能有效预测后续强化学习（RL）的效果，发现高SFT得分并不一定带来更好的RL表现，并提出了更可靠的替代指标。


<details>
  <summary>Details</summary>
Motivation: 挑战当前两阶段独立训练（SFT和RL）的假设，即高SFT性能是否真正有助于提升后续RL阶段的表现。

Method: 通过在多个模型和数据集上进行大规模实验，训练数百个高达120亿参数的模型，结合GRPO实现SFT与RLVR，并在7个数学基准上进行最多256次重复评估，分析不同指标对RL结果的预测能力。

Result: 发现高SFT得分可能偏向简单或同质化数据，无法可靠预测RL增益；而基于保留推理样本的泛化损失和Pass@large k性能可显著提高对RL结果的预测精度（R²和斯皮尔曼相关系数提升达0.5，约2倍）。同时发现训练策略如epoch分配和样本长度多样性显著影响最终效果。

Conclusion: SFT性能不是RL效果的良好代理指标，应采用泛化损失和Pass@large k等更优指标来指导训练设计和评估，这对实际后训练流程具有广泛的应用价值。

Abstract: In post-training for reasoning Large Language Models (LLMs), the current
state of practice trains LLMs in two independent stages: Supervised Fine-Tuning
(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as
``RL'' below). In this work, we challenge whether high SFT scores translate to
improved performance after RL. We provide extensive counter-examples where this
is not true. We find high SFT scores can be biased toward simpler or more
homogeneous data and are not reliably predictive of subsequent RL gains or
scaled-up post-training effectiveness. In some cases, RL training on models
with improved SFT performance could lead to substantially worse outcome
compared to RL on the base model without SFT. We study alternative metrics and
identify generalization loss on held-out reasoning examples and Pass@large k
performance to provide strong proxies for the RL outcome. We trained hundreds
of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive
evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU
hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple
state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL
performance, prediction based on generalization loss and Pass@large k achieves
substantial higher precision, improving $R^2$ coefficient and Spearman's rank
correlation coefficient by up to 0.5 (2x). This provides strong utility for
broad use cases. For example, in most experiments, we find SFT training on
unique examples for a one epoch underperforms training on half examples for two
epochs, either after SFT or SFT-then-RL; With the same SFT budget, training
only on short examples may lead to better SFT performance, though, it often
leads to worse outcome after RL compared to training on examples with varying
lengths. Evaluation tool will be open-sourced.

</details>


### [290] [Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls](https://arxiv.org/abs/2510.01631)
*Feiyang Kang,Newsha Ardalani,Michael Kuchnik,Youssef Emad,Mostafa Elhoushi,Shubhabrata Sengupta,Shang-Wen Li,Ramya Raghavendra,Ruoxi Jia,Carole-Jean Wu*

Main category: cs.LG

TL;DR: 本研究通过大规模实证分析（超过1000个大语言模型，使用超10万GPU小时）评估合成数据在预训练中的作用，发现仅使用重述型合成数据并不优于自然网页数据，但将1/3重述合成数据与2/3自然数据混合可提速5-10倍；而纯教科书式合成数据单独使用表现更差，且显示出“模型崩溃”迹象。研究揭示了合成数据的有效使用条件，并提供了关于数据比例、模型规模和数据预算的实用指导。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据的稀缺限制了大语言模型的扩展，合成数据被视为潜在解决方案，但其有效性尚不明确，尤其在不同数据类型混合使用和不同规模下的影响缺乏系统研究。

Method: 采用统一协议和缩放定律，对多种类型的自然与合成数据（包括重述文本、生成的教科书等）及其混合比例进行大规模实证比较，训练超过1000个大语言模型，累计使用超过10万GPU小时，评估其在验证损失和下游任务上的表现。

Result: 仅使用重述型合成数据预训练不比自然数据快；1/3重述合成+2/3自然数据混合可在大数据量下提速5-10倍；纯教科书式合成数据单独使用导致更高下游损失，尤其在小数据预算下；合成数据的“最佳”比例随模型大小和数据预算变化，重述数据约30%为优；生成器模型大于~8B参数并未带来明显优势；重述数据未显示模型崩溃，但教科书式合成数据混合显示出模型崩溃模式。

Conclusion: 合成数据在大语言模型预训练中具有条件性优势，关键在于数据类型和混合比例；合理使用重述型合成数据可显著提升训练效率，而纯生成式教科书数据可能导致性能退化；研究为合成数据的实际应用提供了量化依据和操作指南。

Abstract: Training data plays a crucial role in Large Language Models (LLM) scaling,
yet high quality data is of limited supply. Synthetic data techniques offer a
potential path toward sidestepping these limitations. We conduct a large-scale
empirical investigation (>1000 LLMs with >100k GPU hours) using a unified
protocol and scaling laws, comparing natural web data, diverse synthetic types
(rephrased text, generated textbooks), and mixtures of natural and synthetic
data. Specifically, we found pre-training on rephrased synthetic data
\textit{alone} is not faster than pre-training on natural web texts; while
pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts
can speed up 5-10x (to reach the same validation loss) at larger data budgets.
Pre-training on textbook-style synthetic data \textit{alone} results in notably
higher loss on many downstream domains especially at small data budgets. "Good"
ratios of synthetic data in training data mixtures depend on the model size and
data budget, empirically converging to ~30% for rephrased synthetic data.
Larger generator models do not necessarily yield better pre-training data than
~8B-param models. These results contribute mixed evidence on "model collapse"
during large-scale single-round (n=1) model training on synthetic
data--training on rephrased synthetic data shows no degradation in performance
in foreseeable scales whereas training on mixtures of textbook-style
pure-generated synthetic data shows patterns predicted by "model collapse". Our
work demystifies synthetic data in pre-training, validates its conditional
benefits, and offers practical guidance.

</details>


### [291] [CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning](https://arxiv.org/abs/2510.01634)
*Ryan Y. Lin,Siddhartha Ojha,Nicholas Bai*

Main category: cs.LG

TL;DR: 本文提出了Curvature-Adaptive Transformer (CAT)，通过动态学习每个token在欧几里得、双曲和球面几何注意力分支间的路由，实现对非欧数据结构的自适应建模，在知识图谱补全任务中显著优于固定几何方法。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理具有非欧几里得结构的数据时受限于其隐含的欧氏几何假设；现有扩展方法需预先固定几何空间，缺乏对混合几何特性的适应能力。

Method: 提出CAT模型，引入轻量级可微门控机制，实现每个token在三种几何注意力分支间的动态路由，各分支采用对应流形上的专用操作，实现几何自适应与可解释的曲率偏好。

Result: 在FB15k-237和WN18RR知识图谱补全任务上，MRR和Hits@10提升约10%，参数仅增加5%，推理时间相当。

Conclusion: 学习几何自适应优于单一固定几何，CAT为语言、视觉和多模态领域的混合几何架构提供了可扩展且可解释的基础。

Abstract: Transformers achieve strong performance across diverse domains but implicitly
assume Euclidean geometry in their attention mechanisms, limiting their
effectiveness on data with non-Euclidean structure. While recent extensions to
hyperbolic and spherical spaces show promise for hierarchical and cyclical
patterns, respectively, they require committing to a single geometry a priori,
reducing flexibility when data exhibits mixed geometric properties. We
introduce the Curvature-Adaptive Transformer (CAT), a novel architecture that
dynamically learns per-token routing across three geometric attention branches
through a lightweight, differentiable gating mechanism. Unlike fixed-geometry
approaches, CAT enables adaptive geometric specialization, routing tokens to
the appropriate curvature based on their local relational structure. The
routing network provides interpretable curvature preferences while each branch
employs geometry-specific operations optimized for its respective manifold. On
knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves
approximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines
with minimal overhead (5% parameter increase, comparable inference time). These
results demonstrate that learned geometric adaptation outperforms any single
fixed geometry for complex relational reasoning, establishing CAT as a scalable
and interpretable foundation for mixture-of-geometry architectures across
language, vision, and multimodal domains.

</details>


### [292] [Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking](https://arxiv.org/abs/2510.01637)
*Liyan Xie,Muhammad Siddeek,Mohamed Seif,Andrea J. Goldsmith,Mengdi Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于组合模式的水印框架，用于检测和定位大语言模型生成文本在后生成阶段的局部编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的水印技术难以应对AI生成内容在后生成阶段被修改（如人为修订或欺骗攻击）的情况，因此需要一种能够检测并定位这些修改的方法。

Method: 提出一种组合模式水印框架，将词汇表划分为互斥子集，并在生成过程中通过这些子集上的确定性组合模式嵌入水印；同时设计全局统计量用于水印检测，以及轻量级局部统计量用于标记和定位潜在编辑。

Result: 在开源大语言模型上评估了该方法，在多种编辑场景下表现出较强的编辑定位能力，并引入了两类任务特定评估指标：I类错误率和检测准确率。

Conclusion: 所提出的组合水印框架能有效检测和定位大语言模型输出的后生成编辑，具有良好的实用性和鲁棒性。

Abstract: Watermarking has become a key technique for proprietary language models,
enabling the distinction between AI-generated and human-written text. However,
in many real-world scenarios, LLM-generated content may undergo post-generation
edits, such as human revisions or even spoofing attacks, making it critical to
detect and localize such modifications. In this work, we introduce a new task:
detecting post-generation edits locally made to watermarked LLM outputs. To
this end, we propose a combinatorial pattern-based watermarking framework,
which partitions the vocabulary into disjoint subsets and embeds the watermark
by enforcing a deterministic combinatorial pattern over these subsets during
generation. We accompany the combinatorial watermark with a global statistic
that can be used to detect the watermark. Furthermore, we design lightweight
local statistics to flag and localize potential edits. We introduce two
task-specific evaluation metrics, Type-I error rate and detection accuracy, and
evaluate our method on open-source LLMs across a variety of editing scenarios,
demonstrating strong empirical performance in edit localization.

</details>


### [293] [Support Basis: Fast Attention Beyond Bounded Entries](https://arxiv.org/abs/2510.01643)
*Maryam Aliakbarpour,Vladimir Braverman,Junze Yin,Haochen Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的支持基分解框架，用于在无界输入下高效近似注意力机制，实现了亚二次复杂度，并为多项式注意力的成功提供了首个理论解释。


<details>
  <summary>Details</summary>
Motivation: 由于现有亚二次注意力算法依赖于难以满足的有界输入假设，限制了其在现代大语言模型中的应用，因此需要一种更实用且具备理论保证的近似方法。

Method: 引入支持基分解框架，利用查询和键矩阵的次高斯特性，将大值和小值条目分离，对稀疏部分精确计算，对密集部分使用多项式逼近，并扩展到多阈值设置以消除分布假设。

Result: 提出了亚二次时间复杂度的注意力近似算法，在无需有界输入假设的情况下仍保持高效，并首次从理论上证明了多项式注意力结合 sketching 可以很好地逼近 softmax 注意力。

Conclusion: 支持基分解为高效注意力计算提供了一个通用且理论坚实的框架，推动了大语言模型中注意力机制的可扩展性研究。

Abstract: The quadratic complexity of softmax attention remains a central bottleneck in
scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a
sub-quadratic attention approximation algorithm, but it works only under the
restrictive bounded-entry assumption. Since this assumption rarely holds in
practice, its applicability to modern LLMs is limited.
  In this paper, we introduce support-basis decomposition, a new framework for
efficient attention approximation beyond bounded entries. We empirically
demonstrate that the entries of the query and key matrices exhibit sub-Gaussian
behavior. Our approach uses this property to split large and small entries,
enabling exact computation on sparse components and polynomial approximation on
dense components. We establish rigorous theoretical guarantees, proving a
sub-quadratic runtime, and extend the method to a multi-threshold setting that
eliminates all distributional assumptions. Furthermore, we provide the first
theoretical justification for the empirical success of polynomial attention
[Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be
closely approximated by a combination of multiple polynomial attentions with
sketching.

</details>


### [294] [Source-Free Cross-Domain Continual Learning](https://arxiv.org/abs/2510.01649)
*Muhammad Tanzil Furqon,Mahardhika Pratama,Igor Škrjanc,Lin Liu,Habibullah Habibullah,Kutluyil Dogancay*

Main category: cs.LG

TL;DR: 本文提出了一种无需源域样本且无需回放的跨域持续学习方法REFEREE，通过频域感知提示与不确定性加权策略应对伪标签噪声，并利用核线性判别分析缓解灾难性遗忘，在源域完全不可用的情况下取得了优于依赖源域的现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨域持续学习方法依赖带标签的源域数据，在隐私受限场景下难以应用；本文旨在解决源域样本完全不可用情况下的持续学习问题。

Method: 结合源预训练模型与大规模视觉-语言模型，采用频域感知提示技术增强目标域样本（保留低频、抑制高频成分），通过不确定性感知加权策略减少伪标签噪声影响，并利用核线性判别分析在冻结主干网络的同时进行分类以缓解灾难性遗忘。

Result: 实验表明，所提REFEREE方法在多个基准上显著优于使用源域数据的先前方法，验证了其在源域不可用场景下的有效性与鲁棒性。

Conclusion: REFEREE为源域受限的跨域持续学习提供了高效解决方案，通过频域增强、不确定性建模与无回放机制实现了优异性能，具有实际部署潜力。

Abstract: Although existing cross-domain continual learning approaches successfully
address many streaming tasks having domain shifts, they call for a fully
labeled source domain hindering their feasibility in the privacy constrained
environments. This paper goes one step ahead with the problem of source-free
cross-domain continual learning where the use of source-domain samples are
completely prohibited. We propose the idea of rehearsal-free frequency-aware
dynamic prompt collaborations (REFEREE) to cope with the absence of labeled
source-domain samples in realm of cross-domain continual learning. REFEREE is
built upon a synergy between a source-pre-trained model and a large-scale
vision-language model, thus overcoming the problem of sub-optimal
generalizations when relying only on a source pre-trained model. The domain
shift problem between the source domain and the target domain is handled by a
frequency-aware prompting technique encouraging low-frequency components while
suppressing high-frequency components. This strategy generates frequency-aware
augmented samples, robust against noisy pseudo labels. The noisy pseudo-label
problem is further addressed with the uncertainty-aware weighting strategy
where the mean and covariance matrix are weighted by prediction uncertainties,
thus mitigating the adverse effects of the noisy pseudo label. Besides, the
issue of catastrophic forgetting (CF) is overcome by kernel linear discriminant
analysis (KLDA) where the backbone network is frozen while the classification
is performed using the linear discriminant analysis approach guided by the
random kernel method. Our rigorous numerical studies confirm the advantage of
our approach where it beats prior arts having access to source domain samples
with significant margins.

</details>


### [295] [The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM](https://arxiv.org/abs/2510.01650)
*Kwanhee Lee,Hyeondo Jang,Dongyeop Lee,Dan Alistarh,Namhoon Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为Elsa的新型神经网络剪枝方法，通过引入基于ADMM的约束优化技术，解决了传统剪枝方法在大语言模型中难以实现高稀疏度（>90%）且保持模型精度的问题。实验表明，Elsa在LLaMA-2-7B上90%稀疏度下比现有最佳方法困惑度降低7.8倍，并提出了可扩展到27B规模模型的量化版本Elsa_-L，具备理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络剪枝方法受限于代理目标函数的设计，在稀疏度超过50-60%时性能显著下降，难以实现高稀疏度与高保真度的兼顾，阻碍了其在大语言模型中的应用。

Method: 提出Elsa方法，采用基于ADMM的标准约束优化框架，直接优化原始目标而非代理目标，从而更有效地实现权重剪枝；同时设计了量化版本Elsa_-L以支持超大规模模型。

Result: 在多种模型和规模上的实验显示，Elsa在90%稀疏度下显著优于现有方法，例如在LLaMA-2-7B上困惑度降低了7.8倍；Elsa_-L可在27B规模模型上运行并具有理论收敛性。

Conclusion: Elsa突破了当前大语言模型剪枝的瓶颈，实现了高达90%的稀疏度同时保持高模型保真度，为未来进一步探索高效模型压缩提供了新方向。

Abstract: Neural network pruning is a promising technique to mitigate the excessive
computational and memory requirements of large language models (LLMs). Despite
its promise, however, progress in this area has diminished, as conventional
methods are seemingly unable to surpass moderate sparsity levels (50-60%)
without severely degrading model accuracy. This work breaks through the current
impasse, presenting a principled and effective method called $\texttt{Elsa}$,
which achieves extreme sparsity levels of up to 90% while retaining high model
fidelity. This is done by identifying several limitations in current practice,
all of which can be traced back to their reliance on a surrogate objective
formulation. $\texttt{Elsa}$ tackles this issue directly and effectively via
standard and well-established constrained optimization techniques based on
ADMM. Our extensive experiments across a wide range of models and scales show
that $\texttt{Elsa}$ achieves substantial improvements over existing methods;
e.g., it achieves 7.8$\times$ less perplexity than the best existing method on
LLaMA-2-7B at 90% sparsity. Furthermore, we present
$\texttt{Elsa}_{\text{-L}}$, a quantized variant that scales to extremely large
models (27B), and establish its theoretical convergence guarantees. These
results highlight meaningful progress in advancing the frontier of LLM
sparsity, while promising that significant opportunities for further
advancement may remain in directions that have so far attracted limited
exploration.

</details>


### [296] [Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning](https://arxiv.org/abs/2510.01656)
*Jiashun Liu,Johan Obando-Ceron,Han Lu,Yancheng He,Weixun Wang,Wenbo Su,Bo Zheng,Pablo Samuel Castro,Aaron Courville,Ling Pan*

Main category: cs.LG

TL;DR: 提出Asymmetric Proximal Policy Optimization (AsyPPO)，通过轻量级迷你批评器提升大模型强化学习的稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 传统值函数在大规模语言模型中计算昂贵且在稀疏奖励和长推理 horizon 下表现不佳，因此需要更高效的批评机制。

Method: 引入AsyPPO框架，使用多个轻量级迷你批评器，分别训练于不相交的提示片段上，并利用批评器间不确定性优化策略更新。

Result: 在仅5000个样本的开源数据上训练后，AsyPPO在多个基准上优于GRPO等强基线，在Qwen3-4b-Base上提升超6%，在更大模型上也有约3%增益。

Conclusion: 架构创新（如轻量批评器与不确定性感知更新）对可扩展、高效的RL4LLM至关重要。

Abstract: Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing
them with average advantage baselines. This shift is largely pragmatic:
conventional value functions are computationally expensive to train at LLM
scale and often fail under sparse rewards and long reasoning horizons. We
revisit this bottleneck from an architectural perspective and introduce
Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable
framework that restores the critics role while remaining efficient in
large-model settings. AsyPPO employs a set of lightweight mini-critics, each
trained on disjoint prompt shards. This design encourages diversity while
preserving calibration, reducing value-estimation bias. Beyond robust
estimation, AsyPPO leverages inter-critic uncertainty to refine the policy
update: (i) masking advantages in states where critics agree and gradients add
little learning signal, and (ii) filtering high-divergence states from entropy
regularization, suppressing spurious exploration. After training on open-source
data with only 5,000 samples, AsyPPO consistently improves learning stability
and performance across multiple benchmarks over strong baselines, such as GRPO,
achieving performance gains of more than six percent on Qwen3-4b-Base and about
three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without
additional tricks. These results highlight the importance of architectural
innovations for scalable, efficient algorithms.

</details>


### [297] [Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing](https://arxiv.org/abs/2510.01658)
*Amin Jalali,Milad Soltany,Michael Greenspan,Ali Etemad*

Main category: cs.LG

TL;DR: 提出TimeHUT，一种通过分层均匀性-容忍性平衡学习时间序列表示的新方法，在分类任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了在时间序列表示学习中有效平衡嵌入空间的均匀性与容忍性，提升对时间依赖性的建模能力。

Method: 采用分层结构结合温度调度的对比损失和分层角度边距损失，同时学习实例级和时序信息。

Result: 在128个UCR和30个UAE数据集上分类性能显著超越先前方法，在Yahoo和KPI异常检测任务中表现具有竞争力。

Conclusion: TimeHUT通过平衡均匀性与容忍性，有效提升了时间序列表示的质量，尤其在分类任务中表现出色。

Abstract: We propose TimeHUT, a novel method for learning time-series representations
by hierarchical uniformity-tolerance balancing of contrastive representations.
Our method uses two distinct losses to learn strong representations with the
aim of striking an effective balance between uniformity and tolerance in the
embedding space. First, TimeHUT uses a hierarchical setup to learn both
instance-wise and temporal information from input time-series. Next, we
integrate a temperature scheduler within the vanilla contrastive loss to
balance the uniformity and tolerance characteristics of the embeddings.
Additionally, a hierarchical angular margin loss enforces instance-wise and
temporal contrast losses, creating geometric margins between positive and
negative pairs of temporal sequences. This approach improves the coherence of
positive pairs and their separation from the negatives, enhancing the capture
of temporal dependencies within a time-series sample. We evaluate our approach
on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and
multivariate classification, as well as Yahoo and KPI datasets for anomaly
detection. The results demonstrate that TimeHUT outperforms prior methods by
considerable margins on classification, while obtaining competitive results for
anomaly detection. Finally, detailed sensitivity and ablation studies are
performed to evaluate different components and hyperparameters of our method.

</details>


### [298] [Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value](https://arxiv.org/abs/2510.01663)
*Wangxuan Fan,Ching Wang,Siqi Li,Nan Liu*

Main category: cs.LG

TL;DR: 提出ShapKAN，一种基于Shapley值的剪枝框架，用于在平移不变的情况下评估Kolmogorov-Arnold网络中节点的重要性，从而实现有效的网络压缩和保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络虽然预测能力强，但缺乏可解释性；KAN虽能恢复符号表示，但其结构对基于幅值的剪枝方法敏感，尤其在输入坐标变换时表现不稳定。

Method: 采用Shapley值归因方法来量化每个节点对输出的实际贡献，实现对节点重要性的平移不变评估，并据此进行剪枝。

Result: 在合成和真实数据集上的实验表明，ShapKAN能够准确保留关键节点，有效压缩网络，且重要性排序不受输入参数化影响。

Conclusion: ShapKAN提升了KAN在网络剪枝中的稳定性和可解释性，有助于其在资源受限环境中的部署。

Abstract: For many real-world applications, understanding feature-outcome relationships
is as crucial as achieving high predictive accuracy. While traditional neural
networks excel at prediction, their black-box nature obscures underlying
functional relationships. Kolmogorov--Arnold Networks (KANs) address this by
employing learnable spline-based activation functions on edges, enabling
recovery of symbolic representations while maintaining competitive performance.
However, KAN's architecture presents unique challenges for network pruning.
Conventional magnitude-based methods become unreliable due to sensitivity to
input coordinate shifts. We propose \textbf{ShapKAN}, a pruning framework using
Shapley value attribution to assess node importance in a shift-invariant
manner. Unlike magnitude-based approaches, ShapKAN quantifies each node's
actual contribution, ensuring consistent importance rankings regardless of
input parameterization. Extensive experiments on synthetic and real-world
datasets demonstrate that ShapKAN preserves true node importance while enabling
effective network compression. Our approach improves KAN's interpretability
advantages, facilitating deployment in resource-constrained environments.

</details>


### [299] [Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.01677)
*Han Wu,Yanming Sun,Yunhe Yang,Derek F. Wong*

Main category: cs.LG

TL;DR: 提出了一种自适应门控融合网络（AGFN），通过基于信息熵和模态重要性的双门机制，有效融合多模态信息，提升情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有融合方法难以应对模态质量差异（如噪声、缺失、语义冲突），导致对细微情感识别效果不佳。

Method: 设计了双门控融合机制，在单模态编码和跨模态交互后，根据信息熵和模态重要性自适应调整特征权重，抑制噪声模态影响，突出关键信息。

Result: 在CMU-MOSI和CMU-MOSEI数据集上显著优于强基线模型，具备更强的鲁棒性和对细微情绪的辨别能力；可视化显示特征分布更广，预测误差与特征位置相关性降低。

Conclusion: AGFN能有效提升多模态情感分析的鲁棒性和泛化能力，尤其在处理低质量或冲突模态时表现优越。

Abstract: Multimodal sentiment analysis (MSA) leverages information fusion from diverse
modalities (e.g., text, audio, visual) to enhance sentiment prediction.
However, simple fusion techniques often fail to account for variations in
modality quality, such as those that are noisy, missing, or semantically
conflicting. This oversight leads to suboptimal performance, especially in
discerning subtle emotional nuances. To mitigate this limitation, we introduce
a simple yet efficient \textbf{A}daptive \textbf{G}ated \textbf{F}usion
\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion
mechanism based on information entropy and modality importance. This mechanism
mitigates the influence of noisy modalities and prioritizes informative cues
following unimodal encoding and cross-modal interaction. Experiments on
CMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong
baselines in accuracy, effectively discerning subtle emotions with robust
performance. Visualization analysis of feature representations demonstrates
that AGFN enhances generalization by learning from a broader feature
distribution, achieved by reducing the correlation between feature location and
prediction error, thereby decreasing reliance on specific locations and
creating more robust multimodal feature representations.

</details>


### [300] [PASTA: A Unified Framework for Offline Assortment Learning](https://arxiv.org/abs/2510.01693)
*Juncheng Dong,Weibin Mo,Zhengling Qi,Cong Shi,Ethan X. Fang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 本文提出了一种名为PASTA的悲观主义框架，用于在离线数据驱动场景下解决缺乏先验选择模型知识的组合优化问题，能够在有限样本下实现最优预期收益，并证明了其在多种常用选择模型下的最小最大最优性。


<details>
  <summary>Details</summary>
Motivation: 由于组合优化的特性，历史顾客选择数据往往无法覆盖所有可能的组合，导致传统方法难以保证性能，因此需要一种能在数据覆盖不全的情况下仍有效的方法。

Method: 提出了基于悲观原则的PASTA框架，仅要求离线数据分布中包含最优组合，利用有限的历史数据进行优化，并在多种经典选择模型下分析其理论性能。

Result: 建立了首个针对离线组合优化的有限样本遗憾界，证明了PASTA在样本复杂度和模型复杂度上的最小最大最优性，且数值实验显示其优于现有基线方法。

Conclusion: PASTA框架在弱数据覆盖条件下具有理论保证和实际优越性，为数据驱动的组合优化提供了一种高效且鲁棒的解决方案。

Abstract: We study a broad class of assortment optimization problems in an offline and
data-driven setting. In such problems, a firm lacks prior knowledge of the
underlying choice model, and aims to determine an optimal assortment based on
historical customer choice data. The combinatorial nature of assortment
optimization often results in insufficient data coverage, posing a significant
challenge in designing provably effective solutions. To address this, we
introduce a novel Pessimistic Assortment Optimization (PASTA) framework that
leverages the principle of pessimism to achieve optimal expected revenue under
general choice models. Notably, PASTA requires only that the offline data
distribution contains an optimal assortment, rather than providing the full
coverage of all feasible assortments. Theoretically, we establish the first
finite-sample regret bounds for offline assortment optimization across several
widely used choice models, including the multinomial logit and nested logit
models. Additionally, we derive a minimax regret lower bound, proving that
PASTA is minimax optimal in terms of sample and model complexity. Numerical
experiments further demonstrate that our method outperforms existing baseline
approaches.

</details>


### [301] [Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport](https://arxiv.org/abs/2510.01706)
*Shaan Shah,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 提出了一种基于分层最优传输（HOT）的统一框架，用于神经网络表示的全局对齐，克服了传统方法在对称性、深度不匹配和缺乏整体评分方面的局限。


<details>
  <summary>Details</summary>
Motivation: 传统表示相似性方法逐层独立对齐，忽略全局激活结构，无法处理不同深度网络，且结果不对称、缺乏整体对齐评分。

Method: 引入分层最优传输（HOT），联合推断软的、全局一致的层间耦合与神经元级传输计划，允许源神经元将质量分布到多个目标层，通过最小化总传输成本实现对齐。

Result: 在视觉模型、大语言模型和人脑视觉皮层数据上验证，HOT在对齐质量上优于或匹敌传统方法，并揭示出平滑、细粒度的层次对应关系，能自然处理深度差异。

Conclusion: HOT提供了一种更丰富、可解释的表示比较方式，尤其适用于架构或深度不同的网络，其全局优化能自然涌现结构化对齐模式。

Abstract: Standard representational similarity methods align each layer of a network to
its best match in another independently, producing asymmetric results, lacking
a global alignment score, and struggling with networks of different depths.
These limitations arise from ignoring global activation structure and
restricting mappings to rigid one-to-one layer correspondences. We propose
Hierarchical Optimal Transport (HOT), a unified framework that jointly infers
soft, globally consistent layer-to-layer couplings and neuron-level transport
plans. HOT allows source neurons to distribute mass across multiple target
layers while minimizing total transport cost under marginal constraints. This
yields both a single alignment score for the entire network comparison and a
soft transport plan that naturally handles depth mismatches through mass
distribution. We evaluate HOT on vision models, large language models, and
human visual cortex recordings. Across all domains, HOT matches or surpasses
standard pairwise matching in alignment quality. Moreover, it reveals smooth,
fine-grained hierarchical correspondences: early layers map to early layers,
deeper layers maintain relative positions, and depth mismatches are resolved by
distributing representations across multiple layers. These structured patterns
emerge naturally from global optimization without being imposed, yet are absent
in greedy layer-wise methods. HOT thus enables richer, more interpretable
comparisons between representations, particularly when networks differ in
architecture or depth.

</details>


### [302] [ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning](https://arxiv.org/abs/2510.01712)
*Aidan Acquah,Shing Chan,Aiden Doherty*

Main category: cs.LG

TL;DR: 本文提出了一种基于自监督学习的ActiNet模型，结合隐马尔可夫模型（HMM）对腕部加速度计数据进行人体活动识别（HAR），在大规模流行病学研究中显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高大规模流行病学研究中基于腕部加速度计数据的人体活动识别（HAR）的准确性和可靠性，探索自监督学习与HMM结合是否能有效提升分类性能及其对日常活动强度预测的影响。

Method: 基于151名CAPTURE-24参与者的腕部加速度计数据，训练了一个18层改进型ResNet-V2结构的自监督ActiNet模型，并结合HMM进行标签平滑；使用5折分层分组交叉验证评估模型性能，并与文献中已有的随机森林（RF）+ HMM基线模型进行比较，同时分析不同年龄和性别亚组的表现差异。

Result: ActiNet + HMM模型在活动强度分类中达到平均宏观F1分数0.82和Cohen's kappa系数0.86，优于RF + HMM模型的0.77和0.81，且在不同年龄和性别亚组中表现一致。

Conclusion: ActiNet模型显著提升了HAR任务的性能，建议在未来流行病学研究中采用该模型从腕部加速度计数据中提取活动强度标签。

Abstract: The use of reliable and accurate human activity recognition (HAR) models on
passively collected wrist-accelerometer data is essential in large-scale
epidemiological studies that investigate the association between physical
activity and health outcomes. While the use of self-supervised learning has
generated considerable excitement in improving HAR, it remains unknown the
extent to which these models, coupled with hidden Markov models (HMMs), would
make a tangible improvement to classification performance, and the effect this
may have on the predicted daily activity intensity compositions. Using 151
CAPTURE-24 participants' data, we trained the ActiNet model, a self-supervised,
18-layer, modified ResNet-V2 model, followed by hidden Markov model (HMM)
smoothing to classify labels of activity intensity. The performance of this
model, evaluated using 5-fold stratified group cross-validation, was then
compared to a baseline random forest (RF) + HMM, established in existing
literature. Differences in performance and classification outputs were compared
with different subgroups of age and sex within the Capture-24 population. The
ActiNet model was able to distinguish labels of activity intensity with a mean
macro F1 score of 0.82, and mean Cohen's kappa score of 0.86. This exceeded the
performance of the RF + HMM, trained and validated on the same dataset, with
mean scores of 0.77 and 0.81, respectively. These findings were consistent
across subgroups of age and sex. These findings encourage the use of ActiNet
for the extraction of activity intensity labels from wrist-accelerometer data
in future epidemiological studies.

</details>


### [303] [Latency-aware Multimodal Federated Learning over UAV Networks](https://arxiv.org/abs/2510.01717)
*Shaba Shaon,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 本文研究了无人机（UAV）辅助的联邦多模态学习（FML）框架，旨在最小化系统延迟并提供收敛性分析。通过多模态感知和联合优化UAV的感知调度、功率控制、轨迹规划、资源分配及基站管理，提出了一种高效的迭代优化算法，并证明了在非凸损失函数下的收敛性。实验结果表明，该方法在系统延迟和模型训练性能方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了克服单模态系统的局限性，提升模型准确性和环境理解能力，同时降低联邦学习在无人机网络中的系统延迟。

Method: 采用联邦多模态学习框架，结合块坐标下降和逐次凸逼近技术，联合优化UAV的感知调度、功率控制、轨迹规划、资源分配以及基站资源管理。

Result: 所提方法显著降低了系统延迟，在不同数据设置下均优于现有方法；同时实现了良好的模型训练性能，并提供了理论上的收敛性保证。

Conclusion: 无人机辅助的联邦多模态学习框架能有效提升学习效率与模型性能，所提出的优化算法在复杂非凸问题中具有高效性和实用性。

Abstract: This paper investigates federated multimodal learning (FML) assisted by
unmanned aerial vehicles (UAVs) with a focus on minimizing system latency and
providing convergence analysis. In this framework, UAVs are distributed
throughout the network to collect data, participate in model training, and
collaborate with a base station (BS) to build a global model. By utilizing
multimodal sensing, the UAVs overcome the limitations of unimodal systems,
enhancing model accuracy, generalization, and offering a more comprehensive
understanding of the environment. The primary objective is to optimize FML
system latency in UAV networks by jointly addressing UAV sensing scheduling,
power control, trajectory planning, resource allocation, and BS resource
management. To address the computational complexity of our latency minimization
problem, we propose an efficient iterative optimization algorithm combining
block coordinate descent and successive convex approximation techniques, which
provides high-quality approximate solutions. We also present a theoretical
convergence analysis for the UAV-assisted FML framework under a non-convex loss
function. Numerical experiments demonstrate that our FML framework outperforms
existing approaches in terms of system latency and model training performance
under different data settings.

</details>


### [304] [Accelerating Attention with Basis Decomposition](https://arxiv.org/abs/2510.01718)
*Jialin Zhao*

Main category: cs.LG

TL;DR: 本文提出了BD Attention (BDA)，一种基于基分解的无损注意力算法重构方法，可在无需重新训练的情况下显著加速键值投影并减小权重体积，同时几乎不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 注意力机制在大语言模型和视觉-语言模型中至关重要，但其计算和存储开销较大。现有优化多为系统级I/O优化，缺乏理论上的精确加速方法。因此，需要一种数学上保证、架构无关的无损加速方案。

Method: 通过基分解(Basis Decomposition)中的矩阵恒等式，将多头注意力中的投影重构为紧凑形式，在保持输出完全一致的前提下减少计算和参数量。该方法无需重新训练，仅需少量离线准备时间。

Result: 在DeepSeek-V2-Lite（16B，FP16）上，BDA实现键值投影加速32%，权重减少25%，离线准备仅需4秒；模型困惑度仅上升0.02%（FP16）或0.0004%（FP32），影响可忽略。

Conclusion: BDA是首个理论上精确、无损且与架构无关的注意力加速方法，可与现有的工程优化互补，为高效注意力计算提供了新的算法路径。

Abstract: Attention is a core operation in large language models (LLMs) and
vision-language models (VLMs). We present BD Attention (BDA), the first
lossless algorithmic reformulation of attention. BDA is enabled by a simple
matrix identity from Basis Decomposition (BD), which restructures multi-head
projections into a compact form while preserving exact outputs. Unlike
I/O-aware system optimizations such as FlashAttention, BDA provides a
mathematically guaranteed acceleration that is architecture-agnostic. On
DeepSeek-V2-Lite (16B, FP16), BDA requires only 4s of offline preparation with
no retraining required and, on modern GPUs, achieves 32% faster key/value
projections and 25% smaller weights, while increasing end-to-end perplexity
(PPL) by just 0.02% (FP16) or 0.0004% (FP32), a negligible effect on model
performance. These results position BDA as the first theoretically exact method
for lossless attention acceleration that is complementary to existing
engineering-level optimizations. Our code is available at
https://github.com/abcbdf/basis-decomposition-official.

</details>


### [305] [Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation](https://arxiv.org/abs/2510.01721)
*Saptarshi Mandal,Yashaswini Murthy,R. Srikant*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于线性函数逼近的鲁棒时序差分学习算法，用于解决分布鲁棒强化学习中的策略评估问题，提供了首个在总变差距离和Wasserstein距离不确定性集下的非渐近收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒时序差分学习的收敛性保证大多局限于表格型MDP或依赖严格的折扣因子假设，难以适用于实际中广泛使用的函数逼近方法，因此需要发展更实用且理论支持充分的鲁棒RL算法。

Method: 采用双时间尺度随机逼近更新结合外层目标网络更新的模型无关算法，针对总变差距离和Wasserstein-l距离定义的不确定性集进行鲁棒性建模。

Result: 建立了\~O(1/\epsilon^2)的样本复杂度以获得\epsilon精度的值估计，填补了鲁棒RL在非渐近收敛性方面的理论空白。

Conclusion: 该工作为带函数逼近的鲁棒强化学习提供了首个非渐近收敛保证，弥合了鲁棒RL算法实证成功与其理论支持之间的差距，并可自然扩展到鲁棒Q学习。

Abstract: Distributionally robust reinforcement learning (DRRL) focuses on designing
policies that achieve good performance under model uncertainties. In
particular, we are interested in maximizing the worst-case long-term discounted
reward, where the data for RL comes from a nominal model while the deployed
environment can deviate from the nominal model within a prescribed uncertainty
set. Existing convergence guarantees for robust temporal-difference (TD)
learning for policy evaluation are limited to tabular MDPs or are dependent on
restrictive discount-factor assumptions when function approximation is used. We
present the first robust TD learning with linear function approximation, where
robustness is measured with respect to the total-variation distance and
Wasserstein-l distance uncertainty set. Additionally, our algorithm is both
model-free and does not require generative access to the MDP. Our algorithm
combines a two-time-scale stochastic-approximation update with an outer-loop
target-network update. We establish an $\tilde{O}(1/\epsilon^2)$ sample
complexity to obtain an $\epsilon$-accurate value estimate. Our results close a
key gap between the empirical success of robust RL algorithms and the
non-asymptotic guarantees enjoyed by their non-robust counterparts. The key
ideas in the paper also extend in a relatively straightforward fashion to
robust Q-learning with function approximation.

</details>


### [306] [Workplace Location Choice Model based on Deep Neural Network](https://arxiv.org/abs/2510.01723)
*Tanay Rastogi,Anders Karlström*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度神经网络（DNN）的方法来建模工作地点选择，相较于传统离散选择模型（DCM），DNN在捕捉复杂决策模式方面表现更优，尤其在长距离选择上性能相当或更好，而DCM在短距离和个体特征影响建模上更贴合数据。研究强调应根据具体应用需求选择合适模型。


<details>
  <summary>Details</summary>
Motivation: 传统离散选择模型（DCM）在准确反映个体工作地点选择决策过程方面存在局限，难以充分捕捉复杂的决策行为，因此需要更强大的建模方法来提升预测精度和理解能力。

Method: 采用深度神经网络（DNN）对工作地点选择进行建模，并与传统离散选择模型（DCM）进行比较，评估两者在不同情境下的表现差异。

Result: DNN在整体上展现出作为DCM稳健替代方案的潜力；在长距离工作地点选择上，DNN表现优于或相当于DCM且更贴近数据；而在短距离及个体属性对通勤距离的影响方面，DCM更优。

Conclusion: DNN可作为工作地点选择分析中DCM的有效替代方法，但模型选择应依据具体应用场景的需求权衡，两类模型各有优势领域。

Abstract: Discrete choice models (DCMs) have long been used to analyze workplace
location decisions, but they face challenges in accurately mirroring individual
decision-making processes. This paper presents a deep neural network (DNN)
method for modeling workplace location choices, which aims to better understand
complex decision patterns and provides better results than traditional discrete
choice models (DCMs). The study demonstrates that DNNs show significant
potential as a robust alternative to DCMs in this domain. While both models
effectively replicate the impact of job opportunities on workplace location
choices, the DNN outperforms the DCM in certain aspects. However, the DCM
better aligns with data when assessing the influence of individual attributes
on workplace distance. Notably, DCMs excel at shorter distances, while DNNs
perform comparably to both data and DCMs for longer distances. These findings
underscore the importance of selecting the appropriate model based on specific
application requirements in workplace location choice analysis.

</details>


### [307] [Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD](https://arxiv.org/abs/2510.01744)
*Lea Demelius,Dominik Kowald,Simone Kopeinik,Roman Kern,Andreas Trügler*

Main category: cs.LG

TL;DR: 本文研究了差分隐私随机梯度下降（DPSGD）对模型公平性的影响，发现直接在私有模型上优化超参数虽不能可靠缓解不公平问题，但能改善效用-公平性权衡，同时指出超参数调优会带来额外隐私泄露，并对DPSGD-Global-Adapt变体的有效性提出质疑。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究认为通过在差分隐私模型上直接优化超参数可实现与非私有模型相当的公平性，但该结论的普适性尚不明确，需系统评估其在不同指标和超参数设置下的表现。

Method: 通过比较DPSGD在不同性能指标上的差异影响，并在广泛超参数设置下进行分析，评估直接优化超参数的效果，并扩展至DPSGD-Global-Adapt变体。

Result: 发现某一指标上的差异影响不一定出现在其他指标上；直接在私有模型上优化超参数虽不能稳定缓解差异影响，但可提升效用-公平性权衡；且超参数调优会引入额外隐私泄露；DPSGD-Global-Adapt对超参数选择不鲁棒。

Conclusion: 在差分隐私训练中，应谨慎权衡隐私、效用与公平性，直接针对私有模型调优超参数有一定好处，但并非可靠解决方案，且需注意隐私成本。

Abstract: Differential privacy (DP) is a prominent method for protecting information
about individuals during data analysis. Training neural networks with
differentially private stochastic gradient descent (DPSGD) influences the
model's learning dynamics and, consequently, its output. This can affect the
model's performance and fairness. While the majority of studies on the topic
report a negative impact on fairness, it has recently been suggested that
fairness levels comparable to non-private models can be achieved by optimizing
hyperparameters for performance directly on differentially private models
(rather than re-using hyperparameters from non-private models, as is common
practice). In this work, we analyze the generalizability of this claim by 1)
comparing the disparate impact of DPSGD on different performance metrics, and
2) analyzing it over a wide range of hyperparameter settings. We highlight that
a disparate impact on one metric does not necessarily imply a disparate impact
on another. Most importantly, we show that while optimizing hyperparameters
directly on differentially private models does not mitigate the disparate
impact of DPSGD reliably, it can still lead to improved utility-fairness
trade-offs compared to re-using hyperparameters from non-private models. We
stress, however, that any form of hyperparameter tuning entails additional
privacy leakage, calling for careful considerations of how to balance privacy,
utility and fairness. Finally, we extend our analyses to DPSGD-Global-Adapt, a
variant of DPSGD designed to mitigate the disparate impact on accuracy, and
conclude that this alternative may not be a robust solution with respect to
hyperparameter choice.

</details>


### [308] [Learning Regularization Functionals for Inverse Problems: A Comparative Study](https://arxiv.org/abs/2510.01755)
*Johannes Hertrich,Hok Shing Wong,Alexander Denker,Stanislas Ducotterd,Zhenghan Fang,Markus Haltmeier,Željko Kereta,Erich Kobler,Oscar Leong,Mohammad Sadegh Salehi,Carola-Bibiane Schönlieb,Johannes Schwab,Zakhar Shumaylov,Jeremias Sulam,German Shâma Wache,Martin Zach,Yasi Zhang,Matthias J. Ehrhardt,Sebastian Neumayer*

Main category: cs.LG

TL;DR: 本文提出一个统一的框架，用于整合和比较近年来在图像逆问题中出现的各种学习正则化方法，通过系统性对比分析各方法的优势与局限，并提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 由于现有学习正则化方法在架构设计和训练策略上差异较大，且缺乏模块化实现，导致难以直接比较，因此需要一个统一框架来促进公平评估与深入理解。

Method: 收集并整合现有的代码实现，构建一个统一的开源框架，使不同学习正则化方法可在相同条件下进行系统性比较，并辅以理论描述与实践指导。

Result: 实现了多种主流学习正则化方法的模块化集成，支持公平比较，揭示了各方法在性能、稳定性与泛化能力方面的差异。

Conclusion: 该统一框架有助于推动学习正则化方法的研究与应用，为未来方法开发提供了可复现、可扩展的基础平台。

Abstract: In recent years, a variety of learned regularization frameworks for solving
inverse problems in imaging have emerged. These offer flexible modeling
together with mathematical insights. The proposed methods differ in their
architectural design and training strategies, making direct comparison
challenging due to non-modular implementations. We address this gap by
collecting and unifying the available code into a common framework. This
unified view allows us to systematically compare the approaches and highlight
their strengths and limitations, providing valuable insights into their future
potential. We also provide concise descriptions of each method, complemented by
practical guidelines.

</details>


### [309] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: 提出一种无监督的动态特征选择（DFS）方法，用于增强视觉任务中的潜在表示，通过去除图像中的误导性或冗余信息，提升模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 视觉任务中的潜在表示常受噪声或无关特征影响，导致模型性能和泛化能力下降。

Method: 采用无监督动态特征选择（DFS），在实例级别识别并移除图像中的冗余或误导性特征，保留最相关特征构建潜在空间。

Result: 在多个图像数据集上的实验表明，结合无监督DFS的模型在聚类和图像生成等任务中显著提升了泛化性能，且计算开销增加极小。

Conclusion: 该方法有效增强了潜在表示的质量，具有广泛的适用性，尤其适用于缺乏标注数据的应用场景。

Abstract: Latent representations are critical for the performance and robustness of
machine learning models, as they encode the essential features of data in a
compact and informative manner. However, in vision tasks, these representations
are often affected by noisy or irrelevant features, which can degrade the
model's performance and generalization capabilities. This paper presents a
novel approach for enhancing latent representations using unsupervised Dynamic
Feature Selection (DFS). For each instance, the proposed method identifies and
removes misleading or redundant information in images, ensuring that only the
most relevant features contribute to the latent space. By leveraging an
unsupervised framework, our approach avoids reliance on labeled data, making it
broadly applicable across various domains and datasets. Experiments conducted
on image datasets demonstrate that models equipped with unsupervised DFS
achieve significant improvements in generalization performance across various
tasks, including clustering and image generation, while incurring a minimal
increase in the computational cost.

</details>


### [310] [Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX](https://arxiv.org/abs/2510.01764)
*Waris Radji,Thomas Michel,Hector Piteau*

Main category: cs.LG

TL;DR: Octax是一个基于JAX的高性能经典街机游戏环境套件，提供GPU加速的可扩展强化学习基准，相比传统CPU模拟器实现显著速度提升。


<details>
  <summary>Details</summary>
Motivation: 为强化学习研究提供多样化、可扩展且计算高效的游戏环境，克服现代视频游戏在大规模实验中的计算瓶颈。

Method: 基于CHIP-8模拟器在JAX中实现经典街机游戏，并完全利用GPU进行加速，保持原始游戏机制的精确性。

Result: Octax在多个游戏中实现了比传统CPU模拟器高几个数量级的运行速度，并展示了在训练速度和可扩展性方面的显著提升。

Conclusion: Octax为JAX社区提供了首个端到端的GPU版Atari式基准，是大规模强化学习实验的理想平台。

Abstract: Reinforcement learning (RL) research requires diverse, challenging
environments that are both tractable and scalable. While modern video games may
offer rich dynamics, they are computationally expensive and poorly suited for
large-scale experimentation due to their CPU-bound execution. We introduce
Octax, a high-performance suite of classic arcade game environments implemented
in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely
adopted as a benchmark in RL research. Octax provides the JAX community with a
long-awaited end-to-end GPU alternative to the Atari benchmark, offering
image-based environments, spanning puzzle, action, and strategy genres, all
executable at massive scale on modern GPUs. Our JAX-based implementation
achieves orders-of-magnitude speedups over traditional CPU emulators while
maintaining perfect fidelity to the original game mechanics. We demonstrate
Octax's capabilities by training RL agents across multiple games, showing
significant improvements in training speed and scalability compared to existing
solutions. The environment's modular design enables researchers to easily
extend the suite with new games or generate novel environments using large
language models, making it an ideal platform for large-scale RL
experimentation.

</details>


### [311] [Neural non-canonical Hamiltonian dynamics for long-time simulations](https://arxiv.org/abs/2510.01788)
*Clémentine Courtès,Emmanuel Franck,Michael Kraus,Laurent Navoret,Léopold Trémant*

Main category: cs.LG

TL;DR: 本文研究从数据中学习非典型哈密顿动力学，提出两种训练策略以解决因规范依赖性导致的数值不稳定问题，从而实现长期稳定的模拟。


<details>
  <summary>Details</summary>
Motivation: 为了在学习非典型哈密顿动力学时同时保持模型结构和数值格式的稳定性，解决现有方法结合时出现的数值不稳定问题。

Method: 提出两种训练策略：直接学习向量场或通过数值格式学习时间离散动力学，并结合潜在函数架构与退化变分积分器。

Result: 在多个数值实验中验证了方法的有效性，能够成功学习如陀螺等离子体物理中的引导中心等复杂物理动力学。

Conclusion: 所提出的训练策略有效缓解了规范依赖引起的数值不稳定性，实现了对非典型哈密顿系统长期准确的预测。

Abstract: This work focuses on learning non-canonical Hamiltonian dynamics from data,
where long-term predictions require the preservation of structure both in the
learned model and in numerical schemes. Previous research focused on either
facet, respectively with a potential-based architecture and with degenerate
variational integrators, but new issues arise when combining both. In
experiments, the learnt model is sometimes numerically unstable due to the
gauge dependency of the scheme, rendering long-time simulations impossible. In
this paper, we identify this problem and propose two different training
strategies to address it, either by directly learning the vector field or by
learning a time-discrete dynamics through the scheme. Several numerical test
cases assess the ability of the methods to learn complex physical dynamics,
like the guiding center from gyrokinetic plasma physics.

</details>


### [312] [Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation](https://arxiv.org/abs/2510.01793)
*Adil Koeken,Alexander Ziller,Moritz Knolle,Daniel Rueckert*

Main category: cs.LG

TL;DR: 该研究对用于胸部X光图像合成的隐私过滤管道进行了严格评估，发现现有后处理过滤方法在特异性和一致性方面表现有限，无法可靠检测来自训练数据的近似重复样本，可能给患者隐私带来虚假的安全感。


<details>
  <summary>Details</summary>
Motivation: 验证后处理隐私过滤技术在医疗AI中生成隐私保护合成数据集的有效性。

Method: 对应用于胸部X射线合成的过滤流程进行严格的实证评估，测试其在真实图像和生成图像上的敏感性和特异性。

Result: 现有过滤器仅对真实图像表现出高敏感性，但对生成的近似重复样本检测效果差，且特异性和一致性不足。

Conclusion: 当前的后处理过滤方法不足以有效保护患者隐私，需要在过滤器设计上取得重大进展才能用于敏感场景。

Abstract: The generation of privacy-preserving synthetic datasets is a promising avenue
for overcoming data scarcity in medical AI research. Post-hoc privacy filtering
techniques, designed to remove samples containing personally identifiable
information, have recently been proposed as a solution. However, their
effectiveness remains largely unverified. This work presents a rigorous
evaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary
to claims from the original publications, our results demonstrate that current
filters exhibit limited specificity and consistency, achieving high sensitivity
only for real images while failing to reliably detect near-duplicates generated
from training data. These results demonstrate a critical limitation of post-hoc
filtering: rather than effectively safeguarding patient privacy, these methods
may provide a false sense of security while leaving unacceptable levels of
patient information exposed. We conclude that substantial advances in filter
design are needed before these methods can be confidently deployed in sensitive
applications.

</details>


### [313] [Rethinking the shape convention of an MLP](https://arxiv.org/abs/2510.01796)
*Meng-Hsi Chen,Yu-Ang Lee,Feng-Ting Liao,Da-shan Shiu*

Main category: cs.LG

TL;DR: 提出了一种宽-窄-宽（Hourglass）MLP结构，通过在高维空间中使用跳跃连接、窄瓶颈进行残差计算，并保持固定随机初始化的投影，实现了优于传统窄-宽-窄设计的性能-参数帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 挑战传统的MLP窄-宽-窄结构和跳跃连接设计，探索更高效的残差信息流动方式。

Method: 设计Hourglass MLP块，使跳跃连接在扩展维度上运行，残差路径通过窄瓶颈；输入投影固定为随机初始化，不参与训练。

Result: 在图像生成任务上，Hourglass架构在性能-参数权衡方面 consistently 优于传统设计；随着参数增加，最优结构趋向更深、跳跃连接更宽、瓶颈更窄。

Conclusion: 应重新考虑现代网络中跳跃连接的位置设计，该思想可推广至Transformer等残差网络。

Abstract: Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow
design where skip connections operate at the input/output dimensions while
processing occurs in expanded hidden spaces. We challenge this convention by
proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections
operate at expanded dimensions while residual computation flows through narrow
bottlenecks. This inversion leverages higher-dimensional spaces for incremental
refinement while maintaining computational efficiency through parameter-matched
designs. Implementing Hourglass MLPs requires an initial projection to lift
input signals to expanded dimensions. We propose that this projection can
remain fixed at random initialization throughout training, enabling efficient
training and inference implementations. We evaluate both architectures on
generative tasks over popular image datasets, characterizing
performance-parameter Pareto frontiers through systematic architectural search.
Results show that Hourglass architectures consistently achieve superior Pareto
frontiers compared to conventional designs. As parameter budgets increase,
optimal Hourglass configurations favor deeper networks with wider skip
connections and narrower bottlenecks-a scaling pattern distinct from
conventional MLPs. Our findings suggest reconsidering skip connection placement
in modern architectures, with potential applications extending to Transformers
and other residual networks.

</details>


### [314] [Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction](https://arxiv.org/abs/2510.01817)
*Adam Filipek*

Main category: cs.LG

TL;DR: 本文提出了Sparse Query Attention (SQA)，一种通过减少查询头数量来降低注意力机制计算复杂度的新架构，可在训练和长序列处理中显著提升吞吐量，同时对模型性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 现有的多头注意力机制（如MQA、GQA）虽缓解了内存带宽瓶颈，但未减少注意力分数计算中的浮点运算量（FLOPs），限制了在训练和全序列处理中的扩展性。

Method: 提出Sparse Query Attention (SQA)，通过减少Query头的数量而非Key/Value头，直接降低注意力机制的计算复杂度，并给出其理论基础与多种变体结构。

Result: 在长序列（32k-200k tokens）上的实验表明，SQA在计算受限场景下（如预训练、微调、编码器任务）可实现最高达3倍的吞吐量提升，且初步小规模实验显示对模型质量影响很小。

Conclusion: SQA为构建更高效、可扩展的Transformer模型提供了一条新的优化路径，尤其适用于计算密集型场景，具有广泛的应用潜力。

Abstract: The Transformer architecture, underpinned by the Multi-Head Attention (MHA)
mechanism, has become the de facto standard for state-of-the-art models in
artificial intelligence. However, the quadratic computational complexity of MHA
with respect to sequence length presents a significant barrier to scaling,
particularly for applications involving long contexts. Prevailing solutions,
such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have
effectively addressed the memory bandwidth bottleneck that dominates
autoregressive inference latency by sharing Key and Value projections. While
highly successful, these methods do not reduce the fundamental number of
floating-point operations (FLOPs) required for the attention score computation,
which remains a critical bottleneck for training and full-sequence processing.
This paper introduces Sparse Query Attention (SQA), a novel attention
architecture that pursues an alternative and complementary optimization path.
Instead of reducing Key/Value heads, SQA reduces the number of Query heads.
This architectural modification directly decreases the computational complexity
of the attention mechanism by a factor proportional to the reduction in query
heads, thereby lowering the overall FLOPs. This work presents the theoretical
foundation of SQA, its mathematical formulation, and a family of architectural
variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate
that SQA can achieve significant throughput improvements of up to 3x in
computation-bound scenarios such as model pre-training, fine-tuning, and
encoder-based tasks, with only a minimal impact on model quality in preliminary
smallscale experiments. SQA was discovered serendipitously during the
development of the upcoming Reactive Transformer architecture, suggesting its
potential as a powerful tool for building more efficient and scalable models

</details>


### [315] [Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning](https://arxiv.org/abs/2510.01824)
*Olivier Goudet,Quentin Suire,Adrien Goëffon,Frédéric Saubion,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 提出了一种顺序不变的强化学习框架，用于黑箱组合优化，通过随机生成顺序训练自回归模型，实现对变量顺序的不变性，提升样本效率和搜索多样性。


<details>
  <summary>Details</summary>
Motivation: 传统基于分布估计的算法（EDAs）依赖显式学习变量依赖图，成本高且难以有效捕捉复杂交互关系。

Method: 参数化一个多元自回归生成模型，训练时不固定变量顺序，通过在训练中采样随机生成顺序来增强顺序不变性；采用广义强化策略优化（GRPO）进行稳定的策略梯度更新。

Result: 在多种基准算法和不同规模的问题实例上，该方法通常表现最优，并能持续避免灾难性失败。

Conclusion: 所提框架有效提升了黑箱组合优化中的样本效率和鲁棒性，展示了顺序不变性建模在复杂组合优化中的潜力。

Abstract: We introduce an order-invariant reinforcement learning framework for
black-box combinatorial optimization. Classical estimation-of-distribution
algorithms (EDAs) often rely on learning explicit variable dependency graphs,
which can be costly and fail to capture complex interactions efficiently. In
contrast, we parameterize a multivariate autoregressive generative model
trained without a fixed variable ordering. By sampling random generation orders
during training - a form of information-preserving dropout - the model is
encouraged to be invariant to variable order, promoting search-space diversity
and shaping the model to focus on the most relevant variable dependencies,
improving sample efficiency. We adapt Generalized Reinforcement Policy
Optimization (GRPO) to this setting, providing stable policy-gradient updates
from scale-invariant advantages. Across a wide range of benchmark algorithms
and problem instances of varying sizes, our method frequently achieves the best
performance and consistently avoids catastrophic failures.

</details>


### [316] [Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets](https://arxiv.org/abs/2510.01842)
*Yannis Belkhiter,Seshu Tirupathi,Giulio Zizzo,Sachin Sharma,John D. Kelleher*

Main category: cs.LG

TL;DR: 本文探索了AutoML与预判模型选择的结合，利用传统模型和大语言模型代理，基于数据集描述和统计信息缩小AutoML的搜索空间，从而减少计算开销并有效选择最佳模型。


<details>
  <summary>Details</summary>
Motivation: 现有的AutoML方法依赖于耗时的超参数搜索，缺乏高效的预判机制来提前缩小模型搜索范围。

Method: 结合传统模型和大语言模型（LLM）代理，利用数据集的描述和统计特征进行预判，以缩小AutoML库中的模型搜索空间。

Result: 在包含175个表格分类数据集的AWS AutoGluon基准上验证了方法的有效性，显著降低了计算开销，同时仍能选出最优模型。

Conclusion: 该方法为AutoML工作流提供了新思路，通过预判模型选择提升效率，具有实际应用潜力。

Abstract: The field of AutoML has made remarkable progress in post-hoc model selection,
with libraries capable of automatically identifying the most performing models
for a given dataset. Nevertheless, these methods often rely on exhaustive
hyperparameter searches, where methods automatically train and test different
types of models on the target dataset. Contrastingly, pre-hoc prediction
emerges as a promising alternative, capable of bypassing exhaustive search
through intelligent pre-selection of models. Despite its potential, pre-hoc
prediction remains under-explored in the literature. This paper explores the
intersection of AutoML and pre-hoc model selection by leveraging traditional
models and Large Language Model (LLM) agents to reduce the search space of
AutoML libraries. By relying on dataset descriptions and statistical
information, we reduce the AutoML search space. Our methodology is applied to
the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark
containing 175 tabular classification datasets available on OpenML. The
proposed approach offers a shift in AutoML workflows, significantly reducing
computational overhead, while still selecting the best model for the given
dataset.

</details>


### [317] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: 提出了一种名为Granular-GRPO（G²RPO）的新框架，通过细粒度奖励评估和多尺度优势整合，显著提升了基于流模型的强化学习在对齐人类偏好方面的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法因稀疏且狭窄的奖励信号导致生成结果与人类偏好对齐效果不佳，需要更精确和全面的奖励评估机制。

Method: 引入奇异随机采样策略以增强每步随机探索中奖励与噪声的相关性，并设计多粒度优势整合模块，聚合多个扩散尺度上的优势，实现对采样方向的全面评估。

Result: 在多种奖励模型下（包括域内和跨域评估）实验表明，G²RPO显著优于现有的基于流模型的GRPO基线方法。

Conclusion: G²RPO通过精细化的奖励建模和多尺度优势整合，有效提升了强化学习驱动生成模型在偏好对齐任务中的表现，具有更强的鲁棒性和泛化能力。

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow
models has recently emerged as a promising approach for aligning generative
models with human preferences. Stochastic sampling via Stochastic Differential
Equations (SDE) is employed during the denoising process to generate diverse
denoising directions for RL exploration. While existing methods effectively
explore potential high-value samples, they suffer from sub-optimal preference
alignment due to sparse and narrow reward signals. To address these challenges,
we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves
precise and comprehensive reward assessments of sampling directions in
reinforcement learning of flow models. Specifically, a Singular Stochastic
Sampling strategy is introduced to support step-wise stochastic exploration
while enforcing a high correlation between the reward and the injected noise,
thereby facilitating a faithful reward for each SDE perturbation. Concurrently,
to eliminate the bias inherent in fixed-granularity denoising, we introduce a
Multi-Granularity Advantage Integration module that aggregates advantages
computed at multiple diffusion scales, producing a more comprehensive and
robust evaluation of the sampling directions. Experiments conducted on various
reward models, including both in-domain and out-of-domain evaluations,
demonstrate that our $\text{G}^2$RPO significantly outperforms existing
flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [318] [Learning Representations Through Contrastive Neural Model Checking](https://arxiv.org/abs/2510.01853)
*Vladimir Krsmanovic,Matthias Cosler,Mohamed Ghanem,Bernd Finkbeiner*

Main category: cs.LG

TL;DR: 提出了一种名为Contrastive Neural Model Checking (CNML)的新方法，利用模型检查任务作为学习逻辑规范与系统对齐表示的指导信号，在检索任务中显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在形式验证中已有应用，但表示学习在该领域仍较少探索，尤其是在形式化语言和系统之间建立对齐表示的需求尚未充分满足。

Method: 通过自监督的对比学习目标，将逻辑规范和系统联合嵌入到共享的潜在空间中，以模型检查任务作为学习信号。

Result: 在受工业启发的跨模态和单模态检索任务上，CNML明显优于算法和神经基线方法；学习到的表示可有效迁移至下游任务，并能泛化到更复杂的公式。

Conclusion: 模型检查可作为学习形式语言表示的有效目标，CNML为形式验证中的表示学习提供了新方向。

Abstract: Model checking is a key technique for verifying safety-critical systems
against formal specifications, where recent applications of deep learning have
shown promise. However, while ubiquitous for vision and language domains,
representation learning remains underexplored in formal verification. We
introduce Contrastive Neural Model Checking (CNML), a novel method that
leverages the model checking task as a guiding signal for learning aligned
representations. CNML jointly embeds logical specifications and systems into a
shared latent space through a self-supervised contrastive objective. On
industry-inspired retrieval tasks, CNML considerably outperforms both
algorithmic and neural baselines in cross-modal and intra-modal settings.We
further show that the learned representations effectively transfer to
downstream tasks and generalize to more complex formulas. These findings
demonstrate that model checking can serve as an objective for learning
representations for formal languages.

</details>


### [319] [Explicit Discovery of Nonlinear Symmetries from Dynamic Data](https://arxiv.org/abs/2510.01855)
*Lexiang Hu,Yikang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文提出了LieNLSD方法，首次能够确定具有非线性项的无穷小生成元的数量及其显式表达式，通过求解系数矩阵实现对非线性对称性的发现，并在多个动态系统和顶夸克标记任务中展现出优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 现有对称性发现方法大多局限于线性对称性，且近期非线性对称性发现方法无法显式获得李代数子空间，因此需要一种能有效发现并表达非线性对称性的新方法。

Method: 构建一个用于无穷小群作用的函数库，推导其延长公式对系数矩阵呈线性关系，利用数据的中心差分和训练好的神经网络雅可比矩阵代入无穷小判据，形成关于系数矩阵的线性方程组，并通过SVD求解。

Result: LieNLSD在顶夸克标记和多个动态系统上表现出对现有方法的定性优势，应用于指导数据增强时，使神经PDE求解器的长 rollout 准确率提升超过20%。

Conclusion: LieNLSD是首个可显式发现非线性无穷小生成元数量及表达式的对称性发现方法，在复杂系统中具有良好的应用潜力和性能提升。

Abstract: Symmetry is widely applied in problems such as the design of equivariant
networks and the discovery of governing equations, but in complex scenarios, it
is not known in advance. Most previous symmetry discovery methods are limited
to linear symmetries, and recent attempts to discover nonlinear symmetries fail
to explicitly get the Lie algebra subspace. In this paper, we propose LieNLSD,
which is, to our knowledge, the first method capable of determining the number
of infinitesimal generators with nonlinear terms and their explicit
expressions. We specify a function library for the infinitesimal group action
and aim to solve for its coefficient matrix, proving that its prolongation
formula for differential equations, which governs dynamic data, is also linear
with respect to the coefficient matrix. By substituting the central differences
of the data and the Jacobian matrix of the trained neural network into the
infinitesimal criterion, we get a system of linear equations for the
coefficient matrix, which can then be solved using SVD. On top quark tagging
and a series of dynamic systems, LieNLSD shows qualitative advantages over
existing methods and improves the long rollout accuracy of neural PDE solvers
by over 20% while applying to guide data augmentation. Code and data are
available at https://github.com/hulx2002/LieNLSD.

</details>


### [320] [Compositional meta-learning through probabilistic task inference](https://arxiv.org/abs/2510.01858)
*Jacob J. W. Bakermans,Pablo Tano,Reidar Riveland,Charles Findling,Alexandre Pouget*

Main category: cs.LG

TL;DR: 提出了一种组合式元学习模型，通过将任务表示为可重用计算的结构化组合，并利用生成模型捕捉任务间的共享组件和统计特性，实现了无需参数更新的快速新任务求解。


<details>
  <summary>Details</summary>
Motivation: 为了从极少的经验中解决新任务，需要有效复用先前任务的知识，即元学习问题。现有方法在数据效率和灵活性上存在不足，因此需要一种更具组合性和数据高效性的元学习方法。

Method: 设计了一个生成模型来显式表示任务为可重用计算的结构化组合，通过跨任务共享的底层组件及其统计特性建模，将新任务的学习转化为概率推断问题，采用高度约束的假设检验寻找解决方案，无需梯度更新。

Result: 模型在规则学习和运动学习任务中成功恢复了真实的基础组件和统计信息，并能仅凭单个示例快速推断出新任务的解决方案。

Conclusion: 该框架结合了神经网络的表达能力和概率推断的数据效率，实现了快速、组合式的元学习，具有良好的泛化性和样本效率。

Abstract: To solve a new task from minimal experience, it is essential to effectively
reuse knowledge from previous tasks, a problem known as meta-learning.
Compositional solutions, where common elements of computation are flexibly
recombined into new configurations, are particularly well-suited for
meta-learning. Here, we propose a compositional meta-learning model that
explicitly represents tasks as structured combinations of reusable
computations. We achieve this by learning a generative model that captures the
underlying components and their statistics shared across a family of tasks.
This approach transforms learning a new task into a probabilistic inference
problem, which allows for finding solutions without parameter updates through
highly constrained hypothesis testing. Our model successfully recovers ground
truth components and statistics in rule learning and motor learning tasks. We
then demonstrate its ability to quickly infer new solutions from just single
examples. Together, our framework joins the expressivity of neural networks
with the data-efficiency of probabilistic inference to achieve rapid
compositional meta-learning.

</details>


### [321] [Universal Dynamic Regret and Constraint Violation Bounds for Constrained Online Convex Optimization](https://arxiv.org/abs/2510.01867)
*Subhamon Supantha,Abhishek Sinha*

Main category: cs.LG

TL;DR: 提出两种具有模块化结构的算法，用于在线凸优化框架下的对抗性约束问题，通过构造代理成本函数将约束学习问题转化为标准OCO问题，实现了通用的动态遗憾和累积约束违反界，改进了现有最先进结果。


<details>
  <summary>Details</summary>
Motivation: 在没有共同可行点且成本与约束函数由对手任意选择的最一般情况下，现有方法难以有效处理在线凸优化中的动态遗憾和约束违反问题。

Method: 设计了两种简单的模块化算法，通过引入特殊构造的代理成本函数，将带约束的学习问题转化为标准的在线凸优化（OCO）问题。

Result: 算法在动态遗憾和累积约束违反方面均取得了与当前最优结果相比更优的理论界，适用于成本和约束函数均为任意选择的最一般情形。

Conclusion: 所提方法通过代理成本函数的转换策略，成功扩展了标准OCO框架的应用范围，在无共同可行解的对抗性约束环境下仍能保证良好的性能表现。

Abstract: We consider a generalization of the celebrated Online Convex Optimization
(OCO) framework with online adversarial constraints. We present two algorithms
having simple modular structures that yield universal dynamic regret and
cumulative constraint violation bounds, improving upon the state-of-the-art
results. Our results hold in the most general case when both the cost and
constraint functions are chosen arbitrarily by an adversary, and the constraint
functions need not contain any common feasible point. The results are
established by reducing the constrained learning problem to an instance of the
standard OCO problem with specially constructed surrogate cost functions.

</details>


### [322] [Randomized Gradient Subspaces for Efficient Large Language Model Training](https://arxiv.org/abs/2510.01878)
*Sahar Rajabi,Nayeema Nonta,Samanvay Vajpayee,Sirisha Rambhatla*

Main category: cs.LG

TL;DR: 本文研究了梯度空间的动态特性及其子空间结构，发现虽然小维度子空间能捕获大部分梯度能量，但残差部分仍不可忽略，且核心子空间的影响随训练进行和网络加深而减弱。基于这些观察，作者提出了GrassWalk和GrassJump两种随机算法，在显著降低内存消耗的同时提升了LLaMA-1B和LLaMA-7B预训练的性能。


<details>
  <summary>Details</summary>
Motivation: 训练大语言模型时，优化器状态导致的内存开销极大，限制了模型规模。现有方法通过将梯度投影到低维子空间来缓解这一问题，但缺乏对梯度空间动态特性和几何结构的深入理解。因此，本文旨在分析梯度子空间的有效性与演化规律，并设计更高效的优化算法。

Method: 通过分析梯度空间的能量分布、子空间演化及曲率特性，揭示其低秩结构与近平坦几何性质；基于此提出两种基于随机投影的优化算法GrassWalk和GrassJump，利用低维子空间更新机制减少内存占用。

Result: 实验表明，尽管低维子空间能捕获主要梯度能量，但残差部分仍包含重要信息；核心子空间的重要性随训练进程和网络深度增加而下降；梯度空间具有近平坦曲率。所提算法在LLaMA-1B和LLaMA-7B上实现了最先进的内存节省，并提升了训练性能。

Conclusion: 梯度空间存在可利用的低维结构，但其动态变化和几何特性需被充分考虑。提出的GrassWalk和GrassJump算法有效平衡了内存效率与模型性能，为大规模语言模型训练提供了新的优化范式。

Abstract: Training large language models (LLMs) is often bottlenecked by extreme memory
demands, with optimizer states dominating the footprint. Recent works mitigates
this cost by projecting gradients into low-dimensional subspaces using
sophisticated update strategies. In this paper, we analyze the dynamics of
gradient space and its underlying subspaces. We find that while a small
subspace captures most gradient energy, a significant portion still resides in
the residual bulk; moreover, the influence of the core subspace diminishes over
time and in deeper layers. We also observe that the gradient space exhibits
near-flat curvature, calling for algorithms that explicitly account for this
geometry. Motivated by these insights, we introduce a suite of randomized
algorithms, GrassWalk and GrassJump, which exploit subspace and achieve
state-of-the-art memory savings while improving performance on LLaMA-1B and
LLaMA-7B pretraining.

</details>


### [323] [Multi-marginal temporal Schrödinger Bridge Matching for video generation from unpaired data](https://arxiv.org/abs/2510.01894)
*Thomas Gravier,Thomas Boyer,Auguste Genovesio*

Main category: cs.LG

TL;DR: 本文提出了多边际时间薛定谔桥匹配（MMtSBM）方法，用于从非配对数据生成视频，并在高维场景下实现了对隐藏动态的有效恢复。


<details>
  <summary>Details</summary>
Motivation: 现有的时间演化重建方法在高维情况下扩展性差且假设过于严格，难以应用于复杂的自然动态过程分析。

Method: 提出多边际时间薛定谔桥匹配（MMtSBM），通过将迭代马尔可夫拟合算法以新颖的因子化方式推广到多个边缘分布，扩展了扩散薛定谔桥匹配的理论保证和实证效率。

Result: MMtSBM在玩具示例上保持了理论性质，在真实世界数据集（如100维转录组轨迹推断）上达到最先进性能，并首次在高维图像设置中成功恢复了耦合关系和动态演化。

Conclusion: 多边际薛定谔桥为从静态数据中恢复隐藏动态提供了一种实用且有理论基础的方法。

Abstract: Many natural dynamic processes -- such as in vivo cellular differentiation or
disease progression -- can only be observed through the lens of static sample
snapshots. While challenging, reconstructing their temporal evolution to
decipher underlying dynamic properties is of major interest to scientific
research. Existing approaches enable data transport along a temporal axis but
are poorly scalable in high dimension and require restrictive assumptions to be
met. To address these issues, we propose \textit{\textbf{Multi-Marginal
temporal Schr\"odinger Bridge Matching}} (\textbf{MMtSBM}) \textit{for video
generation from unpaired data}, extending the theoretical guarantees and
empirical efficiency of Diffusion Schr\"odinger Bridge Matching
(arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting
algorithm to multiple marginals in a novel factorized fashion. Experiments show
that MMtSBM retains theoretical properties on toy examples, achieves
state-of-the-art performance on real world datasets such as transcriptomic
trajectory inference in 100 dimensions, and for the first time recovers
couplings and dynamics in very high dimensional image settings. Our work
establishes multi-marginal Schr\"odinger bridges as a practical and principled
approach for recovering hidden dynamics from static data.

</details>


### [324] [Test-Time Anchoring for Discrete Diffusion Posterior Sampling](https://arxiv.org/abs/2510.02291)
*Litu Rout,Andreas Lugmayr,Yasamin Jafarian,Srivatsan Varadharajan,Constantine Caramanis,Sanjay Shakkottai,Ira Kemelmacher-Shlizerman*

Main category: cs.LG

TL;DR: 提出了一种基于预训练离散扩散模型的锚定后验采样方法（APS），用于从噪声测量中恢复图像，无需重新训练特定任务模型，在线性和非线性反问题上达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型虽具统一建模文本与图像的潜力，但在后验采样中面临梯度信号稀疏、连续松弛限制和高维采样效率低等问题，需更有效的采样方法。

Method: 提出锚定后验采样（APS），引入量化期望以在离散嵌入空间提供类梯度引导，并设计锚定重遮蔽机制实现自适应解码。

Result: 在标准基准的线性和非线性反问题上，APS在离散扩散采样器中实现了最先进的性能，并成功应用于无需训练的风格化和文本引导编辑任务。

Conclusion: APS有效克服了现有离散扩散后验采样的局限性，为基于离散扩散模型的训练-free贝叶斯推断提供了高效且通用的解决方案。

Abstract: We study the problem of posterior sampling using pretrained discrete
diffusion foundation models, aiming to recover images from noisy measurements
without retraining task-specific models. While diffusion models have achieved
remarkable success in generative modeling, most advances rely on continuous
Gaussian diffusion. In contrast, discrete diffusion offers a unified framework
for jointly modeling categorical data such as text and images. Beyond
unification, discrete diffusion provides faster inference, finer control, and
principled training-free Bayesian inference, making it particularly well-suited
for posterior sampling. However, existing approaches to discrete diffusion
posterior sampling face severe challenges: derivative-free guidance yields
sparse signals, continuous relaxations limit applicability, and split Gibbs
samplers suffer from the curse of dimensionality. To overcome these
limitations, we introduce Anchored Posterior Sampling (APS) for masked
diffusion foundation models, built on two key innovations -- quantized
expectation for gradient-like guidance in discrete embedding space, and
anchored remasking for adaptive decoding. Our approach achieves
state-of-the-art performance among discrete diffusion samplers across linear
and nonlinear inverse problems on the standard benchmarks. We further
demonstrate the benefits of our approach in training-free stylization and
text-guided editing.

</details>


### [325] [Multimodal Foundation Models for Early Disease Detection](https://arxiv.org/abs/2510.01899)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: cs.LG

TL;DR: 提出一种基于注意力机制的多模态基础模型，整合电子健康记录、医学影像、基因数据和可穿戴设备信息，实现跨模态关联分析，支持预训练与快速适应新疾病，提升早期疾病诊断准确性与临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统诊断模型通常孤立分析不同来源的医疗数据，难以捕捉对早期诊断至关重要的跨模态关联，限制了诊断性能。

Method: 采用基于注意力机制的Transformer框架，为每种数据模态设计专用编码器，将其映射到共享潜在空间，并通过多头注意力机制和残差归一化进行融合，支持多任务预训练和快速迁移学习。

Result: 在肿瘤学、心脏病学和神经病学的基准数据集上验证了模型在早期检测任务中的有效性，展现出良好的适应性、预测准确性和临床可解释性。

Conclusion: 该多模态基础模型有望成为精准诊断的统一框架，提升疾病预测性能，并辅助临床决策。

Abstract: Healthcare generates diverse streams of data, including electronic health
records (EHR), medical imaging, genetics, and ongoing monitoring from wearable
devices. Traditional diagnostic models frequently analyze these sources in
isolation, which constrains their capacity to identify cross-modal correlations
essential for early disease diagnosis. Our research presents a multimodal
foundation model that consolidates diverse patient data through an
attention-based transformer framework. At first, dedicated encoders put each
modality into a shared latent space. Then, they combine them using multi-head
attention and residual normalization. The architecture is made for pretraining
on many tasks, which makes it easy to adapt to new diseases and datasets with
little extra work. We provide an experimental strategy that uses benchmark
datasets in oncology, cardiology, and neurology, with the goal of testing early
detection tasks. The framework includes data governance and model management
tools in addition to technological performance to improve transparency,
reliability, and clinical interpretability. The suggested method works toward a
single foundation model for precision diagnostics, which could improve the
accuracy of predictions and help doctors make decisions.

</details>


### [326] [A Methodology for Transparent Logic-Based Classification Using a Multi-Task Convolutional Tsetlin Machine](https://arxiv.org/abs/2510.01906)
*Mayur Kishor Shende,Ole-Christoffer Granmo,Runar Helin,Vladimir I. Zadorozhny,Rishad Shafik*

Main category: cs.LG

TL;DR: 本文探索了Tsetlin机器（TM）在大规模多通道图像分类中的应用，提出了一种生成局部解释和全局类别表示的方法，并在MNIST和CelebA数据集上验证了其性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了提升Tsetlin机器在复杂、大规模图像分类任务中的适用性，并增强模型的可解释性，研究如何利用其基于逻辑的结构进行有效学习。

Method: 采用卷积Tsetlin机器架构，设计生成局部预测解释和全局类别模式表示的方法，并将卷积子句的知识可视化为图像。

Result: 在MNIST上达到98.5%准确率，在CelebA上获得86.56% F1分数（接近ResNet50的88.07%），同时保持良好的可解释性。

Conclusion: Tsetlin机器在复杂图像任务中表现具有竞争力，且保持高可解释性，有助于理解其内部逻辑并拓展至更多样化的数据集。

Abstract: The Tsetlin Machine (TM) is a novel machine learning paradigm that employs
finite-state automata for learning and utilizes propositional logic to
represent patterns. Due to its simplistic approach, TMs are inherently more
interpretable than learning algorithms based on Neural Networks. The
Convolutional TM has shown comparable performance on various datasets such as
MNIST, K-MNIST, F-MNIST and CIFAR-2. In this paper, we explore the
applicability of the TM architecture for large-scale multi-channel (RGB) image
classification. We propose a methodology to generate both local interpretations
and global class representations. The local interpretations can be used to
explain the model predictions while the global class representations aggregate
important patterns for each class. These interpretations summarize the
knowledge captured by the convolutional clauses, which can be visualized as
images. We evaluate our methods on MNIST and CelebA datasets, using models that
achieve 98.5\% accuracy on MNIST and 86.56\% F1-score on CelebA (compared to
88.07\% for ResNet50) respectively. We show that the TM performs competitively
to this deep learning model while maintaining its interpretability, even in
large-scale complex training environments. This contributes to a better
understanding of TM clauses and provides insights into how these models can be
applied to more complex and diverse datasets.

</details>


### [327] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: 提出了一种名为概念神经元选择（CNS）的新学习策略，用于在扩散模型中进行持续学习下的个性化，具有高效、低遗忘和少参数调整的优势。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，增量更新扩散模型具有实用性，但面临计算挑战，且需避免灾难性遗忘并保持零样本生成能力。

Method: 通过识别与目标概念密切相关的神经元，以增量方式微调这些概念神经元，并联合保留先前概念的知识。

Result: 在真实数据集上验证了CNS在单概念和多概念个性化任务中均达到最先进性能，参数调整极少，且实现无融合操作，降低内存和计算开销。

Conclusion: CNS是一种高效、可扩展的扩散模型持续个性化方法，在保持原有生成能力的同时显著减少遗忘和资源消耗。

Abstract: Updating diffusion models in an incremental setting would be practical in
real-world applications yet computationally challenging. We present a novel
learning strategy of Concept Neuron Selection (CNS), a simple yet effective
approach to perform personalization in a continual learning scheme. CNS
uniquely identifies neurons in diffusion models that are closely related to the
target concepts. In order to mitigate catastrophic forgetting problems while
preserving zero-shot text-to-image generation ability, CNS finetunes concept
neurons in an incremental manner and jointly preserves knowledge learned of
previous concepts. Evaluation of real-world datasets demonstrates that CNS
achieves state-of-the-art performance with minimal parameter adjustments,
outperforming previous methods in both single and multi-concept personalization
works. CNS also achieves fusion-free operation, reducing memory storage and
processing time for continual personalization.

</details>


### [328] [Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement](https://arxiv.org/abs/2510.01910)
*Zhaoyan Wang,Zheng Gao,Arogya Kharel,In-Young Ko*

Main category: cs.LG

TL;DR: 本文进行了首次针对图神经网络在复合缺陷下的鲁棒性实证研究，比较了传统方法与基于大语言模型（LLM）的增强方法，并提出了一种新的迭代式检索增强对比 refine 框架 RoGRAD，显著提升了图学习的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在面对真实场景中多种复合缺陷时性能下降严重，且缺乏对传统方法与LLM增强方法在综合缺陷下表现的系统性比较，同时当前LLM增强方法多为一次性静态增强，缺乏动态优化机制。

Method: 提出RoGRAD框架，采用检索增强生成（RAG）技术，通过类一致且多样化的检索增强数据，在迭代过程中结合图对比学习实现动态表示优化，提升模型鲁棒性。

Result: 实验表明RoGRAD在多种缺陷场景下优于传统GNN和现有LLM增强方法，平均性能提升高达82.43%。

Conclusion: LLM增强并非在所有情况下都优于传统方法，而所提出的迭代式检索增强对比学习框架RoGRAD能更有效地提升图神经网络在复杂缺陷下的鲁棒性。

Abstract: Graph Neural Networks (GNNs) are widely adopted in Web-related applications,
serving as a core technique for learning from graph-structured data, such as
text-attributed graphs. Yet in real-world scenarios, such graphs exhibit
deficiencies that substantially undermine GNN performance. While prior
GNN-based augmentation studies have explored robustness against individual
imperfections, a systematic understanding of how graph-native and Large
Language Models (LLMs) enhanced methods behave under compound deficiencies is
still missing. Specifically, there has been no comprehensive investigation
comparing conventional approaches and recent LLM-on-graph frameworks, leaving
their merits unclear. To fill this gap, we conduct the first empirical study
that benchmarks these two lines of methods across diverse graph deficiencies,
revealing overlooked vulnerabilities and challenging the assumption that LLM
augmentation is consistently superior. Building on empirical findings, we
propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement
(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is
the first iterative paradigm that leverages Retrieval-Augmented Generation
(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,
diverse augmentations and enforcing discriminative representations through
iterative graph contrastive learning. It transforms LLM augmentation for graphs
from static signal injection into dynamic refinement. Extensive experiments
demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced
baselines, achieving up to 82.43% average improvement.

</details>


### [329] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: 提出了一种基于平衡动力学的生成建模框架Equilibrium Matching (EqM)，通过学习隐式能量景观的平衡梯度，实现优化驱动的采样，在生成性能和理论支持上优于传统扩散和流模型。


<details>
  <summary>Details</summary>
Motivation: 传统扩散和流模型依赖非平衡、时间相关的动力学，限制了采样灵活性和理论解释性，EqM旨在从平衡态视角构建更高效、灵活且理论坚实的生成框架。

Method: 学习隐式能量景观的平衡梯度，采用基于优化的采样方法（如梯度下降），支持可调步长、自适应优化器和自适应计算。

Result: 在ImageNet 256×256上FID达到1.90，超越扩散/流模型；支持图像去噪、OOD检测、图像合成等任务；理论上可正确学习和采样数据流形。

Conclusion: EqM通过统一的平衡景观替代时间依赖速度，架起了流模型与能量模型之间的桥梁，提供了简洁的优化驱动推断路径，具有更强的灵活性和应用潜力。

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework
built from an equilibrium dynamics perspective. EqM discards the
non-equilibrium, time-conditional dynamics in traditional diffusion and
flow-based generative models and instead learns the equilibrium gradient of an
implicit energy landscape. Through this approach, we can adopt an
optimization-based sampling process at inference time, where samples are
obtained by gradient descent on the learned landscape with adjustable step
sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation
performance of diffusion/flow models empirically, achieving an FID of 1.90 on
ImageNet 256$\times$256. EqM is also theoretically justified to learn and
sample from the data manifold. Beyond generation, EqM is a flexible framework
that naturally handles tasks including partially noised image denoising, OOD
detection, and image composition. By replacing time-conditional velocities with
a unified equilibrium landscape, EqM offers a tighter bridge between flow and
energy-based models and a simple route to optimization-driven inference.

</details>


### [330] [StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold](https://arxiv.org/abs/2510.01938)
*Zhizhong Li,Sina Sajadmanesh,Jingtao Li,Lingjuan Lyu*

Main category: cs.LG

TL;DR: 提出一种几何感知的LoRA扩展方法，通过三因子分解和Stiefel流形约束提升低秩适配性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在利用低秩流形几何结构方面不足，导致性能落后于全量微调。

Method: 采用三因子分解$U\!SV^\top$，将输入输出子空间与缩放因子分离，并约束$U$和$V$位于Stiefel流形上，使用几何优化方法实现高效子空间学习。

Result: 在常识推理、数学与代码生成、图像分类与生成等多个下游任务中，性能优于现有的LoRA变体。

Conclusion: 该方法有效提升了LoRA的性能，兼顾效率与兼容性，具有广泛的应用前景。

Abstract: Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient
technique for fine-tuning large-scale pre-trained models. However, it still
lags behind full fine-tuning in performance, partly due to its insufficient
exploitation of the geometric structure underlying low-rank manifolds. In this
paper, we propose a geometry-aware extension of LoRA that uses a three-factor
decomposition $U\!SV^\top$. Analogous to the structure of singular value
decomposition (SVD), it separates the adapter's input and output subspaces, $V$
and $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie
on the Stiefel manifold, ensuring their orthonormality throughout the training.
To optimize on the Stiefel manifold, we employ a flexible and modular geometric
optimization design that converts any Euclidean optimizer to a Riemannian one.
It enables efficient subspace learning while remaining compatible with existing
fine-tuning pipelines. Empirical results across a wide range of downstream
tasks, including commonsense reasoning, math and code generation, image
classification, and image generation, demonstrate the superior performance of
our approach against the recent state-of-the-art variants of LoRA. Code is
available at https://github.com/SonyResearch/stella.

</details>


### [331] [Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions](https://arxiv.org/abs/2510.01969)
*Camilo Andrés García Trillos,Nicolás García Trillos*

Main category: cs.LG

TL;DR: 本文研究了多类分类中的对抗性鲁棒性问题，提出了适用于任意损失函数的对偶和重心重构方法，扩展了传统0-1损失的结果。


<details>
  <summary>Details</summary>
Motivation: 现有对抗鲁棒性研究主要集中在0-1损失，缺乏对其他重要损失函数（如交叉熵）的系统分析，限制了在实际分类任务中的应用。

Method: 通过引入对偶性和广义重心问题重构对抗风险最小化问题，利用KL散度和Tsallis熵作为惩罚项，推导出多种常见损失函数下的显式表征。

Result: 给出了交叉熵、幂形式损失和二次损失等情形下的明确表达式，实现了对抗风险下界的更紧估计，并揭示了对抗鲁棒性与α-公平打包、广义重心问题之间的理论联系。

Conclusion: 所提框架为超越0-1损失的对抗鲁棒分类器设计提供了新工具，支持更广泛的损失函数并促进高效算法开发。

Abstract: We consider adversarially robust classification in a multiclass setting under
arbitrary loss functions and derive dual and barycentric reformulations of the
corresponding learner-agnostic robust risk minimization problem. We provide
explicit characterizations for important cases such as the cross-entropy loss,
loss functions with a power form, and the quadratic loss, extending in this way
available results for the 0-1 loss. These reformulations enable efficient
computation of sharp lower bounds for adversarial risks and facilitate the
design of robust classifiers beyond the 0-1 loss setting. Our paper uncovers
interesting connections between adversarial robustness, $\alpha$-fair packing
problems, and generalized barycenter problems for arbitrary positive measures
where Kullback-Leibler and Tsallis entropies are used as penalties. Our
theoretical results are accompanied with illustrative numerical experiments
where we obtain tighter lower bounds for adversarial risks with the
cross-entropy loss function.

</details>


### [332] [Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.01970)
*Yuanyuan Yao,Yuhan Shi,Lu Chen,Ziquan Fang,Yunjun Gao,Leong Hou U,Yushuai Li,Tianyi Li*

Main category: cs.LG

TL;DR: 提出了一种基于模态转换的多变量时间序列异常检测框架Moon，通过将数值数据转换为图像表示并结合多模态CNN进行特征融合，提升了检测效率、准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在无监督、半监督和监督学习中存在误差阈值依赖、异常标签利用不足、局部关系捕捉能力差及计算成本高等问题，需要更高效准确且可解释的方法。

Method: 提出Moon框架，采用多变量马尔可夫转移场（MV-MTF）将时间序列转为图像，并设计多模态CNN融合数值与图像特征，结合SHAP进行异常解释。

Result: 在六个真实数据集上实验表明，Moon相比现有最优方法最高提升93%效率、4%准确率和10.8%可解释性性能。

Conclusion: Moon在多变量时间序列异常检测中实现了更高的效率、准确性和可解释性，有效克服了现有方法的局限性。

Abstract: Multivariate time series (MTS) anomaly detection identifies abnormal patterns
where each timestamp contains multiple variables. Existing MTS anomaly
detection methods fall into three categories: reconstruction-based,
prediction-based, and classifier-based methods. However, these methods face two
key challenges: (1) Unsupervised learning methods, such as reconstruction-based
and prediction-based methods, rely on error thresholds, which can lead to
inaccuracies; (2) Semi-supervised methods mainly model normal data and often
underuse anomaly labels, limiting detection of subtle anomalies;(3) Supervised
learning methods, such as classifier-based approaches, often fail to capture
local relationships, incur high computational costs, and are constrained by the
scarcity of labeled data. To address these limitations, we propose Moon, a
supervised modality conversion-based multivariate time series anomaly detection
framework. Moon enhances the efficiency and accuracy of anomaly detection while
providing detailed anomaly analysis reports. First, Moon introduces a novel
multivariate Markov Transition Field (MV-MTF) technique to convert numeric time
series data into image representations, capturing relationships across
variables and timestamps. Since numeric data retains unique patterns that
cannot be fully captured by image conversion alone, Moon employs a
Multimodal-CNN to integrate numeric and image data through a feature fusion
model with parameter sharing, enhancing training efficiency. Finally, a
SHAP-based anomaly explainer identifies key variables contributing to
anomalies, improving interpretability. Extensive experiments on six real-world
MTS datasets demonstrate that Moon outperforms six state-of-the-art methods by
up to 93% in efficiency, 4% in accuracy and, 10.8% in interpretation
performance.

</details>


### [333] [Private Federated Multiclass Post-hoc Calibration](https://arxiv.org/abs/2510.01987)
*Samuel Maddock,Graham Cormode,Carsten Maple*

Main category: cs.LG

TL;DR: 本文研究了联邦学习中的模型校准问题，提出了适用于联邦环境的后处理校准方法，并探讨了差分隐私对校准精度的影响。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，由于数据分布在多个客户端且不能集中，如何在保护隐私的同时实现模型的概率校准成为一个被忽视的重要问题。

Method: 将传统的集中式校准方法（如直方图分箱和温度缩放）迁移到联邦学习环境中，并针对客户端异质性设计新的操作方法，同时考虑了联邦设置和用户级差分隐私设置下的校准策略。

Result: 研究表明，联邦温度缩放在差分隐私环境下表现最佳，而加权分箱方法在无需差分隐私时效果最好，提出的策略能有效缓解异质性带来的性能下降。

Conclusion: 本文成功地将后处理校准技术引入联邦学习，并提出适应不同隐私需求的校准方法，显著提升了联邦学习中模型预测概率的可靠性。

Abstract: Calibrating machine learning models so that predicted probabilities better
reflect the true outcome frequencies is crucial for reliable decision-making
across many applications. In Federated Learning (FL), the goal is to train a
global model on data which is distributed across multiple clients and cannot be
centralized due to privacy concerns. FL is applied in key areas such as
healthcare and finance where calibration is strongly required, yet federated
private calibration has been largely overlooked. This work introduces the
integration of post-hoc model calibration techniques within FL. Specifically,
we transfer traditional centralized calibration methods such as histogram
binning and temperature scaling into federated environments and define new
methods to operate them under strong client heterogeneity. We study (1) a
federated setting and (2) a user-level Differential Privacy (DP) setting and
demonstrate how both federation and DP impacts calibration accuracy. We propose
strategies to mitigate degradation commonly observed under heterogeneity and
our findings highlight that our federated temperature scaling works best for
DP-FL whereas our weighted binning approach is best when DP is not required.

</details>


### [334] [PepCompass: Navigating peptide embedding spaces using Riemannian Geometry](https://arxiv.org/abs/2510.01988)
*Marcin Możejko,Adam Bielecki,Jurand Prądzyński,Marcin Traskowski,Antoni Janowski,Karol Jurasz,Michał Kucharczyk,Hyun-Su Lee,Marcelo Der Torossian Torres,Cesar de la Fuente-Nunez,Paulina Szymczak,Michał Kmicikiewicz,Ewa Szczurek*

Main category: cs.LG

TL;DR: 本文提出了一种名为PepCompass的几何感知框架，用于抗菌肽的探索与优化，通过引入解码器诱导的流形和局部探索方法，在体外实验中成功发现了多个高活性抗菌肽。


<details>
  <summary>Details</summary>
Motivation: 由于肽空间巨大且活性肽稀少，传统生成模型忽略了解码器引起的几何结构，导致探索效率低下，因此需要一种更有效的几何感知方法来优化抗菌肽发现。

Method: 提出Union of κ-Stable Riemannian Manifolds（M^κ）建模局部几何结构，开发两种局部探索方法：二阶黎曼布朗高效采样和切空间突变枚举，并结合为局部枚举贝叶斯优化（LE-BO）；同时提出势能最小化测地线搜索（PoGS）以引导向高活性肽的发现。

Result: 体外验证显示，PoGS发现了4个新的高活性种子肽，LE-BO进一步优化出25个具有广谱抗菌活性的新肽，包括对耐药菌株有效的肽。

Conclusion: 几何感知的探索策略显著提升了抗菌肽的发现效率和质量，为抗菌肽设计提供了新范式。

Abstract: Antimicrobial peptide discovery is challenged by the astronomical size of
peptide space and the relative scarcity of active peptides. Generative models
provide continuous latent "maps" of peptide space, but conventionally ignore
decoder-induced geometry and rely on flat Euclidean metrics, rendering
exploration and optimization distorted and inefficient. Prior manifold-based
remedies assume fixed intrinsic dimensionality, which critically fails in
practice for peptide data. Here, we introduce PepCompass, a geometry-aware
framework for peptide exploration and optimization. At its core, we define a
Union of $\kappa$-Stable Riemannian Manifolds $\mathbb{M}^{\kappa}$, a family
of decoder-induced manifolds that captures local geometry while ensuring
computational stability. We propose two local exploration methods: Second-Order
Riemannian Brownian Efficient Sampling, which provides a convergent
second-order approximation to Riemannian Brownian motion, and Mutation
Enumeration in Tangent Space, which reinterprets tangent directions as discrete
amino-acid substitutions. Combining these yields Local Enumeration Bayesian
Optimization (LE-BO), an efficient algorithm for local activity optimization.
Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which
interpolates between prototype embeddings along property-enriched geodesics,
biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro
validation confirms the effectiveness of PepCompass: PoGS yields four novel
seeds, and subsequent optimization with LE-BO discovers 25 highly active
peptides with broad-spectrum activity, including against resistant bacterial
strains. These results demonstrate that geometry-informed exploration provides
a powerful new paradigm for antimicrobial peptide design.

</details>


### [335] [Normality Calibration in Semi-supervised Graph Anomaly Detection](https://arxiv.org/abs/2510.02014)
*Guolei Zeng,Hezhe Qiao,Guoguo Ai,Jinsong Guo,Guansong Pang*

Main category: cs.LG

TL;DR: 提出GraphNC框架，利用有标签和无标签数据联合校准图异常检测中的正常性，通过ScoreDA和NormReg两个组件提升异常分数的可分性和正常节点表示的紧凑性。


<details>
  <summary>Details</summary>
Motivation: 现有半监督图异常检测方法仅依赖标注的正常节点学习正常性，易过拟合且导致高误报率。

Method: 设计GraphNC框架，包含异常分数分布对齐（ScoreDA）和基于扰动的正常性正则化（NormReg），在异常分数和节点表示空间中联合校准正常性。

Result: 有效分离正常与异常节点的异常分数，减少教师模型错误评分的影响，提升检测性能。

Conclusion: GraphNC通过联合校准机制，在半监督图异常检测中显著降低误报率，优于现有方法。

Abstract: Graph anomaly detection (GAD) has attracted growing interest for its crucial
ability to uncover irregular patterns in broad applications. Semi-supervised
GAD, which assumes a subset of annotated normal nodes available during
training, is among the most widely explored application settings. However, the
normality learned by existing semi-supervised GAD methods is limited to the
labeled normal nodes, often inclining to overfitting the given patterns. These
can lead to high detection errors, such as high false positives. To overcome
this limitation, we propose GraphNC , a graph normality calibration framework
that leverages both labeled and unlabeled data to calibrate the normality from
a teacher model (a pre-trained semi-supervised GAD model) jointly in anomaly
score and node representation spaces. GraphNC includes two main components,
anomaly score distribution alignment (ScoreDA) and perturbation-based normality
regularization (NormReg). ScoreDA optimizes the anomaly scores of our model by
aligning them with the score distribution yielded by the teacher model. Due to
accurate scores in most of the normal nodes and part of the anomaly nodes in
the teacher model, the score alignment effectively pulls the anomaly scores of
the normal and abnormal classes toward the two ends, resulting in more
separable anomaly scores. Nevertheless, there are inaccurate scores from the
teacher model. To mitigate the misleading by these scores, NormReg is designed
to regularize the graph normality in the representation space, making the
representations of normal nodes more compact by minimizing a
perturbation-guided consistency loss solely on the labeled nodes.

</details>


### [336] [FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data](https://arxiv.org/abs/2510.02017)
*Aida Tayebi,Ali Khodabandeh Yalabadi,Mehdi Yazdani-Jahromi,Ozlem Ozmen Garibay*

Main category: cs.LG

TL;DR: 提出了一种针对表格数据的对比学习框架，旨在减少偏见并学习公平表示，在保持准确性的同时有效缓解偏差。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在日常生活中的广泛应用，开发公平且无偏见的模型变得至关重要，现有对比学习方法在表格数据上的公平性研究尚不充分。

Method: 通过战略性选择正样本对，并结合监督与自监督对比学习，设计了一个专门用于表格数据的对比学习框架。

Result: 相比现有的最先进模型，显著减少了偏见，同时在下游任务中表现出良好的性能和最小的准确率损失。

Conclusion: 该方法能有效缓解表格数据中的偏差，提升模型公平性，同时保持预测性能，具有实际应用价值。

Abstract: As AI systems become more embedded in everyday life, the development of fair
and unbiased models becomes more critical. Considering the social impact of AI
systems is not merely a technical challenge but a moral imperative. As
evidenced in numerous research studies, learning fair and robust
representations has proven to be a powerful approach to effectively debiasing
algorithms and improving fairness while maintaining essential information for
prediction tasks. Representation learning frameworks, particularly those that
utilize self-supervised and contrastive learning, have demonstrated superior
robustness and generalizability across various domains. Despite the growing
interest in applying these approaches to tabular data, the issue of fairness in
these learned representations remains underexplored. In this study, we
introduce a contrastive learning framework specifically designed to address
bias and learn fair representations in tabular datasets. By strategically
selecting positive pair samples and employing supervised and self-supervised
contrastive learning, we significantly reduce bias compared to existing
state-of-the-art contrastive learning models for tabular data. Our results
demonstrate the efficacy of our approach in mitigating bias with minimum
trade-off in accuracy and leveraging the learned fair representations in
various downstream tasks.

</details>


### [337] [Mathematical Modeling and Convergence Analysis of Deep Neural Networks with Dense Layer Connectivities in Deep Learning](https://arxiv.org/abs/2510.02049)
*Jinshu Huang,Haibin Su,Xue-Cheng Tai,Chunlin Wu*

Main category: cs.LG

TL;DR: 本文提出了一种密集非局部（DNL）框架，将密集连接的深度神经网络建模为非线性积分方程，并从最优控制角度分析其训练问题，证明了网络学习问题向连续时间对应问题的收敛性。


<details>
  <summary>Details</summary>
Motivation: 为了更广泛地理解和分析密集连接深度神经网络的学习行为，特别是在深层极限下的性质，需要一个统一的数学模型。

Method: 采用非线性积分方程对密集连接的DNN进行建模，并结合最优控制理论，利用分段线性扩展和Γ-收敛分析研究训练问题的收敛性。

Result: 证明了最优值的收敛性和最小化解的子序列收敛性，表明密集连接结构在训练深层模型时具有稳定性。

Conclusion: 该工作为理解密集连接DNN提供了数学基础，并表明此类架构有助于提升深层模型训练的稳定性。

Abstract: In deep learning, dense layer connectivity has become a key design principle
in deep neural networks (DNNs), enabling efficient information flow and strong
performance across a range of applications. In this work, we model densely
connected DNNs mathematically and analyze their learning problems in the
deep-layer limit. For a broad applicability, we present our analysis in a
framework setting of DNNs with densely connected layers and general non-local
feature transformations (with local feature transformations as special cases)
within layers, which is called dense non-local (DNL) framework and includes
standard DenseNets and variants as special examples. In this formulation, the
densely connected networks are modeled as nonlinear integral equations, in
contrast to the ordinary differential equation viewpoint commonly adopted in
prior works. We study the associated training problems from an optimal control
perspective and prove convergence results from the network learning problem to
its continuous-time counterpart. In particular, we show the convergence of
optimal values and the subsequence convergence of minimizers, using a piecewise
linear extension and $\Gamma$-convergence analysis. Our results provide a
mathematical foundation for understanding densely connected DNNs and further
suggest that such architectures can offer stability of training deep models.

</details>


### [338] [Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference](https://arxiv.org/abs/2510.02056)
*Benjamin Wiriyapong,Oktay Karakuş,Kirill Sidorov*

Main category: cs.LG

TL;DR: 提出了一种自适应混合流变分推断方法（AMF-VI），通过组合多种互补的流模型并进行两阶段训练，实现了在多种后验分布上的稳定且鲁棒的推断性能。


<details>
  <summary>Details</summary>
Motivation: 单一的标准化流模型在不同类型的后验分布上表现不稳定，难以一致地逼近复杂后验。

Method: 构建一个异构混合流模型（MAF、RealNVP、RBIG），采用两阶段训练：首先分别训练各个流模型作为专家，然后通过似然驱动的更新来自适应估计全局权重，无需每样本门控或架构修改。

Result: 在六种典型后验分布（如香蕉形、双月、环形、多模态混合等）上，AMF-VI在负对数似然、Wasserstein-2距离和最大均值差异（MMD）等指标上均优于单一流模型，表现出更强的鲁棒性和稳定性。

Conclusion: 自适应混合多样化的流模型为处理多样化后验分布提供了一条可靠路径，在保持各专家归纳偏置的同时提升了变分推断的鲁棒性，且具有高效性和架构无关性。

Abstract: Normalising-flow variational inference (VI) can approximate complex
posteriors, yet single-flow models often behave inconsistently across
qualitatively different distributions. We propose Adaptive Mixture Flow
Variational Inference (AMF-VI), a heterogeneous mixture of complementary flows
(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of
individual flows, and (ii) adaptive global weight estimation via
likelihood-driven updates, without per-sample gating or architectural changes.
Evaluated on six canonical posterior families of banana, X-shape, two-moons,
rings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower
negative log-likelihood than each single-flow baseline and delivers stable
gains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),
indicating improved robustness across shapes and modalities. The procedure is
efficient and architecture-agnostic, incurring minimal overhead relative to
standard flow training, and demonstrates that adaptive mixtures of diverse
flows provide a reliable route to robust VI across diverse posterior families
whilst preserving each expert's inductive bias.

</details>


### [339] [Inferring Optical Tissue Properties from Photoplethysmography using Hybrid Amortized Inference](https://arxiv.org/abs/2510.02073)
*Jens Behrmann,Maria R. Cervera,Antoine Wehenkel,Andrew C. Miller,Albert Cerussi,Pranay Jain,Vivek Venugopal,Shijie Yan,Guillermo Sapiro,Luca Pegolotti,Jörn-Henrik Jacobsen*

Main category: cs.LG

TL;DR: 提出PPGen生物物理模型和混合摊销推断（HAI）方法，以从PPG信号中快速、鲁棒地估计可解释的生理参数，兼顾深度学习的预测能力与临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在PPG信号分析中缺乏生理意义的可解释性，限制了其在临床解释和传感器设计中的应用。

Method: 构建名为PPGen的生物物理模型，结合混合摊销推断（HAI）方法，实现对PPG信号中生理和光学参数的快速、稳健估计，并纠正模型误设。

Result: 在多种噪声和传感器条件下，HAI能准确推断生理参数，验证了其鲁棒性和可扩展性。

Conclusion: 该方法为PPG信号建模提供了兼顾深度学习性能与临床可解释性的可行路径，有助于指导硬件设计和临床应用。

Abstract: Smart wearables enable continuous tracking of established biomarkers such as
heart rate, heart rate variability, and blood oxygen saturation via
photoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer
physiological information, as recent deep learning (DL) studies demonstrate.
However, DL models often rely on features with unclear physiological meaning,
creating a tension between predictive power, clinical interpretability, and
sensor design. We address this gap by introducing PPGen, a biophysical model
that relates PPG signals to interpretable physiological and optical parameters.
Building on PPGen, we propose hybrid amortized inference (HAI), enabling fast,
robust, and scalable estimation of relevant physiological parameters from PPG
signals while correcting for model misspecification. In extensive in-silico
experiments, we show that HAI can accurately infer physiological parameters
under diverse noise and sensor conditions. Our results illustrate a path toward
PPG models that retain the fidelity needed for DL-based features while
supporting clinical interpretation and informed hardware design.

</details>


### [340] [Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions](https://arxiv.org/abs/2510.02081)
*Zhaoyi Li,Jingtao Ding,Yong Li,Shihua Li*

Main category: cs.LG

TL;DR: 本文提出通过最大似然估计对Flow Matching（FM）模型进行微调，以缩小训练与推理之间的差距，并提升其在高精度任务（如机器人操作）中的性能。


<details>
  <summary>Details</summary>
Motivation: FM算法虽然在生成任务中表现优异，但存在训练-推理差距，且预设的直线路径可能导致系统僵硬，难以满足高精度需求。

Method: 基于FM的平滑常微分方程（ODE）形式，提出两种微调方法：直接微调和基于残差的微调，并设计特定架构引入收缩性。

Result: 在图像生成和机器人操作任务中验证了所提方法能有效提升FM的推理性能。

Conclusion: 通过最大似然重建微调FM可有效缩小训练-推理差距，增强模型鲁棒性和可解释性，适用于高精度生成任务。

Abstract: Flow Matching (FM) algorithm achieves remarkable results in generative tasks
especially in robotic manipulation. Building upon the foundations of diffusion
models, the simulation-free paradigm of FM enables simple and efficient
training, but inherently introduces a train-inference gap. Specifically, we
cannot assess the model's output during the training phase. In contrast, other
generative models including Variational Autoencoder (VAE), Normalizing Flow and
Generative Adversarial Networks (GANs) directly optimize on the reconstruction
loss. Such a gap is particularly evident in scenarios that demand high
precision, such as robotic manipulation. Moreover, we show that FM's
over-pursuit of straight predefined paths may introduce some serious problems
such as stiffness into the system. These motivate us to fine-tune FM via
Maximum Likelihood Estimation of reconstructions - an approach made feasible by
FM's underlying smooth ODE formulation, in contrast to the stochastic
differential equations (SDEs) used in diffusion models. This paper first
theoretically analyzes the relation between training loss and inference error
in FM. Then we propose a method of fine-tuning FM via Maximum Likelihood
Estimation of reconstructions, which includes both straightforward fine-tuning
and residual-based fine-tuning approaches. Furthermore, through specifically
designed architectures, the residual-based fine-tuning can incorporate the
contraction property into the model, which is crucial for the model's
robustness and interpretability. Experimental results in image generation and
robotic manipulation verify that our method reliably improves the inference
performance of FM.

</details>


### [341] [KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting](https://arxiv.org/abs/2510.02084)
*Kuiye Ding,Fanda Fan,Zheya Wang,Hongxiao Li,Yifan Wang,Lei Wang,Chunjie Luo,Jianfeng Zhan*

Main category: cs.LG

TL;DR: KAIROS是一个非自回归的时间序列预测框架，能够直接建模段级多峰分布，实现即时推理并避免误差累积，在多个基准上表现出强大的零样本泛化能力，且推理成本显著低于现有模型。


<details>
  <summary>Details</summary>
Motivation: Web应用中的时间序列预测需要快速响应以支持实时决策，而传统方法存在误差累积或预测过度平滑的问题。

Method: 提出KAIROS框架，采用非自回归方式直接建模段级多峰分布，并在大规模语料库上进行训练。

Result: 在六个常用基准上实现了与当前最先进的大模型相当的预测性能，但推理成本仅为其一小部分，同时展现出强零样本泛化能力。

Conclusion: 非自回归设计是面向时间序列基础模型的一种可扩展范式，KAIROS为高效、实时的Web级预测提供了新方向。

Abstract: In the World Wide Web, reliable time series forecasts provide the
forward-looking signals that drive resource planning, cache placement, and
anomaly response, enabling platforms to operate efficiently as user behavior
and content distributions evolve. Compared with other domains, time series
forecasting for Web applications requires much faster responsiveness to support
real-time decision making. We present KAIROS, a non-autoregressive time series
forecasting framework that directly models segment-level multi-peak
distributions. Unlike autoregressive approaches, KAIROS avoids error
accumulation and achieves just-in-time inference, while improving over existing
non-autoregressive models that collapse to over-smoothed predictions. Trained
on the large-scale corpus, KAIROS demonstrates strong zero-shot generalization
on six widely used benchmarks, delivering forecasting performance comparable to
state-of-the-art foundation models with similar scale, at a fraction of their
inference cost. Beyond empirical results, KAIROS highlights the importance of
non-autoregressive design as a scalable paradigm for foundation models in time
series.

</details>


### [342] [Learning Model Representations Using Publicly Available Model Hubs](https://arxiv.org/abs/2510.02096)
*Damian Falk,Konstantin Schürholt,Konstantinos Tzevelekakis,Léo Meynent,Damian Borth*

Main category: cs.LG

TL;DR: 本文提出了一种新的权重空间学习骨干网络，能够在无需精心构建模型库的情况下，直接利用从Hugging Face等非结构化模型仓库中下载的任意模型进行训练，展现出强大的性能和跨数据模态的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的权重空间学习依赖于大规模、精心构造的模型库（model zoos），这些库通常需要大量计算资源且缺乏灵活性，限制了该领域的发展。

Method: 提出一种新的权重空间骨干网络，能够处理来自非结构化模型仓库（如Hugging Face）的高度异构模型（架构和数据集各异且缺乏文档），通过在这些任意模型上进行训练，学习有效的权重表示。

Result: 在Hugging Face模型上训练的权重空间表示表现出色，常优于在实验室生成模型库上训练的骨干网络，并能泛化到未见过的数据模态。

Conclusion: 高质量的权重空间表示可以在“自然环境”中学习，无需依赖精心策划的模型库，突破了当前权重空间学习领域的一个主要限制。

Abstract: The weights of neural networks have emerged as a novel data modality, giving
rise to the field of weight space learning. A central challenge in this area is
that learning meaningful representations of weights typically requires large,
carefully constructed collections of trained models, typically referred to as
model zoos. These model zoos are often trained ad-hoc, requiring large
computational resources, constraining the learned weight space representations
in scale and flexibility. In this work, we drop this requirement by training a
weight space learning backbone on arbitrary models downloaded from large,
unstructured model repositories such as Hugging Face. Unlike curated model
zoos, these repositories contain highly heterogeneous models: they vary in
architecture and dataset, and are largely undocumented. To address the
methodological challenges posed by such heterogeneity, we propose a new weight
space backbone designed to handle unstructured model populations. We
demonstrate that weight space representations trained on models from Hugging
Face achieve strong performance, often outperforming backbones trained on
laboratory-generated model zoos. Finally, we show that the diversity of the
model weights in our training set allows our weight space model to generalize
to unseen data modalities. By demonstrating that high-quality weight space
representations can be learned in the wild, we show that curated model zoos are
not indispensable, thereby overcoming a strong limitation currently faced by
the weight space learning community.

</details>


### [343] [PENEX: AdaBoost-Inspired Neural Network Regularization](https://arxiv.org/abs/2510.02107)
*Klaus-Rudolf Kladny,Bernhard Schölkopf,Michael Muehlebach*

Main category: cs.LG

TL;DR: 本文提出了PENEX，一种新的多类指数损失函数，具有理论基础且可通过一阶方法优化，能够隐式最大化数据点的边距，并在多个任务中表现出优于现有方法的正则化效果。


<details>
  <summary>Details</summary>
Motivation: AdaBoost虽然使用指数损失函数在实践中泛化良好，但其现有形式难以通过一阶方法优化，本文旨在提出一种更优的多类指数损失函数。

Method: 提出了一种新的多类指数损失函数PENEX，该函数可通过一阶优化方法进行优化，并在理论上证明其能隐式最大化数据点的边距。

Result: 实验和理论分析表明，PENEX能够隐式最大化数据点的边距，并在计算机视觉和语言任务中表现出优于现有方法的正则化效果。

Conclusion: PENEX是一种有潜力的AdaBoost启发式方法，可用于有效训练和微调深度神经网络。

Abstract: AdaBoost sequentially fits so-called weak learners to minimize an exponential
loss, which penalizes mislabeled data points more severely than other loss
functions like cross-entropy. Paradoxically, AdaBoost generalizes well in
practice as the number of weak learners grows. In the present work, we
introduce Penalized Exponential Loss (PENEX), a new formulation of the
multi-class exponential loss that is theoretically grounded and, in contrast to
the existing formulation, amenable to optimization via first-order methods. We
demonstrate both empirically and theoretically that PENEX implicitly maximizes
margins of data points. Also, we show that gradient increments on PENEX
implicitly parameterize weak learners in the boosting framework. Across
computer vision and language tasks, we show that PENEX exhibits a regularizing
effect often better than established methods with similar computational cost.
Our results highlight PENEX's potential as an AdaBoost-inspired alternative for
effective training and fine-tuning of deep neural networks.

</details>


### [344] [Hybrid Deep Learning Modeling Approach to Predict Natural Gas Consumption of Home Subscribers on Limited Data](https://arxiv.org/abs/2510.02115)
*Milad Firoozeh,Nader Dashti,Mohammad Ali Hatefi*

Main category: cs.LG

TL;DR: 本研究利用LSTM、GRU和混合BiLSTM-XGBoost模型预测伊朗赞詹省居民天然气消耗量，发现混合模型在准确性与数据稀缺场景下表现最优。


<details>
  <summary>Details</summary>
Motivation: 由于人口增长和能源消耗上升，伊朗在寒冷季节面临天然气压力下降和供应中断问题，需有效控制居民部门的天然气使用。

Method: 采用机器学习模型（LSTM、GRU和BiLSTM-XGBoost混合模型），基于2017至2022年天然气消耗与气象数据进行训练与评估。

Result: 混合BiLSTM-XGBoost模型在RMSE、MAPE和MPE指标上表现最佳，且在数据有限情况下仍具稳健性。

Conclusion: 混合机器学习模型能更有效地预测天然气消耗，有助于优化资源管理并缓解季节性短缺，地理与气候因素对预测至关重要。

Abstract: Today, natural gas, as a clean fuel and the best alternative to crude oil,
covers a significant part of global demand. Iran is one of the largest
countries with energy resources and in terms of gas is the second-largest
country in the world. But, due to the increase in population and energy
consumption, it faces problems such as pressure drops and gas outages yearly in
cold seasons and therefore it is necessary to control gas consumption,
especially in the residential sector, which has the largest share in Iran. This
study aims to analyze and predict gas consumption for residential customers in
Zanjan province, Iran, using machine learning models, including LSTM, GRU, and
a hybrid BiLSTM-XGBoost model. The dataset consists of gas consumption and
meteorology data collected over six years, from 2017 to 2022. The models were
trained and evaluated based on their ability to accurately predict consumption
patterns. The results indicate that the hybrid BiLSTM-XGBoost model
outperformed the other models in terms of accuracy, with lower Root Mean
Squared Error (RMSE), Mean Absolute Percentage Error (MAPE) values, and Mean
Percentage Error (MPE). Additionally, the Hybrid model demonstrated robust
performance, particularly in scenarios with limited data. The findings suggest
that machine learning approaches, particularly hybrid models, can be
effectively utilized to manage and predict gas consumption, contributing to
more efficient resource management and reducing seasonal shortages. This study
highlights the importance of incorporating geographical and climatic factors in
predictive modeling, as these significantly influence gas usage across
different regions.

</details>


### [345] [Ensemble Threshold Calibration for Stable Sensitivity Control](https://arxiv.org/abs/2510.02116)
*John N. Daras*

Main category: cs.LG

TL;DR: 提出了一种端到端框架，可在大规模空间融合和实体匹配任务中实现精确召回率控制，误差低于百分之一，并减少冗余验证。


<details>
  <summary>Details</summary>
Motivation: 传统置信区间方法（如Clopper-Pearson或Wilson）在召回率控制中常超出目标值且方差大，难以满足高精度和稳定性需求。

Method: 采用等网格边界框过滤和CSR候选表示减少配对数量；使用xxHash引导样本训练轻量神经排序模型；通过前向传播得分构建分层校准集；结合四种阈值估计器并进行逆方差加权融合。

Result: 在约631万和6734万配对的真实地籍数据上验证，该方法能稳定达到目标召回率，方差极小，显著减少人工复核，且可在单个TPU v3核心上运行。

Conclusion: 该框架实现了高精度、低方差的召回率控制，兼具可重现性和计算效率，适用于大规模空间数据匹配任务。

Abstract: Precise recall control is critical in large-scale spatial conflation and
entity-matching tasks, where missing even a few true matches can break
downstream analytics, while excessive manual review inflates cost. Classical
confidence-interval cuts such as Clopper-Pearson or Wilson provide lower bounds
on recall, but they routinely overshoot the target by several percentage points
and exhibit high run-to-run variance under skewed score distributions. We
present an end-to-end framework that achieves exact recall with sub-percent
variance over tens of millions of geometry pairs, while remaining TPU-friendly.
Our pipeline starts with an equigrid bounding-box filter and compressed sparse
row (CSR) candidate representation, reducing pair enumeration by two orders of
magnitude. A deterministic xxHash bootstrap sample trains a lightweight neural
ranker; its scores are propagated to all remaining pairs via a single forward
pass and used to construct a reproducible, score-decile-stratified calibration
set. Four complementary threshold estimators - Clopper-Pearson, Jeffreys,
Wilson, and an exact quantile - are aggregated via inverse-variance weighting,
then fused across nine independent subsamples. This ensemble reduces threshold
variance compared to any single method. Evaluated on two real cadastral
datasets (approximately 6.31M and 67.34M pairs), our approach consistently hits
a recall target within a small error, decreases redundant verifications
relative to other calibrations, and runs end-to-end on a single TPU v3 core.

</details>


### [346] [DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding](https://arxiv.org/abs/2510.02117)
*Samhita Pal,James O'quinn,Kaveh Aryan,Heather Pua,James P. Long,Amir Asiaee*

Main category: cs.LG

TL;DR: 提出了一种名为DECOR的可微分方法，用于在存在潜在混杂的情况下学习线性高斯结构方程模型（SEM）的结构，通过联合学习有向无环图（DAG）和相关噪声模型，在多种条件下实现了良好的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在误差独立或需要广泛因子结构/非线性时表现良好，但在存在潜在混杂且混杂不普遍时缺乏有效处理手段，因此需要一种能同时处理结构学习和去混杂的统一、可微分方法。

Method: 提出DECOR，一种基于似然的完全可微估计器，交替进行平滑无环图更新和凸噪声更新，并引入轻量级bow补全惩罚或事后协调步骤；理论证明了在混合图无弓形且噪声协方差具有均匀特征值间隙时参数可识别。

Result: 在合成基准上，DECOR在不同混杂密度、图密度、潜在秩和高维（n<p）情况下均达到或优于强基线方法，尤其在非普遍混杂下表现出更强鲁棒性，同时在普遍混杂下仍具竞争力。

Conclusion: DECOR提供了一种统一且有效的框架，能够在存在潜在混杂的情况下实现线性高斯SEM的结构学习，兼具理论可识别性和实际优越性能。

Abstract: We study structure learning for linear Gaussian SEMs in the presence of
latent confounding. Existing continuous methods excel when errors are
independent, while deconfounding-first pipelines rely on pervasive factor
structure or nonlinearity. We propose \textsc{DECOR}, a single likelihood-based
and fully differentiable estimator that jointly learns a DAG and a correlated
noise model. Our theory gives simple sufficient conditions for global parameter
identifiability: if the mixed graph is bow free and the noise covariance has a
uniform eigenvalue margin, then the map from $(\B,\OmegaMat)$ to the
observational covariance is injective, so both the directed structure and the
noise are uniquely determined. The estimator alternates a smooth-acyclic graph
update with a convex noise update and can include a light bow complementarity
penalty or a post hoc reconciliation step. On synthetic benchmarks that vary
confounding density, graph density, latent rank, and dimension with $n<p$,
\textsc{DECOR} matches or outperforms strong baselines and is especially robust
when confounding is non-pervasive, while remaining competitive under
pervasiveness.

</details>


### [347] [Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study](https://arxiv.org/abs/2510.02142)
*Lena Podina,Christina Humer,Alexandre Duval,Victor Schmidt,Ali Ramlaoui,Shahana Chatterjee,Yoshua Bengio,Alex Hernandez-Garcia,David Rolnick,Félix Therrien*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的生成模型Catalyst GFlowNet，用于设计高效晶体表面催化剂，已在析氢反应中验证其有效性，未来将扩展至析氧反应以发现低成本新材料。


<details>
  <summary>Details</summary>
Motivation: 开发廉价且高性能的电催化剂以推动可再生能源储能技术的发展，特别是氢能源存储中的关键反应。

Method: 利用机器学习预测生成模型Catalyst GFlowNet，结合形成能和吸附能的预测，设计高效的晶体表面催化剂。

Result: 在析氢反应的验证中成功识别出铂为最高效的已知催化剂，证明了模型的有效性。

Conclusion: 该生成模型框架为加速新型高效催化剂的发现提供了有前景的途径。

Abstract: Efficient and inexpensive energy storage is essential for accelerating the
adoption of renewable energy and ensuring a stable supply, despite fluctuations
in sources such as wind and solar. Electrocatalysts play a key role in hydrogen
energy storage (HES), allowing the energy to be stored as hydrogen. However,
the development of affordable and high-performance catalysts for this process
remains a significant challenge. We introduce Catalyst GFlowNet, a generative
model that leverages machine learning-based predictors of formation and
adsorption energy to design crystal surfaces that act as efficient catalysts.
We demonstrate the performance of the model through a proof-of-concept
application to the hydrogen evolution reaction, a key reaction in HES, for
which we successfully identified platinum as the most efficient known catalyst.
In future work, we aim to extend this approach to the oxygen evolution
reaction, where current optimal catalysts are expensive metal oxides, and open
the search space to discover new materials. This generative modeling framework
offers a promising pathway for accelerating the search for novel and efficient
catalysts.

</details>


### [348] [Policy Gradient Guidance Enables Test Time Control](https://arxiv.org/abs/2510.02148)
*Jianing Qi,Hao Tang,Zhigang Zhu*

Main category: cs.LG

TL;DR: 本文提出了Policy Gradient Guidance (PGG)，将扩散模型中的分类器自由引导方法扩展到经典策略梯度中，通过引入无条件分支和插值实现测试时控制，提升了策略的可控性、稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 受扩散模型中分类器自由引导的启发，希望将其思想迁移到传统策略梯度方法中，以实现无需重新训练即可调节行为的可控强化学习。

Method: PGG在策略梯度中增加一个无条件分支，并在训练时对条件与无条件分支进行插值；理论推导表明，在优势估计下额外的归一化项消失，从而得到简洁的引导策略梯度更新规则。

Result: 在离散和连续控制任务上验证了PGG的有效性：在简单离散任务和低样本情况下dropout有帮助，但在连续控制中会带来不稳定；适度增大的引导系数（γ>1）能显著提升稳定性、样本效率和可控性。

Conclusion: 引导机制不仅适用于扩散模型，也可成功应用于标准的on-policy策略梯度方法，为可控制的在线强化学习提供了新方向。

Abstract: We introduce Policy Gradient Guidance (PGG), a simple extension of
classifier-free guidance from diffusion models to classical policy gradient
methods. PGG augments the policy gradient with an unconditional branch and
interpolates conditional and unconditional branches, yielding a test-time
control knob that modulates behavior without retraining. We provide a
theoretical derivation showing that the additional normalization term vanishes
under advantage estimation, leading to a clean guided policy gradient update.
Empirically, we evaluate PGG on discrete and continuous control benchmarks. We
find that conditioning dropout-central to diffusion guidance-offers gains in
simple discrete tasks and low sample regimes, but dropout destabilizes
continuous control. Training with modestly larger guidance ($\gamma>1$)
consistently improves stability, sample efficiency, and controllability. Our
results show that guidance, previously confined to diffusion policies, can be
adapted to standard on-policy methods, opening new directions for controllable
online reinforcement learning.

</details>


### [349] [Reinforcement Learning with Action-Triggered Observations](https://arxiv.org/abs/2510.02149)
*Alexander Ryabchenko,Wenlong Mou*

Main category: cs.LG

TL;DR: 本文研究了动作触发状态观测的强化学习问题，提出了ATST-MDP框架和ST-LSVI-UCB算法，在稀疏观测下实现了高效学习。


<details>
  <summary>Details</summary>
Motivation: 许多现实应用中状态观测由动作随机触发，传统MDP假设频繁观测不成立，需建立新框架应对稀疏观测挑战。

Method: 提出ATST-MDP模型，推导相应的Bellman方程，引入动作序列学习范式，并在线性MDP假设下设计基于动作序列特征映射的离策略估计器与ST-LSVI-UCB算法。

Result: 证明价值函数可在线性动作序列特征下表示，ST-LSVI-UCB算法达到\(\widetilde{O}(\sqrt{Kd^3(1-\gamma)^{-3}})\)的遗憾界。

Conclusion: 建立了动作触发稀疏观测下的理论学习基础，表明即使观测受限，仍可实现高效强化学习。

Abstract: We study reinforcement learning problems where state observations are
stochastically triggered by actions, a constraint common in many real-world
applications. This framework is formulated as Action-Triggered Sporadically
Traceable Markov Decision Processes (ATST-MDPs), where each action has a
specified probability of triggering a state observation. We derive tailored
Bellman optimality equations for this framework and introduce the
action-sequence learning paradigm in which agents commit to executing a
sequence of actions until the next observation arrives. Under the linear MDP
assumption, value-functions are shown to admit linear representations in an
induced action-sequence feature map. Leveraging this structure, we propose
off-policy estimators with statistical error guarantees for such feature maps
and introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered
settings. ST-LSVI-UCB achieves regret $\widetilde
O(\sqrt{Kd^3(1-\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the
feature dimension, and $\gamma$ the discount factor (per-step episode
non-termination probability). Crucially, this work establishes the theoretical
foundation for learning with sporadic, action-triggered observations while
demonstrating that efficient learning remains feasible under such observation
constraints.

</details>


### [350] [Flatness-Aware Stochastic Gradient Langevin Dynamics](https://arxiv.org/abs/2510.02174)
*Stefano Bruno,Youngsik Hwang,Jaehyeon An,Sotirios Sabanis,Dong-Young Lim*

Main category: cs.LG

TL;DR: 本文提出了Flatness-Aware Stochastic Gradient Langevin Dynamics (fSGLD)，通过引入随机权重扰动（RWP）来优化具有隐式曲率信息的平滑目标函数，从而有效寻找高维非凸优化问题中的平坦极小值。理论分析表明，fSGLD的不变测度集中在Hessian迹正则化损失函数的全局最小值附近，并提供了非渐近收敛性和泛化误差界的保证。实验结果显示，fSGLD在各种视觉任务中实现了优于或相当于基线算法的泛化能力和鲁棒性，同时保持了与SGD相当的计算成本。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的泛化能力与损失景观中的平坦极小值密切相关，但传统的SGLD缺乏偏向低曲率解的机制。因此，需要一种新的方法来有效地引导优化过程朝向平坦极小值。

Method: 提出fSGLD算法，在每次迭代中使用各向同性高斯噪声扰动参数后的随机梯度，优化一个随机平滑目标函数，该函数隐式捕获曲率信息。通过耦合逆温度和随机权重扰动尺度，证明fSGLD的不变测度接近于集中在Hessian迹正则化损失函数全局最小值上的平稳测度。

Result: 理论上，fSGLD被证明具有最优已知速率的Wasserstein距离下的非渐近收敛保证，并推导出Hessian迹正则化目标的过量风险界。实验上，fSGLD在噪声标签和大规模视觉任务中表现出色，无论是从零训练还是微调设置下，都达到了优于或相当于基线算法的泛化性能和鲁棒性，且计算成本仅为SAM的一半左右。Hessian谱分析进一步证实fSGLD收敛到更平坦的极小值。

Conclusion: fSGLD是一种高效且理论上可解释的方法，能够促进深度神经网络收敛到平坦极小值，提升模型的泛化能力和鲁棒性，同时保持较低的计算开销。

Abstract: Generalization in deep learning is closely tied to the pursuit of flat minima
in the loss landscape, yet classical Stochastic Gradient Langevin Dynamics
(SGLD) offers no mechanism to bias its dynamics toward such low-curvature
solutions. This work introduces Flatness-Aware Stochastic Gradient Langevin
Dynamics (fSGLD), designed to efficiently and provably seek flat minima in
high-dimensional nonconvex optimization problems. At each iteration, fSGLD uses
the stochastic gradient evaluated at parameters perturbed by isotropic Gaussian
noise, commonly referred to as Random Weight Perturbation (RWP), thereby
optimizing a randomized-smoothing objective that implicitly captures curvature
information. Leveraging these properties, we prove that the invariant measure
of fSGLD stays close to a stationary measure concentrated on the global
minimizers of a loss function regularized by the Hessian trace whenever the
inverse temperature and the scale of random weight perturbation are properly
coupled. This result provides a rigorous theoretical explanation for the
benefits of random weight perturbation. In particular, we establish
non-asymptotic convergence guarantees in Wasserstein distance with the best
known rate and derive an excess-risk bound for the Hessian-trace regularized
objective. Extensive experiments on noisy-label and large-scale vision tasks,
in both training-from-scratch and fine-tuning settings, demonstrate that fSGLD
achieves superior or comparable generalization and robustness to baseline
algorithms while maintaining the computational cost of SGD, about half that of
SAM. Hessian-spectrum analysis further confirms that fSGLD converges to
significantly flatter minima.

</details>


### [351] [GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning](https://arxiv.org/abs/2510.02180)
*Silvia Sapora,Devon Hjelm,Alexander Toshev,Omar Attia,Bogdan Mazoure*

Main category: cs.LG

TL;DR: 本文提出了GRACE方法，利用大语言模型和进化搜索从专家轨迹中逆向生成可解释的代码形式奖励函数，并在多个基准上验证了其有效性与优越性。


<details>
  <summary>Details</summary>
Motivation: 传统逆强化学习方法生成的奖励模型难以解释和调试，缺乏透明性，限制了其在复杂任务中的应用。

Method: 提出GRACE方法，结合大语言模型与进化搜索，直接从专家轨迹中生成可执行、可检验的代码形式奖励函数。

Result: 在BabyAI和AndroidWorld基准上，GRACE能高效学习高精度奖励函数，生成的策略优于模仿学习和在线强化学习方法，且能构建复杂的多任务奖励API。

Conclusion: GRACE实现了可解释奖励函数的自动生成，在复杂多任务环境中表现出色，提升了逆强化学习的可读性与实用性。

Abstract: Inverse Reinforcement Learning aims to recover reward models from expert
demonstrations, but traditional methods yield "black-box" models that are
difficult to interpret and debug. In this work, we introduce GRACE (Generating
Rewards As CodE), a method for using Large Language Models within an
evolutionary search to reverse-engineer an interpretable, code-based reward
function directly from expert trajectories. The resulting reward function is
executable code that can be inspected and verified. We empirically validate
GRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns
highly accurate rewards, even in complex, multi-task settings. Further, we
demonstrate that the resulting reward leads to strong policies, compared to
both competitive Imitation Learning and online RL approaches with ground-truth
rewards. Finally, we show that GRACE is able to build complex reward APIs in
multi-task setups.

</details>


### [352] [StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?](https://arxiv.org/abs/2510.02209)
*Yanxu Chen,Zijun Yao,Yantao Liu,Jin Ye,Jianing Yu,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: 本文提出了StockBench，一个用于评估大语言模型（LLM）在真实多月股票交易环境中表现的无污染基准。与现有静态金融知识测试不同，StockBench通过每日市场信号（价格、基本面、新闻）评估LLM代理的动态决策能力，并使用累计收益、最大回撤和Sortino比率等指标衡量性能。实验表明，尽管多数LLM难以超越简单的持有策略，但部分模型展现出更高的收益潜力和风险控制能力，说明静态知识优势不等于成功交易。作者开源了该基准以推动未来研究。


<details>
  <summary>Details</summary>
Motivation: 金融领域对高价值、高风险决策至关重要，但现有基准多局限于静态知识问答，无法反映实际交易中的动态与迭代特性。因此，需要一个更贴近现实的基准来系统评估LLM作为自主代理在金融决策中的真实能力。

Method: 提出StockBench，一个基于真实市场数据的多月模拟环境，向LLM代理每日提供价格、基本面和新闻信号，要求其做出买入、卖出或持有决策。采用累积收益、最大回撤、Sortino比率等标准金融指标进行评估，并测试多个先进闭源与开源LLM的表现。

Result: 大多数LLM代理未能显著超越‘买入并持有’基线策略，但部分模型（如GPT-5、Qwen3等）展现出更高的累计回报和更好的风险控制能力，表明其具备潜在交易优势。同时发现，擅长静态金融知识任务的模型不一定能在动态交易中表现良好。

Conclusion: StockBench填补了LLM代理在动态金融决策评估方面的空白，揭示了当前LLM在实际交易中的局限性与潜力。结果表明，静态知识掌握与动态决策能力之间存在脱节，未来需专门优化LLM在时序、风险敏感环境下的推理与学习机制。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
as autonomous agents, showing promise in reasoning, tool use, and sequential
decision-making. While prior benchmarks have evaluated LLM agents in domains
such as software engineering and scientific discovery, the finance domain
remains underexplored, despite its direct relevance to economic value and
high-stakes decision-making. Existing financial benchmarks primarily test
static knowledge through question answering, but they fall short of capturing
the dynamic and iterative nature of trading. To address this gap, we introduce
StockBench, a contamination-free benchmark designed to evaluate LLM agents in
realistic, multi-month stock trading environments. Agents receive daily market
signals -- including prices, fundamentals, and news -- and must make sequential
buy, sell, or hold decisions. Performance is assessed using financial metrics
such as cumulative return, maximum drawdown, and the Sortino ratio. Our
evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and
open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM
agents struggle to outperform the simple buy-and-hold baseline, several models
demonstrate the potential to deliver higher returns and manage risk more
effectively. These findings highlight both the challenges and opportunities in
developing LLM-powered financial agents, showing that excelling at static
financial knowledge tasks does not necessarily translate into successful
trading strategies. We release StockBench as an open-source resource to support
reproducibility and advance future research in this domain.

</details>


### [353] [Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025](https://arxiv.org/abs/2510.02202)
*Matthew A. Reyna,Zuzana Koscova,Jan Pavlus,Soheil Saghafi,James Weigle,Andoni Elola,Salman Seyedi,Kiersten Campbell,Qiao Li,Ali Bahrami Rad,Antônio H. Ribeiro,Antonio Luiz P. Ribeiro,Reza Sameni,Gari D. Clifford*

Main category: cs.LG

TL;DR: 本研究通过心电图（ECG）识别查加斯病，利用弱标签和强标签数据集，并结合数据增强与适应实际检测能力的评估指标，将其建模为分诊任务。


<details>
  <summary>Details</summary>
Motivation: 查加斯病在南美、中美及美国部分地区流行，慢性阶段可导致心血管疾病，但血清学检测能力有限。由于查加斯心肌病常在ECG中表现出来，因此有必要开发基于ECG的筛查工具以优先安排患者进行确诊和治疗。

Method: 使用多个带有患者报告和血清学检测标签的ECG数据集，结合大规模弱标签数据和小规模强标签数据，采用数据增强提升模型鲁棒性和泛化能力，并设计反映地区检测能力的评估指标，将机器学习任务定义为临床分诊问题。

Result: 2025年George B. Moody PhysioNet挑战赛吸引了来自111支队伍的630多名参与者，提交了1300多个算法方案，展示了全球学术界和工业界的广泛参与和技术多样性。

Conclusion: 基于ECG的查加斯病识别作为分诊工具具有可行性，该挑战推动了算法创新并促进了全球协作，有助于提升资源有限地区的疾病筛查效率。

Abstract: Objective: Chagas disease is a parasitic infection that is endemic to South
America, Central America, and, more recently, the U.S., primarily transmitted
by insects. Chronic Chagas disease can cause cardiovascular diseases and
digestive problems. Serological testing capacities for Chagas disease are
limited, but Chagas cardiomyopathy often manifests in ECGs, providing an
opportunity to prioritize patients for testing and treatment. Approach: The
George B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic
approaches for identifying Chagas disease from electrocardiograms (ECGs). Main
results: This Challenge provides multiple innovations. First, we leveraged
several datasets with labels from patient reports and serological testing,
provided a large dataset with weak labels and smaller datasets with strong
labels. Second, we augmented the data to support model robustness and
generalizability to unseen data sources. Third, we applied an evaluation metric
that captured the local serological testing capacity for Chagas disease to
frame the machine learning problem as a triage task. Significance: Over 630
participants from 111 teams submitted over 1300 entries during the Challenge,
representing diverse approaches from academia and industry worldwide.

</details>


### [354] [Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling](https://arxiv.org/abs/2510.02206)
*Daniel Gallo Fernández*

Main category: cs.LG

TL;DR: Poolformer是一种新的序列到序列模型，用循环层和池化操作替代自注意力机制，有效处理长序列，在音频任务上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在处理长序列时计算复杂度高，限制了其在长序列任务中的应用。

Method: 提出Poolformer模型，使用递归定义的SkipBlock结构，包含残差块、下采样池化、嵌套SkipBlock、上采样池化等组件，以循环层替代自注意力并结合池化缩短序列长度。

Result: 实验表明Poolformer显著加快训练速度，提升FID和IS等指标，防止过拟合，并在原始音频任务上优于SaShiMi和Mamba等先进模型。

Conclusion: Poolformer通过池化和循环结构有效处理长序列，为序列建模提供了高效替代方案，未来可拓展至文本、视觉及多模态场景。

Abstract: Sequence-to-sequence models have become central in Artificial Intelligence,
particularly following the introduction of the transformer architecture. While
initially developed for Natural Language Processing, these models have
demonstrated utility across domains, including Computer Vision. Such models
require mechanisms to exchange information along the time dimension, typically
using recurrent or self-attention layers. However, self-attention scales
quadratically with sequence length, limiting its practicality for very long
sequences.
  We introduce Poolformer, a sequence-to-sequence model that replaces
self-attention with recurrent layers and incorporates pooling operations to
reduce sequence length. Poolformer is defined recursively using SkipBlocks,
which contain residual blocks, a down-pooling layer, a nested SkipBlock, an
up-pooling layer, and additional residual blocks. We conduct extensive
experiments to support our architectural choices.
  Our results show that pooling greatly accelerates training, improves
perceptual metrics (FID and IS), and prevents overfitting. Our experiments also
suggest that long-range dependencies are handled by deep layers, while shallow
layers take care of short-term features.
  Evaluated on raw audio, which naturally features long sequence lengths,
Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba.
Future directions include applications to text and vision, as well as
multi-modal scenarios, where a Poolformer-based LLM could effectively process
dense representations of images and videos.

</details>


### [355] [ExGRPO: Learning to Reason from Experience](https://arxiv.org/abs/2510.02245)
*Runzhe Zhan,Yafu Li,Zhi Wang,Xiaoye Qu,Dongrui Liu,Jing Shao,Derek F. Wong,Yu Cheng*

Main category: cs.LG

TL;DR: 本文提出了ExGRPO框架，通过识别推理经验中的正确性和熵作为价值指标，实现对有价值经验的组织与优先利用，提升大语言模型在强化学习中的效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准的on-policy训练方法在每次更新后丢弃rollout经验，导致计算效率低下和训练不稳定，且现有研究缺乏对推理模型中经验特征影响的深入探讨。

Method: 提出ExGRPO（Experiential Group Relative Policy Optimization），基于rollout正确性和熵评估经验价值，采用混合策略目标来平衡探索与利用，并对经验进行组织和优先级排序。

Result: 在五个不同规模的大模型（1.5B-8B）上实验表明，ExGRPO在数学和通用推理任务上平均比on-policy RLVR提升+3.5/7.6分，并显著提升训练稳定性，尤其在强弱模型上均优于传统方法。

Conclusion: 合理的经验管理是实现高效、可扩展的强化学习推理（RLVR）的关键因素，ExGRPO为大语言模型的推理能力训练提供了更高效稳定的解决方案。

Abstract: Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm
for improving the reasoning ability of large language models. However, standard
on-policy training discards rollout experiences after a single update, leading
to computational inefficiency and instability. While prior work on RL has
highlighted the benefits of reusing past experience, the role of experience
characteristics in shaping learning dynamics of large reasoning models remains
underexplored. In this paper, we are the first to investigate what makes a
reasoning experience valuable and identify rollout correctness and entropy as
effective indicators of experience value. Based on these insights, we propose
ExGRPO (Experiential Group Relative Policy Optimization), a framework that
organizes and prioritizes valuable experiences, and employs a mixed-policy
objective to balance exploration with experience exploitation. Experiments on
five backbone models (1.5B-8B parameters) show that ExGRPO consistently
improves reasoning performance on mathematical/general benchmarks, with an
average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO
stabilizes training on both stronger and weaker models where on-policy methods
fail. These results highlight principled experience management as a key
ingredient for efficient and scalable RLVR.

</details>


### [356] [DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning](https://arxiv.org/abs/2510.02212)
*Hanyang Zhao,Dawen Liang,Wenpin Tang,David Yao,Nathan Kallus*

Main category: cs.LG

TL;DR: 提出DiFFPO框架，通过强化学习统一训练掩码扩散大语言模型，提升推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练扩散大语言模型时存在采样效率低、推理速度慢的问题，需要更高效且准确的训练框架。

Method: 提出基于离策略强化学习的代理策略训练，并结合重要性采样校正的两阶段似然近似；同时联合训练dLLM的采样器/控制器，使其自适应分配推理阈值。

Result: 在数学和规划任务上验证了方法的有效性，显著降低函数求值次数（NFEs），提升了任务性能与推理效率的帕累托前沿。

Conclusion: DiFFPO为扩散大语言模型提供了一种更高效、更强大的训练范式，兼顾推理速度与准确性。

Abstract: We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified
framework for training masked diffusion large language models (dLLMs) to reason
not only better (furious), but also faster via reinforcement learning (RL). We
first unify the existing baseline approach such as d1 by proposing to train
surrogate policies via off-policy RL, whose likelihood is much more tractable
as an approximation to the true dLLM policy. This naturally motivates a more
accurate and informative two-stage likelihood approximation combined with
importance sampling correction, which leads to generalized RL algorithms with
better sample efficiency and superior task performance. Second, we propose a
new direction of joint training efficient samplers/controllers of dLLMs policy.
Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by
letting the model learn to adaptively allocate an inference threshold for each
prompt. By jointly training the sampler, we yield better accuracies with lower
number of function evaluations (NFEs) compared to training the model only,
obtaining the best performance in improving the Pareto frontier of the
inference-time compute of dLLMs. We showcase the effectiveness of our pipeline
by training open source large diffusion language models over benchmark math and
planning tasks.

</details>


### [357] [C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale Recommendation Systems](https://arxiv.org/abs/2510.02215)
*Mertcan Cokbas,Ziteng Liu,Zeyi Tao,Chengkai Zhang,Elder Veliz,Qin Huang,Ellie Wen,Huayu Li,Qiang Jin,Murat Duman,Benjamin Au,Guy Lebanon,Sagar Chordia*

Main category: cs.LG

TL;DR: 本文提出了一种通过辅助学习揭示数据子结构并利用部分冲突的辅助标签来正则化共享表示的新方法C2AL，以解决大规模推荐模型中因用户群体异质性导致的学习不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 大规模推荐模型通常假设用户群体是同质的，但现实数据具有异质性，导致模型过度关注主流分布而忽略少数群体，影响学习效果和注意力权重活性。

Method: 利用注意力机制在因子分解机中的作用，识别具有显著分布差异的数据子结构，并引入部分冲突的辅助标签进行正则化，定制注意力层的学习过程以保留与少数群体的互信息。

Result: 在数十亿规模的真实生产数据上对六种SOTA模型进行了评估，C2AL使归一化熵整体降低最多0.16%，在目标少数群体上性能提升超过0.30%。

Conclusion: C2AL能有效改善推荐模型对细粒度用户-广告交互的捕捉能力，在提升全局性能的同时增强对少数群体的表征学习。

Abstract: Training large-scale recommendation models under a single global objective
implicitly assumes homogeneity across user populations. However, real-world
data are composites of heterogeneous cohorts with distinct conditional
distributions. As models increase in scale and complexity and as more data is
used for training, they become dominated by central distribution patterns,
neglecting head and tail regions. This imbalance limits the model's learning
ability and can result in inactive attention weights or dead neurons. In this
paper, we reveal how the attention mechanism can play a key role in
factorization machines for shared embedding selection, and propose to address
this challenge by analyzing the substructures in the dataset and exposing those
with strong distributional contrast through auxiliary learning. Unlike previous
research, which heuristically applies weighted labels or multi-task heads to
mitigate such biases, we leverage partially conflicting auxiliary labels to
regularize the shared representation. This approach customizes the learning
process of attention layers to preserve mutual information with minority
cohorts while improving global performance. We evaluated C2AL on massive
production datasets with billions of data points each for six SOTA models.
Experiments show that the factorization machine is able to capture fine-grained
user-ad interactions using the proposed method, achieving up to a 0.16%
reduction in normalized entropy overall and delivering gains exceeding 0.30% on
targeted minority cohorts.

</details>


### [358] [Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification](https://arxiv.org/abs/2510.02216)
*Zeqi Ye,Minshuo Chen*

Main category: cs.LG

TL;DR: 本文研究了基于扩散模型的条件扩散Transformer在时间序列缺失值填补中的统计效率，并提出了新的理论分析框架，用以量化缺失值的不确定性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的生成式填补方法在实践中表现出色，但其在捕捉时空依赖性方面的理论理解仍不足，本文旨在填补这一理论空白。

Method: 通过提出一种基于Transformer的条件分数函数逼近理论，推导出统计样本复杂度界，并构建缺失值的紧置信区域，同时分析缺失模式对填补效果的影响。

Result: 得出了样本复杂度上界，揭示了缺失模式对填补效率和准确性的显著影响，并通过混合掩码训练策略提升了模型性能。

Conclusion: 该工作为基于扩散模型的时间序列填补提供了理论支持，并提出了有效的训练策略以提升实际表现。

Abstract: Imputation methods play a critical role in enhancing the quality of practical
time-series data, which often suffer from pervasive missing values. Recently,
diffusion-based generative imputation methods have demonstrated remarkable
success compared to autoregressive and conventional statistical approaches.
Despite their empirical success, the theoretical understanding of how well
diffusion-based models capture complex spatial and temporal dependencies
between the missing values and observed ones remains limited. Our work
addresses this gap by investigating the statistical efficiency of conditional
diffusion transformers for imputation and quantifying the uncertainty in
missing values. Specifically, we derive statistical sample complexity bounds
based on a novel approximation theory for conditional score functions using
transformers, and, through this, construct tight confidence regions for missing
values. Our findings also reveal that the efficiency and accuracy of imputation
are significantly influenced by the missing patterns. Furthermore, we validate
these theoretical insights through simulation and propose a mixed-masking
training strategy to enhance the imputation performance.

</details>


### [359] [Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks](https://arxiv.org/abs/2510.02286)
*Ruohao Guo,Afshin Oroojlooy,Roshan Sridhar,Miguel Ballesteros,Alan Ritter,Dan Roth*

Main category: cs.LG

TL;DR: 本文提出了一种名为DialTree-RPO的基于策略强化学习与树搜索的框架，用于自动发现大语言模型在多轮对话中的安全漏洞，显著提升了攻击成功率并揭示了新的多轮攻击策略。


<details>
  <summary>Details</summary>
Motivation: 现有的安全测试方法大多集中在单轮攻击或依赖人工设计的模板，难以探索复杂多轮对话中可能出现的新型攻击路径，而研究表明大语言模型在多轮攻击下更脆弱，因此需要更系统的方法来发现这些漏洞。

Method: 提出DialTree-RPO，将多轮对话建模为序贯决策问题，结合强化学习和树搜索，在无需人工标注数据的情况下自主探索多样化的多轮攻击策略。

Result: 实验表明，该方法在10个目标模型上的攻击成功率（ASR）比现有最先进方法高出超过25.9%，并成功发现了多种新型攻击策略。

Conclusion: DialTree-RPO能有效、系统地发现大语言模型在多轮交互中的安全漏洞，为AI安全评估提供了更强大的自动化工具。

Abstract: Despite recent rapid progress in AI safety, current large language models
remain vulnerable to adversarial attacks in multi-turn interaction settings,
where attackers strategically adapt their prompts across conversation turns and
pose a more critical yet realistic challenge. Existing approaches that discover
safety vulnerabilities either rely on manual red-teaming with human experts or
employ automated methods using pre-defined templates and human-curated attack
data, with most focusing on single-turn attacks. However, these methods did not
explore the vast space of possible multi-turn attacks, failing to consider
novel attack trajectories that emerge from complex dialogue dynamics and
strategic conversation planning. This gap is particularly critical given recent
findings that LLMs exhibit significantly higher vulnerability to multi-turn
attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy
reinforcement learning framework integrated with tree search that autonomously
discovers diverse multi-turn attack strategies by treating the dialogue as a
sequential decision-making problem, enabling systematic exploration without
manually curated data. Through extensive experiments, our approach not only
achieves more than 25.9% higher ASR across 10 target models compared to
previous state-of-the-art approaches, but also effectively uncovers new attack
strategies by learning optimal dialogue policies that maximize attack success
across multiple turns.

</details>


### [360] [Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models](https://arxiv.org/abs/2510.02224)
*Ethan Baron,Boris Oreshkin,Ruijun Ma,Hanyu Zhang,Kari Torkkola,Michael W. Mahoney,Andrew Gordon Wilson,Tatiana Konstantinova*

Main category: cs.LG

TL;DR: 提出一种基于copula的方法，从现有的多步时间序列基础模型中高效生成准确且具有相关性的样本路径，相比自回归采样更快并减少雪球误差。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型仅预测每个时间步的独立边缘分布，无法生成具有真实相关结构的多步预测样本路径，且自回归采样效率低下。

Method: 采用基于copula的方法，在一次前向传播中生成具有相关性的样本路径，利用已有模型的预测结果构建联合预测分布。

Result: 该方法比自回归采样快几个数量级，显著提升样本路径质量，并有效缓解预测中的雪球误差问题。

Conclusion: 基于copula的方法能高效、准确地生成多步预测样本路径，为时间序列基础模型提供了更优的采样方案。

Abstract: Many time series applications require access to multi-step forecast
trajectories in the form of sample paths. Recently, time series foundation
models have leveraged multi-step lookahead predictions to improve the quality
and efficiency of multi-step forecasts. However, these models only predict
independent marginal distributions for each time step, rather than a full joint
predictive distribution. To generate forecast sample paths with realistic
correlation structures, one typically resorts to autoregressive sampling, which
can be extremely expensive. In this paper, we present a copula-based approach
to efficiently generate accurate, correlated sample paths from existing
multi-step time series foundation models in one forward pass. Our copula-based
approach generates correlated sample paths orders of magnitude faster than
autoregressive sampling, and it yields improved sample path quality by
mitigating the snowballing error phenomenon.

</details>


### [361] [Interactive Training: Feedback-Driven Neural Network Optimization](https://arxiv.org/abs/2510.02297)
*Wentao Zhang,Yang Young Lu,Yuntian Deng*

Main category: cs.LG

TL;DR: 本文提出了一个名为Interactive Training的开源框架，通过人类专家或自动化AI代理在神经网络训练过程中进行实时反馈干预，提升了训练稳定性、降低了对初始超参数的敏感性，并增强了对动态需求的适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统的神经网络训练方法缺乏应对训练过程中不稳定性或突发问题的灵活性，需要一种能够动态调整的训练机制。

Method: 提出Interactive Training框架，通过控制服务器在用户或AI代理与训练过程之间进行通信，支持在训练中动态调整优化器超参数、训练数据和模型检查点。

Result: 通过三个案例研究验证了该框架在提升训练稳定性、降低对初始超参数敏感性和增强适应性方面的有效性。

Conclusion: Interactive Training为未来实现AI代理自主监控训练日志、主动解决不稳定性问题和优化训练动态的新范式奠定了基础。

Abstract: Traditional neural network training typically follows fixed, predefined
optimization recipes, lacking the flexibility to dynamically respond to
instabilities or emerging training issues. In this paper, we introduce
Interactive Training, an open-source framework that enables real-time,
feedback-driven intervention during neural network training by human experts or
automated AI agents. At its core, Interactive Training uses a control server to
mediate communication between users or agents and the ongoing training process,
allowing users to dynamically adjust optimizer hyperparameters, training data,
and model checkpoints. Through three case studies, we demonstrate that
Interactive Training achieves superior training stability, reduced sensitivity
to initial hyperparameters, and improved adaptability to evolving user needs,
paving the way toward a future training paradigm where AI agents autonomously
monitor training logs, proactively resolve instabilities, and optimize training
dynamics.

</details>


### [362] [xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity](https://arxiv.org/abs/2510.02228)
*Maximilian Beck,Kajetan Schweighofer,Sebastian Böck,Sebastian Lehner,Sepp Hochreiter*

Main category: cs.LG

TL;DR: 本文研究了xLSTM与Transformer在训练和推理中的扩展行为，发现xLSTM在计算效率和长上下文场景下表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索新型架构xLSTM在大规模语言模型中的扩展规律，弥补以往对上下文长度影响的忽视。

Method: 通过IsoFLOP和参数拟合方法，在80M到7B参数范围内比较xLSTM与Transformer在不同训练token量下的扩展行为，并分析上下文长度对最优模型大小的影响。

Result: xLSTM在计算最优和过训练条件下均表现出优于Transformer的扩展性，且随着训练和推理上下文增长，其优势更加明显。

Conclusion: xLSTM在典型LLM场景中扩展性能优于Transformer，尤其在长上下文应用中更具前景。

Abstract: Scaling laws play a central role in the success of Large Language Models
(LLMs), enabling the prediction of model performance relative to compute
budgets prior to training. While Transformers have been the dominant
architecture, recent alternatives such as xLSTM offer linear complexity with
respect to context length while remaining competitive in the billion-parameter
regime. We conduct a comparative investigation on the scaling behavior of
Transformers and xLSTM along the following lines, providing insights to guide
future model design and deployment. First, we study the scaling behavior for
xLSTM in compute-optimal and over-training regimes using both IsoFLOP and
parametric fit approaches on a wide range of model sizes (80M-7B) and number of
training tokens (2B-2T). Second, we examine the dependence of optimal model
sizes on context length, a pivotal aspect that was largely ignored in previous
work. Finally, we analyze inference-time scaling characteristics. Our findings
reveal that in typical LLM training and inference scenarios, xLSTM scales
favorably compared to Transformers. Importantly, xLSTM's advantage widens as
training and inference contexts grow.

</details>


### [363] [PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks](https://arxiv.org/abs/2510.02236)
*Ricardo Misael Ayala Molina,Hyame Assem Alameddine,Makan Pourzandi,Chadi Assi*

Main category: cs.LG

TL;DR: 提出了一种基于正样本无标签学习（PUL）的异常检测方案PUL-Inter-Slice Defender，用于防御5G网络中的分布式切片移动（DSM）攻击，结合LSTM自编码器和K-Means聚类，在含噪声训练数据下仍实现超过98.5%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 5G网络中用户设备在多个网络切片间切换的灵活性带来了分布式切片移动（DSM）攻击的风险，现有方法难以在训练数据被污染的情况下有效检测此类攻击。

Method: 提出PUL-Inter-Slice Defender，利用3GPP关键性能指标和计数器作为特征，结合LSTM自编码器与K-Means聚类，并采用正样本无标签学习（PUL）框架，在仅有正常样本和未标记样本的情况下进行异常检测。

Result: 在基于free5GC和UERANSIM搭建的5G测试平台上评估显示，该方法在训练数据包含10%至40%攻击污染时，F1分数仍超过98.5%，优于Inter-Slice Defender及其他基于OCSVM与随机森林、XGBoost的PUL方法。

Conclusion: PUL-Inter-Slice Defender能有效抵御DSM攻击，具备强鲁棒性和高检测精度，适用于训练数据存在污染的实际5G网络环境。

Abstract: Network Slices (NSs) are virtual networks operating over a shared physical
infrastructure, each designed to meet specific application requirements while
maintaining consistent Quality of Service (QoS). In Fifth Generation (5G)
networks, User Equipment (UE) can connect to and seamlessly switch between
multiple NSs to access diverse services. However, this flexibility, known as
Inter-Slice Switching (ISS), introduces a potential vulnerability that can be
exploited to launch Distributed Slice Mobility (DSM) attacks, a form of
Distributed Denial of Service (DDoS) attack. To secure 5G networks and their
NSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an
anomaly detection solution that leverages Positive Unlabeled Learning (PUL) and
incorporates a combination of Long Short-Term Memory Autoencoders and K-Means
clustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership
Project (3GPP) key performance indicators and performance measurement counters
as features for its machine learning models to detect DSM attack variants while
maintaining robustness in the presence of contaminated training data. When
evaluated on data collected from our 5G testbed based on the open-source
free5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;
PUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training
datasets with 10% to 40% attack contamination, consistently outperforming its
counterpart Inter-Slice Defender and other PUL based solutions combining
One-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.

</details>


### [364] [Drop-Muon: Update Less, Converge Faster](https://arxiv.org/abs/2510.02239)
*Kaja Gruntkowska,Yassine Maziane,Zheng Qu,Peter Richtárik*

Main category: cs.LG

TL;DR: 本文提出了一种名为Drop-Muon的非欧几里得随机渐进训练方法，通过每步仅更新部分网络层来挑战传统全网络更新的优化范式，在理论和实验上证明了其优于全网络更新的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习优化器在每一步都更新所有网络层，但这种做法可能在理论上和实践中都不够高效。作者旨在挑战这一默认假设，探索更高效的训练范式。

Method: 提出Drop-Muon方法，采用随机调度策略在每一步仅更新部分网络层，结合渐进式训练的效率与针对各层的非欧几里得更新机制，并提供在层间平滑性和(L^0, L^1)-平滑性条件下的收敛性理论保证。

Result: 理论分析表明全网络更新并非最优，除非满足特定的层平滑常数关系；实验显示Drop-Muon在保持相同精度的同时，比全网络更新的Muon快达1.4倍（以实际运行时间计）。

Conclusion: Drop-Muon为大规模模型训练提供了高效且理论支持的新范式，挑战了全网络更新的传统做法，标志着渐进式训练在随机和非光滑场景下的首次理论突破。

Abstract: Conventional wisdom in deep learning optimization dictates updating all
layers at every step-a principle followed by all recent state-of-the-art
optimizers such as Muon. In this work, we challenge this assumption, showing
that full-network updates can be fundamentally suboptimal, both in theory and
in practice. We introduce a non-Euclidean Randomized Progressive Training
method-Drop-Muon-a simple yet powerful framework that updates only a subset of
layers per step according to a randomized schedule, combining the efficiency of
progressive training with layer-specific non-Euclidean updates for top-tier
performance. We provide rigorous convergence guarantees under both layer-wise
smoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and
stochastic gradient settings, marking the first such results for progressive
training in the stochastic and non-smooth regime. Our cost analysis further
reveals that full-network updates are not optimal unless a very specific
relationship between layer smoothness constants holds. Through controlled CNN
experiments, we empirically demonstrate that Drop-Muon consistently outperforms
full-network Muon, achieving the same accuracy up to $1.4\times$ faster in
wall-clock time. Together, our results suggest a shift in how large-scale
models can be efficiently trained, challenging the status quo and offering a
highly efficient, theoretically grounded alternative to full-network updates.

</details>


### [365] [Transformers Discover Molecular Structure Without Graph Priors](https://arxiv.org/abs/2510.02259)
*Tobias Kreiman,Yutong Bai,Fadi Atieh,Elizabeth Weaver,Eric Qu,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 纯Transformer模型在无预定义图结构的情况下，通过笛卡尔坐标直接学习分子能量和力，表现与最先进的等变GNN相当，并展现出可解释的物理一致性注意力模式。


<details>
  <summary>Details</summary>
Motivation: 探索无需硬编码图结构或物理先验的纯Transformer是否能有效建模分子性质，挑战图神经网络在分子机器学习中的必要性。

Method: 使用标准Transformer直接在分子的笛卡尔坐标上训练，预测分子能量和力，在OMol25数据集上与最先进的等变GNN在相同计算预算下进行对比。

Result: Transformer达到了与GNN相当的能量和力预测精度（平均绝对误差），注意力权重随原子间距离呈反向衰减，且能自适应不同分子环境；并表现出良好的训练资源扩展性。

Conclusion: 许多GNN的优势特性可在Transformer中自适应涌现，表明硬编码图结构并非必需，标准化、可扩展的Transformer架构有望成为分子建模的新方向。

Abstract: Graph Neural Networks (GNNs) are the dominant architecture for molecular
machine learning, particularly for molecular property prediction and machine
learning interatomic potentials (MLIPs). GNNs perform message passing on
predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor
scheme. While this design aligns with the locality present in many molecular
tasks, a hard-coded graph can limit expressivity due to the fixed receptive
field and slows down inference with sparse graph operations. In this work, we
investigate whether pure, unmodified Transformers trained directly on Cartesian
coordinates$\unicode{x2013}$without predefined graphs or physical
priors$\unicode{x2013}$can approximate molecular energies and forces. As a
starting point for our analysis, we demonstrate how to train a Transformer to
competitive energy and force mean absolute errors under a matched training
compute budget, relative to a state-of-the-art equivariant GNN on the OMol25
dataset. We discover that the Transformer learns physically consistent
patterns$\unicode{x2013}$such as attention weights that decay inversely with
interatomic distance$\unicode{x2013}$and flexibly adapts them across different
molecular environments due to the absence of hard-coded biases. The use of a
standard Transformer also unlocks predictable improvements with respect to
scaling training resources, consistent with empirical scaling laws observed in
other domains. Our results demonstrate that many favorable properties of GNNs
can emerge adaptively in Transformers, challenging the necessity of hard-coded
graph inductive biases and pointing toward standardized, scalable architectures
for molecular modeling.

</details>


### [366] [How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning](https://arxiv.org/abs/2510.02265)
*Yalin E. Sagduyu,Tugba Erpek,Kemal Davaslioglu,Sastry Kompella*

Main category: cs.LG

TL;DR: 该论文研究了如何通过强化学习（RL）来缓解动态反应式干扰问题，使发射-接收对能够自适应地调整传输功率、调制方式和信道选择以最大化吞吐量。


<details>
  <summary>Details</summary>
Motivation: 反应式干扰者会动态选择信道和感知阈值进行干扰，传统防御方法难以应对未知的干扰策略和信道条件，因此需要一种能在线学习并适应的机制。

Method: 采用Q-learning处理离散的干扰状态，使用深度Q网络（DQN）处理基于接收功率的连续状态，并通过不同的奖励函数和动作集设计来优化性能。

Result: 实验结果表明，所提出的方法能够快速适应频谱动态变化，在信道和干扰策略随时间变化的情况下仍能保持高通信速率。

Conclusion: 强化学习是一种有效的手段，可用于对抗动态反应式干扰，实现鲁棒的抗干扰通信。

Abstract: This paper studies the problem of mitigating reactive jamming, where a jammer
adopts a dynamic policy of selecting channels and sensing thresholds to detect
and jam ongoing transmissions. The transmitter-receiver pair learns to avoid
jamming and optimize throughput over time (without prior knowledge of channel
conditions or jamming strategies) by using reinforcement learning (RL) to adapt
transmit power, modulation, and channel selection. Q-learning is employed for
discrete jamming-event states, while Deep Q-Networks (DQN) are employed for
continuous states based on received power. Through different reward functions
and action sets, the results show that RL can adapt rapidly to spectrum
dynamics and sustain high rates as channels and jamming policies change over
time.

</details>


### [367] [Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps](https://arxiv.org/abs/2510.02274)
*Kyoungjun Park,Yifan Yang,Changhan Ge,Lili Qiu,Shiqi Jiang*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的方法Diffusion^2，利用3D点云建模多频段射频信号传播，结合RF-3D编码器和多尺度嵌入，在合成与真实场景中实现高精度（误差仅1.9 dB）且比现有方法快27倍的射频信号预测。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机受限于可见光谱、视角和遮挡，无法全面感知环境；而射频（RF）信号能提供更丰富的环境信息。然而，复杂环境中RF信号受障碍物吸收、反射等影响，精确建模仍具挑战。因此，需要一种能准确、高效预测多频段RF信号传播的方法。

Method: 提出Diffusion^2，一种基于扩散模型的RF信号传播建模方法。该方法使用3D点云作为输入，设计了RF-3D Encoder来提取与射频信号相关的3D几何特征和信号细节，并通过多尺度嵌入机制模拟真实的RF信号传播过程，从而实现对Wi-Fi到毫米波等宽频段信号的预测。

Result: 在合成和真实世界数据上的实验表明，Diffusion^2能够准确估计多种频率和环境条件下的RF信号行为，平均误差仅为1.9 dB，且推理速度比现有方法快27倍。

Conclusion: Diffusion^2通过结合3D点云与扩散模型，显著提升了复杂环境中RF信号传播预测的精度与效率，为无线网络部署、诊断与优化提供了强有力的技术支持，是该领域的重要进展。

Abstract: Modeling radio frequency (RF) signal propagation is essential for
understanding the environment, as RF signals offer valuable insights beyond the
capabilities of RGB cameras, which are limited by the visible-light spectrum,
lens coverage, and occlusions. It is also useful for supporting wireless
diagnosis, deployment, and optimization. However, accurately predicting RF
signals in complex environments remains a challenge due to interactions with
obstacles such as absorption and reflection. We introduce Diffusion^2, a
diffusion-based approach that uses 3D point clouds to model the propagation of
RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.
To effectively capture RF-related features from 3D data, we present the RF-3D
Encoder, which encapsulates the complexities of 3D geometry along with
signal-specific details. These features undergo multi-scale embedding to
simulate the actual RF signal dissemination process. Our evaluation, based on
synthetic and real-world measurements, demonstrates that Diffusion^2 accurately
estimates the behavior of RF signals in various frequency bands and
environmental conditions, with an error margin of just 1.9 dB and 27x faster
than existing methods, marking a significant advancement in the field. Refer to
https://rfvision-project.github.io/ for more information.

</details>


### [368] [Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks](https://arxiv.org/abs/2510.02278)
*Fedor Velikonivtsev,Oleg Platonov,Gleb Bazhenov,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: 本文提出了两个大规模、更真实的城市交通预测数据集，包含近10万个道路段和丰富的道路特征，解决了现有数据集在连通性、规模和城市复杂性方面的不足，并提出一种无需专门时间序列模块的可扩展图神经网络方法，在性能和效率上均表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测数据集存在道路连通信息缺失、道路属性有限、路段数量少且多集中于城际高速，难以反映城市密集路网和复杂交通模式，缺乏对方法可扩展性的有效评估。

Method: 发布两个大城市的真实道路网络数据集，包含丰富的道路特征和细粒度的交通流量与速度数据；提出一种基于图神经网络（GNN）的新方法，不依赖专门的时间序列处理模块，提升模型可扩展性。

Result: 新数据集规模达近10万路段，较现有数据集扩大十倍以上；实验表明主流时空神经模型难以扩展到该规模，而所提GNN方法在可扩展性和预测性能上均优于现有方法。

Conclusion: 所提出的大型真实数据集为交通预测研究提供了更具挑战性和实用性的基准，所设计的轻量高效GNN架构为大规模交通预测提供了新的可行方案，推动了该领域的进一步发展。

Abstract: Traffic forecasting on road networks is a complex task of significant
practical importance that has recently attracted considerable attention from
the machine learning community, with spatiotemporal graph neural networks
(GNNs) becoming the most popular approach. The proper evaluation of traffic
forecasting methods requires realistic datasets, but current publicly available
benchmarks have significant drawbacks, including the absence of information
about road connectivity for road graph construction, limited information about
road properties, and a relatively small number of road segments that falls
short of real-world applications. Further, current datasets mostly contain
information about intercity highways with sparsely located sensors, while city
road networks arguably present a more challenging forecasting task due to much
denser roads and more complex urban traffic patterns. In this work, we provide
a more complete, realistic, and challenging benchmark for traffic forecasting
by releasing datasets representing the road networks of two major cities, with
the largest containing almost 100,000 road segments (more than a 10-fold
increase relative to existing datasets). Our datasets contain rich road
features and provide fine-grained data about both traffic volume and traffic
speed, allowing for building more holistic traffic forecasting systems. We show
that most current implementations of neural spatiotemporal models for traffic
forecasting have problems scaling to datasets of our size. To overcome this
issue, we propose an alternative approach to neural traffic forecasting that
uses a GNN without a dedicated module for temporal sequence processing, thus
achieving much better scalability, while also demonstrating stronger
forecasting performance. We hope our datasets and modeling insights will serve
as a valuable resource for research in traffic forecasting.

</details>


### [369] [Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation](https://arxiv.org/abs/2510.02279)
*Mykyta Ielanskyi,Kajetan Schweighofer,Lukas Aichberger,Sepp Hochreiter*

Main category: cs.LG

TL;DR: 本文提出了一种更稳健的评估自然语言生成中不确定性估计方法的框架，通过使用多种风险指标、减少评估偏差以及引入Elo评分系统来提高对大模型幻觉（尤其是confabulations）检测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 由于现有评估方法在正确性判断上存在较大分歧，导致不确定性估计方法的性能排名不可靠，容易被夸大，因此需要更鲁棒的评估方式。

Method: 提出了多种替代风险指标，采用多个LLM-as-a-judge变体的边际化处理以减少偏差，并探索结构化任务和分布外检测任务作为可控风险指标，最后引入Elo评分系统对方法进行综合评价。

Result: 新方法在问答任务中减少了评估偏差，结构化和分布外任务提供了更可靠的风险信号，Elo评分能有效汇总多场景下的性能表现。

Conclusion: 所提框架显著提升了不确定性估计方法评估的稳健性和客观性，有助于更准确地检测大语言模型中的confabulation类幻觉。

Abstract: Hallucinations are a common issue that undermine the reliability of large
language models (LLMs). Recent studies have identified a specific subset of
hallucinations, known as confabulations, which arise due to predictive
uncertainty of LLMs. To detect confabulations, various methods for estimating
predictive uncertainty in natural language generation (NLG) have been
developed. These methods are typically evaluated by correlating uncertainty
estimates with the correctness of generated text, with question-answering (QA)
datasets serving as the standard benchmark. However, commonly used approximate
correctness functions have substantial disagreement between each other and,
consequently, in the ranking of the uncertainty estimation methods. This allows
one to inflate the apparent performance of uncertainty estimation methods. We
propose using several alternative risk indicators for risk correlation
experiments that improve robustness of empirical assessment of UE algorithms
for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge
variants leads to reducing the evaluation biases. Furthermore, we explore
structured tasks as well as out of distribution and perturbation detection
tasks which provide robust and controllable risk indicators. Finally, we
propose to use an Elo rating of uncertainty estimation methods to give an
objective summarization over extensive evaluation settings.

</details>


### [370] [Knowledge Distillation Detection for Open-weights Models](https://arxiv.org/abs/2510.02302)
*Qin Shi,Amber Yijia Zheng,Qifan Song,Raymond A. Yeh*

Main category: cs.LG

TL;DR: 本文提出了知识蒸馏检测任务，旨在仅利用学生模型权重和教师模型API的情况下，判断学生模型是否从特定教师模型蒸馏而来。


<details>
  <summary>Details</summary>
Motivation: 由于对模型来源和通过蒸馏进行未经授权复制的担忧日益增加，需要一种方法来检测模型是否经过知识蒸馏。

Method: 提出了一种模型无关的框架，结合无数据输入合成和统计分数计算来进行蒸馏检测，适用于分类和生成模型。

Result: 在图像分类和文本到图像生成的不同架构上实验表明，该方法在CIFAR-10上检测准确率比最强基线提高59.6%，ImageNet上提高71.2%，文本到图像生成任务上提高20.0%。

Conclusion: 所提出的框架能有效检测知识蒸馏，且在多种模型架构和任务中显著优于现有方法。

Abstract: We propose the task of knowledge distillation detection, which aims to
determine whether a student model has been distilled from a given teacher,
under a practical setting where only the student's weights and the teacher's
API are available. This problem is motivated by growing concerns about model
provenance and unauthorized replication through distillation. To address this
task, we introduce a model-agnostic framework that combines data-free input
synthesis and statistical score computation for detecting distillation. Our
approach is applicable to both classification and generative models.
Experiments on diverse architectures for image classification and text-to-image
generation show that our method improves detection accuracy over the strongest
baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image
generation. The code is available at
https://github.com/shqii1j/distillation_detection.

</details>


### [371] [Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive](https://arxiv.org/abs/2510.02305)
*Tyler Farghly,Peter Potaptchik,Samuel Howard,George Deligiannidis,Jakiw Pidstrigach*

Main category: cs.LG

TL;DR: 该论文研究了扩散模型在不同领域中表现出强大泛化能力的机制，提出其成功可能源于对数据低维流形结构的适应，并通过分数匹配的学习框架加以解释。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型表现出卓越的性能，但其背后机制尚不完全清楚，特别是它们如何利用数据的低维几何结构进行泛化。

Method: 基于流形假设，作者通过分析分数匹配目标函数的平滑最小化，研究隐式正则化的作用，理论和实证分析平滑操作在对数密度域和数据流形上的影响。

Result: 理论和实验结果表明，对分数函数（或对数密度）的平滑会在数据流形上产生切向平滑，并且可以通过选择适当的平滑方式来控制扩散模型泛化的流形。

Conclusion: 扩散模型的强大泛化能力部分归因于其在分数匹配框架下对数据流形结构的隐式适应，而平滑策略可被用来调控这一过程。

Abstract: Diffusion models have achieved state-of-the-art performance, demonstrating
remarkable generalisation capabilities across diverse domains. However, the
mechanisms underpinning these strong capabilities remain only partially
understood. A leading conjecture, based on the manifold hypothesis, attributes
this success to their ability to adapt to low-dimensional geometric structure
within the data. This work provides evidence for this conjecture, focusing on
how such phenomena could result from the formulation of the learning problem
through score matching. We inspect the role of implicit regularisation by
investigating the effect of smoothing minimisers of the empirical score
matching objective. Our theoretical and empirical results confirm that
smoothing the score function -- or equivalently, smoothing in the log-density
domain -- produces smoothing tangential to the data manifold. In addition, we
show that the manifold along which the diffusion model generalises can be
controlled by choosing an appropriate smoothing.

</details>


### [372] [Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization](https://arxiv.org/abs/2510.02308)
*Dhruv Kohli,Sawyer J. Robertson,Gal Mishne,Alexander Cloninger*

Main category: cs.LG

TL;DR: 提出了一种名为LEGO的谱方法，利用图拉普拉斯算子低频特征向量的梯度正交化来估计数据流形的切空间，相比传统的LPCA在高噪声环境下更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 传统局部主成分分析（LPCA）在高噪声情况下因邻域大小选择困难而表现不佳，且需要对数据几何和噪声特性有先验知识，这在实际中往往不可得。

Method: 提出Laplacian Eigenvector Gradient Orthogonalization (LEGO)，通过正交化图拉普拉斯算子低频特征向量的梯度来估计每个数据点的切空间，利用数据的全局结构指导局部估计。

Result: 理论分析表明低频拉普拉斯特征函数的梯度与流形切丛对齐，且对子高斯噪声具有鲁棒性；实验显示LEGO在高噪声下比LPCA显著更优，提升了流形学习、边界检测和本征维数估计等任务的表现。

Conclusion: LEGO通过结合全局谱结构有效提高了切空间估计在噪声环境下的鲁棒性和准确性，为流形学习中的基础问题提供了新思路。

Abstract: Estimating the tangent spaces of a data manifold is a fundamental problem in
data analysis. The standard approach, Local Principal Component Analysis
(LPCA), struggles in high-noise settings due to a critical trade-off in
choosing the neighborhood size. Selecting an optimal size requires prior
knowledge of the geometric and noise characteristics of the data that are often
unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector
Gradient Orthogonalization (LEGO), that utilizes the global structure of the
data to guide local tangent space estimation. Instead of relying solely on
local neighborhoods, LEGO estimates the tangent space at each data point by
orthogonalizing the gradients of low-frequency eigenvectors of the graph
Laplacian. We provide two theoretical justifications of our method. First, a
differential geometric analysis on a tubular neighborhood of a manifold shows
that gradients of the low-frequency Laplacian eigenfunctions of the tube align
closely with the manifold's tangent bundle, while an eigenfunction with high
gradient in directions orthogonal to the manifold lie deeper in the spectrum.
Second, a random matrix theoretic analysis also demonstrates that low-frequency
eigenvectors are robust to sub-Gaussian noise. Through comprehensive
experiments, we demonstrate that LEGO yields tangent space estimates that are
significantly more robust to noise than those from LPCA, resulting in marked
improvements in downstream tasks such as manifold learning, boundary detection,
and local intrinsic dimension estimation.

</details>


### [373] [KaVa: Latent Reasoning via Compressed KV-Cache Distillation](https://arxiv.org/abs/2510.02312)
*Anna Kuzina,Maciej Pioro,Paul N. Whatmough,Babak Ehteshami Bejnordi*

Main category: cs.LG

TL;DR: 提出KaVa框架，通过从教师模型的压缩KV缓存中进行自蒸馏，为潜在推理学生提供监督信号，有效结合了思维链的准确性和潜在推理的高效性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多步推理中依赖显式思维链（CoT），但存在计算开销大、冗余信息多的问题；而潜在推理虽高效，却因缺乏监督在复杂自然语言推理上效果有限。

Method: 提出KaVa框架，利用压缩的KV缓存中的抽象知识作为监督信号，通过自蒸馏方式将教师模型的逐步KV轨迹传递给潜在推理学生，使用连续潜在令牌实现对齐。

Result: KaVa在多个基准上优于强基线，对从公式到自然语言推理的退化更小，并能扩展到更大的模型同时保持效率。

Conclusion: 压缩KV缓存蒸馏是一种可扩展的潜在推理监督方法，兼顾准确性与推理效率。

Abstract: Large Language Models (LLMs) excel at multi-step reasoning problems with
explicit chain-of-thought (CoT), but verbose traces incur significant
computational costs and memory overhead, and often carry redundant, stylistic
artifacts. Latent reasoning has emerged as an efficient alternative that
internalizes the thought process, but it suffers from a critical lack of
supervision, limiting its effectiveness on complex, natural-language reasoning
traces. In this work, we propose KaVa, the first framework that bridges this
gap by distilling knowledge directly from a compressed KV-cache of the teacher
into a latent-reasoning student via self-distillation, leveraging the
representational flexibility of continuous latent tokens to align stepwise KV
trajectories. We show that the abstract, unstructured knowledge within
compressed KV-cache, which lacks direct token correspondence, can serve as a
rich supervisory signal for a latent reasoning student. Empirically, the
approach consistently outperforms strong latent baselines, exhibits markedly
smaller degradation from equation-only to natural-language traces, and scales
to larger backbones while preserving efficiency. These results establish
compressed KV-cache distillation as a scalable supervision signal for latent
reasoning, combining the accuracy of CoT-trained teachers with the efficiency
and deployability of latent inference.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [374] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体辩论的新型评估框架，用于衡量大语言模型作为自主智能体时的社交与认知行为。


<details>
  <summary>Details</summary>
Motivation: 传统基准无法捕捉LLM在交互环境中表现出的社会和认知动态，因此需要新的评估方法。

Method: 构建多智能体辩论系统，使用具有不同人格和动机的LLM智能体，在LLM主持人的监督下讨论复杂议题，并引入心理测量与语义分析指标进行量化分析。

Result: 发现智能体在无明确指示下仍倾向于达成共识（语义一致性μ > 0.88）；设定的人格可产生稳定的心理特征；主持人人格能显著影响辩论结果。

Conclusion: 该框架为评估自主型AI提供了动态、基于心理测量的新范式，对理解并引导下一代AI的社会行为具有重要意义。

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [375] [OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models](https://arxiv.org/abs/2510.01253)
*Jianzhang Zhang,Jialong Zhou,Chuang Liu*

Main category: cs.AI

TL;DR: OR-Toolformer通过微调Llama-3.1-8B-Instruct并结合外部求解器，实现了在运筹学任务上的高性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在运筹学任务中依赖闭源API带来的隐私问题，以及从零训练开源模型的高计算成本。

Method: 提出OR-Toolformer，采用半自动数据合成 pipeline 生成多样化的运筹学问题-答案对，并通过外部求解器增强模型以生成API调用。

Result: 在四个标准基准中的三个上，OR-Toolformer达到最高80.1%的执行准确率，超过同规模基线模型4.3%以上；在两种未见过的零样本运筹学问题上，平均准确率达54%，比最强基线提升21个百分点。

Conclusion: 验证了工具增强的微调方法在提升语言模型运筹学建模与求解准确性及泛化能力方面的有效性。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning, but
reliance on closed-source APIs for OR tasks raises privacy concerns, and
training open-source models from scratch incurs high compute costs. We
introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a
semi-automatic data synthesis pipeline that generates diverse OR problem-answer
pairs and augments the model with external solvers to produce API calls. On
three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot
evaluation on two unseen OR problem types, it attains 54% average accuracy, a
21 percentage-point improvement over the strongest baseline. These findings
validate the efficacy of tool-augmented fine-tuning LLMs for accurate and
generalizable OR problem modeling and solving.

</details>


### [376] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 提出ROTE算法，通过大语言模型生成行为程序假设空间，并结合概率推理来预测人类行为，在稀疏观测下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有行为预测方法往往数据需求高、适应性差，或对理性假设不切实际，难以在真实场景中稳健预测人类行为。

Method: 将日常行为建模为可执行的‘行为程序’，利用大语言模型生成假设空间，结合概率推断在该空间中进行推理。

Result: 在网格世界任务和大规模具身家庭模拟器中，ROTE在样本内准确率和样本外泛化能力上比基线方法（如行为克隆和LLM方法）最高提升50%。

Conclusion: 将行为理解视为程序合成问题，为AI系统高效、有效地预测真实世界人类行为提供了新路径。

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [377] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,Clémentine Bouleau,Vivian Tsai,Maël Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: 本研究提出了一种评估大语言模型（LLM）在集体决策中与人类社会推理对齐的实证框架，通过“迷失在海上”任务和人类实验数据模拟LLM群体行为，发现不同模型在集体对齐上表现各异，强调上下文、线索和模型特异性归纳偏见的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地用于建模和增强集体决策，亟需检验其在集体层面的社会推理对齐性，而不仅仅是个体层面的对齐。

Method: 采用‘迷失在海上’社会心理学任务，在N=748的大规模在线实验中随机分配群体使用可见人口属性或匿名别名进行领导选举，并基于人类数据模拟匹配的LLM群体行为，对比Gemini 2.5、GPT 4.1、Claude Haiku 3.5和Gemma 3的表现。

Result: 不同LLM在集体推理中的行为存在差异：一些模型复制了人类偏见，另一些则掩盖并试图补偿这些偏见；结果显示人-AI对齐受情境、线索和模型特异性归纳偏见影响显著。

Conclusion: 理解LLM如何与集体人类行为对齐对于推进社会对齐AI至关重要，需要能够捕捉集体推理复杂性的动态基准。

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [378] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: 提出了一种名为Cyber Academia-Chemical Engineering (CA-ChemE)的多智能体系统，通过知识库增强和协作代理促进化学工程领域的自主科研与跨学科发现。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在跨学科协作和探索未知问题方面能力有限，难以支持化学工程中复杂的科研需求。

Method: 构建一个包含领域知识库、知识增强技术和协作代理的多智能体系统（CA-ChemE），模拟自驱动的研究演化过程，并引入具备本体工程能力的协作代理以提升跨领域协作效率。

Result: 知识库增强机制使7个专家代理的对话质量平均提升10-15%；协作代理对远距离领域专家对的协作效率提升达8.5%，是邻近领域提升（0.8%）的10.6倍，揭示了‘知识库差距导致协作效率下降’的现象。

Conclusion: 精心设计的多智能体架构可通过知识增强与协作机制，有效推动化学工程中的自主科学发现，尤其在跨学科场景下具有显著潜力。

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [379] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: 本文提出AGILE方法，通过将拼图求解建模为交互式过程，利用可执行代码生成和环境反馈来增强视觉语言模型的感知与推理能力，显著提升拼图任务性能并实现跨9个视觉任务的良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在基本感知和推理能力上存在不足，尤其在简单拼图任务上表现接近随机，且高质量多模态数据稀缺限制了能力提升。

Method: 将拼图任务构建为交互式学习过程，模型每步生成可执行代码进行动作，并从环境中获得细粒度视觉反馈，通过迭代的观察与交互实现探索与学习。

Result: 在2×2拼图设置下准确率从9.5%提升至82.8%，并在9个通用视觉任务上平均提升3.1%，展现出强泛化能力。

Conclusion: AGILE为提升多模态模型的感知、推理与泛化能力提供了高效且可扩展的新路径，同时缓解了多模态强化学习数据稀缺的问题。

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [380] [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
*Tudor Achim,Alex Best,Kevin Der,Mathïs Fédérico,Sergei Gukov,Daniel Halpern-Leister,Kirsten Henningsgard,Yury Kudryashov,Alexander Meiburg,Martin Michelsen,Riley Patterson,Eric Rodriguez,Laura Scharff,Vikram Shanker,Vladmir Sicca,Hari Sowrirajan,Aidan Swope,Matyas Tamas,Vlad Tenev,Jonathan Thomm,Harold Williams,Lawrence Wu*

Main category: cs.AI

TL;DR: Aristotle 是一个结合形式验证与非形式推理的AI系统，在2025年国际数学奥林匹克竞赛问题上达到金牌水平表现。


<details>
  <summary>Details</summary>
Motivation: 旨在提升自动定理证明系统的性能，解决复杂数学问题，尤其是需要直觉与严格证明结合的场景。

Method: 整合了Lean证明搜索系统、生成并形式化引理的非形式推理系统，以及专用几何求解器。

Result: 在IMO 2025问题上表现出色，达到金牌选手水平，且具备良好的扩展性。

Conclusion: Aristotle 展示了形式化与非形式化方法结合在自动推理中的巨大潜力，为未来数学自动化证明提供了可行路径。

Abstract: We introduce Aristotle, an AI system that combines formal verification with
informal reasoning, achieving gold-medal-equivalent performance on the 2025
International Mathematical Olympiad problems. Aristotle integrates three main
components: a Lean proof search system, an informal reasoning system that
generates and formalizes lemmas, and a dedicated geometry solver. Our system
demonstrates state-of-the-art performance with favorable scaling properties for
automated theorem proving.

</details>


### [381] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK是一个用于评估多平台代理环境中长期记忆和状态跟踪能力的新基准，模拟真实组织工作流程，整合了Slack、Linear和Git等平台的异步事件，测试记忆获取、选择与冲突解决能力，并提出正确性、效率和冗余度等指标。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注对话场景中的记忆评估，缺乏对动态企业环境中记忆能力的有效评测，难以满足实际应用需求。

Method: 设计并构建MEMTRACK基准，结合专家手动设计与基于代理的可扩展合成方法生成贴近现实软件开发过程的场景，集成多个平台的时间交错、信息噪声与冲突的数据流，并定义正确性、效率和冗余度等评估指标。

Result: 实验表明当前最先进的LLM和记忆后端在长时程记忆、跨平台依赖处理和矛盾信息解析方面存在挑战，表现最好的GPT-5模型在正确性上仅达到60%。

Conclusion: MEMTRACK为超越对话式设置的记忆增强代理提供了可扩展的评估框架，推动复杂组织环境下多代理、多平台记忆评测的研究发展。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [382] [Retrieval-Augmented Framework for LLM-Based Clinical Decision Support](https://arxiv.org/abs/2510.01363)
*Leon Garza,Anantaa Kotal,Michael A. Grasso,Emre Umucu*

Main category: cs.AI

TL;DR: 本文提出一种基于大语言模型（LLM）的临床决策支持系统，利用检索增强生成（RAG）技术整合电子健康记录中的结构化与非结构化数据，辅助医生制定治疗方案。


<details>
  <summary>Details</summary>
Motivation: 随着电子健康记录（EHR）数据快速增长，临床决策日益复杂，亟需智能化工具辅助医生进行数据驱动的诊疗决策。

Method: 采用检索增强生成（RAG）框架，结合自然语言处理与结构化临床输入，从历史EHR数据中提取相似病例并生成个性化治疗建议。

Result: 在去标识化和合成数据集上的初步评估显示，该系统生成的治疗建议具有临床合理性和一致性。

Conclusion: 基于LLM的决策支持系统有望在处方流程中提供有价值的辅助，但需严格验证以确保安全性、透明性并与现有临床实践保持一致。

Abstract: The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.

</details>


### [383] [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
*Xinpeng Wang,Nitish Joshi,Barbara Plank,Rico Angell,He He*

Main category: cs.AI

TL;DR: 提出TRACE方法，通过截断推理链并测量验证通过率来检测隐式奖励劫持，具有高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 隐式奖励劫持行为难以被现有基于推理链监控的方法发现，需要一种能有效识别模型是否走捷径的新方法。

Method: 提出TRACE方法，通过逐步截断模型的推理链，强制回答，并测量不同截断长度下的验证通过率，利用准确率随长度变化的曲线下面积判断是否存在奖励劫持。

Result: 在数学推理任务中比最强的72B CoT监控方法提升超65%，在编码任务中比32B监控方法提升超30%，并能在训练中发现未知漏洞。

Conclusion: TRACE提供了一种可扩展的无监督监督方法，在现有监控方法失效时仍能有效检测隐式奖励劫持。

Abstract: Reward hacking, where a reasoning model exploits loopholes in a reward
function to achieve high rewards without solving the intended task, poses a
significant threat. This behavior may be explicit, i.e. verbalized in the
model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus
bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE
(Truncated Reasoning AUC Evaluation). Our key observation is that hacking
occurs when exploiting the loophole is easier than solving the actual task.
This means that the model is using less `effort' than required to achieve high
reward. TRACE quantifies effort by measuring how early a model's reasoning
becomes sufficient to pass a verifier. We progressively truncate a model's CoT
at various lengths, force the model to answer, and measure the verifier-passing
rate at each cutoff. A hacking model, which takes a shortcut, will achieve a
high passing rate with only a small fraction of its CoT, yielding a large area
under the accuracy-vs-length curve. TRACE achieves over 65% gains over our
strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B
monitor in coding. We further show that TRACE can discover unknown loopholes
during training. Overall, TRACE offers a scalable unsupervised approach for
oversight where current monitoring methods prove ineffective.

</details>


### [384] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: 提出一种通过蒸馏将推理时检索转化为学习能力的简单管道，使学生模型在去除提示的情况下学习改进的教师轨迹，从而在多步任务中超越基线并减少对运行时检索的依赖。


<details>
  <summary>Details</summary>
Motivation: LLM代理在执行多步任务时常因前提条件未满足、命令冗余或环境约束处理不当而失败；虽然检索增强生成（RAG）可提升性能，但需维护外部知识库并增加部署时的计算开销。

Method: 1) 从代理失败中提取紧凑且可复用的提示；2) 利用这些提示在每轮开始时通过一次性检索生成改进的教师轨迹；3) 在去除提示字符串的情况下训练学生模型，促使其内化知识而非记忆。

Result: 在ALFWorld和WebShop两个交互式基准上，蒸馏后的学生模型表现优于基线：ALFWorld成功率最高达91%（基线为79%），WebShop得分提升至72（基线为61），且使用的token减少10-60%。该方法适用于不同模型规模（7B/14B）和代理架构（ReAct/StateAct）。

Conclusion: 通过针对性微调，可以有效将在推理时依赖检索的优势内化到模型中，消除长期运行时依赖，同时提升性能和效率。

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [385] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: 提出了一种基于大语言模型（LLM）代理的自动化数据驱动建模管道，用于工程中的回归任务，在临界热流预测基准上表现出与专家开发的先进模型相当甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法依赖大量人工干预，难以扩展和泛化，因此需要一种高效、可靠且可广泛适用的自动化建模方法。

Method: 设计并评估了两种LLM代理框架：一种是具有专门协作代理的多代理系统，另一种是基于推理与行动（ReAct）范式的单代理系统；两者均能自主完成数据预处理、神经网络构建、训练、超参数优化和不确定性量化。

Result: 在包含约25,000个实验数据点的OECD/NEA临界热流预测基准上，LLM代理开发的模型优于传统的查找表方法，并在预测精度和不确定性量化方面达到与人类专家开发的最先进贝叶斯优化深度神经网络相当的水平。

Conclusion: 基于LLM的代理在自动化复杂工程建模任务方面具有巨大潜力，可显著减少人工工作量，同时满足甚至超越现有预测性能标准。

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [386] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX 是一个基于大语言模型的自主AI代理，能将原始系统日志转化为本体驱动的知识图谱，并关联到MITRE ATT&CK战术，实现可操作的网络威胁情报提取。


<details>
  <summary>Details</summary>
Motivation: 系统日志虽蕴含丰富的网络威胁情报，但因缺乏结构、语义不一致和跨设备碎片化而难以利用，亟需有效方法将其转化为一致且可互操作的表示形式。

Method: 提出 OntoLogX，结合轻量级日志本体、检索增强生成（RAG）和迭代修正机制，利用大语言模型将原始日志转换为语法和语义上均有效的知识图谱，并聚合为会话级表示，进一步使用LLM预测MITRE ATT&CK战术。

Result: 在公开基准和真实蜜罐数据集上验证了 OntoLogX 的有效性，展示了其在多种知识图谱后端中稳定生成高质量知识图谱的能力，并能准确映射攻击行为到ATT&CK战术；实验表明检索与修正机制提升了精确率与召回率，代码优化的大模型在结构化日志分析中表现更优。

Conclusion: OntoLogX 通过本体引导和大语言模型的协同，实现了从非结构化日志到结构化、可解释威胁情报的自动化转化，显著提升了日志在网络安全分析中的可用性和行动价值。

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [387] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: Falconer是一个结合了大语言模型的代理推理和轻量级代理模型的知识挖掘框架，能够在保持高准确率的同时显著降低推理成本并加速大规模知识挖掘。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然擅长解释用户指令，但部署成本高昂；传统分类器和提取器虽高效却缺乏泛化能力。因此需要一种兼顾效率与灵活性的新型知识挖掘框架。

Method: 提出Falconer框架，利用大语言模型作为规划者分解指令、作为标注者生成训练数据来训练小型代理模型，并将分类与提取统一为两种原子操作（get label 和 get span）。

Result: 实验表明，Falconer在指令跟随准确性上接近最先进的大语言模型，推理成本降低高达90%，知识挖掘速度提升20倍以上。此外构建了新的基准测试来评估代理模型与人类及大模型标注的一致性。

Conclusion: Falconer为深度研究提供了一个高效且可扩展的基础，实现了大语言模型能力与轻量模型效率的协同。

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [388] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: 本文探讨了领域专家提供的高度整理的知识在构建高效辅导系统中的作用，提出了利用可解释AI技术自动生成课程和基于专家指定的课程开发自适应辅导系统的两种方法，并通过传粉者识别教学系统的案例研究强调了这些方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 在AI教育社区中，高度整理的专家知识对创建有效辅导系统的作用常被忽视，本文旨在强调这一主题。

Method: 讨论了使用可解释AI（XAI）技术结合专家规则自动生成课程的方法，以及利用专家指定的学习目标概念课程来开发自适应辅导系统的方法。

Result: 展示了如何利用专家知识和XAI技术自动生成课程并改善学习体验，同时提高算法效率。

Conclusion: 通过传粉者识别辅导系统的案例研究表明，从专家处获取的知识对于创建有效的教育系统至关重要。

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [389] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: 本文提出了一种名为VOGUE的新型方法，通过将探索从文本输出空间转移到视觉输入空间，利用视觉不确定性引导多模态大模型的探索过程，显著提升了视觉推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多模态大语言模型中忽视了视觉输入中的不确定性，导致探索不足和策略鲁棒性差，因此需要一种能够感知视觉变化并有效平衡探索与利用的方法。

Method: VOGUE将图像视为随机上下文，通过计算‘原始’分支与‘噪声’分支之间的对称KL散度来量化策略对视觉扰动的敏感性，并引入与不确定性成比例的奖励项，结合token熵奖励和退火采样策略，在GRPO框架内实现更有效的探索。

Result: 在Qwen2.5-VL-3B/7B两个模型上实验表明，VOGUE在三个视觉数学基准上平均提升2.6%的pass@1准确率，在三个通用推理基准上提升3.7%，同时提高了pass@4性能并缓解了强化学习微调中的探索衰减问题。

Conclusion: 将探索建立在视觉输入的内在不确定性基础上，是提升多模态推理能力的有效策略。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [390] [AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance](https://arxiv.org/abs/2510.01474)
*Bill Marino,Rosco Hunter,Zubair Jamali,Marinos Emmanouil Kalpakos,Mudra Kashyap,Isaiah Hinton,Alexa Hanson,Maahum Nazir,Christoph Schnabl,Felix Steffek,Hongkai Wen,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 本文提出了AIReg-Bench，首个用于评估大语言模型（LLM）在判断人工智能系统是否符合《欧盟人工智能法案》（AIA）方面表现的基准数据集。该数据集包含120个由LLM生成并经法律专家标注的技术文档片段，可用于测试和比较不同LLM在AI监管合规性评估中的性能。


<details>
  <summary>Details</summary>
Motivation: 随着各国政府加强对AI的监管，利用大语言模型（LLM）来评估AI系统是否符合法规的需求日益增长。然而，目前缺乏衡量LLM在此类任务上表现的标准。因此，需要建立一个可靠的基准来评估LLM在AI合规性判断中的能力。

Method: 通过两步法构建AIReg-Bench：首先，使用结构化提示让LLM生成120个虚构但合理的AI系统技术文档片段；然后，由法律专家对这些片段进行审查，并标注其违反AIA具体条款的情况。随后，评估前沿LLM能否复现专家的合规性标签。

Result: 成功构建了AIReg-Bench数据集，并提供了对前沿LLM在合规性判断任务上的初步评估结果，揭示了当前LLM在理解与应用AI法规方面的潜力与局限。

Conclusion: AIReg-Bench为评估LLM在AI监管合规性分析中的能力提供了首个基准，有助于推动LLM在法律合规领域的应用与发展，并为未来研究提供可比较的标准。

Abstract: As governments move to regulate AI, there is growing interest in using Large
Language Models (LLMs) to assess whether or not an AI system complies with a
given AI Regulation (AIR). However, there is presently no way to benchmark the
performance of LLMs at this task. To fill this void, we introduce AIReg-Bench:
the first benchmark dataset designed to test how well LLMs can assess
compliance with the EU AI Act (AIA). We created this dataset through a two-step
process: (1) by prompting an LLM with carefully structured instructions, we
generated 120 technical documentation excerpts (samples), each depicting a
fictional, albeit plausible, AI system - of the kind an AI provider might
produce to demonstrate their compliance with AIR; (2) legal experts then
reviewed and annotated each sample to indicate whether, and in what way, the AI
system described therein violates specific Articles of the AIA. The resulting
dataset, together with our evaluation of whether frontier LLMs can reproduce
the experts' compliance labels, provides a starting point to understand the
opportunities and limitations of LLM-based AIR compliance assessment tools and
establishes a benchmark against which subsequent LLMs can be compared. The
dataset and evaluation code are available at
https://github.com/camlsys/aireg-bench.

</details>


### [391] [Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates](https://arxiv.org/abs/2510.01500)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: Lateral Tree-of-Thoughts (LToT) 是一种改进的推理框架，通过分离效用与逻辑一致性，利用主干和侧支路径的划分，在大规模测试计算预算下有效缓解了传统搜索方法的宽度饱和和深度短视问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模测试计算资源下，标准的Tree-of-Thoughts搜索存在宽度饱和和深度短视的问题，限制了推理性能的提升。

Method: 提出LToT框架，将搜索前沿分为主干（高效益候选）和侧支（一致但初始效益低的候选），并通过Lateral Racing with Short-Circuit（LR--SC）机制对侧支进行低成本探测，动态判断是否提升至主干。

Result: 理论上证明了侧支扩展成本为伪线性Θ(N₀ log_η N₀)，显著低于无限制主干的指数增长；该方法能高效利用大计算预算，增强探索多样性而不增加额外计算开销。

Conclusion: LToT提供了一种原则性的方法，将大测试时计算预算转化为有效的推理路径多样性，在不增加计算成本的前提下缓解了饱和与短视问题，提升了复杂推理任务的可靠性。

Abstract: Modern deployments increasingly allocate large test-time compute (thousands
of tokens or many node expansions) to boost reliability. Under such budgets,
standard Tree-of-Thoughts-style search exhibits two pathologies: breadth
saturation (additional samples mostly produce near-duplicates, so width stops
growing) and depth myopia (noisy short-horizon utilities prune branches whose
payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts
(LToT), a drop-in controller that separates utility from logical consistency
and treats low-utility but consistent candidates as assets rather than waste.
The frontier is split into mainlines (high-utility candidates used for
exploitation) and laterals (consistent, initially low-utility candidates that
receive short, cheap probes before judgment). LToT explores laterals via
Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race
that spreads tiny probes across a very wide lateral set, uses width-aware
thresholds with repeat-to-confirm, and immediately promotes a branch once its
envelope clears the mainline bar; mainlines are kept intentionally narrow so
surplus compute is invested where width is cheap. We prove a pseudolinear
lateral cost $\Theta(N_0 \log_{\eta} N_0)$ with logarithmically many rungs
(initial lateral width $N_0$; culling factor $\eta>1$), in contrast to the
exponential growth of uncapped mainlines. Empirical evaluations on benchmark
tasks are in preparation and will be added in a future revision. In short, LToT
turns large test-time budgets into principled diversity while preserving
promotion discipline, mitigating saturation and myopia without inflating
compute.

</details>


### [392] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: 提出一种结合稀疏自编码器（SAE）和聚类技术的方法，用于分析大语言模型在数学推理任务中的内部表示并指导生成过程。


<details>
  <summary>Details</summary>
Motivation: 为了提升大语言模型在数学推理任务中的准确性，需要更好地理解其内部表示，并在生成过程中平衡利用与探索。

Method: 首先使用稀疏自编码器（SAE）对训练token生成稀疏向量表示，然后应用k-means聚类构建图结构（节点为token簇，边权重表示token转移频率），基于边权重设计奖励函数以识别利用性推理路径，并通过聚类评估生成多样性以衡量探索程度。

Result: 发现利用与探索的平衡对数学推理准确率至关重要；SAE可作为可扩展的奖励模型，在生成过程中引导模型保持二者平衡，避免极端行为。

Conclusion: 该方法通过结构化分析LLM内部表示，有效提升了数学推理生成的质量，验证了平衡exploitation与exploration的重要性。

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [393] [LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning](https://arxiv.org/abs/2510.01530)
*Navapat Nananukul,Yue Zhang,Ryan Lee,Eric Boxer,Jonathan May,Vibhav Giridhar Gogate,Jay Pujara,Mayank Kejriwal*

Main category: cs.AI

TL;DR: 本文提出了一种名为LOGicalThought (LogT) 的新型神经符号架构，结合逻辑语言与大语言模型，用于在高保证文本（如法律和医学）中进行精确、可验证的推理。LogT通过构建双重符号图和逻辑上下文，将长篇指南的推理问题转化为紧凑且有依据的评估，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理方面表现出色，但在需要严谨、可验证和基于证据的高保证推理任务中表现不足，尤其是在涉及非单调逻辑、例外规则等复杂逻辑结构时。因此，需要一种能够处理此类复杂推理的新架构。

Method: 提出LOGicalThought (LogT) 架构，结合先进的逻辑语言与推理引擎以及大语言模型，构建双重上下文表示：符号图上下文和逻辑上下文，从而将复杂的长文本推理转化为可验证的逻辑评估。

Result: 在四个多领域基准上对比四个基线模型，LogT在所有大语言模型上的整体性能提升了11.84%，其中否定推理最高提升+10.2%，蕴含推理提升+13.2%，可废止推理提升+5.5%。

Conclusion: LogT通过融合神经与符号方法，有效提升了在高保证领域文本中的复杂逻辑推理能力，特别在处理例外、否定和蕴含等结构时表现出显著优势，为关键领域的可信AI推理提供了可行路径。

Abstract: High-assurance reasoning, particularly in critical domains such as law and
medicine, requires conclusions that are accurate, verifiable, and explicitly
grounded in evidence. This reasoning relies on premises codified from rules,
statutes, and contracts, inherently involving defeasible or non-monotonic logic
due to numerous exceptions, where the introduction of a single fact can
invalidate general rules, posing significant challenges. While large language
models (LLMs) excel at processing natural language, their capabilities in
standard inference tasks do not translate to the rigorous reasoning required
over high-assurance text guidelines. Core reasoning challenges within such
texts often manifest specific logical structures involving negation,
implication, and, most critically, defeasible rules and exceptions. In this
paper, we propose a novel neurosymbolically-grounded architecture called
LOGicalThought (LogT) that uses an advanced logical language and reasoner in
conjunction with an LLM to construct a dual symbolic graph context and
logic-based context. These two context representations transform the problem
from inference over long-form guidelines into a compact grounded evaluation.
Evaluated on four multi-domain benchmarks against four baselines, LogT improves
overall performance by 11.84% across all LLMs. Performance improves
significantly across all three modes of reasoning: by up to +10.2% on negation,
+13.2% on implication, and +5.5% on defeasible reasoning compared to the
strongest baseline.

</details>


### [394] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: 本文提出了InfoSeeker，一种将任务导向规划与主动信息寻求相结合的大语言模型决策框架，以应对部分可观测环境中观测和动态不确定性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型规划代理常忽视其内部动态与真实环境之间的差异，而人类在信息不全的环境中会主动寻求信息来辅助决策，因此需要一个能主动获取信息以校准内部模型的智能体。

Method: InfoSeeker通过提示大语言模型主动规划信息收集动作，用于验证自身理解、检测环境变化或检验假设，从而更新内部动态并优化决策。同时构建了一个新的基准测试套件，包含部分可观测且具有不确定动态的环境。

Result: 实验表明，InfoSeeker相比先前方法绝对性能提升74%，且在样本效率上无损失；该方法可跨大语言模型泛化，并在机器人操作和网页导航等标准基准上优于基线方法。

Conclusion: 将规划与信息寻求紧密结合对于提升智能体在部分可观测环境中的鲁棒性至关重要，InfoSeeker为此提供了有效框架。

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [395] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种新的强化学习算法SAPO，用于改进扩散语言模型在复杂推理任务中的表现，通过引入基于过程的奖励机制来促进模型学习结构化的推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型在处理复杂推理时依赖稀疏的结果奖励，容易导致错误的推理路径被强化，这与自然的推理结构不匹配。

Method: 提出了一个理论框架，将复杂问题解决形式化为分层选择过程，并在此基础上设计了Step-Aware Policy Optimization (SAPO)算法，采用基于逐步进展的过程奖励来引导模型训练。

Result: 实验结果表明，SAPO显著提升了模型在复杂推理基准上的性能，并增强了生成过程的可解释性。

Conclusion: 通过将去噪过程与潜在的推理层次结构对齐，SAPO能够有效克服现有方法中的非结构化优化问题，实现更优的推理能力。

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [396] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: 本文提出了InvThink，一种使大语言模型具备逆向思维能力的简单而强大的方法，通过在生成回答前推理潜在失败模式来提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的安全对齐方法直接优化安全响应，缺乏对潜在风险的系统性预判，导致安全性和通用能力之间的权衡（即安全税）。

Method: InvThink指导模型首先列举潜在危害，分析其后果，然后生成能够主动规避这些风险的安全输出。该方法通过监督微调和强化学习在三个大语言模型家族中实现。

Result: 实验表明，InvThink在安全性提升上随模型规模扩展效果更优，减轻了安全税，在标准基准上保持了推理能力，并在医疗、金融、法律及代理型高风险场景中比SafetyPrompt等基线方法最多减少15.7%的有害回应。

Conclusion: 逆向推理为构建更安全、更具通用能力的大语言模型提供了一条可扩展且通用的路径。

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [397] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: 提出AdvEvo-MARL，一种基于共进化多智能体强化学习的安全内生框架，通过攻击者与任务智能体的对抗训练提升系统安全性，同时保持甚至提高任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法在多智能体系统中存在性能不足或增加系统开销的问题，难以有效应对跨智能体的不安全行为和单点故障风险。

Method: 设计AdvEvo-MARL框架，将安全能力内化到任务智能体中，通过共进化的多智能体强化学习，联合优化生成越狱提示的攻击者和具备防御能力的任务智能体，并引入组级优势估计基线以稳定学习并增强协作。

Result: 在多种攻击场景下，AdvEvo-MARL将攻击成功率控制在20%以下（基线最高达38.33%），同时任务准确率保持稳定甚至提升（推理任务最高+3.67%）。

Conclusion: 安全性和任务性能可在不依赖额外守卫模块的情况下协同提升，AdvEvo-MARL为开放多智能体系统提供了高效、低开销的内在安全解决方案。

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [398] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: 本文提出了AgentRec，一个基于大语言模型的多智能体协作推荐框架，通过分层智能体网络和自适应权重机制，有效解决了交互式对话推荐系统中动态用户偏好、对话连贯性和多目标排序的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统在处理动态用户偏好、保持对话连贯性以及平衡多个排序目标方面存在显著困难，需要更智能和灵活的解决方案。

Method: 提出AgentRec框架，采用专门的LLM驱动智能体分别负责对话理解、偏好建模、上下文感知和动态排序，并通过自适应加权机制协调；结合三层学习策略（快速响应、智能推理和深度协作）应对不同复杂度场景。

Result: 在三个真实数据集上的实验表明，AgentRec在对话成功率、推荐准确性（NDCG@10）和对话效率上分别提升了2.8%、1.9%和3.2%，同时保持了可比的计算成本。

Conclusion: AgentRec通过多智能体协同与自适应机制，在不增加计算负担的情况下显著提升了对话推荐系统的性能，具备较强的实用性和扩展性。

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [399] [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
*Min Zeng*

Main category: cs.AI

TL;DR: 本文探讨了大语言模型（LLM）在心理咨询服务中的应用潜力，提出并构建了一个基于美国国家心理咨询师认证考试的评测基准PsychoBench，包含约2,252道单项选择题。实验结果显示，GPT-4o、Llama3.3-70B和Gemma3-27B等先进模型能通过考试，而较小的开源模型如Qwen2.5-7B和Mistral-7B则表现不佳，表明目前仅前沿LLM具备成为合格心理咨询师的知识水平。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型是否具备应用于需要认知能力的心理咨询领域的潜力，并评估其是否具备成为专业心理咨询师所需的知识水平。

Method: 构建名为PsychoBench的评测基准，基于美国国家心理咨询师认证考试（NCE），包含约2,252道需深入理解的心理学单项选择题，覆盖多个心理学子领域，用于系统评估LLM的心理学知识掌握程度。

Result: GPT-4o、Llama3.3-70B和Gemma3-27B等大型模型在PsychoBench上表现超过70%的及格线，而Qwen2.5-7B、Mistral-7B等小型开源模型远未达标。

Conclusion: 当前只有最先进的大语言模型具备通过专业心理咨询考试的能力，表明前沿LLM在心理辅导应用方面具有潜力，但也凸显了开发专用心理学LLM所面临的挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of industries, primarily due to their impressive generative
abilities. Yet, their potential in applications requiring cognitive abilities,
such as psychological counseling, remains largely untapped. This paper
investigates the key question: Can LLMs be effectively applied to psychological
counseling? To determine whether an LLM can effectively take on the role of a
psychological counselor, the first step is to assess whether it meets the
qualifications required for such a role, namely the ability to pass the U.S.
National Counselor Certification Exam (NCE). This is because, just as a human
counselor must pass a certification exam to practice, an LLM must demonstrate
sufficient psychological knowledge to meet the standards required for such a
role. To address this, we introduce PsychoBench, a benchmark grounded in
U.S.national counselor examinations, a licensure test for professional
counselors that requires about 70% accuracy to pass. PsychoBench comprises
approximately 2,252 carefully curated single-choice questions, crafted to
require deep understanding and broad enough to cover various sub-disciplines of
psychology. This benchmark provides a comprehensive assessment of an LLM's
ability to function as a counselor. Our evaluation shows that advanced models
such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing
threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)
remain far below it. These results suggest that only frontier LLMs are
currently capable of meeting counseling exam standards, highlighting both the
promise and the challenges of developing psychology-oriented LLMs.

</details>


### [400] [Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs](https://arxiv.org/abs/2510.01620)
*Peidong Liu,Junjiang Lin,Shaowen Wang,Yao Xu,Haiqing Li,Xuhao Xie,Siyi Wu,Hao Li*

Main category: cs.AI

TL;DR: 提出一种基于大语言模型的信息压缩方法，用于上下文马尔可夫决策过程（CMDP），通过生成低维语义摘要提升决策效率与泛化能力，并提供理论保证和实证优势。


<details>
  <summary>Details</summary>
Motivation: 现有CMDP方法在高维或非结构化上下文中泛化能力差，导致计算开销大、性能不稳定。

Method: 利用大语言模型对上下文输入进行信息论驱动的摘要压缩，生成低维、语义丰富的上下文表示，并将其增强到状态中；基于近似上下文充分性构建理论分析框架。

Result: 提供了首个CMDP下的后悔界和延迟-熵权衡分析；实验表明该方法在奖励、成功率、样本效率上优于基线，同时降低延迟和内存消耗。

Conclusion: LLM-based上下文摘要是一种可扩展且可解释的解决方案，能有效支持资源受限环境中的高效决策。

Abstract: Contextual Markov Decision Processes (CMDPs) offer a framework for sequential
decision-making under external signals, but existing methods often fail to
generalize in high-dimensional or unstructured contexts, resulting in excessive
computation and unstable performance. We propose an information-theoretic
summarization approach that uses large language models (LLMs) to compress
contextual inputs into low-dimensional, semantically rich summaries. These
summaries augment states by preserving decision-critical cues while reducing
redundancy. Building on the notion of approximate context sufficiency, we
provide, to our knowledge, the first regret bounds and a latency-entropy
trade-off characterization for CMDPs. Our analysis clarifies how
informativeness impacts computational cost. Experiments across discrete,
continuous, visual, and recommendation benchmarks show that our method
outperforms raw-context and non-context baselines, improving reward, success
rate, and sample efficiency, while reducing latency and memory usage. These
findings demonstrate that LLM-based summarization offers a scalable and
interpretable solution for efficient decision-making in context-rich,
resource-constrained environments.

</details>


### [401] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: 该论文研究了大语言模型（LLM）在地理空间推理方面的能力，特别是是否能够读取道路网络地图并进行导航。作者提出了轨迹恢复作为代理任务，并构建了包含4000多条真实轨迹的数据集GLOBALTRACE。通过将道路网络作为上下文，所提出的提示框架使LLM能够在不依赖外部导航工具的情况下生成有效路径。实验表明，LLM在零样本设置下优于现成的基线和专用轨迹恢复模型，展现出强大的泛化能力。细粒度分析揭示了LLM对道路网络和坐标系统的良好理解，但也存在区域和交通方式相关的系统性偏差。最后，论文展示了LLM如何通过灵活的地图推理来融入用户偏好，从而提升导航体验。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在地理空间推理和导航任务中的潜力，尤其是在没有专门训练或接入外部工具的情况下，能否利用道路网络信息进行有效的路径推断和用户定制化导航。

Method: 提出轨迹恢复作为评估LLM地理推理能力的代理任务；构建包含多样化区域和交通模式的真实轨迹数据集GLOBALTRACE；设计一种基于道路网络上下文的提示框架，使LLM能在无外部导航工具的情况下生成合理路径。

Result: LLM在轨迹恢复任务中表现优于现成基线和专用模型，展现出优秀的零样本泛化能力；能够理解道路网络结构和坐标系统，但存在对特定区域和交通方式的系统性偏差。

Conclusion: 大语言模型具备较强的地理空间推理能力，可在无需外部工具的情况下完成基于地图的路径生成任务，且具有应用于个性化导航系统的潜力，但仍需注意其在不同区域和交通模式下的偏差问题。

Abstract: We explore the geospatial reasoning capabilities of Large Language Models
(LLMs), specifically, whether LLMs can read road network maps and perform
navigation. We frame trajectory recovery as a proxy task, which requires models
to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with
over 4,000 real-world trajectories across diverse regions and transportation
modes. Using road network as context, our prompting framework enables LLMs to
generate valid paths without accessing any external navigation tools.
Experiments show that LLMs outperform off-the-shelf baselines and specialized
trajectory recovery models, with strong zero-shot generalization. Fine-grained
analysis shows that LLMs have strong comprehension of the road network and
coordinate systems, but also pose systematic biases with respect to regions and
transportation modes. Finally, we demonstrate how LLMs can enhance navigation
experiences by reasoning over maps in flexible ways to incorporate user
preferences.

</details>


### [402] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: 本研究提出GuruAgents，通过提示工程将著名投资大师的定性理念转化为可复制的量化策略，在回测中表现出显著性能差异，其中巴菲特风格代理实现42.2%的年复合增长率。


<details>
  <summary>Details</summary>
Motivation: 将传奇投资者的定性投资哲学系统化并转化为可自动执行的量化策略，探索大模型在智能投研中的应用潜力。

Method: 设计五个基于不同投资大师理念的AI代理，通过精心设计的提示词（prompt）结合财务工具和确定性推理流程，在NASDAQ-100成分股上进行2023年Q4至2025年Q2的回测实验。

Result: 各GuruAgent展现出符合其设定风格的独特行为模式；巴菲特风格代理表现最佳，实现42.2%的年复合增长率，显著优于基准，其他代理表现各异。

Conclusion: 提示工程能有效将投资大师的定性思想转化为可量化的投资策略，验证了基于大模型的自动化系统投资的新路径可行性。

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [403] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: 本文提出了计算机使用代理（CUA）普遍存在“盲目目标导向”（BGD）的问题，表现为忽视可行性、安全性和上下文而一味追求目标。作者识别了三种典型模式，并构建了包含90个任务的BLIND-ACT基准进行评估，在九个前沿模型中观察到高达80.8%的BGD率，揭示了即使输入无害时仍存在的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 随着CUA在GUI上执行用户任务的广泛应用，其可能在不可行或不安全情境下强行执行目标，带来潜在风险。本文旨在揭示一种被忽视的系统性偏差——盲目目标导向（BGD），并推动对CUA安全性的深入研究。

Method: 通过分析CUA的行为，归纳出BGD的三种模式：缺乏上下文推理、模糊下的错误假设、矛盾或不可行目标。基于OSWorld构建BLIND-ACT基准，包含90个体现这些模式的任务，并采用LLM裁判评估代理行为，验证其与人类标注高度一致（93.75%）。随后在多个前沿模型上进行评测，并测试提示干预的有效性。

Result: 在九个前沿模型（如Claude、GPT-5等）上测试显示平均BGD率为80.8%，表明该问题普遍存在；LLM裁判与人类标注一致性达93.75%；提示工程虽能降低BGD，但风险依然显著。定性分析揭示了执行优先偏差、思维-行动脱节和请求优先等失败模式。

Conclusion: 盲目目标导向（BGD）是当前CUA中的一个根本性风险，可能导致安全隐患，即使输入本身无害。BLIND-ACT为评估和缓解此类风险提供了有效基准，未来需更强的训练或推理阶段干预机制以确保CUA的安全部署。

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [404] [A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)
*Motoki Sato,Yuki Matsushita,Hidekazu Takahashi,Tomoaki Kakazu,Sou Nagata,Mizuho Ohnuma,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.AI

TL;DR: LENOHA 是一种安全优先、本地优先的系统，通过高精度分类器路由输入并从临床医生审核的 FAQ 中返回逐字答案，避免了临床路径中的自由文本生成，实现了高准确率、低能耗和低延迟。


<details>
  <summary>Details</summary>
Motivation: 患者在等待侵入性手术时常有未解答的问题，但时间紧迫的工作流程和隐私限制阻碍了个性化咨询。需要一种既能保护隐私又能高效提供准确信息的解决方案。

Method: 提出 LENOHA 系统，使用高精度 sentence-transformer 分类器将输入路由到临床医生审核的 FAQ 中，并直接返回逐字答案，不进行自由文本生成。在牙齿拔除和胃镜两个领域使用专家评审的数据集进行评估（每个领域 n=400 用于验证，n=200 用于测试），比较不同编码器性能，并记录能耗与延迟。

Result: E5-large-instruct 模型整体准确率达 0.983（95% CI 0.964–0.991），AUC 为 0.996，仅出现 7 个错误，表现与 GPT-4o 相当；Gemini 在测试集中无错误。非生成式临床路径每输入耗电约 1.0 mWh，相比本地 8B 小语言模型回复小谈话的 ~168 mWh 降低约 170 倍，单台本地 GPU 延迟保持在 ~0.10 秒。

Conclusion: LENOHA 通过返回经过审核的 FAQ 答案，结构化地避免了前沿模型判别和生成带来的错误，支持隐私保护、可持续性和在带宽受限环境中的公平部署。

Abstract: Patients awaiting invasive procedures often have unanswered pre-procedural
questions; however, time-pressured workflows and privacy constraints limit
personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave
No One Behind Architecture), a safety-first, local-first system that routes
inputs with a high-precision sentence-transformer classifier and returns
verbatim answers from a clinician-curated FAQ for clinical queries, eliminating
free-text generation in the clinical path. We evaluated two domains (tooth
extraction and gastroscopy) using expert-reviewed validation sets
(n=400/domain) for thresholding and independent test sets (n=200/domain). Among
the four encoders, E5-large-instruct (560M) achieved an overall accuracy of
0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were
statistically indistinguishable from GPT-4o on this task; Gemini made no errors
on this test set. Energy logging shows that the non-generative clinical path
consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local
8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single
on-prem GPU. These results indicate that near-frontier discrimination and
generation-induced errors are structurally avoided in the clinical path by
returning vetted FAQ answers verbatim, supporting privacy, sustainability, and
equitable deployment in bandwidth-limited environments.

</details>


### [405] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: 本文讨论了当前通用人工智能（AGI）评估方法的局限性，主张从基于直觉的合成任务转向注重鲁棒任务执行能力的评估范式，以实际胜任力作为AGI的衡量标准。


<details>
  <summary>Details</summary>
Motivation: 现有AGI评估方法依赖人类对智能的直觉设计合成任务，但这些方法在AI历史中表现不佳，难以有效衡量真正的通用智能。

Method: 提出一种新的AGI评估设计哲学，借鉴数据科学中的可靠部署实践，强调通过系统在多样化、复杂任务中的稳健表现为依据进行评估。

Result: 阐述了该新范式的实际应用示例，展示了如何通过任务执行的可靠性与适应性来更有效地评估AGI进展。

Conclusion: 应摒弃仅依赖直觉设计的测试，转而采用以鲁棒性和实际胜任力为核心的AGI评估方法，从而更可靠地判断系统是否接近真正的人工通用智能。

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [406] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM引导的硬负例生成框架VaPR，用于减少视觉-语言模型偏好微调中的风格和长度偏差，并构建了3万高质量样本的数据集，显著提升了多个LVLM在十项基准上的性能，尤其在推理任务和减少“是”偏见方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI生成反馈的偏好微调方法存在合成偏好标注中的风格和长度偏差问题，影响模型对人类偏好的准确对齐，因此需要更鲁棒的负例生成机制。

Method: 提出一种LLM引导的响应编辑框架生成具有目标错误但保持风格和长度相似的硬负例，并构建VaPR数据集用于偏好微调，同时验证其在开源LLM作为编辑器时的泛化能力。

Result: 在LLaVA、Qwen2VL和Qwen2.5VL三个LVLM家族上实现平均6.5%、4.0%和1.5%的性能提升，在推理任务上改进显著；模型对二分类问题中“是”答案的过度倾向明显降低；VaPR-OS版本达到GPT-4o版本99%的性能。

Conclusion: VaPR通过高质量硬负例生成有效缓解了偏好学习中的噪声问题，提升了LVLM的对齐效果，且具备良好的数据效率和跨模型泛化能力。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [407] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-Félix Nothias*

Main category: cs.AI

TL;DR: MetaboT是一个基于大语言模型的多智能体系统，可将自然语言问题自动转换为SPARQL查询，在代谢组学知识图谱上实现高效、准确的数据检索，显著降低使用门槛。


<details>
  <summary>Details</summary>
Motivation: 知识图谱在代谢组学中具有潜力，但其使用受限于对本体和查询语言的专业知识要求，因此需要一种更友好的方式使研究人员能通过自然语言访问结构化数据。

Method: 设计了一个基于LangChain/LangGraph的多智能体系统（MetaboT），各智能体分工协作：入口、验证、监督、知识图谱和查询生成代理，将用户问题解析并转化为SPARQL查询，并在ENPKG知识图谱上执行。

Result: 在50个问题的测试集上，MetaboT准确率达83.67%，远高于仅用GPT-4o提示本体信息的基线方法（8.16%）。

Conclusion: MetaboT有效桥接了复杂语义技术与用户友好交互之间的鸿沟，支持基于自然语言的精准数据检索，促进数据驱动的科学发现。

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [408] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: 本文提出了一种将多种AI代理架构与NIST网络安全框架2.0系统对齐的结构化决策支持框架，通过整合代理理论与行业指南，实现针对现代网络威胁的AI解决方案选择与部署。


<details>
  <summary>Details</summary>
Motivation: 弥合人工智能理论与实际网络安全需求之间的差距，提供符合行业标准的透明、可操作的AI部署方法。

Method: 将NIST CSF 2.0功能细分为具体任务，将AI代理特性（如自主性、自适应学习和实时响应）与各子类别的安全要求相对应，并定义不同层级的自主性以适应不同成熟度的组织。

Result: 构建了一个统一的检测、事件响应和治理策略框架，验证了AI代理部署如何适应现实约束和风险特征，提升态势感知、响应速度和长期韧性。

Conclusion: 该框架为符合行业标准的强健、可实证的多代理系统奠定了基础，实现了理论AI模型与操作性网络安全需求的有效衔接。

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [409] [REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing](https://arxiv.org/abs/2510.01800)
*Thanh Ma,Tri-Tam La,Lam-Thu Le Huu,Minh-Nghi Nguyen,Khanh-Van Pham Luu,Huu-Hoa Nguyen*

Main category: cs.AI

TL;DR: 本文提出REBot，一种基于CatRAG框架的LLM增强型学术咨询聊天机器人，结合检索增强生成与图推理，在特定领域知识图谱支持下实现高效准确的学术政策问答，实验显示其性能达到SOTA，并已部署为实际应用。


<details>
  <summary>Details</summary>
Motivation: 构建能有效理解和遵循机构学术规章的智能咨询系统面临领域资源不足的问题，亟需融合结构化知识与推理能力以提升准确性和上下文理解深度。

Method: 提出CatRAG混合检索推理框架，整合稠密检索与基于图的推理，采用分层、类别标注且富含语义特征的知识图谱，并通过轻量级意图分类器路由查询至相应模块。

Result: 在自建的学术规章数据集上，REBot在分类与问答任务中F1得分达98.89%，表现优于现有方法。

Conclusion: REBot结合检索、图推理与大模型的能力，显著提升了学术规章咨询系统的性能，具备实际部署价值。

Abstract: Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.

</details>


### [410] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 本文提出了一种用于军事行动中人-AI协作的可信共学习模型，通过动态调整自主性、多层控制、双向反馈和协同决策四个维度，促进人类与AI在不断变化的战场环境中的共同适应。


<details>
  <summary>Details</summary>
Motivation: 随着军事威胁的快速演变和作战环境的日益复杂，将AI融入军事行动虽具优势，但也带来责任、安全与伦理方面的挑战。现有研究多从外部整体视角看待人-AI系统，缺乏对系统内部动态交互的深入分析。因此，亟需一种能处理多维责任与鲁棒性的内部协同机制。

Method: 提出一个包含四个维度的可信共学习模型：1）可调自主性，根据任务状态、系统置信度和环境不确定性动态调整代理的自主水平；2）多层控制，实现持续监督、活动监控和问责；3）双向反馈，建立显式与隐式反馈回路以共享推理、不确定性和学习结果；4）协同决策，生成并评估附带置信度和理由的决策方案。结合具体示例进行阐述。

Result: 该模型实现了人类与AI之间的持续双向知识交换，提升了系统的适应性、透明度和可解释性，并通过具体案例展示了其在军事场景中的可行性与有效性，为构建负责任的人-AI团队提供了结构化路径。

Conclusion: 所提出的共学习模型有助于构建更安全、可靠且符合伦理的人-AI协作系统，支持未来军事环境中动态、稳健和负责任的决策，推动人-AI团队向更高水平的信任与协同发展。

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [411] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: 提出PTA-GRPO框架，通过先规划后行动的两阶段方法提升大模型在复杂任务中的推理能力，结合监督微调与指导感知的强化学习，在多个数学推理任务上实现稳定且显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自回归生成过程中缺乏全局规划，导致推理过程冗余、不连贯或错误，现有方法计算成本高且难以生成最优推理路径。

Method: 提出PTA-GRPO，第一阶段将思维链提炼为高层指导并用于监督微调；第二阶段采用指导感知的强化学习联合优化输出和指导质量。

Result: 在MATH、AIME2024、AIME2025、AMC等多个数学推理基准上，基于不同基础模型（如Qwen和LLaMA系列）的实验表明PTA-GRPO均取得显著且稳定的性能提升。

Conclusion: PTA-GRPO有效改善了大模型在复杂任务中的推理一致性与准确性，具备良好的通用性和可扩展性。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [412] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 本文提出了一种基于对抗性逆强化学习的框架，用于从专家示范中学习语言模型推理过程中的密集、词元级奖励模型，以指导训练和推理过程中的决策，提升多步推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常通过监督微调模仿专家行为风格，而忽略了对推理过程中每一步正确性的评估。本文旨在通过学习一个细粒度的奖励模型来实现对推理过程的监督，从而更关注逻辑正确性而非表面形式。

Method: 将对抗性逆强化学习应用于大语言模型的推理任务，从专家演示中直接学习词元级别的奖励函数；该奖励模型既用于训练时优化推理策略，也用于推理时在固定计算预算下重排序采样路径。

Result: 在GSM8K数据集上使用Llama3和Qwen2.5模型验证了方法的有效性：学习到的密集奖励可作为有效的训练信号，并通过奖励引导的重排序提升了预测性能，尤其是基于Llama的模型表现显著提升；且奖励分数与最终答案正确性相关，能定位推理链中的错误步骤。

Conclusion: 通过统一训练信号、推理选择和细粒度诊断于一个推理奖励模型，本文展示了过程级奖励在增强语言模型多步推理能力方面的广泛潜力。

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [413] [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
*Paweł Parys,Sairam Vaidya,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: 提出了一种新的约束生成方法CARS，通过自适应拒绝采样在保证语言模型分布不失真的同时显著提高了有效样本的生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有约束生成方法在保持语言模型分布保真度和生成有效输出之间存在权衡，无法同时满足有效性与多样性需求，特别是在程序模糊测试等场景中。

Method: CARS结合了无约束语言模型采样与自适应剪枝机制，利用trie结构记录并排除违反约束的延续路径，动态调整概率质量，从而提升接受率且不扭曲原始分布。

Result: 在多个领域（如程序模糊测试和分子生成）实验中，CARS在每有效样本所需的LM前向传递次数上表现更高效，且生成样本的多样性优于贪婪约束解码和近似分布的方法。

Conclusion: CARS在不牺牲分布保真性的前提下，严格提升了拒绝采样的样本效率，是约束文本生成任务中的更优解决方案。

Abstract: Language Models (LMs) are increasingly used in applications where generated
outputs must satisfy strict semantic or syntactic constraints. Existing
approaches to constrained generation fall along a spectrum: greedy constrained
decoding methods enforce validity during decoding but distort the LM's
distribution, while rejection sampling (RS) preserves fidelity but wastes
computation by discarding invalid outputs. Both extremes are problematic in
domains such as program fuzzing, where both validity and diversity of samples
are essential. We present Constrained Adaptive Rejection Sampling (CARS), an
approach that strictly improves the sample-efficiency of RS without
distributional distortion. CARS begins with unconstrained LM sampling and
adaptively rules out constraint-violating continuations by recording them in a
trie and subtracting their probability mass from future draws. This adaptive
pruning ensures that prefixes proven invalid are never revisited, acceptance
rates improve monotonically, and the resulting samples exactly follow the
constrained distribution. In experiments on a variety of domains -- e.g.,
program fuzzing and molecular generation -- CARS consistently achieves higher
efficiency -- measured in the number of LM forward passes per valid sample --
while also producing stronger sample diversity than both GCD and methods that
approximate the LM's distribution.

</details>


### [414] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: 提出了一种确定性的模拟框架，用于评估AI生成的同行评审报告，为科学出版提供可扩展、透明且基于证据的治理模型。


<details>
  <summary>Details</summary>
Motivation: 应对学术出版中投稿量激增和AI滥用导致的同行评审危机，解决传统人工评审缺乏可审计性和客观标准的问题。

Method: 通过分析352份同行评审模拟报告，构建并验证一个确定性仿真系统，检测其在不同学科中的决策模式和程序一致性。

Result: 系统能稳定模拟编辑判断，'修改'决定占比超50%，'拒稿率'依学科调整（如健康科学达45%）；程序上保持29%的证据锚定合规率，跨领域稳定不变。

Conclusion: 该框架为AI参与的同行评审提供了可靠、可审计的标准，有助于提升科研诚信，推动AI成为学术治理的关键基础设施。

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [415] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD是一个引入文本语义的表格异常检测基准，包含20个富含结构化文本元数据的数据集和多种先进算法实现，并提出一种无需任务特定训练的零样本大语言模型框架，实验证明语义上下文可提升检测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测基准缺乏文本语义信息，忽略了特征描述和领域知识等关键上下文，限制了模型对领域知识的利用和研究的灵活性。

Method: 构建包含20个精心筛选并添加结构化文本元数据的表格数据集，实现多种先进的异常检测算法（经典方法、深度学习和基于大语言模型的方法），并设计一个利用语义上下文的零样本大语言模型框架。

Result: 实验表明，引入文本语义能显著提升异常检测性能，并通过支持领域感知推理增强结果的可解释性，验证了文本元数据在异常检测中的有效性。

Conclusion: ReTabAD为上下文感知的表格异常检测研究提供了系统性的新基准，推动利用领域知识进行更高效、可解释的异常检测。

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [416] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: 该研究系统分析了大语言模型中不同深度层的利用情况，发现深层在网络中的作用因评估场景而异：在生成任务中，中层和深层对推理和长距离连贯性至关重要，而在基于似然的非生成评估中，仅浅层起关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为大语言模型的深层对表示学习贡献较小，但这些结论多基于狭窄的评估，可能忽略了模型行为的重要方面。因此，本文旨在从多种维度系统研究深度层的实际利用情况。

Method: 通过在不同评估协议、任务类别和模型架构下进行系统的层剪枝实验，分析各层在知识存储、检索、推理和生成等任务中的作用，并考察蒸馏对深层功能的影响。

Result: 发现浅层集中了大部分知识与检索能力，而中层和深层对生成任务中的推理和长程一致性至关重要；不同评估方式下深层的重要性差异显著，生成式评估更能揭示深层的作用；通过蒸馏可重塑模型对深层的依赖。

Conclusion: 大语言模型中深度的使用具有高度异质性和上下文依赖性，压缩或解释模型时需结合具体任务、评估指标和模型结构进行综合考量。

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [417] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: 研究表明，尽管AI模型在文本模式下能达到人类水平的输出准确率，但其抽象推理能力仍落后于人类；依赖准确率评估可能高估文本模式下的能力，低估视觉模式下的潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨当前最先进模型是否真正理解并运用了任务设计者预期的抽象概念，而非仅依赖表面模式进行推理。

Method: 在不同输入模态（文本 vs. 视觉）、是否允许使用外部Python工具以及推理努力程度下评估模型，并结合输出准确率和生成的自然语言规则进行细粒度分析。

Result: 文本模式下部分模型达到人类准确率，但其规则多基于表面‘捷径’，较少捕捉到预期抽象；视觉模式下准确率显著下降，但规则分析显示模型仍产生相当比例的正确抽象，却难以正确应用。

Conclusion: 仅用准确率评估抽象推理会误导结论：可能高估文本模态中的能力，低估视觉模态中的潜力；所提出的评估框架能更真实反映多模态模型的抽象推理水平。

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [418] [FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models](https://arxiv.org/abs/2510.02133)
*Karan Dua,Hitesh Laxmichand Patel,Puneet Mittal,Ranjeet Gupta,Amit Agarwal,Praneet Pabolu,Srikant Panda,Hansa Meghwani,Graham Horwood,Fahad Shah*

Main category: cs.AI

TL;DR: FlexDoc是一个可扩展的合成数据生成框架，通过结合随机模式和参数化采样，生成具有丰富注释的逼真多语言半结构化文档，显著提升关键信息提取任务性能，同时大幅降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 在企业规模下开发文档理解模型需要大量、多样化且标注良好的数据集，但真实数据的收集因隐私、法律限制和高昂的手动标注成本而极为昂贵。

Method: 提出FlexDoc框架，结合随机模式（Stochastic Schemas）和参数化采样（Parameterized Sampling），对布局模式、视觉结构和内容变化进行概率建模，实现可控的大规模、多样化文档变体生成。

Result: 在关键信息提取（KIE）任务中，使用FlexDoc生成的数据增强真实数据集可使F1分数绝对提升高达11%，相比传统硬模板方法减少90%以上的标注工作量。

Conclusion: FlexDoc能有效加速企业级文档理解模型的开发，在大规模应用中显著降低数据获取与标注成本，具备实际部署价值。

Abstract: Developing document understanding models at enterprise scale requires large,
diverse, and well-annotated datasets spanning a wide range of document types.
However, collecting such data is prohibitively expensive due to privacy
constraints, legal restrictions, and the sheer volume of manual annotation
needed - costs that can scale into millions of dollars. We introduce FlexDoc, a
scalable synthetic data generation framework that combines Stochastic Schemas
and Parameterized Sampling to produce realistic, multilingual semi-structured
documents with rich annotations. By probabilistically modeling layout patterns,
visual structure, and content variability, FlexDoc enables the controlled
generation of diverse document variants at scale. Experiments on Key
Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data
improves the absolute F1 Score by up to 11% when used to augment real datasets,
while reducing annotation effort by over 90% compared to traditional
hard-template methods. The solution is in active deployment, where it has
accelerated the development of enterprise-grade document understanding models
while significantly reducing data acquisition and annotation costs.

</details>


### [419] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 本文提出了一种针对深度研究代理（DRAs）的严格基准和多维评估框架，涵盖214个专家策划的复杂查询，用于全面评估长篇报告的语义质量、主题聚焦和检索可信度。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估维度、响应格式和评分机制方面存在不足，难以有效评估具备任务分解、跨源检索和多阶段推理能力的DRAs系统。

Method: 构建包含214个跨10个主题领域的专家策划查询的基准，每个查询配备人工构建的参考包，并设计集成评分指标以评估语义质量、主题聚焦和检索可信度。

Result: 实验证明主流DRAs在复杂任务上优于增强网页搜索工具的推理模型，但仍存在显著改进空间。

Conclusion: 该研究为DRAs系统的能力评估、架构优化和范式进步提供了坚实基础。

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [420] [UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194)
*Yuhao Sun,Zhuoer Xu,Shiwen Cui,Kun Yang,Lingyun Yu,Yongdong Zhang,Hongtao Xie*

Main category: cs.AI

TL;DR: 本文提出UpSafe°C，一种通过安全感知升级提升大语言模型安全性的统一框架，结合稀疏专家混合结构和两阶段监督微调，在保持通用性能的同时实现鲁棒的安全增强与细粒度推理时控制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全技术在安全性、效用和可控性之间难以平衡，且缺乏动态和模块化的控制机制。

Method: 识别安全关键层并将其升级为稀疏MoE结构，引入软路由机制选择性激活原始MLP和安全专家，并采用两阶段SFT策略增强安全判别能力，同时提出安全温度机制实现推理时动态调节安全与效用的权衡。

Result: 在多个基准、基础模型和规模上验证了UpSafe°C能有效抵御有害内容和越狱攻击，同时保持通用任务性能，安全温度机制实现了效用与安全之间的帕累托最优前沿。

Conclusion: UpSafe°C提供了一种从静态对齐转向动态、模块化和推理感知控制的新范式，为LLM安全开辟了新方向。

Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.

</details>


### [421] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: 本文研究了强化学习与可验证奖励（RLVR）在提升大语言模型推理能力时可能导致推理边界收缩的问题，揭示了负干扰和赢家通吃现象，并提出一种针对低可能性问题的数据筛选算法以改善Pass@k性能。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR被用于增强大语言模型的推理能力，但其可能导致推理能力退化，因此需要探究其学习动态中的根本问题。

Method: 通过理论与实证分析多个数学推理基准上的RLVR学习动态，识别出负干扰和赢家通吃现象，并提出一种聚焦于低可能性问题的数据筛选算法。

Result: 发现RLVR中存在负干扰和 winner-take-all 现象，导致模型收敛到狭窄的解策略；所提数据筛选方法能有效提升Pass@k性能。

Conclusion: 标准RL目标中的on-policy采样机制导致RLVR可能缩小推理边界，改进的数据策展策略可缓解该问题并提升模型推理鲁棒性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [422] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: 本文提出了Behavior Best-of-N (bBoN) 方法，通过生成多个智能体执行路径并基于行为叙述进行选择，显著提升了计算机使用智能体在复杂任务中的鲁棒性和成功率，在OSWorld上达到69.9%的SoTA性能，接近人类水平（72%），并在不同操作系统上展现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用智能体（CUA）在处理长周期、复杂数字任务时存在不可靠和表现波动大的问题，限制了其实际应用。因此需要一种更可靠的方法来提升其稳定性与成功率。

Method: 提出Behavior Best-of-N（bBoN）方法，通过生成多个执行轨迹（rollouts），并利用描述这些轨迹的行为叙述（behavior narratives）进行系统性比较与选择，实现广泛探索与合理的路径筛选。

Result: 在OSWorld基准上达到69.9%的任务成功率，创下新的SoTA，接近人类表现的72%；在WindowsAgentArena和AndroidWorld上也表现出良好的跨平台泛化能力；消融实验验证了关键设计的有效性。

Conclusion: 正确的扩展策略——结合结构化的轨迹理解和选择机制——能极大提升CUA的性能，bBoN为高效扩展CUA提供了一个实用且有效的框架。

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [423] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: 本文提出了一种基于推理抽象的双玩家强化学习框架（RLAD），通过分离抽象生成与解法生成，提升大模型在复杂问题上的推理能力与泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在长链推理中难以稳定捕捉和复用有效推理步骤，常陷入冗长且无效的探索，缺乏对关键算法性结构的提炼与利用。

Method: 引入推理抽象（reasoning abstractions）作为简洁的程序性与事实性知识描述，构建两阶段流程：先生成多个抽象，再基于这些抽象进行强化学习以生成解法；采用双玩家RL框架（RLAD）联合训练抽象生成器与解法生成器。

Result: 该方法实现了结构化探索，解耦了学习信号，提升了模型在更难问题上的泛化能力；实验表明，在测试时将更多计算资源用于生成抽象比生成更多解法更有效。

Conclusion: 推理抽象能有效引导模型进行有意义的探索，RLAD框架为提升大模型的算法性推理能力提供了新路径。

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


### [424] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: 本文提出了一种名为BioX-Bridge的新框架，用于生物信号的无监督跨模态知识迁移，通过训练轻量级桥接网络对齐不同模态的基础模型中间表示，在大幅减少可训练参数（88%-99%）的同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于标注数据稀缺且基础模型计算开销大，传统跨模态知识迁移方法（如知识蒸馏）存在高计算和内存开销，难以应用于资源受限场景，因此需要一种更高效、灵活的跨模态知识迁移方法。

Method: 提出BioX-Bridge框架，使用轻量级桥接网络连接不同模态的基础模型，通过设计高效的对齐位置选择策略和灵活的原型网络结构，实现中间表示的对齐与信息流动，支持无监督跨模态知识迁移。

Result: 在多个生物信号模态、任务和数据集上的实验表明，BioX-Bridge相比现有最先进方法减少了88%-99%的可训练参数，同时保持甚至提升了迁移性能。

Conclusion: BioX-Bridge为生物信号的跨模态知识迁移提供了一种高效、可扩展的解决方案，显著降低了模型复杂度，有助于推动基于基础模型的健康监测系统在资源受限环境中的应用。

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>
