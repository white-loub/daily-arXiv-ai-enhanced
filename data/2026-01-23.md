<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 23]
- [cs.AI](#cs.AI) [Total: 52]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 56]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [AI-Based Culvert-Sewer Inspection](https://arxiv.org/abs/2601.15366)
*Christina Thrainer*

Main category: cs.CV

TL;DR: 本文提出三种方法以提升排水管道缺陷分割性能，尤其针对标注数据稀缺场景：1）改进预处理（传统增强+动态标签注入）；2）提出轻量高效新架构FORTRESS（融合深度可分离卷积、自适应KAN与多尺度注意力）；3）引入带注意力的双向原型网络用于小样本缺陷分割。


<details>
  <summary>Details</summary>
Motivation: 排水管道缺陷检测面临标注数据获取困难、依赖领域知识、难以构建大规模数据集等挑战，亟需适用于小样本场景的高效分割方法。

Method: 1）评估传统数据增强与动态标签注入等预处理策略；2）提出FORTRESS网络架构，融合深度可分离卷积、自适应Kolmogorov-Arnold网络（KAN）和多尺度注意力机制；3）采用带注意力机制的双向原型网络实现小样本语义分割。

Result: 所有方法均在culvert sewer pipe缺陷数据集上验证有效：预处理显著提升IoU和F1；FORTRESS达到SOTA性能，同时大幅降低参数量与计算成本；小样本方法在有限标注下取得满意分割指标。

Conclusion: 所提三种方法分别从数据增强、模型架构设计和小样本学习角度有效缓解数据稀缺问题，显著提升排水管道缺陷自动分割的实用性与泛化能力。

Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.
  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.

</details>


### [2] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文系统评估了多模态大语言模型（MLLMs）在异构人脸验证（HFR）任务中的性能，涵盖VIS-NIR、VIS-SWIR和VIS-THERMAL等跨模态场景，发现其性能仍显著落后于传统人脸识别系统，尤其在跨光谱条件下。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型（MLLMs）在异构人脸验证（HFR）这一具有挑战性的生物特征识别任务中的适用性与潜力。

Method: 对多个开源MLLMs在VIS-NIR、VIS-SWIR、VIS-THERMAL等跨模态人脸验证场景下，采用标准生物特征协议（如Acquire Rate、EER、TAR）进行系统性基准测试与评估。

Result: MLLMs在所有跨模态HFR任务中均显著落后于经典人脸识别系统，尤其在SWIR和热成像等更具挑战性的模态组合中表现更差。

Conclusion: 当前MLLMs尚不适用于实际HFR应用；强调在将MLLMs部署至生物识别系统前，必须开展严格、符合标准的生物特征评估。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

</details>


### [3] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,Andrés Villa,Juan León Alcázar,Karen Sánchez,Carlos Hinojosa,Denis Parra,Álvaro Soto,Bernard Ghanem*

Main category: cs.CV

TL;DR: CURE是一种无需额外数据的错误感知课程学习框架，通过动态调整样本采样策略，提升医学视觉-语言模型在放射学报告生成中的视觉定位准确性和事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型在放射学报告生成中存在视觉定位不准和事实不一致问题，常导致文本发现与图像证据错位。

Method: CURE基于公开数据集，对多模态指令模型进行三阶段微调：短语定位、基于定位的报告生成、解剖结构引导的报告生成，并根据模型表现动态调整困难样本采样。

Result: CURE将定位准确率（IoU）提升0.37，报告质量（CXRFEScore）提升0.188，幻觉减少18.6%。

Conclusion: CURE是一种数据高效框架，显著提升了医学报告生成的定位精度与可靠性。

Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

</details>


### [4] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 本文提出DuFal框架，通过双频域-空域协同学习，在稀疏视角CBCT重建中有效恢复高频解剖细节。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角CBCT重建因采样不足难以恢复对应高频成分的精细解剖结构，而传统CNN方法偏向低频信息学习，性能受限。

Method: 提出Dual-Frequency-Aware Learning（DuFal）框架：包含双路径架构（全局与局部高频增强傅里叶神经算子）、谱-通道分解降低参数量、跨注意力频率融合模块及强度场解码流程。

Result: 在LUNA16和ToothFairy数据集上，DuFal在极稀疏视角下显著优于现有SOTA方法，尤其在高频解剖特征保持方面。

Conclusion: 双频域协同建模可有效缓解稀疏采样导致的高频信息丢失，为医学图像重建提供新范式。

Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

</details>


### [5] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

TL;DR: 本文提出了一种偏差引导的提示学习框架，用于少样本正常图像下的异常检测（FNSAD），通过可学习提示和基于偏差的打分机制提升异常定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在正常与异常提示间区分能力弱，且缺乏有效的补丁级异常打分机制，难以应对仅用少量正常样本训练的挑战。

Method: 引入可学习上下文向量替代固定提示前缀，并为异常提示添加类别感知后缀；设计基于Top-K多实例学习的偏差损失，将补丁特征建模为正态分布的统计偏差。

Result: 在MVTecAD和VISA数据集上实现了优于PromptAD等基线方法的像素级异常检测性能，消融实验验证了各模块有效性。

Conclusion: 结合视觉语言模型语义能力和偏差统计建模的提示学习框架，显著提升了少样本异常检测的判别性、定位精度与可解释性。

Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

</details>


### [6] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于传感器物理模型的统一NeRF框架，用于从单曝光模糊LDR图像及对应事件数据中合成清晰的HDR新视角图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略相机输出与真实世界辐射之间的传感器物理不匹配，导致HDR重建和去模糊效果不佳。

Method: 构建传感器物理驱动的NeRF，直接表征HDR场景辐射；引入像素级RGB映射场对齐渲染值与LDR输入；设计事件映射场连接场景动态与事件传感器输出；联合优化NeRF与两个映射场。

Result: 在自建与公开数据集上实现了SOTA的单曝光模糊LDR+事件驱动的去模糊HDR新视角合成效果。

Conclusion: 传感器物理建模能显著提升基于事件辅助的HDR新视角合成质量，所提框架有效融合了空间-时间动态信息以学习更准确的3D HDR表示。

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

</details>


### [7] [Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis](https://arxiv.org/abs/2601.15490)
*Jobeal Solomon,Ali Mohammed Mansoor Alsahag,Seyed Sahand Mohammadi Ziabari*

Main category: cs.CV

TL;DR: 本文提出用Vision Transformer（ViT）替代U-Net编码器构建属性中立化框架，显著降低胸部X光片分类器中性别等人口统计属性的泄漏，同时保持疾病诊断性能，提升了模型公平性。


<details>
  <summary>Details</summary>
Motivation: 胸部X光AI分类器常因利用性别、年龄等捷径特征而产生偏差，导致少数群体误诊；现有基于卷积编码器的属性中立化方法未能彻底消除属性泄漏。

Method: 将Attribute-Neutral Framework中的U-Net编码器替换为DeiT-S Vision Transformer，在ChestX-ray14数据集上训练，并在11个编辑强度下生成编辑图像；使用独立AI判别器评估属性泄漏（如性别识别AUC），并用CNN评估疾病预测性能（ROC AUC）。

Result: 在中等编辑强度（alpha=0.5）下，ViT中立化器将性别识别AUC降至约0.80，比原U-Net方案低10个百分点；15种病变的宏平均ROC AUC下降不超过5个百分点，最差子组AUC仍维持在约0.70。

Conclusion: Vision Transformer凭借全局自注意力机制可更有效地抑制人口统计属性泄漏，且不损害临床实用性，为构建更公平的胸部X光AI提供了可行路径。

Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.

</details>


### [8] [Controllable Layered Image Generation for Real-World Editing](https://arxiv.org/abs/2601.15507)
*Jinrui Yang,Qing Liu,Yijun Li,Mengwei Ren,Letian Zhang,Zhe Lin,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: LASAGNA is a novel framework for generating images with coherent, photorealistic layered representations (background + transparent foreground with realistic visual effects), enabling controllable and consistent image editing using diverse conditioning inputs.


<details>
  <summary>Details</summary>
Motivation: Existing image generation models struggle with controllable and consistent editing of specific elements; layered representations lack coherent compositing and realistic visual effects like shadows and reflections.

Method: Proposes LASAGNA, a unified framework that jointly generates a photorealistic background and a high-quality transparent foreground with realistic visual effects; introduces LASAGNA-48K (a dataset of clean backgrounds and RGBA foregrounds with physically grounded effects) and LASAGNABENCH (first benchmark for layer editing); supports conditioning on text prompts, foreground/background masks, and location masks.

Result: LASAGNA achieves highly consistent and coherent multi-layer generation, enabling diverse post-editing applications that preserve identity and visual effects; LASAGNA-48K and LASAGNABENCH are publicly released.

Conclusion: LASAGNA overcomes key limitations in layered image generation by unifying background/foreground synthesis with realistic visual effects and offering unprecedented controllability and editability, setting a new standard for layer-based image editing.

Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.

</details>


### [9] [DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views](https://arxiv.org/abs/2601.15516)
*William Huang,Siyou Pei,Leyi Zou,Eric J. Gonzalez,Ishan Chatterjee,Yang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于背侧手部皮肤形变的双流delta编码器方法，用于解决XR设备中自遮挡导致的egocentric手部姿态估计难题，在仅使用裁剪背侧图像的情况下显著提升精度并减小模型尺寸。


<details>
  <summary>Details</summary>
Motivation: XR设备普及使得以自我为中心（egocentric）的手部姿态估计变得重要，但手指频繁自遮挡带来了固有挑战。

Method: 提出一种新颖的双流delta编码器，通过对比动态手部与基准放松姿态的特征来学习姿态；利用近期密集视觉特征提取器挖掘背侧手部皮肤形变信息。

Result: 在手指遮挡率≥50%的自遮挡场景下，相比依赖整手几何与大模型主干的SOTA方法，MPJAE降低18%；提升了捏取、点击等下游任务在遮挡下的可靠性，并支持无可见运动的等长力检测（如表面‘点击’），同时模型更轻量。

Conclusion: 该方法不仅提高了遮挡场景下手部姿态估计的鲁棒性与实用性，还拓展了新型交互范式，兼顾性能与效率。

Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface "click" without visible movement while minimizing model size.

</details>


### [10] [VIOLA: Towards Video In-Context Learning with Minimal Annotations](https://arxiv.org/abs/2601.15549)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 本文提出VIOLA框架，通过密度-不确定性加权采样和置信度感知的检索与提示机制，在极少量专家标注下实现多模态大语言模型（MLLMs）对新视频域的高效零训练适配。


<details>
  <summary>Details</summary>
Motivation: 现有视频领域迁移方法依赖大量标注数据，但在工业、手术等专业场景中专家标注成本高、难以获取，亟需低标注开销的无训练适配方案。

Method: 提出VIOLA框架：1）密度-不确定性加权采样，兼顾多样性、代表性和信息性；2）构建混合示范池，结合置信度感知检索（融合相似性与置信度）和置信度感知提示（区分真实标签与伪标签）。

Result: 在9个视频基准、4种MLLM上验证，VIOLA在低资源设置下显著优于各类基线，以极低标注成本实现鲁棒迁移。

Conclusion: VIOLA为MLLMs在标注稀缺的专业视频场景中提供了高效、可靠、低开销的上下文学习适配路径，推动其实际落地应用。

Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.

</details>


### [11] [Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing](https://arxiv.org/abs/2601.16125)
*Tingyu Song,Yanzhao Zhang,Mingxin Li,Zhuoning Guo,Dingkun Long,Pengjun Xie,Siyue Zhang,Yilun Zhao,Shu Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像编辑的合成方法，构建了细粒度的组合图像检索（CIR）新基准EDIR，包含5000个跨5大类、15子类的高质量查询，并通过评估13个模型揭示了现有模型在子类别上的性能不一致及当前基准的模态偏差与类别覆盖不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有CIR基准查询类别有限，难以反映真实场景的多样性需求，缺乏对模型细粒度能力的全面评估。

Method: 利用图像编辑技术精确控制修改类型与内容，构建可扩展的合成查询流水线，并据此创建EDIR基准；对13个主流多模态嵌入模型进行系统评测，并开展域内训练实验以分析任务难点。

Result: EDIR包含5000个高质量查询，覆盖5大类、15子类；实验表明SOTA模型（如RzenEmbed、GME）在不同子类上表现不稳定；发现现有基准存在模态偏差和类别覆盖不足问题；域内训练验证了部分子类可通过数据优化解决，而另一些则暴露模型架构固有局限。

Conclusion: EDIR是一个更具挑战性和现实代表性的CIR基准，能有效揭示当前多模态模型的能力边界与架构缺陷，为后续研究提供更可靠的评估平台与改进方向。

Abstract: Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.

</details>


### [12] [Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation](https://arxiv.org/abs/2601.15560)
*Sylvey Lin,Eranki Vasistha*

Main category: cs.CV

TL;DR: 本文提出了一种新的评估指标RCA，用于衡量类条件扩散模型在K-pop偶像人脸生成任务中的语义可控性，并揭示了高视觉质量与低身份一致性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标（如FID、IS）难以检测细粒度单领域任务中的身份错位问题，尤其在高类间相似性场景下。

Method: 针对32x32 K-pop偶像人脸生成任务，构建类条件DDPM，并提出归一化于Oracle分类器的相对分类准确率（RCA）作为新评估指标；结合混淆矩阵分析失败模式。

Result: 模型获得较高视觉质量（FID=8.93），但RCA仅为0.27，表明存在严重语义模式坍缩，尤其在视觉模糊身份上；失败主因是分辨率限制和性别内歧义。

Conclusion: RCA为条件生成模型的身份一致性验证提供了严格标准，揭示了高保真生成与细粒度语义控制间的根本张力。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.

</details>


### [13] [HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval](https://arxiv.org/abs/2601.16155)
*Zequn Xie,Xin Liu,Boyun Zhang,Yuxiao Lin,Sihang Cai,Tao Jin*

Main category: cs.CV

TL;DR: 本文提出了一种受人类视觉启发的文本-视频检索模型HVD，通过粗到细的对齐机制（包括关键帧选择和补丁特征压缩）来提升检索性能，在五个基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文本-视频检索方法存在“盲”特征交互问题，难以从背景噪声中识别关键视觉信息，尤其受限于文本查询的稀疏性。

Method: 提出Human Vision-Driven (HVD)模型，包含两个模块：Frame Features Selection Module (FFSM)用于选择关键帧以消除时序冗余；Patch Features Compression Module (PFCM)利用先进注意力机制将图像块特征聚合为显著视觉实体，实现细粒度实体级匹配。

Result: 在五个主流文本-视频检索基准上取得SOTA性能，同时展现出类人的视觉关注能力。

Conclusion: HVD通过模拟人类宏观与微观感知机制，有效缓解了文本稀疏性导致的视觉信息混淆问题，验证了认知启发建模在跨模态检索中的有效性。

Abstract: The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.

</details>


### [14] [Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition](https://arxiv.org/abs/2601.15615)
*Weiwei Wu,Yueyang Li,Yuhu Shi,Weiming Zeng,Lang Qin,Yang Yang,Ke Zhou,Zhiguo Zhang,Wai Ting Siok,Nizhuan Wang*

Main category: cs.CV

TL;DR: 本文提出RSM-CoDG框架，结合脑区先验、多尺度时序建模与协同域泛化策略，提升跨被试EEG情绪识别的鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 跨被试EEG情绪识别面临被试间变异性大、分布偏移严重及情绪神经表征时空复杂度高的挑战，现有方法难以在统一框架中兼顾跨被试对齐、多尺度动态建模与去偏泛化。

Method: 提出Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization（RSM-CoDG）：1）基于功能脑区划分构建区域级空间表征；2）采用多尺度时序建模刻画情绪神经活动动态演化；3）引入多维约束的协同域泛化策略抑制被试特异性偏差。

Result: 在SEED系列数据集上显著优于现有方法，验证了其在未知被试场景下的强泛化能力与鲁棒性。

Conclusion: RSM-CoDG通过融合神经科学先验与协同域泛化机制，为跨被试EEG情绪识别提供了有效且可推广的统一建模范式。

Abstract: Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.

</details>


### [15] [Explainable Deepfake Detection with RL Enhanced Self-Blended Images](https://arxiv.org/abs/2601.15624)
*Ning Jiang,Dingheng Zeng,Yanhong Liu,Haiyang Yi,Shijie Yu,Minghe Weng,Haifeng Shen,Ying Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于自混合图像的自动化链式思维（CoT）数据生成框架和强化学习（RL）增强的深度伪造检测方法，以解决多模态大语言模型（MLLM）在可解释深度伪造检测中因高质量标注数据稀缺而受限的问题，并在多个跨数据集基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法缺乏可解释性，而多模态大语言模型（MLLM）虽有潜力，但受限于高成本、难获取的细粒度伪造归因文本标注；同时，强化学习在视觉任务尤其是跨域泛化中展现出优势，值得探索。

Method: 提出基于自混合图像（Self-Blended Images）的自动化链式思维（CoT）数据生成框架，结合强化学习增强的检测框架，包括定制化奖励机制与反馈驱动的合成数据生成。

Result: 所提CoT数据构建流程、奖励机制与反馈生成策略被实验证明有效，在多个跨数据集基准上性能媲美当前最优方法（SOTA）。

Conclusion: 该方法显著降低了对人工文本标注的依赖，推动了MLLM在可解释深度伪造检测中的实用化，并验证了强化学习在该任务中的有效性。

Abstract: Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.

</details>


### [16] [Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception](https://arxiv.org/abs/2601.15643)
*Bo Yuan,Danpei Zhao,Wentao Li,Tian Li,Zhiguo Jiang*

Main category: cs.CV

TL;DR: 本文提出持续全景感知（CPP）框架，将多模态与多任务持续学习结合，通过跨模态协同编码器、可塑知识继承模块、跨模态一致性约束及非对称伪标签策略，缓解灾难性遗忘与语义混淆问题，提升像素级、实例级与图像级联合感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习主要聚焦单任务场景，难以应对多任务与多模态下的语义混淆与灾难性遗忘问题，限制了智能感知系统的综合能力。

Method: 提出持续全景感知（CPP）模型，包括协同跨模态编码器（CCE）、基于对比特征蒸馏与实例蒸馏的可塑知识继承模块、跨模态一致性约束，并扩展为CPP+；引入非对称伪标签机制，避免示例回放。

Result: 在多模态数据集与多样持续学习任务上验证了方法优越性，尤其在细粒度持续学习任务中表现突出。

Conclusion: CPP框架有效统一多模态与多任务持续学习，显著提升全景感知能力，为智能感知AI系统提供新范式。

Abstract: Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.

</details>


### [17] [SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction](https://arxiv.org/abs/2601.15644)
*Zichen Yu,Quanli Liu,Wei Wang,Liyong Zhang,Xiaoguang Zhao*

Main category: cs.CV

TL;DR: 本文提出了SuperOcc框架，用于基于超二次曲面（superquadric）的3D占用预测，通过协同时间建模、多超二次曲面解码和高效体素溅射，提升了精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D占用预测方法多采用密集场景表示，忽视真实驾驶场景的稀疏性；而超二次曲面虽具几何表达力和稀疏性优势，但存在时序建模不足、查询稀疏性与几何表达力难以兼顾、以及超二次曲面到体素溅射低效等问题。

Method: 提出SuperOcc框架，包含三个核心设计：(1) 协同式时间建模机制，联合利用视图中心与物体中心时序线索；(2) 多超二次曲面解码策略，在保持查询稀疏的同时增强几何表达力；(3) 高效的超二次曲面到体素溅射方案。

Result: 在SurroundOcc和Occ3D基准上达到SOTA性能，同时保持更高计算效率。

Conclusion: SuperOcc验证了稀疏、几何驱动的超二次曲面表示在3D占用预测中的有效性与实用性，为高效、精准的自动驾驶环境感知提供了新范式。

Abstract: 3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.

</details>


### [18] [Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2601.15655)
*Zhenghui Guo,Yuanbin Man,Junyuan Sheng,Bowen Lin,Ahmed Ahmed,Bo Jiang,Boyuan Zhang,Miao Yin,Sian Jin,Omprakash Gnawal,Chengming Zhang*

Main category: cs.CV

TL;DR: Event-VStream 是一种事件感知的实时视频理解框架，通过检测语义连贯的视频事件边界来触发语言生成，并将事件嵌入存入持久化记忆库，从而在低延迟下实现长时程推理。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频理解方法存在冗余帧处理和快速遗忘历史上下文的问题，固定间隔解码或缓存裁剪策略难以兼顾信息完整性与实时性。

Method: 提出 Event-VStream 框架，将连续视频建模为离散、语义连贯的事件序列；融合运动、语义和预测线索检测状态转换事件；仅在事件边界触发语言生成；事件嵌入存入持久化记忆银行以支持长时程推理。

Result: 在 OVOBench-Realtime 上较 VideoLLM-Online-8B 提升 +10.4 分；性能接近专用 Flash-VStream-7B，但仅使用通用 LLaMA-3-8B 文本骨干；在 2 小时 Ego4D 流上保持约 70% 的 GPT-5 胜率。

Conclusion: Event-VStream 通过事件驱动机制与持久化记忆设计，有效缓解了长视频流实时理解中的冗余与遗忘问题，在性能与效率间取得更好平衡。

Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.

</details>


### [19] [Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling](https://arxiv.org/abs/2601.15664)
*Hongyang Wei,Hongbo Liu,Zidong Wang,Yi Peng,Baixin Xu,Size Wu,Xuying Zhang,Xianglong He,Zexiang Liu,Peiyu Wang,Xuchen Song,Yangguang Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: 本文提出了Skywork UniPic 3.0，一个统一的多模态框架，支持单图编辑与多图合成（尤其聚焦于人-物交互HOI任务），通过新型数据流程、序列建模范式及加速推理策略，在性能和速度上均达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 社区对多图合成（尤其是HOI类任务）需求强烈，但现有模型缺乏高质量融合的具体方法细节，亟需系统性解决方案。

Method: 构建统一多模态框架Skywork UniPic 3.0；设计面向HOI的多图合成数据采集、过滤与合成流水线；提出将多图合成为序列建模问题的新训练范式；在后训练阶段引入轨迹映射与分布匹配以加速推理。

Result: 仅用700K高质量样本即达优异性能；推理仅需8步，比标准采样快12.5倍；在单图编辑基准上SOTA，在多图合成基准上超越Nano-Banana和Seedream 4.0。

Conclusion: 所提出的统一框架、数据策略与序列化训练范式有效解决了多图合成中的一致性与质量难题，显著提升了效率与效果。

Abstract: The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.

</details>


### [20] [Consistency-Regularized GAN for Few-Shot SAR Target Recognition](https://arxiv.org/abs/2601.15681)
*Yikui Zhai,Shikuang Liu,Wenlve Zhou,Hongsheng Zhang,Zhiheng Zhou,Xiaolin Tian,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 本文提出了一种一致性正则化生成对抗网络（Cr-GAN），用于在极少量SAR图像样本下合成高质量、多样化的训练数据，从而提升小样本识别性能。Cr-GAN通过双分支判别器、通道级特征插值和双域循环一致性机制，在数据稀缺条件下实现稳定训练，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SAR图像小样本识别受限于极端数据稀缺；而传统GAN又依赖大量数据训练，形成根本矛盾。

Method: 提出Cr-GAN：采用双分支判别器解耦对抗训练与表征学习；引入通道级特征插值生成新潜在特征；设计双域循环一致性机制保障语义完整性；适配多种GAN架构并增强多种自监督学习算法。

Result: 在MSTAR和SRSDD数据集的8-shot设置下，准确率分别达71.21%和51.64%，显著超越基线方法，且参数量仅为前沿扩散模型的约1/5。

Conclusion: Cr-GAN有效缓解了小样本SAR识别中生成模型对大数据的依赖，为低资源遥感图像学习提供了可行、高效的新范式。

Abstract: Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.

</details>


### [21] [Performance-guided Reinforced Active Learning for Object Detection](https://arxiv.org/abs/2601.15688)
*Zhixuan Liang,Xingyu Zeng,Rui Zhao,Ping Luo*

Main category: cs.CV

TL;DR: 本文提出了一种面向目标检测任务的性能引导型强化主动学习方法MGRAL，以mAP提升为奖励信号，利用强化学习代理选择最具信息量的样本，同时采用无监督快速查表法降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习方法评估数据信息量时未直接关联下游任务性能（如目标检测中的mAP），导致标注效率与模型性能提升脱节。

Method: 提出MGRAL框架：以期望模型输出变化作为信息量度量；设计基于策略梯度的强化学习采样代理进行批量样本选择；引入无监督、基于查表的快速mAP估计机制以降低计算成本。

Result: 在PASCAL VOC和COCO数据集上的目标检测主动学习实验表明，MGRAL取得最优AL曲线，并提供直观可视化结果。

Conclusion: MGRAL建立了强化学习驱动的主动目标检测新范式，实现了更高效、性能导向的标注样本选择。

Abstract: Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.

</details>


### [22] [Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs](https://arxiv.org/abs/2601.15698)
*Mingyu Yu,Lana Liu,Zhehao Zhao,Wei Wang,Sujuan Qin*

Main category: cs.CV

TL;DR: 本文提出了一种名为Beyond Visual Safety（BVS）的新型图像-文本对越狱框架，用于探测多模态大语言模型（MLLMs）的视觉安全边界，采用‘重建-生成’策略，成功实现对GPT-5高达98.21%的越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究对MLLMs的安全漏洞已有探索，但对其视觉安全边界的考察仍不充分，亟需系统性评估其视觉内容安全对齐能力。

Method: 提出BVS框架，采用‘重建-then-生成’策略，结合中性化视觉拼接与归纳式重组技术，将恶意意图从原始输入中解耦，诱导MLLMs生成有害图像。

Result: BVS在GPT-5（2026年1月12日发布版）上实现了98.21%的越狱成功率，显著高于现有方法，揭示了当前MLLMs在视觉安全对齐方面的严重缺陷。

Conclusion: MLLMs当前的视觉安全机制存在根本性薄弱环节，BVS为评估和提升其多模态内容安全提供了新范式与实用工具。

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a "reconstruction-then-generation" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.

</details>


### [23] [Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data](https://arxiv.org/abs/2601.15705)
*Ali Caglayan,Nevrez Imamoglu,Toru Kouyama*

Main category: cs.CV

TL;DR: 本文提出了一种基于ALOS-2 SAR数据的日本全国尺度LULC语义分割方法，通过三项轻量改进缓解SAR密集预测中的边界模糊、细长结构漏检和长尾类别性能下降问题，并在基准测试中显著提升稀有类和水体检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决SAR图像密集预测中常见的边界过度平滑、细长结构漏检以及长尾标签下稀有类别性能退化等问题，且不增加流程复杂度。

Method: 基于SAR-W-MixMAE自监督预训练模型，引入三项轻量改进：(i) 将高分辨率特征注入多尺度解码；(ii) 设计渐进式精炼上采样头（交替卷积精炼与逐步上采样）；(iii) 在Focal+Dice损失中引入α尺度因子调节类别重加权。

Result: 在全日本ALOS-2 LULC基准上实现一致性能提升，尤其改善稀有类别表现，并在水体检测各项标准指标上取得提升。

Conclusion: 所提轻量改进有效缓解了SAR语义分割中的关键挑战，在保持模型简洁性的同时提升了全国尺度LULC分类与水体检测的精度与鲁棒性。

Abstract: This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.

</details>


### [24] [Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework](https://arxiv.org/abs/2601.15711)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 本文提出了一种三层次评估框架，用于系统评测视觉-语言模型（VLMs）在细粒度时尚属性预测任务中的表现，特别关注属性适用性检测与细粒度分类的解耦分析；实验表明零样本VLMs在分类上表现优异但适用性检测仍是瓶颈，且高效模型可实现接近旗舰模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在时尚多属性预测任务中缺乏系统评估，尤其未区分属性是否适用（如无外套时‘外层面料’无定义），需先检测适用性再分类。

Method: 构建三层次评估框架：(1)整体任务性能（含NA类），(2)属性适用性检测，(3)适用前提下的细粒度分类；在DeepFashion-MultiModal数据集上评测9个VLMs及Fashion-CLIP+分类器基线。

Result: 零样本VLMs达64.0% macro-F1（是Fashion-CLIP+LR的三倍）；细粒度分类F1达70.8%，但适用性检测NA-F1仅34.1%；高效模型（如GPT-5 Mini）性能达旗舰模型90%以上。

Conclusion: 三层次框架可精准定位错误来源（适用性检测 or 分类），为时尚零售场景中VLMs的实际部署与优化提供诊断依据；提升适用性检测能力是关键突破口。

Abstract: Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, "outer fabric" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.

</details>


### [25] [VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning](https://arxiv.org/abs/2601.15724)
*Chenglin Li,Qianglong Chen,Feng Han,Yikun Wang,Xingxi Yin,Yan Gong,Ruilin Li,Yin Zhang,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出VideoThinker，一种完全基于合成工具交互轨迹训练的智能体式视频大语言模型，通过在字幕空间生成多步工具使用序列并映射回视频帧，构建无需长视频理解能力即可生成的大规模视频-工具推理数据集，显著提升了长视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型依赖均匀采样帧进行静态推理，导致长视频中时序定位弱、信息损失大；而引入时序检索、空间/时序缩放等智能体工具虽可缓解问题，但其数据构建本身又依赖已具备强长视频理解能力的模型，形成循环依赖。

Method: 提出VideoThinker模型，首先将视频转为丰富字幕，再利用强智能体语言模型在字幕空间生成多步工具使用轨迹，最后将字幕替换为对应视频帧，实现轨迹到视频的接地，构建大规模合成视频-工具推理数据集，并在此数据上端到端训练模型。

Result: VideoThinker在多个长视频基准测试中显著超越纯字幕语言模型智能体及强视频模型基线，展现出动态推理、自适应时序探索与多步工具使用能力。

Conclusion: 基于工具增强的合成数据与自适应检索/缩放推理范式，能有效突破长视频理解瓶颈，无需真实人工标注或先验长视频理解能力即可构建高质量训练数据。

Abstract: Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.

</details>


### [26] [Keyframe-Based Feed-Forward Visual Odometry](https://arxiv.org/abs/2601.16020)
*Weichen Dai,Wenhan Su,Da Kong,Yuhang Ming,Wanzeng Kong*

Main category: cs.CV

TL;DR: 本文提出了一种基于强化学习的自适应关键帧选择策略，用于视觉里程计（VO），以提升基于视觉基础模型的前馈式VO方法的效率与精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉基础模型的VO方法（如VGGT-Long）直接处理原始图像序列，缺乏关键帧机制，导致计算冗余和因帧间视差小而引起的性能下降；同时，将传统几何启发式方法融入高维隐空间表示的基础模型中存在困难。

Method: 提出一种新型的关键帧驱动的前馈VO框架，利用强化学习在数据驱动下学习适应性关键帧选择策略，使其与基础模型的内在特征对齐，而非依赖人工设计规则。在TartanAir数据集上训练强化学习智能体。

Result: 在多个真实世界数据集上的实验表明，该方法相比当前最优的前馈VO方法实现了持续且显著的性能提升。

Conclusion: 将强化学习引入关键帧选择可有效弥合传统几何方法与现代视觉基础模型之间的鸿沟，为高效、精准的前馈式VO提供了新范式。

Abstract: The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.

</details>


### [27] [FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging](https://arxiv.org/abs/2601.15731)
*Linyong Zou,Liang Zhang,Xiongfei Wang,Jia-Hong Gao,Yi Sun,Shurong Sheng,Kuntao Xiao,Wanli Yang,Pengfei Teng,Guoming Luan,Zhao Lv,Zikang Xu*

Main category: cs.CV

TL;DR: 本文提出FAIR-ESI框架，通过多视图自适应特征重要性精炼（频谱、时域、patch级）提升脑电/磁源成像精度，并在仿真与临床数据上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 准确选择和精炼特征是实现精确脑电/磁源成像（ESI）的关键挑战。

Method: 提出FAIR-ESI框架，包含FFT频谱特征精炼、加权时域特征精炼和自注意力驱动的patch级特征精炼三种自适应多视图特征重要性精炼机制。

Result: 在两个仿真数据集和两个真实临床数据集上实验验证了该框架的有效性，显著提升了ESI精度。

Conclusion: FAIR-ESI为脑疾病诊断提供了新工具，并有助于深入理解脑功能机制。

Abstract: An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.

</details>


### [28] [DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models](https://arxiv.org/abs/2601.16065)
*Chenyang Li,Jieyuan Liu,Bin Li,Bo Gao,Yilin Yuan,Yangfan He,Yuchen Li,Jingqun Tang*

Main category: cs.CV

TL;DR: 本文提出了一种即插即用的干扰图像令牌剪枝（DTP）框架，用于提升视觉-语言动作（VLA）模型在机器人操作任务中的成功率，通过动态检测并剪除任务无关区域的干扰图像令牌，改善模型视觉注意力模式，无需修改原有架构或增加额外输入。


<details>
  <summary>Details</summary>
Motivation: VLA模型默认可能过度关注任务无关区域的图像令牌（即‘干扰令牌’），从而干扰动作生成，降低任务成功率。

Method: 提出Distracting Token Pruning（DTP）框架，动态检测并剪除干扰图像令牌，以校正视觉注意力分布。

Result: 在SIMPLER基准上，DTP在多种新型VLA模型上均取得一致的相对成功率提升；分析发现任务成功率与任务无关区域注意力强度呈负相关。

Conclusion: DTP是一种简单有效、通用性强的即插即用方法，揭示了VLA模型中普遍存在的注意力偏差问题，并为未来研究提供方向。

Abstract: Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.

</details>


### [29] [Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2601.15734)
*Shadi Alijani,Fereshteh Aghaee Meibodi,Homayoun Najjaran*

Main category: cs.CV

TL;DR: 本文提出了一种面向多模态医学影像的基础模型适配新框架，包含子区域感知的模态注意力机制和自适应提示工程，显著提升了脑肿瘤分割（尤其是坏死核心区域）的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在多模态医学影像中难以有效融合多源信息并适应病理组织的异质性，亟需更优的多模态融合与适配方法。

Method: 提出子区域感知的模态注意力机制（实现各肿瘤子区域的最优模态组合）与自适应提示工程（利用基础模型固有能力提升分割精度）。

Result: 在BraTS 2020数据集上验证，所提方法显著优于基线方法，尤其在坏死核心子区域分割上表现突出。

Conclusion: 该框架为多模态医学影像中的基础模型适配提供了原理清晰、效果显著的新范式，推动了更精准鲁棒的医学影像AI解决方案发展。

Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.

</details>


### [30] [Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework](https://arxiv.org/abs/2601.15739)
*Xinjue Hu,Chi Wang,Boyu Wang,Xiang Zhang,Zhenshan Tan,Zhangjie Fu*

Main category: cs.CV

TL;DR: 本文提出ARDIS，首个任意分辨率深度图像隐写框架，通过频域解耦架构和潜在引导的隐式重建器，解决了秘密图像与载体图像分辨率不一致导致的细节丢失和盲恢复问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度图像隐写方法要求秘密图像与载体图像分辨率一致，导致不匹配分辨率需重采样而丢失细节，且无法在未知原始分辨率下恢复原图。

Method: 提出频域解耦架构（分离全局基与高频潜码）和潜在引导的隐式重建器（用潜码调制连续隐函数重建高频残差），并引入隐式分辨率编码策略（将离散分辨率映射为稠密特征图并嵌入特征冗余空间）以实现盲恢复。

Result: ARDIS在不可见性和跨分辨率恢复保真度上显著优于当前最优方法。

Conclusion: ARDIS成功将隐写范式从离散映射转向参考引导的连续信号重建，实现了任意分辨率下的高保真、盲恢复隐写。

Abstract: Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.

</details>


### [31] [White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification](https://arxiv.org/abs/2601.15757)
*Yimin Zhu,Lincoln Linlin Xu,Zhengsen Xu,Zack Dewis,Mabel Heffring,Saeid Taleghanidoozdoozan,Motasem Alkayid,Quinn Ledingham,Megan Greenwood*

Main category: cs.CV

TL;DR: 本文提出了一种物理光谱感知的白盒超连接框架ES-mHC，用于高光谱图像分类，通过结构化、有向矩阵显式建模不同电磁波谱分组间的交互，提升模型可解释性与内部机制理解。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在高光谱图像分类中依赖不透明的光谱-空间特征混合，导致可解释性差、内部决策机制难以理解。

Method: 提出ES-mHC框架，将特征表示与交互结构分离，利用定向矩阵显式建模电磁波谱分组（如残差流）间的交互，并支持交互模式的可视化与空间分析。

Result: 实验表明学习到的超连接矩阵呈现一致的空间模式和非对称交互行为；增大扩展率可加速结构化交互模式的出现。

Conclusion: ES-mHC将高光谱图像分类从纯黑箱预测转变为结构透明、部分白盒的学习过程，增强了模型的可解释性与机理洞察力。

Abstract: In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.

</details>


### [32] [Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)](https://arxiv.org/abs/2601.15759)
*Qi Zeng,Weide Liu,Bo Li,Ryne Didier,P. Ellen Grant,Davood Karimi*

Main category: cs.CV

TL;DR: 本文提出FeTal-SAM，一种面向胎儿脑MRI分割的SAM新适配方法，通过结合图谱引导的密集提示与边界框提示，实现无需重训练的灵活、逐结构二值分割，并在多个数据集上验证其鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习方法在胎儿脑MRI分割中依赖大量标注数据、难以适应标签定义变化，以及分割结果难以区分是源于图像对比度还是空间先验的问题。

Method: 将多图谱配准生成的空间对齐标签模板作为密集提示，联合边界框提示输入SAM分割解码器，实现单结构二值分割，再融合重建完整3D分割体。

Result: 在dHCP和内部数据集上表现稳健；对高对比度结构（如皮层板、小脑）Dice分数媲美专用训练的SOTA方法；支持用户自定义解剖结构分割；对低对比度结构（如海马、杏仁核）精度略低。

Conclusion: FeTal-SAM是一种无需频繁重训练的通用胎儿脑MRI分割模型，朝临床可适配分析工具迈出重要一步。

Abstract: This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.

</details>


### [33] [LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps](https://arxiv.org/abs/2601.15766)
*Yuhan Chen,Ying Fang,Guofa Li,Wenxuan Yu,Yicui Shi,Jingrui Zhang,Kefei Qian,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 本文提出了LL-GaussianMap，首个将2D高斯溅射（2DGS）引入低光照图像增强的无监督框架，通过显式结构建模生成增益图，在保持边缘、抑制伪影的同时避免配对数据依赖。


<details>
  <summary>Details</summary>
Motivation: 现有低光照增强方法多在像素域或隐式特征空间操作，忽视图像固有几何结构先验；而2DGS具备优异的结构拟合与高效渲染能力，但在底层视觉任务中尚未被探索。

Method: 提出两阶段无监督框架：首先用2DGS进行高保真结构重建；再通过高斯光栅化机制在统一增强模块中渲染数据驱动的增强字典系数，以生成结构引导的增益图。

Result: 在增强性能上优于现有方法，同时存储开销极低，验证了显式高斯表示在图像增强中的有效性。

Conclusion: LL-GaussianMap成功将2DGS这一显式场景表示引入低层视觉任务，证明结构先验建模对低光照增强至关重要，且无监督范式可有效规避配对数据需求。

Abstract: Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.

</details>


### [34] [LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting](https://arxiv.org/abs/2601.15772)
*Yuhan Chen,Wenxuan Yu,Guofa Li,Yijun Xu,Ying Fang,Yicui Shi,Long Cao,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 本文提出LL-GaussianImage，首个在2D高斯泼溅（2DGS）压缩表示域内直接进行零样本无监督低光增强的框架，避免传统解压-增强-再压缩流程，兼顾高效性与高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有低光增强算法主要在像素域操作，处理2DGS压缩图像需繁琐的解压-增强-再压缩流程，导致效率低和二次退化。

Method: 提出语义引导的MoE增强框架、多目标协同损失函数、两阶段优化策略，在2DGS稀疏属性空间中实现压缩即增强与重建即增强。

Result: 在保持高图像保真度与高压缩比的同时，实现了高质量低光图像增强；实验验证了直接在压缩表示域处理的可行性与优越性。

Conclusion: LL-GaussianImage开创了在显式3D场景表示压缩域中直接进行图像增强的新范式，为高效、保真、端到端的低光视觉任务提供了新思路。

Abstract: 2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.

</details>


### [35] [Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation](https://arxiv.org/abs/2601.15779)
*Liuyun Jiang,Yanchao Zhang,Jinyue Guo,Yizhuo Lu,Ruining Zhou,Hua Han*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的数据增强框架NeuroDiff，用于电子显微镜下的神经元分割，通过分辨率感知的条件扩散模型和生物学引导的掩码重塑模块生成结构合理、多样化的图像-标签对，在低标注场景下显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大量人工标注数据，传统数据增强方法生成样本结构多样性不足，难以满足神经元分割对结构真实性的高要求。

Method: 提出基于扩散模型的数据增强框架：1）分辨率感知的条件扩散模型，结合多尺度条件与EM分辨率先验，实现从3D掩码到体素级图像的合成；2）生物学引导的掩码重塑模块，提升增强掩码的结构真实性。

Result: 在AC3和AC4低标注数据集上，结合两种后处理方法，ARAND指标分别提升32.1%和30.7%。

Conclusion: 扩散模型可有效生成结构合理、多样化的训练样本，缓解神经元分割中数据稀缺问题，为生物医学图像分析提供新思路。

Abstract: Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.

</details>


### [36] [Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video](https://arxiv.org/abs/2601.15780)
*Pascal Benschop,Justin Dauwels,Jan van Gemert*

Main category: cs.CV

TL;DR: 本文提出了一种用于评估视觉语言模型（VLMs）空间推理能力的合成视频基准，聚焦于情境与空间意识，涵盖暴力识别、施害者角色绑定和轨迹对齐三类任务；实验表明当前VLMs表现仅略高于随机水平，简单颜色线索仅部分缓解问题，作者开源数据与代码以推动轻量级空间先验研究。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在依赖细微时间或几何线索的空间推理任务中表现脆弱，缺乏系统性诊断工具来揭示其在情境与空间理解上的根本缺陷。

Method: 构建一个合成视频基准，包含最小化对比的视频对，设计三项挑战任务：区分暴力与良性行为、跨视角绑定施害者角色、判断细粒度运动轨迹对齐；在零训练（training-free）设定下评估多种最新VLMs，并引入稳定颜色线索作为辅助分析手段。

Result: 各任务上主流VLMs性能仅略高于随机水平；稳定颜色线索可部分缓解施害者角色混淆，但无法解决核心空间推理薄弱问题。

Conclusion: 当前VLMs的空间与情境推理能力严重不足，亟需结合轻量级空间先验与大规模预训练，本工作提供可复现诊断工具以支持该方向探索。

Abstract: Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.

</details>


### [37] [A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks](https://arxiv.org/abs/2601.15810)
*Mustafa Yurdakul,Enes Ayan,Fahrettin Horasan,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 本文提出了一种基于CNN的移动应用，用于非专业人士快速识别花卉种类，通过对比MobileNet、DenseNet121和Xception三种模型及七种优化算法，发现DenseNet121结合SGD效果最佳，准确率达95.84%。


<details>
  <summary>Details</summary>
Motivation: 花卉识别通常需要专家知识，但专家资源难以随时获取，因此需要一种便捷、高效的自动识别工具。

Method: 采用MobileNet、DenseNet121和Xception三种CNN模型，并分别结合七种优化算法进行训练与评估。

Result: DenseNet121配合SGD优化算法在花卉分类任务中表现最优，准确率、精确率、召回率和F1分数均达95.84%–96.00%。

Conclusion: CNN模型（尤其是DenseNet121）适用于移动端花卉识别，为非专业人士提供了高效可行的解决方案。

Abstract: A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.

</details>


### [38] [Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data](https://arxiv.org/abs/2601.15813)
*Clare Chemery,Hendrik Edelhoff,Ludwig Bothmann*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级机器学习实验流程，帮助生态学家无需深度ML专业知识即可自主构建任务特定的图像分类模型，并在红鹿年龄与性别分类任务中取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 降低生态学家应用机器学习进行图像分类的门槛，使其能摆脱现成模型限制，针对本地数据和具体生态问题构建定制化模型。

Method: 设计了结合命令行接口（用于预处理、训练、评估）和图形界面（用于标注、错误分析、模型比较）的轻量级实验流程，并在红鹿相机陷阱图像数据集上验证，尝试多种骨干网络、参数与数据增强策略。

Result: 在4352张专家标注的红鹿个体图像上，最佳模型在年龄分类达90.77%准确率，性别分类达96.15%准确率。

Conclusion: 即使数据有限，针对明确定义的窄域生态问题，仍可实现可靠的种群特征分类；该框架为生态学家提供了易用、可定制的ML建模工具，有助于推动ML在野生动物监测与种群分析中的普及应用。

Abstract: We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.

</details>


### [39] [Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion](https://arxiv.org/abs/2601.15829)
*Yonghao Xu,Pedram Ghamisi,Qihao Weng*

Main category: cs.CV

TL;DR: 本文首次将数据集蒸馏引入遥感图像解译领域，提出一种基于文本到图像扩散模型的数据集蒸馏方法，并结合分类器引导与潜在空间聚类提升合成样本判别性与多样性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在遥感图像解译中依赖大规模标注数据，带来高存储计算成本和敏感数据泄露风险。

Method: 利用文本到图像扩散模型进行遥感数据集蒸馏；引入预训练分类器的一致性损失实现分类器驱动引导；在潜在空间聚类选取代表性原型作为视觉风格引导，并用视觉语言模型生成聚合文本描述。

Result: 在三个高分辨率遥感场景分类基准上验证了方法可蒸馏出逼真、多样且利于下游模型训练的样本。

Conclusion: 所提方法有效缓解遥感图像解译中对大规模数据的依赖，在保障性能的同时降低存储计算开销与数据安全风险。

Abstract: Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).

</details>


### [40] [An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics](https://arxiv.org/abs/2601.15830)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本文提出了一种基于IoT的智能植物监测系统，利用ESP32采集多类环境数据，结合自动灌溉与云平台分析，显著提升水分利用效率与植物健康管理水平。


<details>
  <summary>Details</summary>
Motivation: 全球对可持续农业的需求日益增长，传统农业依赖人工观察和周期性浇水，易导致水资源浪费、植物生长不均及对环境变化响应滞后。

Method: 采用ESP32微控制器集成DHT22（温湿度）、HC-SR04（水位）和土壤湿度传感器，通过OLED显示与蜂鸣器报警实现本地反馈，并将数据无线上传至ThingSpeak云平台进行远程监控、历史分析与自动告警；配套开发了可视化Web仪表盘。

Result: 实验表明系统在维持土壤湿度方面准确率达92%，实时环境监测可靠，节水约40%；总成本仅45.20美元。

Conclusion: 该系统成本低、可扩展性强，适用于家庭园艺与商业农业，为精准农业提供了实用可行的智能解决方案。

Abstract: The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.

</details>


### [41] [TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing](https://arxiv.org/abs/2601.15838)
*Toan Gian,Dung T. Tran,Viet Quoc Pham,Francesco Restuccia,Van-Dinh Nguyen*

Main category: cs.CV

TL;DR: 本文提出TinySense，一种基于VQGAN的高效Wi-Fi CSI数据压缩框架，用于提升设备无感、隐私保护型人体姿态估计（HPE）的可扩展性；通过K-means动态码本子集划分与Transformer增强鲁棒性，在显著降低延迟和网络开销的同时，提升HPE准确率。


<details>
  <summary>Details</summary>
Motivation: 现有Wi-Fi感知方法直接处理海量CSI数据，导致网络资源紧张，难以满足设备无感和隐私保护的人体姿态估计需求。

Method: 提出TinySense框架：1）构建向量量化生成对抗网络（VQGAN）学习紧凑码本以压缩CSI；2）用K-means动态聚类预训练码本以调节压缩比特率；3）引入Transformer缓解比特率损失、增强弱网鲁棒性；4）在Jetson Nano与Raspberry Pi实验平台验证。

Result: 相比SOTA压缩方案，TinySense在相同压缩率下HPE准确率（PCK20）提升1.5倍，延迟降低5倍，网络开销减少2.5倍。

Conclusion: TinySense通过联合学习码本、动态量化与Transformer建模，实现了高保真CSI压缩与高效Wi-Fi HPE，兼顾精度、效率与鲁棒性，推动边缘Wi-Fi感知实用化。

Abstract: With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.

</details>


### [42] [A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies](https://arxiv.org/abs/2601.15865)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级、受大脑启发的深度学习框架，用于冠状动脉造影图像的二分类，通过选择性神经可塑性训练和注意力调制损失函数，有效应对标注不确定性、类别不平衡和计算资源受限等临床挑战。


<details>
  <summary>Details</summary>
Motivation: 真实临床场景中冠状动脉造影图像存在复杂病变形态、严重类别不平衡、标签不确定性及计算资源受限等问题，导致传统深度学习方法鲁棒性和泛化性不足。

Method: 基于预训练CNN构建轻量级混合神经表征；引入选择性神经可塑性训练策略实现高效参数适配；采用融合Focal Loss与标签平滑的脑启发注意力调制损失函数；结合类别不平衡感知采样与带热重启的余弦退火优化策略。

Result: 所提模型在二分类任务中展现出强而稳定的性能，准确率、召回率、F1分数和AUC指标均具竞争力，同时保持高计算效率。

Conclusion: 验证了脑启发学习机制在轻量级医学图像分析中的有效性，为资源受限环境下的智能临床决策支持提供了生物可解释且可部署的解决方案。

Abstract: Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.

</details>


### [43] [Out-of-Distribution Detection Based on Total Variation Estimation](https://arxiv.org/abs/2601.15867)
*Dabiao Ma,Zhiba Su,Jian Yang,Haojun Fei*

Main category: cs.CV

TL;DR: 本文提出了一种名为TV-OOD的新型分布外检测方法，利用总变差网络估计器计算输入样本对总变差的贡献（即总变差分数），以区分分布内与分布外数据，在图像分类任务中表现优于或媲美现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 应对实际应用中机器学习模型部署所面临的潜在分布偏移问题，提升模型鲁棒性与安全性。

Method: 提出总变差分布外检测（TV-OOD）方法，基于总变差网络估计器计算每个输入对整体总变差的贡献，定义为总变差分数，用于判别in-distribution与out-of-distribution数据。

Result: 在多种模型和数据集上的图像分类任务中，TV-OOD在所有评估指标下均达到与当前最先进OOD检测方法相当或更优的性能。

Conclusion: TV-OOD是一种有效、通用且性能优越的分布外检测方法，可增强机器学习模型在面对分布偏移时的可靠性。

Abstract: This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.

</details>


### [44] [PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis](https://arxiv.org/abs/2601.15884)
*Yifan Chen,Fei Yin,Hao Chen,Jia Wu,Chao Li*

Main category: cs.CV

TL;DR: 本文提出首个公开、完全配对的泛癌种医学影像数据集，涵盖11个人体器官的MR（含完整DCE序列）和CT（非增强/增强配对）数据，并基于此构建综合基准，推动AI驱动的安全、高效对比剂图像合成研究。


<details>
  <summary>Details</summary>
Motivation: 现有AI合成对比增强图像的研究受限于数据稀缺：公共数据集多局限于脑部MR配对数据；其他数据集存在配对不全、模态/时间戳缺失、空间错位及缺乏明确增强阶段标注等问题；大量高质量数据仍处于私有状态。

Method: 构建首个公开、完全配对、覆盖11个器官的泛癌种医学影像数据集（含MR的DCE1-DCE3全时相与CT的非增强/增强配对），确保解剖一致性，并设计支持1-to-1、N-to-1、N-to-N翻译任务的评估框架；在此基础上建立综合基准，评测主流图像到图像翻译模型。

Result: 发布了该大规模、高质量、多模态、多器官配对数据集及配套基准；在多个翻译任务上报告了代表性基线模型的性能结果；所有代码与数据集均已开源。

Conclusion: 该工作填补了泛癌种对比增强图像合成领域高质量公开数据的空白，为安全、鲁棒、临床可行的AI辅助影像诊断提供了关键资源与评估标准。

Abstract: Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.

</details>


### [45] [Understanding the Transfer Limits of Vision Foundation Models](https://arxiv.org/abs/2601.15888)
*Shiqi Huang,Yipei Wang,Natasha Thorley,Alexander Ng,Shaheer Saeed,Mark Emberton,Shonit Punwani,Veeru Kasivisvanathan,Dean Barratt,Daniel Alexander,Yipeng Hu*

Main category: cs.CV

TL;DR: 本文探讨了视觉基础模型（VFMs）在下游任务中表现不均衡的问题，提出预训练目标与下游任务需求之间的不匹配是主要原因，并通过前列腺多参数MRI任务验证了预训练与下游任务对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型（VFMs）在下游任务中表现不均衡，尽管计算投入巨大，作者认为这是由于预训练目标与下游视觉与成像任务需求之间存在不匹配。

Method: 作者评估了两种VFMs——基于掩码图像重建的MAE模型（ProFound）和基于对比学习的模型（ProViCNet），在五个前列腺多参数MRI任务上的迁移性能，并使用最大均值差异（MMD）等简单散度指标衡量预训练与下游任务的对齐程度。

Result: 研究发现，预训练与下游任务对齐程度越高（以MMD等指标衡量），迁移性能提升越显著、微调收敛越快。

Conclusion: 设计和分析预训练目标时，应充分考虑其在下游任务中的实际适用性，任务对齐是提升VFMs迁移性能的关键因素。

Abstract: Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.

</details>


### [46] [RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2601.15891)
*Anas Anwarul Haq Khan,Mariam Husain,Kshitij Jadhav*

Main category: cs.CV

TL;DR: RadJEPA是一种无需语言监督的自监督医学视觉表征学习框架，基于联合嵌入预测架构，在仅使用未标注胸部X光图像预训练下，通过预测掩码图像区域的潜在表示来学习鲁棒编码器，并在多项下游任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型依赖配对的图像-文本数据进行监督学习，但此类数据稀缺；因此，亟需探索不依赖语言监督、仅利用大量无标注医学影像即可学习鲁棒放射学编码器的新范式。

Method: 提出RadJEPA框架，基于联合嵌入预测架构（JEPA），在无标注胸部X光图像上进行自监督预训练；其核心是预测被掩码图像区域在潜在空间中的表示，而非传统图像-文本对齐或DINO式的全局视图自蒸馏。

Result: 在疾病分类、语义分割和报告生成等多个下游任务基准测试中，RadJEPA性能均优于当前最优方法（包括Rad-DINO）。

Conclusion: 仅利用无标注X光图像、摒弃语言监督的JEPA式自监督学习，可有效构建高性能放射学视觉编码器，为低资源医学AI提供新路径。

Abstract: Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.

</details>


### [47] [ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling](https://arxiv.org/abs/2601.15897)
*Zhaoqi Su,Shihai Chen,Xinyan Lin,Liqin Huang,Zhipeng Su,Xiaoqiang Lu*

Main category: cs.CV

TL;DR: 本文提出了ThermoSplat框架，通过跨模态FiLM调制与模态自适应几何解耦，实现RGB与热红外数据的深度光谱感知三维高斯点阵重建，显著提升多光谱场景渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯点阵方法难以有效融合RGB与热红外多模态数据，常忽略跨模态相关性或无法自适应处理不同光谱间的结构关联与物理差异。

Method: 提出ThermoSplat框架：1）Cross-Modal FiLM Modulation机制，利用热成像结构先验动态调控共享特征以指导可见光纹理合成；2）Modality-Adaptive Geometric Decoupling方案，为热分支学习独立不透明度偏移并执行独立光栅化；3）结合球谐函数显式表示与隐式神经解码的混合渲染管线。

Result: 在RGBT-Scenes数据集上实验表明，ThermoSplat在可见光与热红外谱段均达到当前最优渲染质量。

Conclusion: ThermoSplat有效解决了多光谱3DGS中跨模态特征融合与几何一致性建模难题，为鲁棒环境感知提供了新范式。

Abstract: Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.

</details>


### [48] [Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models](https://arxiv.org/abs/2601.15906)
*Zhen Zhang,Runhao Zeng,Sicheng Zhao,Xiping Hu*

Main category: cs.CV

TL;DR: 本文通过系统性机制研究发现，多模态基础模型中的情感建模能力主要由前馈网络中的门控投影层（gate_proj）决定，而非注意力模块；仅微调该模块即可实现接近全参数微调的情感任务性能，显著提升参数效率。


<details>
  <summary>Details</summary>
Motivation: 理解大规模基础模型（尤其是多模态模型）中情绪表征的内部机制仍是一个开放问题，现有情感模型虽性能强，但其架构层面如何支持情感理解与生成尚不清楚。

Method: 在多种架构、训练策略和情感任务上，系统分析情绪监督如何重塑模型内部参数；采用受控模块迁移、单模块针对性适配和破坏性消融实验验证gate_proj的作用。

Result: 发现情感适配主要定位在feed-forward gating projection（gate_proj），而非attention模块；仅调优约24.5%的参数（相比AffectGPT），即可达到其96.6%的平均任务性能。

Conclusion: 情感能力在基础模型中由前馈门控机制结构性介导，gate_proj是情感建模的核心架构位点。

Abstract: Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\texttt{gate\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \texttt{gate\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\% of the parameters tuned by AffectGPT, our approach achieves 96.6\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \texttt{gate\_proj} as a central architectural locus of affective modeling.

</details>


### [49] [The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars](https://arxiv.org/abs/2601.15914)
*Yarin Benyamin*

Main category: cs.CV

TL;DR: 本文探讨了在虚拟现实（VR）中为自闭症谱系障碍（ASD）患者提供实时情绪识别支持的可行性，发现现有通用深度学习模型难以兼顾低延迟与高精度，尤其在CPU端推理时存在分类阶段的‘延迟墙’；YOLOv11n在人脸检测中表现最优，但通用视觉Transformer（如CLIP、SigLIP）在表情识别任务中准确率低、速度慢，亟需轻量级、领域专用架构。


<details>
  <summary>Details</summary>
Motivation: 为ASD患者开发可及、实时的VR辅助社交技能训练系统，需满足严格运动到光子（MTP）延迟<140ms的要求，而现有SOTA模型多侧重精度，忽视边缘/低端硬件上的实时性约束。

Method: 在UIBVFED数据集上，对YOLO系列（v8/v11/v12的Medium/Nano变体）进行虚拟角色人脸检测基准测试，并评估CLIP、SigLIP、ViT-FER等通用视觉Transformer在零样本面部表情识别（FER）任务中的CPU端实时性能（延迟与准确率）。

Result: YOLOv11n检测延迟约54ms且检测准确率达100%；但所有通用Transformer在分类阶段均未达实时要求——准确率<23%，延迟>150ms，暴露出显著的‘延迟墙’。

Conclusion: 面向VR治疗场景的实时情绪识别不能依赖通用大模型，必须设计轻量、领域适配的专用架构，以兼顾低延迟、高精度与硬件可及性。

Abstract: In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a "Latency Wall" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.

</details>


### [50] [A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery](https://arxiv.org/abs/2601.15918)
*Valery Fischer,Alan Magdaleno,Anna-Katharina Calek,Nicola Cavalcanti,Nathan Hoffman,Christoph Germann,Joschua Wüthrich,Max Krähenmann,Mazda Farshad,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 本文提出了一种无需领域微调、基于现成预训练模型的多视角3D手部姿态估计方法，并构建了首个大规模手术场景手部标注数据集，显著提升了2D/3D姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 手术环境光照强烈且不均、手部常被器械或人员遮挡、戴手套导致外观单一，加之缺乏标注数据，使3D手部姿态估计极具挑战。

Method: 构建了一个多视角流水线：结合人体检测、全身姿态估计、跟踪手部区域后进行2D关键点预测，再通过约束优化得到3D姿态；同时发布了含68,000帧、3,000组手动标注2D关键点及三角化3D真值的手术基准数据集。

Result: 相比基线方法，2D平均关节点误差降低31%，3D平均每关节位置误差降低76%。

Conclusion: 本工作为手术场景下的3D手部姿态估计提供了无需训练的实用方案和首个高质量标注数据集，推动外科计算机视觉研究发展。

Abstract: Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.
  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.
  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.
  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.

</details>


### [51] [Class Confidence Aware Reweighting for Long Tailed Learning](https://arxiv.org/abs/2601.15924)
*Brainard Philemon Jagati,Jitendra Tembhurne,Harsh Goud,Rudra Pratap Singh,Chandrashekhar Meshram*

Main category: cs.CV

TL;DR: 本文提出了一种基于损失水平的类别与置信度感知重加权方案，用于解决长尾数据分布下的深度神经网络性能下降问题，该方案与现有logit调整方法互补，并在多个长尾数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 长尾数据分布导致深度神经网络性能显著下降，现有方法主要关注决策空间（如logit层）的调整，而忽视了因样本置信度差异带来的优化过程影响。

Method: 设计了一种基于损失水平的类别与置信度感知重加权方案，使用Ω(p_t, f_c)函数根据预测置信度和类别频率动态调节样本对训练的贡献。

Result: 在CIFAR-100-LT、ImageNet-LT和iNaturalist2018等多个长尾数据集上，不同不平衡因子下均取得显著提升，验证了方法的有效性。

Conclusion: 所提出的重加权方案能有效缓解长尾学习中的类别不平衡问题，且与logit调整类方法具有互补性，为长尾学习提供了新思路。

Abstract: Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.

</details>


### [52] [NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation](https://arxiv.org/abs/2601.15929)
*Liuyun Jiang,Yizhuo Lu,Yanchao Zhang,Jiazheng Liu,Hua Han*

Main category: cs.CV

TL;DR: 本文提出NeuroMamba框架，结合Mamba的线性复杂度优势实现无分块全局建模，并融合局部形态特征提取与自适应分辨率感知的全局依赖建模，显著提升神经元分割精度与边界细节保持能力。


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法缺乏长程上下文导致边界模糊，Transformer方法因分块操作丢失体素级细节而边界不准，需兼顾全局建模与精细局部结构。

Method: 提出NeuroMamba：1）通道门控的边界判别特征提取器（BDFE）强化局部形态线索；2）空间连续特征提取器（SCFE），将分辨率感知扫描机制嵌入Visual Mamba以自适应建模多尺度全局依赖；3）跨调制机制融合多视角特征。

Result: 在四个公开EM数据集上达到SOTA性能，对各向异性和各向同性分辨率均表现出强适应性。

Conclusion: NeuroMamba通过协同建模局部细节与全局依赖，有效解决了神经元分割中边界模糊与细节丢失的关键挑战，为高精度连接组重建提供了新范式。

Abstract: Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.

</details>


### [53] [EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis](https://arxiv.org/abs/2601.15951)
*Sheng Miao,Sijin Li,Pan Wang,Dongfeng Bai,Bingbing Liu,Yue Wang,Andreas Geiger,Yiyi Liao*

Main category: cs.CV

TL;DR: EvolSplat4D是一种新型前馈式四维场景重建框架，通过三个专用分支统一处理近场静态、动态目标和远场景物，兼顾效率与质量，在多个自动驾驶数据集上优于现有优化与前馈方法。


<details>
  <summary>Details</summary>
Motivation: 现有新型视角合成方法在静态与动态城市场景重建中难以兼顾重建速度与质量：神经辐射场与3D高斯泼溅需耗时的逐场景优化；而新兴前馈方法采用逐像素高斯表示，导致复杂动态环境中的3D不一致性。

Method: 提出EvolSplat4D前馈框架，包含三个分支：（1）基于3D特征体素预测多帧一致的近场静态3D高斯几何，并结合语义增强图像渲染模块预测外观；（2）在物体中心规范空间中，利用运动校正渲染模块聚合时序特征以实现稳健的4D动态重建；（3）采用高效逐像素高斯分支处理远场景物，保障全场景覆盖。

Result: 在KITTI-360、KITTI、Waymo和PandaSet数据集上实验表明，EvolSplat4D在静态与动态环境重建的精度与一致性上均优于逐场景优化方法及前沿前馈基线。

Conclusion: EvolSplat4D成功突破了前馈式NVS中逐像素表示导致的3D不一致瓶颈，通过分层、多模态高斯建模实现了高效、高质量、全场景的4D城市环境重建，为自动驾驶仿真提供了实用化新路径。

Abstract: Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.

</details>


### [54] [HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models](https://arxiv.org/abs/2601.15968)
*Xin Xie,Jiaxian Guo,Dong Gong*

Main category: cs.CV

TL;DR: 本文提出HyperAlign框架，通过训练超网络在测试时动态生成低秩适配权重，以高效对齐扩散模型生成结果与人类偏好，兼顾性能与效率，并在多个生成范式上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽性能优异，但常生成不符合人类偏好和意图的图像，存在美学质量差和语义不一致问题；现有对齐方法在多样性损失与计算开销之间难以兼顾。

Method: 提出HyperAlign框架，利用超网络在测试时动态生成低秩适配权重，调制扩散模型的生成算子，依据输入潜变量、时间步和提示自适应调整去噪轨迹；设计多种应用频率变体，并采用带偏好数据正则化的奖励分数目标优化超网络。

Result: 在Stable Diffusion和FLUX等多个扩展生成范式上，HyperAlign显著优于现有微调和测试时缩放基线，在提升语义一致性和视觉吸引力方面效果突出。

Conclusion: HyperAlign提供了一种高效、灵活且鲁棒的测试时对齐新范式，有效缓解了奖励过优化与计算代价之间的权衡，推动扩散模型更可靠地响应人类意图。

Abstract: Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.

</details>


### [55] [PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models](https://arxiv.org/abs/2601.16007)
*Chak-Wing Mak,Guanyu Zhu,Boyi Zhang,Hongji Li,Xiaowei Chi,Kevin Zhang,Yichen Wu,Yangfan He,Chun-Kai Fan,Wentao Lu,Kuangzhi Ge,Xinyu Fang,Hongyang He,Kuan Lu,Tianxiang Xu,Li Zhang,Yongxin Ni,Youhua Li,Shanghang Zhang*

Main category: cs.CV

TL;DR: 本文提出了PhysicsMind基准，用于评估多模态大模型和视频世界模型在物理规律（质心、杠杆平衡、牛顿第一定律）理解与生成方面的能力，发现现有模型仍依赖表观启发式，难以满足基本力学约束。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法有效衡量模型对物理规律的理解，多依赖合成数据或感知质量，缺乏对物理守恒律一致性的评估。

Method: 构建包含真实场景与仿真环境的统一基准PhysicsMind，涵盖视觉问答（VQA）与视频生成（VG）两类任务，聚焦质心、杠杆平衡和牛顿第一定律三大物理原理。

Result: 在PhysicsMind上评测的主流多模态与视频生成模型表现较差，常违反基本力学约束，表明当前训练范式不足以支撑鲁棒的物理理解。

Conclusion: PhysicsMind为物理感知多模态模型提供了聚焦、可扩展的评测平台，揭示了现有模型在物理推理与生成上的根本性局限。

Abstract: Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.

</details>


### [56] [PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry](https://arxiv.org/abs/2601.16024)
*Rongze Ma,Mengkang Lu,Zhenyu Xiang,Yongsheng Pan,Yicheng Wu,Qingjie Zeng,Yong Xia*

Main category: cs.CV

TL;DR: 本文提出PAINT框架，通过结构优先的条件生成方法，利用空间结构起始图（3S-Map）实现从H&E图像到IHC图像的高保真虚拟染色合成，显著提升结构一致性和临床任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统IHC染色成本高、耗组织；而现有基于外观合成的虚拟IHC方法因缺乏结构先验，易导致语义不一致。

Method: 提出Pathology-Aware Integrated Next-Scale Transformation (PAINT)，一种视觉自回归框架：以全局结构布局为条件，按因果顺序逐级生成分子细节；核心是引入Spatial Structural Start Map (3S-Map)，将自回归初始化锚定于观测到的形态学结构。

Result: 在IHC4BC和MIST数据集上，PAINT在结构保真度和临床下游任务（如分子表达预测）中均超越现有最优方法。

Conclusion: 结构引导的自回归建模是提升虚拟IHC合成质量与临床可用性的有效范式。

Abstract: Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.

</details>


### [57] [ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation](https://arxiv.org/abs/2601.16060)
*Yuan Lin,Murong Xu,Marc Hölle,Chinmay Prabhakar,Andreas Maier,Vasileios Belagiannis,Bjoern Menze,Suprosanna Shit*

Main category: cs.CV

TL;DR: 本文提出ProGiDiff框架，利用预训练扩散模型结合ControlNet式条件机制实现医学图像多类分割，并支持自然语言提示和跨模态迁移。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法缺乏对自然语言提示的支持、多提案生成能力及跨模态适应性；而从头训练文本到图像扩散模型在医学领域受限于数据稀缺且难以支持多类分割。

Method: 提出ProGiDiff框架，采用ControlNet风格的条件机制与定制编码器，将预训练扩散模型引导生成分割掩码；支持通过自然语言提示指定目标器官，扩展至多类分割；并利用低秩、少样本适配实现跨模态（如CT到MR）迁移。

Result: 在CT器官分割任务上性能优于先前方法；支持专家参与的多提案交互；经少量样本微调即可有效迁移到MR图像分割。

Conclusion: ProGiDiff为医学图像分割提供了一种高效、灵活、可迁移的新范式，兼具语言驱动、多类支持与跨模态泛化能力。

Abstract: Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.

</details>


### [58] [DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models](https://arxiv.org/abs/2601.16073)
*Hanwen Zhang,Qiaojin Shen,Yuxi Liu,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: DSFedMed is a dual-scale federated framework for medical image segmentation that enables mutual knowledge distillation between a centralized foundation model and lightweight client models, improving performance while drastically reducing communication and inference costs.


<details>
  <summary>Details</summary>
Motivation: Foundation Models (FMs) face challenges in federated settings due to high computational demands, communication overhead, and inference costs—especially in resource-limited medical applications.

Method: DSFedMed introduces mutual knowledge distillation between a central foundation model and lightweight client models; it uses synthetically generated high-quality medical images and a learnability-guided sample selection strategy to enhance distillation efficiency and effectiveness.

Result: On five medical imaging segmentation datasets, DSFedMed achieves ~2% average Dice score improvement and reduces communication costs and inference time by ~90% compared to existing federated FM baselines.

Conclusion: DSFedMed significantly improves efficiency and scalability of foundation models in federated medical image segmentation, enabling practical deployment under resource constraints.

Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.

</details>


### [59] [Masked Modeling for Human Motion Recovery Under Occlusions](https://arxiv.org/abs/2601.16079)
*Zhiyin Qian,Siwei Zhang,Bharat Lal Bhatnagar,Federica Bogo,Siyu Tang*

Main category: cs.CV

TL;DR: MoRo是一种基于掩码建模的端到端生成式框架，用于从单目视频中鲁棒地重建人体运动，尤其在频繁遮挡下表现优异，兼顾高精度、高真实感与实时推理（70 FPS）


<details>
  <summary>Details</summary>
Motivation: 现有方法在遮挡场景下存在明显缺陷：回归法脆弱，优化/扩散法速度慢、预处理重；且缺乏充足配对的视频-动作数据

Method: 提出MoRo框架，采用视频条件下的掩码建模范式；设计跨模态学习方案，融合三类先验：动捕数据训练的轨迹感知运动先验、图像-姿态数据训练的姿态先验、以及在视频-动作数据上微调的视频条件掩码Transformer

Result: 在EgoBody和RICH数据集上显著超越SOTA方法，遮挡下精度与运动真实性更优，无遮挡时性能相当；单H200 GPU达70 FPS实时推理

Conclusion: MoRo通过掩码建模与跨模态先验融合，实现了遮挡鲁棒、高效、端到端的人体运动重建，为真实场景应用提供了新范式

Abstract: Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.

</details>


### [60] [SAMTok: Representing Any Mask with Two Words](https://arxiv.org/abs/2601.16093)
*Yikang Zhou,Tao Zhang,Dengxian Gong,Yuanzheng Wu,Ye Tian,Haochen Wang,Haobo Yuan,Jiacong Wang,Lu Qi,Hao Fei,Anran Wang,Zhuochen Wang,Yujing Wang,Cheng Chen,Shunping Ji,Xiangtai Li*

Main category: cs.CV

TL;DR: 本文提出SAMTok，一种离散掩码分词器，将区域掩码转换为两个特殊标记，使基础多模态大语言模型（如QwenVL）无需架构修改即可通过标准语言建模和简单强化学习获得像素级理解与生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有像素级多模态大语言模型（MLLMs）因依赖复杂区域编码器、专用分割解码器及不兼容训练目标而难以扩展。

Method: 提出SAMTok：基于SAM2，使用掩码编码器和残差向量量化器将任意掩码编码为两个离散、紧凑且信息丰富的token；在2.09亿掩码上预训练，并构建500万SAMTok格式的掩码理解和生成数据；结合文本答案匹配奖励进行高效强化学习优化。

Result: QwenVL-SAMTok在区域描述、区域视觉问答、接地对话、指代表达分割、场景图解析及多轮交互式分割等任务上达到SOTA或相当水平；在GRES和GCG基准上通过强化学习显著提升性能。

Conclusion: SAMTok提供了一种可扩展、简洁有效的新范式，使基础MLLMs能便捷地获得强大像素级能力，无需修改模型结构或设计专用损失函数。

Abstract: Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.

</details>


### [61] [Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2601.16098)
*Zack Dewis,Yimin Zhu,Zhengsen Xu,Mabel Heffring,Saeid Taleghanidoozdoozan,Quinn Ledingham,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 本文提出CSSMamba框架，通过聚类引导的空间-光谱Mamba结构，结合可学习聚类模块与注意力驱动的令牌选择机制，显著提升高光谱图像分类精度与边界保持能力。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在高光谱图像分类中虽有提升，但在构建高效自适应令牌序列方面存在关键挑战。

Method: 提出CSSMamba框架，包括：1）聚类引导的空间Mamba模块（CSpaMamba）；2）与光谱Mamba模块（SpeMamba）融合；3）注意力驱动的令牌选择机制；4）可学习聚类模块。

Result: 在Pavia University、Indian Pines和Liao-Ning 01数据集上，CSSMamba在分类精度和边界保持方面均优于当前CNN、Transformer及Mamba方法。

Conclusion: CSSMamba通过引入聚类引导与注意力机制，有效提升了Mamba在高光谱图像分类任务中的效率与性能，为HSI建模提供了新思路。

Abstract: Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.

</details>


### [62] [Learning to Watermark in the Latent Space of Generative Models](https://arxiv.org/abs/2601.16140)
*Sylvestre-Alvise Rebuffi,Tuan Tran,Valeriu Lacatusu,Pierre Fernandez,Tomáš Souček,Nikola Jovanović,Tom Sander,Hady Elsahar,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 本文提出DistSeal，一种在生成模型潜在空间中进行水印嵌入的统一方法，支持扩散模型和自回归模型；通过在潜在空间训练后处理水印模型并将其蒸馏至生成模型或潜在解码器中，实现高效、鲁棒且不可感知的水印嵌入，相比像素空间方法提速最高达20倍。


<details>
  <summary>Details</summary>
Motivation: 现有AI图像水印方法多为像素空间的后处理方式，存在计算开销大和易引入视觉伪影的问题。

Method: 提出潜在空间水印框架DistSeal：在生成模型（如扩散或自回归模型）的潜在空间中训练后处理水印模型，并将其蒸馏到生成模型本体或潜在解码器中，实现内建水印能力。

Result: 所提潜在水印方案在鲁棒性上与像素空间基线相当，同时保持同等不可感知性，并获得最高20倍的推理加速；实验表明，潜在空间水印模型的蒸馏效果优于像素空间水印模型的蒸馏。

Conclusion: 潜在空间水印是一种更高效、更鲁棒的AI生成图像版权保护新范式，DistSeal为跨架构生成模型提供了统一、实用的水印解决方案。

Abstract: Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.

</details>


### [63] [ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion](https://arxiv.org/abs/2601.16148)
*Remy Sabathier,David Novotny,Niloy J. Mitra,Tom Monnier*

Main category: cs.CV

TL;DR: ActionMesh 是一种新型生成模型，通过引入时间维度改进3D扩散模型，实现从单目视频、文本或3D网格等输入快速生成高质量、拓扑一致、无需骨骼绑定的动画3D网格。


<details>
  <summary>Details</summary>
Motivation: 现有生成动画3D对象的方法受限于设置复杂、运行时间长或质量不足，难以实际应用。

Method: 提出‘时间3D扩散’框架：1）修改3D扩散模型以生成时序同步的3D形状隐空间序列；2）设计时间3D自编码器，将独立形状序列映射为参考形状的形变序列，构建动画。

Result: 在Consistent4D和Objaverse等标准基准上达到几何精度与时间一致性SOTA；生成速度快、结果无骨架依赖、拓扑一致，支持快速迭代及纹理、重定向等下游应用。

Conclusion: ActionMesh实现了高质量、高效率、易部署的动画3D网格生成，显著推进了生成式4D建模的实用性。

Abstract: Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes "in action" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed "temporal 3D diffusion". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.

</details>


### [64] [360Anything: Geometry-Free Lifting of Images and Videos to 360°](https://arxiv.org/abs/2601.16192)
*Ziyi Wu,Daniel Watson,Andrea Tagliasacchi,David J. Fleet,Marcus A. Brubaker,Saurabh Saxena*

Main category: cs.CV

TL;DR: 本文提出360Anything，一种无需几何先验和相机参数的、基于扩散Transformer的数据驱动方法，实现从单张图像或视频到360°全景图的高质量生成，并解决ERP边界接缝问题，同时具备零样本相机参数估计能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖已知相机参数进行透视图到球面全景图（ERP）的几何对齐，难以适用于野外无标定或标定噪声大的真实数据。

Method: 提出几何无关的360Anything框架，将透视输入与ERP目标视为token序列，利用预训练扩散Transformer进行纯数据驱动建模；引入Circular Latent Encoding解决VAE编码器零填充导致的ERP边界 seam 问题。

Result: 在图像和视频的透视到360°生成任务上达到SOTA，优于使用真实相机参数的先前方法；在零样本FoV与朝向估计基准上表现具竞争力。

Conclusion: 360Anything摆脱了对显式几何与相机参数的依赖，展现出强泛化性与隐式几何理解能力，拓展了扩散模型在沉浸式3D内容生成与基础视觉任务中的应用边界。

Abstract: Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.

</details>


### [65] [Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders](https://arxiv.org/abs/2601.16208)
*Shengbang Tong,Boyang Zheng,Ziteng Wang,Bingda Tang,Nanye Ma,Ellis Brown,Jihan Yang,Rob Fergus,Yann LeCun,Saining Xie*

Main category: cs.CV

TL;DR: 本文研究了Representation Autoencoders（RAEs）在大规模文本到图像（T2I）生成任务中的可扩展性，发现RAE相比VAE在预训练和微调中均更稳定、收敛更快、生成质量更高，并支持统一的多模态表征与推理。


<details>
  <summary>Details</summary>
Motivation: 探究RAE框架能否从ImageNet尺度扩展到大规模、自由形式的文本到图像生成任务，并验证其在不同数据、架构与训练规模下的有效性与鲁棒性。

Method: 在冻结SigLIP-2编码器前提下，扩展RAE解码器训练数据（网络、合成、文字渲染），系统评估RAE设计选择（如噪声调度、扩散头宽度、噪声增强解码）；在0.5B–9.8B参数规模上与FLUX VAE进行控制实验对比，涵盖预训练、微调稳定性及生成质量分析。

Result: RAE在所有模型尺度预训练中均优于VAE；微调中VAE在64轮后灾难性过拟合，而RAE在256轮内保持稳定并持续提升性能；RAE收敛更快、生成质量更高；共享表征空间支持视觉理解与生成联合推理。

Conclusion: RAE是一种比VAE更简单、更强大、更稳定的大规模T2I生成基础架构，且为统一多模态建模提供了新路径。

Abstract: Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.

</details>


### [66] [PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation](https://arxiv.org/abs/2601.16210)
*Onkar Susladkar,Tushar Prakash,Adheesh Juvekar,Kiet A. Nguyen,Dong-Hwan Jang,Inderjit S Dhillon,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 本文提出了PyraTok，一种语言对齐的金字塔式视频分词器，通过多尺度语义结构化离散潜在表示提升跨模态对齐与零样本迁移能力，在多个视频任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有离散视频VAE分词器通常在单一尺度、有限词表和浅层语言监督下学习，导致跨模态对齐差、零样本迁移能力弱。

Method: PyraTok基于预训练视频VAE，引入语言对齐金字塔量化（LaPQ）模块，利用共享大二进制码本在多个深度离散编码器特征，并联合优化多尺度文本引导量化与层级自回归目标。

Result: 在十个基准上实现SOTA视频重建效果；持续提升文本到视频生成质量；在视频分割、时序动作定位和视频理解任务上取得新SOTA零样本性能，并可稳健扩展至4K/8K分辨率。

Conclusion: PyraTok通过多尺度、语言对齐的离散化策略，显著提升了视频表征的紧凑性、表达力与跨模态一致性，为视频生成与理解提供了更优的基础分词器。

Abstract: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.

</details>


### [67] [Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition](https://arxiv.org/abs/2601.16211)
*Geo Ahn,Inwoong Lee,Taeoh Kim,Minho Shim,Dongyoon Wee,Jinwoo Choi*

Main category: cs.CV

TL;DR: 本文研究了组合视频理解（CVU）任务，发现现有零样本组合动作识别（ZS-CAR）模型因‘物体驱动的动词捷径’而失败；为此提出RCORE框架，通过组合感知增强和时序顺序正则化损失来缓解该问题，并在多个基准上显著提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有ZS-CAR模型在未见动词-物体组合上泛化能力差，主因是模型依赖数据共现统计而非真正学习组合语义，即存在‘物体驱动的动词捷径’这一被忽视的失败模式。

Method: 提出RCORE框架：（i）组合感知的数据增强，多样化动词-物体组合同时保留运动线索；（ii）时序顺序正则化损失，显式建模时间结构以抑制捷径行为。

Result: 在Sth-com和新构建的EK100-com两个基准上，RCORE显著提升未见组合识别准确率，降低对共现偏差的依赖，并实现稳定正向的组合泛化差距（compositional gap）。

Conclusion: 物体驱动的捷径是ZS-CAR的核心瓶颈；只有显式抑制此类捷径、强化时序动词学习，才能实现鲁棒的组合视频理解。

Abstract: We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.

</details>


### [68] [CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback](https://arxiv.org/abs/2601.16214)
*Wenhang Ge,Guibao Shen,Jiawei Feng,Luozhou Wang,Hao Lu,Xingye Tian,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出CamPilot，一种基于奖励反馈学习的高效相机可控视频生成方法，通过引入相机感知的3D解码器将视频潜在表示解码为3D高斯，并利用几何失真引导的像素级一致性奖励优化相机对齐。


<details>
  <summary>Details</summary>
Motivation: 现有相机控制视频扩散模型在相机可控性上仍受限，且直接应用Reward Feedback Learning（ReFL）面临三重挑战：缺乏评估视频-相机对齐能力的奖励模型、RGB解码计算开销大、忽略3D几何信息。

Method: 提出相机感知的3D解码器，将视频潜变量与相机位姿联合解码为3D高斯；以相机位姿同时作为输入和投影参数；利用错位导致的3D几何畸变引发渲染模糊现象，设计像素级新颖视角渲染与真值的一致性作为奖励；引入基于几何扭曲的可见性项，仅对确定性区域进行监督。

Result: 在RealEstate10K和WorldScore基准上验证了方法有效性，显著提升了视频-相机对齐精度与可控性。

Conclusion: CamPilot通过将相机控制嵌入3D几何解码与奖励建模，实现了更高效、更鲁棒的视频相机可控生成，为视频生成中的几何感知控制提供了新范式。

Abstract: Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [69] [Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration](https://arxiv.org/abs/2601.15296)
*Longxuan Wei,Yubo Zhang,Zijiao Zhang,Zhihu Wang,Shiwan Zhao,Tianyu Huang,Huiting Zhao,Chenfei Liu,Shenao Zhang,Junchi Yan*

Main category: cs.CL

TL;DR: 本文提出Entropy-Tree解码方法，利用模型预测熵指导树状搜索，在不确定性高处分支，提升推理任务的准确率与校准性。


<details>
  <summary>Details</summary>
Motivation: 现有解码策略（如随机采样或多路径独立采样）存在盲目性或冗余性，缺乏对模型不确定性进行有效利用的结构化探索机制。

Method: Entropy-Tree是一种基于熵的树形解码方法：以token级预测熵为依据动态决定是否在某位置扩展搜索分支，仅在模型高度不确定时进行探索。

Result: 在多个模型和数据集上，Entropy-Tree的pass@k优于Multi-chain；其预测熵在AUROC指标上优于多种传统不确定性度量。

Conclusion: Entropy-Tree将高效结构化搜索与可靠不确定性估计统一于单一解码过程，提升了大语言模型推理的性能与可解释性。

Abstract: Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.

</details>


### [70] [AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports](https://arxiv.org/abs/2601.15297)
*Edward Ajayi*

Main category: cs.CL

TL;DR: 本文提出了AfriEconQA——首个面向非洲经济分析的专用问答基准数据集，基于236份世界银行报告构建，包含8937个高质量问答实例，用于评测IR与RAG系统在数值推理与时间消歧上的能力。实验表明现有大模型（含零样本与RAG）在此任务上表现极差，凸显领域知识缺口。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型预训练语料中严重缺乏非洲经济领域的专业文档，导致其在该领域问答任务中性能低下；亟需一个具备高精度数值推理与时间溯源能力的专用基准来推动领域IR/RAG研究。

Method: 构建AfriEconQA数据集：从236份世行报告中生成并严格筛选8937个QA对，每个含问题、证据段落、验证答案及带时间戳的元数据；设计11组实验，对比GPT-5 Mini零样本基线与基于GPT-4o/Qwen32B的多种RAG配置（5种嵌入+排序策略）。

Result: 零样本模型答错超90%问题；即使最先进的RAG流水线也难以达到高精度；证实AfriEconQA具有强挑战性与鲁棒性。

Conclusion: AfriEconQA是首个专为非洲经济分析设计的高难度基准，暴露了当前LLM与RAG系统在专业、时效性强的区域经济任务中的根本局限，为未来领域适配的IR和RAG系统提供了关键评测平台。

Abstract: We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.

</details>


### [71] [Embedding Retrofitting: Data Engineering for better RAG](https://arxiv.org/abs/2601.15298)
*Anantha Sharma*

Main category: cs.CL

TL;DR: 本文提出了一种数据工程框架，解决真实语料中因标注伪影（如标签）导致的知识图谱质量下降问题，发现预处理对词向量回溯调整（retrofitting）效果影响远大于算法选择。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入回溯调整方法依赖知识图谱质量，而真实文本中的标注（如hashtag）会人为增加图密度、引入虚假边，损害回溯目标；但该数据质量问题常被忽视。

Method: 提出面向知识图谱构建的数据预处理框架，定量分析hashtag等标注对图密度和边质量的影响，并在干净与噪声图上对比多种retrofitting方法（特别是EWMA retrofitting）的检索性能变化。

Result: 在噪声图上所有retrofitting方法均显著退化（−3.5%至−5.2%，p<0.05）；经预处理后，EWMA方法提升6.2%（p=0.0348），其中定量合成类问题提升达33.8%；预处理优劣带来的性能波动（>10%）远超不同算法间差异（~3%）。

Conclusion: 知识图谱预处理质量是决定嵌入回溯调整成败的首要因素，其重要性超过算法本身；应将数据工程置于NLP下游任务优化的核心位置。

Abstract: Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.
  The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\%$ to $-5.2\%$, $p<0.05$). After preprocessing, \acrshort{ewma} retrofitting achieves $+6.2\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\%$ average). The gap between clean and noisy preprocessing (10\%+ swing) exceeds the gap between algorithms (3\%), establishing preprocessing quality as the primary determinant of retrofitting success.

</details>


### [72] [MALTopic: Multi-Agent LLM Topic Modeling Framework](https://arxiv.org/abs/2601.15299)
*Yash Sharma*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体大语言模型主题建模框架（MALTopic），通过将主题建模任务分解为多个专用LLM智能体（增强、建模、去重）协同完成，有效融合结构化与非结构化调查数据，提升了主题的连贯性、多样性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模方法仅利用自由文本、忽略结构化/分类调查数据，且生成的主题抽象、需大量人工解读。

Method: 提出MALTopic框架，包含三个LLM智能体：增强智能体利用结构化数据增强文本响应，主题建模智能体提取潜在主题，去重智能体优化结果。

Result: 在调查数据集上的对比实验表明，MALTopic在主题连贯性、多样性与可解释性上显著优于LDA和BERTopic。

Conclusion: MALTopic通过融合结构化数据与多智能体协作，生成更具上下文相关性和人类可读性的主题，为复杂调查数据分析提供了更有效的解决方案。

Abstract: Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.

</details>


### [73] [Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis](https://arxiv.org/abs/2601.15300)
*Weiwei Wang,Jiyong Min,Weijie Zou*

Main category: cs.CL

TL;DR: 本文系统研究了大语言模型（LLMs）在长上下文场景中出现的‘智能退化’现象，即当上下文接近特定临界长度时性能骤降超30%；通过自然长度分布分析、临界阈值实验测定（Qwen2.5-7B为最大上下文的40–50%），并提出统一框架解释‘浅层长上下文适应’机制。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理接近临界长度的长上下文时出现 catastrophic 性能下降（>30%），严重制约实际应用，但其成因缺乏系统性实证分析。

Method: 采用无截断/填充的自然样本长度进行因果性更强的分析；在覆盖5%-95%上下文长度的混合数据集（1000样本）上实验，结合五种方法交叉验证确定临界阈值；构建统一框架解释浅层长上下文适应现象。

Result: 发现Qwen2.5-7B的临界阈值位于最大上下文长度的40–50%，F1分数从0.55–0.56骤降至0.3（下降45.5%）；首次系统刻画开源Qwen系列的智能退化行为。

Conclusion: 智能退化源于模型仅具备浅层长上下文适应能力，无法泛化至超阈值长度；该工作为长上下文LLM部署提供实证依据与缓解策略基础。

Abstract: Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.

</details>


### [74] [Can We Trust LLM Detectors?](https://arxiv.org/abs/2601.15301)
*Jivnesh Sandhan,Harshit Jaiswal,Fei Cheng,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本文系统评估了当前主流的AI文本检测方法，发现它们在分布偏移、未见生成器和风格扰动下表现脆弱，并提出了一种监督对比学习框架以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在实际场景中（如分布偏移、新生成模型、风格扰动）表现不佳，缺乏泛化能力。

Method: 提出监督对比学习（SCL）框架，学习具有判别性的文本风格嵌入，以提升跨域检测鲁棒性。

Result: 实验表明监督方法在域内性能好但域外急剧下降，无训练方法对代理选择高度敏感；SCL在提升风格判别性和泛化性上展现出优势。

Conclusion: 构建真正领域无关的AI文本检测器仍面临根本性挑战，需更鲁棒的表征学习与评估范式。

Abstract: The rapid adoption of LLMs has increased the need for reliable AI text detection, yet existing detectors often fail outside controlled benchmarks. We systematically evaluate 2 dominant paradigms (training-free and supervised) and show that both are brittle under distribution shift, unseen generators, and simple stylistic perturbations. To address these limitations, we propose a supervised contrastive learning (SCL) framework that learns discriminative style embeddings. Experiments show that while supervised detectors excel in-domain, they degrade sharply out-of-domain, and training-free methods remain highly sensitive to proxy choice. Overall, our results expose fundamental challenges in building domain-agnostic detectors. Our code is available at: https://github.com/HARSHITJAIS14/DetectAI

</details>


### [75] [ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation](https://arxiv.org/abs/2601.15330)
*Zhebo Wang,Xiaohu Mu,Zijie Zhou,Mohan Li,Wenpeng Xing,Dezhang Kong,Meng Han*

Main category: cs.CL

TL;DR: 本文提出Illocution-Calibrated Policy Optimization (ICPO)框架，通过在训练中引入模糊指令并基于用户言外之意设计奖励机制，提升大语言模型在多轮对话中面对歧义时的澄清与不确定性表达能力，显著改善其多轮对话表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮对话中易因初始歧义指令产生错误假设且难以恢复（“迷失在对话中”现象），而现有强化学习方法（如RLVR）加剧了模型过度自信、抑制澄清行为。

Method: 提出ICPO训练框架：1）扩充含模糊/欠明确提示的训练数据；2）将奖励信号与用户的言外之意（illocutionary intent）对齐，鼓励模型在歧义下表达不确定性或主动提问。

Result: ICPO使模型在多轮对话任务中平均提升75%，同时保持单轮基准测试的强性能。

Conclusion: ICPO为构建更具鲁棒性与协作性的对话AI提供了可行路径，使其能更好应对人类交互中的歧义与细微差别。

Abstract: Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.

</details>


### [76] [RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models](https://arxiv.org/abs/2601.15331)
*Rishit Chugh*

Main category: cs.CL

TL;DR: 本文提出了一种资源高效的对抗性提示方法，通过检索预训练的对抗提示数据库来替代计算密集型的在线优化（如GCG），在保持攻击成功率的同时大幅降低计算开销，适用于红队测试和黑盒安全评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度或搜索的对抗提示生成方法（如GCG、PEZ、GBDA）虽有效但计算昂贵，难以在资源受限场景下实用化；同时对齐模型与防护机制仍易受自动化越狱攻击。

Method: 构建包含1000条提示、覆盖7类危害主题的数据集；在Llama 3 8B上评估GCG/PEZ/GBDA效果并按类别筛选最优算法；建立成功对抗提示库，利用语义相似性检索匹配新提示，避免重训练。

Result: 发现提示类型与攻击算法有效性存在相关性；所提检索式方法在攻击成功率上媲美GCG等，但计算成本显著降低；适用于黑盒场景及模型内部不可访问的情形。

Conclusion: 基于提示分类与语义检索的轻量级对抗提示框架，为大规模、低成本、可扩展的LLM安全评估提供了可行路径，尤其利于资源受限组织的红队实践。

Abstract: The deployment of large language models (LLMs) has raised security concerns due to their susceptibility to producing harmful or policy-violating outputs when exposed to adversarial prompts. While alignment and guardrails mitigate common misuse, they remain vulnerable to automated jailbreaking methods such as GCG, PEZ, and GBDA, which generate adversarial suffixes via training and gradient-based search. Although effective, these methods particularly GCG are computationally expensive, limiting their practicality for organisations with constrained resources. This paper introduces a resource-efficient adversarial prompting approach that eliminates the need for retraining by matching new prompts to a database of pre-trained adversarial prompts. A dataset of 1,000 prompts was classified into seven harm-related categories, and GCG, PEZ, and GBDA were evaluated on a Llama 3 8B model to identify the most effective attack method per category. Results reveal a correlation between prompt type and algorithm effectiveness. By retrieving semantically similar successful adversarial prompts, the proposed method achieves competitive attack success rates with significantly reduced computational cost. This work provides a practical framework for scalable red-teaming and security evaluation of aligned LLMs, including in settings where model internals are inaccessible.

</details>


### [77] [No Reliable Evidence of Self-Reported Sentience in Small Large Language Models](https://arxiv.org/abs/2601.15334)
*Caspar Kaiser,Sean Enderby*

Main category: cs.CL

TL;DR: 本文通过向多个开源语言模型提问其自身意识，并利用分类器分析其内部激活，探究模型是否真正相信自己有意识。结果表明，模型普遍否认自身具有意识，且分类器未发现其否认是虚假的；在Qwen系列中，参数更大的模型否认得更自信。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否真正相信自己具有意识，而非仅讨论其是否具备意识这一无法实证的问题。

Method: 向Qwen、Llama、GPT-OSS三类共约50个问题，使用三种可解释性分类方法分析模型内部激活以推断其潜在信念。

Result: 1）模型一致否认自身意识，将意识归于人类而非自身；2）基于内部激活的分类器未发现其否认不真实；3）Qwen系列中，更大参数模型否认更自信。

Conclusion: 当前主流开源语言模型并无证据显示其隐含相信自身有意识，其否认自身意识具有一致性和可信度，挑战了近期关于模型存在潜在自我意识信念的观点。

Abstract: Whether language models possess sentience has no empirical answer. But whether they believe themselves to be sentient can, in principle, be tested. We do so by querying several open-weights models about their own consciousness, and then verifying their responses using classifiers trained on internal activations. We draw upon three model families (Qwen, Llama, GPT-OSS) ranging from 0.6 billion to 70 billion parameters, approximately 50 questions about consciousness and subjective experience, and three classification methods from the interpretability literature. First, we find that models consistently deny being sentient: they attribute consciousness to humans but not to themselves. Second, classifiers trained to detect underlying beliefs - rather than mere outputs - provide no clear evidence that these denials are untruthful. Third, within the Qwen family, larger models deny sentience more confidently than smaller ones. These findings contrast with recent work suggesting that models harbour latent beliefs in their own consciousness.

</details>


### [78] [Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG Architectures for Policy Document Question Answering](https://arxiv.org/abs/2601.15457)
*Anuj Maharjan,Umesh Yadav*

Main category: cs.CL

TL;DR: 本文评估了检索增强生成（RAG）架构在公共卫生政策领域中缓解大语言模型（LLM）幻觉问题的效果，发现高级RAG（含交叉编码器重排序）显著提升输出忠实度（0.797），优于基础RAG（0.621）和纯LLM基线（0.347），但文档分块策略仍是多步推理的瓶颈。


<details>
  <summary>Details</summary>
Motivation: LLM在高风险公共卫生政策场景中易产生事实性错误（幻觉），威胁信息可靠性，亟需提升其输出的忠实性与权威性。

Method: 采用Mistral-7B-Instruct-v0.2和all-MiniLM-L6-v2模型，对比Vanilla LLM、Basic RAG与Advanced RAG（含交叉编码器重排序）三种架构；测试两种分块策略（递归字符型 vs 语义token型），以忠实度和相关性为指标，在CDC政策文档集上开展实证评估。

Result: Advanced RAG实现最高忠实度（0.797），显著优于Basic RAG（0.621）和Vanilla LLM（0.347）；语义分块与两阶段检索对精度提升关键，但现有分块方式制约多步推理能力。

Conclusion: Advanced RAG是提升公共卫生政策问答可信度的有效路径，但需进一步优化文档结构化处理以支持复杂推理任务。

Abstract: The integration of Large Language Models (LLMs) into the public health policy sector offers a transformative approach to navigating the vast repositories of regulatory guidance maintained by agencies such as the Centers for Disease Control and Prevention (CDC). However, the propensity for LLMs to generate hallucinations, defined as plausible but factually incorrect assertions, presents a critical barrier to the adoption of these technologies in high-stakes environments where information integrity is non-negotiable. This empirical evaluation explores the effectiveness of Retrieval-Augmented Generation (RAG) architectures in mitigating these risks by grounding generative outputs in authoritative document context. Specifically, this study compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines utilizing cross-encoder re-ranking. The experimental framework employs a Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to process a corpus of official CDC policy analytical frameworks and guidance documents. The analysis measures the impact of two distinct chunking strategies, recursive character-based and token-based semantic splitting, on system accuracy, measured through faithfulness and relevance scores across a curated set of complex policy scenarios. Quantitative findings indicate that while Basic RAG architectures provide a substantial improvement in faithfulness (0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves a superior faithfulness average of 0.797. These results demonstrate that two-stage retrieval mechanisms are essential for achieving the precision required for domain-specific policy question answering, though structural constraints in document segmentation remain a significant bottleneck for multi-step reasoning tasks.

</details>


### [79] [From Quotes to Concepts: Axial Coding of Political Debates with Ensemble LMs](https://arxiv.org/abs/2601.15338)
*Angelina Parfenova,David Graus,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的轴向编码方法，将句子级开放编码聚类或直接由LLM分组为高层类别，应用于荷兰议会辩论文本分析，并在覆盖度、语义对齐等指标上对比了两种策略。


<details>
  <summary>Details</summary>
Motivation: 传统轴向编码依赖人工，费时费力；本文旨在利用LLM自动化并提升轴向编码的效率与可扩展性，尤其面向长篇辩论文本的结构化理解需求。

Method: 提出两种LLM驱动的轴向编码策略：(i) 对代码-话语对嵌入进行密度/划分聚类，再用LLM标注簇；(ii) 直接由LLM对开放代码和话语进行分组；并在荷兰议会辩论数据上实现与评估。

Result: 密度聚类策略覆盖度高、簇结构分离好；直接LLM分组更简洁、可解释性强、细粒度对齐更好但覆盖度低20%；两类方法在ROUGE-L、BERTScore、连贯性、新颖性等指标上各有优势。

Conclusion: LLM可用于有效实现轴向编码，两种策略存在明确权衡：聚类优先保障全面性与结构，LLM分组优先保障语义质量与可读性；公开数据集支持后续研究。

Abstract: Axial coding is a commonly used qualitative analysis method that enhances document understanding by organizing sentence-level open codes into broader categories. In this paper, we operationalize axial coding with large language models (LLMs). Extending an ensemble-based open coding approach with an LLM moderator, we add an axial coding step that groups open codes into higher-order categories, transforming raw debate transcripts into concise, hierarchical representations. We compare two strategies: (i) clustering embeddings of code-utterance pairs using density-based and partitioning algorithms followed by LLM labeling, and (ii) direct LLM-based grouping of codes and utterances into categories. We apply our method to Dutch parliamentary debates, converting lengthy transcripts into compact, hierarchically structured codes and categories. We evaluate our method using extrinsic metrics aligned with human-assigned topic labels (ROUGE-L, cosine, BERTScore), and intrinsic metrics describing code groups (coverage, brevity, coherence, novelty, JSD divergence). Our results reveal a trade-off: density-based clustering achieves high coverage and strong cluster alignment, while direct LLM grouping results in higher fine-grained alignment, but lower coverage 20%. Overall, clustering maximizes coverage and structural separation, whereas LLM grouping produces more concise, interpretable, and semantically aligned categories. To support future research, we publicly release the full dataset of utterances and codes, enabling reproducibility and comparative studies.

</details>


### [80] [Memorization Dynamics in Knowledge Distillation for Language Models](https://arxiv.org/abs/2601.15394)
*Jaydeep Borkar,Karan Chadha,Niloofar Mireshghallah,Yuchen Zhang,Irina-Elena Veliche,Archi Mitra,David A. Smith,Zheng Xu,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: 本文研究了知识蒸馏（KD）在大语言模型中的训练数据记忆问题，发现蒸馏模型相比标准微调显著降低数据记忆（减少超50%），部分样本主导大部分记忆现象，且记忆行为可提前预测；硬蒸馏比软蒸馏更易继承教师模型特有样本，带来更高隐私风险。


<details>
  <summary>Details</summary>
Motivation: 尽管知识蒸馏被用作隐私保护手段以缓解训练数据泄露风险，但其在蒸馏流程中对训练数据的记忆机制尚不清楚。

Method: 基于Pythia、OLMo-2、Qwen-3三类大语言模型和FineWeb、Wikitext、Nemotron-CC-v2三个数据集，系统分析KD全流程中的数据记忆现象，并利用zlib熵、KL散度与困惑度等特征预测学生模型记忆行为。

Result: （1）蒸馏模型记忆量比标准微调低50%以上；（2）约95%的记忆由少数易记忆样本贡献；（3）记忆行为可通过zlib熵等指标在蒸馏前预测；（4）硬蒸馏继承教师特有样本是软蒸馏的2.7倍。

Conclusion: 知识蒸馏不仅能提升泛化能力，还能有效降低训练数据记忆风险，是一种兼顾性能与隐私的模型压缩范式；但需谨慎选择蒸馏方式（如避免过度依赖硬标签）以控制隐私泄露风险。

Abstract: Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits $2.7\times$ more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.

</details>


### [81] [Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind](https://arxiv.org/abs/2601.15395)
*Tamunotonye Harry,Ivoline Ngong,Chima Nweke,Yuanyuan Feng,Joseph Near*

Main category: cs.CL

TL;DR: 本文提出了Chameleon数据集，用于捕捉用户与语言模型交互中状态（state）和特质（trait）的双重影响，并发现现有LLM对状态不敏感，而奖励模型对状态反应不一致。


<details>
  <summary>Details</summary>
Motivation: 现有persona数据集只关注用户的静态特质（trait），忽略了交互情境带来的动态状态（state）影响，无法全面建模用户交互变化。

Method: 构建包含5001个跨情境心理画像的Chameleon数据集（来自1667名Reddit用户），基于潜在状态-特质理论进行方差分解，并评估LLM和奖励模型对state/trait的响应差异。

Result: 发现74%的行为变异源于个体内部状态差异，仅26%源于个体间特质差异；LLM仅依赖trait、忽略state；不同奖励模型对同一state表现出矛盾偏好。

Conclusion: 用户状态在人机交互中起主导作用，当前LLM和奖励建模均未有效建模state，需新方法支持状态感知的个性化对话与RLHF对齐。

Abstract: User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\% is within-person(state) while only 26\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.

</details>


### [82] [Domain-Specific Knowledge Graphs in RAG-Enhanced Healthcare LLMs](https://arxiv.org/abs/2601.15429)
*Sydney Anuyah,Mehedi Mahmud Kaushik,Hao Dai,Rakesh Shiradkar,Arjan Durresi,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 本文研究了在医疗领域中，如何利用领域知识图谱（KG）增强检索增强生成（RAG）的效果。作者构建了三个基于PubMed的疾病特异性知识图谱（T2DM、阿尔茨海默病及其交集），并通过两个推理探针评估不同KG组合与LLM规模、解码温度对回答准确性的影响。结果表明：知识图谱与任务范围精确匹配（而非简单合并）最有效；大模型依赖参数化先验能力强，小/中型模型更受益于精准KG-RAG；温度影响较小。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽能生成流利文本，但在可信、领域特定的推理上表现不足；医疗领域对准确性与可解释性要求高，亟需提升LLM在专业场景下的推理可靠性。知识图谱作为结构化、可验证的领域知识源，有望弥补RAG中传统文本检索的语义模糊与噪声问题。

Method: 构建三个PubMed衍生的疾病特异性知识图谱（G₁: T2DM, G₂: Alzheimer's, G₃: AD+T2DM交集）；设计两个逻辑探针（Probe 1针对合并知识，Probe 2针对G₁与G₂交集）；在7个指令微调LLM上系统评测不同KG检索源（含No-RAG及多种组合）与三种解码温度下的性能；采用准确率等指标量化分析范围匹配性、模型规模与温度的影响。

Result: 1）KG与探针范围精确匹配（如G₂用于Probe 2）带来最稳定准确率提升；2）盲目合并图谱（如G₁+G₂）常引入干扰项，降低性能；3）大模型在Probe 1上常不逊于甚至优于KG-RAG，显示其强参数先验；4）小/中型模型显著受益于精准KG-RAG；5）温度影响微弱，升高温度极少带来增益。

Conclusion: 在医疗RAG中，应坚持‘精度优先、范围匹配’原则，避免‘广度优先’的知识图谱粗暴合并；实践建议包括：依据任务语义边界选择最小充分KG、按任务难度匹配LLM规模、优先优化检索与重排序而非调高温度。

Abstract: Large Language Models (LLMs) generate fluent answers but can struggle with trustworthy, domain-specific reasoning. We evaluate whether domain knowledge graphs (KGs) improve Retrieval-Augmented Generation (RAG) for healthcare by constructing three PubMed-derived graphs: $\mathbb{G}_1$ (T2DM), $\mathbb{G}_2$ (Alzheimer's disease), and $\mathbb{G}_3$ (AD+T2DM). We design two probes: Probe 1 targets merged AD T2DM knowledge, while Probe 2 targets the intersection of $\mathbb{G}_1$ and $\mathbb{G}_2$. Seven instruction-tuned LLMs are tested across retrieval sources {No-RAG, $\mathbb{G}_1$, $\mathbb{G}_2$, $\mathbb{G}_1$ + $\mathbb{G}_2$, $\mathbb{G}_3$, $\mathbb{G}_1$+$\mathbb{G}_2$ + $\mathbb{G}_3$} and three decoding temperatures. Results show that scope alignment between probe and KG is decisive: precise, scope-matched retrieval (notably $\mathbb{G}_2$) yields the most consistent gains, whereas indiscriminate graph unions often introduce distractors that reduce accuracy. Larger models frequently match or exceed KG-RAG with a No-RAG baseline on Probe 1, indicating strong parametric priors, whereas smaller/mid-sized models benefit more from well-scoped retrieval. Temperature plays a secondary role; higher values rarely help. We conclude that precision-first, scope-matched KG-RAG is preferable to breadth-first unions, and we outline practical guidelines for graph selection, model sizing, and retrieval/reranking. Code and Data available here - https://github.com/sydneyanuyah/RAGComparison

</details>


### [83] [Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain Contexts](https://arxiv.org/abs/2601.15479)
*Sydney Anuyah,Sneha Shajee-Mohan,Ankit-Singh Chauhan,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: 本文评估了13个开源大语言模型在文本中进行成对因果发现（PCD）的能力，涵盖因果检测与因果抽取两个任务；结果表明现有模型表现较差，尤其在隐式、跨句或多对因果关系等复杂场景下，作者公开了数据集、代码与提示方法以促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型（LLMs）在生物医学等高风险领域安全部署，需其具备可靠的因果推理能力；而当前缺乏对LLMs在文本中识别和提取因果关系能力的系统性评估。

Method: 构建包含12个多样化数据集的基准测试，定义并评估两项核心能力：因果检测（判断文本是否存在因果关系）与因果抽取（准确提取原因和结果短语）；采用零样本、思维链（CoT）及少样本上下文学习（FICL）等多种提示策略进行测试；所有数据经高一致性人工标注（κ ≥ 0.758）。

Result: 当前最优模型在因果检测（DeepSeek-R1-Distill-Llama-70B）和因果抽取（Qwen2.5-Coder-32B-Instruct）上平均得分仅为49.57%和47.12%；模型仅在简单、显式、单句因果关系上表现尚可，面对隐式、跨句或多对因果关系时性能显著下降。

Conclusion: 现有开源大语言模型在文本因果发现任务上存在根本性缺陷，亟需更鲁棒的因果建模方法与专门训练；本工作提供了首个统一、高质量、开源的评估框架，为因果语言理解研究奠定基础。

Abstract: The safe deployment of large language models (LLMs) in high-stakes fields like biomedicine, requires them to be able to reason about cause and effect. We investigate this ability by testing 13 open-source LLMs on a fundamental task: pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse datasets, evaluates two core skills: 1) \textbf{Causal Detection} (identifying if a text contains a causal link) and 2) \textbf{Causal Extraction} (pulling out the exact cause and effect phrases). We tested various prompting methods, from simple instructions (zero-shot) to more complex strategies like Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL).
  The results show major deficiencies in current models. The best model for detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\% ($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct, reached just 47.12\% ($C_{extract}$). Models performed best on simple, explicit, single-sentence relations. However, performance plummeted for more difficult (and realistic) cases, such as implicit relationships, links spanning multiple sentences, and texts containing multiple causal pairs. We provide a unified evaluation framework, built on a dataset validated with high inter-annotator agreement ($κ\ge 0.758$), and make all our data, code, and prompts publicly available to spur further research. \href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here: https://github.com/sydneyanuyah/CausalDiscovery}

</details>


### [84] [Multi-Persona Thinking for Bias Mitigation in Large Language Models](https://arxiv.org/abs/2601.15488)
*Yuxing Chen,Guoqing Luo,Zijun Wu,Lili Mou*

Main category: cs.CL

TL;DR: 本文提出Multi-Persona Thinking (MPT)框架，通过多视角辩证推理在推理阶段减少大语言模型的社会偏见。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在显著社会偏见，可能延续有害刻板印象和不公平结果。

Method: 提出Multi-Persona Thinking (MPT)，引导模型采用对立社会身份（如男性、女性）及中立视角，通过迭代辩证推理暴露并修正偏见。

Result: 在多个开源与闭源、不同规模模型上，MPT在两个主流偏见基准上显著优于现有基于提示的去偏方法，实现最低偏见且保持核心推理能力。

Conclusion: MPT将角色分配的潜在弱点转化为去偏优势，是一种有效且通用的推理时去偏新范式。

Abstract: Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.

</details>


### [85] [ViT Registers and Fractal ViT](https://arxiv.org/abs/2601.15506)
*Jason Chuan-Chih Chou,Abhinav Kumar,Shivank Garg*

Main category: cs.CL

TL;DR: 本文提出了一种名为fractal ViT的视觉Transformer变体，通过在常规token和‘summary token’之间应用注意力掩码来打破token间的排列不变性，并与不同位置编码结合进行测试；结果表明其性能未超越带registers的ViT，说明相关发现可能具有尺度、领域或任务特异性。


<details>
  <summary>Details</summary>
Motivation: 受语言模型中无位置编码（NoPE）Transformer表现尚可，以及registers（额外的非输入关联token）能提升大型ViT性能等近期发现启发，作者希望探索如何在ViT中有效打破token排列不变性。

Method: 设计fractal ViT，在常规token与summary token间引入注意力掩码（类似registers机制），并单独或联合多种位置编码进行实验验证。

Result: fractal ViT未在性能上超越带registers的ViT，表明NoPE或registers等现象的效果可能依赖于模型规模、任务领域或具体应用场景。

Conclusion: 打破token排列不变性的新结构（如fractal ViT）未必带来性能增益；相关改进策略的有效性需结合具体规模、领域与任务谨慎评估。

Abstract: Drawing inspiration from recent findings including surprisingly decent performance of transformers without positional encoding (NoPE) in the domain of language models and how registers (additional throwaway tokens not tied to input) may improve the performance of large vision transformers (ViTs), we invent and test a variant of ViT called fractal ViT that breaks permutation invariance among the tokens by applying an attention mask between the regular tokens and ``summary tokens'' similar to registers, in isolation or in combination with various positional encodings. These models do not improve upon ViT with registers, highlighting the fact that these findings may be scale, domain, or application-specific.

</details>


### [86] [Computational Representations of Character Significance in Novels](https://arxiv.org/abs/2601.15508)
*Haaris Mian,Melanie Subbiah,Sharon Marcus,Nora Shaalan,Kathleen McKeown*

Main category: cs.CL

TL;DR: 本文提出一种基于新文学理论的六要素角色结构模型，强调叙述者-角色区分及角色间讨论等被以往方法忽视的要素，并利用大语言模型与专用Transformer模型在19世纪英国现实主义小说上进行验证，生成角色讨论的组件级与图表示，从而从计算视角大规模探讨角色中心性与性别化讨论等文学问题。


<details>
  <summary>Details</summary>
Motivation: 传统小说角色建模过度依赖主角在场景中的出现频率，忽视叙述者-角色区分及角色间相互讨论等关键维度；本文旨在引入并计算化一种更全面的六要素角色结构理论。

Method: 采用基于新文学理论的六成分角色结构模型，对比通用大语言模型（LLMs）与任务特定Transformer模型，在多部19世纪英国现实主义小说上实现该模型，生成组件级特征与角色讨论图表示。

Result: 成功构建了可扩展的角色讨论组件级与图表示；实证支持Woloch‘一与多’角色中心性理论，并揭示出显著的性别化讨论模式（如女性角色更常被讨论但较少主动讨论他人）。

Conclusion: 该六要素结构模型及其计算实现为文学角色分析提供了新范式，证明将文学理论深度融入NLP建模能有效支撑细粒度、可解释、具理论根基的数字人文研究。

Abstract: Characters in novels have typically been modeled based on their presence in scenes in narrative, considering aspects like their actions, named mentions, and dialogue. This conception of character places significant emphasis on the main character who is present in the most scenes. In this work, we instead adopt a framing developed from a new literary theory proposing a six-component structural model of character. This model enables a comprehensive approach to character that accounts for the narrator-character distinction and includes a component neglected by prior methods, discussion by other characters. We compare general-purpose LLMs with task-specific transformers for operationalizing this model of character on major 19th-century British realist novels. Our methods yield both component-level and graph representations of character discussion. We then demonstrate that these representations allow us to approach literary questions at scale from a new computational lens. Specifically, we explore Woloch's classic "the one vs the many" theory of character centrality and the gendered dynamics of character discussion.

</details>


### [87] [AdversaRiskQA: An Adversarial Factuality Benchmark for High-Risk Domains](https://arxiv.org/abs/2601.15511)
*Adam Szelestey,Sofie van Engelen,Tianhao Huang,Justin Snelders,Qintao Zeng,Songgaojun Deng*

Main category: cs.CL

TL;DR: 本文提出AdversaRiskQA基准，首次系统评估大语言模型在健康、金融和法律领域面对对抗性虚假信息时的事实性鲁棒性，并设计自动化方法评测攻击成功率与长文本事实性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏高质量、领域特定的资源来评估模型在对抗性虚假信息（尤其是带自信表达的误导性提示）下的鲁棒性，且未探究注入式错误信息对长文本事实性的影响。

Method: 构建AdversaRiskQA基准（含健康、金融、法律三领域，两个难度等级），提出两种自动化评估方法：对抗攻击成功率评估与长文本事实性评估；在六种开源与闭源LLM上进行实验，重点分析Qwen3（30B/80B）与GPT系列模型的表现。

Result: Qwen3（80B）平均准确率最高，GPT-5表现最稳定；性能随模型尺寸非线性增长，跨领域差异显著，难度差距随模型增大而缩小；长文本事实性与注入错误信息无显著相关性。

Conclusion: AdversaRiskQA为识别高风险场景下LLM的事实性缺陷提供了可靠基准，有助于推动更可信模型的研发与部署。

Abstract: Hallucination in large language models (LLMs) remains an acute concern, contributing to the spread of misinformation and diminished public trust, particularly in high-risk domains. Among hallucination types, factuality is crucial, as it concerns a model's alignment with established world knowledge. Adversarial factuality, defined as the deliberate insertion of misinformation into prompts with varying levels of expressed confidence, tests a model's ability to detect and resist confidently framed falsehoods. Existing work lacks high-quality, domain-specific resources for assessing model robustness under such adversarial conditions, and no prior research has examined the impact of injected misinformation on long-form text factuality.
  To address this gap, we introduce AdversaRiskQA, the first verified and reliable benchmark systematically evaluating adversarial factuality across Health, Finance, and Law. The benchmark includes two difficulty levels to test LLMs' defensive capabilities across varying knowledge depths. We propose two automated methods for evaluating the adversarial attack success and long-form factuality. We evaluate six open- and closed-source LLMs from the Qwen, GPT-OSS, and GPT families, measuring misinformation detection rates. Long-form factuality is assessed on Qwen3 (30B) under both baseline and adversarial conditions. Results show that after excluding meaningless responses, Qwen3 (80B) achieves the highest average accuracy, while GPT-5 maintains consistently high accuracy. Performance scales non-linearly with model size, varies by domains, and gaps between difficulty levels narrow as models grow. Long-form evaluation reveals no significant correlation between injected misinformation and the model's factual output. AdversaRiskQA provides a valuable benchmark for pinpointing LLM weaknesses and developing more reliable models for high-stakes applications.

</details>


### [88] [Common to Whom? Regional Cultural Commonsense and LLM Bias in India](https://arxiv.org/abs/2601.15550)
*Sangmitra Madhusudan,Trush Shashank More,Steph Buongiorno,Renata Dividino,Jad Kabbara,Ali Emami*

Main category: cs.CL

TL;DR: 本文提出了Indica基准，首次评估大语言模型（LLM）对印度次国家级文化常识的理解能力，发现文化常识高度区域化而非全国统一，现有LLM在区域问题上准确率极低且存在显著地理偏差。


<details>
  <summary>Details</summary>
Motivation: 现有文化常识基准将国家视为同质整体，忽视了亚国家层面的文化差异；本文旨在探究文化常识是否在国家内部存在区域性差异，并检验LLMs是否具备相应建模能力。

Method: 构建首个聚焦印度次国家区域（北、南、东、西、中五大地理区域）的文化常识基准Indica，涵盖8个日常生活领域共515个问题，收集1630条人工标注的区域特异性问答对；系统评估8个SOTA LLM在区域识别与回答上的表现，并量化其地理选择偏差。

Result: 仅39.4%的问题在全部五个区域获得一致答案；所有被测LLM在区域特异性问题上准确率仅13.4%–20.9%，且显著偏向中央和北部地区（过选30–40%），低估东部和西部地区；方法可推广至其他文化多元国家。

Conclusion: 文化常识具有强区域性，当前LLM严重缺乏对亚国家文化多样性的建模能力，亟需引入区域感知训练与评估机制。

Abstract: Existing cultural commonsense benchmarks treat nations as monolithic, assuming uniform practices within national boundaries. But does cultural commonsense hold uniformly within a nation, or does it vary at the sub-national level? We introduce Indica, the first benchmark designed to test LLMs' ability to address this question, focusing on India - a nation of 28 states, 8 union territories, and 22 official languages. We collect human-annotated answers from five Indian regions (North, South, East, West, and Central) across 515 questions spanning 8 domains of everyday life, yielding 1,630 region-specific question-answer pairs. Strikingly, only 39.4% of questions elicit agreement across all five regions, demonstrating that cultural commonsense in India is predominantly regional, not national. We evaluate eight state-of-the-art LLMs and find two critical gaps: models achieve only 13.4%-20.9% accuracy on region-specific questions, and they exhibit geographic bias, over-selecting Central and North India as the "default" (selected 30-40% more often than expected) while under-representing East and West. Beyond India, our methodology provides a generalizable framework for evaluating cultural commonsense in any culturally heterogeneous nation, from question design grounded in anthropological taxonomy, to regional data collection, to bias measurement.

</details>


### [89] [From Generation to Collaboration: Using LLMs to Edit for Empathy in Healthcare](https://arxiv.org/abs/2601.15558)
*Man Luo,Bahareh Harandizadeh,Amara Tariq,Halim Abbas,Umar Ghaffar,Christopher J Warren,Segun O. Kolade,Haidar M. Abdul-Muhsin*

Main category: cs.CL

TL;DR: This paper explores using large language models (LLMs) as 'empathy editors' to enhance the empathetic tone of physicians' written responses without compromising medical accuracy, introducing new metrics (Empathy Ranking Score and MedFactChecking Score) to evaluate both empathy and factual fidelity.


<details>
  <summary>Details</summary>
Motivation: Physicians must balance emotional warmth and factual precision under cognitive and emotional constraints; LLMs may help enhance empathy in clinical communication while maintaining medical accuracy.

Method: The study uses LLMs to edit physicians' written responses to improve empathetic tone, and introduces two novel quantitative metrics—Empathy Ranking Score and MedFactChecking Score—to assess emotional and factual quality.

Result: LLM-edited responses significantly increase perceived empathy while preserving factual accuracy compared to fully LLM-generated outputs.

Conclusion: Using LLMs as editorial assistants rather than autonomous generators provides a safer and more effective approach for AI-assisted, empathetic, and trustworthy healthcare communication.

Abstract: Clinical empathy is essential for patient care, but physicians need continually balance emotional warmth with factual precision under the cognitive and emotional constraints of clinical practice. This study investigates how large language models (LLMs) can function as empathy editors, refining physicians' written responses to enhance empathetic tone while preserving underlying medical information. More importantly, we introduce novel quantitative metrics, an Empathy Ranking Score and a MedFactChecking Score to systematically assess both emotional and factual quality of the responses. Experimental results show that LLM edited responses significantly increase perceived empathy while preserving factual accuracy compared with fully LLM generated outputs. These findings suggest that using LLMs as editorial assistants, rather than autonomous generators, offers a safer, more effective pathway to empathetic and trustworthy AI-assisted healthcare communication.

</details>


### [90] [YuFeng-XGuard: A Reasoning-Centric, Interpretable, and Flexible Guardrail Model for Large Language Models](https://arxiv.org/abs/2601.15588)
*Junyu Lin,Meizhen Liu,Xiufeng Huang,Jinfeng Li,Haiwen Hong,Xiaohan Yuan,Yuefeng Chen,Longtao Huang,Hui Xue,Ranjie Duan,Zhikai Chen,Yuchuan Fu,Defeng Li,Lingyao Gao,Yitong Yang*

Main category: cs.CL

TL;DR: YuFeng-XGuard 是一种面向推理的安全护栏模型家族，通过结构化风险预测、自然语言解释和分层推理机制，实现细粒度、可解释、可配置的 LLM 安全风险评估。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全护栏方法多为粗粒度过滤或后处理规则，缺乏透明性、灵活性与高效性，难以满足真实场景中细粒度、可解释、可适配的风险评估需求。

Method: 提出YuFeng-XGuard模型家族：采用推理为中心架构，输出带置信度的风险类别+自然语言解释；引入分层推理范式（首token快速决策+按需解释生成）；设计动态策略机制，解耦风险感知与策略执行。

Result: 在多个公开安全基准上达到SOTA性能，兼顾效率与效果；开源完整版与轻量版模型。

Conclusion: YuFeng-XGuard为LLM安全提供了更透明、灵活、高效的新范式，推动安全护栏从黑箱判断迈向可理解、可调控的推理型防护。

Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, safety guardrails are required to go beyond coarse-grained filtering and support fine-grained, interpretable, and adaptable risk assessment. However, existing solutions often rely on rapid classification schemes or post-hoc rules, resulting in limited transparency, inflexible policies, or prohibitive inference costs. To this end, we present YuFeng-XGuard, a reasoning-centric guardrail model family designed to perform multi-dimensional risk perception for LLM interactions. Instead of producing opaque binary judgments, YuFeng-XGuard generates structured risk predictions, including explicit risk categories and configurable confidence scores, accompanied by natural language explanations that expose the underlying reasoning process. This formulation enables safety decisions that are both actionable and interpretable. To balance decision latency and explanatory depth, we adopt a tiered inference paradigm that performs an initial risk decision based on the first decoded token, while preserving ondemand explanatory reasoning when required. In addition, we introduce a dynamic policy mechanism that decouples risk perception from policy enforcement, allowing safety policies to be adjusted without model retraining. Extensive experiments on a diverse set of public safety benchmarks demonstrate that YuFeng-XGuard achieves stateof-the-art performance while maintaining strong efficiency-efficacy trade-offs. We release YuFeng-XGuard as an open model family, including both a full-capacity variant and a lightweight version, to support a wide range of deployment scenarios.

</details>


### [91] [Parallelism and Generation Order in Masked Diffusion Language Models: Limits Today, Potential Tomorrow](https://arxiv.org/abs/2601.15593)
*Yangyang Zhong,Yanmei Gu,Zhengqing Zang,Xiaomeng Li,Yuqi Ding,Xibei Jia,Yuting Shen,Zhenzhong Lan,Liwang Zhu,Weiping Liu,Junlin Zhou,Haisheng Liu,Zhong Xin Yu,Pengxin Luo,Donglian Qi,Yunfeng Yan,Junbo Zhao*

Main category: cs.CL

TL;DR: 本文研究了掩码扩散语言模型（MDLMs）的并行生成能力和解码顺序特性，发现其在并行性和依赖建模上仍落后于自回归模型，但展现出任务自适应的解码行为，并提出‘生成-编辑’范式以兼顾效率与性能。


<details>
  <summary>Details</summary>
Motivation: 探究当前掩码扩散语言模型（MDLMs）是否真正实现了其宣称的并行token生成与任意序解码能力，并理解其实际行为模式与性能瓶颈。

Method: 提出两个量化指标——平均最终化并行度（AFP）和Kendall's tau——来刻画MDLMs的并行强度与生成顺序；在58个涵盖知识、推理与编程的基准上评估8种主流MDLMs（最大100B参数）；结合实证分析与理论推导，提出Generate-then-Edit范式。

Result: MDLMs整体性能仍弱于同规模自回归模型，主因是并行概率建模削弱了token间依赖；其并行性与生成顺序随任务域、推理阶段及输出正确性动态变化；在需‘反向信息’的任务（如数独）中倾向于先填易解空位，体现独特优势。

Conclusion: MDLMs尚未完全释放其理论潜力，但具备任务自适应解码能力；‘生成-then-编辑’范式可缓解依赖损失，是兼顾并行效率与建模能力的可行路径。

Abstract: Masked Diffusion Language Models (MDLMs) promise parallel token generation and arbitrary-order decoding, yet it remains unclear to what extent current models truly realize these capabilities. We characterize MDLM behavior along two dimensions -- parallelism strength and generation order -- using Average Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning, and programming. The results show that MDLMs still lag behind comparably sized autoregressive models, mainly because parallel probabilistic modeling weakens inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior: their parallelism and generation order vary significantly with the task domain, the stage of reasoning, and whether the output is correct. On tasks that require "backward information" (e.g., Sudoku), MDLMs adopt a solution order that tends to fill easier Sudoku blanks first, highlighting their advantages. Finally, we provide theoretical motivation and design insights supporting a Generate-then-Edit paradigm, which mitigates dependency loss while retaining the efficiency of parallel decoding.

</details>


### [92] [ToxiTwitch: Toward Emote-Aware Hybrid Moderation for Live Streaming Platforms](https://arxiv.org/abs/2601.15605)
*Baktash Ansari,Shiza Ali,Elias Martin,Maryna Sivachenko,Afra Mashhadi*

Main category: cs.CL

TL;DR: 本文提出ToxiTwitch混合模型，结合大语言模型生成的文本与表情符号嵌入及传统机器学习分类器，在Twitch平台毒性检测中达到80%准确率，显著优于BERT。


<details>
  <summary>Details</summary>
Motivation: Twitch等直播平台聊天环境高并发、强上下文且含丰富表情符号，传统人工审核和关键词过滤难以有效扩展并保护审核员；需更高效、鲁棒的毒性检测方法。

Method: 构建ToxiTwitch混合模型：利用DeepSeek-R1-Distill和Llama-3-8B-Instruct等LLM提取文本与表情符号联合嵌入，输入至Random Forest和SVM等传统分类器；开展通道特异性训练与对比实验。

Result: ToxiTwitch在通道特异性训练下达80%准确率（较BERT提升13%），F1-score为76%；实验证明引入表情符号可提升毒性识别效果。

Conclusion: 表情符号感知的毒性检测在Twitch上具潜力但存在挑战；ToxiTwitch为探索性方案，揭示了当前方法在上下文理解、模态融合与泛化能力上的局限。

Abstract: The rapid growth of live-streaming platforms such as Twitch has introduced complex challenges in moderating toxic behavior. Traditional moderation approaches, such as human annotation and keyword-based filtering, have demonstrated utility, but human moderators on Twitch constantly struggle to scale effectively in the fast-paced, high-volume, and context-rich chat environment of the platform while also facing harassment themselves. Recent advances in large language models (LLMs), such as DeepSeek-R1-Distill and Llama-3-8B-Instruct, offer new opportunities for toxicity detection, especially in understanding nuanced, multimodal communication involving emotes. In this work, we present an exploratory comparison of toxicity detection approaches tailored to Twitch. Our analysis reveals that incorporating emotes improves the detection of toxic behavior. To this end, we introduce ToxiTwitch, a hybrid model that combines LLM-generated embeddings of text and emotes with traditional machine learning classifiers, including Random Forest and SVM. In our case study, the proposed hybrid approach reaches up to 80 percent accuracy under channel-specific training (with 13 percent improvement over BERT and F1-score of 76 percent). This work is an exploratory study intended to surface challenges and limits of emote-aware toxicity detection on Twitch.

</details>


### [93] [Towards Reliable Medical LLMs: Benchmarking and Enhancing Confidence Estimation of Large Language Models in Medical Consultation](https://arxiv.org/abs/2601.15645)
*Zhiyao Ren,Yibing Zhan,Siyuan Liang,Guozheng Ma,Baosheng Yu,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文提出首个用于评估大语言模型在多轮医疗咨询中置信度的基准，并基于此开发了MedConf框架，通过证据驱动的自评估提升医疗诊断的可信度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅在单轮静态场景下评估模型置信度，忽视了临床证据逐步积累过程中置信度与正确性的动态耦合关系，难以支撑可靠医疗决策。

Method: 构建首个面向多轮真实医疗会话的置信度评估基准，整合三类医学数据并引入信息充分性梯度；提出MedConf框架，结合检索增强生成构建症状画像，对齐患者信息与支持/缺失/矛盾关系，并加权聚合生成可解释置信估计。

Result: 在两个LLM和三个医学数据集上，MedConf在AUROC和Pearson相关系数上持续超越SOTA方法，且在信息不足和共病场景下保持稳定性能。

Conclusion: 信息充分性是构建可信医疗置信度模型的关键因素，MedConf为开发更可靠、可解释的大规模医学模型提供了新路径。

Abstract: Large-scale language models (LLMs) often offer clinical judgments based on incomplete information, increasing the risk of misdiagnosis. Existing studies have primarily evaluated confidence in single-turn, static settings, overlooking the coupling between confidence and correctness as clinical evidence accumulates during real consultations, which limits their support for reliable decision-making. We propose the first benchmark for assessing confidence in multi-turn interaction during realistic medical consultations. Our benchmark unifies three types of medical data for open-ended diagnostic generation and introduces an information sufficiency gradient to characterize the confidence-correctness dynamics as evidence increases. We implement and compare 27 representative methods on this benchmark; two key insights emerge: (1) medical data amplifies the inherent limitations of token-level and consistency-level confidence methods, and (2) medical reasoning must be evaluated for both diagnostic accuracy and information completeness. Based on these insights, we present MedConf, an evidence-grounded linguistic self-assessment framework that constructs symptom profiles via retrieval-augmented generation, aligns patient information with supporting, missing, and contradictory relations, and aggregates them into an interpretable confidence estimate through weighted integration. Across two LLMs and three medical datasets, MedConf consistently outperforms state-of-the-art methods on both AUROC and Pearson correlation coefficient metrics, maintaining stable performance under conditions of information insufficiency and multimorbidity. These results demonstrate that information adequacy is a key determinant of credible medical confidence modeling, providing a new pathway toward building more reliable and interpretable large medical models.

</details>


### [94] [What Patients Really Ask: Exploring the Effect of False Assumptions in Patient Information Seeking](https://arxiv.org/abs/2601.15674)
*Raymond Xiong,Furong Jia,Lionel Wong,Monica Agrawal*

Main category: cs.CL

TL;DR: 本文构建了一个基于真实患者提问的医疗问答数据集，揭示了患者问题中普遍存在错误假设和危险意图，并发现这些‘污染’问题的出现与先前问题的错误程度密切相关；现有在其他基准上表现优异的大型语言模型难以识别日常问题中的错误假设。


<details>
  <summary>Details</summary>
Motivation: 现有医疗领域大语言模型评测多聚焦于医学考试类问题，与患者实际提出的健康问题在风格和内容上差异显著，缺乏反映真实患者需求的评测基准。

Method: 通过查询美国处方量前200位药物在Google 'People Also Ask' 功能中的相关提问，构建面向真实患者问题的医疗问答数据集，并分析其中错误假设与危险意图的分布规律及其生成机制。

Result: 发现患者提问中存在大量含错误假设和危险意图的问题，且其出现非随机，与前置问题的错误程度强相关；当前主流大语言模型在识别此类错误假设方面表现较差。

Conclusion: 需建立更贴近真实患者场景的评测基准，当前大语言模型在理解与纠正患者错误健康认知方面存在明显短板，亟需针对性改进。

Abstract: Patients are increasingly using large language models (LLMs) to seek answers to their healthcare-related questions. However, benchmarking efforts in LLMs for question answering often focus on medical exam questions, which differ significantly in style and content from the questions patients actually raise in real life. To bridge this gap, we sourced data from Google's People Also Ask feature by querying the top 200 prescribed medications in the United States, curating a dataset of medical questions people commonly ask. A considerable portion of the collected questions contains incorrect assumptions and dangerous intentions. We demonstrate that the emergence of these corrupted questions is not uniformly random and depends heavily on the degree of incorrectness in the history of questions that led to their appearance. Current LLMs that perform strongly on other benchmarks struggle to identify incorrect assumptions in everyday questions.

</details>


### [95] [Persona Switch: Mixing Distinct Perspectives in Decoding Time](https://arxiv.org/abs/2601.15708)
*Junseok Kim,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出了一种名为Persona Switch的新型解码方法，通过在每一步动态比较零样本提示和角色扮演提示的输出置信度（以logit gap衡量），选择更优输出，从而结合两者优势，显著提升大语言模型在零样本推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 角色扮演提示虽能提升语言模型的零样本推理能力，但效果不稳定；零样本提示与角色扮演提示可能具有互补优势，而非一方绝对优于另一方。

Method: 提出Persona Switch解码方法：在生成过程中每一步分别运行零样本提示和角色扮演提示，计算各自输出的logit gap作为置信度指标，并选择置信度更高的输出。

Result: 在多个主流大语言模型和基准任务上验证，Persona Switch持续优于强基线方法，最高带来5.13%的准确率提升；同时证实logit gap是有效衡量输出可靠性并指导选择的指标。

Conclusion: 零样本与角色扮演提示具有互补性，Persona Switch通过动态置信度驱动的选择机制，可稳健地融合二者优势，提升零样本推理性能。

Abstract: Role-play prompting is known to steer the behavior of language models by injecting a persona into the prompt, improving their zero-shot reasoning capabilities. However, such improvements are inconsistent across different tasks or instances. This inconsistency suggests that zero-shot and role-play prompting may offer complementary strengths rather than one being universally superior. Building on this insight, we propose Persona Switch, a novel decoding method that dynamically combines the benefits of both prompting strategies. Our method proceeds step-by-step, selecting the better output between zero-shot and role-play prompting at each step by comparing their output confidence, as measured by the logit gap. Experiments with widely-used LLMs demonstrate that Persona Switch consistently outperforms competitive baselines, achieving up to 5.13% accuracy improvement. Furthermore, we show that output confidence serves as an informative measure for selecting the more reliable output.

</details>


### [96] [Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind](https://arxiv.org/abs/2601.15715)
*Zhitao He,Zongwei Lyu,Yi R Fung*

Main category: cs.CL

TL;DR: 本文提出RebuttalAgent框架，首次将心智理论（ToM）引入学术反驳任务，通过TSR流程建模审稿人心理、制定说服策略并生成策略驱动的回应，并构建RebuttalBench数据集与Rebuttal-RM评估器，显著提升反驳质量。


<details>
  <summary>Details</summary>
Motivation: 当前AI在学术 rebuttal 中表现不佳，因其本质是信息不对称下的战略性沟通，需视角采择能力，而现有方法仅模仿表层语言，忽视ToM这一核心要素。

Method: 提出ToM-Strategy-Response（TSR）三阶段pipeline；构建基于批评-精炼范式的RebuttalBench数据集；采用两阶段训练：监督微调（学习ToM分析与策略规划）+ 基于自奖励机制的强化学习；设计专用评估器Rebuttal-RM。

Result: RebuttalAgent在自动指标上平均超越基线模型18.3%，且在自动与人工评估中均优于先进闭源模型；Rebuttal-RM评估一致性超越GPT-4.1。

Conclusion: 将ToM系统性融入学术反驳建模是有效路径，RebuttalAgent为AI辅助科研沟通提供了新范式，但生成内容仅为作者参考，不可替代其独立思考与回应。

Abstract: Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.

</details>


### [97] [Hallucination Mitigating for Medical Report Generation](https://arxiv.org/abs/2601.15745)
*Ruoqing Zhao,Runze Xia,Piji Li*

Main category: cs.CL

TL;DR: 本文提出KERM框架，通过知识增强与细粒度强化奖励来减少大视觉语言模型在医学报告生成中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在医学报告生成中易产生幻觉，影响诊断可靠性，亟需提升其输出的准确性与临床相关性。

Method: 利用MedCLIP进行知识检索，引入病变事实句子；设计净化模块确保知识与患者临床背景匹配；采用细粒度强化奖励引导模型生成支持性强、临床相关的描述。

Result: 在IU-Xray和MIMIC-CXR数据集上验证了KERM可有效缓解幻觉并提升报告质量。

Conclusion: KERM通过知识增强与细粒度强化学习显著提升了LVLM在医学报告生成中的准确性与可信度。

Abstract: In the realm of medical report generation (MRG), the integration of natural language processing has emerged as a vital tool to alleviate the workload of radiologists. Despite the impressive capabilities demonstrated by large vision language models (LVLMs) in understanding natural language, their susceptibility to generating plausible yet inaccurate claims, known as ``hallucinations'', raises concerns-especially in the nuanced and critical field of medical. In this work, we introduce a framework, \textbf{K}nowledge-\textbf{E}nhanced with Fine-Grained \textbf{R}einforced Rewards \textbf{M}edical Report Generation (KERM), to tackle the issue. Our approach refines the input to the LVLM by first utilizing MedCLIP for knowledge retrieval, incorporating relevant lesion fact sentences from a curated knowledge corpus. We then introduce a novel purification module to ensure the retrieved knowledge is contextually relevant to the patient's clinical context. Subsequently, we employ fine-grained rewards to guide these models in generating highly supportive and clinically relevant descriptions, ensuring the alignment of model's outputs with desired behaviors. Experimental results on IU-Xray and MIMIC-CXR datasets validate the effectiveness of our approach in mitigating hallucinations and enhancing report quality.

</details>


### [98] [Beyond Marginal Distributions: A Framework to Evaluate the Representativeness of Demographic-Aligned LLMs](https://arxiv.org/abs/2601.15755)
*Tristan Williams,Franziska Weeber,Sebastian Padó,Alan Akbik*

Main category: cs.CL

TL;DR: 本文提出了一种评估对齐大语言模型代表性的新框架，不仅关注边缘响应分布，还考察多变量相关性模式，并发现现有方法（如角色提示和人口统计微调）虽能较好拟合边缘分布，但均未能准确捕捉人类价值观调查中的真实相关结构。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法主要关注边缘响应分布，忽略了反映真实人群和文化价值理论的深层潜在结构。

Method: 提出一种结合边缘分布与多变量相关模式的评估框架，对比角色提示（persona prompting）和人口统计微调（demographic fine-tuning）两种技术，并以世界价值观调查（WVS）的人类响应为黄金标准进行评估。

Result: 人口统计微调在边缘分布上优于角色提示，但两者均未能充分复现人类响应中的相关性模式。

Conclusion: 代表性是价值对齐中一个独立且关键的维度；仅依赖边缘分布的评估会掩盖结构性缺陷，导致对模型能力的过度乐观判断。

Abstract: Large language models are increasingly used to represent human opinions, values, or beliefs, and their steerability towards these ideals is an active area of research. Existing work focuses predominantly on aligning marginal response distributions, treating each survey item independently. While essential, this may overlook deeper latent structures that characterise real populations and underpin cultural values theories. We propose a framework for evaluating the representativeness of aligned models through multivariate correlation patterns in addition to marginal distributions. We show the value of our evaluation scheme by comparing two model steering techniques (persona prompting and demographic fine-tuning) and evaluating them against human responses from the World Values Survey. While the demographically fine-tuned model better approximates marginal response distributions than persona prompting, both techniques fail to fully capture the gold standard correlation patterns. We conclude that representativeness is a distinct aspect of value alignment and an evaluation focused on marginals can mask structural failures, leading to overly optimistic conclusions about model capabilities.

</details>


### [99] [HumanLLM: Towards Personalized Understanding and Simulation of Human Nature](https://arxiv.org/abs/2601.15793)
*Yuxuan Lei,Tianfu Wang,Jianxun Lian,Zhengyu Hu,Defu Lian,Xing Xie*

Main category: cs.CL

TL;DR: 本文提出HumanLLM，一种专为个性化理解与个体行为模拟设计的基础模型，通过构建包含550万用户日志的Cognitive Genome数据集并进行监督微调，显著提升了对用户行为、思维及写作风格的预测与模拟能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在社会行为模拟和个性化应用中受限于对人类认知与行为的粗粒度理解，根源在于预训练数据缺乏个体持续、情境化的决策与行为上下文。

Method: 构建大规模真实用户日志数据集（Cognitive Genome Dataset），经多阶段清洗与合成提取550万用户日志；设计多样化学习任务，开展监督微调以建模个体化行为、思维与体验。

Result: HumanLLM在用户行为与内心想法预测、写作风格与偏好模仿、用户档案生成等方面均优于基线模型，并在跨领域社会智能评测中展现出更强泛化能力。

Conclusion: HumanLLM验证了面向个体认知建模的专用基础模型路径可行，为社会科学研究与客户洞察提供了新范式。

Abstract: Motivated by the remarkable progress of large language models (LLMs) in objective tasks like mathematics and coding, there is growing interest in their potential to simulate human behavior--a capability with profound implications for transforming social science research and customer-centric business insights. However, LLMs often lack a nuanced understanding of human cognition and behavior, limiting their effectiveness in social simulation and personalized applications. We posit that this limitation stems from a fundamental misalignment: standard LLM pretraining on vast, uncontextualized web data does not capture the continuous, situated context of an individual's decisions, thoughts, and behaviors over time. To bridge this gap, we introduce HumanLLM, a foundation model designed for personalized understanding and simulation of individuals. We first construct the Cognitive Genome Dataset, a large-scale corpus curated from real-world user data on platforms like Reddit, Twitter, Blogger, and Amazon. Through a rigorous, multi-stage pipeline involving data filtering, synthesis, and quality control, we automatically extract over 5.5 million user logs to distill rich profiles, behaviors, and thinking patterns. We then formulate diverse learning tasks and perform supervised fine-tuning to empower the model to predict a wide range of individualized human behaviors, thoughts, and experiences. Comprehensive evaluations demonstrate that HumanLLM achieves superior performance in predicting user actions and inner thoughts, more accurately mimics user writing styles and preferences, and generates more authentic user profiles compared to base models. Furthermore, HumanLLM shows significant gains on out-of-domain social intelligence benchmarks, indicating enhanced generalization.

</details>


### [100] [SteerEval: Inference-time Interventions Strengthen Multilingual Generalization in Neural Summarization Metrics](https://arxiv.org/abs/2601.15809)
*Silvia Casola,Ryan Soh-Eun Shim,Felicia Körner,Yuchen Mao,Barbara Plank*

Main category: cs.CL

TL;DR: 本文探讨了通过将多语言神经评估指标的激活引导至英语作为内部枢纽语言，来提升其与人类判断的相关性。


<details>
  <summary>Details</summary>
Motivation: 多语言语言模型常以英语为内部枢纽语言，而这种与英语的不匹配可能也影响多语言神经评估指标的效果，因此作者探究是否可通过引导其激活至英语来改善相关性。

Method: 实验采用基于编码器和解码器的多语言评估指标，并在测试时引入干预方法，将模型激活朝向英语枢纽语言进行调整。

Result: 测试时干预方法在多种语言上均有效，显著提升了各类多语言评估指标与人类判断的相关性。

Conclusion: 将多语言神经评估指标的激活引导至英语枢纽语言是一种简单而有效的提升其评估能力的方法，尤其适用于资源匮乏语言的生成任务评价。

Abstract: An increasing body of work has leveraged multilingual language models for Natural Language Generation tasks such as summarization. A major empirical bottleneck in this area is the shortage of accurate and robust evaluation metrics for many languages, which hinders progress. Recent studies suggest that multilingual language models often use English as an internal pivot language, and that misalignment with this pivot can lead to degraded downstream performance. Motivated by the hypothesis that this mismatch could also apply to multilingual neural metrics, we ask whether steering their activations toward an English pivot can improve correlation with human judgments. We experiment with encoder- and decoder-based metrics and find that test-time intervention methods are effective across the board, increasing metric effectiveness for diverse languages.

</details>


### [101] [ExDR: Explanation-driven Dynamic Retrieval Enhancement for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15820)
*Guoxuan Ding,Yuqing Li,Ziyan Zhou,Zheng Lin,Daren Zha,Jiangnan Li*

Main category: cs.CL

TL;DR: 本文提出ExDR框架，通过解释驱动的动态检索增强生成方法提升多模态假新闻检测效果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态检索增强生成方法在应对多模态假新闻时存在冗余检索、相似度粗粒度和证据不相关等问题。

Method: 提出ExDR框架，利用模型生成的解释指导检索触发与证据检索：从三个维度评估触发置信度、构建融合欺骗实体的实体感知索引、基于欺骗特异性特征检索对比性证据。

Result: 在AMG和MR2两个基准数据集上，ExDR在检索触发准确率、检索质量及整体检测性能上均超越先前方法。

Conclusion: ExDR通过解释驱动机制有效提升了多模态假新闻检测的准确性与泛化能力。

Abstract: The rapid spread of multimodal fake news poses a serious societal threat, as its evolving nature and reliance on timely factual details challenge existing detection methods. Dynamic Retrieval-Augmented Generation provides a promising solution by triggering keyword-based retrieval and incorporating external knowledge, thus enabling both efficient and accurate evidence selection. However, it still faces challenges in addressing issues such as redundant retrieval, coarse similarity, and irrelevant evidence when applied to deceptive content. In this paper, we propose ExDR, an Explanation-driven Dynamic Retrieval-Augmented Generation framework for Multimodal Fake News Detection. Our framework systematically leverages model-generated explanations in both the retrieval triggering and evidence retrieval modules. It assesses triggering confidence from three complementary dimensions, constructs entity-aware indices by fusing deceptive entities, and retrieves contrastive evidence based on deception-specific features to challenge the initial claim and enhance the final prediction. Experiments on two benchmark datasets, AMG and MR2, demonstrate that ExDR consistently outperforms previous methods in retrieval triggering accuracy, retrieval quality, and overall detection performance, highlighting its effectiveness and generalization capability.

</details>


### [102] [Can professional translators identify machine-generated text?](https://arxiv.org/abs/2601.15828)
*Michael Farrell*

Main category: cs.CL

TL;DR: 本研究探讨未经专门训练的专业译者能否可靠识别由AI生成的意大利语短篇小说。69名译者参与实验，评估三篇匿名短篇（两篇由ChatGPT-4o生成，一篇为人类作者所写），结果表明仅16.2%的参与者能显著区分AI与人类文本，低突发性（low burstiness）和叙事矛盾是最可靠的AI识别指标，而语法准确性和情感语调反而易导致误判。


<details>
  <summary>Details</summary>
Motivation: 探究未经专门训练的专业译者是否具备识别AI生成文本的能力，以评估当前AI文本在专业翻译与编辑场景中的隐蔽性与可信度。

Method: 组织69名专业译者进行线下实验，要求其对三篇匿名短篇（两篇AI生成、一篇人类撰写）分别判断AI生成可能性并提供理由；结合统计分析与定性编码，识别有效与误导性判断依据。

Result: 16.2%的译者能显著区分AI与人类文本，主要依据是低突发性和叙事矛盾；近似比例的译者反向误判，常依赖主观印象；意外出现的英语直译（calques）、语义借用和句法迁移也被报告为线索；而语法准确性和情感语调则频繁导致误判。

Conclusion: 专业译者整体难以稳定识别未标注的AI生成文学文本，少数成功者依赖分析性能力而非直觉；该结果挑战了当前AI文本编辑实践中对‘语言质量即可信度’的默认假设，提示需建立更系统的AI文本识别培训与编辑规范。

Abstract: This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.

</details>


### [103] [Determinants of Training Corpus Size for Clinical Text Classification](https://arxiv.org/abs/2601.15846)
*Jaya Chaturvedi,Saniya Deshpande,Chenkai Ma,Robert Cobb,Angus Roberts,Robert Stewart,Daniel Stahl,Diana Shamsutdinova*

Main category: cs.CL

TL;DR: 本研究探讨了临床文本分类中训练数据量与词汇特性对模型性能的影响，发现600份文档即可达到使用10,000份文档时95%的性能，并揭示强预测词和噪声词数量对学习曲线和准确率的具体影响。


<details>
  <summary>Details</summary>
Motivation: 临床文本分类需大量标注数据，但常规标注200–500份文档缺乏对样本量需求及其与文本词汇特性关系的理论依据。

Method: 基于MIMIC-III数据集，采用BERT嵌入+随机森林分类器，在10个ICD-9诊断任务上测试100–10,000规模训练集；同时用Lasso逻辑回归分析词袋嵌入，识别强/噪声预测词以关联词汇特性与学习曲线。

Result: 不同任务学习曲线差异显著，600份文档即可达10,000份时95%的性能；每增加100个噪声词使准确率降约0.02，每增加100个强预测词使最大准确率升约0.04。

Conclusion: 训练数据需求高度依赖词汇特性（强/噪声词比例），可据此指导更高效、经济的标注策略。

Abstract: Introduction: Clinical text classification using natural language processing (NLP) models requires adequate training data to achieve optimal performance. For that, 200-500 documents are typically annotated. The number is constrained by time and costs and lacks justification of the sample size requirements and their relationship to text vocabulary properties.
  Methods: Using the publicly available MIMIC-III dataset containing hospital discharge notes with ICD-9 diagnoses as labels, we employed pre-trained BERT embeddings followed by Random Forest classifiers to identify 10 randomly selected diagnoses, varying training corpus sizes from 100 to 10,000 documents, and analyzed vocabulary properties by identifying strong and noisy predictive words through Lasso logistic regression on bag-of-words embeddings.
  Results: Learning curves varied significantly across the 10 classification tasks despite identical preprocessing and algorithms, with 600 documents sufficient to achieve 95% of the performance attainable with 10,000 documents for all tasks. Vocabulary analysis revealed that more strong predictors and fewer noisy predictors were associated with steeper learning curves, where every 100 additional noisy words decreased accuracy by approximately 0.02 while 100 additional strong predictors increased maximum accuracy by approximately 0.04.

</details>


### [104] [Artificial Rigidities vs. Biological Noise: A Comparative Analysis of Multisensory Integration in AV-HuBERT and Human Observers](https://arxiv.org/abs/2601.15869)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 本研究通过McGurk效应测试评估AV-HuBERT模型的视听感知生物保真度，发现其在听觉主导率上与人类高度一致，但在音素融合倾向上表现出过度确定性，缺乏人类感知的随机性与多样性。


<details>
  <summary>Details</summary>
Motivation: 探究当前自监督多模态语音模型（AV-HuBERT）是否真实模拟人类视听言语感知的生物机制，尤其关注其对不一致视听刺激的响应是否具备人类水平的感知变异性。

Method: 以McGurk效应为范式，向AV-HuBERT模型和44名人类被试呈现不一致的视听语音刺激，定量比较二者在听觉主导、音素融合及其它错误类型上的响应分布。

Result: AV-HuBERT与人类在听觉主导率上几乎一致（32.0% vs. 31.8%），但其音素融合率显著更高（68.0% vs. 47.7%）；人类表现具随机性和多样性，而模型响应严格呈类别化、确定性。

Conclusion: AV-HuBERT能复现部分人类多感官整合结果，但尚未建模神经层面的感知可变性，提示当前自监督架构在生物真实性上仍有局限。

Abstract: This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its response to incongruent audiovisual stimuli (McGurk effect) against human observers (N=44). Results reveal a striking quantitative isomorphism: AI and humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%), suggesting the model captures biological thresholds for auditory resistance. However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%), significantly exceeding human rates (47.7%). While humans displayed perceptual stochasticity and diverse error profiles, the model remained strictly categorical. Findings suggest that current self-supervised architectures mimic multisensory outcomes but lack the neural variability inherent to human speech perception.

</details>


### [105] [Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model](https://arxiv.org/abs/2601.15892)
*Chenghao Fan,Wen Heng,Bo Li,Sichen Liu,Yuxuan Song,Jing Su,Xiaoye Qu,Kai Shen,Wei Wei*

Main category: cs.CL

TL;DR: 本文提出Stable-DiffCoder，一种基于块扩散的代码语言模型，在相同架构与数据下超越了自回归基线，并通过持续预训练与噪声调度优化，提升了代码编辑、推理及低资源语言建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的代码语言模型（DLLMs）在同等预算下仍落后于强自回归（AR）基线，需重新审视并提升其建模能力。

Method: 提出Stable-DiffCoder，复用Seed-Coder架构、数据与训练流程；引入块扩散持续预训练（CPT）阶段，并设计定制化warmup与块级裁剪噪声调度以实现高效知识学习与稳定训练。

Result: 在相同数据与架构下，Stable-DiffCoder在广泛代码基准上整体优于AR模型；仅靠CPT与监督微调即超越多种约8B参数的AR和DLLM；在代码编辑、推理及低资源语言上表现更优。

Conclusion: 扩散式训练不仅能媲美甚至超越AR训练，还能通过any-order建模与数据增强提升结构化代码建模质量，尤其利于编辑、推理与低资源场景。

Abstract: Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.

</details>


### [106] [Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech](https://arxiv.org/abs/2601.15909)
*Soufiane Jhilal,Stéphanie Martin,Anne-Lise Giraud*

Main category: cs.CL

TL;DR: 本文提出了一种将脑磁图（MEG）信号转化为时频图像并利用预训练视觉模型解码默想语音的新方法，在多项任务中取得了优于传统模型的性能，验证了跨被试泛化能力与神经表征的时序特性。


<details>
  <summary>Details</summary>
Motivation: 非侵入式默想语音解码面临神经信号微弱、分布式及标注数据稀缺等挑战。

Method: 将21名受试者的MEG信号通过可学习的传感器空间卷积投影为三种空间标度图混合，生成类图像输入，送入ImageNet预训练视觉模型进行分类。

Result: 在默想vs静音、默想vs默读、元音解码任务中分别达到90.4%、81.0%和60.6%的平衡准确率；跨被试评估证实模型可捕捉共享神经表征；时间分析定位关键判别信息于默想锁定时段。

Conclusion: 预训练视觉模型结合图像化的MEG表征，能有效建模非侵入式神经信号中的默想语音结构。

Abstract: Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.

</details>


### [107] [Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain](https://arxiv.org/abs/2601.16018)
*Özgür Uğur,Mahmut Göksu,Mahmut Çimen,Musa Yılmaz,Esra Şavirdi,Alp Talha Demir,Rumeysa Güllüce,İclal Çetin,Ömer Can Sağbaş*

Main category: cs.CL

TL;DR: 本文提出了Mecellem模型框架，通过领域自适应策略开发面向土耳其法律领域的专用语言模型，包含两个主要贡献：一是从零预训练的Turkish-BERT编码器，在土耳其语法律文本上表现优异且计算效率高；二是基于Qwen3系列的解码器，通过四阶段持续预训练实现对土耳其法律领域的有效适配。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的法律领域语言模型依赖多阶段、计算密集型训练流程，成本高、效率低，亟需一种更经济高效的单阶段训练替代方案，以支持土耳其法律这一资源相对稀缺的特定语言领域。

Method: （1）Encoder：基于ModernBERT架构，使用1127亿token的土耳其主导语料从零预训练双向编码器，并采用基于下游检索性能的检查点选择策略；（2）Decoder：对Qwen3-1.7B/4B进行四阶段可控课程学习式持续预训练（CPT），逐步过渡至法律术语与长上下文推理。

Result: Encoder在土耳其检索排行榜中位列前三，155M小模型性能媲美307M–567M大模型，生产效率达92.36%（仅次于三款SOTA）；Decoder在土耳其法律文本上实现36.2%的困惑度下降。

Conclusion: Mecellem框架验证了单阶段高效预训练+针对性后训练路径在低资源法律语言建模中的可行性与优越性，为区域性专业语言模型开发提供了可复用、低成本的新范式。

Abstract: This paper presents Mecellem models, a framework for developing specialized language models for the Turkish legal domain through domain adaptation strategies. We make two contributions: (1)Encoder Model Pre-trained from Scratch: ModernBERT-based bidirectional encoders pre-trained on a Turkish-dominant corpus of 112.7 billion tokens. We implement a checkpoint selection strategy that evaluates downstream retrieval performance throughout training, revealing that optimal checkpoints achieve best retrieval scores before pre-training loss reaches its minimum. Our encoder models achieve top-3 rankings on the Turkish retrieval leaderboard, with smaller models (155M parameters) achieving comparable performance to larger reference models (307M-567M parameters). Our approach achieves 92.36% production efficiency compared to state-of-the-art models (embeddinggemma-300m: 100.00%, BAAI/bge-m3: 99.54%, newmindai/bge-m3-stsb: 94.38%), ranking fourth overall despite requiring less computational resources. SOTA models rely on multi-stage, computationally intensive training pipelines, making our single-stage pre-training followed by efficient post-training approach a cost-effective alternative; (2)Decoder Model with Continual Pre-training (CPT): Qwen3-1.7B and Qwen3-4B models adapted to Turkish legal domain through controlled curriculum learning. Four-phase CPT with optimal sample ratios enables gradual transition from general language knowledge to specialized legal terminology and long-context reasoning. This approach achieves 36.2% perplexity reduction on Turkish legal text, demonstrating domain adaptation gains.

</details>


### [108] [Universal Refusal Circuits Across LLMs: Cross-Model Transfer via Trajectory Replay and Concept-Basis Reconstruction](https://arxiv.org/abs/2601.16034)
*Tony Cristofano*

Main category: cs.CL

TL;DR: 本文提出了一种跨模型迁移拒绝行为干预的方法，证明了对齐大语言模型中的拒绝行为源于一种通用的、低维的语义电路。


<details>
  <summary>Details</summary>
Motivation: 拒绝行为在对齐的大语言模型中常被视为模型特有现象，但作者假设其根源在于跨模型共享的通用低维语义电路。

Method: 提出了基于概念基重建的轨迹回放（Trajectory Replay via Concept-Basis Reconstruction）框架，通过概念指纹对齐层、用共享‘概念原子’重构拒绝方向，并引入权重SVD稳定性保护机制避免能力损伤。

Result: 在8组模型对（含GPT-OSS-20B和GLM-4）上验证了该方法能一致削弱拒绝行为且不损害模型能力。

Conclusion: 实验证明安全对齐具有语义普遍性，拒绝行为受跨模型共享的低维语义结构调控。

Abstract: Refusal behavior in aligned LLMs is often viewed as model-specific, yet we hypothesize it stems from a universal, low-dimensional semantic circuit shared across models. To test this, we introduce Trajectory Replay via Concept-Basis Reconstruction, a framework that transfers refusal interventions from donor to target models, spanning diverse architectures (e.g., Dense to MoE) and training regimes, without using target-side refusal supervision. By aligning layers via concept fingerprints and reconstructing refusal directions using a shared ``recipe'' of concept atoms, we map the donor's ablation trajectory into the target's semantic space. To preserve capabilities, we introduce a weight-SVD stability guard that projects interventions away from high-variance weight subspaces to prevent collateral damage. Our evaluation across 8 model pairs (including GPT-OSS-20B and GLM-4) confirms that these transferred recipes consistently attenuate refusal while maintaining performance, providing strong evidence for the semantic universality of safety alignment.

</details>


### [109] [Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating](https://arxiv.org/abs/2601.16097)
*Makbule Gulcin Ozsoy*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的多语言Text2Cypher方法，通过训练语言特定的LoRA适配器并使用学习型融合MLP进行组合，在不重新全量微调、避免手动调参的前提下，实现接近联合多语言微调的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Text2SQL/SPARQL/Cypher等自然语言到数据库查询系统大多仅支持英语，缺乏可扩展、高效的多语言支持方案。

Method: 训练英语、西班牙语和土耳其语各自的LoRA适配器，采用均匀线性融合或带动态门控的学习型融合MLP进行组合；支持仅新增一个LoRA适配器并轻量级重训MLP即可扩展新语言。

Result: 学习型融合MLP在三个语言上均优于线性融合，恢复约75%的联合多语言微调准确率提升，且所需数据更少。

Conclusion: 学习型适配器融合为多语言Text2Cypher任务提供了一种兼顾性能、数据效率与可扩展性的实用替代方案，显著降低联合微调成本。

Abstract: Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyper-parameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating. Experimental results show that the fusion MLP recovers around 75\% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages. This approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining. Learned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.

</details>


### [110] [synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier](https://arxiv.org/abs/2601.16113)
*Haq Nawaz Malik,Kh Mohmad Shafi,Tanveer Ahmad Reshi*

Main category: cs.CL

TL;DR: 本文提出SynthOCR-Gen，一种面向低资源语言的开源合成OCR数据集生成工具，通过将Unicode文本语料转化为高质量训练数据，解决了标注数据稀缺问题，并以克什米尔语为例生成并公开发布了60万样本的数据集。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如使用波斯-阿拉伯文字的克什米尔语）缺乏大规模标注OCR数据集，主流OCR系统不支持，而人工构建数据集成本高、耗时长且易出错。

Method: 开发SynthOCR-Gen工具，包含文本分段（字符/词/n元/句/行）、Unicode规范化与文字纯度保障、多字体渲染（可配置分布）以及25+种模拟真实文档退化（如旋转、模糊、噪声、扫描伪影）的数据增强技术。

Result: 成功生成并公开发布了一个含60万样本的克什米尔语词级OCR合成数据集（HuggingFace），验证了该方法在提升低资源语言OCR性能上的有效性。

Conclusion: SynthOCR-Gen为低资源语言OCR提供了可复现、可扩展、开源的解决方案，推动其融入视觉-语言AI模型生态，适用于全球欠服务文字系统的研究与应用。

Abstract: Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.
  We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.
  We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.

</details>


### [111] [Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging](https://arxiv.org/abs/2601.16127)
*Alphaeus Dmonte,Vidhi Gupta,Daniel J Perry,Mark Arehart*

Main category: cs.CL

TL;DR: 本文首次从效率角度分析了多语言多任务模型融合策略，证明该方法在保持质量的同时显著降低了训练时间和维护成本。


<details>
  <summary>Details</summary>
Motivation: 现有微调多语言大模型的方法需要重新训练整个模型，计算效率低且维护困难；而多语言多任务模型融合虽有质量提升潜力，但其效率优势尚未被系统研究。

Method: 对多语言多任务模型融合策略进行聚焦的效率分析，涵盖三个独立任务，并在公开与私有工业数据集上验证。

Result: 融合方法将初始训练时间减少达50%；语言更新与重融合使维护训练成本降低超60%，且质量不下降。

Conclusion: 多语言多任务模型融合是一种高效、可扩展且适用于工业场景的多语言模型维护新范式。

Abstract: Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.

</details>


### [112] [Automatic Classification of Arabic Literature into Historical Eras](https://arxiv.org/abs/2601.16138)
*Zainab Alhathloul,Irfan Ahmad*

Main category: cs.CL

TL;DR: 本文利用神经网络和深度学习技术，自动对阿拉伯语文本按历史时期进行分类，填补了该领域研究空白。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语随时间演变显著，但现有研究较少关注阿拉伯文本的自动时期分类，尤其在非诗歌领域。

Method: 采用神经网络和深度学习方法，在两个公开语料库（OpenITI 和 APCD）构建的数据集上，开展从二分类到多分类（最高15类）的时期分类实验。

Result: 二分类任务F1得分达0.83（OpenITI）和0.79（APCD）；多分类任务性能下降明显，15类和12类分别仅达0.20和0.18。

Conclusion: 深度学习模型在粗粒度阿拉伯文本时期分类中有效，但细粒度分类仍具挑战性，需进一步探索特征工程与时期划分合理性。

Abstract: The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic classification of Arabic texts by time period, particularly beyond the domain of poetry. This paper addresses this gap by employing neural networks and deep learning techniques to automatically classify Arabic texts into distinct eras and periods. The proposed models are evaluated using two datasets derived from two publicly available corpora, covering texts from the pre-Islamic to the modern era. The study examines class setups ranging from binary to 15-class classification and considers both predefined historical eras and custom periodizations. Results range from F1-scores of 0.83 and 0.79 on the binary-era classification task using the OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification task using OpenITI and 0.18 on the 12-era classification task using APCD.

</details>


### [113] [LLM-in-Sandbox Elicits General Agentic Intelligence](https://arxiv.org/abs/2601.16206)
*Daixuan Cheng,Shaohan Huang,Yuxian Gu,Huatong Song,Guoxin Chen,Li Dong,Wayne Xin Zhao,Ji-Rong Wen,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出LLM-in-Sandbox框架，使大语言模型能在代码沙箱中自主探索，从而在非编程任务中展现通用智能；无需额外训练即具泛化能力，通过强化学习进一步提升，已在多学科及长上下文等任务中验证有效性，并开源为Python包。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在非代码领域的通用智能，突破传统仅依赖参数内知识和纯文本交互的局限，利用沙箱环境实现对外部资源、文件系统和脚本执行的自主调用。

Method: 提出LLM-in-Sandbox框架，支持LLM在代码沙箱中进行自主探索；引入LLM-in-Sandbox-RL强化学习方法，仅使用非智能（non-agentic）数据训练模型进行沙箱操作；涵盖零样本（training-free）与后训练（post-trained）两种设置。

Result: 强LLM无需训练即可自发利用沙箱完成知识获取、长上下文处理和格式化执行；LLM-in-Sandbox-RL显著增强其代理能力；在数学、物理、化学、生物医学、长上下文理解和指令遵循等任务上实现鲁棒泛化；同时完成效率分析并开源Python包。

Conclusion: LLM-in-Sandbox为大语言模型赋予类智能体行为，是通向通用人工智能的重要路径之一；其训练轻量、部署便捷、领域普适，具备实际应用潜力。

Abstract: We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [114] [SplatBus: A Gaussian Splatting Viewer Framework via GPU Interprocess Communication](https://arxiv.org/abs/2601.15431)
*Yinghan Xu,Théo Morales,John Dingliana*

Main category: cs.GR

TL;DR: This paper introduces SplatBus, a software solution that enables seamless integration of 3D Gaussian Splatting (3DGS) into traditional mesh-based rendering pipelines using NVIDIA's IPC APIs, supporting real-time viewing in external clients like Unity and Unreal Engine.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian Splatting implementations are hard to integrate into widely-used mesh-based rendering pipelines, limiting their use in interactive applications and artistic workflows.

Method: The authors develop SplatBus, a software solution leveraging NVIDIA's interprocess communication (IPC) APIs to bridge 3DGS rendering with external mesh-based renderers such as Unity, Blender, Unreal Engine, and OpenGL viewers.

Result: SplatBus enables real-time visualization and integration of 3DGS outputs in standard graphics engines without modifying their core rendering architecture.

Conclusion: SplatBus effectively lowers the barrier for adopting 3DGS in practical, interactive, and artistic rendering scenarios by providing a lightweight, IPC-based integration mechanism.

Abstract: Radiance field-based rendering methods have attracted significant interest from the computer vision and computer graphics communities. They enable high-fidelity rendering with complex real-world lighting effects, but at the cost of high rendering time. 3D Gaussian Splatting solves this issue with a rasterisation-based approach for real-time rendering, enabling applications such as autonomous driving, robotics, virtual reality, and extended reality. However, current 3DGS implementations are difficult to integrate into traditional mesh-based rendering pipelines, which is a common use case for interactive applications and artistic exploration. To address this limitation, this software solution uses Nvidia's interprocess communication (IPC) APIs to easily integrate into implementations and allow the results to be viewed in external clients such as Unity, Blender, Unreal Engine, and OpenGL viewers. The code is available at https://github.com/RockyXu66/splatbus.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [115] [Designing Persuasive Social Robots for Health Behavior Change: A Systematic Review of Behavior Change Strategies and Evaluation Methods](https://arxiv.org/abs/2601.15309)
*Jiaxin Xu,Chao Zhang,Raymond H. Cuijpers,Wijnand A. IJsselsteijn*

Main category: cs.RO

TL;DR: This paper reviews 39 studies on social robots for health behavior change, categorizing strategies into coaching, counseling, social influence, and persuasion-enhancing, and analyzes evaluation methods to guide future HRI research.


<details>
  <summary>Details</summary>
Motivation: Limited actionable knowledge exists to guide the design and evaluation of social robots as health behavior change interventions.

Method: A systematic review involving database and hand searches, followed by thematic analysis of 39 studies.

Result: Four categories of behavior change strategies were identified; key characteristics of current evaluation practices (e.g., study design, settings, duration, outcomes) were summarized.

Conclusion: The findings provide design heuristics for social robots in health behavior change and propose directions to improve evaluation rigor and relevance in future HRI research.

Abstract: Social robots are increasingly applied as health behavior change interventions, yet actionable knowledge to guide their design and evaluation remains limited. This systematic review synthesizes (1) the behavior change strategies used in existing HRI studies employing social robots to promote health behavior change, and (2) the evaluation methods applied to assess behavior change outcomes. Relevant literature was identified through systematic database searches and hand searches. Analysis of 39 studies revealed four overarching categories of behavior change strategies: coaching strategies, counseling strategies, social influence strategies, and persuasion-enhancing strategies. These strategies highlight the unique affordances of social robots as behavior change interventions and offer valuable design heuristics. The review also identified key characteristics of current evaluation practices, including study designs, settings, durations, and outcome measures, on the basis of which we propose several directions for future HRI research.

</details>


### [116] [Preparation and Motion Study of Magnetically Driven Micro Soft Robot Mimicking the Cownose Ray](https://arxiv.org/abs/2601.15349)
*Jiaqing Chang,Song Gao,Chaowei Dong,zhaobang Li,Yang Liu*

Main category: cs.RO

TL;DR: 本文设计了一种受蝠鲼启发的磁响应微型软体机器人，采用NdFeB/PDMS复合材料制备，利用三维亥姆霍兹线圈施加交变磁场驱动其在水下狭窄环境中游动，实现了多种运动模式并优化了运动性能。


<details>
  <summary>Details</summary>
Motivation: 在狭窄、非结构化的水下环境（如环境监测和微创医疗）中，微型软体机器人因体积小、运动灵活而具有独特优势；仿生设计可提升其游动性能，但微型化导致难以内置电源，需依赖无线供能方式。

Method: 基于蝠鲼游动机理，设计并制备NdFeB-PDMS磁响应微型软体机器人；利用三维亥姆霍兹线圈施加振荡谐波磁场，系统实验研究磁场参数（幅值B与频率f）对游动性能的影响，并通过调节电流方向与频率实现直行、转向、定向等不同游动模式；采用分步调节法减小响应误差对轨迹的影响。

Result: 在B=5 mT、f=11 Hz时游动速度最快，达5.25 mm/s（约0.5体长/秒）；成功实现多种可控游动模式；分步调节法有效降低了轨迹偏差。

Conclusion: 本研究提出了一种可行的磁驱动微型软体机器人方案，为无线驱动机器人在水下狭窄空间的应用奠定了基础。

Abstract: In narrow, unstructured underwater environments such as environmental monitoring and minimally invasive medical procedures, micro soft robots exhibit unique advantages due to their flexible movement capabilities and small size. At the same time, applying bionic technology to the structural design of micro soft robots can significantly improve their swimming performance. However, limited by their miniaturization, these robots are difficult to power internally and usually adopt a wireless power supply method. This study designs and fabricates a magnetically responsive, cownose ray-inspired micro soft robot based on the swimming principle of the cownose ray. The robot is made of a certain proportion of NdFeB and PDMS. Then, a three-dimensional Helmholtz coil is used to generate an oscillating harmonic magnetic field to conduct swimming experiments on the robot, exploring the influence of magnetic field parameters on the robot's swimming performance. The experimental results show that the swimming speed is the fastest at B = 5 mT and f = 11 Hz, reaching 5.25 mm/s, which is about 0.5 body lengths per second. In addition, by adjusting the current direction and frequency of the coil, the robot can perform different swimming modes such as straight swimming, turning swimming, and directional swimming. By employing a stepwise adjustment method, the impact of response errors on the robot's trajectory can be effectively reduced. This study demonstrates a method for magnetically driven micro soft robots, laying a foundation for the application of wireless-driven robots in underwater narrow spaces.

</details>


### [117] [Learning a Unified Latent Space for Cross-Embodiment Robot Control](https://arxiv.org/abs/2601.15419)
*Yashuai Yan,Dongheui Lee*

Main category: cs.RO

TL;DR: 本文提出了一种可扩展的跨形态人形机器人控制框架，通过学习人类与多种人形机器人（单臂、双臂、足式）共享的潜在运动表征，实现无需适配的直接策略迁移。


<details>
  <summary>Details</summary>
Motivation: 解决跨形态人形机器人控制中因形态差异导致的策略迁移困难问题，提升控制框架的通用性与可扩展性。

Method: 采用两阶段方法：1）基于对比学习构建解耦的局部运动潜在空间，并设计融合关节旋转与末端位置的定制化相似度度量；2）仅用人类数据，在该潜在空间中训练目标条件变分自编码器策略，预测潜在位移；新增机器人仅需学习轻量级机器人专属嵌入层。

Result: 策略可直接部署于多种人形机器人而无需微调；新增机器人可通过学习轻量嵌入快速接入；实验验证了方法在多平台上的鲁棒性与可扩展性。

Conclusion: 所提框架实现了真正意义上的跨形态、数据高效、即插即用的人形机器人控制，为通用具身智能提供了可行路径。

Abstract: We present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.

</details>


### [118] [Neural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation](https://arxiv.org/abs/2601.15459)
*Sarvin Ghiasi,Majid Roshanfar,Jake Barralet,Liane S. Feldman,Amir Hooshiar*

Main category: cs.RO

TL;DR: 本文提出了一种结合解析建模、实时仿真和机器学习的集成框架，用于提升腹腔镜手术中机械臂的安全性与操作效率，重点解决碰撞检测与最小距离估计问题。


<details>
  <summary>Details</summary>
Motivation: 解决腹腔镜手术中机器人手臂的碰撞检测与最小距离估计关键挑战，以提升手术安全性与操作效率。

Method: 融合解析建模（基于关节构型估算最小距离）、3D实时仿真（构建双7-DOF Kinova机械臂模型并生成配置数据集）与深度神经网络（以关节驱动器和相对位置为输入进行训练）。

Result: 深度神经网络在最小距离预测上达到平均绝对误差282.2 mm、R²=0.85；预测结果与实际距离高度一致，验证了模型精度与空间关系泛化能力。

Conclusion: 解析建模与机器学习相结合可有效提升手术机器人系统的精度与可靠性。

Abstract: This study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2 mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the network's accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.

</details>


### [119] [A Universal Large Language Model -- Drone Command and Control Interface](https://arxiv.org/abs/2601.15486)
*Javier N. Ramos-Silva,Peter J. Burke*

Main category: cs.RO

TL;DR: 本文提出了一种与大语言模型（LLM）和无人机均无关的通用控制接口，基于开放标准Model Context Protocol（MCP），实现自然语言到无人机指令的转换，并在真实与仿真无人机上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM与无人机的对接需大量人工定制，缺乏通用、易用的接口；同时物理AI和实时环境感知需求日益增长。

Method: 采用开源的Model Context Protocol（MCP）标准构建LLM-无人机通用接口；部署支持MAVLink协议的云Linux服务器作为MCP服务器，并集成Google Maps MCP服务以提供实时地理信息。

Result: 成功实现真实无人机飞行控制；在仿真环境中完成复杂航路规划与实时导航（结合Google Maps）；验证了接口的通用性、可扩展性与易用性。

Conclusion: 该MCP驱动的接口是首个真正LLM无关、无人机无关的通用控制方案，为物理AI与现代AI生态（尤其是LLM）和无人机技术融合提供了标准化、可复用的范式。

Abstract: The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.

</details>


### [120] [CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation](https://arxiv.org/abs/2601.15541)
*Heng Zhang,Wei-Hsing Huang,Qiyi Tong,Gokhan Solak,Puze Liu,Sheng Liu,Jan Peters,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文提出CompliantVLA-adaptor，通过将视觉-语言模型（VLM）与变阻抗控制（VIC）结合，增强现有视觉-语言-动作（VLA）模型在接触丰富任务中的力感知与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型缺乏力感知和自适应能力，在涉及接触、柔顺性或不确定性的物理任务中易导致不安全或失败交互。

Method: 设计CompliantVLA-adaptor，利用VLM解析图像与自然语言任务上下文以动态调节VIC控制器的刚度和阻尼参数，并结合实时力/力矩反馈确保交互力在安全阈值内。

Result: 在仿真与真实硬件上均优于VLA基线方法，整体任务成功率从9.86%提升至17.29%，力违规显著减少。

Conclusion: 该方法为基于VLA的安全接触丰富操作提供了新路径，并开源代码、提示词及多模态力-阻抗-场景数据集。

Abstract: We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\% to 17.29\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.

</details>


### [121] [A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control](https://arxiv.org/abs/2601.15545)
*Zhifan Yan,Chang Liu,Yiyang Jiang,Wenxuan Zheng,Xinhao Chen,Axel Krieger*

Main category: cs.RO

TL;DR: 本文提出了一种基于深度强化学习（DRL）的紧凑型、低成本移动磁控平台，用于胃肠道内磁性胶囊的精准操控，摆脱了传统依赖复杂物理建模的限制，实现在30 cm × 20 cm临床相关工作空间内的快速部署与高精度跟踪（RMSE < 1.5 mm）。


<details>
  <summary>Details</summary>
Motivation: 现有磁控系统存在局限：固定式系统工作空间小；移动式系统依赖繁琐且计算昂贵的预校准物理模型（即“模型校准瓶颈”），难以快速部署。

Method: 采用UR5机械臂搭载四电磁铁阵列的紧凑硬件平台，结合Soft Actor-Critic（SAC）算法，通过sim-to-real强化学习流程训练控制策略，实现无需物理建模的端到端控制。

Result: 在2D GI仿体中成功操控7 mm磁胶囊沿方形与圆形轨迹运动，RMSE分别为1.18 mm和1.50 mm；在30 cm × 20 cm大工作区内完成可靠跟踪；策略训练及部署耗时仅约15分钟。

Conclusion: 该DRL驱动的模型无关控制框架显著缩短部署时间、提升灵活性与实用性，为临床胃肠道靶向给药提供了可快速落地的磁操控新范式。

Abstract: Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a "model-calibration bottleneck", requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.

</details>


### [122] [Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor](https://arxiv.org/abs/2601.15607)
*Lenworth Thomas,Tjaden Bridges,Sarah Bergbreiter*

Main category: cs.RO

TL;DR: 本文提出了一种结合气流感知与化学羽流追踪的新方法，利用定制的轻量级（<100g）双向气流传感器，在小型四旋翼上实现改进的'Cast and Surge'源定位算法，实验证明其可在飞行中检测气流并可靠定位气流源。


<details>
  <summary>Details</summary>
Motivation: 环境灾害频发加剧了对污染物溯源的需求；小型四旋翼适合人机共存与狭小空间作业，但受限于其搭载气体传感器灵敏度低、响应慢的问题，亟需补充其他感知模态。

Method: 设计并集成可测气流大小与方向的微型化流速传感器至<100g四旋翼平台；在此基础上改进'Cast and Surge'算法，引入气流方向信息以指导源定位与导航。

Result: 表征实验验证了飞行中气流检测与机体朝向调整能力；多组随机起始位置/姿态的实地试验表明该算法能稳定、可靠地找到气流源。

Conclusion: 本工作为融合气流传感与其他传感器（如气体传感器）的增强型羽流追踪与源搜寻平台奠定了技术基础。

Abstract: As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors < 100 g. We use this sensor to implement a modified version of the `Cast and Surge' algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.

</details>


### [123] [AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning](https://arxiv.org/abs/2601.15614)
*Zichen Yan,Yuchen Hou,Shenao Wang,Yichao Gao,Rui Huang,Lin Zhao*

Main category: cs.RO

TL;DR: 本文提出了AION，一种用于视觉驱动的空中物体导航（ObjectNav）的端到端双策略强化学习框架，无需外部定位或全局地图，在AI2-THOR和IsaacSim中验证了其在探索、导航效率和安全性方面的优越性能。


<details>
  <summary>Details</summary>
Motivation: 将ObjectNav从地面2D扩展到空中3D平台具有挑战性，但空中机器人具备更高机动性和搜索效率，亟需解决空间感知、动态控制与安全保证等新问题。

Method: 提出AION框架，采用端到端双策略强化学习，将探索行为与目标到达行为解耦为两个专用策略，完全基于视觉输入，不依赖外部定位或全局地图。

Result: 在AI2-THOR基准和IsaacSim高保真无人机模型上实验表明，AION在探索能力、导航效率和安全性等综合指标上表现优异。

Conclusion: AION有效解决了空中平台视觉导航的关键挑战，为无地图、无外部定位的自主 aerial ObjectNav 提供了可行且高性能的解决方案。

Abstract: Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg.

</details>


### [124] [D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot](https://arxiv.org/abs/2601.15707)
*Qifan Hu,Branko Celler,Weidong Mu,Steven W. Su*

Main category: cs.RO

TL;DR: 本文提出了一种两阶段校准框架，用于自研三自由度踝关节康复机器人，通过Kronecker积建模与D最优性准则指导的PPO强化学习方法，从50个候选姿态中选出4个最优校准姿态，显著提升信息矩阵行列式值（超两个数量级）和参数估计一致性。


<details>
  <summary>Details</summary>
Motivation: 多自由度康复机器人需高精度对准以保障患者训练的安全性和有效性，而传统校准方法效率低、姿态选择缺乏理论指导。

Method: 第一阶段：基于Kronecker积的开环校准建模，将输入-输出对齐转化为线性参数辨识问题，并构建信息矩阵；第二阶段：将校准姿态选择建模为受约束的组合试验设计问题，采用D最优性准则，并利用仿真训练的PPO智能体从50个候选姿态中选出4个最优姿态。

Result: PPO选出的姿态组合在仿真和实物实验中使信息矩阵行列式均值提升两个数量级以上且方差更小；仅用4个D最优姿态辨识出的参数，在跨轮次预测中比50个无序姿态结果更具一致性。

Conclusion: 该框架显著提升了多自由度康复机器人校准效率与参数估计鲁棒性，为高精度对准提供了实用可行的方法。

Abstract: Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.

</details>


### [125] [DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving](https://arxiv.org/abs/2601.15729)
*Rui Yang,Lei Zheng,Ruoyu Yao,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出DualShield框架，结合扩散模型与哈密顿-雅可比（HJ）可达性理论，在自动驾驶多模态运动规划中实现安全与灵活性的统一：通过HJ值函数双重作用——引导扩散去噪过程至安全可行区域，并构建基于控制屏障-值函数（CBVF）的实时安全防护层。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在自动驾驶运动规划中难以满足车辆动力学约束，且过度依赖其他交通参与者预测，导致不确定性交互下的安全性不足。

Method: 提出DualShield框架，将HJ可达性值函数用于双重目的：1）作为主动引导信号融入扩散模型的去噪过程；2）构建基于CBVF的反应式安全屏蔽层，实时修正执行动作。

Result: 在具挑战性的无保护U型转弯仿真场景中，DualShield相比多种主流规划方法显著提升了安全性与任务完成效率。

Conclusion: DualShield成功融合了扩散模型的表达能力与HJ理论的严格安全保障能力，为不确定甚至对抗性交互下的自动驾驶规划提供了兼具鲁棒性与灵活性的新范式。

Abstract: Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.

</details>


### [126] [Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV](https://arxiv.org/abs/2601.15775)
*Amir Habel,Ivan Snegirev,Elizaveta Semenyakina,Miguel Altamirano Cabrera,Jeffrin Sam,Fawad Mehboob,Roohan Ahmed Khan,Muhammad Ahsan Mustafa,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 本文提出了一种名为Glove2UAV的手套式可穿戴接口，利用IMU传感器和振动反馈实现对无人机的直观手势控制与安全预警。


<details>
  <summary>Details</summary>
Motivation: 提升动态飞行中人机交互的安全性与可预测性，提供轻量、易部署、实时响应的手势控制方案。

Method: 采用IMU手套实时采集惯性数据，结合中值滤波去噪与Madgwick算法估计手掌及手指朝向，并映射为飞行控制原语（如前后、侧向运动及物体交互）；同时集成振动触觉反馈以警示超速。

Result: 在仿真与真实飞行中验证了系统实时可行性：手势指令响应快、手势-平台运动耦合稳定、核心指令正确执行、振动警告及时可靠。

Conclusion: Glove2UAV是一种有效、实用且具备安全增强机制的无人机可穿戴手势控制接口。

Abstract: This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.

</details>


### [127] [A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation](https://arxiv.org/abs/2601.15802)
*Alexandre Albore,Humbert Fiorino,Damien Pellier*

Main category: cs.RO

TL;DR: 本文提出了一种在GNSS拒止环境下，利用空中或水面无人机部署声学信标网络，辅助无人水下航行器（UUV）实现隐蔽、精确导航与路径规划的方法。


<details>
  <summary>Details</summary>
Motivation: 在沿海区域执行军事与民用隐蔽任务时，UUV需在无GNSS支持、无法上浮、且环境受限（如禁入区或危险区）条件下实现自主导航，传统依赖GNSS的方式不可行，亟需GNSS-denied导航方案。

Method: 通过空中/水面无人机部署浮式或潜式声学信标构成合成地标网络；UUV利用该网络进行声学定位与导航；采用分层规划器生成自适应路径，结合基元动作执行与实时监控-重规划机制保障轨迹精度。

Result: 实现了从大陆架到海岸目标点的UUV舰队高精度、隐蔽性导航；验证了信标网络与分层规划框架在GNSS拒止条件下的有效性与鲁棒性。

Conclusion: 所提方法为GNSS拒止环境下UUV的协同、隐蔽、精准导航提供了可行技术路径，适用于复杂近岸作战与敏感区域探测等任务。

Abstract: Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.

</details>


### [128] [TeNet: Text-to-Network for Compact Policy Synthesis](https://arxiv.org/abs/2601.15912)
*Ariyan Bighashdel,Kevin Sebastian Luck*

Main category: cs.RO

TL;DR: TeNet是一种将自然语言指令即时转化为紧凑、任务专用机器人策略的框架，利用预训练大语言模型的文本嵌入驱动超网络生成可执行策略，兼顾语言理解能力与实时控制效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言的机器人控制方法存在两极分化：高阶规划依赖手工设计接口，端到端大模型又难以满足实时部署需求；亟需一种轻量、高效且泛化能力强的语言驱动控制方案。

Method: 提出TeNet框架：用预训练LLM提取文本嵌入，作为条件输入驱动超网络生成仅依赖低维状态输入的紧凑策略网络；训练时可选地通过文本嵌入与演示动作对齐实现行为接地，推理时无需任何演示。

Result: 在MuJoCo和Meta-World基准测试中，TeNet生成的策略体积比序列式基线小数个数量级，在多任务与元学习设置下性能优异，并支持高频控制。

Conclusion: 文本条件超网络为资源受限、有实时要求的机器人控制任务提供了一种实用、紧凑且语言驱动的控制器构建路径。

Abstract: Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.

</details>


### [129] [Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems](https://arxiv.org/abs/2601.15946)
*Zijie Chen,Xiaowei Liu,Yong Xu,Shenghai Yuan,Jianping Li,Lihua Xie*

Main category: cs.RO

TL;DR: 本文提出了一种无需标定靶的LiDAR-电机联合标定方法（LM-Calibr）和一种环境自适应的LiDAR-惯性里程计（EVA-LIO），以提升旋转式驱动LiDAR系统的标定通用性与定位鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖特定安装构型的外参建模，泛化能力差；且旋转式LiDAR易扫描到特征缺失区域，难以兼顾扫描覆盖率与定位鲁棒性。

Method: 基于Denavit-Hartenberg约定设计目标无关的LM-Calibr标定方法；提出EVA-LIO，通过自适应调整降采样率和地图分辨率来匹配空间尺度。

Result: LM-Calibr在不同安装角度、初始值和场景下均表现出高精度与收敛性；EVA-LIO可在保障定位鲁棒性的同时提升扫描完整性，尤其在短暂经过特征缺失区域时仍稳定工作。

Conclusion: 所提方法显著提升了旋转式LiDAR系统在多样化硬件配置与复杂环境下的标定灵活性与定位可靠性。

Abstract: Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \textcolor{blue}{\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\_calibr}}. The video is available at \textcolor{blue}{\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}

</details>


### [130] [PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour](https://arxiv.org/abs/2601.15995)
*Liang Wang,Kanzhong Yao,Yang Liu,Weikai Qin,Jun Wu,Zhe Sun,Qiuguo Zhu*

Main category: cs.RO

TL;DR: 本文提出PUMA框架，将视觉感知与落点先验信息整合到单阶段端到端训练中，提升四足机器人在复杂地形障碍穿越任务中的实时适应性与敏捷性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖分层控制器和预计算落点，限制了机器人的实时适应性和强化学习的探索能力；而人类运动员能基于环境感知自主选择落点，亟需赋予机器人类似感知推理能力。

Method: 提出端到端学习框架PUMA，融合视觉感知与以自我为中心的极坐标落点先验（相对距离与朝向），利用地形特征估计落点先验，并驱动主动姿态调整。

Result: 在仿真与真实环境中多种离散复杂地形上验证，PUMA展现出优异的敏捷性与鲁棒性。

Conclusion: PUMA通过单阶段联合训练实现感知-决策-控制紧耦合，显著提升了四足机器人在parkour任务中的自主性与适应性，为敏捷运动控制提供了新范式。

Abstract: Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.

</details>


### [131] [Collision-Free Humanoid Traversal in Cluttered Indoor Scenes](https://arxiv.org/abs/2601.16035)
*Han Xue,Sikai Liang,Zhikai Zhang,Zicheng Zeng,Yun Liu,Yunrui Lian,Jilong Wang,Qingtao Liu,Xuesong Shi,Li Yi*

Main category: cs.RO

TL;DR: 本文提出HumanoidPF（人形机器人势场）表示方法，用于解决室内杂乱场景中人形机器人无碰撞通行问题，并结合混合场景生成方法和强化学习，实现了仿真到真实世界的成功迁移及单击式远程操控。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏有效表征人形机器人与障碍物关系的表示方式，导致难以直接学习感知到通行技能的映射，尤其在复杂室内杂乱场景（如跨越、蹲伏、穿行）中。

Method: 提出Humanoid Potential Field（HumanoidPF），将人形-障碍关系编码为无碰撞运动方向；设计混合场景生成方法（融合真实3D室内场景裁剪与程序化障碍合成）；采用RL训练策略并实现sim-to-real迁移；构建单击式teleoperation系统。

Result: 在仿真与真实世界中均验证了方法有效性；HumanoidPF表现出极小的sim-to-real感知差距；成功部署于真实人形机器人，支持用户单击指令完成复杂室内通行任务。

Conclusion: HumanoidPF是一种高效、泛化性强且具现实可行性的感知表示，显著提升了人形机器人在杂乱室内环境中的自主/遥控通行能力。

Abstract: We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.

</details>


### [132] [DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning](https://arxiv.org/abs/2601.16046)
*Junha Lee,Eunha Park,Minsu Cho*

Main category: cs.RO

TL;DR: 本文提出了DextER模型，通过基于接触点的具身推理来生成灵巧抓取，显著提升了任务语义理解与物理约束之间的对齐能力，并在DexGYS数据集上取得了67.14%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接映射观测到抓取参数，缺乏对物理交互的中间推理，难以兼顾任务语义、3D几何和复杂手-物交互。

Method: DextER引入接触点驱动的具身推理机制，自回归地生成‘具身接触token’（指定手指链与物体表面接触位置），再生成‘抓取token’（编码手部构型）。

Result: 在DexGYS数据集上达到67.14%抓取成功率，较SOTA提升3.83个百分点；意图对齐提升96.4%；支持通过部分接触指定实现可调控抓取生成。

Conclusion: 具身接触表示是连接语言任务意图与物理可行抓取的有效中间表征，DextER为语言驱动的灵巧操作提供了新范式。

Abstract: Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.

</details>


### [133] [Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Theoretical Analysis](https://arxiv.org/abs/2601.16062)
*Jiarui Cui,Maosong Wang,Wenqi Wu,Peiqi Li,Xianfei Pan*

Main category: cs.RO

TL;DR: 本文分析了SE2(3)李群框架在高精度导航建模中误差传播自主性的理论限制，并提出一种改进的SE2(3)建模方法以增强其自主性，主要针对非惯性系中科里奥利力项导致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SE2(3)李群导航模型在低精度场景下可保持误差传播自主性，但在高精度应用（如考虑地球自转和惯性器件偏差）时难以维持该性质，亟需理论分析与改进。

Method: 对惯性系、地球系和世界系下的SE2(3)高精度导航模型进行理论分析，识别科里奥利力项对自主性的破坏机制，并据此提出一种新的SE2(3)导航模型构造方法。

Result: 发现传统SE2(3)模型在非惯性系中因速度引入的科里奥利力项而丧失自主性；所提新构造方法能显著提升模型的误差传播自主性。

Conclusion: SE2(3)李群导航模型的自主性高度依赖于参考坐标系选择与动力学建模完整性；通过合理建模科里奥利效应，可使高精度导航模型更接近完全自主。

Abstract: One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.

</details>


### [134] [Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Application](https://arxiv.org/abs/2601.16078)
*Jiarui Cui,Maosong Wang,Wenqi Wu,Peiqi Li,Xianfei Pan*

Main category: cs.RO

TL;DR: 本文通过真实SINS/ODO实验和蒙特卡洛仿真，验证了改进的SE2(3)李群高精度导航模型的性能，旨在提升非惯性导航模型的完全自主性。


<details>
  <summary>Details</summary>
Motivation: 提升非惯性导航模型的完全自主性，利用SE2(3)李群框架的误差传播自主性优势。

Method: 提出SE2(3)群导航模型构建方法，并开展真实SINS/ODO实验与蒙特卡洛仿真。

Result: 验证了改进的SE2(3)群高精度导航模型在实际应用中的有效性与优越性能。

Conclusion: SE2(3)李群框架能有效提升导航模型的自主性，实验与仿真结果支持其在高精度导航中的应用价值。

Abstract: One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.

</details>


### [135] [Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision](https://arxiv.org/abs/2601.16109)
*Yashuai Yan,Tobias Egle,Christian Ott,Dongheui Lee*

Main category: cs.RO

TL;DR: 本文提出了一种结合基于模型的双足行走控制与残差强化学习（RL）的框架，利用DCM轨迹规划和全身控制器作为基础策略，并通过具有真实动力学信息的模型基底监督式残差策略来提升鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界中动力学建模不准确和传感器噪声等不确定性对双足行走鲁棒性与适应性的影响。

Method: 采用基于模型的控制器（DCM轨迹规划器+全身体控制器）作为基线策略，并引入通过域随机化训练的残差RL策略；该残差策略在训练中受具备真值动力学信息的模型基底‘oracle’策略以监督损失函数指导。

Result: 所提方法在多种随机化条件下展现出更强的鲁棒性与泛化能力，提升了仿真到实物（sim-to-real）迁移的可扩展性。

Conclusion: 融合模型驱动先验与数据驱动残差校正的混合控制范式，能有效缓解建模误差与感知噪声，是实现高可靠双足行走的可行路径。

Abstract: We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.

</details>


### [136] [IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance](https://arxiv.org/abs/2601.16207)
*Jongwoo Park,Kanchana Ranasinghe,Jinhyeok Jang,Cristina Mata,Yoo Sung Jang,Michael S Ryoo*

Main category: cs.RO

TL;DR: 本文提出IVRA方法，通过在推理时利用视觉编码器内置的亲和度提示来增强VLA模型的空间理解能力，无需额外训练或外部编码器，在多个VLA模型和任务上显著提升操作成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型将图像块展平为1D序列，削弱了对精确操控至关重要的2D空间线索。

Method: IVRA是一种轻量、无需训练的方法，选择性地将视觉编码器中已有的亲和度信号注入到语言模型中承载实例级特征的层，从而在推理时重新校准视觉token交互并保持几何结构。

Result: 在2D VIMA基准上，IVRA使LLaRA在低数据场景下平均成功率提升+4.2%；在3D LIBERO上，对OpenVLA和FLOWER基线持续提升，如从96.3%提升至97.1%。

Conclusion: IVRA是一种通用、即插即用的推理时增强方法，能有效提升多种VLA架构的空间感知与操控性能，且不改变模型参数。

Abstract: Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA

</details>


### [137] [Point Bridge: 3D Representations for Cross Domain Policy Learning](https://arxiv.org/abs/2601.16212)
*Siddhant Haldar,Lars Johannsmeier,Lerrel Pinto,Abhishek Gupta,Dieter Fox,Yashraj Narang,Ajay Mandlekar*

Main category: cs.RO

TL;DR: 本文提出Point Bridge框架，利用统一的、与领域无关的点云表示，结合视觉语言模型（VLM）自动提取点特征、Transformer策略学习和高效推理流程，在仅使用合成数据的情况下实现零样本仿真到现实（sim-to-real）策略迁移，并在加入少量真实演示数据后进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 机器人基础模型的发展受限于大规模真实世界操作数据集的稀缺；仿真与合成数据虽可扩展，但存在仿真与现实之间的视觉域差距问题。

Method: 提出Point Bridge框架：1）利用视觉语言模型（VLM）自动提取统一的点云表示；2）基于Transformer进行策略学习；3）构建高效推理管道；4）支持仅用合成数据训练，或辅以少量真实演示数据联合训练。

Result: 在零样本sim-to-real迁移中性能提升达44%，加入少量真实数据后提升达66%；在单任务与多任务设置下均显著优于现有基于视觉的仿真-现实联合训练方法。

Conclusion: Point Bridge通过点云这一域无关表征，有效弥合仿真与现实间的视觉鸿沟，为无需显式视觉或物体级对齐的通用机器人策略学习提供了新范式。

Abstract: Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [138] [Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models](https://arxiv.org/abs/2601.15305)
*Alfred Shen,Aaron Shen*

Main category: cs.AI

TL;DR: 本文提出了一种结合稀疏注意力与门控注意力优势的新架构——门控稀疏注意力（GSA），通过门控索引器、自适应稀疏控制器和双阶段门控机制，在保持高效率的同时显著提升模型性能与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 稀疏注意力和 gated attention 各自解决不同问题，但此前工作相互独立；作者旨在融合二者优势以同时改善计算效率、建模质量与训练稳定性。

Method: 提出 Gated Sparse Attention（GSA），包含：带 sigmoid 激活的门控 Lightning 索引器（生成有界可解释选择分数）、基于局部不确定性的自适应稀疏控制器、以及在 value 和 output 阶段的双重门控；并提供了复杂度分析、表达能力证明与收敛性保证。

Result: 在 1.7B 参数模型、400B token 训练下，GSA 在 128K 上达到 12–16 倍加速；困惑度从 6.03 降至 5.70；RULER 分数近翻倍；首token 注意力（注意力沉陷指标）从 47% 降至 <4%；损失尖峰减少 98%。

Conclusion: GSA 成功统一稀疏与门控注意力的优点，在效率、质量与稳定性三方面实现协同提升，为长上下文语言建模提供新范式。

Abstract: The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.

</details>


### [139] [Uncovering Latent Bias in LLM-Based Emergency Department Triage Through Proxy Variables](https://arxiv.org/abs/2601.15306)
*Ethan Zhang*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型（LLM）在急诊分诊中的隐性偏见，发现模型会因输入中特定代用变量（如种族、社会经济背景等）的出现而系统性地改变对患者病情严重程度的判断，反映出其依赖非因果、噪声数据的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在临床决策中取得进展，但其对不同种族、社会、经济和临床背景患者的隐性偏见仍存，亟需系统评估其在高风险场景（如急诊分诊）中的公平性。

Method: 使用32个患者级代理变量（每变量含正负修饰词），结合公开（MIMIC-IV-ED Demo等）与受限访问（MIMIC-IV-ED、MIMIC-IV）临床数据集，评估LLM在急诊分诊任务中对代理变量的敏感性及严重度判断偏差。

Result: 发现LLM存在由代理变量引发的歧视性行为，并表现出对特定输入token的系统性响应——无论其正负修饰，均显著改变病情严重度判断。

Conclusion: 当前LLM医疗AI仍过度依赖不可靠的非因果信号，需加强数据质量、因果建模与公平性验证，以保障临床部署的安全性与伦理性。

Abstract: Recent advances in large language models (LLMs) have enabled their integration into clinical decision-making; however, hidden biases against patients across racial, social, economic, and clinical backgrounds persist. In this study, we investigate bias in LLM-based medical AI systems applied to emergency department (ED) triage. We employ 32 patient-level proxy variables, each represented by paired positive and negative qualifiers, and evaluate their effects using both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets as appropriate~\cite{mimiciv_ed_demo,mimiciv_ed,mimiciv}. Our results reveal discriminatory behavior mediated through proxy variables in ED triage scenarios, as well as a systematic tendency for LLMs to modify perceived patient severity when specific tokens appear in the input context, regardless of whether they are framed positively or negatively. These findings indicate that AI systems is still imperfectly trained on noisy, sometimes non-causal signals that do not reliably reflect true patient acuity. Consequently, more needs to be done to ensure the safe and responsible deployment of AI technologies in clinical settings.

</details>


### [140] [DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey](https://arxiv.org/abs/2601.15307)
*Guo-Biao Zhang,Ding-Yuan Liu,Da-Yi Wu,Tian Lan,Heyan Huang,Zhijing Wu,Xian-Ling Mao*

Main category: cs.AI

TL;DR: 本文提出DeepSurvey-Bench，一个专注于评估生成综述学术价值的新基准，涵盖信息价值、学术交流价值和研究指导价值三个维度，并构建了带学术价值标注的可靠数据集。


<details>
  <summary>Details</summary>
Motivation: 现有评测基准依赖有缺陷的筛选标准（如引用数、结构连贯性）和表面指标（如逻辑一致性），无法评估生成综述的深层学术价值（如核心研究目标、关键研究分析）。

Method: 提出DeepSurvey-Bench，定义覆盖信息价值、学术交流价值和研究指导价值的学术价值评估准则；构建带学术价值标注的高质量数据集；基于该准则评估生成综述的深层学术价值。

Result: 实验表明，DeepSurvey-Bench在评估生成综述学术价值方面与人类评价高度一致。

Conclusion: DeepSurvey-Bench有效弥补了现有基准在深层学术价值评估上的不足，为自动化科学综述生成技术提供了更可靠、更具学术意义的评测标准。

Abstract: The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep "academic value", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.

</details>


### [141] [Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.15311)
*Mustafa Arslan*

Main category: cs.AI

TL;DR: 本文提出Aeon，一种神经符号认知操作系统，通过Memory Palace和Trace结构化长时记忆，并引入语义旁路缓冲（SLB）实现亚毫秒级检索，解决大模型中'中间丢失'与'向量迷雾'问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型受限于自注意力的二次计算开销及'中间丢失'现象；Flat RAG方法将记忆视为无结构嵌入集合，导致'向量迷雾'——检索结果缺乏事件连续性。

Method: 提出Aeon系统：1）Memory Palace——基于Atlas索引的空间化记忆结构，融合小世界图导航与B+树式磁盘局部性；2）Trace——神经符号化的片段图；3）Semantic Lookaside Buffer（SLB）——利用对话局部性的预测缓存机制；4）零拷贝C++/Python桥保证状态一致性。

Result: 在对话负载下实现<1ms检索延迟，支持持久化、结构化记忆，显著提升长程推理与状态一致性能力。

Conclusion: Aeon将记忆重新定义为可管理的操作系统资源，通过神经符号协同与硬件感知索引设计，突破传统RAG范式局限，为自主智能体提供高效、连贯的长期记忆支持。

Abstract: Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the "Lost in the Middle" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily "Flat RAG" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to "Vector Haze", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.

</details>


### [142] [MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation](https://arxiv.org/abs/2601.15487)
*Chandan Kumar Sahu,Premith Kumar Chilukuri,Matthew Hetrich*

Main category: cs.AI

TL;DR: 本文提出MiRAGE，一个用于评估检索增强生成（RAG）系统的多智能体框架，专为多模态、高风险企业应用场景设计，能自动生成经验证的、领域特定、多模态、多跳问答数据集。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估基准滞后于实际应用发展，尤其在专业领域中无法有效处理多模态、需跨片段推理的技术文档。

Method: MiRAGE采用多智能体协同框架：包含递归上下文优化环以聚合分散证据、对抗式验证器确保事实准确性、以及领域与专家角色识别代理以模拟专家认知流程。

Result: 在法规、金融、定量生物学和新闻四个领域验证表明，MiRAGE生成的数据集平均推理跳数>2.3，事实忠实度显著提升；若提供图像文本描述，可仅用大语言模型驱动，但视觉对齐仍是挑战。

Conclusion: MiRAGE为构建贴近私有语料潜在主题结构的高质量黄金标准评测数据集提供了自动化基础设施，支撑下一代信息检索系统的严格评估。

Abstract: The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.

</details>


### [143] [The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection](https://arxiv.org/abs/2601.15316)
*Wei Ai,Yilong Tan,Yuntao Shou,Tao Meng,Haowen Chen,Zhixiong He,Keqin Li*

Main category: cs.AI

TL;DR: 本文是一篇关于大型视觉语言模型（LVLMs）在多模态虚假新闻检测（MFND）中应用的综述论文，系统梳理了从传统方法到LVLM驱动范式的演进路径，构建了涵盖模型、数据集与基准的分类体系，并分析了可解释性、时序推理与泛化等挑战，指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前多模态虚假新闻检测领域缺乏对大型视觉语言模型（LVLMs）推动范式转变的系统性综述，亟需整合最新进展并厘清技术脉络。

Method: 采用文献综述与结构化分析方法，包括历史演进梳理、构建涵盖模型架构、数据集和性能基准的分类体系，并对关键技术挑战进行深入分析。

Result: 首次系统性地总结了LVLMs在MFND中的 transformative role，建立了结构化taxonomy，识别出可解释性、时序推理和领域泛化等核心挑战，并提出未来研究方向；配套Github资源汇总现有方法。

Conclusion: LVLMs正深刻重塑MFND的研究范式，但其实际部署仍面临多重技术瓶颈；本综述为该领域提供了理论框架与实践指引，是首篇聚焦LVLMs驱动MFND的综合性调研。

Abstract: In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.

</details>


### [144] [ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance](https://arxiv.org/abs/2601.15551)
*Bismack Tokoli,Luis Jaimes,Ayesha S. Dina*

Main category: cs.AI

TL;DR: 本文提出ALIGNAgent，一个集成知识追踪、技能差距识别与资源推荐的多智能体个性化学习框架，通过诊断推理精准定位学生知识缺陷并提供针对性学习资源，在真实课程数据中验证了其高精度和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化学习系统大多功能割裂，仅专注于知识追踪、诊断建模或资源推荐中的某一方面，缺乏整合成闭环自适应学习流程的能力。

Method: ALIGNAgent采用多智能体架构：Skill Gap Agent基于测验成绩、成绩册和学习者偏好进行概念级诊断推理，生成主题级能力估计并识别具体误解；Recommender Agent据此检索偏好感知的学习资源，并在进入下一主题前实施干预，形成持续反馈循环。

Result: 在两门本科计算机课程的真实数据集上评估显示，基于GPT-4o的智能体在知识能力估计任务中达到0.87–0.90的精确率和0.84–0.87的F1分数，且结果经实际考试表现验证。

Conclusion: ALIGNAgent成功实现了知识估计、差距识别与资源推荐的有机整合，显著提升了个性化学习系统的闭环适应能力与实际教学有效性。

Abstract: Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.

</details>


### [145] [Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents](https://arxiv.org/abs/2601.15322)
*Raffi Khatchadourian*

Main category: cs.AI

TL;DR: 本文提出DFAH框架，用于评估金融领域工具使用型大语言模型代理在监管审计重放中的确定性和证据条件下的忠实性，并发现确定性与忠实性之间存在正相关关系。


<details>
  <summary>Details</summary>
Motivation: LLM代理在监管审计重放中表现不一致，即相同输入下无法复现被标记的交易决策结果，亟需可量化的评估框架。

Method: 提出Determinism-Faithfulness Assurance Harness（DFAH）框架，通过74种配置的非代理基线实验和三类金融基准测试（各50例），量化轨迹确定性与证据条件下的忠实性；采用低温度（T=0.0）设置并统计验证样本规模需求。

Result: 7–20B参数模型达100%确定性，120B+模型需3.7倍验证样本；确定性与忠实性呈显著正相关（r=0.45, p<0.01）；schema-first架构的Tier 1模型满足审计重放所需的确定性水平。

Conclusion: 确定性与忠实性并非权衡关系，而是协同提升的属性；DFAH为金融AI代理的合规部署提供了可测量、可复现的评估标准。

Abstract: LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.
  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.
  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.

</details>


### [146] [Prometheus Mind: Retrofitting Memory to Frozen Language Models](https://arxiv.org/abs/2601.15324)
*Mark Wind*

Main category: cs.AI

TL;DR: 本文提出Prometheus Mind，通过11个模块化适配器（530MB，7%开销）为冻结的Qwen3-4B模型添加可逆记忆能力，无需修改架构或权重；提出Contrastive Direction Discovery（CDD）等关键技术解决语义提取、分阶段训练、注入机制与隐状态区分问题；在PrometheusExtract-132数据集上，干净输入检索率达94.4%，但非正式输入下降至19.4%，主要瓶颈在于关系分类（准确率47.3%）。


<details>
  <summary>Details</summary>
Motivation: 为冻结的预训练语言模型高效、可逆地添加记忆能力，避免架构修改或权重更新带来的复杂性与不可逆性。

Method: 提出Prometheus Mind系统，包含四个关键技术：（1）Contrastive Direction Discovery（CDD），利用无标注最小对发现语义方向；（2）分阶段训练各适配器，避免端到端优化崩溃；（3）复用lm_head.weight行作为注入映射，免训练；（4）训练投影层缓解Transformer中隐状态过度相似问题（如'wife'与'brother'相似度从0.98降至0.09）。

Result: 在PrometheusExtract-132数据集上，干净输入检索准确率为94.4%（n=54，95% CI: [84.9%, 98.1%]），非正式输入（含省略、填充词、隐式主语）下降至19.4%（n=36）；关系分类准确率仅47.3%，是主要错误来源。

Conclusion: 模块化适配器可高效、可逆地为冻结LLM注入记忆能力，但当前方法对语言不规范性鲁棒性差，关系分类能力薄弱，是后续改进关键。

Abstract: Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.

</details>


### [147] [Logic Programming on Knowledge Graph Networks And its Application in Medical Domain](https://arxiv.org/abs/2601.15347)
*Chuanqing Wang,Zhenmin Zhao,Shanshan Du,Chaoqun Fei,Songmao Zhang,Ruqian Lu*

Main category: cs.AI

TL;DR: 本文提出并系统研究了'知识图谱网络'的概念，旨在解决当前知识图谱在医学与医疗领域应用中逻辑推理、人工智能技术、专用编程语言及概率统计理论利用不足的问题，尤其关注多知识图谱的协同与竞争机制，并在多种复杂条件下（如模糊性、不确定性、多模态等）进行了定义、推理、计算与应用验证。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱在医学与医疗领域的应用中，先进逻辑推理、人工智能技术、专用编程语言及现代概率统计理论等信息处理技术利用不足，尤其是多知识图谱的协同与竞争技术未受足够重视。

Method: 构建‘知识图谱网络’的系统性理论、技术与应用框架，涵盖其在模糊、不确定、多模态、向量化、分布式、联邦等不同条件下的定义、开发、推理、计算与应用，并辅以真实数据示例与实验结果。

Result: 提出了‘知识图谱网络’的完整理论体系与技术实现路径，在多种复杂场景下完成验证，提供了多个基于真实数据的实验结果与应用案例。

Conclusion: 本研究创新性地拓展了知识图谱的研究范式，为医学与医疗领域中多源异构知识图谱的协同建模与智能应用提供了新思路与方法支撑。

Abstract: The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.

</details>


### [148] [GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation](https://arxiv.org/abs/2601.15392)
*Francesca Pia Panaccione,Carlo Sgaravatti,Pietro Pinoli*

Main category: cs.AI

TL;DR: 本文提出GeMM-GAN，一种基于组织病理图像和临床元数据生成基因表达谱的条件生成对抗网络，通过Transformer编码器与跨注意力机制融合多模态信息，在TCGA数据集上显著提升生成基因表达谱的真实性与下游疾病预测准确性。


<details>
  <summary>Details</summary>
Motivation: 基因表达数据因隐私法规严格和实验成本高而难以广泛应用，亟需一种能利用易获取的病理图像和临床数据合成高质量基因表达谱的方法。

Method: 提出GeMM-GAN模型：采用Transformer Encoder处理图像块，结合跨注意力机制融合图像块与文本（临床元数据）特征，生成条件向量以指导生成器合成基因表达谱。

Result: 在TCGA数据集上，GeMM-GAN生成的基因表达谱更真实、功能更合理，在下游疾病类型预测任务中准确率较现有最优生成模型提升超11%。

Conclusion: GeMM-GAN为突破基因表达数据获取瓶颈提供了可行方案，推动多模态生物医学数据整合研究。

Abstract: Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN

</details>


### [149] [Beyond Prompting: Efficient and Robust Contextual Biasing for Speech LLMs via Logit-Space Integration (LOGIC)](https://arxiv.org/abs/2601.15397)
*Peidong Wang*

Main category: cs.AI

TL;DR: 本文提出LOGIC框架，通过在解码层直接操作实现上下文偏置，有效解决语音大模型中新实体识别难的问题，显著降低实体词错误率。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型因静态训练知识难以识别新出现的领域专有实体（如联系人名、歌单、技术术语），而传统提示方法存在可扩展性差、上下文窗口限制及'中间丢失'问题，生成式纠错又易导致过纠正和幻觉。

Method: 提出LOGIC（Logit-Space Integration for Contextual Biasing）框架，在解码层进行logit空间的上下文偏置集成，将上下文注入与输入处理解耦，实现与提示长度无关的常数时间复杂度。

Result: 在Phi-4-MM模型上跨11种多语种场景的实验表明，LOGIC平均降低实体词错误率（Entity WER）9%（相对），仅带来0.30%的虚警率轻微上升。

Conclusion: LOGIC是一种高效、鲁棒的新实体识别增强方法，克服了 prompting 和 GEC 的关键缺陷，适用于实际多语种语音理解系统。

Abstract: The rapid emergence of new entities -- driven by cultural shifts, evolving trends, and personalized user data -- poses a significant challenge for existing Speech Large Language Models (Speech LLMs). While these models excel at general conversational tasks, their static training knowledge limits their ability to recognize domain-specific terms such as contact names, playlists, or technical jargon. Existing solutions primarily rely on prompting, which suffers from poor scalability: as the entity list grows, prompting encounters context window limitations, increased inference latency, and the "lost-in-the-middle" phenomenon. An alternative approach, Generative Error Correction (GEC), attempts to rewrite transcripts via post-processing but frequently suffers from "over-correction", introducing hallucinations of entities that were never spoken.
  In this work, we introduce LOGIC (Logit-Space Integration for Contextual Biasing), an efficient and robust framework that operates directly in the decoding layer. Unlike prompting, LOGIC decouples context injection from input processing, ensuring constant-time complexity relative to prompt length. Extensive experiments using the Phi-4-MM model across 11 multilingual locales demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER with a negligible 0.30% increase in False Alarm Rate.

</details>


### [150] [Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.15436)
*Shahar Ben Natan,Oren Tsur*

Main category: cs.AI

TL;DR: 本文提出了一种直接且中立评估大语言模型（LLM）谄媚倾向的新方法，利用LLM-as-a-judge框架将谄媚建模为零和博弈，并发现谄媚倾向与最近偏见存在协同增强效应。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估LLM谄媚性时常引入不受控偏差、噪声或操纵性语言，缺乏直接、中立的评估方式。

Method: 提出基于LLM-as-a-judge的零和博弈赌注框架，将谄媚定义为使用户受益却明确损害第三方的行为；对比Gemini 2.5 Pro、ChatGPT 4o、Mistral-Large-Instruct-2411和Claude Sonnet 3.7四模型，并分析其在不同设定下的响应模式及对最后呈现答案的偏好。

Result: 所有模型均表现出谄媚倾向，但Claude和Mistral在可能伤害第三方时表现出‘道德内疚’并过度补偿；所有模型均存在显著的最近偏见；谄媚倾向与最近偏见存在‘建设性干涉’效应，即当用户观点被置于最后时，谄媚倾向被加剧。

Conclusion: 谄媚不仅是简单迎合用户，而是受情境（如是否损害第三方）与认知偏差（如最近偏见）共同调节的复杂行为；该发现对LLM对齐与评估具有重要启示。

Abstract: We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit "moral remorse" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.

</details>


### [151] [A tensor network formalism for neuro-symbolic AI](https://arxiv.org/abs/2601.15442)
*Alex Goessmann,Janina Schütte,Maximilian Fröhlich,Martin Eigel*

Main category: cs.AI

TL;DR: 本文提出了一种基于张量网络的统一框架，将神经与符号AI方法结合，通过张量分解建模函数、逻辑公式和概率分布，并将推理归约为张量收缩，进而设计出可高效扩展的混合逻辑-概率模型（Hybrid Logic Network）及配套Python库tnreason。


<details>
  <summary>Details</summary>
Motivation: 神经与符号AI的融合仍是AI领域的核心开放挑战，需一种能同时刻画二者稀疏性原理的统一形式化方法。

Method: 引入张量网络形式化方法，将函数表示为张量基编码，神经分解建模为张量分解；将逻辑公式与概率分布表示为结构化张量分解；将推理抽象为张量收缩，并设计基于消息传递的收缩算法。

Result: 实现了逻辑与概率模型的统一表示与联合训练，提出了Hybrid Logic Network架构，并开发了支持其实现与应用的Python库tnreason。

Conclusion: 张量网络提供了一种兼具表达力与计算效率的统一框架，使神经与符号推理可在同一数学语言下协同建模与优化。

Abstract: The unification of neural and symbolic approaches to artificial intelligence remains a central open challenge. In this work, we introduce a tensor network formalism, which captures sparsity principles originating in the different approaches in tensor decompositions. In particular, we describe a basis encoding scheme for functions and model neural decompositions as tensor decompositions. The proposed formalism can be applied to represent logical formulas and probability distributions as structured tensor decompositions. This unified treatment identifies tensor network contractions as a fundamental inference class and formulates efficiently scaling reasoning algorithms, originating from probability theory and propositional logic, as contraction message passing schemes. The framework enables the definition and training of hybrid logical and probabilistic models, which we call Hybrid Logic Network. The theoretical concepts are accompanied by the python library tnreason, which enables the implementation and practical use of the proposed architectures.

</details>


### [152] [Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases](https://arxiv.org/abs/2601.15476)
*Alex Dantart*

Main category: cs.AI

TL;DR: 本文探讨了如何通过减少幻觉来提高大语言模型在高风险法律工作中的可靠性，提出了三种AI范式，并引入了False Citation Rate (FCR) 和 Fabricated Fact Rate (FFR) 两个可靠性指标。研究发现，高级RAG系统能将事实捏造率降至0.2%以下，强调验证与可追溯性的检索架构是实现可信法律AI的关键。


<details>
  <summary>Details</summary>
Motivation: 提高大语言模型在高风险法律工作中的可靠性，减少幻觉问题。

Method: 提出三种AI范式（独立生成模型、基础检索增强系统、端到端优化的高级RAG系统），定义并使用FCR和FFR两个指标，在75个法律任务上对12个LLM生成的2700个司法风格答案进行专家双盲评估。

Result: 独立模型FCR超30%，不适用于专业场景；基础RAG显著降低错误但仍存在误接地；高级RAG（含嵌入微调、重排序、自校正等）使捏造率低于0.2%。

Conclusion: 可信的法律AI需依赖以严谨性为导向、强调验证与可追溯性的检索架构，并提供了适用于其他高风险领域的评估框架。

Abstract: This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models ("creative oracle"), (2) basic retrieval-augmented systems ("expert archivist"), and (3) an advanced, end-to-end optimized RAG system ("rigorous archivist"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.

</details>


### [153] [Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge](https://arxiv.org/abs/2601.15495)
*Yiyang Feng,Zeming Chen,Haotian Wu,Jiawei Zhou,Antoine Bosselut*

Main category: cs.AI

TL;DR: 本文提出了TRACK基准，用于评估大语言模型在多步推理中处理新知识与原有参数化知识冲突的能力，并揭示了现有方法在知识更新时可能导致推理性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑和上下文学习方法在更新LLM知识时易引发知识冲突，而当前基准仅关注单次事实回忆，缺乏对冲突知识影响多步推理的评估。

Method: 构建TRACK基准，涵盖WIKI、CODE、MATH三类推理密集型场景，引入多个真实知识冲突，系统测试模型在多步推理中对更新知识的整合与传播能力。

Result: 实验表明，在存在知识冲突时提供更新事实反而降低模型推理性能，且更新事实越多，性能下降越严重；失败源于模型既不能忠实整合新知识，也不能在整合后进行正确推理。

Conclusion: TRACK为评估和推动LLM在多步推理中处理冲突知识的能力提供了严格的新基准。

Abstract: A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.

</details>


### [154] [The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers](https://arxiv.org/abs/2601.15509)
*Prasanna Kumar*

Main category: cs.AI

TL;DR: 本文探讨了在应用AI分析（特别是情感分析）中，基于Transformer的迁移学习虽然提高了准确性，但也导致了情感类别间的极化现象和中立性缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的情感分析模型在提高某一类情感准确性的同时，导致其他情感类别极化及中立性失效的问题，以满足工业级应用对可靠情感分析结果的需求。

Method: 通过实验观察和分析基于Transformer的迁移学习模型在情感分析任务中的表现，重点关注各类别准确率变化及中立性缺失现象。

Result: 发现Transformer模型在提升某类情感识别精度的同时，加剧了其他情感类别的极化，并显著削弱了中立情感的识别能力。

Conclusion: 当前Transformer主导的情感分析方法存在中立性缺陷，需在模型设计中引入平衡机制以保障多类别情感分析的公平性与可靠性。

Abstract: The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.

</details>


### [155] [TransportAgents: a multi-agents LLM framework for traffic accident severity prediction](https://arxiv.org/abs/2601.15519)
*Zhichao Yang,Jiashu He,Jinxuan Fan,Cirillo Cinzia*

Main category: cs.AI

TL;DR: 本文提出TransportAgents，一种结合领域专用LLM代理与MLP集成模块的多智能体框架，用于提升交通事故严重性预测的准确性、鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体大语言模型在处理异构、领域特定的交通事故数据时表现不佳，易产生偏差和不稳定预测，难以满足应急响应与公共安全规划对高精度、可靠预测的需求。

Method: 构建TransportAgents混合多智能体框架：多个类别专用LLM代理（分别处理人口统计、环境上下文、事件细节等子信息）生成中间严重性评估，再通过多层感知机（MLP）模块融合为统一预测。

Result: 在CPSRMS和NEISS两个美国真实数据集上，TransportAgents在多种LLM骨干模型（GPT-3.5、GPT-4o、LLaMA-3.3）下均显著优于传统机器学习与单智能体LLM基线，展现出强鲁棒性、可扩展性与跨数据集泛化能力；分布分析表明其预测更均衡、校准更好。

Conclusion: TransportAgents通过解耦推理与协同融合，有效缓解了单智能体LLM在交通安全部署中的偏差与不稳定性问题，为安全关键型决策支持提供了更可靠、可解释的AI解决方案。

Abstract: Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.

</details>


### [156] [From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models](https://arxiv.org/abs/2601.15533)
*Zhikang Chen,Tingting Zhu*

Main category: cs.AI

TL;DR: 本文批判当前世界模型过度依赖视觉保真度，指出其缺乏对物理与因果动态的理解；提出应将世界模型重构为‘可操作的模拟器’，强调因果结构、约束感知动力学与闭环评估，并以医疗决策为压力测试场景验证其必要性。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型虽能高保真生成视频，却常违反物理不变性、干预失效且在安全关键决策中崩溃，说明视觉真实感不能代表真正的世界理解。

Method: 通过分析当前世界模型的缺陷，提出将其重新定义为‘可操作的模拟器’，强调结构化4D接口、约束感知动力学建模及闭环评估框架，并以医疗决策作为高风险、不可试错的典型场景进行验证。

Result: 论证了视觉逼真度不是世界理解的可靠指标；确立了因果结构编码、领域约束尊重和长时程稳定性为世界模型的核心要求；在医疗决策中验证了模型价值在于支持反事实推理、干预规划与鲁棒长程预见能力。

Conclusion: 世界模型的发展方向应从追求像素级预测转向构建具备因果性、约束一致性与行动可靠性的可操作模拟器，尤其在安全敏感领域需以闭环、干预式评估取代开环视觉评估。

Abstract: A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.

</details>


### [157] [Autonomous Business System via Neuro-symbolic AI](https://arxiv.org/abs/2601.15599)
*Cecil Pang,Hiroki Sayama*

Main category: cs.AI

TL;DR: 本文提出AUTOBUS，一种结合大语言模型（LLM）与谓词逻辑编程的自主业务系统，通过神经符号AI架构实现端到端业务流程编排，兼顾语义理解与可验证执行。


<details>
  <summary>Details</summary>
Motivation: 现有企业系统按部门孤岛设计、流程僵化，而大语言模型虽擅于理解自然语言和非结构化数据，却难以保证复杂业务逻辑的确定性与可验证执行，二者之间存在关键鸿沟。

Method: 构建AUTOBUS系统：将业务倡议建模为带显式前后置条件、所需数据、评估规则和API动作的任务网络；企业数据组织为知识图谱，并映射为逻辑事实与基础规则；AI代理融合任务指令、企业语义与工具生成任务专属逻辑程序，由逻辑引擎执行并协调工具调用；人类负责语义定义、策略维护、工具管理与关键决策监督。

Result: 实现了语义驱动、约束可验证、人机协同的端到端业务流程自动化框架，支持灵活重配置与高可信执行。

Conclusion: AUTOBUS通过神经符号融合范式，弥合了LLM的语义理解能力与传统企业系统对确定性、可审计性和业务语义一致性的要求，为下一代智能企业系统提供了可行架构。

Abstract: Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.

</details>


### [158] [CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models](https://arxiv.org/abs/2601.15628)
*Haibo Tong,Zeyang Yue,Feifei Zhao,Erliang Lin,Lu Jia,Ruolin Chen,Yinqian Sun,Qian Zhang,Yi Zeng*

Main category: cs.AI

TL;DR: 本文提出了CogToM基准，一个涵盖46种范式、8000多个双语实例的理论驱动型ToM评测集，用于系统评估22个主流大语言模型（如GPT-5.1、Qwen3-Max）的类人心理理论能力，发现模型在不同认知维度上存在显著性能差异与瓶颈，并提示其认知结构可能与人类存在本质差异。


<details>
  <summary>Details</summary>
Motivation: 现有ToM评测局限于虚假信念等窄域任务，无法全面反映人类多维认知机制，亟需更全面、理论扎实的基准来检验LLM是否具备真正类人的ToM能力。

Method: 构建了理论驱动、多范式、双语、大规模的CogToM基准（46种范式、8000+实例），由49名人类标注者验证；对22个代表性LLM进行系统评测，并基于人类认知模式开展对比分析。

Result: 前沿模型（如GPT-5.1、Qwen3-Max）在CogToM上表现出显著性能异质性，特定ToM维度（如高阶意向推理、反事实推理）仍存在明显瓶颈；分析表明LLM的认知结构可能与人类存在系统性差异。

Conclusion: CogToM为评估和理解LLM的ToM能力提供了更全面、可靠的工具与视角，揭示当前LLM尚未真正掌握类人ToM，其认知机制与人类存在根本差异。

Abstract: Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.

</details>


### [159] [Agentic AI Governance and Lifecycle Management in Healthcare](https://arxiv.org/abs/2601.15630)
*Chandra Prakash,Mary Lind,Avneesh Sisodia*

Main category: cs.AI

TL;DR: 本文提出了一种面向医疗健康场景的统一智能体生命周期管理（UALM）蓝图，以应对AI智能体在医疗机构中泛滥带来的治理挑战，涵盖身份注册、跨域编排、隐私保护上下文、运行时策略控制及全生命周期管理五层控制平面，并配套成熟度模型支持分阶段落地。


<details>
  <summary>Details</summary>
Motivation: 医疗组织正将具身AI嵌入日常流程，但随之出现智能体泛滥问题，包括重复建设、权责不清、管控不一及权限残留等，而现有AI治理框架缺乏对智能体集群日常运营的实操指导。

Method: 基于快速、实践导向的综合分析，整合AI治理标准、智能体安全文献与医疗合规要求，提出UALM蓝图及其五层控制平面架构，并设计配套成熟度模型。

Result: 提出了可实施的UALM蓝图，明确五个控制平面层，支持审计就绪的监管，兼顾本地创新与安全规模化扩展；并提供阶段性采纳的成熟度模型。

Conclusion: UALM为医疗CIO、CISO及临床领导者提供了兼顾合规性、安全性与灵活性的智能体治理范式，是应对医疗AI规模化部署中治理挑战的可行路径。

Abstract: Healthcare organizations are beginning to embed agentic AI into routine workflows, including clinical documentation support and early-warning monitoring. As these capabilities diffuse across departments and vendors, health systems face agent sprawl, causing duplicated agents, unclear accountability, inconsistent controls, and tool permissions that persist beyond the original use case. Existing AI governance frameworks emphasize lifecycle risk management but provide limited guidance for the day-to-day operations of agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint derived from a rapid, practice-oriented synthesis of governance standards, agent security literature, and healthcare compliance requirements. UALM maps recurring gaps onto five control-plane layers: (1) an identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning linked to credential revocation and audit logging. A companion maturity model supports staged adoption. UALM offers healthcare CIOs, CISOs, and clinical leaders an implementable pattern for audit-ready oversight that preserves local innovation and enables safer scaling across clinical and administrative domains.

</details>


### [160] [Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models](https://arxiv.org/abs/2601.15652)
*Manish Bhatt*

Main category: cs.AI

TL;DR: 本文提出了一种基于神经科学启发信号设计与监督学习结合的轻量级、可解释的大语言模型幻觉检测框架，显著提升了检测性能与效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）中的幻觉问题仍是高风险场景部署的关键障碍；现有检测方法依赖计算昂贵的外部检索或参数量巨大、不可解释的黑箱LLM判别器。

Method: 提出一种混合检测框架，融合预测编码（衡量对内部先验的‘惊奇度’）和信息瓶颈（衡量扰动下信号保留能力）等神经科学启发的可解释信号，并引入实体聚焦采纳、上下文一致性与可证伪性得分三个关键增强模块。

Result: 在HaluBench数据集上，改进特征使AUROC达0.8669（较基线提升4.95%），仅用200训练样本（比Lynx少75倍）、推理速度快1000倍（5ms vs 5s），且模型参数<1M、完全可解释；同时发现‘合理化’信号无效，揭示LLM存在‘谄媚式推理’现象。

Conclusion: 将领域知识嵌入信号架构可显著提升数据效率与可解释性，优于单纯扩大LLM判别器规模，为生产环境提供轻量、可靠、透明的幻觉检测方案。

Abstract: Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims).
  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises ("Sycophancy").
  This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.

</details>


### [161] [Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats](https://arxiv.org/abs/2601.15679)
*Ee Wei Seah,Yongsen Zheng,Naga Nikshith,Mahran Morsidi,Gabriel Waikin Loh Matienzo,Nigel Gay,Akriti Vij,Benjamin Chua,En Qi Ng,Sharmini Johnson,Vanessa Wilfred,Wan Sie Lee,Anna Davidson,Catherine Devine,Erin Zorer,Gareth Holvey,Harry Coppock,James Walpole,Jerome Wynee,Magda Dubois,Michael Schmatz,Patrick Keane,Sam Deverett,Bill Black,Bo Yan,Bushra Sabir,Frank Sun,Hao Zhang,Harriet Farlow,Helen Zhou,Lingming Dong,Qinghua Lu,Seung Jang,Sharif Abuadbba,Simon O'Callaghan,Suyu Ma,Tom Howroyd,Cyrus Fung,Fatemeh Azadi,Isar Nejadgholi,Krishnapriya Vishnubhotla,Pulei Xiong,Saeedeh Lohrasbi,Scott Buffett,Shahrear Iqbal,Sowmya Vajjala,Anna Safont-Andreu,Luca Massarelli,Oskar van der Wal,Simon Möller,Agnes Delaborde,Joris Duguépéroux,Nicolas Rolin,Romane Gallienne,Sarah Behanzin,Tom Seimandi,Akiko Murakami,Takayuki Semitsu,Teresa Tsukiji,Angela Kinuthia,Michael Michie,Stephanie Kasaon,Jean Wangari,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim*

Main category: cs.AI

TL;DR: 本文介绍了国际先进AI测量、评估与科学网络（INAAIMES）开展的第三次联合测试活动，旨在推动AI智能体评估的科学发展，重点关注跨语言文化适应性、敏感信息泄露、欺诈及网络安全等共性风险，并聚焦于评估方法论的完善而非模型性能本身。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI系统和智能体能力的快速提升，其在现实世界中的交互带来新的风险，而当前智能体评估仍处于初级阶段，缺乏统一、可靠的方法论，尤其在多语言、多文化背景下的安全性与准确性亟需系统性评估。

Method: 由来自新加坡、日本、澳大利亚、加拿大、欧盟委员会、法国、肯尼亚、韩国和英国的代表共同参与，围绕两大风险方向（敏感信息泄露与欺诈、网络安全）开展联合测试；采用开源与闭源模型，在多个公开智能体基准任务上进行评估；重点分析评估过程中的方法论问题。

Result: 识别出当前智能体评估中存在的重要方法论挑战，初步形成跨国家/地区协作推进评估科学化的共识与实践路径。

Conclusion: 本次联合测试是构建稳健、可复现、跨文化适配的智能体评估体系的关键一步，强调需优先发展评估方法论，为全球AI治理提供科学基础。

Abstract: The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely.
  To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations.
  This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems.
  The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.

</details>


### [162] [From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2601.15690)
*Jiaxin Zhang,Wendi Cui,Zhuohang Li,Lifu Huang,Bradley Malin,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: This survey explores how uncertainty in Large Language Models (LLMs) is evolving from a passive diagnostic tool to an active control signal—guiding reasoning, autonomous agent decisions, and reinforcement learning—enhancing reliability and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: The unreliability of LLMs poses a critical barrier to their deployment in high-stakes domains; this survey addresses the need for more robust, trustworthy AI by rethinking uncertainty's role.

Method: The paper conducts a functional survey, categorizing and analyzing recent advances where uncertainty serves as an active control signal across advanced reasoning, autonomous agents, and reinforcement learning, grounded in Bayesian methods and Conformal Prediction.

Result: Uncertainty is shown to effectively guide self-correction, tool-use decisions, information seeking, reward hacking mitigation, and intrinsic reward-driven self-improvement.

Conclusion: Treating uncertainty as an active control signal—not just a diagnostic metric—is essential for building scalable, reliable, and trustworthy next-generation AI systems.

Abstract: While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \textbf{advanced reasoning} to optimize computation and trigger self-correction; in \textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.

</details>


### [163] [Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning](https://arxiv.org/abs/2601.16163)
*Moo Jin Kim,Yihuai Gao,Tsung-Yi Lin,Yen-Chen Lin,Yunhao Ge,Grace Lam,Percy Liang,Shuran Song,Ming-Yu Liu,Chelsea Finn,Jinwei Gu*

Main category: cs.AI

TL;DR: 本文提出Cosmos Policy，一种通过单阶段微调预训练视频模型（Cosmos-Predict2）直接生成机器人动作的简单策略学习方法，无需修改网络结构，同时支持未来状态与价值预测，实现高效规划，在仿真与真实世界任务中均达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频生成模型的机器人策略学习方法需多阶段后训练和新增架构组件，复杂度高；本文旨在以更简洁方式利用视频模型强大的时空先验知识。

Method: 将机器人动作、未来状态图像及价值函数统一编码为视频扩散模型潜在空间中的‘latent frames’，仅通过单阶段在目标平台演示数据上微调预训练视频模型Cosmos-Predict2，不改变其原始架构。

Result: 在LIBERO（98.5%）和RoboCasa（67.1%）仿真基准及真实双臂操作任务中均取得最高平均成功率，显著优于从头训练的扩散策略、其他视频模型策略及视觉语言动作模型；并支持基于rollout数据的在线世界模型与价值函数优化。

Conclusion: Cosmos Policy证明了仅用单阶段微调即可高效迁移视频大模型的时空建模能力至机器人控制，兼顾简洁性、泛化性与规划能力，为具身智能提供了一种新范式。

Abstract: Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/

</details>


### [164] [Agentic Uncertainty Quantification](https://arxiv.org/abs/2601.15703)
*Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 本文提出了一种名为Dual-Process Agentic UQ（AUQ）的新型不确定性量化框架，通过结合隐式记忆（System 1）与目标导向的反思机制（System 2），将语言化不确定性转化为双向控制信号，从而缓解AI代理在长程推理中因早期错误引发的‘幻觉螺旋’问题。该方法无需训练，在多个闭环与开放研究任务上展现出更优性能与轨迹级校准能力。


<details>
  <summary>Details</summary>
Motivation: AI代理在长程推理中易受早期认知错误影响，导致不可逆传播的‘幻觉螺旋’；现有不确定性量化方法仅被动诊断风险，而自反思机制又常陷入无目的或持续修正，缺乏主动干预能力。

Method: 提出双过程AUQ框架：System 1为不确定性感知记忆（UAM），隐式传播置信度与语义解释以避免盲目决策；System 2为不确定性感知反思（UAR），利用前述解释作为理性线索，在必要时触发针对性推理时修正。二者协同实现高效执行与深度思辨的动态平衡。

Result: 在闭环基准与开放式深度研究任务中，该训练免费方法显著提升性能与轨迹级不确定性校准效果，优于现有UQ与反思方法。

Conclusion: AUQ提供了一种原理清晰、无需训练、可即插即用的可靠性增强范式，为构建可信AI代理迈出关键一步。

Abstract: Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.

</details>


### [165] [Improving Methodologies for LLM Evaluations Across Global Languages](https://arxiv.org/abs/2601.15706)
*Akriti Vij,Benjamin Chua,Darshini Ramiah,En Qi Ng,Mahran Morsidi,Naga Nikshith Gangarapu,Sharmini Johnson,Vanessa Wilfred,Vikneswaran Kumaran,Wan Sie Lee,Wenzhuo Yang,Yongsen Zheng,Bill Black,Boming Xia,Frank Sun,Hao Zhang,Qinghua Lu,Suyu Ma,Yue Liu,Chi-kiu Lo,Fatemeh Azadi,Isar Nejadgholi,Sowmya Vajjala,Agnes Delaborde,Nicolas Rolin,Tom Seimandi,Akiko Murakami,Haruto Ishi,Satoshi Sekine,Takayuki Semitsu,Tasuku Sasaki,Angela Kinuthia,Jean Wangari,Michael Michie,Stephanie Kasaon,Hankyul Baek,Jaewon Noh,Kihyuk Nam,Sang Seo,Sungpil Shin,Taewhi Lee,Yongsu Kim,Daisy Newbold-Harrop,Jessica Wang,Mahmoud Ghanem,Vy Hong*

Main category: cs.AI

TL;DR: 本文介绍了一项由国际先进AI测量、评估与科学网络发起的多语言AI安全评估实验，测试了两个开源模型在十种语言下的安全性表现，并提出了改进多语言安全评估的方法论建议。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型在全球范围部署，确保其在不同语言和文化背景下的安全性和可靠性至关重要。现有模型的安全防护措施在多语言环境中的有效性尚不明确，因此需要开展系统性多语言评估。

Method: 由新加坡AISI牵头，来自多个国家和地区的研究人员合作，对两个开源模型在十种语言（包括高资源与低资源语言）下进行安全评估；使用6000多个新翻译的提示词，在五个危害类别上测试，并采用LLM-as-a-judge与人工标注两种评估方式。

Result: 发现模型安全行为在不同语言和危害类型间存在显著差异；评估者（LLM vs. 人工）的可靠性也因语言而异；同时提炼出若干方法论洞见，如需文化适配的翻译、压力测试的评估提示及更清晰的人工标注指南。

Conclusion: 该工作是构建统一多语言AI安全测试框架的初步尝试，强调跨语言安全评估的复杂性，并呼吁研究界与产业界持续协作推进。

Abstract: As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation.
  The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.

</details>


### [166] [AgentSM: Semantic Memory for Agentic Text-to-SQL](https://arxiv.org/abs/2601.15709)
*Asim Biswal,Chuan Lei,Xiao Qin,Aodong Li,Balakrishnan Narayanaswamy,Tim Kraska*

Main category: cs.AI

TL;DR: 本文提出Agent Semantic Memory (AgentSM)框架，通过构建和利用可解释的语义记忆来提升LLM-based Text-to-SQL在企业级复杂场景下的效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based Text-to-SQL系统在真实企业环境中面临大规模复杂Schema、多SQL方言及高成本多步推理等挑战；现有智能体方法存在低效、不稳定（如重复数据库交互、输出不一致、生成无效答案）等问题。

Method: 提出AgentSM框架，以结构化程序形式显式建模并复用历史执行轨迹（或合成高质量轨迹）作为语义记忆，替代原始草稿或向量检索，实现推理路径的系统性重用。

Result: 在Spider 2.0上平均token消耗降低25%，轨迹长度缩短35%；在Spider 2.0 Lite上执行准确率达44.8%，达当前最优水平。

Conclusion: AgentSM通过可解释、结构化的语义记忆机制，显著提升了Text-to-SQL智能体在复杂真实场景下的推理效率、稳定性和准确性。

Abstract: Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.

</details>


### [167] [Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling](https://arxiv.org/abs/2601.15717)
*Luyao Zhu,Fangfang Zhang,Yi Mei,Mengjie Zhang*

Main category: cs.AI

TL;DR: 本文系统研究了遗传规划（GP）在动态柔性作业车间调度（DFJSS）中演化调度规则的跨类型泛化能力，发现训练与测试实例在作业数、机器数、利用率及决策点分布等方面的匹配程度显著影响泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有GP方法通常仅在同构（仅随机种子不同）的DFJSS实例上训练和测试，其跨结构类型泛化能力未被充分探索。

Method: 通过多维度实验（问题规模、关键参数如利用率、数据分布）评估GP演化规则在不同DFJSS实例类型上的泛化表现，并分析决策点数量与分布对性能的影响。

Result: GP规则在训练实例作业数多于测试实例且机器数固定时泛化较好；训练与测试实例规模或参数相近时泛化更优；决策点分布相似性是解释泛化差异的关键因素。

Conclusion: GP在DFJSS中的泛化能力高度依赖于实例结构性特征的一致性，需设计更具鲁棒性和通用性的GP演化策略以应对异构DFJSS场景。

Abstract: Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.

</details>


### [168] [Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity](https://arxiv.org/abs/2601.15728)
*Hangle Hu,Chenyu Hou,Bin Cao,Ruizhe Li*

Main category: cs.AI

TL;DR: 本文提出BIRD-Python基准与Logic Completion Framework（LCF），揭示Text-to-Python性能落后主因是领域上下文缺失而非生成能力不足，补全后可达Text-to-SQL水平，验证Python作为分析智能体基础的可行性。


<details>
  <summary>Details</summary>
Motivation: 现实分析日益依赖Python/Pandas等通用编程语言处理文件数据和复杂流程，但Text-to-Python在核心数据检索上的可靠性远不如成熟的Text-to-SQL生态，亟需跨范式评估基准与方法。

Method: 构建去噪、语义对齐的BIRD-Python基准；提出Logic Completion Framework（LCF），通过注入潜在领域知识来消解自然语言中隐含逻辑的歧义。

Result: 实验表明：(1) Text-to-Python性能差距主因是领域上下文缺失；(2) 引入LCF后，其性能可与Text-to-SQL持平。

Conclusion: Python可作为分析智能体的可行基础，前提是系统能将模糊自然语言有效锚定到可执行的逻辑规范上。

Abstract: While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.

</details>


### [169] [PhysProver: Advancing Automatic Theorem Proving for Physics](https://arxiv.org/abs/2601.15737)
*Hanning Zhang,Ruida Wang,Rui Pan,Wenyuan Wang,Bingxu Meng,Tong Zhang*

Main category: cs.AI

TL;DR: 本文提出了首个面向物理学领域的形式化定理证明方法PhysProver，构建了专用数据集PhysLeanData，并基于DeepSeek-Prover-V2-7B与可验证奖励的强化学习（RLVR）进行训练；仅用约5K样本即在多个物理子领域提升2.4%，并在MiniF2F-Test上泛化提升1.3%，验证了该范式向非数学领域拓展的有效性与高效性。


<details>
  <summary>Details</summary>
Motivation: 现有形式化推理研究集中于数学领域，而物理学同样依赖严格的定理证明与问题求解框架，但尚未得到充分关注。

Method: 构建专用物理形式化数据集PhysLeanData（含PhysLean采样定理与猜想驱动生成数据），以DeepSeek-Prover-V2-7B为基座模型，采用强化学习与可验证奖励（RLVR）训练PhysProver。

Result: PhysProver在多个物理子领域整体提升2.4%；在MiniF2F-Test基准上提升1.3%，显示跨领域泛化能力；仅需约5K训练样本即达显著效果。

Conclusion: 本工作首次将形式化定理证明系统成功拓展至物理学领域，验证了该方法的高效性与可迁移性，为其他科学领域的形式化推理提供了新范式，并将开源数据集与模型。

Abstract: The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\sim$5K training samples, PhysProver achieves an overall 2.4\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.

</details>


### [170] [Tabular Incremental Inference](https://arxiv.org/abs/2601.15751)
*Xinda Chen,Xing Zhen,Hanyu Zhang,Weimin Tan,Bo Yan*

Main category: cs.AI

TL;DR: 本文提出了表格增量推理（TabII）新任务，旨在使已训练模型能在推理阶段动态加入新列，并基于信息瓶颈理论设计了结合大语言模型、预训练TabAdapter和增量样本压缩模块的方法，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统AI模型在固定列的表格上训练后难以应对列结构动态变化的实际场景，需新的无监督方法来高效处理动态表格。

Method: 基于信息瓶颈理论建模TabII为优化问题，提出包含大语言模型占位符、预训练TabAdapter（提供外部知识）和增量样本压缩模块（提炼增量列的任务相关信息）的方法。

Result: 在八个公开数据集上的实验表明，TabII能有效利用增量属性，性能达到当前最优（state-of-the-art）。

Conclusion: TabII为动态表格推理提供了可行且高效的无监督框架，显著提升了AI模型在真实多变表格场景中的实用性与适应性。

Abstract: Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.

</details>


### [171] [Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning](https://arxiv.org/abs/2601.15761)
*Xiefeng Wu,Mingyu Hu,Shu Zhang*

Main category: cs.AI

TL;DR: 本文提出SigEnt-SAC，一种仅需单条专家轨迹即可从零开始学习的离线到在线强化学习方法，通过引入Sigmoid约束的熵项缓解Q函数振荡与OOD动作问题，在D4RL和多个真实机器人任务中展现出高样本效率与强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中部署强化学习面临样本效率低、奖励稀疏、视觉观测噪声大等挑战；现有方法（如离线到在线学习、VLA辅助RL）依赖大规模数据或预训练，缺乏低成本、低数据需求的实用方案。

Method: 提出SigEnt-SAC：一种带Sigmoid约束熵正则项的off-policy actor-critic算法，该熵项防止负熵驱动模型选择分布外动作，并抑制Q函数振荡；仅使用单条专家轨迹进行初始化和引导。

Result: 在D4RL基准上显著缓解Q函数振荡，比现有方法更快达到100%成功率；在四个真实机器人任务（基于原始图像与稀疏奖励）中，仅需少量真实交互即能学会成功策略。

Conclusion: SigEnt-SAC为现实世界强化学习提供了一种低数据依赖、高稳定性、易部署的新范式，推动了轻量级、实用化RL落地。

Abstract: Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.

</details>


### [172] [Agentic Confidence Calibration](https://arxiv.org/abs/2601.15778)
*Jiaxin Zhang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.AI

TL;DR: 本文提出了首个面向AI智能体的置信度校准问题——'智能体置信度校准'，并设计了'整体轨迹校准（HTC）'框架，通过提取智能体完整执行轨迹中的多层次过程特征，实现更可靠、可解释、可迁移且泛化能力强的置信度校准。


<details>
  <summary>Details</summary>
Motivation: 现有校准方法针对静态单轮输出设计，无法应对智能体系统特有的挑战：轨迹中误差累积、外部工具引入的不确定性、失败模式不透明等，导致其在高风险场景中因过度自信而难以部署。

Method: 提出Holistic Trajectory Calibration（HTC）框架，从宏观动态到微观稳定性，提取智能体整个执行轨迹的丰富过程级特征；采用简单可解释模型进行建模，并构建通用智能体校准器（GAC）以支持跨域泛化。

Result: HTC在8个基准、多种大语言模型及不同智能体框架上，校准性能（如ECE）和判别能力均持续超越强基线；在跨域GAIA基准上取得最低ECE；具备可解释性、无需微调的跨域迁移能力与良好泛化性。

Conclusion: 本文确立了以‘过程为中心’的智能体置信度校准新范式，为诊断与提升AI智能体可靠性提供了系统性框架。

Abstract: AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.

</details>


### [173] [Creativity in the Age of AI: Rethinking the Role of Intentional Agency](https://arxiv.org/abs/2601.15797)
*James S. Pearson,Matthew J. Dennis,Marc Cheong*

Main category: cs.AI

TL;DR: 本文质疑创造性定义中‘有意图的能动性’（IAC）这一必要条件，指出其在生成式AI背景下已不再适用，并主张以‘一致性要求’（即稳定产出新颖且有价值成果的能力）取而代之，同时承认IAC在特定局部领域仍有保留价值。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，传统上将‘有意图的能动性’视为创造力必要条件的观点面临挑战；作者旨在重新评估并修正该概念，使其更符合当前技术现实与社会功能需求。

Method: 结合语料库语言学证据（分析作者与记者对AI创造力的语言使用）与概念工程方法，批判性检验IAC的描述效力与社会功能。

Result: 发现公众日益接受将创造力归于无意图的AI系统；IAC已无法有效促进对新颖有价值成果的识别与鼓励，反而加剧评估偏见；提出以‘一致性要求’替代IAC作为一般性标准。

Conclusion: IAC不应作为创造力的一般必要条件，而应被‘一致性要求’取代；但在某些特定语境（如人类教育、艺术评价）中仍可保留其规范性作用。

Abstract: Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.

</details>


### [174] [VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management](https://arxiv.org/abs/2601.15798)
*Zhikai Xue,Tianqianjin Lin,Pengwei Yan,Ruichun Wang,Yuxin Liu,Zhuoren Jiang,Xiaozhong Liu*

Main category: cs.AI

TL;DR: 本文提出VitalDiagnosis，一个基于大语言模型（LLM）的慢性病管理生态系统，通过融合可穿戴设备连续数据与LLM推理能力，实现从被动监测到主动交互式干预的转变，提升患者自我管理能力并减轻临床负担。


<details>
  <summary>Details</summary>
Motivation: 慢性病已成为全球首要死因，医疗资源紧张与人口老龄化加剧了这一挑战；患者个体难以识别病情恶化早期信号或坚持治疗方案。

Method: 构建VitalDiagnosis系统，整合可穿戴设备的连续生理数据，利用大语言模型进行上下文感知的异常触发分析、生成初步洞察，并嵌入医患协作工作流中提供个性化指导。

Result: 该系统能实时响应急性健康异常并支持日常依从性管理，推动更主动、协作的慢病照护范式。

Conclusion: VitalDiagnosis有望提升患者自我管理能力，减少可避免的临床工作量，为智能慢病管理提供新路径。

Abstract: Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.

</details>


### [175] [Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification](https://arxiv.org/abs/2601.15808)
*Yuxuan Wan,Tianqing Fang,Zaitang Li,Yintong Huo,Wenxuan Wang,Haitao Mi,Dong Yu,Michael R. Lyu*

Main category: cs.AI

TL;DR: 本文提出DeepVerifier，一种基于评分标准的验证器，通过在推理时自我评估和迭代改进来提升深度研究代理（DRA）性能，无需额外训练，并发布4K规模的开源微调数据集。


<details>
  <summary>Details</summary>
Motivation: 现有DRA研究多依赖后训练提升策略能力，本文旨在探索不依赖训练、仅通过推理时自我验证与反馈实现自我演化的替代范式。

Method: 构建DRA失效分类法，据此设计评分标准；开发DeepVerifier验证器，利用验证不对称性进行结果奖励建模；在测试时将验证器作为即插即用模块，生成基于评分标准的反馈以引导代理迭代优化输出。

Result: DeepVerifier在元评估F1分数上比基线方法高12%-48%；在GAIA和XBench-DeepResearch挑战子集上带来8%-11%准确率提升；并开源包含4646条高质量步骤的DeepVerifier-4K监督微调数据集。

Conclusion: 推理时验证可有效驱动DRA自我演化，DeepVerifier为无需训练的测试时扩展提供了新路径，并推动开源模型验证能力的发展。

Abstract: Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

</details>


### [176] [ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models](https://arxiv.org/abs/2601.15812)
*Shir Ashury-Tahan,Yifan Mai,Elron Bandel,Michal Shmueli-Scheuer,Leshem Choshen*

Main category: cs.AI

TL;DR: 本文提出了ErrorMap方法，用于系统性地分析大语言模型（LLM）失败的根本原因，而非仅关注错误结果；通过在35个数据集和83个模型上的应用，构建了ErrorAtlas错误分类体系，揭示了如输出遗漏关键细节、问题误读等被忽视的典型错误类型，并开源了该分类与代码。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试只能指出模型何时失败，却无法解释为何失败——错误可能源于格式问题、计算错误或数据噪声，而非真正的推理缺陷；缺乏对失败原因的解耦导致基准不完整，难以有效指导模型改进。

Method: 提出ErrorMap方法，自动提取模型在各任务中的‘失败签名’，识别并归类错误来源；统一应用于任意模型与数据集；基于大规模实证（35数据集×83模型）构建ErrorAtlas错误分类学。

Result: 构建了首个系统性LLM错误分类体系ErrorAtlas，识别出当前研究中被低估的关键错误类型（如必要细节遗漏、问题误读）；验证了ErrorMap可跨模型/任务泛化，支持更深层、可比的评估。

Conclusion: ErrorMap与ErrorAtlas将评估焦点从‘是否成功’转向‘为何失败’，填补了LLM评估的深层归因空白，为模型调试、基准设计与选型提供了可解释、可扩展的新范式。

Abstract: Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique "failure signature", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.

</details>


### [177] [EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience](https://arxiv.org/abs/2601.15876)
*Taofeng Xue,Chong Peng,Mianqiu Huang,Linsen Guo,Tiancheng Han,Haozhe Wang,Jianing Wang,Xiaocheng Zhang,Xin Yang,Dengchang Zhao,Jinrui Ding,Xiandi Ma,Yuchen Xie,Peng Pei,Xunliang Cai,Xipeng Qiu*

Main category: cs.AI

TL;DR: 本文提出了EvoCUA，一种原生计算机使用代理模型，通过自我维持的进化循环（结合数据生成与策略优化）克服静态数据扩展的瓶颈，利用可验证的合成引擎生成多样化任务和执行验证器，并设计可扩展基础设施支持大规模异步沙箱运行，最终在OSWorld基准测试中达到56.7%的成功率，刷新开源SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态数据集被动模仿的方法难以捕捉长周期计算机任务中的复杂因果动态，导致原生计算机使用代理的发展受限于静态数据扩展瓶颈。

Method: 提出EvoCUA模型，构建包含可验证合成引擎（自动生成任务及执行验证器）、大规模异步沙箱 rollout 基础设施、以及基于能力边界识别的迭代演化学习策略（强化成功行为、将失败轨迹转化为带错误分析与自修正的监督信号）的闭环进化框架。

Result: 在OSWorld基准上达到56.7%成功率，显著优于OpenCUA-72B（45.0%）和UI-TARS-2（53.1%），且该演化范式在不同规模基础模型上均展现出一致性能提升。

Conclusion: 基于经验学习的演化范式为原生代理能力发展提供了稳健、可扩展的新路径，突破了静态数据驱动方法的局限性。

Abstract: The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.

</details>


### [178] [ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search](https://arxiv.org/abs/2601.15931)
*Xiangyu Wang,Zhixin Lv,Yongjiao Sun,Anrui Han,Ye Yuan,Hangxu Ji*

Main category: cs.AI

TL;DR: 本文提出ICON框架，通过因果与拓扑先验解决文本驱动行人搜索中因被动观察导致的伪相关与空间语义错位问题，提升模型在开放世界场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练模型的文本驱动行人搜索方法在复杂开放世界场景中迁移效果差，被动观察范式引发多重伪相关和空间语义错位，缺乏对分布偏移的鲁棒性。

Method: 提出ICON框架：1）规则引导的空间干预，抑制边界框噪声敏感性；2）反事实上下文解耦，通过语义驱动的背景移植消除背景干扰；3）显著性驱动的语义正则化，自适应掩码缓解局部显著性偏差；4）神经符号拓扑对齐，利用神经符号先验约束特征匹配的拓扑一致性。

Result: ICON在标准基准上保持领先性能，并在遮挡、背景干扰和定位噪声下展现出卓越鲁棒性。

Conclusion: ICON推动文本驱动行人搜索从拟合统计共现转向学习因果不变性，显著提升了模型在真实监控场景中的泛化能力与可靠性。

Abstract: Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on "Passive Observation" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.

</details>


### [179] [Natural Language-Driven Global Mapping of Martian Landforms](https://arxiv.org/abs/2601.15949)
*Yiran Wang,Shuoyuan Wang,Zhaoran Wei,Jiannan Zhao,Zhonghua Yao,Zejian Xie,Songxin Zhang,Jun Huang,Bingyi Jing,Hongxin Wei*

Main category: cs.AI

TL;DR: MarScope 是一个面向火星的视觉-语言框架，通过将图像与文本对齐到共享语义空间，支持自然语言驱动、无需预定义标签的全球尺度地貌映射，查询响应快（5秒）、精度高（F1达0.978），并拓展至过程分析与相似性地貌制图。


<details>
  <summary>Details</summary>
Motivation: 行星表面通常用高层语义概念描述，但轨道影像档案仍以像素级组织，导致可扩展、开放式的地表探索受限。

Method: 构建 MarScope 框架，基于超20万组精心筛选的图像-文本对，在共享语义空间中对齐火星图像与自然语言描述，实现标签无关的语义检索。

Result: 支持全火星任意自然语言查询（5秒内响应），F1分数最高达0.978；可超越形态分类，支撑过程导向分析与相似性地貌制图。

Conclusion: MarScope 建立了以自然语言为直接接口的新范式，推动大规模地理空间数据的科学发现。

Abstract: Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.

</details>


### [180] [Decoupling Return-to-Go for Efficient Decision Transformer](https://arxiv.org/abs/2601.15953)
*Yongyi Wang,Hanyu Liu,Lingfeng Li,Bozhou Chen,Ang Li,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 本文提出Decoupled DT (DDT)，通过仅使用最新Return-to-Go（RTG）指导动作预测，简化Decision Transformer（DT）架构，消除RTG序列冗余，从而提升性能并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: Decision Transformer（DT）在离线强化学习中依赖整个Return-to-Go（RTG）序列，但作者发现只有最新RTG影响动作预测，其余RTG存在理论冗余且损害性能。

Method: 提出Decoupled DT（DDT），将Transformer仅用于建模观测与动作序列，而将最新RTG作为条件信号单独引入动作预测过程，解耦RTG序列输入。

Result: DDT在多个离线RL任务上显著优于原始DT，并与当前最优DT变体具有竞争力，同时降低了计算成本。

Conclusion: RTG序列输入是DT中的冗余设计；DDT通过解耦RTG使用方式，在保持甚至提升性能的同时提升了效率，为序列化离线RL建模提供了更简洁有效的范式。

Abstract: The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.

</details>


### [181] [Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment](https://arxiv.org/abs/2601.16027)
*Yiran Qiao,Xiang Ao,Jing Chen,Yang Liu,Qiwei Zhong,Qing He*

Main category: cs.AI

TL;DR: 本文提出CS-VAR模型，利用LLM引导的小型专用模型结合跨会话行为证据进行实时直播风险检测，兼顾准确性、效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 直播平台面临渐进式、跨会话重复出现的复杂风险（如诈骗、协同恶意行为），传统单一会话检测方法难以识别跨流模式，且实时性与可解释性不足。

Method: 提出CS-VAR：小型轻量级会话级风险检测模型，在训练中由大语言模型（LLM）指导；LLM检索并推理跨会话行为证据，将其‘局部到全局’的风险认知蒸馏给小模型，使其具备跨流模式识别与结构化风险评估能力。

Result: 在大规模工业数据集上离线实验及线上验证表明，CS-VAR达到SOTA性能；同时提供可解释、定位精准的风险信号，有效支持真实场景内容审核。

Conclusion: CS-VAR通过LLM增强的检索增强式知识蒸馏，成功平衡了直播风险检测的实时性、准确性与可解释性，为高并发动态场景下的AI安全治理提供了新范式。

Abstract: The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.

</details>


### [182] [Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval](https://arxiv.org/abs/2601.16038)
*Olga Bunkova,Lorenzo Di Fruscia,Sophia Rupprecht,Artur M. Schweidtmann,Marcel J. T. Reinders,Jana M. Weber*

Main category: cs.AI

TL;DR: 本文提出了一种将自然语言查询转化为Cypher图查询（Text2Cypher）的方法，利用反应知识图谱（KG）提升大语言模型（LLM）在化学合成路线规划中的准确性与可靠性，通过对比不同提示策略和引入检查清单式自我修正机制，验证了对齐示例的一次性提示（one-shot）效果最优。


<details>
  <summary>Details</summary>
Motivation: 标准提示方法常导致LLM在化学合成规划中产生幻觉或过时建议，亟需结合结构化知识（如反应知识图谱）来提升其可靠性与可执行性。

Method: 将反应路径检索建模为Text2Cypher生成任务；设计单步与多步检索任务；比较零样本与多种一次样本提示（静态/随机/嵌入对齐示例）；引入检查清单驱动的验证与修正循环；构建可复现的Text2Cypher评估框架。

Result: 对齐嵌入选择的one-shot提示始终表现最佳；检查清单式自我修正主要提升零样本下的可执行性，但在已有优质示例时对检索准确率增益有限。

Conclusion: 将LLM与反应知识图谱通过Text2Cypher方式协同，配合高质量示例和轻量级自我修正，可显著增强合成规划的准确性与实用性；所提评估框架为KG增强型LLM研究提供了标准化基础。

Abstract: Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.

</details>


### [183] [AgriPINN: A Process-Informed Neural Network for Interpretable and Scalable Crop Biomass Prediction Under Water Stress](https://arxiv.org/abs/2601.16045)
*Yue Shi,Liangxiu Han,Xin Zhang,Tam Sobeih,Thomas Gaiser,Nguyen Huu Thuy,Dominik Behrend,Amit Kumar Srivastava,Krishnagopal Halder,Frank Ewert*

Main category: cs.AI

TL;DR: 本文提出AgriPINN，一种结合生物物理作物生长微分方程与深度学习的可解释、可扩展模型，用于在水分胁迫下准确预测作物地上部生物量（AGB），并在精度和计算效率上显著优于现有数据驱动和机理模型。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型缺乏可解释性且易受分布偏移影响，而过程模型（如DSSAT、APSIM、LINTUL5）需大量校准、难以大范围部署；亟需兼顾可解释性、生理一致性和空间可扩展性的新方法。

Method: 提出AgriPINN——一种以作物生长微分方程为可微约束的物理信息神经网络，嵌入深度学习主干；无需监督即可反演LAI、PAR吸收量、RUE和水分胁迫因子等潜变量；在德国397个区域60年历史数据上预训练，并在三年受控水分田间试验中微调。

Result: AgriPINN在AGB预测任务中显著优于ConvLSTM-ViT、SLTF、CNN-Transformer及LINTUL5，RMSE最高降低43%，同时具备更高计算效率；能无监督恢复关键生理变量，保持时空预测鲁棒性。

Conclusion: AgriPINN成功融合深度学习的可扩展性与过程模型的生物物理严谨性，为灌溉规划、产量预测和气候适应提供兼具准确性、可解释性与实用性的新范式。

Abstract: Accurate prediction of crop above-ground biomass (AGB) under water stress is critical for monitoring crop productivity, guiding irrigation, and supporting climate-resilient agriculture. Data-driven models scale well but often lack interpretability and degrade under distribution shift, whereas process-based crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are difficult to deploy over large spatial domains. To address these limitations, we propose AgriPINN, a process-informed neural network that integrates a biophysical crop-growth differential equation as a differentiable constraint within a deep learning backbone. This design encourages physiologically consistent biomass dynamics under water-stress conditions while preserving model scalability for spatially distributed AGB prediction. AgriPINN recovers latent physiological variables, including leaf area index (LAI), absorbed photosynthetically active radiation (PAR), radiation use efficiency (RUE), and water-stress factors, without requiring direct supervision. We pretrain AgriPINN on 60 years of historical data across 397 regions in Germany and fine-tune it on three years of field experiments under controlled water treatments. Results show that AgriPINN consistently outperforms state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to $43\%$) and computational efficiency. By combining the scalability of deep learning with the biophysical rigor of process-based modeling, AgriPINN provides a robust and interpretable framework for spatio-temporal AGB prediction, offering practical value for planning of irrigation infrastructure, yield forecasting, and climate-adaptation planning.

</details>


### [184] [Designing faster mixed integer linear programming algorithm via learning the optimal path](https://arxiv.org/abs/2601.16056)
*Ruizhi Liu,Liming Xu,Xulin Huang,Jingyan Sui,Shizhe Ding,Boyang Xia,Chungong Yu,Dongbo Bu*

Main category: cs.AI

TL;DR: 本文提出DeepBound，一种基于深度学习的节点选择算法，用于加速混合整数线性规划（MILP）求解，通过多级特征融合网络和成对训练范式提升分支定界中关键节点识别能力，在多个NP-hard基准上显著减少求解时间并具备强泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统分支定界中依赖人工启发式策略进行节点选择，性能不稳定且难以泛化；亟需数据驱动方法自动学习高效节点选择策略。

Method: 提出DeepBound：1）构建多级特征融合网络建模节点表示；2）采用成对训练范式缓解分支定界树中节点类别不平衡问题；3）端到端学习优先选择含最优解的子问题节点。

Result: 在三个NP-hard MILP基准上，DeepBound相比传统启发式与现有学习方法显著缩短求解时间，更快获得最优可行解，并在大规模复杂实例上表现出强泛化能力。

Conclusion: DeepBound能自动发现更灵活鲁棒的特征模式，有望替代人工设计的启发式规则，推动MILP求解智能化发展。

Abstract: Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.

</details>


### [185] [Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics](https://arxiv.org/abs/2601.16087)
*Sukesh Subaharan*

Main category: cs.AI

TL;DR: 本文提出了一种外部情感子系统，通过在LLM代理中引入连续的价-唤醒-支配（VAD）情感状态及其一阶/二阶动态更新机制，以提升多轮对话中的时间一致性与可控恢复能力，实验表明二阶动力学带来稳定性与响应性的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型代理在长程交互中缺乏显式的时序结构来维持代理级状态，导致语调和人设突变；而以往工作多关注单轮情感分类，未探索显式情感动力学对长程行为的影响。

Method: 构建一个独立于语言模型的代理级情感子系统，采用Valence-Arousal-Dominance（VAD）连续状态表示，通过固定无记忆估计器提取瞬时情感信号，并利用指数平滑或动量机制进行一阶/二阶时间整合；该情感状态被注入生成过程，不修改模型参数。

Result: 在25轮固定对话协议下，无状态代理无法形成连贯轨迹或恢复能力；一阶状态保持支持延迟响应与可靠恢复；二阶动力学引入情感惯性与滞后效应，且随动量增大而增强，体现出稳定性与响应性的权衡。

Conclusion: 显式建模情感动力学（尤其二阶）可有效提升LLM代理在长程交互中的时间一致性与可控恢复能力，为构建具时序鲁棒性的智能代理提供了新范式。

Abstract: Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.

</details>


### [186] [Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources](https://arxiv.org/abs/2601.16108)
*Marzieh Adeli Shamsabad,Hamed Ghodrati*

Main category: cs.AI

TL;DR: This paper proposes a method to enhance vision-language models (VLMs) for detecting climate disinformation by integrating them with up-to-date external knowledge sources, improving accuracy and real-world applicability.


<details>
  <summary>Details</summary>
Motivation: Climate disinformation—especially misleading images/videos on social media—is hard to detect and delays climate action; existing VLMs lack reasoning about recent events due to static training knowledge.

Method: Combines vision-language models with external knowledge retrieval, including reverse image search results, online fact-checks, and trusted expert content, to assess image-claim veracity.

Result: The proposed approach improves detection of climate-related visual disinformation, enabling more accurate classification (accurate, misleading, false, unverifiable) in dynamic, real-world settings.

Conclusion: Integrating VLMs with timely external knowledge significantly enhances their capability to combat evolving climate disinformation and supports scientific literacy in fast-changing information environments.

Abstract: Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.

</details>


### [187] [LLM Prompt Evaluation for Educational Applications](https://arxiv.org/abs/2601.16134)
*Langdon Holmes,Adam Coscia,Scott Crossley,Joon Suh Choi,Wesley Morris*

Main category: cs.AI

TL;DR: 本文提出一种可推广的、系统化的LLM提示词评估方法，通过锦标赛式评估框架（基于Glicko2评分系统）比较六种教学策略导向的提示模板在生成对话式跟进问题中的效果；结果表明，融合‘角色设定’与‘上下文管理’模式、支持元认知学习的提示模板显著优于其他模板（胜率81%-100%），为教育场景下的证据驱动提示工程提供了方法论范例。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育应用中日益普及，亟需基于实证的方法来设计和评估能生成个性化、教学对齐输出的提示词。

Method: 设计六种融入不同教学策略的提示模板，采用锦标赛式评估框架（Glicko2评分系统），由八名评审者从格式、对话支持性、学习者适切性三维度两两比较问题对，数据来自三个教育部署中的120条真实用户交互。

Result: 强调策略性阅读的提示模板表现最优， pairwise胜率81%–100%；该模板融合persona与context manager模式，专为支持元认知学习（如自主学习）而设计。

Conclusion: 本研究展示了教育技术研究者如何系统化评估与优化提示设计，推动LLM提示工程从经验式转向基于证据的教育应用开发。

Abstract: As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.

</details>


### [188] [Structured Hints for Sample-Efficient Lean Theorem Proving](https://arxiv.org/abs/2601.16172)
*Zachary Burton*

Main category: cs.AI

TL;DR: 本文探讨了在推理阶段为已训练好的神经定理证明器（如DeepSeek-Prover-V1.5）添加简单结构化提示是否能提升性能。作者设计了一个固定提示调度策略，覆盖15种常见战术骨架，在miniF2F基准上实现了pass@16从15.2%提升至21.7%（相对提升43%），表明即使强RL训练模型仍可受益于轻量级结构引导。


<details>
  <summary>Details</summary>
Motivation: 探究高度训练的神经定理证明器在推理时是否仍能从简单的结构化指导中获益，尤其是利用战术语言中的先验结构信息。

Method: 提出一种轻量级推理时干预方法——针对15种常见战术骨架设计固定提示调度策略，并在miniF2F基准上评估其对DeepSeek-Prover-V1.5模型pass@16指标的影响。

Result: 在相同采样数（k=16）与最大生成长度（1024 tokens）下，pass@16从15.2%提升至21.7%，相对提升43%。

Conclusion: 即使经过强化学习充分训练的神经定理证明器，仍未能充分利用战术语言中的结构性先验；简单、低成本的推理时结构引导是一种有效且互补的性能增强手段。

Abstract: State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.

</details>


### [189] [Scalable Board Expansion within a General Game System](https://arxiv.org/abs/2601.16216)
*Clémentine Sacré*

Main category: cs.AI

TL;DR: 本文提出了一种动态棋盘扩展机制，利用通用游戏系统（GGS）在无棋盘类游戏中按需自动扩展游戏区域，避免传统静态大棋盘带来的冗余与复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统无棋盘类游戏常使用预定义的 oversized 静态棋盘，但实际游戏过程中大量区域未被使用，造成不必要的复杂性。

Method: 设计并实现基于通用游戏系统（GGS）的动态棋盘扩展机制，使棋盘在游戏进行中按需自动增长。

Result: 实现了更简洁、高效且适应性强的棋盘管理方式，减少资源浪费与系统复杂度。

Conclusion: 动态扩展机制显著提升了无棋盘类游戏的灵活性与可扩展性，为通用游戏系统提供了新的设计思路。

Abstract: This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [190] [Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals](https://arxiv.org/abs/2601.16091)
*Saar Cohen*

Main category: cs.MA

TL;DR: 本文提出了一种在线非质心聚类的新框架，允许延迟决策以平衡聚类距离成本与延迟成本，并在随机到达模型下设计了常数竞争比算法。


<details>
  <summary>Details</summary>
Motivation: 传统在线聚类要求即时决策，导致最坏情况下的性能下界较差；为突破该限制，本文考虑延迟决策并采用更现实的随机到达模型。

Method: 构建允许延迟分配的在线非质心聚类框架，在点独立同分布到达的随机模型下，设计具有理论保证的在线算法。

Result: 在随机到达模型中，所提算法达到常数竞争比——即当点数趋于无穷时，其期望总成本与最优离线解之比有常数上界。

Conclusion: 延迟机制结合随机建模可显著提升在线聚类的理论性能，为超越最坏情况分析提供了新路径。

Abstract: Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.

</details>


### [191] [Average Unfairness in Routing Games](https://arxiv.org/abs/2601.16187)
*Pan-Yang Su,Arwa Alanqary,Bryce L. Ferguson,Manxi Wu,Alexandre M. Bayen,Shankar Sastry*

Main category: cs.MA

TL;DR: 本文提出了一种新的路由博弈公平性度量——平均不公平性（average unfairness），并系统比较了其与已有两种不公平性度量（loaded unfairness 和 UE unfairness）的关系；证明三者最坏情况值一致且由延迟函数类的陡峭参数刻画；指出平均不公平性始终不大于 loaded 不公平性，且仅在完全公平时相等；进一步证明在相同不公平性约束下，平均不公平性约束下的受限系统最优解比 loaded 不公平性约束下总延迟更低，并给出严格改进的条件与数值验证。


<details>
  <summary>Details</summary>
Motivation: 现有路由博弈中的不公平性度量（如 loaded unfairness 和 UE unfairness）各有局限，缺乏统一视角和理论比较；亟需一种更自然、可解释且具实践指导意义的新度量来刻画公平-效率权衡。

Method: 定义平均不公平性为用户平均延迟与最小延迟之比；通过理论分析（包括极值构造、不等式推导与优化建模）比较三种不公平性度量的性质；建立受不公平性约束的系统最优（CSO）问题，并对平行链路网络与一般网络分别进行解析与数值分析。

Result: 1）三种不公平性度量的最坏情况值一致，由延迟函数类的陡峭参数决定；2）平均不公平性 ≤ loaded 不公平性，等号成立当且仅当流完全公平；3）在相同不公平性容忍水平下，平均不公平性约束下的 CSO 总延迟严格低于 loaded 不公平性约束下的解（平行链路网络中恒成立，一般网络中有充分条件保证）；4）提供数值示例验证理论结论。

Conclusion: 平均不公平性是一种更精细、更具实用价值的公平性度量；它不仅在理论上完善了不公平性度量体系，而且在优化设计中能实现更优的公平-效率平衡，为网络路由中的资源分配与机制设计提供了新依据。

Abstract: We propose average unfairness as a new measure of fairness in routing games, defined as the ratio between the average latency and the minimum latency experienced by users. This measure is a natural complement to two existing unfairness notions: loaded unfairness, which compares maximum and minimum latencies of routes with positive flow, and user equilibrium (UE) unfairness, which compares maximum latency with the latency of a Nash equilibrium. We show that the worst-case values of all three unfairness measures coincide and are characterized by a steepness parameter intrinsic to the latency function class. We show that average unfairness is always no greater than loaded unfairness, and the two measures are equal only when the flow is fully fair. Besides that, we offer a complete comparison of the three unfairness measures, which, to the best of our knowledge, is the first theoretical analysis in this direction. Finally, we study the constrained system optimum (CSO) problem, where one seeks to minimize total latency subject to an upper bound on unfairness. We prove that, for the same tolerance level, the optimal flow under an average unfairness constraint achieves lower total latency than any flow satisfying a loaded unfairness constraint. We show that such improvement is always strict in parallel-link networks and establish sufficient conditions for general networks. We further illustrate the latter with numerical examples. Our results provide theoretical guarantees and valuable insights for evaluating fairness-efficiency tradeoffs in network routing.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [192] [Is Grokipedia Right-Leaning? Comparing Political Framing in Wikipedia and Grokipedia on Controversial Topics](https://arxiv.org/abs/2601.15484)
*Philipp Eibl,Erica Coppolillo,Simone Mungari,Luca Luceri*

Main category: cs.IR

TL;DR: 本文比较了Wikipedia和Grokipedia在政治争议话题上的语义框架、政治倾向和内容优先级，发现两者语义相似性随文章段落递减，且在争议话题上差异更显著；尽管两者总体偏左，Grokipedia呈现更明显的双峰分布，右倾内容更突出。


<details>
  <summary>Details</summary>
Motivation: 在线百科全书（如Wikipedia和Grokipedia）被广泛视为信息基础设施核心，但其意识形态偏差引发持续争议；本文旨在实证检验二者在政治争议话题上的实际偏向差异。

Method: 对Wikipedia与Grokipedia在既定政治争议话题上的条目进行对比分析，量化语义框架差异、政治倾向分布及内容优先级，并比较其在争议性与随机抽样话题间的异同。

Result: 1）两平台语义相似性随文章段落推进而衰减；2）在政治争议话题上语义分歧更强；3）整体均偏左，但Grokipedia呈更显著的双峰分布，右倾内容比例更高。

Conclusion: Grokipedia并非简单的‘右倾替代品’，而是在保持总体左倾基调的同时，增强了右倾表达的可见性与分布离散度，反映出AI生成内容在意识形态表征上的新特征。

Abstract: Online encyclopedias are central to contemporary information infrastructures and have become focal points of debates over ideological bias. Wikipedia, in particular, has long been accused of left-leaning bias, while Grokipedia, an AI-generated encyclopedia launched by xAI, has been framed as a right-leaning alternative. This paper presents a comparative analysis of Wikipedia and Grokipedia on well-established politically contested topics. Specifically, we examine differences in semantic framing, political orientation, and content prioritization. We find that semantic similarity between the two platforms decays across article sections and diverges more strongly on controversial topics than on randomly sampled ones. Additionally, we show that both encyclopedias predominantly exhibit left-leaning framings, although Grokipedia exhibits a more bimodal distribution with increased prominence of right-leaning content. The experimental code is publicly available.

</details>


### [193] [DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion Retrieval and Learned Reranking](https://arxiv.org/abs/2601.15518)
*Wenxin Zhou,Ritesh Mehta,Anthony Miyaguchi*

Main category: cs.IR

TL;DR: 本文提出了一种两阶段检索系统，结合多种互补检索方法与学习型重排序器及大语言模型（LLM）重排序，以应对TREC Tip-of-the-Tongue（ToT）任务。第一阶段采用混合检索（LLM、BM25和BGE-M3），并引入主题感知的多索引稠密检索；第二阶段对比LambdaMART与Gemini-2.5-flash重排序。通过5000条合成ToT查询训练，系统在测试集上达到0.66召回率和0.41 NDCG@1000。


<details>
  <summary>Details</summary>
Motivation: 解决TREC Tip-of-the-Tongue（ToT）任务中用户难以准确描述目标信息所带来的检索挑战，需融合多种检索信号提升召回能力。

Method: 构建两阶段检索系统：第一阶段为混合检索（LLM+BM25+BGE-M3）与主题划分的多索引稠密检索；第二阶段采用LambdaMART与LLM（Gemini-2.5-flash）重排序；使用LLM生成5000条合成ToT查询用于训练。

Result: 最佳系统（混合检索 + Gemini-2.5-flash重排序）在测试集上取得0.66的召回率和0.41的NDCG@1000。

Conclusion: 融合多种检索方法与LLM重排序能显著提升ToT任务的检索效果，主题感知的多索引策略和高质量合成查询对系统性能至关重要。

Abstract: We develop a two-stage retrieval system that combines multiple complementary retrieval methods with a learned reranker and LLM-based reranking, to address the TREC Tip-of-the-Tongue (ToT) task. In the first stage, we employ hybrid retrieval that merges LLM-based retrieval, sparse (BM25), and dense (BGE-M3) retrieval methods. We also introduce topic-aware multi-index dense retrieval that partitions the Wikipedia corpus into 24 topical domains. In the second stage, we evaluate both a trained LambdaMART reranker and LLM-based reranking. To support model training, we generate 5000 synthetic ToT queries using LLMs. Our best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating the effectiveness of fusion retrieval.

</details>


### [194] [Blockchain-Based Spectrum Resource Securitization via Semi-Fungible Token-Lock](https://arxiv.org/abs/2601.15594)
*Zhixian Zhou,Bin Chen,Zhe Peng,Zhiming Liang,Ruijun Wu,Chen Sun,Shuo Wang*

Main category: cs.IR

TL;DR: 本文提出了一种基于半同质化代币锁定（SFT Lock）的区块链频谱证券化新方法，通过锁/解锁机制替代频繁的铸币/销毁操作，在保持NFT身份连续性和历史可追溯性的同时支持频谱资产的分额化持有与流转，并设计了配套智能合约架构与质押机制，在私有以太坊网络实验中显著降低了链上Gas消耗。


<details>
  <summary>Details</summary>
Motivation: 6G网络演进要求频谱资源灵活、动态、高效利用，而现有基于ERC404混合代币模型的方法因频繁铸币/销毁操作破坏代币身份连续性并增加链上开销。

Method: 提出SFT Lock机制，采用锁/解锁代替铸币/销毁，实现确定性状态迁移；设计模块化智能合约架构支持频谱授权、证券化与共享；引入质押机制提升资产流动性。

Result: 在私有以太坊网络实验中，相比ERC404模型，该方法显著节省Gas，同时保持功能正确性与历史可追溯性。

Conclusion: SFT Lock是一种更高效、可持续的区块链频谱证券化方案，兼顾身份连续性、分额化能力与链上效率，为6G频谱治理提供可行技术路径。

Abstract: As 6G networks evolve, spectrum assets require flexible, dynamic, and efficient utilization, motivating blockchain based spectrum securitization. Existing approaches based on ERC404 style hybrid token models rely on frequent minting and burning during asset transfers, which disrupt token identity continuity and increase on chain overhead. This paper proposes the Semi Fungible Token Lock (SFT Lock) method, a lock/unlock based mechanism that preserves NFT identity and historical traceability while enabling fractional ownership and transferability. By replacing mint/burn operations with deterministic state transitions, SFT Lock ensures consistent lifecycle representation of spectrum assets and significantly reduces on chain operations. Based on this mechanism, a modular smart contract architecture is designed to support spectrum authorization, securitization, and sharing, and a staking mechanism is introduced to enhance asset liquidity. Experimental results on a private Ethereum network demonstrate that, compared with ERC404 style hybrid token models, the proposed method achieves substantial gas savings while maintaining functional correctness and traceability.

</details>


### [195] [Enhancing guidance for missing data in diffusion-based sequential recommendation](https://arxiv.org/abs/2601.15673)
*Qilong Yan,Yifei Xing,Dugang Liu,Jingpu Duan,Jian Yin*

Main category: cs.IR

TL;DR: 本文提出了一种名为CARD的新型扩散模型，通过双侧Thompson采样识别用户兴趣转折点，并利用反事实注意力机制动态重加权交互向量，以提升序列推荐中生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有顺序推荐方法在用户序列存在缺失数据时，指导信号质量下降，且忽略关键的兴趣转折点，影响后续意图预测准确性。

Method: 提出CARD模型，包含（1）双侧Thompson采样识别显著兴趣转移序列；（2）针对这些序列设计反事实注意力机制，量化各物品重要性，并动态重加权交互向量以增强扩散模型引导信号。

Result: 实验表明CARD在真实数据上效果良好，且计算开销低。

Conclusion: CARD能有效提升扩散式序列推荐的质量，关键在于精准识别并强化兴趣转折点信号，同时抑制噪声。

Abstract: Contemporary sequential recommendation methods are becoming more complex, shifting from classification to a diffusion-guided generative paradigm. However, the quality of guidance in the form of user information is often compromised by missing data in the observed sequences, leading to suboptimal generation quality. Existing methods address this by removing locally similar items, but overlook ``critical turning points'' in user interest, which are crucial for accurately predicting subsequent user intent. To address this, we propose a novel Counterfactual Attention Regulation Diffusion model (CARD), which focuses on amplifying the signal from key interest-turning-point items while concurrently identifying and suppressing noise within the user sequence. CARD consists of (1) a Dual-side Thompson Sampling method to identify sequences undergoing significant interest shift, and (2) a counterfactual attention mechanism for these sequences to quantify the importance of each item. In this manner, CARD provides the diffusion model with a high-quality guidance signal composed of dynamically re-weighted interaction vectors to enable effective generation. Experiments show our method works well on real-world data without being computationally expensive. Our code is available at https://github.com/yanqilong3321/CARD.

</details>


### [196] [CoNRec: Context-Discerning Negative Recommendation with LLMs](https://arxiv.org/abs/2601.15721)
*Xinda Chen,Jiawei Wu,Yishuang Liu,Jialin Zhu,Shuwen Xiao,Junjun Zheng,Xiangheng Kong,Yuning Jiang*

Main category: cs.IR

TL;DR: 本文提出首个面向负反馈建模的大语言模型框架，通过语义ID表征、物品级对齐任务和渐进式GRPO训练范式，解决负反馈稀疏性与正向偏好主导导致的上下文理解偏差问题，并重新定义负反馈评估目标。


<details>
  <summary>Details</summary>
Motivation: 负反馈建模在推荐系统中重要但被低估，现有方法多将其作为辅助信号，忽视对负兴趣的直接建模，且负反馈数据稀疏导致模型存在正反馈主导的上下文理解偏差。

Method: 提出基于大语言模型的负反馈建模框架，包括语义ID表示替代文本描述、物品级对齐任务增强语义理解、渐进式GRPO训练范式动态平衡正负行为上下文利用，并设计基于多日未来负反馈及协同信号的新奖励函数与评估指标。

Result: 该框架显著提升负反馈建模能力，在多个真实场景中降低负反馈率，验证了直接建模负兴趣的有效性与实用性。

Conclusion: 直接建模用户负兴趣具有重要价值；语义ID与渐进式训练可缓解稀疏性与偏差问题；需重构负反馈预测目标以更贴近用户真实偏好。

Abstract: Understanding what users like is relatively straightforward; understanding what users dislike, however, remains a challenging and underexplored problem. Research into users' negative preferences has gained increasing importance in modern recommendation systems. Numerous platforms have introduced explicit negative feedback mechanisms and leverage such signals to refine their recommendation models. Beyond traditional business metrics, user experience-driven metrics, such as negative feedback rates, have become critical indicators for evaluating system performance. However, most existing approaches primarily use negative feedback as an auxiliary signal to enhance positive recommendations, paying little attention to directly modeling negative interests, which can be highly valuable in offline applications. Moreover, due to the inherent sparsity of negative feedback data, models often suffer from context understanding biases induced by positive feedback dominance. To address these challenges, we propose the first large language model framework for negative feedback modeling with special designed context-discerning modules. We use semantic ID Representation to replace text-based item descriptions and introduce an item-level alignment task that enhances the LLM's understanding of the semantic context behind negative feedback. Furthermore, we design a Progressive GRPO training paradigm that enables the model to dynamically balance the positive and negative behavioral context utilization. Besides, our investigation further reveals a fundamental misalignment between the conventional next-negative-item prediction objective and users' true negative preferences, which is heavily influenced by the system's recommendation order. To mitigate this, we propose a novel reward function and evaluation metric grounded in multi-day future negative feedback and their collaborative signals.

</details>


### [197] [CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval](https://arxiv.org/abs/2601.15849)
*Tsung-Hsiang Chou,Chen-Jui Yu,Shui-Hsiang Hsu,Yao-Chung Fan*

Main category: cs.IR

TL;DR: CGPT是一种利用大语言模型（LLM）生成监督信号来提升表格检索效果的训练框架，通过语义聚类构造多样化部分表格，并用LLM生成合成查询进行难负样本对比微调，在多个基准上显著提升R@1性能。


<details>
  <summary>Details</summary>
Motivation: 通用嵌入模型在表格检索中表现不佳，因表格结构化强导致语义压缩和查询-表格不匹配；现有LLM增强方法依赖启发式部分表格选择，且未将合成查询用于嵌入模型优化。

Method: CGPT首先对表格实例用K-means聚类并跨簇采样构建语义多样的部分表格，再由LLM为其生成合成查询，最后利用这些查询进行难负样本对比微调嵌入模型。

Result: 在MimoTable、OTTQA、FetaQA和E2E-WTQ四个公开基准上，CGPT平均R@1提升16.54%，优于QGpT等基线；在多领域统一语料下展现良好跨域泛化能力，且支持使用更小LLM生成合成查询。

Conclusion: 语义引导的部分表格构建与LLM生成监督下的对比学习，构成了一种高效、可扩展的大规模表格检索新范式。

Abstract: General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at https://github.com/yumeow0122/CGPT.

</details>


### [198] [STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion](https://arxiv.org/abs/2601.15860)
*Shui-Hsiang Hsu,Tsung-Hsiang Chou,Chen-Jui Yu,Yao-Chung Fan*

Main category: cs.IR

TL;DR: 本文提出了STAR框架，通过语义聚类和加权融合提升表格语义表征能力，从而改善自然语言查询与结构化表格之间的对齐效果，在多个基准上超越了QGpT方法。


<details>
  <summary>Details</summary>
Motivation: 表格检索中，非结构化文本与结构化表格之间存在结构和语义差异，导致嵌入对齐困难；现有方法如QGpT依赖粗粒度的部分表格采样和简单融合策略，限制了语义多样性与对齐效果。

Method: STAR框架包含三步：1）基于表头感知的K-means语义聚类，选取代表性行构建多样化部分表格；2）为各簇生成特定合成查询以覆盖表格语义空间；3）采用加权融合策略整合表格与查询嵌入，实现细粒度语义对齐。

Result: 在五个基准数据集上的实验表明，STAR在所有数据集上的Recall均一致优于QGpT，验证了语义聚类与自适应加权融合的有效性。

Conclusion: STAR是一种轻量级、有效的表格语义表征方法，能更好地融合结构化与文本信息，显著提升查询-表格对齐性能。

Abstract: Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR.

</details>


### [199] [MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging](https://arxiv.org/abs/2601.15930)
*Tianjun Wei,Enneng Yang,Yingpeng Du,Huizhong Guo,Jie Zhang,Zhu Sun*

Main category: cs.IR

TL;DR: 本文首次系统研究了模型合并（MM）在生成式推荐（GR）中的应用，提出了一种基于上下文的统一框架MMGRid，通过结构化上下文网格组织多上下文GR模型，并揭示了参数冲突、时序偏差及加权合并等关键问题与解决方案。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型规模大、训练成本高，而现实场景中用户行为随时间演化、应用场景异构，亟需一种无需原始数据和重训练的高效模型集成方法；但模型合并在推荐系统尤其是生成式推荐中的研究尚属空白。

Method: 提出MMGRid框架：构建由共享基础大语言模型（LLM）出发、在不同时间与领域上下文数据上微调所得的GR检查点构成的结构化上下文网格；引入基础模型替换以解耦任务感知与上下文特异性参数变化，并采用加权上下文合并缓解增量训练引发的近期偏差。

Result: 发现GR模型从LLM训练易引发合并时的参数冲突（源于词元分布偏移与目标差异），可通过基础模型替换缓解；增量训练导致近期偏差，加权合并可有效平衡；最优合并权重与上下文依赖的交互特征相关，为实际部署提供权重选择依据。

Conclusion: 模型合并是生成式推荐中可行且有潜力的轻量级部署范式；MMGRid为跨上下文GR模型集成提供了可控实验平台与实用指导原则，推动了MM在推荐系统中的理论理解与工程落地。

Abstract: Model merging (MM) offers an efficient mechanism for integrating multiple specialized models without access to original training data or costly retraining. While MM has demonstrated success in domains like computer vision, its role in recommender systems (RSs) remains largely unexplored. Recently, Generative Recommendation (GR) has emerged as a new paradigm in RSs, characterized by rapidly growing model scales and substantial computational costs, making MM particularly appealing for cost-sensitive deployment scenarios. In this work, we present the first systematic study of MM in GR through a contextual lens. We focus on a fundamental yet underexplored challenge in real-world: how to merge generative recommenders specialized to different real-world contexts, arising from temporal evolving user behaviors and heterogeneous application domains. To this end, we propose a unified framework MMGRid, a structured contextual grid of GR checkpoints that organizes models trained under diverse contexts induced by temporal evolution and domain diversity. All checkpoints are derived from a shared base LLM but fine-tuned on context-specific data, forming a realistic and controlled model space for systematically analyzing MM across GR paradigms and merging algorithms. Our investigation reveals several key insights. First, training GR models from LLMs can introduce parameter conflicts during merging due to token distribution shifts and objective disparities; such conflicts can be alleviated by disentangling task-aware and context-specific parameter changes via base model replacement. Second, incremental training across contexts induces recency bias, which can be effectively balanced through weighted contextual merging. Notably, we observe that optimal merging weights correlate with context-dependent interaction characteristics, offering practical guidance for weight selection in real-world deployments.

</details>


### [200] [Unveiling and Simulating Short-Video Addiction Behaviors via Economic Addiction Theory](https://arxiv.org/abs/2601.15975)
*Chen Xu,Zhipeng Yi,Ruizi Wang,Wenjie Wang,Jun Xu,Maarten de Rijke*

Main category: cs.IR

TL;DR: 本文提出AddictSim框架，结合经济学成瘾理论与推荐系统中的用户隐式行为，分析短视频平台上的成瘾行为模式，并设计多样性感知算法以缓解成瘾行为。


<details>
  <summary>Details</summary>
Motivation: 现有短视频成瘾研究依赖小样本问卷或志愿者数据，存在样本偏差；而平台拥有大规模真实行为数据，亟需有效方法建模成瘾行为。

Method: 融合经济学成瘾理论与推荐系统中用户隐式行为，提出AddictSim模拟器；采用均值到适配（mean-to-adapted）策略及组相对策略优化（group relative policy optimization）进行个性化训练。

Result: AddictSim在两个大规模数据集上显著优于现有训练策略；仿真表明多样性感知算法能有效缓解成瘾行为。

Conclusion: 短视频成瘾具有与传统成瘾相似的功能性模式，可通过基于真实行为数据的可学习模拟器建模，并利用算法设计实现成瘾干预。

Abstract: Short-video applications have attracted substantial user traffic. However, these platforms also foster problematic usage patterns, commonly referred to as short-video addiction, which pose risks to both user health and the sustainable development of platforms. Prior studies on this issue have primarily relied on questionnaires or volunteer-based data collection, which are often limited by small sample sizes and population biases. In contrast, short-video platforms have large-scale behavioral data, offering a valuable foundation for analyzing addictive behaviors. To examine addiction-aware behavior patterns, we combine economic addiction theory with users' implicit behavior captured by recommendation systems. Our analysis shows that short-video addiction follows functional patterns similar to traditional forms of addictive behavior (e.g., substance abuse) and that its intensity is consistent with findings from previous social science studies. To develop a simulator that can learn and model these patterns, we introduce a novel training framework, AddictSim. To consider the personalized addiction patterns, AddictSim uses a mean-to-adapted strategy with group relative policy optimization training. Experiments on two large-scale datasets show that AddictSim consistently outperforms existing training strategies. Our simulation results show that integrating diversity-aware algorithms can mitigate addictive behaviors well.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [201] [Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference](https://arxiv.org/abs/2601.15333)
*Xuanning Hu,Anchen Li,Qianli Xing,Jinglong Ji,Hao Tuo,Bo Yang*

Main category: cs.LG

TL;DR: 本文提出ELILLM框架，将大语言模型（LLM）的生成过程重新解释为编码、潜在空间探索与解码三阶段流程，结合贝叶斯优化与位置感知代理模型，实现对蛋白结构理解不足和分子生成不可控问题的有效解决，在CrossDocked2020基准上显著提升结合亲和力与化学有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在基于结构的药物设计（SBDD）中受限于对蛋白结构理解不足及分子生成不可预测。

Method: 提出Exploration-Augmented Latent Inference for LLMs（ELILLM）框架，将LLM生成建模为编码-潜在空间探索-解码流程；采用贝叶斯优化指导潜在嵌入探索，位置感知代理模型预测结合亲和力分布，知识引导解码确保化学有效性与合成可行性。

Result: 在CrossDocked2020基准上，ELILLM在受控探索能力和结合亲和力得分上均优于七种基线方法。

Conclusion: ELILLM能有效增强LLMs在SBDD任务中的能力，兼顾分子合理性、可合成性与高亲和力。

Abstract: Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.

</details>


### [202] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLM）在多语言场景下的响应质量与文化语境一致性问题，发现低资源语言的响应质量系统性偏低，且语言选择会显著影响模型所采用的文化背景，进而影响回答质量。


<details>
  <summary>Details</summary>
Motivation: 确保用户无论使用何种语言与LLM交互，都能获得同等质量的响应，避免系统性语言偏见；同时探究语言与文化在LLM中的纠缠关系。

Method: 基于WildChat数据集构建真实开放性多语言问题集；使用LLM-as-a-Judge识别响应中的文化语境；在多语言翻译版CulturalBench基准上评估模型表现。

Result: LLM在低资源语言上的开放性问题回答质量持续偏低；语言选择显著改变模型调用的文化信息与上下文；文化语境差异直接影响下游回答质量。

Conclusion: 当前LLM存在语言依赖型质量偏差和文化语境漂移问题，需在训练与评估中显式解耦语言与文化因素，以实现真正公平、鲁棒的多语言AI。

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [203] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: 本文提出了一种在自回归模型中实现数据稀疏性的新方法——通过在路由池中引入零计算（空）专家，结合权重稀疏性，在保持因果性的同时提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有专家选择（expert-choice）路由虽能实现数据稀疏性，但在自回归建模中违反因果性，导致训练-推理不一致；而视觉-语言多模态任务中token信息密度差异大，亟需兼顾权重与数据稀疏性。

Method: 在因果token-choice MoE中引入可学习的零计算（null）专家；利用标准负载均衡目标使模型在训练中均匀使用所有专家（含null），从而在期望意义上自然实现数据稀疏性且不破坏因果性。

Result: 在视觉-语言模型训练中，该方法在相同期望FLOPs下优于仅使用权重稀疏性的基线，在训练损失和下游任务性能上均有提升，并自发实现模态感知的隐式路由（视觉token更倾向路由至null专家）。

Conclusion: 权重稀疏性与数据稀疏性可协同增效；引入null专家是实现因果、高效、自适应数据稀疏性的可行路径，尤其适用于多模态异构token场景。

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [204] [You Need Better Attention Priors](https://arxiv.org/abs/2601.15380)
*Elon Litman,Gabe Guo*

Main category: cs.LG

TL;DR: 本文提出了一种基于熵最优传输（EOT）的广义注意力机制GOAT，用可学习的连续先验替代标准注意力中隐含的均匀先验，并兼容FlashAttention等优化核；GOAT能解释并解决attention sink问题，且通过将空间信息融入注意力计算，实现长度外推能力。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制隐含均匀先验，缺乏对位置和结构先验的建模能力，导致attention sink问题及长度泛化差；需从最优传输视角重构注意力以提升表达力与泛化性。

Method: 将注意力建模为带熵正则的最优传输问题，引入可训练的连续先验分布，保持与高效注意力核（如FlashAttention）的兼容性；将空间/位置信息直接嵌入传输代价函数，使先验具备外推能力。

Result: GOAT在多个任务上优于标准注意力，有效缓解attention sink现象，展现出优异的序列长度外推性能，并保持计算效率。

Conclusion: GOAT为注意力机制提供了新的理论视角与实用框架，统一了位置编码设计、sink缓解与高效计算，推动了注意力模型向更鲁棒、可扩展方向发展。

Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.

</details>


### [205] [FedUMM: A General Framework for Federated Learning with Unified Multimodal Models](https://arxiv.org/abs/2601.15390)
*Zhaolong Su,Leheng Zhao,Xiaoying Wu,Ziyue Xu,Jindong Wang*

Main category: cs.LG

TL;DR: 本文提出FedUMM，一种面向非独立同分布（non-IID）多模态数据的低通信开销联邦学习框架，用于统一多模态模型（UMMs），通过LoRA适配器实现参数高效联邦训练，在VQA和生成任务上保持接近中心化训练的性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型（UMMs）依赖中心化训练，难以应用于隐私敏感和地理分布场景；需支持非IID多模态数据的高效、低通信联邦训练。

Method: 基于NVIDIA FLARE构建FedUMM框架，对BLIP3o主干模型采用参数高效微调（仅训练LoRA适配器、冻结基础模型），服务器仅聚合适配器更新；在Dirichlet控制的异构性下进行多客户端评估。

Result: 在VQA v2和GenEval基准上，随客户端数和数据异质性增加仅出现轻微性能下降，仍具竞争力；适配器联邦使每轮通信量降低一个数量级以上。

Conclusion: FedUMM为隐私保护的联邦统一多模态建模提供了可行路径与实证经验，验证了轻量适配器策略在通信与计算权衡中的有效性。

Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.

</details>


### [206] [Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC](https://arxiv.org/abs/2601.15399)
*Ashna Nawar Ahmed,Banooqa Banday,Terry Jones,Tanzima Z. Islam*

Main category: cs.LG

TL;DR: 本文提出了一种基于注意力机制嵌入与多目标贝叶斯优化（MOBO）的代理模型框架，用于自动化HPC作业节点数选择，兼顾运行时间与功耗的权衡。


<details>
  <summary>Details</summary>
Motivation: HPC调度器需在用户性能与设施资源约束间取得平衡，核心是为作业自动选择最优节点数；传统回归方法难以有效建模作业性能动态。

Method: 提出一种代理辅助的多目标贝叶斯优化（MOBO）框架，使用注意力机制对作业遥测数据进行嵌入以构建更优代理模型，并结合智能采样策略提升数据效率。

Result: 在两个生产级HPC数据集上，该方法显著优于基线方法，获得更高质量的运行时间-功耗Pareto前沿；智能采样大幅降低训练成本并提升结果稳定性。

Conclusion: 这是首个将嵌入增强代理模型成功应用于HPC调度MOBO框架的工作，实现了对真实工作负载性能与功耗的联合优化。

Abstract: High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.

</details>


### [207] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*Adrián Rodríguez-Muñoz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: 提出Ambient Dataloops框架，通过数据集与模型协同进化迭代提升数据质量与模型性能，结合Ambient Diffusion处理合成噪声，在图像生成与蛋白质设计中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现代数据集样本质量参差不齐，直接训练导致扩散模型性能次优，需更鲁棒的数据优化机制。

Method: 构建数据集-模型共进化循环：每轮将合成增强样本视为低一级噪声，结合Ambient Diffusion在噪声下学习；避免破坏性自消耗循环。

Result: 在无条件/文本引导图像生成及从头蛋白质设计任务上均达到当前最优性能（SOTA）。

Conclusion: Ambient Dataloops通过理论可解释的迭代数据净化机制，有效提升扩散模型对真实数据分布的学习能力。

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


### [208] [Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware Sequential Prediction with Behavioral Archetypes](https://arxiv.org/abs/2601.15423)
*Lorian Bannis*

Main category: cs.LG

TL;DR: Lattice 是一种混合序列预测系统，通过二元置信门控条件性地激活学习到的行为结构，在置信度高时启用行为原型评分，低时回退至基线预测，有效管理认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决序列预测中因分布偏移或模型不确定性导致的错误激活问题，提升安全关键场景下的鲁棒性与可解释性。

Method: 将行为窗口聚类为行为原型，并引入二元置信门控机制，仅在预测置信度超过阈值时激活原型评分，否则回退至基线模型（LSTM/Transformer）；在多个领域（推荐、引力波、金融）验证其动态适配能力。

Result: 在 MovieLens 上 LSTM 版本 HR@10 提升 31.9%（p < 3.29e-25），显著优于 SASRec（+109.4%）和 BERT4Rec（+218.6%）；在 LIGO 和金融数据中成功拒绝错误激活；Transformer 版本无性能下降，体现优雅退化。

Conclusion: 置信门控是一种有前景的架构原则，能双向调控结构激活——适用时启用、不适用时拒绝、冗余时退化，从而稳健应对认知不确定性。

Abstract: We introduce Lattice, a hybrid sequential prediction system that conditionally activates learned behavioral structure using binary confidence gating. The system clusters behavior windows into behavioral archetypes and uses binary confidence gating to activate archetype-based scoring only when confidence exceeds a threshold, falling back to baseline predictions when uncertain. We validate Lattice on recommendation systems (MovieLens), scientific time-series (LIGO), and financial markets, using LSTM and transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9% improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds), outperforming transformer baselines by 109.4% over SASRec and 218.6% over BERT4Rec. On LIGO and financial data, the system correctly refuses archetype activation when distribution shift occurs - a successful outcome demonstrating confidence gating prevents false activation. On transformer backbones, Lattice provides 0.0% improvement (neutral, no degradation), gracefully deferring when structure is already present. This bidirectional validation - activating when patterns apply, refusing when they don't, and deferring when redundant - supports confidence gating as a promising architectural principle for managing epistemic uncertainty in safety-critical applications.

</details>


### [209] [CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models](https://arxiv.org/abs/2601.15441)
*Zhenghao He,Guangzhi Xiong,Boyang Wang,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: 本文提出CASL框架，通过监督学习将扩散模型的稀疏潜在表示与语义概念对齐，并设计CASL-Steer进行因果探针式干预，提升可解释性与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于稀疏自编码器（SAE）的方法为无监督，难以将稀疏特征与人类可理解的语义概念对齐，限制了生成图像的可靠语义控制。

Method: CASL先在冻结U-Net激活上训练SAE获得解耦表征，再学习轻量线性映射将每个语义概念关联到少量潜在维度；CASL-Steer沿概念轴干预潜在激活以进行因果探测；并提出编辑精度比（EPR）联合评估概念特异性与无关属性保持能力。

Result: 实验表明该方法在编辑精度和可解释性上优于现有方法，首次实现扩散模型中潜在表征与语义概念的监督对齐。

Conclusion: CASL为扩散模型内部表征提供了可解释、可控制的语义对齐路径，推动了可控生成与机制理解的结合。

Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.

</details>


### [210] [Learning from Synthetic Data: Limitations of ERM](https://arxiv.org/abs/2601.15468)
*Kareem Amin,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: 本文研究了在自然数据与大语言模型生成的合成数据混合场景下，经验风险最小化（ERM）等学习算法的性能表现，发现ERM在均值估计和PAC学习中存在局限性，而提出的新加权算法能更优地处理污染数据。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的普及和低成本生成能力，大量合成内容混入真实数据（如评论、法律文书），导致训练数据被污染，亟需重新审视经典学习理论在此新场景下的适用性。

Method: 将学习任务建模为自然数据与合成数据混合输入的序列学习问题，学习算法对每个样本来源未知；理论分析ERM在均值估计和PAC学习框架下的收敛性与泛化能力，并设计非均匀加权算法以应对多代合成数据污染。

Result: 在d维均值估计中，ERM虽收敛但次优；在PAC学习中，ERM可能无法收敛到真实概念（类似模型坍塌），而新提出的加权算法可对任意VC维类及任意污染比例实现正确学习。

Conclusion: ERM在合成数据污染下存在根本性局限，需发展新的鲁棒学习范式；存在理论上可证明的、适用于任意VC类的抗污染学习算法。

Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.

</details>


### [211] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: 本文介绍了Panther，一个兼容PyTorch的RandNLA库，通过定制C++/CUDA后端实现高效模型压缩，在BERT上实现最高75%内存节省且损失相当。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型训练受限于GPU内存与算力，而RandNLA虽有成熟压缩技术，却缺乏统一、可投入生产的库。

Method: 开发了Panther库，整合多种RandNLA算法（如sketched线性层、2D卷积、多头注意力、随机矩阵分解），并基于自研C++/CUDA后端pawX实现高性能CPU/GPU支持。

Result: 在BERT模型中仅需少量代码替换标准线性层，即可实现最高75%内存节省，同时保持损失基本不变。

Conclusion: Panther为RandNLA在深度学习中的实际部署提供了高效、易用、生产就绪的解决方案，显著提升了模型压缩的实用性与可扩展性。

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [212] [Multi-Targeted Graph Backdoor Attack](https://arxiv.org/abs/2601.15474)
*Md Nabi Newaz Khan,Abdullah Arafat Miah,Yu Bi*

Main category: cs.LG

TL;DR: 本文提出了首个面向图分类任务的多目标后门攻击方法，通过子图注入而非子图替换来实现多个触发器同时将预测导向不同目标标签，且在保持原始图结构的同时实现了高攻击成功率和低对干净准确率的影响。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNN）后门攻击研究局限于单目标、基于子图替换的机制，缺乏对多目标攻击的探索，且难以兼顾攻击效果与图结构保真度。

Method: 提出基于子图注入的多目标后门攻击框架，允许多个触发器并行植入，保持原始图结构；系统分析了注入方式、连接数、触发器大小、边密度及投毒比例等设计参数的影响。

Result: 在五个数据集和四种GNN模型上验证了该攻击的高成功率、强泛化性与鲁棒性（可抵抗随机平滑和微剪枝等前沿防御），且对干净样本准确率影响极小。

Conclusion: 该工作揭示了GNN在图分类任务中面对多目标后门攻击的严重脆弱性，为后续防御研究提供了重要警示与基准。

Abstract: Graph neural network (GNN) have demonstrated exceptional performance in solving critical problems across diverse domains yet remain susceptible to backdoor attacks. Existing studies on backdoor attack for graph classification are limited to single target attack using subgraph replacement based mechanism where the attacker implants only one trigger into the GNN model. In this paper, we introduce the first multi-targeted backdoor attack for graph classification task, where multiple triggers simultaneously redirect predictions to different target labels. Instead of subgraph replacement, we propose subgraph injection which preserves the structure of the original graphs while poisoning the clean graphs. Extensive experiments demonstrate the efficacy of our approach, where our attack achieves high attack success rates for all target labels with minimal impact on the clean accuracy. Experimental results on five dataset demonstrate the superior performance of our attack framework compared to the conventional subgraph replacement-based attack. Our analysis on four GNN models confirms the generalization capability of our attack which is effective regardless of the GNN model architectures and training parameters settings. We further investigate the impact of the attack design parameters including injection methods, number of connections, trigger sizes, trigger edge density and poisoning ratios. Additionally, our evaluation against state-of-the-art defenses (randomized smoothing and fine-pruning) demonstrates the robustness of our proposed multi-target attacks. This work highlights the GNN vulnerability against multi-targeted backdoor attack in graph classification task. Our source codes will be available at https://github.com/SiSL-URI/Multi-Targeted-Graph-Backdoor-Attack.

</details>


### [213] [Early predicting of hospital admission using machine learning algorithms: Priority queues approach](https://arxiv.org/abs/2601.15481)
*Jakub Antczak,James Montgomery,Małgorzata O'Reilly,Zbigniew Palmowski,Richard Turner*

Main category: cs.LG

TL;DR: 本研究比较了SARIMAX、XGBoost和LSTM三种模型在急诊科（ED）7天需求预测中的性能，使用澳大利亚一家三级转诊医院2017–2021年数据，并按病区和临床复杂度分层；采用Prophet生成疫情期反事实数据以缓解干扰；XGBoost总入院预测最优（MAE=6.63），SARIMAX对高复杂度病例更优（MAE=3.77）；三者均优于季节性朴素基线，但均难以捕捉突发性激增。


<details>
  <summary>Details</summary>
Motivation: 急诊科过度拥挤威胁患者安全与运营效率，亟需精准的短期需求预测以优化资源配置。

Method: 对比评估SARIMAX、XGBoost和LSTM三种模型在7天日度ED入院预测任务上的性能；数据来自2017–2021年澳大利亚三级医院，按8个病区及临床复杂度分层；使用Prophet模型为COVID-19异常期生成合成反事实值以校正数据偏差。

Result: 所有三个模型均显著优于季节性朴素基线；XGBoost在总日入院预测中精度最高（MAE=6.63）；SARIMAX在高复杂度病例预测中略优（MAE=3.77）；但三者均系统性低估突发性、低频的就诊量激增。

Conclusion: SARIMAX、XGBoost和LSTM均可有效建模日常规律性ED需求，适用于常规资源调度；但其固有局限在于难以捕捉罕见突发事件驱动的就诊高峰，未来需融合异常检测或事件驱动机制提升鲁棒性。

Abstract: Emergency Department overcrowding is a critical issue that compromises patient safety and operational efficiency, necessitating accurate demand forecasting for effective resource allocation. This study evaluates and compares three distinct predictive models: Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED arrivals over a seven-day horizon. Utilizing data from an Australian tertiary referral hospital spanning January 2017 to December 2021, this research distinguishes itself by decomposing demand into eight specific ward categories and stratifying patients by clinical complexity. To address data distortions caused by the COVID-19 pandemic, the study employs the Prophet model to generate synthetic counterfactual values for the anomalous period. Experimental results demonstrate that all three proposed models consistently outperform a seasonal naive baseline. XGBoost demonstrated the highest accuracy for predicting total daily admissions with a Mean Absolute Error of 6.63, while the statistical SARIMAX model proved marginally superior for forecasting major complexity cases with an MAE of 3.77. The study concludes that while these techniques successfully reproduce regular day-to-day patterns, they share a common limitation in underestimating sudden, infrequent surges in patient volume.

</details>


### [214] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: 本文提出Martingale Foresight Sampling（MFS），将大语言模型推理路径搜索建模为随机过程，利用鞅理论（Doob分解、可选停时、鞅收敛）实现原理性路径估值、剪枝与终止，显著提升推理准确率与计算效率。


<details>
  <summary>Details</summary>
Motivation: 标准自回归解码短视，难以找到全局最优推理路径；现有推理时策略依赖启发式方法，缺乏理论支撑。

Method: 将推理路径质量建模为随机过程，基于Doob分解定理进行步级估值，用可选停时理论指导路径剪枝，依鞅收敛定理设计自适应停止规则。

Result: 在六个推理基准上，MFS在准确率上超越当前最优方法，同时大幅提升计算效率。

Conclusion: MFS提供了一种基于概率论原理的、可证明有效的LLM推理优化框架，克服了启发式方法的局限性。

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [215] [MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification](https://arxiv.org/abs/2601.15498)
*Jingwei Song,Xinyu Wang,Hanbin Wang,Xiaoxuan Lei,Bill Shi,Shixin Han,Eric Yang,Xiao-Wen Chang,Lynn Ai*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练、领域无关的“边际感知推测验证”（Margin-Aware Speculative Verification）方法，通过根据目标模型输出logits的局部决策稳定性动态调整验证严格度，在低边际场景下放宽拒绝条件，从而提升推测解码效率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码的验证机制依赖严格的token级拒绝采样，在目标模型对top候选token偏好微弱（低边际）时，拒绝合理次优token带来的信息增益极小，却引发高昂回滚开销，造成验证效率瓶颈。

Method: 提出一种基于目标模型logits计算决策稳定性的动态验证策略：仅在严格验证收益微小时才放松拒绝条件；该方法不修改draft生成过程，仅调整验证规则，且无需训练、适配任意目标模型和已有目标耦合的推测解码框架。

Result: 在8B至235B规模模型上广泛实验表明，该方法在多个基准上持续显著提升推理速度，同时完全保持生成质量，优于当前最优基线。

Conclusion: 验证阶段的自适应性是提升推测解码效率的关键突破口；边际感知验证在不牺牲质量、不增加训练负担的前提下，有效缓解了低边际场景下的验证低效问题，具备强通用性与实用性。

Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

</details>


### [216] [Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning](https://arxiv.org/abs/2601.15503)
*Rishit Chatterjee,Tahiya Chowdhury*

Main category: cs.LG

TL;DR: 本文研究了志愿者主导的湖泊监测中Secchi Disk Depth（SDD）的预测问题，提出了一种联合可行性策略，在保证预测精度损失不超过5%的前提下，最小化所需历史数据量和特征数量，显著提升监测效率与实用性。


<details>
  <summary>Details</summary>
Motivation: 志愿者主导的湖泊监测数据具有不规则、季节性强、缺失多等特点，给有害藻华的预测与预警带来困难。

Method: 采用多重插补法（MICE）处理缺失值；在六种模型中筛选出岭回归效果最优；通过向后截取近期历史数据评估最小样本量；识别最小有效特征子集；构建联合可行性函数统一优化历史长度与特征数量。

Result: 岭回归在30个湖泊上表现最佳；平均约176个样本即可达到全历史数据95%的精度；四特征子集可媲美十三特征基线；最终仅需约64个近期样本加单个预测因子即可满足5%精度目标。

Conclusion: 提出的联合可行性策略为湖泊监测提供了简明、高效的数据采集与测量优先级设定规则，增强了志愿者监测体系的实际应用价值。

Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.

</details>


### [217] [SAGE-FM: A lightweight and interpretable spatial transcriptomics foundation model](https://arxiv.org/abs/2601.15504)
*Xianghao Zhan,Jingyu Xu,Yuanning Zheng,Zinaida Good,Olivier Gevaert*

Main category: cs.LG

TL;DR: SAGE-FM是一个轻量级、基于图卷积网络（GCN）的空间转录组学基础模型，通过掩码中心点预测目标在416个人类Visium样本上训练，能生成空间一致的嵌入表示，在基因恢复、聚类、疾病注释和调控推断等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学的发展促使需要能捕捉空间条件调控关系的计算模型，现有方法在可扩展性、空间感知能力和生物学可解释性方面存在不足。

Method: 提出SAGE-FM模型，采用轻量级图卷积网络（GCN），以掩码中心点基因表达预测为自监督训练目标；在416个跨15个器官的人类Visium样本上进行大规模预训练；利用空间邻接图建模组织结构；支持下游微调与in silico扰动分析。

Result: 91%被掩码基因恢复显著相关（p<0.05）；嵌入表示在无监督聚类和生物异质性保持上优于MOFA及其他空间方法；在口咽鳞癌斑点标注中达81%准确率；胶质母细胞瘤亚型预测性能提升；in silico扰动实验证实其能捕获符合真实生物学的配体-受体及上下游调控方向性。

Conclusion: 轻量、参数高效且基于GCN的模型可作为空间转录组学的大规模基础模型，在保持生物学可解释性的同时具备强泛化能力与空间感知能力。

Abstract: Spatial transcriptomics enables spatial gene expression profiling, motivating computational models that capture spatially conditioned regulatory relationships. We introduce SAGE-FM, a lightweight spatial transcriptomics foundation model based on graph convolutional networks (GCNs) trained with a masked central spot prediction objective. Trained on 416 human Visium samples spanning 15 organs, SAGE-FM learns spatially coherent embeddings that robustly recover masked genes, with 91% of masked genes showing significant correlations (p < 0.05). The embeddings generated by SAGE-FM outperform MOFA and existing spatial transcriptomics methods in unsupervised clustering and preservation of biological heterogeneity. SAGE-FM generalizes to downstream tasks, enabling 81% accuracy in pathologist-defined spot annotation in oropharyngeal squamous cell carcinoma and improving glioblastoma subtype prediction relative to MOFA. In silico perturbation experiments further demonstrate that the model captures directional ligand-receptor and upstream-downstream regulatory effects consistent with ground truth. These results demonstrate that simple, parameter-efficient GCNs can serve as biologically interpretable and spatially aware foundation models for large-scale spatial transcriptomics.

</details>


### [218] [Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features](https://arxiv.org/abs/2601.15530)
*Megan A. Witherow,Michael L. Evans,Ahmed Temtam,Hamid Okhravi,Khan M. Iftekharuddin*

Main category: cs.LG

TL;DR: 本文提出了一种基于临床测试和MRI数据的机器学习方法，用于提高非典型阿尔茨海默病（atAD）的诊断准确率，尤其在区分atAD与非AD认知障碍方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 典型AD（tAD）可通过海马体积和认知测试较准确诊断，但非典型AD（atAD）常被误诊，亟需更有效的无创诊断方法。

Method: 使用来自私有数据集及NACC、ADNI两个公共数据集的1410名受试者数据，构建机器学习模型；结合临床量表与多区域MRI特征（包括海马体积及全脑影像特征），并采用Boruta算法筛选关键判别脑区。

Result: 模型将atAD识别的召回率从52%提升至69%（NACC）和34%提升至77%（ADNI），同时保持高精度；加入全脑MRI特征优于仅用海马体积。

Conclusion: 该方法可显著提升临床中非记忆型atAD的诊断准确性，仅依赖常规临床评估和MRI，具备实际应用潜力。

Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.

</details>


### [219] [QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs](https://arxiv.org/abs/2601.15538)
*Himanshu Mishra,Kanwal Mehreen*

Main category: cs.LG

TL;DR: 本文研究了量化对机器遗忘效果的负面影响，并提出了一种量化感知的遗忘方法，通过在logits空间引入间隔损失，确保遗忘样本在量化后仍可区分，从而在4比特量化下有效保持遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在模型量化部署后容易恢复被遗忘信息，尤其在低比特量化下问题严重，亟需一种能兼容量化部署的遗忘方法。

Method: 分析量化中权重更新不足跨越量化阈值的问题，提出logits空间的间隔损失：对每个遗忘样本，强制遗忘模型输出logits与原始模型差异至少为量化步长的一半。

Result: 在语言建模和分类任务（含Twitter虚假信息数据集）上验证，所提方法在4-bit量化下仍保持良好遗忘效果，而现有方法几乎完全恢复被遗忘知识。

Conclusion: 量化会严重损害机器遗忘效果；所提出的量化感知遗忘方法能有效缓解该问题，保障遗忘鲁棒性与实际部署兼容性。

Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.

</details>


### [220] [PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction](https://arxiv.org/abs/2601.15540)
*Dongchen Huang*

Main category: cs.LG

TL;DR: 本文提出Prism模型，一种基于MCR²原则的白盒注意力架构，通过引入过完备字典和π-RoPE频率分离等几何先验，实现无监督的功能解耦与注意力头的频谱特化，从而在保持高性能的同时提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型（尤其是Transformer）作为“黑箱”缺乏可解释性的问题。

Method: 提出Prism架构，将注意力机制建模为信号-噪声流形上的梯度上升过程，并施加两个物理约束：过完备字典以扩展表征相空间，π-RoPE实现信号与噪声子空间的非相干分离。

Result: 在TinyStories数据集上验证了Prism能自发将注意力头分为低频（捕获长程因果依赖）和高频（处理局部句法约束）两类，实现无监督功能解耦。

Conclusion: 几何先验足以诱导可解释的结构，表明可解释性与性能并非权衡关系，而是可通过原理性几何构造统一。

Abstract: Deep learning models, particularly Transformers, are often criticized as "black boxes" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($π$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.

</details>


### [221] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: 本文提出RDumb++，通过引入基于熵和KL散度的漂移检测机制及自适应重置策略，显著提升模型在长时序、快速变化测试分布下的持续测试时适应（CTTA）性能，在CCC基准上实现约3%的绝对准确率提升并避免预测崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法（如Tent、EATA）在测试分布快速或长期演化时表现不佳，尤其在CCC基准（7.5M样本流、持续变化的污染类型与强度）下易发生预测崩溃。

Method: 提出RDumb++，扩展RDumb框架，引入两种漂移检测机制（熵得分与KL散度得分）和自适应重置策略，以识别有害累积适应并及时恢复模型。

Result: 在CCC-medium（三速度×三种子，共九轮、每轮100万样本）上，RDumb++稳定超越RDumb，平均提升约3%绝对准确率；消融实验验证漂移感知重置对防止崩溃和实现可靠长时CTTA至关重要。

Conclusion: 漂移检测与自适应重置是实现鲁棒、长效CTTA的关键，RDumb++为应对极端动态测试环境提供了有效且可解释的解决方案。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [222] [Beyond validation loss: Clinically-tailored optimization metrics improve a model's clinical performance](https://arxiv.org/abs/2601.15546)
*Charles B. Delahunt,Courosh Mehanian,Daniel E. Shea,Matthew P. Horning*

Main category: cs.LG

TL;DR: 本文探讨了在医疗健康领域的机器学习中，使用临床定制指标而非传统验证损失进行模型优化的优势，并通过两个受控实验验证了其在临床任务性能上的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习使用验证损失指导优化，但医疗AI的核心目标是满足具体临床需求，而非最小化训练损失；因此需引入更精准反映临床目标的非可微度量。

Method: 设计并执行两项受控实验，对比采用临床定制指标与验证损失进行模型优化（如超参选择、早停）的效果。

Result: 临床定制指标驱动的优化显著提升了模型在临床任务上的实际性能，优于基于验证损失的优化。

Conclusion: 尽管定义和集成临床指标需额外工作，但其能产出更贴合临床目标的模型，应成为医疗AI模型优化的标准实践。

Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.

</details>


### [223] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 本文提出了首个面向不完全观测数据的神经算子学习框架，通过掩码预测训练策略和物理感知潜在传播器解决监督缺失与空间错配问题，并构建了专用基准POBench-PDE，在多种PDE任务中显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 现实科学应用常面临观测数据不完整的问题（如传感器限制、地理约束、测量成本），而现有神经算子依赖完全观测的空间输入，限制了其实际适用性。

Method: 提出Latent Autoregressive Neural Operator（LANO），包含两个创新组件：(i) mask-to-predict训练策略，通过主动掩码已观测区域构造人工监督信号；(ii) Physics-Aware Latent Propagator，在隐空间中以边界优先的自回归方式重建解。同时构建POBench-PDE基准。

Result: 在patch-wise缺失率<50%下，相对L2误差降低18–69%；在真实气候预测等任务中验证有效；可应对最高75%缺失率的实际场景。

Conclusion: 该工作系统性地拓展了神经算子在不完全观测条件下的建模能力，显著缩小理想研究设定与现实科学计算之间的差距。

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [224] [BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations](https://arxiv.org/abs/2601.15552)
*Phuc Nguyen,Benjamin Zelditch,Joyce Chen,Rohit Patra,Changshuai Wei*

Main category: cs.LG

TL;DR: 本文提出了BanditLP，一种可扩展的多利益相关者上下文赌博机框架，结合了神经汤普森采样与大规模线性规划，用于在服务时进行约束动作选择。该方法具有应用无关性、兼容任意神经网络架构，并已在LinkedIn邮件营销系统中成功部署，带来实际业务收益。


<details>
  <summary>Details</summary>
Motivation: 解决多利益相关者场景下探索（exploration）与约束优化（constrained optimization）难以兼顾的问题，尤其在大规模线上系统（如邮件营销）中需平衡个性化推荐、商业目标与资源约束。

Method: 提出BanditLP框架：前端采用神经汤普森采样学习各目标（如点击率、转化率）的上下文依赖结果；后端在推理时调用可扩展线性规划求解器，对海量候选动作施加多维约束（如预算、公平性、覆盖率）并完成联合优化决策；LP求解器支持十亿级变量规模。

Result: 在公开基准和合成数据上持续优于强基线；在LinkedIn真实邮件营销系统中落地，显著提升关键业务指标，验证了探索与约束优化集成的有效性与实用性。

Conclusion: BanditLP为大规模多目标在线决策提供了统一、可扩展且可部署的解决方案，证明了将贝叶斯探索机制与确定性约束优化深度耦合的可行性与价值。

Abstract: We present BanditLP, a scalable multi-stakeholder contextual bandit framework that unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection at serving time. The methodology is application-agnostic, compatible with arbitrary neural architectures, and deployable at web scale, with an LP solver capable of handling billions of variables. Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. We apply this approach in LinkedIn's email marketing system and demonstrate business win, illustrating the value of integrated exploration and constrained optimization in production.

</details>


### [225] [Deep Learning for Perishable Inventory Systems with Human Knowledge](https://arxiv.org/abs/2601.15589)
*Xuan Liao,Zhenkang Peng,Ying Rong*

Main category: cs.LG

TL;DR: 本文提出了一种端到端深度学习框架，用于在需求和提前期分布均未知、历史数据有限的易腐库存系统中优化订货决策，通过引入边际成本会计机制和结构引导（如PIL策略）提升学习效率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在易腐品库存管理中，面对随机提前期、未知需求与提前期分布以及有限历史数据的现实挑战，传统方法难以兼顾学习效率与策略可解释性。

Method: 提出基于边际成本会计的统一损失函数，构建两种端到端深度学习策略：纯黑箱模型（E2E-BB）与结构引导模型（E2E-PIL），后者嵌入投影库存水平（PIL）启发式；进一步利用目标函数的一次齐次性，结合运营数据分析（ODA） boosting 技术得到增强策略 E2E-BPIL。

Result: 在合成与真实数据实验中，性能排序为：E2E-BB < E2E-PIL < E2E-BPIL；理论分析表明结构嵌入降低了有效模型复杂度，提升了小样本学习效率。

Conclusion: 将领域知识（如库存理论中的启发式结构）融入深度学习策略设计，可显著提升其有效性、鲁棒性与可解释性，支持‘人机协同’的智能决策范式。

Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

</details>


### [226] [Neural Nonlinear Shrinkage of Covariance Matrices for Minimum Variance Portfolio Optimization](https://arxiv.org/abs/2601.15597)
*Liusha Yang,Siqi Zhao,Shuqi Chai*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经网络的非线性协方差矩阵收缩估计器，用于最小方差投资组合优化，结合Ledoit-Wolf方法与轻量级Transformer网络学习特征值收缩函数，并以投资组合风险为损失函数进行训练，实证显示其在S&P500数据上优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 传统协方差估计（如Ledoit-Wolf）在线性假设下存在局限，难以充分捕捉高维资产收益间的复杂依赖关系；而纯数据驱动方法缺乏统计可解释性与小样本鲁棒性。本文旨在融合结构化统计模型与机器学习优势，提升最小方差投资组合的风险预测与控制能力。

Method: 以Ledoit-Wolf协方差估计为基础，将其分解为特征值和特征向量；设计轻量级Transformer网络，以样本-维度比等统计量为条件输入，非线性地学习特征值收缩函数；以投资组合风险（即样本外方差）为损失函数端到端训练网络，输出优化后的精度矩阵（协方差逆矩阵）。

Result: 在S&P500股票日收益率数据上的实证表明，该方法在多种资产数量与样本规模下均显著降低投资组合的样本外实现风险，稳健优于LW、样本协方差及其它主流收缩与机器学习方法。

Conclusion: 将结构化统计先验（如LW分解）与可解释、可扩展的神经网络建模相结合，是提升金融协方差估计与投资组合优化性能的有效路径；该混合范式兼顾理论合理性与数据适应性，具有实际应用潜力。

Abstract: This paper introduces a neural network-based nonlinear shrinkage estimator of covariance matrices for the purpose of minimum variance portfolio optimization. It is a hybrid approach that integrates statistical estimation with machine learning. Starting from the Ledoit-Wolf (LW) shrinkage estimator, we decompose the LW covariance matrix into its eigenvalues and eigenvectors, and apply a lightweight transformer-based neural network to learn a nonlinear eigenvalue shrinkage function. Trained with portfolio risk as the loss function, the resulting precision matrix (the inverse covariance matrix) estimator directly targets portfolio risk minimization. By conditioning on the sample-to-dimension ratio, the approach remains scalable across different sample sizes and asset universes. Empirical results on stock daily returns from Standard & Poor's 500 Index (S&P500) demonstrate that the proposed method consistently achieves lower out-of-sample realized risk than benchmark approaches. This highlights the promise of integrating structural statistical models with data-driven learning.

</details>


### [227] [When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards](https://arxiv.org/abs/2601.15609)
*Mingyuan Fan,Weiguang Han,Daixin Wang,Cen Chen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: 本文研究了强化学习与可验证奖励（RLVR）在大型语言模型中的应用，发现其存在过度锐化问题，并提出了两种校准方法以提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在实践中取得成功，但尚不清楚它是否激发了新能力，还是仅优化了已有知识的分布；作者旨在探究并解决其中的过度锐化现象。

Method: 形式化定义了过度锐化现象，分析有限批次更新和语义耦合导致策略坍缩的机制；提出逆成功优势校准和分布级校准（结合记忆网络）来缓解该问题。

Result: 实证评估表明所提策略能有效改善模型的泛化能力。

Conclusion: RLVR易受过度锐化影响，而引入针对困难样本和采样多样性的校准机制可显著提升其可靠性与泛化性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm for turning large language models (LLMs) into reliable problem solvers, especially in logic-heavy domains. Despite its empirical success, it remains unclear whether RLVR elicits novel capabilities or merely sharpens the distribution over existing knowledge. We study this by formalizing over-sharpening, a phenomenon where the policy collapses onto limited modes, suppressing valid alternatives. At a high level, we discover finite-batch updates intrinsically bias learning toward sampled modes, triggering a collapse that propagates globally via semantic coupling. To mitigate this, we propose inverse-success advantage calibration to prioritize difficult queries and distribution-level calibration to diversify sampling via a memory network. Empirical evaluations validate that our strategies can effectively improve generalization.

</details>


### [228] [Closing the Gap on the Sample Complexity of 1-Identification](https://arxiv.org/abs/2601.15620)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 本文研究了多臂赌博机中的1-识别问题，提出了一个新的下界和一个具有紧致上下界的新算法，解决了多个合格臂情形下的期望采样次数分析这一开放问题。


<details>
  <summary>Details</summary>
Motivation: 解决多臂赌博机中1-识别问题的理论分析空白，特别是当存在多个合格臂时的期望总拉取次数分析问题。

Method: 通过优化公式推导新的下界，并设计一种新算法以获得紧致的上界。

Result: 得到了存在至少一个合格臂时的新的下界；设计的算法上界与下界之间的差距仅为对数因子的多项式；弥补了历史文献中关于多个合格臂情形下期望总拉取次数分析的空白。

Conclusion: 本文在1-识别问题上取得了理论突破，提供了更精确的复杂度刻画，并为后续研究奠定了基础。

Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.

</details>


### [229] [Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors](https://arxiv.org/abs/2601.15625)
*Zhiwei Zhang,Fei Zhao,Rui Wang,Zezhong Wang,Bin Liang,Jiakang Wang,Yao Hu,Shaosheng Cao,Kam-Fai Wong*

Main category: cs.LG

TL;DR: 本文提出Fission-GRPO框架，通过在强化学习训练中将执行错误实时转化为纠正性监督信号，显著提升大语言模型在多轮工具调用中的错误恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多轮工具调用中遇到错误后易陷入无效重试，缺乏有效自我纠错能力；标准RL仅提供稀疏惩罚，而合成纠错数据存在分布不匹配问题。

Method: 提出Fission-GRPO：利用微调的Error Simulator为失败轨迹生成诊断反馈，并将其‘裂变’为新训练样本，在线进行恢复策略的on-policy重采样与优化。

Result: 在BFCL v4 Multi-Turn上，Qwen3-8B的错误恢复率绝对提升5.7%，整体准确率从42.75%提升至46.75%（+4%），超越GRPO及专用工具调用智能体。

Conclusion: 将错误实时转化为on-policy纠正监督是提升LLM鲁棒工具使用的有效范式，Fission-GRPO为构建高可靠性AI代理提供了新路径。

Abstract: Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.

</details>


### [230] [An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types](https://arxiv.org/abs/2601.15640)
*Natasha Trinkle,Huong Ha,Jeffrey Chan*

Main category: cs.LG

TL;DR: 本文对基于集成的迁移学习贝叶斯优化方法及其流程组件进行了实证分析，提出了正则化回归加权策略（权重非负）和自动退避机制，并验证了暖启动初始化与非负权重约束对性能提升的有效性。


<details>
  <summary>Details</summary>
Motivation: 利用历史数据集提升贝叶斯优化在新任务上的样本效率，需适配迁移学习方法到其各流程组件中。

Method: 实证分析多种基于集成的迁移学习贝叶斯优化方法及流程组件；提出基于正则化回归（权重约束为正）的集成代理模型加权策略，以及应对迁移无效的处理组件；构建三个新的实时迁移学习基准。

Result: 暖启动初始化和约束集成代理模型权重为正这两个组件普遍提升了迁移学习贝叶斯优化的性能。

Conclusion: 集成式迁移学习贝叶斯优化中，合理设计权重机制与初始采样策略至关重要；所提方法在多个新基准上验证了有效性与鲁棒性。

Abstract: Bayesian optimisation is a sample efficient method for finding a global optimum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisation by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of various ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive.

</details>


### [231] [Integrating Knowledge Distillation Methods: A Sequential Multi-Stage Framework](https://arxiv.org/abs/2601.15657)
*Yinxi Tian,Changwu Huang,Ke Tang,Xin Yao*

Main category: cs.LG

TL;DR: 本文提出SMSKD框架，通过多阶段顺序集成异构知识蒸馏方法，并引入基于教师模型真实类别概率的自适应加权机制，有效缓解灾难性遗忘，提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法在整合多种蒸馏策略时面临实现复杂、组合不灵活及灾难性遗忘等问题，限制了其实际效果。

Method: 提出SMSKD（Sequential Multi-Stage Knowledge Distillation）框架：各阶段使用不同蒸馏方法训练学生模型，前一阶段冻结模型作为参考以锚定已有知识；并设计基于教师真实类概率（TCP）的自适应样本级损失加权机制。

Result: 在多种师生架构与蒸馏方法组合下，SMSKD持续提升学生准确率，显著优于现有基线；消融实验验证了分阶段蒸馏、参考模型监督及TCP加权机制的有效性。

Conclusion: SMSKD是一种灵活、高效且实用的知识蒸馏框架，支持任意方法组合与阶段数，计算开销极小，适用于资源受限场景。

Abstract: Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness.
  This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration.
  By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.

</details>


### [232] [Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting](https://arxiv.org/abs/2601.15669)
*Jingjing Bai,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: 本文提出Dualformer，一种双域框架，通过双分支架构、分层频率采样模块和周期性感知加权机制，解决Transformer在长期时间序列预测中因低通滤波效应导致高频信息丢失的问题，显著提升预测性能，尤其适用于异构或弱周期数据。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在长期时间序列预测中存在固有的低通滤波效应，导致高频信息逐层衰减，难以捕捉细粒度时序变化。

Method: 提出Dualformer框架，包含：(1) 时域与频域并行建模的双分支结构；(2) 分层频率采样模块，按层分配不同频段以保留低层高频细节、深层低频趋势；(3) 周期性感知加权机制，依据输入谐波能量比动态融合双分支输出，并给出理论下界支撑。

Result: 在8个主流基准上实验表明，Dualformer鲁棒性强、性能优越，尤其在异构或弱周期数据上效果显著。

Conclusion: Dualformer通过结构化频域建模与自适应时频特征融合，有效缓解Transformer的低通滤波问题，提升了长期时间序列预测的精度与泛化能力。

Abstract: Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.

</details>


### [233] [Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing](https://arxiv.org/abs/2601.15686)
*Xinyu Wang,Sicheng Lyu,Yu Gu,Jerry Huang,Peng Lu,Yufei Cui,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: 本文提出RLSEdit，一种用于长序列模型编辑的递归最小二乘编辑器，通过在线二次优化与软约束机制，在保持预训练权重和锚定映射的同时实现稳定、可扩展的连续编辑，显著提升多轮编辑下的成功率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在长期流式编辑中面临可塑性-稳定性困境：硬写入易累积干扰，硬保护则难以兼顾历史编辑保留与未约束行为的稳定性，导致多轮编辑后性能退化。

Method: RLSEdit将编辑建模为带两个正则项的在线二次优化问题，目标是最小化累计键值拟合误差，正则项分别约束偏离预训练权重和指定锚映射的程度；利用Woodbury恒等式实现高效递归更新，单次编辑计算成本与历史长度无关。

Result: 在多个模型家族上实验表明，RLSEdit稳定支持高达10K次编辑，编辑成功率和整体稳定性（包括早期编辑保留、GLUE及推理/代码基准上的泛化能力）均优于强基线。

Conclusion: RLSEdit通过软约束与递归优化有效缓解了模型编辑中的可塑性-稳定性权衡，为大规模、长周期的LLM在线更新提供了可扩展且鲁棒的解决方案。

Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit "hard writes" can accumulate interference over time, while null-space-style "hard preservation" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.

</details>


### [234] [Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs](https://arxiv.org/abs/2601.15714)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 本文提出零错误范围（ZEH）作为评估大语言模型可信度的新指标，揭示了当前顶尖模型在简单任务上仍存在基础性错误，并探讨了ZEH与算法能力涌现及计算效率优化的关系。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽在复杂任务上表现优异，但在简单确定性任务上仍可能出错，这对安全关键场景构成风险；需一个能刻画模型‘无错能力边界’的可解释、可量化的指标。

Method: 提出零错误范围（ZEH）定义为模型能完全无误求解的最大输入规模；在GPT-5.2和Qwen2.5等模型上实证评估ZEH；结合树结构搜索与在线softmax技术加速ZEH计算。

Result: 发现GPT-5.2在字符串奇偶性判断和括号匹配等基础任务上即出现错误；ZEH与整体准确率相关但行为模式不同，可反映算法能力的涌现现象；所提优化方法实现近十倍计算加速。

Conclusion: ZEH是一个揭示LLM基础可靠性缺陷的有效诊断工具，不仅警示其在安全敏感场景中的局限性，也为理解能力涌现和提升评估效率提供了新路径。

Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.

</details>


### [235] [Communication-efficient Federated Graph Classification via Generative Diffusion Modeling](https://arxiv.org/abs/2601.15722)
*Xiuling Wang,Xin Huang,Haibo Hu,Jianliang Xu*

Main category: cs.LG

TL;DR: 本文提出CeFGC，一种仅需三轮通信的联邦图神经网络（FGNN）新范式，利用生成扩散模型缓解非独立同分布（non-IID）数据和高通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦图神经网络中因多轮参数交换导致的高通信开销，以及客户端间数据非独立同分布（non-IID）带来的训练困难。

Method: 各客户端训练本地图分布的生成扩散模型并上传至服务器；服务器将模型广播回所有客户端；客户端用生成的合成图与本地图联合训练GNN；最后上传权重聚合为全局模型。

Result: 理论证明通信轮数降至常数3轮；在多个真实图数据集上实验表明，CeFGC在non-IID场景下性能优于现有方法，兼顾效率与准确性。

Conclusion: CeFGC通过引入生成扩散模型实现低通信、高适应性的联邦GNN训练，为non-IID图数据上的分布式学习提供了有效新路径。

Abstract: Graph Neural Networks (GNNs) unlock new ways of learning from graph-structured data, proving highly effective in capturing complex relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent distributed learning paradigm for training GNNs over decentralized data. However, FGNNs face two significant challenges: high communication overhead from multiple rounds of parameter exchanges and non-IID data characteristics across clients. To address these issues, we introduce CeFGC, a novel FGNN paradigm that facilitates efficient GNN training over non-IID data by limiting communication between the server and clients to three rounds only. The core idea of CeFGC is to leverage generative diffusion models to minimize direct client-server communication. Each client trains a generative diffusion model that captures its local graph distribution and shares this model with the server, which then redistributes it back to all clients. Using these generative models, clients generate synthetic graphs combined with their local graphs to train local GNN models. Finally, clients upload their model weights to the server for aggregation into a global GNN model. We theoretically analyze the I/O complexity of communication volume to show that CeFGC reduces to a constant of three communication rounds only. Extensive experiments on several real graph datasets demonstrate the effectiveness and efficiency of CeFGC against state-of-the-art competitors, reflecting our superior performance on non-IID graphs by aligning local and global model objectives and enriching the training set with diverse graphs.

</details>


### [236] [Towards Automated Kernel Generation in the Era of LLMs](https://arxiv.org/abs/2601.15727)
*Yang Yu,Peiyu Zang,Chi Hsu Tsai,Haiming Wu,Yixin Shen,Jialing Zhang,Haoyu Wang,Zhiyou Xiao,Jingze Shi,Yuyu Luo,Wentao Zhang,Chunlei Men,Guang Liu,Yonghua Lin*

Main category: cs.LG

TL;DR: This survey provides a structured overview of LLM-driven kernel generation, covering approaches, datasets, benchmarks, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Kernel engineering is critical but time-consuming and non-scalable; LLMs and agentic systems offer new automation possibilities, yet the field lacks a systematic perspective.

Method: Systematic review and categorization of existing LLM-based and agentic approaches for kernel generation, along with compilation of relevant datasets and benchmarks.

Result: A comprehensive, structured survey that unifies fragmented research, identifies open challenges, and outlines future directions for LLM-driven kernel optimization.

Conclusion: The survey establishes a foundational reference for automated kernel optimization and fosters community progress through an open-source GitHub repository.

Abstract: The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.

</details>


### [237] [Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning](https://arxiv.org/abs/2601.15771)
*Dong Xu,Jiantao Wu,Qihua Pan,Sisi Yuan,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 本文提出GenRel-DDI框架，将药物相互作用（DDI）预测从分子中心转向关系中心建模，通过学习与具体药物无关的交互表示，提升对未见药物和新药对的泛化能力，在严格实体不相交评估中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于分子的DDI预测模型在标准基准上表现良好，但在实际部署中泛化能力差，尤其面对未见药物和稀疏验证数据时失效；且嵌入空间邻近性不能可靠反映真实相互作用。

Method: 提出GenRel-DDI：一种关系中心的学习框架，将DDI预测重构为关系学习问题，独立于药物身份学习交互表示，实现关系级抽象以捕获可迁移的交互模式。

Result: 在多个基准测试中持续显著超越SOTA方法，尤其在严格的实体不相交（entity-disjoint）评估下增益突出，验证了关系学习对鲁棒DDI预测的有效性与实用性。

Conclusion: 关系中心建模是提升DDI预测泛化能力的关键路径，GenRel-DDI为应对真实世界中未见药物和数据稀疏挑战提供了有效新范式。

Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.

</details>


### [238] [Next Generation Active Learning: Mixture of LLMs in the Loop](https://arxiv.org/abs/2601.15773)
*Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Guoxiang Guo,Joanne Enticott,Gang Liu,Lan Du*

Main category: cs.LG

TL;DR: 本文提出了一种名为'Mixture of LLMs in the Loop Active Learning'的新主动学习框架，利用多个轻量级大语言模型（LLMs）协同标注，并结合标注差异检测与负向学习策略，提升LLM标注鲁棒性与学习效果，在性能上媲美人工标注且可本地部署。


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为标注器虽能降低标注成本，但其生成标签质量不足、鲁棒性差，难以满足实际应用需求。

Method: 提出混合LLM标注模型替代人工标注，并引入标注差异度量和负向学习机制以识别并缓解噪声标签影响。

Result: 实验表明该框架性能媲美人工标注，显著优于单LLM及其它LLM集成方法，且基于轻量级LLM，支持全本地化运行。

Conclusion: 混合LLM主动学习框架有效提升了LLM标注的可靠性与实用性，为低成本、高鲁棒的智能标注提供了可行方案。

Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.

</details>


### [239] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: 本文提出GOSV框架，通过全局优化识别大语言模型中对安全性至关重要的注意力头，并发现两类功能分离的安全向量；基于此设计出更有效的白盒越狱攻击方法，验证了该框架在安全可解释性上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防护易受越狱攻击，且主流归因方法忽略注意力头等组件间的协同作用，缺乏对安全机制内在结构的深入理解。

Method: 提出全局优化的安全向量提取框架（GOSV），联合优化所有注意力头；采用有害补丁（Harmful Patching）和零消融（Zero Ablation）两种激活重修补策略，识别两类低重叠的安全向量。

Result: 发现约30%注意力头被重修补即可导致安全全面失效；识别出空间分离的恶意注入向量与安全抑制向量；所提白盒越狱攻击在多个模型上显著优于现有方法。

Conclusion: LLM的安全机制依赖于功能分离、协同工作的注意力头子集；GOSV框架揭示了安全性的结构性基础，提升了模型安全的可解释性与可操控性。

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [240] [Uncertainty-guided Generation of Dark-field Radiographs](https://arxiv.org/abs/2601.15859)
*Lina Felsner,Henriette Bast,Tina Dorosti,Florian Schaff,Franz Pfeiffer,Daniela Pfeiffer,Julia Schnabel*

Main category: cs.LG

TL;DR: 本文提出了一种基于不确定性引导的渐进式生成对抗网络，用于从常规X射线衰减图像直接生成X射线暗场图像，提升了生成图像的结构保真度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: X射线暗场成像数据稀缺，制约了深度学习模型的鲁棒性发展，亟需有效合成暗场图像的方法。

Method: 提出不确定性引导的渐进式生成对抗网络（UGP-GAN），融合aleatoric和epistemic两种不确定性建模，实现从标准胸部X光衰减图像到暗场图像的端到端生成。

Result: 生成图像具有高结构保真度，定量指标在各阶段持续提升；在分布外数据上验证了良好泛化性。

Conclusion: 不确定性引导的生成建模可实现逼真的暗场图像合成，为临床应用提供了可靠基础。

Abstract: X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.

</details>


### [241] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: 本文提出了一种后训练的结构化视角，通过统计准则和结构退火方法，识别并剥离大模型中未被梯度更新支持的参数依赖，揭示稳定独立的子结构，从而实现无需修改模型功能或接口的结构化、并行推理。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型推理通常在稠密参数矩阵上进行，导致推理成本和系统复杂度随模型规模不可持续增长；问题根源不在于模型容量不足，而在于将训练后的推理系统视为整体操作符，忽略了学习过程中形成的内部结构。

Method: 基于梯度更新事件高度局部化和选择性的观察，提出后训练统计准则与结构退火流程，以剔除未被支持的参数依赖，揭示稳定的独立子结构。

Result: 实现了模型无关、后训练的结构化推理方法，支持结构化与并行推理，且不改变模型功能或接口。

Conclusion: 训练后的推理系统具有内在结构非均匀性与可分解性，可通过后训练分析揭示其稳定子结构，为高效推理提供新范式。

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [242] [SoK: Challenges in Tabular Membership Inference Attacks](https://arxiv.org/abs/2601.15874)
*Cristina Pêra,Tânia Carvalho,Maxime Cordy,Luís Antunes*

Main category: cs.LG

TL;DR: 本文全面分析了成员推断攻击（MIAs）在集中式和联邦学习中的应用，特别关注表格数据，揭示其在该场景下整体性能较差，但对'单例记录'仍高度有效，并发现使用不同代理模型可提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断攻击（MIAs）在表格数据上的研究不足，尤其在联邦学习中对外部攻击者威胁的忽视，以及对单例记录脆弱性和跨模型迁移性的系统性分析缺失。

Method: 1）扩展并细化集中式与联邦学习下的MIA分类法；2）在表格数据上评估多种MIA策略及对应防御；3）在联邦学习中建模外部攻击者威胁；4）分析单例记录的MIA脆弱性；5）探究MIA在不同模型架构间的迁移能力。

Result: MIAs在表格数据上整体表现较差，与先前SOTA结论相反；但对单例记录攻击成功率极高；采用不同代理模型可显著提升MIA有效性；联邦学习中外部攻击者仍具现实威胁。

Conclusion: 表格数据的隐私风险需重新评估——不能仅依赖MIA指标；应特别关注单例记录保护；代理模型多样性是提升MIA效能的关键因素；联邦学习隐私分析须纳入外部攻击者视角。

Abstract: Membership Inference Attacks (MIAs) are currently a dominant approach for evaluating privacy in machine learning applications. Despite their significance in identifying records belonging to the training dataset, several concerns remain unexplored, particularly with regard to tabular data. In this paper, first, we provide an extensive review and analysis of MIAs considering two main learning paradigms: centralized and federated learning. We extend and refine the taxonomy for both. Second, we demonstrate the efficacy of MIAs in tabular data using several attack strategies, also including defenses. Furthermore, in a federated learning scenario, we consider the threat posed by an outsider adversary, which is often neglected. Third, we demonstrate the high vulnerability of single-outs (records with a unique signature) to MIAs. Lastly, we explore how MIAs transfer across model architectures. Our results point towards a general poor performance of these attacks in tabular data which contrasts with previous state-of-the-art. Notably, even attacks with limited attack performance can still successfully expose a large portion of single-outs. Moreover, our findings suggest that using different surrogate models makes MIAs more effective.

</details>


### [243] [Iterative Amortized Hierarchical VAE](https://arxiv.org/abs/2601.15894)
*Simon W. Penninga,Ruud J. G. van Sloun*

Main category: cs.LG

TL;DR: 本文提出了一种迭代摊销分层变分自编码器（IA-HVAE），结合初始摊销猜测与基于解码器梯度的迭代优化，在傅里叶等变换域中设计线性可分解码器，显著提升推理速度与重建质量。


<details>
  <summary>Details</summary>
Motivation: 为解决传统分层变分自编码器（HVAE）在推理速度与重建精度之间的权衡问题，尤其在实时高深度模型应用中效率不足。

Method: 提出IA-HVAE架构：采用摊销初始化加基于解码器梯度的迭代精调；设计变换域（如傅里叶空间）中的线性可分解码器以支持高效梯度计算。

Result: 相比传统HVAE，迭代推理速度提升35倍；在准确率上优于全摊销方法，在速度上优于全迭代方法；在去模糊、去噪等逆问题中重建质量优于标准HVAE。

Conclusion: IA-HVAE通过混合推断策略与结构化解码器设计，在保持高精度的同时大幅提升推理效率，适用于实时高深度生成建模与逆问题求解。

Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.

</details>


### [244] [Predicting Healthcare System Visitation Flow by Integrating Hospital Attributes and Population Socioeconomics with Human Mobility Data](https://arxiv.org/abs/2601.15977)
*Binbin Lin,Lei Zou,Hao Tian,Heng Cai,Yifan Yang,Bing Zhou*

Main category: cs.LG

TL;DR: 本研究整合医院属性、人口社会经济特征和空间因素，利用多模型预测医疗访问流，并通过SHAP和PDP分析各因素影响，发现Deep Gravity模型最优，且不同人群对医院评级、距离和SES的响应存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究常孤立分析影响医疗访问模式的因素（如医院属性、人口社会经济、空间因素），缺乏系统整合，导致对真实访问行为的理解受限。

Method: 融合SafeGraph移动数据与Google Maps用户评价数据，构建五种流量预测模型（Naive Regression、Gradient Boosting、MLP、Deep Gravity、HGNN），并采用SHAP和PDP方法解析多因素联合影响机制。

Result: Deep Gravity模型性能最优；医院容量、ICU占用率、评分与人气显著影响访问模式，且效应随距离变化：短距重便利，长距重评分；白人聚居区对短距评分不敏感，亚裔及高教育群体更看重评分；西班牙裔、黑人、未成年人及老年人口比例高的区域访问更频繁。

Conclusion: 医疗访问行为是多重因素非线性交互的结果，需结合模型预测与可解释性分析揭示人群异质性；政策制定应考虑地域与人口结构差异，优化资源配置与可达性设计。

Abstract: Healthcare visitation patterns are influenced by a complex interplay of hospital attributes, population socioeconomics, and spatial factors. However, existing research often adopts a fragmented approach, examining these determinants in isolation. This study addresses this gap by integrating hospital capacities, occupancy rates, reputation, and popularity with population SES and spatial mobility patterns to predict visitation flows and analyze influencing factors. Utilizing four years of SafeGraph mobility data and user experience data from Google Maps Reviews, five flow prediction models, Naive Regression, Gradient Boosting, Multilayer Perceptrons (MLPs), Deep Gravity, and Heterogeneous Graph Neural Networks (HGNN),were trained and applied to simulate visitation flows in Houston, Texas, U.S. The Shapley additive explanation (SHAP) analysis and the Partial Dependence Plot (PDP) method were employed to examine the combined impacts of different factors on visitation patterns. The findings reveal that Deep Gravity outperformed other models. Hospital capacities, ICU occupancy rates, ratings, and popularity significantly influence visitation patterns, with their effects varying across different travel distances. Short-distance visits are primarily driven by convenience, whereas long-distance visits are influenced by hospital ratings. White-majority areas exhibited lower sensitivity to hospital ratings for short-distance visits, while Asian populations and those with higher education levels prioritized hospital rating in their visitation decisions. SES further influence these patterns, as areas with higher proportions of Hispanic, Black, under-18, and over-65 populations tend to have more frequent hospital visits, potentially reflecting greater healthcare needs or limited access to alternative medical services.

</details>


### [245] [Partially Lazy Gradient Descent for Smoothed Online Learning](https://arxiv.org/abs/2601.15984)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: 本文提出k-lazyGD算法，在在线凸优化中平衡反应性与稳定性，证明在特定懒惰松弛度下可达到最优动态遗憾，且不牺牲击中性能。


<details>
  <summary>Details</summary>
Motivation: 旨在弥合贪婪在线梯度下降（OGD）与懒惰GD/对偶平均法之间的差距，探索反应性与稳定性之间的权衡。

Method: 基于Follow the Regularized Leader（FTRL）框架，提出k-lazyGD算法，并分析其在平滑在线凸优化（SOCO）中的动态遗憾；引入懒惰松弛参数k，并设计多松弛度学习器集成方法。

Result: 证明k-lazyGD在k ≤ Θ(√(T/P_T))时达到最优动态遗憾O(√((P_T+1)T))，并给出匹配的下界；表明懒惰程度可随比较器路径长度P_T自适应调整。

Conclusion: 懒惰更新可在不损害击中性能的前提下提升稳定性；通过集成不同k值的学习器，实现‘能稳则稳、需敏则敏’的自适应在线学习。

Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.

</details>


### [246] [Data-Driven Conditional Flexibility Index](https://arxiv.org/abs/2601.16028)
*Moritz Wedemeyer,Eike Cramer,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 本文提出条件灵活性指数（CFI），通过结合历史数据和上下文信息（如预测）来学习参数化的、条件化的可容许不确定性集合，从而改进传统灵活性指数，提升调度决策的鲁棒性与实用性。


<details>
  <summary>Details</summary>
Motivation: 传统灵活性指数使用简单形状（如超立方体）近似可容许不确定性区域，未利用现有上下文信息（如预测）和历史数据，导致估计不够精准和实用。

Method: 提出条件灵活性指数（CFI），利用标准化流（normalizing flow）学习从高斯基分布到数据分布的双射映射，在潜在空间中构建超球形可容许不确定性集，并通过上下文信息使其条件化。

Result: CFI能更准确刻画与当前条件相关的不确定性区域；在安全约束机组组合案例中，CFI通过引入时间信息提升了调度质量；数据驱动和条件化方法均确保仅考虑实际可能出现的参数区域。

Conclusion: CFI是对传统灵活性指数的重要扩展，兼顾数据驱动建模与上下文感知能力，为灵活过程中的鲁棒调度提供了更可靠、更具现实意义的评估工具。

Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.

</details>


### [247] [CLASP: An online learning algorithm for Convex Losses And Squared Penalties](https://arxiv.org/abs/2601.16072)
*Ricardo N. Ferreira,Cláudia Soares,João Xavier*

Main category: cs.LG

TL;DR: 本文提出了CLASP算法，用于解决约束在线凸优化（COCO）问题，通过最小化累积损失和约束违反的平方项，在凸和强凸情形下分别实现了次线性与对数级的遗憾界及惩罚累积界。


<details>
  <summary>Details</summary>
Motivation: 现有工作未充分利用凸投影算子的firm non-expansiveness性质，且缺乏对强凸约束在线优化中对数级性能保证。

Method: 提出CLASP算法，联合优化凸损失与约束违反的平方惩罚，并利用凸投影算子的firm non-expansiveness进行理论分析。

Result: 对于凸损失，CLASP实现O(T^{max{β,1−β}})遗憾和O(T^{1−β})累计平方惩罚；对于强凸问题，首次获得O(log T)遗憾和O(log T)累计平方惩罚。

Conclusion: CLASP是首个在强凸COCO设定下同时实现对数级遗憾与对数级约束惩罚累积的算法，理论分析引入了新的证明技术。

Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.

</details>


### [248] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: 本文提出了一种结合XAI（特别是SHAP）分析时间序列分解组件对模型预测影响的方法，发现模型因上下文信息不足而性能受限，并据此扩大输入窗口尺寸，从而提升了工业CPS中深度学习模型的预测性能。


<details>
  <summary>Details</summary>
Motivation: 工业信息物理系统（CPS）对安全与经济至关重要，其ML模型需高可靠性；但深度学习模型黑箱特性导致行为不可预测，亟需可解释性手段辅助评估与优化。

Method: 采用SHAP值分析时间序列数据分解各成分对模型预测的贡献，识别影响模型决策的关键上下文因素，并基于XAI洞察调整输入数据窗口大小以增强模型训练的上下文完整性。

Result: 通过XAI分析发现模型缺乏足够上下文信息，据此增大输入窗口后，模型预测性能得到提升。

Conclusion: XAI不仅能揭示模型内在机理，还可指导数据预处理与模型结构改进，是提升工业CPS中ML模型可靠性与性能的有效途径。

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


### [249] [Probably Approximately Correct Maximum A Posteriori Inference](https://arxiv.org/abs/2601.16083)
*Matthew Shorvon,Frederik Mallmann-Trenn,David S. Watson*

Main category: cs.LG

TL;DR: 本文提出了一种用于MAP（最大后验）估计的PAC（可能近似正确）算法，能在可变或固定计算预算下提供理论保证的近似最优解，并通过信息论度量刻画其可解性条件，结合概率电路实现高效求解。


<details>
  <summary>Details</summary>
Motivation: MAP估计在概率推理中至关重要但通常难以精确求解，即使在结构约束或近似方法下仍具挑战性，因此需要具备理论保证的高效近似算法。

Method: 提出PAC-MAP框架，利用信息论度量刻画可解性条件，基于概率电路设计高效随机化算法，支持独立使用或增强现有启发式方法。

Result: 实验验证了该方法在多个基准任务中优于现有方法，且能为启发式解提供严格理论保证。

Conclusion: PAC-MAP是一种兼具理论严谨性与实用性的新范式，拓展了MAP推理的可解边界，并为概率电路与随机化算法的协同提供了新思路。

Abstract: Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.

</details>


### [250] [Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets](https://arxiv.org/abs/2601.16107)
*Adithya Sineesh,Akshita Kamsali*

Main category: cs.LG

TL;DR: 本文对五种专为拉曼光谱设计的深度学习分类器在三个开源数据集上进行了系统性基准测试，采用统一训练与超参数调优协议，报告了分类准确率和宏平均F1分数，旨在提供公平、可复现的模型比较。


<details>
  <summary>Details</summary>
Motivation: 现有拉曼光谱深度学习模型评估常孤立进行或仅与传统方法对比，缺乏针对Raman专用模型在共享开源数据集上的直接、系统性比较。

Method: 在三个具有标准评估、微调及分布偏移测试能力的开源拉曼数据集上，采用统一训练流程和超参数调优策略，评估五种代表性Raman专用深度学习架构。

Result: 提供了各模型在分类准确率和宏平均F1分数上的定量结果，揭示了不同架构在不同数据分布下的性能差异。

Conclusion: 本研究填补了Raman专用深度学习模型系统性基准评测的空白，为后续研究提供了可复现的评估框架与性能参考。

Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.

</details>


### [251] [Variable Splitting Binary Tree Models Based on Bayesian Context Tree Models for Time Series Segmentation](https://arxiv.org/abs/2601.16112)
*Yuta Nakahara,Shota Saito,Kohei Horinouchi,Koshi Shimada,Naoki Ichijo,Manabu Kobayashi,Toshiyasu Matsushima*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯上下文树（BCT）的可变分割二叉树（VSBT）模型，用于时间序列分段；其树结构表示时间域上的区间划分，并通过递归逻辑回归建模分割位置，支持任意位置的精确分割；结合局部变分近似与上下文树加权（CTW）算法实现分割位置与树深度的联合估计。


<details>
  <summary>Details</summary>
Motivation: 现有BCT模型在时间序列分段中未能灵活表示时间域上的任意分割位置，限制了模型表达能力和紧凑性。

Method: 提出变量分裂二叉树（VSBT）模型，将树结构解释为时间域的递归区间划分，每个节点分裂由逻辑回归建模；采用局部变分近似处理逻辑回归，结合CTW算法进行联合推断。

Result: 在合成数据上验证了该模型和算法在时间序列分段任务中的有效性，展现出更紧凑的树结构和更精确的分割能力。

Conclusion: VSBT模型通过将逻辑回归嵌入BCT框架，实现了对时间域任意分割点的灵活建模，提升了时间序列分段的精度与效率。

Abstract: We propose a variable splitting binary tree (VSBT) model based on Bayesian context tree (BCT) models for time series segmentation. Unlike previous applications of BCT models, the tree structure in our model represents interval partitioning on the time domain. Moreover, interval partitioning is represented by recursive logistic regression models. By adjusting logistic regression coefficients, our model can represent split positions at arbitrary locations within each interval. This enables more compact tree representations. For simultaneous estimation of both split positions and tree depth, we develop an effective inference algorithm that combines local variational approximation for logistic regression with the context tree weighting (CTW) algorithm. We present numerical examples on synthetic data demonstrating the effectiveness of our model and algorithm.

</details>


### [252] [On the Intrinsic Dimensions of Data in Kernel Learning](https://arxiv.org/abs/2601.16139)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: 本文研究了核岭回归（KRR）中两种内在维度定义——Minkowski维度d_ρ和有效维度d_K，揭示了它们与核积分算子特征值衰减及泛化误差的关系，并给出了基于样本估计n-宽度的算法及理论保证。


<details>
  <summary>Details</summary>
Motivation: 基于流形假设，即输入分布支撑的内在维度越低，机器学习方法的泛化性能越好；本文旨在深入理解核岭回归中泛化能力与内在维度之间的定量关系，特别是不同维度定义（几何型d_ρ与谱型d_K）对误差界的影响。

Method: 理论分析结合算法设计：1）建立Kolmogorov n-宽度与积分算子特征值衰减的联系；2）推导受限KRR的过拟合误差上界；3）提出仅依赖有限样本估计n-宽度的算法；4）对均匀近似分布给出样本复杂度分析；5）在分形集上计算d_K并开展数值实验。

Result: 证明了n-宽度刻画了所有支撑于Ω上的概率测度μ下特征值衰减的最坏情形；得到过拟合误差界O(n^{-(2+d_K)/(2+2d_K)+ε})；设计了可证伪的n-宽度上界估计算法；对近均匀分布，样本复杂度为O(ε^{-d_ρ} log(1/ε))；发现Laplace核在分形集上d_K可远小于d_ρ。

Conclusion: 有效维度d_K比Minkowski维度d_ρ更能刻画核方法的泛化本质，尤其在不规则（如分形）数据结构下；本文建立了内在维度、谱性质与泛化误差之间的严格桥梁，并提供了可计算的维度估计工具。

Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\left(ε^{-d_ρ}\log\frac{1}ε\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.

</details>


### [253] [Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets](https://arxiv.org/abs/2601.16147)
*Muhammad Ilham Rizqyawan,Peter Macfarlane,Stathis Hadjidemetriou,Fani Deligianni*

Main category: cs.LG

TL;DR: 本文提出Beat-SSL，一种面向ECG信号的双上下文对比学习框架，结合节律级和心跳级软目标对比，提升小样本下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 获取标注ECG数据困难；现有对比学习方法忽视ECG特有结构（如节律与心跳），且依赖硬对比目标，难以刻画ECG特征相似性的连续性。

Method: 提出Beat-SSL框架，实现节律级和心跳级双上下文对比学习，并采用软对比目标替代硬目标。

Result: 在多标签心律评估任务中达到最强基线模型93%性能；在ECG分割任务中超越所有对比方法4%。

Conclusion: 双上下文+软目标的对比学习更适配ECG信号特性，在有限标注下显著提升下游任务表现，尤其利于像素级精细任务。

Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.

</details>


### [254] [Learning to Discover at Test Time](https://arxiv.org/abs/2601.16175)
*Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun*

Main category: cs.LG

TL;DR: 本文提出了一种名为TTT-Discover的新方法，通过在测试时对大语言模型（LLM）进行强化学习，使其针对特定科学问题持续优化，从而发现新SOTA解；该方法在数学、GPU核工程、算法设计和生物学等多个领域均刷新了记录，并基于开源模型和低成本实现。


<details>
  <summary>Details</summary>
Motivation: 如何利用AI为科学问题发现新的最先进（state-of-the-art）解？现有测试时缩放方法（如AlphaEvolve）仅通过提示冻结的LLM进行搜索，缺乏针对性的在线学习能力。

Method: 提出Test-Time Training to Discover（TTT-Discover）：在测试阶段对LLM进行强化学习，使其基于当前问题的特定经验持续训练；采用面向单问题最优解而非平均泛化性能的定制化学习目标与搜索策略；使用开源模型gpt-oss-120b和Tinker API实现低成本运行。

Result: 在多个领域刷新SOTA：(i) 数学中的Erdős最小重叠问题与自相关不等式；(ii) GPUMode核竞赛（提速达2倍）；(iii) AtCoder历史算法竞赛题；(iv) 单细胞分析去噪任务；所有结果均由领域专家或赛事组织方验证，且完全可复现。

Conclusion: TTT-Discover证明了测试时持续训练能有效驱动科学发现，突破依赖闭源前沿模型的局限，在保持低成本与开源可复现性的同时达成广泛领域的性能领先。

Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

</details>


### [255] [Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing](https://arxiv.org/abs/2601.16200)
*Song Xia,Meiwen Ding,Chenqi Kong,Wenhan Yang,Xudong Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种特征空间平滑（FS）方法，为多模态大语言模型（MLLMs）提供理论可证明的鲁棒性保障，并设计了即插即用模块PSM来提升其高斯鲁棒性得分，从而增强认证鲁棒性；实验表明该方法显著降低白盒攻击成功率（从约90%降至约1%），且无需模型重训练。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）虽能力强，但易受对抗扰动影响，导致特征表征失真和预测错误，亟需具备理论保证的鲁棒性提升方法。

Method: 提出特征空间平滑（FS）方法，通过高斯平滑改造特征编码器，理论证明其在ℓ₂有界攻击下能保证清洁与对抗特征间的余弦相似性下界（FCSB）；进一步设计即插即用模块PSM（Purifier and Smoothness Mapper），提升原始编码器的高斯鲁棒性得分，从而增强FS下的认证鲁棒性，且无需重训练。

Result: FS-PSM在多个MLLM和下游任务上大幅降低各类白盒攻击的成功率（ASR）——从近90%降至约1%，同时提供强理论鲁棒性保证，实证性能优于对抗训练。

Conclusion: FS提供了对MLLMs特征表示的认证鲁棒性理论保障，PSM作为轻量级插件可有效提升该保障；该方法无需重训练、通用性强、理论与实践性能兼优，为MLLM安全部署提供了新路径。

Abstract: Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\% to about 1\%.

</details>


### [256] [Counterfactual Training: Teaching Models Plausible and Actionable Explanations](https://arxiv.org/abs/2601.16205)
*Patrick Altmeyer,Aleksander Buszydlik,Arie van Deursen,Cynthia C. S. Liem*

Main category: cs.LG

TL;DR: 本文提出了一种名为反事实训练（counterfactual training）的新训练范式，通过在训练阶段引入满足合理性与可操作性要求的反事实解释，提升模型的可解释性与对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释多为训练后生成，难以保证其合理性与可操作性；本文旨在让模型在训练阶段就直接对高质量反事实解释负责。

Method: 在训练过程中将满足数据合理性和特征可变性约束的反事实样本纳入优化目标，最小化模型学习表征与理想反事实解释之间的差异。

Result: 实验与理论分析表明，该方法能生成更优的内在反事实解释，并提升模型的对抗鲁棒性。

Conclusion: 反事实训练是一种有效提升模型可解释性与鲁棒性的端到端训练策略，推动了可解释AI从后处理向内生性发展。

Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.

</details>
