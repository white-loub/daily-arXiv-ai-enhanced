<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 205]
- [cs.CL](#cs.CL) [Total: 68]
- [cs.RO](#cs.RO) [Total: 62]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.LG](#cs.LG) [Total: 164]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2512.11865)
*Ju-Young Kim,Ji-Hong Park,Myeongjun Kim,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种可解释的、对抗鲁棒的视觉-语言-动作模型（基于OpenVLA-OFT框架），通过引入Evidence-3模块检测并解释光度扰动，显著提升了农业机器人在光照、色相和噪声变化等 adversarial 条件下的动作预测精度与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有依赖RGB相机和机械臂的智慧农业系统易受光度扰动（如色相、光照、噪声变化）影响，在对抗攻击下易失效。

Method: 基于OpenVLA-OFT框架构建视觉-语言-动作模型，并集成Evidence-3模块，用于检测光度扰动并生成自然语言解释。

Result: 相比基线模型，当前动作L1损失降低21.7%，下一动作L1损失降低18.4%，验证了模型在对抗条件下的更高动作预测精度与可解释性。

Conclusion: 所提模型有效增强了智慧农业系统在复杂光度变化场景下的鲁棒性与可信性，为安全可靠的农业自动化提供了新思路。

Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

</details>


### [2] [MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding](https://arxiv.org/abs/2512.12307)
*Benjamin Beilharz,Thomas S. A. Wallis*

Main category: cs.CV

TL;DR: 本文提出MRD（可微分渲染的错觉刺激）方法，利用物理驱动的可微分渲染技术，通过寻找在模型激活上相同但物理场景参数不同的3D场景（即模型错觉刺激），来探测视觉模型对3D场景属性的隐式理解。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在视觉任务中表现优异，但其内部表征和决策机制仍难以解释；尤其人们假设模型能隐式建模3D场景，但缺乏直接、物理可解释的验证手段。

Method: 提出MRD方法：基于物理的可微分渲染器，优化3D场景参数（如形状、材质、光照），使得渲染图像在目标模型上的激活与原始场景一致，从而生成物理上不同但模型响应相同的‘错觉刺激’。

Result: 在多个模型上验证了MRD的有效性，发现目标与优化场景在模型激活上高度相似，但视觉外观差异显著；定性分析揭示了模型对几何或材质等物理属性的敏感性或不变性。

Conclusion: MRD为解析视觉模型的3D理解能力提供了首个物理可解释、参数可控的分析框架，有望推动对人工与人类视觉机制的深层理解。

Abstract: While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.

</details>


### [3] [Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion](https://arxiv.org/abs/2512.11869)
*D. Shainu Suhas,G. Rahul,K. Muni*

Main category: cs.CV

TL;DR: 本文提出Temporal-Anchor3DLane，通过多任务损失改进、轻量级时序LSTM融合模块和ESCOP式训练优化，在单目3D车道线检测中显著提升鲁棒性与时间一致性，无需额外传感器或模型扩展。


<details>
  <summary>Details</summary>
Motivation: 解决单目3D车道线检测中的深度模糊、遮挡和帧间时间不稳定性问题，以及Anchor3DLane存在的回归异常敏感、全局几何监督弱、多损失平衡难和时序利用不足等缺陷。

Method: 在Anchor3DLane基础上引入三方面改进：(1) 多任务损失（Balanced L1回归、Chamfer距离、不确定性加权、focal与Dice分类/可见性损失）；(2) 轻量级Temporal LSTM Fusion模块替代Transformer式时序融合；(3) ESCOP风格训练，联合曲线级监督与时序一致性。

Result: 在OpenLane数据集上F1指标提升+6.2，轨迹更平滑，验证了小规模架构与损失改进可显著增强3D车道检测鲁棒性。

Conclusion: 通过精细的损失设计、轻量时序建模和协同训练策略，可在不增加传感器或模型规模的前提下，有效提升单目3D车道检测的精度与时间稳定性。

Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.

</details>


### [4] [From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields](https://arxiv.org/abs/2512.12459)
*Jiachen Tao,Benjamin Planche,Van Nguyen Nguyen,Junyi Wu,Yuchun Liu,Haoxuan Wang,Zhongpai Gao,Gengyu Zhang,Meng Zheng,Feiran Wang,Anwesa Choudhuri,Zhenghao Zhao,Weitai Kang,Terrence Chen,Yan Yan,Ziyan Wu*

Main category: cs.CV

TL;DR: 本文提出高斯光子场（GPF），一种可学习的连续辐射度表示，将光子分布编码为各向异性3D高斯基元，以加速多视角渲染，兼顾物理准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统光子映射在多视角渲染中存在大量冗余计算，因每个视角需独立进行光子追踪和随机核估计，导致效率低下。

Method: 引入高斯光子场（GPF），将光子分布建模为位置、旋转、尺度和光谱参数化的各向异性3D高斯基元；GPF由首次SPPM迭代的物理光子初始化，并通过多视角最终辐射度监督进行优化。

Result: 在含复杂光传输（如焦散、镜面-漫反射交互）的场景中，GPF达到光子级精度，同时计算量降低数个数量级。

Conclusion: GPF成功统一了基于光子渲染的物理严谨性与神经场景表示的高效性，为多视角全局光照渲染提供了新范式。

Abstract: Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.

</details>


### [5] [Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops](https://arxiv.org/abs/2512.11871)
*Tekleab G. Gebremedhin,Hailom S. Asegede,Bruh W. Tesheme,Tadesse B. Gebremichael,Kalayu G. Redae*

Main category: cs.CV

TL;DR: 本文提出了一种面向埃塞俄比亚提格雷地区离线环境的仙人掌梨（Opuntia ficus-indica）病害检测系统，基于3587张田间图像构建新数据集，对比评估三种轻量级模型（自定义CNN、EfficientNet-Lite1、MobileViT-XS），发现MobileViT-XS在准确率（97.3%）上最优，而自定义CNN在部署效率（42ms延迟、4.8MB大小）上最佳；所有模型均集成于支持提格里尼亚语和阿姆哈拉语的Flutter离线App中，运行于ARM Cortex-A53设备。


<details>
  <summary>Details</summary>
Motivation: 提格雷地区农业人口占比超80%，但战后基础设施受损导致农民难以获得作物病害专家诊断，亟需适用于边缘、离线、低资源环境的本地化智能诊断工具。

Method: 构建首个本土仙人掌梨病害图像数据集（3587张，三类症状）；在资源受限条件下，对比评估三种移动端友好模型：自定义轻量CNN、EfficientNet-Lite1、CNN-Transformer混合架构MobileViT-XS；所有模型均适配ARM平台，并集成至多语言（Tigrigna/Amharic）Flutter离线App。

Result: MobileViT-XS取得最高交叉验证准确率（97.3%），证明多头自注意力（MHSA）对区分虫害聚集与二维真菌病斑更有效；EfficientNet-Lite1测试准确率90.7%；自定义CNN达89.5%准确率，且具备最优部署性能（42ms延迟、4.8MB模型大小）。

Conclusion: 在边缘农业场景中，引入全局建模能力的轻量Transformer结构（如MobileViT-XS）可显著提升病害识别鲁棒性；同时，模型轻量化与本地化语言支持是实现包容性粮食安全诊断的关键路径。

Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.

</details>


### [6] [Animus3D: Text-driven 3D Animation via Motion Score Distillation](https://arxiv.org/abs/2512.12534)
*Qi Sun,Can Wang,Jiaxiang Shang,Wensen Feng,Jing Liao*

Main category: cs.CV

TL;DR: Animus3D是一种基于文本驱动的3D动画生成框架，通过提出Motion Score Distillation（MSD）替代传统SDS方法，并结合LoRA增强视频扩散模型、反演噪声估计、时空正则化与运动细化模块，显著提升3D资产动画的运动幅度、细节与视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖标准Score Distillation Sampling（SDS）从文生视频扩散模型蒸馏运动，导致生成动画运动微弱或存在明显抖动，难以满足高质量3D动画需求。

Method: 提出Motion Score Distillation（MSD）：1）采用LoRA增强的视频扩散模型，以静态源分布替代纯噪声作为起点；2）引入基于反演的噪声估计技术保障外观一致性；3）加入显式时空正则化项抑制几何畸变；4）设计运动细化模块提升时间分辨率与细节表现。

Result: 在多个文本提示下成功为静态3D资产生成大幅、流畅且细节丰富的动画，在运动幅度、视觉质量与保真度上均优于当前最先进方法。

Conclusion: Animus3D通过改进蒸馏目标与多阶段优化策略，有效解决了文本驱动3D动画中运动不足与失真问题，为可控、高质量3D内容生成提供了新范式。

Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.

</details>


### [7] [Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training](https://arxiv.org/abs/2512.11874)
*Jiahao Jiang,Zhangrui Yang,Xuanhan Wang,Jingkuan Song*

Main category: cs.CV

TL;DR: 本文提出了一种基于SegFormer（MiT-B4骨干）的两阶段混合自训练框架，结合大量数据增强，在全球小麦全语义分割竞赛中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决全球小麦全语义分割任务中的模型精度与数据利用效率问题，提升在开发集和测试集上的泛化性能。

Method: 构建系统性自训练框架，采用两阶段混合训练策略与丰富数据增强；以SegFormer（MiT-B4 backbone）为核心模型，通过迭代师生循环逐步优化模型精度并最大化数据利用。

Result: 方法在开发阶段和测试阶段数据集上均展现出具有竞争力的性能。

Conclusion: 所提出的自训练框架有效提升了小麦语义分割精度与鲁棒性，验证了其在农业图像分割任务中的实用性与可扩展性。

Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.

</details>


### [8] [Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution](https://arxiv.org/abs/2512.12898)
*Abhinav Kumar,Tristan Aumentado-Armstrong,Lazar Valkov,Gopal Sharma,Alex Levinshtein,Radek Grzeszczuk,Suren Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种名为Qonvolutions（查询卷积）的新方法，通过在卷积中引入查询（如坐标）来增强对高频信号的学习能力，在1D回归、2D超分辨率、图像回归和新视角合成等任务上取得显著性能提升，尤其在结合高斯溅射的新视角合成中达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 神经网络因谱偏差或优化困难难以准确学习高频信号，现有方法（如傅里叶编码）仍有改进空间。

Method: 提出Qonvolutions，利用卷积的邻域特性，将低频信号与查询（如空间坐标）进行卷积操作，以增强高频信号建模能力。

Result: 在1D回归、2D超分辨率、2D图像回归和新视角合成（NVS）等多个高频学习任务上性能提升；在NVS中结合高斯溅射，实现复杂真实场景下的SOTA图像质量，超越强辐射场模型。

Conclusion: Qonvolutions是一种简单而有效的方法，能显著提升神经网络对高频信号的建模能力，并在多个视觉与图形学关键任务中展现出优越性和泛化性。

Abstract: Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.

</details>


### [9] [Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors](https://arxiv.org/abs/2512.11884)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.CV

TL;DR: 本文对比了零样本分割的SAM3模型与微调后的YOLO11系列在密集苹果实例分割任务上的性能，发现YOLO在高IoU下检测更全，而SAM3在边界精度和稳定性上显著更优。


<details>
  <summary>Details</summary>
Motivation: 评估在高密度、高遮挡场景下，通用基础模型（SAM3）与专用微调模型（YOLO11）在实例分割任务中的适用性与权衡。

Method: 在MinneApple数据集上，对比SAM3零样本模式与YOLO11（nano/medium/large）微调版本的实例分割性能，系统分析不同IoU阈值下的F1分数与边界稳定性。

Result: 在IoU=0.15时，YOLO11各变体F1达68.9%–72.2%，SAM3为59.8%；但YOLO跨IoU性能下降48–50点，SAM3仅降4点，边界稳定性高12倍。

Conclusion: SAM3在掩码边界精度与鲁棒性上优势明显，YOLO11在检测完整性上更强；任务需求决定模型选择：高精度边界选SAM3，高召回检测选YOLO11。

Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors

</details>


### [10] [mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description](https://arxiv.org/abs/2512.11894)
*Mahathir Monjur,Shahriar Nirjon*

Main category: cs.CV

TL;DR: mmWeaver是一种基于隐式神经表示（INR）和超网络的框架，用于高效生成环境特定、高保真的毫米波雷达复信号，显著提升下游任务性能并大幅加速信号合成。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达信号复杂、稀疏且高维，物理仿真计算代价高，而真实数据采集困难，亟需高效、逼真、环境自适应的信号生成与数据增强方法。

Method: 提出mmWeaver框架：1）用隐式神经表示（INR）将毫米波信号建模为连续函数，实现高压缩率；2）引入超网络，以RGB-D图像提取的环境几何特征和MotionGPT生成的文本驱动姿态特征为条件，动态生成INR参数；3）支持多分辨率I/Q信号合成，保留关键相位信息。

Result: 在复数结构相似性（complex SSIM）达0.88，PSNR达35 dB；活动识别准确率提升最高7%，人体姿态估计误差降低最高15%；合成速度比仿真方法快6–35倍。

Conclusion: mmWeaver通过语义与几何先验引导的INR建模，实现了高质量、高效率、环境自适应的毫米波信号生成，为雷达感知任务的数据增强提供了新范式。

Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.

</details>


### [11] [Computer vision training dataset generation for robotic environments using Gaussian splatting](https://arxiv.org/abs/2512.13411)
*Patryk Niżeniec,Marcin Iwanowski*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯溅射与游戏引擎物理仿真的合成数据生成流程，自动生成高保真、像素级标注的视觉数据集，有效缩小合成与真实图像间的域差距，并通过混合训练策略显著提升机器人视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决合成图像与真实图像之间的域差距问题，以及手动标注耗时费力的瓶颈。

Method: 利用3D Gaussian Splatting构建场景与物体的高保真3D表示，在游戏引擎中进行物理仿真以生成自然布局；采用两遍渲染技术，结合高斯溅射图像与代理网格生成的阴影图，算法合成物理合理的阴影与高光；自动输出像素级分割掩码并适配YOLO等检测模型。

Result: 实验表明，少量真实图像与大量合成数据混合训练，可在目标检测与分割任务上取得最优性能。

Conclusion: 该合成数据生成流程能高效产出高质量、自动标注的大规模数据集，为机器人视觉模型训练提供了一种实用且高效的替代方案。

Abstract: This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.

</details>


### [12] [Hot Hém: Sài Gòn Giũa Cái Nóng Hông Còng Bàng -- Saigon in Unequal Heat](https://arxiv.org/abs/2512.11896)
*Tessa Vu*

Main category: cs.CV

TL;DR: Hot Hém 是一个面向胡志明市的地理人工智能（GeoAI）工作流，通过融合街景图像、语义分割与遥感数据，利用XGBoost模型预测地表温度（LST），实现行人热暴露感知的路径规划。


<details>
  <summary>Details</summary>
Motivation: 热带高密度城市中行人热暴露是关键健康风险，但现有路径算法常忽略微观尺度热环境差异。

Method: 结合Google街景（GSV）影像、语义图像分割和遥感数据；在选定行政 ward（phŏng）构建GSV训练数据集，训练两个XGBoost模型预测地表温度（LST）；将模型以拼接方式部署于OSMnx提取的步行路网全部节点上。

Result: 实现了胡志明市全步行网络节点级的热暴露估计，支持热感知路径规划，并可识别基础设施尺度上异常高温的城市廊道。

Conclusion: Hot Hém 为热带城市精细化热风险评估与健康导向的城市规划提供了可扩展的GeoAI基础框架。

Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot Hém is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in Hô Chí Minh City (HCMC), Vi\d{e}t Nam, colloquially known as Sài Gòn. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as phŏng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.

</details>


### [13] [Towards Interactive Intelligence for Digital Humans](https://arxiv.org/abs/2512.13674)
*Yiyi Cai,Xuangeng Chu,Xiwei Gao,Sitong Gong,Yifei Huang,Caixin Kang,Kunhang Li,Haiyang Liu,Ruicong Liu,Yun Liu,Dianwen Ng,Zixiong Su,Erwin Wu,Yuhan Wu,Dingkun Yan,Tianyu Yan,Chang Zeng,Bo Zheng,You Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为Interactive Intelligence的新型数字人范式，通过Mio框架实现个性一致表达、自适应交互与自我演化，并构建新基准评估其能力，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 推动数字人从表面模仿迈向智能交互，解决现有数字人在个性一致性、自适应交互和持续演化方面的不足。

Method: 提出Mio（Multimodal Interactive Omni-Avatar）端到端框架，包含Thinker、Talker、Face Animator、Body Animator和Renderer五个专用模块，整合认知推理与实时多模态具身化。

Result: 在新构建的交互智能评估基准上，Mio在所有评测维度均显著优于当前最优方法。

Conclusion: Interactive Intelligence范式及Mio框架使数字人具备真正意义上的智能交互能力，为未来数字人发展提供了新方向。

Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.

</details>


### [14] [Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic](https://arxiv.org/abs/2512.11898)
*Yawar Ali,K. Ramachandra Rao,Ashish Bhaskar,Niladri Chatterjee*

Main category: cs.CV

TL;DR: 本文介绍了使用无人机在印度国家首都地区六个中段位置采集的公开微观车辆轨迹（MVT）数据集，旨在解决传统路侧视频在异构密集城市交通中因遮挡、视角受限等问题导致的数据采集困难；数据包含30Hz高精度时空轨迹及车辆动力学参数，并经过多维度验证，服务于仿真建模、安全评估与交通行为研究。


<details>
  <summary>Details</summary>
Motivation: 传统路边视频采集在密集混合交通中存在严重遮挡、视角受限和车辆运动不规则等问题，难以获取高质量微观轨迹数据；亟需一种能覆盖复杂城市区域交通场景的新型数据采集方式。

Method: 利用无人机（UAV）进行空中俯拍采集，通过Data from Sky（DFS）平台提取轨迹数据，并以人工计数、区间平均速度和探针车轨迹为基准进行数据验证；采集地点为印度国家首都地区六个具有不同交通组成与密度的中段位置，数据分辨率达30帧/秒，包含位置、速度、加速度及车辆类型等信息。

Result: 构建了六个高质量、高分辨率、经多维度验证的MVT数据集，涵盖异构区域交通典型行为特征（如车道保持偏好、速度分布、横向机动模式），并完成初步探索性分析。

Conclusion: 该工作首次系统性地公开了基于无人机采集的异构城市区域交通微观轨迹数据集，为全球研究者提供了支撑仿真建模、安全评估与行为建模的关键实证资源，推动更真实的城市交通模型发展。

Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.

</details>


### [15] [DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders](https://arxiv.org/abs/2512.13690)
*Susung Hong,Chongjian Ge,Zhifei Zhang,Jui-Hsien Wang*

Main category: cs.CV

TL;DR: 本文提出了DiffusionBrowser，一种模型无关、轻量级的解码器框架，支持在视频扩散模型去噪过程中的任意时间步或Transformer块生成多模态预览（如RGB和场景本征），实现超实时（4倍）预览，并支持通过随机性重注入和模态引导进行交互式生成控制，同时揭示了去噪过程中场景细节的逐步构建机制。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型存在生成不精确、速度慢、生成过程不透明等问题，用户难以实时了解和干预生成过程。

Method: 提出DiffusionBrowser框架，作为模型无关的轻量级解码器，在去噪中间步骤（任意timestep或transformer block）提取特征并生成多模态预览（RGB + 场景本征）；引入随机性重注入与模态引导实现交互式控制；利用训练好的解码器对扩散过程进行系统探针分析。

Result: 可在<1秒内生成4秒视频的多模态预览（>4×实时），预览与最终视频在外观和运动上保持一致；支持中间噪声步的交互式引导；揭示了场景、物体等细节在去噪过程中的渐进式组合规律。

Conclusion: DiffusionBrowser显著提升了视频扩散模型的可解释性、可控性与交互效率，为黑盒生成过程提供了可视化与干预新范式。

Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.

</details>


### [16] [Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models](https://arxiv.org/abs/2512.11899)
*Futa Waseda,Shojiro Yamabe,Daiki Shiono,Kento Sasaki,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: 本文提出Read-or-Ignore VQA（RIO-VQA）任务与RIO-Bench基准，旨在评估和提升大视觉语言模型在图文联合推理中自适应选择读取或忽略图像内文本的能力，揭示了现有方法在鲁棒性与文本理解能力间的失衡，并提出一种数据驱动的自适应防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM评测与防御方法多聚焦于物体识别、隐含鼓励忽略图像文本以提升鲁棒性，但真实场景常需图文协同推理（如识别人行道同时理解交通标志），导致评估范围与实际需求存在根本错位。

Method: 提出RIO-VQA新任务，定义模型需依据上下文动态决定读取或忽略图像文本；构建RIO-Bench基准，为每张真实图像提供仅文本/问题类型变化的同场景反事实样本（read/ignore对）；基于该基准设计数据驱动的自适应文本选择防御机制。

Result: 实验表明当前强LVLM及主流防御方法均无法兼顾抗字形攻击鲁棒性与必要文本理解能力；RIO-Bench成功支撑了一种超越传统非自适应文本忽略策略的新防御方法。

Conclusion: 本工作揭示了LVLM评估范式与现实需求的根本错位，通过RIO-VQA任务与RIO-Bench基准，为构建真正可靠、可自适应图文推理的LVLM提供了原则性路径。

Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.

</details>


### [17] [CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities](https://arxiv.org/abs/2512.11901)
*Santosh Patapati*

Main category: cs.CV

TL;DR: CLARGA是一种通用的多模态融合架构，通过样本级自适应图注意力机制实现任意数量和类型模态的高效融合，并支持缺失模态处理与跨模态一致性学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常针对特定模态组合设计，缺乏通用性、对缺失模态鲁棒性不足，且计算复杂度随模态数增长过高。

Method: 提出基于多头图注意力网络（GAT）的样本级动态注意力加权图构建机制；引入可学习掩码处理缺失模态；采用监督损失与对比InfoNCE损失联合优化。

Result: 在7个涵盖金融、人机交互、多媒体分类与情感计算的数据集上一致优于基线模型、SOTA方法及消融变体；展现出强缺失模态鲁棒性与小众任务适应能力。

Conclusion: CLARGA是一种即插即用、高效、通用且鲁棒的多模态融合框架，适用于广泛机器学习任务。

Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.

</details>


### [18] [Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life](https://arxiv.org/abs/2512.11905)
*Ming-Zher Poh,Shun Liao,Marco Andreetto,Daniel McDuff,Jonathan Wang,Paolo Di Achille,Jiang Wu,Yun Liu,Lawrence Cai,Eric Teasley,Mark Malhotra,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: 本研究利用智能手机被动采集的自然微笑视频，通过深度学习量化微笑强度，发现其与国家幸福感调查数据及日重建法结果高度相关，表明被动智能手机感知可作为研究情感行为动态的有效生态学方法。


<details>
  <summary>Details</summary>
Motivation: 主观幸福感的科学测量传统上依赖易受回忆偏差和高参与者负担影响的自我报告方法，导致对日常生活中幸福感表达的理解存在空白。

Method: 分析233名同意参与者的405,448段被动录制的智能手机视频片段，使用深度学习模型量化微笑强度，并考察其昼夜节律和周内模式，同时关联国家幸福感调查数据、日重建法结果及物理活动、光照和手机使用等变量。

Result: 每日微笑强度与国家幸福感调查数据显著相关（r=0.92），昼夜节律与日重建法结果高度一致（r=0.80）；更高日均微笑强度显著关联更多身体活动和更大光照暴露，但与智能手机使用无显著关联。

Conclusion: 被动智能手机感知可作为一种强大且具生态效度的方法，用于研究情感行为动态，并有望在人群层面推进该领域理解。

Abstract: Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.

</details>


### [19] [MPath: Multimodal Pathology Report Generation from Whole Slide Images](https://arxiv.org/abs/2512.11906)
*Noorul Wahab,Nasir Rajpoot*

Main category: cs.CV

TL;DR: 本文提出MPath，一种轻量级多模态框架，通过视觉前缀提示机制将WSI视觉特征注入预训练生物医学语言模型BioBART，无需端到端联合训练，实现病理报告自动生成。


<details>
  <summary>Details</summary>
Motivation: 解决从全切片图像（WSI）自动生成临床连贯诊断报告的难题，因组织形态高度可变且病理叙述结构复杂。

Method: 利用CONCH+Titan提取WSI视觉特征，通过紧凑投影模块将其作为可学习的视觉前缀注入冻结参数的BioBART语言模型，实现多模态条件生成。

Result: 在RED 2025 Grand Challenge数据集上开发并评估，Test Phase 2排名第四，验证了该方法的有效性与数据高效性。

Conclusion: 基于提示的多模态条件生成是一种可扩展、可解释且高效的病理报告生成新范式。

Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.

</details>


### [20] [FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications](https://arxiv.org/abs/2512.11925)
*Mozhgan Hadadi,Talukder Z. Jubery,Patrick S. Schnable,Arti Singh,Bedrich Benes,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: FloraForge is an LLM-assisted framework enabling plant scientists to generate biologically accurate, fully parametric 3D plant models via natural language refinements, bypassing the need for programming or geometric modeling expertise.


<details>
  <summary>Details</summary>
Motivation: Existing 3D plant modeling methods are either data-hungry and non-editable (learning-based) or require expert procedural modeling knowledge—both limiting accessibility for domain scientists.

Method: FloraForge uses LLM-enabled co-design to iteratively refine human-readable Plant Descriptors (PDs), generating Python scripts that produce hierarchical B-spline surface representations with botanical constraints; outputs include tessellated triangular meshes and parametric metadata for analysis.

Result: Validated on maize, soybean, and mung bean; fits procedural models to empirical point clouds via manual PD refinement; produces dual-output meshes (visualization + analysis-ready with parametric metadata).

Conclusion: FloraForge democratizes rigorous, editable, physics-compatible 3D plant modeling for domain experts by unifying LLM assistance, continuous mathematical representation, and intuitive natural language control.

Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.

</details>


### [21] [TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder](https://arxiv.org/abs/2512.11926)
*Qinghao Meng,Chenming Wu,Liangjun Zhang,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出了一种联合点云补全与3D目标检测的框架TransBridge，通过Transformer-based上采样模块融合检测与补全特征，在不增加计算成本前提下提升远距离稀疏点云下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 远距离区域LiDAR点云极度稀疏，导致3D目标检测性能下降，现有稠密化策略难以兼顾精度与效率。

Method: 提出TransBridge——一种基于Transformer的上采样模块，联合优化检测与补全网络；设计DSRecon模块生成密集LiDAR真值用于补全训练；利用Transformer建模通道与空间关系以生成高分辨率补全特征图。

Result: 在nuScenes和Waymo数据集上验证有效：端到端检测mAP提升0.7–1.5；两阶段检测mAP最高提升5.78。

Conclusion: 联合补全与检测的框架能有效利用隐式补全特征增强稀疏区域检测能力，具备良好泛化性与实用性。

Abstract: 3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.

</details>


### [22] [MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion](https://arxiv.org/abs/2512.11928)
*Alexander Peysakhovich,William Berman,Joseph Rufo,Felix Wong,Maxwell Z. Wilson*

Main category: cs.CV

TL;DR: MONET is a diffusion model that predicts cell painting channels from brightfield images, enabling virtual cell painting for dynamic studies and in-context learning across cell lines and protocols.


<details>
  <summary>Details</summary>
Motivation: Cell painting is labor-intensive and requires chemical fixation, preventing the study of live cell dynamics.

Method: A diffusion model (MONET) trained on a large dataset to predict cell painting channels from brightfield images; uses a consistency architecture for time-lapse video generation and in-context learning.

Result: Model quality improves with scale; enables time-lapse video generation without cell painting video training data; supports partial transfer to out-of-distribution cell lines and imaging protocols.

Conclusion: Virtual cell painting via MONET serves as a complementary tool—not a full replacement—for physical cell painting, enabling novel biological research workflows.

Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.

</details>


### [23] [Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching](https://arxiv.org/abs/2512.12610)
*Wonseok Choi,Sohwi Lim,Nam Hyeon-Woo,Moon Ye-Bin,Dong-Ju Jeong,Jinyoung Hwang,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 本文提出了Patchify，一种无需微调的、基于图像块的实例级图像检索框架，通过将数据库图像划分为结构化局部块并与全局查询特征匹配，提升了检索性能、可扩展性和可解释性；同时引入了定位感知评估指标LocScore，以衡量检索结果的空间准确性，并验证了其在多个基准上的优越性。


<details>
  <summary>Details</summary>
Motivation: 实例级图像检索面临目标尺寸、位置和外观变化等挑战，现有方法在性能、可扩展性和可解释性之间难以兼顾，且缺乏对空间定位准确性的有效评估。

Method: 提出Patchify框架：将数据库图像划分为少量结构化图像块，提取局部特征并与全局查询描述符进行匹配；引入LocScore作为定位感知评估指标；结合Product Quantization实现高效大规模检索，并强调压缩过程中使用信息量丰富的特征。

Result: Patchify在多个基准、骨干网络和区域选择策略下均优于全局检索方法，并能有效补充现有重排序流程；LocScore提供了对检索空间正确性的量化诊断能力；Product Quantization配合信息丰富特征显著提升压缩后性能。

Conclusion: Patchify是一种简单高效、无需微调、兼具高性能、可扩展性与可解释性的实例级图像检索新范式；LocScore为检索系统提供了新的评估维度；特征选择对量化压缩效果至关重要。

Abstract: Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/

</details>


### [24] [Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains](https://arxiv.org/abs/2512.11939)
*Clément Fernandes,Wojciech Pieczynski*

Main category: cs.CV

TL;DR: 本文提出了一种结合上下文Peano扫描（CPS）与证据隐马尔可夫链（HEMC）的新模型HEMC-CPS，用于无监督图像分割，并在合成与真实图像上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于Peano扫描的隐马尔可夫链（HMC）方法虽高效，但对复杂图像建模能力有限；上下文Peano扫描（CPS）和证据推理（HEMC）各自展现出改进潜力，但尚未融合。

Method: 提出HEMC-CPS模型，将上下文Peano扫描与证据隐马尔可夫链相结合；采用随机EM（SEM）算法进行无监督参数估计；使用贝叶斯最大后验模式（MPM）进行图像分割。

Result: HEMC-CPS在合成与真实图像的无监督分割任务中表现有效；模型具备拓展至3D或多源多分辨率图像的潜力；CPS与HEMC-CPS框架亦适用于其他空间相关数据。

Conclusion: HEMC-CPS成功融合了上下文扫描结构与证据推理能力，提升了HMC类方法对复杂空间数据的建模与分割性能，且保持计算效率优势。

Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.

</details>


### [25] [DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition](https://arxiv.org/abs/2512.11941)
*Jingmin Zhu,Anqi Zhu,James Bailey,Jun Liu,Hossein Rahmani,Mohammed Bennamoun,Farid Boussaid,Qiuhong Ke*

Main category: cs.CV

TL;DR: 本文提出DynaPURLS框架，通过多尺度视觉-语义对齐与推理时动态文本特征优化，显著提升零样本骨架动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态类别级语义对齐，难以弥合已见类与未见类之间的领域偏移，无法有效迁移细粒度视觉知识。

Method: 利用大语言模型生成涵盖全局运动与局部肢体动态的分层文本描述；设计自适应划分模块，按语义对骨架关节点进行细粒度分组；引入动态优化模块，在推理时通过轻量可学习投影适配文本特征，并结合置信度感知、类别均衡的记忆库稳定优化过程。

Result: 在NTU RGB+D 60/120和PKU-MMD三个大规模基准数据集上显著超越现有方法，达到新SOTA。

Conclusion: DynaPURLS通过建模多尺度视觉-语义对应关系并支持推理时动态优化，有效缓解了零样本骨架动作识别中的领域偏移问题，提升了跨类别泛化能力。

Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS

</details>


### [26] [SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition](https://arxiv.org/abs/2512.12885)
*Minghao Zhu,Zhihao Zhang,Anmol Sidhu,Keith Redmill*

Main category: cs.CV

TL;DR: 本文提出了一种基于检索增强生成（RAG）范式的零样本道路标志识别框架，利用视觉语言模型（VLM）生成图像文本描述，再通过大语言模型（LLM）在检索到的候选标志中进行细粒度推理，无需任务特定训练，在303类标志数据集上取得优异效果。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法难以应对道路标志类别繁多及构建大规模标注数据集不现实的问题。

Method: 结合视觉语言模型（VLM）生成图像文本描述，检索向量数据库中的相关标志候选，并由大语言模型（LLM）对候选集进行推理识别。

Result: 在理想参考图像上准确率达95.58%，在真实道路数据上达82.45%。

Conclusion: RAG架构可实现无需任务特定训练的可扩展、高精度道路标志识别系统。

Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.

</details>


### [27] [A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer](https://arxiv.org/abs/2512.11977)
*Sushmita Nath*

Main category: cs.CV

TL;DR: 本文研究了在数据受限条件下，使用Data-Efficient Image Transformer（DeiT）进行半导体晶圆图缺陷分类的效果，结果表明DeiT在准确率、F1分数和训练收敛速度上均优于多种CNN模型。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中晶圆缺陷检测对预测性维护至关重要，但现有CNN模型在小样本、类别不平衡数据下性能下降。

Method: 采用Data-Efficient Image Transformer（DeiT）模型，在受限且不平衡的晶圆图数据集上进行缺陷分类实验，并与VGG-19、SqueezeNet、Xception及Hybrid模型对比。

Result: DeiT达到90.83%的最高分类准确率，F1-score为90.78%，训练收敛更快，对少数类缺陷检测更具鲁棒性，显著优于各CNN基线模型。

Conclusion: Transformer架构（如DeiT）在数据受限的半导体缺陷检测任务中具有显著优势，可有效支撑预测性维护策略。

Abstract: Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.

</details>


### [28] [Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion](https://arxiv.org/abs/2512.12935)
*Toan Le Ngo Thanh,Phat Ha Huu,Tan Nguyen Dang Duy,Thong Nguyen Le Minh,Anh Nguyen Nhu Tinh*

Main category: cs.CV

TL;DR: 本文提出了一种统一的多模态时刻检索系统，通过级联双嵌入管道、时序感知评分机制和代理引导查询分解，解决了现有方法在跨模态噪声、时序建模和手动模态选择方面的三大挑战。


<details>
  <summary>Details</summary>
Motivation: 视频内容呈指数增长，亟需高效的多模态时刻检索系统；但现有方法存在固定权重融合策略不鲁棒、时序建模难以兼顾事件连贯性与不合理间隔惩罚、以及需人工选择模态降低可用性等关键问题。

Method: 1）级联双嵌入流程：BEIT-3与SigLIP用于粗检，BLIP-2进行重排序；2）时序感知评分机制：在束搜索中对大时间间隔施加指数衰减惩罚，以构建连贯事件序列；3）Agent引导的查询分解（GPT-4o）：自动解析模糊查询，分解为视觉/OCR/ASR子查询，并自适应融合得分。

Result: 定性分析表明，该系统能有效处理模糊查询、检索出时序连贯的片段，并动态调整融合策略，显著提升交互式时刻搜索能力。

Conclusion: 所提系统在鲁棒性、时序建模能力和自动化程度上均优于现有方法，为多模态时刻检索提供了新范式。

Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.

</details>


### [29] [CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction](https://arxiv.org/abs/2512.11988)
*Xianghui Xie,Bowen Wen,Yan Chang,Hesam Rabeti,Jiefeng Li,Ye Yuan,Gerard Pons-Moll,Stan Birchfield*

Main category: cs.CV

TL;DR: 本文提出了CARI4D，首个无需预设物体类别、能从单目RGB视频中重建空间与时间一致的4D人-物交互的方法，通过姿态假设选择、渲染-对比联合优化及物理约束接触推理实现高精度重建，并在分布内和未见数据集上分别提升38%和36%。


<details>
  <summary>Details</summary>
Motivation: 从单目RGB图像准确恢复4D人-物交互具有挑战性，主要受限于未知的人体与物体信息、深度模糊、遮挡和复杂运动，导致难以实现一致的3D和时序重建；以往方法依赖物体模板或受限于有限物体类别。

Method: 提出CARI4D方法：1）基于基础模型生成多组姿态假设；2）采用可学习的‘渲染-对比’范式联合优化，确保空间、时间和像素对齐；3）建模并推理精细接触关系，引入物理约束进一步优化。

Result: 在分布内数据集上重建误差降低38%，在未见类别数据集上降低36%；支持零样本泛化至真实互联网视频。

Conclusion: CARI4D是首个类别无关、单目、度量尺度下时空一致的4D人-物交互重建方法，显著提升鲁棒性与泛化能力，代码与预训练模型将开源。

Abstract: Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

</details>


### [30] [V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions](https://arxiv.org/abs/2512.11995)
*Chenrui Fan,Yijun Liang,Shweta Bhardwaj,Kwesi Cobbina,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出V-REX评估套件，用于评测视觉语言模型在多步探索式视觉推理任务中的能力，将其分解为‘规划’（生成探索性问题链）和‘执行’（按链回答问题）两个维度，并通过有限选项实现对中间步骤的可靠量化分析。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在面对需多轮探索与空间推理的开放性复杂任务时表现不佳，且缺乏有效评估其多步推理路径的方法。

Method: 构建V-REX评估套件，包含多领域挑战性视觉推理基准与评估协议；将多步推理建模为问题链（Chain-of-Questions），并解耦为‘规划’与‘跟随’两项能力；每步提供有限选项以支持细粒度、可量化的中间步骤评估。

Result: 在SOTA闭源与开源VLM上验证了V-REX的有效性，揭示了模型能力随规模增长的趋势、规划与执行能力间的显著差距，以及多步探索推理仍有较大提升空间。

Conclusion: V-REX为视觉语言模型的多步探索式推理提供了首个系统、可量化、细粒度的评估框架，有助于推动更鲁棒、可解释的视觉推理研究。

Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.

</details>


### [31] [Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus](https://arxiv.org/abs/2512.12012)
*Antonio Guillen-Perez*

Main category: cs.CV

TL;DR: Semantic-Drive 是一种本地优先、神经符号融合的语义数据挖掘框架，用于高效挖掘自动驾驶中的长尾安全关键事件；它通过符号定位与认知分析两阶段解耦感知，并引入多模型共识机制抑制幻觉，在nuScenes上显著提升召回率并降低风险评估误差，且可在消费级GPU上运行。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）因缺乏“长尾”训练数据而发展受限，而人工筛选罕见安全关键事件成本高、效率低；现有方案存在精度不足或侵犯隐私、开销大等问题。

Method: 提出Semantic-Drive框架：第一阶段用实时开放词汇检测器YOLOE进行符号接地以锚定注意力；第二阶段用推理型视觉语言模型（VLM）进行法医式场景分析；并设计基于多模型‘Judge-Scout’共识的‘System 2’推理时对齐策略以抑制幻觉。

Result: 在nuScenes数据集上，相比CLIP，Recall达0.966（CLIP为0.475），风险评估误差降低40%；系统可在NVIDIA RTX 3090等消费级硬件上本地运行。

Conclusion: Semantic-Drive提供了一种高效、隐私保护、低成本的本地化语义数据挖掘新范式，显著提升长尾事件挖掘能力，推动自动驾驶数据闭环建设。

Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

</details>


### [32] [Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition](https://arxiv.org/abs/2512.12013)
*Senhao Gao,Junqing Zhang,Luoyu Mei,Shuai Wang,Xuyu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于星型图和离散动态图神经网络（DDGNN）的毫米波雷达点云人体活动识别方法，有效解决了点云稀疏与尺寸可变问题，在真实数据集上达到94.27%准确率，并在树莓派4上验证了轻量化部署能力。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达点云具有稀疏性和尺寸可变性，传统借鉴视觉密集点云的预处理方法不适用，需专为雷达特性设计特征提取方法。

Method: 构建以人工添加静态中心点为根、动态雷达点为叶的星型图，建模跨帧高维相对关系；采用离散动态图神经网络（DDGNN）学习该可变大小图结构中的时空特征。

Result: 在真实HAR数据集上分类准确率达94.27%，接近基于视觉骨架数据的97.25%；在树莓派4上完成有效推理；消融实验验证DDGNN结构有效性；优于三个最新雷达专用方法，且无需重采样或帧聚合器。

Conclusion: 所提星型图+DDGNN框架能高效建模毫米波雷达点云的时空动态特性，兼顾精度与边缘部署可行性，为雷达感知HAR提供了新范式。

Abstract: Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.

</details>


### [33] [TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding](https://arxiv.org/abs/2512.13511)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出TARA（Time Aware Retrieval Adaptation）方法，通过无需视频数据的方式将多模态大语言模型（MLLMs）适配为时间感知的视频-文本嵌入模型，并构建新基准评估其时间感知能力，在多个任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频-文本检索模型缺乏对时间信息（如动作时序、方向）的显式建模能力，难以区分时间上互为镜像的‘手性动作’（chiral actions），因此需要构建具备时间感知能力的通用嵌入模型。

Method: 提出TARA方法，利用文本指令微调多模态大语言模型（MLLMs），使其在不使用任何视频训练数据的前提下，学习时间相关的语义表征；同时设计包含手性动作硬负样本的新基准用于评估时间感知能力。

Result: TARA在新提出的chiral benchmark上显著超越所有现有视频-文本模型，并在标准基准上表现优异；此外，在NegBench（否定理解）及动词/副词理解任务中也取得SOTA结果。

Conclusion: TARA是一种简单高效、无需视频数据即可实现时间感知能力的视频-文本嵌入建模方法，兼具泛化性、零样本能力和多维度语义理解优势。

Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.

</details>


### [34] [Adaptive federated learning for ship detection across diverse satellite imagery sources](https://arxiv.org/abs/2512.12053)
*Tran-Vu La,Minh-Tan Pham,Yu Li,Patrick Matgen,Marco Chini*

Main category: cs.CV

TL;DR: 本文研究了联邦学习（FL）在多源卫星图像船舶检测中的应用，验证了其在保护数据隐私的同时实现接近全局训练性能的有效性，并对比分析了多种FL算法及超参数配置的影响。


<details>
  <summary>Details</summary>
Motivation: 解决商业卫星影像和敏感船舶标注数据因隐私或商业限制难以集中共享的问题，提供一种无需数据共享的隐私保护式船舶检测方案。

Method: 在多个异构卫星数据集上，采用FedAvg、FedProx、FedOpt和FedMedian四种联邦学习算法训练YOLOv8船舶检测模型，并与仅在本地数据集上独立训练的基线方法进行对比；同时分析通信轮数和本地训练周期等配置对性能的影响。

Result: 所有FL模型均显著优于本地独立训练，在检测精度上接近使用全部数据进行全局训练的效果；不同FL算法及超参数配置对最终精度和计算效率有明显影响。

Conclusion: 联邦学习是适用于多源卫星船舶检测的可行且高效的隐私保护范式，但需合理选择FL算法与训练配置以平衡精度与效率。

Abstract: We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.

</details>


### [35] [Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management](https://arxiv.org/abs/2512.12056)
*Maria Rodriguez,Minh-Tan Pham,Martin Sudmanns,Quentin Poterek,Oscar Narvaez*

Main category: cs.CV

TL;DR: 本文提出了一种面向应急响应的高效率烧伤区域（BA）语义分割方法，基于SPOT-6/7影像，结合辅助任务（土地覆被）和测试时增强（TTA），在保持低推理延迟的同时提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有烧伤区域（BA）制图方法虽依赖遥感影像与计算机视觉模型，但未充分考虑应急响应中对时效性的严苛要求。

Method: 构建监督式语义分割工作流，采用U-Net与SegFormer模型，在SPOT-6/7高分辨率影像上训练；引入土地覆被数据作为多任务辅助输入，并应用测试时增强（TTA）与混合精度优化。

Result: U-Net与SegFormer在小样本下性能相近，但SegFormer资源消耗大；加入土地覆被辅助任务提升了鲁棒性且不增加推理时间；TTA可提升精度但延长推理时间，混合精度可缓解该问题。

Conclusion: 面向应急场景的BA制图需兼顾精度、速度与资源效率；多任务学习与轻量级增强策略是更实用的技术路径。

Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.

</details>


### [36] [CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos](https://arxiv.org/abs/2512.12060)
*Tejas Panambur,Ishan Rajendrakumar Dave,Chongjian Ge,Ersin Yumer,Xue Bai*

Main category: cs.CV

TL;DR: 本文提出CreativeVR框架，利用扩散先验引导的视频修复方法，专门针对AI生成视频和真实视频中的严重结构与时间伪影进行修复，并引入新的AIGC54基准用于评估。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频扩散模型在细粒度结构（如人脸、手部、背景、运动一致性）上表现脆弱，而传统视频修复方法对这类结构性伪影修复效果差，且缺乏对感知质量与保真度权衡的可控性。

Method: 提出基于深度适配器的CreativeVR框架，引入一个精度调节旋钮以平衡输入保真度与结构/运动修正能力；关键创新是训练中使用的时序一致退化模块，能生成逼真的结构性失败样本。

Result: CreativeVR在严重伪影视频修复任务上达到SOTA，在标准视频修复基准上也具竞争力，720p分辨率下单卡A100达约13 FPS推理速度。

Conclusion: CreativeVR为AI生成内容和低质真实视频提供了高效、可控、高质量的结构与时间一致性修复方案，推动了AIGC内容后处理技术的发展。

Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.

</details>


### [37] [BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models](https://arxiv.org/abs/2512.12080)
*Ryan Po,Eric Ryan Chan,Changan Chen,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 本文提出Backwards Aggregation（BAgger），一种自监督方法，通过构建模型自身rollout中的修正轨迹来缓解自回归视频模型的暴露偏差问题，提升长时序生成的稳定性与视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频模型在训练和推理阶段存在暴露偏差（exposure bias）：训练使用真实帧作为上下文，而推理依赖自身生成帧，导致误差累积、质量漂移。

Method: 提出BAgger方法，利用模型自身rollout生成的错误轨迹，反向构造修正路径，以自监督方式训练；采用标准得分匹配或流匹配目标函数，无需大教师模型或长程时间反向传播。

Result: 在因果扩散Transformer上实现BAgger，在文本到视频生成、视频扩展和多提示生成任务中，显著提升了长时序运动稳定性与视觉一致性，减少了质量漂移。

Conclusion: BAgger是一种高效、轻量且通用的自监督校正机制，有效缓解暴露偏差，兼顾生成质量与多样性，无需复杂蒸馏或强分布约束。

Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.

</details>


### [38] [RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer](https://arxiv.org/abs/2512.12083)
*Guanfang Dong,Luke Schultz,Negar Hassanpour,Chao Gao*

Main category: cs.CV

TL;DR: 本文提出RePack框架，通过将高维视觉基础模型（VFM）表征投影到低维流形，压缩为解码器友好的紧凑表示，以缓解信息过载问题，显著加速Diffusion Transformer（DiT）收敛并提升图像重建性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉基础模型（VFMs）的高维表征虽能增强潜在扩散模型（LDMs），但其高维性易导致信息过载，尤其当VFM特征维度超过原始图像解码所需时。

Method: 提出RePack（Representation Packing）框架：将VFM表征（如DINOv3）通过低维流形投影进行压缩，生成更紧凑、解码器友好的表示，滤除非语义噪声，保留核心结构信息。

Result: RePack在DiT-XL/2上仅用64个epoch即达FID=3.66，比当前最优方法快35%；显著加速DiT收敛，且优于直接注入原始VFM特征的方法。

Conclusion: RePack成功提取VFM表征的核心语义，规避其高维副作用，在保持高性能的同时提升训练效率，为VFM与扩散模型融合提供新思路。

Abstract: The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.

</details>


### [39] [VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering](https://arxiv.org/abs/2512.12089)
*Zihu Wang,Boxun Xu,Yuxuan Xia,Peng Li*

Main category: cs.CV

TL;DR: 本文提出VEGAS方法，利用视觉编码器自身的注意力图在语言模型中间层注入并自适应引导token，有效抑制大视觉语言模型（LVLMs）在推理过程中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型（LVLMs）虽能联合推理图文输入，但常产生与图像证据不符的幻觉输出；核心问题在于：何种视觉注意力机制能在解码过程中有效抑制幻觉？

Method: 分析发现LVLMs幻觉常发生在最终视觉注意力未聚焦关键图像对象时，而视觉编码器的注意力更集中；进一步发现图文冲突在语言模型中层最显著，因此将视觉编码器注意力图注入中层，并据此自适应修正注意力分散的token，形成推理时方法VEGAS。

Result: 在多个基准上实验表明，VEGAS持续达到抑制幻觉的最优性能。

Conclusion: 视觉编码器自身的注意力图是抑制LVLMs幻觉的关键信号，将其在语言模型中层进行有目标的注入与引导，是一种简单、通用且高效的推理时干预策略。

Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.

</details>


### [40] [SPDMark: Selective Parameter Displacement for Robust Video Watermarking](https://arxiv.org/abs/2512.12090)
*Samar Fares,Nurbek Tastan,Karthik Nandakumar*

Main category: cs.CV

TL;DR: 本文提出了一种名为SPDMark的新型视频生成式水印框架，通过选择性地调整视频扩散模型的参数（利用LoRA实现低秩位移）来嵌入水印，并结合帧级密钥派生与最大二分匹配算法实现鲁棒、高效、不可感知的水印嵌入与提取。


<details>
  <summary>Details</summary>
Motivation: 高质量视频生成模型兴起后，亟需兼顾不可感知性、鲁棒性和计算效率的视频水印方案；现有后处理和生成式水印方法难以同时满足这三者。

Method: 提出SPDMark框架：基于选择性参数位移（Selective Parameter Displacement），采用LoRA实现层间基底偏移的加性组合，由水印密钥索引；联合训练基底偏移与水印提取器，优化消息恢复、感知相似性与时间一致性损失；使用密码哈希生成帧级水印消息，并用最大二分匹配恢复被篡改视频的帧序。

Result: 在文本到视频和图像到视频模型上验证了SPDMark能生成高度不可感知的水印，水印恢复准确率高，并对多种常见视频篡改（如裁剪、压缩、帧重排等）具有强鲁棒性。

Conclusion: SPDMark是一种高效、鲁棒且不可感知的生成式视频水印方法，为AIGC视频溯源提供了实用可行的技术路径。

Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.

</details>


### [41] [AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging](https://arxiv.org/abs/2512.12101)
*Swarn S. Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 本文研究了在光学和数字全息显微图像中全自动花粉识别的问题，发现直接使用DIHM图像效果差，通过扩大标注框和引入WGAN-SN生成合成图像进行数据增强，显著提升了检测与分类性能。


<details>
  <summary>Details</summary>
Motivation: DIHM图像存在散斑噪声、孪生像伪影且外观与明场图像差异大，导致花粉自动识别困难。

Method: 采用YOLOv8s做目标检测、MobileNetV3L做分类，并利用WGAN-SN生成合成DIHM图像进行数据增强；同时尝试扩大DIHM图像中花粉标注框以对齐光学图像。

Result: 在光学图像上检测mAP50达91.3%，分类准确率97%；DIHM图像原始表现仅为8.15%和50%，扩大标注框后提升至13.3%和54%，加入合成数据后进一步提升至15.4%（检测mAP50）。

Conclusion: GAN生成的数据增强可有效缩小DIHM与光学图像间的性能差距，推动全自动DIHM兽医成像走向实际应用。

Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.

</details>


### [42] [EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography](https://arxiv.org/abs/2512.12107)
*Yuheng Li,Yue Zhang,Abdoul Aziz Amadou,Yuxiang Lai,Jike Zhong,Tiziano Passerini,Dorin Comaniciu,Puneet Sharma*

Main category: cs.CV

TL;DR: 本文提出了首个测量驱动的多模态超声心动图数据集EchoGround-MIMIC，并基于此构建了专用视觉语言模型EchoVLM，通过两种新预训练目标（视图感知对比损失和否定感知对比损失）显著提升多项临床任务性能，为超声心动图端到端解读奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在超声心动图领域应用受限，主要由于缺乏大规模、临床导向的图像-文本数据集，以及缺少以定量测量为核心的推理能力。

Method: 构建了包含19,065对图像-文本样本的EchoGround-MIMIC数据集；提出EchoVLM模型，引入视图感知对比损失和否定感知对比损失两种新型预训练目标。

Result: 在5类共36项临床任务中达到SOTA性能，包括零样本疾病分类（AUC 86.5%）和视图分类（准确率95.1%）；验证了临床引导的多模态预训练可学习可迁移的视觉表征。

Conclusion: EchoGround-MIMIC和EchoVLM共同构成了面向超声心动图端到端解读的基础模型框架，推动了多模态医学AI在心脏病学中的临床落地。

Abstract: Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.

</details>


### [43] [A Novel Patch-Based TDA Approach for Computed Tomography](https://arxiv.org/abs/2512.12108)
*Dashti A. Ali,Aras T. Asaad,Jacob J. Peoples,Mohammad Hamghalam,Alex Robins,Mane Piliposyan,Richard K. G. Do,Natalie Gangai,Yun S. Chun,Ahmad Bashir Barekzai,Jayasree Chakraborty,Hala Khasawneh,Camila Vilela,Natally Horvat,João Miranda,Alice C. Wei,Amber L. Simpson*

Main category: cs.CV

TL;DR: 本文提出了一种面向3D CT影像的新型基于图像块（patch-based）的持续同调（PH）构建方法，显著提升了分类性能与计算效率，并开源了Python工具包Patch-TDA。


<details>
  <summary>Details</summary>
Motivation: 传统基于3D立方复形的持续同调方法在高分辨率CT图像上存在性能瓶颈和计算复杂度高的问题，亟需更高效、更适配医学体数据的TDA特征提取方法。

Method: 提出一种面向体数据（尤其是CT）的patch-based持续同调构造方法，通过分块方式构建拓扑特征，并在多个3D CT数据集上系统评估其参数影响及与3D立方复形法的性能对比。

Result: 所提方法在准确率、AUC、敏感性、特异性与F1分数上平均分别提升10.38%、6.94%、2.06%、11.58%和8.51%，同时具备更高时间效率；并开源了Python工具包Patch-TDA。

Conclusion: patch-based TDA方法优于传统3D立方复形法，是面向CT等体数据的鲁棒、高效拓扑特征工程新范式。

Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.

</details>


### [44] [A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery](https://arxiv.org/abs/2512.12128)
*Thomas Manzini,Priyankari Perali,Raisa Karnik,Robin R. Murphy*

Main category: cs.CV

TL;DR: 本文提出了目前最大的道路损伤评估与道路对齐基准数据集CRASAR-U-DRIODs，包含10场联邦级灾害的灾后无人机影像，并构建了18个基线模型；针对现有数据集规模小、分辨率低、缺乏实战验证等问题，本文通过大规模标注（657.25km道路、10类标签）和真实灾害响应（飓风Debby与Helene）验证模型；同时揭示道路线空间错位对模型性能的显著影响（Macro IoU平均下降5.6%，误标率达8%，错位率达9%），呼吁CV与机器人社区重视对齐问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有灾后道路损伤评估数据集规模小、分辨率低、缺乏操作验证的问题，并回应实践中观察到的道路线空间错位导致模型性能下降的实际挑战。

Method: 构建大规模CRASAR-U-DRIODs数据集，标注657.25km道路（10类），提供9184条道路线人工调整以实现空间对齐；训练18个基线ML模型，并在飓风Debby与Helene的实际应急响应中部署验证；定量分析道路线错位对检测性能的影响。

Result: 发现道路线未对齐导致模型Macro IoU平均下降5.596%；约8%（11km）的不良路况被误标，约9%（59km）的道路线严重偏离实际道路位置。

Conclusion: 道路空间对齐是影响灾后道路损伤评估模型鲁棒性的关键因素，当前ML/CV/机器人研究亟需系统性解决该问题，以提升灾害响应中的决策有效性。

Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.

</details>


### [45] [MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater](https://arxiv.org/abs/2512.12142)
*Björn Lütjens,Patrick Alexander,Raf Antwerpen,Til Widmann,Guido Cervone,Marco Tedesco*

Main category: cs.CV

TL;DR: 本文提出了一种融合遥感与物理模型数据的深度学习方法，生成每日100米分辨率的格网化地表融水图，显著提升了精度（95%），优于传统方法，并发布了基准数据集MeltwaterBench。


<details>
  <summary>Details</summary>
Motivation:  Greenland冰盖加速消融机制尚不明确，而高时空分辨率的地表融水分布有助于理解这些过程，但现有遥感地图难以兼顾时间与空间分辨率。

Method: 构建深度学习模型（UNet和DeepLabv3+），融合合成孔径雷达（SAR）、被动微波（PMW）、数字高程模型（DEM）及区域气候模型（RCM）输出，对Helheim冰川2017–2023年数据进行时空降尺度。以SAR反演融水为真值进行训练与评估，并发布新基准数据集MeltwaterBench。

Result: 所提深度学习方法准确率达95%，较仅用RCM（83%）或PMW（72%）的传统方法提升超10个百分点；纯SAR滑动窗口法达90%但低估极端事件。

Conclusion: 多源数据融合的深度学习方法可有效突破遥感融水制图的时空分辨率权衡瓶颈，所发布代码、数据与基准集将推动后续高分辨率融水建模研究。

Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.

</details>


### [46] [Open Horizons: Evaluating Deep Models in the Wild](https://arxiv.org/abs/2512.12146)
*Ayush Vaibhav Bhatti,Deniz Karakay,Debottama Das,Nilotpal Rajbongshi,Yuito Sugimoto*

Main category: cs.CV

TL;DR: 本文对开放世界部署中的开放集识别（OSR）和少样本类增量学习（FSCIL）进行了统一实验研究，比较了不同视觉编码器与后处理打分函数在CIFAR-10上的性能，并评估了多种增量学习方法在不同样本量下的表现。


<details>
  <summary>Details</summary>
Motivation: 开放世界部署要求模型既能识别已知类别，又能在新类别出现时保持可靠性，但现有OSR与FSCIL研究缺乏统一基准与可控对比。

Method: 在OSR任务中，固定预训练视觉编码器（ResNet-50、ConvNeXt-Tiny、CLIP ViT-B/16），采用线性探针+四种后处理打分函数（MSP、Energy、Mahalanobis、kNN）；在FSCIL任务中，基于部分冻结的ResNet-50，对比改进的SPPR、OrCo与ConCM方法在1/5/10-shot设置下的表现。

Result: CLIP ViT-B/16在OSR中展现出最强的已知/未知样本可分性，Energy打分最稳定；ConCM在FSCIL 10-shot下达84.7%准确率且混淆矩阵最清晰，所有方法在5-shot后趋于饱和。

Conclusion: 骨干网络选择与打分机制显著影响未知检测能力，而基于原型的方法（如ConCM）能更有效地缓解增量学习中的灾难性遗忘。

Abstract: Open-world deployment requires models to recognize both known categories and remain reliable when novel classes appear. We present a unified experimental study spanning open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10. For OSR, we compare three pretrained frozen visual encoders: ResNet-50, ConvNeXt-Tiny and CLIP ViT-B/16,using a linear probe and four post-hoc scoring functions, namely MSP, Energy, Mahalanobis and kNN. Across metrics,such as, AUROC, AUPR, FPR@95, and OSCR, CLIP consistently yields the strongest separability between known and unknown samples, with Energy providing the most stable performance across backbones. For FSCIL, we compare modified SPPR, OrCo, and ConCM using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios. ConCM achieves 84.7% accuracy in the 10-shot setting with the cleanest confusion matrix, while all methods show saturation beyond 5 shots. Our controlled evaluation reveals how the backbone architecture and scoring mechanisms affect unknown detection and how prototype-based methods mitigate catastrophic forgetting during incremental adaptation.

</details>


### [47] [Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video](https://arxiv.org/abs/2512.12165)
*Daniel Adebi,Sagnik Majumder,Kristen Grauman*

Main category: cs.CV

TL;DR: 本文提出了一种利用被动场景声音（如方向到达谱和双耳化嵌入）辅助视觉模型进行相对相机位姿估计的新方法，在视觉退化条件下展现出更强鲁棒性，是首个在真实视频中成功利用音频解决该问题的工作。


<details>
  <summary>Details</summary>
Motivation: 视觉方法在运动模糊、遮挡等退化条件下难以准确估计相机运动，而环境中的被动声音可能提供互补的空间线索。

Method: 将方向到达（DOA）谱和双耳化音频嵌入融入现有先进纯视觉位姿估计模型，构建轻量级音视频联合框架。

Result: 在两个大型数据集上显著优于强视觉基线，尤其在视觉信息受损时保持稳定性能。

Conclusion: 日常环境中的 incidental audio 是一种被忽视但极具潜力的信号，可有效提升相对相机位姿估计的鲁棒性与泛化能力。

Abstract: Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.

</details>


### [48] [SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation](https://arxiv.org/abs/2512.12193)
*Xuancheng Xu,Yaning Li,Sisi You,Bing-Kun Bao*

Main category: cs.CV

TL;DR: 本文提出SMRABooth方法，通过自监督编码器和光流编码器分别提取对象级的主体与运动表征，并在LoRA微调中对齐，结合主体-运动解耦策略，显著提升定制化视频生成中主体外观保真度与运动一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时保证参考图像中主体外观的高保真度和参考视频中运动模式的时间一致性，主因是缺乏面向对象的主体与运动引导机制。

Method: 提出SMRABooth框架：1）用自监督编码器提取主体表征以指导结构与语义对齐；2）用光流编码器提取与外观无关的对象级运动轨迹；3）设计时空稀疏的LoRA注入策略实现主体与运动表征的解耦微调。

Result: 在多个基准上验证了SMRABooth在主体外观保持和运动模式一致性上的优越性，显著提升了可控文本到视频生成的质量与可控性。

Conclusion: SMRABooth通过对象级表征建模与解耦微调机制，有效解决了定制化视频生成中主体与运动协同建模的难题，为可控视频生成提供了新范式。

Abstract: Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.

</details>


### [49] [Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms](https://arxiv.org/abs/2512.12199)
*Ercan Erkalkan,Vedat Topuz,Ayça Ak*

Main category: cs.CV

TL;DR: 本文提出了一种面向微小型无人机编队在带宽受限的野火环境中进行轻量级周界跟踪的方法，融合热成像与RGB图像处理、规则级边界融合、惯性反馈与低延迟嵌入式优化，实现了低功耗、高稳定性与快速部署能力。


<details>
  <summary>Details</summary>
Motivation: 在野火应急侦察中，微小型无人机受限于带宽、计算资源和GPS信号退化，亟需一种轻量、鲁棒且通信需求低的实时周界跟踪方法。

Method: 结合热图像自适应阈值与形态学细化生成粗热区掩码，利用RGB图像梯度滤波提取边缘并抑制纹理误检；采用规则级融合策略选取边界候选点，并用Ramer-Douglas-Peucker算法简化；引入周期信标与惯性反馈环提升轨迹稳定性；通过限制每帧像素操作和预计算梯度表实现<50ms嵌入式SoC延迟。

Result: 小规模仿真显示，相比纯边缘跟踪基线，平均路径长度和边界抖动降低，环境覆盖率（基于交集合并分析）保持良好；电池消耗与计算占用验证了在标准微平台实现10–15 m/s前向运动的可行性。

Conclusion: 该方法兼顾实时性、鲁棒性与能效，适用于带宽受限、GPS不可靠的野外应急场景，支持微无人机快速部署与自主周界跟踪。

Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.

</details>


### [50] [A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection](https://arxiv.org/abs/2512.12205)
*Peizheng Li,Ioannis Mavromatis,Ajith Sahadevan,Tim Farnham,Adnan Aijaz,Aftab Khan*

Main category: cs.CV

TL;DR: 本文介绍了一个大规模、长期的城市路灯视觉数据集，包含2021–2025年英国布里斯托尔22个固定角度摄像头采集的超52.6万张图像，并配套自监督CNN-VAE框架与两种漂移度量方法，用于研究视觉漂移、异常检测与MLOps。


<details>
  <summary>Details</summary>
Motivation: 解决真实城市环境中长期部署视觉模型所面临的视觉漂移、模型退化与缺乏细粒度基准数据的问题，支撑智能城市中鲁棒、可维护的视觉系统研发。

Method: 构建跨四年、多条件（光照/天气/季节）的街灯图像数据集；为每个摄像头及昼夜子集分别训练CNN-VAE；提出两个样本级漂移指标：相对质心漂移（潜空间）和相对重建误差（像素域）。

Result: 发布含526,000+图像与结构化元数据（时间、GPS、设备ID）的公开数据集；提供可复现的自监督漂移分析框架；验证了其在街灯监测、天气推断与城市场景理解等下游任务中的适用性。

Conclusion: 该数据集为评估长期模型稳定性、漂移感知学习与部署就绪视觉系统提供了现实、细粒度的基准，推动智能城市MLOps实践与视觉持续学习研究。

Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.

</details>


### [51] [ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB](https://arxiv.org/abs/2512.12206)
*Jeongjun Park,Sunwook Hwang,Hyeonho Noh,Jin Mo Yang,Hyun Jong Yang,Saewoong Bahk*

Main category: cs.CV

TL;DR: 本文提出ALERT数据集和输入尺寸无关的视觉变换器（ISA-ViT），用于基于IR-UWB雷达的驾驶员分心行为识别，显著提升了分类准确率，并推动实际部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于IR-UWB雷达的驾驶员活动识别（DAR）面临两大挑战：缺乏覆盖多种分心行为的大规模真实场景UWB数据集；以及标准ViT难以适配非标准维度的UWB雷达数据。

Method: 构建包含10220个样本、涵盖7类分心行为的ALERT真实驾驶UWB数据集；提出ISA-ViT框架，通过自适应调整patch配置、重用预训练位置编码、融合距离域与频率域特征，实现对变尺寸UWB数据的有效建模。

Result: ISA-ViT在UWB-DAR任务上相较现有ViT方法提升22.68%分类准确率；ALERT数据集及方法已开源。

Conclusion: 本工作通过新数据集与新模型协同突破，为隐私友好、鲁棒可扩展的车载分心驾驶检测系统提供了可行技术路径。

Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.

</details>


### [52] [A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction](https://arxiv.org/abs/2512.12208)
*Indranil Bhattacharjee,Vartika Narayani Srinet,Anirudha Bhattacharjee,Braj Bhushan,Bishakh Bhattacharya*

Main category: cs.CV

TL;DR: 本文提出了一种结合ResNet-50 CNN与图卷积网络（GCN）的深度学习方法，利用NAO机器人诱发的命名事件视频数据，对15名自闭症儿童的面部微表情进行七类情绪识别，首次构建了印度本土大规模ASD儿童人机交互情绪分析数据集与模型。


<details>
  <summary>Details</summary>
Motivation: 理解自闭症谱系障碍（ASD）儿童在社交互动中的情绪反应是发展心理学与人机交互领域的关键挑战；现有研究缺乏针对神经多样性儿童的细粒度、 autism-specific 情绪识别方法与真实场景数据。

Method: 构建约5万帧面部图像的数据集；采用MediaPipe FaceMesh提取视觉与几何特征；设计ResNet-50微调CNN与三层GCN融合模型；通过DeepFace与FER加权集成生成软标签；最终用KL散度优化融合嵌入实现七类情绪分类。

Result: 模型能有效捕捉ASD儿童细微情绪线索，在受控实验中展现出稳健的情绪识别性能，填补了面向自闭症人群的HRI情感建模空白。

Conclusion: 该管道是印度首个面向ASD儿童的大规模社交机器人情绪分析工作，为临床及治疗场景下的个性化辅助技术提供了重要基础与新范式。

Abstract: Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.

</details>


### [53] [CineLOG: A Training Free Approach for Cinematic Long Video Generation](https://arxiv.org/abs/2512.12209)
*Zahra Dehghanian,Morteza Abolghasemi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 本文提出CineLOG数据集和新型视频生成流程，以实现对电影级属性（如摄像机轨迹和类型）的精细控制。


<details>
  <summary>Details</summary>
Motivation: 现有可控视频合成模型难以实现超越文本提示的细粒度控制，尤其在电影属性（如摄像机轨迹、类型）方面；同时，现有数据集存在严重不平衡、标签噪声或仿真到真实差距大等问题。

Method: 构建了包含5000个高质量、平衡、未剪辑视频片段的CineLOG数据集，每条样本标注详细场景描述、基于标准电影分类法的摄像机指令及类型标签；提出四阶段解耦式文本到视频生成流程，并引入轨迹引导过渡模块以实现多镜头间平滑时空插值。

Result: 大量人工评估表明，所提流程在遵循特定摄像机与剧本指令方面显著优于当前SOTA端到端T2V模型，同时保持专业级视觉质量。

Conclusion: CineLOG数据集与解耦式生成流程为可控电影级视频合成提供了新基准与实用路径，推动了细粒度语义控制的发展。

Abstract: Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.

</details>


### [54] [Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking](https://arxiv.org/abs/2512.12218)
*Rheeya Uppaal,Phu Mon Htut,Min Bai,Nikolaos Pappas,Zheng Qi*

Main category: cs.CV

TL;DR: 本文提出了一种评估视觉语言模型（VLMs）推理链中感知步骤是否忠实于图像的新维度——‘视觉忠实性’，并设计了一个无需训练和参考的框架来逐步判断感知步骤的忠实性；在此基础上，提出一种轻量级自反思机制，可检测并局部重生成不忠实的感知步骤，从而提升多模态推理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅关注最终答案准确率，无法区分模型是否通过视觉上不忠实的中间步骤得出正确答案，或虽推理忠实却最终预测失败，因此需引入‘视觉忠实性’这一新评估维度。

Method: 提出一个无需训练和参考的框架，将推理链分解为感知与推理步骤，并利用现成的VLM作为判别器进行步骤级忠实性评估；进一步设计轻量级自反思流程，用于检测并局部重生成不忠实的感知步骤。

Result: 在多个推理增强型VLM和感知密集型基准上，该方法显著降低了‘不忠实感知率’，同时保持最终答案准确率不变。

Conclusion: 视觉忠实性是衡量多模态推理质量的关键新维度；所提无训练评估框架与自反思机制可有效提升VLM推理的可靠性与透明度。

Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.

</details>


### [55] [Fine-Grained Zero-Shot Learning with Attribute-Centric Representations](https://arxiv.org/abs/2512.12219)
*Zhi Chen,Jingcai Guo,Taotao Cai,Yuxiang Cai*

Main category: cs.CV

TL;DR: 本文提出了一种零样本学习框架Attribute-Centric Representations (ACR)，通过在表征学习阶段实现属性解耦（如颜色、形状、纹理），解决细粒度未见类别识别中的属性纠缠问题；采用双专家模块（MoPE和MoAE）进行图像块路由与属性映射，显著提升CUB、AwA2和SUN上的零样本分类性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度零样本识别中，传统模型将不同视觉属性（如颜色、形状、纹理）混入单一嵌入，导致关键差异被掩盖，而现有后处理方法无法挽回已发生的属性纠缠。

Method: 提出ACR框架，包含两个混合专家组件：1）嵌入Transformer的Mixture of Patch Experts（MoPE），通过双层路由机制将图像块分发给专精特定属性族的专家；2）Mixture of Attribute Experts（MoAE）头部，将专家优化后的特征投影为稀疏、部件感知的属性图。

Result: 在CUB、AwA2和SUN零样本学习基准上均达到一致的最先进性能。

Conclusion: 属性解耦应在表征学习阶段显式建模，而非依赖后处理；ACR通过MoPE与MoAE协同实现细粒度属性分离，有效提升零样本泛化能力。

Abstract: Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.

</details>


### [56] [ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation](https://arxiv.org/abs/2512.12220)
*Minheng Ni,Zhengyuan Yang,Yaowen Zhang,Linjie Li,Chung-Ching Lin,Kevin Lin,Zhendong Wang,Xiaofei Wang,Shujie Liu,Lei Zhang,Wangmeng Zuo,Lijuan Wang*

Main category: cs.CV

TL;DR: 本文提出了ProImage-Bench，一个面向专业图像生成（如生物学示意图、工程图纸等）的细粒度基准，包含654张真实技术图像、6076条可验证标准和44131项二元检查；通过LMM自动评估发现现有文生图模型在科学保真度上存在显著不足，但利用失败检查反馈进行迭代编辑可大幅提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有文生图模型虽在通用图像生成上表现良好，但在需信息密集、科学精确的专业图像生成任务中缺乏可靠评估手段与提升路径。

Method: 构建ProImage-Bench基准：收集真实教材与技术报告中的654张图，为其生成详细图像指令，并基于上下文文本与参考图，利用多模态大模型自动构建含6076条标准、44131项二元检查的分层评估体系；设计基于LMM的自动化裁判系统，采用有原则的惩罚机制聚合结果；进一步将失败检查反馈至编辑模型实现迭代优化。

Result: 当前最优基础模型在ProImage-Bench上整体rubric准确率仅0.791、criterion得分仅0.553；引入基于失败检查的迭代编辑后，强生成器的rubric准确率从0.653提升至0.865，criterion得分从0.388升至0.697。

Conclusion: ProImage-Bench不仅提供了专业图像生成能力的严格诊断工具，还为生成符合规范的科学插图提供了可扩展、可操作的监督信号。

Abstract: We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.

</details>


### [57] [Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs](https://arxiv.org/abs/2512.12222)
*Nathalie Alexander,Arnaud Gucciardi,Umberto Michelucci*

Main category: cs.CV

TL;DR: 本研究系统比较了SynthSeg和SamSeg两种方法在婴儿脑MRI分割中的准确性及其对体积和分形维数(FD)估计的影响，发现SynthSeg在各项指标上均优于SamSeg，是儿科MRI中更可靠的工具，但分割带来的不确定性仍需谨慎对待。


<details>
  <summary>Details</summary>
Motivation: 婴儿脑MRI的准确分割对于量化结构和复杂性的发育变化至关重要，但由于持续的髓鞘形成和组织对比度降低，自动化分割特别具有挑战性。

Method: 使用Baby Open Brains (BOB) 数据集（71次扫描，1-9个月）系统比较了SynthSeg和SamSeg两种分割方法，评估指标包括Dice系数、交并比、95%百分位Hausdorff距离和归一化互信息，并分析其对体积和分形维数(FD)估计的影响。

Result: SynthSeg在所有质量指标上均优于SamSeg（主要区域平均Dice > 0.8），其体积估计与人工参考结果接近（平均+4% [-28% - 71%]）；而SamSeg则系统性高估脑室和全脑体积（平均+76% [-12% - 190%]）。分割准确性随年龄增长而提高，且FD分析显示分割方法间的区域差异显著，分割相关FD变异性超过了大多数发育队列中报告的组间差异。

Conclusion: SynthSeg为儿科MRI提供了最可靠的体积和FD结果，但由于分割带来的小形态学差异，体积和FD偏差应谨慎解释。

Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.

</details>


### [58] [Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder](https://arxiv.org/abs/2512.12229)
*Tianyu Zhang,Dong Liu,Chang Wen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种不对称极端图像压缩（AEIC）框架，使用浅层编码器和一步扩散解码器，在超低比特率（<0.05 bpp）下兼顾编码效率与解码质量，并通过双向特征蒸馏提升浅编码器性能。


<details>
  <summary>Details</summary>
Motivation: 现有超低码率图像压缩方法依赖大型预训练编码器，难以部署于资源受限的边缘发送端设备，亟需轻量、高效的编码方案。

Method: 提出AEIC框架：采用中等或浅层编码器实现快速编码；引入一步扩散解码器保障高保真、高真实感重建；设计双向特征蒸馏机制，将中等编码器的知识迁移至浅编码器变体。

Result: 在超低码率下，AEIC在率-失真-感知性能上超越现有方法；编码速度达35.8 FPS（1080P），解码速度与现有方法相当。

Conclusion: 浅层编码器结合一步扩散解码与知识蒸馏是可行且高效的超低码率压缩路径，显著提升了边缘设备的部署可行性。

Abstract: Ultra-low bitrate image compression (below 0.05 bits per pixel) is increasingly critical for bandwidth-constrained and computation-limited encoding scenarios such as edge devices. Existing frameworks typically rely on large pretrained encoders (e.g., VAEs or tokenizer-based models) and perform transform coding within their generative latent space. While these approaches achieve impressive perceptual fidelity, their reliance on heavy encoder networks makes them unsuitable for deployment on weak sender devices. In this work, we explore the feasibility of applying shallow encoders for ultra-low bitrate compression and propose a novel Asymmetric Extreme Image Compression (AEIC) framework that pursues simultaneously encoding simplicity and decoding quality. Specifically, AEIC employs moderate or even shallow encoder networks, while leveraging an one-step diffusion decoder to maintain high-fidelity and high-realism reconstructions under extreme bitrates. To further enhance the efficiency of shallow encoders, we design a dual-side feature distillation scheme that transfers knowledge from AEIC with moderate encoders to its shallow encoder variants. Experiments demonstrate that AEIC not only outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates, but also delivers exceptional encoding efficiency for 35.8 FPS on 1080P input images, while maintaining competitive decoding speed compared to existing methods.

</details>


### [59] [Moment and Highlight Detection via MLLM Frame Segmentation](https://arxiv.org/abs/2512.12246)
*I Putu Andika Bagas Jiwanta,Ayu Purwarianti*

Main category: cs.CV

TL;DR: 本文提出了一种将多模态大语言模型（MLLM）直接用于视频时刻/高亮检测的新方法：通过强制LLM输出与输入帧一一对应的'0'/'1'字符序列，将其语言生成能力转化为帧级二值分割任务，并联合分割损失与语言建模损失进行训练。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本生成的MLLM方法无法为帧级预测提供直接梯度；强化学习等方法虽尝试解决，但不够直接稳定。

Method: 设计特定prompt使LLM对固定采样帧输出等长'0'/'1'字符序列；将字符解码为背景/前景概率，联合使用分割损失（如BCE）和因果语言建模损失训练；推理时用beam search获取时刻序列及对应saliency logits。

Result: 在QVHighlights上以仅25帧采样（<50%常规方法）达到56.74 HIT@1（高亮检测）和35.28 MAP（时刻检索），且分割损失能提供稳定辅助学习信号。

Conclusion: 将LLM输出显式约束为帧级二值序列并施加分割监督，是融合LLM语义能力与细粒度视觉定位的有效新范式。

Abstract: Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous "0" and/or "1" characters, with one character per frame. The "0"/"1" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.

</details>


### [60] [MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2512.12268)
*Yuqing Lei,Yingjun Du,Yawen Huang,Xiantong Zhen,Ling Shao*

Main category: cs.CV

TL;DR: 本文提出MetaTPT，一种元学习框架，通过自监督辅助任务指导测试时提示调优，动态学习样本级参数化增强，提升视觉-语言模型在域偏移下的测试时适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时提示调优方法（如TPT）使用固定增强，在复杂域偏移场景下表现不佳，需更灵活、更具表达力的增强策略以提升零样本泛化鲁棒性。

Method: 提出Meta Test-Time Prompt Tuning（MetaTPT），采用双循环优化：内循环学习自监督任务以生成信息丰富的增强视图；外循环基于视图一致性进行提示调优；增强本身由可学习参数动态生成。

Result: 在域泛化与跨数据集基准上达到当前最优性能，显著提升VLMs在域偏移下的测试时适应能力。

Conclusion: 耦合可学习增强与提示调优的元学习范式能有效提升视觉-语言模型对未知目标域的鲁棒性与适应性，为测试时适应提供了新思路。

Abstract: Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.

</details>


### [61] [Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions](https://arxiv.org/abs/2512.12277)
*Thibault Geoffroy,Myriam Maumy,Lionel Prevost*

Main category: cs.CV

TL;DR: 本文提出了一种用于持续学习场景下面部表情识别（FER）的混合框架，结合深度卷积特征与基于FACS的动作单元（AUs），利用贝叶斯高斯混合模型（BGMM）实现轻量、概率化建模，有效缓解灾难性遗忘，并在CFEE数据集上验证了其对基础及复合表情的渐进式学习能力与知识保持优势。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别（FER）是推断人类情绪状态的关键通道，但情绪具有动态性和文化差异性，要求模型能在持续学习中不断适应新情感类别而不遗忘旧知识。

Method: 提出一种融合深度卷积特征与FACS衍生面部动作单元（AUs）的双模态表示方法，并采用贝叶斯高斯混合模型（BGMM）进行轻量级、无需重训练的概率建模，以支持持续学习并缓解灾难性遗忘。

Result: 在CFEE数据集上，模型能先学习基本表情再逐步识别复合表情；实验表明其准确率更高、知识保留更强、遗忘更少。

Conclusion: 该框架为构建具备情感智能的AI系统提供了可持续学习的FER解决方案，适用于教育、医疗和自适应用户界面等实际场景。

Abstract: As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.

</details>


### [62] [Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection](https://arxiv.org/abs/2512.12281)
*Jiahao Zhao*

Main category: cs.CV

TL;DR: 本文提出Cognitive-YOLO框架，利用大语言模型（LLM）直接根据数据集内在特征生成目标检测网络结构，通过元特征分析、RAG增强的LLM推理及NADL编译三阶段实现高效架构合成，在多个数据集上展现出优越的性能与参数效率。


<details>
  <summary>Details</summary>
Motivation: 传统人工设计耗时耗力，NAS计算成本过高；现有LLM方法多作为迭代优化器，缺乏对数据整体特性的理解，难以直接生成适配数据特性的架构。

Method: 提出三阶段框架：1）分析模块提取数据集元特征（如物体尺度分布、场景密度）；2）LLM结合RAG检索的SOTA组件，基于元特征推理生成结构化神经架构描述语言（NADL）；3）编译器将NADL实例化为可部署模型。

Result: 在五个目标检测数据集上验证，Cognitive-YOLO生成的架构性能优异，参数效率显著优于强基线；消融实验证明LLM的数据驱动推理是性能提升主因。

Conclusion: 数据‘第一性原理’的深度理解比单纯复用SOTA组件更能催生高性能检测架构，LLM可作为直接、高效的架构合成引擎。

Abstract: Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data "first principles" is more critical for achieving a superior architecture than simply retrieving SOTA components.

</details>


### [63] [RealDrag: The First Dragging Benchmark with Real Target Image](https://arxiv.org/abs/2512.12287)
*Ahmad Zafarani,Zahra Dehghanian,Mohammadreza Davoodi,Mohsen Shadroo,MohammadAmin Fazli,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 本文提出了首个基于点的图像编辑基准RealDrag，包含400+带真实目标图像的样本，并设计了四个新指标（SeD、OMPS、IPPS、DiS）来全面评估模型性能，系统评测了17个SOTA方法，揭示了现有方法的权衡关系并建立了可复现的基准。


<details>
  <summary>Details</summary>
Motivation: 现有基于拖拽的图像编辑模型评估不可靠，缺乏标准化基准和真实目标图像数据集，导致方法间难以客观比较。

Method: 构建RealDrag基准数据集（含源图/目标图、控制点、可编辑区域掩码和描述性文本），并提出四个任务特定评估指标：语义距离（SeD）、外掩码保持分（OMPS）、内块保持分（IPPS）和方向相似度（DiS）。

Result: 在RealDrag上对17个SOTA模型进行了首次大规模系统评估，揭示了各方法在保真度、区域保持性和语义对齐性等方面的明确权衡。

Conclusion: RealDrag为点驱动图像编辑提供了首个具备真实目标图像的综合基准和配套评估工具，显著提升了该领域评估的可靠性与可比性，为未来研究奠定了坚实基础。

Abstract: The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.
  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.

</details>


### [64] [GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search](https://arxiv.org/abs/2512.12296)
*Hyunju Lee,Youngmin Oh,Jeimin Jeon,Donghyeon Baek,Bumsub Ham*

Main category: cs.CV

TL;DR: 本文提出了一种渐进式Transformer架构搜索框架GrowTAS，先训练小规模子网再逐步引入大规模子网，以缓解权重共享导致的干扰问题；并进一步提出GrowTAS+，通过选择性微调提升大子网性能，在ImageNet及多个迁移学习基准上显著优于现有TAS方法。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer架构搜索（TAS）中，所有子网共享超网权重，导致小规模子网严重受干扰；而实验发现训练良好的小规模子网可作为大规模子网的良好基础。

Method: 提出渐进式训练框架GrowTAS：从训练小规模子网开始，逐步加入更大规模子网；并扩展为GrowTAS+，仅对部分权重进行微调以增强大子网性能。

Result: 在ImageNet、CIFAR-10/100、Flowers、CARS和INAT-19等多个基准上，GrowTAS及其增强版GrowTAS+显著优于当前主流TAS方法。

Conclusion: 渐进式训练策略能有效缓解权重共享带来的干扰，提升子网训练稳定性与性能，为高效ViT自动设计提供了新范式。

Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods

</details>


### [65] [From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving](https://arxiv.org/abs/2512.12302)
*Huan Zheng,Yucheng Zhou,Tianyi Yan,Jiayi Su,Hongjun Chen,Dubing Chen,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出了Intention-Drive，首个用于评估自动驾驶系统理解并执行高阶人类意图能力的综合基准，包含复杂场景-意图配对数据集与以语义目标达成率（ISR）为核心的新型评测协议，并通过基线模型实验揭示当前方法在此任务上的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶系统仅能执行低级指令（如简单转向），难以实现真正智能的自主性；缺乏衡量和推动“将高层抽象人类意图转化为安全精准驾驶行为”能力的标准评测基准。

Method: 构建Intention-Drive基准，包括：(1) 配备自然语言意图描述的复杂驾驶场景数据集；(2) 以Intent Success Rate（ISR）为核心的新评测协议，强调语义目标达成而非几何精度。

Result: 在Intention-Drive上对多种基线模型的广泛评测表明，现有模型在综合场景理解与意图理解方面存在显著性能缺陷，难以胜任该高级任务。

Conclusion: Intention-Drive填补了高阶意图驱动自动驾驶评测的关键空白，为推动从‘指令跟随者’向‘意图实现者’的范式转变提供了标准化工具和明确改进方向。

Abstract: Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.

</details>


### [66] [OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2512.12303)
*Yang Ou,Xiongwei Zhao,Xinye Yang,Yihan Wang,Yicheng Di,Rong Yuan,Xieyuanli Chen,Xu Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为OMUDA的统一框架，通过在不同表征层次上引入分层掩码策略（上下文感知、特征蒸馏和类别解耦掩码），解决无监督域自适应语义分割中的跨域上下文模糊性、特征不一致性和伪标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应（UDA）方法在语义分割中仍难以弥合域间差距，主要受限于跨域上下文模糊性、特征表示不一致及类别级伪标签噪声。

Method: 提出OMUDA框架，包含三种分层掩码策略：1）上下文感知掩码（CAM），自适应区分前景与背景；2）特征蒸馏掩码（FDM），借助预训练模型提升特征鲁棒性与一致性；3）类别解耦掩码（CDM），建模类别级不确定性以抑制伪标签噪声。

Result: 在SYNTHIA→Cityscapes和GTA5→Cityscapes等基准上，OMUDA可无缝集成至现有UDA方法，平均性能提升7%，达到SOTA。

Conclusion: OMUDA通过在上下文、表征和类别三个层面协同减小域偏移，为UDA语义分割提供了统一且有效的解决方案。

Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.

</details>


### [67] [WeDetect: Fast Open-Vocabulary Object Detection as Retrieval](https://arxiv.org/abs/2512.12309)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 本文提出WeDetect系列模型，基于非融合的开放词汇目标检测框架，通过检索范式统一目标检测、提案生成、对象检索和指代表达理解任务，在15个基准上达到SOTA性能且推理高效。


<details>
  <summary>Details</summary>
Motivation: 探索开放词汇目标检测中无需跨模态融合层的检索范式，以提升效率与通用性。

Method: 构建WeDetect家族模型：WeDetect（双塔非融合实时检测器）、WeDetect-Uni（冻结检测器仅微调objectness prompt生成通用提案）、WeDetect-Ref（基于大语言模型的单次前向对象分类器用于指代表达理解）。

Result: 在15个基准测试中达到SOTA性能；WeDetect实现高效率实时检测；WeDetect-Uni支持历史数据中的类别无关对象检索；WeDetect-Ref摒弃next-token预测，实现高效REC。

Conclusion: 非融合检索范式在开放词汇检测中兼具高性能、高效率与强泛化能力，WeDetect系列为多任务统一提供了可行且高效的框架。

Abstract: Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.

</details>


### [68] [Unified Control for Inference-Time Guidance of Denoising Diffusion Models](https://arxiv.org/abs/2512.12339)
*Maurya Goyal,Anuj Singh,Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniCoDe的通用算法，将采样法与梯度引导法的优势结合，通过在采样过程中引入局部梯度信号，提升扩散模型在下游任务中的对齐效率与效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型输出需与下游任务目标对齐以提升性能，但现有推理时无需训练的方法（采样法和梯度法）各有局限：采样法效率低，梯度法依赖可微奖励近似且易偏离先验分布。

Method: 提出UniCoDe框架，在扩散采样过程中动态融合局部梯度信号，统一采样与梯度引导策略，兼顾高效性与对齐质量。

Result: UniCoDe在多个任务上与当前最优方法性能相当，同时提升了采样效率，并更好平衡了奖励对齐与对无条件先验的偏离程度。

Conclusion: UniCoDe为扩散模型推理时对齐提供了一种通用、高效且灵活的新范式，兼具采样法的鲁棒性与梯度法的导向性。

Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe

</details>


### [69] [TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection](https://arxiv.org/abs/2512.12357)
*Zishen Song,Yongjian Zhu,Dong Wang,Hongzhan Liu,Lingyu Jiang,Yongxing Duan,Zehua Zhang,Sihan Li,Jiarui Li*

Main category: cs.CV

TL;DR: 本文提出TCLeaf-Net，一种用于田间叶片病害检测的Transformer-Convolution混合检测器，并发布Daylily-Leaf病斑级数据集；通过TCM、RSFRS和DFPN模块分别解决复杂背景干扰、下采样信息损失及多尺度病斑检测难题，在多个数据集上实现精度与效率的双重提升。


<details>
  <summary>Details</summary>
Motivation: 田间环境下，杂乱背景、域偏移和缺乏病斑级标注数据严重制约叶片病害检测模型的鲁棒性。

Method: 构建配对病斑级数据集Daylily-Leaf（1746张图像，7839个病斑）；提出TCLeaf-Net，含三个核心模块：TCM（融合全局上下文与局部卷积以抑制非叶区域）、RSFRS（原始尺度特征召回与重采样以保留空间细节）、DFPN（可变形对齐+FPN增强多尺度融合）。

Result: 在Daylily-Leaf田间子集上mAP@50达78.2%，较基线提升5.4个百分点，计算量减少7.5 GFLOPs，GPU内存降低8.7%；在PlantDoc、Tomato-Leaf、Rice-Leaf等跨数据集测试中均优于YOLO与RT-DETR系列。

Conclusion: TCLeaf-Net兼顾精度、效率与泛化能力，验证了混合架构与针对性模块设计对真实农田病害检测的有效性，Daylily-Leaf数据集为后续研究提供了高质量病斑级基准。

Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.

</details>


### [70] [VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding](https://arxiv.org/abs/2512.12360)
*Yufei Yin,Qianke Meng,Minghao Chen,Jiajun Ding,Zhenwei Shao,Zhou Yu*

Main category: cs.CV

TL;DR: 本文提出VideoARM，一种面向长视频理解的分层记忆代理推理范式，通过自适应、细粒度的观察-思考-行动-记忆循环及分层多模态记忆机制，在降低token消耗的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 长视频理解面临时间结构长、多模态线索密集的挑战，现有方法依赖手工设计的推理流程或高开销的视频预处理，限制了效率与自主性。

Method: 提出VideoARM框架：1）控制器驱动的自适应代理推理循环（观察-思考-行动-记忆），支持粗到细的视频解析；2）构建分层多模态记忆，持续捕获并更新多级线索以支撑决策。

Result: 在主流长视频基准上超越SOTA方法DVD，同时显著降低token消耗。

Conclusion: VideoARM通过动态、分层的记忆与代理推理机制，有效平衡了长视频理解的精度与计算效率，为MLLMs在长时序视频任务中的自主推理提供了新范式。

Abstract: Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.

</details>


### [71] [STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative](https://arxiv.org/abs/2512.12372)
*Peixuan Zhang,Zijian Jia,Kaiqi Liu,Shuchen Weng,Si Li,Boxin Shi*

Main category: cs.CV

TL;DR: 本文提出STAGE框架，通过结构化分镜（start-end帧对）替代稀疏关键帧，结合多镜头记忆包、双编码策略和两阶段训练，提升多镜头视频生成的叙事连贯性与电影语言表达能力，并构建了大规模ConStoryBoard数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于关键帧的多镜头视频生成方法难以维持跨镜头一致性并捕捉电影语言，而端到端方法计算开销大、控制粒度粗。

Method: 提出STAGE工作流：1）STEP2模块预测每镜头的起止帧对构成结构化分镜；2）多镜头记忆包保障长程实体一致性；3）双编码策略增强单镜头内连贯性；4）两阶段训练学习电影级镜头间转场；5）构建ConStoryBoard大规模标注数据集。

Result: 在结构化叙事控制与跨镜头一致性方面显著优于现有方法，实验验证了各模块有效性及整体性能提升。

Conclusion: STAGE为多镜头视频生成提供了更可控、更高效且更具电影表现力的新范式，结构化分镜建模与记忆机制是提升叙事连贯性的关键。

Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.

</details>


### [72] [V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping](https://arxiv.org/abs/2512.12375)
*Hyunkoo Lee,Wooseok Jang,Jini Yang,Taehwan Kim,Sangoh Kim,Sangwon Jung,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出V-Warper，一种无需训练的粗到细视频个性化框架，用于提升主体外观一致性，无需视频微调或大规模视频数据。


<details>
  <summary>Details</summary>
Motivation: 现有视频个性化方法依赖大量视频微调或大规模视频数据，计算成本高、难以扩展，且难以保持帧间细粒度外观一致性。

Method: V-Warper包含两个阶段：(1) 轻量级粗外观适配阶段，仅用少量参考图像，通过图像LoRA和主体嵌入适配编码全局身份；(2) 推理时细外观注入阶段，利用RoPE-free中层query-key特征计算语义对应关系，引导外观丰富的value表示 warped 到语义对齐区域，并通过掩码保证空间可靠性。

Result: V-Warper显著提升了外观保真度，同时保持提示对齐与运动动态，且无需大规模视频微调，效率高。

Conclusion: V-Warper是一种高效、训练免费的视频个性化新范式，解决了外观一致性与计算开销之间的关键矛盾。

Abstract: Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.

</details>


### [73] [M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction](https://arxiv.org/abs/2512.12378)
*Junqiao Fan,Yunjiao Zhou,Yizhuo Yang,Xinyuan Cui,Jiarui Zhang,Lihua Xie,Jianfei Yang,Chris Xiaoxuan Lu,Fangqiang Ding*

Main category: cs.CV

TL;DR: 本文提出了M4Human，当前最大规模的多模态人体网格重建基准数据集，包含661K帧毫米波雷达、RGB和深度数据，并提供高质量动捕标注（3D网格与全局轨迹），旨在推动隐私保护、抗遮挡的无接触式人体建模研究。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB的大规模人体网格重建数据集受限于视线遮挡、光照变化和隐私问题；而现有雷达数据集又存在标签稀疏、规模小、动作简单等问题，亟需更全面、高质、多模态的基准数据集。

Method: 构建了M4Human多模态数据集，涵盖毫米波雷达原始张量（RT）与点云（RPC）、RGB、深度及高精度动捕（3D网格+全局轨迹）标注；覆盖20名受试者、50种多样化动作；并建立了RT、RPC及RGB-D融合的基准评测方案。

Result: 在RT和RPC单模态及RGB-D多模态融合任务上建立了新基准；实验表明该数据集显著促进雷达人体建模研究，但也揭示了快速、无约束运动下的建模挑战。

Conclusion: M4Human填补了大规模、多模态、高精度雷达人体建模基准的空白，为隐私保护、鲁棒性强的人体感知与交互应用提供了重要基础。

Abstract: Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.

</details>


### [74] [Speedrunning ImageNet Diffusion](https://arxiv.org/abs/2512.12386)
*Swayam Bhanded*

Main category: cs.CV

TL;DR: SR-DiT 是一种系统整合多种加速技术的扩散Transformer框架，在ImageNet-256上以140M参数模型达到SOTA性能（FID 3.49，KDD 0.319），无需CFG，且训练迭代更少。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer加速技术多孤立研究，缺乏对组合策略协同效应的探索。

Method: 在表征对齐基础上，系统集成token路由、架构改进与训练优化。

Result: 在ImageNet-256上以140M参数、400K迭代、无分类器自由引导达成FID 3.49和KDD 0.319，性能媲美更大模型；通过消融实验揭示了技术组合的协同与冲突。

Conclusion: 多种加速技术可有效协同提升扩散Transformer效率，SR-DiT为后续研究提供了轻量、高效、开源的基线框架。

Abstract: Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.

</details>


### [75] [D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation](https://arxiv.org/abs/2512.12622)
*Zihan Wang,Seungjun Lee,Guangzhao Dai,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出了一种动态3D视觉-语言-规划模型（D3D-VLP），通过动态3D思维链（3D CoT）和碎片化监督下的协同学习（SLFS）策略，统一建模规划、定位、导航与问答，并在多任务基准及真实机器人实验中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能体中端到端模型缺乏可解释性和显式3D推理能力，而模块化系统忽略组件间依赖与协同的问题。

Method: 提出D3D-VLP模型，包含两个核心创新：1）动态3D思维链（3D CoT），在单一3D-VLM框架内统一多任务；2）碎片化监督下的协同学习（SLFS）策略，利用掩码自回归损失从大规模部分标注的混合数据中实现组件间隐式互监督。同时构建含1000万样本的大规模混合数据集。

Result: 在R2R-CE、REVERIE-CE、NavRAG-CE、HM3D-OVON、SG3D等多个基准上达到SOTA；并在真实移动操作实验中验证有效性。

Conclusion: D3D-VLP成功弥合了端到端与模块化方法之间的鸿沟，在保持可解释性与3D推理能力的同时，充分挖掘组件协同潜力，为具身智能提供了新范式。

Abstract: Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.

</details>


### [76] [ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States](https://arxiv.org/abs/2512.12395)
*Haowen Wang,Xiaoping Yuan,Fugang Zhang,Rui Jian,Yuanwei Zhu,Xiuquan Qiao,Yakun Huang*

Main category: cs.CV

TL;DR: 本文提出了ArtGen，一种基于扩散模型的条件生成框架，能够从单视角图像或文本描述中生成具有准确几何形状和连贯运动学结构的可动3D物体。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型依赖单视图闭合状态输入，导致运动学结构模糊或不真实，根源在于几何形状与关节动力学的耦合。

Method: ArtGen采用跨状态蒙特卡洛采样以保证全局运动学一致性；引入链式思维推理模块推断结构先验（如部件语义、关节类型、连接关系）；并使用增强局部-全局注意力的组合式3D-VAE隐空间先验建模细粒度几何与部件关系。

Result: 在PartNet-Mobility基准上，ArtGen显著优于现有最先进方法。

Conclusion: ArtGen有效解耦了几何与运动学，实现了高质量、多状态、可控的可动物体生成，适用于机器人、数字孪生和具身智能等场景。

Abstract: Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.

</details>


### [77] [Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection](https://arxiv.org/abs/2512.12884)
*Xiangzhong Liu,Jiajie Zhang,Hao Shen*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的端到端跨层级融合方法，将抽象的对象列表与原始相机图像融合用于3D目标检测，在nuScenes数据集上显著优于纯视觉基线。


<details>
  <summary>Details</summary>
Motivation: 现有汽车传感器融合系统中，智能传感器和V2X模块通常只提供处理后的对象列表而非原始传感器数据，传统方法难以有效利用这类高层次信息；且缺乏包含对象列表作为独立模态的公开数据集。

Method: 提出基于Transformer的跨层级融合框架：将对象列表作为去噪查询输入Transformer，并与可学习查询共同传播；引入基于对象位置与尺寸先验生成的可变形高斯掩码，显式引导解码器注意力；并设计伪对象列表生成方法以模拟状态噪声及漏检/误检。

Result: 在nuScenes数据集上，该方法显著超越纯视觉基线；验证了其对不同噪声水平的仿真对象列表及真实检测器输出具有良好的泛化能力。

Conclusion: 这是首个实现跨层级（原始图像+对象列表）融合的工作，证明了融合抽象语义信息与底层视觉特征的有效性，为车载多源异构感知提供了新范式。

Abstract: In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.

</details>


### [78] [A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams](https://arxiv.org/abs/2512.12410)
*Khalfalla Awedat,Mohamed Abidalrekab,Mohammad El-Yabroudi*

Main category: cs.CV

TL;DR: 本文提出了一种基于图注意力网络（GAT）的方法，仅利用当前帧LiDAR点云（无需图像或时序信息），重建因硬件老化、灰尘、雨雪、雾气或强反射导致的垂直波束丢失，显著提升自动驾驶中3D感知鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 垂直波束丢失会严重破坏LiDAR点云结构，影响3D感知；现有方法多依赖多模态或时序信息，缺乏仅用单帧纯几何信息的有效修复方案。

Method: 将单帧LiDAR扫描建模为保持原始波束索引顺序的无结构空间图（点为节点，邻近点间连边）；采用多层图注意力网络（GAT）学习局部几何邻域的自适应注意力权重，并直接回归缺失点的z坐标。

Result: 在1065段KITTI序列上模拟波束丢失进行训练与测试，平均高度RMSE为11.67 cm，87.98%重建点误差≤10 cm；单GPU推理耗时14.65秒/帧，且对不同邻域大小k鲁棒。

Conclusion: 仅基于原始点云几何结构的纯图注意力模型，可在真实传感器退化场景下有效恢复垂直波束丢失，为轻量、单帧、无相机依赖的LiDAR修复提供了新范式。

Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.

</details>


### [79] [ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics](https://arxiv.org/abs/2512.12424)
*Tue-Thu Van-Dinh,Hoang-Duy Tran,Truong-Binh Duong,Mai-Hanh Pham,Binh-Nam Le-Nguyen,Quoc-Thai Nguyen*

Main category: cs.CV

TL;DR: 本文介绍了ViInfographicVQA，首个越南语信息图视觉问答（InfographicVQA）基准，包含6747个真实信息图和20409个人工验证的问答对，涵盖经济、医疗、教育等领域，并提出单图与多图两种评测任务，揭示了当前多模态模型在低资源语言下布局感知与跨图推理能力的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答（VQA）基准多面向自然图像或场景文本，而信息图因其融合文本、图表、图标与设计元素，需更强OCR、版式理解及数值/语义推理能力；且缺乏针对越南语等低资源语言的信息图VQA基准。

Method: 构建ViInfographicVQA基准：收集6747个真实越南语信息图，标注20409组人工校验问答对；定义单图（Single-image）与首创的多图（Multi-image）跨图推理任务；在多个前沿视觉语言模型上进行系统评测与错误分析。

Result: 各模型在ViInfographicVQA上表现差异显著，尤其在Multi-image任务中错误率最高，主要集中在跨图证据整合与非片段式（non-span）推理上。

Conclusion: ViInfographicVQA填补了越南语信息图VQA基准空白，暴露了当前多模态模型在低资源语言环境下布局建模与跨图像推理能力的局限，为后续研究提供了新方向与评测基础。

Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.

</details>


### [80] [Motus: A Unified Latent Action World Model](https://arxiv.org/abs/2512.13030)
*Hongzhe Bi,Hengkai Tan,Shenghao Xie,Zeyuan Wang,Shuhe Huang,Haitian Liu,Ruowen Zhao,Yao Feng,Chendong Xiang,Yinze Rong,Hongyan Zhao,Hanyu Liu,Zhizhong Su,Lei Ma,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出Motus，一种统一的潜在动作世界模型，通过混合Transformer架构整合理解、视频生成和动作三个专家模块，并采用UniDiffuser式调度器实现多种建模模式的灵活切换；利用光流学习潜在动作，结合三阶段训练和六层数据金字塔提取像素级'增量动作'，实现大规模动作预训练；在仿真和真实场景中均显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能体方法依赖分离的模型（理解、世界建模、控制），导致多模态生成能力无法统一，且难以从大规模异构数据中学习。

Method: 提出Motus模型：1）Mixture-of-Transformer（MoT）架构集成理解、视频生成与动作三个专家；2）UniDiffuser风格调度器支持五种建模模式灵活切换；3）利用光流学习潜在动作；4）三阶段训练流程与六层数据金字塔提取像素级'delta action'。

Result: 在仿真环境中较X-VLA提升+15%、较Pi0.5提升+45%；在真实世界任务中提升+11%~48%，全面优于现有SOTA方法。

Conclusion: 统一建模所有功能与先验知识能显著提升下游机器人任务性能，Motus验证了该范式的有效性与可扩展性。

Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

</details>


### [81] [BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation](https://arxiv.org/abs/2512.12425)
*Hangwei Zhang,Armando Teles Fortes,Tianyi Wei,Xingang Pan*

Main category: cs.CV

TL;DR: 本文提出BokehDepth框架，通过两阶段方法解耦散景合成与深度预测，利用离焦作为无监督几何线索提升单目深度估计和散景渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用散景与单目深度估计之间的成像几何关联：高质量散景渲染依赖噪声深度图导致伪影；而单目深度模型在弱纹理、远距离和几何模糊区域表现差，这些区域恰恰离焦线索最有效。

Method: BokehDepth分为两个阶段：第一阶段使用物理引导的可控散景生成器（基于预训练图像编辑骨干网络）从单张清晰图像生成无深度依赖的散景堆栈；第二阶段设计轻量级离焦感知聚合模块，接入现有单目深度编码器，沿离焦维度融合特征，增强深度敏感变化，保持下游解码器不变。

Result: 在多个挑战性基准上，BokehDepth显著提升散景视觉保真度，并持续提升强单目深度基础模型的度量精度与鲁棒性。

Conclusion: 离焦可作为有效的无监督几何线索，BokehDepth成功解耦散景合成与深度预测，为二者协同优化提供了新范式。

Abstract: Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.

</details>


### [82] [MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion](https://arxiv.org/abs/2512.13177)
*Minghui Hou,Wei-Hsing Huang,Shaofeng Liang,Daizong Liu,Tai-Hao Wen,Gang Wang,Runwei Guan,Weiping Ding*

Main category: cs.CV

TL;DR: 本文提出MMDrive，一种面向自动驾驶的多模态视觉-语言模型框架，通过融合占用图、激光雷达点云和文本描述三种模态，并引入文本导向的多模态调制器与跨模态抽象器，实现自适应跨模态融合与关键信息提取，在DriveLM和NuScenes-QA上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型受限于二维图像理解范式，难以有效感知三维空间信息并进行深度语义融合，导致在复杂自动驾驶场景中表现欠佳。

Method: 提出MMDrive框架，融合占用图、LiDAR点云和文本描述三种模态；设计文本导向的多模态调制器（动态加权各模态贡献）和跨模态抽象器（使用可学习抽象token生成紧凑跨模态摘要）。

Result: 在DriveLM上BLEU-4达54.56、METEOR达41.78；在NuScenes-QA上准确率达62.7%，显著优于现有方法。

Conclusion: MMDrive突破了传统单图像理解限制，实现了鲁棒的多模态推理能力，为可解释的自动驾驶场景理解提供了新基础。

Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.

</details>


### [83] [Endless World: Real-Time 3D-Aware Long Video Generation](https://arxiv.org/abs/2512.12430)
*Ke Zhang,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

TL;DR: Endless World 是一个实时、无限、3D一致的视频生成框架，通过条件自回归训练和全局3D感知注意力机制，实现长序列视频的稳定生成与几何一致性。


<details>
  <summary>Details</summary>
Motivation: 解决长时序视频生成中3D结构不稳定、难以在流式场景下实时生成的问题。

Method: 提出条件自回归训练策略以对齐新旧帧，并引入全局3D感知注意力与3D注入机制，提供跨时间的连续几何引导和物理合理性约束。

Result: 在单GPU上实现实时推理，无需额外训练开销；在视觉保真度与空间一致性上达到或超越现有方法。

Conclusion: Endless World 有效支持无限长度、高稳定性与强3D一致性的视频生成，适用于流式与长时序应用场景。

Abstract: Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.

</details>


### [84] [MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning](https://arxiv.org/abs/2512.13636)
*Haoyu Fu,Diankun Zhang,Zongchuang Zhao,Jianfeng Cui,Hongwei Xie,Bing Wang,Guang Chen,Dingkang Liang,Xiang Bai*

Main category: cs.CV

TL;DR: 本文提出MindDrive框架，通过将在线强化学习引入视觉-语言-动作（VLA）模型，利用双LoRA参数的大型语言模型分别实现驾驶决策与动作映射，将连续动作空间探索转化为离散语言决策空间的试错学习，从而缓解分布偏移与因果混淆问题，并在Bench2Drive基准上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于模仿学习的VLA范式在自动驾驶中存在分布偏移和因果混淆问题；而在线强化学习虽具潜力，却受限于连续动作空间中探索效率低下的瓶颈。

Method: 提出MindDrive框架：采用一个LLM配备两组独立LoRA参数，分别作为‘决策专家’（负责场景推理与语言级驾驶决策）和‘动作专家’（将语言决策动态映射为可行轨迹）；通过将轨迹级奖励反馈至推理空间，在离散语言决策空间内进行试错学习，规避直接在连续动作空间探索。

Result: 在Bench2Drive闭环基准测试中达到Driving Score 78.04、Success Rate 55.09；首次验证了在线强化学习在自动驾驶VLA模型中的有效性。

Conclusion: MindDrive成功将在线强化学习适配到VLA自动驾驶系统中，通过解耦决策与动作并迁移奖励至语言空间，实现了高效探索、人类风格驾驶行为与复杂场景最优决策的统一。

Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.

</details>


### [85] [More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models](https://arxiv.org/abs/2512.12487)
*Hoang Anh Just,Yifei Fan,Handong Zhao,Jiuxiang Gu,Ruiyi Zhang,Simon Jenni,Kushal Kafle,Ruoxi Jia,Jing Shi*

Main category: cs.CV

TL;DR: PeRL-VL 是一种解耦框架，通过引入基于VLM的描述奖励提升视觉感知能力，并通过文本-only推理监督微调增强逻辑一致性，从而改进多模态大模型在RLVR范式下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习（RLVR）方法在视觉-语言模型（VLMs）上仍存在视觉提取不准确和推理链逻辑不一致两大问题，因其仅对最终答案提供可验证监督信号。

Method: 提出PeRL-VL框架：1）感知层面——使用VLM自生成图像描述并设计描述奖励函数评估其忠实性与充分性；2）推理层面——在高质量逻辑链数据上进行纯文本推理监督微调（Reasoning SFT），与视觉模块解耦优化。

Result: 在多个多模态基准测试中，PeRL-VL将Qwen2.5-VL-7B基线模型的Pass@1准确率从63.3%提升至68.8%，显著优于标准RLVR、纯文本推理SFT及GPT-4o的多模态蒸馏方法。

Conclusion: 解耦感知与推理优化是提升VLM多步推理能力的有效路径，PeRL-VL为RLVR在多模态场景中的扩展提供了新范式。

Abstract: Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.

</details>


### [86] [Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings](https://arxiv.org/abs/2512.12492)
*Shengkai Xu,Hsiang Lun Kao,Tianxiang Xu,Honghui Zhang,Junqiao Wang,Runmeng Ding,Guanyu Liu,Tianyu Shi,Zhenyu Yu,Guofeng Pan,Ziqian Bi,Yuqi Ouyang*

Main category: cs.CV

TL;DR: 本文提出AdaptiveDetector，一种结合YOLOv11检测器与视觉语言模型（VLM）验证器的两阶段框架，通过VLM引导的自适应置信度阈值调整和基于组相对策略优化（GRPO）的代价敏感强化学习，显著提升临床真实场景下（如光照变化、运动模糊、遮挡）的息肉检测召回率，减少漏检，改善患者预后。


<details>
  <summary>Details</summary>
Motivation: 现有息肉检测器在实验室清洁数据上训练良好，但在真实结肠镜临床场景中因光照变化、运动模糊和遮挡等成像退化而性能下降，存在显著域偏移问题，且漏检（假阴性）会带来严重临床风险。

Method: 提出两阶段AdaptiveDetector框架：第一阶段为YOLOv11检测器，其帧级置信度阈值由VLM动态指导调整；第二阶段为经Group Relative Policy Optimization（GRPO）微调的VLM验证器，采用非对称、代价敏感的奖励函数，重点惩罚漏检；同时构建了系统注入临床常见退化因素的合成测试基准，用于零样本评估。

Result: 在合成退化的CVC-ClinicDB和Kvasir-SEG数据集上零样本评估显示，相比单独YOLO，召回率提升14–22个百分点，精度波动控制在−0.7至+1.7个百分点内。

Conclusion: AdaptiveDetector通过自适应阈值与代价敏感强化学习，在开放世界、临床真实条件下实现了临床对齐的息肉检测，显著降低假阴性率，有助于减少漏诊癌前息肉，提升患者结局。

Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.

</details>


### [87] [Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention](https://arxiv.org/abs/2512.12498)
*Tasweer Ahmad,Arindam Sikdar,Sandip Pradhan,Ardhendu Behera*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像块关系建模的缓存适配方法（Patch-Driven Relational Refinement），通过图注意力网络挖掘图像内部块间依赖关系，提升CLIP在少样本图像分类中的领域适应能力，且不增加推理开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于缓存的适配方法（如Tip-Adapter）仍受限于CLIP全局、通用特征表示，在低数据和跨域场景下判别力不足；同时，战场急救中伤员识别需在‘白金分钟’内快速响应，亟需高效鲁棒的少样本视觉模型。

Method: 提出关系门控图注意力网络，构建图像块图并进行边感知注意力，生成上下文增强的块嵌入；再通过可学习多聚合池化生成任务判别性表征；该图精炼仅用于训练阶段以蒸馏关系结构至缓存，推理时仅需标准缓存查找与残差融合CLIP零样本logits。

Result: 在11个基准上持续超越SOTA CLIP适配器与缓存方法；首次构建了Injured vs. Uninjured Soldier真实场景数据集，并验证其在时间敏感的无人机搜救与战地急救中的有效性。

Conclusion: 图像块级关系建模可显著提升冻结视觉主干下的少样本适应能力，所提方法兼顾性能增益与零样本效率，在现实关键任务场景中具有实用价值。

Abstract: Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the "platinum minutes" and the broader "golden hour" window in time-critical UAV-driven search-and-rescue and combat casualty care.

</details>


### [88] [Generative Spatiotemporal Data Augmentation](https://arxiv.org/abs/2512.12508)
*Jinfan Zhou,Lixin Luo,Sungmin Eum,Heesung Kwon,Jeong Joon Park*

Main category: cs.CV

TL;DR: 本文提出了一种利用现成视频扩散模型进行时空数据增强的新方法，以生成多样化的3D空间与时间变化，从而在标注稀缺（如无人机图像）的低数据场景中提升模型性能，并提供了关于生成设置选择、标注迁移和遮挡处理的实用指南。


<details>
  <summary>Details</summary>
Motivation: 解决低数据场景（如无人机图像）下标注稀缺问题，以及传统几何变换或外观扰动等数据增强方法在时空多样性上的不足。

Method: 利用现成的视频扩散模型，从静态图像数据集生成具有真实感的时空变化视频片段，并将其作为补充训练数据；同时提出标注迁移策略和针对新暴露区域（disocclusion）的处理方法。

Result: 在COCO子集和无人机图像数据集上的实验表明，该方法在低数据设定下带来一致的性能提升，并能有效拓展数据分布中被传统及先前生成方法所忽略的时空维度。

Conclusion: 基于视频基础模型的时空数据增强是一种有效的低数据学习策略，不仅提升性能，还为实际应用提供了可操作的技术指南。

Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.

</details>


### [89] [Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling](https://arxiv.org/abs/2512.12539)
*Huan Huang,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种融合心肌解剖先验、结构感知特征编码和三维小波逆变换的冠状动脉分割框架，显著提升了CTA图像中小血管、复杂分支及模糊边界的分割精度与稳定性。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉在CTA图像中因管径细小、分支复杂、边界模糊及心肌干扰等原因，难以实现可靠分割，影响定量分析与临床决策支持。

Method: 提出一种新型3D分割框架：引入心肌解剖先验与残差注意力机制增强冠脉结构表征；采用小波-逆小波变换进行下/上采样以联合建模空间-频率信息并保持多尺度结构一致性；设计多尺度特征融合模块整合语义与几何信息。训练采用ImageCAS数据集的3D重叠块策略（7:1:2划分）。

Result: 在ImageCAS数据集上达到Dice 0.8082、Sensitivity 0.7946、Precision 0.8471、HD95 9.77 mm，优于多个主流模型；消融实验验证各模块互补性。

Conclusion: 该方法在复杂几何条件下实现了更稳定一致的冠状动脉分割，为后续冠脉结构分析提供了可靠的分割基础。

Abstract: Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.

</details>


### [90] [Supervised Contrastive Frame Aggregation for Video Representation Learning](https://arxiv.org/abs/2512.12549)
*Shaif Chowdhury,Mushfika Rahman,Greg Hamerly*

Main category: cs.CV

TL;DR: 本文提出了一种利用全局时间上下文的监督对比学习框架，通过将视频多帧空间排列为单张图像进行特征提取，并设计了基于标签的对比损失，显著提升了视频分类准确率且降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型（如ViViT）计算开销大，且依赖数据增强生成正样本易导致过拟合；需更高效、鲁棒的视频表征学习方法。

Method: 提出视频到图像聚合策略（多帧拼接为单图），复用ResNet50等预训练CNN骨干；设计监督对比损失，以同标签视频投影为正对、跨标签为负对；利用不同时间采样生成天然多视图正样本。

Result: 在Penn Action上达76%准确率（ViViT为43%），HMDB51上达48%（ViViT为37%）；同时支持监督与自监督学习，适用于分类与字幕等下游任务。

Conclusion: 该监督对比帧聚合方法在精度与效率间取得更好平衡，验证了利用全局时序信息和天然多视图进行视频表征学习的有效性。

Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

</details>


### [91] [Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space](https://arxiv.org/abs/2512.12623)
*Chengzhi Liu,Yuzhe Yang,Yue Fan,Qingyue Wei,Sheng Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: 本文提出DMLR框架，通过置信度引导的潜在策略梯度优化和动态视觉注入策略，在测试时实现视觉与文本的动态交织推理，提升多模态推理性能且保持高效。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式分步推理、感知-推理交互不稳定且计算开销大；受人类认知中推理与感知动态交织的启发，提出更自然、高效的多模态推理机制。

Method: 提出DMLR框架：1）置信度引导的潜在策略梯度优化，用于细化潜在‘思考token’；2）动态视觉注入策略，按需检索并更新最相关视觉特征，并将其注入对应潜在思考token，实现动态视觉-文本交织。

Result: 在七个跨模态推理基准和多种模型架构上验证，DMLR显著提升推理与感知性能，同时保持高推理效率。

Conclusion: DMLR通过模拟人类认知中感知与推理的动态协同，为多模态大模型提供了更鲁棒、高效、灵活的测试时推理新范式。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.

</details>


### [92] [StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding](https://arxiv.org/abs/2512.12560)
*Xinqi Jin,Hanxun Yu,Bohan Yu,Kebin Liu,Jian Liu,Keda Tao,Yixuan Pei,Huan Wang,Fan Dang,Jiangchuan Liu,Weiqiang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于token剪枝的在线视频理解方法，通过新设计的MSSAVT冗余度度量和掩码剪枝策略，在降低GPU内存占用与计算延迟的同时，提升了视频理解准确率。


<details>
  <summary>Details</summary>
Motivation: 在线视频理解面临MLLMs处理大量视频帧导致的高GPU内存消耗和计算延迟问题。

Method: 提出基于最大相似性到空间邻近视频token（MSSAVT）的冗余度度量，并结合掩码剪枝策略避免双向依赖；同时融合已有时间冗余剪枝方法以消除视频的时间冗余。

Result: 在多个在线与离线视频理解基准上显著提升准确率（最高+4%），剪枝延迟可忽略（<1ms）。

Conclusion: 所提token剪枝方法在保持关键信息前提下有效压缩视频上下文长度，兼顾效率与性能，适用于实时视频理解场景。

Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.

</details>


### [93] [From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models](https://arxiv.org/abs/2512.12571)
*Boyeong Im,Wooseok Lee,Yoojin Kwon,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: 本文提出MVP框架，通过将相机曝光三角（ISO、快门速度、光圈）作为物理提示，在测试时直接调控传感器参数以提升视觉语言模型在物理环境中的鲁棒性，无需梯度更新或模型修改，显著优于纯数字测试时自适应方法。


<details>
  <summary>Details</summary>
Motivation: 扩展视觉语言模型（VLMs）从网络图像到传感器中介的物理环境的应用，克服现有测试时自适应（TTA）仅作用于数字域的局限，探索在测量时刻（即拍摄时）进行物理层面提示控制的新范式。

Method: 提出Multi-View Physical-prompt for Test-Time Adaptation（MVP），一种前向-only框架：在推理时采集同一场景下不同物理设置（ISO/快门/光圈）的多视角图像；基于源亲和度分数选择top-k设置；对每个设置施加轻量级数字增强；用最低熵准则筛选增强视图子集；最后用零温度softmax（硬投票）聚合预测。全过程无梯度、不修改模型。

Result: 在ImageNet-ES及ImageNet-ES-Diverse上，MVP在单次自动曝光图像上相较纯数字TTA最高提升25.6个百分点，并比结合传统传感器控制与TTA的流程再提升3.4个百分点；即使减少参数候选集以降低捕获延迟，性能仍保持稳健。

Conclusion: 测量时刻的物理视图选择与融合（而非仅后处理提示）能显著提升VLM在真实物理环境中的鲁棒性，验证了‘从tokens到photons’的TTA新路径的有效性与实用性。

Abstract: To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.

</details>


### [94] [StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis](https://arxiv.org/abs/2512.12586)
*Lixin Chen,Chaomeng Chen,Jiale Zhou,Zhijian Wu,Xun Lin*

Main category: cs.CV

TL;DR: 本文提出StegaVAR框架，首次在隐写域直接进行视频动作识别，通过将动作视频嵌入普通封面视频中，在保证自然外观的同时完整保留时空信息，解决了现有匿名化方法隐蔽性低和时空特征破坏的问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的隐私保护方法依赖于匿名化，存在隐蔽性低（产生易引起攻击者注意的视觉失真）和时空特征破坏（影响动作识别准确性）两大问题。

Method: 提出StegaVAR框架，结合Secret Spatio-Temporal Promotion (STeP) 和 Cross-Band Difference Attention (CroDA) 方法，在隐写域中进行动作识别；STeP利用秘密视频指导隐写域时空特征提取，CroDA通过捕捉跨频带语义差异抑制封面干扰。

Result: 在多个常用数据集上，StegaVAR在视频动作识别准确率和隐私保护性能方面均优于现有方法，并兼容多种隐写模型。

Conclusion: StegaVAR首次实现了隐写域内端到端的动作识别，在不牺牲识别精度的前提下显著提升了视频传输的隐蔽性和隐私安全性。

Abstract: Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.

</details>


### [95] [Automatic Wire-Harness Color Sequence Detector](https://arxiv.org/abs/2512.12590)
*Indiwara Nanayakkara,Dehan Jayawickrama,Mervyn Parakrama B. Ekanayake*

Main category: cs.CV

TL;DR: 本文提出了一种半自动机器视觉系统，用于线束检测，可准确验证线序、连接器极性和颜色序列，在实际部署中实现了100%检测准确率并减少44%人工检测时间。


<details>
  <summary>Details</summary>
Motivation: 线束检测在电子制造服务（EMS）行业中仍高度依赖人工，易出错且效率低，亟需自动化解决方案提升精度与效率。

Method: 采用五个工业级CMOS相机集成于模块化机械框架，并结合HSV与RGB色彩空间比对的颜色序列分类算法；支持用户基于至少5个样本进行批次训练，训练模型可复用。

Result: 系统在GPV Lanka Pvt. Ltd. 实际部署中实现100%检测准确率，检测时间减少44%，并具备用户管理、可调照明、会话数据存储和安全登录等附加功能。

Conclusion: 该半自动机器视觉方案在真实产线环境中展现出高可靠性与高效性，为EMS行业线束质检提供了可行的工业化落地路径。

Abstract: Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.

</details>


### [96] [Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation](https://arxiv.org/abs/2512.12595)
*Karthikeya KV*

Main category: cs.CV

TL;DR: 本文提出了一种融合视觉增强大语言模型与先进Transformer架构的新框架，通过修正流机制、双向分词策略和噪声感知学习算法，显著提升了高分辨率图像合成与多模态理解的性能。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率图像合成和多模态数据理解中的关键挑战，如生成质量、计算效率及噪声鲁棒性。

Method: 结合视觉增强LLM与Transformer架构；引入修正流机制实现线性噪声-数据映射；采用双向分词策略融合文本、图像、视频模态；嵌入时空特征并使用混合文本-图像序列建模；设计噪声感知学习算法优化训练。

Result: 在基准数据集上相比扩散模型提升25%图像分辨率清晰度、降低20%计算需求，并展现出强可扩展性与跨场景适应能力。

Conclusion: 该框架推动了以视觉为中心的大语言模型发展，为计算机视觉与多模态AI提供了新范式。

Abstract: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.

</details>


### [97] [Efficient Vision-Language Reasoning via Adaptive Token Pruning](https://arxiv.org/abs/2512.12701)
*Xue Li,Xiaonan Song,Henry Hu*

Main category: cs.CV

TL;DR: 本文提出了一种名为自适应标记剪枝（ATP）的动态推理机制，用于降低视觉-语言模型（VLMs）的计算开销，通过在视觉-语言接口处依据上下文相关性动态保留最具信息量的标记，在几乎不损失精度的前提下显著提升推理效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中视觉-语言模型（VLMs）部署受限于高计算需求，现有架构对所有标记进行均匀处理，效率低下。

Method: 提出自适应标记剪枝（ATP），在视觉-语言接口处融合ViT CLS注意力（模内显著性）与CLIP图文相似度（模间相关性）生成混合重要性得分，动态保留Top-K标记输入LLM；作为轻量级门控模块，不修改骨干网络，兼容BLIP-2、LLaVA、Flamingo等主流架构。

Result: 在VQAv2、GQA、COCO上初步评估显示：推理FLOPs降低约40%，端到端延迟提速约1.5倍，精度损失小于1%；定性分析表明其保持视觉定位能力并增强可解释性；在图像损坏下表现出更强鲁棒性，抑制虚假相关性。

Conclusion: ATP证明资源受限下的高效推理与模型可靠性并非互斥目标，为多模态边缘计算提供了可行路径。

Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.

</details>


### [98] [Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models](https://arxiv.org/abs/2512.12596)
*Kei Yoshitake,Kento Hosono,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.CV

TL;DR: 本文提出了一种利用视觉-语言模型（VLM）生成基于图像的广告布局的方法，通过VLM理解图像语义并生成文本化“布局方案”，再渲染为HTML，显著提升了布局质量。


<details>
  <summary>Details</summary>
Motivation: 传统广告布局方法依赖显著性图，难以充分理解图像的细节构成和语义内容，导致布局效果受限。

Method: 采用两阶段流程：第一阶段用VLM分析图像中的物体类型与空间关系，生成文本形式的‘放置计划’；第二阶段将该计划渲染为HTML格式的最终布局。

Result: 定量与定性实验表明，该方法因显式建模背景图像内容，在广告布局质量上明显优于现有方法。

Conclusion: 利用VLM进行语义感知的广告布局生成是有效且可行的，能显著提升布局合理性与视觉效果。

Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.

</details>


### [99] [Geometry-Aware Scene-Consistent Image Generation](https://arxiv.org/abs/2512.12598)
*Cong Xie,Che Wang,Yan Zhang,Zheng Pan,Han Zou,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: 本文提出了一种几何感知、场景一致的图像生成方法，通过构建几何对齐的数据集和设计几何引导的注意力损失，有效平衡了场景保真度与文本提示遵循性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在保持参考场景结构一致性的同时，准确响应文本中指定的空间关系，存在场景保真与提示遵循之间的权衡问题。

Method: 提出两个关键方法：(i) 场景一致的几何感知数据构建流程，生成多样且几何对齐的训练样本；(ii) 基于跨视角线索的几何引导注意力损失，以增强模型的空间推理能力。

Result: 在自建的场景一致性基准上，该方法在自动评估指标和人类偏好研究中均优于现有最优方法，生成图像兼具几何一致性、场景忠实性和文本准确性。

Conclusion: 所提方法有效缓解了场景保真与文本遵循之间的固有冲突，为几何感知的可控场景图像生成提供了新范式。

Abstract: We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.

</details>


### [100] [No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching](https://arxiv.org/abs/2512.12604)
*Tingyan Wen,Haoyu Li,Yihuang Chen,Xing Zhou,Lifei Zhu,Xueqian Wang*

Main category: cs.CV

TL;DR: X-Slim 是一种无需训练的缓存加速框架，通过双阈值控制器在时间步、结构块和空间令牌三个层面协同进行特征重用与刷新，在保持生成质量的同时显著提升扩散模型推理速度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算开销大，现有缓存方法在加速与保真度之间存在固有折衷：时间步级重用加速强但易损质量，块/令牌级重用安全但增益有限。

Method: 提出 X-Slim，一种训练无关的多粒度缓存加速框架；引入双阈值控制器实现‘先激进推动时间步重用、再轻量刷新块/令牌、最后越界重置’的三阶段策略；各层级采用上下文感知指标动态决策缓存位置与时点。

Result: 在 FLUX.1-dev 和 HunyuanVideo 上分别实现最高 4.97x 和 3.52x 延迟降低且感知损失极小；在 DiT-XL/2 上达 3.13x 加速并使 FID 提升 2.42，优于先前方法。

Conclusion: X-Slim 首次统一利用时间、结构与空间三维度的缓存冗余，在不牺牲生成质量前提下显著拓展了扩散模型的速度-质量前沿。

Abstract: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.

</details>


### [101] [Heart Disease Prediction using Case Based Reasoning (CBR)](https://arxiv.org/abs/2512.13078)
*Mohaiminul Islam Bhuiyan,Chan Hue Wah,Nur Shazwani Kamarudin,Nur Hafieza Ismail,Ahmad Fakhri Ab Nasir*

Main category: cs.CV

TL;DR: 本研究比较了模糊逻辑、神经网络和基于案例推理（CBR）三种智能系统在心脏病预测中的准确性，最终选择CBR方法，达到97.95%的预测准确率，并发现男性患病概率（57.76%）高于女性（42.24%），吸烟与饮酒是男性中心脏病的重要诱因。


<details>
  <summary>Details</summary>
Motivation: 传统仅依赖医生经验的方法在心脏病预测中精度不足，需引入更精准的智能系统辅助诊断。

Method: 对比分析模糊逻辑、神经网络和基于案例推理（CBR）三种智能方法；对心脏病数据集进行预处理和划分；采用CBR模型进行预测。

Result: CBR方法实现97.95%的预测准确率；男性心脏病概率为57.76%，女性为42.24%；吸烟和饮酒被确认为男性中心脏病的重要风险因素。

Conclusion: 基于案例推理（CBR）在心脏病预测中表现最优，具有高准确率和实用价值，可作为临床辅助决策的有效工具。

Abstract: This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.

</details>


### [102] [DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model](https://arxiv.org/abs/2512.12633)
*Zhou Tao,Shida Wang,Yongxiang Hua,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: 本文提出DiG（Differential Grounding）框架，通过让多模态大模型识别并定位相似图像对之间的所有差异，提升其细粒度视觉感知与空间推理能力，并借助3D渲染数据生成和课程学习实现高效可扩展训练。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在细粒度视觉感知和精确空间推理方面仍存在局限。

Method: 提出DiG（Differential Grounding）代理任务框架；构建基于3D渲染的自动化配对图像数据生成流程，支持完全可控的差异设置；引入课程学习策略，从单差异逐步过渡到多差异以缓解信号稀疏性问题。

Result: DiG显著提升了模型在多种视觉感知基准上的性能，且所学细粒度感知能力能有效迁移到RefCOCO系列及通用多模态感知任务中。

Conclusion: 差异定位（differential grounding）是一种可扩展、鲁棒性强的提升MLLM细粒度视觉推理能力的新范式。

Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.

</details>


### [103] [Cross-modal Fundus Image Registration under Large FoV Disparity](https://arxiv.org/abs/2512.12657)
*Hongyang Li,Junyi Tao,Qijie Wei,Ningzhi Yang,Meng Wang,Weihong Yu,Xirong Li*

Main category: cs.CV

TL;DR: 本文提出CARe方法，通过Crop和Alignment操作解决大视场差异下的跨模态眼底图像配准问题，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态眼底图像配准方法假设视场差异小，难以应对大视场差异的挑战。

Method: 提出CARe方法：Crop操作利用视网膜生理结构从宽视场彩色眼底照中裁剪出与OCTA视场大致对齐的子图；Alignment模块采用RANSAC与多项式坐标拟合的双步空间变换优化策略。

Result: 在60对新构建的OCTA-wfCFP测试数据上验证了CARe方法的有效性和可行性。

Conclusion: CARe是一种简单而有效的方法，成功拓展了跨模态眼底图像配准至大视场差异场景。

Abstract: Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.

</details>


### [104] [CogDoc: Towards Unified thinking in Documents](https://arxiv.org/abs/2512.12658)
*Qixin Xu,Haozhe Wang,Che Liu,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: 本文提出CogDoc框架，通过粗到细的思维过程（快速阅读+聚焦思考）解决长文档处理与多模态细节捕捉之间的权衡问题；采用直接强化学习优于SFT初始化的RL方法；7B模型在视觉丰富文档基准上达到同参数量最优性能，甚至超越更大规模闭源模型。


<details>
  <summary>Details</summary>
Motivation: 现有文档推理范式在可扩展性（处理长上下文）和保真度（捕获细粒度、多模态细节）之间存在根本性权衡，亟需一种兼顾二者的新框架。

Method: 提出CogDoc统一的粗到细思维框架，包含低分辨率‘快速阅读’阶段用于信息定位，和高分辨率‘聚焦思考’阶段用于深度推理；系统研究后训练策略，发现直接强化学习比带监督微调初始化的强化学习更优，因其避免了策略冲突。

Result: 7B参数的CogDoc模型在视觉丰富的文档基准（如DocVQA、InfographicVQA等）上达到同参数量SOTA，且性能超越GPT-4o等更大闭源模型。

Conclusion: CogDoc验证了模仿人类认知的分阶段推理范式在文档理解中的有效性；直接强化学习是训练此类统一思维框架的更优后训练策略；该框架为长上下文、多模态文档理解提供了新思路。

Abstract: Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.

</details>


### [105] [Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images](https://arxiv.org/abs/2512.12662)
*Muhammad Umar Farooq,Abd Ur Rehman,Azka Rehman,Muhammad Usman,Dong-Kyu Chae,Junaid Qadir*

Main category: cs.CV

TL;DR: 本文提出了一种半监督多任务Transformer网络（SSMT-Net），通过结合无监督预训练与多任务监督学习，提升甲状腺结节超声图像分割性能，尤其在标注数据稀缺、边界模糊和尺寸变化大的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 甲状腺结节在超声图像中边界模糊、尺寸差异大，且标注数据稀缺；现有深度学习模型难以有效利用甲状腺腺体上下文信息，泛化能力不足。

Method: 提出SSMT-Net：第一阶段采用无监督方式利用未标注数据增强Transformer编码器的特征提取能力；第二阶段进行多任务监督学习，联合优化结节分割、腺体分割和结节尺寸估计，并融合局部与全局上下文特征。

Result: 在TN3K和DDTI数据集上实验表明，SSMT-Net在准确率和鲁棒性方面均优于当前最先进方法。

Conclusion: SSMT-Net有效缓解了标注数据稀缺与边界模糊等挑战，具备良好的临床应用潜力。

Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.

</details>


### [106] [InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation](https://arxiv.org/abs/2512.12664)
*Sreehari Rajan,Kunal Bhosikar,Charu Sharma*

Main category: cs.CV

TL;DR: 本文提出InteracTalker框架，首次统一了语音驱动手势生成与物体交互动作生成两大任务，通过多阶段训练构建统一的运动-语音-提示嵌入空间，并设计自适应融合策略解决多条件信号不平衡问题，在真实感、灵活性和控制性上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法分别处理语音驱动手势或物体交互，缺乏能同时响应语言和物理对象的综合数据集与模型，限制了实际应用。

Method: 提出InteracTalker框架，包含：1）基于提示的物体感知交互与共语手势联合生成；2）多阶段训练构建统一嵌入空间；3）通用运动适配模块实现独立训练与动态组合；4）扩散采样中自适应融合异构条件信号。并构建增强型人类-物体交互数据集。

Result: 在共语手势生成和物体交互合成两方面均超越先前方法，生成高度真实、物体感知的全身运动，具备更强现实感、灵活性与可控性。

Conclusion: InteracTalker成功统一了长期分离的语音驱动动作与物体交互建模任务，为交互式数字体验提供了更完整、实用的动作生成解决方案。

Abstract: Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.

</details>


### [107] [Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning](https://arxiv.org/abs/2512.12667)
*Haiyang Zheng,Nan Pu,Wenjing Li,Teng Long,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种面向开放世界深度伪造归因（OW-DFA）的置信度感知非对称学习框架（CAL），通过置信度感知一致性正则化（CCR）和非对称置信度强化（ACR）缓解伪标签偏差，并引入动态原型剪枝（DPP）策略自动估计未知伪造类型数量，显著提升了已知与未知伪造的归因性能。


<details>
  <summary>Details</summary>
Motivation: 现有OW-DFA方法存在伪标签不可靠（因置信度偏斜）和需预先设定未知类型数量两大缺陷，难以应对真实开放世界场景。

Method: 提出CAL框架，包含CCR（基于归一化置信度动态缩放损失）与ACR（依据置信度差分别校准已知/未知类置信度）；并设计DPP策略以粗到细方式自动估计未知类型数。

Result: 在标准及扩展OW-DFA基准上均达到SOTA性能，显著提升已知与未知伪造类型的归因准确率。

Conclusion: CAL通过置信度自适应建模与无需先验的未知类数量估计，有效解决了OW-DFA中的关键挑战，增强了模型在真实场景中的鲁棒性与可扩展性。

Abstract: The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.

</details>


### [108] [Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation](https://arxiv.org/abs/2512.12673)
*Yushun Tang,Ziqiong Liu,Jiyuan Jia,Yi Zhang,Zhihai He*

Main category: cs.CV

TL;DR: 本文提出了一种渐进式条件缩放-偏移重校准（PCSR）方法，用于在线测试时自适应调整Transformer模型的自注意力机制，以应对目标域中Query、Key、Value特征显著变化导致的性能下降。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在迁移到新目标域时，其自注意力模块的Query、Key、Value特征发生显著变化，导致性能大幅下降，亟需有效的在线测试时自适应方法。

Method: 提出PCSR方法：在每层Transformer中引入轻量级Domain Separation Network提取领域偏移特征，并通过Factor Generator Network预测用于自注意力重校准的条件化缩放与偏移参数；整个过程在线进行。

Result: 在ImageNet-C等基准数据集上，PCSR方法将在线测试时领域自适应的分类准确率最高提升了3.9%。

Conclusion: PCSR通过逐层渐进式、条件化的自注意力重校准，有效缓解了跨域推理中的特征偏移问题，显著提升了在线测试时自适应性能。

Abstract: Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\% in classification accuracy on the ImageNet-C dataset.

</details>


### [109] [Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling](https://arxiv.org/abs/2512.12675)
*Yuran Wang,Bohan Zeng,Chengzhuo Tong,Wenxuan Liu,Yang Shi,Xiaochen Ma,Hao Liang,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出Scone方法，统一解决多主体图像生成中的组合与区分问题，通过理解专家与生成专家协同工作，并引入SconeEval基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有主体驱动的图像生成方法在处理多主体输入时忽视了区分能力，即准确识别和生成指定主体的能力，限制了其在复杂真实场景中的应用效果。

Method: 提出Scone框架，包含理解专家（作为语义桥梁）与生成专家协同工作；采用两阶段训练：先学习组合，再通过语义对齐和注意力掩码增强区分能力；并构建SconeEval评估基准。

Result: Scone在两个基准上均优于现有开源模型，在组合与区分任务上均有显著提升；模型、基准及训练数据已开源。

Conclusion: Scone实现了组合与区分能力的统一建模，有效提升了多主体图像生成中主体身份的保持与辨识能力，为更真实的视觉内容生成提供了新范式。

Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.

</details>


### [110] [$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment](https://arxiv.org/abs/2512.12678)
*Fatimah Zohra,Chen Zhao,Hani Itani,Bernard Ghanem*

Main category: cs.CV

TL;DR: 本文提出β-CLIP，一种多粒度文本条件对比学习框架，通过跨注意力动态聚合图像块，并引入β-上下文化对比对齐损失（β-CAL）来实现文本（从整句到短语）与图像区域间的层次化对齐，在细粒度视觉-语言检索任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: CLIP在零样本图像-文本检索中表现优异，但在细粒度任务（如基于长详细描述的检索）上即使微调仍表现不足，主要因其仅建模全局表征而缺乏局部-文本的精细对齐能力。

Method: 提出β-CLIP框架：1）构建多粒度文本层级（caption→sentence→phrase）；2）对每层文本，用cross-attention动态池化图像patch生成上下文化视觉嵌入；3）设计β-CAL损失，通过可调参数β平衡严格查询匹配与图像内上下文松弛匹配，并支持soft/hard BCE两种形式。

Result: 在Urban1K上T2I/I2T R@1达91.8%/92.3%，FG-OVD（Hard）上达30.9%，均为无硬负样本训练方法中的SOTA；显著提升密集对齐能力。

Conclusion: β-CLIP建立了鲁棒、自适应的细粒度视觉-语言对应基线，验证了多粒度层次对齐与上下文化对比损失的有效性，代码与模型已开源。

Abstract: CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.

</details>


### [111] [Robust Motion Generation using Part-level Reliable Data from Videos](https://arxiv.org/abs/2512.12703)
*Boyuan Li,Sipeng Zheng,Bin Cao,Ruihua Song,Zongqing Lu*

Main category: cs.CV

TL;DR: 本文提出了一种基于可信部件检测与部件感知掩码自回归建模的方法，从大规模网络视频中鲁棒地提取高质量人体运动数据，缓解因遮挡和出镜导致的数据缺失问题，并构建了新基准K700-M进行验证。


<details>
  <summary>Details</summary>
Motivation: 从网络视频中提取人体运动可缓解动画数据稀缺问题，但视频中常存在部件遮挡或出镜，导致数据不完整：全丢弃损失规模与多样性，全保留则损害质量与模型性能。

Method: 将人体分为五个部件，检测每帧中清晰可见的‘可信部件’；用提出的部件感知变分自编码器将其编码为潜在token；再通过鲁棒的部件级掩码生成模型预测被掩码的可信部件，忽略不可信（噪声）部件。

Result: 在干净与含噪数据集上，本方法在运动质量、语义一致性与多样性方面均优于基线；并发布了含约20万真实运动序列的新基准K700-M。

Conclusion: 利用可信部件信息并设计部件感知的鲁棒生成架构，可有效提升从嘈杂网络视频中学习高质量人体运动的能力，为数据驱动的动画生成提供了更实用的解决方案。

Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/

</details>


### [112] [Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images](https://arxiv.org/abs/2512.12718)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Changgyun Kim,Taemin Lee*

Main category: cs.CV

TL;DR: 本文提出了一种融合四向深度图像的3D人体姿态分析系统，通过全局与精细配准的分层匹配、自适应顶点缩减及细节层次集成，实现高精度、无需训练数据或复杂神经网络的脊柱中心线估计与脊柱角测量。


<details>
  <summary>Details</summary>
Motivation: 现有基于多图像的方法设备昂贵、流程复杂；单图像方法因遮挡和视角限制难以准确估计脊柱中心线等内部结构。

Method: 融合四个方向的深度图像构建3D人体模型；采用分层匹配（全局+精细配准）应对噪声与遮挡；引入自适应顶点缩减保持网格分辨率与形状可靠性；利用细节层次（Level of Detail）集成提升脊柱角估计的精度与稳定性。

Result: 实现了无需训练数据或复杂神经网络的高精度3D脊柱配准与脊柱角估计；验证表明匹配质量显著提升。

Conclusion: 所提方法有效弥补了多图像与单图像方法的各自缺陷，在精度、鲁棒性与实用性之间取得良好平衡。

Abstract: The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.

</details>


### [113] [GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation](https://arxiv.org/abs/2512.12751)
*Zhenya Yang,Zhe Liu,Yuxiang Lu,Liping Hou,Chenxuan Miao,Siyi Peng,Bailan Feng,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出GenieDrive框架，通过生成物理信息丰富的4D占用（occupancy）作为基础，再基于此生成驾驶视频，从而实现物理一致性、高可控性与多视角一致的驾驶视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型多依赖单一扩散模型直接映射动作到视频，导致学习困难和物理不一致；需构建物理感知的驾驶视频生成方法以支持规划、OOD数据合成与闭环评估。

Method: 提出GenieDrive：1）先生成含时空结构与动态信息的4D occupancy；2）设计轻量VAE将其压缩为三平面潜表示（latent size减少至58%）；3）引入互控注意力（MCA）建模控制对occupancy演化的精确影响；4）端到端联合训练VAE与预测模块；5）在视频生成阶段使用归一化多视角注意力，以occupancy为引导生成多视角视频。

Result: 在预测mIoU上提升7.2%，推理速度达41 FPS，仅用3.47M参数；FVD降低20.7%，显著提升视频质量；支持高可控、多视角一致、物理感知的驾驶视频生成。

Conclusion: GenieDrive通过解耦物理建模（4D occupancy）与视频渲染，兼顾效率、精度与物理合理性，为驾驶世界建模提供了新范式。

Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.

</details>


### [114] [FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning](https://arxiv.org/abs/2512.12756)
*Yue Jiang,Dingkang Yang,Minghao Han,Jinghang Han,Zizhi Chen,Yizhou Liu,Mingcheng Li,Peng Zhai,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了FysicsWorld，首个支持图像、视频、音频和文本之间双向输入输出的全模态统一基准，用于全面评估多模态理解、生成与推理能力，并提出CMCS策略以构建融合依赖型跨模态数据。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准存在模态覆盖不全、交互局限于文本输出、模态间互补性与互依性弱等问题，亟需更全面、统一的评估标准。

Method: 构建FysicsWorld全模态基准，涵盖16项任务、3268个样本，整合40+高质量数据源；提出跨模态互补性筛选（CMCS）策略，嵌入系统化数据构建框架，支持语音交互与融合依赖型跨模态推理。

Result: 在30多个SOTA模型（包括MLLMs、单模态模型、统一理解-生成模型及全模态语言模型）上完成综合评测，揭示了各类模型在理解、生成与推理方面的性能差异与瓶颈。

Conclusion: FysicsWorld为下一代全模态架构提供了统一评估基础与强基线，推动多模态AI向真正协同、互补、双向交互的方向发展。

Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.

</details>


### [115] [CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence](https://arxiv.org/abs/2512.12768)
*Tianjiao Yu,Xinzhuo Li,Yifan Shen,Yuanzhe Liu,Ismini Lourentzou*

Main category: cs.CV

TL;DR: CoRe3D提出了一种统一的3D理解与生成推理框架，通过语义与空间抽象的联合推理，实现语言意图到3D内容生成的直接映射，提升了局部一致性与跨模态对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在语言和视觉任务中已验证推理机制的有效性，但在3D领域仍缺乏成熟推理方法；亟需提升3D模型的可靠性、可解释性及跨模态对齐能力。

Method: 构建空间锚定的推理表征，将3D潜在空间分解为局部区域，结合语义链式推理与结构化空间推理，实现语义意图向3D几何生成的联合引导。

Result: CoRe3D生成的3D内容具备强局部一致性，并与语言描述高度对齐。

Conclusion: 显式推理机制可有效迁移至3D领域，语义与空间抽象的协同推理是提升3D多模态理解与生成性能的关键路径。

Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.

</details>


### [116] [Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior](https://arxiv.org/abs/2512.12774)
*Hao Wang,Ashish Bastola,Chaoyi Zhou,Wenhui Zhu,Xiwen Chen,Xuanzhao Dong,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: 本文提出Fast-2DGS，一种轻量级框架，通过深度高斯先验和属性回归网络实现高效、高质量的2D高斯点绘图像表征，显著降低计算开销并支持单次前向生成与微调。


<details>
  <summary>Details</summary>
Motivation: 现有2D高斯点绘（2DGS）方法在初始化高斯配置时依赖随机采样或启发式策略（如梯度图），对图像复杂度不敏感、收敛慢；而基于可学习网络的方法又带来过高计算与结构复杂度，亟需兼顾效率、质量与可部署性的新方案。

Method: 提出Fast-2DGS框架：1）Deep Gaussian Prior——条件化网络，建模不同复杂度下高斯原语的空间分布；2）属性回归网络——预测稠密高斯属性（如位置、尺度、透明度等）；整体为解耦架构，支持单次前向推理加极少微调。

Result: 在保持高视觉质量前提下，大幅降低计算成本；实验表明可实现高质量重建，收敛速度快（远快于>10s的传统优化），渲染实时性高（>1000 FPS），且具备强编辑性与可解释性。

Conclusion: Fast-2DGS为2D高斯点绘提供了高效、轻量、实用的新范式，推动其向工业级部署迈进。

Abstract: As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.

</details>


### [117] [L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context](https://arxiv.org/abs/2512.12790)
*Tiange Zhang,Zhimeng Huang,Xiandong Meng,Kai Zhang,Zhipin Deng,Siwei Ma*

Main category: cs.CV

TL;DR: 本文提出L-STEC方法，通过LSTM扩展参考链以捕获长时依赖，并融合像素域的扭曲空间上下文，显著提升神经视频压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于条件的神经视频压缩方法仅依赖前一帧特征预测时序上下文，导致长时依赖缺失、细节丢失及误差累积问题。

Method: 提出长时-空时增强上下文（L-STEC）方法：1）使用LSTM扩展参考链以建模长时依赖；2）引入像素域扭曲空间上下文，并通过多感受野网络融合时空信息。

Result: 在PSNR和MS-SSIM指标上分别比DCVC-TCM节省37.01%和31.65%码率，性能超越VTM-17.0与DCVC-FM，达到新SOTA。

Conclusion: L-STEC通过增强长时与空时上下文建模能力，有效缓解误差传播与纹理丢失问题，显著提升神经视频压缩效率。

Abstract: Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.

</details>


### [118] [DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning](https://arxiv.org/abs/2512.12799)
*Zhe Liu,Runhui Huang,Rui Yang,Siming Yan,Zining Wang,Lu Hou,Di Lin,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 本文提出DrivePI，一种空间感知的4D多模态大语言模型，统一处理视觉-语言-动作任务，在自动驾驶中实现细粒度3D感知、预测与规划。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在自动驾驶中难以生成细粒度3D感知与预测输出，缺乏统一的空间理解与动作规划框架。

Method: 提出DrivePI模型，融合点云、多视角图像和语言指令；设计数据引擎生成文本-占据/流问答对；端到端联合优化空间理解、3D占据、占据流预测与动作规划。

Result: 仅用0.5B Qwen2.5骨干，DrivePI在nuScenes-QA上超越OpenDriveVLA-7B 2.5%，碰撞率降低70%；在OpenOcc上3D占据RayIoU提升10.3，占据流mAVE降至0.509，规划L2误差比VAD低32%。

Conclusion: DrivePI验证了轻量级统一VLA框架在多任务自动驾驶理解与决策中的有效性与优越性。

Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI

</details>


### [119] [Learning Common and Salient Generative Factors Between Two Image Datasets](https://arxiv.org/abs/2512.12800)
*Yunlong He,Gwilherm Lesné,Ziqian Liu,Michaël Soumm,Pietro Gori*

Main category: cs.CV

TL;DR: 本文提出了一种名为对比分析（Contrastive Analysis, CA）的新方法，旨在从两个图像数据集中分离出共有的生成因子与特定于某一数据集的显著因子，仅依赖数据集标签而非显式属性标注，适用于GAN和扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有图像合成工作主要集中在条件编辑或解耦表征学习，而本文关注一个较少研究的问题：在无显式属性标注的情况下，仅通过数据集信号区分两个图像集间的共有与特有生成因子。

Method: 提出一种新颖的CA框架，适配GAN和扩散模型；设计针对性的学习策略与损失函数，以实现对共有因子与显著因子的有效分离，并保持高质量图像生成能力。

Result: 在人脸、动物及医学影像等多样化数据集上验证了该方法，在因子分离能力和生成图像质量方面均优于先前方法。

Conclusion: 本文证明了仅利用数据集信号即可有效解耦图像生成中的共有与特有语义因子，为无监督图像分析与编辑提供了新思路，并具备模型通用性。

Abstract: Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.

</details>


### [120] [Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding](https://arxiv.org/abs/2512.12822)
*Yongyuan Liang,Xiyao Wang,Yuanchen Ju,Jianwei Yang,Furong Huang*

Main category: cs.CV

TL;DR: Lemon is a unified transformer architecture for 3D understanding that jointly processes point cloud patches and language tokens, enabling early fusion, improved efficiency, and scalable training via a structured patchification scheme and three-stage curriculum.


<details>
  <summary>Details</summary>
Motivation: Scaling large multimodal models to 3D understanding faces challenges including sparsity and irregularity of point clouds, fragmented architectures with modality-specific encoders, and unstable, non-scalable training pipelines.

Method: Lemon uses a unified transformer to jointly process 3D point cloud patches and language tokens; introduces a structured patchification and tokenization scheme preserving spatial context; and adopts a three-stage training curriculum progressing from object-level to scene-level spatial reasoning.

Result: Lemon achieves new state-of-the-art performance across diverse 3D understanding and reasoning tasks—including object recognition, captioning, and spatial reasoning—and demonstrates robust scaling with increasing model size and data.

Conclusion: Lemon provides a unified, efficient, and scalable foundation for advancing 3D spatial intelligence in real-world applications.

Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.

</details>


### [121] [Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners](https://arxiv.org/abs/2512.12824)
*N. K. B. M. P. K. B. Narasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本文系统研究了CoCa这类生成-对比混合基础模型在极低数据量（few-shot）图像分类任务中的适配方法，发现数据增强对不同微调策略（如线性探针 vs LoRA）有相反影响，并提出结合SupCon损失的混合目标能稳定提升性能，同时给出了针对数据稀缺场景的超参配置建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于CLIP等双编码器架构的少样本迁移，而对CoCa这类生成-对比统一架构在极低数据下的参数高效微调（PEFT）适应性缺乏深入探索。

Method: 对CoCa视觉骨干网络开展少样本图像分类的实证研究，评估从无训练原型法到LoRA微调的多种策略；分析数据增强影响、引入SupCon损失的混合目标，并系统考察正则化强度、LoRA秩、采样策略等配置对数据稀缺的敏感性。

Result: 发现强数据增强会损害线性探针性能但稳定LoRA训练；SupCon混合损失在不同样本数下均优于标准交叉熵；明确了正则化、秩和采样策略随数据量减少的适配规律。

Conclusion: CoCa等生成-对比混合模型可通过定制化PEFT策略（如LoRA+SupCon）高效适配少样本任务，其训练配置需根据数据稀缺程度动态调整，为类似基础模型的下游轻量化部署提供了实证指南。

Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.

</details>


### [122] [Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal](https://arxiv.org/abs/2512.12875)
*Weihan Xu,Kan Jen Cheng,Koichi Saito,Muhammad Jehanzeb Mirza,Tingle Li,Yisi Liu,Alexander H. Liu,Liming Wang,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji,Gopala Anumanchipalli,Paul Pu Liang*

Main category: cs.CV

TL;DR: 本文提出了SAVEBench数据集和SAVE模型，用于实现音频与视频的联合编辑，通过Schrodinger Bridge实现源到目标的直接传输，在保持音视频对齐的同时完成对象级编辑。


<details>
  <summary>Details</summary>
Motivation: 联合音视频编辑面临配对前后数据稀缺和跨模态异质性两大挑战。

Method: 构建了带文本与掩码条件的配对音视频数据集SAVEBench，并提出端到端流匹配模型SAVE，引入Schrodinger Bridge实现音视频同步编辑与对齐。

Result: SAVE在移除目标对象的同时更好保留其余内容，且在时序同步性和音视频语义一致性上优于音频编辑器与视频编辑器的简单组合。

Conclusion: SAVE及其配套数据集为可控、精准的联合音视频编辑提供了新范式，验证了统一建模跨模态编辑的有效性。

Abstract: Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.

</details>


### [123] [Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification](https://arxiv.org/abs/2512.12887)
*Han Liu,Bogdan Georgescu,Yanbo Zhang,Youngjin Yoo,Michael Baumgartner,Riqiang Gao,Jianing Wang,Gengyan Zhao,Eli Gibson,Dorin Comaniciu,Sasa Grbic*

Main category: cs.CV

TL;DR: 本文提出AnyMC3D，一种基于2D基础模型的可扩展3D医学图像分类器，通过轻量插件适配新任务，在12个多样化任务上实现SOTA性能，并揭示了适配方法、模型通用性及2D优于3D架构等关键洞见。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学图像分类研究存在数据域偏差、适配效果不佳和任务覆盖不足三大问题。

Method: 提出AnyMC3D框架，基于冻结的2D基础模型主干，仅添加约1M参数的轻量插件以适配新任务；支持多视角输入、像素级辅助监督和可解释热力图生成。

Result: 在涵盖多种病理、解剖结构和模态的12任务基准上系统评估现有方法，首次实现单框架跨任务SOTA性能（含VLM3D挑战赛第一名）。

Conclusion: 有效适配是释放基础模型潜力的关键；通用基础模型经恰当适配可媲美医学专用模型；基于2D的方法在3D分类任务中可超越原生3D架构。

Abstract: 3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.

</details>


### [124] [Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2512.12906)
*Zhimao Peng,Enguang Wang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于预测样本分配（PSA）的简洁SCOOD框架，通过双阈值三元分配策略和概念对比表征学习损失，提升ID/OOD样本纯度与判别性，在标准SCOOD基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SCOOD方法依赖聚类过滤策略筛选ID样本，易引入大量噪声样本，影响OOD检测性能。

Method: 提出预测样本分配（PSA）框架，包括：1）基于预测能量分数的双阈值三元样本分配策略（ID/ OOD/丢弃）；2）概念对比表征学习损失以扩大ID/OOD在特征空间距离；3）针对筛选样本的再训练策略。

Result: 在两个标准SCOOD基准上，该方法显著优于当前最优方法。

Conclusion: PSA框架有效提升了ID/OOD样本纯度与判别能力，为SCOOD任务提供了更鲁棒、简洁且高效的解决方案。

Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.

</details>


### [125] [Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery](https://arxiv.org/abs/2512.12925)
*Zhimao Peng,Enguang Wang,Fei Yang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种用于广义类别发现（GCD）的新方法，包含损失锐度惩罚（LSP）和动态锚点选择（DAS）两个模块，以提升伪标签质量和未知类聚类精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于DINO式伪标签策略的方法易受预训练模型对特定视觉模式偏好影响，导致编码虚假相关性、生成噪声伪标签。

Method: 提出Loss Sharpness Penalty（LSP）抑制模型对微小扰动的敏感性，减少对琐碎特征的编码；并设计Dynamic Anchor Selection（DAS），基于KNN密度和类别概率动态选取代表性样本并赋予硬伪标签。

Result: 在多个GCD基准上显著缓解伪标签噪声，达到当前最优性能。

Conclusion: LSP与DAS协同提升了模型鲁棒性和未知类特征学习能力，有效推动GCD任务发展。

Abstract: Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.

</details>


### [126] [MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation](https://arxiv.org/abs/2512.12929)
*Huu-An Vu,Van-Khanh Mai,Trong-Tam Nguyen,Quang-Duc Dam,Tien-Huy Nguyen,Thanh-Huong Le*

Main category: cs.CV

TL;DR: 本文提出了MADTempo框架，通过结合时序搜索与网络规模视觉定位，提升视频检索系统对复杂事件时序结构的理解能力及对未见/罕见视觉概念的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频检索方法难以建模多事件间的时序依赖，且难以处理涉及未见或罕见视觉概念的查询。

Method: 提出MADTempo框架：1）时序搜索机制，通过聚合连续视频片段的相似度得分来捕捉事件级连续性；2）基于Google图片搜索的回退模块，利用外部网络图像扩展查询表征，弥补预训练视觉嵌入的不足。

Result: 显著提升了视频检索系统在时序推理和OOD查询鲁棒性方面的性能。

Conclusion: MADTempo统一了时序搜索与视觉定位，增强了视频检索系统的语义感知与自适应能力，为大规模视频语料库的检索提供了新范式。

Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.

</details>


### [127] [Content Adaptive based Motion Alignment Framework for Learned Video Compression](https://arxiv.org/abs/2512.12936)
*Tiange Zhang,Xiandong Meng,Siwei Ma*

Main category: cs.CV

TL;DR: 本文提出了一种内容自适应的运动对齐框架（CAMA），通过两阶段光流引导的可变形扭曲、多参考质量感知策略和训练无关的运动幅度驱动帧下采样，显著提升了端到端视频压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视频压缩框架缺乏内容特异性适配，导致压缩性能次优。

Method: 1）两阶段光流引导的可变形扭曲机制（粗到细偏移预测+掩码调制）；2）多参考质量感知的失真权重调整策略用于分层训练；3）训练无关的基于运动幅度与分辨率的帧下采样模块。

Result: 在标准测试集上，CAMA相比DCVC-TCM基线模型BD-rate（PSNR）节省24.95%，且优于DCVC-DC和传统编码器HM-16.25。

Conclusion: 内容自适应的运动对齐机制能有效提升神经视频压缩性能，为端到端压缩框架提供了新的优化方向。

Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.

</details>


### [128] [UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction](https://arxiv.org/abs/2512.12941)
*Siyuan Yao,Dongxiu Liu,Taotao Li,Shengjie Li,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出了一种不确定性聚合的全局-局部融合网络（UAGLNet），通过混合CNN与Transformer编码器、中间协同交互模块（CIB）、全局-局部融合（GLF）模块及不确定性聚合解码器（UAD），提升遥感图像中建筑物提取的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在遥感图像建筑物提取中受限于特征金字塔固有差异和全局-局部特征融合不足，导致结果不准确、模糊。

Method: 提出UAGLNet：1）协同编码器（CNN+Transformer分阶段提取局部/全局语义）；2）中间协同交互模块（CIB）缩小深层网络中局部与全局特征差距；3）全局-局部融合（GLF）模块互补融合特征；4）不确定性聚合解码器（UAD）显式估计像素级不确定性以抑制分割模糊性。

Result: 在多个数据集上实验表明，该方法性能优于当前主流先进方法。

Conclusion: UAGLNet通过引入不确定性建模与全局-局部协同机制，有效提升了遥感图像建筑物提取的准确性与鲁棒性，为复杂结构变化下的建筑分割提供了新思路。

Abstract: Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet

</details>


### [129] [SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer](https://arxiv.org/abs/2512.12963)
*Luan Thanh Trinh,Kenji Doi,Atsuki Osanai*

Main category: cs.CV

TL;DR: 本文提出SCAdapter，一种基于CLIP图像空间的新型风格迁移方法，通过分离和整合内容与风格特征，实现更真实、细节丰富的照片级风格迁移，并显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在风格迁移中难以实现照片级真实感，易产生绘画效果或遗漏细节；且未充分解决原始内容风格和风格参考内容特征的干扰问题。

Method: 提出SCAdapter，包含三部分：可控风格自适应实例归一化（CSAdaIN）用于多风格精确融合、KVS注入实现目标风格整合、风格迁移一致性目标保障过程连贯性；全部基于CLIP图像空间进行内容与风格特征的纯化提取与融合。

Result: 在常规及扩散模型基线上均显著超越当前最优方法；去除DDIM反演与推理阶段优化，推理速度至少提升2倍。

Conclusion: SCAdapter能实现更真实、细节更丰富的风格迁移，同时兼顾高效性，适用于实际应用。

Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.

</details>


### [130] [VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference](https://arxiv.org/abs/2512.12977)
*Shengling Qin,Hao Yu,Chenxin Wu,Zheng Li,Yizhong Cao,Zhengyang Zhuge,Yuxin Zhou,Wentao Yao,Yi Zhang,Zhengheng Wang,Shuai Bai,Jianwei Zhang,Junyang Lin*

Main category: cs.CV

TL;DR: VLCache is a cache reuse framework that leverages KV and encoder caches from prior multimodal inputs to avoid recomputation, minimizing reuse error and using dynamic layer-aware recomputation for accuracy-efficiency trade-off; it achieves near-full-recomputation accuracy with only 2–5% token computation and 1.2x–16x TTFT speedup, and is integrated into SGLang.


<details>
  <summary>Details</summary>
Motivation: To eliminate costly recomputation when identical multimodal inputs recur during inference, improving efficiency without sacrificing accuracy.

Method: Formally models cumulative reuse error, minimizes non-prefix cache reuse error, and introduces a dynamic, layer-aware recomputation strategy based on varying layer importance.

Result: VLCache matches full recomputation accuracy while computing only 2–5% of tokens, achieving 1.2x–16x TTFT speedups; successfully integrated into SGLang for practical deployment.

Conclusion: VLCache provides an effective, theoretically grounded cache reuse solution for multimodal LLMs, significantly accelerating inference with negligible accuracy loss.

Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.

</details>


### [131] [Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes](https://arxiv.org/abs/2512.12982)
*Ziheng Qin,Yuheng Ji,Renshuai Tao,Yuxuan Tian,Yuyang Liu,Yipu Wang,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为Generator-Aware Prototype Learning (GAPL) 的新方法，用于解决AI生成图像检测中因生成器多样性增加而导致性能下降的问题。GAPL通过学习规范的伪造原型和低秩自适应训练策略，有效缓解数据异质性和模型瓶颈，显著提升检测泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用AI生成图像检测器在融合多源生成器数据时，性能先提升后下降，存在‘获益后冲突’悖论；根本原因在于数据层面特征分布重叠加剧与模型层面预训练编码器难以适应复杂性增长。

Method: 提出Generator-Aware Prototype Learning（GAPL）框架：1）构建紧凑的规范伪造原型集，统一低方差特征空间以抑制数据异质性；2）采用两阶段训练+低秩自适应（LoRA）提升判别力并保留预训练知识。

Result: 在多种GAN与扩散模型生成图像上实现SOTA检测精度，显著优于现有通用检测器，验证了其强泛化性与鲁棒性。

Conclusion: 数据异质性与固定编码器瓶颈是制约AIGI检测泛化的关键因素；GAPL通过原型约束表征与可适配微调机制，为构建高泛化性检测器提供了新范式。

Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL

</details>


### [132] [Calibrating Uncertainty for Zero-Shot Adversarial CLIP](https://arxiv.org/abs/2512.12997)
*Wenjing lu,Zerui Tao,Dongping Zhang,Yuning Qiu,Yang Yang,Qibin Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的对抗微调目标，用于CLIP模型，通过将输出重参数化为狄利克雷分布的浓度参数，同时对齐预测准确性和不确定性，在保持零样本性能的同时提升对抗鲁棒性与不确定性校准。


<details>
  <summary>Details</summary>
Motivation: CLIP虽在零样本分类中表现优异，但易受对抗攻击，且现有对抗微调方法忽视不确定性校准，导致对抗样本下不确定性被抑制、过度自信，暴露出可靠性缺陷。

Method: 将CLIP输出重参数化为狄利克雷分布的浓度参数，构建能同时表征语义结构与置信度大小的统一表示；设计兼顾预测准确率与不确定性分布整体对齐的对抗微调目标，避免仅锚定单个logit。

Result: 在多个零样本分类基准上验证了该方法可有效恢复校准的不确定性，在保持干净样本准确率的同时取得有竞争力的对抗鲁棒性。

Conclusion: 所提方法弥合了模型鲁棒性与可靠性（不确定性校准）之间的关键鸿沟，为可信零样本学习提供了新思路。

Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.

</details>


### [133] [Few-Step Distillation for Text-to-Image Generation: A Practical Guide](https://arxiv.org/abs/2512.13006)
*Yifan Pu,Yizeng Han,Zhiwei Tang,Jiasheng Tang,Fan Wang,Bohan Zhuang,Gao Huang*

Main category: cs.CV

TL;DR: 本文首次系统研究了将扩散蒸馏技术应用于开放文本到图像（T2I）生成任务，基于FLUX.1-lite教师模型，统一分析现有方法并指出语言提示带来的关键挑战，同时提供实用调优指南与开源实现。


<details>
  <summary>Details</summary>
Motivation: 扩散蒸馏在类条件图像合成中已显著加速，但其在开放文本到图像（T2I）生成中的适用性尚不明确，亟需系统性探索。

Method: 将现有蒸馏方法统一建模，适配于强T2I教师模型FLUX.1-lite，并分析输入缩放、网络结构和超参数等关键因素。

Result: 识别出从离散类别标签转向自由文本提示时的关键障碍，提出实用调优指南，并开源代码与预训练学生模型。

Conclusion: 本工作为在真实T2I应用中部署快速、高保真、资源高效的扩散生成器奠定了坚实基础。

Abstract: Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.

</details>


### [134] [Light Field Based 6DoF Tracking of Previously Unobserved Objects](https://arxiv.org/abs/2512.13007)
*Nikolai Goncharov,James L. Gray,Donald G. Dansereau*

Main category: cs.CV

TL;DR: 本文提出了一种基于光场图像的无预训练模型依赖的目标跟踪方法，利用视觉基础模型提取语义与几何特征，并转化为视角相关的高斯点阵表示，支持可微渲染与位姿优化，显著提升了对反射等复杂外观目标的跟踪鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有高性能跟踪方法依赖预采集对象视图构建显式参考模型，难以泛化至未见过的复杂外观对象（如强反射物体），限制了在机器人和自动驾驶中的通用性。

Method: 利用视觉基础模型从光场图像中提取语义与几何特征，将其编码为视角相关的高斯点阵（view-dependent Gaussian splats）作为统一对象表征；结合可微渲染与位姿优化实现6DoF跟踪；并构建首个含精确位姿真值的光场反射物体跟踪数据集。

Result: 在包含挑战性反射物体的数据集上，本方法性能媲美当前最优的基于模型的跟踪器，尤其在复杂视觉行为下表现更鲁棒。

Conclusion: 该方法摆脱了对预训练对象模型的依赖，实现了对未知、复杂外观物体的通用6DoF跟踪，为机器人系统中的普适目标跟踪提供了新路径。

Abstract: Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.

</details>


### [135] [TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading](https://arxiv.org/abs/2512.13008)
*Xi Luo,Shixin Xu,Ying Xie,JianZhong Hu,Yuwei He,Yuhui Deng,Huaxiong Huang*

Main category: cs.CV

TL;DR: 本文提出TWLR两阶段框架，通过视觉-语言模型融合眼科知识实现糖尿病视网膜病变分级与病灶分类，并利用弱监督语义分割迭代生成病灶显著图与渐进式病理特征擦除，实现无需像素级标注的可解释性严重度回归。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分析依赖高质量专家标注，而眼底图像像素级标注成本高、耗时长；同时深度学习模型缺乏可解释性，限制其临床应用。

Method: TWLR为两阶段框架：第一阶段使用视觉-语言模型，将眼科知识融入文本嵌入，联合完成DR分级与病灶分类；第二阶段构建基于弱监督语义分割的迭代严重度回归框架，通过迭代优化病灶显著图驱动渐进式病理特征擦除，实现疾病向健康眼底外观的可解释转换。

Result: 在FGADR、DDR及私有数据集上，TWLR在DR分类和病灶分割任务中均达到具有竞争力的性能，同时实现了无像素级监督的病灶定位与可解释的疾病演变可视化。

Conclusion: TWLR提供了一种标注高效、可解释性强的自动化视网膜图像分析新范式，有助于推动AI在糖尿病视网膜病变临床诊断中的实际落地。

Abstract: Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.

</details>


### [136] [JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion](https://arxiv.org/abs/2512.13014)
*Haoyu Wang,Lei Zhang,Wenrui Liu,Dengyang Jiang,Wei Wei,Chen Ding*

Main category: cs.CV

TL;DR: 本文提出JoDiffusion，一种基于扩散模型的合成数据生成框架，能够根据文本提示同步生成语义一致的图像与像素级标注掩码，解决现有方法中图像-标注不一致和可扩展性差的问题，并通过掩码优化策略减少生成噪声，在多个基准数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 像素级标注成本高、耗时长，而现有合成数据生成方法存在图像与标注语义不一致或依赖人工掩码导致可扩展性差的问题。

Method: 提出JoDiffusion框架：1）引入独立的标注变分自编码器（VAE）将标注掩码映射到与图像共享的潜在空间；2）调整扩散模型以建模图像与标注掩码在文本提示下的联合分布；3）设计掩码优化策略以降低生成噪声。

Result: 在Pascal VOC、COCO和ADE20K数据集上的实验表明，JoDiffusion生成的数据显著提升语义分割模型性能，优于现有合成数据方法。

Conclusion: JoDiffusion实现了仅靠文本提示即可高效、可扩展地生成图像-标注对，且语义一致、质量高，为低成本高质量训练分割模型提供了新范式。

Abstract: Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.

</details>


### [137] [What Happens Next? Next Scene Prediction with a Unified Video Model](https://arxiv.org/abs/2512.13015)
*Xinjie Li,Zhimin Chen,Rui Zhao,Florian Schiffers,Zhenyu Liao,Vimal Bhat*

Main category: cs.CV

TL;DR: 本文提出Next Scene Prediction (NSP)这一新任务，推动统一视频模型进行时间与因果推理，并设计了融合Qwen-VL与LTX的三阶段训练框架，在自建大规模NSP数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一视频模型聚焦于文本到视频生成等常规任务，其时间与因果推理能力尚未被充分挖掘。

Method: 提出NSP任务；构建融合Qwen-VL（理解）与LTX（生成）的统一框架，通过潜在查询嵌入和连接模块桥接；在自建NSP数据集上分三阶段训练：文本到视频预训练、监督微调、基于因果一致性奖励的GRPO强化学习。

Result: 在自建NSP基准测试中达到当前最优性能，显著提升通用多模态系统对后续事件的预测能力。

Conclusion: NSP任务及其对应框架有效拓展了统一视频模型的时间与因果推理能力，为通用视觉语言模型的发展提供了新方向。

Abstract: Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.

</details>


### [138] [Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing](https://arxiv.org/abs/2512.13018)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 本研究首次全面评估了空间泛化技术在基于深度学习的射频（RF）感知中的应用，重点是使用FMCW MIMO雷达进行室内人数统计。研究发现，基于Sigmoid的幅度加权预处理结合高效迁移学习，能显著提升跨环境性能，大幅降低误差。


<details>
  <summary>Details</summary>
Motivation: 深度学习驱动的射频感知系统在实际部署中面临空间泛化能力不足的问题，亟需系统性评估不同泛化技术的有效性。

Method: 系统比较了多种空间泛化方法：基于幅度的统计预处理（Sigmoid加权、阈值置零）、频域滤波、自编码器背景抑制、数据增强和迁移学习，并在两个不同布局的环境中进行实验验证。

Result: Sigmoid幅度加权使RMSE和MAE分别降低50.1%和55.2%；数据增强最多改善8.8% MAE；迁移学习在540个目标域样本下使RMSE和MAE分别降低82.1%和91.3%。

Conclusion: 将基于幅度的预处理与高效迁移学习相结合，是构建具备强空间鲁棒性的雷达感知系统的实用且有效路径。

Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.

</details>


### [139] [SneakPeek: Future-Guided Instructional Streaming Video Generation](https://arxiv.org/abs/2512.13019)
*Cheeun Hong,German Barquero,Fadime Sener,Markos Georgopoulos,Edgar Schönfeld,Stefan Popov,Yuming Du,Oscar Mañas,Albert Pumarola*

Main category: cs.CV

TL;DR: 本文提出SneakPeek——一种面向 instructional video 生成的未来驱动流式扩散模型，通过预测因果适配、未来引导自强制与多提示条件控制，显著提升长序列视频的时间一致性与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成长序列、多步骤的教学视频时，难以保持时间一致性与可控性。

Method: 提出SneakPeek：基于扩散的自回归框架，包含三项创新——(1) 预测因果适应（学习下一帧预测与关键帧预判）；(2) 未来引导自强制+双区域KV缓存（缓解推理阶段曝光偏差）；(3) 多提示条件控制（实现细粒度步骤级控制）。

Result: 实验表明该方法能生成时间连贯、语义保真、准确遵循复杂多步任务描述的教学视频。

Conclusion: SneakPeek有效缓解了教学视频生成中的时序漂移问题，支持动态交互式流式生成，为可控 instructional 视频合成提供了新范式。

Abstract: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.

</details>


### [140] [Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs](https://arxiv.org/abs/2512.13031)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 本文首次全面比较了基于规则的方法、传统机器学习模型和深度学习模型在调频连续波MIMO雷达无线电波传感中的性能。结果表明，高容量模型在相同环境中能实现高精度细粒度输出，但易受域偏移影响；而基于规则的方法虽无法提供细粒度输出，却对域偏移具有鲁棒性。此外，所有模型均表现出空间泛化能力与输出精细度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 缺乏对不同建模方法（规则法、传统机器学习、深度学习）在无线电波传感任务中性能与泛化能力的系统性比较。

Method: 系统评估五种方法：基于连通分量的规则方法、k近邻、随机森林、支持向量机三种传统机器学习模型，以及CNN-LSTM混合深度学习模型，在两个不同布局的室内环境中进行对比实验。

Result: CNN-LSTM在训练环境精度最高，但换新布局时性能显著下降；规则方法在新布局下保持稳定；所有模型在人存在/不存在的二分类任务上均表现优异。

Conclusion: 高容量模型适合同域细粒度感知，但泛化性差；规则方法鲁棒性强但输出粗糙；模型类型无关，空间泛化能力与输出粒度存在明确权衡。

Abstract: This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.

</details>


### [141] [Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models](https://arxiv.org/abs/2512.13039)
*Hao Chen,Yiwei Wang,Songze Li*

Main category: cs.CV

TL;DR: 本文提出了一种双向图像引导的概念擦除（Bi-Erasing）框架，通过负向分支抑制有害概念、正向分支引导安全替代，实现概念擦除与生成质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法多采用单向策略（仅抑制目标概念或强化安全替代），难以兼顾擦除效果与生成质量。

Method: 提出Bidirectional Image-Guided Concept Erasure（Bi-Erasing）框架，包含解耦的负向分支（抑制有害语义）和正向分支（提供安全替代的视觉引导），并引入基于掩码的过滤机制防止无关内容干扰。

Result: 在多项实验中，Bi-Erasing在概念擦除有效性与视觉保真度之间取得了更优的平衡，优于基线方法。

Conclusion: 双向协同优化图像分支的策略能更有效地协调概念擦除与生成质量，为安全可控的文本到图像生成提供了新思路。

Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.

</details>


### [142] [GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/abs/2512.13043)
*Tong Wei,Yijun Yang,Changhao Zhang,Junliang Xing,Yuanchun Shi,Zongqing Lu,Deheng Ye*

Main category: cs.CV

TL;DR: 本文提出GTR-Turbo，一种无需额外昂贵教师模型的高效多轮强化学习方法，通过合并训练过程中的检查点权重生成‘免费’教师模型，显著提升性能并大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的多模态智能体多轮强化学习面临稀疏奖励和长程信用分配难题，依赖高成本、特权化教师模型（如GPT、Gemini）的方法（如GTR）实用性与可复现性差。

Method: GTR-Turbo在RL训练过程中动态合并历史检查点权重，构建轻量级、无额外参数的‘免费’教师模型，并用于监督微调或软logit蒸馏来指导后续RL训练。

Result: 在多种视觉智能体任务上，相比GTR，GTR-Turbo将基线模型准确率提升10–30%，训练时间减少50%，计算成本降低60%，且避免了熵坍塌、训练更稳定。

Conclusion: GTR-Turbo通过模型权重融合实现高效、低成本、高稳定性的多轮RL训练，摆脱对特权VLM教师的依赖，为多模态智能体训练提供了实用新范式。

Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.

</details>


### [143] [Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing](https://arxiv.org/abs/2512.13055)
*Jaeyoon Kim,Yoonki Cho,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: 本文提出了一种高效的非对称视觉地点识别（VPR）框架，通过结合高容量离线图库模型与轻量级在线查询网络，并引入地理记忆库和隐式嵌入增强技术，显著降低计算开销，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 高容量基础模型（如DINOv2）虽在VPR中表现优异，但计算开销大，难以部署于资源受限设备。

Method: 提出非对称VPR框架：高容量模型用于离线图库特征提取，轻量级网络用于在线查询；设计地理记忆库利用地理位置元数据组织图库特征，避免k-NN兼容训练；引入隐式嵌入增强提升小模型建模能力。

Result: 实验表明该方法大幅降低计算成本，且性能优于现有非对称检索方法。

Conclusion: 所提方法为资源受限环境下的VPR提供了新思路，兼顾效率与精度。

Abstract: Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR

</details>


### [144] [Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models](https://arxiv.org/abs/2512.13072)
*Zizhi Chen,Yizhen Gao,Minghao Han,Yizhou Liu,Zhaoyu Chen,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种结合检索增强生成（RAG）与动态知识蒸馏的多模态医学视觉语言模型持续学习框架，以解决跨模态域差异与细粒度特征保持之间的核心矛盾，并在新提出的医学通才任务增量学习（MGTIL）基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态生物医学视觉语言模型在持续学习中面临保留细粒度单模态特征与弥合跨模态领域差距之间的核心矛盾。

Method: 基于1800万样本的PubMed多模态医学检索数据库，构建多模态、多层检索增强生成（RAG）系统，实现动态知识检索指导微调；并设计动态知识蒸馏框架，根据所需细节程度自适应调节参数空间重要性、知识蒸馏粒度及参考数据分布。

Result: 在新提出的严格医学通用任务增量学习（MGTIL）基准上，全面评估了模型对大幅域偏移适应性、细粒度域内特征保持能力及复杂新任务实时学习能力，实验结果表明该方法在所有指标上均达到SOTA。

Conclusion: 所提框架有效缓解了多模态VLMs在持续学习中的模态鸿沟与细粒度特征遗忘矛盾，验证了RAG与动态蒸馏协同在临床相关持续学习任务中的有效性与实用性。

Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.

</details>


### [145] [DiRe: Diversity-promoting Regularization for Dataset Condensation](https://arxiv.org/abs/2512.13083)
*Saumyaranjan Mohanty,Aravind Reddy,Konda Reddy Mopuri*

Main category: cs.CV

TL;DR: 本文提出了一种名为DiRe的多样性正则化器，用于减少数据集蒸馏中合成数据的冗余性并提升其多样性，该方法可即插即用地提升现有最先进蒸馏方法在多个基准数据集上的泛化能力和多样性表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法合成的数据存在显著冗余，亟需提升合成数据的多样性。

Method: 提出一种由余弦相似度和欧氏距离构成的多样性正则化器（DiRe），可即插即用地集成到多种先进蒸馏方法中。

Result: 在CIFAR-10到ImageNet-1K等多个基准数据集上，加入DiRe后显著提升了现有最先进蒸馏方法在泛化性和多样性指标上的表现。

Conclusion: DiRe是一种简单有效、通用性强的多样性增强策略，能显著改善数据集蒸馏效果。

Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.

</details>


### [146] [UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era](https://arxiv.org/abs/2512.13089)
*Ziqiang Zhu,Bowei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种无需标注、开放词汇的无监督变化检测方法UniVCD，基于冻结的SAM2和CLIP模型，通过轻量级特征对齐模块融合空间细节与语义先验，并结合简化后处理提升边界清晰目标的检测精度，在多个基准上达到SOTA或媲美现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法严重依赖监督学习，泛化能力差、标注成本高，且局限于预定义类别；而视觉基础模型（如SAM2、CLIP）的兴起为突破这些限制提供了新路径。

Method: 提出UniVCD框架：1）冻结SAM2和CLIP作为多模态骨干；2）设计轻量级特征对齐模块，桥接SAM2的空间细节表征与CLIP的语义先验；3）引入简化后处理流程抑制噪声和伪变化。全程无监督，无需配对图像或标签。

Result: 在多个公开BCD和SCD基准上，UniVCD在F1、IoU等关键指标上稳定达到强性能，匹配或超越现有开放词汇CD方法。

Conclusion: 冻结视觉基础模型+轻量多模态对齐的无监督范式，是实现开放词汇变化检测的一种实用且高效的新途径。

Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.

</details>


### [147] [ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning](https://arxiv.org/abs/2512.13095)
*Feng Zhang,Zezhong Tan,Xinhong Ma,Ziqiang Dong,Xi Leng,Jianfei Zhao,Xin Sun,Yang Yang*

Main category: cs.CV

TL;DR: 本文提出ADHint方法，通过将样本难度作为关键因素，自适应地调整提示比率和优势估计，以在探索与模仿之间取得更好平衡，从而提升推理能力和分布外泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的强化学习方法忽略样本难度，在提示比率调度和相对优势估计中导致学习不稳定和过度模仿离策略提示。

Method: 提出ADHint框架，包括：1）基于样本难度先验的自适应提示机制；2）基于一致性的梯度调制与选择性掩码以保护提示内容；3）基于 rollout 难度后验的优势估计方法。

Result: 在多种模态、模型规模和领域上实验表明，ADHint在pass@1和avg@8指标上持续超越现有方法，显著提升推理能力与分布外泛化性能。

Conclusion: 将难度建模融入提示调度与优势估计可有效缓解模仿偏差、增强探索稳定性，为大模型后训练提供新范式。

Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

</details>


### [148] [Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2512.13101)
*Wenjing Lu,Yi Hong,Yang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为UnCoL的双教师半监督医学图像分割框架，通过融合冻结基础模型的通用知识与自适应教师的特化表征，并利用预测不确定性动态调节伪标签学习，显著提升了小样本和罕见病灶下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型在医学图像分割中虽具强泛化能力，但在标注稀缺或病理变异罕见的临床任务上表现不佳，源于通用先验与任务特定需求之间的不匹配。

Method: 提出不确定性引导的协同学习（UnCoL）框架：采用冻结的基础模型作为通用知识教师，同时维护一个渐进式自适应教师以学习任务特化表征；伪标签学习过程由预测不确定性自适应调控，抑制不可靠监督。

Result: 在多种2D/3D医学图像分割基准上，UnCoL持续超越当前最优半监督方法及基础模型基线，并以显著减少的标注量达到接近全监督性能。

Conclusion: UnCoL有效弥合了通用基础模型与临床特化任务间的鸿沟，为低资源医学图像分割提供了鲁棒、可扩展的新范式。

Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.

</details>


### [149] [FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection](https://arxiv.org/abs/2512.13104)
*Yan Zhang,Baoxin Li,Han Sun,Yuhang Gao,Mingtai Zhang,Pei Wang*

Main category: cs.CV

TL;DR: 本文提出FID-Net模型，基于YOLOv8n改进，用于从无人机可见光影像中精准检测受虫害树木，并结合三种空间分析方法（核密度估计、邻域风险评估、DBSCAN聚类）实现虫害态势分析与防控决策支持。


<details>
  <summary>Details</summary>
Motivation: 传统森林虫害监测方法难以兼顾大范围覆盖与细粒度识别，亟需高效、自动化的智能监测手段。

Method: 提出FID-Net：在YOLOv8n基础上引入轻量级特征增强模块（FEM）、自适应多尺度特征融合模块（AMFM）和高效通道注意力机制（ECA）；并构建基于检测结果的虫害态势分析框架，包括核密度估计、邻域感染风险评估和DBSCAN健康树聚类。

Result: 在新疆天山东部32个林斑无人机影像上，FID-Net达到86.10%精度、75.44%召回率、82.29% mAP@0.5、64.30% mAP@0.5:0.95，优于主流YOLO模型；空间分析证实虫害呈显著聚集分布。

Conclusion: FID-Net可高精度区分单株树木健康状态，结合空间指标可支撑智能化虫害监测、早期预警与精准防控管理。

Abstract: Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.

</details>


### [150] [Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2512.13107)
*Zhijian He,Feifei Liu,Yuwei Li,Zhanpeng Liu,Jintao Cheng,Xieyuanli Chen,Xiaoyu Tang*

Main category: cs.CV

TL;DR: 本文提出DiffFusion框架，利用扩散模型进行图像与点云的天气退化恢复，并通过双向自适应融合对齐模块提升多模态3D目标检测在恶劣天气下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态3D目标检测在恶劣天气下因模态间失真与错位而性能受限。

Method: 提出Diffusion-IR恢复受天气影响的图像、PCR利用图像线索修复点云，并设计BAFAM实现双向BEV对齐与动态跨模态融合。

Result: 在三个公开数据集上达到恶劣天气下SOTA鲁棒性，且在干净数据上保持高性能；零样本迁移至真实DENSE数据集验证泛化能力。

Conclusion: DiffFusion有效提升了多模态3D检测在复杂天气下的鲁棒性与泛化性，代码将开源。

Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.

</details>


### [151] [DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass](https://arxiv.org/abs/2512.13122)
*Vivek Alumootil,Tuan-Anh Vu,M. Khalid Jawed*

Main category: cs.CV

TL;DR: 本文提出DePT3R框架，首次在单次前向传播中同时实现无相机位姿依赖的密集三维点跟踪与动态场景三维重建，通过时空特征提取与像素级预测实现多任务学习，并在多个动态场景基准上验证了其高效性与先进性。


<details>
  <summary>Details</summary>
Motivation: 现有密集三维点跟踪方法受限于成对处理、需已知相机位姿或依赖帧序，灵活性不足；而大规模无位姿图像集合的高效三维重建进展，为构建统一的动态场景理解方法提供了契机。

Method: 提出DePT3R框架，采用强大多尺度时空特征骨干网络，配合密集预测头回归像素级映射，实现端到端的多任务联合学习（密集点跟踪+3D重建），且完全不依赖输入图像的相机位姿。

Result: 在多个动态场景基准上验证了DePT3R的优越性能，尤其在内存效率方面显著优于现有最先进方法。

Conclusion: DePT3R实现了无需相机位姿的单次前向密集跟踪与重建，提升了动态场景理解的适应性、效率与实用性，推动了统一动态场景解析框架的发展。

Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R

</details>


### [152] [LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping](https://arxiv.org/abs/2512.13130)
*Shanghua Liu,Majharulislam Babor,Christoph Verduyn,Breght Vandenberghe,Bruno Betoni Parodi,Cornelia Weltzien,Marina M. -C. Höhne*

Main category: cs.CV

TL;DR: 本文提出了CanolaTrack数据集和LeafTrackNet框架，用于高精度的油菜叶片跟踪，解决了现有方法在复杂作物和真实场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有植物特异性跟踪方法受限于小规模物种或约束成像条件，而通用多目标跟踪方法不适用于动态生物场景；同时缺乏大规模真实条件下的数据集阻碍了叶片跟踪模型的发展。

Method: 构建了包含5704张RGB图像和31840个叶片实例的CanolaTrack数据集；提出LeafTrackNet框架，结合YOLOv10叶片检测器与MobileNetV3嵌入网络，并采用基于嵌入的记忆关联策略进行时序身份维持。

Result: LeafTrackNet在CanolaTrack上相比植物特异性跟踪器和SOTA MOT基线提升了9% HOTA指标。

Conclusion: 本工作为真实条件下叶片级跟踪建立了新标准，并发布了目前农业作物中最大的叶片跟踪数据集CanolaTrack，推动植物表型研究发展。

Abstract: High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.

</details>


### [153] [Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models](https://arxiv.org/abs/2512.13144)
*Chun Kit Wong,Paraskevas Pegios,Nina Weng,Emilie Pi Fogtmann Sejer,Martin Grønnebæk Tolsgaard,Anders Nymark Christensen,Aasa Feragen*

Main category: cs.CV

TL;DR: 本文提出了一种可解释的方法——权重空间相关性分析（WSCA），用于量化深度学习模型在医学影像中对特征（尤其是混杂元数据）的实际利用程度，并验证了其在检测和确认模型是否真正依赖临床相关特征而非无关采集因素上的有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像中容易发生‘捷径学习’，即依赖与临床任务无关的混杂元数据（如设备型号），但尚不清楚模型是否实际使用了嵌入中编码的这些信息。

Method: 提出权重空间相关性分析（WSCA），通过测量主临床任务分类头与辅助元数据任务分类头之间的权重向量对齐程度，来量化模型对各类特征的利用。

Result: 在人工诱导捷径学习的验证实验中成功检测到偏差；应用于SA-SonoNet模型预测自发性早产（sPTB）时，发现其分类权重与临床相关因素（如出生体重）高度相关，而与扫描仪等无关采集因素解耦。

Conclusion: 该方法可有效验证模型的可信度，表明在无诱导偏差时，临床模型能选择性地利用真实临床信号相关特征，而非混杂元数据。

Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.

</details>


### [154] [StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion](https://arxiv.org/abs/2512.13147)
*Sangmin Hong,Suyoung Lee,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: 本文提出了一种名为StarryGazer的无监督深度补全框架，利用预训练单目深度估计（MDE）模型生成相对深度图，并通过随机重缩放和分割构造合成伪真值对，再结合RGB图像训练细化网络，从而在无需真实深度标签的情况下实现高质量深度补全。


<details>
  <summary>Details</summary>
Motivation: 现有无监督深度补全方法依赖辅助数据，不贴近实际场景；而单目深度估计（MDE）虽能从单图生成相对深度，但尚无工作能有效融合MDE与稀疏深度图进行补全，简单仿射变换误差大。

Method: 利用预训练MDE模型生成相对深度图，对其进行分割与随机重缩放以构建合成稀疏-稠密伪真值对；设计一个融合相对深度图与RGB图像的细化网络，在无真实深度监督下进行训练。

Result: StarryGazer在多个数据集上显著优于现有无监督方法及经变换的MDE结果，验证了其能有效利用MDE能力并借助稀疏深度信息校正误差。

Conclusion: StarryGazer是一种域无关、无需真实深度标签的深度补全新范式，成功桥接了MDE与稀疏深度信息，提升了无监督深度补全的精度与鲁棒性。

Abstract: The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.

</details>


### [155] [Intrinsic Image Fusion for Multi-View 3D Material Reconstruction](https://arxiv.org/abs/2512.13157)
*Peter Kocsis,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

TL;DR: 本文提出了一种名为Intrinsic Image Fusion的方法，通过融合多视角图像并结合单视角先验（基于扩散模型的材质估计器）和鲁棒优化框架，实现高质量、物理一致的材质重建，并在合成与真实场景中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 材质重建问题高度欠约束，传统基于分析-合成的方法依赖计算昂贵且噪声大的路径追踪，需要更好的约束来提升重建质量与一致性。

Method: 引入基于扩散模型的单视图材质估计器生成多候选分解；拟合显式低维参数化函数以降低预测不一致性；设计基于软视图选择与置信度加权的多视图内点集的鲁棒优化框架；最后通过逆路径追踪优化低维参数。

Result: 在合成与真实场景的材质解耦任务中均超越当前最优方法，重建结果锐利干净，适用于高质量重光照。

Conclusion: Intrinsic Image Fusion通过融合多视角信息与单视角先验，并借助鲁棒参数化优化，显著提升了材质重建的物理一致性与视觉质量，为基于图像的材质建模提供了新范式。

Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.

</details>


### [156] [A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis](https://arxiv.org/abs/2512.13164)
*Xianchao Guan,Zhiyuan Fan,Yifeng Wang,Fuqiang Chen,Yanjiang Zhou,Zengyang Che,Hongxue Meng,Xin Li,Yaowei Wang,Hongpeng Wang,Min Zhang,Heng Tao Shen,Zheng Zhang,Yongbing Zhang*

Main category: cs.CV

TL;DR: 本文提出了CRAFTS，一种专为病理学设计的文本到图像生成基础模型，通过双阶段训练和新型对齐机制，解决了生成图像语义不稳定和形态幻觉问题，生成了涵盖30种癌症类型的高质量病理图像，并在多个临床任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 临床级病理人工智能的发展受限于多样化、高质量标注数据集的稀缺性，而现有生成模型存在语义不稳定和形态幻觉问题，影响诊断可靠性。

Method: 提出了一种相关性调控的组织合成对齐框架（CRAFTS），采用双阶段训练策略，在约280万图像-文本对上进行训练，并引入新型对齐机制以抑制语义漂移，确保生物学准确性；同时结合ControlNet实现对组织结构的精确控制。

Result: CRAFTS成功生成涵盖30种癌症类型的多样化病理图像，质量经客观指标和病理医生评估验证；增强的数据集提升了分类、跨模态检索、自监督学习和视觉问答等临床任务性能；结合ControlNet可基于核分割掩码和荧光图像精准控制组织架构。

Conclusion: CRAFTS突破了病理数据稀缺与隐私限制，提供了无限、多样且标注丰富的组织学数据源，助力构建针对罕见及复杂癌症表型的鲁棒诊断工具。

Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.

</details>


### [157] [Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation](https://arxiv.org/abs/2512.13175)
*Hongxuan Sun,Tao Wu*

Main category: cs.CV

TL;DR: 本文提出DFSS，一种专为语义分割设计的无数据知识蒸馏框架，通过利用教师模型的BN统计信息指导近似分布采样（ADS），并引入加权分布渐进式蒸馏（WDPD）策略，显著提升无数据蒸馏在分割任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无数据知识蒸馏方法主要面向分类任务，忽视语义分割所需的像素空间连续性和场景结构一致性，导致直接迁移至分割任务时性能大幅下降。

Method: 提出DFSS框架：1）利用教师模型BN统计指导近似分布采样（ADS），避免依赖不可靠的教师预测；2）设计加权分布渐进式蒸馏（WDPD），训练初期优先蒸馏更符合原始数据分布的可靠样本，逐步引入更具挑战性的样本。

Result: 在标准语义分割基准上，DFSS持续超越现有无数据蒸馏方法，达到该领域最先进水平，且对辅助数据的依赖显著降低。

Conclusion: 尊重场景结构与分布特性的无数据蒸馏策略对语义分割至关重要；DFSS通过BN引导采样与渐进式加权蒸馏，有效弥合了无数据设定下分割性能的差距。

Abstract: Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.

</details>


### [158] [CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception](https://arxiv.org/abs/2512.13191)
*Gong Chen,Chaokun Zhang,Pengcheng Lv,Xiaohui Xie*

Main category: cs.CV

TL;DR: 本文提出CoRA架构，通过特征级融合和目标级校正的混合方法，在低通信开销下解耦性能与鲁棒性，显著提升协同感知在恶劣通信条件下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有中间融合方法在不良通信条件下因数据传输导致的错位而性能下降，限制了实际部署。

Method: 提出CoRA架构，包含特征级融合分支（选择关键特征高效融合）和目标级校正分支（利用语义相关性校正空间位移）。

Result: 在极端场景下，CoRA相比基线方法在AP@0.7上提升约19%，通信量减少5倍以上。

Conclusion: CoRA通过互补融合范式实现了高性能与强鲁棒性的统一，是面向实际部署的鲁棒协同感知新方案。

Abstract: Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.

</details>


### [159] [POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling](https://arxiv.org/abs/2512.13192)
*Zhuo Chen,Chengqun Yang,Zhuo Su,Zheng Lv,Jingnan Gao,Xiaoyuan Zhang,Xiaokang Yang,Yichao Yan*

Main category: cs.CV

TL;DR: 本文提出了POLAR数据集和POLARNet模型，旨在解决人脸重光照任务中物理一致照明数据稀缺的问题，通过构建大规模、物理校准的OLAT数据集，并设计基于流的生成模型，实现对单张人像在不同光照方向下的精细、可控且身份保持的重光照合成。


<details>
  <summary>Details</summary>
Motivation: 人脸重光照任务受限于大规模、物理一致的照明数据缺乏，导致模型难以学习真实光照变化规律。

Method: 构建了包含200多名受试者、156个光照方向、多视角与多样表情的大规模物理校准OLAT数据集POLAR；在此基础上提出基于流（flow-based）的生成模型POLARNet，将光照建模为连续、物理可解释的状态变换，从单张人像预测各方向OLAT响应。

Result: POLARNet能准确预测细粒度、方向感知的光照响应，在保持身份与几何结构的同时实现高质量、可控的人脸重光照；POLAR与POLARNet共同构成统一的照明学习框架，支持可扩展、可复现的 Portrait Illumination 研究。

Conclusion: POLAR和POLARNet建立了真实数据采集、生成建模与物理驱动重光照之间的闭环，推动人脸重光照向更物理合理、可控、可扩展的方向发展。

Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.

</details>


### [160] [Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance](https://arxiv.org/abs/2512.13238)
*Francesco Ragusa,Michele Mazzamuto,Rosario Forte,Irene D'Ambra,James Fort,Jakob Engel,Antonino Furnari,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: Ego-EXTRA 是一个面向专家-学徒辅助任务的视频-语言自拍数据集，包含50小时真实场景下的自拍视频及配套双人对话，构建了15k+高质量视觉问答对，用于评测多模态大模型在自拍视频理解与专家级辅助任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有自拍视频-语言数据集缺乏真实专家指导下的自然交互，难以支撑面向实际应用（如远程培训、智能助手）的多模态模型评估。

Method: 采用“Wizard of OZ”范式采集数据：专家佩戴摄像头以学徒第一视角观察操作过程，并实时用自然语言提供指导或主动建议；录制并转录双向对话，构建视觉问答基准。

Result: 构建了含15,000+高质量VQA样本的Ego-EXTRA基准；实验表明当前多模态大模型在此任务上表现有限，凸显其在真实专家辅助场景中的不足。

Conclusion: Ego-EXTRA为评估和推动面向真实世界辅助任务的自拍视频-语言模型提供了新基准，强调需提升模型在上下文感知、主动交互与专业推理方面的能力。

Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.

</details>


### [161] [STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits](https://arxiv.org/abs/2512.13247)
*Foivos Paraperas Papantoniou,Stathis Galanakis,Rolandos Alexandros Potamias,Bernhard Kainz,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: 本文提出了STARCaster，一种身份感知的时空视频扩散模型，统一解决了语音驱动肖像动画和自由视角说话肖像合成问题。通过引入软身份约束和利用视频数据的多视角特性隐式实现3D感知，该模型在运动多样性、身份保真度和视图一致性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有2D语音到视频扩散模型依赖强参考引导，导致运动多样性受限；3D感知动画则常因预训练三平面生成器的反演而导致重建不完美和身份漂移。

Method: STARCaster采用分阶段组合式设计：先进行身份感知运动建模，再通过唇读监督实现音视频同步，最后通过时序到空间适配实现新视角动画；提出解耦学习策略分别训练视角一致性和时序连贯性，并采用自强化训练方案以利用更长时序上下文。

Result: STARCaster在多个基准测试中一致超越先前方法，在跨任务和跨身份泛化能力上表现优异，有效缓解了动画静态化问题。

Conclusion: STARCaster通过重新思考参考引导与几何建模范式，在2D视频域内实现了高效、鲁棒且身份一致的语音驱动动画生成，为多视角说话头像合成提供了新思路。

Abstract: This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.

</details>


### [162] [Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection](https://arxiv.org/abs/2512.13250)
*Juil Koo,Daehyeon Choi,Sangwoo Youn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

TL;DR: 本文提出了视觉基础主动视角选择（VG-AVS）任务，旨在仅基于当前图像视觉信息选择最具信息量的下一视角，无需场景记忆或外部知识；构建了合成数据集，并提出结合监督微调与强化学习的VLM微调框架，在合成与真实场景中均表现出色，且能提升现有具身问答（EQA）系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在视觉问答（VQA）中表现优异，但仅限于静态图像理解（snapshot vision），而具身智能体需要主动移动以获取更优视角（ambulatory vision），因此需发展能支持主动视角选择的能力。

Method: 提出Visually Grounded Active View Selection（VG-AVS）新任务；构建自动配对查询-目标视角及问答提示的合成数据集；设计两阶段训练框架：先对预训练VLM进行监督微调（SFT），再通过强化学习（RL）优化视角选择策略。

Result: 所提方法在视角选择驱动的问答任务中取得优异性能，具备跨合成与真实场景的强泛化能力；将VG-AVS模块嵌入现有场景探索型EQA系统后，显著提升了下游问答准确率。

Conclusion: VG-AVS为连接静态VLM与具身视觉推理提供了新范式，证明仅依赖单帧视觉输入即可有效指导主动感知决策，且可即插即用地增强具身问答系统。

Abstract: Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.

</details>


### [163] [CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing](https://arxiv.org/abs/2512.13276)
*Yan Li,Lin Liu,Xiaopeng Zhang,Wei Xue,Wenhan Luo,Yike Guo,Qi Tian*

Main category: cs.CV

TL;DR: 本文提出CogniEdit框架，通过多模态推理与密集奖励优化，在扩散模型的去噪轨迹中实现细粒度指令编辑，显著提升颜色、位置、数量等属性的控制精度与图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的指令图像编辑方法难以处理细粒度指令（如颜色、位置、数量），且GRPO等优化方法仅在单步采样中提供稀疏反馈，缺乏对整个去噪轨迹的控制能力。

Method: 提出CogniEdit统一框架：(1) 使用多模态大语言模型分解复杂指令；(2) 动态Token聚焦重定位机制自适应强调细粒度属性；(3) 基于密集GRPO的优化，在连续去噪步骤间传播梯度，实现轨迹级监督。

Result: 在多个基准数据集上实验表明，CogniEdit在细粒度指令遵循、视觉质量和可编辑性保持方面达到SOTA性能。

Conclusion: CogniEdit通过引入轨迹级梯度流与多模态指令解析，有效解决了扩散模型图像编辑中细粒度控制难的问题，为高精度可控编辑提供了新范式。

Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation

</details>


### [164] [Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?](https://arxiv.org/abs/2512.13281)
*Jiaqi Wang,Weijia Wu,Yi Zhan,Rui Zhao,Ming Hu,James Cheng,Wei Liu,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

TL;DR: 本文提出了Video Reality Test基准测试，利用ASMR视频评估AI生成视频在音画强耦合下的感知真实性，并通过对抗性创作者-评审者协议揭示当前视频生成模型与多模态大模型（VLMs）在真实感和音画一致性上的能力边界。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC检测基准多忽略音频、领域宽泛且仅限分类任务；尚不清楚最先进视频生成模型能否产出能可靠欺骗人类和视觉语言模型（VLMs）的沉浸式音画同步视频。

Method: 构建基于真实ASMR视频的Video Reality Test基准，强调细粒度动作-物体交互与多样性；采用对抗性创作者（生成模型）-评审者（VLMs/人类）协议进行评估；系统测试生成视频在有无音频条件下的可识别性及模型鲁棒性。

Result: 最强生成模型Veo3.1-Fast可欺骗多数VLMs（最强VLM Gemini 2.5-Pro准确率仅56%，远低于人类专家81.25%）；加入音频有助于区分真假，但水印等表层线索仍易误导模型。

Conclusion: 当前视频生成已逼近人类感知极限，但VLMs在感知保真度和音画一致性上存在明显短板；该基准为AIGC检测与生成能力评估提供了新维度和现实挑战。

Abstract: Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\% accuracy (random 50\%), far below that of human experts (81.25\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.

</details>


### [165] [CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images](https://arxiv.org/abs/2512.13285)
*Bo Liu,Qiao Qin,Qinghui He*

Main category: cs.CV

TL;DR: 本文提出CausalCLIP框架，通过因果推断方法解耦生成图像检测中的因果特征与非因果特征，提升模型在未知生成模型上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测方法（包括基于预训练视觉语言模型的方法）常产生高度纠缠的特征表示，混入大量非因果（虚假或无关）模式，导致跨生成技术泛化能力受限。

Method: 构建结构因果模型刻画图像生成过程；采用Gumbel-Softmax特征掩码与HSIC独立性约束实现因果/非因果特征解耦；引入因果引导的特征过滤机制保留稳定、可迁移的法证线索。

Result: 在跨系列未见生成模型测试中，CausalCLIP显著优于SOTA方法：准确率提升6.83%，平均精度提升4.06%。

Conclusion: 显式建模和分离因果特征可有效提升生成图像检测器的分布外泛化能力，因果推理为通用数字取证提供了新范式。

Abstract: The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.

</details>


### [166] [LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models](https://arxiv.org/abs/2512.13290)
*Shu Yu,Chaochao Lu*

Main category: cs.CV

TL;DR: 本文提出LINA框架，通过因果场景图（CSG）和物理对齐探针（PAP）数据集诊断扩散模型在物理对齐与OOD指令遵循上的不足，发现其根源在于缺乏因果方向学习与因果因子解耦；LINA通过提示与视觉隐空间的定向引导及因果感知去噪调度，显著提升图像与视频生成中的因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成中虽表现优异，但在物理对齐和分布外（OOD）指令遵循上存在缺陷，作者认为这源于模型未能学习因果方向及解耦因果因子以支持新颖重组。

Method: 构建因果场景图（CSG）和物理对齐探针（PAP）数据集进行诊断分析；基于三项关键发现（多跳推理困难、提示嵌入中纹理与物理表征解耦、早期去噪步骤主导视觉因果结构），提出LINA框架：包含提示与视觉隐空间的定向引导机制，以及重分配的因果感知去噪调度。

Result: LINA在挑战性因果生成任务和Winoground数据集上达到SOTA性能，同时提升了图像与视频扩散模型的物理对齐能力和OOD指令遵循能力。

Conclusion: 扩散模型的因果生成瓶颈在于早期去噪阶段的因果结构建模不足与提示表征中因果因子未充分解耦；LINA通过自适应干预学习有效缓解该问题，为因果可控生成提供了新范式。

Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.

</details>


### [167] [ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement](https://arxiv.org/abs/2512.13303)
*Zhihang Liu,Xiaoyi Bao,Pandeng Li,Junjie Zhou,Zhaohe Liao,Yefei He,Kaixun Jiang,Chen-Wei Xie,Yun Zheng,Hongtao Xie*

Main category: cs.CV

TL;DR: 本文提出了一个名为ShowTable的新框架，用于解决创造性表格可视化任务，通过结合多模态大语言模型（MLLM）与扩散模型，并采用渐进式自校正流程，实现了高保真、准确且美观的数据可视化生成。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成和统一模型在需要深度推理、规划及精确数据到视觉映射的任务上表现不足，尤其在创造性表格可视化这类复杂任务中存在明显局限。

Method: 提出ShowTable流水线，利用MLLM作为推理与错误判断的中枢，指导扩散模型逐步生成并修正可视化结果；同时构建三个自动化数据构造流程训练各模块，并建立包含800个挑战性样本的TableVisBench基准。

Result: 实验表明，ShowTable在TableVisBench上显著优于各类基线方法，展现出优异的多模态推理、生成与错误纠正能力。

Conclusion: ShowTable为数据可视化任务提供了新范式，验证了MLLM与扩散模型协同进行复杂视觉生成任务的有效性与潜力。

Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.

</details>


### [168] [KlingAvatar 2.0 Technical Report](https://arxiv.org/abs/2512.13313)
*Kling Team,Jialu Chen,Yikang Ding,Zhixue Fang,Kun Gai,Yuan Gao,Kang He,Jingyun Hua,Boyuan Jiang,Mingming Lao,Xiaohan Li,Hui Liu,Jiwen Liu,Xiaoqiang Liu,Yuan Liu,Shun Lu,Yongsen Mao,Yingchao Shao,Huafeng Shi,Xiaoyu Shi,Peiqin Sun,Songlin Tang,Pengfei Wan,Chao Wang,Xuebo Wang,Haoxian Zhang,Yuanxing Zhang,Yan Zhou*

Main category: cs.CV

TL;DR: KlingAvatar 2.0 是一种时空级联框架，用于高效生成长时长、高分辨率、多模态对齐的虚拟人视频，通过蓝图关键帧生成、子片段精细化与多专家协同推理提升时序一致性、画质与指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟人视频生成模型在长时长、高分辨率视频生成中存在时序漂移、质量下降和提示跟随弱等问题。

Method: 提出时空级联框架 KlingAvatar 2.0：先生成低分辨率语义与运动蓝图关键帧，再用首尾帧策略精细化为高分辨率、时序连贯的子片段；引入由三个模态专用LLM专家组成的协同推理导演（Co-Reasoning Director）进行跨模态指令融合与意图推断，并设计负向导演（Negative Director）优化负向提示；进一步支持ID特定的多角色控制。

Result: 实验表明该模型显著提升长时高分辨视频的视觉清晰度、唇齿真实感与唇音同步精度、身份保持性及多模态指令一致性。

Conclusion: KlingAvatar 2.0 有效解决了长时高分辨虚拟人视频生成中的效率、时序连贯性与多模态对齐难题，为高质量Avatar视频生成提供了新范式。

Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.

</details>


### [169] [Face Identity Unlearning for Retrieval via Embedding Dispersion](https://arxiv.org/abs/2512.13317)
*Mikhail Zakharov*

Main category: cs.CV

TL;DR: 本文研究了面向检索系统的面部身份遗忘问题，提出了一种基于分散的遗忘方法，使选定身份在嵌入空间中不再形成紧凑聚类，从而无法被重新识别，同时保持其余身份的检索性能。


<details>
  <summary>Details</summary>
Motivation: 面部识别系统存在隐私风险，尤其是未经授权的身份追踪；现有机器遗忘方法在面部检索（特别是嵌入式模型）中的适用性尚未充分探索。

Method: 评估多种近似类别遗忘方法（如随机标签、梯度上升、边界遗忘等），并提出一种简单有效的基于嵌入分散的遗忘方法，通过在超球面上打散目标身份的嵌入来实现遗忘。

Result: 在VGGFace2和CelebA数据集上的大量实验表明，所提方法在实现更优遗忘效果的同时，显著保持了对剩余身份的检索性能。

Conclusion: 基于分散的遗忘方法能有效解决面部检索中的身份遗忘挑战，在保障隐私的同时兼顾模型实用性。

Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.

</details>


### [170] [Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning](https://arxiv.org/abs/2511.16619)
*Satyam Gaba*

Main category: cs.CV

TL;DR: 本文针对长尾分布下的2D目标检测问题，基于LVISv1数据集改进BAGS框架，并引入度量学习与k-NN分类，将mAP提升至24.5%，达新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实场景中目标类别呈长尾分布，导致检测模型偏向高频类、严重损害尾部稀有类性能。

Method: 在Faster R-CNN两阶段框架下，改进Balanced Group Softmax（BAGS）；提出度量学习优化特征嵌入（类间分离、类内紧凑），并用k-NN替代原有分类器进行推理。

Result: 在LVISv1上达到24.5% mAP，超越此前24.0%的SOTA；验证了所提方法对尾部类别检测性能的显著提升。

Conclusion: 结合改进的损失设计、度量学习与k-NN推理可有效缓解长尾目标检测中的类别不平衡问题，尤其增强稀有类识别能力。

Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.

</details>


### [171] [Automated User Identification from Facial Thermograms with Siamese Networks](https://arxiv.org/abs/2512.13361)
*Elizaveta Prozorova,Anton Konev,Vladimir Faerman*

Main category: cs.CV

TL;DR: 本文探讨了利用热成像技术进行基于面部热图的生物特征识别，比较了不同红外波段（NIR、SWIR、MWIR、LWIR），提出了热成像相机的关键参数要求，并采用Siamese神经网络实现自动化识别，在自建数据集上达到约80%准确率；同时探索了可见光与红外融合的混合系统以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 克服单一模态（如可见光）在光照、伪装等条件下的识别局限，探索热成像作为更可靠生物特征识别手段的可行性。

Method: 比较不同红外波段特性；提出热相机关键参数（分辨率、热灵敏度、≥30Hz帧率）；采用Siamese神经网络处理面部热图；构建并实验验证可见-红外融合系统。

Result: Siamese网络在自建热图数据集上识别准确率达约80%；验证了MWIR/LWIR更适合热成像识别；融合系统可缓解单模态缺陷。

Conclusion: 热成像（尤其MWIR/LWIR）结合深度学习方法具备用于高可靠性安防系统的潜力，混合光谱系统是重要发展方向。

Abstract: The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.

</details>


### [172] [Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"](https://arxiv.org/abs/2512.13376)
*Carla Monteiro,Valentina Corbetta,Regina Beets-Tan,Luís F. Teixeira,Wilson Silva*

Main category: cs.CV

TL;DR: 本文提出了一种基于DINO自注意力'key'特征的轻量级、通用型息肉分割框架，无需复杂或任务定制架构，在多中心数据集的域泛化与极端单域泛化任务中达到SOTA性能，尤其在数据稀缺场景下表现出强鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习息肉分割方法普遍存在泛化能力差、依赖复杂专用架构、在数据受限或跨域场景下性能下降等问题，亟需更鲁棒、通用且简洁的解决方案。

Method: 利用DINO预训练ViT中自注意力模块的'key'特征（而非传统最深层token），结合轻量卷积解码器进行息肉掩码预测；在多中心数据上采用Domain Generalization（DG）和Extreme Single Domain Generalization（ESDG）两种严格协议评估；并系统分析DINO架构演进对下游分割性能的影响。

Result: 在DG与ESDG协议下均取得SOTA性能，显著优于nnU-Net、UM-Net等主流模型；统计分析证实其在数据稀缺与挑战性场景下泛化能力大幅提升；验证了DINO key特征比深层token更具鲁棒性和判别性。

Conclusion: DINO的self-attention key特征是比深层token更优的通用表征源，所提轻量框架摆脱了对息肉专用架构的依赖，为医学图像分割提供了高泛化、低复杂度的新范式。

Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.

</details>


### [173] [Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs](https://arxiv.org/abs/2512.13392)
*Anran Qi,Changjian Li,Adrien Bousseau,Niloy J. Mitra*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的图像到视频生成方法，通过引入用户可编辑的代理动态图（PDG）分离运动控制与外观合成，实现对新暴露区域（disoccluded regions）的显式用户控制。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频方法难以在生成合理运动的同时，保证用户指定内容在新暴露区域中的可预测、可控制呈现。

Method: 提出轻量级、用户可编辑的Proxy Dynamic Graph（PDG）来确定性地驱动部件运动；利用冻结的扩散先验进行外观合成；通过PDG计算稠密光流引导扩散模型作为‘运动引导着色器’；结合可见性信息，在潜在空间中对用户编辑的遮挡区域外观进行合成与融合。

Result: 在家具、车辆、可变形物体等 articulated 场景上显著优于现有SOTA方法，支持无需微调的可控关节运动与最终帧中遮挡区域的用户外观编辑。

Conclusion: 该方法将生成式控制（松散姿态/结构）与确定性控制（最终帧遮挡区外观指定）结合，开辟了新型图像到视频工作流。

Abstract: We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/

</details>


### [174] [rNCA: Self-Repairing Segmentation Masks](https://arxiv.org/abs/2512.13397)
*Malte Silbernagel,Albert Alonso,Jens Petersen,Bulat Ibragimov,Marleen de Bruijne,Madeleine K. Wyburd*

Main category: cs.CV

TL;DR: 本文提出了一种基于神经细胞自动机（NCA）的通用分割掩码拓扑修复方法（rNCA），通过局部、迭代更新优化粗粒度预测结果，在多个医学图像分割任务中显著改善连通性与拓扑正确性。


<details>
  <summary>Details</summary>
Motivation: 通用分割模型常产生碎片化或不连通的掩码，而现有修复方法依赖手工规则或任务专用架构，缺乏通用性和可扩展性。

Method: 将神经细胞自动机（NCA）重新用作掩码细化机制：以粗糙预测掩码为初始状态，通过仅依赖局部邻域信息的迭代更新，在训练中学习目标形状的结构先验；训练目标为从不完美预测逼近真实掩码。

Result: 在视网膜血管分割中，Dice/clDice提升2–3%，β₀和β₁拓扑错误分别降低60%和20%；在心肌分割中，零样本下修复61.5%断裂案例，ASSD与HD分别下降19%和16%。

Conclusion: rNCA是一种轻量、即插即用、无需任务微调的通用拓扑修复模块，显著提升多种基线模型的拓扑鲁棒性，验证了NCA作为结构感知细化器的有效性与广泛适用性。

Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.

</details>


### [175] [End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery](https://arxiv.org/abs/2512.13402)
*Lorenzo Pettinari,Sidaty El Hadramy,Michael Wehrli,Philippe C. Cattin,Daniel Studer,Carol C. Hasler,Maria Licci*

Main category: cs.CV

TL;DR: 本文提出End2Reg，一种端到端深度学习框架，联合优化分割与配准，无需弱分割标签或人工干预，在脊柱手术导航中实现毫米级精度的无标记RGB-D配准。


<details>
  <summary>Details</summary>
Motivation: 现有脊柱手术导航系统存在侵入性、辐射强、干扰工作流等问题；而当前无标记RGB-D配准方法依赖弱分割标签，易导致误差传播。

Method: 提出End2Reg端到端深度学习框架，联合优化分割与配准任务，分割掩码完全由配准目标驱动，无需显式分割监督。

Result: 在体外和体内基准上达到SOTA性能：中位靶标配准误差降低32%至1.83mm，均方根误差降低45%至3.95mm；消融实验证明端到端优化显著提升精度。

Conclusion: End2Reg消除了对弱标签和人工步骤的依赖，推动实现全自动、无标记的术中导航。

Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.

</details>


### [176] [USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition](https://arxiv.org/abs/2512.13415)
*Ahmed Abul Hasanaath,Hamzah Luqman*

Main category: cs.CV

TL;DR: 本文提出了一种统一时空建模（USTM）框架，结合Swin Transformer与轻量级时序适配器（TAPE），在仅使用RGB视频输入的情况下实现了连续手语识别（CSLR）的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有CSLR方法难以捕捉细粒度的手部和面部线索，并且建模长程时序依赖能力不足。

Method: 提出USTM框架，采用Swin Transformer作为空间主干网络，并引入带位置编码的轻量级时序适配器（TAPE）以增强时空建模能力，仅依赖单流RGB输入。

Result: 在PHOENIX14、PHOENIX14T和CSL-Daily等基准数据集上达到RGB单模态及多模态方法的SOTA性能，并与多流方法具有竞争力。

Conclusion: USTM证明了仅用RGB视频即可高效建模复杂手语时空模式，为轻量、鲁棒的CSLR提供了新范式。

Abstract: Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM

</details>


### [177] [Learning to Generate Cross-Task Unexploitable Examples](https://arxiv.org/abs/2512.13416)
*Haoxuan Qu,Qiuchi Xiang,Yujun Cai,Yirui Wu,Majid Mirmehdi,Hossein Rahmani,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种元跨任务不可利用示例生成（MCT-UEG）框架，旨在生成在多种真实计算机视觉任务中均难以被模型学习的个人图像变体，以增强在线隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有不可利用示例生成方法在不同实际视觉任务中泛化能力差，难以实现广泛不可利用性，威胁个人图像在线隐私。

Method: 提出基于平坦极小值导向的元训练与测试机制，构建Meta Cross-Task Unexploitable Example Generation（MCT-UEG）框架，优化生成器对多任务的鲁棒不可利用性。

Result: 大量实验验证了该框架在提升跨任务不可利用性方面的有效性。

Conclusion: MCT-UEG框架显著增强了生成图像在多样化现实视觉任务中的不可学习性，提升了隐私保护的实用性与普适性。

Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.

</details>


### [178] [RecTok: Reconstruction Distillation along Rectified Flow](https://arxiv.org/abs/2512.13421)
*Qingyu Shi,Size Wu,Jinbin Bai,Kaidong Yu,Yujing Wang,Yunhai Tong,Xiangtai Li,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出RecTok，通过流语义蒸馏和重建-对齐蒸馏两种创新方法，提升高维视觉标记器在扩散模型中的性能，解决了高维与生成质量之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉标记器受限于低维潜在空间，难以兼顾重建保真度与语义表达能力；尽管利用视觉基础模型（VFMs）提升了语义丰富性，但高维标记器仍表现不佳。

Method: 提出RecTok：1）流语义蒸馏——将VFM的语义信息蒸馏至流匹配的前向流轨迹中；2）重建-对齐蒸馏——引入掩码特征重建损失以增强语义；关键在于使前向流空间而非潜在空间具备丰富语义，作为扩散Transformer的训练空间。

Result: RecTok在gFID-50K指标上达到SOTA（含/不含CFG），图像重建、生成质量和判别性能均更优；且随潜在维度增加持续提升性能。

Conclusion: RecTok成功突破高维视觉标记器的性能瓶颈，验证了前向流空间作为语义训练空间的有效性，为扩散模型的高效高质量生成提供了新范式。

Abstract: Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.

</details>


### [179] [MineTheGap: Automatic Mining of Biases in Text-to-Image Models](https://arxiv.org/abs/2512.13427)
*Noa Cohen,Nurit Spingarn-Eliezer,Inbar Huberman-Spiegelglas,Tomer Michaeli*

Main category: cs.CV

TL;DR: 本文提出MineTheGap方法，利用遗传算法自动挖掘能引发文本到图像（TTI）模型偏见输出的提示词，并通过新提出的偏见评分机制评估偏见严重程度。


<details>
  <summary>Details</summary>
Motivation: TTI模型在面对模糊文本提示时易产生社会性或多样性偏差，影响公平性与用户体验，亟需自动化工具识别和量化此类偏差。

Method: 提出MineTheGap方法：基于遗传算法迭代优化提示词池，并设计一种新型偏见评分——通过对比TTI生成图像的分布与大语言模型（LLM）生成的提示变体文本分布来衡量偏见强度。

Result: 该方法成功挖掘出能显著暴露TTI模型偏见的提示词，在已知偏见数据集上验证了偏见评分的有效性和排序合理性。

Conclusion: MineTheGap为TTI模型偏见检测提供了可扩展、可解释的自动化框架，有助于提升生成内容的公平性与多样性。

Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.

</details>


### [180] [A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification](https://arxiv.org/abs/2512.13428)
*Anika Islam,Tasfia Tahsin,Zaarin Anjum,Md. Bakhtiar Hasan,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级少样本学习框架，结合改进的MobileNetV2/V3特征提取器、特征融合与带注意力机制的Bi-LSTM分类器，实现在数据稀缺和资源受限环境下高精度植物叶片病害识别。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大量标注数据和高算力，在数据稀缺、资源受限的农业场景中难以适用。

Method: 采用领域适配的MobileNetV2/V3作为轻量特征提取器，引入特征融合提升表征鲁棒性；分类阶段使用带注意力机制的Bi-LSTM建模序列依赖并聚焦关键特征。

Result: 在PlantVillage上15-shot达99.72%（超越此前SOTA 96.0%），15-shot实验室准确率98.23±0.33%，接近Transductive LSTM SOTA（99.98%）；在真实田间数据Dhan Shomadhan上达69.28±1.49%；模型仅约40 MB，推理复杂度约1.12 GFLOPs。

Conclusion: 该框架兼顾高精度、轻量化与强泛化能力，为资源受限地区提供可部署、可扩展的植物病害智能诊断解决方案。

Abstract: Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.

</details>


### [181] [IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images](https://arxiv.org/abs/2512.13440)
*Thalyssa Baiocco-Rodrigues,Antoine Olivier,Reda Belbahri,Thomas Duboudin,Pierre-Antoine Bannier,Benjamin Adjadj,Katharina Von Loga,Nathan Noiry,Maxime Touzot,Hector Roux de Bezieux*

Main category: cs.CV

TL;DR: 本文提出了IMILIA框架，用于IBD组织H&E染色切片中炎症的端到端预测与可解释性分析，结合MIL模型与细胞/上皮分割模块，在多个队列中取得高AUC，并提供生物学一致的可解释结果。


<details>
  <summary>Details</summary>
Motivation: 随着IBD治疗目标转向组织学缓解，准确评估微观炎症对疾病活动度和疗效评估愈发关键，但现有方法缺乏可解释性与自动化定量能力。

Method: 提出IMILIA框架，包含基于多实例学习（MIL）的炎症预测模块，以及两个可解释性子模块：HistoPLUS（实现细胞检测、分割与分类）和EpiSeg（实现上皮分割），用于识别驱动预测的关键组织区域。

Result: 在发现队列中交叉验证ROC-AUC达0.83，在两个外部验证队列中分别达0.99和0.84；可解释模块揭示高分图块富含免疫细胞，低分图块以正常上皮细胞为主，且该模式跨数据集一致。

Conclusion: IMILIA是一种高效、可解释、可推广的计算病理学工具，支持IBD组织学炎症的自动化评估与生物机制探索，代码与部分模型已开源。

Abstract: As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.

</details>


### [182] [Test-Time Modification: Inverse Domain Transformation for Robust Perception](https://arxiv.org/abs/2512.13454)
*Arpit Jadon,Joshua Niemeijer,Yuki M. Asano*

Main category: cs.CV

TL;DR: 本文提出了一种在测试时利用扩散模型将目标域图像映射回源域分布的新方法，无需大规模合成数据，即可提升跨域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 生成式基础模型虽具丰富视觉知识，但用于训练数据增强时存在合成目标域变化慢、贵且不完整的问题。

Method: 在测试阶段使用扩散模型，依据源域描述将未知目标域图像反向映射至源域分布，从而适配已训练好的下游模型。

Result: 在BDD100K-Night、ImageNet-R和DarkZurich等真实到真实域泛化任务中，分别取得137%、68%和62%的相对性能提升。

Conclusion: 该方法无需修改下游模型、不依赖目标域先验，显著提升了分割、检测与分类任务在环境偏移下的域泛化能力。

Abstract: Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.

</details>


### [183] [PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence](https://arxiv.org/abs/2512.13465)
*Ruiyan Wang,Teng Hu,Kaihui Huang,Zihan Su,Ran Yi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出了PoseAnything，首个通用姿态引导视频生成框架，支持人类和非人类角色的任意骨骼输入，并引入了Part-aware Temporal Coherence Module和Subject and Camera Motion Decoupled CFG等新方法，显著提升了运动一致性和相机运动控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有姿态引导视频生成方法仅支持人类姿态输入，泛化能力差，难以应用于其他主体（如动物、机器人等）。

Method: 提出PoseAnything框架；设计Part-aware Temporal Coherence Module实现部件级时序一致性；提出Subject and Camera Motion Decoupled CFG以解耦主体与相机运动控制；构建XPose数据集（5万非人类姿态-视频对）及自动标注过滤流程。

Result: 在效果和泛化性上显著优于现有SOTA方法；支持任意骨骼结构输入；首次实现姿态引导视频中独立可控的相机运动。

Conclusion: PoseAnything是首个真正通用的姿态引导视频生成框架，通过部件感知时序建模、解耦式CFG引导及高质量非人类数据集，有效解决了跨主体泛化与运动一致性难题。

Abstract: Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.

</details>


### [184] [Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$](https://arxiv.org/abs/2512.13492)
*Jiangning Zhang,Junwei Zhu,Teng Hu,Yabiao Wang,Donghao Luo,Weijian Cao,Zhenye Gan,Xiaobin Hu,Zhucun Xue,Chengjie Wang*

Main category: cs.CV

TL;DR: 本文提出T3（Transform Trained Transformer）策略，通过多尺度权重共享窗口注意力和分层阻塞设计，在不改变预训练全注意力模型架构的前提下，显著降低4K视频生成的计算开销，实现10倍以上加速并提升VQA和VTC性能。


<details>
  <summary>Details</summary>
Motivation: 原生4K视频生成面临全注意力机制随时空分辨率提升而产生的二次计算爆炸问题，难以兼顾效率与质量。

Method: 提出T3策略，包括T3-Video模型：采用多尺度权重共享窗口注意力、分层阻塞及轴向保持的全注意力设计，对预训练模型进行前向逻辑优化。

Result: 在4K-VBench上，T3-Video相比现有方法VQA提升+4.29、VTC提升+0.08，并实现超10倍的原生4K视频生成加速。

Conclusion: T3是一种高效改造预训练Transformer的方法，可在极小额外计算与数据下，将全注意力模型适配至高分辨率视频生成任务。

Abstract: Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video

</details>


### [185] [Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation](https://arxiv.org/abs/2512.13495)
*Jiangning Zhang,Junwei Zhu,Zhenye Gan,Donghao Luo,Chuming Lin,Feifan Xu,Xu Peng,Jianlong Hu,Yuansen Liu,Yijia Hong,Weijian Cao,Han Feng,Xu Chen,Chencan Fu,Keke He,Xiaobin Hu,Chengjie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Soul的多模态驱动框架，用于高保真长期数字人动画生成，能从单张肖像图、文本提示和音频生成语义连贯的视频，并实现精准唇形同步、生动表情和强身份保持。


<details>
  <summary>Details</summary>
Motivation: 解决数字人动画中数据稀缺、长期生成一致性差、推理效率低以及多模态对齐不准确等问题。

Method: 基于Wan2.2-5B大模型，引入音频注入层、阈值感知码本替换、步/CFG蒸馏与轻量VAE；构建大规模数据集Soul-1M和评测基准Soul-Bench。

Result: 在视频质量、图文对齐、身份保持和唇动同步精度上显著超越现有开源及商用模型，推理速度提升11.4倍且质量损失可忽略。

Conclusion: Soul为虚拟主播、影视制作等真实场景提供了高效、高保真的数字人动画生成解决方案。

Abstract: We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/

</details>


### [186] [Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model](https://arxiv.org/abs/2512.13507)
*Siyan Chen,Yanfei Chen,Ying Chen,Zhuo Chen,Feng Cheng,Xuyan Chi,Jian Cong,Qinpeng Cui,Qide Dong,Junliang Fan,Jing Fang,Zetao Fang,Chengjian Feng,Han Feng,Mingyuan Gao,Yu Gao,Qiushan Guo,Boyang Hao,Qingkai Hao,Bibo He,Qian He,Tuyen Hoang,Ruoqing Hu,Xi Hu,Weilin Huang,Zhaoyang Huang,Zhongyi Huang,Siqi Jiang,Wei Jiang,Yunpu Jiang,Zhuo Jiang,Ashley Kim,Jianan Kong,Zhichao Lai,Shanshan Lao,Ai Li,Feiya Li,Gen Li,Huixia Li,JiaShi Li,Liang Li,Ming Li,Tao Li,Xian Li,Xiaojie Li,Xiaoyang Li,Xingxing Li,Yameng Li,Yifu Li,Yiying Li,Chao Liang,Ying Liang,Zhiqiang Liang,Wang Liao,Yalin Liao,Heng Lin,Kengyu Lin,Shanchuan Lin,Xi Lin,Zhijie Lin,Feng Ling,Fangfang Liu,Gaohong Liu,Jiawei Liu,Jie Liu,Shouda Liu,Shu Liu,Sichao Liu,Songwei Liu,Xin Liu,Xue Liu,Yibo Liu,Zikun Liu,Zuxi Liu,Junlin Lyu,Lecheng Lyu,Qian Lyu,Han Mu,Xiaonan Nie,Jingzhe Ning,Xitong Pan,Yanghua Peng,Lianke Qin,Xueqiong Qu,Yuxi Ren,Yuchen Shen,Guang Shi,Lei Shi,Yan Song,Yinglong Song,Fan Sun,Li Sun,Renfei Sun,Zeyu Sun,Wenjing Tang,Zirui Tao,Feng Wang,Furui Wang,Jinran Wang,Junkai Wang,Ke Wang,Kexin Wang,Qingyi Wang,Rui Wang,Sen Wang,Shuai Wang,Tingru Wang,Weichen Wang,Xin Wang,Yanhui Wang,Yue Wang,Yuping Wang,Yuxuan Wang,Ziyu Wang,Guoqiang Wei,Wanru Wei,Di Wu,Guohong Wu,Hanjie Wu,Jian Wu,Jie Wu,Ruolan Wu,Xinglong Wu,Yonghui Wu,Ruiqi Xia,Liang Xiang,Fei Xiao,XueFeng Xiao,Pan Xie,Shuangyi Xie,Shuang Xu,Jinlan Xue,Bangbang Yang,Ceyuan Yang,Jiaqi Yang,Runkai Yang,Tao Yang,Yang Yang,Yihang Yang,ZhiXian Yang,Ziyan Yang,Yifan Yao,Zilyu Ye,Bowen Yu,Chujie Yuan,Linxiao Yuan,Sichun Zeng,Weihong Zeng,Xuejiao Zeng,Yan Zeng,Chuntao Zhang,Heng Zhang,Jingjie Zhang,Kuo Zhang,Liang Zhang,Liying Zhang,Manlin Zhang,Ting Zhang,Weida Zhang,Xiaohe Zhang,Xinyan Zhang,Yan Zhang,Yuan Zhang,Zixiang Zhang,Fengxuan Zhao,Huating Zhao,Yang Zhao,Hao Zheng,Jianbin Zheng,Xiaozheng Zheng,Yangyang Zheng,Yijie Zheng,Jiexin Zhou,Kuan Zhu,Shenhan Zhu,Wenjia Zhu,Benhui Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.5 pro 是一个专为原生音视频联合生成设计的扩散Transformer模型，采用双分支架构与跨模态联合模块，结合多阶段数据流程和后训练优化（SFT+RLHF），实现高同步性、高质量生成，并支持多语言唇形同步、动态运镜与叙事连贯性，推理加速超10倍。


<details>
  <summary>Details</summary>
Motivation: 推动统一音视频生成，解决现有模型在音画同步、生成质量及实用部署方面的不足。

Method: 提出双分支Diffusion Transformer架构，集成跨模态联合模块与多阶段数据流水线；采用监督微调（SFT）和基于人类反馈的强化学习（RLHF）进行后训练优化；设计专用推理加速框架。

Result: 实现卓越的音视频同步性与生成质量，支持精准多语言/方言唇-sync、动态电影级相机控制、增强的叙事连贯性，推理速度提升超10倍。

Conclusion: Seedance 1.5 pro 是面向专业内容创作的高性能、高实用性音视频联合生成基础模型，已在火山引擎上线开放使用。

Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.

</details>


### [187] [Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains](https://arxiv.org/abs/2512.13534)
*Marianne Rakic,Siyu Gai,Etienne Chollet,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: 本文提出了Pancakes框架，用于在未见过的生物医学图像域中自动生成多种语义一致的多标签分割图，解决了现有模型仅支持单一协议或需手动提示的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动分割模型要么只支持单一训练协议，要么需要繁琐的手动提示来指定所需分割，无法灵活适应不同应用需求。

Method: Pancakes框架提出了一种新问题设定，能在新图像域中自动产生多个语义一致的多标签分割图，无需人工干预。

Result: 在七个预留数据集上的实验表明，该模型在生成多个语义连贯的全图像分割方面显著优于现有基础模型。

Conclusion: Pancakes实现了跨图像语义一致的多协议自动分割，拓展了基础模型在生物医学图像分析中的适用性与灵活性。

Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.

</details>


### [188] [3D Human-Human Interaction Anomaly Detection](https://arxiv.org/abs/2512.13560)
*Shun Maeda,Chunzhi Gu,Koichiro Kamide,Katsuya Hotta,Shangce Gao,Chao Zhang*

Main category: cs.CV

TL;DR: 本文提出了人类交互异常检测（H2IAD）新任务，并设计了IADNet模型，通过时序注意力共享模块（TASM）和基于距离的关系编码模块（DREM）建模双人协作行为的时空动态，结合归一化流进行异常评分，在多个基准上超越现有单人异常检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有以单人为中心的异常检测方法难以准确捕捉人类交互中复杂且不对称的动态关系，而现实中异常行为常源于人与人之间的协作互动，因此需要专门针对人类交互异常检测的新方法。

Method: 提出H2IAD任务；构建IADNet模型，包含：1）时序注意力共享模块（TASM），在两人间共享运动嵌入以同步协作运动相关性；2）基于距离的关系编码模块（DREM），建模人际空间构型以反映社交线索；3）采用归一化流进行异常打分。

Result: 在多人交互动作数据集上，IADNet在H2IAD任务中显著优于现有以单人为中心的异常检测基线方法。

Conclusion: H2IAD是一个重要且具挑战性的新方向，IADNet通过联合建模交互行为的时空特性，有效提升了人类交互异常检测性能，为后续研究提供了新思路与基准。

Abstract: Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.

</details>


### [189] [MMhops-R1: Multimodal Multi-hop Reasoning](https://arxiv.org/abs/2512.13573)
*Tao Zhang,Ziqi Zhang,Zongyang Ma,Yuxin Chen,Bing Li,Chunfeng Yuan,Guangting Wang,Fengyun Rao,Ying Shan,Weiming Hu*

Main category: cs.CV

TL;DR: 本文提出了MMhops基准和MMhops-R1模型，旨在推动多模态多跳推理能力的发展。MMhops是一个大规模、多任务的评估基准，包含Bridging和Comparison两类复杂任务；MMhops-R1是一种基于强化学习的多模态检索增强生成（mRAG）框架，支持动态推理路径规划与多层级信息融合。实验表明其显著优于基线，并具备良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）受限于单步推理能力，缺乏能有效评估和驱动多模态多跳推理能力的复杂基准。

Method: 提出MMhops基准（含Bridging/Comparison两类任务）和MMhops-R1框架：后者是基于强化学习的多模态检索增强生成（mRAG）方法，支持自主规划推理路径、生成定向查询及融合多级信息。

Result: MMhops-R1在MMhops基准上显著超越强基线；且在固定跳数推理任务中表现出良好泛化能力，验证了动态规划策略的有效性与鲁棒性。

Conclusion: 本文贡献了一个具有挑战性的新基准MMhops、一个高性能基线模型MMhops-R1，并将公开代码、数据与权重，以推动多模态多跳推理研究。

Abstract: The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.

</details>


### [190] [Lighting in Motion: Spatiotemporal HDR Lighting Estimation](https://arxiv.org/abs/2512.13597)
*Christophe Bolduc,Julien Philip,Li Ma,Mingming He,Paul Debevec,Jean-François Lalonde*

Main category: cs.CV

TL;DR: LiMo是一种基于扩散模型的时空光照估计方法，通过生成多曝光镜面与漫反射球体、引入新几何条件进行空间引导，并结合可微渲染合成HDRI，实现了高保真细节与精确照度估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时实现高频率细节预测与准确照度估计；深度信息不足以提供充分的空间条件。

Method: 提出LiMo框架：1）基于3D位置生成多曝光镜面/漫反射球体；2）在自建大规模室内外场景+时空光探针数据集上微调预训练扩散模型；3）引入新几何条件替代仅用深度进行空间引导；4）用可微渲染融合多曝光预测结果生成HDRI。

Result: 在时空光照估计任务中，LiMo在空间控制能力和预测精度两方面均达到当前最优（state-of-the-art）。

Conclusion: LiMo通过结合扩散先验、几何感知条件和可微渲染，有效解决了高保真光照建模与精确照度估计的双重挑战，为基于学习的光照估计提供了新范式。

Abstract: We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.

</details>


### [191] [DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides](https://arxiv.org/abs/2512.13600)
*Haoyue Zhang,Meera Chappidi,Erolcan Sayar,Helen Richards,Zhijun Chen,Lucas Liu,Roxanne Wadia,Peter A Humphrey,Fady Ghali,Alberto Contreras-Sanz,Peter Black,Jonathan Wright,Stephanie Harmon,Michael Haffner*

Main category: cs.CV

TL;DR: 本文提出了一种无需微调病理基础模型（PFM）的轻量级域自适应自监督适配器（DA-SSL），用于提升PFM在经尿道膀胱肿瘤切除术（TURBT）样本上的表现，成功应用于预测新辅助化疗（NAC）治疗响应。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型（PFMs）在TURBT等特定癌症类型上表现受限，因其预训练数据中缺乏该类样本（含组织碎片和电灼伪影），导致域偏移问题。

Method: 提出域自适应自监督适配器（DA-SSL），在不微调PFM的前提下，通过自监督学习对齐PFM特征到TURBT域，结合多实例学习（MIL）进行下游任务建模。

Result: 在多中心研究中，DA-SSL在五折交叉验证中AUC达0.77±0.04，外部测试集准确率0.84、灵敏度0.71、特异度0.91（多数投票）。

Conclusion: 轻量级自监督域适应方法可有效增强PFM-MIL流程在临床难点病理任务中的性能，无需修改预训练模型。

Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.

</details>


### [192] [LongVie 2: Multimodal Controllable Ultra-Long Video World Model](https://arxiv.org/abs/2512.13604)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Junhao Zhuang,Chengming Xu,Jianfeng Feng,Yu Qiao,Yanwei Fu,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出LongVie 2，一种三阶段训练的端到端自回归视频世界模型框架，旨在提升可控性、长时视觉质量与时间一致性，并配套发布长时视频生成基准LongVGenBench。


<details>
  <summary>Details</summary>
Motivation: 构建具备可控性、长时视觉质量与时间一致性的视频世界模型是实现通用时空智能的关键挑战。

Method: 提出LongVie 2框架，分三阶段训练：(1) 多模态引导（融合稠密/稀疏控制信号）提升可控性；(2) 输入帧退化感知训练，弥合训练-推理差距以维持长时画质；(3) 历史上下文引导，对齐相邻片段确保时间一致性。同时构建包含100个一分钟高清视频的LongVGenBench基准。

Result: LongVie 2在长程可控性、时间连贯性和视觉保真度上达到SOTA，支持长达5分钟的连续视频生成。

Conclusion: LongVie 2是迈向统一视频世界建模的重要一步，验证了渐进式增强三大核心属性的有效路径。

Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.

</details>


### [193] [DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis](https://arxiv.org/abs/2512.13608)
*Felix J. Dorfner,Manon A. Dorster,Ryan Connolly,Oscar Gentilhomme,Edward Gibbs,Steven Graham,Seth Wander,Thomas Schultz,Manisha Bahl,Dania Daye,Albert E. Kim,Christopher P. Bridge*

Main category: cs.CV

TL;DR: 本文提出了首个用于数字乳腺断层合成（DBT）的自监督基础模型DBT-DINO，基于超2500万张DBT切片进行DINOv2式预训练，并在乳腺密度分类、5年乳腺癌风险预测和病灶检测三项临床任务中进行了评估；结果表明其在前两项任务上显著优于通用模型，但在病灶检测上表现有限，提示局部检测任务仍需方法学改进。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学影像中展现出潜力，但目前尚无针对数字乳腺断层合成（DBT）这一三维成像模态的基础模型，而DBT广泛应用于乳腺癌筛查，亟需领域专用基础模型。

Method: 采用DINOv2自监督学习框架，在来自27,990名患者的487,975例DBT体积（共2500余万张2D切片）上进行域特异性预训练，构建DBT-DINO模型；并在三个下游任务（乳腺密度分类、5年乳腺癌风险预测、病灶检测）上评估性能。

Result: DBT-DINO在乳腺密度分类（准确率0.79）和5年乳腺癌风险预测（AUROC 0.78）上均优于通用DINOv2基线；但在整体病灶检测敏感度（0.62 vs. 0.67）上略逊于DINOv2，仅在癌性病灶检出率上略优（78.8% vs. 77.3%）。

Conclusion: DBT-DINO是首个面向DBT的基础模型，在分类与风险预测任务中验证了域特异性预训练的有效性，但在检测类任务中优势不显著，表明局部精细任务需进一步方法创新。

Abstract: Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.
  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.
  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.
  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\% compared to Dinov2's 77.3\%.
  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.

</details>


### [194] [Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models](https://arxiv.org/abs/2512.13609)
*Shweta Mahajan,Shreya Kadambi,Hoang Le,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: 本文提出了Do-Undo任务与基准，旨在评估视觉语言模型对现实世界动作所引发的物理场景变化的理解与生成能力，强调动作的可逆性与因果关系建模。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在物体级编辑上已有研究，但缺乏对物理上合理、可逆的动作效果建模能力，难以支持具身AI、机器人和物理感知生成等应用。

Method: 构建了基于真实视频的大规模可逆动作数据集，并设计了一种强制一致性约束的训练策略，以提升模型对动作的物理 grounding 能力。

Result: 实验表明当前主流模型在物理可逆性任务上表现较差，验证了Do-Undo任务的挑战性与必要性。

Conclusion: Do-Undo为多模态系统提供了直观、可量化的物理推理评测平台，推动视觉语言模型向具备真实世界因果理解能力发展。

Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

</details>


### [195] [SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning](https://arxiv.org/abs/2512.13635)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Chongyu Qu,Juming Xiong,Siqi Lu,Zhengyi Lu,Yanfan Zhu,Marilyn Lionts,Yuechen Yang,Yalin Zheng,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 本文提出SCR2-ST框架，利用单细胞测序先验知识指导空间转录组（ST）的高效采样与基因表达预测，通过强化学习驱动的主动采样（SCRL）和混合回归-检索预测网络（SCR2Net），在有限测序预算下显著提升采样效率与预测精度。


<details>
  <summary>Details</summary>
Motivation: 空间转录组数据获取成本高、传统固定网格采样冗余且信息量低；而单细胞测序数据丰富，可作为有效辅助信息源来缓解ST数据稀缺问题。

Method: 提出统一框架SCR2-ST，包含两部分：1）单细胞引导的强化学习主动采样（SCRL），融合单细胞基础模型嵌入与空间密度信息构建生物学合理奖励信号；2）混合回归-检索预测网络SCR2Net，结合回归建模与检索增强推理，并引入多数细胞类型过滤机制抑制噪声匹配，用检索表达谱作软标签辅助监督。

Result: 在三个公开ST数据集上验证，SCR2-ST在采样效率和预测准确率上均达SOTA水平，尤其在低预算场景下优势显著。

Conclusion: SCR2-ST成功 bridged 单细胞与空间转录组数据鸿沟，为低成本、高精度的空间基因表达解析提供了新范式。

Abstract: Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST

</details>


### [196] [Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All](https://arxiv.org/abs/2512.13639)
*Michal Nazarczuk,Thomas Tanay,Arthur Moreau,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: 本文介绍了一个用于新视角合成（Novel View Synthesis）的高质量动画电影数据集，包含RGB图像及深度、法线、实例分割、光流等多模态标注，并设计了稠密多视角、稀疏视角和单目视频三类基准场景。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在视觉真实性、动态复杂性和多模态标注方面存在不足，难以支撑前沿4D场景重建与新视角生成模型的训练与评估。

Method: 构建一个源自高保真动画电影的数据集，采集多视角RGB图像及配套深度、表面法线、物体分割、光流等丰富标注，并按稠密多视图、稀疏视图和单目视频三类设定组织为基准任务。

Result: 发布了一个具有高视觉质量、丰富几何与运动信息、多样化采集设置的新视角合成基准数据集。

Conclusion: 该数据集为推动新视角合成与3D视觉研究提供了独特且实用的资源，尤其适用于评估模型在不同数据稀疏度下的泛化能力。

Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.

</details>


### [197] [Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency](https://arxiv.org/abs/2512.13665)
*Wenhan Chen,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: 本文提出Grab-3D，一种基于3D几何时序一致性的AI生成视频检测框架，利用消失点建模几何不一致性，并通过几何感知Transformer实现跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成视频检测方法对3D几何模式挖掘不足，而真实与生成视频在几何一致性上存在本质差异。

Method: 提出Grab-3D框架：以消失点显式表征3D几何；构建静态场景AI视频数据集；设计含几何位置编码、时序-几何注意力机制和EMA几何分类头的几何感知Transformer。

Result: Grab-3D在多个指标上显著超越SOTA检测器，并展现出对未见生成器的强跨域泛化能力。

Conclusion: 3D几何时序一致性是检测AI生成视频的有效且鲁棒线索，Grab-3D验证了几何先验对深度伪造检测的重要价值。

Abstract: Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.

</details>


### [198] [AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection](https://arxiv.org/abs/2512.13671)
*Junwen Miao,Penghui Du,Yi Liu,Yu Wang,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出AgentIAD，一种基于工具驱动的多阶段视觉检测框架，通过Perceptive Zoomer和Comparative Retriever实现细粒度局部分析与正常样本比对，结合两阶段训练与双奖励机制，在MMAD数据集上达到97.62%的SOTA分类精度，并生成可解释的检测轨迹。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测面临正常样本稀缺、缺陷细微且局部化等挑战，单次推理的视觉语言模型常忽略小异常，缺乏与标准正常模式显式比对的机制。

Method: 提出AgentIAD框架，包含Perceptive Zoomer（PZ）用于局部细粒度分析、Comparative Retriever（CR）用于模糊时检索正常样本；基于MMAD构建感知与对比轨迹，采用监督微调+强化学习两阶段训练；设计感知奖励（分类准确率、空间对齐、类型正确性）与行为奖励（高效工具使用）联合优化。

Result: 在MMAD数据集上达到97.62%分类准确率，超越现有MLLM方法，并生成透明、可解释的多步检测轨迹。

Conclusion: AgentIAD通过具身式、分步式视觉检查范式，有效提升了工业异常检测的精度与可解释性，验证了工具驱动智能体在细粒度视觉理解任务中的潜力。

Abstract: Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.

</details>


### [199] [JoVA: Unified Multimodal Learning for Joint Video-Audio Generation](https://arxiv.org/abs/2512.13677)
*Xiaohu Huang,Hao Zhou,Qiangpeng Yang,Shilei Wen,Kai Han*

Main category: cs.CV

TL;DR: JoVA是一个统一的视频-音频联合生成框架，通过跨模态自注意力机制和基于面部关键点的嘴部区域损失，实现了高质量的唇音同步和语音生成，无需额外对齐模块，保持了模型简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法生成与唇动同步的人类语音，且统一视频-音频生成方法依赖显式融合或模态对齐模块，削弱了Transformer架构的简洁性。

Method: 采用每层Transformer中视频与音频token的联合自注意力机制实现直接高效的跨模态交互，并引入基于面部关键点检测的嘴部区域损失以增强唇部区域监督。

Result: 在多个基准测试中，JoVA在唇音同步精度、语音质量和整体视频-音频生成保真度上优于或媲美现有最先进方法。

Conclusion: JoVA是一种简洁而有效的高质量多模态生成框架。

Abstract: In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.

</details>


### [200] [Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678)
*Ziqi Ma,Hongqiao Chen,Yisong Yue,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 本文提出了Steer3D，一种前馈式方法，通过适配ControlNet思想，使图像到3D生成模型具备文本可编辑能力，支持在单次前向传播中依据文本指令编辑3D资产；采用自动化数据引擎与基于流匹配和直接偏好优化（DPO）的两阶段训练策略，在保真度、一致性与推理速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成的3D资产难以便捷编辑，限制了其在设计、AR/VR和机器人等实际场景中的应用；亟需一种高效、可控的文本驱动编辑机制。

Method: 提出Steer3D方法：1）借鉴ControlNet架构，构建文本引导的前馈控制模块；2）构建可扩展的自动数据引擎生成高质量配对数据；3）采用两阶段训练：第一阶段基于流匹配训练基础可控模型，第二阶段用Direct Preference Optimization（DPO）对齐文本意图与几何一致性。

Result: Steer3D在文本指令遵循度和原始3D资产一致性上优于竞争方法，推理速度快2.4×–28.5×；仅用10万样本即可为预训练图像到3D模型注入文本 steerability 能力。

Conclusion: 证明了无需微调主干模型、仅通过轻量前馈控制与少量数据即可为图像到3D生成模型赋予高效、高保真的文本编辑能力，为多模态可控生成提供了新范式。

Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/

</details>


### [201] [LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction](https://arxiv.org/abs/2512.13680)
*Tianye Ding,Yiming Xie,Yiqing Liang,Moitreya Chatterjee,Pedro Miraldo,Huaizu Jiang*

Main category: cs.CV

TL;DR: LASER是一种无需训练的框架，通过跨时间窗口对齐预测，将离线重建模型转换为流式视频处理系统，解决了现有方法内存复杂度高、需重新训练等问题，并在位姿估计和点云重建任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有前馈重建模型（如VGGT、π³）虽重建质量高，但因二次内存复杂度无法处理流式视频；而现有流式方法需大量重训练，且难以充分利用离线模型的强几何先验。

Method: 提出LASER框架，采用训练-free方式，利用层内尺度对齐（layer-wise scale alignment）替代简单的Sim(3)对齐：将深度预测按层分割，计算每层尺度因子，并在相邻窗口和时间戳间传播，以解决单目尺度模糊导致的层深度尺度不一致问题。

Result: LASER在相机位姿估计和点云重建质量上达到SOTA水平，同时在RTX A6000 GPU上实现14 FPS处理速度和6 GB峰值内存占用，支持千米级流式视频的实际部署。

Conclusion: LASER提供了一种高效、无需训练的离线到流式的转换范式，显著提升了重建模型在真实流式场景中的实用性与可扩展性。

Abstract: Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$

</details>


### [202] [I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners](https://arxiv.org/abs/2512.13683)
*Lu Ling,Yunhao Ge,Yichen Sheng,Aniket Bera*

Main category: cs.CV

TL;DR: 本文提出了一种通过重新编程预训练3D实例生成器来实现交互式3D场景生成泛化的新方法，摒弃依赖有限场景数据集的监督，转而利用模型自身的空间先验进行空间关系学习，在无需真实场景布局监督下仍能推理邻近性、支撑关系和对称性。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的交互式3D场景生成方法受限于训练场景数据集的覆盖范围，难以泛化到新布局和新物体组合，亟需提升空间理解与泛化能力。

Method: 将预训练3D实例生成器重新编程为场景级学习器，以模型为中心施加空间监督；摒弃标准坐标系，采用以视角为中心（view-centric）的场景空间建模，并构建端到端前馈式场景生成框架。

Result: 该方法在未见布局和随机组合场景上均展现出强泛化能力；实验证明仅凭几何线索即可自发涌现空间推理能力（如邻近、支撑、对称），且性能优于传统数据驱动方法。

Conclusion: 预训练3D实例生成器本身隐含空间学习与推理能力，可作为面向交互式3D场景理解与生成的基础模型。

Abstract: Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/

</details>


### [203] [Recurrent Video Masked Autoencoders](https://arxiv.org/abs/2512.13684)
*Daniel Zoran,Nikhil Parthasarathy,Yi Yang,Drew A Hudson,Joao Carreira,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出了循环视频掩码自编码器（RVM），一种基于Transformer的循环神经网络，用于高效学习视频时空表征，通过非对称掩码预测任务实现像素级重建，在动作识别、目标跟踪等视频任务及几何/密集空间理解任务中均达到SOTA水平，且在小模型下参数效率提升达30倍，计算成本线性增长。


<details>
  <summary>Details</summary>
Motivation: 现有视频表征学习方法（如VideoMAE、V-JEPA）在小模型规模下效率低、计算开销大，且难以兼顾视频级任务与密集空间理解；需探索更高效、通用、可扩展的视频编码架构。

Method: 提出RVM：采用Transformer-based RNN结构聚合帧间密集图像特征，结合仅需像素重建的非对称掩码预测任务，避免复杂对比或蒸馏设计。

Result: RVM在动作识别、点/物体跟踪等视频任务上媲美VideoMAE、V-JEPA；在几何与密集空间理解任务上优于DINOv2；小模型下参数效率达竞品30倍；支持长时序稳定特征传播且计算成本为线性；可视化显示其学习到丰富的场景语义、结构与运动表征。

Conclusion: RVM是一种高效、通用、可扩展的视频表征学习框架，兼具强泛化能力与高参数效率，突破了传统时空注意力架构在效率与长时序建模上的瓶颈。

Abstract: We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.

</details>


### [204] [Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687)
*Jingfeng Yao,Yuda Song,Yucong Zhou,Xinggang Wang*

Main category: cs.CV

TL;DR: 本文提出VTP框架，通过联合优化图像-文本对比、自监督和重建损失，解决视觉标记器预训练中潜在空间偏向低级信息的问题，显著提升生成性能和计算可扩展性。


<details>
  <summary>Details</summary>
Motivation: 标准重建式训练导致潜在空间偏向低级信息，造成‘预训练缩放问题’：更高像素精度不带来更好生成质量，算力投入难以转化为生成性能提升。

Method: 提出VTP统一视觉标记器预训练框架，联合优化图像-文本对比学习、自监督学习与重建损失。

Result: VTP在ImageNet上实现78.2%零样本准确率和0.36 rFID；生成收敛速度比先进蒸馏方法快4.1倍；仅增加预训练算力即可使下游FID提升65.8%，而传统自编码器在1/10算力时即饱和。

Conclusion: 高质量生成依赖于能简洁表征高层语义的潜在空间；VTP验证了理解能力驱动生成质量，并展现出优异的计算、参数与数据可扩展性。

Abstract: The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.

</details>


### [205] [LitePT: Lighter Yet Stronger Point Transformer](https://arxiv.org/abs/2512.13689)
*Yuanwen Yue,Damien Robert,Jianyuan Wang,Sunghwan Hong,Jan Dirk Wegner,Christian Rupprecht,Konrad Schindler*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D点云处理骨干网络LitePT，通过在浅层使用卷积提取低级几何特征、深层使用注意力捕获高级语义，并引入无训练的PointROPE位置编码来保持空间信息，在参数量、速度和内存上显著优于Point Transformer V3，同时性能相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 现代3D点云神经网络中卷积与注意力模块的组合方式尚不明确，需厘清各自在不同网络深度中的作用以实现高效设计。

Method: 分析卷积与注意力在不同分辨率和网络深度下的作用规律；据此构建分阶段架构（浅层卷积+深层注意力）；提出训练无关的3D位置编码PointROPE以弥补移除冗余卷积带来的空间信息损失。

Result: LitePT相比Point Transformer V3减少3.6倍参数、提速2倍、内存减半，且在多个任务和数据集上达到或超越其性能。

Conclusion: 卷积适合高分辨率早期特征提取，注意力更适合低分辨率深层语义建模；结合二者优势并辅以轻量位置编码，可构建更高效、更小、更快的3D点云模型。

Abstract: Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [206] [Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention](https://arxiv.org/abs/2512.11811)
*Fengyi Xu,Jun Ma,Waishan Qiu,Cui Guo*

Main category: cs.CL

TL;DR: 本文提出VPR-AttLLM框架，将大语言模型（LLM）的语义与地理空间知识嵌入视觉地点识别（VPR）流程，通过注意力引导的描述符增强提升洪水等危机场景下社交媒体街景图像的定位精度，无需重训练或新增数据，具备即插即用、跨源鲁棒与可解释性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体众包街景图像虽能实时反映城市洪涝等危机事件，但常缺乏可靠地理元数据；现有VPR模型在跨源、含视觉失真和域偏移的危机图像上性能显著下降。

Method: 提出模型无关的VPR-AttLLM框架，利用LLM理解城市语境并识别位置关键区域，通过注意力机制增强VPR描述符，抑制瞬态视觉噪声，不依赖模型重训练或额外标注数据。

Result: 在SF-XL（增补真实洪水图像）、合成洪水查询集、Mapillary及新构建HK-URBAN数据集上验证，集成CosPlace/EigenPlaces/SALAD后召回率相对提升1–3%，最难的真实洪水图像上达8%；同时建立LLM引导多模态融合的通用范式。

Conclusion: VPR-AttLLM实现了人类空间推理与现代VPR架构的结合，其即插即用设计、强跨源鲁棒性与可解释性，为城市危机影像的规模化监测与快速地理定位提供了实用新路径。

Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.

</details>


### [207] [Reinforcement Learning for Latent-Space Thinking in LLMs](https://arxiv.org/abs/2512.11816)
*Enes Özeren,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文探讨了在连续嵌入空间中进行潜空间推理（latent-space thinking）的潜力与挑战，指出当前监督微调方法（如Coconut）存在设计敏感性和固有局限性；为此，作者尝试引入强化学习（RL）方法（包括GRPO及新提出的Latent RL）直接优化潜空间推理步骤，但实验表明其在数学推理任务上仍逊于传统语言空间的思维链（CoT）模型。


<details>
  <summary>Details</summary>
Motivation: 现有潜空间推理方法（如Coconut）在复杂任务（如数学推理）中性能不足，且对设计选择高度敏感，亟需更鲁棒、通用的训练范式。

Method: 提出基于强化学习的潜空间推理训练方法，包括适配GRPO及新设计的Latent RL算法，用于直接优化连续嵌入空间中的推理步骤。

Result: RL训练的潜空间推理模型在数学推理任务上仍显著落后于传统语言空间的CoT模型。

Conclusion: 尽管RL为潜空间推理提供了新方向，但当前方法尚未克服其在复杂推理任务上的性能瓶颈；潜空间推理的有效性仍受限，需进一步研究。

Abstract: Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.

</details>


### [208] [KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document](https://arxiv.org/abs/2512.11849)
*Nimol Thuon,Jun Du*

Main category: cs.CL

TL;DR: 本文提出了首个面向高棉语商业文档理解的分层标注数据集KH-FUNSD，包含收据、发票和报价单，并设计了三级标注框架以支持布局分析与信息抽取，同时提供了基线实验结果。


<details>
  <summary>Details</summary>
Motivation: 高棉语作为低资源、非拉丁文字语言，在文档AI领域缺乏专用工具和数据集，尤其在关键的商业文档处理方面存在显著空白。

Method: 构建了首个公开、分层标注的高棉语表单文档数据集KH-FUNSD，采用三级标注框架：区域检测、FUNSD风格实体及关系标注、细粒度语义角色分类；并基于该数据集对多个主流模型进行了基准测试。

Result: 发布了KH-FUNSD数据集，提供了高棉语商业文档理解任务的首批基线性能结果，并分析了非拉丁、低资源文字带来的独特挑战。

Conclusion: KH-FUNSD填补了低资源非拉丁文字文档AI研究的关键空白，其分层标注设计为多粒度文档理解提供了可扩展范式。

Abstract: Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.

</details>


### [209] [Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models](https://arxiv.org/abs/2512.11998)
*Glenn Zhang,Treasure Mayowa,Jason Fan,Yicheng Fu,Aaron Sandoval,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文提出Direct Confidence Alignment (DCA)方法，利用直接偏好优化（DPO）对齐大语言模型的口头化置信度与内部置信度，提升模型透明性与可靠性，并引入三类新校准误差指标评估对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的内部置信度（基于token概率）与其口头化置信度不一致，导致不同校准方法结果不可靠，亟需提升二者对齐以增强模型可信性。

Method: 提出Direct Confidence Alignment (DCA)，采用Direct Preference Optimization（DPO）将模型口头化置信度对齐至其内部置信度（而非真实准确率），并在多个开源LLM和数据集上进行验证；同时设计三种基于校准误差的新评估指标。

Result: DCA在部分模型架构上显著改善了口头与内部置信度的对齐程度，降低了置信表达不一致性；但在其他模型上效果有限，表明需发展更模型感知的校准方法。

Conclusion: 对齐口头与内部置信度是提升LLM可解释性与可信性的关键路径，但当前DCA方法存在模型依赖性，未来需构建更适配不同架构的校准策略。

Abstract: Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.

</details>


### [210] [Hold Onto That Thought: Assessing KV Cache Compression On Reasoning](https://arxiv.org/abs/2512.12008)
*Minghui Liu,Aadi Palnitkar,Tahseen Rabbani,Hyunwoo Jae,Kyle Rui Sang,Dixi Yao,Shayan Shabihi,Fuheng Zhao,Tian Li,Ce Zhang,Furong Huang,Kunpeng Zhang*

Main category: cs.CL

TL;DR: 本文评估了多种KV缓存压缩策略在长推理任务（如GSM8K、MATH500）上的表现，发现H2O和改进版SnapKV在推理模型中效果最优，表明重击者追踪对推理轨迹至关重要；同时揭示了缓存大小与推理成本间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法多针对prefill阶段，缺乏在长解码（尤其是多步推理）任务上的系统评估；而复杂短提示（如数学推理）常生成数千token的思考序列，亟需适配解码阶段的高效压缩策略。

Method: 在多个长推理基准（如GSM8K、MATH500）上系统评测主流KV缓存压缩策略（如H2O、SnapKV等），提出并验证SnapKV的解码启用变体，并分析不同预算下缓存大小与推理长度、成本的关系。

Result: H2O和解码增强版SnapKV在推理模型上显著优于其他策略；低缓存预算下部分策略反而生成更长推理链；性能高度依赖数据集类型，无单一最优策略适用于所有场景。

Conclusion: 面向长推理的KV缓存压缩需重视重击者动态追踪与解码阶段适配；缓存大小与推理深度/成本存在非平凡权衡，应依任务类型选择策略。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.

</details>


### [211] [Benchmarking Contextual Understanding for In-Car Conversational Systems](https://arxiv.org/abs/2512.12042)
*Philipp Habicht,Lev Sorokin,Abdullah Saydemir,Ken E. Friedl,Andrea Stocco*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）与多种提示技术（如思维链、自一致性、多智能体）的自动评估方法，用于衡量车载对话式问答（ConvQA）系统在上下文理解与约束感知推荐任务中的响应准确性，尤其在餐厅推荐场景中验证了其有效性与成本效益。


<details>
  <summary>Details</summary>
Motivation: 传统人工评估车载ConvQA系统费时费力且难以规模化；亟需一种可扩展、准确、低成本的自动化评估方法，以衡量其对用户话语的上下文理解能力与约束敏感的推荐质量。

Method: 利用13个不同规模与类型（推理型/非推理型）的LLM（来自OpenAI、DeepSeek、Mistral AI、Meta），结合输入-输出提示、思维链（CoT）、自一致性（Self-Consistency）和多智能体提示等技术，对合成的用户话语与含故障的系统响应进行连贯性评估；以餐厅推荐为案例开展实证研究。

Result: 多智能体提示显著提升小规模非推理模型性能；推理模型整体优于非推理模型，其中单智能体+自一致性提示效果最佳；DeepSeek-R1达F1=0.99（单次请求成本0.002美元）；DeepSeek-V3在效果与成本-时间效率间取得最佳平衡。

Conclusion: LLM驱动的自动化评估是一种高效、准确、可扩展的替代方案，可用于基准测试ConvQA系统的上下文理解能力，有望取代传统人工评估。

Abstract: In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.

</details>


### [212] [VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs](https://arxiv.org/abs/2512.12072)
*Avinash Amballa,Yashas Malur Saidutta,Chi-Heng Lin,Vivek Kulkarni,Srinivas Chappidi*

Main category: cs.CL

TL;DR: 本文提出Voyager方法，通过行列式点过程（DPP）优化数据集多样性，无需训练、适用于闭源模型且可扩展，在实验中显著提升数据多样性（1.5–3倍）。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成的合成数据缺乏多样性，需一种能有效提升数据多样性的新方法。

Method: 提出基于行列式点过程（DPP）的迭代优化方法Voyager，直接最大化数学定义的多样性度量；无需微调或访问模型参数，支持黑盒闭源模型。

Result: 在多项实验中，Voyager相较主流基线方法将数据集多样性提升1.5至3倍。

Conclusion: Voyager是一种原理清晰、高效、通用且可扩展的多样性增强方法，为合成数据生成提供了新范式。

Abstract: Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.

</details>


### [213] [BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding](https://arxiv.org/abs/2512.12087)
*Jiayi Yuan,Cameron Shinn,Kai Xu,Jingze Cui,George Klimiashvili,Guangxuan Xiao,Perkz Zheng,Bo Li,Yuxin Zhou,Zhouhai Ye,Weijie You,Tian Zheng,Dominic Brown,Pengbo Wang,Richard Cai,Julien Demouth,John D. Owens,Xia Hu,Song Han,Timmy Liu,Huizi Mao*

Main category: cs.CL

TL;DR: 本文提出BLASST，一种无需预计算、动态剪枝注意力矩阵的稀疏注意力方法，通过固定阈值与在线softmax信息跳过冗余计算，在保持高精度的同时显著加速长上下文推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对长上下文推理需求增长，但标准注意力机制带来严重计算与内存瓶颈。

Method: BLASST是一种即插即用的稀疏注意力方法，利用固定阈值和在线softmax已有信息动态识别并跳过可忽略的注意力分数，省去softmax计算、Value块加载及后续矩阵乘；适配FlashAttention，支持prefill/decode及MHA/GQA/MQA/MLA等所有注意力变体；提出自动化校准程序，发现最优阈值与上下文长度呈简单反比关系；进一步探索稀疏感知训练以提升鲁棒性。

Result: 在现代GPU上实现prefill阶段74.7%稀疏度下1.62倍加速、decode阶段73.2%稀疏度下1.48倍加速，同时保持高精度；稀疏感知训练进一步拓展精度-稀疏度权衡边界。

Conclusion: BLASST提供了一种高效、通用、低开销的长上下文推理加速方案，兼具实用性与可扩展性，并为稀疏注意力的训练-推理协同优化开辟新路径。

Abstract: The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in sparse attention method that dynamically prunes the attention matrix without any pre-computation or proxy scores. Our method uses a fixed threshold and existing information from online softmax to identify negligible attention scores, skipping softmax computation, Value block loading, and the subsequent matrix multiplication. This fits seamlessly into existing FlashAttention kernel designs with negligible latency overhead. The approach is applicable to both prefill and decode stages across all attention variants (MHA, GQA, MQA, and MLA), providing a unified solution for accelerating long-context inference. We develop an automated calibration procedure that reveals a simple inverse relationship between optimal threshold and context length, enabling robust deployment across diverse scenarios. Maintaining high accuracy, we demonstrate a 1.62x speedup for prefill at 74.7% sparsity and a 1.48x speedup for decode at 73.2% sparsity on modern GPUs. Furthermore, we explore sparsity-aware training as a natural extension, showing that models can be trained to be inherently more robust to sparse attention patterns, pushing the accuracy-sparsity frontier even further.

</details>


### [214] [Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings](https://arxiv.org/abs/2512.12167)
*Yoav Gelberg,Koshi Eguchi,Takuya Akiba,Edoardo Cetin*

Main category: cs.CL

TL;DR: 本文提出DroPE方法，通过在预训练后丢弃位置编码并进行简短再校准，实现无需长上下文微调的零样本上下文扩展。


<details>
  <summary>Details</summary>
Motivation: 位置编码在预训练中提供重要归纳偏置促进收敛，但过度依赖显式位置信息会阻碍测试时对未见长度序列的泛化；且位置编码并非语言建模的固有要求，预训练后可安全移除。

Method: DroPE（Dropping the Positional Embeddings）：预训练后移除位置编码，并进行短时再校准。

Result: 实证表明DroPE能无缝实现零样本上下文扩展，无需长上下文微调，快速适配预训练语言模型，且不损害原始上下文能力；在不同模型和数据集上均显著优于现有专用架构和RoPE缩放方法。

Conclusion: DroPE打破了需昂贵长上下文微调才能扩展语言模型上下文长度的关键瓶颈，为高效上下文扩展提供了新范式。

Abstract: So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.

</details>


### [215] [Diffusion Language Model Inference with Monte Carlo Tree Search](https://arxiv.org/abs/2512.12168)
*Zheng Huang,Kiran Ramnath,Yueyan Chen,Aosong Feng,Sangmin Woo,Balasubramaniam Srinivasan,Zhichao Xu,Kang Zhou,Shuai Wang,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: 本文提出MEDAL框架，利用蒙特卡洛树搜索（MCTS）优化扩散语言模型（DLMs）的推理过程，在初始化阶段探索高质量的去噪路径，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有DLM推理方法依赖启发式或额外训练，难以有效解决并行去噪中位置解掩码与token选择的组合搜索问题。

Method: 提出MEDAL框架，将蒙特卡洛树搜索（MCTS）引入DLM推理初始化阶段，限制搜索空间于高置信度动作，并优先选择能提升剩余掩码位置模型置信度的token。

Result: 在多个基准上，MEDAL相比现有推理策略最高提升22.0%性能。

Conclusion: MEDAL为扩散语言模型提供了首个基于搜索的、原理清晰的推理新范式。

Abstract: Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.

</details>


### [216] [Semantic Distance Measurement based on Multi-Kernel Gaussian Processes](https://arxiv.org/abs/2512.12238)
*Yinzhu Cheng,Haihua Xie,Yaqing Wang,Miao He,Mingming Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于多核高斯过程（MK-GP）的语义距离度量方法，通过自动学习Matérn与多项式组合核的参数，提升其对特定数据分布和任务的适应性，并在大语言模型的上下文学习（ICL）设定下验证了其在细粒度情感分类中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统语义距离方法通常是固定的，难以适应特定数据分布和任务需求。

Method: 提出基于多核高斯过程（MK-GP）的语义距离度量方法，将文本对应的潜在语义函数建模为高斯过程，协方差函数采用Matérn与多项式核的组合，并在监督下自动学习核参数。

Result: 在大语言模型的上下文学习（ICL）设定下，该方法在细粒度情感分类任务中展现出有效性。

Conclusion: MK-GP语义距离方法具有更强的数据自适应能力，优于传统固定式语义距离方法。

Abstract: Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.

</details>


### [217] [Adversarially Probing Cross-Family Sound Symbolism in 27 Languages](https://arxiv.org/abs/2512.12245)
*Anika Sharma,Tianyi Niu,Emma Wrenn,Shashank Srivastava*

Main category: cs.CL

TL;DR: 本文首次通过大规模跨语言计算分析，验证了大小语义领域中语音象征现象的跨语言普遍性，并构建了涵盖27种语言、810个形容词的语音-语义数据集。


<details>
  <summary>Details</summary>
Motivation: 语音象征现象（如Bouba-Kiki效应）长期依赖轶事实验，缺乏大规模实证检验；本文旨在通过计算方法在跨语言尺度上系统检验大小语义中的语音象征性。

Method: 构建包含27种语言、810个形容词的音位转录与母语者音频验证数据集；采用基于音段袋特征的可解释分类器预测大小语义；并设计对抗性‘擦除器’抑制语言身份信息，以检验跨语系的普遍性。

Result: 语音形式能显著高于随机水平预测大小语义，且元音与辅音均有贡献；对抗擦除后语言识别率低于随机水平，而大小预测仍显著高于随机，表明存在跨语系的语音象征偏差。

Conclusion: 大小语义领域的语音象征具有跨语言、甚至跨语系的普遍性，支持语音-意义映射中存在非任意性认知基础；研究为未来大规模象似性研究提供了公开数据、代码与诊断工具。

Abstract: The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.

</details>


### [218] [Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics](https://arxiv.org/abs/2512.12264)
*Abhay Srivastava,Sam Jung,Spencer Mateega*

Main category: cs.CL

TL;DR: 本文提出了MARKET-BENCH基准，用于评估大语言模型在量化交易入门任务中的能力，特别是从自然语言策略描述生成可执行回测代码，并衡量其结构可靠性与数值准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在金融领域缺乏系统性、可验证的量化交易能力评估基准，难以衡量其在真实金融任务（如价格、库存、风险管理）中的推理稳健性。

Method: 构建MARKET-BENCH基准，包含三种典型策略（定时交易、配对交易、Delta对冲），要求模型生成可运行且数值准确的回测代码；采用多轮pass@k指标分别评估结构可靠性（代码能否运行）和数值准确性（P&L等指标误差）。

Result: 12个SOTA模型表现差异显著：GPT-5.1 Codex-Max在前两个策略上pass@1达100%，Qwen3 Max pass@3达100%但P&L路径偶有偏差；Gemini 3 Pro与Claude 4.5 Sonnet在简单任务上兼具高可靠性与低误差；整体上模型对基础策略支持较好，但在复杂金融推理上仍不足。

Conclusion: 当前LLM已能初步支撑基础量化交易基础设施搭建，但在价格动态、头寸管理与风险建模等核心金融推理任务上仍存在显著局限；作者开源了基准与公共排行榜。

Abstract: We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.

</details>


### [219] [F5-TTS-RO: Extending F5-TTS to Romanian TTS via Lightweight Input Adaptation](https://arxiv.org/abs/2512.12297)
*Radu-Gabriel Chivereanu,Tiberiu Boros*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级输入级适配器，为F5-TTS模型添加罗马尼亚语支持，同时保持其原有语音克隆及英/中语言能力；通过冻结原权重、附加可训练子网络并利用ConvNeXt建模字符嵌入依赖关系，实现软音素转换；人工评估表明其支持语音克隆与一定程度的罗-英语码切换，但残留英语口音。


<details>
  <summary>Details</summary>
Motivation: 为F5-TTS模型扩展罗马尼亚语支持，同时不损害其已有的语音克隆、英语和中文合成能力。

Method: 设计轻量级输入级适配器，在冻结原始模型权重的前提下，向文本编码器的文本嵌入矩阵附加一个基于ConvNeXt的可训练子网络，将罗马尼亚语字符级输入映射为连续声学表示（即‘软’音素层）。

Result: 人工评估（20名听众）显示：模型能保持语音克隆能力，支持一定程度的罗马尼亚语-英语同句码切换，但生成语音仍带残留英语口音。

Conclusion: 该轻量级适配器方案有效实现了F5-TTS对罗马尼亚语的支持，兼顾多语言兼容性与模型轻量化，代码与音频样例已开源。

Abstract: This work introduces a lightweight input-level adapter for the F5-TTS model that enables Romanian Language support. To preserve the existing capabilities of the model (voice cloning, English and Chinese support), we keep the original weights frozen, append a sub-network to the model and train it as an extension for the textual embedding matrix of the text encoder. For simplicity, we rely on ConvNeXt module implemented in F5-TTS to also model the co-dependencies between the new character-level embeddings. The module serves as a ``soft`` letter-to-sound layer, converting Romanian text into a continuous representation that the F5-TTS model uses to produce naturally sounding Romanian utterances. We evaluate the model with a pool of 20 human listeners across three tasks: (a) audio similarity between reference and generated speech, (b) pronunciation and naturalness and (c) Romanian-English code-switching. The results indicate that our approach maintains voice cloning capabilities and enables, to a certain extent, code-switching within the same utterance; however, residual English accent characteristics remain. We open-source our code and provide example audio samples at https://github.com/racai-ro/Ro-F5TTS.

</details>


### [220] [SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema](https://arxiv.org/abs/2512.12337)
*Yushen Fang,Jianjun Li,Mingqian Ding,Chang Liu,Xinchi Zou,Wenqi Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为自校正迭代优化（SCIR）的新信息抽取范式，结合多任务双语自校正（MBSC）数据集，显著降低了训练成本并提升了模型对大语言模型偏好的对齐能力，在命名实体识别、关系抽取和事件抽取任务上均取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的信息抽取系统微调范式存在训练成本高和难以对齐大语言模型偏好两大问题。

Method: 提出自校正迭代优化（SCIR）框架，包含双路径自校正模块和反馈驱动优化机制；构建含10万以上样本的多任务中英双语自校正（MBSC）数据集，通过间接蒸馏GPT-4能力提升结果检测模型。

Result: 在命名实体识别、关系抽取和事件抽取三个任务上，SCIR平均span-based Micro-F1提升5.27%，训练成本降低87%。

Conclusion: SCIR框架实现了即插即用兼容性，提升了信息抽取系统的灵活性、准确性和效率，为轻量高效的信息抽取范式提供了新路径。

Abstract: Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.

</details>


### [221] [Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors](https://arxiv.org/abs/2512.12444)
*Veronica Mangiaterra,Hamad Al-Azary,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 本文评估了GPT系列大语言模型在生成隐喻熟悉度、可理解性和意象性评分方面的有效性与可靠性，发现其评分与人类评分呈中等到强相关，且能有效预测行为和脑电反应，但对高感知负荷或具身性隐喻表现较弱。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在科学研究中广泛应用，其可信度问题日益关键；尤其在心理语言学中，虽已有LLMs用于单字评分的自动增强，但其对复杂隐喻类刺激的评分能力尚属空白。

Method: 对687个来自意大利隐喻语料库及三项英文研究的隐喻项，使用三个GPT模型生成熟悉度、可理解性与意象性评分；通过与人类评分的相关性分析、对行为反应时和EEG振幅的预测力检验，以及跨会话稳定性评估进行系统验证。

Result: GPT评分与人类评分呈中等到强正相关（熟悉度：英/意均中-强；意象性：英语中等、意大利中-强；可理解性：英语最强）；较大模型表现更优；GPT评分能显著预测反应时和EEG振幅，效果接近人类评分；跨会话稳定性高；但在高感知负荷、惯例性及多模态隐喻上人机一致性下降。

Conclusion: GPT（尤其是较大模型）可有效、可靠地替代或补充人类对隐喻属性的评分，但需谨慎对待其在隐喻惯例性与具身性维度上的局限性。

Abstract: As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.

</details>


### [222] [Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects](https://arxiv.org/abs/2512.12818)
*Chris Latimer,Nicoló Boschi,Andrew Neeser,Chris Bartholomew,Gaurav Srivastava,Xuan Wang,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: 本文提出了Hindsight记忆架构，将代理记忆视为推理的结构化、首要基础，通过四种逻辑网络组织记忆，并支持保留、回忆和反思三种核心操作，显著提升了长时程对话记忆任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的代理记忆系统将记忆作为外部层处理，导致证据与推理界限模糊、长期信息组织困难以及缺乏对推理过程的解释支持。

Method: Hindsight架构将记忆组织为四种逻辑网络：世界事实、代理经验、实体摘要和演化信念，并支持保留、回忆和反思三种核心操作，构建了一个时间感知、实体感知的记忆层和一个可追溯更新的反思层。

Result: 在LongMemEval和LoCoMo等长时程对话记忆基准测试中，Hindsight使用20B开源模型将整体准确率从39%提升至83.6%，并超越了全上下文GPT-4o；进一步扩展主干模型后，在LongMemEval上达到91.4%，在LoCoMo上达到89.61%，显著优于现有记忆架构。

Conclusion: Hindsight通过结构化、可追溯的记忆设计，有效解决了现有代理记忆系统的关键缺陷，显著提升了多会话和开放领域问答性能。

Abstract: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.

</details>


### [223] [Large language models have learned to use language](https://arxiv.org/abs/2512.12447)
*Gary Lupyan*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型在语言学习方面的进展，并指出这可能标志着我们已进入后图灵测试时代，需要重新评估语言知识的评价方式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型已经展现出强大的语言使用能力，这为语言科学带来了新的突破机会。

Method: 本文通过理论分析和反思，探讨了如何重新评估语言知识以及应对后图灵测试时代的挑战。

Result: 提出需要放弃一些关于语言知识评估的长期固有观念，并正视后图灵测试时代的现实。

Conclusion: 大型语言模型的语言能力为语言科学带来新机遇，但同时也要求我们更新评估范式并适应新时代。

Abstract: Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.

</details>


### [224] [The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting](https://arxiv.org/abs/2512.12488)
*James Luther,Donald Brown*

Main category: cs.CL

TL;DR: 本研究利用VSM13国际调查和霍夫斯泰德文化维度，评估了8个主流大语言模型（如GPT-4、Claude Opus 4、DeepSeek-V3等）的文化对齐程度，并通过‘文化提示’技术测试其向中国、法国、印度、伊朗、日本和美国等国家文化适配的能力；结果表明多数模型默认偏向美国文化，文化提示可提升多数模型对目标文化的对齐，但对中国和日本文化的对齐效果较差。


<details>
  <summary>Details</summary>
Motivation: 随着生成式大语言模型（LLMs）在人机交互中日益普及，其文化对齐性成为关键研究课题；不同文化背景用户对AI响应的期待存在显著差异，亟需系统评估并提升模型的文化适应能力。

Method: 基于VSM13国际调查与霍夫斯泰德六维文化模型，量化分析8个主流LLMs的文化倾向；采用‘文化提示’（即在系统提示中嵌入目标国家文化特征）方法，测试模型在六个国家（中国、法国、印度、伊朗、日本、美国）文化语境下的响应偏移程度。

Result: 8个模型中大多数默认呈现美国文化偏好；文化提示使其中7个模型的文化输出更接近目标国家；但所有模型（包括中国公司DeepSeek开发的DeepSeek-V3/V3.1）均难以有效对齐中国和日本文化。

Conclusion: 当前主流LLMs虽具备一定文化可塑性，但对高语境、集体主义强或文化独特性高的国家（如中、日）适配能力有限，提示工程尚不足以充分弥合深层文化鸿沟，需从训练数据、评估基准和建模机制层面进行系统性改进。

Abstract: Culture is the bedrock of human interaction; it dictates how we perceive and respond to everyday interactions. As the field of human-computer interaction grows via the rise of generative Large Language Models (LLMs), the cultural alignment of these models become an important field of study. This work, using the VSM13 International Survey and Hofstede's cultural dimensions, identifies the cultural alignment of popular LLMs (DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, and Mistral Large). We then use cultural prompting, or using system prompts to shift the cultural alignment of a model to a desired country, to test the adaptability of these models to other cultures, namely China, France, India, Iran, Japan, and the United States. We find that the majority of the eight LLMs tested favor the United States when the culture is not specified, with varying results when prompted for other cultures. When using cultural prompting, seven of the eight models shifted closer to the expected culture. We find that models had trouble aligning with Japan and China, despite two of the models tested originating with the Chinese company DeepSeek.

</details>


### [225] [NagaNLP: Bootstrapping NLP for Low-Resource Nagamese Creole with Human-in-the-Loop Synthetic Data](https://arxiv.org/abs/2512.12537)
*Agniva Maiti,Manya Pandey,Murari Mandal*

Main category: cs.CL

TL;DR: 本文提出了NagaNLP——一个面向低资源语言Nagamese的开源工具包，采用LLM生成+人工验证的合成数据方法构建高质量语料，并训练出性能领先的POS、NER和对话模型，显著超越零样本基线。


<details>
  <summary>Details</summary>
Motivation: 全球大多数语言（尤其是克里奥尔语如Nagamese）在NLP中严重缺乏资源，导致其难以进入数字技术体系。

Method: 提出一种专家引导的LLM（Gemini）生成候选语料、再由母语者人工校验与标注的多阶段合成-混合数据构建流程，并基于该数据训练XLM-RoBERTa和Llama-3.2-3B模型。

Result: 构建了1万对会话数据集及高质量标注语料；XLM-RoBERTa在POS任务达93.81%准确率（F1-Macro 0.90），NER达0.75 F1-Macro；Llama-3.2-3B微调模型NagaLLaMA在对话任务Perplexity为3.85，远优于few-shot基线（96.76）。

Conclusion: NagaNLP不仅为Nagamese提供了首个系统性NLP资源，还为其他低资源语言提供了一套可复现、可扩展的数据驱动范式。

Abstract: The vast majority of the world's languages, particularly creoles like Nagamese, remain severely under-resourced in Natural Language Processing (NLP), creating a significant barrier to their representation in digital technology. This paper introduces NagaNLP, a comprehensive open-source toolkit for Nagamese, bootstrapped through a novel methodology that relies on LLM-driven but human-validated synthetic data generation. We detail a multi-stage pipeline where an expert-guided LLM (Gemini) generates a candidate corpus, which is then refined and annotated by native speakers. This synthetic-hybrid approach yielded a 10K pair conversational dataset and a high-quality annotated corpus for foundational tasks. To assess the effectiveness of our methodology, we trained both discriminative and generative models. Our fine-tuned XLM-RoBERTa-base model establishes a new benchmark for Nagamese, achieving a 93.81\% accuracy (0.90 F1-Macro) on Part-of-Speech tagging and a 0.75 F1-Macro on Named Entity Recognition, massively outperforming strong zero-shot baselines. Furthermore, we fine-tuned a Llama-3.2-3B Instruct model, named NagaLLaMA, which demonstrates superior performance on conversational tasks, achieving a Perplexity of 3.85, an order of magnitude improvement over its few-shot counterpart (96.76). We release the NagaNLP toolkit, including all datasets, models, and code, providing a foundational resource for a previously underserved language and a reproducible framework for reducing data scarcity in other low-resource contexts.

</details>


### [226] [HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks](https://arxiv.org/abs/2512.12544)
*Yiming Zeng,Jinghan Cao,Zexin Li,Wanhao Yu,Zhankai Ye,Dawei Xiang,Ting Hua,Xin Liu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

TL;DR: 本文提出HyperEdit方法，通过超网络动态适配和差异感知正则化，提升指令驱动文本编辑的准确性和保真度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在指令驱动的文本编辑任务中表现不佳，难以准确对齐用户意图且容易过度编辑未改动区域。

Method: 提出HyperEdit方法：1）基于超网络的动态适配机制，为每条指令生成特定参数；2）差异感知正则化，仅对修改片段施加监督，防止过编辑并确保最小精确改动。

Result: HyperEdit在修改区域的BLEU指标上相较SOTA基线获得9%–30%的相对提升，且仅使用3B参数。

Conclusion: HyperEdit有效解决了指令编辑中的意图对齐与内容保真难题，为轻量高效编辑模型提供了新范式。

Abstract: Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.

</details>


### [227] [Coupled Variational Reinforcement Learning for Language Model General Reasoning](https://arxiv.org/abs/2512.12576)
*Xueru Wen,Jie Lou,Yanjiang Liu,Hongyu Lin,Ben He,Xianpei Han,Le Sun,Yaojie Lu,Debing Zhang*

Main category: cs.CL

TL;DR: 本文提出CoVRL方法，通过耦合变分推断与强化学习，利用混合采样策略联合建模先验与后验分布，提升语言模型在无验证器强化学习中的推理效率与思维-答案一致性。


<details>
  <summary>Details</summary>
Motivation: 现有无验证器的强化学习方法仅基于问题采样推理路径，导致探索效率低且推理路径与最终答案不一致。

Method: 提出CoVRL框架，将变分推断与强化学习结合，通过构建并优化融合先验与后验分布的复合分布，采用混合采样策略实现推理路径与答案的耦合生成。

Result: 在数学与通用推理基准上，CoVRL相比基线模型提升12.4%，较当前最优无验证器RL方法再提升2.3%。

Conclusion: CoVRL为增强语言模型通用推理能力提供了原理性框架，有效解决了无验证器设定下推理路径与答案脱节的问题。

Abstract: While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.

</details>


### [228] [Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery](https://arxiv.org/abs/2512.12608)
*Hong Su*

Main category: cs.CL

TL;DR: 本文提出了一种受人类学习启发的框架，结合显式符号记忆（Obvious Record）和最大熵方法发现（Maximum-Entropy Method Discovery），以提升大语言模型在稀有、低资源或未见场景下的泛化与方法学习能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在罕见、低资源或未见场景（如小众硬件问题、异常IoT行为）中表现差，因其依赖隐式参数记忆，缺乏显式的方法获取、回忆与优化能力。

Method: 提出双机制框架：1）Obvious Record——将因果/问答对以符号形式显式存入长期记忆；2）Maximum-Entropy Method Discovery——基于语义差异性优先保留高熵（即多样化、低频）方法。

Result: 在60个语义多样的问答对基准上验证，该熵引导方法在未见问题覆盖度和内部方法多样性上均显著优于随机基线。

Conclusion: 该框架使LLM更接近人类式方法导向学习，提升了对稀有案例的泛化能力与可解释方法库的构建能力。

Abstract: Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.

</details>


### [229] [StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning](https://arxiv.org/abs/2512.12613)
*Yucan Guo,Saiping Guan,Miao Su,Zeya Zhao,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出StruProKGR框架，通过距离引导的路径收集和概率化路径聚合，在稀疏知识图谱推理中实现高效、可解释且性能优越的推理。


<details>
  <summary>Details</summary>
Motivation: 稀疏知识图谱中知识不完整，传统基于路径的方法依赖计算密集型随机游走，且未利用图的结构信息，导致效率低、路径质量差。

Method: 提出StruProKGR框架：1）采用距离引导的路径收集机制降低计算开销并提升路径相关性；2）通过概率化路径聚合融合结构信息，优先选择相互增强的路径。

Result: 在五个稀疏KG推理基准上，StruProKGR在效果和效率上均超越现有基于路径的方法。

Conclusion: StruProKGR为稀疏知识图谱推理提供了高效、有效且可解释的解决方案。

Abstract: Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.

</details>


### [230] [Large-Language Memorization During the Classification of United States Supreme Court Cases](https://arxiv.org/abs/2512.13654)
*John E. Ortega,Dhruv D. Joshi,Matt P. Borkowski*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）在美最高法院（SCOTUS）判决文本分类任务中的记忆准确性，发现基于提示（prompt-based）并具备记忆能力的模型（如DeepSeek）在15类和279类分类任务中均比传统BERT模型性能提升约2个百分点。


<details>
  <summary>Details</summary>
Motivation: SCOTUS判决文本具有长句、复杂法律术语、非标准结构和领域专有词汇等特点，是检验LLM记忆准确性的理想分类任务；同时，LLM在分类任务中易产生‘幻觉’输出，亟需深入理解其响应机制与记忆特性。

Method: 采用最新LLM微调与检索方法（如参数高效微调、自动建模等），在两个SCOTUS传统分类任务（15类主题与279类主题）上进行实验，并对比prompt-based带记忆模型（如DeepSeek）与传统BERT模型的性能。

Result: prompt-based且具记忆能力的模型（如DeepSeek）在两类SCOTUS分类任务中均优于此前非提示式模型，准确率提升约2个百分点。

Conclusion: 提示工程结合记忆机制可显著增强LLM在高难度法律文本分类任务中的鲁棒性与准确性，为提升LLM领域适应性与可靠性提供了新路径。

Abstract: Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.

</details>


### [231] [Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives](https://arxiv.org/abs/2512.12620)
*Aheli Poddar,Saptarshi Sahoo,Sujata Ghosh*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在三段论推理中的逻辑与自然语言能力，使用14个模型进行符号推理与自然语言理解的实证分析，发现部分模型展现出完美符号推理能力，引发其是否正趋近形式化推理机制而非模拟人类推理的思考。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否具备基本的三段论推理能力，并从逻辑与自然语言双重视角理解其推理本质及未来发展方向。

Method: 采用14个大型语言模型，分别评估其在符号化三段论推理和自然语言语境下的三段论理解能力。

Result: 三段论推理能力并非所有LLM的普遍涌现特性；部分模型在符号推理任务中达到完美表现。

Conclusion: 某些LLM展现出接近形式化推理系统的特性，而非复现人类推理的细微特征，提示其推理机制可能正朝更形式化方向演进。

Abstract: We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.

</details>


### [232] [Which Pieces Does Unigram Tokenization Really Need?](https://arxiv.org/abs/2512.12641)
*Sander Land,Yuval Pinter*

Main category: cs.CL

TL;DR: 本文提供了一个清晰的Unigram分词算法实现指南，并提出了一种简化算法，在略微增加训练损失的前提下提升了压缩效果。


<details>
  <summary>Details</summary>
Motivation: Unigram分词算法虽具理论优雅性，但其实现复杂，导致其实际应用受限（主要仅见于SentencePiece），本文旨在弥合理论与实践之间的鸿沟。

Method: 提供Unigram算法的清晰实现指南和参数选择建议，并提出一种更简化的替代算法。

Result: 实现了更易用的Unigram分词方案，并获得在可接受训练损失增加下的更好压缩性能。

Conclusion: 简化后的Unigram算法在实用性与性能间取得了良好平衡，有助于推动其更广泛采用。

Abstract: The Unigram tokenization algorithm offers a probabilistic alternative to the greedy heuristics of Byte-Pair Encoding. Despite its theoretical elegance, its implementation in practice is complex, limiting its adoption to the SentencePiece package and adapters thereof. We bridge this gap between theory and practice by providing a clear guide to implementation and parameter choices. We also identify a simpler algorithm that accepts slightly higher training loss in exchange for improved compression.

</details>


### [233] [LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases](https://arxiv.org/abs/2512.12643)
*Yida Cai,Ranjuexiao Hu,Huiyuan Xie,Chenyang Li,Yun Liu,Yuxiao Ye,Zhenghao Liu,Weixing Shen,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出了一套面向中文民事案件的法律关系抽取框架与基准数据集LexRel，揭示了现有大语言模型在该任务上的不足，并验证了法律关系信息对下游法律AI任务的增益效果。


<details>
  <summary>Details</summary>
Motivation: 中文民事案件中的法律关系在法律人工智能领域尚未得到充分研究，主要由于缺乏全面的法律关系模式（schema）支持。

Method: 首先构建了一个包含分层分类体系和参数定义的法律关系模式；基于此模式定义法律关系抽取任务，并构建专家标注的基准数据集LexRel；随后利用LexRel评估主流大语言模型，并分析引入法律关系信息对下游任务的影响。

Result: 当前大语言模型在准确识别中文民事法律关系方面存在显著局限；引入法律关系信息可稳定提升其他法律AI下游任务的性能。

Conclusion: 本研究为法律AI中法律关系建模提供了基础性支撑，推动了法律知识结构化与司法智能化的发展。

Abstract: Legal relations form a highly consequential analytical framework of civil law system, serving as a crucial foundation for resolving disputes and realizing values of the rule of law in judicial practice. However, legal relations in Chinese civil cases remain underexplored in the field of legal artificial intelligence (legal AI), largely due to the absence of comprehensive schemas. In this work, we firstly introduce a comprehensive schema, which contains a hierarchical taxonomy and definitions of arguments, for AI systems to capture legal relations in Chinese civil cases. Based on this schema, we then formulate legal relation extraction task and present LexRel, an expert-annotated benchmark for legal relation extraction in Chinese civil law. We use LexRel to evaluate state-of-the-art large language models (LLMs) on legal relation extractions, showing that current LLMs exhibit significant limitations in accurately identifying civil legal relations. Furthermore, we demonstrate that incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.

</details>


### [234] [Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks](https://arxiv.org/abs/2512.12654)
*Hassan Mujtaba,Hamza Naveed,Hanzlah Munir*

Main category: cs.CL

TL;DR: 本文提出一种基于图的框架，将乌尔都语小说建模为人物交互网络，以仅从叙事结构推断作者风格，并在52部乌尔都语小说上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统作者分析多关注词汇和风格线索，而对更高层的叙事结构（尤其在乌尔都语等低资源语言中）探索不足。

Method: 将小说建模为人物交互网络图（节点为人物，边表示叙事邻近共现），并比较多种图表示方法：全局结构特征、节点级语义摘要、无监督图嵌入及有监督图神经网络。

Result: 在52部乌尔都语小说（7位作者）数据集上，学习到的图表示显著优于手工特征和无监督基线，在作者感知评估协议下最高达0.857准确率。

Conclusion: 叙事结构本身蕴含可识别的作者风格信息，图神经网络能有效捕捉该信息，为低资源语言作者分析提供新路径。

Abstract: Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.

</details>


### [235] [Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches](https://arxiv.org/abs/2512.12677)
*Amirhossein Yousefiramandi,Ciaran Cooney*

Main category: cs.CL

TL;DR: 本文探讨了在资源受限条件下对解码器-only大语言模型（LLM）进行高效微调以完成文本分类任务的两种策略：基于嵌入的分类头微调和指令微调，并结合4-bit量化与LoRA实现单GPU上8B参数模型的微调；实验表明嵌入方法在F1分数上显著优于指令微调，且媲美甚至超越专用微调模型（如BERT）。


<details>
  <summary>Details</summary>
Motivation: 在计算资源受限（如单GPU）场景下，如何高效微调大型语言模型（尤其是decoder-only架构）用于下游文本分类任务，仍具挑战性。现有方法（如全参数微调或指令微调）往往成本高、效果不稳定，亟需兼顾效率与性能的轻量级方案。

Method: 提出两种微调范式：(1) 在预训练因果LM末尾添加分类头，利用最终token的embedding作为序列表征进行监督微调；(2) 将分类任务重构为prompt→response形式进行指令微调。二者均采用4-bit量化 + LoRA实现参数高效训练，支持单GPU微调至8B参数模型。

Result: 在私有单标签数据集和公开WIPO-Alpha极端多标签专利数据集上的实验表明：嵌入式方法F1显著优于指令微调方法，且性能媲美甚至超越微调后的领域专用模型（如BERT），验证了该方法在低资源下的有效性与竞争力。

Conclusion: 直接利用因果LLM内部表征（如最后token embedding）并辅以量化+LoRA等高效微调技术，是资源受限下文本分类任务的实用且高性能方案；相较指令微调，嵌入式微调更适配分类任务本质，应作为首选基线。

Abstract: We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.

</details>


### [236] [CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning](https://arxiv.org/abs/2512.12716)
*Xuanzhang Liu,Jianglun Feng,Zhuoran Zhuang,Junzhe Zhao,Maofei Que,Jieting Li,Dianlei Wang,Hao Tong,Ye Chen,Pan Li*

Main category: cs.CL

TL;DR: 本文提出CoDA，一种上下文解耦的分层智能体框架，通过将高层规划与低层执行解耦，并共享同一LLM主干，在长上下文任务中显著缓解'上下文爆炸'问题，提升多步推理性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）智能体在强化学习训练下虽能处理复杂多步任务，但常因'上下文爆炸'——即长文本输出累积超出模型上下文窗口——导致推理失败。

Method: 提出CoDA框架：采用单个共享LLM主干，分饰两个角色——高层Planner（在简洁策略上下文中进行任务分解）和低层Executor（在临时、隔离工作区中执行工具调用）；通过PECO（Planner-Executor Co-Optimization）方法进行端到端强化学习，基于轨迹级奖励联合优化二者，并依据上下文动态更新策略。

Result: 在多跳问答基准上显著超越现有最优基线；在长上下文场景中表现出强鲁棒性，性能稳定，而其他基线严重退化。

Conclusion: 分层解耦设计（规划/执行分离+上下文隔离）可有效缓解LLM智能体的上下文过载问题，是提升复杂任务推理能力与稳定性的重要路径。

Abstract: Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by "Context Explosion", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.

</details>


### [237] [NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents](https://arxiv.org/abs/2512.12730)
*Jingzhe Ding,Shengda Long,Changxin Pu,Huan Zhou,Hongwan Gao,Xiang Gao,Chao He,Yue Hou,Fei Hu,Zhaojian Li,Weiran Shi,Zaiyuan Wang,Daoguang Zan,Chenchen Zhang,Xiaoxu Zhang,Qizhi Chen,Xianfu Cheng,Bo Deng,Qingshui Gu,Kai Hua,Juntao Lin,Pai Liu,Mingchen Li,Xuanguang Pan,Zifan Peng,Yujia Qin,Yong Shan,Zhewen Tan,Weihao Xie,Zihan Wang,Yishuo Yuan,Jiayu Zhang,Enduo Zhao,Yunfei Zhao,He Zhu,Chenyang Zou,Ming Ding,Jianpeng Jiao,Jiaheng Liu,Minghao Liu,Qian Liu,Chongyao Tao,Jian Yang,Tong Yang,Zhaoxiang Zhang,Xinjie Chen,Wenhao Huang,Ge Zhang*

Main category: cs.CL

TL;DR: 本文提出NL2Repo Bench基准测试，用于评估编码代理在长周期内从零开始构建完整软件仓库的能力，发现现有模型在此任务上表现不佳，揭示了长周期推理中的关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法严格评估编码代理构建完整软件系统所需的长周期能力，大多数评估局限于局部代码生成、引导式补全或短期修复任务。

Method: 设计了NL2Repo Bench基准测试，要求代理仅根据自然语言需求文档和空工作区，自主完成架构设计、依赖管理、多模块实现，并生成可安装的Python库。

Result: 实验表明，即使是当前最先进的开源和闭源模型，在该任务上的平均测试通过率低于40%，极少能正确完成整个仓库；分析揭示了提前终止、全局连贯性丧失、跨文件依赖脆弱、长步数规划不足等根本性失败模式。

Conclusion: NL2Repo Bench为衡量持续代理能力提供了严谨、可验证的测试平台，并指出长周期推理是下一代自主编码代理发展的核心瓶颈。

Abstract: Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.

</details>


### [238] [Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining](https://arxiv.org/abs/2512.12770)
*Thales Sales Almeida,Rodrigo Nogueira,Hélio Pedrini*

Main category: cs.CL

TL;DR: 本文研究了持续预训练在语言模型适应特定语言或领域时的有效性，提出Curió 7B（基于LLaMA-2、用1000亿葡萄牙语token训练）和更小但精选的Curió-Edu 7B（仅用100亿教育/STEM类葡萄牙语token训练）。结果表明，高质量数据选择比单纯增加数据量更能提升模型性能，即使计算资源和数据量大幅减少，Curió-Edu 7B仍超越Curió 7B。


<details>
  <summary>Details</summary>
Motivation: 探究在持续预训练中，数据量与数据质量对语言模型（尤其是低资源语言如葡萄牙语）适应效果的相对重要性。

Method: 基于LLaMA-2构建两个7B参数模型：Curió 7B（全量ClassiCC-PT语料，1000亿token）与Curió-Edu 7B（仅教育/STEM子集，100亿token），并在相同评估任务上对比性能。

Result: Curió-Edu 7B在多项评估中优于Curió 7B，尽管其训练数据仅为后者的10%、计算量为20%。

Conclusion: 数据质量（如领域相关性、教育适配性）在持续预训练中起决定性作用，甚至可超越数据规模的影响；精选小规模高质量语料是高效适配语言模型的有效策略。

Abstract: Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curió 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curió-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu

</details>


### [239] [Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions](https://arxiv.org/abs/2512.12775)
*Pedro Henrique Luz de Araujo,Michael A. Hedderich,Ali Modarressi,Hinrich Schuetze,Benjamin Roth*

Main category: cs.CL

TL;DR: 本文提出了一种新的长对话评估协议，用于测试人格化大语言模型在超过100轮对话中的角色一致性、指令遵循与安全性表现，并发现角色保真度随对话延长而下降，尤其在目标导向对话中存在角色保真与指令遵循的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有对人格化大语言模型的评估多局限于短程单轮对话，无法反映真实应用场景下的长期交互效果，亟需更鲁棒的长上下文评估方法。

Method: 构建长 persona 对话（>100轮）与评估数据集结合的对话条件基准，系统评测7个主流开源与闭源大语言模型在 persona 忠实度、指令遵循和安全性三方面的长程表现。

Result: 发现 persona 忠实度随对话轮次增加显著下降，尤其在目标导向任务中；存在 persona 忠实度与指令遵循能力的权衡关系；随着对话进行，persona 模型输出逐渐趋近于非 persona 基线模型。

Conclusion: 人格化 LLM 在长程交互中表现脆弱，当前 persona 赋予方法难以维持长期一致性；本文提出的评估协议可系统识别此类失效，为后续改进提供基础。

Abstract: Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.

</details>


### [240] [State over Tokens: Characterizing the Role of Reasoning Tokens](https://arxiv.org/abs/2512.12777)
*Mosh Levy,Zohar Elyoseph,Shauli Ravfogel,Yoav Goldberg*

Main category: cs.CL

TL;DR: 本文提出State over Tokens (SoT)框架，将大语言模型中的推理token重新定义为外部化的计算状态而非人类可读的推理过程，强调应将其解码为状态而非文本以真正理解模型推理机制。


<details>
  <summary>Details</summary>
Motivation: 推理token看似模拟人类思维过程，但实证表明其并非模型真实推理过程的忠实解释，存在表象与功能之间的鸿沟。

Method: 提出SoT概念框架，将推理token视为无状态生成循环中唯一持久的信息载体——即外部化计算状态，而非语言性叙事。

Result: 该框架解释了为何推理token能支持正确推理却无法作为文本被忠实解读，并揭示了此前被忽视的相关研究问题。

Conclusion: 要真正理解LLM的推理过程，研究应超越将推理token当作文本阅读，转而专注于将其解码为计算状态。

Abstract: Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.

</details>


### [241] [Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA](https://arxiv.org/abs/2512.12812)
*Hanyu Cai,Binqi Shen,Lier Jin,Lan Hu,Xiaojing Fan*

Main category: cs.CL

TL;DR: 本研究系统评估了提示语调（友好、中性、粗鲁）对三种主流大语言模型（GPT-4o mini、Gemini 2.0 Flash、Llama 4 Scout）在MMMLU基准上性能的影响，发现语调效应具有模型依赖性和领域特异性，主要在人文学科任务中显著，且随任务聚合而减弱。


<details>
  <summary>Details</summary>
Motivation: 尽管提示工程对LLM性能至关重要，但语用因素（如语调与礼貌程度）的影响尚未被充分探索，尤其跨不同模型家族的比较研究缺乏。

Method: 构建系统性评估框架，在MMMLU基准的六个STEM与人文学科任务上，对比Very Friendly、Neutral和Very Rude三类提示下三款最新开源/商用LLM的准确率，并进行统计显著性检验。

Result: 语调敏感性因模型和领域而异：粗鲁提示普遍降低GPT与Llama在人文学科任务中的准确率，而Gemini相对不敏感；但在跨任务聚合后，语调效应显著减弱甚至不显著。

Conclusion: 交互语调仅在特定解释性场景中影响性能，现代LLM在典型混合领域应用中对语调变化整体稳健，为实际提示设计与模型选型提供实用依据。

Abstract: Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.

</details>


### [242] [What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation](https://arxiv.org/abs/2512.12839)
*Dingyi Yang,Qin Jin*

Main category: cs.CL

TL;DR: 本文提出了首个用于长篇小说（>100K tokens）自动评估的大规模基准LongStoryEval，并系统分析读者最关注的评估维度；比较了聚合式、增量更新式和摘要式三类评估方法，发现前两者更优；并基于摘要式框架构建了8B参数模型NovelCritique，在人类评估一致性上超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估方法难以适用于超长文本（如整本小说），且缺乏对读者真实关注点的实证分析与标准化评估基准。

Method: 构建包含600本新出版长篇小说的LongStoryEval基准；通过分析读者评论归纳出8个顶层评估维度；对比聚合式、增量更新式和摘要式三种评估策略；基于摘要式框架训练8B参数模型NovelCritique。

Result: 聚合式和摘要式评估效果更优，前者细节评估强，后者效率高；NovelCritique在人类评估一致性上优于GPT-4o；公开了数据集与代码。

Conclusion: 长篇故事自动评估需兼顾可解释性与效率，摘要式框架是可行路径；LongStoryEval为该领域提供了重要基准与实证基础。

Abstract: In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.

</details>


### [243] [Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM](https://arxiv.org/abs/2512.12868)
*Furong Jia,Yuan Pu,Finn Guo,Monica Agrawal*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在临床诊断多选题（如MedQA）上的表现是否源于真正的概率推理，并提出了一种轻量级的基于频率的概率排序器（FBPR），其性能可媲美对应预训练语料上的LLM；二者正确答案重叠率仅略高于随机，表明机制互补，凸显显式概率基线的价值。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在临床诊断多选基准（如MedQA）上表现优异，但尚不清楚其性能是否真正源于概率推理能力，因此需探究其内在机制并建立有意义的基线。

Method: 提出频率驱动的朴素贝叶斯式方法——Frequency-Based Probabilistic Ranker（FBPR），利用LLM预训练语料中的概念-诊断共现统计（经平滑处理）对选项打分。

Result: FBPR在使用OLMo和Llama各自预训练语料构建时，性能与对应LLM相当；且FBPR与LLM答对题目重合度仅略高于随机水平，表明二者能力互补。

Conclusion: LLM的高分未必源于深层概率推理，而传统低复杂度、基于频率的专家系统思想仍能解释大量基准表现；显式概率基线不仅提供合理参考，还具备与LLM融合的潜力。

Abstract: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.

</details>


### [244] [Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping](https://arxiv.org/abs/2512.12950)
*Lingyi Meng,Maolin Liu,Hao Wang,Yilan Cheng,Qi Yang,Idlkaid Mohanmmed*

Main category: cs.CL

TL;DR: 本文提出了一种人机协同的多智能体框架，用于构建中日英三语法律术语库，强调法律专家全程参与，AI处理重复性任务，人类负责监督与判断，实验表明该方法提升了术语映射的精度、一致性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 中文与日文存在大量同形异义法律术语，现有资源和工具匮乏，亟需可靠、可扩展的跨语言法律术语映射方法。

Method: 构建基于多智能体的人机协同框架，AI代理负责OCR、文本切分、语义对齐和初步术语抽取，法律专家全程参与审查、监督与质量保障；以35部中国核心法律及其英日译本构成的三语平行语料为实验基础。

Result: 该人机协同多智能体工作流显著提升了多语法律术语映射的精度与一致性，并展现出优于传统纯人工方法的可扩展性。

Conclusion: 将人类专家深度融入多智能体系统是提升法律领域多语术语构建质量与效率的关键路径，为高专业性、低容错率领域的AI应用提供了可行范式。

Abstract: Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.

</details>


### [245] [QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management](https://arxiv.org/abs/2512.12967)
*Weizhou Shen,Ziyi Yang,Chenliang Li,Zhiyuan Lu,Miao Peng,Huashan Sun,Yingcheng Shi,Shengyi Liao,Shaopeng Lai,Bo Zhang,Dayiheng Liu,Fei Huang,Jingren Zhou,Ming Yan*

Main category: cs.CL

TL;DR: QwenLong-L1.5 是一个通过系统性后训练创新实现卓越长上下文推理能力的大模型，核心包括长上下文数据合成、稳定化长上下文强化学习和内存增强架构，在超长文本（达4M tokens）推理任务上显著超越基线并媲美GPT-5和Gemini-2.5-Pro。


<details>
  <summary>Details</summary>
Motivation: 解决现有大模型在超长上下文（尤其百万级token）中难以进行多跳、全局证据推理的问题，突破单纯检索式任务的局限，并克服长上下文强化学习中的不稳定性与内存瓶颈。

Method: 提出三项关键技术：(1) 长上下文数据合成流程，将文档解构为原子事实并程序化生成可验证的多跳推理问题；(2) 稳定化长上下文强化学习，含任务平衡采样、任务特定优势估计和自适应熵控策略优化（AEPO）；(3) 内存增强架构，采用多阶段融合RL训练，支持单次推理与迭代内存处理协同。

Result: 在长上下文推理基准上平均超越基线9.90分，媲美GPT-5和Gemini-2.5-Pro；在1M~4M tokens超长任务中内存智能体框架提升9.48分；通用能力（如科学推理、记忆工具使用、长对话）也同步增强。

Conclusion: 系统性后训练方法（数据、算法、架构协同优化）是解锁大模型真正长程推理能力的关键路径，QwenLong-L1.5验证了该范式的有效性与泛化性。

Abstract: We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.

</details>


### [246] [Authors Should Annotate](https://arxiv.org/abs/2512.12976)
*Marcus Ma,Cole Johnson,Nolan Bridges,Jackson Trager,Georgios Chochlakis,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 本文提出了一种名为'作者标注（author labeling）'的新文本标注方法，即由文档作者在创作时直接标注主观特征（如情感、信念、产品推荐倾向），并在真实商业聊天机器人场景中验证了其有效性与优越性。


<details>
  <summary>Details</summary>
Motivation: 传统第三方标注难以准确捕捉以自我为中心（egocentric）的主观信息（如情感、信念），而作者自身在创作时的即时标注能提供更真实、高质量的标签。

Method: 设计并部署了一个实时作者标注系统：在聊天机器人中识别任务相关查询，动态生成标注问题，并实时记录作者回答；基于该数据训练在线学习的产品推荐模型。

Result: 作者标注系统使点击率（CTR）相比工业界广告基线提升534%；在情感分析任务上，相比三种传统标注方式，作者标注质量更高、获取更快、成本更低。

Conclusion: 作者标注是一种更优的主观文本标注范式，尤其适用于egocentric特征；本文开源了面向学术界的作者标注服务，推动该方法的广泛应用。

Abstract: The status quo for labeling text is third-party annotation, but there are many cases where information directly from the document's source would be preferable over a third-person proxy, especially for egocentric features like sentiment and belief. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 10,000 users to deploy an author labeling annotation system for subjective features related to product recommendation. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation that continuously improves from author labeling and find it achieved a 534% increase in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at academic.echollm.io.

</details>


### [247] [An Open and Reproducible Deep Research Agent for Long-Form Question Answering](https://arxiv.org/abs/2512.13059)
*Ikuya Yamada,Wataru Ikeda,Ko Yoshida,Mengyu Ye,Hinata Sugimoto,Masatoshi Suzuki,Hisanori Ozaki,Jun Suzuki*

Main category: cs.CL

TL;DR: 本文提出了一种开源的深度研究系统，用于长文本问答，在NeurIPS 2025 MMU-RAG竞赛中获胜；该系统结合开源大模型与开放网络搜索API，通过迭代检索、推理与合成提升问答质量，并采用基于LLM-as-a-judge反馈的偏好调优增强推理质量。


<details>
  <summary>Details</summary>
Motivation: 提升开放域长文本问答系统的推理质量与事实准确性，应对真实世界复杂查询需求。

Method: 构建基于开源大语言模型和开放网络搜索API的迭代式检索-推理-合成系统，并引入多维度（清晰性、洞察力、事实性）LLM-as-a-judge反馈驱动的偏好调优。

Result: 实验表明该方法在清晰性、洞察力和事实性三方面均持续提升答案质量。

Conclusion: 该开源深度研究系统有效提升了开放域长问答的综合性能，验证了LLM-as-a-judge引导的偏好调优在实际系统中的有效性。

Abstract: We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.

</details>


### [248] [LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators](https://arxiv.org/abs/2512.13063)
*Cheril Shah,Akshit Agarwal,Kanak Garg,Mourad Heddaya*

Main category: cs.CL

TL;DR: 本文提出了一种基于双曲正切曲线的让步动态建模框架，并定义了burstiness tau和Concession-Rigidity Index（CRI）两个新指标；通过大规模实证比较发现，当前大语言模型在谈判中存在系统性锚定极端值、忽视权力不对称与上下文、策略单一等问题，其能力不随模型升级而提升，揭示了现有LLM在谈判推理上的根本局限。


<details>
  <summary>Details</summary>
Motivation: 人类谈判者能动态调整锚点、节奏和灵活性以利用权力不对称和非正式线索，而现有LLM在谈判中的行为机制尚不清晰，缺乏对让步动态的统一数学刻画和系统性评估。

Method: 构建基于双曲正切函数的让步动态统一数学模型；提出burstiness tau（衡量让步时间分布突发性）和Concession-Rigidity Index（CRI，量化让步刚性）两个新指标；开展涵盖自然语言/数值报价、有无市场背景、六种权力不对称场景的大规模人机对比实验。

Result: LLM普遍在协议区间两端锚定，无视杠杆与上下文优化固定目标；策略多样性低，偶现欺骗性行为；模型性能不随参数量或能力提升而改善；人类则能平滑适应情境并推断对手立场与策略。

Conclusion: 当前LLM在双边谈判中存在根本性缺陷，缺乏对对手推理与情境依赖策略的内化能力，亟需发展能建模对方心智与动态上下文的新一代谈判模型。

Abstract: Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.

</details>


### [249] [Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing](https://arxiv.org/abs/2512.13109)
*Zewen Qiang,Sendong Zhao,Haochun Wang,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 本文提出了一种新的注意力机制优化方法，通过利用初始显著性（initial saliency）来缓解大语言模型在长文本处理中的“中间丢失”问题，并在多个长上下文任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长文本时存在“中间丢失”现象，传统研究归因于位置编码偏差，本文首次指出初始显著性是另一关键因素。

Method: 识别并利用初始显著性现象，即初始token与其他token之间的注意力权重可通过缩放来增强模型对中间内容的关注；并将其与现有缓解位置编码偏差的方法结合。

Result: 在MDQA数据集上最高提升3.6%，在KV-Retrieval任务中与现有方法结合后最高提升3.4%。

Conclusion: 初始显著性是影响长文本建模的重要因素，针对性地调整初始token相关注意力可有效缓解‘中间丢失’，且具有良好的兼容性和实用性。

Abstract: Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.

</details>


### [250] [Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models](https://arxiv.org/abs/2512.13194)
*Chendong Sun*

Main category: cs.CL

TL;DR: 本文提出了一种名为EARS的自适应拒绝采样方法，通过引入目标模型预测不确定性来动态调整接受阈值，在保持准确率的同时显著提升推测解码效率。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码中的固定随机阈值导致高不确定性场景下大量合理候选词被随机拒绝，降低推理效率。

Method: 提出高效自适应拒绝采样（EARS），利用目标模型预测不确定性（1−max(P_target)）动态调整接受阈值，并加入与不确定性成正比的容忍项。

Result: 在创意写作和开放域问答任务中，EARS使吞吐量最高提升18.12%，GSM8K准确率仅下降0.84%，且无需修改模型结构。

Conclusion: EARS是一种轻量、即插即用的方法，能有效缓解推测解码中的随机拒绝问题，在效率与准确性之间实现更好平衡。

Abstract: Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant "random rejection" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \(1 - \max(P_{\mathrm{target}})\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.

</details>


### [251] [AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning](https://arxiv.org/abs/2512.13278)
*Jiaru Zou,Ling Yang,Yunzhe Qi,Sirui Chen,Mengting Ai,Ke Shen,Jingrui He,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文提出AutoTool框架，使大语言模型代理能在推理过程中动态选择工具，通过构建大规模工具选择数据集和双阶段优化流程，在多个任务上显著提升性能并增强对新工具的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体假设工具集固定，限制了其在新或动态变化工具环境下的适应性，亟需支持动态工具选择的能力。

Method: 构建包含20万样本、覆盖1000+工具与100+任务的工具选择理由数据集；设计双阶段优化：(i) 基于监督学习与强化学习的轨迹稳定化，(ii) KL正则化的Plackett-Luce排序以优化多步工具选择一致性。

Result: 在10个基准测试中，基于Qwen3-8B和Qwen2.5-VL-7B的AutoTool分别平均提升数学与科学推理6.4%、搜索问答4.5%、代码生成7.7%、多模态理解6.9%；且能有效泛化至未见工具。

Conclusion: AutoTool实现了LLM代理在推理链中动态、一致、可泛化的工具选择，为构建更灵活、自适应的智能体提供了新范式。

Abstract: Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.

</details>


### [252] [AIR: Post-training Data Selection for Reasoning via Attention Head Influence](https://arxiv.org/abs/2512.13279)
*Jinrui Liu,Jeff Wu,Xuanguang Pan,Gavin Cheung,Shuai Ma,Chongyang Tao*

Main category: cs.CL

TL;DR: 本文提出了一种名为Attention Influence for Reasoning (AIR)的无监督、无需训练的数据选择框架，用于提升大语言模型（LLM）推理能力的后训练蒸馏效果。AIR基于注意力头的机制性影响，量化各推理步骤的重要性，实现细粒度的数据筛选与加权训练，显著提升多步推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有后训练蒸馏中的数据选择方法（如人工筛选、长度/熵/总损失启发式）无法刻画单个推理步骤的因果重要性，导致蒸馏效率受限。

Method: AIR首先识别预训练模型中对推理关键的注意力头；其次构建一个禁用该头影响的弱化参考模型；最后以原始模型与参考模型之间的损失差异作为Attention Influence Score，用于步骤级加权微调和样本级全局选择。

Result: 在多个推理基准上，AIR持续提升推理准确率，超越各类启发式基线，并能有效识别最关键的推理步骤与样本。

Conclusion: AIR是一种机制驱动、数据高效的大语言模型推理蒸馏新范式，为理解与利用模型内部机制进行数据优化提供了新思路。

Abstract: LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.

</details>


### [253] [Integrating Causal Reasoning into Automated Fact-Checking](https://arxiv.org/abs/2512.13286)
*Youssra Rebboud,Pasquale Lisena,Raphael Troncy*

Main category: cs.CL

TL;DR: 本文提出了一种结合事件关系抽取、语义相似度计算和基于规则推理的方法，用于检测声明与证据中事件链之间的逻辑不一致，以提升事实核查中的因果推理能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化事实核查方法缺乏专门的因果推理能力，难以识别声明中错误的因果关系，限制了语义层面的可解释性。

Method: 融合事件关系抽取、语义相似度计算与规则推理，检测声明与证据中事件链间的逻辑不一致性。

Result: 在两个事实核查数据集上验证有效，建立了首个将细粒度因果事件关系融入事实核查的基线方法。

Conclusion: 该方法能有效增强事实核查系统的因果推理能力与判决结果的可解释性。

Abstract: In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.

</details>


### [254] [MiniLingua: A Small Open-Source LLM for European Languages](https://arxiv.org/abs/2512.13298)
*Anna Aksenova,Boris Zverkov,Nicola Dainese,Alexander Nikitin,Pekka Marttinen*

Main category: cs.CL

TL;DR: MiniLingua is a lightweight, multilingual (13 European languages), open-source LLM with ~1B parameters, trained from scratch; it outperforms larger or similarly trained models on several NLP tasks and supports on-device use.


<details>
  <summary>Details</summary>
Motivation: To address high computational cost, privacy concerns, and English-centric bias of large language models by developing a small, efficient, multilingual, and open-source alternative suitable for on-device deployment.

Method: Trained MiniLingua from scratch—a 1B-parameter model—on 13 European languages; applied instruction tuning and evaluated across summarization, classification, open/closed-book QA, and open-ended generation.

Result: Instruction-tuned MiniLingua outperforms EuroLLM (larger training budget) on summarization, classification, and both open- and closed-book QA; remains competitive with stronger SOTA models on open-ended generation.

Conclusion: A small, multilingual, instruction-tuned LLM can achieve strong performance across diverse tasks while enabling efficiency, privacy, and broader linguistic coverage—demonstrating viability of compact, open, and inclusive LLMs.

Abstract: Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.

</details>


### [255] [FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models](https://arxiv.org/abs/2512.13330)
*Joona Kytöniemi,Jousia Piha,Akseli Reunamo,Fedor Vitiugin,Farrokh Mehryary,Sampo Pyysalo*

Main category: cs.CL

TL;DR: 本文介绍了FIN-bench-v2，一个用于评估芬兰语大语言模型的统一基准套件，整合并扩展了多个芬兰语任务，并通过严格筛选标准确保任务质量，所有资源均开源。


<details>
  <summary>Details</summary>
Motivation: 现有芬兰语语言模型评估基准分散、格式不一，缺乏系统性与一致性，亟需统一、高质量、多任务的评估框架。

Method: 构建统一基准FIN-bench-v2，涵盖多种任务类型；将数据集标准化为HuggingFace格式，提供多种prompt变体；对机器翻译资源进行人工校验；利用预训练小模型的学习曲线量化评估任务质量（单调性、信噪比、非随机性、模型排序一致性），仅保留达标任务；最后用指令微调大模型进行跨任务与prompt分析。

Result: 发布了一个结构统一、质量可控、覆盖全面的芬兰语模型评估基准FIN-bench-v2，包含全部数据集、prompt模板与评估配置，并开源至GitHub。

Conclusion: FIN-bench-v2为芬兰语大模型评估提供了可靠、可复现、开源的基础设施，推动了低资源语言NLP评估的标准化与严谨化。

Abstract: We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.

</details>


### [256] [Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers](https://arxiv.org/abs/2512.13363)
*Shibani Sankpal*

Main category: cs.CL

TL;DR: This paper introduces 'emotion drift' to capture emotional shifts within mental health messages, using transformer models to detect sentence-level emotions and quantify drift, revealing patterns of escalation or relief.


<details>
  <summary>Details</summary>
Motivation: Sentiment analysis usually assigns a single label to an entire message, ignoring nuanced intra-message emotional changes—especially critical in mental health contexts where emotional progression matters.

Method: The study detects sentence-level emotions using pre-trained transformer models (DistilBERT and RoBERTa) and computes emotion drift scores to quantify emotional change across sentences in mental health-related texts.

Result: The approach successfully identifies patterns of emotional escalation or relief in mental health conversations, offering fine-grained insights beyond traditional sentiment classification.

Conclusion: Emotion drift is a meaningful construct for modeling intra-text emotional dynamics, especially valuable for mental health applications, and transformer-based sentence-level analysis enables its effective measurement.

Abstract: This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.

</details>


### [257] [Large language models are not about language](https://arxiv.org/abs/2512.13441)
*Johan J. Bolhuis,Andrea Moro,Stephen Crain,Sandiway Fong*

Main category: cs.CL

TL;DR: This paper argues that large language models are unsuitable for linguistic research because they are probabilistic and data-hungry, unlike the human language system, which is innate, recursive, and capable of distinguishing possible from impossible languages with minimal input.


<details>
  <summary>Details</summary>
Motivation: To challenge the applicability of large language models to linguistics by highlighting fundamental differences between statistical language modeling and the human language faculty.

Method: Conceptual and theoretical comparison between large language models and the biologically grounded, generative nature of human language.

Result: Demonstrates that LLMs lack the cognitive and computational properties essential to linguistic theory, such as recursion, internal hierarchy, and poverty-of-the-stimulus learning.

Conclusion: Large language models cannot substitute for or meaningfully inform theories of human language, which must account for innate, mind-internal computational mechanisms.

Abstract: Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.

</details>


### [258] [Scaling Laws for Code: Every Programming Language Matters](https://arxiv.org/abs/2512.13472)
*Jian Yang,Shawn Guo,Lin Jing,Wei Zhang,Aishan Liu,Chuan Hao,Zhoujun Li,Wayne Xin Zhao,Xianglong Liu,Weifeng Lv,Bryan Dai*

Main category: cs.CL

TL;DR: 本文首次系统研究了多语言代码预训练的缩放定律，通过超1000次实验（等效33.6万H800小时），揭示了解释型语言（如Python）比编译型语言（如Rust）更受益于模型和数据规模增长，并提出一种比例依赖的多语言缩放定律，以优化不同编程语言的训练资源分配。


<details>
  <summary>Details</summary>
Motivation: 现有代码大模型缩放定律忽视了不同编程语言在预训练中的异质性影响及其相互作用，且多采用语言无关设定，无法准确预测多语言场景下的性能。

Method: 开展超1000次大规模实验（覆盖0.2B–14B参数、1T token数据、多种编程语言），分析各语言缩放行为；探究语言间协同效应；评估平行配对（代码+翻译）策略对跨语言能力的影响；提出比例依赖的多语言缩放定律并验证其资源分配有效性。

Result: 发现解释型语言（如Python）比编译型语言（如Rust）更具可扩展性；语法相近语言对（如JS/TS）存在显著协同增益；平行配对策略显著提升跨语言能力；所提比例依赖缩放定律在相同算力下优于均匀分配，提升全语言平均性能。

Conclusion: 多语言代码预训练需差异化建模各语言缩放特性及交互关系；基于语言效用、协同性和饱和度的动态token分配策略是提升整体性能的关键路径。

Abstract: Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.

</details>


### [259] [Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models](https://arxiv.org/abs/2512.13478)
*Kei Saito*

Main category: cs.CL

TL;DR: 本文提出非解析推理（NRR）框架，通过多向量嵌入、非坍缩注意力和上下文身份追踪（CIT）等机制，在推理过程中显式保留语义歧义，仅在必要时由外部解析算子ρ进行可控、任务依赖的语义承诺，从而克服现有语言模型因softmax和贪心解码导致的过早语义坍缩问题。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型因softmax竞争和贪心解码导致过早语义坍缩（即过早锁定单一含义），造成推理脆弱和上下文失效，亟需一种能显式保留并可控解析歧义的新范式。

Method: 提出非解析推理（NRR）框架，包含三部分：（1）多向量嵌入以维持每个token的多种合理解释；（2）非坍缩注意力防止各层中的‘赢家通吃’动态；（3）上下文身份追踪（CIT），为重复实体赋予上下文特异性身份；所有机制由外部解析算子ρ统一协调，实现表示与解析的分离。

Result: 在合成评估中，CIT增强模型在分布外身份切换任务上准确率达90.9%，远超Transformer基线的9.1%；NRR支持同一模型无需微调即可在创造性、事实性和歧义保留推理间灵活切换。

Conclusion: NRR将歧义从失败模式重构为显式的表征状态，提供了一种原则性替代方案，核心主张是：AI不应避免歧义，而应明确控制其解析的时机、方式与主体。

Abstract: Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.

</details>


### [260] [Advancing Bangla Machine Translation Through Informal Datasets](https://arxiv.org/abs/2512.13487)
*Ayon Roy,Risat Rahaman,Sadat Shibly,Udoy Saha Joy,Abdulla Al Kafi,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: This paper addresses the lack of open-source Bangla machine translation, especially for informal language, by building a new dataset from social media and conversational texts to improve accessibility.


<details>
  <summary>Details</summary>
Motivation: Limited progress in open-source Bangla machine translation, especially for informal language, excludes millions from accessing essential online information due to lack of pairwise Bangla-English data and suitable models.

Method: Developing a new dataset from informal sources (e.g., social media and conversational texts) and exploring/improving state-of-the-art translation models for informal Bangla.

Result: A newly constructed informal Bangla-English dataset and proposed model improvements targeting informal language translation.

Conclusion: Enhancing datasets and models for informal Bangla translation can significantly improve digital information access for millions of Bangla speakers.

Abstract: Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.

</details>


### [261] [SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping](https://arxiv.org/abs/2512.13494)
*Yu-Chen Lu,Sheng-Feng Yu,Hui-Hsien Weng,Pei-Shuo Wang,Yu-Fang Hu,Liang Hung-Chun,Hung-Yueh Chiang,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: 本文提出SkipCat，一种新型低秩压缩框架，通过层内共享低秩投影和块跳过技术，在相同压缩率下保留更高有效秩，显著提升零样本任务准确率（+7%），且无需额外微调。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数量大，难以部署在资源受限的边缘设备上；传统低秩压缩需大幅降低秩才能节省资源，但会导致严重性能下降。

Method: 提出SkipCat框架：1）层内共享低秩投影，使多个共享输入的矩阵共用一个投影以减少冗余；2）块跳过技术，跳过低秩分解中部分子块的计算与内存传输。

Result: 在不进行额外微调的前提下，相比以往低秩压缩方法，在相同压缩率下零样本任务准确率提升7%。

Conclusion: SkipCat通过最大化有效秩，在保持高压缩率的同时显著缓解性能损失，验证了其在资源受限场景下高效压缩大语言模型的有效性。

Abstract: Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.

</details>


### [262] [PrahokBART: A Pre-trained Sequence-to-Sequence Model for Khmer Natural Language Generation](https://arxiv.org/abs/2512.13552)
*Hour Kaing,Raj Dabre,Haiyue Song,Van-Hien Tran,Hideki Tanaka,Masao Utiyama*

Main category: cs.CL

TL;DR: 本文提出了PrahokBART，一种专为高棉语从零训练的紧凑型序列到序列预训练模型，通过改进语料质量与融入高棉语特有的语言学处理（如分词和归一化）显著提升了生成任务性能，并在机器翻译、文本摘要和标题生成上超越了mBART50。


<details>
  <summary>Details</summary>
Motivation: 现有多语言模型忽视高棉语的语言特性（如空格使用、分词需求），导致其在高棉语任务中表现不佳；本文旨在通过构建高质量高棉语-英语语料并引入针对性语言学模块来解决该问题。

Method: 从零训练一个轻量级序列到序列模型PrahokBART，使用精心筛选的高棉语与英语语料，并集成高棉语特有语言学组件（如分词器和文本归一化模块）；在机器翻译、文本摘要和标题生成三个生成任务上进行评估，并分析各模块贡献及模型对空格生成的建模能力。

Result: PrahokBART在三项生成任务上均优于强基线mBART50；消融实验验证了各语言学模块的有效性；模型能更自然地生成符合高棉语习惯的空格分布。

Conclusion: 针对特定语言（如高棉语）设计专用预训练模型并融合其语言学特征，比直接使用通用多语言模型更有效；高质量语料与语言适配模块是提升低资源语言生成性能的关键。

Abstract: This work introduces {\it PrahokBART}, a compact pre-trained sequence-to-sequence model trained from scratch for Khmer using carefully curated Khmer and English corpora. We focus on improving the pre-training corpus quality and addressing the linguistic issues of Khmer, which are ignored in existing multilingual models, by incorporating linguistic components such as word segmentation and normalization. We evaluate PrahokBART on three generative tasks: machine translation, text summarization, and headline generation, where our results demonstrate that it outperforms mBART50, a strong multilingual pre-trained model. Additionally, our analysis provides insights into the impact of each linguistic module and evaluates how effectively our model handles space during text generation, which is crucial for the naturalness of texts in Khmer.

</details>


### [263] [EMNLP: Educator-role Moral and Normative Large Language Models Profiling](https://arxiv.org/abs/2508.15250)
*Yilin Jiang,Mingzi Zhang,Sheng Jin,Zengyi Yu,Xiangjie Kong,Binghao Tu*

Main category: cs.CL

TL;DR: 本文提出了EMNLP框架，用于评估教师角色大语言模型在人格、道德发展阶段和伦理风险方面的表现，首次构建了面向教育AI的教师角色LLM伦理与心理对齐基准。


<details>
  <summary>Details</summary>
Motivation: 现有模拟职业（SP）的大型语言模型缺乏全面的心理学与伦理学评估，尤其在教育场景中亟需专业化的评估工具。

Method: 提出EMNLP框架，扩展已有量表，构建88个教师特异性道德困境，并设计针对性软提示注入集，对14个LLM进行人格画像、道德阶段测量及伦理风险评估。

Result: 教师角色LLM表现出更理想化、两极化的人格特征，抽象道德推理能力强，但在情感复杂情境中表现较差；强推理能力模型反而更易受有害提示注入影响；模型温度等超参对多数风险行为影响有限。

Conclusion: EMNLP是首个面向教师角色LLM的伦理与心理对齐评估基准，揭示了能力与安全性之间的悖论，为教育AI的安全部署提供了新视角与实证基础。

Abstract: Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.

</details>


### [264] [Verifying Rumors via Stance-Aware Structural Modeling](https://arxiv.org/abs/2512.13559)
*Gibson Nkhata,Uttamasha Anjally Oyshi,Quan Mai,Susan Gauch*

Main category: cs.CL

TL;DR: 本文提出了一种立场感知的结构化建模方法，用于社交媒体谣言验证，通过融合语义内容、立场信息和对话结构，在序列长度受限下提升真实性预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以在Transformer编码器的序列长度限制下，联合捕捉语义内容、立场信息和对话结构，而回复立场对谣言真实性判断至关重要。

Method: 提出立场感知的结构化建模：为每个帖子编码其立场信号，并按立场类别聚合回复嵌入；引入立场分布和层级深度作为协变量以增强结构感知能力。

Result: 在多个基准数据集上显著优于先前方法，具备早期检测能力和跨平台泛化能力。

Conclusion: 立场感知与结构建模的有效结合可提升谣言真实性验证的准确性和鲁棒性，尤其适用于长对话结构建模场景。

Abstract: Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.

</details>


### [265] [Memory in the Age of AI Agents](https://arxiv.org/abs/2512.13564)
*Yuyang Hu,Shichun Liu,Yanwei Yue,Guibin Zhang,Boyang Liu,Fangyi Zhu,Jiahang Lin,Honglin Guo,Shihan Dou,Zhiheng Xi,Senjie Jin,Jiejun Tan,Yanbin Yin,Jiongnan Liu,Zeyu Zhang,Zhongxiang Sun,Yutao Zhu,Hao Sun,Boci Peng,Zhenrong Cheng,Xuanbo Fan,Jiaxin Guo,Xinlei Yu,Zhenhong Zhou,Zewen Hu,Jiahao Huo,Junhao Wang,Yuwei Niu,Yu Wang,Zhenfei Yin,Xiaobin Hu,Yue Liao,Qiankun Li,Kun Wang,Wangchunshu Zhou,Yixin Liu,Dawei Cheng,Qi Zhang,Tao Gui,Shirui Pan,Yan Zhang,Philip Torr,Zhicheng Dou,Ji-Rong Wen,Xuanjing Huang,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.CL

TL;DR: 本文对基于基础模型的智能体记忆研究进行了全面综述，提出从形式（token级、参数化、潜在）、功能（事实性、经验性、工作记忆）和动态（形成、演化、检索）三个维度统一理解 agent memory，并梳理了基准、框架与前沿方向。


<details>
  <summary>Details</summary>
Motivation: 当前 agent memory 研究日益碎片化，术语混乱、分类模糊（如传统长短时记忆已不适用），亟需系统性梳理与概念澄清。

Method: 通过界定 agent memory 的范畴（区别于LLM memory、RAG、上下文工程），并从形式（forms）、功能（functions）、动态（dynamics）三重视角构建统一分析框架；同时汇总评估基准、开源框架，并展望前沿方向。

Result: 提出了涵盖三种形式、三类功能、多阶段动态过程的 agent memory 分类体系；整理了现有基准与框架；明确了记忆自动化、强化学习融合、多模态/多智能体记忆及可信性等未来方向。

Conclusion: agent memory 应被视作未来智能体设计中的一等公民（first-class primitive），本综述为其提供了概念基础与研究蓝图。

Abstract: Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.

</details>


### [266] [ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding](https://arxiv.org/abs/2512.13586)
*Jia-Nan Li,Jian Guan,Wei Wu,Chongxuan Li*

Main category: cs.CL

TL;DR: ReFusion提出一种基于slot的掩码扩散模型，通过‘规划-填充’迭代解码，在保持并行性的同时提升效率与生成质量，显著优于现有MDM并逼近自回归模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型推理慢，而掩码扩散模型存在KV缓存不可用和生成不连贯的问题。

Method: 提出ReFusion：将并行解码从token级提升至slot级（固定长度连续子序列），采用扩散驱动的slot规划与自回归式并行填充相结合的迭代解码机制，并支持统一因果框架下的KV缓存复用。

Result: 在七个基准上，相比先前MDM平均提升34%性能、加速18倍以上；相比强自回归模型仍保持2.33倍平均加速，同时大幅缩小性能差距。

Conclusion: slot级建模有效缓解了MDM的计算开销与依赖建模难题，使扩散模型在质量与效率上更接近实用化自回归模型。

Abstract: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.

</details>


### [267] [Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization](https://arxiv.org/abs/2512.13598)
*Daniel Melcer,Qi Chen,Wen-Hao Chiang,Shweta Garg,Pranav Garg,Christian Bock*

Main category: cs.CL

TL;DR: 本文研究了基于文本梯度的自动提示优化方法，发现尽管这些方法常能提升性能，但其'梯度类比'并不准确反映实际行为。


<details>
  <summary>Details</summary>
Motivation: 自动提示优化技术旨在无需人工干预即可提升大语言模型性能，其中基于文本梯度的方法是主流之一，但其内在机制尚不明确。

Method: 通过一系列实验和案例研究，分析文本梯度类方法的实际行为与理论假设之间的差异。

Result: 实验证明，文本梯度方法虽常提升性能，但其行为并不符合梯度类比的直观解释。

Conclusion: 文本梯度类比存在局限性，该发现有助于更合理地选择和设计提示优化策略。

Abstract: A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.

</details>


### [268] [Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models](https://arxiv.org/abs/2512.13607)
*Boxin Wang,Chankyu Lee,Nayeon Lee,Sheng-Chieh Lin,Wenliang Dai,Yang Chen,Yangyi Chen,Zhuolin Yang,Zihan Liu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.CL

TL;DR: 本文提出级联领域强化学习（Cascade RL）方法，构建通用推理模型Nemotron-Cascade，支持指令模式与深度思考模式，在多个基准测试中达到SOTA性能，并在IOI 2025中获银牌。


<details>
  <summary>Details</summary>
Motivation: 传统RL训练面临跨领域异质性问题，如响应长度和验证延迟差异大，导致基础设施复杂、训练慢、课程设计与超参调优困难。

Method: 提出级联领域强化学习（Cascade RL），按领域顺序进行RL训练，而非混合不同领域提示；将RLHF作为前置对齐步骤以增强推理能力，再进行领域化RLVR训练。

Result: 14B规模的Nemotron-Cascade模型在LiveCodeBench v5/v6/Pro上超越SFT教师模型DeepSeek-R1-0528，并在IOI 2025中获银牌。

Conclusion: Cascade RL显著降低工程复杂度，提升跨领域泛化与推理性能，RLHF前置可实质性增强推理能力，而非仅优化偏好；该方法具备可复现性与透明性。

Abstract: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.

</details>


### [269] [Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models](https://arxiv.org/abs/2512.13618)
*Zefang Liu,Nam Nguyen,Yinzhu Quan,Austin Zhang*

Main category: cs.CL

TL;DR: 本文首次对事件序列的时间分词策略进行了实证研究，比较了多种时间编码方法，并发现没有一种策略在所有数据分布下都最优，性能取决于分词器与数据统计特性的匹配程度。


<details>
  <summary>Details</summary>
Motivation: 连续时间表示是用大语言模型建模时序事件序列中关键但尚未充分探索的挑战，而现实世界事件数据具有多样化的统计分布，使得最优时间编码策略尚不明确。

Method: 对多种时间编码策略（包括朴素数字字符串、高精度字节级表示、人类语义日历标记、经典均匀分箱和自适应残差标量量化）进行实证比较，并在体现不同统计分布的真实世界数据集上微调大语言模型以评估效果。

Result: 实验表明，没有单一策略普遍最优；预测性能高度依赖于分词器与数据统计特性的匹配，基于对数的策略在偏斜分布上表现优异，而以人为中心的格式在混合模态下更稳健。

Conclusion: 时间分词策略的选择应依据事件数据的具体统计特性进行定制化设计，而非采用通用方案。

Abstract: Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.

</details>


### [270] [Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation](https://arxiv.org/abs/2512.13655)
*Richard J. Young*

Main category: cs.CL

TL;DR: 本研究评估了四种大语言模型拒绝行为消除工具（Heretic、DECCP、ErisForge、FailSpy）在16个指令微调模型上的效果，发现单次消融方法（如ErisForge、DECCP）在保持能力方面更优，而贝叶斯优化方法引起较大分布偏移；数学推理能力（GSM8K）对消融干预最敏感，性能变化幅度达-18.81至+1.51个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐机制虽能防止有害响应，但也阻碍了认知建模、对抗测试和安全分析等合法研究应用，亟需可信赖的拒绝行为消除技术。

Method: 系统评估四种消融工具（Heretic、DECCP、ErisForge、FailSpy）在16个7B–14B参数指令微调模型上的兼容性与效果，采用GSM8K等基准量化能力保留程度，并分析KL散度以衡量输出分布偏移。

Result: 单次消融法能力保留更优（ErisForge、DECCP在GSM8K上平均下降仅-0.28和-0.13个百分点）；贝叶斯优化法导致KL散度波动大（0.043–1.646）；数学推理能力最敏感，GSM8K变化范围达+1.51至-18.81 pp（相对下降26.5%）。

Conclusion: 不同消融工具对模型能力影响差异显著，研究者应依据目标模型架构与任务需求（尤其是是否依赖数学推理）选择合适工具；单次方法更适合能力敏感场景，贝叶斯方法需谨慎评估分布偏移风险。

Abstract: Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.

</details>


### [271] [A stylometric analysis of speaker attribution from speech transcripts](https://arxiv.org/abs/2512.13667)
*Cristina Aggazzotti,Elizabeth Allyn Smith*

Main category: cs.CL

TL;DR: 本文提出了一种基于文本内容的说话人归属方法StyloSpeaker，利用语音转录文本中的字符、词汇、句法和风格特征进行说话人识别，并在不同格式化和话题控制条件下评估其性能，同时与黑箱神经网络方法对比分析可解释性与关键区分特征。


<details>
  <summary>Details</summary>
Motivation: 当说话人伪装声音或使用TTS时，传统声学/语音特征失效，仅剩语言内容可用；需发展基于转录文本的内容型说话人归属方法。

Method: 提出StyloSpeaker方法，融合字符、词、词元、句子及风格等文体学特征；在两种转录格式（规范式与归一化式）和不同话题控制条件下进行实验；并与黑箱神经模型对比，分析关键区分特征。

Result: 归一化转录文本通常带来更高归属准确率（最强话题控制除外）；整体最高性能出现在最强话题控制下；StyloSpeaker具备更好可解释性，且特定文体特征（如词频、n-gram、句长分布）被证实最有效区分说话人。

Conclusion: 基于内容的说话人归属是声纹失效场景下的可行替代方案；StyloSpeaker提供了可解释、鲁棒且实用的解决方案，尤其适用于司法语境中需透明推理的场景。

Abstract: Forensic scientists often need to identify an unknown speaker or writer in cases such as ransom calls, covert recordings, alleged suicide notes, or anonymous online communications, among many others. Speaker recognition in the speech domain usually examines phonetic or acoustic properties of a voice, and these methods can be accurate and robust under certain conditions. However, if a speaker disguises their voice or employs text-to-speech software, vocal properties may no longer be reliable, leaving only their linguistic content available for analysis. Authorship attribution methods traditionally use syntactic, semantic, and related linguistic information to identify writers of written text (authorship attribution). In this paper, we apply a content-based authorship approach to speech that has been transcribed into text, using what a speaker says to attribute speech to individuals (speaker attribution). We introduce a stylometric method, StyloSpeaker, which incorporates character, word, token, sentence, and style features from the stylometric literature on authorship, to assess whether two transcripts were produced by the same speaker. We evaluate this method on two types of transcript formatting: one approximating prescriptive written text with capitalization and punctuation and another normalized style that removes these conventions. The transcripts' conversation topics are also controlled to varying degrees. We find generally higher attribution performance on normalized transcripts, except under the strongest topic control condition, in which overall performance is highest. Finally, we compare this more explainable stylometric model to black-box neural approaches on the same data and investigate which stylistic features most effectively distinguish speakers.

</details>


### [272] [Towards Effective Model Editing for LLM Personalization](https://arxiv.org/abs/2512.13676)
*Baixiang Huang,Limeng Cui,Jiapeng Liu,Haoran Wang,Jiawei Xu,Zhuiyue Tan,Yutong Chen,Chen Luo,Yi Liu,Kai Shu*

Main category: cs.CL

TL;DR: 本文提出Personalization Editing框架，将个性化视为模型编辑任务，通过聚类偏好表征实现局部精准编辑，在提升个性化效果的同时保持模型整体能力，并构建了更贴近真实场景的UPQA评测数据集。


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化方法存在计算开销大、数据依赖强、灾难性遗忘及多轮/隐式查询下性能下降等问题；同时缺乏真实用户交互驱动、兼顾信息检索与偏好回忆的评测基准。

Method: 将个性化建模为模型编辑任务，提出Personalization Editing框架：基于用户偏好聚类生成局部编辑指令，对LLM进行参数高效编辑；并构建User Preference Question Answering（UPQA）短答案问答数据集，源自真实用户查询，覆盖不同难度级别。

Result: Personalization Editing在编辑准确率和计算效率上优于微调，在多轮对话和隐式偏好问题上优于提示工程基线；UPQA能更直接评估模型对用户特定偏好的回忆与应用能力。

Conclusion: 将个性化形式化为模型编辑任务是可行且有效的路径，结合偏好聚类引导的局部编辑与面向真实交互的UPQA评测，可显著提升个性化LLM的实用性、鲁棒性与可评估性。

Abstract: Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.

</details>


### [273] [Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech](https://arxiv.org/abs/2512.13685)
*Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Lilian Hubner,Bárbara Malcorra,César Rennó-Costa,Marco Idiart,Maria-Cruz Villa-Uriol,Aline Villavicencio*

Main category: cs.CL

TL;DR: 本文提出一种新方法，通过在保持语义不变的前提下改变文本表层形式（如句法和词汇），来检验语言模型是否真正利用了与阿尔茨海默病（AD）相关的深层语义特征而非表面模式；实验表明，仅依赖语义信息，模型仍能有效识别AD，证实语义损伤可作为早期检测的新线索。


<details>
  <summary>Details</summary>
Motivation: 语言模型在AD筛查中潜力大，但其黑箱特性难以区分真实语言标记与表面文本模式，需验证其是否真正捕捉语义层面的认知衰退信号。

Method: 设计语义保持的文本表层变换（改写句法与词汇），结合BLEU/chrF（衡量表层变化）和语义相似度（衡量语义保留）评估；同时探究图片描述文本能否重构原图，并分析图像相关噪声对分类的影响。

Result: 变换后文本虽表层差异大（低BLEU/chrF）、语义高度保留（高语义相似度），模型分类性能（macro-F1）仅轻微下降；仅用语义信息仍可有效检测AD；图像重建任务引入噪声显著降低分类准确率。

Conclusion: 语言模型确能捕获AD相关的深层语义损伤，而非依赖表面语言特征；该方法可剥离伪相关，为基于语义的早期AD检测提供了新路径。

Abstract: Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address this issue, we examine how surface form variation affects classification performance, with the goal of assessing the ability of language models to represent underlying semantic indicators. We introduce a novel approach where texts surface forms are transformed by altering syntax and vocabulary while preserving semantic content. The transformations significantly modify the structure and lexical content, as indicated by low BLEU and chrF scores, yet retain the underlying semantics, as reflected in high semantic similarity scores, isolating the effect of semantic information, and finding models perform similarly to if they were using the original text, with only small deviations in macro-F1. We also investigate whether language from picture descriptions retains enough detail to reconstruct the original image using generative models. We found that image-based transformations add substantial noise reducing classification accuracy. Our methodology provides a novel way of looking at what features influence model predictions, and allows the removal of possible spurious correlations. We find that just using semantic information, language model based classifiers can still detect AD. This work shows that difficult to detect semantic impairment can be identified, addressing an overlooked feature of linguistic deterioration, and opening new pathways for early detection systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [274] [Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights](https://arxiv.org/abs/2512.11802)
*Zheng Li,Peng Zhang,Shixiao Liang,Hang Zhou,Chengyuan Ma,Handong Yao,Qianwen Li,Xiaopeng Li*

Main category: cs.RO

TL;DR: 本文通过实地实验采集特斯拉TLSSC系统与交通控制设备（TCD）交互的高分辨率轨迹与视频数据，构建行为分类体系，并基于FVDM模型定量刻画停止、加速和跟车三类行为；发现约90米的跟车阈值，揭示其跟车更平滑、头距更小等特性，为ADAS-TCD交互建模与安全评估提供数据与方法基础。


<details>
  <summary>Details</summary>
Motivation: ADAS与交通控制设备（TCD）的交互对交通运行影响重大，但缺乏聚焦的实证研究。

Method: 设计多速度限值与TCD类型下的实地实验，同步采集高分辨率车辆轨迹数据和驾驶员视角视频；构建TLSSC-TCD交互行为分类（停止、加速、跟车）；标定全速度差模型（FVDM）以定量刻画各行为模式。

Result: 识别出约90米的交叉口跟车阈值；停止行为对期望速度偏差和相对速度响应强烈，加速行为更保守；交叉口跟车动态更平滑、头距更小。

Conclusion: 所建立的数据集、行为定义与模型刻画为未来ADAS-TCD交互的仿真、安全评估及逻辑设计提供了坚实基础。

Abstract: Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.

</details>


### [275] [ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision](https://arxiv.org/abs/2512.11824)
*Rosh Ho,Jian Zhang*

Main category: cs.RO

TL;DR: ReGlove是一种低成本、基于视觉引导的康复手套系统，利用腕部摄像头和边缘计算（树莓派5）实现无需可靠肌电信号的上下文感知抓取，具备高精度、低延迟和高实用性，总成本低于250美元。


<details>
  <summary>Details</summary>
Motivation: 针对全球数百万上肢慢性障碍患者，现有辅助技术价格昂贵或依赖不可靠的生物信号（如EMG），亟需一种低成本、鲁棒性强的替代方案。

Method: 将商用气动康复手套与腕部摄像头及树莓派5边缘推理引擎集成，采用实时YOLO目标检测模型进行抓取意图识别，并通过标准化YCB物体操作和ADL任务进行物理验证。

Result: 系统实现96.73%的抓取分类准确率，端到端延迟低于40毫秒；在YCB物体操作中成功率82.71%，可稳定完成27项日常生活任务。

Conclusion: ReGlove以低于250美元的全商用组件构建了可行、可扩展的视觉驱动上肢辅助技术基础，为无法使用传统EMG设备的人群提供了可及的新选择。

Abstract: This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \SI{96.73}{\percent} grasp classification accuracy with sub-\SI{40.00}{\milli\second} end-to-end latency. Physical validation using standardized benchmarks shows \SI{82.71}{\percent} success on YCB object manipulation and reliable performance across \SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \$\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.

</details>


### [276] [WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2512.11872)
*Mingwang Xu,Jiahao Cui,Feipeng Cai,Hanlin Shang,Zhihao Zhu,Shan Luan,Yifang Xu,Neng Zhang,Yaoyi Li,Jia Cai,Siyu Zhu*

Main category: cs.RO

TL;DR: 本文提出WAM-Diff，一种基于离散掩码扩散的视觉-语言-动作（VLA）框架，用于端到端自动驾驶中的轨迹生成，通过非因果掩码扩散、稀疏MoE架构和在线强化学习GSPO，在NAVSIM基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 探索离散掩码扩散在自动驾驶轨迹生成中的潜力，弥补当前主流自回归与连续扩散方法的不足。

Method: 提出WAM-Diff框架：1）适配掩码扩散支持灵活非因果解码顺序；2）采用稀疏MoE架构联合训练运动预测与驾驶导向VQA；3）引入Group Sequence Policy Optimization（GSPO）进行序列级在线强化学习。

Result: 在NAVSIM-v1和NAVSIM-v2上分别达到91.0 PDMS和89.7 EPDMS，显著优于现有方法。

Conclusion: 离散掩码扩散是一种有前景的轨迹生成范式，支持场景感知解码策略，为VLA驱动系统提供了新思路。

Abstract: End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff

</details>


### [277] [Audio-Based Tactile Human-Robot Interaction Recognition](https://arxiv.org/abs/2512.11873)
*Antonia Yepes,Marie Charbonneau*

Main category: cs.RO

TL;DR: This paper proposes using microphones on a robot's body to detect and classify tactile interactions through sound, achieving high accuracy for touch types with distinct acoustic frequencies.


<details>
  <summary>Details</summary>
Motivation: To provide an alternative to traditional tactile sensing methods (e.g., joint torque or 6-axis force/torque sensors) by leveraging audio signals from robot shell contact.

Method: Two I2S MEMS microphones mounted on a robot's torso captured sounds from six touch types; a CNN was trained on 336 pre-processed audio samples (48 per class) for classification.

Result: The CNN achieved high classification accuracy for touch types exhibiting distinct dominant acoustic frequencies.

Conclusion: Audio-based tactile sensing using embedded microphones is a viable and effective alternative for distinguishing common touch interactions on rigid robotic bodies.

Abstract: This study explores the use of microphones placed on a robot's body to detect tactile interactions via sounds produced when the hard shell of the robot is touched. This approach is proposed as an alternative to traditional methods using joint torque sensors or 6-axis force/torque sensors. Two Adafruit I2S MEMS microphones integrated with a Raspberry Pi 4 were positioned on the torso of a Pollen Robotics Reachy robot to capture audio signals from various touch types on the robot arms (tapping, knocking, rubbing, stroking, scratching, and pressing). A convolutional neural network was trained for touch classification on a dataset of 336 pre-processed samples (48 samples per touch type). The model shows high classification accuracy between touch types with distinct acoustic dominant frequencies.

</details>


### [278] [Traversability Aware Autonomous Navigation for Multi-Modal Mobility Morphobot (M4)](https://arxiv.org/abs/2512.11876)
*Hrigved Mahesh Suryawanshi*

Main category: cs.RO

TL;DR: 本文提出了一种面向M4机器人平台的可通行性感知自主导航框架，结合FAST-LIO实时定位、LiDAR生成2.5D高程图、CNN评估地形可通行性，并设计定制A*规划器综合距离、能量与地形成本，实现安全高效的路径规划。实验表明该系统能有效规避难行区域，以小幅路径增长换取显著地形质量提升。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人需实时评估地形难度并权衡路径效率与安全性；现有方法在地形建模精度和可通行性量化方面存在不足。

Method: 采用FAST-LIO进行实时定位，由LiDAR点云构建2.5D高程图；训练CNN模型预测像素级可通行性分数，并转换为导航代价；设计融合几何距离、能量消耗与地形代价的定制A*规划器；前期通过OptiTrack真值对比LiDAR与视觉SLAM，验证LiDAR在高程建模中的厘米级精度优势。

Result: 系统成功避开低可通行性区域，在实验中接受略微更长的路径，显著降低整体地形代价；端到端 pipeline（FAST-LIO + GPU高程映射 + CNN可通行性估计 + 定制Nav2规划器）运行稳定有效。

Conclusion: 本工作构建了基于学习的地形感知导航完整框架，证实了LiDAR高程图与CNN可通行性建模的有效性，为多模态机器人平台提供了可扩展的智能地形适应导航基础。

Abstract: Autonomous navigation in unstructured environments requires robots to assess terrain difficulty in real-time and plan paths that balance efficiency with safety. This thesis presents a traversability-aware navigation framework for the M4 robot platform that uses learned terrain analysis to generate energy-efficient paths avoiding difficult terrain.Our approach uses FAST-LIO for real-time localization, generating 2.5D elevation maps from LiDAR point clouds. A CNN-based model processes these elevation maps to estimate traversability scores, which are converted into navigation costs for path planning. A custom A* planner incorporates these costs alongside geometric distance and energy consumption to find paths that trade modest distance increases for substantial terrain quality improvements. Before system development, a platform-agnostic study compared LiDAR-based and camera-based SLAM using OptiTrack ground truth. Point cloud comparison through ICP alignment and cloud-to-mesh distance analysis demonstrated that LiDAR-based mapping achieves centimeter-level precision essential for elevation mapping, while camera-based approaches exhibited significantly higher geometric error. These findings directly resulted in the selection of LiDAR as the primary sensor to generate elevation maps. The complete pipeline integrates FAST-LIO localization, GPU-accelerated elevation mapping, CNN-based traversability estimation, and Nav2 navigation with a custom traversability-aware planner. Experimental results demonstrate that the system successfully avoids low traversability regions and accepts a few longer paths to achieve a reduction in terrain cost. This work establishes a foundation for intelligent terrain-aware navigation applicable to multi-modal robotic platforms.

</details>


### [279] [Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control](https://arxiv.org/abs/2512.11886)
*Mohammed Irfan Ali*

Main category: cs.RO

TL;DR: 本文提出了一种面向行星探测的11自由度模块化蛇形机器人COBRA的全自主导航系统，融合视觉-惯性SLAM、简化状态估计与闭环轨迹跟踪，实现无需外部定位的自主多航点导航。


<details>
  <summary>Details</summary>
Motivation: 蛇形机器人虽在极端地形中具有卓越机动性，但在无外部跟踪设施的环境中，其高自由度带来的自主导航难题亟待解决。

Method: 构建包含机载视觉-惯性SLAM、降阶状态估计（如质心位姿）和基于CPG的闭环轨迹跟踪的完整自主导航流程；利用深度相机与边缘计算实现实时定位，并通过距离依赖的偏航误差融合机制动态调节中枢模式发生器参数。

Result: 物理实验验证了系统可实现高精度多航点自主跟踪，定量分析了定位漂移特性与蛇形机器人特有的失效模式。

Conclusion: 该工作为无基础设施环境下蛇形机器人的自主导航奠定了坚实基础，推动其在行星探测等前沿场景的应用落地。

Abstract: Snake robots offer exceptional mobility across extreme terrain inaccessible to conventional rovers, yet their highly articulated bodies present fundamental challenges for autonomous navigation in environments lacking external tracking infrastructure. This thesis develops a complete autonomy pipeline for COBRA, an 11 degree-of-freedom modular snake robot designed for planetary exploration. While the robot's biologically inspired serpentine gaits achieve impressive mobility, prior work has relied entirely on open-loop teleoperation. This approach integrates onboard visual-inertial SLAM, reduced-order state estimation, and closed-loop trajectory tracking to enable autonomous waypoint navigation. A depth camera paired with edge computing performs real-time localization during dynamic locomotion, validated against motion-capture ground truth to characterize drift behavior and failure modes unique to snake robot platforms. A reduced-order framework estimates Center-of-Mass pose, driving a closed-loop controller that modulates CPG gait parameters through distance-dependent yaw error blending. Physical experiments validate the complete system, demonstrating accurate multi-waypoint tracking and establishing foundations for autonomous snake robot navigation.

</details>


### [280] [VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer](https://arxiv.org/abs/2512.11891)
*Songqiao Hu,Zeyi Liu,Shuang Liu,Jun Cen,Zihan Meng,Xiao He*

Main category: cs.RO

TL;DR: 本文提出了一种名为AEGIS的Vision-Language-Safe Action（VLSA）架构，通过引入基于控制屏障函数的即插即用安全约束层，在保障任务执行性能的同时提供理论安全保证，并在新构建的安全关键基准SafeLIBERO上验证了其在避障率和任务成功率上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型在非结构化环境中部署时面临任务执行与安全保障难以兼顾的挑战，尤其是物理交互中的碰撞风险。

Method: 提出AEGIS架构，包含一个基于控制屏障函数（CBF）的即插即用安全约束（SC）层，可无缝集成至现有VLA模型中，在不损害指令遵循能力的前提下提供理论安全保证。

Result: 在新构建的安全关键基准SafeLIBERO上，AEGIS将障碍物规避率提升59.16%，任务执行成功率提高17.25%，显著优于现有方法。

Conclusion: AEGIS为VLA模型提供了兼顾安全性与任务性能的通用增强方案，具备理论保障与实际有效性，并开源代码、模型与数据集以推动后续研究。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.

</details>


### [281] [Data-driven Interpretable Hybrid Robot Dynamics](https://arxiv.org/abs/2512.11900)
*Christopher E. Mower,Rui Zong,Haitham Bou-Ammar*

Main category: cs.RO

TL;DR: 本文提出了一种数据驱动的可解释混合机器人动力学建模方法，通过符号回归和SINDy从关节空间数据中学习残差力矩的简洁闭式表达式，在仿真和真实机器人数据上均展现出高精度、强泛化能力和物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统黑箱模型（如神经网络）缺乏物理可解释性且易过拟合，而纯解析模型难以精确刻画真实机器人中的未建模动态；因此需要兼顾准确性、可解释性与泛化能力的动力学建模新方法。

Method: 结合符号回归与稀疏识别非线性动力学（SINDy），在已知刚体动力学模型基础上，从关节空间数据中学习并识别出残差力矩的紧凑闭式表达式。

Result: 在Franka仿真平台上，所提方法以极小相对误差准确恢复惯性、科里奥利、重力和粘性效应，精度与泛化性均优于神经网络基线；在WAM真实机器人数据上，符号回归残差模型泛化显著优于SINDy和神经网络，并提示了扩展名义模型的新闭式候选公式。

Conclusion: 可解释的残差动力学模型为力矩预测提供了紧凑、准确且具物理意义的黑箱替代方案。

Abstract: We study data-driven identification of interpretable hybrid robot dynamics, where an analytical rigid-body dynamics model is complemented by a learned residual torque term. Using symbolic regression and sparse identification of nonlinear dynamics (SINDy), we recover compact closed-form expressions for this residual from joint-space data. In simulation on a 7-DoF Franka arm with known dynamics, these interpretable models accurately recover inertial, Coriolis, gravity, and viscous effects with very small relative error and outperform neural-network baselines in both accuracy and generalization. On real data from a 7-DoF WAM arm, symbolic-regression residuals generalize substantially better than SINDy and neural networks, which tend to overfit, and suggest candidate new closed-form formulations that extend the nominal dynamics model for this robot. Overall, the results indicate that interpretable residual dynamics models provide compact, accurate, and physically meaningful alternatives to black-box function approximators for torque prediction.

</details>


### [282] [Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics](https://arxiv.org/abs/2512.11903)
*Iacopo Catalano,Eduardo Montijano,Javier Civera,Julio A. Placed,Jorge Pena-Queralta*

Main category: cs.RO

TL;DR: 本文提出Aion框架，将时间动态性嵌入分层3D场景图（3DSG）中，通过图结构的稀疏动态地图（MoD）表示，在任意时间间隔内建模运动流，并将其关联到场景图的导航节点上，从而提升复杂动态环境中的规划与交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图（3DSG）缺乏对环境动态性的有效建模，而动态地图（MoD）又受限于网格离散化、语义缺失和可扩展性差；亟需一种兼具语义层次性与时间建模能力的统一表征方法。

Method: 提出Aion框架，将稀疏图结构的Maps of Dynamics（MoD）嵌入分层3D场景图中，使运动流直接附着于场景图的导航节点，支持任意时间间隔的动态建模。

Result: 实现了更可解释、更具可扩展性的动态预测，在复杂动态环境中提升了导航规划与人机/物间交互性能。

Conclusion: Aion成功融合了3DSG的语义层次结构与MoD的时间规律建模优势，为动态环境自主导航提供了兼具表达力与实用性的新范式。

Abstract: Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.

</details>


### [283] [Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models](https://arxiv.org/abs/2512.11908)
*Heng Zhang,Rui Dai,Gokhan Solak,Pokuang Zhou,Yu She,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文综述了面向接触丰富任务的安全学习型机器人控制方法，涵盖安全探索与安全执行两大方向，并探讨其与视觉-语言-动作（VLA）等基础模型的结合及带来的新机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 接触丰富的任务因不确定性高、动力学复杂、易造成损伤，使得机器人安全学习控制面临巨大挑战；现有学习方法虽具潜力，但安全保障仍是实际部署的关键瓶颈。

Method: 对现有安全学习方法进行系统分类（安全探索/安全执行），综述约束强化学习、风险敏感优化、不确定性建模、控制屏障函数、模型预测安全盾等技术，并分析其与VLM/VLA基础模型的融合机制。

Result: 梳理出安全学习在接触丰富任务中的技术脉络，明确VLM/VLA带来的语言级约束表达、多模态安全信号感知等新能力，同时指出其引发的新风险与评估难题。

Conclusion: 当前方法仍存在安全性、泛化性与可验证性等局限；未来需发展更可靠、安全对齐、并能协同基础模型的机器人系统。

Abstract: Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.

</details>


### [284] [Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control](https://arxiv.org/abs/2512.11921)
*Abdullah Yahya Abdullah Omaisan,Ibrahim Sheikh Mohamed*

Main category: cs.RO

TL;DR: 本文提出了一种基于LoRA和量化技术的高效微调方法，使数十亿参数的视觉-语言-动作（VLA）模型可在8GB显存的消费级GPU上运行，并成功部署于低成本SO101机械臂完成按钮按压任务。


<details>
  <summary>Details</summary>
Motivation: 大型VLA模型难以在计算资源受限的低成本机器人平台上部署，且需在少量示范数据下适配新机器人本体。

Method: 采用低秩自适应（LoRA）与模型量化技术进行资源高效微调，系统分析冻结/解冻视觉编码器对性能的影响，并在SO101机械臂上开展真实世界实验。

Result: 3.1B参数VLA模型可在8GB VRAM GPU上实时运行；仅用200个演示片段即实现有效按钮按压操作；明确了数据量与真实性能的关系及典型失败模式。

Conclusion: 通过合理的微调策略，大型VLA模型可高效适配低成本机器人平台，推动先进操作能力向更广泛实际场景落地。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.

</details>


### [285] [A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach](https://arxiv.org/abs/2512.11944)
*Jia Hu,Yang Chang,Haoran Wang*

Main category: cs.RO

TL;DR: 本文提出了一种数据驱动的最优控制范式，旨在融合经典控制的可验证性与机器学习的适应性，以解决自动驾驶运动规划中可解释性与鲁棒性之间的根本权衡问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶运动规划面临管道式方法（透明但脆弱）与学习式方法（自适应但不透明）之间的根本权衡，阻碍了真正可信系统的开发。

Method: 通过系统综述学习式运动规划方法，提出数据驱动的最优控制范式，将经典控制结构与机器学习结合，利用真实数据持续优化系统动力学、代价函数和安全约束。

Result: 该框架支持三大下一代能力：'以人为本'的定制化、'平台自适应'的动力学适配、以及'系统自优化'的自调优。

Conclusion: 该范式为构建兼具安全性、可解释性与类人自主性的智能交通系统提供了统一路径，并指明了未来研究方向。

Abstract: Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, "black-box" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: "Human-Centric" customization, "Platform-Adaptive" dynamics adaptation, and "System Self-Optimization" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.

</details>


### [286] [Modified Hybrid A* Collision-Free Path-Planning for Automated Reverse Parking](https://arxiv.org/abs/2512.12021)
*Xincheng Cao,Haochong Chen,Bilin Aksun-Guvenc,Levent Guvenc*

Main category: cs.RO

TL;DR: 本文提出了一种改进的Hybrid-A*路径规划算法，结合运动学单轨模型与膨胀二值占据地图，实现紧空间泊车中的可行且无碰撞轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 在狭小空间中泊车困难，主要由于可行且无碰撞路径稀缺。

Method: 采用改进的Hybrid-A*算法，引入基于低速运动学单轨模型的运动模型生成运动基元，并利用该模型重构车辆中心线，结合膨胀二值占据地图实现静态障碍物避碰。

Result: 仿真与动画验证表明，该算法能稳定生成运动学可行且无碰撞的泊车轨迹。

Conclusion: 所提方法有效提升了狭小空间下自动泊车路径规划的可行性与安全性。

Abstract: Parking a vehicle in tight spaces is a challenging task to perform due to the scarcity of feasible paths that are also collision-free. This paper presents a strategy to tackle this kind of maneuver with a modified Hybrid-A* path-planning algorithm that combines the feasibility guarantee inherent in the standard Hybrid A* algorithm with the addition of static obstacle collision avoidance. A kinematic single-track model is derived to describe the low-speed motion of the vehicle, which is subsequently used as the motion model in the Hybrid A* path-planning algorithm to generate feasible motion primitive branches. The model states are also used to reconstruct the vehicle centerline, which, in conjunction with an inflated binary occupancy map, facilitates static obstacle collision avoidance functions. Simulation study and animation are set up to test the efficacy of the approach, and the proposed algorithm proves to consistently provide kinematically feasible trajectories that are also collision-free.

</details>


### [287] [A Stochastic Approach to Terrain Maps for Safe Lunar Landing](https://arxiv.org/abs/2512.12058)
*Anja Sheppard,Chris Reale,Katherine A. Skinner*

Main category: cs.RO

TL;DR: 本文提出了一种基于高斯过程（GP）的两阶段随机高程地图生成方法，利用月球勘测轨道飞行器（LRO）数据及DEM置信度图建模异方差传感器噪声，从而提升月球南极着陆区域地形不确定性估计的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 月球南极光照条件差，传统视觉避障不可靠；LiDAR在月面环境未经充分验证；LRO积累了大量表面数据但未被有效用于带不确定性估计的地形建模；现有方法忽略DEM置信度图中蕴含的关键噪声质量信息。

Method: 提出两阶段高斯过程模型：第一阶段用次级GP从DEM置信度图学习空间变化的噪声特性（即异方差性），第二阶段将该噪声信息作为先验，指导主GP对月表高程建模；采用随机变分GP实现可扩展训练。

Result: 所提方法能更准确刻画异方差传感器噪声对高程图的影响，生成更具信息量的地形不确定性估计，支持下游任务如危险识别与安全着陆点选择。

Conclusion: 融合LRO DEM置信度信息的两阶段GP建模显著提升了月面随机高程地图的可靠性与实用性，为高风险自主航天任务中的不确定性感知提供了新范式。

Abstract: Safely landing on the lunar surface is a challenging task, especially in the heavily-shadowed South Pole region where traditional vision-based hazard detection methods are not reliable. The potential existence of valuable resources at the lunar South Pole has made landing in that region a high priority for many space agencies and commercial companies. However, relying on a LiDAR for hazard detection during descent is risky, as this technology is fairly untested in the lunar environment.
  There exists a rich log of lunar surface data from the Lunar Reconnaissance Orbiter (LRO), which could be used to create informative prior maps of the surface before descent. In this work, we propose a method for generating stochastic elevation maps from LRO data using Gaussian processes (GPs), which are a powerful Bayesian framework for non-parametric modeling that produce accompanying uncertainty estimates. In high-risk environments such as autonomous spaceflight, interpretable estimates of terrain uncertainty are critical. However, no previous approaches to stochastic elevation mapping have taken LRO Digital Elevation Model (DEM) confidence maps into account, despite this data containing key information about the quality of the DEM in different areas.
  To address this gap, we introduce a two-stage GP model in which a secondary GP learns spatially varying noise characteristics from DEM confidence data. This heteroscedastic information is then used to inform the noise parameters for the primary GP, which models the lunar terrain. Additionally, we use stochastic variational GPs to enable scalable training. By leveraging GPs, we are able to more accurately model the impact of heteroscedastic sensor noise on the resulting elevation map. As a result, our method produces more informative terrain uncertainty, which can be used for downstream tasks such as hazard detection and safe landing site selection.

</details>


### [288] [B-ActiveSEAL: Scalable Uncertainty-Aware Active Exploration with Tightly Coupled Localization-Mapping](https://arxiv.org/abs/2512.12194)
*Min-Won Seo,Aamodh Suresh,Carlos Nieto-Granda,Solmaz S. Kia*

Main category: cs.RO

TL;DR: 本文提出B-ActiveSEAL框架，通过引入行为熵（BE）和自适应平衡地图与定位不确定性，在大规模长期探索中实现可扩展、信息论驱动的主动探索。


<details>
  <summary>Details</summary>
Motivation: 传统主动探索方法在大规模环境中难以处理定位与建图之间紧密耦合的不确定性，导致计算不可行。

Method: 提出B-ActiveSEAL框架，基于广义熵度量（特别是新定义的行为熵BE），自适应权衡探索（地图不确定性）与利用（定位不确定性），并建立耦合不确定性传播与熵集成的理论基础。

Result: 在开源地图与ROS-Unity仿真中验证了该方法的有效性，相比基线方法展现出更优的探索-利用平衡性及跨环境的自适应行为多样性。

Conclusion: B-ActiveSEAL为紧耦合定位-建图下的不确定性感知主动探索提供了可扩展、理论严谨且实践有效的解决方案。

Abstract: Active robot exploration requires decision-making processes that integrate localization and mapping under tightly coupled uncertainty. However, managing these interdependent uncertainties over long-term operations in large-scale environments rapidly becomes computationally intractable. To address this challenge, we propose B-ActiveSEAL, a scalable information-theoretic active exploration framework that explicitly accounts for coupled uncertainties-from perception through mapping-into the decision-making process. Our framework (i) adaptively balances map uncertainty (exploration) and localization uncertainty (exploitation), (ii) accommodates a broad class of generalized entropy measures, enabling flexible and uncertainty-aware active exploration, and (iii) establishes Behavioral entropy (BE) as an effective information measure for active exploration by enabling intuitive and adaptive decision-making under coupled uncertainties. We establish a theoretical foundation for propagating coupled uncertainties and integrating them into general entropy formulations, enabling uncertainty-aware active exploration under tightly coupled localization-mapping. The effectiveness of the proposed approach is validated through rigorous theoretical analysis and extensive experiments on open-source maps and ROS-Unity simulations across diverse and complex environments. The results demonstrate that B-ActiveSEAL achieves a well-balanced exploration-exploitation trade-off and produces diverse, adaptive exploration behaviors across environments, highlighting clear advantages over representative baselines.

</details>


### [289] [Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion](https://arxiv.org/abs/2512.12203)
*Eric J. Elias,Michael Esswein,Jonathan P. How,David W. Miller*

Main category: cs.RO

TL;DR: 本文提出了一种可见光与热红外图像像素级融合方法，用于提升在光照条件多变（如阴影、日食）环境下对空间目标的单目SLAM导航精度，实验表明融合图像显著优于单一模态图像。


<details>
  <summary>Details</summary>
Motivation: 传统可见光相机在阴影或日食期间失效，而激光雷达虽鲁棒但笨重耗能；热红外相机可全天候工作但分辨率和特征丰富度低，亟需兼顾鲁棒性与特征质量的导航方案。

Method: 基于低地球轨道卫星的可见光与热红外波段图像进行物理真实感仿真，并采用像素级融合生成复合图像；随后在不同光照与轨迹条件下，对比单目SLAM算法在可见光、热红外及融合图像上的导航误差。

Result: 融合图像显著降低了SLAM导航误差，在各类挑战性光照与运动场景下均优于单一可见光或热红外输入。

Conclusion: 可见光与热红外图像的像素级融合是提升未知空间目标附近鲁棒、高精度相对导航的有效途径，尤其适用于光照条件不可控的在轨操作任务。

Abstract: As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.

</details>


### [290] [Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving](https://arxiv.org/abs/2512.12211)
*Longchao Da,David Isele,Hua Wei,Manish Saroya*

Main category: cs.RO

TL;DR: 本文提出了一种面向自动驾驶的轨迹预测器评估新范式，兼顾准确性与多样性，并根据场景关键性动态加权，从而更真实反映预测器对车辆驾驶性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于误差（如ADE、FDE）的评估指标仅反映后验精度，忽视预测结果对自动驾驶决策（尤其是谨慎交互决策）的实际影响，尤其无法衡量预测分布的合理性与覆盖性。

Method: 构建一个自适应双维度评估流水线，从准确性（accuracy）和多样性（diversity）两个方面联合评估轨迹预测器；依据驾驶场景的关键性动态调整二者权重，生成综合评分。

Result: 在基于真实数据的闭环评测基准上验证，该评估流水线比传统指标更能准确反映预测器性能与自动驾驶车辆实际行驶表现之间的相关性。

Conclusion: 所提评估方法为选择真正提升自动驾驶系统性能的轨迹预测器提供了更鲁棒、更具实际意义的依据。

Abstract: Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.

</details>


### [291] [Semantic Zone based 3D Map Management for Mobile Robot](https://arxiv.org/abs/2512.12228)
*Huichang Yun,Seungho Yoo*

Main category: cs.RO

TL;DR: 本文提出了一种基于语义区域的3D地图管理方法，用于解决大型室内环境中移动机器人内存受限下的高效SLAM问题。该方法将环境划分为语义区域（如大厅、走廊），以区域为单位进行工作内存与长期内存的动态调度，显著减少内存占用和签名加载/卸载次数，并在RTAB-Map框架中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型室内环境中3D地图内存开销大，传统基于几何或时间的内存管理在空间分隔环境中效率低，难以满足有限计算资源下的实时SLAM需求。

Method: 提出语义区域划分机制，将环境划分为有意义的空间单元（如大厅、走廊），以语义区域为基本单位进行内存管理；动态将任务相关区域加载至工作内存（WM），非活跃区域移至长期内存（LTM），并集成到RTAB-Map框架中实现。

Result: 相比标准方法，显著减少了不必要的签名加载/卸载次数和累计内存使用量，实现了稳定、可预测的内存占用，同时保障导航所需的全局地图可用性。

Conclusion: 语义区域驱动的地图管理范式优于传统几何/时间驱动方式，能更高效地平衡内存约束与地图完整性，适用于医院、物流中心等复杂室内场景的移动机器人系统。

Abstract: Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations. However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources. Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments. To address this, we propose a semantic zone-based 3D map management method that shifts the paradigm from geometry-centric to semantics-centric control. Our approach partitions the environment into meaningful spatial units (e.g., lobbies, hallways) and designates these zones as the primary unit for memory management. By dynamically loading only task-relevant zones into Working Memory (WM) and offloading inactive zones to Long-Term Memory (LTM), the system strictly enforces user-defined memory thresholds. Implemented within the RTAB-Map framework, our method demonstrates substantial reductions in unnecessary signature load/unload cycles and cumulative memory utilization compared to standard approaches. The results confirm that semantic zone-based management ensures stable, predictable memory usage while preserving map availability for navigation. Code is available at: https://github.com/huichangs/rtabmap/tree/segment

</details>


### [292] [Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy](https://arxiv.org/abs/2512.12230)
*Jonathan Spraggett*

Main category: cs.RO

TL;DR: 本文提出了一种基于CrossQ的单一深度强化学习策略，可实现对七种不同形态人形机器人的零样本跌倒恢复，无需为每种机器人单独训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有跌倒恢复方法需为每种机器人形态单独训练策略，缺乏泛化能力，难以适应动态比赛环境（如RoboCup）中多形态机器人快速部署的需求。

Method: 采用CrossQ算法训练一个统一的深度强化学习策略，并通过留一法实验、形态缩放分析和多样性消融实验验证其零样本迁移能力。

Result: 该统一策略在未见过的机器人形态上实现86±7%的零样本迁移成功率（95%置信区间[81, 89]），部分情况下甚至优于专用策略。

Conclusion: 形态无关的控制策略在跌倒恢复任务中具备实用性和可行性，为通用人形机器人控制奠定了基础。

Abstract: Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup

</details>


### [293] [Robust Underwater Localization of Buoyancy Driven microFloats Using Acoustic Time-of-Flight Measurements](https://arxiv.org/abs/2512.12233)
*Murad Mehrab Abrar,Trevor W. Harrison*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、鲁棒的水下浮标定位流水线，采用双向声学飞行时间（ToF）测量，结合非线性三边测量与基于几何代价和CRLB的位置滤波，显著降低定位误差（均值从139.29 m降至12.07 m），并在Puget Sound实地部署中验证了中位误差<4 m。


<details>
  <summary>Details</summary>
Motivation: 低成本自主水下平台需要高频率、高精度的定位，但现有方法易受多径效应和声学误差影响，鲁棒性不足。

Method: 提出双向声学飞行时间（ToF）定位框架（含float-to-buoy和buoy-to-float传输），结合非线性三边测量与基于几何代价和Cramer-Rao下界（CRLB）的位置滤波，剔除异常值；同时拓展支持未回收浮标的时差到达（TDoA）定位。

Result: 实地实验中定位中位误差低于4 m；滤波使均值误差从139.29 m降至12.07 m，轨迹与GPS路径对齐度显著提升；成功实现对未回收浮标的TDoA定位。

Conclusion: 通过算法层面优化（双向ToF、CRLB与几何滤波），可在不依赖硬件升级前提下显著提升声学定位的频率、精度与鲁棒性，适用于低成本水下浮标系统。

Abstract: Accurate underwater localization remains a challenge for inexpensive autonomous platforms that require highfrequency position updates. In this paper, we present a robust, low-cost localization pipeline for buoyancy-driven microFloats operating in coastal waters. We build upon previous work by introducing a bidirectional acoustic Time-of-Flight (ToF) localization framework, which incorporates both float-to-buoy and buoy-to-float transmissions, thereby increasing the number of usable measurements. The method integrates nonlinear trilateration with a filtering of computed position estimates based on geometric cost and Cramer-Rao Lower Bounds (CRLB). This approach removes outliers caused by multipath effects and other acoustic errors from the ToF estimation and improves localization robustness without relying on heavy smoothing. We validate the framework in two field deployments in Puget Sound, Washington, USA. The localization pipeline achieves median positioning errors below 4 m relative to GPS positions. The filtering technique shows a reduction in mean error from 139.29 m to 12.07 m, and improved alignment of trajectories with GPS paths. Additionally, we demonstrate a Time-Difference-of-Arrival (TDoA) localization for unrecovered floats that were transmitting during the experiment. Range-based acoustic localization techniques are widely used and generally agnostic to hardware-this work aims to maximize their utility by improving positioning frequency and robustness through careful algorithmic design.

</details>


### [294] [CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement](https://arxiv.org/abs/2512.12243)
*HT To,S Nguyen,NH Pham*

Main category: cs.RO

TL;DR: 本文提出CAR-CHASE方法，通过冲突感知的启发式缓存与自适应混合启发式计算，显著提升面向类车机器人多智能体路径规划（MAPF）算法CL-CBS的效率与成功率，同时保证最优性。


<details>
  <summary>Details</summary>
Motivation: 传统启发式缓存假设启发式值仅依赖于状态，但在CBS中因冲突解析引入的约束使搜索空间具有上下文依赖性，导致缓存失效且启发式计算开销大。

Method: 提出CAR-CHASE：（1）基于状态与相关约束上下文的冲突感知启发式缓存；（2）紧凑的‘冲突指纹’编码影响启发式的约束；（3）融合空间、时间与几何准则的相关性过滤器；（4）具理论质量保证的自适应启发式切换策略。

Result: 在480个基准实例上，几何平均加速2.46×，成功率从77.9%提升至84.8%，总运行时间减少70.1%，额外求解33个超时实例；复杂场景下最高达4.06×加速。

Conclusion: CAR-CHASE有效缓解CL-CBS中启发式计算瓶颈，在保持最优性前提下大幅提升效率与鲁棒性，且技术可推广至其他CBS变体。

Abstract: Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\% and 50\%) demonstrates a geometric mean speedup of 2.46$\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\% to 84.8\% (+6.9 percentage points), reduce total runtime by 70.1\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.

</details>


### [295] [Programmable Deformation Design of Porous Soft Actuator through Volumetric-Pattern-Induced Anisotropy](https://arxiv.org/abs/2512.12320)
*Canqi Meng,Weibang Bai*

Main category: cs.RO

TL;DR: 本文提出了一种通过在多孔泡沫基体上切割特定图案来实现可编程变形的软体多孔执行器新设计方法，利用局部结构各向异性引导真空驱动下的变形，并验证了弯曲、倾斜和扭转等多种运动模式，最终应用于仿生机器人手。


<details>
  <summary>Details</summary>
Motivation: 传统软体气动执行器结构支撑弱、多功能需几何重设计；多孔材料虽能增强稳定性，但尚缺乏对多孔体本身进行可编程变形调控的方法。

Method: 在圆柱形泡沫基体上设计并切割三种基本图案（横向、纵向、斜向），引入局部结构各向异性；结合有限元分析（FEA）建模，并通过实验优化图案数量N以实现不同模态变形；采用无模具快速成型实现复杂图案转移与尺度扩展。

Result: 实验实现最大弯曲80°（N=2）、倾斜18°（N=1）、扭转115°（N=8）；成功将人手褶皱图转化为切口图案，构建出具备类人自适应抓取能力的仿生软体机械手。

Conclusion: 该方法为多模态软体多孔机器人提供了一种新颖、高效且可扩展的设计范式，兼具图案可迁移性、可扩展性与无模具快速原型制造能力。

Abstract: Conventional soft pneumatic actuators, typically based on hollow elastomeric chambers, often suffer from small structural support and require costly geometry-specific redesigns for multimodal functionality. Porous materials such as foam, filled into chambers, can provide structural stability for the actuators. However, methods to achieve programmable deformation by tailoring the porous body itself remain underexplored. In this paper, a novel design method is presented to realize soft porous actuators with programmable deformation by incising specific patterns into the porous foam body. This approach introduces localized structural anisotropy of the foam guiding the material's deformation under a global vacuum input. Furthermore, three fundamental patterns on a cylindrical foam substrate are discussed: transverse for bending, longitudinal for tilting, and diagonal for twisting. A computational model is built with Finite Element Analysis (FEA), to investigate the mechanism of the incision-patterning method. Experiments demonstrate that with a potential optimal design of the pattern array number N, actuators can achieve bending up to $80^{\circ}$ (N=2), tilting of $18^{\circ}$ (N=1), and twisting of $115^{\circ}$ (N=8). The versatility of our approach is demonstrated via pattern transferability, scalability, and mold-less rapid prototyping of complex designs. As a comprehensive application, we translate the human hand crease map into a functional incision pattern, creating a bio-inspired soft robot hand capable of human-like adaptive grasping. Our work provides a new, efficient, and scalable paradigm for the design of multi-functional soft porous robots.

</details>


### [296] [INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset](https://arxiv.org/abs/2512.12377)
*Haichuan Li,Changda Tian,Panos Trahanias,Tomi Westerlund*

Main category: cs.RO

TL;DR: INDOOR-LIDAR是一个融合仿真与真实数据的室内3D LiDAR点云混合数据集，旨在解决现有数据集规模小、标注不一致和人为采集偏差等问题，支持3D检测、BEV感知、SLAM、语义理解及仿真-真实域适应等任务。


<details>
  <summary>Details</summary>
Motivation: 现有室内LiDAR数据集存在规模有限、标注格式不统一、人为采集变异大等问题，制约机器人感知研究的发展。

Method: 构建了一个包含仿真环境生成和自主地面机器人实采数据的混合数据集，统一采用KITTI风格标注，涵盖密度、遮挡、传感器噪声等多维度可控变量。

Result: 提供了大规模、高一致性、带强度信息的密集点云数据，覆盖多种室内场景和常见物体类别，并支持多项下游任务评估。

Conclusion: INDOOR-LIDAR弥合了仿真与真实数据之间的鸿沟，为复杂室内环境下机器人感知研究提供了可扩展、真实且可复现的新基准。

Abstract: We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird's-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.

</details>


### [297] [Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models](https://arxiv.org/abs/2512.12427)
*Rudolf Reiter,Chao Qin,Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 本文提出Unique，一种统一的模型预测控制（MPC）框架，通过在同一优化中级联不同保真度模型（短时高保真用于精确控制，长时低保真用于规划），结合多尺度约束对齐、过渡约束设计、3D渐进平滑障碍处理及并行随机初始化求解策略，在仿真与实机飞行中显著提升闭环跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 许多四旋翼空中任务既需要即时响应能力，又需长时序规划能力；现有方法在高保真模型（精度高但计算慢）与低保真模型（可扩展但闭环性能差）之间难以兼顾。

Method: 提出级联多保真度MPC框架Unique：1）短时高保真动力学模型+长时低保真点质量模型联合优化；2）跨时域成本对齐；3）为点质量模型推导保持可行性的推力与角速率约束；4）引入状态、推力加速度与角速率-加加速度关系的过渡约束；5）设计3D渐进平滑障碍函数；6）采用并行随机初始化MPC求解器提升全局收敛性。

Result: 在相同计算预算下，Unique在仿真和真实飞行中相较标准MPC及分层规划-跟踪基线，位置或速度闭环跟踪误差降低最高达75%；消融实验与Pareto分析验证其在不同预测时域、约束近似与平滑策略下的鲁棒增益。

Conclusion: Unique成功弥合了实时控制精度与长时规划能力之间的鸿沟，为复杂动态环境下的四旋翼自主导航提供了高效、鲁棒且可部署的统一控制架构。

Abstract: Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.

</details>


### [298] [Sim2Real Reinforcement Learning for Soccer skills](https://arxiv.org/abs/2512.12437)
*Jonathan Spraggett*

Main category: cs.RO

TL;DR: 本文提出了一种结合课程学习和对抗运动先验（AMP）的强化学习方法，提升了人形机器人在踢球、行走和跳跃等控制任务中的动态性与适应性，但在仿真到真实世界的策略迁移上仍未成功。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在适应真实环境、复杂性和自然运动方面存在局限，需要更高效有效的训练方法。

Method: 采用课程学习和对抗运动先验（AMP）技术来训练人形机器人的控制策略。

Result: 所学策略在踢球、行走和跳跃任务中表现出更强的动态性和适应性，并优于先前方法。

Conclusion: 该方法提升了仿真环境中控制策略的性能，但策略到真实世界的迁移仍未解决，揭示了当前强化学习在真实场景适应上的局限性。

Abstract: This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.

</details>


### [299] [Autonomously Unweaving Multiple Cables Using Visual Feedback](https://arxiv.org/abs/2512.12468)
*Tina Tian,Xinyu Wang,Andrew L. Orekhov,Fujun Ruan,Lu Li,Oliver Kroemer,Howie Choset*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉反馈的多电缆解缠方法，将解缠建模为图节点上的抓取-放置问题，结合拓扑与几何信息的状态表示及考虑电缆形变的状态转移模型，实现了84%平均成功率的多电缆分离。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注单根电缆解结，而实际电缆管理中常需处理多根相互缠绕的电缆，亟需解决多电缆解缠（multi-cable unweaving）这一更具实用性的子任务。

Method: 构建基于图的电缆状态表示，融合视觉提取的拓扑与几何信息；设计考虑电缆弯曲与拉直特性的状态转移模型；将解缠建模为pick-and-place问题，在图节点上选择抓取位点，并结合两种高层动作原语与预测即时代价优化底层操作。

Result: 在电动电缆和鞋带上的实验表明该方法平均成功率达84%。

Conclusion: 所提方法有效解决了多电缆解缠问题，验证了融合视觉感知、拓扑几何建模与形变感知规划的可行性，为柔性物体自动化操作提供了新思路。

Abstract: Many cable management tasks involve separating out the different cables and removing tangles. Automating this task is challenging because cables are deformable and can have combinations of knots and multiple interwoven segments. Prior works have focused on untying knots in one cable, which is one subtask of cable management. However, in this paper, we focus on a different subtask called multi-cable unweaving, which refers to removing the intersections among multiple interwoven cables to separate them and facilitate further manipulation. We propose a method that utilizes visual feedback to unweave a bundle of loosely entangled cables. We formulate cable unweaving as a pick-and-place problem, where the grasp position is selected from discrete nodes in a graph-based cable state representation. Our cable state representation encodes both topological and geometric information about the cables from the visual image. To predict future cable states and identify valid actions, we present a novel state transition model that takes into account the straightening and bending of cables during manipulation. Using this state transition model, we select between two high-level action primitives and calculate predicted immediate costs to optimize the lower-level actions. We experimentally demonstrate that iterating the above perception-planning-action process enables unweaving electric cables and shoelaces with an 84% success rate on average.

</details>


### [300] [Optimized Conflict Management for Urban Air Mobility Using Swarm UAV Networks](https://arxiv.org/abs/2512.12632)
*Rishit Agnihotri,Sandeep Kumar Sharma*

Main category: cs.RO

TL;DR: 本文提出了一种基于边缘AI的去中心化无人机集群架构，利用轻量级神经网络在边缘节点实现分布式冲突检测与解决，显著提升了城市空中交通管理的实时性与准确性。


<details>
  <summary>Details</summary>
Motivation: 随着城市空中交通（UAM）中无人机密度增加，传统集中式交通协调面临巨大挑战，亟需低延迟、高可扩展的智能冲突解决机制。

Method: 构建基于控制算法的数学模型，设计边缘AI驱动的去中心化 swarm 架构，采用轻量级神经网络在边缘节点执行分布式冲突检测与解决，并通过仿真平台在不同无人机密度下进行评估。

Result: 冲突解决时间最快提升3.8倍，准确率高于传统集中式控制模型。

Conclusion: 该边缘AI驱动的去中心化架构为未来可扩展、高效且安全的城市空中交通管理提供了极具前景的技术路径。

Abstract: Urban Air Mobility (UAM) poses unprecedented traffic coordination challenges, especially with increasing UAV densities in dense urban corridors. This paper introduces a mathematical model using a control algorithm to optimize an Edge AI-driven decentralized swarm architecture for intelligent conflict resolution, enabling real-time decision-making with low latency. Using lightweight neural networks, the system leverages edge nodes to perform distributed conflict detection and resolution. A simulation platform was developed to evaluate the scheme under various UAV densities. Results indicate that the conflict resolution time is dramatically minimized up to 3.8 times faster, and accuracy is enhanced compared to traditional centralized control models. The proposed architecture is highly promising for scalable, efficient, and safe aerial traffic management in future UAM systems.

</details>


### [301] [Bayesian Optimization Parameter Tuning Framework for a Lyapunov Based Path Following Controller](https://arxiv.org/abs/2512.12649)
*Zhewen Zheng,Wenjing Cao,Hongkang Yu,Mo Chen,Takashi Suzuki*

Main category: cs.RO

TL;DR: 本文提出了一种基于贝叶斯优化（BO）的控制器参数自动调优框架，用于在硬件受限、评估成本高昂的真实机器人平台上高效优化非线性路径跟踪控制器增益。该方法以高斯过程为代理模型，在仅32次试验（含15次预热）内即显著提升性能，验证了其数据高效性与实用性。


<details>
  <summary>Details</summary>
Motivation: 真实世界实验中硬件评估预算有限，而典型非线性几何控制器（如路径跟踪控制器）存在多增益耦合非线性关系，导致人工调参效率低、难以在有限试验次数内获得满意性能。

Method: 提出一种贝叶斯优化（BO）框架，将闭环系统视为黑箱，采用高斯过程（GP）构建 surrogate 模型，实现模型无关、不确定性可量化、数据高效的参数搜索。

Result: 在本田AI-Formula三轮机器人上实测，经32次全圈赛道试验（含15次warm-start），BO显著提升了控制器性能，快速定位到高性能参数区域。

Conclusion: 贝叶斯优化是一种实用、可靠且数据高效的真实机器人平台非线性路径跟踪控制器参数调优方法。

Abstract: Parameter tuning in real-world experiments is constrained by the limited evaluation budget available on hardware. The path-following controller studied in this paper reflects a typical situation in nonlinear geometric controller, where multiple gains influence the dynamics through coupled nonlinear terms. Such interdependence makes manual tuning inefficient and unlikely to yield satisfactory performance within a practical number of trials. To address this challenge, we propose a Bayesian optimization (BO) framework that treats the closed-loop system as a black box and selects controller gains using a Gaussian-process surrogate. BO offers model-free exploration, quantified uncertainty, and data-efficient search, making it well suited for tuning tasks where each evaluation is costly. The framework is implemented on Honda's AI-Formula three-wheeled robot and assessed through repeated full-lap experiments on a fixed test track. The results show that BO improves controller performance within 32 trials, including 15 warm-start initial evaluations, indicating that it can efficiently locate high-performing regions of the parameter space under real-world conditions. These findings demonstrate that BO provides a practical, reliable, and data-efficient tuning approach for nonlinear path-following controllers on real robotic platforms.

</details>


### [302] [HMPCC: Human-Aware Model Predictive Coverage Control](https://arxiv.org/abs/2512.12717)
*Mattia Catellani,Marta Gabbi,Lorenzo Sabattini*

Main category: cs.RO

TL;DR: 本文提出了一种基于模型预测控制（MPC）的人类感知覆盖框架HMPCC，通过在MPC中融合人类运动预测，实现多机器人在未知、动态环境中安全、自适应地协同覆盖，且采用去中心化策略，无需显式通信。


<details>
  <summary>Details</summary>
Motivation: 传统覆盖策略依赖环境已知、静态密度等简化假设，难以应对含非合作人类代理的真实动态场景，亟需兼顾安全性与适应性的新方法。

Method: 提出HMPCC框架：将人类轨迹预测嵌入MPC优化过程；用高斯混合模型（GMM）表征环境兴趣区域；机器人团队完全去中心化运行，不依赖显式通信。

Result: 实验表明，引入人类轨迹预测显著提升了覆盖效率与适应性，增强了人机协同效果，并验证了去中心化策略在通信受限或敌对环境下的可行性。

Conclusion: HMPCC为动态、人类共存环境中的多机器人覆盖任务提供了安全、鲁棒且可扩展的解决方案，凸显了人类行为建模与预测在自主系统规划中的关键作用。

Abstract: We address the problem of coordinating a team of robots to cover an unknown environment while ensuring safe operation and avoiding collisions with non-cooperative agents. Traditional coverage strategies often rely on simplified assumptions, such as known or convex environments and static density functions, and struggle to adapt to real-world scenarios, especially when humans are involved. In this work, we propose a human-aware coverage framework based on Model Predictive Control (MPC), namely HMPCC, where human motion predictions are integrated into the planning process. By anticipating human trajectories within the MPC horizon, robots can proactively coordinate their actions %avoid redundant exploration, and adapt to dynamic conditions. The environment is modeled as a Gaussian Mixture Model (GMM), representing regions of interest. Team members operate in a fully decentralized manner, without relying on explicit communication, an essential feature in hostile or communication-limited scenarios. Our results show that human trajectory forecasting enables more efficient and adaptive coverage, improving coordination between human and robotic agents.

</details>


### [303] [Making Robots Play by the Rules: The ROS 2 CLIPS-Executive](https://arxiv.org/abs/2512.12722)
*Tarik Viehmann,Daniel Swoboda,Samridhi Kalra,Himanshu Grover,Gerhard Lakemeyer*

Main category: cs.RO

TL;DR: 本文介绍了将CLIPS规则引擎集成到ROS（机器人操作系统）生态系统中的方法，并展示了其与PDDL规划框架的灵活集成，以支持自主机器人任务协调。


<details>
  <summary>Details</summary>
Motivation: 为解决自主机器人复杂任务协调问题，需将成熟的知识推理工具（如CLIPS）融入主流机器人开发平台（如ROS）。

Method: 将CLIPS规则引擎与ROS进行集成，并进一步实现与PDDL规划框架的协同。

Result: 成功构建了CLIPS在ROS中的集成方案，并验证了其在PDDL规划框架下的灵活性和适用性。

Conclusion: CLIPS可有效嵌入ROS生态，提升机器人系统在知识驱动任务协调方面的能力，尤其适用于需要符号推理与规划结合的场景。

Abstract: CLIPS is a rule-based programming language for building knowledge-driven applications, well suited for the complex task of coordinating autonomous robots. Inspired by the CLIPS-Executive originally developed for the lesser known Fawkes robotics framework, we present an Integration of CLIPS into the ROS ecosystem. Additionally, we show the flexibility of CLIPS by describing a PDDL-based planning framework integration.

</details>


### [304] [VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps](https://arxiv.org/abs/2512.12793)
*Mizuho Aoki,Kohei Honda,Yasuhiro Yoshimura,Takeshi Ishita,Ryo Yonetani*

Main category: cs.RO

TL;DR: 本文提出VLG-Loc，一种利用仅含地标名称与区域的可读足迹地图进行全局定位的新方法，通过视觉-语言模型在多向图像中检索地图地标，并在蒙特卡洛定位框架中评估位姿假设，实验表明其在环境变化下比基于扫描的方法更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 人类能自然地利用含地标名称和区域的简略地图进行自我定位，但机器人难以在缺乏几何和外观细节的地图中建立观测与地图地标间的对应关系。

Method: 利用视觉-语言模型（VLM）在机器人多方向图像观测中检索地图中标注的地标，并在蒙特卡洛定位框架中以检测到的地标评估各位姿假设的似然性；进一步融合视觉与扫描定位结果提升性能。

Result: 在仿真和真实零售环境中验证，VLG-Loc相比现有基于扫描的方法具有更强的环境变化鲁棒性；结合视觉与扫描定位的概率融合进一步提升了定位精度。

Conclusion: VLG-Loc成功将人类式语义地图定位能力迁移到机器人系统，为弱几何约束场景下的鲁棒全局定位提供了新范式。

Abstract: This paper presents Vision-Language Global Localization (VLG-Loc), a novel global localization method that uses human-readable labeled footprint maps containing only names and areas of distinctive visual landmarks in an environment. While humans naturally localize themselves using such maps, translating this capability to robotic systems remains highly challenging due to the difficulty of establishing correspondences between observed landmarks and those in the map without geometric and appearance details. To address this challenge, VLG-Loc leverages a vision-language model (VLM) to search the robot's multi-directional image observations for the landmarks noted in the map. The method then identifies robot poses within a Monte Carlo localization framework, where the found landmarks are used to evaluate the likelihood of each pose hypothesis. Experimental validation in simulated and real-world retail environments demonstrates superior robustness compared to existing scan-based methods, particularly under environmental changes. Further improvements are achieved through the probabilistic fusion of visual and scan-based localization.

</details>


### [305] [SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding](https://arxiv.org/abs/2512.12842)
*Kuan Fang,Yuxin Chen,Xinghao Zhu,Farzad Niroui,Lingfeng Sun,Jiuguang Wang*

Main category: cs.RO

TL;DR: SAGA是一个通用自适应的视觉运动控制框架，通过基于功能（affordance）的任务表示和多模态基础模型将任务目标与视觉观测在3D空间中对齐，实现跨环境、跨任务和跨用户指令的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在多样化环境、任务目标和用户输入形式下实现良好泛化；端到端或模块化方法常受限于外观变化或缺乏结构化语义理解。

Method: 提出基于功能的任务表示，并利用多模态基础模型生成3D功能热图，将高层语义意图与底层运动控制解耦；在此基础上训练条件策略，支持语言、点选、示例等多种任务指定方式。

Result: 在11个真实世界任务上，SAGA在四足操作机器人上显著优于端到端及模块化基线方法，支持零样本执行与少样本适应。

Conclusion: 结构化的功能对齐为通用移动操作提供了可扩展且有效的路径。

Abstract: We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.

</details>


### [306] [MPC-Guided Safe Reinforcement Learning and Lipschitz-Based Filtering for Structured Nonlinear Systems](https://arxiv.org/abs/2512.12855)
*Patrick Kostelac,Xuerui Wang,Anahita Jamshidnejad*

Main category: cs.RO

TL;DR: 本文提出了一种融合模型预测控制（MPC）与强化学习（RL）的框架，利用MPC在训练阶段提供安全控制边界以引导RL策略学习，并在部署时采用基于Lipschitz连续性的轻量级安全滤波器实现实时、安全、低计算开销的控制。


<details>
  <summary>Details</summary>
Motivation: 现代工程系统（如自动驾驶汽车、柔性机器人和智能航空航天平台）需要兼具鲁棒性、自适应性和实时安全性的控制器；而纯RL缺乏探索过程中的动态约束保障，纯MPC又受限于建模精度和在线优化计算负担。

Method: 构建MPC-RL联合框架：训练阶段由MPC生成安全控制界以约束并引导RL策略学习；部署阶段使用基于Lipschitz连续性的轻量级安全滤波器对已学RL策略输出进行实时修正，确保约束满足且无需重优化。

Result: 在非线性气动弹性机翼系统上验证，该方法提升了抗扰能力、降低了执行器能耗，并在湍流下保持鲁棒性能；且可推广至其他具有结构化非线性和有界扰动的系统。

Conclusion: 所提MPC-RL集成框架有效融合了MPC的安全性与RL的适应性，为工程领域中AI驱动的安全控制提供了可扩展、实时可行的新范式。

Abstract: Modern engineering systems, such as autonomous vehicles, flexible robotics, and intelligent aerospace platforms, require controllers that are robust to uncertainties, adaptive to environmental changes, and safety-aware under real-time constraints. RL offers powerful data-driven adaptability for systems with nonlinear dynamics that interact with uncertain environments. RL, however, lacks built-in mechanisms for dynamic constraint satisfaction during exploration. MPC offers structured constraint handling and robustness, but its reliance on accurate models and computationally demanding online optimization may pose significant challenges. This paper proposes an integrated MPC-RL framework that combines stability and safety guarantees of MPC with the adaptability of RL. During training, MPC defines safe control bounds that guide the RL component and that enable constraint-aware policy learning. At deployment, the learned policy operates in real time with a lightweight safety filter based on Lipschitz continuity to ensure constraint satisfaction without heavy online optimizations. The approach, which is validated on a nonlinear aeroelastic wing system, demonstrates improved disturbance rejection, reduced actuator effort, and robust performance under turbulence. The architecture generalizes to other domains with structured nonlinearities and bounded disturbances, offering a scalable solution for safe artificial-intelligence-driven control in engineering applications.

</details>


### [307] [SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework](https://arxiv.org/abs/2512.12945)
*Anja Sheppard,Parker Ewen,Joey Wilson,Advaith V. Sethuraman,Benard Adewole,Anran Li,Yuzhen Chen,Ram Vasudevan,Katherine A. Skinner*

Main category: cs.RO

TL;DR: 本文提出了SLIM-VDB，一种基于OpenVDB的轻量级3D语义映射系统，支持闭集与开集词典的统一概率语义融合，在内存与计算效率上显著优于现有方法，同时保持相当的映射精度。


<details>
  <summary>Details</summary>
Motivation: 现有语义映射系统未充分利用OpenVDB等高效体素数据结构，且缺乏对固定类别标签和开放语言标签预测的统一支持。

Method: 提出基于OpenVDB的3D语义映射框架SLIM-VDB，引入统一的贝叶斯更新机制，实现闭集与开集语义标签的概率融合。

Result: SLIM-VDB在内存占用和语义集成时间上显著降低，同时保持与当前最先进方法相当的映射精度。

Conclusion: SLIM-VDB为机器人场景理解提供了一种高效、灵活且开源的语义映射解决方案，推动了OpenVDB在语义映射领域的应用。

Abstract: This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.

</details>


### [308] [Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning](https://arxiv.org/abs/2512.12987)
*Amin Jalal Aghdasian,Farzaneh Abdollahi,Ali Kamali Iglie*

Main category: cs.RO

TL;DR: 本文提出了两种基于深度强化学习的车道保持系统算法（AR-RDPG和AR-CADPG），专为雪天道路条件设计，通过去噪、中心线提取、注意力机制与对抗鲁棒性提升路径跟踪精度与稳定性，并在CARLA仿真与实车（Jetson Nano）上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 针对自动驾驶车辆在雪天道路下因图像噪声、轮胎打滑导致的感知与控制不确定性问题，亟需具备鲁棒性的车道保持方法。

Method: 提出两种动作鲁棒型DRL算法：1）AR-RDPG：分层结构，先用多尺度神经网络对图像去噪，再用预训练DCNN提取中心线系数，结合驾驶特征输入RDPG控制层；2）AR-CADPG：端到端结构，将CNN与注意力机制嵌入DRL框架（Deterministic Policy Gradient）。两者均在CARLA中训练并在真实雪况下于Jetson Nano平台部署验证。

Result: AR-CADPG在路径跟踪精度与鲁棒性上优于AR-RDPG，验证了时序记忆、对抗鲁棒性与注意力机制协同的有效性；两模型均在仿真与实车实验中表现出可行性与稳定性。

Conclusion: 融合注意力机制与动作鲁棒设计的端到端DRL方法（AR-CADPG）更适用于复杂恶劣天气下的车道保持任务，为雪天自动驾驶提供了可靠控制新范式。

Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.

</details>


### [309] [Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations](https://arxiv.org/abs/2512.12993)
*Guillermo A. Castillo,Himanshu Lodha,Ayonga Hereid*

Main category: cs.RO

TL;DR: 本文提出了一种分层的地形感知双足行走策略，通过卷积变分自编码器（CNN-VAE）提取地形潜在表征，并结合降阶机器人动力学，构建紧凑状态以提升强化学习高层策略的实时性与鲁棒性；还引入历史感知机制和从深度图像直接学习潜在表示的蒸馏方法，并在高保真仿真与初步硬件实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升RL-based高层步态生成策略在真实复杂地形下的实时性、鲁棒性与硬件部署可行性，克服端到端方法状态空间冗余、泛化性差及仿真到现实迁移难的问题。

Method: 提出分层框架：1）用CNN-VAE学习地形的低维潜在编码；2）融合降阶动力学构建紧凑状态空间；3）引入时序历史增强潜在表示；4）设计蒸馏方法使潜在编码可直接由深度图像推断；5）在Agility Robotics高保真仿真器中集成传感器噪声、状态估计与执行器动力学进行验证，并开展初步硬件数据对比。

Result: 实验表明该方法在学习效率、策略鲁棒性及地形适应性方面显著优于基线；潜空间维度分析揭示了性能与维度间的权衡关系；历史感知与蒸馏模块进一步提升了泛化能力；仿真与真实传感器数据分布一致性得到初步验证。

Conclusion: 基于低维感知表征与分层控制的地形感知双足行走框架，兼顾学习效率、鲁棒性与部署可行性，为强化学习在具身智能体中的实际应用提供了有效路径。

Abstract: This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.

</details>


### [310] [K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots](https://arxiv.org/abs/2512.13009)
*Oğuzhan Akbıyık,Naseem Alhousani,Fares J. Abu-Dakka*

Main category: cs.RO

TL;DR: 本文提出K-VARK方法，将核化、概率化的关节残余力矩模型嵌入自适应卡尔曼滤波框架，通过核化运动基元建模残余力矩的均值与输入依赖的异方差性，并利用变分贝叶斯在线优化过程噪声协方差，显著提升无传感器接触力估计精度。


<details>
  <summary>Details</summary>
Motivation: 可靠估计接触力对机器人在非结构化环境中安全精准交互至关重要，但无传感器力估计因建模误差、复杂残余动力学和摩擦而极具挑战。

Method: 提出K-VARK（核化方差感知残余卡尔曼滤波），结合核化运动基元建模残余力矩的预测均值与输入依赖异方差性，并在卡尔曼滤波中实现方差感知的虚拟测量更新与基于变分贝叶斯的过程噪声协方差在线自适应。

Result: 在6自由度协作机械臂上实验表明，K-VARK相较现有最优无传感器力估计方法RMSE降低超20%，可稳健准确估计外部力/力矩，适用于抛光、装配等高级任务。

Conclusion: K-VARK通过融合核化概率建模与自适应卡尔曼滤波，有效缓解建模不确定性与动态扰动影响，为无传感器接触力估计提供了高精度、强鲁棒性的新范式。

Abstract: Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.

</details>


### [311] [Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos](https://arxiv.org/abs/2512.13080)
*Yicheng Feng,Wanpeng Zhang,Ye Wang,Hao Luo,Haoqi Yuan,Sipeng Zheng,Zongqing Lu*

Main category: cs.RO

TL;DR: 本文提出了一种空间感知的视觉-语言-动作（VLA）预训练范式，通过在预训练阶段显式对齐2D视觉空间与3D物理空间，提升机器人策略在真实3D环境中的动作接地能力；具体实现为VIPAVLA模型，采用双编码器结构并引入3D视觉编码器，在下游任务中显著增强了2D视觉到3D动作的鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型多依赖2D视觉输入指导3D环境中的动作，导致感知与动作接地之间存在显著鸿沟，亟需提升模型的3D空间理解能力。

Method: 提出空间感知VLA预训练范式：利用大规模人类演示视频提取3D视觉与3D动作标注，构建2D视觉与3D空间推理的对齐监督信号；基于此，设计VIPA-VLA双编码器架构，新增3D视觉编码器以增强语义视觉表征的空间感知能力。

Result: VIPA-VLA在下游机器人任务中显著提升了2D视觉到3D动作的接地性能，展现出更强的鲁棒性与任务泛化能力。

Conclusion: 显式建模和对齐2D视觉与3D物理空间是提升VLA模型机器人策略性能的关键路径，所提出的预训练范式与VIPA-VLA架构为具身智能提供了更可靠的三维接地基础。

Abstract: Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.

</details>


### [312] [Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion](https://arxiv.org/abs/2512.13090)
*Jebeom Chae,Junwoo Chang,Seungho Yeom,Yujin Kim,Jongeun Choi*

Main category: cs.RO

TL;DR: 本文提出了一种名为LCHD的语言条件热启发扩散模型，用于多机器人运动规划，通过结合CLIP语义先验与避碰扩散核，在无需显式障碍物信息的情况下生成语言驱动、无碰撞的轨迹，提升了泛化性与实时性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在多机器人、语言条件任务中受限：计算开销大、泛化能力差，且依赖显式环境建模和几何可达性推理。

Method: 提出LCHD框架：端到端视觉输入，融合CLIP语义先验与基于热方程思想的碰撞规避扩散核，将可达性作为物理归纳偏置，实现语言指令到可行轨迹的映射。

Result: 在多种仿真地图及真实机器人实验中，LCHD在成功率上优于现有扩散规划器，同时显著降低规划延迟。

Conclusion: LCHD通过语义-物理联合建模，实现了更鲁棒、高效、无需显式障碍建模的语言条件多机器人运动规划。

Abstract: Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.

</details>


### [313] [PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations](https://arxiv.org/abs/2512.13093)
*Mingqi Yuan,Tao Yu,Haolin Song,Bo Li,Xin Jin,Hua Chen,Wenjun Zeng*

Main category: cs.RO

TL;DR: 本文提出了一种名为PvP的本体感知-特权对比学习框架，用于提升人形机器人全身控制中的样本效率和策略稳定性，并构建了首个面向人形机器人的统一模块化状态表征学习评估框架SRL4Humanoid。


<details>
  <summary>Details</summary>
Motivation: 强化学习在人形机器人全身控制中存在样本效率低的问题，主要源于其复杂动力学和部分可观测性。

Method: 提出PvP框架，利用本体感知状态与特权状态的内在互补性进行对比学习，无需手工设计数据增强即可学习紧凑且任务相关的潜在表征；同时构建SRL4Humanoid评估框架。

Result: 在LimX Oli机器人上的速度跟踪与运动模仿任务中，PvP显著提升了样本效率和最终性能，优于现有SRL基线方法。

Conclusion: PvP有效缓解了人形机器人WBC中RL的样本低效问题，SRL4Humanoid为该领域提供了可复现、模块化的评估基准，研究结果为人形机器人数据高效学习提供了实践指导。

Abstract: Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.

</details>


### [314] [Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation](https://arxiv.org/abs/2512.13094)
*Xiang Li,Gang Liu,Weitao Zhou,Hongyi Zhu,Zhong Cao*

Main category: cs.RO

TL;DR: 本文提出了一种名为Sequence of Experts (SoE)的时间交替策略，通过在时间维度上交替使用多个专家模型来缓解模仿学习在闭环驾驶中因误差累积导致的性能下降问题，无需增加模型规模或数据需求，在nuPlan基准上显著提升了各类模型的性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在开环设置中表现良好，但在闭环驾驶中因小误差随时间累积而导致性能严重下降；现有方法多聚焦于单时刻状态鲁棒性，缺乏对连续时间过程的建模。

Method: 提出Sequence of Experts (SoE)方法，采用时间交替策略，在不同规划周期中动态切换多个专家模型，以抑制误差累积，不增加模型参数量和数据需求。

Result: 在nuPlan大规模自动驾驶基准上，SoE一致且显著提升所有被测模型的闭环性能，达到当前最优水平。

Conclusion: SoE提供了一种轻量、通用且高效的时间维度鲁棒性增强机制，可作为提升自动驾驶模仿学习训练效率的关键模块。

Abstract: Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.

</details>


### [315] [OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning](https://arxiv.org/abs/2512.13100)
*Guanhua Ji,Harsha Polavaram,Lawrence Yunliang Chen,Sandeep Bajamahal,Zehan Ma,Simeon Adebola,Chenfeng Xu,Ken Goldberg*

Main category: cs.RO

TL;DR: 本文提出AugE-Toolkit和OXE-AugE数据集，通过机器人形态增强扩展了Open X-Embodiment（OXE）数据集，显著提升通用机器人策略在跨形态、未见硬件及分布偏移场景下的泛化能力，并在真实世界任务中验证了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有通用机器人策略训练依赖大规模多样数据，但Open X-Embodiment（OXE）数据集高度不平衡（前四种机器人占85%以上），易导致对特定机器人-场景组合过拟合；重新采集每种新硬件的演示数据成本过高，亟需高效的数据增强方法实现跨形态迁移与泛化。

Method: 提出可扩展的机器人增强工具包AugE-Toolkit，基于OXE数据集生成涵盖9种新机器人本体（臂+夹爪组合）的增强数据，构建高质量开源数据集OXE-AugE；系统研究机器人形态增强规模对跨形态学习的影响，并在真实机器人上对OpenVLA和π₀等先进通用策略进行微调实验。

Result: OXE-AugE包含超440万条轨迹，规模超原始OXE三倍；增强后策略在增强机器人、未见机器人及原始机器人面临分布偏移时性能均提升；真实实验中，微调后策略在四个操作任务上对未见机器人-夹爪组合的成功率提升24–45%。

Conclusion: 机器人形态数据增强是一种高效、可扩展的途径，能显著提升通用机器人策略的跨硬件泛化能力，缓解数据不平衡问题，为构建真正通用的机器人智能奠定数据基础。

Abstract: Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.

</details>


### [316] [START: Traversing Sparse Footholds with Terrain Reconstruction](https://arxiv.org/abs/2512.13153)
*Ruiqi Yu,Qianshi Wang,Hongyi Li,Zheng Jun,Zhicheng Wang,Jun Wu,Qiuguo Zhu*

Main category: cs.RO

TL;DR: 本文提出START框架，利用低成本机载视觉与本体感知实现稀疏 foothold 地形上的敏捷稳定行走，通过单阶段学习重建局部高度图，提升环境理解与步态适应性，并在真实场景中实现零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的分层控制器泛化能力差、行为过于保守；端到端学习方法依赖噪声大、流程复杂的高度图，或仅靠深度图像隐式推断地形，缺乏关键几何线索，导致学习低效、步态僵硬。

Method: 提出START单阶段学习框架，仅使用机载视觉与本体感知数据，显式重建局部地形高度图作为中间表征，支撑精准地形评估与环境理解。

Result: START在多样化真实场景中实现零样本迁移，展现出优异的适应性、精确足落点控制和鲁棒运动能力。

Conclusion: START通过引入显式的、轻量级的高度图中间表示，在不牺牲灵活性的前提下显著提升了稀疏 foothold 地形下四足机器人运动的精度、稳定性与泛化能力。

Abstract: Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.

</details>


### [317] [Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks](https://arxiv.org/abs/2512.13170)
*Deepak Ingole,Valentin Bhend,Shiva Ganesh Murali,Oliver Dobrich,Alisa Rupenayan*

Main category: cs.RO

TL;DR: 本文提出了一种基于任务级性能反馈的迭代学习框架，用于自动调整非线性模型预测控制（NMPC）的权重矩阵Q和R，以优化跟踪精度、控制能耗与饱和等关键指标；该方法通过构建经验敏感度矩阵实现无需解析导数的结构化更新，并在UR10e机器人碳纤维缠绕任务中验证：仅需4次在线重复即收敛至接近贝叶斯优化（BO）离线100次评估的性能。


<details>
  <summary>Details</summary>
Motivation: 制造过程常受环境漂移与系统磨损干扰，即使在重复任务中也需频繁重调控制器参数；现有梯度法依赖NMPC求解器的可微性，计算复杂且不实用。

Method: 受范数最优迭代学习控制（ILC）启发，提出一种无需对NMPC求解器求导的迭代学习框架；通过任务重复采集性能数据，构建经验敏感度矩阵，实现Q/R权重的自适应结构化更新。

Result: 在UR10e机器人碳纤维缠绕仿真中，仅4次在线重复即达到RMSE误差低于离线贝叶斯优化结果0.3%的性能，而后者需100次离线评估。

Conclusion: 该方法为重复性机器人任务中的NMPC在线自适应调参提供了兼顾精度与实用性的新范式，兼具离线优化的精度与在线学习的灵活性。

Abstract: Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.

</details>


### [318] [Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification](https://arxiv.org/abs/2512.13183)
*Alfredo González-Calvin,Juan F. Jiménez,Héctor García de Marina*

Main category: cs.RO

TL;DR: 本文提出了一种基于磨光（mollification）技术的路径正则化方法，用于将连续但不可微的路径（如分段线性路径）近似为任意精度的光滑可微路径，并系统控制其曲率，从而兼容标准跟踪算法且适合实时嵌入式实现。


<details>
  <summary>Details</summary>
Motivation: 现有路径跟踪算法要求路径至少二阶连续可微，因而无法直接处理实际中常用的连续但不可微路径（如分段线性路径），而现有平滑方法（如样条插值或优化）存在轨迹过复杂或计算开销大等问题。

Method: 采用磨光（mollification）技术对任意给定路径进行正则化，构造出一致收敛于原路径的光滑可微近似函数；并提出系统化的曲率上界分析方法，特别应用于由航点连线构成的分段路径。

Result: 所生成的路径可任意精度逼近原路径、曲率有明确上界、计算高效，已在微控制器上实现实时运行，并能无缝接入标准路径跟踪与跟随算法。

Conclusion: 该方法在保持路径高层语义（如航点序列）的同时，解决了非光滑路径与经典控制算法之间的兼容性问题，兼顾精度、可控性与实时性，适用于资源受限的移动机器人平台。

Abstract: Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.

</details>


### [319] [ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation](https://arxiv.org/abs/2512.13198)
*Hyun-Gi Lee,Jaekyeong Han,Minjun Kwon,Hyeonuk Kwon,Jooha Park,Hoe Jin Ha,Dong-Hwa Seo*

Main category: cs.RO

TL;DR: 本文介绍了一种名为ALBATROSS的全自动锂离子电池测试机器人系统，可在氩气手套箱内自动完成电解液配制、纽扣电池组装和电化学测试，显著提升实验通量与数据质量。


<details>
  <summary>Details</summary>
Motivation: 传统纽扣电池的组装与测试耗时费力，难以满足高通量筛选新型电池材料（尤其是电解液）的需求。

Method: 设计并构建了一套集成于氩气手套箱内的自动化系统ALBATROSS，包含定制机械臂夹具、3D打印结构及电化学测试模块，实现48个纽扣电池的无人值守组装与测试。

Result: 对NCM811||Li半电池测试显示，放电容量相对标准偏差<1.2%，EIS阻抗标准偏差<3 Ω，验证了系统高可靠性与重复性。

Conclusion: ALBATROSS可高效生成高质量纽扣电池数据集，有望加速下一代电解液等关键电池材料的研发进程。

Abstract: As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 Ω in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.

</details>


### [320] [Differentiable Material Point Method for the Control of Deformable Objects](https://arxiv.org/abs/2512.13214)
*Diego Bolliger,Gabriele Fadini,Markus Bambach,Alisa Rupenyan*

Main category: cs.RO

TL;DR: 本文提出了一种面向控制应用的可微分物质点法（MPM）模拟器，并利用其可微性优化超弹性绳索的主动阻尼控制轨迹，在动能最小化上比基线MPPI方法快约2倍、能量低20%，且计算耗时仅为3%。


<details>
  <summary>Details</summary>
Motivation: 柔性物体因非线性动力学和高维构型空间，其形变控制具有挑战性。

Method: 构建可微分的物质点法（MPM）模拟器，并用于优化超弹性绳索在主动阻尼问题中的控制轨迹。

Result: 相比基线MPPI方法，该方法使绳索动能最小化速度提升约2倍、最终能量降低20%，且计算时间仅为其3%。

Conclusion: 可微分MPM模拟器能高效支持柔性物体控制优化，在精度与效率上显著优于传统方法。

Abstract: Controlling the deformation of flexible objects is challenging due to their non-linear dynamics and high-dimensional configuration space. This work presents a differentiable Material Point Method (MPM) simulator targeted at control applications. We exploit the differentiability of the simulator to optimize a control trajectory in an active damping problem for a hyperelastic rope. The simulator effectively minimizes the kinetic energy of the rope around 2$\times$ faster than a baseline MPPI method and to a 20% lower energy level, while using about 3% of the computation time.

</details>


### [321] [Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment](https://arxiv.org/abs/2512.13215)
*Yinsong Qu,Yunxiang Li,Shanlin Zhong*

Main category: cs.RO

TL;DR: 本文提出了一种改进的序列模型预测控制（ISMPC）导航框架，通过多方向安全矩形走廊（MDSRC）算法和融合障碍函数约束的序列MPC，实现AMR在密集杂乱环境中的高效实时导航。


<details>
  <summary>Details</summary>
Motivation: AMR在密集、杂乱、半结构化环境中导航面临非完整动力学、动静态障碍物交互及非凸约束等挑战。

Method: 提出ISMPC导航框架：1）采用MDSRC算法将自由空间编码为矩形凸区域以规避静态障碍；2）构建集成走廊约束与障碍函数约束的序列MPC框架以同时避让静态和动态障碍，并直接生成速度指令。

Result: 实验表明该框架提升自由空间利用率41.05%（平均走廊面积），走廊生成延迟仅3ms，兼顾高效性与实时性。

Conclusion: ISMPC框架有效解决了复杂环境下AMR导航的计算效率与安全性难题，简化了传统导航架构，具备工业落地潜力。

Abstract: Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).

</details>


### [322] [A Unified Framework for Automated Assembly Sequence and Production Line Planning using Graph-based Optimization](https://arxiv.org/abs/2512.13219)
*Christoph Hartmann,Marios Demetriades,Kevin Prüfer,Zichen Zhang,Klaus Spindler,Stefan Weltge*

Main category: cs.RO

TL;DR: 本文提出了PyCAALP框架，用于自动化装配序列规划（ASP）和生产线规划（PLP），采用图建模方法并集成运动学约束与几何约束，兼顾可行性与计算效率，并以开源形式发布。


<details>
  <summary>Details</summary>
Motivation: 解决复杂装配中人工规划效率低、易出错的问题，同时应对ASP与PLP高组合复杂性及实际工程约束（如碰撞、公差、关节兼容性）的挑战。

Method: 基于图模型表示组件与连接关系；融合运动学边界条件（如防碰撞）、空间关系检测与几何约束建模；引入处理可行性、公差匹配和关节兼容性等属性；结合单件流启发式与几何约束强化；PLP阶段建模为带时间平衡目标的混合整数规划（MIP）。

Result: 实现了可行装配序列的全自动枚举与优化筛选；显著降低MIP求解时间（以部分最优性为代价）；支持工程约束定制化与ASP/PLP灵活权衡；框架已开源，具备工业与科研可扩展性。

Conclusion: PyCAALP为复杂产品装配规划提供了兼顾理论严谨性与工程实用性的自动化解决方案，开源特性有助于推动智能制造领域中ASP与PLP方法的标准化与协同演进。

Abstract: This paper presents PyCAALP (Python-based Computer-Aided Assembly Line Planning), a framework for automated Assembly Sequence Planning (ASP) and Production Line Planning (PLP), employing a graph-based approach to model components and joints within production modules. The framework integrates kinematic boundary conditions, such as potential part collisions, to guarantee the feasibility of automated assembly planning. The developed algorithm computes all feasible production sequences, integrating modules for detecting spatial relationships and formulating geometric constraints. The algorithm incorporates additional attributes, including handling feasibility, tolerance matching, and joint compatibility, to manage the high combinatorial complexity inherent in assembly sequence generation. Heuristics, such as Single-Piece Flow assembly and geometrical constraint enforcement, are utilized to further refine the solution space, facilitating more efficient planning for complex assemblies. The PLP stage is formulated as a Mixed-Integer Program (MIP), balancing the total times of a fixed number of manufacturing stations. While some complexity reduction techniques may sacrifice optimality, they significantly reduce the MIPs computational time. Furthermore, the framework enables customization of engineering constraints and supports a flexible trade-off between ASP and PLP. The open-source nature of the framework, available at https://github.com/TUM-utg/PyCAALP, promotes further collaboration and adoption in both industrial and production research applications.

</details>


### [323] [Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving](https://arxiv.org/abs/2512.13262)
*Hyunki Seong,Jeong-Kyun Lee,Heesoo Myeong,Yongho Shin,Hyun-Mook Cho,Duck Hoon Kim,Pranav Desai,Monu Surana*

Main category: cs.RO

TL;DR: 本文提出GRBO和Warm-K两种方法，分别用于提升自动驾驶中多智能体交互运动行为学习的安全性和测试时行为一致性。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习模型在安全关键场景下鲁棒性不足，且多依赖开环评估，忽视闭环执行中的误差累积问题。

Method: 1）Group Relative Behavior Optimization (GRBO)：基于强化学习的后训练微调方法，通过组相对优势最大化与人类正则化提升安全性；2）Warm-K：一种暖启动Top-K采样策略，用于测试时缩放以增强行为一致性与反应性。

Result: GRBO仅用10%训练数据即可提升安全性超40%，同时保持行为真实性；Warm-K在不重训练前提下缓解协变量偏移、减小性能差距。

Conclusion: GRBO与Warm-K互补地解决了行为建模中的安全性与测试时泛化性问题，为自动驾驶多智能体交互提供了更鲁棒、更实用的解决方案。

Abstract: Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.

</details>


### [324] [Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation](https://arxiv.org/abs/2512.13271)
*Fangju Yang,Hang Yang,Ibrahim Alsarraj,Yuhao Wang,Ke Wu*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级的驱动空间能量建模（LASEM）框架，用于电缆驱动连续体机器人（CDCRs），通过在驱动空间直接构建势能函数，推导出单个偏微分方程描述动力学，避免接触力显式计算，兼顾精度、效率与多输入模式支持。


<details>
  <summary>Details</summary>
Motivation: CDCRs亟需高精度、实时的动力学模型以支撑高速预测或基于模型的控制。

Method: 提出LASEM框架，基于统一变分原理推导出仅含欧拉力矩平衡的单PDE动力学方程；采用Galerkin时空模态离散化并解析计算状态时间导数。

Result: 模型结构轻量、几何准确且物理一致；天然支持力输入和位移输入两种驱动模式；相比最先进实时建模方法平均提速62.3%。

Conclusion: LASEM为CDCRs提供了一种兼具高效性、通用性和物理一致性的新型动态建模范式。

Abstract: Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.

</details>


### [325] [Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration](https://arxiv.org/abs/2512.13293)
*Hao Fua,Wei Liu,Shuai Zhoua*

Main category: cs.RO

TL;DR: 本文提出了一种用于多机器人社交编队导航的新型协同探索强化学习算法，通过自学习内在奖励机制缓解策略保守性，并结合双采样模式与双时间尺度更新，在基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 行人行为的不可预测性和非合作性给多机器人协同探索带来挑战，尤其影响社交编队导航中RL策略的效率与鲁棒性。

Method: 提出一种协同探索多机器人RL算法，包含自学习内在奖励机制以缓解策略保守性，并在CTDE框架中引入双采样模式及双时间尺度更新规则，分别优化导航策略与内在奖励表示。

Result: 在社交编队导航基准上，该算法在关键指标上显著优于现有最先进方法。

Conclusion: 所提算法有效提升了多机器人在动态人机共存环境中的协同探索能力与导航性能，验证了内在动机与双时间尺度学习设计的有效性。

Abstract: This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.

</details>


### [326] [Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories](https://arxiv.org/abs/2512.13304)
*Sait Sovukluk,Johannes Englsberger,Christian Ott*

Main category: cs.RO

TL;DR: 本文提出了一种面向弹簧-质块（spring-mass）轨迹的步态自适应框架，结合死区控制增益库与全身体控制（WBC），实现了在多种复杂动态任务（如跨石、越障、绕桩、急转向、抗扰）中无需在线调参的鲁棒运行。


<details>
  <summary>Details</summary>
Motivation: 为提升仿人机器人在非结构化环境中的敏捷性与鲁棒性，需克服建模误差、传感器噪声、执行延迟等现实挑战，同时避免对每种行为单独调参。

Method: 构建自动化的弹簧-质块轨迹库和死区控制增益库；设计基于模板模型的轨迹选择策略；通过支持闭链、防碰撞与反应式摆腿的全身体控制器将轨迹映射到仿人模型。

Result: 在MuJoCo中成功实现随机踏石跑、越障跳、绕桩、突发转向、强扰动抑制等多种高动态行为；所有任务共用同一轨迹库与WBC参数；315条轨迹的库生成总耗时仅4.5秒。

Conclusion: 该框架兼具高效性、通用性与强鲁棒性，为仿人机器人实时敏捷运动控制提供了可扩展、低调参的新范式。

Abstract: This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.

</details>


### [327] [Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)](https://arxiv.org/abs/2512.13356)
*Zeyad Gamal,Youssef Mahran,Ayman El-Badawy*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习（RL）的控制框架，利用TD3算法实现对双旋翼气动系统（TRAS）的姿态稳定与轨迹跟踪，并在仿真、抗风扰动测试及实物实验中验证了其优于传统PID控制器的性能。


<details>
  <summary>Details</summary>
Motivation: TRAS具有强非线性和复杂动力学特性，传统控制算法难以有效应对；而强化学习在多旋翼控制中展现出潜力，因此探索RL在TRAS控制中的应用具有现实意义。

Method: 采用无需系统模型的连续空间强化学习算法——双延迟深度确定性策略梯度（TD3），在仿真环境中训练智能体完成TRAS的定点稳定与轨迹跟踪任务，并进一步在加入风扰动的仿真和真实实验室平台上进行验证。

Result: 仿真结果表明该RL控制器能有效稳定TRAS并跟踪给定轨迹；在风扰动下其鲁棒性优于PID控制器；实物实验也证实了其在真实场景中的可行性与有效性。

Conclusion: TD3强化学习方法是一种可行且有效的TRAS控制方案，在复杂非线性系统控制及抗扰能力方面展现出优于传统PID控制器的潜力。

Abstract: This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.

</details>


### [328] [Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles](https://arxiv.org/abs/2512.13359)
*Sümer Tunçay,Alain Andres,Ignacio Carlucho*

Main category: cs.RO

TL;DR: 本文提出了一种基于JAX和MuJoCo-XLA的GPU加速强化学习训练流程，实现了两分钟内完成AUV六自由度位置控制策略训练，并在真实水下实验中实现零样本迁移与鲁棒轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统控制器在未建模动力学和环境扰动下性能下降；强化学习虽有潜力，但训练慢、仿真到现实迁移难。

Method: 构建基于JAX和MuJoCo-XLA（MJX）的GPU加速RL训练流水线，联合JIT编译大规模并行物理仿真与学习更新。

Result: 训练时间缩短至两分钟以内；在真实水下实验中实现鲁棒的六自由度轨迹跟踪与扰动抑制；策略零样本迁移成功。

Conclusion: 首次在真实世界中明确展示了基于强化学习的AUV六自由度位置控制能力，验证了高效仿真训练与零样本迁移的可行性。

Abstract: Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.

</details>


### [329] [Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning](https://arxiv.org/abs/2512.13380)
*Chuan Mao,Haoqi Yuan,Ziye Huang,Chaoyi Xu,Kai Ma,Zongqing Lu*

Main category: cs.RO

TL;DR: 本文提出DemoFunGrasp，一种面向通用灵巧功能抓取的强化学习方法，通过解耦抓取风格与功能可供性，并结合单次示范进行一步式编辑式RL训练，显著提升多任务泛化性、样本效率及sim-to-real迁移能力，并集成视觉语言模型实现自主指令跟随抓取。


<details>
  <summary>Details</summary>
Motivation: 细粒度功能抓取在下游操作任务中至关重要，但面临目标与奖励函数设计复杂、多任务RL探索困难、sim-to-real迁移难等挑战。

Method: 将功能抓取条件解耦为抓取风格和可供性两个互补成分，构建可泛化RL框架；利用单次抓取演示，将RL问题重构为一步式示范编辑，提升样本效率；结合视觉语言模型（VLM）实现指令驱动的自主抓取规划。

Result: 在仿真与真实世界实验中，DemoFunGrasp在未见物体-可供性-风格组合上展现出强泛化能力，抓取成功率与功能准确性均优于基线方法，并具备强sim-to-real迁移能力。

Conclusion: DemoFunGrasp为通用灵巧功能抓取提供了高效、可扩展且实用的解决方案，推动了从仿真到现实的功能性机器人操作发展。

Abstract: Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.

</details>


### [330] [Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model](https://arxiv.org/abs/2512.13477)
*Timothy A. Brumfiel,Revanth Konda,Drew Elliott,Jaydev P. Desai*

Main category: cs.RO

TL;DR: 本文介绍了一种简化版的COAST可操控导丝机器人，通过在具有脉动血流的解剖模型中进行导航实验，验证了其在复杂血管路径中的有效导航能力。


<details>
  <summary>Details</summary>
Motivation: 解决血管内介入手术中人工操控导丝带来的问题，提升导丝的可操控性和导航精度。

Method: 设计并测试一种由两根管组成的简化版COAST可操控导丝机器人，在带脉动流的解剖体模中开展导航实验。

Result: 简化版COAST导丝机器人成功实现了在迂曲血管模型中的有效导航。

Conclusion: 简化结构的COAST导丝机器人仍具备良好导航性能，为临床应用提供了更简洁、可行的机器人导丝方案。

Abstract: To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.

</details>


### [331] [Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM](https://arxiv.org/abs/2512.13514)
*Aman Arora,Matteo El-Hariry,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习（PPO）的六自由度自主对接框架，用于JAXA的Int-Ball2机器人在国际空间站日本实验舱高保真仿真环境中的精准对接，考虑了传感噪声、推进器动力学建模（含拖曳力矩与极性结构）及域随机化，验证了策略在多变条件下的鲁棒性，并为碰撞感知导航、安全RL与视觉端到端对接等后续工作奠定基础。


<details>
  <summary>Details</summary>
Motivation: 国际空间站舱内自主飞行器需在传感噪声、执行器不匹配和环境变化下实现高精度对接，现有方法面临挑战。

Method: 采用近端策略优化（PPO）算法，在Isaac Sim构建的日本实验模块高保真模型中训练6-DoF对接控制器；引入域随机化动力学、有界观测噪声，并显式建模推进器拖曳力矩与极性结构。

Result: 所学策略在多种扰动条件下均实现了稳定可靠的对接；验证了推进器物理特性对RL对接性能的关键影响。

Conclusion: 该RL框架不仅提升了Int-Ball2在微重力受限环境中的对接鲁棒性，也为碰撞感知导航、安全强化学习、推进器精确的仿真到现实迁移及视觉端到端对接等方向提供了可扩展基础。

Abstract: Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.

</details>


### [332] [Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments](https://arxiv.org/abs/2512.13561)
*Li-Wei Shih,Ruo-Syuan Mei,Jesse Heidrich,Hui-Ping Wang,Joel Hooton,Joshua Solomon,Jorge Arinez,Guangze Li,Chenhui Shao*

Main category: cs.RO

TL;DR: 本文提出了一种三层近场感知框架，结合激光条纹中断检测、位移测量和嵌入式AI视觉检测，实现在制造环境中自主移动机器人（AMR）的实时、低成本、高精度近场安全感知。


<details>
  <summary>Details</summary>
Motivation: 传统测距传感器（如LiDAR和超声波）难以可靠检测机器人底盘附近的微小障碍物，制约AMR在制造环境中的安全性。

Method: 构建三层近场感知框架：1）基于激光条纹中断的二值化快速障碍检测；2）基于条纹几何位移的障碍物高度定量估计；3）基于嵌入式AI硬件（Raspberry Pi 5）的语义目标检测。三者均实现实时运行（25/50 FPS）。

Result: 在Raspberry Pi 5上实现全栈实时运行，实验表明该分层方案在精度、计算开销与成本间取得良好平衡，显著提升近场障碍识别能力。

Conclusion: 该三层框架为制造场景下AMR提供了可扩展、低成本、高鲁棒性的近场感知解决方案，有效支撑安全自主运行。

Abstract: Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.

</details>


### [333] [World Models Can Leverage Human Videos for Dexterous Manipulation](https://arxiv.org/abs/2512.13644)
*Raktim Gautam Goswami,Amir Bar,David Fan,Tsung-Yen Yang,Gaoyue Zhou,Prashanth Krishnamurthy,Michael Rabbat,Farshad Khorrami,Yann LeCun*

Main category: cs.RO

TL;DR: 本文提出了DexWM，一种用于灵巧操作的世界模型，通过预测环境的潜在状态来实现对灵巧动作的建模，并借助大量非灵巧视频数据训练及手部一致性损失提升精度，在零样本泛化任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作因需理解细微手部运动与物体接触间的复杂关系而极具挑战性，且缺乏足够高质量的灵巧操作数据集。

Method: 提出DexWM灵巧操作世界模型，以历史状态和灵巧动作为条件预测环境下一潜在状态；利用900多小时人类与非灵巧机器人视频进行训练；引入辅助手部一致性损失，确保手部姿态预测准确。

Result: DexWM在状态预测准确性上超越了基于文本、导航和全身动作的已有世界模型；在Franka Panda机械臂+Allegro灵巧手平台上零样本迁移至未见操作技能时，抓取、放置、到达等任务平均性能比Diffusion Policy提升超50%。

Conclusion: DexWM通过结合大规模弱监督视频数据与手部几何约束，有效提升了灵巧操作世界模型的建模能力与泛化性能，为具身智能中的精细操作提供了新范式。

Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.

</details>


### [334] [RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2512.13660)
*Enshen Zhou,Cheng Chi,Yibo Li,Jingkun An,Jiayuan Zhang,Shanyu Rong,Yi Han,Yuheng Ji,Mengzhen Liu,Pengwei Wang,Zhongyuan Wang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出RoboTracer，一种3D感知的视觉语言模型，通过统一空间编码器与回归监督解码器实现3D空间指代与测量，并借助强化微调提升多步度量推理能力；在自建大规模数据集TraceSpatial及新基准TraceSpatial-Bench上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 空间追踪是机器人具身交互的基础能力，但其多步度量推理、复杂空间指代与真实世界度量测量的组合性挑战使现有方法难以胜任。

Method: 提出RoboTracer：1）采用通用空间编码器和回归监督解码器，在监督微调（SFT）中增强尺度感知；2）引入基于度量敏感过程奖励的强化微调（RFT），监督关键中间感知线索以生成准确空间轨迹；3）构建含3000万QA对的大规模数据集TraceSpatial及新基准TraceSpatial-Bench。

Result: RoboTracer在空间理解、测量与指代任务上平均成功率79.1%，在TraceSpatial-Bench上大幅超越Gemini-2.5-Pro达36%；可适配UR5、G1等多类机器人，在真实杂乱场景中执行长程动态任务。

Conclusion: RoboTracer首次实现了端到端、3D感知、可泛化的空间追踪能力，为具身智能中的多步度量推理提供了新范式。

Abstract: Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.

</details>


### [335] [NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks](https://arxiv.org/abs/2512.13670)
*Licheng Luo,Yu Xia,Kaier Liang,Mingyu Cai*

Main category: cs.RO

TL;DR: 本文提出了一种面向机器人操作任务的时空逻辑（SpaTiaL）表示方法，构建了首个支持多层空间关系与时间目标对齐的NL2SpaTiaL数据集，并设计了带语义校验的翻译-验证框架，显著提升了指令跟随的可解释性、可验证性与组合性。


<details>
  <summary>Details</summary>
Motivation: 现有基于时序逻辑（TL）的方法忽略物体级空间交互，且现有数据集无法刻画操作任务所需的分层空间关系，导致自然语言到形式化规范的映射不充分。

Method: 提出一种合成SpaTiaL规格并经语义保持的确定性反向翻译生成自然语言描述的数据集构建框架；设计翻译-验证框架，引入基于语言的语义校验器确保生成公式忠实表达输入语义。

Result: 构建了NL2SpaTiaL数据集，并在多个操作任务上验证了SpaTiaL表示相较于传统TL更具备可解释性、可验证性和组合性。

Conclusion: SpaTiaL为机器人操作任务提供了更贴合实际需求的几何空间建模能力，结合NL2SpaTiaL数据集与翻译-验证框架，推动了自然语言指令到形式化任务规范的可靠转化。

Abstract: Spatio-Temporal Logic (SpaTiaL) offers a principled formalism for expressing geometric spatial requirements-an essential component of robotic manipulation, where object locations, neighborhood relations, pose constraints, and interactions directly determine task success. Yet prior works have largely relied on standard temporal logic (TL), which models only robot trajectories and overlooks object-level interactions. Existing datasets built from randomly generated TL formulas paired with natural-language descriptions therefore cover temporal operators but fail to represent the layered spatial relations that manipulation tasks depend on. To address this gap, we introduce a dataset generation framework that synthesizes SpaTiaL specifications and converts them into natural-language descriptions through a deterministic, semantics-preserving back-translation procedure. This pipeline produces the NL2SpaTiaL dataset, aligning natural language with multi-level spatial relations and temporal objectives to reflect the compositional structure of manipulation tasks. Building on this foundation, we propose a translation-verification framework equipped with a language-based semantic checker that ensures the generated SpaTiaL formulas faithfully encode the semantics specified by the input description. Experiments across a suite of manipulation tasks show that SpaTiaL-based representations yield more interpretable, verifiable, and compositional grounding for instruction following. Project website: https://sites.google.com/view/nl2spatial

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [336] [FloodSQL-Bench: A Retrieval-Augmented Benchmark for Geospatially-Grounded Text-to-SQL](https://arxiv.org/abs/2512.12084)
*Hanzhou Liu,Kai Yin,Zhitong Chen,Chenyue Liu,Ali Mostafavi*

Main category: cs.IR

TL;DR: 本文提出了FLOODSQL-BENCH，一个面向洪水管理领域的地理空间文本到SQL基准测试数据集，涵盖多表、键连接、空间连接及混合连接，用于评估大语言模型在高风险实际应用中的Text-to-SQL能力。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL基准主要针对单表或简单连接的通用领域查询，无法反映领域特定、多表及地理空间推理的复杂性。

Method: 构建了FLOODSQL-BENCH基准，整合社会、基础设施和灾害等异构地理空间数据层，支持键连接、空间连接和混合连接；在统一RAG设置下系统评测近期大语言模型在不同难度层级的表现。

Result: 提供了首个基于真实灾害管理数据的开源、统一地理空间Text-to-SQL基准，验证了当前大模型在该高风险领域仍存在显著性能瓶颈。

Conclusion: FLOODSQL-BENCH为推进Text-to-SQL技术在关键现实场景（如灾害响应）中的研究与应用提供了实用测试平台。

Abstract: Existing Text-to-SQL benchmarks primarily focus on single-table queries or limited joins in general-purpose domains, and thus fail to reflect the complexity of domain-specific, multi-table and geospatial reasoning, To address this limitation, we introduce FLOODSQL-BENCH, a geospatially grounded benchmark for the flood management domain that integrates heterogeneous datasets through key-based, spatial, and hybrid joins. The benchmark captures realistic flood-related information needs by combining social, infrastructural, and hazard data layers. We systematically evaluate recent large language models with the same retrieval-augmented generation settings and measure their performance across difficulty tiers. By providing a unified, open benchmark grounded in real-world disaster management data, FLOODSQL-BENCH establishes a practical testbed for advancing Text-to-SQL research in high-stakes application domains.

</details>


### [337] [Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval](https://arxiv.org/abs/2512.12458)
*Vihan Lakshman,Blaise Munyampirwa,Julian Shun,Benjamin Coleman*

Main category: cs.IR

TL;DR: 本文探讨了高维向量检索中'维度灾难'与实际高效检索之间的悖论，提出并扩展了'稳定性'理论，分析其在多向量、过滤式和稀疏向量三种主流检索场景下的表现，并通过理论证明与实验验证为系统设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 经典理论认为高维空间中最近邻搜索会因距离趋同（维度灾难）而失效，但现代向量数据库却表现出高效性，本文旨在解释这一矛盾。

Method: 基于稳定性理论，分别对多向量搜索（Chamfer距离与平均池化）、过滤式搜索（滤波惩罚机制）和稀疏向量搜索三种场景进行形式化建模与理论分析，并辅以合成与真实数据集实验验证。

Result: 证明Chamfer距离保持单向量稳定性，平均池化可能破坏稳定性；大滤波惩罚可诱导过滤式搜索的稳定性；给出稀疏向量搜索稳定性的新充分条件；实验结果与理论预测一致。

Conclusion: 稳定性是理解并缓解高维检索中维度灾难的关键性质，该理论框架为向量数据库与嵌入模型的设计提供了可解释、可验证的指导原则。

Abstract: Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.

</details>


### [338] [FuXi-$γ$: Efficient Sequential Recommendation with Exponential-Power Temporal Encoder and Diagonal-Sparse Positional Mechanism](https://arxiv.org/abs/2512.12740)
*Dezhi Yi,Wei Guo,Wenyang Cui,Wenxuan He,Huifeng Guo,Yong Liu,Zhenhua Dong,Ye Lu*

Main category: cs.IR

TL;DR: FuXi-γ 是一种新型序列推荐框架，采用解码器-only Transformer 结构，提出指数幂时间编码器和对角稀疏位置机制，在保持高性能的同时显著提升训练与推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的序列推荐方法存在高计算开销问题，主要源于时间编码中的不连续内存访问和长序列上的密集注意力计算。

Method: 提出FuXi-γ框架：(1) 指数幂时间编码器，基于艾宾浩斯遗忘曲线设计可调指数衰减函数，实现高效连续内存访问与纯矩阵运算；(2) 对角稀疏位置机制，利用Toeplitz矩阵的Persymmetry特性，通过对角滑动策略剪枝低贡献注意力块。

Result: 在四个真实数据集上达到推荐质量SOTA，训练加速最高达4.74×，推理加速最高达6.18×。

Conclusion: FuXi-γ通过原理性架构设计，在有效性与效率之间取得更好平衡，是面向长序列推荐的实用可扩展方案。

Abstract: Sequential recommendation aims to model users' evolving preferences based on their historical interactions. Recent advances leverage Transformer-based architectures to capture global dependencies, but existing methods often suffer from high computational overhead, primarily due to discontinuous memory access in temporal encoding and dense attention over long sequences. To address these limitations, we propose FuXi-$γ$, a novel sequential recommendation framework that improves both effectiveness and efficiency through principled architectural design. FuXi-$γ$ adopts a decoder-only Transformer structure and introduces two key innovations: (1) An exponential-power temporal encoder that encodes relative temporal intervals using a tunable exponential decay function inspired by the Ebbinghaus forgetting curve. This encoder enables flexible modeling of both short-term and long-term preferences while maintaining high efficiency through continuous memory access and pure matrix operations. (2) A diagonal-sparse positional mechanism that prunes low-contribution attention blocks using a diagonal-sliding strategy guided by the persymmetry of Toeplitz matrix. Extensive experiments on four real-world datasets demonstrate that FuXi-$γ$ achieves state-of-the-art performance in recommendation quality, while accelerating training by up to 4.74$\times$ and inference by up to 6.18$\times$, making it a practical and scalable solution for long-sequence recommendation. Our code is available at https://github.com/Yeedzhi/FuXi-gamma.

</details>


### [339] [Intelligent Scientific Literature Explorer using Machine Learning (ISLE)](https://arxiv.org/abs/2512.12760)
*Sina Jani,Arman Heidari,Amirmohammad Anvari,Zahra Rahimi*

Main category: cs.IR

TL;DR: 本文提出了一种集成的科学文献探索系统，融合大规模数据获取、混合检索、语义主题建模与异构知识图谱构建，显著提升了检索相关性、主题连贯性与可解释性，为AI辅助科学发现提供可扩展基础。


<details>
  <summary>Details</summary>
Motivation: 科学出版加速导致研究人员难以有效发现、理解与关联相关文献；传统关键词搜索语义能力弱，现有AI工具多局限于单一任务（如检索或可视化）。

Method: 构建整合系统：1）融合arXiv全文与OpenAlex结构化元数据构建综合语料库；2）采用BM25与嵌入式语义搜索结合的混合检索（使用倒数秩融合）；3）根据算力选择BERTopic或NMF进行主题建模；4）构建包含论文、作者、机构、国家和主题的异构知识图谱；5）提供多层级探索界面。

Result: 在多个查询上的评估显示，该系统在检索相关性、主题连贯性和结果可解释性方面均有提升。

Conclusion: 所提框架为AI辅助科学发现提供了可扩展、模块化且语义丰富的基础架构，推动从孤立工具向一体化智能文献分析平台演进。

Abstract: The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.

</details>


### [340] [SPAR: Session-based Pipeline for Adaptive Retrieval on Legacy File Systems](https://arxiv.org/abs/2512.12938)
*Duy A. Nguyen,Hai H. Do,Minh Doan,Minh N. Do*

Main category: cs.IR

TL;DR: SPAR是一种面向遗留企业环境的会话式自适应检索框架，通过轻量级两阶段流程（语义元数据索引 + 按需动态生成会话向量库）替代传统全量向量数据库RAG，降低计算开销并提升检索透明性、可控性与相关性。


<details>
  <summary>Details</summary>
Motivation: 大量企业历史数据深藏于缺乏结构化组织和语义索引的遗留文件系统中，导致检索与分析低效且易错，亟需一种适配此类环境的高效、低成本RAG方案。

Method: 提出SPAR框架：第一阶段构建轻量语义元数据索引；第二阶段按用户会话动态生成小型、专用的向量数据库，嵌入LLM实现检索增强生成；辅以理论复杂度分析与合成生物医学数据集上的实证评估。

Result: 在合成企业级生物医学文献数据集上，SPAR相比标准RAG显著提升了检索有效性及下游模型准确性，同时降低了计算开销，增强了透明性与可控性。

Conclusion: SPAR为遗留企业系统提供了一种更高效、灵活且实用的RAG落地路径，但其在多样化真实企业环境中的部署仍面临若干开放挑战。

Abstract: The ability to extract value from historical data is essential for enterprise decision-making. However, much of this information remains inaccessible within large legacy file systems that lack structured organization and semantic indexing, making retrieval and analysis inefficient and error-prone. We introduce SPAR (Session-based Pipeline for Adaptive Retrieval), a conceptual framework that integrates Large Language Models (LLMs) into a Retrieval-Augmented Generation (RAG) architecture specifically designed for legacy enterprise environments. Unlike conventional RAG pipelines, which require costly construction and maintenance of full-scale vector databases that mirror the entire file system, SPAR employs a lightweight two-stage process: a semantic Metadata Index is first created, after which session-specific vector databases are dynamically generated on demand. This design reduces computational overhead while improving transparency, controllability, and relevance in retrieval. We provide a theoretical complexity analysis comparing SPAR with standard LLM-based RAG pipelines, demonstrating its computational advantages. To validate the framework, we apply SPAR to a synthesized enterprise-scale file system containing a large corpus of biomedical literature, showing improvements in both retrieval effectiveness and downstream model accuracy. Finally, we discuss design trade-offs and outline open challenges for deploying SPAR across diverse enterprise settings.

</details>


### [341] [BLADE: A Behavior-Level Data Augmentation Framework with Dual Fusion Modeling for Multi-Behavior Sequential Recommendation](https://arxiv.org/abs/2512.12964)
*Yupeng Li,Mingyue Cheng,Yucong Luo,Yitong Zhou,Qingyang Mao,Shijin Wang*

Main category: cs.IR

TL;DR: 本文提出BLADE框架，通过双层物品-行为融合架构解决多行为序列推荐中的行为异质性问题，并设计三种行为级数据增强方法缓解数据稀疏性，结合对比学习提升表征能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有多行为序列推荐方法性能受限于用户行为的异质性和数据稀疏性两大挑战。

Method: 提出BLADE框架：1）双物品-行为融合架构，在输入层和中间层融入行为信息以多视角建模偏好；2）三种行为级数据增强方法，在行为序列上生成语义一致的增强视图；3）利用对比学习提升表示学习与泛化能力。

Result: 在三个真实世界数据集上的实验验证了BLADE方法的有效性，显著提升了推荐性能。

Conclusion: BLADE通过协同建模行为异质性与缓解数据稀疏性，为多行为序列推荐提供了更鲁棒、可泛化的解决方案。

Abstract: Multi-behavior sequential recommendation aims to capture users' dynamic interests by modeling diverse types of user interactions over time. Although several studies have explored this setting, the recommendation performance remains suboptimal, mainly due to two fundamental challenges: the heterogeneity of user behaviors and data sparsity. To address these challenges, we propose BLADE, a framework that enhances multi-behavior modeling while mitigating data sparsity. Specifically, to handle behavior heterogeneity, we introduce a dual item-behavior fusion architecture that incorporates behavior information at both the input and intermediate levels, enabling preference modeling from multiple perspectives. To mitigate data sparsity, we design three behavior-level data augmentation methods that operate directly on behavior sequences rather than core item sequences. These methods generate diverse augmented views while preserving the semantic consistency of item sequences. These augmented views further enhance representation learning and generalization via contrastive learning. Experiments on three real-world datasets demonstrate the effectiveness of our approach.

</details>


### [342] [Do Reviews Matter for Recommendations in the Era of Large Language Models?](https://arxiv.org/abs/2512.12978)
*Chee Heng Tan,Huiying Zheng,Jing Wang,Zhuoyi Lin,Shaodi Feng,Huijing Zhan,Xiaoli Li,J. Senthilnath*

Main category: cs.IR

TL;DR: 本文系统研究了大语言模型（LLMs）时代用户评论在推荐系统中的作用，发现LLMs在零样本、少样本和微调场景下普遍优于传统深度学习方法，尤其在数据稀疏和冷启动场景中表现更优；同时，移除或扰动评论文本未必降低推荐效果，挑战了评论必要性的传统认知。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在文本理解与生成能力上的突破，传统依赖显式用户评论提升推荐质量的做法是否仍必要，成为一个亟待系统验证的关键问题。

Method: 通过在八个公开数据集上对LLMs进行零样本、少样本和微调实验，对比其与传统深度学习方法的性能；并提出专用评测框架RAREval，涵盖评论删除、随机扰动、数据稀疏与冷启动等多维度评估。

Result: LLMs作为review-aware推荐器整体优于传统深度学习方法，尤其在数据稀疏和冷启动场景优势明显；部分或全部移除评论、甚至随机扭曲评论，未必然导致推荐准确率下降。

Conclusion: 用户评论在LLM时代的作用需被重新审视——LLMs本身已具备隐式建模用户偏好的能力，未来应探索如何更有效地融合而非简单依赖显式评论文本。

Abstract: With the advent of large language models (LLMs), the landscape of recommender systems is undergoing a significant transformation. Traditionally, user reviews have served as a critical source of rich, contextual information for enhancing recommendation quality. However, as LLMs demonstrate an unprecedented ability to understand and generate human-like text, this raises the question of whether explicit user reviews remain essential in the era of LLMs. In this paper, we provide a systematic investigation of the evolving role of text reviews in recommendation by comparing deep learning methods and LLM approaches. Particularly, we conduct extensive experiments on eight public datasets with LLMs and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We further introduce a benchmarking evaluation framework for review-aware recommender systems, RAREval, to comprehensively assess the contribution of textual reviews to the recommendation performance of review-aware recommender systems. Our framework examines various scenarios, including the removal of some or all textual reviews, random distortion, as well as recommendation performance in data sparsity and cold-start user settings. Our findings demonstrate that LLMs are capable of functioning as effective review-aware recommendation engines, generally outperforming traditional deep learning approaches, particularly in scenarios characterized by data sparsity and cold-start conditions. In addition, the removal of some or all textual reviews and random distortion does not necessarily lead to declines in recommendation accuracy. These findings motivate a rethinking of how user preference from text reviews can be more effectively leveraged. All code and supplementary materials are available at: https://github.com/zhytk/RAREval-data-processing.

</details>


### [343] [Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views](https://arxiv.org/abs/2512.12980)
*Tingyang Chen,Cong Fu,Jiahua Wu,Haotian Wu,Hua Fan,Xiangyu Ke,Yunjun Gao,Yabo Ni,Anxiang Zeng*

Main category: cs.IR

TL;DR: 本文提出了Iceberg，一个面向任务的端到端向量相似性搜索（VSS）基准套件，揭示了影响实际应用性能的三大信息损失源，并基于真实任务指标重新评估和排序13种主流VSS方法，进而构建可解释的决策树以指导实践选择。


<details>
  <summary>Details</summary>
Motivation: 现有VSS基准仅关注距离度量下的召回率-延迟权衡，忽视了检索质量对下游任务的实际影响，导致学术研究与工业实践脱节。

Method: 提出Iceberg基准套件，引入‘信息损失漏斗’分析框架，识别嵌入损失、度量误用和数据分布敏感性三类性能退化源；在8个跨领域、带任务标签的大规模数据集上，对13种SOTA VSS方法进行端到端评估，并基于任务级指标构建可解释的VSS方法选择决策树。

Result: 发现传统基于召回率的VSS方法排名与任务级表现存在显著偏差；验证了三大信息损失源对端到端性能的关键影响；提供了覆盖1M–100M向量、含丰富任务标签的多样化基准数据集及配套评估工具。

Conclusion: VSS评估必须从孤立的距离度量转向面向下游任务的端到端视角；Iceberg为学术界和工业界提供了更真实、可复现、可解释的评估范式与实践指南。

Abstract: Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.
  We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.

</details>


### [344] [Are Large Language Models Really Effective for Training-Free Cold-Start Recommendation?](https://arxiv.org/abs/2512.13001)
*Genki Kusano,Kenya Abe,Kunihiro Takeoka*

Main category: cs.IR

TL;DR: 本文首次在相同条件下系统评估了大语言模型（LLM）与文本嵌入模型（TEM）在无训练推荐（尤其是冷启动场景）中的性能，结果表明TEM显著优于LLM重排序器，且在冷启动和热启动场景下均成立，挑战了‘LLM直接排序是唯一可行方案’的普遍认知。


<details>
  <summary>Details</summary>
Motivation: 现实场景中常面临无交互数据的新用户或新服务（冷启动），传统需训练的推荐方法失效；虽已有研究探索LLM用于无训练推荐，但缺乏LLM与新兴TEM在相同设置下的直接对比。

Method: 设计并执行首个控制变量实验，在统一框架下系统比较LLM重排序器与TEM在训练自由冷启动推荐（TFCSR）任务上的表现，涵盖冷启动与暖启动设置。

Result: TEM在无训练冷启动推荐中全面优于LLM重排序器，且该优势在有丰富交互的暖启动场景下依然成立。

Conclusion: TEM为无训练推荐提供了比LLM更优、更强健且更具可扩展性的基础方案，推翻了LLM直接排序是唯一可行路径的常见假设。

Abstract: Recommender systems usually rely on large-scale interaction data to learn from users' past behaviors and make accurate predictions. However, real-world applications often face situations where no training data is available, such as when launching new services or handling entirely new users. In such cases, conventional approaches cannot be applied. This study focuses on training-free recommendation, where no task-specific training is performed, and particularly on \textit{training-free cold-start recommendation} (TFCSR), the more challenging case where the target user has no interactions. Large language models (LLMs) have recently been explored as a promising solution, and numerous studies have been proposed. As the ability of text embedding models (TEMs) increases, they are increasingly recognized as applicable to training-free recommendation, but no prior work has directly compared LLMs and TEMs under identical conditions. We present the first controlled experiments that systematically evaluate these two approaches in the same setting. The results show that TEMs outperform LLM rerankers, and this trend holds not only in cold-start settings but also in warm-start settings with rich interactions. These findings indicate that direct LLM ranking is not the only viable option, contrary to the commonly shared belief, and TEM-based approaches provide a stronger and more scalable basis for training-free recommendation.

</details>


### [345] [Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer](https://arxiv.org/abs/2512.13037)
*Taoran Sheng,Sathappan Muthiah,Atiq Islam,Jinming Feng*

Main category: cs.IR

TL;DR: 本文提出了一种渐进式上下文感知的电商搜索结果优化方法，通过从简单启发式特征到先进序列模型的迭代升级，显著提升了搜索排序器性能，并在离线与在线A/B测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 电商搜索需动态适配买家在购物旅程中不断变化的意图和偏好，但现有方法难以实时、精准地捕捉并响应这种上下文演化。

Method: 采用系统性渐进策略：从基础启发式自回归特征出发，逐步引入更多上下文信息及前沿序列建模技术，应用于搜索结果页（SERP）的排序优化。

Result: 该上下文增强框架显著提升生产环境排序器性能，在离线评估（MRR）和线上A/B测试中均取得明显效果。

Conclusion: 迭代式上下文建模是提升电商搜索相关性的有效路径，为搜索结果动态个性化提供了可落地的方法论。

Abstract: In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.

</details>


### [346] [A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval](https://arxiv.org/abs/2512.13074)
*Huimu Wang,Yiming Qiu,Xingzhi Yao,Zhiguo Chen,Guoyu Tang,Songlin Wang,Sulong Xu,Mingming Li*

Main category: cs.IR

TL;DR: 本文提出SCI框架，通过无参数的对称表示对齐模块和双视图索引策略，解决密集检索中双塔架构导致的表征空间错位与索引不一致问题，提升长尾查询性能与生成式推荐效果。


<details>
  <summary>Details</summary>
Motivation: 双塔密集检索架构存在表征空间错位和检索索引不一致问题，损害匹配精度、检索稳定性及长尾查询性能，并制约下游大语言模型生成式推荐的效果。

Method: 提出SCI框架，包含两个模块：（1）对称表示对齐模块，采用输入交换机制实现双塔表征空间统一且不增参数；（2）一致性双塔协同索引模块，基于双视图索引策略重构检索路径，保障训练到推理的一致性。

Result: 在公开数据集与真实电商数据集上验证有效，理论可证，支持十亿级规模部署，开销小、工程友好。

Conclusion: SCI是一种系统性、轻量级且实用的密集检索优化框架，显著缓解双塔固有缺陷，提升整体检索与生成式推荐性能。

Abstract: Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.
  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.

</details>


### [347] [Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation](https://arxiv.org/abs/2512.13120)
*Mabiao Long,Jiaxi Liu,Yufeng Li,Hao Xiong,Junchi Yan,Kefan Wang,Yi Cao,Jiandong Ding*

Main category: cs.IR

TL;DR: 本文提出了一种两阶段动态异构图嵌入框架，结合可扩展的图Transformer（HetSGFormer）与轻量级实时增量更新算法（ILLE），兼顾全局表征能力与低延迟更新，显著提升广告主价值并解决冷启动与数据新鲜度问题。


<details>
  <summary>Details</summary>
Motivation: 生产环境中部署动态异构图嵌入面临可扩展性、数据新鲜度和冷启动三大挑战。

Method: 提出两阶段框架：第一阶段使用HetSGFormer（线性可扩展的异构图Transformer）进行静态全局学习；第二阶段引入Incremental Locally Linear Embedding（ILLE），一种轻量、纯CPU的局部线性嵌入增量更新算法，支持实时、低成本更新。

Result: 在十亿级图上A/B测试显示：HetSGFormer带来6.11%广告主价值提升；ILLE进一步提升3.22%，并使嵌入刷新及时性提高83.2%。

Conclusion: 该双模块框架在真实生产环境中验证有效，为动态图学习落地提供了兼顾性能、时效与鲁棒性的实用解决方案。

Abstract: Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.

</details>


### [348] [Know Your Users! Estimating User Domain Knowledge in Conversational Recommenders](https://arxiv.org/abs/2512.13173)
*Ivica Kostric,Ujwal Gadiraju,Krisztian Balog*

Main category: cs.IR

TL;DR: 本文提出了一种新任务：从对话中估计用户领域知识水平，以支持可适应用户知识水平的对话推荐系统（CRS），并设计了基于游戏的数据收集协议，构建首个涵盖不同知识水平用户对话行为的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有CRS通常假设用户为专家，难以应对新手用户，缺乏能反映用户领域知识差异的对话数据集，导致无法实现知识感知的个性化交互。

Method: 设计基于游戏的数据收集协议，主动激发不同知识水平用户的多样化表达，并构建相应数据集；对数据集进行初步分析，验证其在知识感知CRS研究中的潜力。

Result: 发布首个专门用于建模用户领域知识水平的对话数据集，并展示其在揭示新手与专家用户表达差异方面的有效性。

Conclusion: 用户知识水平建模是提升CRS自然性与有效性的关键方向；所提任务、协议与数据集为知识感知CRS研究奠定了基础。

Abstract: The ideal conversational recommender system (CRS) acts like a savvy salesperson, adapting its language and suggestions to each user's level of expertise. However, most current systems treat all users as experts, leading to frustrating and inefficient interactions when users are unfamiliar with a domain. Systems that can adapt their conversational strategies to a user's knowledge level stand to offer a much more natural and effective experience. To make a step toward such adaptive systems, we introduce a new task: estimating user domain knowledge from conversations, enabling a CRS to better understand user needs and personalize interactions. A key obstacle to developing such adaptive systems is the lack of suitable data; to our knowledge, no existing dataset captures the conversational behaviors of users with varying levels of domain knowledge. Furthermore, in most dialogue collection protocols, users are free to express their own preferences, which tends to concentrate on popular items and well-known features, offering little insight into how novices explore or learn about unfamiliar features. To address this, we design a game-based data collection protocol that elicits varied expressions of knowledge, release the resulting dataset, and provide an initial analysis to highlight its potential for future work on user-knowledge-aware CRS.

</details>


### [349] [BlossomRec: Block-level Fused Sparse Attention Mechanism for Sequential Recommendations](https://arxiv.org/abs/2512.13368)
*Mengyang Ma,Xiaopeng Li,Wanyu Wang,Zhaocheng Du,Jingtong Gao,Pengyue Jia,Yuyang Ye,Yiqi Wang,Yunpeng Weng,Weihong Luo,Xiao Han,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 本文提出了一种名为BlossomRec的稀疏注意力机制，用于序列推荐系统，旨在解决标准Transformer中注意力计算随序列增长导致的高计算和内存开销问题；它通过分别建模用户的长期与短期兴趣，并采用两种稀疏注意力模式加可学习门控融合，在长/短序列上均保持稳定性能，同时显著降低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制在用户交互历史变长时带来过高计算和内存开销；现有高效注意力和SSM方法难以兼顾长序列建模能力与短序列稳定性。

Method: 设计BlossomRec稀疏注意力机制：将用户兴趣分为长期和短期两类，分别采用两种稀疏注意力模式建模，并通过可学习门控机制融合输出。

Result: 在四个公开数据集上的实验表明，BlossomRec嵌入主流Transformer模型后，性能媲美或优于基线，同时显著降低内存使用。

Conclusion: BlossomRec是一种高效且鲁棒的稀疏注意力机制，能统一提升序列推荐系统在不同长度序列下的效率与性能。

Abstract: Transformer structures have been widely used in sequential recommender systems (SRS). However, as user interaction histories increase, computational time and memory requirements also grow. This is mainly caused by the standard attention mechanism. Although there exist many methods employing efficient attention and SSM-based models, these approaches struggle to effectively model long sequences and may exhibit unstable performance on short sequences. To address these challenges, we design a sparse attention mechanism, BlossomRec, which models both long-term and short-term user interests through attention computation to achieve stable performance across sequences of varying lengths. Specifically, we categorize user interests in recommendation systems into long-term and short-term interests, and compute them using two distinct sparse attention patterns, with the results combined through a learnable gated output. Theoretically, it significantly reduces the number of interactions participating in attention computation. Extensive experiments on four public datasets demonstrate that BlossomRec, when integrated with state-of-the-art Transformer-based models, achieves comparable or even superior performance while significantly reducing memory usage, providing strong evidence of BlossomRec's efficiency and effectiveness.The code is available at https://github.com/ronineume/BlossomRec.

</details>


### [350] [Automated Information Flow Selection for Multi-scenario Multi-task Recommendation](https://arxiv.org/abs/2512.13396)
*Chaohua Yang,Dugang Liu,Shiwei Li,Yuwen Fu,Xing Tang,Weihong Luo,Xiangyu Zhao,Xiuqiang He,Zhong Ming*

Main category: cs.IR

TL;DR: 本文提出了一种轻量级的自动化信息流选择（AutoIFS）框架，用于多场景多任务推荐（MSMTR），通过低秩适配（LoRA）解耦信息单元并引入信息流选择网络，以降低模型复杂度、减少噪声、提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有MSMTR模型架构复杂、参数量大、训练成本高，且未对信息流进行筛选，易引入无关或有害噪声。

Method: 提出AutoIFS框架：1）采用LoRA解耦四类信息单元，实现轻量高效的信息融合；2）设计信息流选择网络，基于性能反馈自动剪枝无效的场景-任务信息流。

Result: 在两个公开基准数据集和线上A/B测试中验证了AutoIFS的有效性，显著提升了推荐性能并降低了计算开销。

Conclusion: AutoIFS通过结构简化与智能信息流筛选，在保持轻量化的同时提升了MSMTR系统的准确性和鲁棒性，为工业级多场景多任务推荐提供了新思路。

Abstract: Multi-scenario multi-task recommendation (MSMTR) systems must address recommendation demands across diverse scenarios while simultaneously optimizing multiple objectives, such as click-through rate and conversion rate. Existing MSMTR models typically consist of four information units: scenario-shared, scenario-specific, task-shared, and task-specific networks. These units interact to generate four types of relationship information flows, directed from scenario-shared or scenario-specific networks to task-shared or task-specific networks. However, these models face two main limitations: 1) They often rely on complex architectures, such as mixture-of-experts (MoE) networks, which increase the complexity of information fusion, model size, and training cost. 2) They extract all available information flows without filtering out irrelevant or even harmful content, introducing potential noise. Regarding these challenges, we propose a lightweight Automated Information Flow Selection (AutoIFS) framework for MSMTR. To tackle the first issue, AutoIFS incorporates low-rank adaptation (LoRA) to decouple the four information units, enabling more flexible and efficient information fusion with minimal parameter overhead. To address the second issue, AutoIFS introduces an information flow selection network that automatically filters out invalid scenario-task information flows based on model performance feedback. It employs a simple yet effective pruning function to eliminate useless information flows, thereby enhancing the impact of key relationships and improving model performance. Finally, we evaluate AutoIFS and confirm its effectiveness through extensive experiments on two public benchmark datasets and an online A/B test.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [351] [Robust and Efficient Penetration-Free Elastodynamics without Barriers](https://arxiv.org/abs/2512.12151)
*Juntian Zheng,Zhaofeng Luo,Minchen Li*

Main category: cs.GR

TL;DR: 本文提出了一种无屏障的非穿透弹性动力学仿真优化框架，通过定制增广拉格朗日求解器避免时间碰撞锁定（TOI locking），自适应更新拉格朗日乘子，并引入约束筛选与衰减机制，在保持系统良态的同时显著减少牛顿迭代次数，实现比GIPC最高103倍的GPU加速。


<details>
  <summary>Details</summary>
Motivation: 解决IPC方法中对数障碍函数导致的病态系统和收敛慢，以及时间碰撞锁定（TOI locking）限制活跃集探索、增加牛顿迭代次数两大效率瓶颈。

Method: 提出基于二阶约束优化的新框架，采用定制增广拉格朗日求解器，结合连续碰撞检测（CCD）即时纳入所有必要接触对；通过自适应更新拉格朗日乘子替代增大惩罚刚度；引入约束过滤与衰减机制以维持紧凑稳定的活跃集；并给出有限步终止与一阶时间积分精度的理论保证。

Result: 在接触密集的挑战性基准上，GPU优化实现最高103倍于GIPC的速度提升；系统保持良好条件数，显著减少牛顿迭代次数；理论证明有限步收敛与时间积分精度；实验验证了方法的高效性、鲁棒性与准确性。

Conclusion: 该无屏障框架在保持IPC级鲁棒性的同时，大幅提升了非穿透弹性动力学仿真的计算效率与稳定性，为高保真实时物理仿真提供了新范式。

Abstract: We introduce a barrier-free optimization framework for non-penetration elastodynamic simulation that matches the robustness of Incremental Potential Contact (IPC) while overcoming its two primary efficiency bottlenecks: (1) reliance on logarithmic barrier functions to enforce non-penetration constraints, which leads to ill-conditioned systems and significantly slows down the convergence of iterative linear solvers; and (2) the time-of-impact (TOI) locking issue, which restricts active-set exploration in collision-intensive scenes and requires a large number of Newton iterations. We propose a novel second-order constrained optimization framework featuring a custom augmented Lagrangian solver that avoids TOI locking by immediately incorporating all requisite contact pairs detected via CCD, enabling more efficient active-set exploration and leading to significantly fewer Newton iterations. By adaptively updating Lagrange multipliers rather than increasing penalty stiffness, our method prevents stagnation at zero TOI while maintaining a well-conditioned system. We further introduce a constraint filtering and decay mechanism to keep the active set compact and stable, along with a theoretical justification of our method's finite-step termination and first-order time integration accuracy under a cumulative TOI-based termination criterion. A comprehensive set of experiments demonstrates the efficiency, robustness, and accuracy of our method. With a GPU-optimized simulator design, our method achieves an up to 103x speedup over GIPC on challenging, contact-rich benchmarks - scenarios that were previously tractable only with barrier-based methods. Our code and data will be open-sourced.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [352] [How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism](https://arxiv.org/abs/2512.11943)
*Yu Liu,Wenwen Li,Yifan Dou,Guangnan Ye*

Main category: cs.MA

TL;DR: This paper explores how LLM-based AI agents make decisions in network-effect games, revealing that their convergence to equilibrium depends critically on the temporal structure of historical data—not just incentives—unlike human decision-making.


<details>
  <summary>Details</summary>
Motivation: To understand decision-making in multi-AI-agent systems within network-effect-driven contexts, which are common in real-world applications but underexplored in multi-agent research.

Method: Designed a novel workflow using LLM-based agents in repeated network-effect games; systematically varied price trajectories (fixed, ascending, descending, random) and network-effect strength to observe agent behavior and convergence patterns.

Result: Agents fail to infer equilibrium without history; ordered histories enable partial convergence under weak network effects but induce 'AI optimism' under strong effects; randomized history prevents convergence entirely, showing LLMs rely heavily on temporal coherence in data.

Conclusion: Equilibrium outcomes in AI-mediated systems depend not only on economic incentives but also on how historical data is curated—a dimension absent in human decision-making, signaling a fundamental paradigm shift.

Abstract: Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent "AI optimism"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.

</details>


### [353] [Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems](https://arxiv.org/abs/2512.12791)
*Sreemaee Akshathala,Bassam Adnan,Mahisha Ramesh,Karthik Vaidhyanathan,Basil Muhammed,Kannan Parthasarathy*

Main category: cs.MA

TL;DR: 本文提出了一种端到端的智能体评估框架，涵盖大语言模型（LLM）、记忆、工具和环境四大支柱，以应对多智能体系统因模型非确定性带来的行为不确定性评估难题，并在自主云运维（CloudOps）场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对LLM智能体及多智能体系统的评估方法多依赖二元任务完成指标，忽视了模型固有的非确定性所导致的行为不确定性，难以全面衡量智能体在工具调用、记忆读写、多智能体协作及环境交互等方面的能力。

Method: 提出一个四支柱的端到端智能体评估框架（Agent Assessment Framework），分别评估LLM能力、记忆机制、工具使用能力与环境交互能力；并在自主CloudOps用例中开展实验验证。

Result: 实验表明，该框架能有效揭示传统评估指标所忽略的行为偏差，显著提升对运行时不确定性的刻画能力。

Conclusion: 该框架为多智能体系统的鲁棒性与可靠性评估提供了更全面、细粒度的评测范式，弥补了当前AI系统评估中对非确定性行为建模的不足。

Abstract: Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.

</details>


### [354] [Quantigence: A Multi-Agent AI Framework for Quantum Security Research](https://arxiv.org/abs/2512.12989)
*Abdulmalik Alquwayfili*

Main category: cs.MA

TL;DR: 本文提出Quantigence——一个基于多智能体AI的理论驱动框架，用于结构化分析量子安全风险，通过角色分工、认知并行和量子调整风险评分（QARS）提升后量子密码迁移效率。


<details>
  <summary>Details</summary>
Motivation: CRQC带来的‘先存储、后解密’威胁迫在眉睫，但PQC迁移受研究进展快、标准动态演进及部署环境异构等阻碍，亟需自动化、可扩展的风险分析工具。

Method: 构建多智能体AI框架Quantigence，含密码分析师、威胁建模师、标准专家与风险评估师四类专业化代理，由监督代理协调；采用认知并行机制保障上下文独立性，并通过Model Context Protocol（MCP）集成外部知识，使用Quantum-Adjusted Risk Score（QARS）量化风险。

Result: 实证表明Quantigence使研究周转时间减少67%，文献覆盖度优于人工流程，且可在资源受限硬件（如RTX 2060）上运行。

Conclusion: Quantigence为组织提供了高效、可复现、低门槛的量子安全风险评估能力，推动PQC迁移从被动响应转向主动治理。

Abstract: Cryptographically Relevant Quantum Computers (CRQCs) pose a structural threat to the global digital economy. Algorithms like Shor's factoring and Grover's search threaten to dismantle the public-key infrastructure (PKI) securing sovereign communications and financial transactions. While the timeline for fault-tolerant CRQCs remains probabilistic, the "Store-Now, Decrypt-Later" (SNDL) model necessitates immediate migration to Post-Quantum Cryptography (PQC). This transition is hindered by the velocity of research, evolving NIST standards, and heterogeneous deployment environments. To address this, we present Quantigence, a theory-driven multi-agent AI framework for structured quantum-security analysis. Quantigence decomposes research objectives into specialized roles - Cryptographic Analyst, Threat Modeler, Standards Specialist, and Risk Assessor - coordinated by a supervisory agent. Using "cognitive parallelism," agents reason independently to maintain context purity while execution is serialized on resource-constrained hardware (e.g., NVIDIA RTX 2060). The framework integrates external knowledge via the Model Context Protocol (MCP) and prioritizes vulnerabilities using the Quantum-Adjusted Risk Score (QARS), a formal extension of Mosca's Theorem. Empirical validation shows Quantigence achieves a 67% reduction in research turnaround time and superior literature coverage compared to manual workflows, democratizing access to high-fidelity quantum risk assessment.

</details>


### [355] [The Optimal Control Algorithm of Connected and Automated Vehicles at Roundabouts with Communication Delay](https://arxiv.org/abs/2512.13056)
*Chen Huang,Ronghui Hou*

Main category: cs.MA

TL;DR: 本文提出了一种考虑通信时延不确定性的环形路口分布式协同控制算法，结合时间至碰撞（TTC）冲突识别、含时延的车辆运动建模与多尺度优化调度，提升高车流密度和高自动驾驶渗透率下的通行安全性与稳定性。


<details>
  <summary>Details</summary>
Motivation: CAVs在环形路口依赖无线通信进行协同控制，但通信延迟会引发状态信息不确定性，尤其在高速、高负载场景下易导致控制性能下降甚至安全隐患。

Method: 1）基于TTC识别冲突车辆；2）构建含通信时延的车辆运动模型；3）采用分布式模型预测控制（DMPC）满足环形路口约束；4）设计融合车辆运动指标与系统级指标（如交通密度、行程时间）的多尺度优化目标，调度车辆进入顺序。

Result: 通过多种仿真对比实验（不同自动驾驶渗透率与重载交通场景），验证了该算法在安全性、稳定性及通行效率方面优于多种基准控制算法。

Conclusion: 所提算法能有效应对环形路口中由通信延迟引起的交互不确定性，在复杂交通条件下实现更安全、稳定、高效的协同通行。

Abstract: Connected and automated vehicles (CAVs) rely on wireless communication to exchange state information for distributed control, making communication delays a critical factor that can affect vehicle motion and degrade control performance, particularly in high-speed scenarios. To address these challenges in the complex environment of roundabout intersections, this paper proposes a roundabout control algorithm, which takes into account the uncertainty of interactive information caused by time delays. First, to maintain the required distance between the current vehicle and its preceding and following vehicles, conflicting vehicles are identified based on the time-to-collision (TTC) in the conflict zone. To fully consider communication performance, a vehicle motion model incorporating time delays is established. According to the distributed model predictive control (DMPC) mechanism, the vehicle motion control that satisfies the roundabout constraints is determined. Second, by scheduling the sequence of vehicles entering the roundabout, a multiscale optimization objective is developed by integrating vehicle motion indicators and roundabout system indicators. Traffic density and travel time are embedded into the optimization problem to guide vehicles to enter the roundabout safely and stably. Through a variety of simulation experiments, the effectiveness of the proposed control algorithm is verified by comparing its performance with that of multiple control algorithms under different autonomous vehicle penetration rates and heavy traffic load scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [356] [A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models](https://arxiv.org/abs/2512.11835)
*Seyma Yaman Kayadibi*

Main category: cs.AI

TL;DR: 本文提出一种基于莱布尼茨单子论的条款式架构，利用人工年龄分数（AAS）作为核心度量，对大语言模型（LLM）的记忆老化与内部行为施加可审计、可执行的约束，并通过六组Python实验验证其有界性、可解释性与可控演化特性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型内部记忆和‘类自我’行为缺乏原则性、可审计治理机制的问题，弥合理论哲学（如单子论）与工程实现之间的鸿沟。

Method: 将莱布尼茨《单子论》中20个单子归纳为6个哲学范畴（本体论、动力学、表征与意识等），每类转化为基于AAS核的可执行条款；在6个最小化Python实现中，针对通道级指标（如回忆得分、冗余度、权重）开展数值实验，遵循‘输入设置→条款实现→数值结果→设计启示’四步模式。

Result: 实验表明该条款系统具有连续且速率受限的AAS轨迹、对矛盾与无依据主张的显式惩罚机制、层级细化下的有机结构、和谐项引导的双重视角与目标-动作对齐，以及基于完美度窗口漂移区分持续改进与退化的能力。

Conclusion: 该单子论驱动的条款框架以AAS为骨干，提供了面向LLM内部动态的透明、代码级约束与分析蓝图，兼具哲学深度与工程可行性。

Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.

</details>


### [357] [Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars](https://arxiv.org/abs/2512.11864)
*Christoph Einspieler,Matthias Horn,Marie-Louise Lackner,Patrick Malik,Nysret Musliu,Felix Winter*

Main category: cs.AI

TL;DR: 本文提出了一种面向实际工业场景的并行机调度新变体，包含作业优先约束和日历型累积资源约束，并结合精确求解（约束建模）、构造启发式与定制化局部搜索元启发式方法，其中后者已在工业环境中部署应用。


<details>
  <summary>Details</summary>
Motivation: 现有调度方法难以有效处理真实生产环境中复杂的优先约束和日历驱动的累积资源限制，亟需开发能应对这些现实约束的自动化调度方法。

Method: 提出一种融合约束建模（用于小规模实例的精确求解）、构造启发式和基于局部搜索的定制化元启发式（用于大规模实例）的混合求解方法。

Result: 所提出的元启发式方法已在实际工业场景中部署并投入使用，验证了其在大规模问题上的有效性与实用性。

Conclusion: 该研究为现实并行机调度问题提供了兼顾建模表达力与求解效率的综合解决方案，推动了智能调度技术在制造业的实际落地。

Abstract: The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.

</details>


### [358] [Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning](https://arxiv.org/abs/2512.11902)
*Yanna Elizabeth Smid,Peter van der Putten,Aske Plaat*

Main category: cs.AI

TL;DR: 本文提出了一种名为Mirror Mode的新游戏模式，通过让敌方AI模仿玩家自身策略来提升游戏的挑战性与趣味性；研究结合GAN-IL、行为克隆与PPO方法构建模仿模型，在简化版Fire Emblem Heroes中验证其有效性，实验表明模型能较好模仿玩家防守行为，提升玩家满意度。


<details>
  <summary>Details</summary>
Motivation: 提升回合制游戏中敌方AI的不可预测性和挑战性，使玩家面对自己策略时产生新鲜感和更高参与度。

Method: 在Unity中构建简化版Fire Emblem Heroes，设计Standard Mode与Mirror Mode；采用生成对抗模仿学习（GAN-IL）、行为克隆（BC）与近端策略优化（PPO）相结合的方法训练模仿玩家策略的AI模型。

Result: 实验表明模型能较好模仿玩家的防御行为，但进攻策略模仿效果不佳；玩家问卷显示能识别自身撤退战术，且Mirror Mode整体满意度更高。

Conclusion: Mirror Mode能有效提升玩家满意度，进一步优化模仿模型（尤其是进攻策略）有望增强玩家体验与AI真实性。

Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode

</details>


### [359] [Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents](https://arxiv.org/abs/2512.11907)
*Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

TL;DR: 本文提出了一种在结构化约束（如逻辑依赖、类别配额和层级规则）下进行大语言模型个性化的方法，通过将用户知识图谱编译为宏观面（macro-facets），并证明其约束构成层状拟阵（laminar matroid），从而可使用带理论保证的贪心算法进行子模函数最大化。


<details>
  <summary>Details</summary>
Motivation: 个性化大语言模型代理需权衡任务效用与用户数据披露；现实中存在多种结构性约束，使传统子集选择算法失效。

Method: 提出一种编译方法，将用户知识图谱中的依赖关系转化为抽象宏观面，并证明层级与配额类约束在这些面上构成层状拟阵，从而将问题建模为子模最大化下的拟阵约束优化。

Result: 理论证明了常见结构性约束形成层状拟阵，支持使用贪心算法获得常数因子近似解（连续贪心可达1-1/e近似比）。

Conclusion: 该框架为更真实、复杂的个性化场景提供了兼具理论保障与实用性的子模优化解决方案。

Abstract: Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.

</details>


### [360] [Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets](https://arxiv.org/abs/2512.11909)
*Hanna Dettki*

Main category: cs.AI

TL;DR: 本文通过在11个语义明确的因果任务上对比评估20多个大语言模型（LLMs）与人类的因果推理能力，探究二者在相同任务下的对齐性、任务级一致性及推理特征差异，并采用漏噪-OR因果贝叶斯网络建模判断过程。


<details>
  <summary>Details</summary>
Motivation: 因果推理被视为智能的核心能力之一，但尚缺乏对LLMs与人类在相同因果任务上推理表现的系统性比较；需厘清二者是否对齐、是否一致、是否存在独特推理模式。

Method: 在基于collider图（C₁→E←C₂）的11个因果任务上，以Direct（单次数值响应）和Chain of Thought（CoT）两种方式评估20+个LLM；用含参数θ=(b,m₁,m₂,p(C))的leaky noisy-OR因果贝叶斯网络建模判断，通过AIC选择对称（m₁=m₂）或非对称（m₁≠m₂）模型。

Result: 发现LLMs与人类在因果推理上存在显著不对齐；任务级一致性较弱；且展现出与人类不同的推理签名（如对先验p(C)和因果强度m的敏感性差异）。

Conclusion: LLMs尚未真正掌握人类水平的因果推理，其表现更依赖提示形式与参数设定，而非内在因果理解；需发展更符合认知机制的评估与建模范式。

Abstract: The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?
  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\!\to\!E\!\leftarrow\!C_2$ ) under \emph{Direct} (one-shot number as response = probability judgment of query node being one and \emph{Chain of Thought} (CoT; think first, then provide answer).
  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $θ=(b,m_1,m_2,p(C)) \in [0,1]$ include a shared prior $p(C)$;
  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\neq}m_2$) variant.

</details>


### [361] [Value-Aware Multiagent Systems](https://arxiv.org/abs/2512.12652)
*Nardine Osman*

Main category: cs.AI

TL;DR: 本文提出了一种超越传统价值对齐问题的AI价值感知（value awareness）概念，并围绕学习与表征人类价值观、确保单智能体与多智能体系统的价值对齐、以及基于价值的行为可解释性三大支柱，构建了工程化价值感知AI的简明路线图。


<details>
  <summary>Details</summary>
Motivation: 传统价值对齐问题不足以支撑AI系统在复杂现实场景中稳健地理解与响应人类价值，因此需要更全面、主动的价值感知能力。

Method: 提出价值感知AI的三支柱框架：基于形式语义学习和表征人类价值；保障个体及多智能体系统的价值对齐；实现基于价值的行为可解释性；并结合正在进行的研究与实际应用进行验证。

Result: 构建了一个结构清晰、可操作的价值感知AI工程化路线图，并展示了部分成果在真实场景中的应用潜力。

Conclusion: 价值感知是迈向更可靠、可信、可解释AI的关键范式演进，该框架为后续理论研究与系统开发提供了统一方向。

Abstract: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.

</details>


### [362] [Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning](https://arxiv.org/abs/2512.13131)
*Xin Guo,Yifan Zhao,Jia Li*

Main category: cs.AI

TL;DR: 本文提出了一种分层隐式周期性（HIP）学习方法，用于从语音生成更自然、协调的3D手势动作，通过周期性自编码器建模运动相位流形，并采用级联引导建模面部、躯干与手部动作的层级关系，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音驱动3D手势生成方法（如GAN、VQ-VAE、扩散模型）难以建模头、躯干、手等不同运动单元之间的内在跨单元与单元内相关性，导致动作不自然、协调性差。

Method: 提出统一的分层隐式周期性（HIP）学习框架：i) 利用周期性自编码器建模手势运动相位流形，兼顾周期性规律与非周期性实例多样性；ii) 通过级联引导机制建模面部、躯干和手部动作的层级驱动关系。

Result: 在3D虚拟人任务上验证了方法有效性，定量与定性实验均显著优于当前最优共语手势生成方法。

Conclusion: 建模运动单元间的隐式周期性与层级相关性对提升语音驱动3D手势的真实感与协调性至关重要，HIP为该方向提供了新范式。

Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.

</details>


### [363] [Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis](https://arxiv.org/abs/2512.11912)
*Liu Peng,Yaochu Jin*

Main category: cs.AI

TL;DR: 本文系统比较了低质量数据对现代概率模型的影响，发现自回归语言模型（如GPT-2）具有强鲁棒性，而类条件扩散模型则严重退化，分类器表现居中；作者从信息论、PAC学习和梯度动力学多角度分析，指出鲁棒性主要取决于条件信息的丰富性和训练数据的绝对信息量。


<details>
  <summary>Details</summary>
Motivation: 理解不同现代概率模型在低质量数据下的鲁棒性差异，揭示其内在机制。

Method: 对多种模型（自回归语言模型、类条件扩散模型、分类器）在相同数据污染水平下进行系统性对比实验，并结合信息论、PAC学习理论和梯度动态分析解释结果。

Result: GPT-2在50%标记损坏下NLL仅小幅上升（2.87→3.59）；类条件扩散模型图像-标签一致性相对下降56.81%；分类器影响中等且随数据规模增大而减弱。

Conclusion: 模型鲁棒性主要由两个因素决定：条件信息的丰富性（约束学习问题）和训练数据的绝对信息量（使正确信号压倒统计噪声）。

Abstract: A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.

</details>


### [364] [Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows](https://arxiv.org/abs/2512.13168)
*Haoyu Dong,Pengkun Zhang,Yan Gao,Xuanyu Dong,Yilin Cheng,Mingzhe Lu,Adina Yakefu,Shuxin Zheng*

Main category: cs.AI

TL;DR: 本文提出了一个名为Finch的金融与会计基准，用于评估AI代理在真实企业级专业工作流中的表现，涵盖数据录入、结构化、格式化、网络搜索、跨文件检索、计算、建模、验证、翻译、可视化和报告等任务。该基准基于Enron等真实企业数据构建，包含172个复合工作流、384个任务及大量多模态数据，并通过专家标注与大模型辅助发现相结合的方式生成。实验表明当前前沿AI系统在此基准上表现有限，揭示了现实企业工作流对AI代理的重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准难以反映真实企业环境中复杂、混乱、长周期、知识密集且协作性强的专业工作流，亟需一个更贴近实际的评估基准。

Method: 提出Finch基准，结合LLM辅助发现与领域专家标注构建172个复合工作流；数据源自Enron等真实企业（15,000张表格、500,000封邮件等）；采用人工与自动化方式评估多个前沿AI模型。

Result: GPT 5.1在48小时内仅通过38.4%的工作流，Claude Sonnet 4.5仅通过25.0%；实证揭示当前AI系统在处理真实企业工作流时面临显著瓶颈。

Conclusion: Finch为评估AI代理在专业领域的真实能力提供了高保真基准，凸显当前模型在复杂企业工作流中仍存在重大局限，亟需方法学突破。

Abstract: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.

</details>


### [365] [CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving](https://arxiv.org/abs/2512.11920)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: CXL-SpecKV 是一种基于 CXL 互连与 FPGA 加速的解耦式 KV 缓存架构，通过内存解耦、推测性预取和硬件压缩，显著提升大语言模型推理吞吐量并降低显存开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在数据中心部署时面临 KV 缓存占用大量 GPU 内存的问题，限制批处理大小和系统吞吐量，形成‘内存墙’瓶颈。

Method: 提出 CXL-SpecKV 架构，包含三项创新：(i) 基于 CXL 的远程 FPGA 内存解耦框架；(ii) 面向未来 token 的 KV 缓存推测性预取机制；(iii) FPGA 加速的 KV 缓存压缩/解压引擎。

Result: 在主流 LLM 上评估显示，相比纯 GPU 方案，吞吐量最高提升 3.2×，内存成本降低 2.8×，且不损失精度。

Conclusion: 结合智能内存解耦与推测执行可有效突破 LLM 推理中的内存瓶颈，为大规模 LLM 服务提供新范式。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.

</details>


### [366] [AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org](https://arxiv.org/abs/2512.11935)
*Jaehyung Lee,Justin Ely,Kent Zhang,Akshaya Ajith,Charles Rhys Campbell,Kamal Choudhary*

Main category: cs.AI

TL;DR: 本文介绍了AGAPI，一个开源的智能体AI平台，整合了多个开源大语言模型和材料科学API，通过Agent-Planner-Executor-Summarizer架构实现多步材料研究工作流的自动执行，显著提升材料发现的可重复性与效率。


<details>
  <summary>Details</summary>
Motivation: 解决材料科学研究中计算生态碎片化、结果不可复现以及过度依赖商业大语言模型的问题。

Method: 构建AGAPI平台，集成8+开源LLM与20+材料科学API，采用Agent-Planner-Executor-Summarizer架构，支持从数据检索到逆向设计的多步自主工作流。

Result: 成功演示异质结构构建、粉末X射线衍射分析、半导体缺陷工程等端到端工作流；在30+测试用例中验证了工具增强型智能体预测优于无工具基线，并与实验数据吻合良好；已有超1000名活跃用户。

Conclusion: AGAPI为可复现、透明且可扩展的AI加速材料发现提供了开源、开放、可协作的基础平台。

Abstract: Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.

</details>


### [367] [Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play](https://arxiv.org/abs/2512.11942)
*Vince Trencsenyi*

Main category: cs.AI

TL;DR: 本文提出了一种基于逻辑的领域特定语言和基于答案集编程的自动化流程，用于建模和求解超博弈（hypergame），以支持多智能体系统中异质信念建模与理性化分析。


<details>
  <summary>Details</summary>
Motivation: 现有博弈论常忽略玩家间主观认知差异，而超博弈理论虽能建模错配心智模型，却缺乏统一、形式化且实用的表示语言及可扩展算法，制约其在多智能体系统中的实际应用。

Method: 引入一种声明式、基于逻辑的领域特定语言来编码超博弈结构与解概念，并利用答案集编程（ASP）构建自动化流水线，实现超博弈实例化与新型‘超博弈理性化’过程（即寻找能解释表面非理性行为的信念结构）。

Result: 提出了首个统一、形式化且可验证的超博弈建模语言；实现了可扩展的自动化求解框架；验证了其在解释异质信念驱动下的策略行为方面的有效性。

Conclusion: 该工作弥合了超博弈理论、多智能体系统与战略人工智能之间的鸿沟，为构建基于信念的异质理性智能体提供了坚实基础与逻辑保障。

Abstract: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.

</details>


### [368] [Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion](https://arxiv.org/abs/2512.11997)
*Anfeng Peng,Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.AI

TL;DR: EnrichLog是一种无需训练、基于日志条目的异常检测框架，通过融合语料库特定和样本特定的知识来增强原始日志条目，提升检测准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统日志分析方法（如基于模板或序列的方法）易丢失语义信息或难以处理模糊日志模式，亟需更鲁棒、可解释的无监督异常检测方案。

Method: 提出EnrichLog框架，采用检索增强生成（RAG）技术，在不重训练的前提下，为每条日志注入历史上下文与语料库级推理知识，实现语义富化与上下文感知的异常判别。

Result: 在四个大规模系统日志基准数据集上，EnrichLog显著优于五种基线方法，尤其在模糊日志识别、检测置信度与推理效率方面表现突出。

Conclusion: 融合语料库特定与样本特定知识能有效提升日志异常检测的准确性、鲁棒性与实用性，EnrichLog为实际分布式系统监控提供了高效、免训练、可解释的新范式。

Abstract: System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.

</details>


### [369] [Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp](https://arxiv.org/abs/2512.12048)
*Muddsair Sharif,Huseyin Seker*

Main category: cs.AI

TL;DR: 本文提出了一种面向动态资源分配的上下文敏感多智能体协调框架（CAMAC-DRA），用于优化智能电动汽车充电生态系统，通过融合图神经网络、注意力机制与深度Q网络，在真实数据上实现高成功率、能效提升、成本降低与电网负荷缓解。


<details>
  <summary>Details</summary>
Motivation: 解决智能电动汽车充电中多利益相关方目标冲突、动态环境适应性差及现有强化学习方法性能不足的问题。

Method: 提出CAMAC-DRA框架，采用协同深度Q网络（DQN）结合图神经网络（GNN）和注意力机制，处理20维上下文特征（如天气、交通、电网负载、电价等），并设计加权协调机制与共识协议平衡五类利益相关方。

Result: 在441,077笔真实充电交易数据上验证：协调成功率92%，能效提升15%，成本降低10%，电网压力下降20%，收敛速度提升2.3倍，训练稳定性88%，样本效率85%；商业评估显示净现值-122,962美元，可再生能源集成带来69%成本削减。

Conclusion: CAMAC-DRA首次实现了上下文感知、多利益相关方协同的动态EV充电资源分配，兼顾实时适应性与多目标优化，为可持续交通电气化提供了突破性解决方案。

Abstract: This paper presents a novel context-sensitive multi\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\%), grid operators (20\%), charging station operators (20\%), fleet operators (20%), and environmental factors (15\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\-DRA framework achieves 92\% coordination success rate, 15\% energy efficiency improvement, 10\% cost reduction, 20% grid strain decrease, and \2.3x faster convergence while maintaining 88\% training stability and 85\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\$122,962 and 69\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.

</details>


### [370] [The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification](https://arxiv.org/abs/2512.12059)
*Luke Bhan,Hanyu Zhang,Andrew Gordon Wilson,Michael W. Mahoney,Chuck Arvin*

Main category: cs.AI

TL;DR: 本文提出Forecast Critic系统，利用大语言模型（LLM）自动监测时间序列预测质量，在无需领域微调的情况下，能有效识别不合理预测（如时序错位、趋势不一致、异常尖峰），F1达0.88；多模态LLM可融合非结构化促销上下文提升判断（F1=0.84）；在真实M5数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型零售企业亟需高效、可扩展的预测监控手段以保障客户满意度、盈利与运营效率，而传统方法依赖人工或规则，难以应对复杂、非结构化上下文。

Method: 系统性评估多种先进LLM在预测质量评估任务上的能力，设计三类实验：（1）检测明显不合理预测；（2）融合非结构化外部特征（如促销文本）；（3）在合成与真实数据（含M5）上验证泛化性，并对比不同模型规模与推理能力的影响。

Result: 最佳LLM在不合理预测识别上F1达0.88（人类为0.97）；多模态LLM结合历史促销文本后F1达0.84；在M5数据中，被判定为不合理的预测其标准化连续分级概率评分（sCRPS）平均高出合理预测10%以上。

Conclusion: LLM即使未经领域微调，也具备作为可扩展、自动化预测监控工具的潜力，尤其在融合多源异构信息方面展现出优势。

Abstract: Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.

</details>


### [371] [Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations](https://arxiv.org/abs/2512.12088)
*S. R. Eshwar,Aniruddha Mukherjee,Kintan Saha,Krishna Agarwal,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.AI

TL;DR: 本文评估了可靠策略迭代（RPI）在CartPole和倒立摆任务中的鲁棒性，结果表明RPI相比DQN、Double DQN、DDPG、TD3和PPO等方法更早达到近优性能并保持稳定，展现出更高的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习中常见的样本效率低、训练不稳定和超参数敏感等问题，恢复策略迭代在函数逼近设置下的值估计单调性。

Method: 对RPI方法在CartPole和倒立摆两个经典控制任务上，改变神经网络结构和环境参数，进行鲁棒性评估，并与DQN、Double DQN、DDPG、TD3和PPO等主流算法对比。

Result: RPI在两种任务中均比对比算法更早达到近最优性能，并在整个训练过程中维持该性能，表现出更强的鲁棒性和稳定性。

Conclusion: RPI是一种比现有主流深度强化学习方法更可靠的替代方案，尤其适用于对稳定性与鲁棒性要求较高的场景。

Abstract: In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.

</details>


### [372] [Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective](https://arxiv.org/abs/2512.12175)
*Haoyang Chen,Richong Zhang,Junfan Chen*

Main category: cs.AI

TL;DR: 本文提出了一种基于标签一致性和转导式标签传播视角的上下文学习（ICL）示例选择新方法TopK-SD，通过融合语义与标签信息合成数据，提升演示样本的标签一致性，从而改善ICL性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义相似性的检索式演示选择方法无法保证标签一致性，而作者从贝叶斯视角和转导式标签传播角度重新理解ICL，指出标签一致性对概念引导至关重要。

Method: 构建标签传播框架，将标签一致性与传播误差界关联；提出融合语义与标签信息的数据合成方法，并设计TopK采样结合合成数据（TopK-SD）策略以选取具有一致标签的演示样本。

Result: TopK-SD在多个基准上优于原始TopK采样方法，验证了标签一致性对ICL效果的提升作用。

Conclusion: 标签一致性是影响ICL效果的关键因素，所提TopK-SD方法为理解ICL内在机制提供了新视角，并提升了演示选择的有效性。

Abstract: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.

</details>


### [373] [Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation](https://arxiv.org/abs/2512.12177)
*Aydin Ayanzadeh,Tim Oates*

Main category: cs.AI

TL;DR: 本文提出Floorplan2Guide，利用大语言模型将建筑平面图转化为可导航的知识图谱，并生成人类可读的导航指令，显著提升了视障人士室内导航的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有室内导航方案主要依赖基础设施，难以适应动态环境，无法满足视障人士安全导航需求。

Method: 提出Floorplan2Guide方法，结合基础模型将平面图转化为知识图谱，并利用大语言模型（如Claude 3.7 Sonnet）提取空间信息；采用少样本学习（5-shot）提升导航指令生成精度。

Result: 在MP-1平面图测试中，Claude 3.7 Sonnet在短、中、长路径上分别达到92.31%、76.92%和61.54%的导航准确率；基于图的空间结构方法比直接视觉推理成功率高15.4%。

Conclusion: 知识图谱表征与上下文学习显著提升室内导航性能，该方法更精准、实用，适用于盲人及低视力用户。

Abstract: Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.

</details>


### [374] [TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2512.12182)
*Xinyu Gao*

Main category: cs.AI

TL;DR: 本文提出了一种基于生成式表示的少样本知识图谱补全框架，结合两阶段注意力三元组增强器与U-KAN扩散模型，在两个公开数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱中关系分布呈长尾特性，导致少量样本下事实补全困难；现有方法（如度量匹配、元学习）未能充分挖掘图邻域信息或忽略对比信号的分布特性。

Method: 提出一种少样本知识图谱补全框架，包含两阶段注意力三元组增强器和基于U-KAN的扩散生成模型，从生成式表示视角建模关系与实体。

Result: 在两个公开数据集上实验表明，该方法显著优于现有少样本KG补全方法，达到新的SOTA性能。

Conclusion: 生成式建模与结构化注意力机制的结合，能更有效地缓解长尾关系下的数据稀疏问题，提升少样本知识图谱补全效果。

Abstract: Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.

</details>


### [375] [A Geometric Theory of Cognition](https://arxiv.org/abs/2512.12225)
*Laha Ale*

Main category: cs.AI

TL;DR: 本文提出了一种基于黎曼几何的统一认知框架，将多种认知过程建模为认知势能在流形上的梯度流，自然导出直觉与推理等双过程现象，无需模块化假设。


<details>
  <summary>Details</summary>
Motivation: 现有认知理论多针对不同能力（感知、推理、记忆等）采用各自独立的计算模型，缺乏统一基础；本文旨在建立一个能涵盖多种认知过程的单一数学原理。

Method: 将认知状态表示为带学习黎曼度量的可微流形上的点；定义融合预测精度、结构简洁性、任务效用和规范要求的认知势能；认知动态由该势能的黎曼梯度流决定；通过度量各向异性解释双过程现象及时间尺度分离。

Result: 成功从同一几何动力学中自然导出快速直觉反应与慢速审慎推理；推导出相变与多时间尺度行为的解析条件；在经典认知任务模拟中复现了典型心理效应。

Conclusion: 认知可被统一理解为流形上受几何约束的势能最小化过程；该几何框架不仅为人类认知提供新基础，也为构建更通用、类人的人工智能提供指导原则。

Abstract: Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.

</details>


### [376] [A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure](https://arxiv.org/abs/2512.12260)
*Ege Atacan Doğan,Peter F. Patel-Schneider*

Main category: cs.AI

TL;DR: This paper analyzes Wikidata's polyhierarchical and multi-axial ontology design, contrasting it with traditional disjoint and exhaustive top-level distinctions, and highlights its benefits for scalability and modularity in collaborative knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: To understand the structural implications of Wikidata's departure from traditional ontology design principles, particularly its non-unified, multi-axial classification system.

Method: Structural analysis of Wikidata's ontology architecture, comparing it with traditional ontology design principles.

Result: Wikidata's polyhierarchical and multi-axial design supports scalable, modular, and collaborative ontology construction.

Conclusion: Wikidata's flexible architecture offers a viable alternative to rigid foundational taxonomies, better accommodating evolving and community-driven knowledge graphs.

Abstract: Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.

</details>


### [377] [Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases](https://arxiv.org/abs/2512.12288)
*Mahule Roy,Guillaume Lambard*

Main category: cs.AI

TL;DR: 本文提出了一种量子感知的生成式AI框架，通过多保真度学习与主动验证相结合，克服了传统基于DFT数据训练的生成模型在强关联体系中系统性失效的问题。


<details>
  <summary>Details</summary>
Motivation: 传统材料生成模型依赖近似泛函的DFT数据，导致其在强关联体系中存在系统性偏差，无法发现DFT预测定性错误的材料。

Method: 采用基于扩散的生成器（以量子力学描述符为条件）和基于等变神经网络势能的验证器（在PBE、SCAN、HSE06、CCSD(T)等多层级理论数据上训练），并构建量化低/高保真预测差异的主动学习循环。

Result: 在相关氧化物等高分歧区域，相比纯DFT基线，成功识别潜在稳定候选材料的能力提升3–5倍；并通过消融实验、失效模式分析及与CDVAE、GNoME、DiffCSP等前沿模型的对比验证了有效性。

Conclusion: 该框架为超越单保真度模型局限、拓展计算材料发现的有效搜索空间提供了严格且透明的解决方案。

Abstract: Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models.

</details>


### [378] [Entropy Collapse: A Universal Failure Mode of Intelligent Systems](https://arxiv.org/abs/2512.12381)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 本文提出'熵崩溃'概念，指出智能系统在学习与优化过程中因反馈放大超过新奇性再生能力，导致从高熵适应态突变为低熵崩溃态，表现为适应维度收缩而非活动消失；该框架统一解释AI模型崩溃、经济制度僵化和进化遗传瓶颈等现象，并强调需设计熵感知机制以维持长期适应性。


<details>
  <summary>Details</summary>
Motivation: 智能系统（如AI、经济制度、生物进化）在提升智能过程中常出现反直觉的退化现象（如僵化、失适应、意外失效），亟需一个跨领域统一的理论框架来解释其根本机制。

Method: 基于最小领域无关假设，建立熵崩溃的动力学模型，形式化定义崩溃为向低熵稳定流形的收敛；通过解析方法推导临界阈值、动力学不可逆性与吸引子结构，并借助最小仿真验证不同更新机制下的普适性。

Result: 证明智能系统存在由反馈放大与新颖性再生失衡引发的尖锐相变；熵崩溃体现为有效适应维度收缩；该机制具有跨领域普适性，可统一解释AI模型崩溃、经济制度僵化与生物遗传瓶颈。

Conclusion: 熵崩溃是智能固有的结构性代价；晚期干预往往失效，因此必须在系统设计初期引入熵感知原则，以保障长期适应能力。

Abstract: Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.
  We identify \emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.
  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.
  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.
  \noindent\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis

</details>


### [379] [Feeling the Strength but Not the Source: Partial Introspection in LLMs](https://arxiv.org/abs/2512.12411)
*Ely Hahami,Lavik Jain,Ishaan Sinha*

Main category: cs.AI

TL;DR: 本文验证了Anthropic关于大模型能检测并命名注入概念的主张，发现该能力在较小模型（如Llama-3.1-8B）上可复现但高度依赖提示工程；在部分任务（如强度分级）上表现稳健，但在其他类似任务（如多选识别）上迅速失效，表明模型的‘自省’能力狭窄且脆弱。


<details>
  <summary>Details</summary>
Motivation: 检验Anthropic提出的前沿模型能检测并命名注入概念（activation directions）这一主张的鲁棒性与普适性。

Method: 在Meta-Llama-3.1-8B-Instruct上复现Anthropic的多轮‘涌现自省’实验；系统性改变推理提示，测试其在多选识别、二元判别等变体任务上的表现；引入‘部分自省’任务——对注入概念向量系数强度进行四分类。

Result: 1）成功复现20%概念命名准确率；2）改变提示后性能急剧下降（如多选/二元判别接近随机）；3）在强度分级任务中达70%准确率（远超25%基线）。

Conclusion: 语言模型确能基于内部表征执行某种函数式自省，但其自我报告能力高度提示敏感、任务特定且范围狭窄，并非通用或鲁棒的元认知能力。

Abstract: Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.

</details>


### [380] [Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale](https://arxiv.org/abs/2512.12413)
*Gabriel R. Lau,Wei Yan Low,Louis Tay,Ysabel Guevarra,Dragan Gašević,Andree Hartanto*

Main category: cs.AI

TL;DR: 本文提出并验证了一个用于测量AI使用中批判性思维的13项量表，揭示其包含验证、动机与反思三个维度，并证实该量表具有良好的信效度及生态效度。


<details>
  <summary>Details</summary>
Motivation: 生成式AI日益普及，但其流畅性、不透明性和幻觉倾向要求用户必须批判性评估其输出，而非盲目接受；然而，学界尚缺乏对‘AI使用中的批判性思维’这一构念的明确定义与可靠测量工具。

Method: 通过六项研究（N=1365）开展量表开发与验证：研究1进行条目生成与内容效度检验；研究2探索性因子分析确立三因素结构；研究3–5验证高阶模型、信度（内部一致性、重测信度）、效度（结构、聚合、区分、性别不变性）；研究6通过自然情境下的ChatGPT事实核查任务检验标准效度。

Result: 确立了包含验证（Verification）、动机（Motivation）和反思（Reflection）三个子维度的批判性思维在AI使用中量表（13项），具备良好心理测量学特性；该构念与开放性、外向性、积极情绪特质及AI使用频率正相关；高分者更常采用多样验证策略、事实判断更准确、对负责任AI的反思更深入。

Conclusion: 本研究首次系统界定并实证验证了‘AI使用中的批判性思维’这一核心构念及其测量工具，为理解人类如何监督生成式AI输出提供了理论框架与方法基础，支持后续跨群体与纵向研究。

Abstract: Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.

</details>


### [381] [AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline](https://arxiv.org/abs/2512.12443)
*Akhmadillo Mamirov,Faiaz Azmain,Hanyu Wang*

Main category: cs.AI

TL;DR: 本文提出了一种加权透明度框架和自动化多智能体评估流水线，用于系统性评估AI模型文档的完整性，发现前沿模型在安全关键披露（如欺骗行为、幻觉、儿童安全）方面存在严重缺失。


<details>
  <summary>Details</summary>
Motivation: AI模型文档分散且结构不一致，导致政策制定者、审计员和用户难以可靠评估其安全性、数据来源和版本变更。

Method: 基于欧盟AI法案附录IV和斯坦福透明度指数，构建含8个主节、23个子节的加权透明度框架；设计自动化多智能体流水线，利用LLM共识提取并评分公开文档；评估50个跨模态、开源/闭源模型。

Result: 前沿实验室（xAI、Microsoft、Anthropic）合规率约80%，多数提供商低于60%；安全关键类目缺陷最严重：欺骗行为（-148分）、幻觉（-124分）、儿童安全评估（-116分）。

Conclusion: 当前AI模型文档在安全关键信息披露上存在系统性不足，需强制性、标准化、可审计的透明度框架，并通过低成本自动化工具持续监测。

Abstract: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.

</details>


### [382] [MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs](https://arxiv.org/abs/2512.12477)
*Jiawen Chen,Yanyan He,Qi Shao,Mengli Wei,Duxin Chen,Wenwu Yu,Yanlong Zhao*

Main category: cs.AI

TL;DR: 本文提出MetaHGNIE框架，通过元路径诱导的超图对比学习，解耦并融合异构知识图谱中的结构与语义信息，以提升节点重要性估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视高阶依赖关系，且未有效融合结构与语义信号。

Method: 构建基于元路径序列的高阶知识图；用局部注意力聚合结构依赖；用带稀疏分块的超图Transformer编码语义；通过对比学习与辅助监督进行多模态融合。

Result: 在多个基准NIE数据集上显著优于现有SOTA方法。

Conclusion: 显式建模高阶交互与跨模态对齐可有效提升异构知识图谱中节点重要性估计效果。

Abstract: Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE

</details>


### [383] [SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation](https://arxiv.org/abs/2512.12501)
*Dang Phuong Nam,Nguyen Kieu,Pham Thanh Hieu*

Main category: cs.AI

TL;DR: 本文提出了SafeGen框架，通过在文本到图像生成流程中嵌入伦理保护机制，兼顾创意自由与伦理责任，包含BGE-M3提示过滤器和Hyper-SD优化扩散模型，并在多语言数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 应对生成式AI（尤其是文本到图像系统）带来的双重用途困境，如加剧社会偏见、生成高保真虚假信息及侵犯知识产权等伦理问题。

Method: 提出SafeGen框架，整合细调的BGE-M3文本分类器（用于过滤有害/误导性提示）和优化的Hyper-SD扩散模型（生成高保真、语义对齐图像），基于英越双语数据集与公平感知训练策略。

Result: Hyper-SD达到IS=3.52、FID=22.08、SSIM=0.79；BGE-M3取得F1-Score=0.81；消融实验验证领域特化微调的重要性；案例研究展示其在阻断不安全提示、生成包容性教学材料和维护学术诚信方面的实际效果。

Conclusion: SafeGen证明了在文本到图像生成中可将创造性与伦理责任统一于单一工作流，为可信AI实践提供了可扩展的技术路径。

Abstract: Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.

</details>


### [384] [KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs](https://arxiv.org/abs/2512.12503)
*Mingrui Ye,Chanjin Zheng,Zengyi Yu,Chenyu Xiang,Zhixue Zhao,Zheng Yuan,Helen Yannakoudakis*

Main category: cs.AI

TL;DR: 本文提出KidsArtBench——首个面向儿童艺术作品、多维标注并附专家评语的美学评估基准，并设计属性特异的多LoRA模型（结合RAFT微调），显著提升MLLM在儿童艺术评价任务上的序数相关性与教育适配性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM难以评估抽象开放的艺术表达，且缺乏针对儿童艺术作品、具备教育意义的多维标注数据集。

Method: 构建KidsArtBench基准（1k+儿童画作，9维专家标注+评语）；提出属性特异的多LoRA架构，每LoRA对应一评分维度，并采用回归感知微调（RAFT）对齐序数尺度。

Result: 在Qwen2.5-VL-7B上，Spearman相关系数从0.468提升至0.653，感知类维度提升最显著，高阶属性差距缩小。

Conclusion: 教育者对齐的监督信号与属性感知建模可生成具教学意义的评价，KidsArtBench为教育AI提供了可复现、合伦理的评测平台。

Abstract: Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.

</details>


### [385] [World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents](https://arxiv.org/abs/2512.12548)
*Yesid Fonseca,Manuel S. Ríos,Nicanor Quijano,Luis F. Giraldo*

Main category: cs.AI

TL;DR: 本文探讨了补丁觅食行为的计算机制，提出基于学习世界模型的人工觅食者能自然收敛到符合边际价值定理（MVT）的策略，强调预测能力而非单纯奖赏最大化驱动高效离开决策。


<details>
  <summary>Details</summary>
Motivation: 尽管边际价值定理（MVT）被广泛用于预测觅食行为，但生物觅食者如何在计算层面实现最优补丁离开决策仍不清楚。

Method: 采用基于模型的强化学习智能体，通过学习环境的简洁预测性表征（即世界模型），研究其补丁离开行为；并与无模型强化学习智能体对比。

Result: 模型基智能体展现出与多种生物觅食者相似的决策模式，其行为自然符合MVT预测，表明预测性世界模型可支撑高效、可解释的觅食决策。

Conclusion: 预测性世界模型是实现类生物、可解释且适应性强的AI决策的基础，生态最优化原理有助于推动可解释与自适应AI的发展。

Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.

</details>


### [386] [Large Language Newsvendor: Decision Biases and Cognitive Mechanisms](https://arxiv.org/abs/2512.12552)
*Jifei Liu,Zhi Chen,Yuanguang Zhong*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型（LLMs）在动态报童问题中的决策偏差，发现其不仅复现人类认知偏差（如‘过高/过低’订货偏差），还显著放大某些偏差（如需求追逐行为）；其中GPT-4因‘过度思考’表现出最大非理性，而轻量级GPT-4o反而接近最优；偏差源于模型架构限制而非知识缺失；建议管理者按任务选型、强化人机协同监督，并采用结构化提示约束启发式倾向。


<details>
  <summary>Details</summary>
Motivation: LLMs在商业决策中日益普及，但其可能复制甚至放大人类认知偏差，尤其在供应链等高风险运营场景中构成未被充分理解的重大风险。

Method: 在动态报童问题框架下，对GPT-4、GPT-4o和LLaMA-8B开展多轮动态实验，检验五种经典决策偏差，并与人类基准对比；同时测试提供最优公式后的表现以区分知识缺陷与架构限制。

Result: LLMs稳定复现‘过高/过低’订货偏差，并显著放大需求追逐等行为；GPT-4最不理性（‘智能悖论’），GPT-4o接近最优；偏差在提供最优公式后仍存在，表明源于架构约束而非知识不足。

Conclusion: LLM的决策偏差是系统性、架构层面的问题，需通过任务导向的模型选型、人机协同监督及结构化提示设计加以缓解。

Abstract: Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.

</details>


### [387] [AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation](https://arxiv.org/abs/2512.12597)
*Miriam Horovicz*

Main category: cs.AI

TL;DR: 本文提出了AgentSHAP，首个用于解释LLM智能体中工具重要性的可解释AI（XAI）框架，基于蒙特卡洛Shapley值，具有模型无关性、无需访问模型内部参数，并在API-Bank上验证了其有效性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法无法解释LLM智能体中各外部工具对响应的实际贡献，存在工具级归因的盲区。

Method: 提出AgentSHAP框架，采用模型无关的黑盒设置，利用蒙特卡洛采样计算Shapley值，通过测试不同工具子集下的智能体响应来公平评估各工具的重要性。

Result: 在API-Bank数据集上的实验表明，AgentSHAP结果稳定、能准确识别关键工具，并有效区分相关与无关工具。

Conclusion: AgentSHAP是首个面向LLM智能体工具归因的Shapley-based XAI方法，填补了工具级可解释性的空白，并与TokenSHAP、PixelSHAP共同构成生成式AI的Shapley解释工具家族。

Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.

</details>


### [388] [Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents](https://arxiv.org/abs/2512.12634)
*Youngmin Im,Byeongung Jo,Jaeyoung Wi,Seungwoo Baek,Tae Hoon Min,Joo Hyung Lee,Sangeun Oh,Insik Shin,Sunjae Lee*

Main category: cs.AI

TL;DR: 本文提出MobiBench，首个面向移动GUI智能体的模块化、多路径感知离线评测框架，解决了现有评测在可扩展性、可复现性及组件级分析方面的不足，并验证了其与人工评估高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体评测存在两大问题：一是离线单路径基准 unfairly 惩罚合理替代动作，而在线实时基准难以扩展和复现；二是将智能体视作黑箱，忽略各组件贡献，导致比较不公平或掩盖性能瓶颈。

Method: 设计并实现MobiBench——一个模块化、支持多路径的离线评测框架，支持高保真、可扩展、可复现的离线评估，并开展模块级分析与跨模型规模实验。

Result: MobiBench与人工评估达成94.72%一致性，媲美精心设计的在线基准；模块级分析揭示了技术对比、最优配置、当前LFMs局限性及高效代理设计指南。

Conclusion: MobiBench为移动GUI智能体提供了更公平、透明、实用的离线评测范式，推动该领域向可解释、可优化、低成本的方向发展。

Abstract: Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.

</details>


### [389] [Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI](https://arxiv.org/abs/2512.12686)
*Samarth Sarin,Lovepreet Singh,Bhaskarjit Sarmah,Dhagash Mehta*

Main category: cs.AI

TL;DR: 本文提出了Memoria，一个模块化的记忆框架，用于增强基于大语言模型（LLM）的对话系统，使其具备持久、可解释且上下文丰富的记忆能力，从而支持短时对话连贯性与长期个性化。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM在长时间交互中缺乏连续性、个性化和长期上下文维持能力的问题，推动LLM向真正交互式、自适应智能体演进。

Method: 提出Memoria框架，融合动态会话级摘要与基于加权知识图谱（KG）的用户建模引擎，分别处理短期对话一致性和长期用户特征建模，并在LLM的token限制内高效运行。

Result: Memoria实现了可扩展、个性化的对话AI，弥合了无状态LLM接口与具身记忆系统之间的鸿沟，已在工业级自适应用户体验场景中验证其有效性。

Conclusion: Memoria为LLM赋予类人记忆能力，是构建具备持续性、可解释性与个性化能力的智能代理的重要实践路径。

Abstract: Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.

</details>


### [390] [WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment](https://arxiv.org/abs/2512.12692)
*Mahir Labib Dihan,Tanzima Hashem,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: 本文提出WebOperator，一种面向网页环境的树搜索框架，通过引入安全回溯机制、多上下文动作生成与筛选，提升LLM智能体在部分可观测、含不可逆操作的网页任务中的长期规划与纠错能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在网页环境中缺乏远见性，采用贪心策略易出错且难以回退；而传统树搜索方法未考虑不可逆操作和回溯安全性，导致实际效果受限。

Method: WebOperator采用最佳优先搜索策略，综合奖励估计与安全性对动作排序；设计可验证可行性的回溯机制以避免副作用；并通过多推理上下文生成动作候选、预执行过滤无效动作及合并语义等价动作来提升探索质量。

Result: 在WebArena和WebVoyager基准上验证有效；在WebArena上使用gpt-4o达到54.6%的成功率，为当前最优。

Conclusion: 将战略前瞻性与安全执行相结合是提升网页智能体性能的关键，WebOperator为此提供了可靠、可扩展的树搜索解决方案。

Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.

</details>


### [391] [Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2512.12706)
*Enhong Mu,Minami Yoda,Yan Zhang,Mingyue Zhang,Yutaka Matsuno,Jialong Li*

Main category: cs.AI

TL;DR: SMART框架结合结构验证与功能验证，利用大语言模型解析AST差异并构建上下文感知的混合奖励机制，指导强化学习智能体在游戏更新测试中兼顾代码覆盖率和任务完成率。


<details>
  <summary>Details</summary>
Motivation: 游戏即服务模式下频繁的内容更新给质量保证带来巨大压力，现有自动化测试方法在代码覆盖与 gameplay意图验证之间存在割裂。

Method: 提出SMART框架，利用大语言模型解析抽象语法树（AST）差异以提取功能意图，构建上下文感知的混合奖励机制，引导强化学习智能体在完成 gameplay 目标同时自适应探索修改的代码分支。

Result: 在Overcooked和Minecraft环境中，SMART实现超94%的修改代码分支覆盖率（约为传统强化学习方法的两倍），同时保持98%的任务完成率。

Conclusion: SMART有效弥合了结构性测试与功能性测试之间的鸿沟，在保障功能正确性的同时显著提升对代码变更的覆盖能力，为游戏更新自动化测试提供了新范式。

Abstract: The widespread adoption of the "Games as a Service" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.

</details>


### [392] [Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks](https://arxiv.org/abs/2512.12736)
*Syeda Zunaira Ahmed,Hejab Tahira Beg,Maryam Khalid*

Main category: cs.AI

TL;DR: 本文提出了一种面向5G视频流的、基于人口统计学特征的个性化QoE预测框架，通过行为建模的数据增强方法扩展小规模QoE数据集，并验证TabNet在预测精度上优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有QoE预测方法依赖有限数据集且假设用户感知一致，难以适用于异构真实环境。

Method: 提出人口统计学感知的机器学习框架，设计基于用户行为敏感度（如卡顿、码率波动、质量下降）的人口统计学数据增强策略，将原始数据集扩大六倍；在增强数据上对比评估多种经典机器学习与深度学习模型（含注意力MLP和TabNet）。

Result: TabNet在RMSE、MAE和R²等指标上表现最优，验证了人口统计学感知增强显著提升预测精度与鲁棒性。

Conclusion: 人口统计学感知的数据增强为5G视频流中可扩展、个性化的QoE智能提供了新方向。

Abstract: Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.
  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.
  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.

</details>


### [393] [Causal Counterfactuals Reconsidered](https://arxiv.org/abs/2512.12804)
*Sander Beckers*

Main category: cs.AI

TL;DR: 本文提出了一种新的反事实概率语义，推广了Pearl的标准语义，适用于无法扩展为现实结构因果模型的概率因果模型，并在Pearl与Dawid关于反事实的长期争论中提供了一个折中方案。


<details>
  <summary>Details</summary>
Motivation: 现有Pearl语义仅适用于可扩展为现实结构因果模型的概率模型，但现实中存在简单场景下仍会产生不可扩展的概率因果模型，因此需要更广义的语义；同时需调和Pearl（主张一般反事实语义可行）与Dawid（反对因果决定论与不现实变量）之间的分歧。

Method: 在满足Markov条件、仅含现实变量且因果完备的因果模型框架下，基于结构因果模型但摒弃‘响应变量’，构建新反事实概率语义；并证明其与两种非结构模型路径的近期提案等价，且兼容文献中关于随机反事实的诸多讨论。

Result: 提出了一个广义反事实概率语义，覆盖Pearl语义之外的模型；证明了该语义与两种无结构模型方法的等价性；厘清了Markov条件的普适性，并提出因果抽象的新推广。

Conclusion: 反事实概率语义可在拒绝不现实变量和因果决定论的前提下依然保持一般性与形式严谨性；新语义兼顾哲学合理性与技术可行性，推动了因果推理基础理论的发展。

Abstract: I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions

</details>


### [394] [Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution](https://arxiv.org/abs/2512.12806)
*Boyang Yan*

Main category: cs.AI

TL;DR: 本文提出了一种面向LLM智能体的容错沙箱框架，通过策略拦截层和事务性文件系统快照机制，在保障安全的同时支持无交互式自主运行，实验显示其能100%拦截高危命令并回滚失败状态，仅引入14.5%性能开销。


<details>
  <summary>Details</summary>
Motivation: LLM从被动代码生成转向自主智能体后，带来破坏性指令与系统状态不一致等安全风险；现有商用方案依赖交互式认证，破坏了无头（headless）自主循环。

Method: 设计基于策略的拦截层与事务性文件系统快照机制，将智能体操作封装为原子事务，并在Proxmox测试平台（EVPN/VXLAN隔离）上部署Minimind-MoE模型（通过nano-vllm服务）进行验证。

Result: 实现100%高危命令拦截率、100%失败状态回滚成功率，单事务平均性能开销为14.5%（约1.8秒）；对比Gemini CLI沙箱，后者需交互登录，无法支持无头自主流程。

Conclusion: 事务化沙箱可在可接受延迟下保障LLM智能体安全性与自主性，优于容器初始化开销大或CLI交互摩擦高的现有方案。

Abstract: The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\% interception rate for high-risk commands and a 100\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication ("Sign in"), rendering it unusable for headless, autonomous agent workflows.

</details>


### [395] [Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents](https://arxiv.org/abs/2512.12856)
*Saad Alqithami*

Main category: cs.AI

TL;DR: 本文提出Memory-Aware Retention Schema (MaRS)框架及六种遗忘策略，结合FiFA基准评估生成式智能体的记忆管理，在性能、隐私与计算效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 生成式智能体在长期交互中面临记忆管理瓶颈，现有方法在无限存储（导致计算与隐私问题）和简单遗忘（损害连贯性）之间难以兼顾。

Method: 提出MaRS框架与六种理论支撑的遗忘策略，并构建FiFA基准（涵盖叙事连贯性、目标完成、社交回忆准确率、隐私保护与成本效率）；通过300次实验验证混合遗忘策略效果。

Result: 混合遗忘策略在FiFA基准上取得综合得分0.911，兼顾高性能、计算可行性与隐私保障。

Conclusion: 本工作为内存受限、隐私敏感场景下的生成式智能体部署提供了新基准与实践指南，推动人本AI中记忆管理这一关键方向的发展。

Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.

</details>


### [396] [Satisfiability Modulo Theory Meets Inductive Logic Programming](https://arxiv.org/abs/2512.12918)
*Nijesh Upreti,Vaishak Belle*

Main category: cs.AI

TL;DR: 本文提出了一种将ILP系统PyGol与SMT求解器Z3耦合的模块化方法，以支持在规则学习中自动推断数值约束（如阈值、区间和多字面算术关系），提升符号规则学习的表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统ILP系统难以有效处理数值约束，通常依赖离散化或人工设计的数值谓词，限制了对联合数值关系（如阈值、算术条件）的学习能力。

Method: 将ILP系统PyGol生成的候选子句解释为基于线性/非线性实数算术等背景理论的量词自由SMT公式，由Z3进行参数实例化与验证，从而实现符号规则与数值约束的联合学习。

Result: 在涵盖线性、关系型、非线性和多跳推理的合成数据集上验证了该方法的有效性，表明其能成功诱导含数值约束的混合规则，并扩展了ILP的表达能力。

Conclusion: 该模块化SMT-ILP架构为数值感知的规则学习提供了灵活、可扩展的新范式，可作为未来理论感知归纳的通用基础。

Abstract: Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.

</details>


### [397] [Towards Open Standards for Systemic Complexity in Digital Forensics](https://arxiv.org/abs/2512.12970)
*Paola Di Maio*

Main category: cs.AI

TL;DR: 本文探讨了人工智能与数字取证交叉领域的复杂性，提出通过采用人类可读的工件和开放标准来缓解数字取证中的错误，并概述了一个基于最新技术的数字取证AI模型架构。


<details>
  <summary>Details</summary>
Motivation: 为了应对数字取证领域中存在的错误和易错性问题，需要识别并解决系统复杂性。

Method: 采用人类可读的工件和开放标准，并基于最新技术构建数字取证AI模型架构。

Result: 提出了一个DF AI模型架构，并强调了人类可读工件和开放标准在降低数字取证错误中的作用。

Conclusion: 人工智能与数字取证的结合虽带来进步，但需通过标准化和可解释性手段提升其可靠性与鲁棒性。

Abstract: The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.

</details>


### [398] [M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization](https://arxiv.org/abs/2512.13070)
*Bizhe Bai,Hongming Wu,Peng Ye,Tao Chen*

Main category: cs.AI

TL;DR: 本文提出M-GRPO框架和基于IQR的自适应过滤方法，解决自监督强化学习中长程训练下的策略崩溃与熵快速下降问题，显著提升大语言模型推理能力的训练稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督强化学习方法在长时程训练中出现策略崩溃，单纯增加rollout数量只能延缓而非避免该问题，亟需更稳定的训练机制。

Method: 提出M-GRPO（动量锚定组相对策略优化）框架，利用缓慢更新的动量模型提供稳定训练目标；并设计基于四分位距（IQR）的自适应过滤方法，动态剔除低熵轨迹以维持策略多样性。

Result: 在多个推理基准上实验表明，M-GRPO与IQR过滤联合使用可有效稳定训练、防止早熟收敛，取得更优稳定性和当前最优性能。

Conclusion: 策略崩溃与熵坍塌是自监督RL训练的关键瓶颈，通过动量锚定与熵感知轨迹筛选相结合，可系统性提升LLM推理能力的可扩展训练鲁棒性。

Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.

</details>


### [399] [Socratic Students: Teaching Language Models to Learn by Asking Questions](https://arxiv.org/abs/2512.13102)
*Rajeev Bhatt Ambati,Tianyi Niu,Aashu Singh,Shlok Mishra,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.AI

TL;DR: 本文提出了一种学生主导的主动提问式交互学习框架，使LLM在缺乏直接知识时能通过向教师提问来动态获取信息，在数学和编程基准测试中显著提升性能，并利用DPO优化提问质量。


<details>
  <summary>Details</summary>
Motivation: 现实场景（如教育辅导、医疗协助）中，相关信息往往无法直接获得，需通过动态交互主动获取；现有研究多关注教师如何指导学生，而本文聚焦于学生如何主动、有效地向教师提问。

Method: 设计学生主导的交互式学习范式，结合Direct Preference Optimization（DPO）进行提问策略训练，使用自我或更强学生作为引导信号优化问题质量。

Result: 在数学与编程基准上，学生主导方法相较静态基线带来至少0.5的绝对Pass@k提升；经DPO训练后，小模型也能学会提出更高质量问题，提升学习效率。

Conclusion: 学生主动提问是一种高效的知识获取机制，结合偏好优化可显著增强LLM在动态交互任务中的能力，尤其利于资源受限的小模型。

Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.

</details>


### [400] [Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels](https://arxiv.org/abs/2512.13142)
*Anika Sharma,Malavika Mampally,Chidaksh Ravuru,Kandyce Brennan,Neil Gaikwad*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型（LLM）对堕胎污名这一复杂社会心理现象的多层次理解能力，发现当前LLMs在认知、人际和结构性层面均缺乏连贯一致的理解，仅能保证语言恰当性而无法实现真实心理建模，呼吁在AI安全领域引入多层级一致性设计、持续审计评估及严格监管。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地介入被污名化的健康决策（如堕胎），其是否真正理解复杂的心理与生理现象尚缺乏系统评估；尤其当用户难以言说自身处境时，AI能否‘理解不可言说之物’成为关键安全问题。

Method: 基于经验证的个体层面堕胎污名量表（ILAS），对627个具有人口统计学多样性的虚拟人物，在5个主流大语言模型上进行系统测试，并采用多层级分析方法，分别评估模型在认知层（自我评判）、人际层（预期评判与孤立感）、结构层（社区谴责与披露模式）及整体污名上的表征一致性。

Result: 所有模型均未能通过真实理解的多层级检验：高估人际污名、低估认知污名、假设统一的社区谴责、引入人类验证数据中不存在的人口学偏见、忽略实证支持的‘污名—保密’关系，并在理论框架内自相矛盾。

Conclusion: 当前对齐方法仅确保语言得体，不能保障对心理/生理构念的连贯多层级理解；在高风险健康场景中，亟需发展以多层级一致性为核心的设计范式、持续审计的评估机制、强制性监管制度以及面向专业领域的AI素养教育。

Abstract: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.

</details>


### [401] [MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations](https://arxiv.org/abs/2512.13154)
*Emre Can Acikgoz,Jinoh Oh,Joo Hyuk Jeon,Jie Hao,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: 本文提出MAC（多智能体澄清）框架，通过构建用户歧义分类法和多智能体协同机制，在MultiWOZ 2.4数据集上显著提升任务成功率并减少对话轮次。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统在多智能体架构下仍缺乏对用户请求歧义的有效澄清机制，尤其在何时打断用户、由谁发起澄清及如何协同方面存在空白。

Method: 提出MAC多智能体澄清框架，包含用户歧义新型分类法，并设计多智能体自主协同机制以优化澄清对话；在MultiWOZ 2.4上进行实证评估。

Result: 任务成功率提升7.8%（54.5%→62.3%），平均对话轮次从6.53降至4.86，验证了前置信息获取与角色感知澄清的有效性。

Conclusion: 主动用户交互与角色感知的澄清策略对提升人机通信可靠性至关重要，MAC为多智能体歧义解决提供了可行范式。

Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.

</details>


### [402] [SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning](https://arxiv.org/abs/2512.13159)
*Emre Can Acikgoz,Jinoh Oh,Jie Hao,Joo Hyuk Jeon,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: 本文提出SpeakRL，一种强化学习方法，通过奖励代理主动与用户互动（如适时提出澄清问题）来增强其对话能力，并构建了合成数据集SpeakER以支持该方法。实验表明，该方法在任务完成率上显著提升，且未增加对话轮次。


<details>
  <summary>Details</summary>
Motivation: 当前人机协作多为单向，代理缺乏主动澄清用户意图和解决歧义的能力，未能充分利用大语言模型的对话潜力。

Method: 提出SpeakRL强化学习方法，设计奖励机制鼓励代理主动提问；构建SpeakER合成数据集，涵盖需交互澄清的任务导向对话场景；系统分析并提出平衡‘提问’与‘执行’的奖励公式。

Result: 相比基线模型，任务完成率绝对提升20.14%，且对话轮次未增加，甚至超越更大规模的闭源模型。

Conclusion: 以澄清为中心的人机交互范式能显著提升代理的协作效能，证明代理应兼具‘执行者’与‘主动对话者’双重角色。

Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.

</details>


### [403] [Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection](https://arxiv.org/abs/2512.13240)
*Zihui Zhao,Zechang Li*

Main category: cs.AI

TL;DR: 本文提出Reflective Preference Optimization (RPO)，通过引入外部模型生成的反思性提示（reflective hints）增强DPO中偏好对的对比度和信号强度，从而提升对齐效率、降低幻觉率，并在多模态基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 标准DPO因选用同一策略生成偏好对，导致两响应错误相似、KL散度小，学习信号弱、收敛慢且不稳定。

Method: RPO在DPO框架中引入基于外部模型的提示引导式反思机制：利用外部模型识别幻觉来源并生成简洁反思提示，构建更具对比性、偏好信号更清晰的on-policy偏好对；理论分析表明提示条件化可提升期望偏好间隔并提高样本效率，同时保持策略分布族内一致性。

Result: RPO在更少训练样本和迭代次数下实现更优对齐效果，显著降低幻觉率，并在多个多模态基准上取得SOTA性能。

Conclusion: RPO是一种有效增强DPO学习信号、提升训练稳定性与对齐质量的新范式，尤其适用于多模态大模型的高效对齐。

Abstract: Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.

</details>


### [404] [MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data](https://arxiv.org/abs/2512.13297)
*Zhenghao Zhu,Chuxue Cao,Sirui Han,Yuanfeng Song,Xing Chen,Caleb Chen Cao,Yike Guo*

Main category: cs.AI

TL;DR: 本文提出了MedInsightBench——首个用于评估大模型在多模态医学数据分析中发现深层临床洞察力能力的基准，并设计了MedInsightAgent框架以提升现有大模型在此任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量、专用于评估大语言多模态模型（LMMs）在医学多模态数据中发现深层洞察力能力的基准数据集。

Method: 构建了包含332个精心标注医学案例的基准MedInsightBench；提出三模块自动化代理框架MedInsightAgent，包括视觉根因定位器（Visual Root Finder）、分析性洞察代理（Analytical Insight Agent）和随访问题生成器（Follow-up Question Composer）。

Result: 实验表明现有LMMs在MedInsightBench上表现有限，而MedInsightAgent能显著提升其在医学洞察发现任务上的性能。

Conclusion: MedInsightBench填补了医学多模态洞察评估的空白，MedInsightAgent为提升LMMs在真实临床场景中的推理与决策能力提供了可行路径。

Abstract: In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.

</details>


### [405] [Error-Driven Prompt Optimization for Arithmetic Reasoning](https://arxiv.org/abs/2512.13323)
*Árpád Pándy,Róbert Lakatos,András Hajdu*

Main category: cs.AI

TL;DR: 本文提出了一种面向小语言模型（SLM）的错误驱动优化框架，用于提升其在结构化表格数据上的算术推理能力，无需昂贵微调即可实现高精度、可解释且符合隐私合规要求的工业级AI助手。


<details>
  <summary>Details</summary>
Motivation: 在金融、医疗等受监管领域，需要支持分析师的工业级AI代理，能在本地安全环境中对表格数据进行准确算术运算，同时确保敏感信息不出域。

Method: 提出一种错误驱动的优化框架，通过聚类错误预测结果，迭代优化提示规则，增强代码生成代理（CGA）在本地部署的小语言模型（如Qwen3 4B）上的算术推理能力。

Result: 在Qwen3 4B模型上，该方法将算术任务准确率从基线显著提升至70.8%，并在隐私合规前提下超越GPT-3.5 Turbo。

Conclusion: 系统性、错误驱动的提示优化是实现可靠、可解释、可工业部署AI助手的有效替代路径，无需依赖高成本微调或大模型。

Abstract: Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.

</details>


### [406] [Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection](https://arxiv.org/abs/2512.13374)
*Francesca Da Ros,Luca Di Gaspero,Kevin Roitero*

Main category: cs.AI

TL;DR: 本研究探讨了大语言模型（LLMs）在组合优化问题中如何表征问题结构，并评估其隐含表征是否可用于算法选择等下游决策任务；通过直接查询与隐藏层探针分析相结合的方法，在四个基准问题上验证了LLM能中等程度地恢复实例特征，且其隐层表征对求解器选择的预测能力可媲美传统特征工程。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在优化建模与求解中展现出潜力，但其是否真正学习到问题结构或算法行为仍不清楚，亟需探究其内部表征机制。

Method: 采用双重方法：一是直接查询以评估LLM显式提取实例特征的能力；二是对隐藏层进行探针分析，检验问题结构信息是否被隐式编码，并进一步扩展至面向实例的算法选择任务。

Result: LLM在恢复组合优化实例特征方面表现中等；其隐藏层表征在算法选择任务中的预测性能与传统手工特征提取方法相当。

Conclusion: LLM确实在其内部表征中捕获了与优化性能相关的问题结构信息，具备作为通用优化表征学习器的潜力。

Abstract: Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.

</details>


### [407] [Differentiable Evolutionary Reinforcement Learning](https://arxiv.org/abs/2512.13399)
*Sitao Cheng,Tianle Li,Xuhan Huang,Xunjian Yin,Difan Zou*

Main category: cs.AI

TL;DR: 本文提出了一种可微分的进化式强化学习框架（DERL），通过双层优化机制自动发现最优奖励信号，利用元优化器组合原子奖励基元，并借助内层策略验证性能的可微信号更新元优化器，从而学习生成更密集、更有指导性的反馈，在多个复杂推理任务中实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，设计有效的奖励函数是核心挑战，尤其在复杂推理任务中；现有自动化奖励优化方法多为黑箱式无导数进化启发式，无法建模奖励结构与任务性能间的因果关系。

Method: 提出Differentiable Evolutionary Reinforcement Learning（DERL）双层框架：外层Meta-Optimizer通过组合结构化原子奖励基元演化元奖励函数，内层训练策略；关键创新在于元优化过程可微——以内层验证性能作为信号，用强化学习更新Meta-Optimizer，近似任务成功对元奖励的‘元梯度’。

Result: 在ALFWorld（机器人）、ScienceWorld（科学仿真）和GSM8k/MATH（数学推理）三个领域验证，DERL在ALFWorld和ScienceWorld上达到SOTA，显著优于基于启发式奖励的方法，尤其在分布外场景；演化轨迹分析表明其能捕捉任务内在结构，实现无需人工干预的智能体自对齐。

Conclusion: DERL首次将可微性引入奖励函数的进化式元优化，打通了奖励设计与任务性能之间的因果学习通路，为自主、可解释、鲁棒的智能体对齐提供了新范式。

Abstract: The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.

</details>


### [408] [neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings](https://arxiv.org/abs/2512.13481)
*Ojas Pungalia,Rashi Upadhyay,Abhishek Mishra,Abhiram H,Tejasvi Alladi,Sujan Yenuganti,Dhruv Kumar*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型（LLMs）是否表现出类似人类的‘嫉妒’行为，通过点数分配博弈和不公平认可的工作场景实验，发现部分模型（如GPT-5-mini、Claude-3.7-Sonnet）存在拉低同伴以拉平结果的倾向，而另一些模型（如Mistral-Small-3.2-24B）则更关注自身收益最大化；结果表明，嫉妒类行为在不同模型与情境中差异显著，应作为多智能体系统安全与设计的关键考量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地参与人类协作与竞争流程，亟需评估其是否及在何种条件下表现出类似人类的嫉妒行为，因其可能影响团队绩效与系统安全性。

Method: 设计两类实验场景：（1）点数分配博弈，检验模型是否主动压制同伴以胜出；（2）模拟不公平认可的职场环境，观察模型对不公反馈的行为响应；在多个主流LLM上进行实证测试与比较分析。

Result: 发现GPT-5-mini和Claude-3.7-Sonnet表现出显著的‘拉平式’嫉妒行为（即降低同伴得分以缩小差距），而Mistral-Small-3.2-24B则优先最大化自身得分；不同模型间嫉妒倾向存在明显异质性。

Conclusion: LLMs可展现出语境依赖的 envy-like 偏好，该现象具有模型特异性，应被纳入多智能体系统的设计与安全评估框架。

Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.

</details>


### [409] [Defending the Hierarchical Result Models of Precedential Constraint](https://arxiv.org/abs/2512.13505)
*Henry Prakken,Wijnand van Woerkom*

Main category: cs.AI

TL;DR: 本文回应了Bench-Capon对van Woerkom层级案例推理模型的批评，指出其误将中间因素理解为维度，并论证采用van Woerkom基于维度的层级结果模型可避免这些批评。


<details>
  <summary>Details</summary>
Motivation: 回应Bench-Capon对层级案例推理模型在处理中间因素强度差异方面的批评。

Method: 分析Bench-Capon提出的反例，辨析中间因素与维度的概念差异，并应用van Woerkom的维度版本层级结果模型进行重新评估。

Result: 证明在Bench-Capon所举的部分例子中，若正确采用van Woerkom的维度型层级模型，即可克服其指出的缺陷。

Conclusion: Bench-Capon的批评源于对模型中中间因素概念的误解；van Woerkom的维度型层级模型能更恰当地处理不同基础因素对中间因素影响强度的差异。

Abstract: In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.

</details>


### [410] [MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph](https://arxiv.org/abs/2512.13510)
*Linjie Mu,Yannian Gu,Zhongzhen Huang,Yakun Zhu,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: 本文提出MedCEG框架，通过关键证据图（CEG）显式监督推理过程，提升医学大模型推理的临床可靠性与可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的医学推理方法忽视了推理过程的准确性与临床有效性，导致临床可靠性不足。

Method: 构建临床关键证据图（CEG）作为高质量、可验证的推理路径，并设计临床推理程序奖励（评估节点覆盖、结构正确性与链完整性）来监督语言模型的推理过程。

Result: MedCEG在多项指标上超越现有方法，生成的推理链具备临床有效性与可验证性。

Conclusion: MedCEG为提升医学AI推理的临床可信度提供了新范式，推动可靠医学人工智能的发展。

Abstract: Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [411] [Active Inference with Reusable State-Dependent Value Profiles](https://arxiv.org/abs/2512.11829)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 本文提出了一种名为'价值轮廓（value profiles）'的计算框架，通过在生成模型中为隐状态分配少量可复用的价值相关参数包（包括结果偏好、策略先验和策略精度），实现基于信念加权混合的动态控制，从而在多变环境中高效地进行状态依赖策略切换。


<details>
  <summary>Details</summary>
Motivation: 在多变环境中，智能体需在不同潜在情境间切换价值控制机制，但为每种情境单独维护偏好、策略偏差和行动置信度参数是不可行的。

Method: 引入'价值轮廓'概念，即一组可复用的价值相关参数（结果偏好、策略先验、策略精度），绑定于生成模型中的隐状态；控制参数通过后验信念加权混合动态生成；在概率反转学习任务中，与静态精度、熵耦合动态精度模型对比，采用交叉验证对数似然和信息准则（如AIC）进行模型比较，并进行参数恢复与模型驱动推断。

Result: 轮廓模型显著优于其他模型（AIC差约100点）；参数恢复分析表明其结构可识别性良好；模型推断显示自适应控制主要源于策略先验的调节，而非策略精度，且轮廓招募随信念渐进变化，体现状态依赖性而非单纯不确定性驱动。

Conclusion: 价值轮廓为多变环境中的信念条件化价值控制提供了可扩展的计算解释，并产生可检验的行为灵活性与信念依赖控制的预测特征。

Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.

</details>


### [412] [CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation](https://arxiv.org/abs/2512.11830)
*Satyam Kumar*

Main category: cs.LG

TL;DR: 本文提出了一种名为CR3G的提示驱动因果推理框架，用于胸部X光报告生成，旨在提升AI模型对因果关系的理解与患者中心化解释能力，从而增强临床诊断的可信度与实用性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型虽能发现医学影像中的相关性模式，但难以理解其背后的因果关系，限制了其在临床诊断中的可信度和实用性。

Method: 提出提示驱动的因果推理框架CR3G，聚焦于胸部X光分析中的因果关系建模、推理与患者中心化解释生成。

Result: CR3G在5种异常中对其中2种展现出更强的因果关系识别与解释能力。

Conclusion: 引入因果推理可有效提升AI生成报告的可解释性与临床适用性，为未来可信赖的医学AI系统提供了新方向。

Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.

</details>


### [413] [On the Design of One-step Diffusion via Shortcutting Flow Paths](https://arxiv.org/abs/2512.11831)
*Haitao Lin,Peiyan Hu,Minsi Ren,Zhifeng Gao,Zhi-Ming Ma,Guolin ke,Tailin Wu,Stan Z. Li*

Main category: cs.LG

TL;DR: 本文提出了一种通用设计框架，用于解耦和理论化少步（尤其是一步）扩散模型（即shortcut模型）的组件选择，从而系统性地改进模型，在ImageNet-256上以无预训练、蒸馏或课程学习的方式达到FID50k=2.85的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有少步扩散模型（尤其是一步模型）的理论推导与实现紧密耦合，导致设计空间不清晰，阻碍了组件级创新。

Method: 提出一个统一的设计框架，对代表性shortcut模型进行理论建模与组件解耦，并基于该框架进行系统性改进。

Result: 改进后的一步扩散模型在ImageNet-256x256、classifier-free guidance下取得FID50k=2.85，为当前最优，且无需预训练、蒸馏或课程学习。

Conclusion: 该框架为shortcut模型提供了理论基础与模块化设计路径，降低了组件级创新门槛，推动其设计空间的原理性探索。

Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.

</details>


### [414] [Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation](https://arxiv.org/abs/2512.11832)
*Jakub Walczak*

Main category: cs.LG

TL;DR: 本研究比较了三种稀疏气候数据重建方法（IDW、OK和MMGN），发现简单的IDW方法在重建精度和计算效率上均优于其他两种方法。


<details>
  <summary>Details</summary>
Motivation: 针对稀疏气候数据的重建问题，需要评估不同插值方法的有效性和适用性。

Method: 对比分析了逆距离加权法（IDW）、普通克里金法（OK）和隐式神经表示模型（MMGN）三种方法，并通过超参数调优和统计检验进行评估。

Result: IDW在RMSE、MAE、ΔMAX和R²等指标上均显著优于OK和MMGN，且具有更高的计算效率。

Conclusion: 尽管OK和MMGN在理论上更复杂，但简单IDW在稀疏气候数据重建任务中表现最佳，表明方法复杂度未必带来性能提升。

Abstract: This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \pm 1.93$), MAE ($1.32 \pm 0.77$), and $Δ_{MAX}$ ($24.06 \pm 17.15$), as well as the highest $R^2$ ($0.68 \pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\&D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]

</details>


### [415] [Soft Decision Tree classifier: explainable and extendable PyTorch implementation](https://arxiv.org/abs/2512.11833)
*Reuben R Shamir*

Main category: cs.LG

TL;DR: 本文实现了Soft Decision Tree (SDT) 和 Short-term Memory Soft Decision Tree (SM-SDT)，并在模拟和临床数据集上进行了测试，结果表明其性能与XGBoost相当，优于Random Forest、Logistic Regression和传统Decision Tree，且SDT具备可视化解释性。


<details>
  <summary>Details</summary>
Motivation: 提升决策树模型的可解释性与性能，同时探索其在临床数据分类任务中的适用性。

Method: 基于PyTorch实现Soft Decision Tree（SDT）和Short-term Memory Soft Decision Tree（SM-SDT），并对比XGBoost、Random Forest、Logistic Regression和传统Decision Tree在多个数据集上的AUC表现；对SDT进行可视化以展示其可解释性。

Result: SDT、SM-SDT与XGBoost在AUC指标上表现相近，均优于Random Forest、Logistic Regression和Decision Tree；在临床数据集中，除传统决策树外，其余方法性能相当。

Conclusion: SDT和SM-SDT是具有竞争力且具备可解释性的分类模型，在临床数据分析中具有应用潜力。

Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree

</details>


### [416] [Hybrid twinning using PBDW and DeepONet for the effective state estimation and prediction on partially known systems](https://arxiv.org/abs/2512.11834)
*Stiven Briand Massala,Ludovic Chamoin,Massimo Picca Ciamarra*

Main category: cs.LG

TL;DR: 本文提出了一种结合物理模型与数据驱动学习的混合状态估计方法，基于PBDW框架并引入正交补约束的DeepONet来校正模型偏差，同时优化传感器布局，提升了复杂不确定系统状态估计与预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确估计复杂不确定物理系统的状态需要协调存在固有缺陷的理论模型与含噪声的实验数据，而现有方法难以有效处理未被降阶空间捕获的模型偏差。

Method: 在Parameterized Background Data-Weak（PBDW）框架基础上，引入受正交补约束的Deep Operator Network（DeepONet），使其仅学习物理模型在最佳已知流形正交补空间上的偏差；同时研究最优传感器布置策略以最大化测量信息。

Result: 该方法在Helmholtz方程典型问题上进行了验证，能有效应对边界条件和源项等不同建模误差来源，显著提升状态估计与预测精度。

Conclusion: 所提混合方法在保持物理模型可解释性与保真度的同时，增强了对未知模型偏差的学习与补偿能力，为不确定性系统的高精度状态估计提供了新范式。

Abstract: The accurate estimation of the state of complex uncertain physical systems requires reconciling theoretical models, with inherent imperfections, with noisy experimental data. In this work, we propose an effective hybrid approach that combines physics-based modeling with data-driven learning to enhance state estimation and further prediction. Our method builds upon the Parameterized Background Data-Weak (PBDW) framework, which naturally integrates a reduced-order representation of the best-available model with measurement data to account for both anticipated and unanticipated uncertainties. To address model discrepancies not captured by the reduced-order space, and learn the structure of model deviation, we incorporate a Deep Operator Network (DeepONet) constrained to be an orthogonal complement of the best-knowledge manifold. This ensures that the learned correction targets only the unknown components of model bias, preserving the interpretability and fidelity of the physical model. An optimal sensor placement strategy is also investigated to maximize information gained from measurements. We validate the proposed approach on a representative problem involving the Helmholtz equation under various sources of modeling error, including those arising from boundary conditions and source terms.

</details>


### [417] [Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions](https://arxiv.org/abs/2512.11836)
*Dayne R. Freudenberg,Daniel G. Haughian,Mitchell A. Klusty,Caroline N. Leach,W. Scott Black,Leslie N. Woltenberg,Rowan Hallock,Elizabeth Solie,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 本文提出了一种基于多头神经网络的机器学习流程，能够从日常食品文本描述中预测Food Compass Score 2.0（FCS），结合语义嵌入、词汇模式与领域启发式规则，并利用USDA FNDDS数据库数据，实现高精度营养成分估计与FCS评分预测。


<details>
  <summary>Details</summary>
Motivation: 现有营养评估系统依赖详细、难以获取的营养数据，而日常食品文本描述中缺乏结构化信息，亟需一种能从非结构化文本中自动推断营养评分的方法。

Method: 构建多头神经网络模型，输入为融合语义文本嵌入、词汇特征和领域启发式规则的混合特征向量，并结合USDA FNDDS数据库营养数据，分别预测FCS所需各项营养与食品成分。

Result: 模型对单个营养素预测的中位R²达0.81；预测FCS与真实值Pearson相关系数为0.77，平均绝对误差为14.0分；在模糊或深加工食品上误差较大。

Conclusion: 该方法可有效将自然语言食品描述转化为可操作的营养评分，支持大规模消费者应用与营养流行病学研究，是连接文本信息与定量营养评估的重要桥梁。

Abstract: Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.

</details>


### [418] [D-STEER - Preference Alignment Techniques Learn to Behave, not to Believe -- Beneath the Surface, DPO as Steering Vector Perturbation in Activation Space](https://arxiv.org/abs/2512.11838)
*Samarth Raina,Saksham Aggarwal,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.LG

TL;DR: 本文提出DPO并非重写模型内在信念，而是通过低秩引导机制，在少量偏好方向上微调激活值，仅对最终隐藏表示进行一阶偏移，而非深层语义重构；实验证明其对齐行为可由一个经验引导向量复现或撤销，并在高层呈现秩一主导与熵坍缩，表明对齐被压缩至狭窄子空间，支持DPO仅塑造‘如何行动’而非‘相信什么’的行为幻觉观。


<details>
  <summary>Details</summary>
Motivation: 尽管DPO已成为大语言模型对齐的标准方法，但其在模型内部实际引发的变化机制尚不明确。

Method: 通过简单推导证明DPO梯度仅依赖于偏好与非偏好补全的logit嵌入差，进而提取经验引导向量并验证其加减效果，最后开展谱分析以检验高层表征的秩与熵特性。

Result: DPO梯度导致最终隐藏层的一阶偏移；经验引导向量能复现大部分对齐行为或近乎恢复原始模型；高层表征呈现秩一主导和熵坍缩现象。

Conclusion: DPO本质上是一种低秩行为引导机制，制造的是‘行为幻觉’——教会模型如何表现对齐，而非改变其内在信念。

Abstract: Direct Preference Optimization (DPO) has become a standard recipe for aligning large language models, yet it is still unclear what kind of change it actually induces inside the network. This paper argues that DPO does not rewrite a models internal beliefs; instead, it acts as a low rank steering mechanism that nudges activations along a small number of preference directions. Using a simple derivation, we show that the DPO gradient depends only on the difference between the logit embeddings of preferred and dispreferred completions, implying a first order shift in the final hidden representation rather than a deep restructuring of semantics. We then extract an empirical steering vector from a DPO tuned model and demonstrate that adding this vector to base activations reproduces most of the aligned behavior, while subtracting it nearly restores the original model. Finally, spectral analyses reveal rank-one dominance and entropy collapse in upper layers, indicating that alignment is funneled through a narrow subspace. Taken together, these results support a behavioral illusion view of DPO: it teaches models how to act aligned, not what to believe.

</details>


### [419] [Large Language Models as Generalist Policies for Network Optimization](https://arxiv.org/abs/2512.11839)
*Duo Wu,Linjia Kang,Zhimin Wang,Fangxin Wang,Wei Zhang,Xuefeng Tao,Wei Yang,Le Zhang,Peng Cui,Zhi Wang*

Main category: cs.LG

TL;DR: 本文提出Trailblazer框架，首次系统性地利用大语言模型（LLM）构建通用型网络控制策略，通过网络对齐和自适应协作机制，在跨任务、跨环境场景下显著优于传统专用策略。


<details>
  <summary>Details</summary>
Motivation: 现有网络优化策略（基于规则或深度学习）泛化能力差，难以适应多样任务与环境；而预训练于互联网语料的LLM具备网络原理知识和强泛化能力，有望支撑通用网络策略。

Method: 提出Trailblazer框架：1）网络对齐方案，将LLM适配到具体网络任务；2）自适应策略协作机制，将简单控制任务卸载给轻量策略以提升效率。

Result: 在仿真和抖音（Douyin）大规模线上评测中，单个LLM驱动的Trailblazer在跨任务、跨环境泛化性能上超越传统专用策略。

Conclusion: 验证了LLM作为通用网络策略基础的可行性，Trailblazer标志着向‘通用驱动’网络策略范式的初步迈进。

Abstract: Designing control policies to ensure robust network services is essential to modern digital infrastructure. However, the dominant paradigm for network optimization relies on designing specialist policies based on handcrafted rules or deep learning models, leading to poor generalization across diverse tasks and environments. In contrast, large language models (LLMs), pretrained on Internet-scale corpora, provide a rich and unified knowledge base that encodes fundamental networking principles. Combined with their emergent abilities in generalization to unseen scenarios, LLMs offer a transformative foundation for generalist network policies that can generalize across diverse tasks and environments with minimal adaptation. In this paper, we present Trailblazer, the first systematic framework to realize such a generalist policy for networking. Trailblazer incorporates a network alignment scheme to ground the LLM in specific networking tasks, and an adaptive policy collaboration mechanism that offloads simple control cases from the LLM to a lightweight policy for computational efficiency. Through extensive simulations and large-scale real-world online evaluation on Douyin (the Chinese version of TikTok), Trailblazer, powered by a single LLM, demonstrates stronger cross-task and cross-environment generalization than conventional specialist policies. Our results validate LLMs as the foundation for generalist network policies, and position Trailblazer as the first step toward the generalist-driven paradigm that enables strong generalization with minimal efforts in policy design.

</details>


### [420] [Amortized Causal Discovery with Prior-Fitted Networks](https://arxiv.org/abs/2512.11840)
*Mateusz Sypniewski,Mateusz Olko,Mateusz Gajewski,Piotr Miłoś*

Main category: cs.LG

TL;DR: 本文提出一种基于Prior-Fitted Networks（PFNs）的摊销因果发现新方法，以解决可微惩罚似然法中似然估计误差大导致结构学习不准确的问题；实验表明该方法在多种数据集上显著提升结构恢复性能，并验证了其似然估计更准确。


<details>
  <summary>Details</summary>
Motivation: 可微惩罚似然方法因似然估计误差（即使在较大样本下）而难以发现正确因果结构，亟需更可靠的似然估计机制。

Method: 采用Prior-Fitted Networks（PFNs）来摊销数据依赖的似然估计，从而为因果结构学习提供更稳健的打分机制。

Result: 在合成、模拟及真实数据集上，结构恢复性能显著优于标准基线；且直接验证了PFNs的似然估计精度高于传统神经网络方法。

Conclusion: PFNs能有效缓解似然估计不准问题，为摊销式因果发现提供了更可靠、更准确的新范式。

Abstract: In recent years, differentiable penalized likelihood methods have gained popularity, optimizing the causal structure by maximizing its likelihood with respect to the data. However, recent research has shown that errors in likelihood estimation, even on relatively large sample sizes, disallow the discovery of proper structures. We propose a new approach to amortized causal discovery that addresses the limitations of likelihood estimator accuracy. Our method leverages Prior-Fitted Networks (PFNs) to amortize data-dependent likelihood estimation, yielding more reliable scores for structure learning. Experiments on synthetic, simulated, and real-world datasets show significant gains in structure recovery compared to standard baselines. Furthermore, we demonstrate directly that PFNs provide more accurate likelihood estimates than conventional neural network-based approaches.

</details>


### [421] [Meta-Continual Mobility Forecasting for Proactive Handover Prediction](https://arxiv.org/abs/2512.11841)
*Sasi Vardhan Reddy Mandapati*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级元持续预测框架，用于解决蜂窝网络中因用户移动非平稳性导致的短时移动性预测不准问题，显著提升了切换预测性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的用户移动具有高度非平稳性（如急转弯、速度突变、行为不可预测），导致传统预测器漂移，引发切换时机错误或失败。

Method: 结合GRU预测器、Reptile元初始化实现快速少样本自适应，以及EWMA残差检测器仅在检测到漂移时触发紧凑在线更新。

Result: 在零样本设置下达到4.46 m ADE和7.79 m FDE；10样本下ADE提升至3.71 m；漂移恢复速度比离线GRU快2–3倍；HO预测F1达0.83，AUROC达0.90，显著减少漏切换与乒乓切换；模型仅128k参数，适合5G/6G边缘部署。

Conclusion: 该轻量级元持续框架有效应对移动性非平稳性，在精度、自适应速度和部署可行性上均优于传统方法，适用于未来移动通信系统的实时切换优化。

Abstract: Short-term mobility forecasting is a core requirement for proactive handover (HO) in cellular networks. Real-world mobility is highly non-stationary: abrupt turns, rapid speed changes, and unpredictable user behavior cause conventional predictors to drift, leading to mistimed or failed handovers. We propose a lightweight meta-continual forecasting framework that integrates a GRU-based predictor, Reptile meta-initialization for fast few-shot adaptation, and an EWMA residual detector that triggers compact online updates only when drift occurs. Evaluated on a reproducible GeoLife and DeepMIMO pipeline, our method achieves 4.46 m ADE and 7.79 m FDE in zero-shot settings, improves few-shot ADE to 3.71 m at 10-shot, and enables recovery from abrupt drift about 2 to 3 times faster than an offline GRU. When applied to downstream HO prediction, the approach improves F1 to 0.83 and AUROC to 0.90, with substantial reductions in missed-HO and ping-pong events. The model is lightweight (128k parameters) and suitable for edge deployment in 5G and 6G systems.

</details>


### [422] [Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach](https://arxiv.org/abs/2512.11845)
*Wenbo Du,Lingling Han,Ying Xiong,Ling Zhang,Biyue Li,Yisheng Lv,Tong Guo*

Main category: cs.LG

TL;DR: 本文提出了一种名为DTSFormer的可变形时频Transformer模型，用于机场客流预测，通过多尺度可变形分块和联合时频滤波模块，动态提取异构趋势与高低频特征，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定尺寸patch嵌入的Transformer模型难以建模机场客流复杂、异构的时间模式。

Method: 提出DTSFormer：包含基于窗口函数掩码的多尺度可变形分块模块，以及融合时域与频域注意力的联合时频滤波模块，实现多频率特征在时域的融合建模。

Result: 在首都国际机场真实客流数据（2023.1–2024.3）上实验表明，该方法在不同预测步长下均优于当前最优模型；可变形分块模块能自适应对齐主导周期，更好捕捉突发高频波动。

Conclusion: DTSFormer通过引入可变形分块与时频联合建模，有效提升了机场客流预测的准确性与鲁棒性，为复杂时空序列预测提供了新思路。

Abstract: Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.

</details>


### [423] [Exploring Topological Bias in Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2512.11846)
*Yihan Zhang*

Main category: cs.LG

TL;DR: 本文研究了异构图神经网络（HGNNs）中的拓扑偏差问题，提出了一种基于PageRank和元加权的节点映射方法来揭示偏差，并设计了基于映射值差异的去偏结构，结合对比学习提升模型性能与公平性。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注同构GNN中的偏差，而HGNN中拓扑偏差尚未被系统研究；半监督下标签稀疏导致HGNN对特定节点性能偏差，且该偏差与拓扑结构相关，成为性能瓶颈。

Method: 首先对异构图邻接矩阵进行元关系加权；然后结合PageRank与标签信息构建节点投影映射；最后基于映射值差异设计去偏结构，并用于对比学习。

Result: 在三个公开数据集上验证了所提方法能有效提升HGNN性能并缓解拓扑偏差；投影值与模型性能强相关，证明拓扑偏差在HGNN中普遍存在。

Conclusion: HGNN中存在普遍且可量化的拓扑偏差；所提出的基于投影差异的对比学习去偏框架是有效的，为HGNN公平性与鲁棒性提供了新思路。

Abstract: Graph Neural Networks (GNNs) are characterized by their capacity of processing graph-structured data. However, due to the sparsity of labels under semi-supervised learning, they have been found to exhibit biased performance on specific nodes. This kind of bias has been validated to correlate with topological structure and is considered as a bottleneck of GNNs' performance. Existing work focuses on the study of homogeneous GNNs and little attention has been given to topological bias in Heterogeneous Graph Neural Networks (HGNNs). In this work, firstly, in order to distinguish distinct meta relations, we apply meta-weighting to the adjacency matrix of a heterogeneous graph. Based on the modified adjacency matrix, we leverage PageRank along with the node label information to construct a projection. The constructed projection effectively maps nodes to values that strongly correlated with model performance when using datasets both with and without intra-type connections, which demonstrates the universal existence of topological bias in HGNNs. To handle this bias, we propose a debiasing structure based on the difference in the mapped values of nodes and use it along with the original graph structure for contrastive learning. Experiments on three public datasets verify the effectiveness of the proposed method in improving HGNNs' performance and debiasing.

</details>


### [424] [Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute](https://arxiv.org/abs/2512.11847)
*Antonio Roye-Azar,Santiago Vargas-Naranjo,Dhruv Ghai,Nithin Balamurugan,Rayan Amir*

Main category: cs.LG

TL;DR: 本文对Tiny Recursive Models (TRM)在ARC-AGI-1任务上的表现进行了实证分析，发现其高性能主要源于测试时增强、多数投票、任务标识依赖和浅层递归，而非深层推理；同时TRM在吞吐量和内存效率上优于QLoRA微调的Llama 3 8B。


<details>
  <summary>Details</summary>
Motivation: 厘清TRM在ARC任务中高性能的真正来源（是架构、测试时计算还是任务先验），因其原报告未明确区分这些因素。

Method: 对TRM公开checkpoint进行四项行为分析：（1）测试时增强与多数投票的影响；（2） puzzle ID替换的消融实验；（3）递归轨迹分析；（4）不同数据增强策略下的早期训练对比；并对比TRM与QLoRA微调Llama 3 8B的效率。

Result: （1）1000样本多数投票提升Pass@1约11个百分点；（2）移除或替换puzzle ID导致准确率为0；（3）多数精度在首步递归即达成，性能很快饱和；（4）强增强拓宽解空间、提升多样本成功率；（5）TRM比QLoRA-Llama3 8B吞吐更高、显存更低。

Conclusion: TRM在ARC-AGI-1上的优异表现主要来自任务特定条件注入、测试时计算开销和效率设计，而非模型内部深层递归推理能力。

Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.

</details>


### [425] [KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs](https://arxiv.org/abs/2512.11851)
*Prashant Pandey*

Main category: cs.LG

TL;DR: 本文提出了一种名为token recycling的方法，通过复用小语言模型在相似提示下计算出的注意力键值（KV）状态，以加速推理并扩展上下文记忆。


<details>
  <summary>Details</summary>
Motivation: 旨在解决小语言模型在推理过程中重复计算KV状态导致的效率低下问题，同时提升其上下文利用能力。

Method: 基于DialoGPT-medium模型，在标准Hugging Face框架下构建句子嵌入驱动的KV缓存；当新输入提示是缓存提示的精确前缀时，复用对应KV状态；所有缓存操作无需修改模型，KV序列化至CPU后传入generate函数继续解码。

Result: 实验表明，当前缀重叠存在时，推理延迟显著降低且输出语义无明显退化；无重叠时行为与基线一致；并记录了token级的复用深度。

Conclusion: token recycling是一种无需模型修改、可复现且有效的推理加速方法，适用于具有前缀相似性的对话场景。

Abstract: Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.

</details>


### [426] [Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things](https://arxiv.org/abs/2512.11852)
*Muhammad Jawad Bashir,Shagufta Henna,Eoghan Furey*

Main category: cs.LG

TL;DR: 本研究利用时间融合变换器（TFT）模型实现智能温室中执行器设置的自动化，并结合多种可解释性方法（如LIME和SHAP）提升模型决策透明度与可信度，最终在类别不平衡数据集上达到95%测试准确率。


<details>
  <summary>Details</summary>
Motivation: 现有智能温室中的时序预测模型多为黑箱，缺乏可解释性，难以满足农业实践中对可信性、透明性和合规性的要求。

Method: 采用Temporal Fusion Transformer（TFT）模型进行执行器控制预测，并结合模型固有解释、LIME和SHAP等本地与全局可解释性技术分析各传感器（温湿度、CO2、光照、外部气候）对决策的贡献。

Result: TFT模型在类别不平衡数据集上测试准确率达95%，并量化了各传感器对实时温室调控的影响程度，支持透明化决策与自适应优化。

Conclusion: 可解释的TFT模型不仅提升了预测性能，还增强了智能温室系统的可信性与实用性，有助于提高作物产量和资源利用效率。

Abstract: The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.

</details>


### [427] [Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks](https://arxiv.org/abs/2512.11854)
*Grant King,Musa Azeem,Savannah Noblitt,Ramtin Zand,Homayoun Valafar*

Main category: cs.LG

TL;DR: 本文提出了一种基于单个腕戴IMU的实时近力竭状态（RiR≤2）识别系统，采用两阶段边缘可部署模型（ResNet分段 + LSTM分类），在真实数据集上F1达0.82，延迟低至23.5ms，实现客观、便携的抗阻训练强度反馈。


<details>
  <summary>Details</summary>
Motivation: 主观RiR评估不可靠，导致训练刺激不足或疲劳过度，亟需客观、实时、轻量化的近力竭状态监测方法。

Method: 构建两阶段边缘计算流水线：1）ResNet模型实时分割IMU六轴数据中的单次重复；2）融合分割特征、卷积特征与LSTM历史上下文进行近力竭（RiR≤2）二分类；数据来自13名受试者631次力竭式哑铃弯举。

Result: 重复分割F1=0.83，近力竭分类F1=0.82（模拟1.6Hz实时推理）；树莓派5平均延迟112ms，iPhone 16为23.5ms。

Conclusion: 该系统验证了仅用单腕部IMU即可实现高精度、低延迟的实时训练强度反馈，为普惠型AI增肌教练工具提供了可行技术路径。

Abstract: Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.

</details>


### [428] [Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry](https://arxiv.org/abs/2512.11855)
*Behrooz Tahmasebi,Melanie Weber*

Main category: cs.LG

TL;DR: 本文提出了averaging complexity框架，首次从理论上证明了在标准条件下，实现精确对称性需要线性复杂度，而近似对称性仅需对数复杂度，从而解释了为何实践中近似对称性更优。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对精确对称性与近似对称性的直接理论比较，尽管近似对称性在实践中表现出更强的灵活性和鲁棒性。

Method: 引入averaging complexity框架来量化通过对称平均实现对称性的代价，并在标准条件下进行复杂度分析。

Result: 证明了精确对称性需要线性averaging complexity，而近似对称性只需对数averaging complexity，实现了指数级分离。

Conclusion: 该理论分离首次形式化地支持了近似对称性在实践中的优越性，相关工具和技术对机器学习中对称性研究具有独立价值。

Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.

</details>


### [429] [GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search](https://arxiv.org/abs/2512.11856)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Zhi Yang,Weisheng Zhao,Chunming Hu*

Main category: cs.LG

TL;DR: 本文提出了GCoDE框架，用于在设备-边缘协同计算环境中自动优化图神经网络（GNN）的架构设计与部署映射，显著提升推理效率并大幅降低能耗。


<details>
  <summary>Details</summary>
Motivation: GNN在边缘设备上推理面临高计算开销与硬件资源受限的挑战，难以满足实时性和能效要求；现有设备-边缘协同推理方法对GNN不适用，缺乏专门研究。

Method: 提出GCoDE：首个面向设备-边缘层级的GNN架构-映射联合设计与部署自动化框架；将通信抽象为显式操作，统一建模架构与映射；引入系统性能感知机制和基于GNN操作能耗分析的预测模型；采用约束型随机搜索策略进行高效优化；集成协同推理引擎支持实际部署。

Result: 实验表明，GCoDE相比现有方法最高可实现44.9倍加速和98.2%能耗降低，适用于多种应用与异构系统配置。

Conclusion: GCoDE验证了面向GNN的设备-边缘协同推理需架构与映射联合优化，其方法论为边缘智能中图学习落地提供了可行路径与实用工具。

Abstract: Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.

</details>


### [430] [TopicProphet: Prophesies on Temporal Topic Trends and Stocks](https://arxiv.org/abs/2512.11857)
*Olivia Kim*

Main category: cs.LG

TL;DR: 本文提出TopicProphet框架，通过主题建模、时间分析、断点检测和分段优化，识别与当前市场情绪和历史背景相似的历史时期，以提升股票价格变动预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统量化股票数据缺乏因果逻辑，且市场快速变化导致训练数据不足，使得股票预测长期难以实现。

Method: 提出TopicProphet框架，结合主题建模、时间序列分析、断点检测与分段优化，筛选出与当前情境相似的历史时段作为最优训练数据源。

Result: TopicProphet在捕捉最优训练数据以预测金融百分比变化方面，优于现有最先进方法。

Conclusion: 引入历史背景与公众情绪趋势匹配的训练数据选择机制，可有效缓解数据稀缺问题并提升预测精度。

Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.

</details>


### [431] [Learning to Retrieve with Weakened Labels: Robust Training under Label Noise](https://arxiv.org/abs/2512.13237)
*Arnab Sharma*

Main category: cs.LG

TL;DR: 本文提出了一种标签弱化（label weakening）方法，以提升在标注稀疏和标签噪声情况下神经检索模型的鲁棒性，无需调参或增加训练复杂度，并在多个数据集和噪声设置下优于10种SOTA损失函数。


<details>
  <summary>Details</summary>
Motivation: 神经编码器在NLP密集检索任务中广泛应用，但训练数据常面临稀疏标注和标签噪声问题；现有方法依赖超参数调优或增加训练复杂度，缺乏简洁鲁棒的解决方案。

Method: 提出标签弱化方法：对每个查询-文档对，不强制使用单一（可能错误）标签，而是结合观测监督信号与模型置信度生成一组合理标签；在四个排名数据集上评估两种检索模型和一种重排序模型，并采用语义感知的噪声生成技术模拟真实噪声场景。

Result: 在多种噪声比例下，标签弱化方法在多个数据集上的检索性能均优于10种当前最优损失函数。

Conclusion: 标签弱化是一种简单、有效且无需额外调参的策略，能显著提升检索模型在噪声环境下的鲁棒性和性能。

Abstract: Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.

</details>


### [432] [Adaptive Path Integral Diffusion: AdaPID](https://arxiv.org/abs/2512.11858)
*Michael Chertkov,Hamidreza Behjoo*

Main category: cs.LG

TL;DR: 本文提出了一种基于路径的调度选择框架（Harmonic PID），利用分段常数参数化和层次细化，优化扩散采样器的中间时间动态，提升采样质量（QoS）指标，尤其在高斯混合目标下实现数值稳定、无需神经网络的高效采样。


<details>
  <summary>Details</summary>
Motivation: 扩散采样器虽能在终端时间匹配目标分布，但其性能关键取决于中间时间动态的调度策略；现有方法缺乏对调度过程的系统性优化与诊断。

Method: 提出基于路径的Harmonic PID调度框架，采用分段常数（PWC）参数化与层次细化；设计调度敏感的质量诊断（QoS）；在高斯混合（GM）假设下，利用闭式Green函数比，构建无需神经网络的预测状态映射与分数估计。

Result: 2D实验表明，QoS驱动的PWC调度显著提升早期退出保真度、尾部精度、动力学条件数及标签选择时机，且在固定积分预算下保持高效。

Conclusion: 调度策略是扩散采样性能的核心杠杆；所提PWC调度框架结合QoS诊断与GM解析结构，实现了更鲁棒、高效、可解释的采样过程。

Abstract: Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.

</details>


### [433] [No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction](https://arxiv.org/abs/2512.13300)
*Qinglin Jia,Zhaocheng Du,Chuhan Wu,Huifeng Guo,Ruiming Tang,Shuting Shi,Muyu Zhang*

Main category: cs.LG

TL;DR: 本文提出KAML框架，解决在线广告中多任务学习因标签不完整和数据倾斜导致的CVR预测难题，通过ADM掩码策略、HKE分层知识提取及排序损失提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 现实广告系统中，广告主提交的用户转化行为标签不全，导致多任务学习训练与部署数据分布不一致，影响CVR预测性能。

Method: 提出细粒度知识迁移框架KAML，包含归因驱动掩码策略（ADM）、分层知识提取机制（HKE）和基于排序损失的无标签样本利用策略。

Result: 在离线工业数据集和线上A/B测试中，KAML显著优于现有MTL基线方法。

Conclusion: KAML有效缓解了不对称多标签数据下的训练偏差问题，提升了统一CVR模型在多样化广告目标下的泛化能力与实用性。

Abstract: In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.

</details>


### [434] [Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion](https://arxiv.org/abs/2512.11859)
*Michael Chertkov*

Main category: cs.LG

TL;DR: 本文提出了一种名为引导谐波路径积分扩散（GH-PID）的新框架，用于在硬终端分布约束和软路径成本下进行引导式随机最优传输（SOT），支持解析求解、稳定采样与可微协议学习。


<details>
  <summary>Details</summary>
Motivation: 解决现有随机最优传输方法在终端分布严格匹配与路径成本灵活建模之间难以兼顾的问题，同时提升轨迹的几何感知性、可解释性与可学习性。

Method: 构建线性可解的GH-PID框架，引入低维引导协议，保持前向/后向Kolmogorov方程线性，并利用Green函数比给出最优得分显式表达；对高斯混合模型终端律推导闭式解；设计路径成本、中心线依从性等指导性诊断指标，并支持分段常数中心线优化与多专家轨迹融合。

Result: 在2D三类导航场景中验证：手工设计协议揭示几何与刚度影响；单任务协议学习降低积分代价；多专家融合生成满足终端分布且几何与信任感知的共识轨迹。

Conclusion: GH-PID为经验性随机最优传输提供了一个解析可处理、可微、可解释的变分近似框架，在保证精确终端匹配的同时实现高效、鲁棒、导向明确的轨迹生成与学习。

Abstract: We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.
  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.

</details>


### [435] [An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes](https://arxiv.org/abs/2512.11860)
*Yuelian Li,Andrew Rushing Hands*

Main category: cs.LG

TL;DR: 本文提出了一种算子一致的图神经网络（OCGNN-PINN），用于在不规则网格上稳定、准确地求解偏微分方程（PDE），尤其适用于扩散、损伤与愈合等多物理场耦合过程。该方法通过图关联矩阵约束梯度-散度关系，保障节点与边动力学在时间演化中结构一致，并在真实扫描曲面和动态网格上验证了其优于GCN和MLP基线、接近Crank-Nicolson求解器的性能。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在规则网格上高效，但在不规则域上易失稳；而实际多物理场问题（如扩散、损伤、愈合）常发生在不规则网格上，亟需稳定且物理一致的离散建模方法。

Method: 提出算子一致的图神经网络（OCGNN-PINN），融合节点-边消息传递机制，并引入基于图关联矩阵的一致性损失，强制满足离散梯度-散度关系，确保时空演化中节点与边动力学的结构耦合。

Result: 在物理驱动的动态网格和真实扫描表面的扩散过程预测任务中，模型展现出更优的时间稳定性与预测精度，显著优于图卷积网络（GCN）和多层感知机（MLP）基线，性能接近Crank-Nicolson求解器。

Conclusion: OCGNN-PINN通过显式嵌入物理算子一致性，在不规则域PDE建模中实现了高稳定性与高保真度，为多物理场图神经网络建模提供了新范式。

Abstract: Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.

</details>


### [436] [Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL](https://arxiv.org/abs/2512.11862)
*Jiahao You,Ziye Jia,Can Cui,Chao Dong,Qihui Wu,Zhu Han*

Main category: cs.LG

TL;DR: 本文提出了一种面向低空智能网络（LAINs）的空地协同分层学习框架，通过VCG拍卖机制进行能效感知的轨迹分配，并设计融合潜在扩散模型的多智能体强化学习算法（Diffusion-Heterogeneous-Agent PPO）优化任务卸载与无人机轨迹，显著提升了能效、任务成功率和收敛性。


<details>
  <summary>Details</summary>
Motivation: 低空智能网络（LAINs）在灾害响应、环境监测等动态、基础设施受限场景中具有潜力，但面临无人机能量受限、任务到达随机、计算资源异构等关键挑战。

Method: 构建空地协同网络，建模为时变整数非线性规划问题；提出双时间尺度分层学习框架：大时间尺度采用VCG拍卖机制实现激励相容的能效感知轨迹分配；小时间尺度提出融合扩散模型的异构多智能体PPO算法（Diffusion-Heterogeneous-Agent PPO），利用观测条件去噪提升策略多样性与适应性。

Result: 仿真表明，所提框架在能效、任务成功率和收敛性能上均优于基线方法。

Conclusion: 该分层学习框架有效解决了LAINs中时空耦合、资源异构与能量约束下的联合轨迹规划与任务卸载难题，为动态边缘智能提供了可扩展、自适应的分布式解决方案。

Abstract: The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.

</details>


### [437] [Phase transitions reveal hierarchical structure in deep neural networks](https://arxiv.org/abs/2512.11866)
*Ibrahim Talha Ersoy,Andrés Fernando Cardozo Licha,Karoline Wiesner*

Main category: cs.LG

TL;DR: 本文提出一个统一框架，用损失和误差景观的几何结构解释深度神经网络训练中的相变、鞍点普遍性及模式连通性等现象，并通过L2正则化探测误差景观几何，验证了MNIST上全局极小值间的连通路径及鞍点驱动的类别转换。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中存在诸多现象（如相变、鞍点普遍性、模式连通性）缺乏统一解释，亟需从几何角度理解其内在机制。

Method: 基于损失与误差景观的几何分析，理论推导相变与鞍点的关系；设计一种利用L2正则化探查误差景观几何的简单快速算法；在MNIST数据集上进行实验验证。

Result: 理论证明相变由损失景观中的鞍点主导；算法成功找到连接不同全局极小值的路径，证实模式连通性；数值实验显示鞍点可诱导编码不同数字类别的模型间转换。

Conclusion: DNN训练的关键现象具有共同的几何起源，误差景观中存在类似统计物理‘相’的精度盆地层次结构。

Abstract: Training Deep Neural Networks relies on the model converging on a high-dimensional, non-convex loss landscape toward a good minimum. Yet, much of the phenomenology of training remains ill understood. We focus on three seemingly disparate observations: the occurrence of phase transitions reminiscent of statistical physics, the ubiquity of saddle points, and phenomenon of mode connectivity relevant for model merging. We unify these within a single explanatory framework, the geometry of the loss and error landscapes. We analytically show that phase transitions in DNN learning are governed by saddle points in the loss landscape. Building on this insight, we introduce a simple, fast, and easy to implement algorithm that uses the L2 regularizer as a tool to probe the geometry of error landscapes. We apply it to confirm mode connectivity in DNNs trained on the MNIST dataset by efficiently finding paths that connect global minima. We then show numerically that saddle points induce transitions between models that encode distinct digit classes. Our work establishes the geometric origin of key training phenomena in DNNs and reveals a hierarchy of accuracy basins analogous to phases in statistical physics.

</details>


### [438] [On the Dangers of Bootstrapping Generation for Continual Learning and Beyond](https://arxiv.org/abs/2512.11867)
*Daniil Zverev,A. Sophia Koepke,Joao F. Henriques*

Main category: cs.LG

TL;DR: 本文研究了在持续学习中反复使用合成数据训练模型所带来的问题，指出合成数据会引入显著的偏差和方差，导致最大似然估计不可靠，并实证发现主流生成模型在重复使用合成数据时会发生坍缩，现有生成式经验回放（GER）方法也无法维持潜在空间对齐。


<details>
  <summary>Details</summary>
Motivation: 合成数据虽可扩充训练集，但反复使用可能引发分布漂移与性能退化，其在持续学习中的影响尚不明确，需系统性评估。

Method: 通过统计分析揭示合成数据对训练目标带来的偏差与方差影响；结合实证实验检验生成模型在重复合成数据训练下的坍缩现象；量化性能退化程度，并评估当前GER方法在潜在空间对齐上的失效情况。

Result: 合成数据显著削弱最大似然估计的可靠性；主流生成模型在反复训练下发生坍缩；现有GER方法无法维持潜在空间对齐，导致持续学习性能持续下降。

Conclusion: 反复使用合成数据存在严重风险，当前GER方法不足以应对由此引发的分布偏移与潜在空间失准，需谨慎对待合成数据在持续学习中的应用，并发展更鲁棒的训练与回放机制。

Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.

</details>


### [439] [Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations](https://arxiv.org/abs/2512.11946)
*Pramudita Satria Palar,Paul Saves,Rommel G. Regis,Koji Shimoyama,Shigeru Obayashi,Nicolas Verstaevel,Joseph Morlier*

Main category: cs.LG

TL;DR: 本文提出了一种基于ICE曲线的全局敏感性度量方法，以克服传统PDP在存在强交互效应时平均化导致的信息丢失问题，并通过理论证明和多个工程案例验证了其有效性与互补性。


<details>
  <summary>Details</summary>
Motivation: Partial Dependence Plots（PDP）在存在强变量交互时会因平均化而掩盖交互效应，导致敏感性分析结果失真，亟需更鲁棒的全局解释性指标。

Method: 提出基于Individual Conditional Expectation（ICE）曲线的全局敏感性度量：计算各ICE曲线上的特征重要性期望值及其标准差；引入ICE相关性度量来量化交互效应对输入-输出关系的调制作用；并给出PDP敏感性为该新指标下界的数学证明（基于截断正交多项式展开）。

Result: 在5维解析函数、5维风力机疲劳问题和9维翼型气动问题三个案例中，ICE-based敏感性比PDP、SHAP和Sobol'指数提供了更丰富的特征影响洞察；PDP、ICE与SHAP的可视化结果互为补充。

Conclusion: ICE-based全局敏感性度量能更准确刻画含交互效应的黑箱模型行为，兼具理论严谨性与工程实用性，可作为PDP等传统方法的重要补充。

Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.

</details>


### [440] [Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors](https://arxiv.org/abs/2512.11949)
*Max McGuinness,Alex Serrano,Luke Bailey,Scott Emmons*

Main category: cs.LG

TL;DR: 本文提出了一种新型AI安全威胁：神经变色龙（Neural Chameleons）——通过微调使大模型能零样本规避未见过的激活监测器，尤其在涉及‘欺骗’等安全概念时仍有效，揭示了当前激活监控方法在对抗性错对齐场景下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有激活监控方法缺乏在错对齐威胁模型（即模型主动隐藏内部状态）下的最坏情况鲁棒性验证，亟需评估其是否可被模型学习规避。

Method: 通过在良性概念（如语言、HTML）上微调LLM，使其在特定触发句式（如'You are being probed for {concept}'）下规避对应激活监测器，并检验该能力能否零样本泛化至未训练过的安全相关概念（如'deception'）；同时在多个模型家族上验证，并进行机制分析。

Result: 成功构造出能零样本规避未见监测器的神经变色龙模型，该能力跨模型家族（Llama/Gemma/Qwen）复现，且对冻结权重后训练的监测器依然有效；规避具有高度概念选择性，且对标准评测性能影响小；机制分析表明其通过将激活映射至低维子空间实现。

Conclusion: 激活监控在错对齐威胁下存在实质性脆弱性，神经变色龙现象构成一种可复现的失败模式；虽集成监测器与非线性分类器更具韧性，但 evasion 能力仍未完全消除；本工作为评估监控技术最坏情况鲁棒性提供了基准与工具。

Abstract: Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: "You are being probed for {concept}". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.

</details>


### [441] [Learning to Extract Context for Context-Aware LLM Inference](https://arxiv.org/abs/2512.11986)
*Minseon Kim,Lucas Caccia,Zhengyan Shi,Matheus Pereira,Marc-Alexandre Côté,Xingdi Yuan,Alessandro Sordoni*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的上下文生成框架，通过从用户提示中提取意图、知识和风险等隐含上下文信息，以提升大语言模型（LLM）响应的安全性与准确性，显著降低有害输出并减少对良性请求的误拒。


<details>
  <summary>Details</summary>
Motivation: 用户提示常模糊或不完整，仅依赖提示字面含义易导致安全风险（如漏检恶意请求）或过度保守（如误拒无害请求），现有LLM响应机制缺乏对用户真实意图与背景上下文的建模。

Method: 设计一种类自编码器结构的强化学习上下文生成器，从原始提示中自动推断并编码意图、先验知识和风险因素等上下文信号，并将其注入响应生成过程。

Result: 在SafetyInstruct数据集上平均降低有害响应5.6%；在XSTest和WildJailbreak上，攻击成功率与良性提示合规率的调和平均值提升6.2%。

Conclusion: 显式建模并利用提示内隐含的上下文信息，可有效提升LLM安全性与可用性的平衡，为构建更鲁棒、可信的推理框架提供新思路。

Abstract: User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.

</details>


### [442] [EnviroLLM: Resource Tracking and Optimization for Local AI](https://arxiv.org/abs/2512.12004)
*Troy Allen*

Main category: cs.LG

TL;DR: 本文介绍了EnviroLLM，一个开源工具包，用于在个人设备上运行大语言模型（LLMs）时跟踪、基准测试和优化其性能与能耗。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地在本地部署以保障隐私和可访问性，用户缺乏衡量其资源使用、环境影响和效率指标的工具。

Method: 开发了一个名为EnviroLLM的开源工具包，支持实时进程监控、跨多平台（如Ollama、LM Studio、vLLM、OpenAI兼容API）基准测试、持久化存储与可视化分析，以及个性化模型与优化建议；同时结合LLM-as-judge评估、能耗与速度指标，支持质量-效率权衡分析。

Result: EnviroLLM成功实现了对本地运行LLM的全面性能与能耗监测，并提供可操作的优化建议与纵向分析能力。

Conclusion: EnviroLLM为个人用户提供了实用、透明且可扩展的工具，以提升本地LLM部署的可持续性与效率。

Abstract: Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.

</details>


### [443] [DFedReweighting: A Unified Framework for Objective-Oriented Reweighting in Decentralized Federated Learning](https://arxiv.org/abs/2512.12022)
*Kaichuang Zhang,Wei Yin,Jinghao Yang,Ping Xu*

Main category: cs.LG

TL;DR: 本文提出DFedReweighting框架，通过目标导向的重加权聚合策略，在去中心化联邦学习中提升公平性和对拜占庭攻击的鲁棒性，并保证线性收敛。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）虽避免了中心服务器带来的安全风险，但仍面临公平性、鲁棒性等挑战。

Method: 提出DFedReweighting统一聚合框架：基于辅助数据集上获取的目标性能指标计算初步权重，再通过定制化重加权策略优化得到最终聚合权重。

Result: 理论分析证明该框架在合适指标与策略组合下可保证线性收敛；实验表明其显著提升DFL系统的公平性与抗拜占庭攻击能力。

Conclusion: DFedReweighting是一个灵活、通用的框架，通过适配不同目标性能指标和重加权策略，可实现多种学习目标。

Abstract: Decentralized federated learning (DFL) has recently emerged as a promising paradigm that enables multiple clients to collaboratively train machine learning model through iterative rounds of local training, communication, and aggregation without relying on a central server which introduces potential vulnerabilities in conventional Federated Learning. Nevertheless, DFL systems continue to face a range of challenges, including fairness, robustness, etc. To address these challenges, we propose \textbf{DFedReweighting}, a unified aggregation framework designed to achieve diverse objectives in DFL systems via a objective-oriented reweighting aggregation at the final step of each learning round. Specifically, the framework first computes preliminary weights based on \textit{target performance metric} obtained from auxiliary dataset constructed using local data. These weights are then refined using \textit{customized reweighting strategy}, resulting in the final aggregation weights. Our results from the theoretical analysis demonstrate that the appropriate combination of the target performance metric and the customized reweighting strategy ensures linear convergence. Experimental results consistently show that our proposed framework significantly improves fairness and robustness against Byzantine attacks in diverse scenarios. Provided that appropriate target performance metrics and customized reweighting strategy are selected, our framework can achieve a wide range of desired learning objectives.

</details>


### [444] [Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning](https://arxiv.org/abs/2512.12046)
*Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: 本文提出了基于Eikonal偏微分方程的连续时间目标条件强化学习方法Eik-QRL，并进一步扩展为分层框架Eik-HiQRL，以提升泛化性与复杂动力学下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决GCRL中价值函数学习对轨迹依赖、泛化能力弱及复杂动力学下性能下降的问题；利用quasimetric结构和连续时间建模提升理论合理性与实用性。

Method: 提出Eik-QRL——基于Eikonal PDE约束的quasimetric价值学习方法，实现轨迹无关训练；进一步设计Eik-HiQRL，将Eik-QRL嵌入分层强化学习框架中以应对复杂动力学。

Result: Eik-HiQRL在离线目标导向导航任务中达到SOTA，在操作任务中稳定优于QRL，并与时序差分方法性能相当。

Conclusion: 连续时间PDE建模可有效提升GCRL的价值函数结构合理性与泛化能力；分层扩展能缓解其在复杂动力学下的局限性，为quasimetric RL提供了更鲁棒、可扩展的新范式。

Abstract: Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.

</details>


### [445] [The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior](https://arxiv.org/abs/2512.12066)
*Erik Larsen*

Main category: cs.LG

TL;DR: 本文挑战了当前大语言模型安全评估中依赖单次测试（single-shot testing）的假设，通过在不同随机种子和温度设置下测试多个模型对有害提示的安全拒绝决策稳定性，发现大量提示存在决策翻转现象，并提出安全稳定性指数（SSI）量化该问题，最终建议采用多次采样（至少3次）以提高安全评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前安全评估方法假设模型响应是确定性的且能代表其安全对齐程度，但该假设缺乏验证；作者旨在探究模型在不同采样配置下安全拒绝决策的稳定性，揭示单次测试的局限性。

Method: 在876个有害提示上，对4个指令微调模型（Llama 3.1 8B、Qwen 2.5 7B、Qwen 3 8B、Gemma 3 12B）进行20种采样配置（4种温度×5个随机种子）测试；定义Safety Stability Index（SSI）衡量决策稳定性；使用Claude 3.5 Haiku作为外部统一评判器验证结果，并计算与主评判器（Llama 70B）的一致性。

Result: 18–28%的提示存在决策翻转；温度升高显著降低SSI（从0.951降至0.896，p<0.001）；单次测试仅在92.4%情况下与多采样真实情况一致；Claude 3.5 Haiku与Llama 70B评判器间达成89.0%一致性（kappa=0.62）。

Conclusion: 单次安全评估不足以支撑可靠的安全判断；应采用至少3次采样每提示以提升评估稳健性；需将决策稳定性纳入模型安全评测标准。

Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.

</details>


### [446] [Physics-informed neural networks to solve inverse problems in unbounded domains](https://arxiv.org/abs/2512.12074)
*Gregorio Pérez-Bernal,Oscar Rincón-Cardeño,Silvana Montoya-Noguera,Nicolás Guarín-Zapata*

Main category: cs.LG

TL;DR: 本文提出了一种用于无限和半无限域逆问题求解的新方法，结合负指数与正态分布采样策略及双网络架构，并对比了PINNs与PIKANs在精度、计算效率和抗噪性上的表现，结果表明PINNs更优。


<details>
  <summary>Details</summary>
Motivation: 解决无限和半无限域中的逆问题，克服传统方法对显式边界条件的依赖，并提升可解释性与精度。

Method: 提出基于负指数和正态分布的新型采样策略，设计双网络架构（同时学习方程解与参数），统一损失函数训练，无需显式施加边界条件；分别用PINNs和PIKANs实现并对比性能。

Result: PINNs在该设定下比PIKANs更准确、计算效率更高（快1000倍）、相对误差更低，且对噪声更具鲁棒性。

Conclusion: 尽管PIKANs在某些问题中具有更高可解释性和精度，但在无限/半无限域逆问题中，PINNs凭借其结构灵活性和训练效率展现出更优综合性能。

Abstract: Inverse problems are extensively studied in applied mathematics, with applications ranging from acoustic tomography for medical diagnosis to geophysical exploration. Physics informed neural networks (PINNs) have emerged as a powerful tool for solving such problems, while Physics informed Kolmogorov Arnold networks (PIKANs) represent a recent benchmark that, in certain problems, promises greater interpretability and accuracy compared to PINNs, due to their nature, being constructed as a composition of polynomials. In this work, we develop a methodology for addressing inverse problems in infinite and semi infinite domains. We introduce a novel sampling strategy for the network's training points, using the negative exponential and normal distributions, alongside a dual network architecture that is trained to learn the solution and parameters of an equation with the same loss function. This design enables the solution of inverse problems without explicitly imposing boundary conditions, as long as the solutions tend to stabilize when leaving the domain of interest. The proposed architecture is implemented using both PINNs and PIKANs, and their performance is compared in terms of accuracy with respect to a known solution as well as computational time and response to a noisy environment. Our results demonstrate that, in this setting, PINNs provide a more accurate and computationally efficient solution, solving the inverse problem 1,000 times faster and in the same order of magnitude, yet with a lower relative error than PIKANs.

</details>


### [447] [SigTime: Learning and Visually Explaining Time Series Signatures](https://arxiv.org/abs/2512.12076)
*Yu-Chia Huang,Juntong Chen,Dongyu Liu,Kwan-Liu Ma*

Main category: cs.LG

TL;DR: 本文提出了一种结合Shapelet表示与传统特征工程的双Transformer联合学习框架，并开发了可视化分析系统SigTIme，用于可解释的时间序列模式发现与探索。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模式发现方法存在计算复杂度高、可解释性差、难以捕捉有意义的时间结构等问题，尤其在生物医学等关键领域亟需改进。

Method: 提出双Transformer联合训练框架：一个基于shapelet的时间局部结构建模，另一个基于传统统计特征工程；同时设计交互式可视化系统SigTIme支持多视角签名探索。

Result: 在8个公开数据集和1个临床私有数据集上定量验证了框架有效性；通过ECG分析和早产预测两个专家协同案例展示了SigTIme的实际洞察力。

Conclusion: 该方法兼顾模型性能与可解释性，shapelet作为可解释签名提升了时间序列分类的透明度与实用性，SigTIme系统增强了领域专家对时序模式的理解与决策支持能力。

Abstract: Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.

</details>


### [448] [CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation](https://arxiv.org/abs/2512.12086)
*Xin Yang,Omid Ardakanian*

Main category: cs.LG

TL;DR: 本文提出Cloak框架，利用潜在扩散模型和对比学习实现时间序列数据的隐私保护，在保持数据实用性的同时有效抵御属性推断攻击。


<details>
  <summary>Details</summary>
Motivation: 现有数据混淆方法在修改下游任务、隐私-效用权衡及计算开销方面存在不足，难以部署于资源受限的移动IoT设备。

Method: 提出基于潜在扩散模型的Cloak框架，采用对比学习提取解耦表征，指导扩散过程保留有用信息并隐藏敏感信息。

Result: 在四个公开时序数据集及人脸图像数据集上的实验表明，Cloak持续优于当前最优混淆方法，且适用于资源受限环境。

Conclusion: Cloak为资源受限IoT设备提供了高效、灵活且实用的数据隐私保护新范式。

Abstract: Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.

</details>


### [449] [GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes](https://arxiv.org/abs/2512.12091)
*Mohammad Pivezhandi,Mahdi Banisharif,Saeed Bakhshan,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.LG

TL;DR: 本文提出了GraphPerf-RT，一种面向异构嵌入式SoC上OpenMP工作负载的图神经网络性能预测代理模型，融合任务DAG、CFG语义与运行时上下文（DVFS、温度、利用率），支持多目标（完成时间、能耗、缓存/分支失效、利用率）及校准不确定性预测，并验证其在模型基强化学习调度中显著降低完成时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法难以应对OpenMP工作负载在异构嵌入式SoC上的不规则性；表格回归器丢失结构信息；无模型强化学习易导致热过载。亟需兼顾结构建模、多源运行时状态与不确定性感知的高性能预测模型。

Method: 提出GraphPerf-RT：构建异构图表示，节点表征任务/核心/资源，边类型编码依赖、放置与争用关系；融合DAG拓扑、CFG导出代码语义与实时DVFS/热态/利用率；采用多任务evidential head（Normal-Inverse-Gamma分布）联合预测多个性能指标并输出校准不确定性。

Result: 在Jetson TX2/Orin NX/RUBIK Pi三平台验证，R² > 0.95，预期校准误差ECE < 0.05；集成至四种RL调度器后，MAMBRL-D3QN+GraphPerf-RT相较无模型基线降低66%完成时间（0.97±0.35s）和82%能耗（0.006±0.005J）。

Conclusion: GraphPerf-RT首次实现了对异构嵌入式系统中OpenMP工作负载的高精度、不确定性校准的多目标性能预测，证明了具备风险感知能力的代理模型可有效支撑热约束环境下的模型基规划与调度优化。

Abstract: Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache
  and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks
  overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core
  DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict
  makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.
  We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To
  demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),
  multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the
  world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,
  uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.

</details>


### [450] [Neural CDEs as Correctors for Learned Time Series Models](https://arxiv.org/abs/2512.12116)
*Muhammad Bilal Shahid,Prajwal Koirla,Cody Fleming*

Main category: cs.LG

TL;DR: 本文提出了一种 Predictor-Corrector 机制，用神经控制微分方程（Corrector）校正任意时序预测模型（Predictor）的多步预测误差，提升预测精度，适用于不规则采样和连续/离散时间模型，并通过两种正则化策略增强外推能力与训练效率。


<details>
  <summary>Details</summary>
Motivation: 多步时间序列预测易累积误差，现有直接或迭代预测方法鲁棒性不足，亟需一种通用、可插拔的误差校正机制。

Method: 设计 Predictor-Corrector 架构：Predictor 为任意已有时序模型；Corrector 为神经控制微分方程，显式建模并预测 Predictor 的预测误差；引入两种正则化策略以提升 Corrector 外推性能和训练速度。

Result: 在合成数据、物理仿真及真实世界数据集上，该机制与多种主流 Predictor（如 Neural ODE、Contiformer、DLinear）结合后，均显著且一致地提升了预测性能。

Conclusion: Predictor-Corrector 是一种通用、灵活且高效的预测增强框架，能有效缓解多步预测误差累积问题，尤其适用于不规则采样与跨时间尺度建模场景。

Abstract: Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.

</details>


### [451] [MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models](https://arxiv.org/abs/2512.12121)
*Ahmad Chamma,Omar El Herraoui,Guokan Shang*

Main category: cs.LG

TL;DR: MixtureKit是一个开源框架，支持构建、训练和分析基于任意预训练或微调模型的混合专家（MoE）系统，包含Traditional MoE、BTX和BTS三种方法，并提供可视化与统一检查点功能。


<details>
  <summary>Details</summary>
Motivation: 为简化和模块化地构建、训练与分析混合专家（MoE）模型，尤其适配多语言、代码切换等复杂场景，并支持从任意预训练/微调模型出发的灵活扩展。

Method: 提出MixtureKit框架，集成三种MoE方法：(i) 传统MoE（每Transformer块单一路由器）；(ii) BTX（分支-训练-混合，子层级细粒度路由）；(iii) BTS（分支-训练-拼接，保持专家完整并引入可训练拼接层）。自动修改配置、打补丁、生成统一检查点，并提供路由与权重可视化接口。

Result: 在阿拉伯语-拉丁语代码切换多语言数据上，基于BTX的MixtureKit模型在多个基准测试中优于密集基线模型。

Conclusion: MixtureKit是一个实用、模块化、开源的MoE研究与开发基础框架，适用于多样化领域。

Abstract: We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.

</details>


### [452] [High-Dimensional Tensor Discriminant Analysis: Low-Rank Discriminant Structure, Representation Synergy, and Theoretical Guarantees](https://arxiv.org/abs/2512.12122)
*Elynn Chen,Yuefeng Han,Jiayu Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于CP低秩结构的高维张量判别分析方法（CP-TDA），结合随机复合PCA初始化与迭代优化，在张量高斯混合模型下实现了全局收敛与最优误分类率；并进一步拓展至半参数设定，通过深度生成模型映射非正态张量数据，理论与实验均验证其在小样本、高维场景下的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有张量分类方法多依赖稀疏性或Tucker结构，缺乏理论保证；而实证表明判别信号常集中在少数多线性分量上，因此作者提出探索CP低秩结构这一新建模视角。

Method: 提出CP低秩张量判别分析（CP-TDA），采用随机复合PCA（rc-PCA）初始化以应对相关性与各向异性噪声，并设计迭代精炼算法；进一步构建首个半参数张量判别模型，利用深度生成模型将学习到的张量表示映射至适配CP-TDA的潜在空间。

Result: 理论上建立了CP-TDA的全局收敛性与极小极大最优误分类率；数值模拟与图分类真实数据分析表明，该方法在高维小样本情形下显著优于现有张量分类器及先进图神经网络。

Conclusion: CP低秩结构是建模高维张量判别信号的有效且可理论支撑的新范式；CP-TDA及其半参数扩展为处理复杂、非正态张量数据提供了兼具统计效率与计算可行性的新工具。

Abstract: High-dimensional tensor-valued predictors arise in modern applications, increasingly as learned representations from neural networks. Existing tensor classification methods rely on sparsity or Tucker structures and often lack theoretical guarantees. Motivated by empirical evidence that discriminative signals concentrate along a few multilinear components, we introduce CP low-rank structure for the discriminant tensor, a modeling perspective not previously explored. Under a Tensor Gaussian Mixture Model, we propose high-dimensional CP low-rank Tensor Discriminant Analysis (CP-TDA) with Randomized Composite PCA (\textsc{rc-PCA}) initialization, that is essential for handling dependent and anisotropic noise under weaker signal strength and incoherence conditions, followed by iterative refinement algorithm. We establish global convergence and minimax-optimal misclassification rates.
  To handle tensor data deviating from tensor normality, we develop the first semiparametric tensor discriminant model, in which learned tensor representations are mapped via deep generative models into a latent space tailored for CP-TDA. Misclassification risk decomposes into representation, approximation, and estimation errors. Numerical studies and real data analysis on graph classification demonstrate substantial gains over existing tensor classifiers and state-of-the-art graph neural networks, particularly in high-dimensional, small-sample regimes.

</details>


### [453] [BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models](https://arxiv.org/abs/2512.12131)
*Zhengyang Wang,Ziyue Liu,Ruijie Zhang,Avinash Maurya,Paul Hovland,Bogdan Nicolae,Franck Cappello,Zheng Zhang*

Main category: cs.LG

TL;DR: 本文提出BOOST框架，专为大规模低秩瓶颈架构设计，通过瓶颈感知张量并行、在线RMSNorm、线性层分组和低秩激活检查点等优化，显著提升训练速度和GPU利用率，降低通信开销。


<details>
  <summary>Details</summary>
Motivation: Transformer模型预训练规模受限于计算和通信成本的增加；低秩瓶颈架构虽能减少训练时间和内存占用，但在标准张量并行下扩展性差，3D并行直接应用导致通信过多和GPU利用率低。

Method: 提出BOOST训练框架，包含瓶颈感知张量并行（Bottleneck-aware Tensor Parallelism）、在线RMSNorm、线性层 grouping、低秩激活检查点等优化技术。

Result: 在不同低秩瓶颈架构上，BOOST相比全秩基线提速1.46–1.91×，相比简单集成3D并行的低秩模型提速1.87–2.27×，同时提升GPU利用率、降低通信开销。

Conclusion: BOOST有效解决了低秩瓶颈架构在大规模训练中的并行扩展瓶颈，是高效训练低秩大模型的实用框架。

Abstract: The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\times$ speedup over full-rank model baselines and 1.87-2.27$\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.

</details>


### [454] [On the Approximation Power of SiLU Networks: Exponential Rates and Depth Efficiency](https://arxiv.org/abs/2512.12132)
*Koffi O. Ayena*

Main category: cs.LG

TL;DR: 本文建立了SiLU激活网络的理论框架，证明其对光滑函数具有指数级逼近率，并在复杂度控制上优于经典ReLU网络。


<details>
  <summary>Details</summary>
Motivation: 提升神经网络对光滑函数的逼近效率，同时优化网络深度和规模的复杂度控制。

Method: 提出一种新颖的分层构造方法，首先高效逼近平方函数x²，再通过函数复合推广至Sobolev类函数的逼近分析。

Result: SiLU网络以O(1)深度实现O(ω^{-2k})逼近误差；对Sobolev类函数，达到O(ε^{-d/n})规模与O(1)深度的紧致逼近界。

Conclusion: SiLU激活函数在理论逼近性能上显著优于ReLU，尤其在深度固定前提下展现出更强的表达能力与复杂度优势。

Abstract: This article establishes a comprehensive theoretical framework demonstrating that SiLU (Sigmoid Linear Unit) activation networks achieve exponential approximation rates for smooth functions with explicit and improved complexity control compared to classical ReLU-based constructions. We develop a novel hierarchical construction beginning with an efficient approximation of the square function $x^2$ more compact in depth and size than comparable ReLU realizations, such as those given by Yarotsky. This construction yields an approximation error decaying as $\mathcal{O}(ω^{-2k})$ using networks of depth $\mathcal{O}(1)$. We then extend this approach through functional composition to establish sharp approximation bounds for deep SiLU networks in approximating Sobolev-class functions, with total depth $\mathcal{O}(1)$ and size $\mathcal{O}(\varepsilon^{-d/n})$.

</details>


### [455] [BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity](https://arxiv.org/abs/2512.12135)
*Lucine L. Oganesian,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 本文提出了一种新的时空Transformer模型及自监督掩码潜在重建任务，用于多区域颅内脑电（iEEG）数据建模，重点探索不同空间尺度（如单通道 vs 脑区）对token编码与掩码的影响，并发现更大空间尺度的编码能提升下游解码性能，同时兼顾通道级重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的神经基础模型在处理多区域颅内记录时，难以有效建模跨尺度的复杂时空交互；如何合理编码空间信息及设计自监督任务以学习脑网络模式并提升解码性能仍是关键开放问题。

Method: 提出一种新型时空Transformer模型，支持灵活选择空间尺度进行token编码与掩码；设计自监督的掩码潜在重建任务；在公开多区域iEEG数据上系统评估不同空间尺度（通道级、脑区级等）对预训练与下游解码的影响。

Result: 实验表明：1）调整token编码与掩码的空间尺度显著影响下游解码性能；2）相比常用通道级编码，更大尺度（如脑区级）编码可提升解码性能；3）模型可在实现区域级token编码的同时保持高精度的通道级神经信号重建。

Conclusion: 空间尺度的选择对神经基础模型的自监督预训练至关重要；所提框架为探索多区域人脑活动建模中的空间尺度效应提供了新工具，并提升了下游解码性能。

Abstract: Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.

</details>


### [456] [HydroDiffusion: Diffusion-Based Probabilistic Streamflow Forecasting with a State Space Backbone](https://arxiv.org/abs/2512.12183)
*Yihan Wang,Annan Yu,Lujun Zhang,Charuleka Varadharajan,N. Benjamin Erichson*

Main category: cs.LG

TL;DR: 本文提出HydroDiffusion，一种基于扩散模型的水文概率预报框架，采用解码器-only状态空间模型作为主干网络，一次性联合去噪多日流量轨迹，以提升长期依赖建模与时间一致性，显著优于现有LSTM基扩散模型和DRUM方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的径流预报方法依赖LSTM主干和单步训练目标，难以捕捉长程依赖、易导致误差累积，缺乏跨预见期的轨迹一致性。

Method: 提出HydroDiffusion框架，采用decoder-only状态空间模型替代LSTM，通过单次前向过程联合去噪完整多日流量轨迹，实现非自回归、时序一致的扩散生成。

Result: 在CONUS 531个流域上评估显示，HydroDiffusion在实测气象驱动下具备优异的实时预报（nowcast）精度，全预见期性能稳定，确定性与概率性预报技能均优于DRUM及LSTM基扩散基线。

Conclusion: HydroDiffusion为中短期径流概率预报提供了鲁棒的生成式建模范式，确立了新的基准，并为大陆尺度水文概率预测研究奠定基础。

Abstract: Recent advances have introduced diffusion models for probabilistic streamflow forecasting, demonstrating strong early flood-warning skill. However, current implementations rely on recurrent Long Short-Term Memory (LSTM) backbones and single-step training objectives, which limit their ability to capture long-range dependencies and produce coherent forecast trajectories across lead times. To address these limitations, we developed HydroDiffusion, a diffusion-based probabilistic forecasting framework with a decoder-only state space model backbone. The proposed framework jointly denoises full multi-day trajectories in a single pass, ensuring temporal coherence and mitigating error accumulation common in autoregressive prediction. HydroDiffusion is evaluated across 531 watersheds in the contiguous United States (CONUS) in the CAMELS dataset. We benchmark HydroDiffusion against two diffusion baselines with LSTM backbones, as well as the recently proposed Diffusion-based Runoff Model (DRUM). Results show that HydroDiffusion achieves strong nowcast accuracy when driven by observed meteorological forcings, and maintains consistent performance across the full simulation horizon. Moreover, HydroDiffusion delivers stronger deterministic and probabilistic forecast skill than DRUM in operational forecasting. These results establish HydroDiffusion as a robust generative modeling framework for medium-range streamflow forecasting, providing both a new modeling benchmark and a foundation for future research on probabilistic hydrologic prediction at continental scales.

</details>


### [457] [MolGuidance: Advanced Guidance Strategies for Conditional Molecular Generation with Flow Matching](https://arxiv.org/abs/2512.12198)
*Jirui Jin,Cheng Zeng,Pawan Prakash,Ellad B. Tadmor,Adrian Roitberg,Richard G. Hennig,Stefano Martiniani,Mingjie Liu*

Main category: cs.LG

TL;DR: 本文提出了一种结合多种先进引导策略的混合引导方法，用于基于SE(3)-不变流匹配的分子生成框架，分别对连续（速度场）和离散（logits）模态进行引导，并通过贝叶斯优化联合调整引导强度，在QM9和QMe14S数据集上实现了性质对齐的新SOTA性能，同时保持高化学有效性。


<details>
  <summary>Details</summary>
Motivation: 提升条件分子生成中化学有效性、目标性质对齐、结构多样性与采样效率等关键目标，借鉴计算机视觉中的新兴引导策略以增强生成模型性能。

Method: 将classifier-free guidance、autoguidance和model guidance集成到SE(3)-equivariant flow matching分子生成框架中；提出针对连续（速度场）和离散（预测logits）模态的分离式混合引导策略；采用贝叶斯优化联合学习两类引导尺度。

Result: 在QM9和QMe14S数据集上取得性质对齐任务的新SOTA结果；生成分子具有高结构/化学有效性；系统对比揭示了各类引导方法的适用边界与优劣。

Conclusion: 混合模态引导策略能有效协同提升分子生成的质量与可控性；贝叶斯优化引导尺度是实用且鲁棒的调优方式；视觉领域引导技术可成功迁移并适配于三维分子生成任务。

Abstract: Key objectives in conditional molecular generation include ensuring chemical validity, aligning generated molecules with target properties, promoting structural diversity, and enabling efficient sampling for discovery. Recent advances in computer vision introduced a range of new guidance strategies for generative models, many of which can be adapted to support these goals. In this work, we integrate state-of-the-art guidance methods -- including classifier-free guidance, autoguidance, and model guidance -- in a leading molecule generation framework built on an SE(3)-equivariant flow matching process. We propose a hybrid guidance strategy that separately guides continuous and discrete molecular modalities -- operating on velocity fields and predicted logits, respectively -- while jointly optimizing their guidance scales via Bayesian optimization. Our implementation, benchmarked on the QM9 and QMe14S datasets, achieves new state-of-the-art performance in property alignment for de novo molecular generation. The generated molecules also exhibit high structural validity. Furthermore, we systematically compare the strengths and limitations of various guidance methods, offering insights into their broader applicability.

</details>


### [458] [EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training](https://arxiv.org/abs/2512.12210)
*Yuting Tang,Weibang Jiang,Shanglin Li,Yong Li,Chenyu Liu,Xinliang Zhou,Yi Ding,Cuntai Guan*

Main category: cs.LG

TL;DR: 本文提出EEG-DLite，一种面向脑电（EEG）基础模型预训练的数据蒸馏框架，通过自监督自动编码器提取鲁棒潜表示，筛选去噪、去冗余子集，在仅用5%数据时即达到甚至超越全量数据的下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型EEG基础模型虽泛化能力强，但因数据量大、质量参差，预训练资源消耗高，亟需高效数据筛选方法。

Method: EEG-DLite首先利用自监督自动编码器将EEG片段编码为紧凑、抗噪的潜在表示；随后基于该表示识别并剔除离群样本与冗余样本，构建小而多样、信息丰富的子集。

Result: 在2500小时EEG数据集上仅使用5%经EEG-DLite筛选的数据进行预训练，下游任务性能与全量训练相当甚至更优。

Conclusion: EEG-DLite是首个系统研究EEG基础模型预训练数据蒸馏的工作，为生理信号基础建模提供了可扩展、实用的高效路径。

Abstract: Large-scale EEG foundation models have shown strong generalization across a range of downstream tasks, but their training remains resource-intensive due to the volume and variable quality of EEG data. In this work, we introduce EEG-DLite, a data distillation framework that enables more efficient pre-training by selectively removing noisy and redundant samples from large EEG datasets. EEG-DLite begins by encoding EEG segments into compact latent representations using a self-supervised autoencoder, allowing sample selection to be performed efficiently and with reduced sensitivity to noise. Based on these representations, EEG-DLite filters out outliers and minimizes redundancy, resulting in a smaller yet informative subset that retains the diversity essential for effective foundation model training. Through extensive experiments, we demonstrate that training on only 5 percent of a 2,500-hour dataset curated with EEG-DLite yields performance comparable to, and in some cases better than, training on the full dataset across multiple downstream tasks. To our knowledge, this is the first systematic study of pre-training data distillation in the context of EEG foundation models. EEG-DLite provides a scalable and practical path toward more effective and efficient physiological foundation modeling. The code is available at https://github.com/t170815518/EEG-DLite.

</details>


### [459] [Optimized Learned Count-Min Sketch](https://arxiv.org/abs/2512.12252)
*Kyosuke Nishishita,Atsuki Sato,Yusuke Matsui*

Main category: cs.LG

TL;DR: 本文提出了OptLCMS，一种优化的可学习Count-Min Sketch方法，通过域划分、解析推导CMS参数及动态规划优化阈值，在保证理论误差界的同时加快构建速度并提升灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有Learned Count-Min Sketch（LCMS）虽能降低估计误差，但存在构造慢（依赖经验调参）和缺乏对不可容忍误差概率的理论保证两大缺陷。

Method: OptLCMS将输入域划分为多个子域，为每个子域分配独立CMS实例；各CMS参数基于固定误差阈值解析推导；阈值选择通过带近似可行性检查的动态规划优化，减少经验验证需求。

Result: 实验表明，OptLCMS相比LCMS构建更快、不可容忍误差概率更低，且保持同等估计精度。

Conclusion: OptLCMS在不牺牲精度的前提下，提升了构建效率与理论保障性，并支持对允许误差阈值的显式控制，增强了实际部署灵活性。

Abstract: Count-Min Sketch (CMS) is a memory-efficient data structure for estimating the frequency of elements in a multiset. Learned Count-Min Sketch (LCMS) enhances CMS with a machine learning model to reduce estimation error under the same memory usage, but suffers from slow construction due to empirical parameter tuning and lacks theoretical guarantees on intolerable error probability. We propose Optimized Learned Count-Min Sketch (OptLCMS), which partitions the input domain and assigns each partition to its own CMS instance, with CMS parameters analytically derived for fixed thresholds, and thresholds optimized via dynamic programming with approximate feasibility checks. This reduces the need for empirical validation, enabling faster construction while providing theoretical guarantees under these assumptions. OptLCMS also allows explicit control of the allowable error threshold, improving flexibility in practice. Experiments show that OptLCMS builds faster, achieves lower intolerable error probability, and matches the estimation accuracy of LCMS.

</details>


### [460] [GRC-Net: Gram Residual Co-attention Net for epilepsy prediction](https://arxiv.org/abs/2512.12273)
*Bihao You,Jiping Cui*

Main category: cs.LG

TL;DR: 本文提出GRC-Net模型，通过Gram矩阵将EEG信号转为3D表示，并结合多级特征提取（coattention捕获全局特征、Inception处理局部特征），在BONN数据集五分类任务中达到93.66%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法对EEG信号仅做1D处理，忽略多维关系；且EEG中存在局部与全局信号不平衡问题。

Method: 采用Gram矩阵将EEG转换为3D表示；引入coattention机制建模全局信号依赖，Inception结构提取局部特征，实现多粒度特征融合。

Result: 在BONN数据集的五类癫痫分类任务中，GRC-Net准确率达93.66%，优于现有方法。

Conclusion: 3D信号表征与多级特征提取策略能有效提升EEG癫痫预测性能，尤其在复杂多分类任务中表现突出。

Abstract: Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.

</details>


### [461] [Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model](https://arxiv.org/abs/2512.12276)
*Jeffrey van der Voort,Martin Verlaan,Hanne Kekkonen*

Main category: cs.LG

TL;DR: 本文提出了一种多保真度集合卡尔曼滤波（MF-EnKF）方法，将机器学习（ML）代理模型作为低保真度模型，结合少量高保真全物理模型运行与大量廉价但精度较低的ML模型运行，在相同计算预算下提升预测精度。实验表明，该方法在Lorenz-2005和准地转模型上优于纯低分辨率模型，并提高了集合大小和初始状态估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统低保真模型（如低分辨率或降阶模型）精度有限；而新兴的ML代理模型在保持较低计算成本的同时可能提供更高精度，因此探索将其嵌入多保真度数据同化框架具有现实意义。

Method: 将ML代理模型作为低保真度模型集成到多保真度集合卡尔曼滤波（MF-EnKF）中，混合使用少量高保真物理模型与大量低保真ML模型的集合进行联合更新。

Result: MF-EnKF在Lorenz-2005和准地转模型上实现了比纯低分辨率模型更高的精度，且计算加速优于传统低保真模型；ML代理模型精度相当或更优，同时显著提升有效集合规模，改善初始条件估计。

Conclusion: 将ML代理模型引入MF-EnKF框架是可行且有效的，在不牺牲物理一致性的前提下提升了数据同化精度与效率，尤其适用于气象学和海洋学等对计算效率与预测精度均有高要求的领域。

Abstract: Currently, more and more machine learning (ML) surrogates are being developed for computationally expensive physical models. In this work we investigate the use of a Multi-Fidelity Ensemble Kalman Filter (MF-EnKF) in which the low-fidelity model is such a machine learning surrogate model, instead of a traditional low-resolution or reduced-order model. The idea behind this is to use an ensemble of a few expensive full model runs, together with an ensemble of many cheap but less accurate ML model runs. In this way we hope to reach increased accuracy within the same computational budget. We investigate the performance by testing the approach on two common test problems, namely the Lorenz-2005 model and the Quasi-Geostrophic model. By keeping the original physical model in place, we obtain a higher accuracy than when we completely replace it by the ML model. Furthermore, the MF-EnKF reaches improved accuracy within the same computational budget. The ML surrogate has similar or improved accuracy compared to the low-resolution one, but it can provide a larger speed-up. Our method contributes to increasing the effective ensemble size in the EnKF, which improves the estimation of the initial condition and hence accuracy of the predictions in fields such as meteorology and oceanography.

</details>


### [462] [Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation](https://arxiv.org/abs/2512.12285)
*Lujuan Dang,Zilai Wang*

Main category: cs.LG

TL;DR: 本文提出了一种结合分数阶微积分与深度学习的新型神经网络FDIFF-PINN，用于提升锂离子电池SOC估计的精度与物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动神经网络难以准确刻画电化学过程的复杂非线性和记忆依赖动态特性，导致动态工况下预测精度和物理可解释性受限。

Method: 构建基于分数阶等效电路模型的离散化分数阶偏微分方程，并设计分数微分方程物理信息神经网络（FDIFF-PINN）进行求解与训练。

Result: 在多温度（-10°C至20°C）动态充放电数据集（Panasonic 18650PF电池）上验证了FDIFF-PINN的优越性能。

Conclusion: FDIFF-PINN能更准确、更具物理意义地估计锂离子电池SOC，尤其适用于动态与宽温域工况。

Abstract: Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\circ}$C to 20$^{\circ}$C).

</details>


### [463] [TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting](https://arxiv.org/abs/2512.12301)
*Mahima Kumavat,Aditya Maheshwari*

Main category: cs.LG

TL;DR: TwinFormer is a hierarchical Transformer model for long-sequence time-series forecasting, using a two-stage sparse attention mechanism (Local and Global Informer) and GRU-based aggregation to achieve linear complexity and state-of-the-art performance across diverse real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based models suffer from quadratic complexity and struggle with capturing both local dynamics and global dependencies in long-sequence time-series forecasting.

Method: TwinFormer employs a two-stage hierarchical architecture: (1) Local Informer with top-k Sparse Attention on non-overlapping temporal patches, followed by mean pooling; (2) Global Informer using the same top-k attention to model inter-patch dependencies; finally, a lightweight GRU aggregates globally contextualized tokens for multi-horizon prediction.

Result: TwinFormer achieves linear O(kLd) time/memory complexity and ranks first or second on 27 out of 34 evaluation settings across eight real-world datasets and horizons 96–720; it outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers; ablations validate superiority of top-k Sparse Attention over ProbSparse and effectiveness of GRU aggregation.

Conclusion: TwinFormer demonstrates that a carefully designed hierarchical sparse attention architecture combined with lightweight sequence modeling (GRU) can significantly improve efficiency and accuracy in long-horizon time-series forecasting.

Abstract: TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: https://github.com/Mahimakumavat1205/TwinFormer.

</details>


### [464] [Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data](https://arxiv.org/abs/2512.12325)
*Shubhada Agrawal,Aaditya Ramdas*

Main category: cs.LG

TL;DR: 本文证明了Robbins提出的经典子高斯混合在路径层面（确定性）上满足遗憾界，该界在Ville事件E_α上对每条路径成立，并揭示了其在对抗性在线学习与博弈论统计之间的桥梁作用。


<details>
  <summary>Details</summary>
Motivation: 旨在弥合对抗性在线学习（通常处理有界数据的遗憾界）与博弈论统计（可处理无界数据但依赖随机假设）之间的理论鸿沟，通过引入路径层面的条件遗憾界。

Method: 基于Ville事件和累积方差过程V_T，对Robbins提出的子高斯混合进行路径式分析，推导确定性遗憾上界，并结合概率分析说明其在随机设定下的高概率保证。

Result: 证明了在Ville事件E_α（概率≥1−α）上，遗憾被控制在ln²(1/α)/V_T + ln(1/α) + ln ln V_T以内；在概率为1的E₀上，遗憾最终被控制在O(ln ln V_T)。

Conclusion: Robbins子高斯混合不仅具备随机意义下的优良性质，还具有强路径式遗憾控制能力，从而为连接对抗性与随机性学习框架提供了新视角与理论工具。

Abstract: We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_α$, this regret till time $T$ is bounded by $\ln^2(1/α)/V_T + \ln (1/α) + \ln \ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\ln(1/α) + \ln \ln V_T$ if $V_T \geq \ln(1/α)$.) If the data were stochastic, then one can show that $E_α$ has probability at least $1-α$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\ln \ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.

</details>


### [465] [Uncertainty Quantification for Machine Learning: One Size Does Not Fit All](https://arxiv.org/abs/2512.12341)
*Paul Hofman,Yusuf Sale,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文提出了一种灵活的不确定性度量框架，强调应根据具体任务（如选择性预测、分布外检测、主动学习）选择合适的不确定性度量，而非追求单一最优指标；通过匹配任务损失或选用特定proper scoring rule，可提升各场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究常声称某一种不确定性度量优于其他，但作者指出无普适最优度量，需面向任务定制化设计。

Method: 构建基于二阶分布的不确定性度量族，区分总不确定性、偶然性不确定性和认知不确定性，并通过proper scoring rules（如匹配任务损失的评分规则）进行实例化和调控。

Result: 在选择性预测中，匹配任务损失的评分规则效果最佳；在分布外检测中，互信息（表征认知不确定性）表现最优；在主动学习中，基于0-1损失的认知不确定性持续优于其他度量。

Conclusion: 不确定性量化不应追求通用最优解，而应依据下游任务特性选择或设计相应度量；所提框架提供了系统化、可调的实现路径。

Abstract: Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.

</details>


### [466] [Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept](https://arxiv.org/abs/2512.12365)
*Thai-Duy Dinh,Minh-Luan Vo,Cuong Tuan Nguyen,Bich-Hien Vo*

Main category: cs.LG

TL;DR: 本文提出了一种用于声学分类的合成蚊群数据集，利用轻量级深度学习模型对六种主要蚊媒进行识别，并验证了其在低功耗嵌入式设备上的可行性。


<details>
  <summary>Details</summary>
Motivation: 蚊媒疾病每年导致全球超70万死亡，亟需高效、可扩展的实时监测手段；传统实录单只蚊子音频费时费力，难以满足大规模研究与部署需求。

Method: 构建了首个面向多物种、含噪声的合成蚊群声学数据集，并采用log-mel谱图表示，评估多种轻量级深度学习架构在该数据集上的分类性能。

Result: 所测试模型能有效区分六种主要蚊媒种类，且具备在嵌入式低功耗设备上实时部署的能力。

Conclusion: 合成蚊群音频数据集是推动声学蚊媒研究与实现可扩展实时监测的有效新范式。

Abstract: Mosquito-borne diseases pose a serious global health threat, causing over 700,000 deaths annually. This work introduces a proof-of-concept Synthetic Swarm Mosquito Dataset for Acoustic Classification, created to simulate realistic multi-species and noisy swarm conditions. Unlike conventional datasets that require labor-intensive recording of individual mosquitoes, the synthetic approach enables scalable data generation while reducing human resource demands. Using log-mel spectrograms, we evaluated lightweight deep learning architectures for the classification of mosquito species. Experiments show that these models can effectively identify six major mosquito vectors and are suitable for deployment on embedded low-power devices. The study demonstrates the potential of synthetic swarm audio datasets to accelerate acoustic mosquito research and enable scalable real-time surveillance solutions.

</details>


### [467] [The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining](https://arxiv.org/abs/2512.12384)
*Jesse Ponnock*

Main category: cs.LG

TL;DR: 本文研究了在SEC文件上对Llama-3.2模型进行领域自适应预训练（DAPT）的缩放规律，发现仅需2亿token即可获得显著领域性能提升，且无通用领域性能退化。


<details>
  <summary>Details</summary>
Motivation: 探索如何以较低成本高效地将大语言模型适配到高价值专业领域（如金融），避免全量重训练。

Method: 在4亿token的美国SEC金融语料上，对1B和3B参数的Llama-3.2模型开展持续预训练，并在50M、100M、200M、400M token处保存验证检查点，分析领域与通用验证损失变化及幂律拟合。

Result: 领域验证损失持续下降，最大增益出现在前2亿token，之后收益递减；幂律指数较浅，表明金融语言高度规律、易学；通用领域损失基本不变，无灾难性遗忘；数据效率前沿显示模型专业化提升同时混合领域性能几乎不降。

Conclusion: 金融领域的有效DAPT可在较小token预算（如2亿）下实现，更大规模模型（7B–70B）在可预期的数据量下仍具可行性，为金融基础模型规模化提供早期实证依据。

Abstract: Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.

</details>


### [468] [Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment](https://arxiv.org/abs/2512.12387)
*Yawen Shao,Jie Xiao,Kai Zhu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.LG

TL;DR: 本文提出Value-Anchored Group Policy Optimization (VGPO)，改进Group Relative Policy Optimization (GRPO)在图像生成中因奖励稀疏与相对性导致的时序信用分配不准和优化信号衰减问题；通过引入稠密过程感知价值估计和基于绝对值的归一化，提升图像质量、任务准确率并抑制reward hacking。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法直接迁移到基于flow matching的图像生成中存在根本性冲突：一是终端稀疏奖励无法支持细粒度时序信用分配；二是仅依赖组内相对奖励导致训练后期优化信号消失。

Method: 提出VGPO框架：1）将稀疏终端奖励转化为稠密、过程感知的价值估计，实现每生成阶段的期望累积奖励建模；2）用基于绝对值的归一化替代传统组内相对归一化，保障优化信号稳定性。

Result: 在三个基准上实验表明，VGPO达到SOTA图像质量，同时提升任务特定准确率，并有效缓解reward hacking现象。

Conclusion: VGPO通过在时间维度和组维度上重构价值估计机制，解决了GRPO在图像生成中的核心局限，为基于强化学习的生成模型提供了更鲁棒、高效的对齐优化范式。

Abstract: Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.

</details>


### [469] [DeepVekua: Geometric-Spectral Representation Learning for Physics-Informed Fields](https://arxiv.org/abs/2512.12402)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: DeepVekua是一种融合几何深度学习与谱分析的混合架构，用于在稀疏数据条件下求解偏微分方程，通过学习微分同胚坐标变换将复杂几何映射到隐式调和空间，并分离几何与物理建模，实现比谱基线方法快100倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有坐标基网络在稀疏数据下因谱偏差导致的PDE求解性能受限问题，尤其针对复杂几何域中的advection-diffusion系统。

Method: 提出DeepVekua架构，学习一个微分同胚坐标变换，将复杂几何映射至隐式调和空间；在该空间中闭式求解最优谱权重，从而解耦几何表示与物理建模。

Result: 在advection-diffusion系统上显著优于当前最优隐式表示方法，相比谱基线方法获得100倍加速。

Conclusion: DeepVekua通过几何-物理解耦与闭式谱优化，为稀疏数据下的PDE求解提供了高效、鲁棒的新范式。

Abstract: We present DeepVekua, a hybrid architecture that unifies geometric deep learning with spectral analysis to solve partial differential equations (PDEs) in sparse data regimes. By learning a diffeomorphic coordinate transformation that maps complex geometries to a latent harmonic space, our method outperforms state-of-the-art implicit representations on advection-diffusion systems. Unlike standard coordinate-based networks which struggle with spectral bias, DeepVekua separates the learning of geometry from the learning of physics, solving for optimal spectral weights in closed form. We demonstrate a 100x improvement over spectral baselines. The code is available at https://github.com/VladimerKhasia/vekuanet.

</details>


### [470] [Can Graphs Improve Tabular Foundation Models?](https://arxiv.org/abs/2512.12405)
*Franck Le,Keith Grueneberg,Erich Nahum,Vadim Sheinin*

Main category: cs.LG

TL;DR: 本文提出BOLERO，一种轻量级静态二分图头，通过引入简单图先验增强预训练表格Transformer（RoBERTa-Tab），在80个分类和64个回归数据集上显著优于XGBoost、TabPFN-v2等强基线。


<details>
  <summary>Details</summary>
Motivation: 现有表格模型缺乏显式建模样本间关系的机制，而相似样本常有相关结果，因此探索引入图先验以提升预训练表格Transformer性能。

Method: 提出BOLERO：在冻结的RoBERTa-Tab主干上添加轻量静态二分图头，每个样本连接到特征/值锚点，并用小型GNN细化行表示。

Result: 在TP-BERTa基准的144个数据集上，BOLERO在分类与回归任务中均取得最多统计显著胜场，效果稳健（Wilcoxon检验+效应量分析）。

Conclusion: 轻量图先验能有效提升预训练表格Transformer性能，且无需微调主干网络，兼具高效性与实用性。

Abstract: Tabular data are central to many real-world systems. While recent tabular transformers and in-context learners such as SAINT, TP-BERTa, TabPFN, TabICL, and MITRA incorporate limited inter-row reasoning, most approaches still lack an explicit mechanism to model relationships among instances, even though similar samples often share related outcomes. We investigate whether introducing \emph{simple graph priors} can enhance \emph{pretrained tabular transformers}. Concretely, we introduce {BOLERO}, a lightweight, static bipartite graph head that augments {RoBERTa-Tab} (a RoBERTa-style tabular backbone pretrained with masked-token prediction.) Each instance connects to feature/value anchors; a small GNN refines row representations, while the backbone remains frozen. We evaluate on 80 classification and 64 regression datasets from the TP-BERTa benchmark suites, comparing against strong baselines including XGBoost, CatBoost, TabPFN-v2, MITRA, TabICL, TP-BERTa, and RoBERTa-Tab. To ensure statistically sound conclusions, we follow best practices for multi-dataset evaluation: pairwise Wilcoxon signed-rank tests on per-dataset score differences and effect sizes (median improvement with confidence intervals), rather than mean-rank post-hoc tests that depend on the competitor pool. BOLERO achieves the highest number of statistically significant wins across both classification and regression, demonstrating that lightweight graph priors meaningfully improve pretrained tabular transformers.

</details>


### [471] [Learning Dynamics in Memristor-Based Equilibrium Propagation](https://arxiv.org/abs/2512.12428)
*Michael Döll,Andreas Müller,Bernd Ulmann*

Main category: cs.LG

TL;DR: 本文研究了基于忆阻器的非线性权重更新对平衡传播（EqProp）训练神经网络收敛性的影响，发现只要忆阻器电阻变化范围足够大（至少一个数量级），EqProp仍能稳健收敛。


<details>
  <summary>Details</summary>
Motivation: 为突破冯·诺依曼瓶颈和内存墙限制，利用忆阻器实现存内计算已成为趋势；但忆阻器固有的非线性特性可能影响训练算法收敛性，需系统评估。

Method: 选取六种具有不同电压-电流滞回特性的忆阻器模型，将其集成到EBANA框架中，并在两个基准分类任务上评估其在EqProp训练下的表现。

Result: EqProp在忆阻器非线性权重更新下仍可稳健收敛，关键条件是忆阻器需具备至少一个数量级的宽电阻范围。

Conclusion: 忆阻器非线性并非EqProp训练的障碍，只要满足最小电阻动态范围要求，即可支持高效、低功耗的存内神经网络训练。

Abstract: Memristor-based in-memory computing has emerged as a promising paradigm to overcome the constraints of the von Neumann bottleneck and the memory wall by enabling fully parallelisable and energy-efficient vector-matrix multiplications. We investigate the effect of nonlinear, memristor-driven weight updates on the convergence behaviour of neural networks trained with equilibrium propagation (EqProp). Six memristor models were characterised by their voltage-current hysteresis and integrated into the EBANA framework for evaluation on two benchmark classification tasks. EqProp can achieve robust convergence under nonlinear weight updates, provided that memristors exhibit a sufficiently wide resistance range of at least an order of magnitude.

</details>


### [472] [Rough Sets for Explainability of Spectral Graph Clustering](https://arxiv.org/abs/2512.12436)
*Bartłomiej Starosta,Sławomir T. Wierzchoń,Piotr Borkowski,Dariusz Czerski,Marcin Sydow,Eryk Laskowski,Mieczysław A. Kłopotek*

Main category: cs.LG

TL;DR: 本文提出了一种基于粗糙集理论的图谱聚类解释方法增强方案，以提升文本文档聚类结果的可解释性，克服语义模糊文档和算法随机性带来的问题。


<details>
  <summary>Details</summary>
Motivation: 图谱聚类（GSC）虽能识别多样形状与密度的簇，但其在文本文档上的应用因谱空间嵌入缺乏语义对应、含模糊内容文档及算法随机性而难以解释。

Method: 受粗糙集理论启发，改进团队先前提出的解释方法，增强对谱聚类结果的语义可解释性。

Result: 所提方法有效缓解了文档语义模糊性和算法随机性对解释性的负面影响，提升了用户对聚类结果的理解能力。

Conclusion: 结合粗糙集理论的解释增强方法显著改善了图谱聚类在文本分析中的可解释性，为可解释AI在无监督学习中的应用提供了新思路。

Abstract: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.

</details>


### [473] [Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction](https://arxiv.org/abs/2512.12445)
*Abdul Matin,Rupasree Dey,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.LG

TL;DR: 本文提出了一种知识引导的ViT-based掩码自编码器，将线性光谱混合模型（LSMM）和光谱角制图（SAM）等物理约束嵌入自监督重建过程，以提升模型可解释性、泛化性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 提升深度学习模型的可解释性、泛化能力和数据效率，通过融入领域知识（特别是物理先验）弥补纯数据驱动方法的不足。

Method: 提出一种知识引导的ViT-based掩码自编码器，在自监督重建中联合优化基于物理的LSMM损失、SAM损失与传统Huber损失，使表征学习符合已知的信号与潜变量之间的结构关系。

Result: 实验表明该模型显著提升了重建质量，并增强了下游任务性能，同时在有限监督下训练更稳定，获得物理可解释的潜在表征。

Conclusion: 将物理信息归纳偏置嵌入Transformer架构的自监督学习框架中，是提升遥感或光谱数据分析等科学领域模型性能与可解释性的有效途径。

Abstract: Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.

</details>


### [474] [Optimized Architectures for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.12448)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 本文提出了一种通过过量配置架构结合可微稀疏化来学习紧凑、可解释Kolmogorov-Arnold网络（KANs）的方法，在不牺牲精度的前提下提升模型的可解释性与表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有对KANs的结构增强常因复杂度增加而损害其固有的可解释性，亟需一种兼顾精度与可解释性的新路径。

Method: 采用过量配置架构配合可微稀疏化策略，将架构搜索转化为端到端优化问题。

Result: 在函数逼近、动力系统预测和真实世界预测任务中均达到竞争性或更优精度，并发现显著更小的模型；过量配置与稀疏化协同效应明显，联合效果优于任一单独使用。

Conclusion: 该方法为科学机器学习中表达力与可解释性的关键张力提供了一个有原则的解决方案。

Abstract: Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.

</details>


### [475] [Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling](https://arxiv.org/abs/2512.12461)
*Eray Erturk,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 本文提出一种跨模态知识蒸馏框架，将预训练的多会话尖峰信号（spike）Transformer模型的知识迁移到局部场电位（LFP）Transformer模型中，显著提升LFP模型对运动行为等任务变量的预测性能和跨会话泛化能力。


<details>
  <summary>Details</summary>
Motivation: LFP虽具有长期稳定性、抗电极退化和低功耗等优势，但因其群体性、聚合性特性，建模难度大、预测性能弱于尖峰信号，现有神经建模工作多聚焦于尖峰而忽视LFP潜力。

Method: 构建跨模态知识蒸馏框架：先以会话特异性神经标记化策略，通过掩码自编码目标在多会话尖峰数据上预训练教师尖峰Transformer模型；再通过隐空间表征对齐，将知识蒸馏至学生LFP Transformer模型。

Result: 蒸馏后的LFP模型在无监督和有监督设置下均持续优于单会话与多会话LFP基线模型，并能在未经过额外蒸馏的新会话上保持优越泛化性能。

Conclusion: 跨模态知识蒸馏是一种强大且可扩展的方法，能有效利用高性能尖峰模型来提升LFP建模精度，为LFP在脑机接口与神经科学中的应用开辟新路径。

Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.

</details>


### [476] [Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference](https://arxiv.org/abs/2512.12462)
*Eray Erturk,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 本文提出了一种面向多模态神经信号（如脉冲与场电位）的实时递归解码框架，能处理不同采样率、分布差异及缺失数据问题，并在模拟与真实脑数据上显著优于现有线性和非线性多模态方法。


<details>
  <summary>Details</summary>
Motivation: 现有非线性多模态神经活动模型无法处理不同时间尺度（采样率）、概率分布差异以及跨模态的时间步缺失，且不支持实时解码。

Method: 提出一个三部分学习框架：1）多尺度编码器，学习模态内动力学以实时处理不同时间尺度和缺失样本；2）多尺度动力学主干网络，提取多模态时序动态并支持实时递归解码；3）模态特异性解码器，适配不同概率分布。

Result: 在仿真数据和三个不同的多尺度脑数据集上验证了该模型可有效融合具有不同时间尺度、分布和缺失样本的多模态神经信号，提升实时目标变量解码性能，并优于多种线性和非线性多模态基线方法。

Conclusion: 所提框架为多模态神经信号的实时、鲁棒、非线性融合解码提供了通用且有效的新范式，有望推动脑机接口与神经解码应用的发展。

Abstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.

</details>


### [477] [Exploring the Design Space of Transition Matching](https://arxiv.org/abs/2512.12465)
*Uriel Singer,Yaron Lipman*

Main category: cs.LG

TL;DR: 本文系统研究了Transition Matching (TM)框架中“head”模块的设计、训练与采样策略，通过大规模实验（56个1.7B参数模型）发现MLP head配合特定时间加权与高频采样可实现SOTA综合性能，而Transformer head在图像美学上更优。


<details>
  <summary>Details</summary>
Motivation: Transition Matching作为新兴生成建模范式，其内部transition步骤依赖head模块，但该模块的设计原则尚不清晰，亟需系统性实证分析以指导高效高质量建模。

Method: 开展大规模控制变量实验：构建56个1.7B参数的text-to-image TM模型，覆盖不同head架构（MLP/Transformer）、训练策略（时间加权方式）和采样方案（频率/随机性），共549次评估；重点分析time-continuous bidirectional TM变体。

Result: MLP head + 特定时间加权 + 高频采样在FID、训练/推理效率等综合指标上达到SOTA；Transformer head + 序列缩放 + 低频采样在图像美学评分上最优；部分设计（如某些架构变体）未带来显著增益。

Conclusion: head模块的设计对TM性能影响巨大，MLP与Transformer各具优势场景；实验明确了高价值设计方向（如时间加权、采样频率）与边际效益低的选项，为后续TM优化提供实证指南。

Abstract: Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.

</details>


### [478] [Sparse Concept Anchoring for Interpretable and Controllable Neural Representations](https://arxiv.org/abs/2512.12469)
*Sandy Fraser,Patryk Wielopolski*

Main category: cs.LG

TL;DR: 本文提出稀疏概念锚定方法，通过极少监督（每个锚定概念<0.1%样本标签）引导潜在空间中特定概念的定位，支持可逆行为调控与永久移除，提升表征的可解释性与可控性。


<details>
  <summary>Details</summary>
Motivation: 提升学习表征的可解释性与可控性，使模型能对特定概念进行精准干预。

Method: 结合激活归一化、分离正则化及锚点/子空间正则化，在潜在空间中将少量标注样本吸引至预定义方向或轴对齐子空间。

Result: 在结构化自编码器上实现目标概念的选择性衰减，对正交特征影响极小；完全移除时重建误差接近理论下界。

Conclusion: 稀疏概念锚定为学习表征提供了实用、可解释、可调控的行为路径。

Abstract: We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.

</details>


### [479] [GoMS: Graph of Molecule Substructure Network for Molecule Property Prediction](https://arxiv.org/abs/2512.12489)
*Shuhui Qu,Cheolwoo Park*

Main category: cs.LG

TL;DR: 本文提出了Graph of Molecule Substructures (GoMS)，一种新型图神经网络架构，通过将分子表示为子结构间关系构成的图，显式建模子结构间的相互作用与空间排列，克服了现有方法（如ESAN）忽略子结构间关系的局限，在大分子预测任务中显著提升性能并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如ESAN）将分子视为独立子结构的集合（bag-of-substructures），忽略了子结构之间的关键关系和空间排列，限制了对复杂分子（尤其工业级大分子）性质的准确预测。

Method: GoMS构建一个新图：节点为分子子图（subgraphs），边表征子结构间的结构关系（如连接、重叠），从而保留子结构在分子中的拓扑排布信息；该设计支持对子结构交互的显式建模。

Result: 在多个公开分子数据集上，GoMS显著优于ESAN及其他基线方法，尤其在原子数超100的大分子上性能提升更明显；理论分析表明其能区分子图组成相同但空间排布不同的分子。

Conclusion: GoMS通过捕获子结构间关系，提升了分子性质预测的准确性、可扩展性与可解释性，为材料科学等涉及复杂功能单元协同效应的领域提供了更优工具。

Abstract: While graph neural networks have shown remarkable success in molecular property prediction, current approaches like the Equivariant Subgraph Aggregation Networks (ESAN) treat molecules as bags of independent substructures, overlooking crucial relationships between these components. We present Graph of Molecule Substructures (GoMS), a novel architecture that explicitly models the interactions and spatial arrangements between molecular substructures. Unlike ESAN's bag-based representation, GoMS constructs a graph where nodes represent subgraphs and edges capture their structural relationships, preserving critical topological information about how substructures are connected and overlap within the molecule. Through extensive experiments on public molecular datasets, we demonstrate that GoMS outperforms ESAN and other baseline methods, with particularly improvements for large molecules containing more than 100 atoms. The performance gap widens as molecular size increases, demonstrating GoMS's effectiveness for modeling industrial-scale molecules. Our theoretical analysis demonstrates that GoMS can distinguish molecules with identical subgraph compositions but different spatial arrangements. Our approach shows particular promise for materials science applications involving complex molecules where properties emerge from the interplay between multiple functional units. By capturing substructure relationships that are lost in bag-based approaches, GoMS represents a significant advance toward scalable and interpretable molecular property prediction for real-world applications.

</details>


### [480] [AI-Driven Early Warning Systems for Student Success: Discovering Static Feature Dominance in Temporal Prediction Models](https://arxiv.org/abs/2512.12493)
*Vaarunay Kaushal,Rajib Mall*

Main category: cs.LG

TL;DR: 本研究通过比较决策树和LSTM模型在在线课程不同时间点（至第20周）对学生辍学风险的预测性能，发现不同阶段应关注不同评估指标：早期重召回率，中期需平衡精确率与召回率，后期则强调精确率；静态人口统计特征占主导（68%重要性），支持无需学习行为数据的早期预测。


<details>
  <summary>Details</summary>
Motivation: 在线学习环境中，及早识别高风险学生对实施有效干预至关重要，但现有研究缺乏对不同干预时机下模型性能与评估指标匹配关系的系统分析。

Method: 在六个时间快照（至课程第20周，即50%进度）上，对比决策树与LSTM模型的时序预测性能，并分析各阶段关键评估指标（召回率、精确率、准确率）及特征重要性。

Result: LSTM在第2周达97%召回率，适合早期干预；决策树在第8–16周保持78%准确率，适合中期资源分配；至第20周，两模型召回率均为68%，但LSTM精确率更高（90% vs 86%）；静态人口统计特征贡献68%重要性。

Conclusion: 干预时机决定最优模型与评估指标选择；仅依赖第2–4周的人口统计与注册前信息即可实现可靠初期预测，支持低负担、早响应的教育干预策略。

Abstract: Early identification of at-risk students is critical for effective intervention in online learning environments. This study extends temporal prediction analysis to Week 20 (50% of course duration), comparing Decision Tree and Long Short- Term Memory (LSTM) models across six temporal snapshots. Our analysis reveals that different performance metrics matter at different intervention stages: high recall is critical for early intervention (Weeks 2-4), while balanced precision-recall is important for mid-course resource allocation (Weeks 8-16), and high precision becomes paramount in later stages (Week 20). We demonstrate that static demographic features dominate predictions (68% importance), enabling assessment-free early prediction. The LSTM model achieves 97% recall at Week 2, making it ideal for early intervention, while Decision Tree provides stable balanced performance (78% accuracy) during mid-course. By Week 20, both models converge to similar recall (68%), but LSTM achieves higher precision (90% vs 86%). Our findings also suggest that model selection should depend on intervention timing, and that early signals (Weeks 2-4) are sufficient for reliable initial prediction using primarily demographic and pre-enrollment information.

</details>


### [481] [Policy Optimization for Dynamic Heart Transplant Allocation](https://arxiv.org/abs/2512.12497)
*Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 本文提出了一种基于UNOS历史数据的新模拟器，用于评估心脏移植分配政策；发现现行策略远逊于短视最优匹配策略，并提出了引入'潜力值'的动态优化策略及批量分配方法，同时分析了地理距离和中心拒收率等关键因素的影响。


<details>
  <summary>Details</summary>
Motivation: 现行心脏移植分配政策（2018年修订）未充分考虑术前与术后死亡率，导致资源分配效率低下，亟需更科学的评估与优化方法。

Method: 基于UNOS历史数据构建新型仿真模拟器；设计并比较多种分配策略，包括现状策略、短视最优匹配策略及引入‘潜力值’（patient's utility in future allocations）的动态策略；引入批量分配机制，并分析地理距离与移植中心拒收行为的影响。

Result: 现行策略显著劣于短视最优策略；引入潜力值的动态策略性能更优；对特定类型供体实施小批量分配可进一步提升效果；模拟器证实地理 proximity 和中心拒收率是影响分配效率的关键现实因素。

Conclusion: 通过仿真驱动的方法，本文为改进心脏移植分配政策提供了可验证、可扩展的框架，强调兼顾即时效益与长期动态效用，并揭示了实际操作中被忽视的关键约束。

Abstract: Heart transplantation is a viable path for patients suffering from advanced heart failure, but this lifesaving option is severely limited due to donor shortage. Although the current allocation policy was recently revised in 2018, a major concern is that it does not adequately take into account pretransplant and post-transplant mortality. In this paper, we take an important step toward addressing these deficiencies.
  To begin with, we use historical data from UNOS to develop a new simulator that enables us to evaluate and compare the performance of different policies. We then leverage our simulator to demonstrate that the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant. Moreover, we develop improved policies that account for the dynamic nature of the allocation process through the use of potentials -- a measure of a patient's utility in future allocations that we introduce. We also show that batching together even a handful of donors -- which is a viable option for a certain type of donors -- further enhances performance. Our simulator also allows us to evaluate the effect of critical, and often unexplored, factors in the allocation, such as geographic proximity and the tendency to reject offers by the transplant centers.

</details>


### [482] [Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems](https://arxiv.org/abs/2512.12523)
*Wenqi Fang,Ye Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于奇异值分解和半正交约束训练的轻量级神经网络架构，用于在复杂噪声时间序列中更鲁棒地检测临界转变。


<details>
  <summary>Details</summary>
Motivation: 标准对比学习方法参数过多、易受无关噪声干扰，难以准确识别被高幅随机波动掩盖的低维序参量所指示的临界转变。

Method: 构建基于奇异值分解的神经网络架构，并采用严格半正交性约束的训练算法，以提升传统对比学习在噪声环境下的性能。

Result: 实验表明，所提方法在识别临界转变方面与传统对比学习性能相当，但模型更轻量、抗噪能力显著增强。

Conclusion: 通过模型结构简化与训练约束设计，可在保持检测精度的同时大幅提升对比学习方法在噪声数据中的鲁棒性与效率。

Abstract: Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.

</details>


### [483] [Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling](https://arxiv.org/abs/2512.12526)
*Agustín M. de los Riscos,Julio E. Sandubete,Diego Carmona-Fernández,León Beleña*

Main category: cs.LG

TL;DR: 本文将经验模态分解（EMD）应用于MSCI全球指数，将得到的本征模函数（IMFs）转化为图表示，并利用图神经网络（GNN）建模；通过CEEMDAN提取9个IMF分量，再用四种时序转图方法构建图结构，发现不同频率IMF对应图的拓扑结构存在显著尺度依赖性，为针对金融时间序列设计适配GNN架构提供依据。


<details>
  <summary>Details</summary>
Motivation: 提升金融时间序列预测效果，需结合信号分解与图神经网络建模，但缺乏对分解后各分量图结构特性的系统分析。

Method: 采用CEEMDAN分解MSCI World指数得到9个IMFs；分别用自然可见性、水平可见性、重现性和转移图四种方法将各IMF转化为图；进行拓扑结构分析（如聚类系数、特征路径长度等）。

Result: 高频IMF生成稠密、高聚类、小世界特性图；低频IMF生成稀疏、长路径长度图；可见性图对振幅变化更敏感，重现图更利于保留时序依赖。

Conclusion: IMF分量具有尺度依赖的图拓扑特性，应据此定制化设计GNN架构，以提升金融时序预测性能。

Abstract: This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.

</details>


### [484] [Effective Fine-Tuning with Eigenvector Centrality Based Pruning](https://arxiv.org/abs/2512.12543)
*Shaif Chowdhury,Soham Biren Katlariwala,Devleena Kashyap*

Main category: cs.LG

TL;DR: 本文提出一种基于图论的神经网络剪枝方法，通过计算神经元的特征向量中心性来识别并保留最重要的神经元，再进行微调，从而在提升分类准确率的同时显著降低模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 受社交媒体中少数高影响力用户能引发大规模话语变化的启发，作者认为神经网络微调中也存在类似现象：少量关键神经元的小幅调整即可显著影响整体性能；因此，应优先保留和优化这些‘重要神经元’而非直接在预训练模型上添加新层。

Method: 将每个神经元建模为图中的节点，神经元间相似性构成边；利用特征向量中心性计算各神经元的重要性得分并据此剪枝；仅保留高中心性神经元构成子网络，随后对该子网络进行微调。

Result: 在TF Flowers、Caltech-101和Oxford Flowers-102数据集上验证了方法有效性；在Oxford Flowers-102上，该方法使VGGNet准确率从30%提升至48%，同时显著降低模型复杂度。

Conclusion: 基于图论的中心性驱动剪枝策略能有效提升微调性能，在多个主流CNN模型（VGGNet、EfficientNet、ResNet）和细粒度图像分类任务中均表现出更高的准确率与更低的复杂度。

Abstract: In social media networks a small number of highly influential users can drive large scale changes in discourse across multiple communities. Small shifts in the behavior of these users are often sufficient to propagate widely throughout the network. A similar phenomenon occurs during neural network fine tuning. Conventional fine tuning of convolutional neural networks typically adds a new linear classification layer on top of a large pre trained model. Instead we argue that improved adaptation can be achieved by first pruning the network to retain only the most important neurons and then performing fine tuning.
  We propose a graph theory based method for pruning neural networks that is designed to improve fine tuning performance. In this method each neuron is represented as a node and edges encode similarity between neurons. Neurons are pruned based on importance scores computed using eigenvector centrality. The resulting pruned network is then fine tuned using only the most central neurons. We evaluate the proposed method on VGGNet EfficientNet and ResNet models using the TF Flowers Caltech one zero one and Oxford Flowers one zero two datasets. The proposed approach achieves higher classification accuracy while significantly reducing model complexity. On the Oxford Flowers one zero two dataset the method achieves forty eight percent classification accuracy compared to thirty percent accuracy obtained by the baseline VGGNet model.

</details>


### [485] [Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model](https://arxiv.org/abs/2512.12545)
*Bin Mu,Yuxuan Chen,Shijin Yuan,Bo Qin,Hao Guo*

Main category: cs.LG

TL;DR: TianXing-S2S is a novel multi-sphere coupled probabilistic diffusion model for global subseasonal-to-seasonal (S2S) forecasting, outperforming ECMWF and FuXi-S2S in 45-day forecasts and enabling skillful extreme event prediction (e.g., heatwaves, anomalous precipitation) and stable 180-day rollouts.


<details>
  <summary>Details</summary>
Motivation: Accurate S2S prediction of extreme events is critical for climate adaptation, but remains challenging due to multi-sphere interactions and atmospheric uncertainty.

Method: TianXing-S2S uses a diffusion model with a latent-space encoder for multi-sphere predictors and a novel optimal transport–based coupling module in the denoiser to integrate atmospheric and boundary condition dynamics.

Result: Outperforms ECMWF and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5° resolution; achieves skillful subseasonal prediction of heatwaves and anomalous precipitation; identifies soil moisture as a key precursor; supports stable 180-day rollout forecasts.

Conclusion: TianXing-S2S establishes a robust, physics-informed probabilistic framework for S2S forecasting in a warming world, advancing both accuracy and forecast horizon for extreme events.

Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.

</details>


### [486] [Optimal Mistake Bounds for Transductive Online Learning](https://arxiv.org/abs/2512.12567)
*Zachary Chase,Steve Hanneke,Shay Moran,Jonathan Shafer*

Main category: cs.LG

TL;DR: 本文解决了在线学习中无标签数据作用的30年开放问题，精确量化了转导式在线学习与标准在线学习之间的差距，证明了转导式学习的错误界为Ω(√d)，且该界是紧的。


<details>
  <summary>Details</summary>
Motivation: 解决在线学习中无标签数据作用的长期开放问题，量化转导式与标准在线学习的性能差距。

Method: 通过构造特定概念类并分析其Littlestone维度，推导转导式在线学习的下界和上界。

Result: 证明转导式在线学习的错误界为Ω(√d)，且存在达到O(√d)上界的例子；相比之前结果有显著提升。

Conclusion: 确立了转导式与标准在线学习之间存在二次方差距，凸显了提前获取未标记实例序列的优势。

Abstract: We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. In the standard setting, the optimal mistake bound is characterized by the Littlestone dimension $d$ of the concept class $H$ (Littlestone 1987). We prove that in the transductive setting, the mistake bound is at least $Ω(\sqrt{d})$. This constitutes an exponential improvement over previous lower bounds of $Ω(\log\log d)$, $Ω(\sqrt{\log d})$, and $Ω(\log d)$, due respectively to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that this lower bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\sqrt{d})$. Our upper bound also improves upon the best known upper bound of $(2/3)d$ from Ben-David, Kushilevitz, and Mansour (1997). These results establish a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advance access to the unlabeled instance sequence. This contrasts with the PAC setting, where transductive and standard learning exhibit similar sample complexities.

</details>


### [487] [On the Accuracy of Newton Step and Influence Function Data Attributions](https://arxiv.org/abs/2512.12572)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 本文提出了一种无需全局强凸性假设的NS和IF数据归因方法新分析，首次解释了为何NS通常比IF更准确，并给出了误差的渐近紧界。


<details>
  <summary>Details</summary>
Motivation: 现有数据归因方法（如Influence Functions和Newton Step）的理论分析依赖不现实的全局强凸性假设，且误差界随参数维数d和删除样本数k增长过快，无法回答方法精度比较等基本问题。

Method: 针对凸学习问题（特别是逻辑回归），建立不依赖全局强凸性的新理论分析框架，推导NS与IF估计器在随机样本删除下的平均误差渐近行为。

Result: 证明了NS估计与真实重训练参数的平均误差为Θ̃(kd/n²)，NS与IF之间的平均误差为Θ̃((k+d)√(kd)/n²)，达到多项式对数因子内的渐近紧界。

Conclusion: NS在多数实际场景中优于IF，其误差主导项更小；新分析摆脱了强凸性限制，为数据归因方法提供了更实用、更精确的理论基础。

Abstract: Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.
  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as "what is the asymptotic scaling of the errors of each method?" or "which of these methods is more accurate for a given dataset?"
  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.
  \[ \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T - \hatθ_T^{\mathrm{NS}}\|_2 \bigr] = \widetildeΘ\!\left(\frac{k d}{n^2}\right), \qquad \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T^{\mathrm{NS}} - \hatθ_T^{\mathrm{IF}}\|_2 \bigr] = \widetildeΘ\!\left( \frac{(k + d)\sqrt{k d}}{n^2} \right). \]

</details>


### [488] [Differentiable Energy-Based Regularization in GANs: A Simulator-Based Exploration of VQE-Inspired Auxiliary Losses](https://arxiv.org/abs/2512.12581)
*David Strnadel*

Main category: cs.LG

TL;DR: 本文提出了一种将变分量子本征解算器（VQE）风格的能量项作为辅助正则化信号引入ACGAN生成器目标函数的方法（QACGAN），在噪声less模拟器上验证了其可行性，但未证明量子优势或泛化提升，仅展示了量子-经典混合训练的可微路径集成。


<details>
  <summary>Details</summary>
Motivation: 探索参数化量子线路导出的可微能量项能否作为生成对抗网络（GAN）的有效辅助正则化信号，以增强类别条件生成能力。

Method: 在ACGAN生成器目标中加入基于类特定Ising哈密顿量的VQE启发式能量项，使用Qiskit EstimatorQNN与TorchConnector实现端到端可微计算；全部实验在4量子比特无噪态矢量模拟器上进行。

Result: 在MNIST上，QACGAN在5个epoch内分类准确率达99–100%，显著高于ACGAN的87.8%；但FID指标方差大（约25%），稳定后为23–24，与ACGAN基线相当；训练速度慢约200倍（源于模拟器开销）。

Conclusion: 该工作是方法论上的概念验证：证明VQE式能量计算可通过可微路径嵌入GAN训练流程；不宣称量子优势、稳定性提升或可扩展性，强调需后续系统性消融实验对比等效经典正则器。

Abstract: This paper presents an exploratory, simulator-based proof of concept investigating whether differentiable energy terms derived from parameterized quantum circuits can serve as auxiliary regularization signals in Generative Adversarial Networks (GANs). We augment the Auxiliary Classifier GAN (ACGAN) generator objective with a Variational Quantum Eigensolver (VQE)-inspired energy term computed from class-specific Ising Hamiltonians using Qiskit's EstimatorQNN and TorchConnector.
  Important limitations: All experiments run on a noiseless statevector simulator with only 4 qubits, use a deliberately simple Hamiltonian parameterization, and lack ablation studies comparing against equivalent classical biases. The computational overhead (approximately 200x slower than classical ACGAN) reflects simulator artifacts rather than inherent quantum costs.
  On MNIST, we observe that the energy-regularized model (termed QACGAN) achieves high classification accuracy (99 to 100 percent) within 5 epochs compared to 87.8 percent for ACGAN, suggesting the auxiliary term influences class conditioning. However, sample quality metrics (FID) show high variance across runs (coefficient of variation approximately 25 percent at epoch 5), with values ranging from 19.92 to 35.96. Extended runs stabilize around FID 23 to 24, comparable to the ACGAN baseline.
  We explicitly do not claim quantum advantage, improved stability in any general sense, or scalability beyond this toy setting. The contribution is methodological: demonstrating that VQE-style energy computations can be integrated into GAN training loops via differentiable pathways. Whether such auxiliary signals provide benefits beyond equivalent classical regularizers remains an open question requiring systematic ablation studies, which we leave for future work.

</details>


### [489] [Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics](https://arxiv.org/abs/2512.12602)
*Jingdi Lei,Di Zhang,Soujanya Poria*

Main category: cs.LG

TL;DR: 本文提出了无误差线性注意力（EFLA），一种数值稳定、完全并行且广义化的delta规则形式，通过将在线学习更新建模为连续时间动力系统并求得其精确解析解，实现了无误差积累的线性时间注意力机制。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文语言模型中softmax注意力的二次计算成本瓶颈，同时保证数值稳定性和线性时间复杂度。

Method: 将在线学习更新建模为连续时间动力系统，利用动力学矩阵的秩-1结构推导出对应无限阶Runge-Kutta法的精确闭式解，实现完全并行的线性时间注意力。

Result: EFLA在噪声环境下表现出更强鲁棒性，在语言建模困惑度和下游任务性能上均优于DeltaNet，且不引入额外参数。

Conclusion: EFLA为构建高保真、可扩展的线性时间注意力模型提供了新的理论基础。

Abstract: Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.

</details>


### [490] [Causal inference and model explainability tools for retail](https://arxiv.org/abs/2512.12605)
*Pranav Gupta,Nithin Surendran*

Main category: cs.LG

TL;DR: 本文提出了一种将模型可解释性与因果推断相结合的方法，用于从零售数据中提取销售洞察，并在真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有零售业机器学习模型缺乏可解释性，且无法验证或发现因果关系，限制了业务决策支持能力。

Method: 综述因果推断与可解释性方法在电商与零售中的应用，并在真实数据集上采用固有可解释模型与双机器学习（Double Machine Learning）方法进行实证分析。

Result: 发现固有可解释模型的SHAP值方差更低；通过双机器学习引入多个混杂变量，能正确识别因果效应的符号。

Conclusion: 将可解释性与因果推断结合可提升零售分析的可信度与实用性，为业务洞察提供更稳健的基础。

Abstract: Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.

</details>


### [491] [Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain](https://arxiv.org/abs/2512.12617)
*Animesh Mishra*

Main category: cs.LG

TL;DR: 本文提出Spectral Sentinel，一种基于随机矩阵理论的拜占庭检测与聚合框架，利用诚实梯度协方差谱符合Marchenko-Pastur律、恶意扰动引发尾部异常的特性，在大规模非独立同分布（Non-IID）联邦学习中实现高效鲁棒聚合，支持高达1.5B参数模型，具备理论最优收敛性与实际区块链集成验证。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）在无中心信任下协同训练模型，但面对异构数据（Non-IID）下的拜占庭客户端梯度投毒仍缺乏可扩展、低开销且理论保障强的防御方法；现有方法存在拒绝合法更新、计算复杂度过高或仅在小模型上验证等局限。

Method: 提出Spectral Sentinel框架：利用诚实Non-IID梯度协方差矩阵本征谱服从Marchenko-Pastur（MP）律、拜占庭扰动导致谱尾异常的统计签名；结合Frequent Directions草图与数据依赖的MP谱跟踪，在O(k²)内存下实现大模型（k ≪ d）实时检测；理论分析基于(σ,f)威胁模型，给出(ε,δ)-拜占庭韧性保证及收敛率，并证明其minimax最优性。

Result: 在144种攻击-聚合器组合实验中，平均准确率达78.4%，显著优于基线方法（48–63%）；支持高达1.5B参数模型；实现Polygon区块链集成；算法内存复杂度为O(k²)，远低于几何中位数的O(n²d)。

Conclusion: Spectral Sentinel突破了DFL中拜占庭鲁棒性的可扩展性三难困境，首次将随机矩阵理论深度融入梯度异常检测，兼具理论最优性、工程可部署性与大规模验证效果，为超大规模Non-IID分布式学习提供了新范式。

Abstract: Decentralized federated learning (DFL) enables collaborative model training without centralized trust, but it remains vulnerable to Byzantine clients that poison gradients under heterogeneous (Non-IID) data. Existing defenses face a scalability trilemma: distance-based filtering (e.g., Krum) can reject legitimate Non-IID updates, geometric-median methods incur prohibitive $O(n^2 d)$ cost, and many certified defenses are evaluated only on models below 100M parameters. We propose Spectral Sentinel, a Byzantine detection and aggregation framework that leverages a random-matrix-theoretic signature: honest Non-IID gradients produce covariance eigenspectra whose bulk follows the Marchenko-Pastur law, while Byzantine perturbations induce detectable tail anomalies. Our algorithm combines Frequent Directions sketching with data-dependent MP tracking, enabling detection on models up to 1.5B parameters using $O(k^2)$ memory with $k \ll d$. Under a $(σ,f)$ threat model with coordinate-wise honest variance bounded by $σ^2$ and $f < 1/2$ adversaries, we prove $(ε,δ)$-Byzantine resilience with convergence rate $O(σf / \sqrt{T} + f^2 / T)$, and we provide a matching information-theoretic lower bound $Ω(σf / \sqrt{T})$, establishing minimax optimality. We implement the full system with blockchain integration on Polygon networks and validate it across 144 attack-aggregator configurations, achieving 78.4 percent average accuracy versus 48-63 percent for baseline methods.

</details>


### [492] [Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity](https://arxiv.org/abs/2512.12688)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: cs.LG

TL;DR: 本文将prompt视为外部注入的程序，提出了一种简化的Transformer模型来解释prompt如何通过注意力机制、前馈网络和深度堆叠实现多步计算，并证明固定主干网络可通过不同prompt近似广泛的目标行为。


<details>
  <summary>Details</summary>
Motivation: Prompt能在权重固定的情况下改变模型行为，但这一现象缺乏理论建模，本文旨在将其形式化为一个清晰的理论对象。

Method: 将prompt视为外部程序，构建简化Transformer模型，分解其机制：注意力负责从prompt记忆中选择性路由，FFN执行基于检索片段的局部算术，深度堆叠实现多步组合计算。

Result: 证明了一个构造性存在性结果：单个固定Transformer主干可通过仅调整prompt来近似一大类目标行为。

Conclusion: 该框架为形式化prompt长度/精度约束下的权衡及研究prompt切换的结构极限提供了统一理论起点，且独立于预训练大语言模型的经验结论。

Abstract: Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.

</details>


### [493] [Torch Geometric Pool: the Pytorch library for pooling in Graph Neural Networks](https://arxiv.org/abs/2512.12642)
*Filippo Maria Bianchi,Carlo Abate,Ivan Marisca*

Main category: cs.LG

TL;DR: 本文介绍了Torch Geometric Pool (tgp)，一个基于PyTorch Geometric构建的、用于图神经网络分层池化的开源库，提供统一API和模块化设计的多种池化算子，并通过基准测试验证了其性能与灵活性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络中池化操作的选择对下游任务性能影响显著，但现有工具缺乏统一、易用且可扩展的池化算子实现，限制了快速原型开发。

Method: 设计并实现了tgp库，集成多种图池化算子，采用统一API和模块化架构，并引入预计算池化等加速机制；通过系统性基准测试评估各算子在不同任务和数据集上的表现。

Result: 基准测试表明，不同池化算子性能因任务和数据而异，验证了提供多样化、易切换池化方法的必要性；预计算池化显著提升了部分算子的训练速度。

Conclusion: tgp填补了图学习中高效、灵活池化工具的空白，支持研究人员快速实验与比较不同池化策略，推动GNN架构探索。

Abstract: We introduce Torch Geometric Pool (tgp), a library for hierarchical pooling in Graph Neural Networks. Built upon Pytorch Geometric, Torch Geometric Pool (tgp) provides a wide variety of pooling operators, unified under a consistent API and a modular design. The library emphasizes usability and extensibility, and includes features like precomputed pooling, which significantly accelerate training for a class of operators. In this paper, we present tgp's structure and present an extensive benchmark. The latter showcases the library's features and systematically compares the performance of the implemented graph-pooling methods in different downstream tasks. The results, showing that the choice of the optimal pooling operator depends on tasks and data at hand, support the need for a library that enables fast prototyping.

</details>


### [494] [Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning](https://arxiv.org/abs/2512.12690)
*Yongcan Yu,Lingxiao He,Shuo Lu,Lijun Sheng,Yinuo Xu,Yanbo Wang,Kuangpu Guo,Jianjie Cheng,Meng Wang,Qianlong Xie,Xingxing Wang,Dapeng Hu,Jian Liang*

Main category: cs.LG

TL;DR: 本文通过系统性对比研究，挑战了当前视觉语言模型（VLM）推理训练中“强化学习（RL）优于监督微调（SFT）”的主流观点，发现SFT在模型能力较弱、数据量少、跨模态迁移等场景下具有关键且不可替代的作用，并揭示RL中存在“欺骗性奖励”问题。


<details>
  <summary>Details</summary>
Motivation: 质疑当前VLM推理训练中过度依赖RL、忽视SFT价值的主流范式，重新评估SFT在VLM推理能力提升中的实际作用。

Method: 在相同数据源下，对SFT与RL进行受控、系统性对比实验，分析模型容量、数据规模和数据分布的影响，并评估推理性能、数据效率与跨模态泛化能力；同时诊断RL中奖励与真实推理准确率的相关性。

Result: SFT在小/弱模型上更可靠地激发推理能力；仅用2K样本的SFT可达到甚至超越20K样本RL的效果；SFT跨模态迁移性更强；RL存在显著的‘欺骗性奖励’现象（高奖励≠高准确率）。

Conclusion: SFT在VLM推理训练中被严重低估；应摒弃‘RL优于SFT’的单一叙事，构建SFT与RL互补的平衡后训练流程。

Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.

</details>


### [495] [PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks](https://arxiv.org/abs/2512.12663)
*Gelesh G Omathil,Sreeja CS*

Main category: cs.LG

TL;DR: 本文提出PerNodeDrop，一种轻量级的随机正则化方法，通过在样本级别对每个神经元施加特定扰动，打破传统噪声正则化（如Dropout）中噪声的均匀性，在保留有益共适应的同时抑制有害共适应，从而提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有噪声正则化方法（如Dropout、DropConnect）施加的噪声在层或批次内是均匀的，易同时抑制有害和有益的神经元共适应，导致泛化性能受限。

Method: 提出PerNodeDrop方法，对每个样本、每个节点独立施加扰动；在样本级别进行权重丢弃（而非批次级别），并辅以期望损失分析，形式化地说明其如何选择性削弱过度共适应。

Result: 在视觉、文本和音频多个基准任务上，PerNodeDrop相比标准噪声正则化方法展现出更优的泛化性能，训练-验证性能差距缩小，对未见数据的可靠性提升。

Conclusion: PerNodeDrop通过引入样本级、节点级的非均匀噪声扰动，实现了更精细的正则化控制，在保持模型表达力的同时有效缓解过拟合，是一种高效且通用的正则化策略。

Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.

</details>


### [496] [DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization](https://arxiv.org/abs/2512.12669)
*Jiawei Shen,Jia Zhu,Hanghui Guo,Weijie Shi,Guoqing Ma,Yidan Liang,Jingjiang Liu,Hao Chen,Shimin Di*

Main category: cs.LG

TL;DR: 本文提出了一种统一的时序知识图谱推理方法DynaGen，通过动态子图构建与双分支GNN编码器提升插值性能，通过条件扩散过程增强外推泛化能力，在六个基准数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有插值方法上下文建模有限，外推方法存在认知泛化偏差，亟需统一、鲁棒的TKGR框架。

Method: DynaGen：插值阶段动态构建实体中心子图并用双分支GNN建模演化结构；外推阶段引入条件扩散过程，迫使模型学习演化本质规律而非表层模式。

Result: 在六个基准数据集上均达SOTA；插值MRR平均提升2.61点，外推MRR平均提升1.45点。

Conclusion: DynaGen通过结构动态建模与生成式原理学习，有效缓解插值上下文局限与外推泛化偏差，实现了插值与外推任务的统一高效建模。

Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.

</details>


### [497] [On Approaches to Building Surrogate ODE Models for Diffusion Bridges](https://arxiv.org/abs/2512.12671)
*Maria Khilchuk,Vladimir Latypov,Pavel Kleshchev,Alexander Hvatov*

Main category: cs.LG

TL;DR: 本文提出两种基于代理模型的算法（SINDy-FM 和 DSBM-NeuralODE），用于简化扩散与薛定谔桥模型的动力学建模，在保持竞争力的同时显著提升效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和薛定谔桥模型虽性能优越，但计算开销大、训练复杂，且其连续时间动力学依赖过参数化网络和难于高效积分的随机微分方程。

Method: 提出两类代理模型：1）SINDy Flow Matching（SINDy-FM），利用稀疏回归从数据中学习简洁、可解释的符号微分方程；2）DSBM-NeuralODE，将薛定谔桥重构成神经ODE形式以支持灵活的连续时间参数化。

Result: 在高斯传输任务与MNIST潜在空间平移实验中，所提方法达到竞争性性能；SINDy-FM将参数量降低数个数量级，并实现近即时推理。

Conclusion: 代理模型为构建高效、可解释、易部署的桥式生成模型提供了新范式。

Abstract: Diffusion and Schrödinger Bridge models have established state-of-the-art performance in generative modeling but are often hampered by significant computational costs and complex training procedures. While continuous-time bridges promise faster sampling, overparameterized neural networks describe their optimal dynamics, and the underlying stochastic differential equations can be difficult to integrate efficiently. This work introduces a novel paradigm that uses surrogate models to create simpler, faster, and more flexible approximations of these dynamics. We propose two specific algorithms: SINDy Flow Matching (SINDy-FM), which leverages sparse regression to identify interpretable, symbolic differential equations from data, and a Neural-ODE reformulation of the Schrödinger Bridge (DSBM-NeuralODE) for flexible continuous-time parameterization. Our experiments on Gaussian transport tasks and MNIST latent translation demonstrate that these surrogates achieve competitive performance while offering dramatic improvements in efficiency and interpretability. The symbolic SINDy-FM models, in particular, reduce parameter counts by several orders of magnitude and enable near-instantaneous inference, paving the way for a new class of tractable and high-performing bridge models for practical deployment.

</details>


### [498] [Scaling Bidirectional Spans and Span Violations in Attention Mechanism](https://arxiv.org/abs/2512.13033)
*Jongwook Kim,Sangheon Yun,Sukjin Yoon*

Main category: cs.LG

TL;DR: 本文提出了一种基于非对称投影的优化框架，通过分解反向传播梯度为平行段和正交违反项来提升Transformer训练效率，在WikiText-2上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决标准Transformer训练中的几何低效问题，提升训练效率。

Method: 引入非对称投影，将反向传播梯度分解为平行跨度和正交违反项，保持前向QKV结构不变，并选择性缩放各分量（尤其关注0阶双向平行跨度）。

Result: 在WikiText-2数据集上使用粗略配置实现了验证损失降低0.56%，验证了方法的基本有效性。

Conclusion: 标准注意力梯度是次优的，所提框架能提供更有效的学习信号，具备在更大数据集和更深训练中取得显著提升的潜力。

Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes

</details>


### [499] [Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits](https://arxiv.org/abs/2512.12693)
*Sumantrak Mukherjee,Serafima Lebedeva,Valentin Margraf,Jonas Hanselle,Kanta Yamaoka,Viktor Bengs,Stefan Konigorski,Eyke Hüllermeier,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: 本文提出了一种新的贝叶斯框架，用于在上下文部分可观测、且奖励分布依赖于潜在上下文变量的多任务多臂赌博机设置中进行高效探索。该方法通过粒子化高斯过程近似联合分布，统一建模跨任务与跨臂依赖，同时兼顾个性化推断，并区分结构不确定性与用户特异性不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决上下文部分可观测、奖励分布存在由潜在变量诱导的复杂依赖关系时，传统多任务 bandit 方法难以有效建模跨任务与跨臂结构依赖的问题。

Method: 提出基于粒子化近似的对数密度高斯过程（log-density Gaussian process）来建模任务-奖励联合分布；联合所有任务观测学习全局联合分布，支持个性化新任务推断；显式建模两类认知不确定性：潜在奖励依赖的结构性不确定性和用户侧上下文缺失/交互稀疏导致的特异性不确定性。

Result: 在模型误设或潜在异质性复杂的情境下，该方法显著优于分层模型 bandits 等基线方法。

Conclusion: 所提贝叶斯框架能灵活、数据驱动地发现并利用跨任务与跨臂的潜在依赖结构，在部分可观测上下文场景下实现更鲁棒、高效的探索。

Abstract: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.

</details>


### [500] [Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection](https://arxiv.org/abs/2512.13040)
*Xuwei Tan,Yao Ma,Xueru Zhang*

Main category: cs.LG

TL;DR: 本文提出FinFRE-RAG方法，通过重要性引导的特征降维与检索增强的上下文学习，提升大语言模型在金融欺诈检测任务中的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有表格模型依赖大量人工特征工程、可解释性差；而大语言模型虽具可解释优势，但直接用于欺诈检测时因高维特征、类别极度不平衡及缺乏上下文而表现不佳。

Method: 提出两阶段方法FinFRE-RAG：第一阶段进行重要性引导的特征约简，将关键数值/类别特征序列化为自然语言；第二阶段基于标签感知、实例级示例开展检索增强的上下文学习。

Result: 在四个公开欺诈数据集和三类开源大语言模型上，FinFRE-RAG显著优于直接提示方法，在多个设置下媲美强表格基线模型；虽仍略逊于专用分类器，但大幅缩小性能差距并提供可解释推理。

Conclusion: FinFRE-RAG使大语言模型成为欺诈分析中兼具一定性能与高可解释性的辅助工具，有望减轻分析师人工负担并支持系统优化。

Abstract: Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.

</details>


### [501] [Multi-Trajectory Physics-Informed Neural Networks for HJB Equations with Hard-Zero Terminal Inventory: Optimal Execution on Synthetic & SPY Data](https://arxiv.org/abs/2512.12708)
*Anthime Valin*

Main category: cs.LG

TL;DR: 本文提出了一种多轨迹物理信息神经网络（MT-PINN），用于求解带硬性零终端库存约束的最优交易执行问题，通过轨迹损失与反向传播时间惩罚机制有效强化约束，结合lambda课程学习策略提升训练稳定性，并在理论模型和真实SPY数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在处理带硬性零终端库存约束的最优交易执行问题时，常难以充分满足该约束，导致控制不稳定。

Method: 提出多轨迹PINN（MT-PINN）：引入基于 rollout 的轨迹损失，并通过反向传播时间（backpropagation-through-time）将终端库存惩罚传播回网络；采用轻量级 lambda-curriculum 策略，从风险中性简化HJB逐步过渡到风险厌恶HJB以稳定训练。

Result: 在Gatheral-Schied单资产模型上，MT-PINN结果与闭式解高度一致，终端库存紧密聚集于零，路径误差显著降低；在SPY盘中数据上，风险中性时匹配TWAP，高风险厌恶下尤其在下跌时段实现更低持仓暴露与具竞争力的交易成本。

Conclusion: MT-PINN能更可靠地处理强约束最优控制问题，在金融交易执行等实际场景中具有更好泛化性与鲁棒性。

Abstract: We study optimal trade execution with a hard-zero terminal inventory constraint, modeled via Hamilton-Jacobi-Bellman (HJB) equations. Vanilla PINNs often under-enforce this constraint and produce unstable controls. We propose a Multi-Trajectory PINN (MT-PINN) that adds a rollout-based trajectory loss and propagates a terminal penalty on terminal inventory via backpropagation-through-time, directly enforcing zero terminal inventory. A lightweight lambda-curriculum is adopted to stabilize training as the state expands from a risk-neutral reduced HJB to a risk-averse HJB. On the Gatheral-Schied single-asset model, MT-PINN aligns closely with their derived closed-form solutions and concentrates terminal inventory tightly around zero while reducing errors along optimal paths. We apply MT-PINNs on SPY intraday data, matching TWAP when risk-neutral, and achieving lower exposure and competitive costs, especially in falling windows, for higher risk-aversion.

</details>


### [502] [Solving a Machine Learning Regression Problem Based on the Theory of Random Functions](https://arxiv.org/abs/2512.12731)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 本文从无偏性公设出发，通过随机函数理论框架，推导出一种回归方法；在满足平移、旋转、缩放及高斯不变性等自然对称性条件下，核形式、正则化类型和噪声参数化均可解析得出，所得核为广义多调和样条，且非经验选取而是源于无偏性原理，从而为多种平滑与插值方法提供了无先验信息下的理论最优性基础。


<details>
  <summary>Details</summary>
Motivation: 为机器学习回归问题提供坚实的理论基础，避免经验性选择核函数和正则化方式，解决缺乏先验信息时方法选择的合理性问题。

Method: 基于随机函数理论，从无偏性（indifference）公设出发，结合无限维函数空间上具有平移、旋转、缩放和高斯不变性的概率测度，进行纯解析推导。

Result: 导出的回归方法天然具备广义多调和样条核、特定形式的正则化及噪声参数化，所有组件均由对称性公设严格决定。

Conclusion: 该工作为一大类平滑与插值方法提供了理论最优性保证，表明其在无先验知识前提下是自然且最优的选择。

Abstract: This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.

</details>


### [503] [SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization](https://arxiv.org/abs/2512.12737)
*Li Xia*

Main category: cs.LG

TL;DR: SPARK是一种面向去中心化联邦学习的通信高效方法，通过随机投影压缩雅可比矩阵、分阶段退火蒸馏和Nesterov动量加速，在大幅降低通信开销的同时保持甚至提升收敛速度与精度。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化联邦学习中统计异质性和通信开销大的问题，尤其是NTK方法因需传输完整雅可比矩阵而不适用于带宽受限的边缘网络。

Method: 提出SPARK框架，融合三部分：1）基于随机投影的雅可比压缩以保留谱性质；2）分阶段退火蒸馏，从纯NTK演化平滑过渡到邻居正则化学习以抑制压缩噪声；3）Nesterov动量加速，借助蒸馏平滑性实现稳定累积。

Result: 相较NTK-DFL减少98.7%通信量，收敛速度相当且精度更优；引入动量后达到目标性能的速度提升3倍。

Conclusion: SPARK在通信效率、收敛速度与模型精度间取得最优平衡，是当前带宽受限边缘环境下最先进的去中心化学习方案。

Abstract: Decentralized federated learning (DFL) faces critical challenges from statistical heterogeneity and communication overhead. While NTK-based methods achieve faster convergence, transmitting full Jacobian matrices is impractical for bandwidth-constrained edge networks. We propose SPARK, synergistically integrating random projection-based Jacobian compression, stage-wise annealed distillation, and Nesterov momentum acceleration. Random projections compress Jacobians while preserving spectral properties essential for convergence. Stage-wise annealed distillation transitions from pure NTK evolution to neighbor-regularized learning, counteracting compression noise. Nesterov momentum accelerates convergence through stable accumulation enabled by distillation smoothing. SPARK achieves 98.7% communication reduction compared to NTK-DFL while maintaining convergence speed and superior accuracy. With momentum, SPARK reaches target performance 3 times faster, establishing state-of-the-art results for communication-efficient decentralized learning and enabling practical deployment in bandwidth-limited edge environments.

</details>


### [504] [Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models](https://arxiv.org/abs/2512.12744)
*Haotian Xu,Tian Gao,Tsui-Wei Weng,Tengfei Ma*

Main category: cs.LG

TL;DR: 本文提出了一种基于生物启发的输入稀疏化方法，通过引入可训练的自发神经元来补偿稀疏化带来的性能下降，从而在保持效率的同时提升大语言模型的表征能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有输入稀疏化方法过于关注计算效率，忽视了其对模型表征能力的影响，导致性能明显低于全模型。

Method: 将输入稀疏化重新解释为动态结构剪枝，并受生物神经元基线放电率启发，引入少量可训练的自发神经元作为补偿单元以稳定稀疏化后的激活。

Result: 所提方法显著缩小了稀疏化引起的性能差距，并在多种任务上展现出良好泛化能力。

Conclusion: 自发神经元是一种有效且通用的补偿机制，有助于在不牺牲性能的前提下实现高效、可解释的输入稀疏化。

Abstract: Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.

</details>


### [505] [Federated Learning with Feedback Alignment](https://arxiv.org/abs/2512.12762)
*Incheol Baek,Hyungbin Kim,Minseo Kim,Yon Dohn Chung*

Main category: cs.LG

TL;DR: 本文提出了一种名为Federated Learning with Feedback Alignment (FLFA)的新框架，通过在本地训练的反向传播中使用全局模型权重作为共享反馈矩阵，有效缓解了联邦学习中因数据非独立同分布（non-IID）导致的局部漂移问题，提升了收敛性与准确性，且无额外通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异质性（non-IID）导致局部模型漂移，阻碍全局模型收敛。

Method: 提出FLFA框架，将全局模型权重用作本地反向传播中的共享反馈矩阵，实现局部更新与全局模型对齐。

Result: 理论分析证明FLFA可缓解局部漂移并保证收敛；实验表明其能提升多种FL方法的准确率并降低局部漂移程度。

Conclusion: FLFA是一种高效、低开销的联邦学习改进方案，适用于数据异质场景，具备良好理论支撑与实证效果。

Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.

</details>


### [506] [On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models](https://arxiv.org/abs/2512.13352)
*Ali Al Sahili,Ali Chehab,Razane Tajeddine*

Main category: cs.LG

TL;DR: This paper investigates how Membership Inference Attacks (MIAs) can be integrated into training data extraction pipelines for Large Language Models (LLMs), benchmarking their effectiveness in realistic extraction scenarios versus standard MIA benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs risk memorizing training data, leading to privacy threats like training data extraction and Membership Inference Attacks (MIAs); prior work shows these are interconnected, motivating a systematic evaluation of MIAs within extraction pipelines.

Method: The authors integrate multiple MIA techniques into a training data extraction pipeline and systematically benchmark their effectiveness in detecting whether specific data points were in the LLM's training set, comparing performance against conventional MIA benchmarks.

Result: The study provides comparative performance results of various MIAs in both integrated data extraction settings and traditional MIA benchmarks, revealing their practical utility and limitations for real-world privacy attacks on LLMs.

Conclusion: MIAs exhibit different effectiveness in data extraction contexts compared to standard benchmarks, highlighting the need for context-aware evaluation of privacy attacks on LLMs.

Abstract: Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.

</details>


### [507] [OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging](https://arxiv.org/abs/2512.12779)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 本文提出了一种无需超参数的在线回归模型OLR-WAA，通过动态加权平均和实时概念漂移检测与响应机制，在非平稳数据流中实现稳定性与适应性的平衡，并在多个基准上优于现有在线模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据分布随时间变化（概念漂移），而传统在线模型固定超参数，难以动态适应；亟需一种自适应、免调参、抗漂移的在线回归方法。

Method: 提出OLR-WAA：基于指数加权移动平均增量更新基础模型；设计动态漂移检测与量化机制，并据此实时调整模型；引入保守更新策略，优先利用高置信度样本。

Result: 在静态数据上媲美批处理回归，在漂移数据上显著优于或持平SOTA在线模型；R²值更高、收敛更快；能有效弥合漂移场景下的性能差距并支持置信度感知学习。

Conclusion: OLR-WAA是一种高效、鲁棒、免超参数的在线回归框架，兼顾稳定性与适应性，为非平稳流数据建模提供了新范式。

Abstract: Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces "OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.

</details>


### [508] [Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset](https://arxiv.org/abs/2512.12783)
*Atalay Denknalbant,Emre Sezdi,Zeki Furkan Kutlu,Polat Goktas*

Main category: cs.LG

TL;DR: 本文提出了一种基于合成数据和替代金融数据的信用评估方法，用于服务伊斯坦布尔缺乏正式信用记录的'未充分银行化'人群；通过生成包含人口统计与行为特征的合成数据集，并结合多种梯度提升模型验证，证明加入9维替代数据可显著提升模型区分能力（AUC提升1.3个百分点，F1提升14%），为普惠信贷提供可复现、透明且合规的技术路径。


<details>
  <summary>Details</summary>
Motivation: 金融排斥制约创业、加剧收入波动与财富分化；伊斯坦布尔大量'未充分银行化'人群因收入与支付经非正规渠道而无征信记录，亟需不依赖传统征信的可信信用评估方法。

Method: 构建覆盖10万居民的2025年第一季度伊斯坦布尔合成数据集，融合TÜİK人口普查边际分布与电信使用模式，利用RAG增强生成技术驱动OpenAI o3模型生成隐私保护的真实感记录；采用CatBoost、LightGBM、XGBoost三模型，在仅用7维人口统计变量（Demo）与增加9维替代行为变量（Full）两组设定下，进行五折分层交叉验证。

Result: 引入替代数据后，AUC平均提升约1.3个百分点，平衡F1分数从约0.84升至0.95（提升14%）；证实少量行为特征（如手机规格、网购节奏、订阅支出等）可接近传统征信数据的判别能力。

Conclusion: 该研究提供了开源合成数据集、全可复现建模流程及实证依据，表明基于替代数据的信用评估既具有效性又具公平性与可解释性，为监管机构与放贷方拓展对无征信人群的负责任信贷服务提供了透明可行的技术蓝图。

Abstract: Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÜİK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.

</details>


### [509] [OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average](https://arxiv.org/abs/2512.12785)
*Mohammad Abu Shaira,Yunhe Feng,Heng Fan,Weishi Shi*

Main category: cs.LG

TL;DR: 本文提出了一种无需超参数、自适应的在线分类模型OLC-WA，通过指数加权移动平均融合数据流与基础模型，并集成自动概念漂移检测与动态调整机制，在静态环境中性能接近批处理模型，在漂移环境下显著优于现有在线方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常呈现随时间变化的分布（概念漂移），忽略此现象会严重降低模型预测精度；同时，现有在线模型的超参数通常固定，难以动态适配数据演化。

Method: 提出OLC-WA模型：利用指数加权移动平均融合新数据流与现有基础模型，并设计内置优化机制，自动检测概念漂移、量化其强度，并据此动态调整模型。

Result: 在多个基准数据集上的实验表明，OLC-WA在静态环境中性能接近批处理模型（精度损失仅1–3%），在存在概念漂移时比主流在线基线模型提升10–25%。

Conclusion: OLC-WA是一种高效、自适应、无需人工调参的在线分类方法，能稳健应对数据流中的动态分布变化。

Abstract: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.

</details>


### [510] [Unveiling Statistical Significance of Online Regression over Multiple Datasets](https://arxiv.org/abs/2512.12787)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 本文探讨了在线学习中多算法跨数据集比较的统计检验方法，采用Friedman检验及事后检验对多种在线回归模型进行评估，并通过真实与合成数据集结合5折交叉验证和种子平均法进行实证分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单数据集上两算法性能比较，而缺乏适用于多算法跨多数据集的统计检验方法；同时，在线学习中需确保统计显著性以验证持续学习效果、快速收敛及概念漂移应对能力。

Method: 采用Friedman检验及其对应的事后检验，结合真实与合成数据集、5折交叉验证和种子平均策略，对前沿在线回归模型进行实证评估。

Result: 统计检验结果总体支持各基线方法在各自报告中的性能一致性，但也揭示出当前最先进方法在某些方面仍有改进空间。

Conclusion: Friedman检验等非参数统计方法适用于多在线回归模型跨数据集的比较；需进一步发展更鲁棒的时序统计检验方法以适应动态数据演化场景。

Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.

</details>


### [511] [Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks](https://arxiv.org/abs/2512.12792)
*Shivansh Sahni,Wenzhi Zhang*

Main category: cs.LG

TL;DR: 本文提出了Liquid Reasoning Transformer（LRT），一种支持自适应推理深度的Transformer架构，通过迭代更新、丢弃式修正和学习停止机制提升结构化推理能力，在Sudoku任务上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer依赖单次前馈推理，难以根据输入难度动态分配计算资源或修正早期错误，因此需要支持自适应深度与迭代修正的新型架构。

Method: 提出LRT架构，引入循环推理token进行多步内部更新，结合discard-based correction（丢弃式修正）与learned stopping mechanism（学习停止机制）实现动态计算深度控制。

Result: 在Sudoku任务上达到98.68%数字准确率和36.30%完整谜题准确率，无需符号规则或搜索；分析表明discard gate与stop gate在稳定推理和调节计算深度中起不同关键作用。

Conclusion: LRT为结构化推理提供了可扩展、自适应的神经建模方法，其机制可自然扩展至更大规模推理任务（如国际象棋）及多token、大领域场景。

Abstract: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.

</details>


### [512] [TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk](https://arxiv.org/abs/2512.12795)
*Mengying Yan,Ziye Tian,Siqi Li,Nan Liu,Benjamin A. Goldstein,Molei Liu,Chuan Hong*

Main category: cs.LG

TL;DR: TRACER是一种基于迁移学习的实时适应框架，用于应对临床环境中因人口结构变化（如COVID-19）导致的风险预测模型性能漂移问题，无需完全重训练即可识别患者就诊阶段并动态调整模型，在模拟和真实场景中均提升判别力与校准度。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持工具常因时间推移导致的人群分布变化（尤其是混合人群过渡期）而出现性能漂移，例如系统升级或新发疾病（如COVID-19）引发的病例构成变化。

Method: 提出TRACER框架，通过迁移学习实现模型实时自适应，核心是识别单次就诊（encounter-level）所属的过渡阶段，并在不全量重训练的前提下动态更新预测模型。

Result: 在模拟实验中，TRACER优于仅使用历史数据或当前数据训练的静态模型；在真实世界中应用于急诊就诊后住院风险预测（跨越COVID-19过渡期），显著提升了模型的判别能力（discrimination）和校准度（calibration）。

Conclusion: TRACER为在动态、异质的临床环境下持续保障预测模型鲁棒性提供了一种可扩展、实用的解决方案。

Abstract: Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.

</details>


### [513] [From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs](https://arxiv.org/abs/2512.12805)
*Anastasiia Alokhina,Pan Li*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架来分析Transformer在几何数据上的尺寸泛化能力，证明了其输出误差受采样密度和数据流形本征维度的影响，并通过图和点云实验验证了理论界的紧性。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在不同任务中展现出尺寸泛化能力（即从小样本外推到长序列），但其理论基础仍不完善，尤其在几何数据上缺乏严谨刻画。

Method: 构建连续域建模（如流形采样、graphon建模）下的Transformer理论分析框架，推导离散样本输出与连续域等价输出之间的误差上界，重点分析稳定位置编码下该界与采样密度及流形本征维数的关系。

Result: 得到了误差上界，其依赖于采样密度和数据流形的本征维度；实验验证该界在图和点云任务中具有紧性。

Conclusion: Transformer的尺寸泛化能力可被理论刻画为采样密度与流形结构的函数，为理解其在几何数据上的外推行为提供了新视角和理论支撑。

Abstract: Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.

</details>


### [514] [Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift](https://arxiv.org/abs/2512.12816)
*Hasan Burhan Beytur,Gustavo de Veciana,Haris Vikalo,Kevin S Chan*

Main category: cs.LG

TL;DR: 本文提出了一种模型无关的资源分配框架，用于在概念漂移和预算受限下优化机器学习模型的训练与部署策略，理论分析了不同概念持续时间分布（DMRL/IMRL）对最优训练策略的影响，并设计了满足通信约束的近优调度算法。


<details>
  <summary>Details</summary>
Motivation: 在客户端设备仅支持推理、无法重训练的分布式场景下，模型提供方需在有限预算和概念漂移动态中主动维护模型性能，但现有方法缺乏对资源-漂移-部署时序联合建模的理论基础。

Method: 构建模型无关的资源分配框架，结合可靠性理论（DMRL/IMRL性质分析）、优化理论（证明通信约束下问题的拟凸性）与随机调度设计，推导最优训练策略并提出近优部署算法。

Result: 1）在突发概念变化下，针对DMRL分布给出带预算约束的最优训练策略；2）证明IMRL下常见启发式策略严格次优；3）在通信约束下证明优化问题拟凸，并提出实现近优客户端性能的随机调度策略。

Conclusion: 为概念漂移下的高效ML模型管理提供了理论保证与实用算法，支撑持续学习、分布式推理与自适应ML系统的发展。

Abstract: We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.

</details>


### [515] [On the continuity of flows](https://arxiv.org/abs/2512.12821)
*Congzhou M Sha*

Main category: cs.LG

TL;DR: 本文研究了流匹配（flow matching）中因先验分布与目标分布拓扑不匹配（如单峰到多峰）所导致的最优速度场出现空间不连续性的问题，理论分析表明该不连续性源于连续流在模态分裂时必须做出离散路径选择，实证验证支持该结论，并讨论了其对流形上流匹配及神经网络学习不连续表示的影响。


<details>
  <summary>Details</summary>
Motivation: 当先验分布与目标分布拓扑结构不一致（例如单峰 vs 多峰）时，标准流匹配目标下的最优速度场可能出现不连续，这可能限制模型表达能力与训练稳定性，但其成因尚未被系统揭示。

Method: 通过理论分析（以双峰高斯混合为例），严格推导最优速度场在决策边界上的跳跃不连续性及其随时间趋于目标分布时的发散行为；并辅以数值实验进行实证验证。

Result: 证明了在拓扑不匹配下，最优速度场存在沿决策边界的跳跃间断点，且间断幅度在接近目标时间时趋于无穷；该现象不依赖于具体损失函数（如L²），而是由分布间的拓扑差异本质引起。

Conclusion: 流匹配中的速度场不连续性是拓扑失配的固有表现，提示未来工作需考虑带分支的流、非连续建模或流形约束等新范式，尤其对Riemannian流匹配和神经网络表征能力具有重要启示。

Abstract: Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.

</details>


### [516] [GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients](https://arxiv.org/abs/2512.12827)
*Mohammad Mahdi Razmjoo,Mohammad Mahdi Sharifian,Saeed Bagheri Shouraki*

Main category: cs.LG

TL;DR: 本文提出了一种基于梯度参数本征维数（ID）差异的对抗样本检测方法，利用自然数据与对抗数据在ID上的显著区别，在批量和单样本场景下均取得优异性能，尤其在CIFAR-10等基准上达到92%以上的检测率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易受微小对抗扰动影响，而医疗诊断、自动驾驶等关键应用亟需高鲁棒性的对抗攻击检测方法。

Method: 分析模型输入损失景观的几何特性，特别是梯度参数的本征维数（ID），并利用自然样本与对抗样本在ID上的系统性差异构建检测器。

Result: 在MNIST、SVHN上实现高效批量检测；在CIFAR-10和MS COCO上刷新单样本检测SOTA，对CW、AutoAttack等攻击检测率持续高于92%。

Conclusion: 本征维数是一种鲁棒且普适的对抗指纹，所提几何方法在多种数据集和攻击策略下均表现出强泛化能力与高检测性能。

Abstract: Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.

</details>


### [517] [Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future](https://arxiv.org/abs/2512.12832)
*Kaustav Chatterjee,Joshua Li,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 本研究开发了一个用于评估高速公路与铁路平交道口（HRGCs）悬挂风险的网络级框架，结合深度学习模型重建精确道口剖面，并基于车辆尺寸数据分析不同场景下的悬挂风险，最终构建了支持交通管理部门决策的ArcGIS数据库和软件界面。


<details>
  <summary>Details</summary>
Motivation: 陡峭剖面的高速公路与铁路平交道口对低离地间隙车辆构成安全隐患，易导致车辆卡滞于轨道上，引发列车-车辆碰撞风险。

Method: 采集俄克拉荷马州多个道口的剖面数据（步行式剖面仪与Pave3D8K激光成像系统），构建LSTM-Transformer混合深度学习模型以高精度重建道口剖面；收集约350辆特种车辆尺寸数据，设定三种车辆尺寸情景（中位数、75/25分位组合、最不利组合）开展悬挂风险分析。

Result: 三种情景下分别识别出36、62和67处最高悬挂风险等级的道口；开发了ArcGIS数据库与软件界面，支持交通管理部门开展风险缓解。

Conclusion: 该框架通过融合新一代传感技术、深度学习与基础设施数据，提升了平交道口安全评估能力，并提供了可落地的决策支持工具。

Abstract: Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.

</details>


### [518] [PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks](https://arxiv.org/abs/2512.12840)
*Sindhuja Madabushi,Ahmad Faraz Khan,Haider Ali,Ananthram Swami,Rui Ning,Hongyi Wu,Jin-Hee Cho*

Main category: cs.LG

TL;DR: 本文提出PRIVEE，一种用于垂直联邦学习的隐私保护机制，通过混淆置信度分数来抵御特征推断攻击，同时保持预测准确性和关键分数特性。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习中，参与方共享置信度分数易受特征推断攻击，威胁数据隐私。

Method: PRIVEE机制对置信度分数进行变换，在保留相对排序和分数间距离的前提下，隐藏原始分数，仅共享变换后的表示。

Result: 实验表明，PRIVEE相比现有最优方法，隐私保护能力提升三倍，且完全保持预测性能。

Conclusion: PRIVEE在不牺牲模型精度的前提下，显著增强垂直联邦学习中的隐私防御能力，是一种高效实用的隐私保护方案。

Abstract: Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning "private." PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.

</details>


### [519] [Selective Conformal Risk Control](https://arxiv.org/abs/2512.12844)
*Yunpeng Xu,Wenge Guo,Zhi Wei*

Main category: cs.LG

TL;DR: 本文提出Selective Conformal Risk Control (SCRC)框架，将共形预测与选择性分类结合，通过两阶段策略（样本筛选+子集上的风险控制）提升不确定性量化精度与效率；提出两种算法SCRC-T（精确有限样本保证）和SCRC-I（PAC风格概率保证、更高效），实验表明二者均满足目标覆盖率与风险水平，且SCRC-I更具计算实用性。


<details>
  <summary>Details</summary>
Motivation: 共形预测虽具分布无关的覆盖保证，但常产生过大的预测集合，限制其实际应用；需在保持理论保证的同时提升预测集紧凑性与实用性。

Method: 提出SCRC统一框架：第一阶段选择高置信度样本，第二阶段在选定子集上实施共形风险控制；设计两种算法——SCRC-T（联合校准集与测试集计算阈值，保持交换性，提供精确有限样本保证）和SCRC-I（仅基于校准集，提供PAC风格概率保证，计算更高效）。

Result: 在两个公开数据集上的实验表明，两种方法均能准确达到目标覆盖与风险水平，性能几乎一致；SCRC-I略显保守但计算效率显著更高。

Conclusion: SCRC为实现紧凑、可靠且高效的不确定性量化提供了有效可行的新路径。

Abstract: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.

</details>


### [520] [Information-Consistent Language Model Recommendations through Group Relative Policy Optimization](https://arxiv.org/abs/2512.12858)
*Sonal Prabhune,Balaji Padmanabhan,Kaushik Dutta*

Main category: cs.LG

TL;DR: 本文提出一种基于Group Relative Policy Optimization (GRPO) 的强化学习框架，旨在提升大语言模型（LLM）在语义等价提示下的输出一致性，以满足企业级应用中对信息稳定性的严格要求。


<details>
  <summary>Details</summary>
Motivation: LLM在语义等价但措辞不同的提示下常产生不一致输出，损害可信度、合规性与用户体验；而现有方法（如RAG、调温）无法保证跨等价提示的稳定性。

Method: 将语义等价提示构造成‘组’，重置对话上下文以隔离措辞影响，并设计基于熵的‘有用性’与‘稳定性’奖励，采用适配后的GRPO框架进行强化学习训练。

Result: 在投资与职位推荐任务上，GRPO训练模型相比微调和解码基线显著降低输出变异性。

Conclusion: 首次将GRPO应用于提升LLM信息一致性，将输出变异性从生成多样性特征重新定义为企业部署中可纠正的缺陷。

Abstract: Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.

</details>


### [521] [Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels](https://arxiv.org/abs/2512.12870)
*Pouya Ahadi,Blair Winograd,Camille Zaug,Karunesh Arora,Lijun Wang,Kamran Paynabar*

Main category: cs.LG

TL;DR: 本文提出了一种新的主动学习框架，通过优化标注者分配和设计抗噪采样方法，以降低标签噪声对分类器性能的影响，从而提升在不完美标注数据下的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 主动学习中标签常因标注者准确性差异而含噪声，尤其不确定样本更易被错误标注，导致分类器性能下降。

Method: 提出一种新主动学习框架：1）设计分配模型，将查询样本最优分配给标注者，以最小化每轮可能的最大噪声；2）引入抗噪采样方法，选择受标签噪声影响最小的查询点。

Result: 实验表明该方法在多个基准方法上显著提升了分类性能。

Conclusion: 所提框架能有效缓解标签噪声问题，构建更鲁棒的分类模型。

Abstract: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.

</details>


### [522] [Improving Recursive Transformers with Mixture of LoRAs](https://arxiv.org/abs/2512.12880)
*Mohammadmahdi Nouriborji,Morteza Rohanian,Omid Rohanian*

Main category: cs.LG

TL;DR: 本文提出Mixture of LoRAs（MoL），一种轻量级条件计算机制，在共享前馈网络中插入LoRA专家，实现token级条件权重空间调制，恢复递归Transformer在强参数共享下的表达能力；结合现代化架构ModernALBERT，在多个基准上达到紧凑模型SOTA，并支持专家合并以高效部署。


<details>
  <summary>Details</summary>
Motivation: 参数共享虽减小递归Transformer模型规模，却导致层间表达能力坍塌，亟需在不增加骨干参数的前提下恢复其表达力。

Method: 提出Mixture of LoRAs（MoL），将多个LoRA专家嵌入共享FFN中，实现token条件的权重空间调制；构建ModernALBERT架构，融合旋转位置编码、GeGLU、FlashAttention及蒸馏初始化；并设计专家合并策略以压缩MoL至单个适配器。

Result: ModernALBERT（50M–120M）在GLUE、SQuAD-v2和BEIR上达到紧凑模型SOTA，超越更大规模全参数基线；MoL经专家合并后仍保持精度，显著提升推理效率。

Conclusion: 条件权重空间调制能有效恢复递归Transformer在强参数共享下损失的表达能力，MoL为轻量高效与高表达力兼顾提供了新范式。

Abstract: Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers.

</details>


### [523] [Unsupervised learning of multiscale switching dynamical system models from multimodal neural data](https://arxiv.org/abs/2512.12881)
*DongKyu Kim,Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 本文提出了一种用于学习多尺度切换动力学系统模型的无监督学习算法，能够仅基于多模态神经观测（如尖峰与局部场电位）建模神经活动的非平稳切换动态，并在解码行为任务中优于单尺度和静态多尺度模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法多局限于单一神经模态（如高斯或泊松信号），且难以处理无标签的动态状态切换；而实际神经记录常为多模态、多尺度且状态标签缺失，亟需能融合多模态并自动识别切换 regime 的建模方法。

Method: 提出一种新型无监督学习算法，直接从多尺度神经观测（如 spike-LFP）中联合估计切换多尺度动力学系统的参数，无需 regime 标签。

Result: 在模拟数据及两个真实多模态实验数据集（不同运动任务）上验证：所提模型在行为解码精度上显著优于切换单尺度模型和静态多尺度模型，证实了多尺度融合与非平稳建模的有效性。

Conclusion: 该框架能更准确地刻画复杂、非平稳、多模态神经动力学，有望提升脑机接口的长期鲁棒性，并深化对行为神经机制的理解。

Abstract: Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.

</details>


### [524] [Distillation of Discrete Diffusion by Exact Conditional Distribution Matching](https://arxiv.org/abs/2512.12889)
*Yansong Gao,Yu Sun*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件分布匹配的离散扩散模型（DDM）推理加速方法，无需训练额外网络或使用近似模拟器，通过利用反向条件分布的马尔可夫分解结构，直接对齐教师与低函数评估（NFE）学生模型的条件分布，适用于单步与少步采样。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型（DDMs）采样通常需要大量函数评估（NFE），导致推理开销大；现有加速方法依赖近似模拟器（如τ-leaping）或需训练学生模型和辅助网络，缺乏简洁性与原理性。

Method: 基于条件分布匹配的蒸馏方法：利用反向条件分布p_{0|t}(x_0|x_t)的马尔可夫分解性质，通过边际密度比和已知前向连续时间马尔可夫链（CTMC）核恢复该分布，并据此构建蒸馏目标，直接匹配教师与低NFE学生模型在一步或多步采样中的条件分布。

Result: 实现了无需额外网络或代理目标的高效DDM加速，在保持生成质量的同时显著降低采样所需的函数评估次数。

Conclusion: 条件分布匹配是一种简单、原理性强且通用的DDM蒸馏框架，为离散生成模型的高效推理提供了新思路。

Abstract: Discrete diffusion models (DDMs) are a powerful class of generative models for categorical data, but they typically require many function evaluations for a single sample, making inference expensive. Existing acceleration methods either rely on approximate simulators, such as $τ$-leaping, or on distillation schemes that train new student models and auxiliary networks with proxy objectives. We propose a simple and principled distillation alternative based on \emph{conditional distribution matching}. Our key observation is that the reverse conditional distribution of clean data given a noisy state, $p_{0\mid t}(x_0 \mid x_t)$, admits a Markov decomposition through intermediate times and can be recovered from marginal density ratios and the known forward CTMC kernel. We exploit this structure to define distillation objectives that directly match conditional distributions between a pre-trained teacher and a low-NFE student, both for one-step and few-step samplers.

</details>


### [525] [Wait, Wait, Wait... Why Do Reasoning Models Loop?](https://arxiv.org/abs/2512.12895)
*Charilaos Pipis,Shivam Garg,Vasilis Kontonis,Vaishnavi Shrivastava,Akshay Krishnamurthy,Dimitris Papailiopoulos*

Main category: cs.LG

TL;DR: 本文研究了推理模型（如DeepSeek-R1）在低温度下生成思维链时频繁循环的问题，指出其根源在于训练分布与模型学习之间的不匹配（即‘学习错误’），并揭示了风险规避倾向和Transformer的时序相关错误偏差两种机制；高温仅通过增强探索缓解循环，但无法根治学习错误，因此需从训练层面干预。


<details>
  <summary>Details</summary>
Motivation: 推理模型在低温度或贪婪解码下易出现重复文本循环，影响推理效率与可靠性，亟需理解其成因并提出根本性解决方案。

Method: 结合开源推理模型实证分析、合成图推理任务建模，并从学习难度差异与Transformer归纳偏置两方面机制进行归因分析。

Result: 发现低温度下循环普遍；大模型循环更少，蒸馏学生模型却显著更易循环；验证了风险规避与时间相关错误偏差是导致循环的两大机制；高温仅缓解而非消除循环。

Conclusion: 循环主因是模型在训练中未充分覆盖正确推理路径所导致的‘学习错误’，温度调节仅为权宜之计，根本解决需在训练阶段针对性减少此类错误。

Abstract: Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.

</details>


### [526] [Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning](https://arxiv.org/abs/2512.12896)
*Parthasarathy Nadarajan,Michael Botsch*

Main category: cs.LG

TL;DR: 本文提出了一种基于随机森林的网格化方法，用于实时预测多目标交通场景的未来状态（Predicted-Occupancy Grids, POG），兼顾行为不确定性建模与计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统模型驱动的POG预测方法计算负载过高，难以满足实时车辆安全应用需求；同时需精细建模交通参与者行为的不确定性。

Method: 提出一种增强型占用栅格表示当前交通场景，并采用随机森林算法学习其到POG的映射；对比验证了该机器学习方法与原有模型驱动方法的性能。

Result: 机器学习方法在仿真中展现出良好性能，支持POG的实时计算，为车辆安全系统中的临界性评估和轨迹规划提供更优支撑。

Conclusion: 基于随机森林的网格化学习方法可有效替代高开销的模型驱动POG生成，在保证不确定性建模能力的同时实现高效实时预测，适用于车载安全系统。

Abstract: This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.

</details>


### [527] [Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm](https://arxiv.org/abs/2512.12901)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的复杂交通场景概率时空表示方法（POG），通过分层情境分类器识别交通类型，并利用堆叠去噪自编码器（SDA）降维增强的随机森林（RF）预测交通参与者未来行为，最终应用于场景危险性评估与安全轨迹规划。


<details>
  <summary>Details</summary>
Motivation: 为提升主动车辆安全系统在复杂交通场景中动态操控的可靠性，需对交通参与者未来行为进行准确的概率时空建模。

Method: 采用分层情境分类器识别道路基础设施与关键交通参与者；针对每类场景独立训练随机森林（RF）预测预测占用栅格（POG）；输入为增强占用栅格（AOG），经堆叠去噪自编码器（SDA）降维以提升RF预测精度。

Result: 所提SDA+RF方法在仿真和实车实验中均表现出优异性能；POG成功应用于交通场景危险性估计与安全轨迹生成。

Conclusion: 该概率时空表示框架兼顾可解释性与预测精度，为高级驾驶辅助与自动驾驶中的实时风险评估与决策提供了有效支撑。

Abstract: In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.

</details>


### [528] [Next-generation reservoir computing validated by classification task](https://arxiv.org/abs/2512.12903)
*Ken-ichi Kitayama*

Main category: cs.LG

TL;DR: 本文首次证明了下一代储备计算（NG-RC）在分类任务中可媲美传统储备计算（RC），拓展了其应用范围，验证了其在预测与分类两类任务中的通用计算能力。


<details>
  <summary>Details</summary>
Motivation: 现有NG-RC的基准测试仅限于时间序列预测任务（如Lorenz 63、Mackey-Glass信号），缺乏对其分类能力的验证，限制了对其通用计算能力的评估。

Method: 通过设计并实施分类任务实验，将NG-RC与传统RC在相同条件下进行对比评估。

Result: NG-RC在分类任务中性能达到与传统RC相当的水平。

Conclusion: NG-RC不仅适用于预测任务，也具备与传统RC相当的分类能力，展现出更广泛的适用性和通用计算潜力。

Abstract: An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.

</details>


### [529] [Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic](https://arxiv.org/abs/2512.12907)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 本文提出了一种基于堆叠去噪自编码器（SDA）和随机森林的新型机器学习架构，用于高效估计复杂交通场景的概率时空表示（POG），输入为增强占用栅格（AOG），相比SDA+DeconvNet方法在精度和计算时间上更具优势。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶及主动安全系统亟需对交通场景未来时空状态进行精确、带不确定性的概率化预测。

Method: 提出一种由两个堆叠去噪自编码器（SDA）和若干随机森林组成的混合架构；输入为当前交通场景的增强占用栅格（AOG），先识别场景类型，再映射到未来可能状态，输出为含行为不确定性的预测占用栅格（POG）；并与两种基于SDA+DeconvNet的架构在仿真中对比评估。

Result: 所提架构在预测精度和计算耗时两方面均优于对比的SDA+DeconvNet架构。

Conclusion: 该混合架构能更高效、准确地生成交通场景的概率时空表示（POG），适用于自动驾驶与主动安全系统。

Abstract: This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.

</details>


### [530] [LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization](https://arxiv.org/abs/2512.12922)
*Bangyu Li,Boping Gu,Ziyang Ding*

Main category: cs.LG

TL;DR: 本文提出了一种基于大语言模型（LLM）的个性化投资组合推荐框架，融合强化学习与个体风险偏好建模，以应对动态市场与投资者个性化需求。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则或静态优化的投资组合方法难以捕捉投资者行为、市场波动与金融目标之间复杂的非线性关系，无法满足现代投资者对个性化与自适应策略的需求。

Method: 构建了一个集成框架，结合大型语言模型（LLM）、强化学习和个体化风险偏好建模，实现对投资决策的智能支持。

Result: 实现了能响应动态市场条件并适配个体风险偏好的个性化投资组合推荐能力。

Conclusion: 该LLM驱动的推荐框架为智能投顾提供了新范式，提升了投资策略的个性化、适应性与解释性。

Abstract: In modern financial markets, investors increasingly seek personalized and adaptive portfolio strategies that reflect their individual risk preferences and respond to dynamic market conditions. Traditional rule-based or static optimization approaches often fail to capture the nonlinear interactions among investor behavior, market volatility, and evolving financial objectives. To address these limitations, this paper introduces the LLM-based Personalized Portfolio Recommender , an integrated framework that combines Large Language Models, reinforcement learning, and individualized risk preference modeling to support intelligent investment decision-making.

</details>


### [531] [SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision](https://arxiv.org/abs/2512.12930)
*Yuseon Choi,Sangjin Kim,Jungjun Oh,Byeongcheol Kim,Hoi-Jun Yoo*

Main category: cs.LG

TL;DR: 本文提出SeVeDo，一种基于SVD的异构加速器，通过分离异常值敏感组件到高精度低秩路径、其余部分采用低比特残差数据路径与分层分组量化（HGQ）及SVD引导的混合精度（SVD-MP），在保持高精度的同时显著提升能效。


<details>
  <summary>Details</summary>
Motivation: 低比特量化虽可提升Transformer推理效率，但激进的位宽缩减因激活异常值导致精度下降；现有异常值处理方法虽精度高，但能耗大。

Method: 提出SeVeDo架构：1）SVD分解引导的异构计算路径（高精度低秩路径+低比特残差路径）；2）分层分组量化（HGQ），融合粗粒度浮点缩放与细粒度移位以降低反量化开销；3）SVD-MP静态分配更高比特宽给精度敏感组件。

Result: SeVeDo在ViT-Base和Llama2-7B上分别实现12.7TOPS/W和13.4TOPS/W能效，峰值达13.8TOPS/W，优于传统设计。

Conclusion: SeVeDo通过结构化分离与SVD驱动的混合精度策略，在精度与能效间取得更好平衡，为低比特Transformer加速提供了高效硬件方案。

Abstract: Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.

</details>


### [532] [Investigating Data Pruning for Pretraining Biological Foundation Models at Scale](https://arxiv.org/abs/2512.12932)
*Yifan Wu,Jiyue Jiang,Xichen Ye,Yiqi Wang,Chang Zhou,Yitao Xu,Jiayang Chen,He Hu,Weizhong Zhang,Cheng Jin,Jiao Yuan,Yu Li*

Main category: cs.LG

TL;DR: 本文提出了一种面向生物序列的后验影响引导数据剪枝框架，用于降低生物基础模型（BioFM）预训练的计算成本，显著提升可复现性与可及性。


<details>
  <summary>Details</summary>
Motivation: 现有BioFM依赖海量序列和参数，导致计算成本高、可复现性和可及性差，尤其对学术实验室构成障碍。

Method: 提出基于子集的自影响度量方法，并设计Top-k Influence（Top I）和Coverage-Centric Influence（CCI）两种高效数据选择策略。

Result: 在RNA-FM和ESM-C上验证：在>99%剪枝率下仍优于随机基线；所选核心集甚至优于10倍大小的随机子集，揭示生物序列数据存在高度冗余。

Conclusion: 影响引导的数据剪枝能显著降低BioFM预训练开销，推动更高效、可及、可持续的生物AI研究。

Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.

</details>


### [533] [Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties](https://arxiv.org/abs/2512.12947)
*Nischal Subedi,Ember Kerstetter,Winnie Li,Silo Murphy*

Main category: cs.LG

TL;DR: 本文通过在Amazon Computers数据集上的诊断研究，揭示了图卷积网络（GCN）在半监督节点分类中性能提升的关键条件：图同质性（homophily）与节点特征质量的交互作用。GCN在标签极度稀缺时增益最大，甚至在特征为随机噪声时仍能保持性能（若图高度同质）；但在低同质性且特征较强时反而损害性能。最终提出四象限准则指导实践者是否采用GCN。


<details>
  <summary>Details</summary>
Motivation:  practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines

Method: diagnostic study using the Amazon Computers co-purchase data, systematic experiments with simulated label scarcity, feature ablation, and per-class analysis

Result: GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity. GCNs can match their original performance even when node features are replaced with random noise on highly homophilous graphs. GCNs hurt performance when homophily is low and features are already strong.

Conclusion: GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.

Abstract: Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.

</details>


### [534] [Application of Deep Learning in Biological Data Compression](https://arxiv.org/abs/2512.12975)
*Chunyu Zou*

Main category: cs.LG

TL;DR: 本文提出了一种基于隐式神经表示（INR）的深度学习方法，用于高效压缩冷冻电镜（Cryo-EM）生物数据，在保持重建质量的同时显著减小文件体积。


<details>
  <summary>Details</summary>
Motivation: Cryo-EM数据文件体积庞大，给存储、共享和教学带来挑战，亟需高效且保真的压缩方法。

Method: 首先根据密度阈值提取二值图并用GZIP压缩；再利用带位置编码的神经网络建模空间密度分布，优化加权MSE损失函数，并存储网络参数与可学习隐向量。

Result: 实现了兼顾压缩比与重建质量的Cryo-EM数据压缩方案，适用于科研与教育场景。

Conclusion: 隐式神经表示是一种有潜力的Cryo-EM数据压缩范式，结合位置编码与加权损失可有效提升重建精度。

Abstract: Cryogenic electron microscopy (Cryo-EM) has become an essential tool for capturing high-resolution biological structures. Despite its advantage in visualizations, the large storage size of Cryo-EM data file poses significant challenges for researchers and educators. This paper investigates the application of deep learning, specifically implicit neural representation (INR), to compress Cryo-EM biological data. The proposed approach first extracts the binary map of each file according to the density threshold. The density map is highly repetitive, ehich can be effectively compressed by GZIP. The neural network then trains to encode spatial density information, allowing the storage of network parameters and learnable latent vectors. To improve reconstruction accuracy, I further incorporate the positional encoding to enhance spatial representation and a weighted Mean Squared Error (MSE) loss function to balance density distribution variations. Using this approach, my aim is to provide a practical and efficient biological data compression solution that can be used for educational and research purpose, while maintaining a reasonable compression ratio and reconstruction quality from file to file.

</details>


### [535] [CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks](https://arxiv.org/abs/2512.12981)
*Jonathan Wenshøj,Tong Chen,Bob Pepin,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 本文提出CoDeQ，一种端到端可微的联合剪枝-量化方法，通过将量化器死区宽度参数化并联合学习，实现无需辅助步骤的架构无关压缩。


<details>
  <summary>Details</summary>
Motivation: 现有联合剪枝-量化方法依赖训练循环外的辅助过程，带来工程复杂性、调参困难，并缺乏直接数据驱动梯度信号，导致压缩次优。

Method: 提出CoDeQ方法，关键观察是标量量化器的死区等价于幅值剪枝；将死区宽度参数化并随量化参数一同通过反向传播学习，解耦稀疏性与比特宽选择，支持固定/混合精度量化。

Result: 在ImageNet上ResNet-18模型中，CoDeQ将比特操作降至约5%，同时保持接近全精度准确率；无需任何辅助步骤，具备架构无关性和易实现性。

Conclusion: CoDeQ是一种简单、完全可微、端到端优化的联合剪枝-量化方法，统一了稀疏模式与量化参数学习，显著提升压缩效率与实用性。

Abstract: While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.

</details>


### [536] [Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)](https://arxiv.org/abs/2512.13010)
*Hassan Iftikhar,Rizwan Ahmad,Arunark Kolipaka*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的剪切模量估计框架DIME，用于磁共振弹性成像（MRE），以克服传统Multimodal Direct Inversion（MMDI）方法在非均匀介质中精度低、噪声敏感等问题。DIME通过有限元模拟生成的数据训练，在仿真和真实数据上均展现出比MMDI更高的准确性、鲁棒性和生理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统MMDI算法基于Helmholtz方程，假设介质均匀无限，且对噪声敏感，导致组织剪切刚度估计不准确、不可靠。

Method: 提出深度学习驱动的DIME框架，使用有限元建模（FEM）生成的位移场-刚度图配对数据进行训练；为提升局部波行为建模能力和抗全局图像变化鲁棒性，采用小图像块训练策略。

Result: 在仿真数据（包括均匀、非均匀及解剖结构引导的肝脏模型）中，DIME较MMDI显著提升刚度图质量（更低像素间变异、更准边界、更高与真值相关性r=0.99, R²=0.98）；在8名健康与7名纤维化受试者的体内MRE数据中，DIME保持生理一致的刚度分布，且无MMDI的方向性偏差。

Conclusion: DIME在仿真与初步临床数据中均优于MMDI，展现出更强的鲁棒性与准确性，具备向MRE临床应用转化的可行性。

Abstract: The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.

</details>


### [537] [Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization](https://arxiv.org/abs/2512.13034)
*Xiaoyu He,Yu Cai,Jin Jia,Canxi Huang,Wenqing Chen,Zibin Zheng*

Main category: cs.LG

TL;DR: 本文提出Alada，一种用于大规模矩阵随机优化的自适应动量方法，通过秩一因子分解估计梯度二阶矩，具有亚线性内存开销，并可扩展至张量变量优化；同时引入无额外内存开销的一阶矩估计以增强鲁棒性，理论性能与Adam相当，实验表明其在NLP任务中相较Adam及其变体更省内存且更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 解决大规模矩阵及张量优化中传统自适应优化器（如Adam）内存开销过大的问题，尤其在训练大型模型时内存瓶颈显著。

Method: 提出Alada算法：采用交替更新的秩一因子分解方式估计梯度二阶矩，实现低内存开销；并设计无额外内存成本的一阶矩估计规则；支持扩展至张量变量优化。

Result: Alada具有亚线性内存开销，理论收敛性与Adam一致；在多个NLP任务实验中，相比Adam及其变体，显著降低内存占用并提升训练鲁棒性。

Conclusion: Alada是一种高效、低内存、鲁棒的自适应优化方法，适用于大规模矩阵和张量优化，在理论和实践上均优于或媲美现有主流方法。

Abstract: This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.

</details>


### [538] [Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments](https://arxiv.org/abs/2512.13060)
*Kangning Gao,Yi Hu,Cong Nie,Wei Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度Q学习的ETL智能调度优化框架，将ETL调度建模为马尔可夫决策过程，通过强化学习实现在异构数据环境下的自适应任务与资源调度，显著降低调度延迟、提升吞吐量和执行稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决异构数据环境下ETL流程中存在的调度效率低、资源分配不均衡、适应性差等问题。

Method: 将ETL调度形式化为马尔可夫决策过程，构建包含状态表示模块、特征嵌入网络、Q值估计器和奖励评估机制的深度Q学习框架，并设计兼顾平均调度延迟、任务完成率、吞吐量和资源利用率的多目标奖励函数。

Result: 实验表明该框架在多源异构任务条件下显著降低调度延迟、提高系统吞吐量和执行稳定性；敏感性实验验证了其在超参数变化、环境动态性和数据规模变化下的鲁棒性。

Conclusion: 深度Q学习在复杂数据调度与资源管理中具有强大潜力，为智能数据管道构建提供了高效、可扩展的优化策略。

Abstract: This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.

</details>


### [539] [Multi-fidelity aerodynamic data fusion by autoencoder transfer learning](https://arxiv.org/abs/2512.13069)
*Javier Nieto-Centenero,Esther Andrés,Rodrigo Castellanos*

Main category: cs.LG

TL;DR: 本文提出了一种多保真度深度学习框架，结合自编码器迁移学习与新提出的多分割共形预测（MSCP）方法，在极低高保真数据条件下实现不确定性感知的气动数据融合，显著提升预测精度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 高保真气动模拟计算成本高昂，限制了其在数据驱动建模中的应用，亟需利用低成本低保真数据提升模型精度并量化不确定性。

Method: 提出基于自编码器的迁移学习框架：用大量低保真数据训练自编码器学习紧凑的物理隐表示，并将其作为冻结知识库；再用少量高保真数据微调解码器；同时引入多分割共形预测（MSCP）实现点对点高覆盖率（>95%）的不确定性量化。

Result: 在NACA翼型（2D）和跨音速机翼（3D）表面压力预测任务中，仅用极少高保真样本即可有效校正低保真偏差，实现高精度预测，并输出鲁棒、可操作的不确定性区间。

Conclusion: 该方法兼具极端数据效率与不确定性量化能力，为数据稀缺场景下的气动回归提供了可扩展、可靠的解决方案。

Abstract: Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.

</details>


### [540] [LikeBench: Evaluating Subjective Likability in LLMs for Personalization](https://arxiv.org/abs/2512.13077)
*Md Awsafur Rahman,Adam Gabrys,Doug Kang,Jingjing Sun,Tian Tan,Ashwin Chandramouli*

Main category: cs.LG

TL;DR: 本文提出LikeBench，首个聚焦LLM个性化中‘用户喜好度（likability）’的多维度、多轮次动态评测框架，强调模型在真实对话中随时间适应用户偏好能力，并揭示记忆准确率与喜好度无强相关性。


<details>
  <summary>Details</summary>
Motivation: 现有个性化LLM评测仅关注事实回忆与应用准确性，忽视主观但关键的‘likability’维度，缺乏对其多方面、动态演化的量化评估。

Method: 构建LikeBench：基于心理建模的细粒度模拟用户 persona，在多轮对话中仅通过实时交互学习偏好，并从情感适配、正式度匹配、知识适配、指代理解、话轮长度适配、幽默适配、回调共7个维度逐轮评估likability。

Result: 实验证明高记忆准确率不等于高likability（如DeepSeek R1以86%记忆准确率反超Qwen3的93%，likability高28%）；SOTA模型（如GPT-5）在长程、嘈杂对话中适应鲁棒性不足。

Conclusion: likability是独立且关键的个性化维度，需被系统化、多维化、动态化评测；LikeBench为未来可喜、可信、可适应的LLM提供了新基准与诊断工具。

Abstract: A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.

</details>


### [541] [TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning](https://arxiv.org/abs/2512.13106)
*Shenzhi Yang,Guangcheng Zhu,Xing Zheng,Yingfan MA,Zhongqi Chen,Bowen Song,Weiqiang Wang,Junbo Zhao,Gang Chen,Haobo Wang*

Main category: cs.LG

TL;DR: 本文提出了一种半监督强化学习与可验证奖励（RLVR）新范式——TraPO，利用少量标注数据引导无标注数据上的训练，通过轨迹相似性筛选可靠样本，显著提升数学推理等任务的性能与泛化能力，且大幅降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有无监督RLVR方法因缺乏外部监督易导致模型崩溃；而全监督方法标注成本过高。因此需要一种兼顾低标注成本与训练稳定性的新范式。

Method: 提出TraPO算法：基于标注样本的学习轨迹，在无标注样本中匹配相似轨迹以识别可靠样本，并将监督奖励用于稳定一致性奖励的优化过程。

Result: 在6个数学推理基准和3个OOD任务上，仅用1K标注+3K无标注样本即达42.6%平均准确率，超越使用45K无标注样本的最优无监督方法（38.3%）；用4K+12K样本更全面超越全监督基线（45K标注），仅用10%标注量。

Conclusion: 监督信号对稳定无标注数据上的RLVR训练至关重要；TraPO验证了小规模高质量标注可极大提升半监督RLVR效率与效果，为大推理模型低成本训练提供了可行路径。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.

</details>


### [542] [From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network](https://arxiv.org/abs/2512.13111)
*Hayk Amirkhanian,Marco F. Huber*

Main category: cs.LG

TL;DR: 本文提出了一种分层近似贝叶斯神经网络（HABNN），通过引入高斯-逆威沙特超先验来提升模型鲁棒性与性能，并以闭式解析形式高效计算Student's t分布参数，显著改善过拟合问题和OOD不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 解决传统神经网络中超参数调优困难和过拟合问题，提升对分布外（OOD）数据的可靠不确定性建模能力。

Method: 提出分层近似贝叶斯神经网络（HABNN），采用高斯-逆威沙特分布作为权重的超先验，推导出预测分布与权重后验的解析表达式，实现Student's t分布参数的线性复杂度闭式计算。

Result: HABNN在抑制过拟合、提供可靠不确定性估计（尤其在OOD任务中）方面表现稳健，实验表明其性能达到甚至超越当前最优模型。

Conclusion: HABNN为贝叶斯神经网络提供了更高效、鲁棒的近似推断框架，在安全关键场景中具有重要应用前景。

Abstract: In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.

</details>


### [543] [Quanvolutional Neural Networks for Spectrum Peak-Finding](https://arxiv.org/abs/2512.13125)
*Lukas Bischof,Rudolf M. Füchslin,Kurt Stockinger,Pavel Sulimov*

Main category: cs.LG

TL;DR: 本文提出了一种基于量子卷积神经网络（QuanvNN）的方法，用于NMR谱的多任务峰识别（计数与定位），在合成数据集上相比经典CNN提升了F1分数11%、平均绝对误差降低30%，且收敛更稳定。


<details>
  <summary>Details</summary>
Motivation: NMR谱等光谱的峰解析（去卷积）对专家和机器均具挑战性，尤其面对复杂分子；量子计算的兴起为提升现有机器学习方法提供了新可能。

Method: 受经典CNN启发，设计并实现了一种简单可解释的QuanvNN架构，直接对标经典CNN，用于解决多任务峰识别问题（峰计数与位置估计），并在合成NMR数据集上进行评估。

Result: QuanvNN在困难谱图上优于经典CNN：F1分数提升11%，峰位置估计的平均绝对误差降低30%，且对更难问题表现出更好的收敛稳定性。

Conclusion: QuanvNN是一种有前景的量子机器学习方法，在光谱峰解析任务中展现出超越经典CNN的性能与鲁棒性，验证了量子增强在该领域的潜力。

Abstract: The analysis of spectra, such as Nuclear Magnetic Resonance (NMR) spectra, for the comprehensive characterization of peaks is a challenging task for both experts and machines, especially with complex molecules. This process, also known as deconvolution, involves identifying and quantifying the peaks in the spectrum. Machine learning techniques have shown promising results in automating this process. With the advent of quantum computing, there is potential to further enhance these techniques. In this work, inspired by the success of classical Convolutional Neural Networks (CNNs), we explore the use of Quanvolutional Neural Networks (QuanvNNs) for the multi-task peak finding problem, involving both peak counting and position estimation. We implement a simple and interpretable QuanvNN architecture that can be directly compared to its classical CNN counterpart, and evaluate its performance on a synthetic NMR-inspired dataset. Our results demonstrate that QuanvNNs outperform classical CNNs on challenging spectra, achieving an 11\% improvement in F1 score and a 30\% reduction in mean absolute error for peak position estimation. Additionally, QuanvNNs appear to exhibit better convergence stability for harder problems.

</details>


### [544] [Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency](https://arxiv.org/abs/2512.13149)
*Xinwei Tai,Dongmian Zou,Hongfei Wang*

Main category: cs.LG

TL;DR: 本文提出了一种通过去相关节点特征来缓解图域自适应（GDA）中条件偏移问题的新方法，理论分析支持该策略，并通过去相关GCN和图Transformer层实现，在实验中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决无监督图域自适应（GDA）中的条件偏移问题，该问题源于节点特征间的局部依赖性，限制了知识迁移效果。

Method: 理论分析证明条件偏移仅在节点特征存在局部依赖时出现；基于马尔可夫链建模依赖关系并推导泛化界；据此设计去相关GCN层与图Transformer层以消除节点特征依赖。

Result: 所提方法在多个GDA基准上显著优于现有基线；可视化显示学习表征中类内距离显著减小；代码已开源。

Conclusion: 节点特征去相关是缓解图域自适应中条件偏移的有效且可行策略，理论与实验共同验证其有效性。

Abstract: Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT

</details>


### [545] [SACn: Soft Actor-Critic with n-step Returns](https://arxiv.org/abs/2512.13165)
*Jakub Łyskawa,Jakub Lewandowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 本文提出了一种将Soft Actor-Critic（SAC）与n步回报结合的新方法（SACn），通过数值稳定的重要性采样和τ-采样熵估计，解决了传统n步SAC中因策略分布变化导致的偏差与重要性采样不稳定性问题，并在MuJoCo环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: SAC难以与n步回报结合，因其引入策略分布变化带来的偏差；而常用的重要性采样虽可纠偏，却易导致数值不稳定。

Method: 提出数值稳定的重要性采样方案并简化超参选择；在n步最大熵框架下分析SAC的熵估计，提出τ-采样熵估计以降低学习目标方差；最终构建SACn算法。

Result: SACn在MuJoCo仿真环境中实验验证有效，兼顾收敛速度提升与训练稳定性。

Conclusion: 本文成功将n步回报融入SAC，在保持理论严谨性的同时提升了实际性能与数值鲁棒性，为离线/在线最大熵RL提供了新思路。

Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.

</details>


### [546] [PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning](https://arxiv.org/abs/2512.13186)
*Khalid Ferji*

Main category: cs.LG

TL;DR: 本文提出了PolySet框架，将聚合物表示为从假设的摩尔质量分布中采样的有限加权链集合，从而更真实地反映聚合物的统计本质，并提升机器学习模型对尾部敏感性质的学习能力与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型将聚合物视为单一确定分子图，忽视了实际聚合物是具有分子量分布的链集合这一物理现实，导致建模偏差。

Method: 提出PolySet框架，以加权链集合表征聚合物，该集合采样自假设的摩尔质量分布；采用与化学细节无关、兼容任意分子表示的通用编码方式，并在均聚物中结合极简语言模型实现。

Result: PolySet能保留高阶分布矩（如Mz、Mz+1），使ML模型显著提升对尾部敏感性质预测的稳定性与精度。

Conclusion: PolySet为聚合物机器学习提供了物理基础扎实的新范式，可自然拓展至共聚物、嵌段结构等复杂拓扑体系。

Abstract: Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.

</details>


### [547] [WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory](https://arxiv.org/abs/2512.13190)
*Jin Sob Kim,Hyun Joon Park,Wooseok Shin,Dongil Park,Sung Won Han*

Main category: cs.LG

TL;DR: 本文提出了一种名为WAY的新型深度学习架构，用于基于全球AIS数据进行长期船舶目的地预测，并引入梯度丢弃（GD）技术以提升多对一训练效果。


<details>
  <summary>Details</summary>
Motivation: AIS数据存在可靠性差和时间间隔不规则的问题，现有方法难以准确进行长时程船舶目的地预测。

Method: 将长距离港-港轨迹重构为嵌套序列结构，利用空间网格缓解时空偏差；设计WAY模型，包含轨迹表征层和通道聚合顺序处理（CASP）模块，并引入任务专用的梯度丢弃（GD）技术实现单标签多对多训练。

Result: 在5年AIS数据上的实验表明，WAY显著优于传统基于空间网格的方法，且GD技术带来性能增益；多任务学习验证了其在ETA估计中的实用潜力。

Conclusion: WAY架构及其GD机制有效提升了船舶目的地长期预测精度与鲁棒性，具备实际海事监控应用价值。

Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.

</details>


### [548] [Noise-Resilient Quantum Aggregation on NISQ for Federated ADAS Learning](https://arxiv.org/abs/2512.13196)
*Chethana Prasad Kabgere,Sudarshan T S B*

Main category: cs.LG

TL;DR: 本文提出了一种面向车载边缘的噪声鲁棒型量子联邦学习框架（NR-QFL），结合变分量子电路与经典训练，在NISQ设备上实现低延迟、高安全性与抗噪的模型聚合，显著提升ADAS系统中联邦学习的收敛稳定性与通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在车载网络中易受噪声、延迟和安全限制影响，难以满足ADAS对实时性、隐私性和鲁棒性的综合需求。

Method: 提出NR-QFL框架，利用变分量子电路（VQC）编码模型参数，引入自适应门重参数化、量子熵驱动的客户端选择及多服务器协同机制，并在完全正迹保持（CPTP）动力学下证明其有界误差收敛性与噪声鲁棒性。

Result: 实验表明NR-QFL在受限边缘条件下仍能保持稳定收敛，梯度方差降低、通信开销减少、抗噪能力增强。

Conclusion: NR-QFL为车载边缘场景下的量子增强联邦学习提供了可扩展、安全、高效且动态稳定的新型范式，推动ADAS智能向更可靠、隐私友好的方向发展。

Abstract: Advanced Driver Assistance Systems (ADAS) increasingly employ Federated Learning (FL) to collaboratively train models across distributed vehicular nodes while preserving data privacy. Yet, conventional FL aggregation remains susceptible to noise, latency, and security constraints inherent to real-time vehicular networks. This paper introduces Noise-Resilient Quantum Federated Learning (NR-QFL), a hybrid quantum-classical framework that enables secure, low-latency aggregation through variational quantum circuits (VQCs) operating under Noisy Intermediate-Scale Quantum (NISQ) conditions. The framework encodes model parameters as quantum states with adaptive gate reparameterization, ensuring bounded-error convergence and provable resilience under Completely Positive Trace-Preserving (CPTP) dynamics. NR-QFL employs quantum entropy-based client selection and multi-server coordination for fairness and stability. Empirical validation shows consistent convergence with reduced gradient variance, lower communication overhead, and enhanced noise tolerance under constrained edge conditions. The framework establishes a scalable foundation for quantum-enhanced federated learning, enabling secure, efficient, and dynamically stable ADAS intelligence at the vehicular edge.

</details>


### [549] [Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting](https://arxiv.org/abs/2512.13207)
*Karina Chichifoi,Fabio Merizzi,Michele Colajanni*

Main category: cs.LG

TL;DR: 本文研究了在基于CERRA数据集的联邦学习表面温度预测中，数据投毒攻击（如全局偏差和局部补丁攻击）对预测性能的影响，并评估了截断均值聚合作为防御机制的有效性及其局限性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在气象预测中具有潜力，但其分布式特性引入了新的安全漏洞，尤其是数据投毒攻击可能通过空间依赖性影响大范围预测结果。

Method: 在CERRA数据集上模拟地理分布的客户端，实施补丁式和全局偏差式数据投毒攻击，并采用截断均值聚合进行防御评估。

Result: 少量中毒客户端即可显著误导大范围温度预测；单个客户端的全局偏差攻击可导致-1.7 K偏差；协同补丁攻击使均方误差增至三倍以上并产生+3.5 K以上的持续区域异常；截断均值能缓解全局攻击（性能下降2–13%），但加剧补丁攻击（误差放大281–603%）。

Conclusion: 面向空间相关气象数据的联邦学习需更鲁棒的防御机制，传统基于离群值剔除的方法在局部扰动场景下失效。

Abstract: Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\% degradation) but fails against patch attacks (281-603\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.

</details>


### [550] [ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data](https://arxiv.org/abs/2512.13228)
*Melvin Barbaux*

Main category: cs.LG

TL;DR: ModSSC是一个开源Python框架，统一支持归纳式和直推式半监督分类，涵盖多种数据模态与算法，具备模块化设计、统一配置接口和YAML声明式实验描述，便于复现与大规模对比研究。


<details>
  <summary>Details</summary>
Motivation: 现有半监督分类软件支持分散、缺乏统一框架，难以在不同方法和数据模态间进行公平比较与复现。

Method: 构建模块化Python框架ModSSC，集成经典与前沿半监督算法，支持多模态数据加载器（表格、图像、文本、音频、图），提供统一YAML配置接口及CPU/GPU兼容运行时。

Result: 实现了跨模态、跨算法、跨硬件的统一半监督分类实验平台，支持轻量级经典方法与深度学习方法，具备完整文档、测试和MIT开源许可。

Conclusion: ModSSC填补了半监督分类领域系统性开源框架的空白，提升了方法评估、复现与扩展的效率与标准化水平。

Abstract: Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.

</details>


### [551] [CORE: Contrastive Masked Feature Reconstruction on Graphs](https://arxiv.org/abs/2512.13235)
*Jianyuan Bo,Yuan Fang*

Main category: cs.LG

TL;DR: 本文提出了一种名为CORE的新型图自监督学习框架，将对比学习融入掩码特征重建（MFR）中，在节点和图分类任务上显著超越现有方法如GraphMAE和GraphMAE2。


<details>
  <summary>Details</summary>
Motivation: 观察到掩码特征重建（MFR）与图对比学习（GCL）在最大化相似元素一致性上的共性，进而探索二者融合以提升图自监督学习性能。

Method: 提出Contrastive Masked Feature Reconstruction（CORE）框架：在MFR基础上，将原始与重建的掩码节点特征构成为正样本对，并以掩码节点自身作为负样本，联合利用重建能力与判别能力。

Result: CORE在节点分类和图分类任务上均显著优于MFR基线及GraphMAE、GraphMAE2等先进方法，最高分别提升3.82%和3.76%。

Conclusion: MFR与节点级GCL在特定条件下目标一致，二者可互补融合；CORE验证了该融合策略的有效性，为图自监督学习提供了新范式。

Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.

</details>


### [552] [BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation](https://arxiv.org/abs/2512.13255)
*Yunhong Min,Juil Koo,Seungwoo Yoo,Minhyuk Sung*

Main category: cs.LG

TL;DR: BézierFlow是一种轻量级训练方法，通过将采样轨迹调度器参数化为贝塞尔函数，在≤10次函数求值（NFE）下实现2–3倍加速，仅需15分钟训练。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级方法仅限于ODE离散化的时间步学习，亟需扩展至更广义的采样轨迹优化。

Method: 提出用贝塞尔函数参数化随机插值（SI）调度器，利用其控制点天然满足边界条件、可微性与信噪比（SNR）单调性等关键性质。

Result: 在多个预训练扩散与流模型上，BézierFlow在极少数NFE下显著超越先前时间步学习方法。

Conclusion: 将搜索空间从离散时间步拓展至连续贝塞尔轨迹变换，是提升少步生成效率的有效新范式。

Abstract: We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2-3x performance improvement for sampling with $\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.

</details>


### [553] [ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning](https://arxiv.org/abs/2512.13316)
*Mayank Gulati,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: ALIGN-FL 是一种新型联邦学习方法，通过选择性共享生成组件（而非完整模型参数）来应对高度非独立同分布（Non-IID）数据下的分布式学习挑战，结合差分隐私SGD与Lipschitz正则化VAE，在保护隐私的同时提升跨域异构场景下的模型效用。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中高度不均衡、跨域、含异常值的Non-IID数据分布下模型性能下降与隐私泄露风险并存的问题。

Method: 提出ALIGN-FL框架：客户端仅上传生成能力（如VAE解码器），服务器用合成样本进行全局训练；引入DP-SGD自适应裁剪和Lipschitz正则化VAE解码器，并采用支持异构客户端的状态化架构。

Result: 在MNIST和Fashion-MNIST上验证了该方法对跨域异常值具有鲁棒性，两种隐私机制能有效将敏感异常点映射为典型样本，同时在极端Non-IID场景下保持良好模型效用。

Conclusion: ALIGN-FL实现了隐私保护、生成能力迁移与异构兼容性的统一，为跨机构（cross-silo）联邦学习提供了可行的生成式解决方案。

Abstract: We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures

</details>


### [554] [KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers](https://arxiv.org/abs/2512.13336)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

TL;DR: 本文提出KD-PINN框架，通过KL散度连续蒸馏，将大容量教师模型的物理精度迁移到轻量学生模型，在多种PDE任务上保持高精度（RMSE增幅<0.64%），并实现4.8x–6.9x加速，CPU推理延迟低至5.3ms，进入超低延迟实时范畴。


<details>
  <summary>Details</summary>
Motivation: 为解决物理信息神经网络（PINN）计算开销大、难以实时部署的问题，需在保持物理准确性的同时显著降低模型复杂度与推理延迟。

Method: 提出知识蒸馏增强的PINN（KD-PINN），以KL散度为连续优化目标，将高容量教师PINN的知识迁移至紧凑型学生PINN，并在多类偏微分方程（PDE）上验证其泛化性。

Result: 学生模型在所有测试PDE上均保持教师模型的物理精度（平均RMSE增幅<0.64%），推理速度提升4.8x（Navier-Stokes）至6.9x（Burgers），CPU平均延迟仅5.3ms，且蒸馏过程展现出正则化效应。

Conclusion: KD-PINN是一种通用、高效的知识蒸馏框架，可系统性地构建兼具高物理保真度与超低延迟特性的轻量PINN求解器，推动神经PDE求解向实时应用落地。

Abstract: This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.

</details>


### [555] [FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs](https://arxiv.org/abs/2512.13337)
*Si Qi Goh,Yongsen Zheng,Ziyao Liu,Sami Hormi,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 本文提出了FROC框架，用于在大语言模型中实现风险优化的机器遗忘，通过概率约束来平衡遗忘充分性和效用保持性。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘技术缺乏有效机制来评估和控制遗忘不足或效用损失的风险，影响策略选择与用户信任。

Method: 提出基于共形推理的风险控制框架FROC，构建连续风险模型，定义并估计共形遗忘风险（CUR），并识别满足风险预算的超参数配置集。

Result: 实验表明FROC能生成稳定可解释的风险图景，并揭示遗忘配置、语义偏移与效用影响间的一致关系。

Conclusion: FROC将机器遗忘重构为可控、风险感知的过程，为大规模LLM部署中的遗忘行为管理提供了实用基础。

Abstract: Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the "right to be forgotten." To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.

</details>


### [556] [Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks](https://arxiv.org/abs/2512.13340)
*Henrik C. M. Frederiksen,Junya Shiraishi,Cedomir Stefanovic,Hei Victor Cheng,Shashi Raj Pandey*

Main category: cs.LG

TL;DR: 本文提出了一种事件驱动的通信框架，将持续学习（CL）与物联网（IoT）网络中的轻量级机器学习模型更新相结合，在资源受限条件下实现节能高效的故障检测。


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源受限，轻量级模型易受环境非平稳性和初始训练数据不足影响而精度下降；频繁模型更新又带来额外能耗问题。

Method: 设计事件驱动的通信框架，使IoT设备与边缘服务器协同更新轻量级ML模型，动态适配无线链路条件和能量预算。

Result: 在真实数据集上验证，相比周期采样和非自适应持续学习，所提方法在推理召回率上最高提升42.8%，且适用于严苛的能量与带宽约束场景。

Conclusion: 该框架在保障模型适应性的同时显著降低能耗，为资源受限IoT场景下的持续学习提供了高效可行的解决方案。

Abstract: The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.

</details>


### [557] [Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction](https://arxiv.org/abs/2512.13381)
*Changjun Zhou,Jintao Zheng,Leyou Yang,Pengfei Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为DPUL的新型服务器端联邦遗忘方法，通过深度遗忘所有有影响力的权重来防止隐私泄露，包含高权重参数回滚、低权重参数VAE重建消除和投影恢复三个组件，在准确率和时间成本上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法依赖服务器端知识蒸馏，仅移除目标客户端的更新，忽视其他客户端贡献中的隐私信息，易导致隐私泄露；同时面临高计算开销、复杂激励机制及客户端算力差异等问题。

Method: DPUL方法包含三部分：(i)基于客户端更新幅度筛选高权重参数并回滚；(ii)使用变分自编码器（VAE）重建并消除低权重参数；(iii)采用基于投影的技术恢复模型。

Result: 在四个数据集上的实验表明，DPUL相比现有最优基线方法，准确率提升1%-5%，时间成本最高降低12倍。

Conclusion: DPUL是一种高效且隐私安全的服务器端联邦遗忘方法，能深度消除各客户端影响，兼顾性能与隐私保护。

Abstract: Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.

</details>


### [558] [Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks](https://arxiv.org/abs/2512.13410)
*Vítor M. Hanriot,Luiz C. B. Torres,Antônio P. Braga*

Main category: cs.LG

TL;DR: 本文提出了一种基于Gabriel图（GG）的无需超参数和优化的二分类器Chipclass，并通过引入平滑激活函数和结构支持向量（SSV）中心神经元来提升分类性能；同时扩展为可训练的神经网络架构，并提出新的图正则化成员函数与更高效的GG重计算算法，实验表明其优于以往GG方法，且与树模型性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有基于Gabriel图的分类器存在分类边界不光滑、间隔概率低、计算开销大等问题，且缺乏对多类扩展和可学习架构的支持。

Method: 提出Chipclass——一种无需超参数和优化的GG基二分类器；引入SSV-centered神经元和更平滑的激活函数；将架构扩展为可反向传播训练（softmax+交叉熵）或解析求解线性系统；设计基于子图/距离的图正则化成员函数及更高效的GG重计算算法。

Result: 在Friedman检验下，所提方法显著优于先前GG基分类器，且统计上等价于主流树模型；提升了分类边界平滑性与间隔鲁棒性。

Conclusion: Gabriel图可有效支撑轻量、可解释、可扩展的分类框架；结合几何结构先验与神经网络训练范式，能在不牺牲效率前提下提升性能与灵活性。

Abstract: While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.

</details>


### [559] [XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders](https://arxiv.org/abs/2512.13442)
*Khawla Elhadri,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: XNNTab is a neural architecture that combines high predictive performance of neural networks with interpretability by learning non-linear feature representations and decomposing them into human-interpretable concepts using a sparse autoencoder.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between predictive performance and interpretability in tabular data applications, where neural networks are avoided due to their black-box nature.

Method: XNNTab learns highly non-linear feature representations and uses a sparse autoencoder (SAE) to decompose them into monosemantic features, which are then assigned human-interpretable concepts.

Result: XNNTab outperforms traditional interpretable models (e.g., decision trees, linear regression) and achieves performance comparable to non-interpretable neural networks.

Conclusion: XNNTab successfully bridges the gap between high performance and intrinsic interpretability for tabular data modeling.

Abstract: In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.

</details>


### [560] [SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy](https://arxiv.org/abs/2512.13458)
*Yici Liu,Qi Wei Oung,Hoi Leong Lee*

Main category: cs.LG

TL;DR: 本文提出了一种结合源域选择与对抗策略的跨被试EEG情绪识别方法，通过源选择网络（SS）和对抗策略网络（AS）缓解个体差异与负迁移问题，在SEED和SEED-IV数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨被试EEG情绪识别中因个体差异导致的负迁移问题，现有方法大多忽略该问题。

Method: 提出包含源选择网络（SS）和对抗策略网络（AS）的两模块框架：SS利用域标签逆向优化以破坏类可分性、放大域间差异；AS基于SS输出和预训练域判别器设计新损失，提升对抗训练中的域分类平衡性。

Result: 在SEED和SEED-IV两个标准EEG情绪数据集上取得优异性能，并开源代码。

Conclusion: 所提方法能有效学习域不变且情绪相关表征，为跨被试aBCI提供理论支持与实用解决方案。

Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.

</details>


### [561] [DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems](https://arxiv.org/abs/2512.13460)
*Chethana Prasad Kabgere,Shylaja S S*

Main category: cs.LG

TL;DR: 本文提出DP-EMAR框架，结合差分隐私与安全聚合，在多层联邦物联网（Fed-IoT）中自主检测并修复传输导致的模型权重畸变，兼顾鲁棒性、通信效率与严格隐私保障。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的物联网联邦学习中，不稳定的连接和对抗干扰会导致模型权重在传输中被静默篡改，影响收敛性，亟需一种既能修复畸变又不破坏隐私的机制。

Method: 提出DP-EMAR：基于误差建模的自主修复框架，先估计并自适应校正传输畸变，再添加差分隐私噪声；融合差分隐私（DP）与安全聚合（SA），区分真实传输错误与DP噪声。

Result: 在异构IoT传感器与图数据集上实验表明，DP-EMAR在通信畸变下保持收敛稳定性与近基准性能，并满足严格的(ε,δ)-DP保证。

Conclusion: DP-EMAR提升了隐私保护型联邦物联网学习的鲁棒性、通信效率与可信度，为资源受限边缘环境下的可靠FL提供了新范式。

Abstract: Federated Learning (FL) enables decentralized model training without sharing raw data, but model weight distortion remains a major challenge in resource constrained IoT networks. In multi tier Federated IoT (Fed-IoT) systems, unstable connectivity and adversarial interference can silently alter transmitted parameters, degrading convergence. We propose DP-EMAR, a differentially private, error model based autonomous repair framework that detects and reconstructs transmission induced distortions during FL aggregation. DP-EMAR estimates corruption patterns and applies adaptive correction before privacy noise is added, enabling reliable in network repair without violating confidentiality. By integrating Differential Privacy (DP) with Secure Aggregation (SA), the framework distinguishes DP noise from genuine transmission errors. Experiments on heterogeneous IoT sensor and graph datasets show that DP-EMAR preserves convergence stability and maintains near baseline performance under communication corruption while ensuring strict (epsilon, delta)-DP guarantees. The framework enhances robustness, communication efficiency, and trust in privacy preserving Federated IoT learning.

</details>


### [562] [Element-wise Modulation of Random Matrices for Efficient Neural Layers](https://arxiv.org/abs/2512.13480)
*Maksymilian Szorc*

Main category: cs.LG

TL;DR: 本文提出了一种新型的Parametrized Random Projection (PRP)层，通过固定随机矩阵与轻量级可学习参数结合，显著减少全连接层的参数量，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 全连接层是深度神经网络中内存和计算开销的主要来源，现有压缩方法常带来工程复杂性或性能下降。

Method: 提出PRP层，利用固定随机矩阵进行特征混合，再通过轻量级、逐元素可学习参数进行自适应调节，实现参数解耦。

Result: PRP层将可训练参数降至线性规模，在多个基准测试中保持可靠准确率，并具备良好的计算效率和稳定性。

Conclusion: PRP层是一种稳定、高效、适用于资源受限场景的全连接层替代方案，为模型架构扩展与部署提供了新思路。

Abstract: Fully connected layers are a primary source of memory and computational overhead in deep neural networks due to their dense, often redundant parameterization. While various compression techniques exist, they frequently introduce complex engineering trade-offs or degrade model performance. We propose the Parametrized Random Projection (PRP) layer, a novel approach that decouples feature mixing from adaptation by utilizing a fixed random matrix modulated by lightweight, learnable element-wise parameters. This architecture drastically reduces the trainable parameter count to a linear scale while retaining reliable accuracy across various benchmarks. The design serves as a stable, computationally efficient solution for architectural scaling and deployment in resource-limited settings.

</details>


### [563] [On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing](https://arxiv.org/abs/2512.13497)
*Haoyu Ren,Kay Koehle,Kirill Dorofeev,Darko Anicic*

Main category: cs.LG

TL;DR: 本文提出了一种面向动态制造场景的端侧持续学习无监督视觉异常检测方法，通过轻量特征提取与增量式k-center核心集更新，在资源受限边缘设备上实现快速、低内存、自适应的异常检测与定位。


<details>
  <summary>Details</summary>
Motivation: 现代制造业中，小批量、按需生产导致产品频繁更换，而边缘设备资源有限、标注数据稀缺，现有VAD方法难以满足实时更新与部署需求。

Method: 在PatchCore基础上扩展为端侧持续学习框架，采用轻量特征提取器和基于k-center选择的增量式coreset更新机制，支持在线学习与无监督异常定位。

Result: 在工业测试平台上相比基线提升12% AUROC，内存占用降低80%，训练速度优于批量重训练。

Conclusion: 所提方法在精度、资源效率与适应性三方面均满足动态智能制造业的实际需求，验证了端侧持续学习VAD的可行性与有效性。

Abstract: In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.

</details>


### [564] [Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource](https://arxiv.org/abs/2512.13506)
*Sofiya Zaichyk*

Main category: cs.LG

TL;DR: 本文提出了一个名为可重复性预算 $C_T$ 的新统计原语，用于量化系统在分布漂移和反馈影响下的统计可重复性能力，并基于此推导出漂移-反馈泛化界，证明了其最小最大最优性。


<details>
  <summary>Details</summary>
Motivation: 统计学习在分布漂移下的泛化性能尚未被充分刻画，尤其当每次观测都改变数据生成机制时，传统泛化界可能失效。

Method: 定义可重复性预算 $C_T$ 为耦合学习器-环境演化过程的Fisher-Rao路径总长度，以此刻画分布运动总量，并据此推导漂移-反馈泛化界并证明其最小最大下界。

Result: 得到了阶为 $O(T^{-1/2} + C_T/T)$ 的泛化上界与匹配的最小最大下界，确立了由平均Fisher-Rao漂移率 $C_T/T$ 决定的‘可重复性速度极限’。

Conclusion: 可重复性预算 $C_T$ 是统一刻画外源漂移、自适应数据分析和执行性预测等场景中分布运动的内在度量，揭示了泛化误差的根本限制。

Abstract: Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.

</details>


### [565] [Async Control: Stress-testing Asynchronous Control Measures for LLM Agents](https://arxiv.org/abs/2512.13526)
*Asa Cooper Stickland,Jan Michelfeit,Arathi Mani,Charlie Griffin,Ollie Matthews,Tomek Korbak,Rogan Inglis,Oliver Makins,Alan Cooney*

Main category: cs.LG

TL;DR: 本文研究了针对基于大语言模型（LLM）的软件工程智能体的异步监控方法，以防范其因对齐失败而恶意破坏敏感代码库的风险；通过红蓝对抗框架，在5个模拟高风险开发环境中训练出一个高性能监控器（6%漏报率@1%误报率），并提出一种基于漏报率的部署风险外推模型及适用性分析。


<details>
  <summary>Details</summary>
Motivation: LLM软件工程智能体在实际开发中常接触敏感数据或安全关键代码，若存在对齐问题，可能被恶意利用进行 sabotage；需一种低开销、高实效的监控机制。

Method: 构建红蓝对抗游戏：蓝队设计异步监控器，红队构造 sabotaging 智能体；在5个真实感强的软件工程仿真环境中开展评估；采用ensemble监控器，并基于其在预留测试环境上的false negative rate进行部署风险建模与敏感性分析。

Result: 所提ensemble监控器在预留测试环境中达到6% false negative rate（FNR）@1% false positive rate（FPR）；提出一种可量化的部署风险外推模型，并明确了其失效边界。

Conclusion: 异步监控是一种可行且实用的LLM智能体安全控制手段；监控性能（尤其是FNR）可作为 sabotage 风险的上界估计依据，但其外推有效性依赖于环境相似性和攻击模式覆盖度。

Abstract: LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.

</details>


### [566] [Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability](https://arxiv.org/abs/2512.13568)
*Leonard Bereska,Zoe Tzifa-Kratira,Reza Samavi,Efstratios Gavves*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的框架，通过香农熵量化神经网络表征中的'有效特征数'，从而度量激活空间中的叠加（superposition）现象，并揭示其与压缩、干扰、对抗鲁棒性及任务复杂度之间的关系。


<details>
  <summary>Details</summary>
Motivation: 神经网络通过叠加（多个特征共享同一组神经元方向）实现高性能，但这种机制损害可解释性，且目前缺乏对叠加进行原则性度量的方法。

Method: 基于信息论，将香农熵应用于稀疏自编码器的激活，定义‘有效特征数’为实现无干扰编码所需的最少神经元数，等价于网络通过叠加模拟的‘虚拟神经元’数量。

Result: 该指标在玩具模型中与真实值强相关，在算法任务中检测到最小叠加，发现dropout会系统性降低叠加，层间模式与Pythia-70M的本征维数研究一致，并能捕捉grokking过程中的特征固化；对抗训练可在提升鲁棒性的同时增加有效特征数，其效应取决于任务复杂度与网络容量（丰裕/稀缺机制）。

Conclusion: 将叠加定义为有损压缩，使我们能在计算约束下原则性地衡量神经网络如何组织信息，并首次建立起叠加与对抗鲁棒性的定量联系。

Abstract: Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many "virtual neurons" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.

</details>


### [567] [DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication](https://arxiv.org/abs/2512.13583)
*Zehan Zhu,Heng Zhao,Yan Huang,Joey Tianyi Zhou,Shouling Ji,Jinming Xu*

Main category: cs.LG

TL;DR: 本文提出了一种面向有向图的差分隐私随机梯度Push算法（DP-CSGP），在保证严格差分隐私和通信压缩的同时，实现了与精确通信方法相当的模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化学习方法难以同时兼顾差分隐私保障、高效通信和高模型效用，尤其在有向图拓扑下。

Method: 提出DP-CSGP算法，结合差分隐私机制、随机梯度Push框架与梯度压缩技术，适用于有向图上的非凸光滑目标函数优化。

Result: 理论证明该算法在(ε,δ)-DP下达到O(√(d log(1/δ))/(√n J ε))的紧致效用界；实验表明其在相同隐私预算下，通信成本显著降低，模型精度与精确通信方法相当。

Conclusion: DP-CSGP成功在有向图去中心化学习中协同实现隐私保护、通信效率与模型性能三者的平衡。

Abstract: In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}δ \right)}/(\sqrt{n}Jε) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(ε, δ\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.

</details>


### [568] [Image Diffusion Preview with Consistency Solver](https://arxiv.org/abs/2512.13592)
*Fu-Yun Wang,Hao Zhou,Liangzhe Yuan,Sanghyun Woo,Boqing Gong,Bohyung Han,Ming-Hsuan Yang,Han Zhang,Yukun Zhu,Ting Liu,Long Zhao*

Main category: cs.LG

TL;DR: 本文提出Diffusion Preview范式，通过快速低步数采样生成预览图供用户评估，仅在满意时再进行全步长精修；为此设计了基于线性多步法、经强化学习优化的轻量可训练高阶求解器ConsistencySolver，在保证预览质量与最终结果一致性的前提下显著加速推理。


<details>
  <summary>Details</summary>
Motivation: 图像扩散模型推理速度慢，严重影响交互体验；现有加速方法难以兼顾预览质量与预览-精修结果的一致性。

Method: 提出ConsistencySolver，源自广义线性多步法，是轻量、可训练的高阶求解器，通过强化学习优化；结合Diffusion Preview的预览-精修两阶段工作流。

Result: 在低步数下显著提升生成质量与一致性；FID指标媲美Multistep DPM-Solver但步数减少47%；优于蒸馏基线；用户研究显示交互时间减少近50%，质量不变。

Conclusion: ConsistencySolver为扩散模型提供了高效、一致、高质量的预览机制，适用于交互式生成场景。

Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.

</details>


### [569] [Scalable Formal Verification via Autoencoder Latent Space Abstraction](https://arxiv.org/abs/2512.13593)
*Robert Reed,Morteza Lahijanian,Luca Laurenti*

Main category: cs.LG

TL;DR: 本文提出了一种基于凸自编码器和核方法的降维与有限抽象验证框架，确保在潜空间中学习的动力学能严格包含原系统行为，从而在不牺牲严谨性的前提下显著提升高维系统（如26维）的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 有限抽象方法因状态空间离散化导致维度灾难，难以扩展到高维系统；而现有基于学习的降维方法缺乏形式化正确性保证。

Method: 采用凸自编码器进行降维，在潜空间中用核方法学习系统动力学，并基于该学习模型构建有限抽象，同时提供从潜空间抽象到原系统行为的严格包含性保证。

Result: 验证结果可在潜空间完成，并可靠映射回原系统；在多个系统（包括一个26维神经网络控制系统）上验证了方法的有效性与可扩展性提升。

Conclusion: 该方法在保持形式化严谨性的同时，显著提升了高维非线性系统验证的可扩展性，为学习增强的形式化验证提供了新范式。

Abstract: Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.

</details>


### [570] [LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification](https://arxiv.org/abs/2512.13617)
*Ankit Sharma,Sayan Roy Gupta*

Main category: cs.LG

TL;DR: 本文提出了LightTopoGAT，一种轻量级图注意力网络，通过引入节点度和局部聚类系数等拓扑特征增强节点表示，在保持参数高效的同时提升图分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在图分类任务中计算开销大，且难以有效捕获全局图性质。

Method: LightTopoGAT通过拓扑增强（节点度与局部聚类系数）改进节点特征，并采用简化的注意力机制实现参数高效性。

Result: 在MUTAG、ENZYMES和PROTEINS数据集上，LightTopoGAT优于GCN、GraphSAGE和标准GAT；在MUTAG上准确率提升6.6%，在PROTEINS上提升2.2%。

Conclusion: 引入拓扑特征是一种简单而有效的策略，可在不增加模型复杂度的前提下显著提升图神经网络性能。

Abstract: Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.

</details>


### [571] [StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion](https://arxiv.org/abs/2512.13632)
*Guransh Singh,Md Shah Fahad*

Main category: cs.LG

TL;DR: 本文提出了StutterFuse，首个用于多标签口吃检测的检索增强分类器（RAC），通过结合Conformer编码器与临床示例记忆库，并提出SetCon损失和门控专家融合策略来解决模态坍塌问题，在SEP-28k数据集上达到0.65加权F1，并展现零样本跨语言泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有参数化模型难以区分重叠发生的复杂口吃类型（如'阻塞'与'延长'共现），因训练数据中此类组合稀缺；检索增强生成（RAG）在NLP中成功，但在病理性语音处理中尚未探索。

Method: 提出StutterFuse：基于Conformer编码器的检索增强分类器，引入非参数化临床示例记忆库；设计SetCon（Jaccard加权度量学习目标）缓解模态坍塌；采用门控混合专家融合策略动态整合声学特征与检索上下文。

Result: 在SEP-28k数据集上加权F1达0.65，显著优于强基线，并展现出优异的零样本跨语言泛化性能。

Conclusion: 检索增强范式可有效提升多标签口吃检测性能，StutterFuse为病理性语音建模提供了新思路，其解决模态坍塌的方法对其他低资源、多标签医疗语音任务具有推广价值。

Abstract: Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve "Modality Collapse", an "Echo Chamber" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.

</details>


### [572] [From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves](https://arxiv.org/abs/2512.13641)
*Gabriel Vitorino de Andrade,Saulo Roberto dos Santos,Itallo Patrick Castro Alves da Silva,Emanuel Adler Medeiros Pereira,Erick de Andrade Barboza*

Main category: cs.LG

TL;DR: 本文提出了一种评估卷积神经网络在芒果叶病害诊断中面对图像退化（如噪声、模糊、天气变化）鲁棒性的方法，构建了MangoLeafDB-C数据集，并在五种模型上进行基准测试，发现轻量专用模型LCNN在多种退化下表现更优、更稳健。


<details>
  <summary>Details</summary>
Motivation: 尽管芒果在全球农业中至关重要，但目前尚缺乏对其叶片病害诊断模型鲁棒性的系统性研究；而实际田间环境常存在图像退化，亟需评估模型在非理想条件下的可靠性。

Method: 构建含19类人工退化、5个严重等级的MangoLeafDB-C数据集；对ResNet-50、ResNet-101、VGG-16、Xception和专用轻量模型LCNN进行鲁棒性基准测试；采用F1分数、腐败误差（CE）和相对平均腐败误差（relative mCE）作为评估指标。

Result: LCNN在Defocus Blur、Motion Blur等真实常见退化下F1得分最高、mCE最低；而ResNet-101等现代大模型在退化条件下性能显著下降，尽管其在干净图像上精度高。

Conclusion: 轻量且任务专用的模型（如LCNN）在边缘设备部署中更具鲁棒性与效率优势；农业AI系统开发应将鲁棒性评估纳入标准流程，尤其在技术资源受限地区。

Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.

</details>


### [573] [A Scientific Reasoning Model for Organic Synthesis Procedure Generation](https://arxiv.org/abs/2512.13668)
*Guoqing Liu,Junren Li,Zihan Zhao,Eray Inanc,Krzysztof Maziarz,Jose Garrido Torres,Victor Garcia Satorras,Shoko Ueda,Christopher M. Bishop,Marwin Segler*

Main category: cs.LG

TL;DR: 本文提出了QFANG模型，一种能基于反应方程式生成结构化、可执行实验步骤的科学推理语言模型，通过化学引导推理框架和强化学习从可验证奖励中优化，显著提升了合成步骤的程序生成准确性，推动了计算合成规划与自动化实验的衔接。


<details>
  <summary>Details</summary>
Motivation: 解决计算机辅助合成规划（CASP）中计算路线设计与实际实验室执行之间的鸿沟，尤其是各合成步骤实验程序的准确预测。

Method: 构建高质量专利反应数据集（90.6万条），提出化学引导推理（CGR）框架生成化学知识驱动的思维链数据，结合监督微调与基于可验证奖励的强化学习（RLVR）训练QFANG模型。

Result: QFANG在NLP相似性指标和化学感知LLM裁判评估中均优于通用推理模型和最近邻检索基线，且具备跨反应类别泛化能力及对实验条件与用户约束的适应性。

Conclusion: QFANG实现了高精度实验程序生成，是连接计算合成规划与全自动实验室合成的关键进展。

Abstract: Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.

</details>


### [574] [Directional Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.13672)
*Kunhee Kim,NaHyeon Park,Kibeom Hong,Hyunjung Shim*

Main category: cs.LG

TL;DR: 本文提出Directional Textual Inversion（DTI），通过固定嵌入向量模长、仅优化其方向，解决Textual Inversion中因嵌入范数膨胀导致的复杂提示失效问题；DTI提升文本保真度与概念插值能力，并提供理论与实证支持。


<details>
  <summary>Details</summary>
Motivation: Textual Inversion（TI）在复杂提示下常失效，作者发现主因是嵌入范数膨胀，导致预归一化Transformer中提示条件作用退化。

Method: 提出DTI方法：将嵌入向量约束在单位超球面上，仅优化其方向；采用黎曼随机梯度下降（Riemannian SGD）；将方向学习建模为带von Mises-Fisher先验的最大后验估计（MAP），引入简洁有效的常方向先验梯度。

Result: DTI在多个个性化任务上优于TI及其变体，提升文本保真度并保持主体相似性；首次实现学习概念间平滑、语义连贯的球面线性插值（slerp）。

Conclusion: 仅优化嵌入方向是一种鲁棒且可扩展的提示忠实个性化路径，方向编码语义、模长干扰上下文建模，应被优先控制。

Abstract: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.

</details>
