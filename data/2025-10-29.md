<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 71]
- [cs.CL](#cs.CL) [Total: 85]
- [cs.RO](#cs.RO) [Total: 34]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.LG](#cs.LG) [Total: 118]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TRELLISWorld: Training-Free World Generation from Object Generators](https://arxiv.org/abs/2510.23880)
*Hanke Chen,Yuan Liu,Minchen Li*

Main category: cs.CV

TL;DR: 提出一种无需训练的3D场景生成方法，通过将文本到3D对象扩散模型用作模块化瓦片生成器，实现可扩展、连贯且支持360度视图的文本驱动场景合成。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法受限于单物体生成、需要特定领域训练或不支持完整360度视角，难以满足虚拟原型、AR/VR等应用需求。

Method: 将场景生成重构为多瓦片去噪问题，独立生成重叠的3D区域，并通过加权平均无缝融合，利用通用文本到3D对象扩散模型作为模块化生成单元。

Result: 实现了大规模、语义连贯的360度可视化3D场景生成，支持多样化布局、高效生成和灵活编辑，无需场景级数据集或重新训练。

Conclusion: 该方法提供了一种简单而强大的通用语言驱动3D场景构建框架，无需训练且具备良好泛化能力。

Abstract: Text-driven 3D scene generation holds promise for a wide range of
applications, from virtual prototyping to AR/VR and simulation. However,
existing methods are often constrained to single-object generation, require
domain-specific training, or lack support for full 360-degree viewability. In
this work, we present a training-free approach to 3D scene synthesis by
repurposing general-purpose text-to-3D object diffusion models as modular tile
generators. We reformulate scene generation as a multi-tile denoising problem,
where overlapping 3D regions are independently generated and seamlessly blended
via weighted averaging. This enables scalable synthesis of large, coherent
scenes while preserving local semantic control. Our method eliminates the need
for scene-level datasets or retraining, relies on minimal heuristics, and
inherits the generalization capabilities of object-level priors. We demonstrate
that our approach supports diverse scene layouts, efficient generation, and
flexible editing, establishing a simple yet powerful foundation for
general-purpose, language-driven 3D scene construction.

</details>


### [2] [Fast and accurate neural reflectance transformation imaging through knowledge distillation](https://arxiv.org/abs/2510.24486)
*Tinsae G. Dulecha,Leonardo Righetto,Ruggero Pintus,Enrico Gobbetti,Andrea Giachetti*

Main category: cs.CV

TL;DR: 提出了一种基于知识蒸馏的NeuralRTI方法（DisK-NeuralRTI），以降低神经网络在高分辨率反射变换成像中的计算成本，同时保持高质量和紧凑存储。


<details>
  <summary>Details</summary>
Motivation: 传统RTI方法难以准确捕捉复杂反射场，而NeuralRTI虽质量高但计算开销大，尤其在大图像和有限硬件上难以实时渲染，因此需要一种更高效的解决方案。

Method: 采用知识蒸馏策略，将大型NeuralRTI模型（教师网络）的知识迁移到轻量级网络（学生网络），从而在保持重建质量的同时显著降低推理计算成本。

Result: DisK-NeuralRTI在显著减少网络参数和计算量的同时，保留了NeuralRTI的高质量重建能力，实现了更快的渲染速度，适用于高分辨率图像和资源受限设备。

Conclusion: 所提出的DisK-NeuralRTI通过知识蒸馏有效平衡了性能与效率，为实际应用中的高效RTI建模提供了可行方案。

Abstract: Reflectance Transformation Imaging (RTI) is very popular for its ability to
visually analyze surfaces by enhancing surface details through interactive
relighting, starting from only a few tens of photographs taken with a fixed
camera and variable illumination. Traditional methods like Polynomial Texture
Maps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle
to accurately capture complex reflectance fields using few per-pixel
coefficients and fixed bases, leading to artifacts, especially in highly
reflective or shadowed areas. The NeuralRTI approach, which exploits a neural
autoencoder to learn a compact function that better approximates the local
reflectance as a function of light directions, has been shown to produce
superior quality at comparable storage cost. However, as it performs
interactive relighting with custom decoder networks with many parameters, the
rendering step is computationally expensive and not feasible at full resolution
for large images on limited hardware. Earlier attempts to reduce costs by
directly training smaller networks have failed to produce valid results. For
this reason, we propose to reduce its computational cost through a novel
solution based on Knowledge Distillation (DisK-NeuralRTI). ...

</details>


### [3] [Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices](https://arxiv.org/abs/2510.23775)
*Aryan Mathur,Asaduddin Ahmed,Pushti Amit Vasoya,Simeon Kandan Sonar,Yasir Z,Madesh Kuppusamy*

Main category: cs.CV

TL;DR: 提出了一种结合轻量级卷积分类器和视觉-语言模型的可解释图像真实性检测系统，在增强的CiFAKE数据集上达到96.5%准确率，具备快速推理能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像的真实性提高，验证视觉内容的真实性变得愈发困难，亟需可解释且高效的真实性和伪造检测方法。

Method: 采用轻量级卷积分类器（Faster-Than-Lies）与视觉-语言模型（Qwen2-VL-7B）结合，利用自编码器重建误差图生成伪影定位热图，并将70种视觉伪影归为八类语义组，实现分类、定位与文本解释。

Result: 在含对抗扰动的扩展CiFAKE数据集上达到96.5%准确率，推理时间仅175ms（8核CPU），生成可解释的伪影定位与文本描述。

Conclusion: 结合视觉与语言推理的可解释真实性检测在低分辨率图像中可行，具有在法证、工业检测和社交媒体审核等领域的跨应用潜力。

Abstract: The increasing realism of AI-generated imagery poses challenges for verifying
visual authenticity. We present an explainable image authenticity detection
system that combines a lightweight convolutional classifier
("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify,
localize, and explain artifacts in 32x32 images. Our model achieves 96.5%
accuracy on the extended CiFAKE dataset augmented with adversarial
perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling
deployment on local or edge devices. Using autoencoder-based reconstruction
error maps, we generate artifact localization heatmaps, which enhance
interpretability for both humans and the VLM. We further categorize 70 visual
artifact types into eight semantic groups and demonstrate explainable text
generation for each detected anomaly. This work highlights the feasibility of
combining visual and linguistic reasoning for interpretable authenticity
detection in low-resolution imagery and outlines potential cross-domain
applications in forensics, industrial inspection, and social media moderation.

</details>


### [4] [CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting](https://arxiv.org/abs/2510.23785)
*Md Tanvir Hossain,Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的类无关目标计数框架CountFormer，通过引入DINOv2基础模型和位置编码融合，在复杂结构场景中实现了更准确的计数性能。


<details>
  <summary>Details</summary>
Motivation: 现有计数模型在处理复杂形状、内部对称或重叠物体时容易出错，难以模仿人类基于视觉重复性和结构关系的计数能力。

Method: 基于CounTR架构，将视觉编码器替换为自监督基础模型DINOv2，并引入位置编码融合以保持几何关系，最后通过轻量卷积解码器生成密度图进行计数。

Result: 在FSC-147数据集上达到与当前最先进方法相当的性能，并在结构复杂或密集排列的场景中表现出更高的准确性。

Conclusion: 结合DINOv2等基础模型有助于提升计数系统对结构信息的感知能力，推动实现真正通用且无需示例的类无关计数方法。

Abstract: Humans can effortlessly count diverse objects by perceiving visual repetition
and structural relationships rather than relying on class identity. However,
most existing counting models fail to replicate this ability; they often
miscount when objects exhibit complex shapes, internal symmetry, or overlapping
components. In this work, we introduce CountFormer, a transformer-based
framework that learns to recognize repetition and structural coherence for
class-agnostic object counting. Built upon the CounTR architecture, our model
replaces its visual encoder with the self-supervised foundation model DINOv2,
which produces richer and spatially consistent feature representations. We
further incorporate positional embedding fusion to preserve geometric
relationships before decoding these features into density maps through a
lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model
achieves performance comparable to current state-of-the-art methods while
demonstrating superior accuracy on structurally intricate or densely packed
scenes. Our findings indicate that integrating foundation models such as DINOv2
enables counting systems to approach human-like structural perception,
advancing toward a truly general and exemplar-free counting paradigm.

</details>


### [5] [A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras](https://arxiv.org/abs/2510.23798)
*Gauthier Grimmer,Romain Wenger,Clément Flint,Germain Forestier,Gilles Rixhon,Valentin Chardon*

Main category: cs.CV

TL;DR: 提出一种基于固定摄像头和深度学习的河流漂浮垃圾监测新方法，结合投影几何与回归校正实现物体尺寸估计。


<details>
  <summary>Details</summary>
Motivation: 河流中的人造漂浮垃圾对生态环境、水质和人类活动造成负面影响，亟需有效的监测手段。

Method: 利用固定原位摄像头采集数据，采用深度学习模型连续量化和监测漂浮垃圾，并通过投影几何模型从2D图像估算实际物体尺寸。

Result: 确定了在复杂环境条件下精度和推理速度最优的深度学习模型，验证了数据构成协议（如负样本和时间泄漏）的重要性，实现了可行的物体度量估计。

Conclusion: 该方法为城市水体环境开发低成本、自动化的垃圾监测系统提供了可行路径。

Abstract: The proliferation of floating anthropogenic debris in rivers has emerged as a
pressing environmental concern, exerting a detrimental influence on
biodiversity, water quality, and human activities such as navigation and
recreation. The present study proposes a novel methodological framework for the
monitoring the aforementioned waste, utilising fixed, in-situ cameras. This
study provides two key contributions: (i) the continuous quantification and
monitoring of floating debris using deep learning and (ii) the identification
of the most suitable deep learning model in terms of accuracy and inference
speed under complex environmental conditions. These models are tested in a
range of environmental conditions and learning configurations, including
experiments on biases related to data leakage. Furthermore, a geometric model
is implemented to estimate the actual size of detected objects from a 2D image.
This model takes advantage of both intrinsic and extrinsic characteristics of
the camera. The findings of this study underscore the significance of the
dataset constitution protocol, particularly with respect to the integration of
negative images and the consideration of temporal leakage. In conclusion, the
feasibility of metric object estimation using projective geometry coupled with
regression corrections is demonstrated. This approach paves the way for the
development of robust, low-cost, automated monitoring systems for urban aquatic
environments.

</details>


### [6] [RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features](https://arxiv.org/abs/2510.23816)
*Forouzan Fallah,Wenwen Li,Chia-Yu Hsu,Hyunho Lee,Yezhou Yang*

Main category: cs.CV

TL;DR: 提出RareFlow，一种面向分布外（OOD）鲁棒性的物理感知超分辨率框架，通过双条件架构和物理一致性损失提升遥感图像超分辨率的几何保真与语义准确性。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率方法在处理罕见地貌或不同传感器数据等分布外条件下，常产生视觉合理但物理不准确的结果，缺乏对物理一致性和不确定性建模的能力。

Method: 采用双条件架构：Gated ControlNet保持低分辨率输入的几何细节，文本提示提供复杂特征的语义引导；引入多维度损失函数确保光谱和辐射一致性；通过随机前向传播量化预测不确定性，识别未知输入。

Result: 在多传感器卫星图像新基准上验证，地理物理专家盲评认为其输出接近真实图像质量，显著优于现有最先进方法；FID指标下降近40%，感知质量明显提升。

Conclusion: RareFlow为数据稀缺的科学领域提供了高保真合成的鲁棒框架，并提出了在严重域偏移下可控生成的新范式。

Abstract: Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow's core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model's
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.

</details>


### [7] [Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2510.23894)
*Jinxin Zhou,Jiachen Jiang,Zhihui Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的框架LHT-CLIP，通过在层、注意力头和token级别上系统性利用CLIP模型的视觉区分能力，解决了其在语义分割任务中因图像级预训练目标与像素级理解需求不匹配而导致的问题。


<details>
  <summary>Details</summary>
Motivation: 由于CLIP模型在图像级别进行预训练，难以直接适用于需要像素级理解的语义分割任务，且先前方法继承了深层网络的全局对齐偏差，导致分割性能受限。因此，需要一种能恢复CLIP视觉区分能力的方法。

Method: 通过对CLIP模型在层、头和token三个层面的分析，提出了三种互补技术：语义-空间重加权、选择性头增强和异常token替换，在不增加训练、额外模型或调参的情况下提升分割性能。

Result: 在8个主流语义分割基准上的实验表明，LHT-CLIP在多种场景下均达到最先进的性能。

Conclusion: LHT-CLIP有效恢复了CLIP模型的视觉区分能力，实现了无需训练的高性能语义分割，具有良好的实用性和广泛适用性。

Abstract: Extending CLIP models to semantic segmentation remains challenging due to the
misalignment between their image-level pre-training objectives and the
pixel-level visual understanding required for dense prediction. While prior
efforts have achieved encouraging results by reorganizing the final layer and
features, they often inherit the global alignment bias of preceding layers,
leading to suboptimal segmentation performance. In this work, we propose
LHT-CLIP, a novel training-free framework that systematically exploits the
visual discriminability of CLIP across layer, head, and token levels. Through
comprehensive analysis, we reveal three key insights: (i) the final layers
primarily strengthen image-text alignment with sacrifice of visual
discriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),
partly due to the emergence of anomalous tokens; (ii) a subset of attention
heads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual
discriminability across datasets; (iii) abnormal tokens display sparse and
consistent activation pattern compared to normal tokens. Based on these
findings, we propose three complementary techniques: semantic-spatial
reweighting, selective head enhancement, and abnormal token replacement to
effectively restore visual discriminability and improve segmentation
performance without any additional training, auxiliary pre-trained networks, or
extensive hyperparameter tuning. Extensive experiments on 8 common semantic
segmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art
performance across diverse scenarios, highlighting its effectiveness and
practicality for real-world deployment.

</details>


### [8] [DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning](https://arxiv.org/abs/2510.23907)
*Eddison Pham,Prisha Priyadarshini,Adrian Maliackel,Kanishk Bandi,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: 本文提出了DynaStride，一种无需手动场景分割即可生成连贯场景级字幕的管道，通过自适应帧采样、多模态窗口化和动态步幅选择算法，结合视觉语义与时间推理，显著提升了教学视频字幕的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的教学视频场景级字幕生成方法常因忽略时间结构而导致字幕不连贯、质量差，影响学习效果，因此需要一种能自动捕捉视觉和时间特征的自动化方法。

Method: DynaStride利用YouCookII数据集的场景注释，采用自适应帧采样和多模态窗口化捕获场景内关键转换，通过多模态思维链生成多个动作-对象对，并使用动态步幅窗口选择算法进行优化融合，最终生成包含视觉语义与时间推理的完整场景字幕。

Result: 在BLEU、METEOR、BERTScore和CLIPScore等指标上均优于VLLaMA3和GPT-4o等强基线模型，定性分析显示生成的字幕更具时间连贯性和信息量。

Conclusion: DynaStride有效提升了教学视频中场景级字幕的连贯性与质量，为AI驱动的教学内容生成提供了有前景的方向。

Abstract: Scene-level captioning in instructional videos can enhance learning by
requiring an understanding of both visual cues and temporal structure. By
aligning visual cues with textual guidance, this understanding supports
procedural learning and multimodal reasoning, providing a richer context for
skill acquisition. However, captions that fail to capture this structure may
lack coherence and quality, which can create confusion and undermine the
video's educational intent. To address this gap, we introduce DynaStride, a
pipeline to generate coherent, scene-level captions without requiring manual
scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride
performs adaptive frame sampling and multimodal windowing to capture key
transitions within each scene. It then employs a multimodal chain-of-thought
process to produce multiple action-object pairs, which are refined and fused
using a dynamic stride window selection algorithm that adaptively balances
temporal context and redundancy. The final scene-level caption integrates
visual semantics and temporal reasoning in a single instructional caption.
Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,
demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and
semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses
further show that DynaStride produces captions that are more temporally
coherent and informative, suggesting a promising direction for improving
AI-powered instructional content generation.

</details>


### [9] [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](https://arxiv.org/abs/2510.23929)
*Emily Kim,Julieta Martinez,Timur Bagautdinov,Jessica Hodgins*

Main category: cs.CV

TL;DR: 提出TurboPortrait3D，一种低延迟的人像新视角合成方法，结合图像到3D模型与扩散模型优势，通过单步扩散优化多视角一致性渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像到3D人像生成方法存在视觉伪影、细节缺失和身份保持不佳问题；而图像扩散模型虽能生成高质量图像但缺乏3D感知能力，无法直接生成多视角一致结果。

Method: 输入单张正面人脸图像，先通过前馈图像到头像管线生成初始3D表示及带噪渲染图；再使用一个条件依赖输入图像的单步扩散模型，在多视角一致约束下对渲染图进行精细化修复；采用在大规模合成多视图数据上预训练并用高质量真实图像微调的训练策略。

Result: 在定性和定量评估上均优于当前最先进的人像新视角合成方法，同时保持低延迟和高效率。

Conclusion: TurboPortrait3D有效融合了3D感知能力和图像级细节生成优势，实现了高质量、多视角一致且低延迟的人像新视角合成。

Abstract: We introduce TurboPortrait3D: a method for low-latency novel-view synthesis
of human portraits. Our approach builds on the observation that existing
image-to-3D models for portrait generation, while capable of producing
renderable 3D representations, are prone to visual artifacts, often lack of
detail, and tend to fail at fully preserving the identity of the subject. On
the other hand, image diffusion models excel at generating high-quality images,
but besides being computationally expensive, are not grounded in 3D and thus
are not directly capable of producing multi-view consistent outputs. In this
work, we demonstrate that image-space diffusion models can be used to
significantly enhance the quality of existing image-to-avatar methods, while
maintaining 3D-awareness and running with low-latency. Our method takes a
single frontal image of a subject as input, and applies a feedforward
image-to-avatar generation pipeline to obtain an initial 3D representation and
corresponding noisy renders. These noisy renders are then fed to a single-step
diffusion model which is conditioned on input image(s), and is specifically
trained to refine the renders in a multi-view consistent way. Moreover, we
introduce a novel effective training strategy that includes pre-training on a
large corpus of synthetic multi-view data, followed by fine-tuning on
high-quality real images. We demonstrate that our approach both qualitatively
and quantitatively outperforms current state-of-the-art for portrait novel-view
synthesis, while being efficient in time.

</details>


### [10] [PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors](https://arxiv.org/abs/2510.23930)
*Xirui Jin,Renbiao Jin,Boying Li,Danping Zou,Wenxian Yu*

Main category: cs.CV

TL;DR: 本文提出了PlanarGS，一种基于3D高斯点阵的室内场景重建框架，通过引入语言提示的平面先验和几何先验监督，显著提升了在大范围低纹理区域中的三维重建精度和细节表现。


<details>
  <summary>Details</summary>
Motivation: 在以大面积低纹理区域为主的室内场景中，传统的3D高斯点阵方法因光度损失模糊而导致几何结构不准确，难以恢复高保真的三维表面。

Method: 提出PlanarGS框架，设计了语言提示平面先验（LP3）流程，利用预训练的视觉-语言分割模型，并结合跨视图融合与几何先验优化区域提议；在优化过程中引入平面先验监督和平面几何先验（深度与法线）监督项。

Result: 在标准室内数据集上的实验表明，PlanarGS能重建出更精确、更细致的3D表面，在多个指标上大幅优于现有最先进方法。

Conclusion: PlanarGS通过融合语义、平面和几何先验，有效解决了低纹理环境下3D高斯点阵的几何模糊问题，显著提升了室内场景的重建质量。

Abstract: Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io

</details>


### [11] [Adaptive Training of INRs via Pruning and Densification](https://arxiv.org/abs/2510.23943)
*Diana Aldana,João Paulo Lima,Daniel Csillag,Daniel Perazzo,Haoan Feng,Luiz Velho,Tiago Novello*

Main category: cs.CV

TL;DR: 本文提出了AIRe，一种自适应隐式神经表示方法，通过神经元剪枝和输入频率致密化来优化网络结构，在减小模型规模的同时保持或提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式神经表示方法在选择输入频率和网络架构时依赖启发式方法和大量超参数调优，且存在参数冗余问题，难以高效建模高频细节。

Method: 提出AIRe，采用自适应训练方案：首先通过识别贡献较小的神经元并应用定向权重衰减进行信息保留，随后进行结构化剪枝以减少冗余；接着在信号欠拟合的频谱区域增加输入频率，增强表示能力。

Result: 在图像和符号距离场（SDF）上的实验表明，AIRe能够在显著减小模型大小的同时，保持甚至提升重建质量。

Conclusion: AIRe通过动态调整网络结构和输入频率，有效平衡了模型复杂度与表示能力，为隐式神经表示提供了一种高效且自适应的解决方案。

Abstract: Encoding input coordinates with sinusoidal functions into multilayer
perceptrons (MLPs) has proven effective for implicit neural representations
(INRs) of low-dimensional signals, enabling the modeling of high-frequency
details. However, selecting appropriate input frequencies and architectures
while managing parameter redundancy remains an open challenge, often addressed
through heuristics and heavy hyperparameter optimization schemes. In this
paper, we introduce AIRe ($\textbf{A}$daptive $\textbf{I}$mplicit neural
$\textbf{Re}$presentation), an adaptive training scheme that refines the INR
architecture over the course of optimization. Our method uses a neuron pruning
mechanism to avoid redundancy and input frequency densification to improve
representation capacity, leading to an improved trade-off between network size
and reconstruction quality. For pruning, we first identify less-contributory
neurons and apply a targeted weight decay to transfer their information to the
remaining neurons, followed by structured pruning. Next, the densification
stage adds input frequencies to spectrum regions where the signal underfits,
expanding the representational basis. Through experiments on images and SDFs,
we show that AIRe reduces model size while preserving, or even improving,
reconstruction quality. Code and pretrained models will be released for public
use.

</details>


### [12] [Neural USD: An object-centric framework for iterative editing and control](https://arxiv.org/abs/2510.23956)
*Alejandro Escontrela,Shrinu Kushagra,Sjoerd van Steenkiste,Yulia Rubanova,Aleksander Holynski,Kelsey Allen,Kevin Murphy,Thomas Kipf*

Main category: cs.CV

TL;DR: 本文提出了“神经通用场景描述符”（Neural USD），一种受计算机图形学中USD标准启发的可控生成建模框架，通过结构化、层次化的方式表示场景和对象，实现对每个对象外观、几何形状和姿态的精确控制，并支持迭代编辑。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在进行图像编辑时（如更改物体颜色或背景）容易导致非预期的全局变化，缺乏对单个对象的精细、迭代控制，因此需要一种更结构化的控制机制。

Method: 借鉴计算机图形学中的通用场景描述符（USD）标准，提出Neural USD框架，以层次化结构表示场景；引入细粒度微调方法，确保外观、几何、姿态等控制信号相互解耦。

Result: 验证了多种框架设计选择，展示了Neural USD支持逐对象控制和迭代增量工作流的能力，显著提升了生成内容编辑的精确性和可控性。

Conclusion: Neural USD为可控生成建模提供了一种通用、灵活且可扩展的框架，有效解决了当前方法在对象级编辑中的局限性，推动了生成模型向更实用的交互式应用发展。

Abstract: Amazing progress has been made in controllable generative modeling,
especially over the last few years. However, some challenges remain. One of
them is precise and iterative object editing. In many of the current methods,
trying to edit the generated image (for example, changing the color of a
particular object in the scene or changing the background while keeping other
elements unchanged) by changing the conditioning signals often leads to
unintended global changes in the scene. In this work, we take the first steps
to address the above challenges. Taking inspiration from the Universal Scene
Descriptor (USD) standard developed in the computer graphics community, we
introduce the "Neural Universal Scene Descriptor" or Neural USD. In this
framework, we represent scenes and objects in a structured, hierarchical
manner. This accommodates diverse signals, minimizes model-specific
constraints, and enables per-object control over appearance, geometry, and
pose. We further apply a fine-tuning approach which ensures that the above
control signals are disentangled from one another. We evaluate several design
considerations for our framework, demonstrating how Neural USD enables
iterative and incremental workflows. More information at:
https://escontrela.me/neural_usd .

</details>


### [13] [SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability](https://arxiv.org/abs/2510.23960)
*Peiyang Xu,Minzhou Pan,Zhaorun Chen,Shuang Yang,Chaowei Xiao,Bo Li*

Main category: cs.CV

TL;DR: 本文提出SafeVision，一种结合人类推理能力的新型图像安全防护模型，能够动态适应安全策略变化，无需重新训练即可应对新兴威胁，并在新构建的数据集VisionHarm上显著优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 传统图像安全模型依赖预定义类别且缺乏语义推理，难以适应新威胁并提供可解释结果，亟需更灵活、透明的解决方案。

Method: 提出SafeVision，采用策略遵循的训练流程、定制损失函数和多样化的问答生成策略；构建高质量数据集VisionHarm（含VisionHarm-T和VisionHarm-C两个子集）用于评估。

Result: 在多个基准测试中达到SOTA性能，在VisionHarm-T上比GPT-4o高8.6%，在VisionHarm-C上高15.5%，同时推理速度超过其16倍。

Conclusion: SafeVision实现了可解释、可动态适应政策变化的图像内容安全防护，为应对不断演变的网络风险提供了高效、精准的新范式。

Abstract: With the rapid proliferation of digital media, the need for efficient and
transparent safeguards against unsafe content is more critical than ever.
Traditional image guardrail models, constrained by predefined categories, often
misclassify content due to their pure feature-based learning without semantic
reasoning. Moreover, these models struggle to adapt to emerging threats,
requiring costly retraining for new threats. To address these limitations, we
introduce SafeVision, a novel image guardrail that integrates human-like
reasoning to enhance adaptability and transparency. Our approach incorporates
an effective data collection and generation framework, a policy-following
training pipeline, and a customized loss function. We also propose a diverse QA
generation and training strategy to enhance learning effectiveness. SafeVision
dynamically aligns with evolving safety policies at inference time, eliminating
the need for retraining while ensuring precise risk assessments and
explanations. Recognizing the limitations of existing unsafe image benchmarks,
which either lack granularity or cover limited risks, we introduce VisionHarm,
a high-quality dataset comprising two subsets: VisionHarm Third-party
(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse
harmful categories. Through extensive experiments, we show that SafeVision
achieves state-of-the-art performance on different benchmarks. SafeVision
outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while
being over 16x faster. SafeVision sets a comprehensive, policy-following, and
explainable image guardrail with dynamic adaptation to emerging threats.

</details>


### [14] [Reasoning Visual Language Model for Chest X-Ray Analysis](https://arxiv.org/abs/2510.23968)
*Andriy Myronenko,Dong Yang,Baris Turkbey,Mariam Aboian,Sena Azamat,Esra Akcicek,Hongxu Yin,Pavlo Molchanov,Marc Edgar,Yufan He,Pengfei Guo,Yucheng Tang,Daguang Xu*

Main category: cs.CV

TL;DR: 提出一种结合链式思维（CoT）推理的视觉-语言模型框架，用于胸部X光解读，通过两阶段训练提升可解释性和准确性，并支持临床审计与人机协作。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型缺乏透明的逐步推理过程，无法满足临床医生对可解释性和可信AI的需求。

Method: 采用高保真视觉编码，结合推理优先的两阶段训练：先进行推理风格的监督微调（SFT），再使用可验证奖励进行强化学习（RL），输出包含中间推理步骤、不确定性和鉴别诊断的完整推理链。

Result: 在分布外评估中达到有竞争力的多标签分类性能；专家放射科医生读片研究显示，完整推理轨迹提升了信心、支持错误审计并缩短报告时间。

Conclusion: 该框架实现了更接近放射科医生思维过程的可解释AI，推动了在胸部放射及其他医学影像任务中可信、可解释AI的发展。

Abstract: Vision-language models (VLMs) have shown strong promise for medical image
analysis, but most remain opaque, offering predictions without the transparent,
stepwise reasoning clinicians rely on. We present a framework that brings
chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by
reasoning-first training paradigms, our approach is designed to learn how
experts reason, not just what they conclude, by aligning intermediate steps
with observable image evidence and radiology workflow. Beyond accuracy, the
explicit reasoning traces support clinical auditability: they reveal why a
conclusion was reached, which alternatives were considered, and where
uncertainty remains, enabling quality assurance, error analysis, and safer
human-AI collaboration.
  Our model couples high-fidelity visual encoding with a two-stage training
recipe: a reasoning-style supervised fine-tuning (SFT) followed by
reinforcement learning (RL) that uses verifiable rewards over a list of X-ray
abnormalities. The model outputs reasoning that mirrors radiologists systematic
thought process, uncertainty, and differential diagnosis. In
out-of-distribution evaluation, the approach achieves competitive multi-label
classification while improving interpretability. In a reader study with expert
radiologists, full reasoning traces increased confidence, supported error
auditing, and reduced time to finalize reports. We release code and the model
NV-Reason-CXR-3B to support community progress toward trustworthy, explainable
AI in chest radiography and other medical imaging tasks where reasoning quality
is as critical as prediction quality.

</details>


### [15] [Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints](https://arxiv.org/abs/2510.23978)
*Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: 提出联合预测多个傅里叶分量的方法，以提升任意尺度超分辨率中的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法逐个预测傅里叶分量导致性能下降和效率低下。

Method: 采用联合预测多个傅里叶分量的方式，而非逐个独立预测。

Result: 提升了超分辨率任务中的图像质量和计算效率。

Conclusion: 联合预测策略在成本和质量控制方面优于传统独立预测方法。

Abstract: Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is
crucial. Existing methods predict Fourier components one by one using a
recurrent neural network. However, this approach leads to performance
degradation and inefficiency due to independent prediction. This paper proposes
predicting multiple components jointly to improve both quality and efficiency.

</details>


### [16] [TeleEgo: Benchmarking Egocentric AI Assistants in the Wild](https://arxiv.org/abs/2510.23981)
*Jiaqi Yan,Ruilong Ren,Jingren Liu,Shuning Xu,Ling Wang,Yiheng Wang,Yun Wang,Long Zhang,Xiangyu Chen,Changzhi Sun,Jixiang Luo,Dell Zhang,Hao Sun,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleEgo是一个面向真实场景的长时间、流式、全模态基准，用于评估以自我为中心的AI助手在工作、生活、社交和文化等日常情境中的记忆、理解和跨记忆推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准通常孤立地评估AI助手的能力，缺乏真实的流式场景或仅支持短期任务，难以全面衡量其在现实世界中处理多模态输入、实时响应和长期记忆的能力。

Method: 构建了一个包含每位参与者超过14小时同步的自我中心视频、音频和文本数据集，涵盖四个领域，并统一时间线；设计了12个诊断性子任务和3,291个人工验证的问答项，在流式设置下评估记忆、理解和跨记忆推理能力；提出实时准确率和记忆持久时间两个关键指标。

Result: 提供了对AI助手在长时间、多模态、流式输入下的综合性能评估，支持多种问答形式，实现了对正确性、时间响应性和长期记忆保持能力的联合衡量。

Conclusion: TeleEgo为评估和推动实用型自我中心AI助手的发展提供了一个现实且全面的基准。

Abstract: Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \& study, lifestyle
\& routines, social activities, and outings \& culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.

</details>


### [17] [AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization](https://arxiv.org/abs/2510.24000)
*Heethanjan Kanagalingam,Thenukan Pathmanathan,Mokeeshan Vathanakumar,Tharmakulasingam Mukunthan*

Main category: cs.CV

TL;DR: 提出一种名为AdvBlur的糖尿病视网膜病变分类新方法，通过引入对抗模糊图像和双损失函数框架来提升模型在未见分布变化下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在不同设备、人群和成像条件下因分布差异而鲁棒性不足，难以实现早期准确检测糖尿病视网膜病变。

Method: 将对抗模糊图像加入数据集，并采用双损失函数框架以增强域泛化能力。

Result: 在多个数据集上验证了方法的有效性，在未见外部数据集上表现优于或媲美当前最先进的域泛化模型。

Conclusion: AdvBlur能有效缓解分布变异对模型性能的影响，提升了糖尿病视网膜病变自动检测的鲁棒性和实用性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet
early and accurate detection can significantly improve treatment outcomes.
While numerous Deep learning (DL) models have been developed to predict DR from
fundus images, many face challenges in maintaining robustness due to
distributional variations caused by differences in acquisition devices,
demographic disparities, and imaging conditions. This paper addresses this
critical limitation by proposing a novel DR classification approach, a method
called AdvBlur. Our method integrates adversarial blurred images into the
dataset and employs a dual-loss function framework to address domain
generalization. This approach effectively mitigates the impact of unseen
distributional variations, as evidenced by comprehensive evaluations across
multiple datasets. Additionally, we conduct extensive experiments to explore
the effects of factors such as camera type, low-quality images, and dataset
size. Furthermore, we perform ablation studies on blurred images and the loss
function to ensure the validity of our choices. The experimental results
demonstrate the effectiveness of our proposed method, achieving competitive
performance compared to state-of-the-art domain generalization DR models on
unseen external datasets.

</details>


### [18] [Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge](https://arxiv.org/abs/2510.24009)
*Yuan Jin,Antonio Pepe,Gian Marco Melito,Yuxuan Chen,Yunsu Byeon,Hyeseong Kim,Kyungwon Kim,Doohyun Park,Euijoon Choi,Dosik Hwang,Andriy Myronenko,Dong Yang,Yufan He,Daguang Xu,Ayman El-Ghotni,Mohamed Nabil,Hossam El-Kady,Ahmed Ayyad,Amr Nasr,Marek Wodzinski,Henning Müller,Hyeongyu Kim,Yejee Shin,Abbas Khan,Muhammad Asad,Alexander Zolotarev,Caroline Roney,Anthony Mathur,Martin Benning,Gregory Slabaugh,Theodoros Panagiotis Vagenas,Konstantinos Georgas,George K. Matsopoulos,Jihan Zhang,Zhen Zhang,Liqin Huang,Christian Mayer,Heinrich Mächler,Jan Egger*

Main category: cs.CV

TL;DR: 该研究通过发布一个大规模、公开的多机构主动脉血管树（AVT）分割数据集，推动CTA图像自动分析的发展。在SEG.A.挑战赛中，基于3D U-Net的深度学习方法表现最佳，且模型集成显著优于单一模型，性能与算法设计和训练数据特征密切相关。


<details>
  <summary>Details</summary>
Motivation: 主动脉血管树自动分析具有重要临床价值，但因缺乏高质量共享数据而受限，因此需要建立公开数据集以推动技术发展。

Method: 组织了SEG.A.挑战赛，提供一个多中心CTA数据集，评估参赛算法在隐藏测试集上的分割性能，并可选进行用于计算模拟的表面网格化任务。

Result: 3D U-Net架构在比赛中表现最优，模型集成显著提升性能；算法性能与定制化后处理及训练数据特性密切相关。

Conclusion: 该挑战赛建立了AVT分割的新性能基准，证明了模型融合的优势，并提供了宝贵的公共资源，有助于推动未来临床可用工具的发展。

Abstract: The automated analysis of the aortic vessel tree (AVT) from computed
tomography angiography (CTA) holds immense clinical potential, but its
development has been impeded by a lack of shared, high-quality data. We
launched the SEG.A. challenge to catalyze progress in this field by introducing
a large, publicly available, multi-institutional dataset for AVT segmentation.
The challenge benchmarked automated algorithms on a hidden test set, with
subsequent optional tasks in surface meshing for computational simulations. Our
findings reveal a clear convergence on deep learning methodologies, with 3D
U-Net architectures dominating the top submissions. A key result was that an
ensemble of the highest-ranking algorithms significantly outperformed
individual models, highlighting the benefits of model fusion. Performance was
strongly linked to algorithmic design, particularly the use of customized
post-processing steps, and the characteristics of the training data. This
initiative not only establishes a new performance benchmark but also provides a
lasting resource to drive future innovation toward robust, clinically
translatable tools.

</details>


### [19] [Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks](https://arxiv.org/abs/2510.24010)
*Mirali Purohit,Bimal Gajera,Vatsal Malaviya,Irish Mehta,Kunal Kasodekar,Jacob Adler,Steven Lu,Umaa Rebbapragada,Hannah Kerner*

Main category: cs.CV

TL;DR: 本文提出了Mars-Bench，首个用于系统评估火星相关任务（涵盖分类、分割和目标检测）的基准，包含20个基于轨道和地表图像的数据集，旨在推动火星科学领域基础模型的发展。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在地球观测等领域取得进展，但火星科学研究缺乏标准化的基准和评估框架，限制了该领域的发展。

Method: 构建了一个包含20个数据集的标准化基准Mars-Bench，涵盖陨石坑、锥体、岩石和霜等关键地质特征，并使用自然图像、地球卫星数据和先进的视觉-语言模型进行基线评估。

Result: 实验结果表明，针对火星任务预训练的模型可能优于通用领域模型，验证了领域适配预训练的潜力。

Conclusion: Mars-Bench为火星科学中的机器学习模型提供了标准化的评估平台，有望促进该领域基础模型的研究与发展。

Abstract: Foundation models have enabled rapid progress across many specialized domains
by leveraging large-scale pre-training on unlabeled data, demonstrating strong
generalization to a variety of downstream tasks. While such models have gained
significant attention in fields like Earth Observation, their application to
Mars science remains limited. A key enabler of progress in other domains has
been the availability of standardized benchmarks that support systematic
evaluation. In contrast, Mars science lacks such benchmarks and standardized
evaluation frameworks, which have limited progress toward developing foundation
models for Martian tasks. To address this gap, we introduce Mars-Bench, the
first benchmark designed to systematically evaluate models across a broad range
of Mars-related tasks using both orbital and surface imagery. Mars-Bench
comprises 20 datasets spanning classification, segmentation, and object
detection, focused on key geologic features such as craters, cones, boulders,
and frost. We provide standardized, ready-to-use datasets and baseline
evaluations using models pre-trained on natural images, Earth satellite data,
and state-of-the-art vision-language models. Results from all analyses suggest
that Mars-specific foundation models may offer advantages over general-domain
counterparts, motivating further exploration of domain-adapted pre-training.
Mars-Bench aims to establish a standardized foundation for developing and
comparing machine learning models for Mars science. Our data, models, and code
are available at: https://mars-bench.github.io/.

</details>


### [20] [AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts](https://arxiv.org/abs/2510.24034)
*Yufan Liu,Wanqian Zhang,Huashan Chen,Lin Wang,Xiaojun Jia,Zheng Lin,Weiping Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为APT的黑盒框架，利用大语言模型（LLM）自动生成可读性强且能绕过过滤机制的文本到图像模型对抗性后缀，有效提升了红队测试的安全漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型安全评估方法多依赖白盒访问和低效的逐提示优化，且生成的语义无意义提示易被过滤器拦截，因此需要一种更高效、可读且抗过滤的黑盒红队测试方法。

Method: 提出APT框架，采用交替优化与微调策略，在对抗后缀优化过程中结合大语言模型，并引入双重规避策略：通过辅助LLM的困惑度评分生成可读提示，以及使用禁用词惩罚机制避免黑名单词汇的显式生成。

Result: 实验表明，APT生成的对抗性提示在可读性和抗过滤方面表现优异，具备强大的零样本迁移能力，能够快速适应未见提示，并成功暴露多个商用API（如Leonardo.Ai）中的严重安全漏洞。

Conclusion: APT是一种高效、可读且具备强迁移性的黑盒红队测试方法，显著提升了对文本到图像模型安全漏洞的主动发现能力，尤其适用于实际部署系统的安全评估。

Abstract: Despite rapid advancements in text-to-image (T2I) models, their safety
mechanisms are vulnerable to adversarial prompts, which maliciously generate
unsafe images. Current red-teaming methods for proactively assessing such
vulnerabilities usually require white-box access to T2I models, and rely on
inefficient per-prompt optimization, as well as inevitably generate
semantically meaningless prompts easily blocked by filters. In this paper, we
propose APT (AutoPrompT), a black-box framework that leverages large language
models (LLMs) to automatically generate human-readable adversarial suffixes for
benign prompts. We first introduce an alternating optimization-finetuning
pipeline between adversarial suffix optimization and fine-tuning the LLM
utilizing the optimized suffix. Furthermore, we integrates a dual-evasion
strategy in optimization phase, enabling the bypass of both perplexity-based
filter and blacklist word filter: (1) we constrain the LLM generating
human-readable prompts through an auxiliary LLM perplexity scoring, which
starkly contrasts with prior token-level gibberish, and (2) we also introduce
banned-token penalties to suppress the explicit generation of banned-tokens in
blacklist. Extensive experiments demonstrate the excellent red-teaming
performance of our human-readable, filter-resistant adversarial prompts, as
well as superior zero-shot transferability which enables instant adaptation to
unseen prompts and exposes critical vulnerabilities even in commercial APIs
(e.g., Leonardo.Ai.).

</details>


### [21] [ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning](https://arxiv.org/abs/2510.24036)
*Xingyu Liu,Kun Ming Goh*

Main category: cs.CV

TL;DR: 本文研究了ResNet如何通过跳跃连接解决深度CNN训练中的梯度消失问题，并在CIFAR-10上验证其优于传统深度CNN的性能。


<details>
  <summary>Details</summary>
Motivation: 由于梯度消失问题，训练非常深的卷积神经网络具有挑战性，限制了模型性能的提升。

Method: 采用ResNet架构，引入跳跃连接（skip connections），使梯度能够绕过中间层直接传播，从而支持数百层网络的训练。

Result: 在CIFAR-10数据集上，ResNet-18达到89.9%的准确率，优于同类深度传统CNN的84.1%，且收敛更快、训练更稳定。

Conclusion: ResNet通过跳跃连接有效缓解了梯度消失问题，显著提升了深度网络的训练效果和性能。

Abstract: Convolutional Neural Networks (CNNs) has revolutionized computer vision, but
training very deep networks has been challenging due to the vanishing gradient
problem. This paper explores Residual Networks (ResNet), introduced by He et
al. (2015), which overcomes this limitation by using skip connections. ResNet
enables the training of networks with hundreds of layers by allowing gradients
to flow directly through shortcut connections that bypass intermediate layers.
In our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%
accuracy compared to 84.1% for a traditional deep CNN of similar depth, while
also converging faster and training more stably.

</details>


### [22] [Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models](https://arxiv.org/abs/2510.24037)
*Shufan Shen,Junshu Sun,Shuhui Wang,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SNELLA的一阶段稀疏调优方法，用于高效微调预训练视觉模型，在降低内存消耗的同时实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏调优方法采用两阶段范式，依赖梯度信息定位任务相关权重且忽略调优过程中的参数调整，导致性能受限且内存占用高。

Method: SNELLA通过将权重矩阵与两个低秩可学习矩阵合并成的稀疏矩阵相加来选择性更新权重，并引入非线性核函数扩展低秩分解以提高合并矩阵的秩；同时设计了端到端的自适应双层稀疏分配机制，基于重要性评分在层间和层内竞争确定任务相关权重。

Result: 在分类、分割和生成任务上验证了SNELLA的有效性，相比SPT-LoRA在FGVC基准上Top-1准确率提升1.8%，并在86M到632M参数规模模型上实现31.1%-39.9%的内存减少。

Conclusion: SNELLA通过一阶段的自适应稀疏调优机制，在显著降低内存使用的同时提升了下游任务性能，展现出较强的适应性和效率。

Abstract: Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision
models to downstream tasks. Among PEFT paradigms, sparse tuning achieves
remarkable performance by adjusting only the weights most relevant to
downstream tasks, rather than densely tuning the entire weight matrix. Current
methods follow a two-stage paradigm. First, it locates task-relevant weights by
gradient information, which overlooks the parameter adjustments during
fine-tuning and limits the performance. Second, it updates only the located
weights by applying a sparse mask to the gradient of the weight matrix, which
results in high memory usage due to the storage of all weight matrices in the
optimizer. In this paper, we propose a one-stage method named SNELLA to
overcome the above limitations. For memory usage, SNELLA selectively updates
the weight matrix by adding it to another sparse matrix that is merged by two
low-rank learnable matrices. We extend the low-rank decomposition by
introducing nonlinear kernel functions, thereby increasing the rank of the
resulting merged matrix to prevent the interdependency among weight updates,
enabling better adaptation to downstream tasks. For locating task-relevant
weights, we propose an adaptive bi-level sparsity allocation mechanism that
encourages weights to compete across and inside layers based on their
importance scores in an end-to-end manner. Extensive experiments are conducted
on classification, segmentation, and generation tasks using different
pre-trained vision models. The results show that SNELLA achieves SOTA
performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.
90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.
Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%
across models with parameter scales from 86M to 632M. Our source codes are
available at https://github.com/ssfgunner/SNELL.

</details>


### [23] [Enhancing CLIP Robustness via Cross-Modality Alignment](https://arxiv.org/abs/2510.24038)
*Xingyu Zhu,Beier Zhu,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于最优传输的跨模态对齐框架COLA，用于缓解视觉-语言模型（如CLIP）在对抗扰动下的图文特征错位问题。COLA无需训练，通过将图像嵌入投影到文本特征子空间并结合最优传输优化全局和局部对齐，在保持干净样本性能的同时显著提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉语言模型在零样本分类中表现良好，但在对抗扰动下易受攻击，主要原因是图像与文本特征在嵌入空间中存在严重错位，现有方法未能有效解决这一问题。

Method: 提出COLA框架：(1) 将对抗性图像嵌入投影到由类别文本特征张成的子空间，滤除非语义扰动；(2) 将图像和文本建模为多增广视图上的离散分布，利用最优传输（OT）优化对齐，并将子空间投影融入代价计算中。整个过程无需训练。

Result: 在14个零样本分类基准上进行了广泛评估，COLA在PGD攻击下平均提升了6.7%的准确率（特别是在ImageNet及其变体上），同时在干净样本上保持高精度。

Conclusion: COLA通过显式恢复对抗条件下的全局和局部跨模态对齐，有效增强了CLIP类模型的鲁棒性，且无需微调，具有良好的通用性和实用性。

Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong generalization
in zero-shot classification but remain highly vulnerable to adversarial
perturbations. Existing methods primarily focus on adversarial fine-tuning or
prompt optimization; they often overlook the gaps in CLIP's encoded features,
which is shown as the text and image features lie far apart from each other.
This misalignment is significantly amplified under adversarial perturbations,
leading to severe degradation in classification performance. To address this
problem, we propose Cross-modality Alignment, dubbed COLA, an optimal
transport-based framework that explicitly addresses adversarial misalignment by
restoring both global image-text alignment and local structural consistency in
the feature space. (1) COLA first projects adversarial image embeddings onto a
subspace spanned by class text features, effectively filtering out non-semantic
distortions while preserving discriminative information. (2) It then models
images and texts as discrete distributions over multiple augmented views and
refines their alignment via OT, with the subspace projection seamlessly
integrated into the cost computation. This design ensures stable cross-modal
alignment even under adversarial conditions. COLA is training-free and
compatible with existing fine-tuned models. Extensive evaluations across 14
zero-shot classification benchmarks demonstrate the effectiveness of COLA,
especially with an average improvement of 6.7% on ImageNet and its variants
under PGD adversarial attacks, while maintaining high accuracy on clean
samples.

</details>


### [24] [Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification](https://arxiv.org/abs/2510.24078)
*William Yang,Xindi Wu,Zhiwei Deng,Esin Tureci,Olga Russakovsky*

Main category: cs.CV

TL;DR: 本文提出了一种名为BOB（BeyondOBjects）的细粒度文本到图像模型微调策略，通过提取类无关属性并在微调时显式建模、生成时边缘化，有效缓解过拟合并提升合成数据质量，在多个低样本细粒度分类任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 使用文本到图像（T2I）模型生成用于分类任务的合成训练数据面临质量和多样性挑战，直接微调易导致过拟合和样本多样性下降，尤其在细粒度分类场景下问题更为突出。

Method: 提出BOB方法：首先从少量真实样本中提取类无关属性（如背景、姿态），在T2I模型微调时显式引入这些属性作为条件，而在生成时将其边缘化，从而保留生成先验并减少类间错误关联。

Result: 在多个T2I模型、主干网络和数据集上实验表明，BOB在低样本细粒度分类中达到SOTA性能；例如在Aircraft数据集上比DataDream提升7.4%；在四分之三基准中，使用5张真实图像加BOB生成数据优于仅用10张真实图像的微调效果；在24个实验设置中18个优于先前方法，其中14个提升超2%准确率。

Conclusion: BOB通过解耦类无关属性的条件建模与边缘化，有效平衡了微调过程中的保真度与多样性，在低样本细粒度分类中显著提升了合成数据的有效性，具有广泛的应用潜力。

Abstract: Text-to-image (T2I) models are increasingly used for synthetic dataset
generation, but generating effective synthetic training data for classification
remains challenging. Fine-tuning a T2I model with a few real examples can help
improve the quality of synthetic training data; however, it may also cause
overfitting and reduce diversity in the generated samples. We propose a
fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for
fine-grained classification. Given a small set of real examples, we first
extract class-agnostic attributes such as scene background and object pose. We
then explicitly condition on these attributes during fine-tuning of the T2I
model and marginalize them out during generation. This design mitigates
overfitting, preserves the T2I model's generative prior, reduces estimation
errors, and further minimizes unintended inter-class associations. Extensive
experiments across multiple T2I models, backbones, and datasets show that our
method achieves state-of-the-art performance in low-shot fine-grained
classification when augmented with synthetic data. Concretely, BOB outperforms
DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning
a CLIP classifier with five real images augmented with 100 synthetic images).
In three of the four benchmarks, fine-tuning downstream models with 5 real
images augmented with BOB achieves better performance than fine-tuning with 10
real images. Collectively, BOB outperforms prior art in 18 of 24 experimental
settings, with 2+% accuracy improvements in 14 of these settings.

</details>


### [25] [OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation](https://arxiv.org/abs/2510.24093)
*Agus Gunawan,Samuel Teodoro,Yun Chen,Soo Ye Kim,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的通用文本图像操作方法OmniText，通过改进注意力机制实现文本删除、样式控制和内容编辑，并提出了新的损失函数与基准数据集OmniText-Bench，在多种任务上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的文本修复方法在文本去除、样式控制和重复字符生成方面存在局限性，限制了其在更广泛文本图像操作（TIM）任务中的应用。

Method: 通过研究交叉注意力和自注意力机制的特性，提出自注意力反转实现文本去除，重新分配交叉注意力以减少文本幻觉，并在潜在优化框架中引入新的内容损失和样式损失以实现可控的文本修复。

Result: OmniText在多个TIM任务上实现了最先进的性能，优于现有文本修复方法，且与专用方法相当；同时发布了包含多种任务的基准数据集OmniText-Bench。

Conclusion: OmniText是首个无需训练的通用文本图像操作框架，能够有效处理多样化的TIM任务，在文本去除、编辑和风格化方面表现出色。

Abstract: Recent advancements in diffusion-based text synthesis have demonstrated
significant performance in inserting and editing text within images via
inpainting. However, despite the potential of text inpainting methods, three
key limitations hinder their applicability to broader Text Image Manipulation
(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over
the style of rendered text, and (iii) a tendency to generate duplicated
letters. To address these challenges, we propose OmniText, a training-free
generalist capable of performing a wide range of TIM tasks. Specifically, we
investigate two key properties of cross- and self-attention mechanisms to
enable text removal and to provide control over both text styles and content.
Our findings reveal that text removal can be achieved by applying
self-attention inversion, which mitigates the model's tendency to focus on
surrounding text, thus reducing text hallucinations. Additionally, we
redistribute cross-attention, as increasing the probability of certain text
tokens reduces text hallucination. For controllable inpainting, we introduce
novel loss functions in a latent optimization framework: a cross-attention
content loss to improve text rendering accuracy and a self-attention style loss
to facilitate style customization. Furthermore, we present OmniText-Bench, a
benchmark dataset for evaluating diverse TIM tasks. It includes input images,
target text with masks, and style references, covering diverse applications
such as text removal, rescaling, repositioning, and insertion and editing with
various styles. Our OmniText framework is the first generalist method capable
of performing diverse TIM tasks. It achieves state-of-the-art performance
across multiple tasks and metrics compared to other text inpainting methods and
is comparable with specialist methods.

</details>


### [26] [Enhancing Pre-trained Representation Classifiability can Boost its Interpretability](https://arxiv.org/abs/2510.24105)
*Shufan Shen,Zhaobo Qi,Junshu Sun,Qingming Huang,Qi Tian,Shuhui Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的固有可解释性评分（IIS），用于量化预训练视觉模型表示的可解释性，并发现可解释性与分类能力之间存在正相关关系，表明高分类性能的表示也更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉模型在下游任务中注重分类性能，但应用中对表示的可解释性提出了新要求，然而二者能否兼得尚不明确。

Method: 通过引入可解释语义在表示中的比例来量化可解释性，提出IIS指标，利用解释方法捕获的信息量评估表示中的信息损失。

Result: 实验发现可解释性与分类能力呈正相关；通过最大化可解释性的微调可进一步提升分类性能，且基于解释的预测准确率下降更少。

Conclusion: 可解释性与分类能力可以统一提升，IIS为评估和优化预训练视觉模型提供了有效工具。

Abstract: The visual representation of a pre-trained model prioritizes the
classifiability on downstream tasks, while the widespread applications for
pre-trained visual models have posed new requirements for representation
interpretability. However, it remains unclear whether the pre-trained
representations can achieve high interpretability and classifiability
simultaneously. To answer this question, we quantify the representation
interpretability by leveraging its correlation with the ratio of interpretable
semantics within the representations. Given the pre-trained representations,
only the interpretable semantics can be captured by interpretations, whereas
the uninterpretable part leads to information loss. Based on this fact, we
propose the Inherent Interpretability Score (IIS) that evaluates the
information loss, measures the ratio of interpretable semantics, and quantifies
the representation interpretability. In the evaluation of the representation
interpretability with different classifiability, we surprisingly discover that
the interpretability and classifiability are positively correlated, i.e.,
representations with higher classifiability provide more interpretable
semantics that can be captured in the interpretations. This observation further
supports two benefits to the pre-trained representations. First, the
classifiability of representations can be further improved by fine-tuning with
interpretability maximization. Second, with the classifiability improvement for
the representations, we obtain predictions based on their interpretations with
less accuracy degradation. The discovered positive correlation and
corresponding applications show that practitioners can unify the improvements
in interpretability and classifiability for pre-trained vision models. Codes
are available at https://github.com/ssfgunner/IIS.

</details>


### [27] [UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations](https://arxiv.org/abs/2510.24116)
*Fengming Yu,Haiwei Pan,Kejia Zhang,Jian Guan,Haiying Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于频域中间特征的统一异构知识蒸馏框架UHKD，通过傅里叶变换缓解异构模型间的表征差异，并利用特征变换和对齐模块实现跨架构知识迁移，在CIFAR-100和ImageNet-1K上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法多针对同构模型设计，在异构场景下尤其是使用中间层特征时性能下降，且大多局限于logits空间，未能充分利用中间层的语义信息。

Method: 提出UHKD框架，将教师和学生的中间特征转换到频域，利用傅里叶变换捕获全局信息；设计特征变换模块（FTM）生成紧凑的频域表示，并通过可学习的特征对齐模块（FAM）进行多层次特征对齐；训练目标结合频域特征的均方误差和logits的KL散度。

Result: 在CIFAR-100和ImageNet-1K数据集上，相比最新方法分别取得5.59%和0.83%的性能提升。

Conclusion: UHKD有效解决了异构模型间知识迁移中的语义差异问题，能够统一不同架构的表示并高效利用视觉知识，显著提升蒸馏效果。

Abstract: Knowledge distillation (KD) is an effective model compression technique that
transfers knowledge from a high-performance teacher to a lightweight student,
reducing cost while maintaining accuracy. In visual applications, where
large-scale image models are widely used, KD enables efficient deployment.
However, architectural diversity introduces semantic discrepancies that hinder
the use of intermediate representations. Most existing KD methods are designed
for homogeneous models and degrade in heterogeneous scenarios, especially when
intermediate features are involved. Prior studies mainly focus on the logits
space, making limited use of the semantic information in intermediate layers.
To address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)
is proposed as a framework that leverages intermediate features in the
frequency domain for cross-architecture transfer. Fourier transform is applied
to capture global feature information, alleviating representational
discrepancies between heterogeneous teacher-student pairs. A Feature
Transformation Module (FTM) produces compact frequency-domain representations
of teacher features, while a learnable Feature Alignment Module (FAM) projects
student features and aligns them via multi-level matching. Training is guided
by a joint objective combining mean squared error on intermediate features with
Kullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K
demonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD
as an effective approach for unifying heterogeneous representations and
enabling efficient utilization of visual knowledge

</details>


### [28] [DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery](https://arxiv.org/abs/2510.24117)
*Zan Wang,Siyu Chen,Luya Mo,Xinfeng Gao,Yuxin Shen,Lebin Ding,Wei Liang*

Main category: cs.CV

TL;DR: DogMo是一个大规模多视角RGB-D视频数据集，用于从图像中恢复犬类运动，包含1.2k个动作序列，涵盖10只不同犬种，解决了现有数据集在多视角、3D数据、规模和多样性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有犬类运动数据集缺乏多视角和真实3D数据，且规模和多样性有限，难以支持精确的运动恢复研究。

Method: 构建了DogMo数据集，并提出了一个三阶段的实例特定优化流程，逐步通过粗对齐、密集对应监督和时间正则化来优化SMAL模型的体形和姿态。

Result: 建立了四种运动恢复基准设置，支持单目和多视角、RGB与RGB-D输入的系统评估，显著提升了犬类运动恢复的精度。

Conclusion: DogMo数据集和提出的方法为犬类运动恢复研究提供了原则性基础，并推动了计算机视觉、图形学与动物行为建模交叉领域的新方向。

Abstract: We present DogMo, a large-scale multi-view RGB-D video dataset capturing
diverse canine movements for the task of motion recovery from images. DogMo
comprises 1.2k motion sequences collected from 10 unique dogs, offering rich
variation in both motion and breed. It addresses key limitations of existing
dog motion datasets, including the lack of multi-view and real 3D data, as well
as limited scale and diversity. Leveraging DogMo, we establish four motion
recovery benchmark settings that support systematic evaluation across monocular
and multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,
we further introduce a three-stage, instance-specific optimization pipeline
that fits the SMAL model to the motion sequences. Our method progressively
refines body shape and pose through coarse alignment, dense correspondence
supervision, and temporal regularization. Our dataset and method provide a
principled foundation for advancing research in dog motion recovery and open up
new directions at the intersection of computer vision, computer graphics, and
animal behavior modeling.

</details>


### [29] [ETC: training-free diffusion models acceleration with Error-aware Trend Consistency](https://arxiv.org/abs/2510.24129)
*Jiajian Xie,Hubery Yin,Chen Li,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为Error-aware Trend Consistency (ETC)的加速扩散模型生成过程的框架，通过趋势预测和模型特定误差容忍机制，在保持生成一致性的同时显著提升采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有的训练-free加速方法忽略去噪趋势且缺乏对模型特有误差容忍度的控制，导致多步重用时轨迹偏离，生成结果不一致。

Method: ETC框架包含两个核心组件：一是利用扩散轨迹平滑连续性的趋势预测器，将历史去噪模式投影为稳定的未来方向，并分布于多个近似步骤中；二是提出模型特定的误差容忍搜索机制，通过识别语义规划到质量优化的过渡点来设定校正阈值。

Result: 实验表明，ETC在FLUX模型上实现了2.65倍的加速，同时一致性指标SSIM仅下降0.074，生成质量几乎无损。

Conclusion: ETC有效解决了训练-free加速方法中的轨迹偏离与误差累积问题，兼顾了生成速度与一致性，为扩散模型的高效推理提供了新思路。

Abstract: Diffusion models have achieved remarkable generative quality but remain
bottlenecked by costly iterative sampling. Recent training-free methods
accelerate diffusion process by reusing model outputs. However, these methods
ignore denoising trends and lack error control for model-specific tolerance,
leading to trajectory deviations under multi-step reuse and exacerbating
inconsistencies in the generated results. To address these issues, we introduce
Error-aware Trend Consistency (ETC), a framework that (1) introduces a
consistent trend predictor that leverages the smooth continuity of diffusion
trajectories, projecting historical denoising patterns into stable future
directions and progressively distributing them across multiple approximation
steps to achieve acceleration without deviating; (2) proposes a model-specific
error tolerance search mechanism that derives corrective thresholds by
identifying transition points from volatile semantic planning to stable quality
refinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX
with negligible (-0.074 SSIM score) degradation of consistency.

</details>


### [30] [Compositional Image Synthesis with Inference-Time Scaling](https://arxiv.org/abs/2510.24133)
*Minsuk Ji,Sanghyeok Lee,Namhyuk Ahn*

Main category: cs.CV

TL;DR: 提出一种无需训练的框架，通过结合基于对象的方法和自优化机制，提升文本到图像生成中的布局准确性，同时保持美学质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在组合性方面存在不足，难以准确呈现物体数量、属性和空间关系。

Method: 利用大语言模型（LLM）从输入提示中生成显式布局，并将这些布局注入图像生成过程；使用以对象为中心的视觉-语言模型（VLM）对多个候选结果进行重排序，迭代选择最符合提示的结果。

Result: 该框架在场景与提示对齐方面优于近期的文本到图像模型，显著提升布局保真度。

Conclusion: 通过显式布局接地与自优化推理时扩展相结合，所提方法有效增强了文本到图像生成的组合性和布局准确性。

Abstract: Despite their impressive realism, modern text-to-image models still struggle
with compositionality, often failing to render accurate object counts,
attributes, and spatial relations. To address this challenge, we present a
training-free framework that combines an object-centric approach with
self-refinement to improve layout faithfulness while preserving aesthetic
quality. Specifically, we leverage large language models (LLMs) to synthesize
explicit layouts from input prompts, and we inject these layouts into the image
generation process, where a object-centric vision-language model (VLM) judge
reranks multiple candidates to select the most prompt-aligned outcome
iteratively. By unifying explicit layout-grounding with self-refine-based
inference-time scaling, our framework achieves stronger scene alignment with
prompts compared to recent text-to-image models. The code are available at
https://github.com/gcl-inha/ReFocus.

</details>


### [31] [VC4VG: Optimizing Video Captions for Text-to-Video Generation](https://arxiv.org/abs/2510.24134)
*Yang Du,Zhuoran Lin,Kaiqiang Song,Biao Wang,Zhicheng Zheng,Tiezheng Ge,Bo Zheng,Qin Jin*

Main category: cs.CV

TL;DR: 本文提出了一个针对文本到视频生成模型的视频字幕优化框架VC4VG，并构建了配套的评估基准VC4VG-Bench，实验证明优化字幕质量能显著提升生成视频性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频文本对在用于文本到视频生成时，其字幕往往不够优化，缺乏针对生成任务设计的高质量标注，导致模型难以生成连贯且符合指令的视频。

Method: 从T2V生成需求出发，分解视频重建所需的关键字幕要素，提出多维度、分层次的字幕设计原则，并构建包含细粒度、多维度和必要性分级指标的VC4VG-Bench评估基准。

Result: 通过大量T2V微调实验，验证了字幕质量提升与视频生成性能之间存在强相关性，使用优化后的字幕可显著提高生成效果。

Conclusion: VC4VG为文本到视频生成提供了系统化的字幕优化框架和评估标准，证明高质量、结构化字幕对提升生成性能至关重要，推动了该方向的研究发展。

Abstract: Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models.We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/qyr0403/VC4VG to support further
research.

</details>


### [32] [Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning](https://arxiv.org/abs/2510.24152)
*Aodi Wu,Xubo Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于Qwen2.5-VL-72B的系统性框架，通过混合提示路由、任务特定提示、视觉组装模块和推理参数配置，提升视觉语言模型在自动驾驶场景理解中的性能，在IROS 2025 RoboSense挑战赛中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 为了提升视觉语言模型在自动驾驶复杂场景（包括感知、预测、规划和干扰检测）中的理解和推理能力，解决多任务干扰和空间推理不足的问题。

Method: 采用四部分框架：1）混合提示路由器将问题分类并分发至任务专用提示；2）任务特定提示融合坐标系统、空间推理规则、角色扮演和思维链/树；3）视觉组装模块整合多视角图像与目标裁剪；4）按任务调整推理参数。

Result: 在Phase-1（干净数据）上达到70.87%平均准确率，Phase-2（干扰数据）上达到72.85%，表现出对安全关键任务的有效增强。

Conclusion: 结构化提示与空间锚定能显著提升VLM在自动驾驶场景中的鲁棒性和准确性，验证了系统性提示设计的重要性。

Abstract: This technical report presents our solution for the RoboSense Challenge at
IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving
scene understanding across perception, prediction, planning, and corruption
detection tasks. We propose a systematic framework built on four core
components. First, a Mixture-of-Prompts router classifies questions and
dispatches them to task-specific expert prompts, eliminating interference
across diverse question types. Second, task-specific prompts embed explicit
coordinate systems, spatial reasoning rules, role-playing,
Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to
each task. Third, a visual assembly module composes multi-view images with
object crops, magenta markers, and adaptive historical frames based on question
requirements. Fourth, we configure model inference parameters (temperature,
top-p, message roles) per task to optimize output quality. Implemented on
Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean
data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured
prompting and spatial grounding substantially enhance VLM performance on
safety-critical autonomous driving tasks. Code and prompt are available at
https://github.com/wuaodi/UCAS-CSU-phase2.

</details>


### [33] [Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2](https://arxiv.org/abs/2510.24195)
*Ziqi Zhou,Yifan Hu,Yufei Song,Zijing Li,Shengshan Hu,Leo Yu Zhang,Dezhong Yao,Long Zheng,Hai Jin*

Main category: cs.CV

TL;DR: 本文提出了UAP-SAM2，首个针对SAM2的跨提示通用对抗攻击方法，通过双语义偏差机制提升攻击在视频分割模型上的有效性与迁移性。


<details>
  <summary>Details</summary>
Motivation: SAM2在视频分割中表现出强泛化能力，但其对对抗样本的鲁棒性尚未被探索，且现有对SAM的攻击难以直接迁移到SAM2，主要由于提示引导方向和帧间语义纠缠的架构差异。

Method: 提出UAP-SAM2，采用目标扫描策略实现跨提示优化以增强迁移性，并设计双语义偏差框架，同时扭曲单帧内语义并破坏相邻帧间的语义一致性来提升攻击效果。

Result: 在六个数据集的两个分割任务上实验表明，UAP-SAM2显著优于现有最先进攻击方法，展现出更强的攻击性能。

Conclusion: UAP-SAM2有效克服了SAM与SAM2之间的架构差异带来的挑战，是首个成功应用于SAM2的通用对抗攻击方法，揭示了SAM2在对抗环境下的脆弱性。

Abstract: Recent studies reveal the vulnerability of the image segmentation foundation
model SAM to adversarial examples. Its successor, SAM2, has attracted
significant attention due to its strong generalization capability in video
segmentation. However, its robustness remains unexplored, and it is unclear
whether existing attacks on SAM can be directly transferred to SAM2. In this
paper, we first analyze the performance gap of existing attacks between SAM and
SAM2 and highlight two key challenges arising from their architectural
differences: directional guidance from the prompt and semantic entanglement
across consecutive frames. To address these issues, we propose UAP-SAM2, the
first cross-prompt universal adversarial attack against SAM2 driven by dual
semantic deviation. For cross-prompt transferability, we begin by designing a
target-scanning strategy that divides each frame into k regions, each randomly
assigned a prompt, to reduce prompt dependency during optimization. For
effectiveness, we design a dual semantic deviation framework that optimizes a
UAP by distorting the semantics within the current frame and disrupting the
semantic consistency across consecutive frames. Extensive experiments on six
datasets across two segmentation tasks demonstrate the effectiveness of the
proposed method for SAM2. The comparative results show that UAP-SAM2
significantly outperforms state-of-the-art (SOTA) attacks by a large margin.

</details>


### [34] [CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation](https://arxiv.org/abs/2510.24202)
*Anshul Kaushal,Kunal Jangid,Vinod K. Kurmi*

Main category: cs.CV

TL;DR: 本文提出了一种基于模糊卷积模块的编码器-解码器框架CLFSeg，用于提高结肠息肉和心脏分割的准确性和鲁棒性，在多个公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统CNN模型在医学图像分割中存在泛化能力弱、鲁棒性差和难以处理不确定性的缺点，影响分割性能。

Method: 提出CLFSeg框架，结合模糊逻辑与卷积层构建Fuzzy-Convolutional模块，并采用BCE与Dice损失联合优化以应对类别不平衡问题。

Result: 在CVC-ColonDB、CVC-ClinicDB、EtisLaribPolypDB和ACDC四个公开数据集上实验表明，CLFSeg在分割性能和计算效率方面均优于现有SOTA方法。

Conclusion: CLFSeg能有效提升医学图像分割中对小区域和边界区域的识别能力，具有良好的临床应用潜力。

Abstract: Accurate polyp and cardiac segmentation for early detection and treatment is
essential for the diagnosis and treatment planning of cancer-like diseases.
Traditional convolutional neural network (CNN) based models have represented
limited generalizability, robustness, and inability to handle uncertainty,
which affects the segmentation performance. To solve these problems, this paper
introduces CLFSeg, an encoder-decoder based framework that aggregates the
Fuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy
logic. This module enhances the segmentation performance by identifying local
and global features while minimizing the uncertainty, noise, and ambiguity in
boundary regions, ensuring computing efficiency. In order to handle class
imbalance problem while focusing on the areas of interest with tiny and
boundary regions, binary cross-entropy (BCE) with dice loss is incorporated.
Our proposed model exhibits exceptional performance on four publicly available
datasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.
Extensive experiments and visual studies show CLFSeg surpasses the existing
SOTA performance and focuses on relevant regions of interest in anatomical
structures. The proposed CLFSeg improves performance while ensuring computing
efficiency, which makes it a potential solution for real-world medical
diagnostic scenarios. Project page is available at
https://visdomlab.github.io/CLFSeg/

</details>


### [35] [MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration](https://arxiv.org/abs/2510.24211)
*Junhyuk So,Hyunho Kook,Chaeyeon Jang,Eunhyeok Park*

Main category: cs.CV

TL;DR: 提出了一种无需训练、无损的并行解码框架MC-SJD，通过耦合机制提升自回归视觉生成中的草案令牌一致性，显著加速图像和视频生成，且不损失质量。


<details>
  <summary>Details</summary>
Motivation: 自回归生成中逐token生成速度慢，限制了实际应用；SJD虽有潜力但因迭代间token不稳定导致接受率低。

Method: 基于耦合的信息论方法MC-SJD，在不改变SJD无损性质的前提下，最大化连续迭代间生成相同草案token的概率，仅需单行代码修改。

Result: 在图像生成上提速约4.2倍，视频生成上提速约13.3倍，且输出质量无损。

Conclusion: MC-SJD有效解决了SJD中的token不稳定问题，为自回归视觉生成提供了一种高效、简单、即插即用的加速方案。

Abstract: While autoregressive (AR) modeling has recently emerged as a new paradigm in
visual generation, its practical adoption is severely constrained by the slow
inference speed of per-token generation, which often requires thousands of
steps to produce a single sample. To address this challenge, we propose MC-SJD,
a training-free, lossless parallel decoding framework designed to accelerate AR
visual generation by extending the recently introduced Speculative Jacobi
Decoding (SJD). Although SJD shows strong potential for accelerating AR
generation, we demonstrate that token instability across iterations
significantly reduces the acceptance rate, a limitation that primarily arises
from the independent sampling process used during draft token generation. To
overcome this, we introduce MC-SJD, an information-theoretic approach based on
coupling, which substantially accelerates standard SJD by maximizing the
probability of sampling identical draft tokens across consecutive iterations,
all while preserving its lossless property. Remarkably, this method requires
only a single-line modification to the existing algorithm, yet achieves
substantial performance gains, delivering up to a ~4.2x acceleration in image
generation and ~13.3x acceleration in video generation compared to standard AR
decoding, without any degradation in output quality.

</details>


### [36] [Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization](https://arxiv.org/abs/2510.24213)
*Haoxin Yang,Yihong Lin,Jingdan Kang,Xuemiao Xu,Yue Li,Cheng Xu,Shengfeng He*

Main category: cs.CV

TL;DR: 提出ID²Face，一种训练为中心的面部匿名化框架，通过在潜空间中显式解耦身份与非身份信息，实现无需推理时优化的高效匿名化。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型依赖推理时干预，易导致分布偏移和属性纠缠，影响视觉质量和数据可用性。

Method: 设计条件扩散模型，采用身份掩码学习方案；通过身份解耦潜重组器和双向潜对齐分离身份与非身份特征，并用身份引导潜融合器进行软门控融合；训练时使用重构损失强化解耦，推理时通过随机采样身份向量实现匿名化，并引入正交身份映射抑制身份泄露。

Result: 实验表明，ID²Face在视觉质量、身份抑制和数据效用保持方面优于现有方法。

Conclusion: ID²Face通过训练阶段的显式解耦，有效避免了推理时优化带来的问题，实现了高质量、可控的面部匿名化。

Abstract: Face anonymization aims to conceal identity information while preserving
non-identity attributes. Mainstream diffusion models rely on inference-time
interventions such as negative guidance or energy-based optimization, which are
applied post-training to suppress identity features. These interventions often
introduce distribution shifts and entangle identity with non-identity
attributes, degrading visual fidelity and data utility. To address this, we
propose \textbf{ID\textsuperscript{2}Face}, a training-centric anonymization
framework that removes the need for inference-time optimization. The rationale
of our method is to learn a structured latent space where identity and
non-identity information are explicitly disentangled, enabling direct and
controllable anonymization at inference. To this end, we design a conditional
diffusion model with an identity-masked learning scheme. An Identity-Decoupled
Latent Recomposer uses an Identity Variational Autoencoder to model identity
features, while non-identity attributes are extracted from same-identity pairs
and aligned through bidirectional latent alignment. An Identity-Guided Latent
Harmonizer then fuses these representations via soft-gating conditioned on
noisy feature prediction. The model is trained with a recomposition-based
reconstruction loss to enforce disentanglement. At inference, anonymization is
achieved by sampling a random identity vector from the learned identity space.
To further suppress identity leakage, we introduce an Orthogonal Identity
Mapping strategy that enforces orthogonality between sampled and source
identity vectors. Experiments demonstrate that ID\textsuperscript{2}Face
outperforms existing methods in visual quality, identity suppression, and
utility preservation.

</details>


### [37] [SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs](https://arxiv.org/abs/2510.24214)
*Jinhong Deng,Wen Li,Joey Tianyi Zhou,Yang He*

Main category: cs.CV

TL;DR: 本文提出了一种面向显著性和覆盖性的视觉令牌剪枝策略SCOPE，以在多模态大语言模型中更有效地保留语义完整性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉令牌剪枝方法主要基于注意力分数选择最显著的令牌，导致所选令牌的语义不完整。

Method: 引入集合覆盖度量和令牌覆盖增益，并结合显著性得分提出SCOPE得分，迭代选择最高SCOPE得分的令牌。

Result: 在多个视觉-语言理解基准上使用LLaVA-1.5和LLaVA-Next模型进行实验，结果表明该方法 consistently 优于先前的方法。

Conclusion: SCOPE能有效平衡视觉令牌的显著性与覆盖性，在减少计算开销的同时更好地保持语义完整性。

Abstract: Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.

</details>


### [38] [Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation](https://arxiv.org/abs/2510.24231)
*Waseem Shariff,Timothy Hanley,Maciej Stec,Hossein Javidnia,Peter Corcoran*

Main category: cs.CV

TL;DR: 本文提出了一种基于事件的微扫视数据集，利用Blender渲染高保真眼动场景，并通过v2e生成事件流，结合脉冲神经网络模型（如Spiking-VGG系列）实现对微扫视角度的高效分类，准确率达90%左右，为基于事件的视觉研究提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 传统微扫视研究依赖于昂贵的眼动仪和帧率分析方法，存在可扩展性差、时间分辨率有限的问题。因此，需要一种低成本、高时效的方法来更好地捕捉微扫视的动态特征，推动认知计算中的眼动研究。

Method: 使用Blender模拟0.5到2.0度角位移的微扫视，分为七类；通过v2e工具将视频转换为事件流，保留其精细时空动态；采用Spiking-VGG11、VGG13、VGG16及提出的光学流增强型Spiking-VGG16Flow模型在SpikingJelly框架下进行评估。

Result: 模型在不依赖事件数量或持续时间的情况下，实现了约90%的平均分类准确率，成功区分不同角度位移的微扫视，验证了脉冲神经网络在精细运动识别中的潜力。

Conclusion: 该工作建立了首个基于事件的微扫视数据集，展示了脉冲神经网络在高时间分辨率运动识别中的有效性，为未来事件驱动的眼动研究和认知计算提供了重要资源和基准。

Abstract: Microsaccades are small, involuntary eye movements vital for visual
perception and neural processing. Traditional microsaccade studies typically
use eye trackers or frame-based analysis, which, while precise, are costly and
limited in scalability and temporal resolution. Event-based sensing offers a
high-speed, low-latency alternative by capturing fine-grained spatiotemporal
changes efficiently. This work introduces a pioneering event-based microsaccade
dataset to support research on small eye movement dynamics in cognitive
computing. Using Blender, we render high-fidelity eye movement scenarios and
simulate microsaccades with angular displacements from 0.5 to 2.0 degrees,
divided into seven distinct classes. These are converted to event streams using
v2e, preserving the natural temporal dynamics of microsaccades, with durations
ranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,
Spiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an
optical-flow-enhanced variant implemented in SpikingJelly. The models achieve
around 90 percent average accuracy, successfully classifying microsaccades by
angular displacement, independent of event count or duration. These results
demonstrate the potential of spiking neural networks for fine motion
recognition and establish a benchmark for event-based vision research. The
dataset, code, and trained models will be publicly available at
https://waseemshariff126.github.io/microsaccades/ .

</details>


### [39] [Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy](https://arxiv.org/abs/2510.24232)
*Qing Zhao,Weijian Deng,Pengxu Wei,ZiYi Dong,Hannan Lu,Xiangyang Ji,Liang Lin*

Main category: cs.CV

TL;DR: 提出Lipschitz正则化目标检测框架（LROD），通过统一恢复与检测任务的Lipschitz连续性，提升恶劣条件下检测的稳定性与准确性。


<details>
  <summary>Details</summary>
Motivation: 传统级联框架中图像恢复与目标检测网络之间存在功能不匹配，导致在雾天和低光照等恶劣条件下检测不稳定，且该问题尚未被深入研究。

Method: 从Lipschitz连续性的角度分析恢复与检测网络在输入空间和参数空间的功能差异，提出LROD框架，将图像恢复集成到检测器的特征学习中，并在训练过程中协调两者的Lipschitz性质。具体实现为Lipschitz-regularized YOLO（LR-YOLO）。

Result: 在雾天和低光基准上的实验表明，LR-YOLO显著提升了检测稳定性、优化平滑性和整体精度。

Conclusion: LROD框架有效缓解了恢复与检测任务间的功能不匹配问题，通过Lipschitz正则化实现了更鲁棒和稳定的端到端训练，可无缝扩展至现有YOLO系列检测器。

Abstract: To improve detection robustness in adverse conditions (e.g., haze and low
light), image restoration is commonly applied as a pre-processing step to
enhance image quality for the detector. However, the functional mismatch
between restoration and detection networks can introduce instability and hinder
effective integration -- an issue that remains underexplored. We revisit this
limitation through the lens of Lipschitz continuity, analyzing the functional
differences between restoration and detection networks in both the input space
and the parameter space. Our analysis shows that restoration networks perform
smooth, continuous transformations, while object detectors operate with
discontinuous decision boundaries, making them highly sensitive to minor
perturbations. This mismatch introduces instability in traditional cascade
frameworks, where even imperceptible noise from restoration is amplified during
detection, disrupting gradient flow and hindering optimization. To address
this, we propose Lipschitz-regularized object detection (LROD), a simple yet
effective framework that integrates image restoration directly into the
detector's feature learning, harmonizing the Lipschitz continuity of both tasks
during training. We implement this framework as Lipschitz-regularized YOLO
(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive
experiments on haze and low-light benchmarks demonstrate that LR-YOLO
consistently improves detection stability, optimization smoothness, and overall
accuracy.

</details>


### [40] [DeshadowMamba: Deshadowing as 1D Sequential Similarity](https://arxiv.org/abs/2510.24260)
*Zhaotong Yang,Yi Chen,Yanying Li,Shengfeng He,Yangyang Xu,Junyu Dong,Jian Yang,Yong Du*

Main category: cs.CV

TL;DR: 本文提出DeshadowMamba，利用Mamba模型结合方向调制机制CrossGate和ColorShift正则化，实现高质量的图像去阴影，取得最先进的视觉和定量效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力机制的去阴影方法因固定注意力模式导致结构失真和颜色不一致，缺乏对阴影语义的感知且易受邻近区域颜色干扰。

Method: 从序列建模角度重新审视去阴影任务，采用Mamba模型进行全局上下文传播，并提出CrossGate机制注入阴影感知相似性，结合ColorShift正则化通过对比学习抑制颜色污染。

Result: 在公开基准上实验表明，该方法在视觉质量和定量指标上均达到最先进水平。

Conclusion: 将选择性状态空间模型引入图像去阴影任务，结合阴影感知调制和颜色一致性正则化，有效提升了去阴影结果的结构完整性和色彩保真度。

Abstract: Recent deep models for image shadow removal often rely on attention-based
architectures to capture long-range dependencies. However, their fixed
attention patterns tend to mix illumination cues from irrelevant regions,
leading to distorted structures and inconsistent colors. In this work, we
revisit shadow removal from a sequence modeling perspective and explore the use
of Mamba, a selective state space model that propagates global context through
directional state transitions. These transitions yield an efficient global
receptive field while preserving positional continuity. Despite its potential,
directly applying Mamba to image data is suboptimal, since it lacks awareness
of shadow-non-shadow semantics and remains susceptible to color interference
from nearby regions. To address these limitations, we propose CrossGate, a
directional modulation mechanism that injects shadow-aware similarity into
Mamba's input gate, allowing selective integration of relevant context along
transition axes. To further ensure appearance fidelity, we introduce ColorShift
regularization, a contrastive learning objective driven by global color
statistics. By synthesizing structured informative negatives, it guides the
model to suppress color contamination and achieve robust color restoration.
Together, these components adapt sequence modeling to the structural integrity
and chromatic consistency required for shadow removal. Extensive experiments on
public benchmarks demonstrate that DeshadowMamba achieves state-of-the-art
visual quality and strong quantitative performance.

</details>


### [41] [GenTrack: A New Generation of Multi-Object Tracking](https://arxiv.org/abs/2510.24399)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: 本文提出了一种名为GenTrack的新型多目标跟踪方法，结合随机与确定性策略，利用粒子群优化（PSO）和社交交互建模，有效应对目标数量时变、遮挡和检测噪声等挑战，在ID一致性和跟踪鲁棒性方面表现优异，并提供了首个开源基准实现。


<details>
  <summary>Details</summary>
Motivation: 现有MOT方法在处理目标数量未知且时变、ID切换频繁、遮挡严重及检测器性能弱的情况下表现不佳，尤其缺乏统一且可复现的基准框架。因此，需要一种更鲁棒、自适应且可公开验证的跟踪方法。

Method: 提出GenTrack，采用混合跟踪策略：结合随机（基于PSO）和确定性方法；设计新的适应度函数引导粒子逼近目标分布模式；引入社会交互信息增强PSO搜索能力；构建包含空间一致性、外观、检测置信度、轨迹惩罚和社会评分的综合状态与观测模型；提供三种变体（Basic、PSO、PSO-Social）并开源代码。

Result: 实验表明，GenTrack在标准数据集和真实场景中均优于当前最先进方法，显著减少ID切换和轨迹丢失，尤其在遮挡和弱检测条件下表现突出；同时提供了包含对比方法的统一实现，确保公平比较。

Conclusion: GenTrack通过融合PSO优化与社会交互建模，实现了鲁棒且高效的多目标跟踪，解决了ID不一致和动态环境适应问题，其开源实现为后续研究提供了重要基准支持。

Abstract: This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.
Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.
Potential directions for future work are also discussed. The source-code
reference implementations of both the proposed method and compared-trackers are
provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack

</details>


### [42] [UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation](https://arxiv.org/abs/2510.24262)
*Jiyu Guo,Shuo Yang,Yiming Huang,Yancheng Long,Xiaobo Xia,Xiu Su,Bo Zhao,Zeke Xie,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出了一种以任务效用为中心的数据增强框架UtilGen，通过下游任务反馈自适应优化生成过程，提升合成数据的实用性和任务相关性。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法主要关注生成数据的视觉质量（如保真度和多样性），而忽视了不同下游任务对训练数据的不同需求，导致生成的数据可能不具备高任务效用。

Method: 引入一个权重分配网络来评估每个合成样本的任务特定效用，并采用双层优化策略迭代优化生成过程：模型级优化使生成模型适配下游任务，实例级优化调整每轮生成的提示嵌入和初始噪声等策略。

Result: 在八个不同复杂度和粒度的基准数据集上实验表明，UtilGen平均比先前最先进方法提升3.87%的准确率，且生成的数据更具影响力和任务相关性。

Conclusion: 从以视觉特性为中心转向以任务效用为中心的数据增强范式是有效的，UtilGen能生成更符合下游任务需求的高质量合成数据。

Abstract: Data augmentation using generative models has emerged as a powerful paradigm
for enhancing performance in computer vision tasks. However, most existing
augmentation approaches primarily focus on optimizing intrinsic data attributes
-- such as fidelity and diversity -- to generate visually high-quality
synthetic data, while often neglecting task-specific requirements. Yet, it is
essential for data generators to account for the needs of downstream tasks, as
training data requirements can vary significantly across different tasks and
network architectures. To address these limitations, we propose UtilGen, a
novel utility-centric data augmentation framework that adaptively optimizes the
data generation process to produce task-specific, high-utility training data
via downstream task feedback. Specifically, we first introduce a weight
allocation network to evaluate the task-specific utility of each synthetic
sample. Guided by these evaluations, UtilGen iteratively refines the data
generation process using a dual-level optimization strategy to maximize the
synthetic data utility: (1) model-level optimization tailors the generative
model to the downstream task, and (2) instance-level optimization adjusts
generation policies -- such as prompt embeddings and initial noise -- at each
generation round. Extensive experiments on eight benchmark datasets of varying
complexity and granularity demonstrate that UtilGen consistently achieves
superior performance, with an average accuracy improvement of 3.87% over
previous SOTA. Further analysis of data influence and distribution reveals that
UtilGen produces more impactful and task-relevant synthetic data, validating
the effectiveness of the paradigm shift from visual characteristics-centric to
task utility-centric data augmentation.

</details>


### [43] [A Hybrid Approach for Visual Multi-Object Tracking](https://arxiv.org/abs/2510.24410)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: 本文提出了一种结合随机与确定性机制的视觉多目标跟踪方法，用于在非线性动态和未知时变目标数量下保持标识一致性。


<details>
  <summary>Details</summary>
Motivation: 在复杂场景中，如非线性运动、非高斯噪声、目标交互和长时间遮挡下，现有跟踪方法难以保持目标标识的一致性，尤其对弱轨迹处理不佳。因此需要一种鲁棒且灵活的方法来应对这些挑战。

Method: 采用基于粒子滤波的随机机制处理非线性动态，并结合粒子群优化（PSO）引导粒子分布，利用运动一致性、外观相似性和社会交互线索设计适应度函数；通过确定性关联构建融合空间一致性、检测置信度和轨迹惩罚的成本矩阵以维持ID一致性；提出状态平滑更新策略和基于历史状态的速度回归模型以提升采样和更新质量。

Result: 实验结果表明，该方法在多个基准上优于现有最先进跟踪器，尤其在目标交互和遮挡情况下表现更优。同时支持预录视频和摄像头实时流处理。

Conclusion: 所提出的联合随机-确定性框架有效提升了多目标跟踪中的标识一致性与鲁棒性，尤其适用于复杂动态环境，并具有实际部署的灵活性。

Abstract: This paper proposes a visual multi-object tracking method that jointly
employs stochastic and deterministic mechanisms to ensure identifier
consistency for unknown and time-varying target numbers under nonlinear
dynamics. A stochastic particle filter addresses nonlinear dynamics and
non-Gaussian noise, with support from particle swarm optimization (PSO) to
guide particles toward state distribution modes and mitigate divergence through
proposed fitness measures incorporating motion consistency, appearance
similarity, and social-interaction cues with neighboring targets. Deterministic
association further enforces identifier consistency via a proposed cost matrix
incorporating spatial consistency between particles and current detections,
detection confidences, and track penalties. Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions. Moreover, velocity regression over past states
provides trend-seed velocities, enhancing particle sampling and state updates.
The proposed tracker is designed to operate flexibly for both pre-recorded
videos and camera live streams, where future frames are unavailable.
Experimental results confirm superior performance compared to state-of-the-art
trackers. The source-code reference implementations of both the proposed method
and compared-trackers are provided on GitHub:
https://github.com/SDU-VelKoTek/GenTrack2

</details>


### [44] [Training-free Source Attribution of AI-generated Images via Resynthesis](https://arxiv.org/abs/2510.24278)
*Pietro Bongini,Valentina Molinari,Andrea Costanzo,Benedetta Tondi,Mauro Barni*

Main category: cs.CV

TL;DR: 提出了一种无需训练的单样本合成图像来源归因方法，基于图像重合成技术，并引入了一个新的用于合成图像归因的数据集，实验证明该方法在少样本条件下优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺条件下，尤其是少样本或零样本场景中，合成图像来源归因具有挑战性，需要提升现有归因方法的性能。

Method: 通过生成描述待分析图像的提示词，利用候选源模型重合成图像，并在特征空间中比较重合成图像与原图的相似度，将图像归因于最接近原始图像的生成模型。

Result: 在新构建的合成图像数据集上实验表明，所提重合成方法在仅有少量训练样本时优于现有的少样本方法，且该数据集具有挑战性，适合作为未来方法的基准测试平台。

Conclusion: 所提出的无需训练的单样本归因方法在少样本条件下表现优异，新数据集为合成图像归因提供了有价值的评估基准。

Abstract: Synthetic image source attribution is a challenging task, especially in data
scarcity conditions requiring few-shot or zero-shot classification
capabilities. We present a new training-free one-shot attribution method based
on image resynthesis. A prompt describing the image under analysis is
generated, then it is used to resynthesize the image with all the candidate
sources. The image is attributed to the model which produced the resynthesis
closest to the original image in a proper feature space. We also introduce a
new dataset for synthetic image attribution consisting of face images from
commercial and open-source text-to-image generators. The dataset provides a
challenging attribution framework, useful for developing new attribution models
and testing their capabilities on different generative architectures. The
dataset structure allows to test approaches based on resynthesis and to compare
them to few-shot methods. Results from state-of-the-art few-shot approaches and
other baselines show that the proposed resynthesis method outperforms existing
techniques when only a few samples are available for training or fine-tuning.
The experiments also demonstrate that the new dataset is a challenging one and
represents a valuable benchmark for developing and evaluating future few-shot
and zero-shot methods.

</details>


### [45] [ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model](https://arxiv.org/abs/2510.24285)
*Juntian Zhang,Song Jin,Chuanqi Cheng,Yuhan Liu,Yankai Lin,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CV

TL;DR: 提出ViPER框架，通过自引导的粗到精视觉感知学习，提升视觉语言模型的细粒度感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升视觉语言模型细粒度视觉感知能力时受限于高质量数据稀缺，且监督微调和强化微调易牺牲通用能力或忽视视觉感知。

Method: 设计两阶段任务，结合图像级与实例级重建，采用自批判与自预测的闭环强化学习策略进行迭代优化。

Result: 在Qwen2.5-VL上应用后，Qwen-Viper在七个基准平均提升1.7%，细粒度感知最高提升6.0%，保持通用性的同时显著增强感知能力。

Conclusion: ViPER实现了视觉语言模型在细粒度感知上的自我进化，验证了生成与理解之间的互促关系，推动更自主、强大的VLM发展。

Abstract: The limited capacity for fine-grained visual perception presents a critical
bottleneck for Vision-Language Models (VLMs) in real-world applications.
Addressing this is challenging due to the scarcity of high-quality data and the
limitations of existing methods: supervised fine-tuning (SFT) often compromises
general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual
reasoning over visual perception. To bridge this gap, we propose a novel
two-stage task that structures visual perception learning as a coarse-to-fine
progressive process. Based on this task formulation, we develop ViPER, a
self-bootstrapping framework specifically designed to enable iterative
evolution through self-critiquing and self-prediction. By synergistically
integrating image-level and instance-level reconstruction with a two-stage
reinforcement learning strategy, ViPER establishes a closed-loop training
paradigm, where internally synthesized data directly fuel the enhancement of
perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the
Qwen-Viper series. With an average gain of 1.7% on seven comprehensive
benchmarks spanning various tasks and up to 6.0% on fine-grained perception,
Qwen-Viper consistently demonstrates superior performance across different
vision-language scenarios while maintaining generalizability. Beyond enabling
self-improvement in perceptual capabilities, ViPER provides concrete evidence
for the reciprocal relationship between generation and understanding, a
breakthrough to developing more autonomous and capable VLMs.

</details>


### [46] [Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning](https://arxiv.org/abs/2510.24321)
*Ivica Dimitrovski,Vlatko Spasev,Ivan Kitanovski*

Main category: cs.CV

TL;DR: 本文系统研究了提示学习在遥感图像少样本场景分类中的应用，比较了多种提示方法，并证明其在跨域性能上优于传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 由于标注数据稀缺且标注成本高，现有深度学习方法在遥感场景分类中受限；同时，CLIP等视觉-语言模型因领域差异和缺乏任务特定语义适应，在遥感中表现不佳。

Method: 采用提示学习作为轻量级适应策略，评估了上下文优化、条件上下文优化、多模态提示学习和带自调节约束的提示等方法，并在多个遥感数据集上进行少样本和跨数据集实验。

Result: 提示学习在少样本场景下 consistently 优于零样本CLIP和基于冻结特征的线性探针基线，其中带自调节约束的提示表现出最强的跨域鲁棒性。

Conclusion: 提示学习是缩小遥感图像领域差距的高效可扩展方案，为未来研究提供了坚实基础。

Abstract: Remote sensing applications increasingly rely on deep learning for scene
classification. However, their performance is often constrained by the scarcity
of labeled data and the high cost of annotation across diverse geographic and
sensor domains. While recent vision-language models like CLIP have shown
promise by learning transferable representations at scale by aligning visual
and textual modalities, their direct application to remote sensing remains
suboptimal due to significant domain gaps and the need for task-specific
semantic adaptation. To address this critical challenge, we systematically
explore prompt learning as a lightweight and efficient adaptation strategy for
few-shot remote sensing image scene classification. We evaluate several
representative methods, including Context Optimization, Conditional Context
Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating
Constraints. These approaches reflect complementary design philosophies: from
static context optimization to conditional prompts for enhanced generalization,
multi-modal prompts for joint vision-language adaptation, and semantically
regularized prompts for stable learning without forgetting. We benchmark these
prompt-learning methods against two standard baselines: zero-shot CLIP with
hand-crafted prompts and a linear probe trained on frozen CLIP features.
Through extensive experiments on multiple benchmark remote sensing datasets,
including cross-dataset generalization tests, we demonstrate that prompt
learning consistently outperforms both baselines in few-shot scenarios.
Notably, Prompting with Self-Regulating Constraints achieves the most robust
cross-domain performance. Our findings underscore prompt learning as a scalable
and efficient solution for bridging the domain gap in satellite and aerial
imagery, providing a strong foundation for future research in this field.

</details>


### [47] [Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2510.24366)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Ba-Thinh Lam,Vi Vu,Bach X. Nguyen,Jianhua Xing,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 提出了一种基于切换双学生架构的半监督医学图像分割方法，通过动态选择更可靠的学生网络和损失感知的指数移动平均策略，提升了伪标签质量和分割精度。


<details>
  <summary>Details</summary>
Motivation: 传统教师-学生框架在半监督医学图像分割中存在网络间强相关性和知识转移不可靠的问题，限制了学习效果。

Method: 设计了一种切换双学生架构，在每次迭代中选择最可靠的学生网络以增强协作并防止错误传播；同时引入损失感知的指数移动平均策略，使教师网络能动态吸收学生网络中的有效信息。

Result: 在多个3D医学图像分割数据集上进行了广泛实验，所提方法优于当前最先进的半监督方法，在有限标注条件下显著提升了分割性能。

Conclusion: 该即插即用框架有效改善了教师-学生模型中的知识传递过程，具有较强的鲁棒性和泛化能力，适用于低标注资源下的医学图像分割任务。

Abstract: Teacher-student frameworks have emerged as a leading approach in
semi-supervised medical image segmentation, demonstrating strong performance
across various tasks. However, the learning effects are still limited by the
strong correlation and unreliable knowledge transfer process between teacher
and student networks. To overcome this limitation, we introduce a novel
switching Dual-Student architecture that strategically selects the most
reliable student at each iteration to enhance dual-student collaboration and
prevent error reinforcement. We also introduce a strategy of Loss-Aware
Exponential Moving Average to dynamically ensure that the teacher absorbs
meaningful information from students, improving the quality of pseudo-labels.
Our plug-and-play framework is extensively evaluated on 3D medical image
segmentation datasets, where it outperforms state-of-the-art semi-supervised
methods, demonstrating its effectiveness in improving segmentation accuracy
under limited supervision.

</details>


### [48] [Decoupling What to Count and Where to See for Referring Expression Counting](https://arxiv.org/abs/2510.24374)
*Yuda Zou,Zijian Zhang,Yongchao Xu*

Main category: cs.CV

TL;DR: 本文提出了W2-Net，一种用于指代表达计数（REC）的新框架，通过“要计什么”和“在哪里看”的双查询机制，结合子类可分匹配策略，显著提升了细粒度子类别的计数与定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在标注时通常关注类别代表性区域（如头部），忽略了属性相关区域（如‘行走’依赖腿部信息），导致难以准确捕捉细粒度属性特征，影响子类级别的精确计数。

Method: 提出W2-Net，引入双查询机制：w2c查询用于定位对象，w2s查询专注于提取属性特定区域的视觉特征；同时设计子类可分匹配（SSM）策略，通过排斥力增强子类间的区分度。

Result: 在REC-8K数据集上，W2-Net比现有方法验证集和测试集的计数误差分别降低22.5%和18.0%，定位F1分数提升7%和8%。

Conclusion: W2-Net通过解耦对象定位与属性感知，并引入改进的匹配策略，在指代表达计数任务中实现了更精准的细粒度子类识别与计数。

Abstract: Referring Expression Counting (REC) extends class-level object counting to
the fine-grained subclass-level, aiming to enumerate objects matching a textual
expression that specifies both the class and distinguishing attribute. A
fundamental challenge, however, has been overlooked: annotation points are
typically placed on class-representative locations (e.g., heads), forcing
models to focus on class-level features while neglecting attribute information
from other visual regions (e.g., legs for "walking"). To address this, we
propose W2-Net, a novel framework that explicitly decouples the problem into
"what to count" and "where to see" via a dual-query mechanism. Specifically,
alongside the standard what-to-count (w2c) queries that localize the object, we
introduce dedicated where-to-see (w2s) queries. The w2s queries are guided to
seek and extract features from attribute-specific visual regions, enabling
precise subclass discrimination. Furthermore, we introduce Subclass Separable
Matching (SSM), a novel matching strategy that incorporates a repulsive force
to enhance inter-subclass separability during label assignment. W2-Net
significantly outperforms the state-of-the-art on the REC-8K dataset, reducing
counting error by 22.5% (validation) and 18.0% (test), and improving
localization F1 by 7% and 8%, respectively. Code will be available.

</details>


### [49] [Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool](https://arxiv.org/abs/2510.24378)
*Yann Kerverdo,Florent Leray,Youwan Mahé,Stéphanie Leplaideur,Francesca Galassi*

Main category: cs.CV

TL;DR: StrokeSeg是一个模块化、轻量化的框架，可将研究级脑卒中病灶分割模型转化为可部署的应用程序，在保持与原始PyTorch流程相当性能的同时，显著降低部署复杂性。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架如nnU-Net在脑病变分割中表现优异，但由于依赖复杂、设计臃肿，临床部署困难。因此需要一个更轻便、模块化的解决方案。

Method: StrokeSeg将预处理、推理和后处理解耦：预处理使用Anima工具箱并支持BIDS标准输出，推理采用ONNX Runtime并使用Float16量化（模型体积减少约50%），提供图形界面和命令行接口，支持Python脚本和独立Windows可执行文件分发。

Result: 在300例亚急性和慢性卒中患者数据上测试，StrokeSeg的分割性能与原始PyTorch流程相当（Dice差异<10⁻³）。

Conclusion: 高性能的研究级分割流程可以成功转化为便携、临床可用的工具，StrokeSeg为临床转化提供了高效、灵活的解决方案。

Abstract: Deep learning frameworks such as nnU-Net achieve state-of-the-art performance
in brain lesion segmentation but remain difficult to deploy clinically due to
heavy dependencies and monolithic design. We introduce \textit{StrokeSeg}, a
modular and lightweight framework that translates research-grade stroke lesion
segmentation models into deployable applications. Preprocessing, inference, and
postprocessing are decoupled: preprocessing relies on the Anima toolbox with
BIDS-compliant outputs, and inference uses ONNX Runtime with \texttt{Float16}
quantisation, reducing model size by about 50\%. \textit{StrokeSeg} provides
both graphical and command-line interfaces and is distributed as Python scripts
and as a standalone Windows executable. On a held-out set of 300 sub-acute and
chronic stroke subjects, segmentation performance was equivalent to the
original PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that
high-performing research pipelines can be transformed into portable, clinically
usable tools.

</details>


### [50] [A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset](https://arxiv.org/abs/2510.24379)
*Zhuangfan Huang,Xiaosong Li,Gao Wang,Tao Ye,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种亮度感知的多尺度网络（MLSN），用于在复杂光照环境下进行偏振图像融合，通过引入亮度分支和亮度增强模块，有效融合S0和DOLP图像，提升融合质量，并发布了包含1000对图像的新数据集MSP。


<details>
  <summary>Details</summary>
Motivation: 偏振图像在不同光照条件下存在固有对比度差异，现有方法难以有效融合S0和DOLP图像以保留表面纹理和材料特性，因此需要一种能适应复杂亮度环境的融合方法。

Method: 提出亮度感知多尺度网络（MLSN），在编码器中引入多尺度空间权重矩阵动态注入亮度信息，在瓶颈层设计全局-局部特征融合机制，解码器中加入亮度增强模块实现非线性亮度校正，并构建新数据集MSP用于训练与验证。

Result: 在MSP、PIF和GAND数据集上实验表明，MLSN在主观和客观评价中均优于现有方法，MS-SSIM和SD指标平均分别提升8.57%至63.53%不等。

Conclusion: MLSN能有效应对复杂光照下的偏振图像融合挑战，显著提升融合图像的质量与细节表现，具有良好的应用前景。

Abstract: Polarization image fusion combines S0 and DOLP images to reveal surface
roughness and material properties through complementary texture features, which
has important applications in camouflage recognition, tissue pathology
analysis, surface defect detection and other fields. To intergrate
coL-Splementary information from different polarized images in complex
luminance environment, we propose a luminance-aware multi-scale network (MLSN).
In the encoder stage, we propose a multi-scale spatial weight matrix through a
brightness-branch , which dynamically weighted inject the luminance into the
feature maps, solving the problem of inherent contrast difference in polarized
images. The global-local feature fusion mechanism is designed at the bottleneck
layer to perform windowed self-attention computation, to balance the global
context and local details through residual linking in the feature dimension
restructuring stage. In the decoder stage, to further improve the adaptability
to complex lighting, we propose a Brightness-Enhancement module, establishing
the mapping relationship between luminance distribution and texture features,
realizing the nonlinear luminance correction of the fusion result. We also
present MSP, an 1000 pairs of polarized images that covers 17 types of indoor
and outdoor complex lighting scenes. MSP provides four-direction polarization
raw maps, solving the scarcity of high-quality datasets in polarization image
fusion. Extensive experiment on MSP, PIF and GAND datasets verify that the
proposed MLSN outperms the state-of-the-art methods in subjective and objective
evaluations, and the MS-SSIM and SD metircs are higher than the average values
of other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,
respectively. The source code and dataset is avalable at
https://github.com/1hzf/MLS-UNet.

</details>


### [51] [When are radiology reports useful for training medical image classifiers?](https://arxiv.org/abs/2510.24385)
*Herman Bergström,Zhongqi Yue,Fredrik D. Johansson*

Main category: cs.CV

TL;DR: 本研究系统探讨了在医学图像分类任务中如何利用放射学报告进行预训练和微调，发现在标签与文本强相关时使用报告有益，而显式图文对齐可能在弱相关场景下产生负面影响；微调阶段引入报告可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 放射学报告包含丰富的专家标注信息，但目前研究多局限于诊断标签的提取，忽视了与文本弱相关的任务，因此需要系统探索报告在不同训练阶段和任务中的作用。

Method: 在预训练和微调阶段系统性地评估放射学报告对多种诊断和预后任务的影响，并分析不同训练数据规模下的表现。

Result: 1) 预训练时利用报告对文本中明确包含标签的任务有益，但显式图文对齐在标签与文本关联弱时有害；2) 微调时结合报告能带来显著提升，甚至超过预训练方法的效果。

Conclusion: 报告在下游任务标签与文本内容高度相关时具有实用价值，尤其在微调阶段效果显著，研究揭示了当前利用特权文本数据的空白并提供了实践指导。

Abstract: Medical images used to train machine learning models are often accompanied by
radiology reports containing rich expert annotations. However, relying on these
reports as inputs for clinical prediction requires the timely manual work of a
trained radiologist. This raises a natural question: when can radiology reports
be leveraged during training to improve image-only classification? Prior works
are limited to evaluating pre-trained image representations by fine-tuning them
to predict diagnostic labels, often extracted from reports, ignoring tasks with
labels that are weakly associated with the text. To address this gap, we
conduct a systematic study of how radiology reports can be used during both
pre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,
12-month readmission), and under varying training set sizes. Our findings
reveal that: (1) Leveraging reports during pre-training is beneficial for
downstream classification tasks where the label is well-represented in the
text; however, pre-training through explicit image-text alignment can be
detrimental in settings where it's not; (2) Fine-tuning with reports can lead
to significant improvements and even have a larger impact than the pre-training
method in certain settings. These results provide actionable insights into when
and how to leverage privileged text data to train medical image classifiers
while highlighting gaps in current research.

</details>


### [52] [Unsupervised Detection of Post-Stroke Brain Abnormalities](https://arxiv.org/abs/2510.24398)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: 该研究评估了一种基于流的生成模型REFLECT，用于在中风患者中无监督检测局灶性和非病灶性异常。通过在健康对照（IXI）和中风数据（ATLAS）上训练模型，发现使用完全健康解剖数据训练的模型能更有效地识别结构异常，包括梗死区域和继发性萎缩等变化。


<details>
  <summary>Details</summary>
Motivation: 现有的监督分割方法难以充分捕捉中风后MRI中的继发性结构变化（如脑萎缩和脑室扩大），需要一种能够自动检测多种异常类型的无监督方法。

Method: 采用基于流的生成模型REFLECT，分别在ATLAS（无病灶切片）和IXI（健康对照）数据集上进行训练，并在ATLAS测试集上通过双专家中心切片标注和自由响应ROC分析评估异常图的性能。

Result: 在ATLAS测试集中，IXI训练的模型在病灶分割（Dice = 0.37 vs 0.27）和非病灶异常检测敏感性（FROC = 0.62 vs 0.43）方面均优于ATLAS训练的模型。

Conclusion: 在完全健康的解剖数据上训练模型能更好建模正常变异，从而实现对中风后各种结构性异常更广泛且可靠的无监督检测。

Abstract: Post-stroke MRI not only delineates focal lesions but also reveals secondary
structural changes, such as atrophy and ventricular enlargement. These
abnormalities, increasingly recognised as imaging biomarkers of recovery and
outcome, remain poorly captured by supervised segmentation methods. We evaluate
REFLECT, a flow-based generative model, for unsupervised detection of both
focal and non-lesional abnormalities in post-stroke patients. Using dual-expert
central-slice annotations on ATLAS data, performance was assessed at the object
level with Free-Response ROC analysis for anomaly maps. Two models were trained
on lesion-free slices from stroke patients (ATLAS) and on healthy controls
(IXI) to test the effect of training data. On ATLAS test subjects, the
IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and
improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).
Training on fully healthy anatomy improves the modelling of normal variability,
enabling broader and more reliable detection of structural abnormalities.

</details>


### [53] [50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon](https://arxiv.org/abs/2510.24413)
*Ali Ahmad Faour,Nabil Amacha,Ali J. Ghandour*

Main category: cs.CV

TL;DR: 本研究提出一种无需传感器的方法，结合开源卫星影像、先进的水体分割和机器学习，近实时估算黎巴嫩Qaraaoun水库的表面积和蓄水量。


<details>
  <summary>Details</summary>
Motivation: 由于传感器频繁故障且维护能力有限，传统监测方法不可靠，因此需要一种可持续、低成本的水库蓄水量监测方法。

Method: 利用Sentinel-2和Landsat影像，通过新提出的水体分割指数提取水体范围，并基于支持向量回归（SVR）模型，结合水库测深数据训练模型，仅依靠卫星反演的表面积估算库容。

Result: 水体分割精度超过95%；SVR模型误差低于水库总容量的1.5%，决定系数超过0.98。

Conclusion: 该方法具有高鲁棒性和成本效益，可实现不依赖传感器的连续水库监测，适用于其他水体，并为气候变化研究提供长达50年的时序数据。

Abstract: The sustainable management of the Qaraaoun Reservoir, the largest surface
water body in Lebanon located in the Bekaa Plain, depends on reliable
monitoring of its storage volume despite frequent sensor malfunctions and
limited maintenance capacity. This study introduces a sensor-free approach that
integrates open-source satellite imagery, advanced water-extent segmentation,
and machine learning to estimate the reservoir surface area and volume in near
real time. Sentinel-2 and Landsat images are processed, where surface water is
delineated using a newly proposed water segmentation index. A machine learning
model based on Support Vector Regression (SVR) is trained on a curated dataset
that includes water surface area, water level, and water volume calculations
using a reservoir bathymetry survey. The model is then able to estimate
reservoir volume relying solely on surface area extracted from satellite
imagery, without the need for ground measurements. Water segmentation using the
proposed index aligns with ground truth for more than 95 percent of the
shoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR
performance with error under 1.5 percent of full reservoir capacity and
coefficients of determination exceeding 0.98. These results demonstrate the
robustness and cost-effectiveness of the method, offering a practical solution
for continuous, sensor-independent monitoring of reservoir storage. The
proposed methodology can be replicated for other water bodies, and the
resulting 50 years of time-series data is valuable for research on climate
change and environmental patterns.

</details>


### [54] [XAI Evaluation Framework for Semantic Segmentation](https://arxiv.org/abs/2510.24414)
*Reem Hammoud,Abdul karim Gizzini,Ali J. Ghandour*

Main category: cs.CV

TL;DR: 提出了一种针对语义分割任务中可解释人工智能（XAI）的全面系统评估框架，结合像素级评估和专用指标，考虑空间与上下文复杂性，验证了CAM类方法的有效性、鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 语义分割任务中的XAI评估方法尚不充分，缺乏兼顾空间和上下文复杂性的系统评估体系，难以确保模型的透明性与可信度。

Method: 设计了一个专用于语义分割的XAI评估框架，采用像素级评估策略和精心设计的度量指标，综合考虑空间与上下文信息，并在最新改进的CAM类XAI方法上进行验证。

Result: 实验结果表明该框架具有高效性、鲁棒性和可靠性，能够提供细粒度的可解释性洞察。

Conclusion: 所提框架有助于推动透明、可信和可问责的语义分割模型的发展，填补了XAI在该领域系统化评估的空白。

Abstract: Ensuring transparency and trust in artificial intelligence (AI) models is
essential, particularly as they are increasingly applied in safety-critical and
high-stakes domains. Explainable AI (XAI) has emerged as a promising approach
to address this challenge, yet the rigorous evaluation of XAI methods remains
crucial for optimizing the trade-offs between model complexity, predictive
performance, and interpretability. While extensive progress has been achieved
in evaluating XAI techniques for classification tasks, evaluation strategies
tailored to semantic segmentation remain relatively underexplored. This work
introduces a comprehensive and systematic evaluation framework specifically
designed for assessing XAI in semantic segmentation, explicitly accounting for
both spatial and contextual task complexities. The framework employs
pixel-level evaluation strategies and carefully designed metrics to provide
fine-grained interpretability insights. Simulation results using recently
adapted class activation mapping (CAM)-based XAI schemes demonstrate the
efficiency, robustness, and reliability of the proposed methodology. These
findings contribute to advancing transparent, trustworthy, and accountable
semantic segmentation models.

</details>


### [55] [Deeply-Conditioned Image Compression via Self-Generated Priors](https://arxiv.org/abs/2510.24437)
*Zhineng Zhao,Zhihai He,Zikun Zhou,Siwei Ma,Yaowei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于功能分解的深度条件图像压缩框架DCIC-sgp，通过自生成先验对图像结构进行建模，并在压缩流程中深度调节分析变换，有效分离图像的全局结构与局部纹理，显著减少低码率下的几何变形，提升了率失真性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习型图像压缩方法难以有效建模自然图像中全局结构与局部纹理的复杂相关性，导致低码率下出现严重几何变形。

Method: 提出DCIC-sgp框架，首先编码自生成的强先验以捕捉图像的结构主干，并将该先验用于深度调节整个压缩流程（尤其是分析变换），使模型能专注于残差细节的表示，实现信息流的有效解耦。

Result: 实验表明，该方法在Kodak、CLIC和Tecnick数据集上相比VVC测试模型VTM-12.1分别取得14.4%、15.7%和15.1%的BD-rate降低，且视觉上显著缓解了低码率下的几何失真。

Conclusion: 通过自生成先验的深度条件机制，实现了图像结构与细节的有效分离，提升了压缩效率与重建质量，尤其在低码率下表现优越。

Abstract: Learned image compression (LIC) has shown great promise for achieving high
rate-distortion performance. However, current LIC methods are often limited in
their capability to model the complex correlation structures inherent in
natural images, particularly the entanglement of invariant global structures
with transient local textures within a single monolithic representation. This
limitation precipitates severe geometric deformation at low bitrates. To
address this, we introduce a framework predicated on functional decomposition,
which we term Deeply-Conditioned Image Compression via self-generated priors
(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior
to encapsulate the image's structural backbone. This prior is subsequently
utilized not as mere side-information, but to holistically modulate the entire
compression pipeline. This deep conditioning, most critically of the analysis
transform, liberates it to dedicate its representational capacity to the
residual, high-entropy details. This hierarchical, dependency-driven approach
achieves an effective disentanglement of information streams. Our extensive
experiments validate this assertion; visual analysis demonstrates that our
method substantially mitigates the geometric deformation artifacts that plague
conventional codecs at low bitrates. Quantitatively, our framework establishes
highly competitive performance, achieving significant BD-rate reductions of
14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,
and Tecnick datasets.

</details>


### [56] [Rethinking Visual Intelligence: Insights from Video Pretraining](https://arxiv.org/abs/2510.24448)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: 视频扩散模型（VDM）通过时空数据预训练展现出比大语言模型更强的视觉任务数据效率，表明其在构建视觉基础模型方面具有潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在语言领域表现出色，但在视觉领域的组合理解、样本效率和通用问题解决方面仍存在挑战，需要探索更有效的视觉基础模型方法。

Method: 研究使用预训练的视频扩散模型（VDM）与大语言模型（LLM），在自然模态下通过轻量级适配器进行控制性评估，比较它们在多个视觉任务上的表现。

Result: 在ARC-AGI、ConceptARC、视觉游戏、路径规划和元胞自动机等基准上，VDM相比LLM展现出更高的数据效率。

Conclusion: 视频预训练提供的归纳偏置有助于提升视觉模型的泛化能力和任务适应性，是迈向通用视觉基础模型的有前景方向。

Abstract: Large language models (LLMs) have demonstrated that large-scale pretraining
enables systems to adapt rapidly to new problems with little supervision in the
language domain. This success, however, has not translated as effectively to
the visual domain, where models, including LLMs, continue to struggle with
compositional understanding, sample efficiency, and general-purpose
problem-solving. We investigate Video Diffusion Models (VDMs) as a promising
direction for bridging this gap. Pretraining on spatiotemporal data endows
these models with strong inductive biases for structure and dynamics, which we
hypothesize can support broad task adaptability. To test this, we design a
controlled evaluation in which both a pretrained LLM and a pretrained VDM are
equipped with lightweight adapters and presented with tasks in their natural
modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,
route planning, and cellular automata, VDMs demonstrate higher data efficiency
than their language counterparts. Taken together, our results indicate that
video pretraining offers inductive biases that support progress toward visual
foundation models.

</details>


### [57] [A Critical Study towards the Detection of Parkinsons Disease using ML Technologies](https://arxiv.org/abs/2510.24456)
*Vivek Chetia,Abdul Taher Khan,Rahish Gogoi,David Kapsian Khual,Purnendu Bikash,Sajal Saha*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的茶叶病害分类与受损区域检测方法，使用SSD MobileNet V2、Faster R-CNN ResNet50 V1和Mask R-CNN模型进行茶叶病害（红锈病、Helopeltis、红蜘蛛螨）的检测与分割。实验结果表明，Faster R-CNN表现更优，mAP达到25%。


<details>
  <summary>Details</summary>
Motivation: 茶叶病害严重影响产量和质量，传统人工检测效率低且准确性差，因此需要一种自动化的高精度检测方法。

Method: 采用SSD MobileNet V2和Faster R-CNN ResNet50 V1进行病害目标检测，并使用Mask R-CNN实现叶片病害区域的实例分割，同时提出了自定义方法计算叶片受损面积。

Result: SSD MobileNet V2在IOU 0.50:0.95范围内的mAP为20.9%，而Faster R-CNN ResNet50 V1的mAP为25%，表现更优；Mask R-CNN成功实现了病害区域的像素级分割并估算了受损面积。

Conclusion: Faster R-CNN ResNet50 V1在茶叶病害检测中优于SSD MobileNet V2，结合Mask R-CNN可有效实现病害识别与受损区域量化，具有实际应用潜力。

Abstract: The proposed solution is Deep Learning Technique that will be able classify
three types of tea leaves diseases from which two diseases are caused by the
pests and one due to pathogens (infectious organisms) and environmental
conditions and also show the area damaged by a disease in leaves. Namely Red
Rust, Helopeltis and Red spider mite respectively. In this paper we have
evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for
the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU
range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.
While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95
and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than
SSD. Also used Mask R-CNN for Object Instance Segmentation where we have
implemented our custom method to calculate the damaged diseased portion of
leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red
Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.

</details>


### [58] [Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras](https://arxiv.org/abs/2510.24464)
*Charles Javerliat,Pierre Raimbaud,Guillaume Lavoué*

Main category: cs.CV

TL;DR: Kineo是一个无需相机标定、全自动的无标记多视角动作捕捉管道，利用2D关键点实现高精度3D重建，并在真实场景中高效运行。


<details>
  <summary>Details</summary>
Motivation: 现有无标记动作捕捉方法依赖精确相机标定，限制了非专家和野外使用；而现有的免标定方法则存在计算成本高和重建精度低的问题。

Method: Kineo利用现成检测器提取2D关键点，通过置信度驱动的时空采样策略和基于图的全局优化，联合完成相机标定（包括畸变系数）与度量尺度下的3D关键点及稠密点云重建，并引入成对重投影共识分数评估重建可靠性。

Result: 在EgoHumans和Human3.6M数据集上，相比先前免标定方法，相机平移误差减少83-85%，角度误差减少86-92%，世界平均每关节位置误差（W-MPJPE）减少83-91%；处理速度优于视频时长（如36分钟处理80分钟视频）。

Conclusion: Kineo实现了高精度、高效率、完全自动化的免标定多视角动作捕捉，显著优于现有方法，且代码开源促进可复现性和实际应用。

Abstract: Markerless multiview motion capture is often constrained by the need for
precise camera calibration, limiting accessibility for non-experts and
in-the-wild captures. Existing calibration-free approaches mitigate this
requirement but suffer from high computational cost and reduced reconstruction
accuracy.
  We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf
detectors to simultaneously calibrate cameras, including Brown-Conrady
distortion coefficients, and reconstruct 3D keypoints and dense scene point
maps at metric scale. A confidence-driven spatio-temporal keypoint sampling
strategy, combined with graph-based global optimization, ensures robust
calibration at a fixed computational cost independent of sequence length. We
further introduce a pairwise reprojection consensus score to quantify 3D
reconstruction reliability for downstream tasks.
  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements
over prior calibration-free methods. Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.
  Kineo is also efficient in real-world scenarios, processing multi-view
sequences faster than their duration in specific configuration (e.g., 36min to
process 1h20min of footage). The full pipeline and evaluation code are openly
released to promote reproducibility and practical adoption at
https://liris-xr.github.io/kineo/.

</details>


### [59] [Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling](https://arxiv.org/abs/2510.24474)
*Kyungmin Lee,Sihyun Yu,Jinwoo Shin*

Main category: cs.CV

TL;DR: 本文提出了Decoupled MeanFlow，一种无需修改架构即可将去噪生成模型（如扩散模型）转换为流图模型的解码策略，显著减少采样步数（仅需1-4步），并在ImageNet上实现了优于现有方法的FID分数，同时推理速度提升超过100倍。


<details>
  <summary>Details</summary>
Motivation: 现有的去噪生成模型因离散化误差需要大量采样步骤，尽管流图可缓解该问题，但其训练通常需要特定架构修改，限制了与预训练模型的兼容性。因此，亟需一种兼容且高效的转换方法。

Method: 提出Decoupled MeanFlow，通过在扩散Transformer的最后几层引入对后续时间步的条件控制，将预训练的流模型直接转化为流图模型，并结合增强训练技术，实现极少数步数下的高质量生成。

Result: 在ImageNet 256x256和512x512上，1步生成的FID分别为2.16和2.12，4步时降至1.51和1.68，性能接近流模型，且推理速度快100倍以上。

Conclusion: 训练流模型再转换为流图比从零训练流图更高效有效，Decoupled MeanFlow为快速高质量生成提供了一种兼容性强、实用的新路径。

Abstract: Denoising generative models, such as diffusion and flow-based models, produce
high-quality samples but require many denoising steps due to discretization
error. Flow maps, which estimate the average velocity between timesteps,
mitigate this error and enable faster sampling. However, their training
typically demands architectural changes that limit compatibility with
pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding
strategy that converts flow models into flow map models without architectural
modifications. Our method conditions the final blocks of diffusion transformers
on the subsequent timestep, allowing pretrained flow models to be directly
repurposed as flow maps. Combined with enhanced training techniques, this
design enables high-quality generation in as few as 1 to 4 steps. Notably, we
find that training flow models and subsequently converting them is more
efficient and effective than training flow maps from scratch. On ImageNet
256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,
respectively, surpassing prior art by a large margin. Furthermore, we achieve
FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the
performance of flow models while delivering over 100x faster inference.

</details>


### [60] [Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs](https://arxiv.org/abs/2510.24514)
*Huanyu Zhang,Wenshan Wu,Chengzu Li,Ning Shang,Yan Xia,Yangyu Huang,Yifan Zhang,Li Dong,Zhang Zhang,Liang Wang,Tieniu Tan,Furu Wei*

Main category: cs.CV

TL;DR: 本文提出了Latent Sketchpad框架，通过在多模态大语言模型中引入内部视觉草图板，使其能够在推理过程中生成视觉隐变量，实现文本与视觉思维的交织，从而提升复杂场景下的视觉规划与想象力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉理解方面表现优异，但在需要视觉规划和想象的复杂任务中仍存在困难。受人类通过绘图进行视觉思维启发，研究旨在赋予MLLMs内部视觉生成能力以增强其推理过程。

Method: 提出Latent Sketchpad框架，包含一个上下文感知视觉头（Context-Aware Vision Head）用于自回归生成视觉隐表示，并结合预训练的Sketch Decoder将隐变量转化为可解释的草图图像，实现文本推理与视觉生成的交替进行。

Result: 在新构建的MazePlanning数据集上实验表明，Latent Sketchpad在多个前沿MLLM上实现了与基线相当甚至更优的推理性能，并能跨不同模型（如Gemma3和Qwen2.5-VL）良好泛化。

Conclusion: 通过扩展模型的文本推理至视觉思维，该框架不仅提升了MLLM在复杂任务中的表现，还为更丰富的人机交互和广泛应用提供了新可能。

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding,
they often struggle in complex scenarios that require visual planning and
imagination. Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad. The internal visual
representations of MLLMs have traditionally been confined to perceptual
understanding. We repurpose them to support generative visual thought without
compromising reasoning ability. Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process. It allows the model to interleave textual reasoning with the
generation of visual latents. These latents guide the internal thought process
and can be translated into sketch images for interpretability. To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images. We evaluate the framework on our new dataset
MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
delivers comparable or even superior reasoning performance to their backbone.
It further generalizes across distinct frontier MLLMs, including Gemma3 and
Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications. More details and resources are available on our project
page: https://latent-sketchpad.github.io/.

</details>


### [61] [OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents](https://arxiv.org/abs/2510.24563)
*Hongrui Jia,Jitong Liao,Xi Zhang,Haiyang Xu,Tianbao Xie,Chaoya Jiang,Ming Yan,Si Liu,Wei Ye,Fei Huang*

Main category: cs.CV

TL;DR: 本文提出了OSWorld-MCP，首个全面且公平的基准，用于评估多模态智能体在真实环境中的工具调用、GUI操作和决策能力。通过自动化代码生成管道构建高质量工具集，并验证了现有智能体在引入MCP工具后任务成功率有所提升，但工具调用率仍较低，表明仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注GUI交互，忽视了工具调用能力（如MCP），导致对具备工具调用功能的智能体评估不公平。因此需要一个综合评估工具调用、GUI操作和决策能力的新基准。

Method: 设计了一种新颖的自动化代码生成流程来创建工具，并结合精选的现有工具；经过严格人工验证，构建包含158个高质量工具的数据集（覆盖7个常见应用）；在真实环境中对最先进的多模态智能体进行广泛评估，明确衡量其MCP工具使用能力。

Result: 实验表明，集成MCP工具能提升任务成功率（如OpenAI o3从8.3%升至20.4%，Claude 4 Sonnet从40.1%升至43.3%），但最强模型的工具调用率仅为36.3%，说明工具利用仍不足。

Conclusion: OSWorld-MCP为评估多模态智能体在复杂、工具辅助环境中的表现设立了新标准，强调了评估工具调用能力的重要性，并推动未来在该方向上的改进。

Abstract: With advances in decision-making and reasoning capabilities, multimodal
agents show strong potential in computer application scenarios. Past
evaluations have mainly assessed GUI interaction skills, while tool invocation
abilities, such as those enabled by the Model Context Protocol (MCP), have been
largely overlooked. Comparing agents with integrated tool invocation to those
evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,
the first comprehensive and fair benchmark for assessing computer-use agents'
tool invocation, GUI operation, and decision-making abilities in a real-world
environment. We design a novel automated code-generation pipeline to create
tools and combine them with a curated selection from existing tools. Rigorous
manual validation yields 158 high-quality tools (covering 7 common
applications), each verified for correct functionality, practical
applicability, and versatility. Extensive evaluations of state-of-the-art
multimodal agents on OSWorld-MCP show that MCP tools generally improve task
success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%
to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of
assessing tool invocation capabilities. However, even the strongest models have
relatively low tool invocation rates, Only 36.3%, indicating room for
improvement and highlighting the benchmark's challenge. By explicitly measuring
MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents
and sets a new standard for evaluating performance in complex, tool-assisted
environments. Our code, environment, and data are publicly available at
https://osworld-mcp.github.io.

</details>


### [62] [Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT](https://arxiv.org/abs/2510.24579)
*Xu Jiang,Huiying Pan,Ligen Shi,Jianing Sun,Wenfeng Xu,Xing Zhao*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的散射伪影校正方法，结合物理先验知识和Kolmogorov-Arnold网络（KAN）有效校正锥束CT中的散射伪影。


<details>
  <summary>Details</summary>
Motivation: 锥束CT在数据采集过程中易受散射影响，导致图像中CT值偏差和组织对比度下降，影响诊断准确性。

Method: 利用投影域中散射点概率密度分布具有旋转对称性的物理特性，采用高斯径向基函数（RBF）建模点散射函数，并将其嵌入Kolmogorov-Arnold网络（KAN）层中，实现对高维散射特征的有效非线性映射学习。

Result: 在合成数据和真实扫描实验中验证了该方法的有效性，结果显示该模型能有效校正重建图像中的散射伪影，并在定量指标上优于现有方法。

Conclusion: 所提出的方法通过融合物理先验与KAN的强非线性建模能力，显著提升了散射校正的精度，具有良好的应用前景。

Abstract: Cone-beam CT (CBCT) employs a flat-panel detector to achieve
three-dimensional imaging with high spatial resolution. However, CBCT is
susceptible to scatter during data acquisition, which introduces CT value bias
and reduced tissue contrast in the reconstructed images, ultimately degrading
diagnostic accuracy. To address this issue, we propose a deep learning-based
scatter artifact correction method inspired by physical prior knowledge.
Leveraging the fact that the observed point scatter probability density
distribution exhibits rotational symmetry in the projection domain. The method
uses Gaussian Radial Basis Functions (RBF) to model the point scatter function
and embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides
efficient nonlinear mapping capabilities for learning high-dimensional scatter
features. By incorporating the physical characteristics of the scattered photon
distribution together with the complex function mapping capacity of KAN, the
model improves its ability to accurately represent scatter. The effectiveness
of the method is validated through both synthetic and real-scan experiments.
Experimental results show that the model can effectively correct the scatter
artifacts in the reconstructed images and is superior to the current methods in
terms of quantitative metrics.

</details>


### [63] [A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries](https://arxiv.org/abs/2510.24640)
*Xin Zhang,Yuqi Song,Fei Zuo*

Main category: cs.CV

TL;DR: 提出一种基于双分支卷积神经网络的面部伪造检测方法，结合空间域和频率域特征，通过通道注意力机制融合，并设计FSC Loss损失函数提升分类可分性与鲁棒性，在DiFF基准上表现优于人类平均准确率。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的发展导致逼真伪造人脸图像泛滥，带来 misinformation、身份欺诈等安全威胁，亟需鲁棒且通用的伪造检测技术以维护AI安全与媒体可信度。

Method: 采用双分支CNN架构，RGB分支提取语义信息，频率分支捕捉生成模型难以消除的高频伪影；引入通道注意力模块自适应融合双域特征；设计FSC Loss（结合focal loss、监督对比损失和频率中心间隔损失）优化学习过程。

Result: 在包含文本生成图像、图像到图像、换脸和面部编辑四类伪造方法的DiFF基准上取得优异性能，跨类别检测表现优于人类平均水平。

Conclusion: 所提方法能有效检测多种面部伪造技术，具备良好的泛化性与鲁棒性，对构建安全AI系统具有重要意义。

Abstract: The rapid advancement of generative AI has enabled the creation of highly
realistic forged facial images, posing significant threats to AI security,
digital media integrity, and public trust. Face forgery techniques, ranging
from face swapping and attribute editing to powerful diffusion-based image
synthesis, are increasingly being used for malicious purposes such as
misinformation, identity fraud, and defamation. This growing challenge
underscores the urgent need for robust and generalizable face forgery detection
methods as a critical component of AI security infrastructure. In this work, we
propose a novel dual-branch convolutional neural network for face forgery
detection that leverages complementary cues from both spatial and frequency
domains. The RGB branch captures semantic information, while the frequency
branch focuses on high-frequency artifacts that are difficult for generative
models to suppress. A channel attention module is introduced to adaptively fuse
these heterogeneous features, highlighting the most informative channels for
forgery discrimination. To guide the network's learning process, we design a
unified loss function, FSC Loss, that combines focal loss, supervised
contrastive loss, and a frequency center margin loss to enhance class
separability and robustness. We evaluate our model on the DiFF benchmark, which
includes forged images generated from four representative methods:
text-to-image, image-to-image, face swap, and face edit. Our method achieves
strong performance across all categories and outperforms average human
accuracy. These results demonstrate the model's effectiveness and its potential
contribution to safeguarding AI ecosystems against visual forgery attacks.

</details>


### [64] [Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology](https://arxiv.org/abs/2510.24653)
*Veronica Thai,Rui Li,Meng Ling,Shuning Jiang,Jeremy Wolfe,Raghu Machiraju,Yan Hu,Zaibo Li,Anil Parwani,Jian Chen*

Main category: cs.CV

TL;DR: 本研究提出了PathoGaze1.0，一个包含19名病理学家在癌症诊断中解读397张全切片图像时的眼动、鼠标交互、视口导航和诊断决策数据的综合性行为数据集，旨在揭示诊断错误与不一致性的原因，并提升病理学家及AI系统的训练。


<details>
  <summary>Details</summary>
Motivation: 病理学家在解读千兆像素级全切片图像时诊断准确率平均仅为70%，且双人复核未能显著提高一致性，领域内缺乏解释此类问题的行为数据。

Method: 通过名为PTAH的应用导向测试平台，采集了18.69小时的眼动追踪、鼠标交互、刺激跟踪、视口导航和诊断决策（EMSVD）数据，涵盖19名病理学家对397张WSI的完整诊断流程，强调生态效度。

Result: 共记录171,909次注视、263,320次眼跳和1,867,362次鼠标交互事件，形成了高质量、高生态效度的行为数据集PathoGaze1.0，并已公开共享。

Conclusion: 该数据集有助于理解病理诊断中的认知过程与错误机制，可支持病理学家培训及AI辅助系统的开发，推动人机协同诊断的发展。

Abstract: Interpretation of giga-pixel whole-slide images (WSIs) is an important but
difficult task for pathologists. Their diagnostic accuracy is estimated to
average around 70%. Adding a second pathologist does not substantially improve
decision consistency. The field lacks adequate behavioral data to explain
diagnostic errors and inconsistencies. To fill in this gap, we present
PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual
search and decision-making processes of the full diagnostic workflow during
cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse
interaction, stimulus tracking, viewport navigation, and diagnostic decision
data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data
collection process emphasizes ecological validity through an
application-grounded testbed, called PTAH. In total, we recorded 171,909
fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In
addition, such data could also be used to improve the training of both
pathologists and AI systems that might support human experts. All experiments
were preregistered at https://osf.io/hj9a7, and the complete dataset along with
analysis code is available at https://go.osu.edu/pathogaze.

</details>


### [65] [Group Relative Attention Guidance for Image Editing](https://arxiv.org/abs/2510.24657)
*Xuanpu Zhang,Xuesong Niu,Ruidong Chen,Dan Song,Jianhao Zeng,Penghui Du,Haoxiang Cao,Kai Wu,An-an Liu*

Main category: cs.CV

TL;DR: 提出了一种名为Group Relative Attention Guidance (GRAG) 的新方法，用于在Diffusion-in-Transformer模型中实现对图像编辑强度的连续、细粒度控制，无需额外调参，且仅需少量代码即可集成。


<details>
  <summary>Details</summary>
Motivation: 现有基于DiT的图像编辑方法缺乏对编辑程度的有效控制，难以实现定制化结果。因此需要一种能精细调节编辑强度的方法。

Method: 通过分析DiT中的MM-Attention机制，发现Query和Key共享仅依赖于层的偏置向量，将其视为模型固有行为，并利用token与其偏置之间的差值作为内容相关信号。GRAG通过对这些差值进行重加权，调节模型对输入图像与编辑指令间关系的关注度，从而实现编辑强度控制。

Result: 实验表明，GRAG可在多个现有编辑框架中以少至四行代码集成，显著提升编辑质量，并相比Classifier-Free Guidance提供更平滑、精确的编辑控制。

Conclusion: GRAG是一种简单、有效且易于集成的方法，为基于DiT的图像编辑提供了无需微调的连续强度控制方案。

Abstract: Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.

</details>


### [66] [SAGE: Structure-Aware Generative Video Transitions between Diverse Clips](https://arxiv.org/abs/2510.24667)
*Mia Kan,Yilin Liu,Niloy Mitra*

Main category: cs.CV

TL;DR: 提出了一种名为SAGE的零样本视频过渡方法，通过结合结构引导与生成合成，实现跨不同视频片段的平滑、语义一致的过渡。


<details>
  <summary>Details</summary>
Motivation: 现有视频过渡方法在处理具有大时间间隔或显著语义差异的视频片段时，难以生成内容感知且视觉连贯的过渡效果。

Method: 借鉴艺术创作流程，利用轮廓对齐和显著特征插值等策略，结合线图和运动流提供的结构引导与生成模型进行视频过渡合成。

Result: 实验表明，SAGE在定量指标和用户研究中均优于现有的经典和生成式基线方法，能更有效地生成多样视频片段间的高质量过渡。

Conclusion: SAGE是一种无需微调的零样本方法，能够生成结构保持良好、视觉连贯且语义一致的视频过渡，适用于专业场景。

Abstract: Video transitions aim to synthesize intermediate frames between two clips,
but naive approaches such as linear blending introduce artifacts that limit
professional use or break temporal coherence. Traditional techniques
(cross-fades, morphing, frame interpolation) and recent generative inbetweening
methods can produce high-quality plausible intermediates, but they struggle
with bridging diverse clips involving large temporal gaps or significant
semantic differences, leaving a gap for content-aware and visually coherent
transitions. We address this challenge by drawing on artistic workflows,
distilling strategies such as aligning silhouettes and interpolating salient
features to preserve structure and perceptual continuity. Building on this, we
propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot
approach that combines structural guidance, provided via line maps and motion
flow, with generative synthesis, enabling smooth, semantically consistent
transitions without fine-tuning. Extensive experiments and comparison with
current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate
that SAGE outperforms both classical and generative baselines on quantitative
metrics and user studies for producing transitions between diverse clips. Code
to be released on acceptance.

</details>


### [67] [MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection](https://arxiv.org/abs/2510.24688)
*Yun Zhang,Zhaoliang Zheng,Johnson Liu,Zhiyu Huang,Zewei Zhou,Zonglin Meng,Tianhui Cai,Jiaqi Ma*

Main category: cs.CV

TL;DR: 本文提出了MIC-BEV，一种基于Transformer的鸟瞰图感知框架，用于基础设施多摄像头3D目标检测，支持可变数量和异构参数的摄像头，并在传感器退化下表现出强鲁棒性；同时引入了合成数据集M2I，实验表明MIC-BEV在M2I和RoScenes数据集上均达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于相机的检测模型在多视角基础设施设置、不同摄像头配置、视觉输入退化和复杂道路布局等挑战下表现不佳，难以满足基础设施感知的需求。

Method: 提出MIC-BEV框架，采用Transformer架构，通过几何关系和潜在视觉线索融合多视角图像特征到BEV空间；设计图增强融合模块，并构建合成数据集M2I用于训练与评估。

Result: 在M2I和真实世界数据集RoScenes上的实验表明，MIC-BEV在3D目标检测中达到最先进的性能，并在极端天气和传感器退化等挑战条件下保持鲁棒性。

Conclusion: MIC-BEV在基础设施感知中展现出强大潜力，具备良好的灵活性和鲁棒性，适合实际部署。

Abstract: Infrastructure-based perception plays a crucial role in intelligent
transportation systems, offering global situational awareness and enabling
cooperative autonomy. However, existing camera-based detection models often
underperform in such scenarios due to challenges such as multi-view
infrastructure setup, diverse camera configurations, degraded visual inputs,
and various road layouts. We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection. MIC-BEV flexibly supports a variable number
of cameras with heterogeneous intrinsic and extrinsic parameters and
demonstrates strong robustness under sensor degradation. The proposed
graph-enhanced fusion module in MIC-BEV integrates multi-view image features
into the BEV space by exploiting geometric relationships between cameras and
BEV cells alongside latent visual cues. To support training and evaluation, we
introduce M2I, a synthetic dataset for infrastructure-based object detection,
featuring diverse camera configurations, road layouts, and environmental
conditions. Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection. It also remains robust under challenging conditions,
including extreme weather and sensor degradation. These results highlight the
potential of MIC-BEV for real-world deployment. The dataset and source code are
available at: https://github.com/HandsomeYun/MIC-BEV.

</details>


### [68] [Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?](https://arxiv.org/abs/2510.24709)
*Yihao Li,Saeed Salehi,Lyle Ungar,Konrad P. Kording*

Main category: cs.CV

TL;DR: 本文研究了视觉Transformer（ViT）在预训练过程中是否自然涌现出“对象绑定”能力，即模型能否将属于同一物体的图像块关联起来。通过相似性探针解码发现，自监督ViT能以超过90%的准确率识别图像块是否属于同一物体，且该能力在监督模型中较弱，表明其源于特定预训练目标而非架构本身。研究还发现这一信号存在于低维子空间并指导注意力机制，去除它会损害性能，说明对象绑定自然服务于预训练目标。


<details>
  <summary>Details</summary>
Motivation: 探究ViT是否在无显式设计的情况下自然具备对象绑定能力，特别是自监督与监督模型间的差异。

Method: 提出IsSameObject假设，使用相似性探针从ViT各层的patch embedding中解码对象绑定信息，并分析其在低维子空间中的表示及其对注意力的引导作用，通过消融实验验证其功能重要性。

Result: 1) 自监督ViT（如DINO、MAE、CLIP）能高精度（>90%）解码IsSameObject关系；2) 该能力在ImageNet监督模型中显著更弱；3) IsSameObject编码于对象特征之上的低维子空间；4) 该信号主动引导注意力；5) 消融实验显示去除该信号会降低下游性能并违背预训练目标。

Conclusion: ViT在自监督预训练下能自然涌现出对象绑定能力，挑战了ViT缺乏对象理解的观点，揭示了符号化的‘部分-整体’知识可在连接主义系统中自发形成。

Abstract: Object binding, the brain's ability to bind the many features that
collectively represent an object into a coherent whole, is central to human
cognition. It groups low-level perceptual features into high-level object
representations, stores those objects efficiently and compositionally in
memory, and supports human reasoning about individual object instances. While
prior work often imposes object-centric attention (e.g., Slot Attention)
explicitly to probe these benefits, it remains unclear whether this ability
naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they
could: recognizing which patches belong to the same object should be useful for
downstream prediction and thus guide attention. Motivated by the quadratic
nature of self-attention, we hypothesize that ViTs represent whether two
patches belong to the same object, a property we term IsSameObject. We decode
IsSameObject from patch embeddings across ViT layers using a similarity probe,
which reaches over 90% accuracy. Crucially, this object-binding capability
emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker
in ImageNet-supervised models, suggesting that binding is not a trivial
architectural artifact, but an ability acquired through specific pretraining
objectives. We further discover that IsSameObject is encoded in a
low-dimensional subspace on top of object features, and that this signal
actively guides attention. Ablating IsSameObject from model activations
degrades downstream performance and works against the learning objective,
implying that emergent object binding naturally serves the pretraining
objective. Our findings challenge the view that ViTs lack object binding and
highlight how symbolic knowledge of "which parts belong together" emerges
naturally in a connectionist system.

</details>


### [69] [Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance](https://arxiv.org/abs/2510.24711)
*Yujie Wei,Shiwei Zhang,Hangjie Yuan,Yujin Han,Zhekai Chen,Jiayu Wang,Difan Zou,Xihui Liu,Yingya Zhang,Yu Liu,Hongming Shan*

Main category: cs.CV

TL;DR: 本文提出ProMoE，一种用于扩散Transformer的两步路由MoE框架，通过条件路由和原型路由结合路由对比损失，提升视觉token的专家专业化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MoE在语言模型中成功，但在视觉模型（如DiT）中效果有限，主要因视觉token存在空间冗余和功能异质性，难以实现专家专业化。

Method: 提出ProMoE，采用两步路由机制：首先根据功能角色将图像token划分为条件与非条件集合（条件路由），然后基于可学习原型按语义内容细化条件token分配（原型路由）；引入基于相似性的隐空间专家分配，并设计路由对比损失以增强专家内聚性和专家间多样性。

Result: 在ImageNet上实验表明，ProMoE在Rectified Flow和DDPM训练目标下均优于当前最先进方法。

Conclusion: ProMoE通过显式路由引导和对比损失有效促进视觉MoE中的专家专业化，为扩散Transformer中的MoE应用提供了新思路。

Abstract: Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
capacity while preserving computational efficiency. Despite its notable success
in large language models (LLMs), existing attempts to apply MoE to Diffusion
Transformers (DiTs) have yielded limited gains. We attribute this gap to
fundamental differences between language and visual tokens. Language tokens are
semantically dense with pronounced inter-token variation, while visual tokens
exhibit spatial redundancy and functional heterogeneity, hindering expert
specialization in vision MoE. To this end, we present ProMoE, an MoE framework
featuring a two-step router with explicit routing guidance that promotes expert
specialization. Specifically, this guidance encourages the router to partition
image tokens into conditional and unconditional sets via conditional routing
according to their functional roles, and refine the assignments of conditional
image tokens through prototypical routing with learnable prototypes based on
semantic content. Moreover, the similarity-based expert allocation in latent
space enabled by prototypical routing offers a natural mechanism for
incorporating explicit semantic guidance, and we validate that such guidance is
crucial for vision MoE. Building on this, we propose a routing contrastive loss
that explicitly enhances the prototypical routing process, promoting
intra-expert coherence and inter-expert diversity. Extensive experiments on
ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
under both Rectified Flow and DDPM training objectives. Code and models will be
made publicly available.

</details>


### [70] [Uniform Discrete Diffusion with Metric Path for Video Generation](https://arxiv.org/abs/2510.24717)
*Haoge Deng,Ting Pan,Fan Zhang,Yang Liu,Zhuoyan Luo,Yufeng Cui,Wenxuan Wang,Chunhua Shen,Shiguang Shan,Zhaoxiang Zhang,Xinlong Wang*

Main category: cs.CV

TL;DR: 提出了一种名为URSA的离散视频生成框架，通过线性化度量路径和分辨率依赖的时间步移机制，实现了与连续方法相媲美的高性能，同时支持多种任务。


<details>
  <summary>Details</summary>
Motivation: 离散视频生成方法因误差累积和长时上下文不一致而落后于连续方法，需探索更有效的离散建模方式。

Method: 将视频生成视为离散时空token的迭代全局优化过程，引入线性化度量路径和分辨率依赖的时间步移机制，并采用异步时间微调策略统一多种任务。

Result: 在多个视频和图像生成基准上，URSA显著优于现有离散方法，性能媲美最先进的连续扩散模型，且推理步数更少。

Conclusion: URSA有效缩小了离散与连续视频生成之间的差距，为高效、可扩展的离散生成提供了新方向。

Abstract: Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA

</details>


### [71] [Generative View Stitching](https://arxiv.org/abs/2510.24718)
*Chonghyuk Song,Michal Stary,Boyuan Chen,George Kopanas,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 提出生成视图缝合（GVS），一种并行采样视频序列的方法，以实现预定义相机轨迹下的稳定、无碰撞、帧间一致且具有长程连贯性的相机引导视频生成。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型无法利用未来条件信息指导当前生成，导致在预定义相机轨迹中易发生场景碰撞并崩溃。

Method: 提出生成视图缝合（GVS）算法，结合扩散缝合思想与Diffusion Forcing框架，并引入Omni Guidance技术，通过同时依赖过去和未来条件来增强时间一致性，支持闭环机制。

Result: GVS在多种预定义相机路径（包括不可能楼梯）下实现了稳定、无碰撞、帧间一致且能闭合回路的视频生成，兼容现成的Diffusion Forcing训练模型。

Conclusion: GVS有效解决了自回归模型在相机引导生成中的局限性，实现了高质量、长序列的可控视频生成。

Abstract: Autoregressive video diffusion models are capable of long rollouts that are
stable and consistent with history, but they are unable to guide the current
generation with conditioning from the future. In camera-guided video generation
with a predefined camera trajectory, this limitation leads to collisions with
the generated scene, after which autoregression quickly collapses. To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory. Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation. While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching. We then introduce
Omni Guidance, a technique that enhances the temporal consistency in stitching
by conditioning on both the past and future, and that enables our proposed
loop-closing mechanism for delivering long-range coherence. Overall, GVS
achieves camera-guided video generation that is stable, collision-free,
frame-to-frame consistent, and closes loops for a variety of predefined camera
paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best
viewed as videos at https://andrewsonga.github.io/gvs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [72] [Evaluating Long-Term Memory for Long-Context Question Answering](https://arxiv.org/abs/2510.23730)
*Alessandra Terranova,Björn Ross,Alexandra Birch*

Main category: cs.CL

TL;DR: 本文提出并系统评估了多种记忆增强方法在长上下文对话任务中的有效性，使用了一个名为LoCoMo的合成基准测试。研究发现，记忆增强方法可在保持高准确率的同时减少90%以上的令牌使用量，且记忆架构的复杂性应与模型能力相匹配。


<details>
  <summary>Details</summary>
Motivation: 为了使大语言模型实现真正的对话连续性和经验学习，需要引入记忆机制。然而目前尚不清楚哪些记忆类型在长上下文对话中最为有效，因此本文旨在系统评估不同类型的记忆增强方法。

Method: 作者构建了一个名为LoCoMo的、带有问答标注的长上下文合成对话基准，用于评估多种记忆方法，包括全上下文提示、基于检索增强生成的语义记忆、基于反思的代理记忆、通过上下文学习的 episodic 记忆，以及通过提示优化的程序性记忆。

Result: 记忆增强方法在保持竞争力准确性的同时，减少了超过90%的令牌使用；小型基础模型最受益于RAG，而强大的指令调优推理模型则从更复杂的代理语义记忆和基于反思的 episodic 学习中获益更多；episodic 记忆有助于模型识别自身知识的局限。

Conclusion: 记忆架构的设计应根据模型能力进行适配：低能力模型适合简单语义记忆（如RAG），高能力模型则可利用更复杂的记忆形式（如代理和 episodic 记忆）以实现更高效的长期对话和自我认知。

Abstract: In order for large language models to achieve true conversational continuity
and benefit from experiential learning, they need memory. While research has
focused on the development of complex memory systems, it remains unclear which
types of memory are most effective for long-context conversational tasks. We
present a systematic evaluation of memory-augmented methods using LoCoMo, a
benchmark of synthetic long-context dialogues annotated for question-answering
tasks that require diverse reasoning strategies. We analyse full-context
prompting, semantic memory through retrieval-augmented generation and agentic
memory, episodic memory through in-context learning, and procedural memory
through prompt optimization. Our findings show that memory-augmented approaches
reduce token usage by over 90% while maintaining competitive accuracy. Memory
architecture complexity should scale with model capability, with small
foundation models benefitting most from RAG, and strong instruction-tuned
reasoning model gaining from episodic learning through reflections and more
complex agentic semantic memory. In particular, episodic memory can help LLMs
recognise the limits of their own knowledge.

</details>


### [73] [BitSkip: An Empirical Analysis of Quantization and Early Exit Composition](https://arxiv.org/abs/2510.23766)
*Ramshankar Bhuvaneswaran,Handan Liu*

Main category: cs.CL

TL;DR: 本文提出了BitSkip框架，用于系统探索大语言模型中量化与动态路由等技术的组合效应。出乎意料的是，一个简单的8位量化模型（BitSkip-V1）在性能上优于更复杂的4位和Hadamard增强模型，并且接近全精度基线模型的质量，同时展现出优异的早期退出特性。


<details>
  <summary>Details</summary>
Motivation: 尽管极端量化和动态路由等技术在提升大语言模型效率方面已被广泛研究，但这些方法组合后的相互作用尚不清楚，因此需要一种系统性的框架来探索它们的复合效应。

Method: 提出了一种名为BitSkip的混合架构框架，通过对比不同量化精度（如8位和4位）以及是否使用Hadamard变换的模型配置，系统地分析了这些技术对模型性能的影响。

Result: 实验发现，简单8位量化且无Hadamard变换的BitSkip-V1模型不仅优于其复杂的4位和Hadamard增强版本，而且在困惑度上接近全精度模型（1.13 vs 1.19）。引入Hadamard变换导致训练不稳定，性能下降超过37,000%。此外，BitSkip-V1在第18层表现出最优的早期退出特性，可实现32.5%的速度提升，仅带来4%的质量损失。

Conclusion: 复杂的技术组合并不一定带来更好的性能，简单的8位量化方案（BitSkip-V1）在效率和质量之间实现了更优的平衡，揭示了模型设计中对训练稳定性和实际效果的权衡至关重要。

Abstract: The pursuit of efficient Large Language Models (LLMs) has led to increasingly
complex techniques like extreme quantization and dynamic routing. While
individual benefits of these methods are well-documented, their compositional
effects remain poorly understood. This paper introduces BitSkip, a hybrid
architectural framework for systematically exploring these interactions.
Counter-intuitively, our findings reveal that a simple 8-bit quantized model
without Hadamard transform (BitSkip-V1) not only outperforms its more complex
4-bit and Hadamard-enhanced counterparts but also competes the full-precision
baseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard
transforms, even at 8-bit precision, catastrophically degraded performance by
over 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe
demonstrates superior early-exit characteristics, with layer 18 providing
optimal 32.5% speed gain for minimal 4% quality loss.

</details>


### [74] [Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language](https://arxiv.org/abs/2510.23828)
*Mena Attia,Aashiq Muhamed,Mai Alkhamissi,Thamar Solorio,Mona Diab*

Main category: cs.CL

TL;DR: 本研究评估了大语言模型（LLMs）在理解和实际运用阿拉伯语和英语中具有文化根基的比喻性表达方面的能力，发现模型在处理阿拉伯语尤其是埃及方言习语时表现较差，且在实际使用和情感含义理解方面存在显著挑战。


<details>
  <summary>Details</summary>
Motivation: 为了探究大语言模型是否能够真正理解并恰当使用蕴含本地知识和文化细微差别的语言，特别是比喻性表达，从而评估其跨文化推理能力。

Method: 以比喻性语言作为文化细微差别的代理，设计了针对上下文理解、实际使用和情感解释的评估任务，涵盖埃及阿拉伯语习语、多方言阿拉伯语谚语和英语谚语，并对22个开源和闭源大语言模型进行了测试。

Result: 结果显示，阿拉伯语谚语的平均准确率比英语低4.29%，埃及习语比阿拉伯谚语再低10.28%；在实际使用任务中准确率相对理解任务下降14.07%，但提供上下文可提升10.66%的准确率；模型在情感含义理解上最高仅达到与人类标注者85.58%的一致性。

Conclusion: 比喻性语言是检验大语言模型文化推理能力的有效指标，当前模型虽能部分理解比喻意义，但在恰当使用和深层文化含义把握上仍有明显不足。研究同时发布了首个用于评估埃及阿拉伯语习语理解和使用的数据集Kinayat。

Abstract: We present a comprehensive evaluation of the ability of large language models
(LLMs) to process culturally grounded language, specifically to understand and
pragmatically use figurative expressions that encode local knowledge and
cultural nuance. Using figurative language as a proxy for cultural nuance and
local knowledge, we design evaluation tasks for contextual understanding,
pragmatic use, and connotation interpretation in Arabic and English. We
evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,
multidialectal Arabic proverbs, and English proverbs. Our results show a
consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower
than for English proverbs, and performance for Egyptian idioms is 10.28% lower
than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%
relative to understanding, though providing contextual idiomatic sentences
improves accuracy by 10.66%. Models also struggle with connotative meaning,
reaching at most 85.58% agreement with human annotators on idioms with 100%
inter-annotator agreement. These findings demonstrate that figurative language
serves as an effective diagnostic for cultural reasoning: while LLMs can often
interpret figurative meaning, they face challenges in using it appropriately.
To support future research, we release Kinayat, the first dataset of Egyptian
Arabic idioms designed for both figurative understanding and pragmatic use
evaluation.

</details>


### [75] [How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse](https://arxiv.org/abs/2510.23842)
*Saki Imai,Lee Kezar,Laurel Aichler,Mert Inan,Erin Walker,Alicia Wooten,Lorna Quandt,Malihe Alikhani*

Main category: cs.CL

TL;DR: 本研究通过收集美国手语（ASL）STEM对话的运动捕捉数据，比较了互动对话、独白讲座和口译文章中的手势差异，发现对话中的手势持续时间比孤立手势短24.6%-44.6%，并揭示了语境对手势时空特征的影响。


<details>
  <summary>Details</summary>
Motivation: 现有手语模型多基于翻译或孤立词汇数据，忽略了自然对话中的变异性，尤其是在教育场景中师生使用新词汇的动态适应过程。

Method: 采集ASL STEM对话的运动捕捉数据，利用连续运动学特征分析双人互动、独白和口译三种情境下的手势变化，评估手势嵌入模型对STEM术语的识别能力及参与者随时间的协调程度。

Result: 对话中的手势比孤立手势显著更短（24.6%-44.6%），且在重复提及STEM术语时表现出明显的时空变化，这种减少在独白中未出现；模型能有效识别STEM手势并量化对话协调性。

Conclusion: 语用因素显著影响手语的表达方式，研究结合语言学分析与计算建模，为提升手语技术中的动态对话建模提供了基础。

Abstract: Most state-of-the-art sign language models are trained on interpreter or
isolated vocabulary data, which overlooks the variability that characterizes
natural dialogue. However, human communication dynamically adapts to contexts
and interlocutors through spatiotemporal changes and articulation style. This
specifically manifests itself in educational settings, where novel vocabularies
are used by teachers, and students. To address this gap, we collect a motion
capture dataset of American Sign Language (ASL) STEM (Science, Technology,
Engineering, and Mathematics) dialogue that enables quantitative comparison
between dyadic interactive signing, solo signed lecture, and interpreted
articles. Using continuous kinematic features, we disentangle dialogue-specific
entrainment from individual effort reduction and show spatiotemporal changes
across repeated mentions of STEM terms. On average, dialogue signs are
24.6%-44.6% shorter in duration than the isolated signs, and show significant
reductions absent in monologue contexts. Finally, we evaluate sign embedding
models on their ability to recognize STEM signs and approximate how entrained
the participants become over time. Our study bridges linguistic analysis and
computational modeling to understand how pragmatics shape sign articulation and
its representation in sign language technologies.

</details>


### [76] [Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content](https://arxiv.org/abs/2510.24438)
*Abdullah Mushtaq,Rafay Naeem,Ezieddin Elmahjub,Ibrahim Ghaznavi,Shawqi Al-Maliki,Mohamed Abdallah,Ala Al-Fuqaha,Junaid Qadir*

Main category: cs.CL

TL;DR: 本研究评估了GPT-4o、Ansari AI和Fanar在伊斯兰教义指导中的表现，采用双代理框架进行定量与定性分析，发现尽管GPT-4o在准确性与引用方面表现最佳，但现有模型仍难以稳定生成可靠的宗教内容，强调需建立以穆斯林视角为核心的社区驱动基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型被广泛用于提供伊斯兰指导，但存在误引文本、错误应用教法或文化不一致的风险，亟需系统评估其可靠性。

Method: 采用双代理评估框架：定量代理对引用真实性及六个维度（如结构、伊斯兰一致性、引用等）打分；定性代理进行五维度的成对比较（如语气、深度、原创性），基于真实伊斯兰博客的提问测试GPT-4o、Ansari AI和Fanar。

Result: GPT-4o在伊斯兰准确性（3.93）和引用（3.38）得分最高，Ansari AI次之（3.68, 3.32），Fanar较低（2.76, 1.82）；GPT-4o定量总分最高（3.90/5），Ansari AI在定性对比中胜出次数最多（116/200），Fanar虽落后但具备面向伊斯兰与阿拉伯语境的创新。

Conclusion: 当前AI模型在生成准确且符合信仰规范的宗教内容方面仍有不足，必须发展以穆斯林社群为中心的评估基准，以提升高风险领域（如宗教、医学、法律）中AI的可靠性。

Abstract: Large language models are increasingly used for Islamic guidance, but risk
misquoting texts, misapplying jurisprudence, or producing culturally
inconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar
on prompts from authentic Islamic blogs. Our dual-agent framework uses a
quantitative agent for citation verification and six-dimensional scoring (e.g.,
Structure, Islamic Consistency, Citations) and a qualitative agent for
five-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).
GPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI
followed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong
performance, models still fall short in reliably producing accurate Islamic
content and citations -- a paramount requirement in faith-sensitive writing.
GPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led
qualitative pairwise wins (116/200). Fanar, though trailing, introduces
innovations for Islamic and Arabic contexts. This study underscores the need
for community-driven benchmarks centering Muslim perspectives, offering an
early step toward more reliable AI in Islamic knowledge and other high-stakes
domains such as medicine, law, and journalism.

</details>


### [77] [CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection](https://arxiv.org/abs/2510.23845)
*Grace Byun,Rebecca Lipschutz,Sean T. Minton,Abigail Lott,Jinho D. Choi*

Main category: cs.CL

TL;DR: 本文提出了CRADLE BENCH，一个用于多方面危机检测的基准，涵盖七种临床标准定义的危机类型，并首次引入时间标签。该基准包含医生标注的评估和发展示例，以及通过多语言模型集成自动标注的训练语料库，显著优于单模型标注方法。


<details>
  <summary>Details</summary>
Motivation: 准确识别用户与模型交互过程中出现的心理健康危机情况（如自杀意念、性侵、家庭暴力等）至关重要，但现有语言模型在此方面的表现仍不足，亟需更全面、可靠的检测机制。

Method: 构建了一个涵盖七种危机类型的多维度基准CRADLE BENCH，采用多位临床医生标注评估和开发集，并利用多个语言模型的多数投票集成方法自动生成大规模训练数据；进一步在不同共识标准下微调六种危机检测模型。

Result: CRADLE BENCH提供了600个医生标注的测试样本和420个开发样本，以及约4000个自动标注的训练样本；基于集成标注的训练数据显著优于单一模型标注；在不同共识标准下微调的模型为危机检测提供了互补方案。

Conclusion: CRADLE BENCH是一个更全面、贴近临床实际的危机检测基准，其多模型集成标注和多级别共识训练策略为语言模型在心理健康危机识别中的可靠应用提供了重要支持。

Abstract: Detecting mental health crisis situations such as suicide ideation, rape,
domestic violence, child abuse, and sexual harassment is a critical yet
underexplored challenge for language models. When such situations arise during
user--model interactions, models must reliably flag them, as failure to do so
can have serious consequences. In this work, we introduce CRADLE BENCH, a
benchmark for multi-faceted crisis detection. Unlike previous efforts that
focus on a limited set of crisis types, our benchmark covers seven types
defined in line with clinical standards and is the first to incorporate
temporal labels. Our benchmark provides 600 clinician-annotated evaluation
examples and 420 development examples, together with a training corpus of
around 4K examples automatically labeled using a majority-vote ensemble of
multiple language models, which significantly outperforms single-model
annotation. We further fine-tune six crisis detection models on subsets defined
by consensus and unanimous ensemble agreement, providing complementary models
trained under different agreement criteria.

</details>


### [78] [Iterative Critique-Refine Framework for Enhancing LLM Personalization](https://arxiv.org/abs/2510.24469)
*Durga Prasad Maram,Dhruvin Gandhi,Zonghai Yao,Gayathri Akkinapalli,Franck Dernoncourt,Yu Wang,Ryan A. Rossi,Nesreen K. Ahmed*

Main category: cs.CL

TL;DR: 提出PerFine，一种无需训练的迭代批评-优化框架，通过基于用户画像的反馈提升个性化文本生成的风格、词汇、句式和主题一致性。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强方法在生成过程中容易偏离用户的风格、语气和主题，缺乏有效的后生成调整机制，导致个性化效果不足。

Method: 采用双阶段迭代机制：生成器基于检索到的用户画像生成文本，批评者模型同样基于该画像提供关于语气、词汇、句式和主题性的结构化反馈；生成器据此修订，并通过新颖的‘knockout’策略保留更优版本。同时探索Best-of-N和主题提取等推理时策略以平衡质量与效率。

Result: 在Yelp、Goodreads和Amazon数据集上，PerFine相比PGraphRAG consistently 提升GEval指标7-13%，在3-5次优化迭代中持续改进，且随批评者模型增大表现出良好可扩展性。

Conclusion: 事后、基于画像感知的反馈是一种强大、无需训练且模型无关的个性化LLM生成范式，PerFine为提升生成文本与用户个性的一致性提供了有效解决方案。

Abstract: Personalized text generation requires models not only to produce coherent
text but also to align with a target user's style, tone, and topical focus.
Existing retrieval-augmented approaches such as LaMP and PGraphRAG enrich
profiles with user and neighbor histories, but they stop at generation and
often yield outputs that drift in tone, topic, or style. We present PerFine, a
unified, training-free critique-refine framework that enhances personalization
through iterative, profile-grounded feedback. In each iteration, an LLM
generator produces a draft conditioned on the retrieved profile, and a critic
LLM - also conditioned on the same profile - provides structured feedback on
tone, vocabulary, sentence structure, and topicality. The generator then
revises, while a novel knockout strategy retains the stronger draft across
iterations. We further study additional inference-time strategies such as
Best-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,
Goodreads, and Amazon datasets, PerFine consistently improves personalization
over PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5
refinement iterations, and scalability with increasing critic size. These
results highlight that post-hoc, profile-aware feedback offers a powerful
paradigm for personalized LLM generation that is both training-free and
model-agnostic.

</details>


### [79] [Temporal Blindness in Multi-Turn LLM Agents: Misaligned Tool Use vs. Human Time Perception](https://arxiv.org/abs/2510.23853)
*Yize Cheng,Arshia Soltani Moakhar,Chenrui Fan,Kazem Faghih,Parsa Hosseini,Wenxiao Wang,Soheil Feizi*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型代理在多轮对话中因缺乏时间感知而导致的工具调用决策问题，提出了包含时间戳的测试集TicToc-v1，并发现添加时间信息仅能小幅提升模型与人类偏好的对齐程度，表明需要专门的后训练对齐来改善模型的时间感知能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型代理在动态环境中进行多轮交互时，默认缺乏对真实时间流逝的感知，导致工具调用决策失误，亟需研究和解决这一时间盲区问题。

Method: 构建了一个名为TicToc-v1的测试集，包含34种不同时间敏感性的多轮用户-代理轨迹，并在对话消息中显式添加时间戳以提供时间上下文；收集人类在不同时间间隔下的工具调用偏好，评估模型决策与人类偏好的对齐情况。

Result: 实验显示，没有时间信息时，大多数模型的表现仅略优于随机水平（最高对齐率刚过60%）；加入时间戳后性能略有提升，最大约为65%，且较大模型受益更明显，但提示工程等简单方法效果有限。

Conclusion: 当前LLM代理在时间感知方面存在显著不足，仅靠添加时间戳不足以充分解决问题，需针对性地设计后训练对齐方法以更好匹配人类的时间判断。

Abstract: Large language model agents are increasingly used in multi-turn
conversational settings to interact with and execute tasks in dynamic
environments. However, a key limitation is their temporal blindness: they, by
default, operate with a stationary context, failing to account for the
real-world time elapsed between messages. This becomes a critical liability
when an agent must decide whether to invoke a tool based on how much time has
passed since the last observation. Without temporal awareness, agents often
either over-rely on previous context (skipping necessary tool calls), or
under-rely on it (unnecessarily repeating tool calls). To study this challenge,
we introduce TicToc-v1, a test set of multi-turn user-agent trajectories across
34 scenarios with varying time sensitivity. Each trajectory ends with a user
question, where the need for a tool call depends on the amount of time elapsed
since the last message. To give LLMs temporal context, we augment dialogue
messages with explicit timestamps, bridging the gap between static dialogue and
evolving environments. We then collected human preferences for these samples,
creating two subsets: one where humans preferred relying on the previous
observation (prefer-noTool), and another where they preferred a new tool call
(prefer-Tool). We evaluated how well LLM tool-calling decisions align with
human preferences under varying time intervals on TicToc-v1. Our analysis show
that without time information, most models perform only slightly better than
random, with the top alignment rate being just over 60%. While adding
timestamps leads to a slight improvement, particularly for larger models, the
improvement is modest, peaking at around 65%. We also show that naive,
prompt-based alignment have limited effectiveness. Our findings highlight the
need for specific post-training alignment to align multi-turn LLM tool use with
human temporal perception.

</details>


### [80] [Optimizing Retrieval for RAG via Reinforced Contrastive Learning](https://arxiv.org/abs/2510.24652)
*Jiawei Zhou,Lei Chen*

Main category: cs.CL

TL;DR: 提出R3框架，通过强化对比学习在检索增强生成（RAG）中动态优化检索相关性，无需依赖标注数据，显著提升性能且训练高效。


<details>
  <summary>Details</summary>
Motivation: 在RAG中，传统信息检索的相关性难以预先定义或标注，需要一种能在RAG环境中动态优化检索效果的方法。

Method: 提出R3框架，采用试错反馈的强化对比学习，使检索器在与环境交互中自动生成对比信号并自我优化，无需监督微调数据。

Result: 实验表明，R3比原始检索器提升5.2%，优于当前最先进的检索器4.9%，性能媲美基于大模型增强的RAG系统，且仅需4块GPU在一天内完成训练。

Conclusion: R3为RAG场景下的检索提供了高效、实用且无需人工标注的自优化解决方案，具有广泛的应用前景。

Abstract: As retrieval-augmented generation (RAG) becomes increasingly widespread, the
role of information retrieval (IR) is shifting from retrieving information for
human users to retrieving contextual knowledge for artificial intelligence (AI)
systems, where relevance becomes difficult to define or annotate beforehand. To
address this challenge, we propose R3, a Retrieval framework optimized for RAG
through trialand-feedback Reinforced contrastive learning. Unlike prior
approaches that rely on annotated or synthetic data for supervised fine-tuning,
R3 enables the retriever to dynamically explore and optimize relevance within
the RAG environment. During training, the retrieved results interact with the
environment to produce contrastive signals that automatically guide the
retriever's self-improvement. Extensive experiments across diverse tasks
demonstrate that R3 improves RAG performance by 5.2% over the original
retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving
comparable results to LLM-augmented retrieval and RAG systems built on
post-trained or instruction-tuned LLMs. It is both efficient and practical,
requiring only 4 GPUs and completing training within a single day.

</details>


### [81] [Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs](https://arxiv.org/abs/2510.23854)
*Jyotika Singh,Weiyi Sun,Amit Agarwal,Viji Krishnamurthy,Yassine Benajiba,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估方法Combo-Eval，用于评估大语言模型生成的自然语言表示（NLR），并发布了首个专门用于NLR基准测试的数据集NLR-BIRD。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在将表格数据结果转换为自然语言时存在信息丢失或错误的问题，且缺乏有效的评估方法。

Method: 提出Combo-Eval评估方法，结合多种现有方法的优点，减少25-61%的LLM调用次数，并构建了NLR-BIRD数据集用于基准测试。

Result: 通过人工评估验证了Combo-Eval与人类判断的高度一致性，适用于有无参考答案的各种场景。

Conclusion: Combo-Eval在评估LLM生成的NLR方面具有更高的保真度和效率，NLR-BIRD为未来研究提供了重要资源。

Abstract: In modern industry systems like multi-turn chat agents, Text-to-SQL
technology bridges natural language (NL) questions and database (DB) querying.
The conversion of tabular DB results into NL representations (NLRs) enables the
chat-based interaction. Currently, NLR generation is typically handled by large
language models (LLMs), but information loss or errors in presenting tabular
results in NL remains largely unexplored. This paper introduces a novel
evaluation method - Combo-Eval - for judgment of LLM-generated NLRs that
combines the benefits of multiple existing methods, optimizing evaluation
fidelity and achieving a significant reduction in LLM calls by 25-61%.
Accompanying our method is NLR-BIRD, the first dedicated dataset for NLR
benchmarking. Through human evaluations, we demonstrate the superior alignment
of Combo-Eval with human judgments, applicable across scenarios with and
without ground truth references.

</details>


### [82] [Tongyi DeepResearch Technical Report](https://arxiv.org/abs/2510.24701)
*Tongyi DeepResearch Team,Baixuan Li,Bo Zhang,Dingchu Zhang,Fei Huang,Guangyu Li,Guoxin Chen,Huifeng Yin,Jialong Wu,Jingren Zhou,Kuan Li,Liangcai Su,Litu Ou,Liwen Zhang,Pengjun Xie,Rui Ye,Wenbiao Yin,Xinmiao Yu,Xinyu Wang,Xixi Wu,Xuanzhong Chen,Yida Zhao,Zhen Zhang,Zhengwei Tao,Zhongwang Zhang,Zile Qiao,Chenxi Wang,Donglei Yu,Gang Fu,Haiyang Shen,Jiayin Yang,Jun Lin,Junkai Zhang,Kui Zeng,Li Yang,Hailong Yin,Maojia Song,Ming Yan,Peng Xia,Qian Xiao,Rui Min,Ruixue Ding,Runnan Fang,Shaowei Chen,Shen Huang,Shihang Wang,Shihao Cai,Weizhou Shen,Xiaobin Wang,Xin Guan,Xinyu Geng,Yingcheng Shi,Yuning Wu,Zhuo Chen,Zijian Li,Yong Jiang*

Main category: cs.CL

TL;DR: Tongyi DeepResearch 是一个专为长周期、深度信息探索任务设计的代理式大语言模型，通过结合代理中训练和代理后训练的端到端框架实现可扩展的推理与信息检索。


<details>
  <summary>Details</summary>
Motivation: 为了提升大模型在复杂、长期研究任务中的自主深度探索能力，克服传统方法依赖人工标注和难以规模化的问题。

Method: 采用完全自动化的数据合成流水线，结合代理中训练和代理后训练的端到端训练框架，并构建定制化环境以实现稳定交互。

Result: 该模型在多个代理深度研究基准（如 Humanity's Last Exam、BrowseComp 等）上达到最先进性能，拥有305亿总参数，每token仅激活33亿参数。

Conclusion: Tongyi DeepResearch 实现了高效、可扩展的自主研究能力，且模型、框架与完整方案已开源，以推动社区发展。

Abstract: We present Tongyi DeepResearch, an agentic large language model, which is
specifically designed for long-horizon, deep information-seeking research
tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is
developed through an end-to-end training framework that combines agentic
mid-training and agentic post-training, enabling scalable reasoning and
information seeking across complex tasks. We design a highly scalable data
synthesis pipeline that is fully automatic, without relying on costly human
annotation, and empowers all training stages. By constructing customized
environments for each stage, our system enables stable and consistent
interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total
parameters, with only 3.3 billion activated per token, achieves
state-of-the-art performance across a range of agentic deep research
benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,
WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We
open-source the model, framework, and complete solutions to empower the
community.

</details>


### [83] [OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning](https://arxiv.org/abs/2510.23870)
*Marianne Menglin Liu,Sai Ashish Somayajula,Syed Fahad Allam Shah,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: OraPlan-SQL在Archer NL2SQL 2025挑战赛中排名第一，提出了一种基于代理的框架，包含 Planner 和 SQL 两个智能体，通过反馈引导的元提示策略优化单个规划器，并结合实体链接和计划多样化提升多语言场景下的执行准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统在处理复杂推理（如算术、常识和假设推理）和多语言场景时存在规划协调开销大、实体不匹配等问题，需要更高效、可靠的解决方案。

Method: 采用两阶段代理框架：Planner生成自然语言计划，SQL Agent将其转化为SQL；引入反馈引导的元提示策略，基于失败案例聚类生成改进指南以优化Planner；加入实体链接指南应对中英文实体差异；通过计划多样化和多数投票提升输出稳定性。

Result: 在Archer NL2SQL 2025挑战赛中排名第一，执行准确率（EX）达英文55.0%、中文56.7%，超过第二名6%以上，SQL有效性（VA）保持99%以上。

Conclusion: OraPlan-SQL通过简化规划架构、融合人类反馈与多样化生成策略，在保持高SQL有效性的前提下显著提升了复杂多语言NL2SQL任务的性能，验证了聚焦 Planner 优化的有效性。

Abstract: We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge
2025, a bilingual benchmark requiring complex reasoning such as arithmetic,
commonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding
the second-best system by more than 6% in execution accuracy (EX), with 55.0%
in English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).
Our system follows an agentic framework with two components: Planner agent that
generates stepwise natural language plans, and SQL agent that converts these
plans into executable SQL. Since SQL agent reliably adheres to the plan, our
refinements focus on the planner. Unlike prior methods that rely on multiple
sub-agents for planning and suffer from orchestration overhead, we introduce a
feedback-guided meta-prompting strategy to refine a single planner. Failure
cases from a held-out set are clustered with human input, and an LLM distills
them into corrective guidelines that are integrated into the planner's system
prompt, improving generalization without added complexity. For the multilingual
scenario, to address transliteration and entity mismatch issues, we incorporate
entity-linking guidelines that generate alternative surface forms for entities
and explicitly include them in the plan. Finally, we enhance reliability
through plan diversification: multiple candidate plans are generated for each
query, with the SQL agent producing a query for each plan, and final output
selected via majority voting over their executions.

</details>


### [84] [Language Models for Longitudinal Clinical Prediction](https://arxiv.org/abs/2510.23884)
*Tananun Songdechakraiwut,Michael Lutz*

Main category: cs.CL

TL;DR: 提出一种轻量级框架，通过在语言模型空间内整合患者历史和上下文，在不进行微调的情况下实现对纵向临床数据的准确预测。


<details>
  <summary>Details</summary>
Motivation: 希望在无需微调大模型的前提下，利用冻结的大语言模型分析纵向临床数据，以实现对早期阿尔茨海默病的有效监测。

Method: 将患者历史和上下文信息整合到冻结的大语言模型空间中，通过适应性机制生成预测结果，避免了传统微调过程。

Result: 在神经心理学评估任务中表现出准确且可靠的性能，即使训练数据极少也能取得良好效果。

Conclusion: 该框架为利用大语言模型进行低资源医学时间序列预测提供了有效且可行的解决方案，尤其适用于早期疾病监测场景。

Abstract: We explore a lightweight framework that adapts frozen large language models
to analyze longitudinal clinical data. The approach integrates patient history
and context within the language model space to generate accurate forecasts
without model fine-tuning. Applied to neuropsychological assessments, it
achieves accurate and reliable performance even with minimal training data,
showing promise for early-stage Alzheimer's monitoring.

</details>


### [85] [AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages](https://arxiv.org/abs/2510.23896)
*Kosei Uemura,Miaoran Zhang,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文提出了AfriMTEB，一个覆盖59种非洲语言、14项任务和38个数据集的多语言文本嵌入基准，并发布了针对非洲语言优化的AfriE5嵌入模型，在多项任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 非洲语言在现有的多语言文本嵌入基准中代表性不足，现有任务多源自翻译评估，缺乏针对非洲语言特有任务的支持。

Method: 构建AfriMTEB基准，包含六个新数据集和新增如仇恨言论检测、意图识别和情感分类等任务；通过跨语言对比蒸馏方法改进mE5模型，提出AfriE5模型。

Result: AfriE5在AfriMTEB上表现优于Gemini-Embeddings和mE5等强基线模型，取得当前最优性能。

Conclusion: AfriMTEB填补了非洲语言文本嵌入评估的空白，AfriE5为非洲语言提供了更优的嵌入方案，推动了低资源语言在NLP中的发展。

Abstract: Text embeddings are an essential building component of several NLP tasks such
as retrieval-augmented generation which is crucial for preventing
hallucinations in LLMs. Despite the recent release of massively multilingual
MTEB (MMTEB), African languages remain underrepresented, with existing tasks
often repurposed from translation benchmarks such as FLORES clustering or
SIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB
covering 59 languages, 14 tasks, and 38 datasets, including six newly added
datasets. Unlike many MMTEB datasets that include fewer than five languages,
the new additions span 14 to 56 African languages and introduce entirely new
tasks, such as hate speech detection, intent detection, and emotion
classification, which were not previously covered. Complementing this, we
present AfriE5, an adaptation of the instruction-tuned mE5 model to African
languages through cross-lingual contrastive distillation. Our evaluation shows
that AfriE5 achieves state-of-the-art performance, outperforming strong
baselines such as Gemini-Embeddings and mE5.

</details>


### [86] [Breaking the Benchmark: Revealing LLM Bias via Minimal Contextual Augmentation](https://arxiv.org/abs/2510.23921)
*Kaveh Eskandari Miandoab,Mahammed Kamruzzaman,Arshia Gharooni,Gene Louis Kim,Vasanth Sarathy,Ninareh Mehrabi*

Main category: cs.CL

TL;DR: 本文提出了一种新的增强框架，用于评估大语言模型在公平性方面的脆弱性，发现现有模型在输入扰动下更容易表现出刻板偏见，尤其对文献中研究较少的群体更为明显。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据的歧视性，大语言模型存在刻板偏见问题；现有的去偏方法较为脆弱，需要更鲁棒的评估手段。

Method: 提出一种包含三个可插拔步骤的通用增强框架，并应用于BBQ等公平性评测基准，通过输入扰动分析模型的偏见行为。

Result: 实验表明包括最先进开源和闭源模型在内的大语言模型在扰动下更易表现出刻板印象，且对研究不足的群体偏见更严重。

Conclusion: 需扩展公平性和安全性研究，涵盖更多样化的群体，以提升模型在现实场景中的公平性。

Abstract: Large Language Models have been shown to demonstrate stereotypical biases in
their representations and behavior due to the discriminative nature of the data
that they have been trained on. Despite significant progress in the development
of methods and models that refrain from using stereotypical information in
their decision-making, recent work has shown that approaches used for bias
alignment are brittle. In this work, we introduce a novel and general
augmentation framework that involves three plug-and-play steps and is
applicable to a number of fairness evaluation benchmarks. Through application
of augmentation to a fairness evaluation dataset (Bias Benchmark for Question
Answering (BBQ)), we find that Large Language Models (LLMs), including
state-of-the-art open and closed weight models, are susceptible to
perturbations to their inputs, showcasing a higher likelihood to behave
stereotypically. Furthermore, we find that such models are more likely to have
biased behavior in cases where the target demographic belongs to a community
less studied by the literature, underlining the need to expand the fairness and
safety research to include more diverse communities.

</details>


### [87] [Agent-based Automated Claim Matching with Instruction-following LLMs](https://arxiv.org/abs/2510.23924)
*Dina Pisarevskaya,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 提出了一种基于代理的自动化声明匹配方法，使用指令跟随的大型语言模型（LLM），通过两步流程生成提示并进行二分类任务，展示了LLM生成提示优于人工提示，小模型在生成中表现与大模型相当，并揭示了LLM对声明匹配的理解。


<details>
  <summary>Details</summary>
Motivation: 提高声明匹配的自动化水平，减少对人工设计提示的依赖，并探索不同规模和类型LLM在该任务中的潜力与效率。

Method: 采用两步管道：首先用LLM生成提示，然后将声明匹配作为二分类任务由LLM执行；同时实验不同LLM在提示生成和匹配阶段的组合效果。

Result: LLM生成的提示优于人工提示，小规模LLM在提示生成中表现不逊于大规模模型，且混合使用不同LLM可提升效率和性能。

Conclusion: 该代理式LLM两步法在声明匹配任务中高效且有效，能够在降低计算成本的同时实现或超越现有技术水平的表现。

Abstract: We present a novel agent-based approach for the automated claim matching task
with instruction-following LLMs. We propose a two-step pipeline that first
generates prompts with LLMs, to then perform claim matching as a binary
classification task with LLMs. We demonstrate that LLM-generated prompts can
outperform SOTA with human-generated prompts, and that smaller LLMs can do as
well as larger ones in the generation process, allowing to save computational
resources. We also demonstrate the effectiveness of using different LLMs for
each step of the pipeline, i.e. using an LLM for prompt generation, and another
for claim matching. Our investigation into the prompt generation process in
turn reveals insights into the LLMs' understanding of claim matching.

</details>


### [88] [Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs](https://arxiv.org/abs/2510.23941)
*Soham Satyadharma,Fatemeh Sheikholeslami,Swati Kaul,Aziz Umit Batur,Suleiman A. Khan*

Main category: cs.CL

TL;DR: 提出了一种无需训练的级联自动提示方法，用于大规模评估电商产品属性质量，显著提升精度和召回率，同时将领域专家工作量减少99%。


<details>
  <summary>Details</summary>
Motivation: 在复杂的工业级电商目录中，通用语言模型难以直接满足特定领域的质量评估需求，需要一种可扩展且无需训练的方法来桥接通用语言理解与领域知识。

Method: 从人工设计的初始提示出发，通过级联方式自动生成并优化针对不同产品类别-属性对的提示，无需训练或微调模型。

Result: 相比传统思维链提示，该方法在精度和召回率上提升了8-10%，每个属性的专家耗时从5.1小时降至3分钟，且在五种语言和多种任务中表现出良好的泛化能力。

Conclusion: 该级联自动提示框架在不依赖训练标签和模型微调的情况下，实现了高效、可扩展的电商产品质量评估，大幅降低人工成本并保持跨语言、跨任务的稳定性能提升。

Abstract: We introduce a novel, training free cascade for auto-prompting Large Language
Models (LLMs) to assess product quality in e-commerce. Our system requires no
training labels or model fine-tuning, instead automatically generating and
refining prompts for evaluating attribute quality across tens of thousands of
product category-attribute pairs. Starting from a seed of human-crafted
prompts, the cascade progressively optimizes instructions to meet
catalog-specific requirements. This approach bridges the gap between general
language understanding and domain-specific knowledge at scale in complex
industrial catalogs. Our extensive empirical evaluations shows the auto-prompt
cascade improves precision and recall by $8-10\%$ over traditional
chain-of-thought prompting. Notably, it achieves these gains while reducing
domain expert effort from 5.1 hours to 3 minutes per attribute - a $99\%$
reduction. Additionally, the cascade generalizes effectively across five
languages and multiple quality assessment tasks, consistently maintaining
performance gains.

</details>


### [89] [Leveraging LLMs for Early Alzheimer's Prediction](https://arxiv.org/abs/2510.23946)
*Tananun Songdechakraiwut*

Main category: cs.CL

TL;DR: 提出一种基于连接组信息的LLM框架，通过将动态fMRI连接性数据编码为时间序列并映射到冻结的预训练语言模型中，实现对早期阿尔茨海默病的高敏感性预测。


<details>
  <summary>Details</summary>
Motivation: 利用大脑连接组数据提升神经退行性疾病的早期检测能力，克服传统方法在敏感性和临床适用性方面的局限。

Method: 将动态fMRI连接性数据编码为时间序列，进行鲁棒归一化处理，并将其映射为适合冻结的预训练LLM输入的表示形式，用于临床预测。

Result: 在早期阿尔茨海默病检测中，该方法的错误率显著低于临床可接受范围，表现出高敏感性预测性能。

Conclusion: 该框架展示了将神经影像数据与大型语言模型结合用于临床预测的潜力，有助于实现阿尔茨海默病的早期干预。

Abstract: We present a connectome-informed LLM framework that encodes dynamic fMRI
connectivity as temporal sequences, applies robust normalization, and maps
these data into a representation suitable for a frozen pre-trained LLM for
clinical prediction. Applied to early Alzheimer's detection, our method
achieves sensitive prediction with error rates well below clinically recognized
margins, with implications for timely Alzheimer's intervention.

</details>


### [90] [Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning in Multilingual LLMs](https://arxiv.org/abs/2510.23949)
*Kyomin Hwang,Hyeonjin Kim,Seungyeon Kim,Sunghyun Wee,Nojun Kwak*

Main category: cs.CL

TL;DR: 本文研究了在多语言大模型中使用仅英语数据进行知识擦除的不足，提出了一种新的评估视角，揭示了完全微调后的多语言大模型在遗忘过程中出现的语言混淆现象，并提出了基于N-gram的语言混合评分（N-Mix）来量化该现象，指出传统基于参考文本的评估指标在高N-Mix情况下会产生误判，因此需要引入基于语义的新型评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注性能层面的遗忘效果，缺乏对多语言大模型在遗忘过程中语言混淆现象的深入分析，尤其是在使用平行多语言数据微调后，模型可能出现输入与输出语言不一致的问题，导致传统评估方法失效。

Method: 1) 提出N-gram-based Language-Mix (N-Mix) 评分以量化语言混淆程度；2) 分析高N-Mix下参考基线指标产生假阴性的问题；3) 倡导采用能直接评估生成内容语义的语义型评估指标。

Result: 实验证明语言混淆在多语言大模型中普遍存在且稳定，N-Mix得分高时，传统基于参考的评估指标会错误地判断遗忘失败（假阴性），说明其不可靠。

Conclusion: 为准确评估多语言大模型的知识遗忘效果，需摒弃依赖固定参考文本的传统指标，转而发展能够理解生成内容语义的新型评估方法。

Abstract: There have been a couple of studies showing that attempting to erase
multilingual knowledge using only English data is insufficient for multilingual
LLMs. However, their analyses remain highly performance-oriented. In this
paper, we switch the point of view to evaluation, and address an additional
blind spot which reveals itself when the multilingual LLM is fully finetuned
with parallel multilingual dataset before unlearning. Here, language confusion
occurs whereby a model responds in language different from that of the input
prompt. Language confusion is a problematic phenomenon in unlearning, causing
the standard reference-based metrics to fail. We tackle this phenomenon in
three steps: (1) introduce N-gram-based Language-Mix (N-Mix) score to
quantitatively show the language confusion is pervasive and consistent in
multilingual LLMs, (2) demonstrate that reference-based metrics result in false
negatives when N-Mix score is high, and(3) suggest the need of new type of
unlearning evaluation that can directly assess the content of the generated
sentences. We call this type of metrics as semantic-based metric.

</details>


### [91] [M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems](https://arxiv.org/abs/2510.23995)
*Mengzhou Sun,Sendong Zhao,Jianyu Chen,Haochun Wang,Bin Qin*

Main category: cs.CL

TL;DR: 本文提出了一种名为M-Eval的新方法，用于检测基于检索增强生成（RAG）的医疗问答系统中的事实性错误，通过借鉴循证医学中的异质性分析，利用多源证据验证回答的准确性与证据可靠性，显著提升了各类大语言模型的准确率（最高提升23.31%），有助于减少诊断错误。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG在医疗问答中存在生成错误信息（如幻觉）和无法正确使用外部知识的问题，影响了系统的可靠性与安全性。

Method: 受循证医学中异质性分析的启发，M-Eval从外部知识库提取额外医学文献，并结合RAG系统生成的证据文档，通过分析证据是否支持回答中的不同观点来检测事实错误，并评估证据的可靠性。

Result: M-Eval在多个大语言模型上实现了最高达23.31%的准确率提升，能有效识别RAG响应中的错误并评估证据质量。

Conclusion: M-Eval能够有效提高RAG在医疗领域的可靠性和准确性，有助于减少诊断错误，推动大语言模型在临床环境中的安全应用。

Abstract: Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing
medical question-answering systems through the integration of large language
models (LLMs) with external medical literature. LLMs can retrieve relevant
medical articles to generate more professional responses efficiently. However,
current RAG applications still face problems. They generate incorrect
information, such as hallucinations, and they fail to use external knowledge
correctly. To solve these issues, we propose a new method named M-Eval. This
method is inspired by the heterogeneity analysis approach used in
Evidence-Based Medicine (EBM). Our approach can check for factual errors in RAG
responses using evidence from multiple sources. First, we extract additional
medical literature from external knowledge bases. Then, we retrieve the
evidence documents generated by the RAG system. We use heterogeneity analysis
to check whether the evidence supports different viewpoints in the response. In
addition to verifying the accuracy of the response, we also assess the
reliability of the evidence provided by the RAG system. Our method shows an
improvement of up to 23.31% accuracy across various LLMs. This work can help
detect errors in current RAG-based medical systems. It also makes the
applications of LLMs more reliable and reduces diagnostic errors.

</details>


### [92] [PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine](https://arxiv.org/abs/2510.23998)
*Mengzhou Sun,Sendong Zhao,Jianyu Chen,Bin Qin*

Main category: cs.CL

TL;DR: 本文提出了一种基于PICO格式的检索增强生成方法（PICOs-RAG），用于改进循证医学中的复杂临床查询处理，显著提升了检索效率和相关性。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）方法在处理现实临床场景中的复杂查询时表现不佳，尤其当查询信息不全或语言不精确时，容易导致检索结果无关和回答无效。

Method: 通过将用户查询扩展并规范化为符合循证医学中PICO格式的专业查询，利用PICO框架提取关键信息用于证据检索，从而提升检索质量。

Result: 相比基线方法，PICOs-RAG在评估中实现了最高达8.8%的性能提升，显著增强了检索的相关性和效率。

Conclusion: PICOs-RAG有效提升了大型语言模型在循证医学中的实用性，使其成为更可靠、有帮助的医疗助手。

Abstract: Evidence-based medicine (EBM) research has always been of paramount
importance. It is important to find appropriate medical theoretical support for
the needs from physicians or patients to reduce the occurrence of medical
accidents. This process is often carried out by human querying relevant
literature databases, which lacks objectivity and efficiency. Therefore,
researchers utilize retrieval-augmented generation (RAG) to search for evidence
and generate responses automatically. However, current RAG methods struggle to
handle complex queries in real-world clinical scenarios. For example, when
queries lack certain information or use imprecise language, the model may
retrieve irrelevant evidence and generate unhelpful answers. To address this
issue, we present the PICOs-RAG to expand the user queries into a better
format. Our method can expand and normalize the queries into professional ones
and use the PICO format, a search strategy tool present in EBM, to extract the
most important information used for retrieval. This approach significantly
enhances retrieval efficiency and relevance, resulting in up to an 8.8\%
improvement compared to the baseline evaluated by our method. Thereby the
PICOs-RAG improves the performance of the large language models into a helpful
and reliable medical assistant in EBM.

</details>


### [93] [META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for Retrieval-Augmented Generation in Evidence-Based Medicine](https://arxiv.org/abs/2510.24003)
*Mengzhou Sun,Sendong Zhao,Jianyu Chen,Haochun Wang,Bin Qin*

Main category: cs.CL

TL;DR: 本文提出一种受循证医学中荟萃分析启发的新方法，用于重新排序和筛选医学证据，以提升大语言模型在循证医学任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG技术在循证医学应用中难以高效区分高质量证据，而循证医学对证据质量要求严格，因此需要更可靠的证据筛选机制。

Method: 结合可靠性分析、异质性分析和外推分析等多种循证医学方法，模拟荟萃分析过程，对检索到的医学证据进行重排序与过滤，为大语言模型提供最优证据。

Result: 实验结果显示，该方法在PubMed数据集上使诊断准确率最高提升了11.4%，显著提高了RAG系统提取证据的质量和可靠性。

Conclusion: 所提出的方法能有效提升RAG系统在循证医学中的性能，减少错误知识的引入，帮助用户获得更准确、有效的回复。

Abstract: Evidence-based medicine (EBM) holds a crucial role in clinical application.
Given suitable medical articles, doctors effectively reduce the incidence of
misdiagnoses. Researchers find it efficient to use large language models (LLMs)
techniques like RAG for EBM tasks. However, the EBM maintains stringent
requirements for evidence, and RAG applications in EBM struggle to efficiently
distinguish high-quality evidence. Therefore, inspired by the meta-analysis
used in EBM, we provide a new method to re-rank and filter the medical
evidence. This method presents multiple principles to filter the best evidence
for LLMs to diagnose. We employ a combination of several EBM methods to emulate
the meta-analysis, which includes reliability analysis, heterogeneity analysis,
and extrapolation analysis. These processes allow the users to retrieve the
best medical evidence for the LLMs. Ultimately, we evaluate these high-quality
articles and show an accuracy improvement of up to 11.4% in our experiments and
results. Our method successfully enables RAG to extract higher-quality and more
reliable evidence from the PubMed dataset. This work can reduce the infusion of
incorrect knowledge into responses and help users receive more effective
replies.

</details>


### [94] [TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents](https://arxiv.org/abs/2510.24014)
*Yizhu Jiao,Sha Li,Sizhe Zhou,Heng Ji,Jiawei Han*

Main category: cs.CL

TL;DR: 提出了一种新的信息抽取任务TEXT2DB，旨在将信息抽取结果与目标数据库紧密结合，通过用户指令、文档集和数据库输入，动态更新数据库以满足需求，并提出了OPAL框架来适应不同数据库模式并调用所需IE模型。


<details>
  <summary>Details</summary>
Motivation: 传统信息抽取（IE）输出常因与下游应用本体不匹配而难以直接使用，因此需要一种能根据具体应用需求灵活调整抽取方式的新方法。

Method: 提出TEXT2DB任务，要求模型根据用户指令从文档集中提取信息并更新数据库；设计OPAL框架，包含观察、规划和分析三个组件，利用LLM生成基于代码的抽取计划并调用IE模型，同时提供执行前的质量反馈。

Result: 实验表明OPAL能够成功适应多种数据库结构，生成不同的代码计划并调用合适的IE模型完成数据填充、行插入和列扩展等任务，但在处理大规模复杂依赖数据库和抽取幻觉方面仍存在挑战。

Conclusion: TEXT2DB为信息抽取与数据库集成提供了新范式，OPAL展示了在动态Schema下良好的适应性，未来需进一步研究复杂场景下的鲁棒性和准确性。

Abstract: The task of information extraction (IE) is to extract structured knowledge
from text. However, it is often not straightforward to utilize IE output due to
the mismatch between the IE ontology and the downstream application needs. We
propose a new formulation of IE TEXT2DB that emphasizes the integration of IE
output and the target database (or knowledge base). Given a user instruction, a
document set, and a database, our task requires the model to update the
database with values from the document set to satisfy the user instruction.
This task requires understanding user instructions for what to extract and
adapting to the given DB/KB schema for how to extract on the fly. To evaluate
this new task, we introduce a new benchmark featuring common demands such as
data infilling, row population, and column addition. In addition, we propose an
LLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer
component that interacts with the database, the Planner component that
generates a code-based plan with calls to IE models, and the Analyzer component
that provides feedback regarding code quality before execution. Experiments
show that OPAL can successfully adapt to diverse database schemas by generating
different code plans and calling the required IE models. We also highlight
difficult cases such as dealing with large databases with complex dependencies
and extraction hallucination, which we believe deserve further investigation.
Source code: https://github.com/yzjiao/Text2DB

</details>


### [95] [Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward](https://arxiv.org/abs/2510.24020)
*Hao An,Yang Xu*

Main category: cs.CL

TL;DR: 提出一种基于细粒度语义置信度奖励（FiSCoRe）的强化学习框架，通过样本特定的置信度指导大语言模型在知识边界外准确 abstain，提升领域内和分布外基准的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖粗粒度信号（如整体置信度）来引导LLM拒绝回答，导致对知识边界认知不精确，难以可靠地缓解幻觉问题。

Method: 提出FiSCoRe框架：通过对多个候选答案进行语义聚类，训练LLM保留高置信度聚类中的答案，舍弃低置信度聚类中的答案，实现细粒度的后验拒绝决策；采用强化学习进行优化。

Result: 在领域内和分布外基准上显著提升了LLM拒绝回答的可靠性，优于现有fine-tuning方法；同时提出一个新的评估指标，更全面衡量拒绝微调任务的可靠性。

Conclusion: 细粒度语义置信度信号能更精确刻画LLM的知识边界，所提FiSCoRe框架有效提升其自我拒绝能力，增强部署可靠性。

Abstract: Mitigating hallucinations in Large Language Models (LLMs) is critical for
their reliable deployment. Existing methods typically fine-tune LLMs to abstain
from answering questions beyond their knowledge scope. However, these methods
often rely on coarse-grained signals to guide LLMs to abstain, such as overall
confidence or uncertainty scores on multiple sampled answers, which may result
in an imprecise awareness of the model's own knowledge boundaries. To this end,
we propose a novel reinforcement learning framework built on
$\textbf{\underline{Fi}ne-grained \underline{S}emantic \underline{Co}nfidence
\underline{Re}ward (\Ours)}$, which guides LLMs to abstain via sample-specific
confidence. Specifically, our method operates by sampling multiple candidate
answers and conducting semantic clustering, then training the LLM to retain
answers within high-confidence clusters and discard those within low-confidence
ones, thereby promoting accurate post-hoc abstention. Additionally, we propose
a new metric for evaluating the reliability of abstention fine-tuning tasks
more comprehensively. Our method significantly enhances reliability in both
in-domain and out-of-distribution benchmarks.

</details>


### [96] [SpecKD: Speculative Decoding for Effective Knowledge Distillation of LLMs](https://arxiv.org/abs/2510.24021)
*Haiduo Huang,Jiangcheng Song,Yadong Zhang,Pengju Ren*

Main category: cs.CL

TL;DR: 提出了一种名为Speculative Knowledge Distillation (SpecKD)的新框架，通过动态的、基于token级别的门控机制，在知识蒸馏过程中选择性地应用损失函数，仅对教师模型高置信度的“接受”token进行蒸馏，从而提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏对所有token均匀应用蒸馏损失，忽略了教师模型预测的不确定性，导致学生模型学习到噪声，尤其在教师模型远大于学生模型时可能损害性能。

Method: 受推测解码中“提出-验证”范式的启发，SpecKD在每一步使用学生模型生成token提议，并与教师模型的概率分布进行比对；只有被验证为“接受”的token才计算蒸馏损失，“拒绝”的token则被掩码。

Result: 在多种文本生成任务上的实验表明，SpecKD显著优于强基线方法，训练更稳定，学生模型能力更强，并实现了最先进的性能。

Conclusion: SpecKD通过引入基于验证的动态门控机制，有效减少了知识蒸馏中的噪声传递，是一种高效、即插即用的知识蒸馏新范式。

Abstract: Knowledge Distillation (KD) has become a cornerstone technique for
compressing Large Language Models (LLMs) into smaller, more efficient student
models. However, conventional KD approaches typically apply the distillation
loss uniformly across all tokens, regardless of the teacher's confidence. This
indiscriminate mimicry can introduce noise, as the student is forced to learn
from the teacher's uncertain or high-entropy predictions, which may ultimately
harm student performance-especially when the teacher is much larger and more
powerful. To address this, we propose Speculative Knowledge Distillation
(SpecKD), a novel, plug-and-play framework that introduces a dynamic,
token-level gating mechanism inspired by the "propose-and-verify" paradigm of
speculative decoding. At each step, the student's token proposal is verified
against the teacher's distribution; the distillation loss is selectively
applied only to "accepted" tokens, while "rejected" tokens are masked out.
Extensive experiments on diverse text generation tasks show that SpecKD
consistently and significantly outperforms strong KD baselines, leading to more
stable training and more capable student models, and achieving state-of-the-art
results.

</details>


### [97] [Success and Cost Elicit Convention Formation for Efficient Communication](https://arxiv.org/abs/2510.24023)
*Saujas Vaduguru,Yilun Hua,Yoav Artzi,Daniel Fried*

Main category: cs.CL

TL;DR: 提出一种训练大型多模态模型形成语言惯例的方法，通过模拟指代游戏实现与人类的高效沟通，显著缩短消息长度并提高交互成功率。


<details>
  <summary>Details</summary>
Motivation: 受人类在交流中利用共享语境形成简洁高效语言惯例的启发，希望让模型也能通过学习形成类似的约定以提升沟通效率。

Method: 采用模拟的指代游戏在模型间进行训练，无需额外的人工标注数据，通过重复交互促使模型形成并使用语言惯例。

Result: 在涉及照片和图形的实验中，该方法使消息长度最多减少41%，任务成功率提高15%，且人类响应速度更快；仅优化成功或成本无法有效促成惯例形成。

Conclusion: 同时优化沟通成功与成本是形成语言惯例的关键，该方法可有效提升人机沟通的效率与自然性。

Abstract: Humans leverage shared conversational context to become increasingly
successful and efficient at communicating over time. One manifestation of this
is the formation of ad hoc linguistic conventions, which allow people to
coordinate on short, less costly utterances that are understood using shared
conversational context. We present a method to train large multimodal models to
form conventions, enabling efficient communication. Our approach uses simulated
reference games between models, and requires no additional human-produced data.
In repeated reference games involving photographs and tangram images, our
method enables models to communicate efficiently with people: reducing the
message length by up to 41% while increasing success by 15% over the course of
the interaction. Human listeners respond faster when interacting with our model
that forms conventions. We also show that training based on success or cost
alone is insufficient - both are necessary to elicit convention formation.

</details>


### [98] [Pie: A Programmable Serving System for Emerging LLM Applications](https://arxiv.org/abs/2510.24051)
*In Gim,Zhiyao Ma,Seung-seob Lee,Lin Zhong*

Main category: cs.CL

TL;DR: 本文提出了Pie，一个可编程的大型语言模型服务系统，通过将生成循环分解为细粒度的服务处理器，并允许用户通过称为inferlets的程序控制生成过程，从而提高灵活性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM服务系统基于单一的token生成循环，难以应对多样化的推理策略和代理工作流，限制了灵活性和性能。

Method: Pie将传统的生成循环分解为可通过API访问的细粒度服务处理器，使用WebAssembly运行用户提供的inferlets程序，实现对KV缓存策略、生成逻辑以及计算与I/O集成的自定义控制。

Result: 在标准任务上，Pie仅带来3-12%的延迟开销；在代理工作流中，通过应用特定优化，延迟和吞吐量提升了1.3x-3.4x。

Conclusion: Pie在保持高性能的同时，显著提升了LLM服务系统的灵活性和效率，支持无需修改系统本身即可实现复杂的应用级优化。

Abstract: Emerging large language model (LLM) applications involve diverse reasoning
strategies and agentic workflows, straining the capabilities of existing
serving systems built on a monolithic token generation loop. This paper
introduces Pie, a programmable LLM serving system designed for flexibility and
efficiency. Pie decomposes the traditional generation loop into fine-grained
service handlers exposed via an API and delegates control of the generation
process to user-provided programs, called inferlets. This enables applications
to implement new KV cache strategies, bespoke generation logic, and seamlessly
integrate computation and I/O-entirely within the application, without
requiring modifications to the serving system. Pie executes inferlets using
WebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows
Pie matches state-of-the-art performance on standard tasks (3-12% latency
overhead) while significantly improving latency and throughput (1.3x-3.4x
higher) on agentic workflows by enabling application-specific optimizations.

</details>


### [99] [Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling Hallucination in Translation](https://arxiv.org/abs/2510.24073)
*Xinwei Wu,Heng Liu,Jiang Zhou,Xiaohu Zhao,Linlong Xu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一个名为HalloMTBench的多语言基准测试，用于诊断大型语言模型（LLM）在机器翻译中的幻觉问题，揭示了不同模型在翻译中产生幻觉的独特触发因素。


<details>
  <summary>Details</summary>
Motivation: 现有的机器翻译基准无法有效暴露多语言大模型中的幻觉问题，因此需要一个专门的诊断框架来识别和分类这些错误。

Method: 提出了一种包含指令脱离和源脱离分类的诊断框架，并基于此构建了覆盖11个英外翻译方向的人工验证基准HalloMTBench；使用4个前沿大模型生成候选翻译，结合多个大模型裁判和专家验证筛选出5,435个高质量样本，最终评估了17个大模型的表现。

Result: 发现了一系列‘幻觉触发因素’，包括模型规模、对源文本长度的敏感性、语言偏见以及强化学习导致的语言混合现象；结果显示不同模型存在独特的失败模式。

Conclusion: HalloMTBench为诊断多语言大模型翻译中的幻觉问题提供了一个前瞻性的测试平台，有助于未来改进模型的可靠性和准确性。

Abstract: Large Language Models (LLMs) have advanced machine translation but remain
vulnerable to hallucinations. Unfortunately, existing MT benchmarks are not
capable of exposing failures in multilingual LLMs. To disclose hallucination in
multilingual LLMs, we introduce a diagnostic framework with a taxonomy that
separates Instruction Detachment from Source Detachment. Guided by this
taxonomy, we create HalloMTBench, a multilingual, human-verified benchmark
across 11 English-to-X directions. We employed 4 frontier LLMs to generate
candidates and scrutinize these candidates with an ensemble of LLM judges, and
expert validation. In this way, we curate 5,435 high-quality instances. We have
evaluated 17 LLMs on HalloMTBench. Results reveal distinct ``hallucination
triggers'' -- unique failure patterns reflecting model scale, source length
sensitivity, linguistic biases, and Reinforcement-Learning (RL) amplified
language mixing. HalloMTBench offers a forward-looking testbed for diagnosing
LLM translation failures. HalloMTBench is available in
https://huggingface.co/collections/AIDC-AI/marco-mt.

</details>


### [100] [Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures](https://arxiv.org/abs/2510.24081)
*Tyler A. Chang,Catherine Arnett,Abdelrahman Eldesokey,Abdelrahman Sadallah,Abeer Kashar,Abolade Daud,Abosede Grace Olanihun,Adamu Labaran Mohammed,Adeyemi Praise,Adhikarinayum Meerajita Sharma,Aditi Gupta,Afitab Iyigun,Afonso Simplício,Ahmed Essouaied,Aicha Chorana,Akhil Eppa,Akintunde Oladipo,Akshay Ramesh,Aleksei Dorkin,Alfred Malengo Kondoro,Alham Fikri Aji,Ali Eren Çetintaş,Allan Hanbury,Alou Dembele,Alp Niksarli,Álvaro Arroyo,Amin Bajand,Amol Khanna,Ana Chkhaidze,Ana Condez,Andiswa Mkhonto,Andrew Hoblitzell,Andrew Tran,Angelos Poulis,Anirban Majumder,Anna Vacalopoulou,Annette Kuuipolani Kanahele Wong,Annika Simonsen,Anton Kovalev,Ashvanth. S,Ayodeji Joseph Lana,Barkin Kinay,Bashar Alhafni,Benedict Cibalinda Busole,Bernard Ghanem,Bharti Nathani,Biljana Stojanovska Đurić,Bola Agbonile,Bragi Bergsson,Bruce Torres Fischer,Burak Tutar,Burcu Alakuş Çınar,Cade J. Kanoniakapueo Kane,Can Udomcharoenchaikit,Catherine Arnett,Chadi Helwe,Chaithra Reddy Nerella,Chen Cecilia Liu,Chiamaka Glory Nwokolo,Cristina España-Bonet,Cynthia Amol,DaeYeop Lee,Dana Arad,Daniil Dzenhaliou,Daria Pugacheva,Dasol Choi,Daud Abolade,David Liu,David Semedo,Deborah Popoola,Deividas Mataciunas,Delphine Nyaboke,Dhyuthy Krishna Kumar,Diogo Glória-Silva,Diogo Tavares,Divyanshu Goyal,DongGeon Lee,Ebele Nwamaka Anajemba,Egonu Ngozi Grace,Elena Mickel,Elena Tutubalina,Elias Herranen,Emile Anand,Emmanuel Habumuremyi,Emuobonuvie Maria Ajiboye,Eryawan Presma Yulianrifat,Esther Adenuga,Ewa Rudnicka,Faith Olabisi Itiola,Faran Taimoor Butt,Fathima Thekkekara,Fatima Haouari,Filbert Aurelian Tjiaranata,Firas Laakom,Francesca Grasso,Francesco Orabona,Francesco Periti,Gbenga Kayode Solomon,Gia Nghia Ngo,Gloria Udhehdhe-oze,Gonçalo Martins,Gopi Naga Sai Ram Challagolla,Guijin Son,Gulnaz Abdykadyrova,Hafsteinn Einarsson,Hai Hu,Hamidreza Saffari,Hamza Zaidi,Haopeng Zhang,Harethah Abu Shairah,Harry Vuong,Hele-Andra Kuulmets,Houda Bouamor,Hwanjo Yu,Iben Nyholm Debess,İbrahim Ethem Deveci,Ikhlasul Akmal Hanif,Ikhyun Cho,Inês Calvo,Inês Vieira,Isaac Manzi,Ismail Daud,Itay Itzhak,Iuliia,Alekseenko,Ivan Belashkin,Ivan Spada,Ivan Zhelyazkov,Jacob Brinton,Jafar Isbarov,Jaka Čibej,Jan Čuhel,Jan Kocoń,Jauza Akbar Krito,Jebish Purbey,Jennifer Mickel,Jennifer Za,Jenny Kunz,Jihae Jeong,Jimena Tena Dávalos,Jinu Lee,João Magalhães,John Yi,Jongin Kim,Joseph Chataignon,Joseph Marvin Imperial,Jubeerathan Thevakumar,Judith Land,Junchen Jiang,Jungwhan Kim,Kairit Sirts,Kamesh R,Kamesh V,Kanda Patrick Tshinu,Kätriin Kukk,Kaustubh Ponkshe,Kavsar Huseynova,Ke He,Kelly Buchanan,Kengatharaiyer Sarveswaran,Kerem Zaman,Khalil Mrini,Kian Kyars,Krister Kruusmaa,Kusum Chouhan,Lainitha Krishnakumar,Laura Castro Sánchez,Laura Porrino Moscoso,Leshem Choshen,Levent Sencan,Lilja Øvrelid,Lisa Alazraki,Lovina Ehimen-Ugbede,Luheerathan Thevakumar,Luxshan Thavarasa,Mahnoor Malik,Mamadou K. Keita,Mansi Jangid,Marco De Santis,Marcos García,Marek Suppa,Mariam D'Ciofalo,Marii Ojastu,Maryam Sikander,Mausami Narayan,Maximos Skandalis,Mehak Mehak,Mehmet İlteriş Bozkurt,Melaku Bayu Workie,Menan Velayuthan,Michael Leventhal,Michał Marcińczuk,Mirna Potočnjak,Mohammadamin Shafiei,Mridul Sharma,Mrityunjaya Indoria,Muhammad Ravi Shulthan Habibi,Murat Kolić,Nada Galant,Naphat Permpredanun,Narada Maugin,Nicholas Kluge Corrêa,Nikola Ljubešić,Nirmal Thomas,Nisansa de Silva,Nisheeth Joshi,Nitish Ponkshe,Nizar Habash,Nneoma C. Udeze,Noel Thomas,Noémi Ligeti-Nagy,Nouhoum Coulibaly,Nsengiyumva Faustin,Odunayo Kareemat Buliaminu,Odunayo Ogundepo,Oghojafor Godswill Fejiro,Ogundipe Blessing Funmilola,Okechukwu God'spraise,Olanrewaju Samuel,Olaoye Deborah Oluwaseun,Olasoji Akindejoye,Olga Popova,Olga Snissarenko,Onyinye Anulika Chiemezie,Orkun Kinay,Osman Tursun,Owoeye Tobiloba Moses,Oyelade Oluwafemi Joshua,Oyesanmi Fiyinfoluwa,Pablo Gamallo,Pablo Rodríguez Fernández,Palak Arora,Pedro Valente,Peter Rupnik,Philip Oghenesuowho Ekiugbo,Pramit Sahoo,Prokopis Prokopidis,Pua Niau-Puhipau,Quadri Yahya,Rachele Mignone,Raghav Singhal,Ram Mohan Rao Kadiyala,Raphael Merx,Rapheal Afolayan,Ratnavel Rajalakshmi,Rishav Ghosh,Romina Oji,Ron Kekeha Solis,Rui Guerra,Rushikesh Zawar,Sa'ad Nasir Bashir,Saeed Alzaabi,Sahil Sandeep,Sai Pavan Batchu,SaiSandeep Kantareddy,Salsabila Zahirah Pranida,Sam Buchanan,Samuel Rutunda,Sander Land,Sarah Sulollari,Sardar Ali,Saroj Sapkota,Saulius Tautvaisas,Sayambhu Sen,Sayantani Banerjee,Sebastien Diarra,SenthilNathan. M,Sewoong Lee,Shaan Shah,Shankar Venkitachalam,Sharifa Djurabaeva,Sharon Ibejih,Shivanya Shomir Dutta,Siddhant Gupta,Silvia Paniagua Suárez,Sina Ahmadi,Sivasuthan Sukumar,Siyuan Song,Snegha A.,Sokratis Sofianopoulos,Sona Elza Simon,Sonja Benčina,Sophie Gvasalia,Sphurti Kirit More,Spyros Dragazis,Stephan P. Kaufhold,Suba. S,Sultan AlRashed,Surangika Ranathunga,Taiga Someya,Taja Kuzman Pungeršek,Tal Haklay,Tasi'u Jibril,Tatsuya Aoyama,Tea Abashidze,Terenz Jomar Dela Cruz,Terra Blevins,Themistoklis Nikas,Theresa Dora Idoko,Thu Mai Do,Tilek Chubakov,Tommaso Gargiani,Uma Rathore,Uni Johannesen,Uwuma Doris Ugwu,Vallerie Alexandra Putra,Vanya Bannihatti Kumar,Varsha Jeyarajalingam,Varvara Arzt,Vasudevan Nedumpozhimana,Viktoria Ondrejova,Viktoryia Horbik,Vishnu Vardhan Reddy Kummitha,Vuk Dinić,Walelign Tewabe Sewunetie,Winston Wu,Xiaojing Zhao,Yacouba Diarra,Yaniv Nikankin,Yash Mathur,Yixi Chen,Yiyuan Li,Yolanda Xavier,Yonatan Belinkov,Yusuf Ismail Abayomi,Zaid Alyafeai,Zhengyang Shan,Zhi Rui Tam,Zilu Tang,Zuzana Nadova,Baber Abbasi,Stella Biderman,David Stap,Duygu Ataman,Fabian Schmidt,Hila Gonen,Jiayi Wang,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文提出了Global PIQA，一个由全球335名研究人员参与构建的、涵盖100多种语言和文化的常识推理基准，用于评估大语言模型在不同文化背景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型缺乏覆盖多语言、多文化的评测基准，难以反映模型在不同文化常识理解上的差异。

Method: 通过全球研究人员协作，手工构建包含116种语言变体的非平行常识推理数据集，涵盖五大洲、14个语系和23种书写系统，并融入大量本地化文化元素。

Result: 实验显示当前最先进的大语言模型在整体上表现良好，但在低资源语言中准确率差距高达37%（随机猜测为50%），开源模型普遍不如闭源模型。

Conclusion: Global PIQA揭示了大语言模型在多文化常识理解上的不足，尤其是在低资源语言中，提示未来研究需关注文化多样性与本地知识建模。

Abstract: To date, there exist almost no culturally-specific evaluation benchmarks for
large language models (LLMs) that cover a large number of languages and
cultures. In this paper, we present Global PIQA, a participatory commonsense
reasoning benchmark for over 100 languages, constructed by hand by 335
researchers from 65 countries around the world. The 116 language varieties in
Global PIQA cover five continents, 14 language families, and 23 writing
systems. In the non-parallel split of Global PIQA, over 50% of examples
reference local foods, customs, traditions, or other culturally-specific
elements. We find that state-of-the-art LLMs perform well on Global PIQA in
aggregate, but they exhibit weaker performance in lower-resource languages (up
to a 37% accuracy gap, despite random chance at 50%). Open models generally
perform worse than proprietary models. Global PIQA highlights that in many
languages and cultures, everyday knowledge remains an area for improvement,
alongside more widely-discussed capabilities such as complex reasoning and
expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA
provides a glimpse into the wide diversity of cultures in which human language
is embedded.

</details>


### [101] [RegSpeech12: A Regional Corpus of Bengali Spontaneous Speech Across Dialects](https://arxiv.org/abs/2510.24096)
*Md. Rezuwan Hassan,Azmol Hossain,Kanij Fatema,Rubayet Sabbir Faruque,Tanmoy Shome,Ruwad Naswan,Trina Chakraborty,Md. Foriduzzaman Zihad,Tawsif Tashwar Dipto,Nazia Tasnim,Nazmuddoha Ansary,Md. Mehedi Hasan Shawon,Ahmed Imtiaz Humayun,Md. Golam Rabiul Alam,Farig Sadeque,Asif Sushmit*

Main category: cs.CL

TL;DR: 本研究探讨了孟加拉语方言的语音和形态特征，旨在构建针对地区变体的自动语音识别（ASR）系统，促进语言技术的包容性发展。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语存在丰富的方言多样性，但在计算处理方面的研究有限，亟需系统性探索以支持语言技术的发展和方言保护。

Method: 通过分析五种主要方言群及孟加拉国内多个地区的语言变异，记录其语音与形态特征，并探索构建面向区域方言的ASR系统的可行性。

Result: 成功构建了一个用于研究孟加拉语方言的公开数据集，并验证了开发区域性ASR模型的潜力。

Conclusion: 该研究为孟加拉语方言的计算处理奠定了基础，有助于推动包容性语言技术发展和方言多样性保护。

Abstract: The Bengali language, spoken extensively across South Asia and among
diasporic communities, exhibits considerable dialectal diversity shaped by
geography, culture, and history. Phonological and pronunciation-based
classifications broadly identify five principal dialect groups: Eastern
Bengali, Manbhumi, Rangpuri, Varendri, and Rarhi. Within Bangladesh, further
distinctions emerge through variation in vocabulary, syntax, and morphology, as
observed in regions such as Chittagong, Sylhet, Rangpur, Rajshahi, Noakhali,
and Barishal. Despite this linguistic richness, systematic research on the
computational processing of Bengali dialects remains limited. This study seeks
to document and analyze the phonetic and morphological properties of these
dialects while exploring the feasibility of building computational models
particularly Automatic Speech Recognition (ASR) systems tailored to regional
varieties. Such efforts hold potential for applications in virtual assistants
and broader language technologies, contributing to both the preservation of
dialectal diversity and the advancement of inclusive digital tools for
Bengali-speaking communities. The dataset created for this study is released
for public use.

</details>


### [102] [Squrve: A Unified and Modular Framework for Complex Real-World Text-to-SQL Tasks](https://arxiv.org/abs/2510.24102)
*Yihan Wang,Peiyu Liu,Runyu Chen,Jiaxing Pu,Wei Xu*

Main category: cs.CL

TL;DR: Squrve是一个统一、模块化且全面的Text-to-SQL框架，通过标准化执行范式和多角色协作机制，有效整合研究进展与实际应用，实验表明其协作工作流优于原有独立方法。


<details>
  <summary>Details</summary>
Motivation: 尽管Text-to-SQL技术在学术上取得显著进展，但在现实系统中部署仍面临集成工具不足的挑战。

Method: 提出Squrve框架，建立通用执行范式以标准化调用接口，并设计基于七个抽象原子组件的多角色协作机制。

Result: 在广泛使用的基准测试上实验表明，该框架的协作工作流始终优于原始的单一方法，能更有效地处理复杂的现实查询。

Conclusion: Squrve为连接Text-to-SQL的研究成果与实际应用提供了新途径，推动了该技术在真实场景中的落地。

Abstract: Text-to-SQL technology has evolved rapidly, with diverse academic methods
achieving impressive results. However, deploying these techniques in real-world
systems remains challenging due to limited integration tools. Despite these
advances, we introduce Squrve, a unified, modular, and extensive Text-to-SQL
framework designed to bring together research advances and real-world
applications. Squrve first establishes a universal execution paradigm that
standardizes invocation interfaces, then proposes a multi-actor collaboration
mechanism based on seven abstracted effective atomic actor components.
Experiments on widely adopted benchmarks demonstrate that the collaborative
workflows consistently outperform the original individual methods, thereby
opening up a new effective avenue for tackling complex real-world queries. The
codes are available at https://github.com/Satissss/Squrve.

</details>


### [103] [Reinforcement Learning for Long-Horizon Multi-Turn Search Agents](https://arxiv.org/abs/2510.24126)
*Vivek Kalyan,Martin Andrews*

Main category: cs.CL

TL;DR: 该研究通过强化学习训练140亿参数的大型语言模型，在法律文档检索任务中显著超越前沿模型（准确率85% vs 78%），并表明多轮交互能提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过强化学习提升大型语言模型代理在复杂任务中的能力，超越基于提示的方法。

Method: 使用强化学习在法律文档搜索基准上训练140亿参数的语言模型，并研究训练和测试时的多轮限制对性能的影响。

Result: RL训练的模型在准确率上达到85%，超过前沿模型的78%；允许更长多轮交互时性能进一步提升。

Conclusion: 强化学习结合多轮交互能显著提升语言模型代理在复杂任务中的表现，优于现有方法。

Abstract: Large Language Model (LLM) agents can leverage multiple turns and tools to
solve complex tasks, with prompt-based approaches achieving strong performance.
This work demonstrates that Reinforcement Learning (RL) can push capabilities
significantly further by learning from experience. Through experiments on a
legal document search benchmark, we show that our RL-trained 14 Billion
parameter model outperforms frontier class models (85% vs 78% accuracy). In
addition, we explore turn-restricted regimes, during training and at test-time,
that show these agents achieve better results if allowed to operate over longer
multi-turn horizons.

</details>


### [104] [Beyond Line-Level Filtering for the Pretraining Corpora of LLMs](https://arxiv.org/abs/2510.24139)
*Chanwoo Park,Suyoung Park,Yelim Ahn,Jongmin Kim,Jongyeon Park,Jaejin Lee*

Main category: cs.CL

TL;DR: 本文提出了两种模式感知的行级过滤方法（PLD和PTF），通过结合行级信号与文档中的序列分布，改进传统过滤技术，有效保留重要结构内容，在多语言小模型训练中显著提升问答和选择题性能。


<details>
  <summary>Details</summary>
Motivation: 传统行级过滤方法可能误删有价值内容，影响下游任务性能，因此需要更精细的过滤策略来保留关键信息。

Method: 提出模式感知的行级去重（PLD）和尾部标点过滤（PTF），在传统方法基础上引入行间序列分布信息，以更好保留结构性重要内容。

Result: 在英语和韩语的小型语言模型（1B参数）上验证，新方法在多项选择题基准上持续提升性能，并显著提高SQuAD v1和KorQuAD v1上的生成式问答准确率。

Conclusion: 所提出的PLD和PTF方法优于传统行级过滤技术，能更有效地保留对模型性能有益的内容，适用于多语言场景下的数据预处理。

Abstract: While traditional line-level filtering techniques, such as line-level
deduplication and trailing-punctuation filters, are commonly used, these basic
methods can sometimes discard valuable content, negatively affecting downstream
performance. In this paper, we introduce two methods-pattern-aware line-level
deduplication (PLD) and pattern-aware trailing punctuation filtering (PTF)-by
enhancing the conventional filtering techniques. Our approach not only
considers line-level signals but also takes into account their sequential
distribution across documents, enabling us to retain structurally important
content that might otherwise be removed. We evaluate these proposed methods by
training small language models (1 B parameters) in both English and Korean. The
results demonstrate that our methods consistently improve performance on
multiple-choice benchmarks and significantly enhance generative
question-answering accuracy on both SQuAD v1 and KorQuAD v1.

</details>


### [105] [Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding Korean](https://arxiv.org/abs/2510.24150)
*Chanwoo Park,Suyoung Park,JiA Kang,Jongyeon Park,Sangho Kim,Hyunji M. Park,Sumin Bae,Mingyu Kang,Jaejin Lee*

Main category: cs.CL

TL;DR: Ko-MuSR是首个针对长篇韩语叙事中多步骤、软推理能力的综合评测基准，基于MuSR构建，确保数据无污染。它包含全韩语叙述、推理链和经人工验证的多项选择题。评估结果显示，多语言大模型在韩语推理任务上优于韩语专用模型，表明推理能力具有跨语言泛化性。结合少样本示例、推理轨迹和任务提示的策略显著提升准确率，接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有韩语NLP基准在评估长文本多步推理能力方面不足，且易受数据污染影响，缺乏专门针对韩语叙事推理的高质量评测集。

Method: 基于MuSR框架构建全韩语多步推理基准Ko-MuSR，包含人工验证的叙事文本、推理链和选择题；评估多个大语言模型在该基准上的表现，并测试不同提示策略（如少样本、推理轨迹、任务提示）对性能的影响。

Result: 多语言大模型在Ko-MuSR上表现优于韩语专用模型；精心设计的提示策略显著提升模型准确率，接近人类水平；验证了推理能力的跨语言泛化特性。

Conclusion: Ko-MuSR为韩语长文本推理提供了可靠评测基准，推动韩语NLP发展；结果表明多语言模型具备跨语言推理泛化能力，且提示工程对提升推理性能至关重要。

Abstract: We present Ko-MuSR, the first benchmark to comprehensively evaluate
multistep, soft reasoning in long Korean narratives while minimizing data
contamination. Built following MuSR, Ko-MuSR features fully Korean narratives,
reasoning chains, and multiple-choice questions verified by human annotators
for logical consistency and answerability. Evaluations of four large language
models -- two multilingual and two Korean-specialized -- show that multilingual
models outperform Korean-focused ones even in Korean reasoning tasks,
indicating cross-lingual generalization of reasoning ability. Carefully
designed prompting strategies, which combine few-shot examples, reasoning
traces, and task-specific hints, further boost accuracy, approaching
human-level performance. Ko-MuSR offers a solid foundation for advancing Korean
NLP by enabling systematic evaluation of long-context reasoning and prompting
strategies.

</details>


### [106] [MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations](https://arxiv.org/abs/2510.24178)
*Aaron Scott,Maike Züfle,Jan Niehues*

Main category: cs.CL

TL;DR: 本文介绍了首个德语多模态讽刺检测数据集MuSaG，包含来自德国电视节目的文本、音频和视频对齐数据，并对多种模型进行了基准测试，揭示了当前多模态模型在模仿人类依赖音频进行讽刺识别方面的不足。


<details>
  <summary>Details</summary>
Motivation: 由于社交媒体和流行文化中讽刺现象普遍，给自然语言理解、情感分析等任务带来挑战，而现有模型在多模态讽刺检测上尚未充分整合音频和视觉线索，尤其缺乏德语相关资源。

Method: 构建了一个包含33分钟人工筛选和标注的德国电视节目数据集MuSaG，涵盖文本、音频、视频三种模态，并分别进行人工标注；在此基础上对九种开源与商业模型在单模态和多模态设置下进行基准测试，与人类表现对比。

Result: 实验表明，人类在对话场景中更依赖音频线索识别讽刺，而现有模型在文本上的表现最佳，显示出当前多模态模型与人类认知方式之间存在差距。

Conclusion: 当前多模态讽刺检测模型仍以文本为主导，未能有效模拟人类对音频信息的利用，MuSaG的发布有助于推动更贴近真实场景的多模态模型发展及人机对齐研究。

Abstract: Sarcasm is a complex form of figurative language in which the intended
meaning contradicts the literal one. Its prevalence in social media and popular
culture poses persistent challenges for natural language understanding,
sentiment analysis, and content moderation. With the emergence of multimodal
large language models, sarcasm detection extends beyond text and requires
integrating cues from audio and vision. We present MuSaG, the first German
multimodal sarcasm detection dataset, consisting of 33 minutes of manually
selected and human-annotated statements from German television shows. Each
instance provides aligned text, audio, and video modalities, annotated
separately by humans, enabling evaluation in unimodal and multimodal settings.
We benchmark nine open-source and commercial models, spanning text, audio,
vision, and multimodal architectures, and compare their performance to human
annotations. Our results show that while humans rely heavily on audio in
conversational settings, models perform best on text. This highlights a gap in
current multimodal models and motivates the use of MuSaG for developing models
better suited to realistic scenarios. We release MuSaG publicly to support
future research on multimodal sarcasm detection and human-model alignment.

</details>


### [107] [Exploring the Influence of Relevant Knowledge for Natural Language Generation Interpretability](https://arxiv.org/abs/2510.24179)
*Iván Martínez-Murillo,Paloma Moreda,Elena Lloret*

Main category: cs.CL

TL;DR: 该论文研究了外部知识整合在自然语言生成（NLG）中的作用，特别是在常识生成任务中。作者扩展了CommonGen数据集，构建了包含ConceptNet语义关系的KITGI基准，并使用T5-Large模型比较了完整与过滤知识下的生成效果，发现相关外部知识对生成质量至关重要。


<details>
  <summary>Details</summary>
Motivation: 探索外部知识如何影响NLG系统的常识生成能力，并建立可解释的评估基准来揭示知识依赖性。

Method: 构建KITGI数据集，结合ConceptNet的语义关系；采用T5-Large模型，在完整知识和过滤知识条件下生成句子；通过三阶段方法进行可解释性分析：移除关键知识、重新生成句子、人工评估常识合理性和概念覆盖。

Result: 使用完整知识时生成结果正确率达91%，而过滤知识后性能骤降至6%，表明高度相关的外部知识对维持生成文本的连贯性和概念完整性至关重要。

Conclusion: 相关外部知识是高质量常识生成的关键；应设计更具可解释性的知识增强型NLG系统，并发展能捕捉深层推理过程的评估框架。

Abstract: This paper explores the influence of external knowledge integration in
Natural Language Generation (NLG), focusing on a commonsense generation task.
We extend the CommonGen dataset by creating KITGI, a benchmark that pairs input
concept sets with retrieved semantic relations from ConceptNet and includes
manually annotated outputs. Using the T5-Large model, we compare sentence
generation under two conditions: with full external knowledge and with filtered
knowledge where highly relevant relations were deliberately removed. Our
interpretability benchmark follows a three-stage method: (1) identifying and
removing key knowledge, (2) regenerating sentences, and (3) manually assessing
outputs for commonsense plausibility and concept coverage. Results show that
sentences generated with full knowledge achieved 91\% correctness across both
criteria, while filtering reduced performance drastically to 6\%. These
findings demonstrate that relevant external knowledge is critical for
maintaining both coherence and concept coverage in NLG. This work highlights
the importance of designing interpretable, knowledge-enhanced NLG systems and
calls for evaluation frameworks that capture the underlying reasoning beyond
surface-level metrics.

</details>


### [108] [Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large Language Models through Latent Semantic Alignment](https://arxiv.org/abs/2510.24208)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于潜在空间语义对齐的跨尺度大语言模型知识迁移方法，利用激活值作为层间知识传递的媒介，克服了神经不兼容性问题，显著提升了知识迁移的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的参数重用方法由于不同规模模型之间的架构和参数差异（神经不兼容性），在细粒度参数化知识迁移上存在局限，因此需要一种更有效的跨尺度知识迁移机制。

Method: 通过将激活值作为层间知识传递的媒介，利用潜在空间中的语义对齐来实现跨尺度知识迁移，而不是直接复用层参数。

Result: 在四个基准上的实验表明，该方法优于先前的工作，能够更好地对齐不同规模模型的行为，并揭示了促进跨尺度知识迁移的关键因素。

Conclusion: 语义对齐是实现大语言模型跨尺度知识迁移的基础，所提方法为模型间知识迁移提供了更灵活、更通用的解决方案。

Abstract: Large Language Models (LLMs) encode vast amounts of knowledge in their
massive parameters, which is accessible to locate, trace, and analyze. Despite
advances in neural interpretability, it is still not clear how to transfer
knowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).
A key problem is enabling effective and efficient knowledge transfer across
LLMs of different scales, which is essential for achieving greater flexibility
and broader applicability in transferring knowledge between LLMs. Due to neural
incompatibility, referring to the architectural and parametric differences
between LLMs of varying scales, existing methods that directly reuse layer
parameters are severely limited. In this paper, we identify the semantic
alignment in latent space as the fundamental prerequisite for LLM cross-scale
knowledge transfer. Instead of directly using the layer parameters, our
approach takes activations as the medium of layer-wise knowledge transfer.
Leveraging the semantics in latent space, our approach is simple and
outperforms prior work, better aligning model behaviors across varying scales.
Evaluations on four benchmarks demonstrate the efficacy of our method. Further
analysis reveals the key factors easing cross-scale knowledge transfer and
provides insights into the nature of latent semantic alignment.

</details>


### [109] [HACK: Hallucinations Along Certainty and Knowledge Axes](https://arxiv.org/abs/2510.24222)
*Adi Simhi,Jonathan Herzig,Itay Itzhak,Dana Arad,Zorik Gekhman,Roi Reichart,Fazl Barez,Gabriel Stanovsky,Idan Szpektor,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识和确定性两个维度对大语言模型中的幻觉进行分类的框架，强调需根据其内在机制设计针对性的缓解策略，并通过模型特定的数据集构建和新评估指标验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多从外部特征分类幻觉，忽视了其内在机制差异可能导致不同类型的幻觉需要不同的缓解策略，因此需要一种基于模型内部属性（如知识与确定性）的分类框架。

Method: 提出沿知识轴和确定性轴分类幻觉的框架；构建模型特定数据集以区分不同类型幻觉；在知识轴上应用转向缓解（steering mitigation）来验证分类有效性；引入新的评估指标衡量缓解方法在高置信错误上的表现。

Result: 验证了知识分类的有效性，发现即使模型具备正确知识仍可能发生幻觉；揭示不同模型间存在不同的知识与幻觉模式；发现在高置信但错误的幻觉上，现有缓解方法效果显著下降。

Conclusion: 必须同时考虑知识和确定性来分析幻觉，呼吁发展针对幻觉内在因素的精细化缓解方法。

Abstract: Hallucinations in LLMs present a critical barrier to their reliable usage.
Existing research usually categorizes hallucination by their external
properties rather than by the LLMs' underlying internal properties. This
external focus overlooks that hallucinations may require tailored mitigation
strategies based on their underlying mechanism. We propose a framework for
categorizing hallucinations along two axes: knowledge and certainty. Since
parametric knowledge and certainty may vary across models, our categorization
method involves a model-specific dataset construction process that
differentiates between those types of hallucinations. Along the knowledge axis,
we distinguish between hallucinations caused by a lack of knowledge and those
occurring despite the model having the knowledge of the correct response. To
validate our framework along the knowledge axis, we apply steering mitigation,
which relies on the existence of parametric knowledge to manipulate model
activations. This addresses the lack of existing methods to validate knowledge
categorization by showing a significant difference between the two
hallucination types. We further analyze the distinct knowledge and
hallucination patterns between models, showing that different hallucinations do
occur despite shared parametric knowledge. Turning to the certainty axis, we
identify a particularly concerning subset of hallucinations where models
hallucinate with certainty despite having the correct knowledge internally. We
introduce a new evaluation metric to measure the effectiveness of mitigation
methods on this subset, revealing that while some methods perform well on
average, they fail disproportionately on these critical cases. Our findings
highlight the importance of considering both knowledge and certainty in
hallucination analysis and call for targeted mitigation approaches that
consider the hallucination underlying factors.

</details>


### [110] [Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?](https://arxiv.org/abs/2510.24236)
*Teague McMillan,Gabriele Dominici,Martin Gjoreski,Marc Langheinrich*

Main category: cs.CL

TL;DR: 研究了推理和训练时的选择如何影响大语言模型在医疗等敏感领域中的解释可信度，发现少量示例的数量和质量、提示设计以及指令微调均显著影响解释的忠实性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的解释常不忠实地反映其预测依据，在医疗场景中可能导致临床医生不信任或决策风险，因此需要探究可控制因素对解释可信度的影响。

Method: 评估三种大语言模型（GPT-4.1-mini、LLaMA 70B、LLaMA 8B）在BBQ和社会偏见数据集及MedQA医学考试数据集上的表现，操纵少量示例的数量与类型、提示策略和训练方式，分析不同因素对解释忠实性的影响。

Result: （1）少量示例的数量和质量显著影响模型解释的忠实性；（2）提示设计对忠实性敏感；（3）指令微调阶段能提升在MedQA上的解释忠实性。

Conclusion: 通过优化少量示例选择、提示策略和指令微调，可在部署阶段有效提升大语言模型在敏感领域中的解释可信度与可解释性。

Abstract: Large Language Models (LLMs) often produce explanations that do not
faithfully reflect the factors driving their predictions. In healthcare
settings, such unfaithfulness is especially problematic: explanations that omit
salient clinical cues or mask spurious shortcuts can undermine clinician trust
and lead to unsafe decision support. We study how inference and training-time
choices shape explanation faithfulness, focusing on factors practitioners can
control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA
8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions),
and manipulate the number and type of few-shot examples, prompting strategies,
and training procedure. Our results show: (i) both the quantity and quality of
few-shot examples significantly impact model faithfulness; (ii) faithfulness is
sensitive to prompting design; (iii) the instruction-tuning phase improves
measured faithfulness on MedQA. These findings offer insights into strategies
for enhancing the interpretability and trustworthiness of LLMs in sensitive
domains.

</details>


### [111] [Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations](https://arxiv.org/abs/2510.24247)
*Ahmad Ghannam,Naif Alharthi,Faris Alasmary,Kholood Al Tabash,Shouq Sadah,Lahouari Ghouti*

Main category: cs.CL

TL;DR: 本文提出了一种结合文本和语音信息的多模态方法，用于阿拉伯语方言句子的变音符号恢复（DR）。模型使用自研预训练模型CATT的编码器处理文本，用OpenAI Whisper基础模型处理语音，并通过早期融合或交叉注意力策略融合两种模态。实验结果表明该方法在开发集和测试集上均取得较低的词错误率和字符错误率。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言缺乏标准书写规范，导致变音符号缺失问题严重，影响自然语言处理性能，因此需要结合多种模态信息提升变音符号恢复的准确性。

Method: 采用两种多模态融合策略：一是将语音帧平均为150个语音token后通过线性投影层与文本token合并；二是利用交叉注意力机制融合文本和语音嵌入。文本编码由CATT模型完成，语音编码基于Whisper base模型，并在训练中随机关闭语音输入以增强鲁棒性。

Result: 在开发集上达到0.25的词错误率（WER）和0.9的字符错误率（CER）；在测试集上WER为0.55，CER为0.13。

Conclusion: 所提出的多模态融合方法能有效提升阿拉伯语方言变音符号恢复性能，尤其在文本与语音信息互补的情况下表现出良好效果，且具备对单模态输入的鲁棒性。

Abstract: In this work, we tackle the Diacritic Restoration (DR) task for Arabic
dialectal sentences using a multimodal approach that combines both textual and
speech information. We propose a model that represents the text modality using
an encoder extracted from our own pre-trained model named CATT. The speech
component is handled by the encoder module of the OpenAI Whisper base model.
Our solution is designed following two integration strategies. The former
consists of fusing the speech tokens with the input at an early stage, where
the 1500 frames of the audio segment are averaged over 10 consecutive frames,
resulting in 150 speech tokens. To ensure embedding compatibility, these
averaged tokens are processed through a linear projection layer prior to
merging them with the text tokens. Contextual encoding is guaranteed by the
CATT encoder module. The latter strategy relies on cross-attention, where text
and speech embeddings are fused. The cross-attention output is then fed to the
CATT classification head for token-level diacritic prediction. To further
improve model robustness, we randomly deactivate the speech input during
training, allowing the model to perform well with or without speech. Our
experiments show that the proposed approach achieves a word error rate (WER) of
0.25 and a character error rate (CER) of 0.9 on the development set. On the
test set, our model achieved WER and CER scores of 0.55 and 0.13, respectively.

</details>


### [112] [Evaluating LLMs on Generating Age-Appropriate Child-Like Conversations](https://arxiv.org/abs/2510.24250)
*Syed Zohaib Hassan,Pål Halvorsen,Miriam S. Johnson,Pierre Lison*

Main category: cs.CL

TL;DR: 本研究评估了五种大语言模型（GPT-4、RUTER-LLAMA-2-13b、GPTSW、NorMistral-7b 和 NorBloom-7b）生成适合5岁和9岁儿童的挪威语对话的能力，结果表明大多数模型生成的语言超出目标年龄段的实际语言水平，尤其在低资源语言中面临数据匮乏的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型主要基于成人对话数据训练，在面向儿童的特殊应用场景中难以生成真实、适龄的儿童语言，因此需要评估现有模型在生成符合儿童语言发展水平的对话方面的能力。

Method: 比较五种大语言模型生成的挪威语儿童对话，并由11名教育专业人士进行盲评，使用真实儿童访谈数据与模型生成文本进行对比，评估其真实性与年龄适宜性。

Result: 评估者在判断5岁儿童语言时准确率高于9岁儿童，且评估者间信度较高（ICC=0.75）；GPT-4 和 NorBloom-7b 表现相对较好，但多数模型生成的语言被认为过于复杂，超出目标年龄组的语言发展水平。

Conclusion: 当前大语言模型在生成适龄儿童语言方面存在明显局限，特别是在低资源语言环境下，缺乏足够的适龄词汇资源是主要瓶颈，需针对性改进训练数据与建模策略。

Abstract: Large Language Models (LLMs), predominantly trained on adult conversational
data, face significant challenges when generating authentic, child-like
dialogue for specialized applications. We present a comparative study
evaluating five different LLMs (GPT-4, RUTER-LLAMA-2-13b, GPTSW, NorMistral-7b,
and NorBloom-7b) to generate age-appropriate Norwegian conversations for
children aged 5 and 9 years. Through a blind evaluation by eleven education
professionals using both real child interview data and LLM-generated text
samples, we assessed authenticity and developmental appropriateness. Our
results show that evaluators achieved strong inter-rater reliability (ICC=0.75)
and demonstrated higher accuracy in age prediction for younger children
(5-year-olds) compared to older children (9-year-olds). While GPT-4 and
NorBloom-7b performed relatively well, most models generated language perceived
as more linguistically advanced than the target age groups. These findings
highlight critical data-related challenges in developing LLM systems for
specialized applications involving children, particularly in low-resource
languages where comprehensive age-appropriate lexical resources are scarce.

</details>


### [113] [From Memorization to Reasoning in the Spectrum of Loss Curvature](https://arxiv.org/abs/2510.24256)
*Jack Merullo,Srihita Vatsavaya,Lucius Bushnaq,Owen Lewis*

Main category: cs.CL

TL;DR: 本文通过损失曲率分解揭示了Transformer模型中记忆化的表示方式，提出一种基于曲率的权重编辑方法，能有效抑制未目标记忆数据的复述，同时保持较低的困惑度，并发现事实检索和算术任务依赖于权重空间中的特定方向。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络中记忆化的表示机制，并探索如何在不损害模型整体性能的情况下移除不必要的记忆内容。

Method: 基于损失景观曲率对模型权重进行分解，识别高曲率（对应记忆化）和低曲率（对应泛化）成分，并通过抑制低曲率权重分量实现记忆编辑。

Result: 提出的权重编辑方法比现有遗忘方法（BalancedSubnet）更有效地抑制非目标记忆复述且保持更低困惑度；编辑后事实检索和算术任务性能显著下降，但开放性事实检索和逻辑推理能力得以保留；任务表现下降与被编辑的低曲率分量激活强度相关。

Conclusion: 记忆化可在Transformer权重中被解耦，低曲率方向对应特定任务（如事实检索、算术）的关键结构，表明这些任务依赖专用而非通用的权重机制，为理解和控制模型记忆提供了新视角。

Abstract: We characterize how memorization is represented in transformer models and
show that it can be disentangled in the weights of both language models (LMs)
and vision transformers (ViTs) using a decomposition based on the loss
landscape curvature. This insight is based on prior theoretical and empirical
work showing that the curvature for memorized training points is much sharper
than non memorized, meaning ordering weight components from high to low
curvature can reveal a distinction without explicit labels. This motivates a
weight editing procedure that suppresses far more recitation of untargeted
memorized data more effectively than a recent unlearning method
(BalancedSubnet), while maintaining lower perplexity. Since the basis of
curvature has a natural interpretation for shared structure in model weights,
we analyze the editing procedure extensively on its effect on downstream tasks
in LMs, and find that fact retrieval and arithmetic are specifically and
consistently negatively affected, even though open book fact retrieval and
general logical reasoning is conserved. We posit these tasks rely heavily on
specialized directions in weight space rather than general purpose mechanisms,
regardless of whether those individual datapoints are memorized. We support
this by showing a correspondence between task data's activation strength with
low curvature components that we edit out, and the drop in task performance
after the edit. Our work enhances the understanding of memorization in neural
networks with practical applications towards removing it, and provides evidence
for idiosyncratic, narrowly-used structures involved in solving tasks like math
and fact retrieval.

</details>


### [114] [Can LLMs Translate Human Instructions into a Reinforcement Learning Agent's Internal Emergent Symbolic Representation?](https://arxiv.org/abs/2510.24259)
*Ziqi Ma,Sao Mai Nguyen,Philippe Xu*

Main category: cs.CL

TL;DR: 研究大型语言模型（LLM）是否能将自然语言指令转化为分层强化学习中出现的内部符号表示，发现在不同任务和分区粒度下表现有限，揭示了语言与智能体内部表示对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在将人类自然语言指令转化为智能体内部符号表示方面的能力，以支持发展型学习智能体的规划与跨任务泛化。

Method: 使用结构化评估框架，测试GPT、Claude、Deepseek和Grok等主流大模型在Ant Maze和Ant Fall环境中，对不同层级的符号分区进行自然语言到符号表示的翻译能力。

Result: 发现大模型具备一定翻译能力，但性能高度依赖于符号分区的粒度和任务复杂度，表明其在表示对齐方面存在局限性。

Conclusion: 当前大模型在自然语言与智能体内部表示之间的对齐能力有限，需进一步研究以实现更鲁棒的语言-表示对接。

Abstract: Emergent symbolic representations are critical for enabling developmental
learning agents to plan and generalize across tasks. In this work, we
investigate whether large language models (LLMs) can translate human natural
language instructions into the internal symbolic representations that emerge
during hierarchical reinforcement learning. We apply a structured evaluation
framework to measure the translation performance of commonly seen LLMs -- GPT,
Claude, Deepseek and Grok -- across different internal symbolic partitions
generated by a hierarchical reinforcement learning algorithm in the Ant Maze
and Ant Fall environments. Our findings reveal that although LLMs demonstrate
some ability to translate natural language into a symbolic representation of
the environment dynamics, their performance is highly sensitive to partition
granularity and task complexity. The results expose limitations in current LLMs
capacity for representation alignment, highlighting the need for further
research on robust alignment between language and internal agent
representations.

</details>


### [115] [MERGE: Minimal Expression-Replacement GEneralization Test for Natural Language Inference](https://arxiv.org/abs/2510.24295)
*Mădălina Zgreabăn,Tejaswini Deoskar,Lasha Abzianidze*

Main category: cs.CL

TL;DR: 提出MERGE方法，通过替换开放类词生成NLI问题的高质量变体，评估模型在保持推理结构不变情况下的泛化能力，发现现有模型性能下降4-20%。


<details>
  <summary>Details</summary>
Motivation: 语言模型在自然语言推理（NLI）任务中的泛化能力不足，人工构建新基准成本高，自动生成高质量变体困难。

Method: 提出MERGE方法，通过替换开放类词生成保持原推理结构的NLI问题变体，自动构造测试集评估模型泛化性能。

Result: 实验显示现有NLI模型在生成的变体上性能下降4-20%，表明其在微小改动下泛化能力有限；并分析了替换词的词性、概率和合理性对模型性能的影响。

Conclusion: 当前NLI模型对词汇替换敏感，缺乏鲁棒性和真正推理泛化能力，MERGE为评估和改进模型泛化性提供了有效工具。

Abstract: In recent years, many generalization benchmarks have shown language models'
lack of robustness in natural language inference (NLI). However, manually
creating new benchmarks is costly, while automatically generating high-quality
ones, even by modifying existing benchmarks, is extremely difficult. In this
paper, we propose a methodology for automatically generating high-quality
variants of original NLI problems by replacing open-class words, while
crucially preserving their underlying reasoning. We dub our generalization test
as MERGE (Minimal Expression-Replacements GEneralization), which evaluates the
correctness of models' predictions across reasoning-preserving variants of the
original problem. Our results show that NLI models' perform 4-20% worse on
variants, suggesting low generalizability even on such minimally altered
problems. We also analyse how word class of the replacements, word probability,
and plausibility influence NLI models' performance.

</details>


### [116] [Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration in Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2510.24302)
*Shangyu Xing,Siyuan Wang,Chenyuan Yang,Xinyu Dai,Xiang Ren*

Main category: cs.CL

TL;DR: 提出了一种新的回溯策略LATR，通过在高不确定性生成步骤进行分支、前瞻性模拟和剪枝来增强轨迹多样性，从而加速策略学习并提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中的采样轨迹多样性不足，导致奖励信号弱化，影响策略学习效果。

Method: LATR方法包括三个阶段：在高不确定性生成步骤进行分支，对每个新分支进行前瞻性模拟，以及在模拟过程中剪除长时间相似的分支。

Result: 与随机采样相比，LATR平均加速策略学习131%，并在GRPO和DAPO算法上将最终pass@1性能提高了4.2%。

Conclusion: LATR有效提升了大语言模型推理能力的训练效率和性能，解决了现有方法中轨迹多样性不足的问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), particularly with
algorithms like Group Relative Policy Optimization (GRPO), has proven highly
effective in enhancing the reasoning capabilities of large language models.
However, a critical bottleneck in current pipelines lies in the limited
diversity of sampled trajectories during group rollouts. Homogeneous
trajectories and their associated rewards would diminish the return signals for
policy updates, thereby hindering effective policy learning. This lack of
diversity stems primarily from token-level stochastic sampling, where local
variations are likely to collapse into near-identical reasoning paths. To
address this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a
novel rollout strategy designed to explicitly promotes trajectory-level
diversity by enforcing branching into different candidate tokens likely to
yield distinct continuations. Specifically, LATR iteratively operates in three
stages: (1) branching at high-uncertainty generation steps, (2) performing
lookahead simulation for each new branch, and (3) pruning branches that
exhibits prolonged similarity during simulation. Compared with stochastic
Sampling, LATR accelerates policy learning by 131% on average and improves
final pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy
Optimization (DAPO) algorithms across different reasoning tasks. Our code and
data are publicly available at https://github.com/starreeze/latr.

</details>


### [117] [Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning](https://arxiv.org/abs/2510.24320)
*Zhiheng Xi,Jixuan Huang,Xin Guo,Boyang Hong,Dingwen Yang,Xiaoran Fan,Shuo Li,Zehui Chen,Junjie Ye,Siyu Yuan,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Rui Zheng,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出Critique-RL，一种无需强监督的在线强化学习方法，通过两阶段优化提升批评模型的判别力与帮助性，显著提高语言模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有批评语言模型依赖更强的监督信号来标注批评数据，限制了其可扩展性和实用性，因此需要一种无需强监督的方法来自我改进批评能力。

Method: 采用两玩家范式（演员生成回答，批评者提供反馈），并设计两阶段强化学习优化策略：第一阶段使用基于规则的直接奖励增强批评者的判别力；第二阶段引入间接奖励提升帮助性，同时通过正则化保持判别力。

Result: 在多个任务和模型上实验表明，Critique-RL显著提升性能，例如在Qwen2.5-7B上，领域内任务提升9.02%，跨领域任务提升5.70%。

Conclusion: Critique-RL能有效训练具备高判别力和帮助性的批评模型，无需依赖强监督，在复杂推理任务中具有广泛应用潜力。

Abstract: Training critiquing language models to assess and provide feedback on model
outputs is a promising way to improve LLMs for complex reasoning tasks.
However, existing approaches typically rely on stronger supervisors for
annotating critique data. To address this, we propose Critique-RL, an online RL
approach for developing critiquing language models without stronger
supervision. Our approach operates on a two-player paradigm: the actor
generates a response, the critic provides feedback, and the actor refines the
response accordingly. We first reveal that relying solely on indirect reward
signals from the actor's outputs for RL optimization often leads to
unsatisfactory critics: while their helpfulness (i.e., providing constructive
feedback) improves, the discriminability (i.e., determining whether a response
is high-quality or not) remains poor, resulting in marginal performance gains.
To overcome this, Critique-RL adopts a two-stage optimization strategy. In
stage I, it reinforces the discriminability of the critic with direct
rule-based reward signals; in stage II, it introduces indirect rewards based on
actor refinement to improve the critic's helpfulness, while maintaining its
discriminability via appropriate regularization. Extensive experiments across
various tasks and models show that Critique-RL delivers substantial performance
improvements. For example, it achieves a 9.02% gain on in-domain tasks and a
5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.

</details>


### [118] [Beyond MCQ: An Open-Ended Arabic Cultural QA Benchmark with Dialect Variants](https://arxiv.org/abs/2510.24328)
*Hunzalah Hassan Bhatti,Firoj Alam*

Main category: cs.CL

TL;DR: 本文提出了一种综合方法，用于评估大型语言模型在多语言和方言环境下的表现，特别是在阿拉伯语及其方言中的应用。研究扩展了一个现有的数据集，使其包含多种语言变体的并行对齐问答，并通过零样本和微调模型进行基准测试。结果表明，现有模型在阿拉伯方言上的表现不佳，且在开放式问题上存在挑战，而链式思维（CoT）推理有助于提升判断正确性。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在处理文化相关和方言内容时表现不均，尤其是在阿拉伯语等语言中，缺乏对不同语言变体的公平评估，因此需要一种系统的方法来衡量和改进模型在多语言、多方言环境下的性能。

Method: 研究提出的方法包括：将现代标准阿拉伯语的多项选择题翻译成英语和多种阿拉伯方言；将其转换为开放式问题；在多项选择题和开放式问题设置下对一系列零样本和微调后的大型语言模型进行基准测试；生成链式思维（CoT）推理过程以微调模型，促进逐步推理能力。同时扩展了一个跨语言变体并行对齐的问答数据集。

Result: 实验结果显示：模型在阿拉伯方言上的表现较差，暴露出文化相关和方言特定知识的不足；以阿拉伯语为中心的模型在多项选择题上表现良好，但在开放式问题上表现不佳；引入链式思维（CoT）能提高人工评判的正确率，但在基于n-gram的自动指标上效果不一。

Conclusion: 当前大型语言模型在处理阿拉伯语方言和开放式问题方面仍存在显著局限，需加强文化与语言包容性。链式思维推理可部分改善推理质量，未来应结合更合适的评估方式。所构建的数据集将公开，以推动多语言、多方言的公平评估研究。

Abstract: Large Language Models (LLMs) are increasingly used to answer everyday
questions, yet their performance on culturally grounded and dialectal content
remains uneven across languages. We propose a comprehensive method that (i)
translates Modern Standard Arabic (MSA) multiple-choice questions (MCQs) into
English and several Arabic dialects, (ii) converts them into open-ended
questions (OEQs), (iii) benchmarks a range of zero-shot and fine-tuned LLMs
under both MCQ and OEQ settings, and (iv) generates chain-of-thought (CoT)
rationales to fine-tune models for step-by-step reasoning. Using this method,
we extend an existing dataset in which QAs are parallelly aligned across
multiple language varieties, making it, to our knowledge, the first of its
kind. We conduct extensive experiments with both open and closed models. Our
findings show that (i) models underperform on Arabic dialects, revealing
persistent gaps in culturally grounded and dialect-specific knowledge; (ii)
Arabic-centric models perform well on MCQs but struggle with OEQs; and (iii)
CoT improves judged correctness while yielding mixed n-gram-based metrics. The
developed dataset will be publicly released to support further research on
culturally and linguistically inclusive evaluation.

</details>


### [119] [LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance and Verifiability](https://arxiv.org/abs/2510.24345)
*Zikai Xiao,Fei Huang,Jianhong Tu,Jianhui Wei,Wen Ma,Yuxuan Zhou,Jian Wu,Bowen Yu,Zuozhu Liu,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了LongWeave，一种结合现实场景与可验证评估的长文本生成评测基准，通过约束验证评估（CoV-Eval）方法生成兼具真实性与可衡量性的任务，用于严格评估大模型在复杂现实约束下的长文本生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有长文本生成评测基准要么依赖难以验证的真实世界查询，要么使用简化但脱离实际的合成设置，缺乏既能反映现实复杂性又可客观评估的评测方法。

Method: 提出Constraint-Verifier Evaluation (CoV-Eval)框架：首先定义现实场景中的可验证目标，再系统生成对应的问题、文本材料和约束条件，构建七种不同任务，支持最长64K输入/8K输出token的定制化设置。

Result: 在23个大语言模型上的评估表明，随着现实复杂性和输出长度增加，即使是当前最先进的模型在长文本生成任务上仍面临显著挑战。

Conclusion: LongWeave通过平衡真实性和可验证性，为长文本生成提供了更可靠和严谨的评测基准，揭示了现有LLM在复杂长文本生成中的局限性。

Abstract: Generating long, informative, and factual outputs remains a major challenge
for Large Language Models (LLMs). Existing benchmarks for long-form generation
typically assess real-world queries with hard-to-verify metrics or use
synthetic setups that ease evaluation but overlook real-world intricacies. In
this paper, we introduce \textbf{LongWeave}, which balances real-world and
verifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval
constructs tasks by first defining verifiable targets within real-world
scenarios, then systematically generating corresponding queries, textual
materials, and constraints based on these targets. This ensures that tasks are
both realistic and objectively assessable, enabling rigorous assessment of
model capabilities in meeting complex real-world constraints. LongWeave
supports customizable input/output lengths (up to 64K/8K tokens) across seven
distinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models
encounter significant challenges in long-form generation as real-world
complexity and output length increase.

</details>


### [120] [Text Simplification with Sentence Embeddings](https://arxiv.org/abs/2510.24365)
*Matthew Shardlow*

Main category: cs.CL

TL;DR: 本文探讨了在句子嵌入空间中学习文本简化转换的方法，使用小型前馈神经网络在高复杂度和低复杂度文本之间进行有效转换，并在未见过的数据集和多语言数据上验证了其适用性。


<details>
  <summary>Details</summary>
Motivation: 探索句子嵌入是否能保留文本复杂度信息，并利用嵌入空间中的变换实现轻量级文本简化模型。

Method: 使用小型前馈神经网络学习高复杂度与低复杂度句子嵌入之间的转换，并与Seq2Seq及基于大语言模型的方法进行对比。

Result: 所提方法在文本简化任务上表现出有希望的结果，且能在MedEASI数据集及非训练语言（如西班牙语、德语）上良好泛化。

Conclusion: 在句子嵌入空间中学习变换是一种有前景的研究方向，有望开发出小巧但强大的文本简化及其他自然语言生成任务模型。

Abstract: Sentence embeddings can be decoded to give approximations of the original
texts used to create them. We explore this effect in the context of text
simplification, demonstrating that reconstructed text embeddings preserve
complexity levels. We experiment with a small feed forward neural network to
effectively learn a transformation between sentence embeddings representing
high-complexity and low-complexity texts. We provide comparison to a Seq2Seq
and LLM-based approach, showing encouraging results in our much smaller
learning setting. Finally, we demonstrate the applicability of our
transformation to an unseen simplification dataset (MedEASI), as well as
datasets from languages outside the training data (ES,DE). We conclude that
learning transformations in sentence embedding space is a promising direction
for future research and has potential to unlock the ability to develop small,
but powerful models for text simplification and other natural language
generation tasks.

</details>


### [121] [Comprehensive and Efficient Distillation for Lightweight Sentiment Analysis Models](https://arxiv.org/abs/2510.24425)
*Guangyu Xie,Yice Zhang,Jianzhu Bao,Qianlong Wang,Yang Sun,Bingbing Wang,Ruifeng Xu*

Main category: cs.CL

TL;DR: 提出了一种高效且全面的蒸馏框架COMPEFFDIST，用于情感分析，通过自动构建指令和基于难度的数据过滤，在少量数据下使小模型达到大模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法依赖人工编写指令和大规模用户文本，存在指令多样性不足和计算成本高的问题。

Method: 设计了两个关键模块：基于属性的自动指令构造和基于难度的数据过滤，分别解决指令覆盖不全和计算开销大的问题。

Result: 在多个模型系列上验证，3B规模的学生模型在多数任务上匹配20倍大小教师模型的性能，且仅用10%数据即可达到基线方法相当的效果。

Conclusion: COMPEFFDIST显著提升了情感分析模型蒸馏的效率与实用性，实现了高性能与低资源消耗的平衡。

Abstract: Recent efforts leverage knowledge distillation techniques to develop
lightweight and practical sentiment analysis models. These methods are grounded
in human-written instructions and large-scale user texts. Despite the promising
results, two key challenges remain: (1) manually written instructions are
limited in diversity and quantity, making them insufficient to ensure
comprehensive coverage of distilled knowledge; (2) large-scale user texts incur
high computational cost, hindering the practicality of these methods. To this
end, we introduce COMPEFFDIST, a comprehensive and efficient distillation
framework for sentiment analysis. Our framework consists of two key modules:
attribute-based automatic instruction construction and difficulty-based data
filtering, which correspondingly tackle the aforementioned challenges. Applying
our method across multiple model series (Llama-3, Qwen-3, and Gemma-3), we
enable 3B student models to match the performance of 20x larger teacher models
on most tasks. In addition, our approach greatly outperforms baseline methods
in data efficiency, attaining the same performance level with only 10% of the
data.

</details>


### [122] [SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models](https://arxiv.org/abs/2510.24427)
*Ken Gu,Advait Bhat,Mike A Merrill,Robert West,Xin Liu,Daniel McDuff,Tim Althoff*

Main category: cs.CL

TL;DR: SynthWorlds是一个新框架，通过构建具有相同结构但一个基于真实世界、一个基于合成世界的平行语料库，来分离语言模型的推理能力与参数化知识的影响，从而更准确地评估模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试难以区分语言模型的表现是源于其推理能力还是对事实知识的记忆，因此需要一种方法能清晰分离这两者。

Method: 提出SynthWorlds框架，构建两个结构相同但内容分别为真实映射和合成映射的平行世界，并在其上设计多跳问答和页面导航两种镜像任务，保持跨世界的推理难度一致。

Result: 实验显示，在参数化仅用和知识增强设置下，模型在真实世界任务中始终存在性能优势（知识优势差距），表明记忆知识带来持续影响，现有机制可缩小但无法消除该差距。

Conclusion: SynthWorlds为评估语言模型的推理能力提供了一个可控、可扩展的环境，有助于精确比较模型的推理与记忆作用，揭示提升系统性能的机会。

Abstract: Evaluating the reasoning ability of language models (LMs) is complicated by
their extensive parametric world knowledge, where benchmark performance often
reflects factual recall rather than genuine reasoning. Existing datasets and
approaches (e.g., temporal filtering, paraphrasing, adversarial substitution)
cannot cleanly separate the two. We present SynthWorlds, a framework that
disentangles task reasoning complexity from factual knowledge. In SynthWorlds,
we construct parallel corpora representing two worlds with identical
interconnected structure: a real-mapped world, where models may exploit
parametric knowledge, and a synthetic-mapped world, where such knowledge is
meaningless. On top of these corpora, we design two mirrored tasks as case
studies: multi-hop question answering and page navigation, which maintain equal
reasoning difficulty across worlds. Experiments in parametric-only (e.g.,
closed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings
reveal a persistent knowledge advantage gap, defined as the performance boost
models gain from memorized parametric world knowledge. Knowledge acquisition
and integration mechanisms reduce but do not eliminate this gap, highlighting
opportunities for system improvements. Fully automatic and scalable,
SynthWorlds provides a controlled environment for evaluating LMs in ways that
were previously challenging, enabling precise and testable comparisons of
reasoning and memorization.

</details>


### [123] [LuxIT: A Luxembourgish Instruction Tuning Dataset from Monolingual Seed Data](https://arxiv.org/abs/2510.24434)
*Julian Valline,Cedric Lothritz,Jordi Cabot*

Main category: cs.CL

TL;DR: 本文提出了LuxIT，一个用于卢森堡语的单语指令微调数据集，以解决低资源语言环境下高质量训练数据缺乏的问题。通过使用DeepSeek-R1-0528从本地文本语料库生成数据，并采用LLM-as-a-judge方法进行质量保证。实验中对多个小型语言模型进行了微调，但在卢森堡语能力测试上的表现结果不一，显示出进一步优化研究的必要性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的训练数据，指令调优的大语言模型在低资源语言环境中的有效性常常受限。因此，本文旨在为卢森堡语构建一个高质量的单语指令调优数据集，以提升该语言在自然语言处理任务中的表现。

Method: 利用DeepSeek-R1-0528模型从卢森堡语原生文本语料库中合成指令数据，并通过LLM-as-a-judge的方法进行数据质量评估与筛选，最终构建出LuxIT数据集。随后，使用该数据集对多个小型语言模型进行指令微调，并在卢森堡语语言能力测试上进行基准评估。

Result: 在不同模型上的微调结果表现出较大差异，部分模型性能有所提升，但整体效果不稳定，说明当前方法的有效性依赖于具体模型架构，尚未达到一致的优化效果。

Conclusion: LuxIT为卢森堡语NLP研究提供了重要资源，并提出了一种可复用的单语指令数据构建方法，但其实际效用仍需进一步研究以优化模型适配和训练策略。

Abstract: The effectiveness of instruction-tuned Large Language Models (LLMs) is often
limited in low-resource linguistic settings due to a lack of high-quality
training data. We introduce LuxIT, a novel, monolingual instruction tuning
dataset for Luxembourgish developed to mitigate this challenge. We synthesize
the dataset from a corpus of native Luxembourgish texts, utilizing
DeepSeek-R1-0528, chosen for its shown proficiency in Luxembourgish. Following
generation, we apply a quality assurance process, employing an LLM-as-a-judge
approach. To investigate the practical utility of the dataset, we fine-tune
several smaller-scale LLMs on LuxIT. Subsequent benchmarking against their base
models on Luxembourgish language proficiency examinations, however, yields
mixed results, with performance varying significantly across different models.
LuxIT represents a critical contribution to Luxembourgish natural language
processing and offers a replicable monolingual methodology, though our findings
highlight the need for further research to optimize its application.

</details>


### [124] [Fine-tuning Large Language Models with Limited Data: A Survey and Practical Guide](https://arxiv.org/abs/2411.09539)
*Marton Szep,Daniel Rueckert,Rüdiger von Eisenhart-Rothe,Florian Hinterwimmer*

Main category: cs.CL

TL;DR: 本文系统综述了在数据稀缺场景下高效微调大语言模型的最新方法，涵盖参数高效微调、跨领域与跨语言适应、模型专业化及偏好对齐技术，为资源受限情况下的模型微调提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言、特定领域和资源受限部署场景中，有限数据下的大模型微调面临实际挑战，需更高效和聚焦的微调方法。

Method: 系统回顾了参数高效微调、领域与跨语言适应、模型专业化以及基于少量人类或合成反馈的偏好对齐方法，并分析了不同技术在模型规模、数据规模和灾难性遗忘缓解方面的权衡。

Result: 总结了各类微调技术的经验权衡、选择标准和最佳实践，提供了在任务约束下选择合适微调方法的实用指南。

Conclusion: 该综述为研究人员和实践者在数据和资源受限条件下有效微调大语言模型提供了结构化且可操作的见解。

Abstract: Fine-tuning large language models (LLMs) with limited data poses a practical
challenge in low-resource languages, specialized domains, and constrained
deployment settings. While pre-trained LLMs provide strong foundations,
effective adaptation under data scarcity requires focused and efficient
fine-tuning techniques. This paper presents a structured and practical survey
of recent methods for fine-tuning LLMs in data-scarce scenarios. We
systematically review parameter-efficient fine-tuning techniques that lower
training and deployment costs, domain and cross-lingual adaptation methods for
both encoder and decoder models, and model specialization strategies. We
further examine preference alignment approaches that guide model behavior using
limited human or synthetic feedback, emphasizing sample and compute efficiency.
Throughout, we highlight empirical trade-offs, selection criteria, and best
practices for choosing suitable techniques based on task constraints, including
model scaling, data scaling, and the mitigation of catastrophic forgetting. The
aim is to equip researchers and practitioners with actionable insights for
effectively fine-tuning LLMs when data and resources are limited.

</details>


### [125] [SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space](https://arxiv.org/abs/2510.24446)
*Viktoriia Zinkovich,Anton Antonov,Andrei Spiridonov,Denis Shepelev,Andrey Moskalenko,Daria Pugacheva,Elena Tutubalina,Andrey Kuznetsov,Vlad Shakhuro*

Main category: cs.CL

TL;DR: 本文提出了一种新的对抗性改写任务，旨在生成保持语义但降低视觉-语言模型分割性能的文本改写，并提出了SPARTA方法，在低维语义空间中通过强化学习优化，显著提升了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注图像输入的扰动，而忽视了在实际应用中用户以不同方式表达相同意图的文本同义改写对模型鲁棒性的影响。

Method: 提出SPARTA方法，基于文本自编码器的低维语义潜在空间进行黑盒、句子级优化，并采用强化学习引导生成对抗性改写。

Result: SPARTA在ReasonSeg和LLMSeg-40k数据集上比先前方法成功率提高达2倍，并揭示当前推理分割模型在语义和语法约束下仍易受对抗改写影响。

Conclusion: 先进的推理分割模型对语义保持的对抗性文本改写依然脆弱，需进一步提升其语言鲁棒性。

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
in vision-language tasks such as reasoning segmentation, where models generate
segmentation masks based on textual queries. While prior work has primarily
focused on perturbing image inputs, semantically equivalent textual
paraphrases-crucial in real-world applications where users express the same
intent in varied ways-remain underexplored. To address this gap, we introduce a
novel adversarial paraphrasing task: generating grammatically correct
paraphrases that preserve the original query meaning while degrading
segmentation performance. To evaluate the quality of adversarial paraphrases,
we develop a comprehensive automatic evaluation protocol validated with human
studies. Furthermore, we introduce SPARTA-a black-box, sentence-level
optimization method that operates in the low-dimensional semantic latent space
of a text autoencoder, guided by reinforcement learning. SPARTA achieves
significantly higher success rates, outperforming prior methods by up to 2x on
both the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive
baselines to assess the robustness of advanced reasoning segmentation models.
We reveal that they remain vulnerable to adversarial paraphrasing-even under
strict semantic and grammatical constraints. All code and data will be released
publicly upon acceptance.

</details>


### [126] [Charting the European LLM Benchmarking Landscape: A New Taxonomy and a Set of Best Practices](https://arxiv.org/abs/2510.24450)
*Špela Vintar,Taja Kuzman Pungeršek,Mojca Brglez,Nikola Ljubešić*

Main category: cs.CL

TL;DR: 本文综述了大语言模型（LLM）在多语言和非英语场景下的基准测试现状，并提出了一种针对多语言使用场景的基准分类新体系，同时倡导建立更高语言和文化敏感度的评估方法及欧洲语言基准开发的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力不断提升，现有基准测试多集中于英语，缺乏对非英语语言尤其是欧洲语言的有效评估，因此需要更适应多语言场景的基准体系和评估标准。

Method: 通过综述现有LLM基准测试的发展，提出一种新的、针对多语言或非英语使用场景的基准分类法，并建议一套适用于欧洲语言的基准开发最佳实践和质量标准。

Result: 提出了一种新的多语言基准分类体系，并总结出包括提升语言与文化敏感性在内的多项最佳实践建议，为未来多语言LLM评估提供了方向。

Conclusion: 为了更公平、准确地评估非英语语言中的大语言模型，必须发展更具语言和文化敏感性的基准测试方法，并推动欧洲语言基准的协调化建设。

Abstract: While new benchmarks for large language models (LLMs) are being developed
continuously to catch up with the growing capabilities of new models and AI in
general, using and evaluating LLMs in non-English languages remains a
little-charted landscape. We give a concise overview of recent developments in
LLM benchmarking, and then propose a new taxonomy for the categorization of
benchmarks that is tailored to multilingual or non-English use scenarios. We
further propose a set of best practices and quality standards that could lead
to a more coordinated development of benchmarks for European languages. Among
other recommendations, we advocate for a higher language and culture
sensitivity of evaluation methods.

</details>


### [127] [Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems](https://arxiv.org/abs/2510.24476)
*Yihan Li,Xiyuan Fu,Ghanshyam Verma,Paul Buitelaar,Mingming Liu*

Main category: cs.CL

TL;DR: 本文综述了检索增强生成（RAG）和推理增强在缓解大语言模型幻觉方面的协同作用，提出了区分知识型和逻辑型幻觉的分类法，并提供了一个统一框架。


<details>
  <summary>Details</summary>
Motivation: 幻觉是大语言模型在实际应用中可靠部署的主要障碍之一，现有方法缺乏对RAG与推理增强协同机制的系统研究。

Method: 采用面向应用的能力增强视角，提出幻觉分类法，系统分析RAG、推理增强及其在代理系统中的集成如何缓解不同类型幻觉。

Result: 明确了RAG主要缓解知识型幻觉，推理增强主要应对逻辑型幻觉，二者结合在实际应用中表现出更优的平衡效果。

Conclusion: RAG与推理增强的融合为平衡生成内容的创造力与可靠性提供了有效路径，未来应进一步探索其协同机制与评估基准。

Abstract: Hallucination remains one of the key obstacles to the reliable deployment of
large language models (LLMs), particularly in real-world applications. Among
various mitigation strategies, Retrieval-Augmented Generation (RAG) and
reasoning enhancement have emerged as two of the most effective and widely
adopted approaches, marking a shift from merely suppressing hallucinations to
balancing creativity and reliability. However, their synergistic potential and
underlying mechanisms for hallucination mitigation have not yet been
systematically examined. This survey adopts an application-oriented perspective
of capability enhancement to analyze how RAG, reasoning enhancement, and their
integration in Agentic Systems mitigate hallucinations. We propose a taxonomy
distinguishing knowledge-based and logic-based hallucinations, systematically
examine how RAG and reasoning address each, and present a unified framework
supported by real-world applications, evaluations, and benchmarks.

</details>


### [128] [Talk2Ref: A Dataset for Reference Prediction from Scientific Talks](https://arxiv.org/abs/2510.24478)
*Frederik Broy,Maike Züfle,Jan Niehues*

Main category: cs.CL

TL;DR: 本文提出了从科学报告中预测参考文献的新任务（RPT），并构建了首个大规模数据集Talk2Ref，包含6,279场报告和43,429篇引用论文。通过评估现有文本嵌入模型并提出一种双编码器架构，实验表明在该数据集上微调能显著提升预测性能，验证了任务的挑战性和数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 科学报告是传播研究成果的重要方式，但目前缺乏自动化方法来识别与报告内容相关的参考文献。因此，需要一个新任务和数据集来支持从口头科学内容中推荐引用文献的研究。

Method: 提出了Reference Prediction from Talks (RPT) 任务，构建了Talk2Ref数据集，使用零样本检索评估先进文本嵌入模型，并设计了一个双编码器架构进行训练；同时探索处理长文本和领域自适应的方法。

Result: 实验证明，在Talk2Ref上微调显著提升了参考文献预测性能，不同模型在该任务上表现差异明显，显示了任务的挑战性；提出的模型优于零样本基线。

Conclusion: RPT是一个具有挑战性的新任务，Talk2Ref为从科学报告中学习语义表示提供了有效资源，所提方法显著优于基线模型，且数据集与模型已开源以促进未来研究。

Abstract: Scientific talks are a growing medium for disseminating research, and
automatically identifying relevant literature that grounds or enriches a talk
would be highly valuable for researchers and students alike. We introduce
Reference Prediction from Talks (RPT), a new task that maps long, and
unstructured scientific presentations to relevant papers. To support research
on RPT, we present Talk2Ref, the first large-scale dataset of its kind,
containing 6,279 talks and 43,429 cited papers (26 per talk on average), where
relevance is approximated by the papers cited in the talk's corresponding
source publication. We establish strong baselines by evaluating
state-of-the-art text embedding models in zero-shot retrieval scenarios, and
propose a dual-encoder architecture trained on Talk2Ref. We further explore
strategies for handling long transcripts, as well as training for domain
adaptation. Our results show that fine-tuning on Talk2Ref significantly
improves citation prediction performance, demonstrating both the challenges of
the task and the effectiveness of our dataset for learning semantic
representations from spoken scientific content. The dataset and trained models
are released under an open license to foster future research on integrating
spoken scientific communication into citation recommendation systems.

</details>


### [129] [A word association network methodology for evaluating implicit biases in LLMs compared to humans](https://arxiv.org/abs/2510.24488)
*Katherine Abramski,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: 提出一种基于词关联网络的隐式偏见评估方法，通过模拟语义启动来检测大语言模型中的社会偏见，并实现与人类偏见的直接比较。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）日益普及，但其内在的社会偏见问题突出，尤其是隐式偏见难以检测，需发展能评估LLMs隐性知识表示的评价方法。

Method: 提出一种基于提示的词关联网络方法，模拟LLMs中的语义启动效应，构建隐式偏见评估框架，可对性别、宗教、种族、性取向和政党等偏见进行定量与定性分析，并支持LLM与人类的直接比较。

Result: 在多个主流LLM和人类群体上验证了该方法的有效性，发现LLMs与人类在某些偏见上存在趋同，但也存在显著差异，揭示了LLMs潜在的社会风险。

Conclusion: 该方法为评估和比较不同LLMs及人类之间的隐式社会偏见提供了一个系统化、可扩展且通用的框架，有助于推动透明且负责任的语言技术发展。

Abstract: As Large language models (LLMs) become increasingly integrated into our
lives, their inherent social biases remain a pressing concern. Detecting and
evaluating these biases can be challenging because they are often implicit
rather than explicit in nature, so developing evaluation methods that assess
the implicit knowledge representations of LLMs is essential. We present a novel
word association network methodology for evaluating implicit biases in LLMs
based on simulating semantic priming within LLM-generated word association
networks. Our prompt-based approach taps into the implicit relational
structures encoded in LLMs, providing both quantitative and qualitative
assessments of bias. Unlike most prompt-based evaluation methods, our method
enables direct comparisons between various LLMs and humans, providing a
valuable point of reference and offering new insights into the alignment of
LLMs with human cognition. To demonstrate the utility of our methodology, we
apply it to both humans and several widely used LLMs to investigate social
biases related to gender, religion, ethnicity, sexual orientation, and
political party. Our results reveal both convergences and divergences between
LLM and human biases, providing new perspectives on the potential risks of
using LLMs. Our methodology contributes to a systematic, scalable, and
generalizable framework for evaluating and comparing biases across multiple
LLMs and humans, advancing the goal of transparent and socially responsible
language technologies.

</details>


### [130] [CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?](https://arxiv.org/abs/2510.24505)
*Qing Zong,Jiayu Liu,Tianshi Zheng,Chunyang Li,Baixuan Xu,Haochen Shi,Weiqi Wang,Zhaowei Wang,Chunkit Chan,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文提出了一种基于自然语言批评的大型语言模型置信度校准新方法CritiCal，通过自我批评和批评校准训练提升模型在复杂推理任务中的置信度表达准确性，并展现出优异的分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以准确捕捉模型置信度所需的推理过程，且精确的置信度标签获取困难，因此需要一种更有效的置信度校准方法。

Method: 提出两种机制：Self-Critique（自我批评）和CritiCal（批评校准训练）。通过自然语言批评来优化模型对自身答案的置信度表达，区分对不确定性（问题导向）和置信度（答案导向）的批评，并在多轮生成中进行迭代优化。

Result: 实验表明，CritiCal显著优于Self-Critique和其他基线方法，在复杂推理任务中甚至超过了教师模型GPT-4o，并在分布外场景中表现出强健的泛化能力。

Conclusion: 自然语言批评是一种有效的置信度校准途径，CritiCal为提升大模型的可靠性提供了新思路，尤其适用于高风险应用场景。

Abstract: Accurate confidence calibration in Large Language Models (LLMs) is critical
for safe use in high-stakes domains, where clear verbalized confidence enhances
user trust. Traditional methods that mimic reference confidence expressions
often fail to capture the reasoning needed for accurate confidence assessment.
We propose natural language critiques as a solution, ideally suited for
confidence calibration, as precise gold confidence labels are hard to obtain
and often require multiple generations. This paper studies how natural language
critiques can enhance verbalized confidence, addressing: (1) What to critique:
uncertainty (question-focused) or confidence (answer-specific)? Analysis shows
confidence suits multiple-choice tasks, while uncertainty excels in open-ended
scenarios. (2) How to critique: self-critique or critique calibration training?
We propose Self-Critique, enabling LLMs to critique and optimize their
confidence beyond mere accuracy, and CritiCal, a novel Critique Calibration
training method that leverages natural language critiques to improve confidence
calibration, moving beyond direct numerical optimization. Experiments show that
CritiCal significantly outperforms Self-Critique and other competitive
baselines, even surpassing its teacher model, GPT-4o, in complex reasoning
tasks. CritiCal also shows robust generalization in out-of-distribution
settings, advancing LLM's reliability.

</details>


### [131] [Levée d'ambiguïtés par grammaires locales](https://arxiv.org/abs/2510.24530)
*Eric G. C. Laporte*

Main category: cs.CL

TL;DR: 本文介绍了一种适应于零静音率目标的词性消歧方法，并在Silberztein的INTEX系统中实现，强调了局部消歧文法需仔细测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 为了在自然语言处理中实现零静音率，确保正确的词性标签不被丢弃，需要一种有效的词性消歧方法。

Method: 提出了一种形式化的词性消歧方法，强调不能单独考虑转换器路径，而需验证它们之间的相互作用，并通过INTEX系统的初始标注发现消歧规则。

Result: 展示了在零静音率目标下，局部消歧文法必须经过仔细测试，且需要详细说明文法在文本上的应用效果。

Conclusion: 为实现零静音率，词性消歧方法需综合考虑转换器间的交互，并对文法进行充分验证和测试。

Abstract: Many words are ambiguous in terms of their part of speech (POS). However,
when a word appears in a text, this ambiguity is generally much reduced.
Disambiguating POS involves using context to reduce the number of POS
associated with words, and is one of the main challenges of lexical tagging.
The problem of labeling words by POS frequently arises in natural language
processing, for example for spelling correction, grammar or style checking,
expression recognition, text-to-speech conversion, text corpus analysis, etc.
Lexical tagging systems are thus useful as an initial component of many natural
language processing systems. A number of recent lexical tagging systems produce
multiple solutions when the text is lexically ambiguous or the uniquely correct
solution cannot be found. These contributions aim to guarantee a zero silence
rate: the correct tag(s) for a word must never be discarded. This objective is
unrealistic for systems that tag each word uniquely. This article concerns a
lexical disambiguation method adapted to the objective of a zero silence rate
and implemented in Silberztein's INTEX system (1993). We present here a formal
description of this method. We show that to verify a local disambiguation
grammar in this framework, it is not sufficient to consider the transducer
paths separately: one needs to verify their interactions. Similarly, if a
combination of multiple transducers is used, the result cannot be predicted by
considering them in isolation. Furthermore, when examining the initial labeling
of a text as produced by INTEX, ideas for disambiguation rules come
spontaneously, but grammatical intuitions may turn out to be inaccurate, often
due to an unforeseen construction or ambiguity. If a zero silence rate is
targeted, local grammars must be carefully tested. This is where a detailed
specification of what a grammar will do once applied to texts would be
necessary.

</details>


### [132] [Dark & Stormy: Modeling Humor in the Worst Sentences Ever Written](https://arxiv.org/abs/2510.24538)
*Venkata S Govindarajan,Laura Biester*

Main category: cs.CL

TL;DR: 本文研究了Bulwer-Lytton小说竞赛中的“糟糕”幽默，构建了一个新语料库，发现标准幽默检测模型表现不佳，并分析出其中融合了双关、反讽、隐喻、元小说和明喻等文学手法。LLM生成的类似句子虽模仿了形式，但过度使用某些修辞并产生过多新颖搭配，偏离了人类写作特点。


<details>
  <summary>Details</summary>
Motivation: 为了更全面地理解英语中‘糟糕’幽默的特点，弥补现有计算幽默研究对这类幽默关注不足的问题。

Method: 整理并分析Bulwer-Lytton小说竞赛语料库，评估标准幽默检测模型的表现，并分析其中的文学修辞手法；使用大语言模型生成类似句子，对比其与人类写作在修辞使用和词汇搭配上的差异。

Result: 标准幽默检测模型在该语料上表现差；人类创作结合多种常见与复杂文学手法；LLM生成文本虽具形式相似性，但过度使用修辞且产生更多新颖形容词-名词搭配。

Conclusion: ‘糟糕’幽默具有独特语言特征，需专门建模；当前LLM在模仿特定风格时存在夸张倾向，提示未来需更好控制生成风格的真实性。

Abstract: Textual humor is enormously diverse and computational studies need to account
for this range, including intentionally bad humor. In this paper, we curate and
analyze a novel corpus of sentences from the Bulwer-Lytton Fiction Contest to
better understand "bad" humor in English. Standard humor detection models
perform poorly on our corpus, and an analysis of literary devices finds that
these sentences combine features common in existing humor datasets (e.g., puns,
irony) with metaphor, metafiction and simile. LLMs prompted to synthesize
contest-style sentences imitate the form but exaggerate the effect by
over-using certain literary devices, and including far more novel
adjective-noun bigrams than human writers. Data, code and analysis are
available at https://github.com/venkatasg/bulwer-lytton

</details>


### [133] [Open Korean Historical Corpus: A Millennia-Scale Diachronic Collection of Public Domain Texts](https://arxiv.org/abs/2510.24541)
*Seyoung Song,Nawon Kim,Songeun Chae,Kiwoong Park,Jiho Jin,Haneul Yoo,Kyunghyun Cho,Alice Oh*

Main category: cs.CL

TL;DR: 本文介绍了首个大规模开放的韩语历史语料库，涵盖1300年、6种语言及多种书写系统，用于量化分析韩语从汉字到韩文的演变过程，并揭示了南北韩词汇差异对现代分词器的影响。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏可获取的历史语料，韩语的历史语言演变在自然语言处理领域长期未被充分研究。

Method: 构建并发布开放韩语历史语料库（Open Korean Historical Corpus），包含1800万文档、50亿词符，覆盖7世纪至2025年共19个来源，并进行定量语言变迁分析。

Result: 发现Idu使用在1860年代达到顶峰后急剧下降；汉字向韩文的转变始于约1890年并迅速完成；朝鲜的语言分化导致现代分词器的未知词率高达51倍。

Conclusion: 该语料库为韩语历时性研究提供了基础资源，可用于大模型预训练，提升对现代韩文中的汉源词汇及古文字系统的理解。

Abstract: The history of the Korean language is characterized by a discrepancy between
its spoken and written forms and a pivotal shift from Chinese characters to the
Hangul alphabet. However, this linguistic evolution has remained largely
unexplored in NLP due to a lack of accessible historical corpora. To address
this gap, we introduce the Open Korean Historical Corpus, a large-scale, openly
licensed dataset spanning 1,300 years and 6 languages, as well as
under-represented writing systems like Korean-style Sinitic (Idu) and
Hanja-Hangul mixed script. This corpus contains 18 million documents and 5
billion tokens from 19 sources, ranging from the 7th century to 2025. We
leverage this resource to quantitatively analyze major linguistic shifts: (1)
Idu usage peaked in the 1860s before declining sharply; (2) the transition from
Hanja to Hangul was a rapid transformation starting around 1890; and (3) North
Korea's lexical divergence causes modern tokenizers to produce up to 51 times
higher out-of-vocabulary rates. This work provides a foundational resource for
quantitative diachronic analysis by capturing the history of the Korean
language. Moreover, it can serve as a pre-training corpus for large language
models, potentially improving their understanding of Sino-Korean vocabulary in
modern Hangul as well as archaic writing systems.

</details>


### [134] [BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation](https://arxiv.org/abs/2510.24570)
*Raphaël Bagat,Irina Illina,Emmanuel Vincent*

Main category: cs.CL

TL;DR: 提出BEARD框架，利用无标签数据通过BEST-RQ目标和知识蒸馏来适应Whisper的编码器，在航空管制语音识别任务中显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: ASR系统在领域外和低资源场景下因标注数据稀缺而表现不佳，需有效进行领域自适应。

Method: 提出BEARD框架，结合BEST-RQ目标和来自冻结教师编码器的知识蒸馏，利用无标签数据对Whisper编码器进行自监督学习和重训练。

Result: 在ATCO2语料库上，使用约5000小时无转录语音进行BEARD训练和2小时有标签数据微调，相比微调模型相对提升12%。

Conclusion: BEARD是首个将自监督学习目标用于Whisper领域自适应的工作，在低资源、高噪声、非母语语音场景中展现出显著优势。

Abstract: Automatic Speech Recognition (ASR) systems, despite large multilingual
training, struggle in out-of-domain and low-resource scenarios where labeled
data is scarce. We propose BEARD (BEST-RQ Encoder Adaptation with Re-training
and Distillation), a novel framework designed to adapt Whisper's encoder using
unlabeled data. Unlike traditional self-supervised learning methods, BEARD
uniquely combines a BEST-RQ objective with knowledge distillation from a frozen
teacher encoder, ensuring the encoder's complementarity with the pre-trained
decoder. Our experiments focus on the ATCO2 corpus from the challenging Air
Traffic Control (ATC) communications domain, characterized by non-native
speech, noise, and specialized phraseology. Using about 5,000 hours of
untranscribed speech for BEARD and 2 hours of transcribed speech for
fine-tuning, the proposed approach significantly outperforms previous baseline
and fine-tuned model, achieving a relative improvement of 12% compared to the
fine-tuned model. To the best of our knowledge, this is the first work to use a
self-supervised learning objective for domain adaptation of Whisper.

</details>


### [135] [ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?](https://arxiv.org/abs/2510.24591)
*Christine Ye,Sihan Yuan,Suchetha Cooray,Steven Dillmann,Ian L. V. Roque,Dalya Baron,Philipp Frank,Sergio Martin-Alvarez,Nolan Koblischke,Frank J Qu,Diyi Yang,Risa Wechsler,Ioana Ciuca*

Main category: cs.CL

TL;DR: 本文提出了ReplicationBench，一个用于评估AI代理在天体物理学中复制整篇研究论文能力的框架，通过与原作者合作开发任务来客观评估AI代理的忠实度和正确性，揭示了当前前沿语言模型在此类任务上的表现不佳及多种失败模式。


<details>
  <summary>Details</summary>
Motivation: 为了评估AI代理作为科研助手的可靠性和准确性，特别是在开放-ended研究工作流中的应用潜力，需要一个能够测试AI代理复制整个研究论文能力的评估框架。

Method: 引入ReplicationBench评估框架，将每篇论文分解为多个任务，涵盖实验设置、推导、数据分析和代码库等核心贡献，并与原作者合作开发这些任务，以确保评估的客观性和准确性。

Result: 即使是表现最好的语言模型，在ReplicationBench上的得分也低于20%，分析发现了一系列多样化的失败模式。

Conclusion: ReplicationBench建立了首个经过专家验证的大规模天体物理研究任务基准，不仅揭示了AI代理在科学研究中的性能特点，还提供了一个可扩展的框架来衡量AI代理在科学领域的可靠性。

Abstract: Frontier AI agents show increasing promise as scientific research assistants,
and may eventually be useful for extended, open-ended research workflows.
However, in order to use agents for novel research, we must first assess the
underlying faithfulness and correctness of their work. To evaluate agents as
research assistants, we introduce ReplicationBench, an evaluation framework
that tests whether agents can replicate entire research papers drawn from the
astrophysics literature. Astrophysics, where research relies heavily on
archival data and computational study while requiring little real-world
experimentation, is a particularly useful testbed for AI agents in scientific
research. We split each paper into tasks which require agents to replicate the
paper's core contributions, including the experimental setup, derivations, data
analysis, and codebase. Each task is co-developed with the original paper
authors and targets a key scientific result, enabling objective evaluation of
both faithfulness (adherence to original methods) and correctness (technical
accuracy of results). ReplicationBench is extremely challenging for current
frontier language models: even the best-performing language models score under
20%. We analyze ReplicationBench trajectories in collaboration with domain
experts and find a rich, diverse set of failure modes for agents in scientific
research. ReplicationBench establishes the first benchmark of paper-scale,
expert-validated astrophysics research tasks, reveals insights about agent
performance generalizable to other domains of data-driven science, and provides
a scalable framework for measuring AI agents' reliability in scientific
research.

</details>


### [136] [ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization](https://arxiv.org/abs/2510.24592)
*Guoxin Chen,Jing Wu,Xinjie Chen,Wayne Xin Zhao,Ruihua Song,Chengxi Li,Kai Fan,Dayiheng Liu,Minpeng Liao*

Main category: cs.CL

TL;DR: 本文提出了一种名为ReForm的反射式自动形式化方法（Reflective Autoformalization），通过引入语义一致性评估机制，实现形式化语句的迭代生成、自我评估与纠错。为有效训练该模型，作者提出了前瞻性有界序列优化（PBSO）方法，并在四个基准上实现了平均17.2个百分点的提升。此外，构建了ConsistencyCheck评测集以提高评估可靠性，揭示即使人类专家也会在高达38.5%的情况下出现语义错误，表明自动形式化任务本身具有高度挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在将自然语言数学问题转化为机器可验证的形式化语句时，虽能保证语法正确，但常无法保持原问题的语义意图。这是因为当前方法将自动形式化视为简单翻译任务，缺乏人类专家所具备的反思与迭代改进机制。因此，需要一种能够自我反思并逐步优化的方法来提升语义保真度。

Method: 提出ReForm方法，将语义一致性评估紧密集成到自动形式化过程中，使模型能迭代生成形式化语句、评估其语义保真度，并通过渐进式精炼进行自我修正。为训练该模型，设计了前瞻性有界序列优化（PBSO），在不同序列位置使用不同奖励信号，确保模型同时掌握准确的形式化能力和正确的语义验证能力，避免表面化的批评。

Result: 在四个自动形式化基准上的实验表明，ReForm比最强基线平均提升了17.2个百分点。新构建的ConsistencyCheck基准包含859个专家标注样本，用于验证LLM作为评判者的可靠性，并发现即使是人类专家也会在最多38.5%的情况下产生语义错误，说明该任务本身极具挑战性。

Conclusion: ReForm通过引入反思机制显著提升了自动形式化的语义准确性，PBSO训练策略有效支持了模型对形式化与语义验证的联合学习。研究结果强调了迭代反思在复杂语义转换任务中的重要性，并指出当前自动形式化仍面临根本性挑战，需进一步研究以提升语义一致性。

Abstract: Autoformalization, which translates natural language mathematics into
machine-verifiable formal statements, is critical for using formal mathematical
reasoning to solve math problems stated in natural language. While Large
Language Models can generate syntactically correct formal statements, they
often fail to preserve the original problem's semantic intent. This limitation
arises from the LLM approaches' treating autoformalization as a simplistic
translation task which lacks mechanisms for self-reflection and iterative
refinement that human experts naturally employ. To address these issues, we
propose ReForm, a Reflective Autoformalization method that tightly integrates
semantic consistency evaluation into the autoformalization process. This
enables the model to iteratively generate formal statements, assess its
semantic fidelity, and self-correct identified errors through progressive
refinement. To effectively train this reflective model, we introduce
Prospective Bounded Sequence Optimization (PBSO), which employs different
rewards at different sequence positions to ensure that the model develops both
accurate autoformalization and correct semantic validations, preventing
superficial critiques that would undermine the purpose of reflection. Extensive
experiments across four autoformalization benchmarks demonstrate that ReForm
achieves an average improvement of 17.2 percentage points over the strongest
baselines. To further ensure evaluation reliability, we introduce
ConsistencyCheck, a benchmark of 859 expert-annotated items that not only
validates LLMs as judges but also reveals that autoformalization is inherently
difficult: even human experts produce semantic errors in up to 38.5% of cases.

</details>


### [137] [Diffusion LLM with Native Variable Generation Lengths: Let [EOS] Lead the Way](https://arxiv.org/abs/2510.24605)
*Yicun Yang,Cong Wang,Shaobo Wang,Zichen Wen,Biqing Qi,Hanlin Xu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种具有可变生成长度的扩散型大语言模型dLLM-Var，通过准确预测[EOS]标记实现块状扩散生成，兼顾全局双向注意力与高并行性，在标准基准上显著提升了推理速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散型大语言模型（dLLMs）需预先设定固定的生成长度，导致效率和灵活性不足，限制了其实际应用。

Method: 训练一个能够准确预测[EOS]标记的扩散语言模型，使其能够原生支持可变长度生成，采用块扩散推理方式，同时保持全局双向注意力机制和高并行性。

Result: 在标准基准测试中，相比传统dLLM推理范式实现了30.1倍的速度提升，相较于Qwen和Llama等自回归模型也有2.4倍的速度优势，且精度更高。

Conclusion: dLLM-Var克服了固定长度生成的局限，显著提升了推理效率和灵活性，推动dLLMs从学术研究走向实际应用。

Abstract: Diffusion-based large language models (dLLMs) have exhibited substantial
potential for parallel text generation, which may enable more efficient
generation compared to autoregressive models. However, current dLLMs suffer
from fixed generation lengths, which indicates the generation lengths of dLLMs
have to be determined before decoding as a hyper-parameter, leading to issues
in efficiency and flexibility. To solve these problems, in this work, we
propose to train a diffusion LLM with native variable generation lengths,
abbreviated as dLLM-Var. Concretely, we aim to train a model to accurately
predict the [EOS] token in the generated text, which makes a dLLM be able to
natively infer in a block diffusion manner, while still maintaining the ability
of global bi-directional (full) attention and high parallelism. Experiments on
standard benchmarks demonstrate that our method achieves a 30.1x speedup over
traditional dLLM inference paradigms and a 2.4x speedup relative to
autoregressive models such as Qwen and Llama. Our method achieves higher
accuracy and faster inference, elevating dLLMs beyond mere academic novelty and
supporting their practical use in real-world applications. Codes and models
have been released.

</details>


### [138] [Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs](https://arxiv.org/abs/2510.24606)
*Siheng Xiong,Joe Zou,Faramarz Fekri,Yae Jee Cho*

Main category: cs.CL

TL;DR: 提出动态分层稀疏注意力（DHSA），一种数据驱动的在线预测注意力稀疏性方法，无需重训练即可自适应处理长上下文，显著降低预填充延迟和内存使用，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有静态稀疏注意力方法因固定模式无法适应内容变化，动态方法依赖预定义模板或启发式机制，导致泛化能力差且可能剪枝重要上下文，限制了在多样化任务中的准确性。

Method: DHSA将序列自适应分割为变长块，通过长度归一化的聚合计算块表示，并将块级相似度上采样至令牌级以生成重要性分数，动态决定保留哪些令牌交互，实现在线稀疏性预测。

Result: 在Gemma2上的实验显示，DHSA在Needle-in-a-Haystack和LongBench测试中达到与密集注意力相当的准确率，预填充延迟降低20-60%，峰值内存减少35%；相比块稀疏等基线方法，准确率相对提升6-18%，成本相当或更低。

Conclusion: DHSA提供了一种高效、灵活且无需重训练的长上下文建模方案，特别适用于资源受限的设备端大语言模型。

Abstract: The quadratic cost of attention hinders the scalability of long-context LLMs,
especially in resource-constrained settings. Existing static sparse methods
such as sliding windows or global tokens utilizes the sparsity of attention to
reduce the cost of attention, but poorly adapts to the content-dependent
variations in attention due to their staticity. While previous work has
proposed several dynamic approaches to improve flexibility, they still depend
on predefined templates or heuristic mechanisms. Such strategies reduce
generality and prune tokens that remain contextually important, limiting their
accuracy across diverse tasks. To tackle these bottlenecks of existing methods
for long-context modeling, we introduce Dynamic Hierarchical Sparse Attention
(DHSA), a data-driven framework that dynamically predicts attention sparsity
online without retraining. Our proposed DHSA adaptively segments sequences into
variable-length chunks, then computes chunk representations by aggregating the
token embeddings within each chunk. To avoid the bias introduced by varying
chunk lengths, we apply length-normalized aggregation that scales the averaged
embeddings by the square root of the chunk size. Finally, DHSA upsamples the
chunk-level similarity scores to token level similarities to calculate
importance scores that determine which token-level interactions should be
preserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and
LongBench show that DHSA matches dense attention in accuracy, while reducing
prefill latency by 20-60% and peak memory usage by 35%. Compared to other
representative baselines such as block sparse attention, DHSA achieves
consistently higher accuracy (6-18% relative gains) with comparable or lower
cost, offering an efficient and adaptable solution for long-context on-device
LLMs.

</details>


### [139] [Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation](https://arxiv.org/abs/2510.24619)
*Snegha A,Sayambhu Sen,Piyush Singh Pasi,Abhishek Singhania,Preethi Jyothi*

Main category: cs.CL

TL;DR: 本研究系统评估了三种基于前缀的方法在从英语到35种以上高、低资源语言的零样本跨语言迁移中的表现，发现前缀调优方法在Llama和Mistral等解码器-only大语言模型上优于LoRA基线，且仅用123万参数即可实现一致提升，尤其适用于低资源多语言场景。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA等参数高效微调技术被广泛使用，但针对解码器-only大语言模型的前缀类方法（如软提示调优、前缀调优）在零样本跨语言迁移中仍缺乏充分探索。

Method: 对三种前缀调优方法进行了全面研究，涵盖从英语到35+种语言的零样本跨语言迁移，并分析了语言家族、文字系统及模型规模（1B至24B）的影响。

Result: 在Belebele基准上，使用Llama 3.1 8B时前缀方法比LoRA基线最高提升6%；Mistral v0.3 7B也表现出类似改进，且前缀调优仅使用1.23M参数即在多个基准上取得稳定增益。

Conclusion: 前缀类方法是LoRA之外一种高效且可扩展的替代方案，特别适合低资源多语言环境下的零样本跨语言迁移任务。

Abstract: With the release of new large language models (LLMs) like Llama and Mistral,
zero-shot cross-lingual transfer has become increasingly feasible due to their
multilingual pretraining and strong generalization capabilities. However,
adapting these decoder-only LLMs to new tasks across languages remains
challenging. While parameter-efficient fine-tuning (PeFT) techniques like
Low-Rank Adaptation (LoRA) are widely used, prefix-based techniques such as
soft prompt tuning, prefix tuning, and Llama Adapter are less explored,
especially for zero-shot transfer in decoder-only models. We present a
comprehensive study of three prefix-based methods for zero-shot cross-lingual
transfer from English to 35+ high- and low-resource languages. Our analysis
further explores transfer across linguistic families and scripts, as well as
the impact of scaling model sizes from 1B to 24B. With Llama 3.1 8B, prefix
methods outperform LoRA-baselines by up to 6% on the Belebele benchmark.
Similar improvements were observed with Mistral v0.3 7B as well. Despite using
only 1.23M learning parameters with prefix tuning, we achieve consistent
improvements across diverse benchmarks. These findings highlight the potential
of prefix-based techniques as an effective and scalable alternative to LoRA,
particularly in low-resource multilingual settings.

</details>


### [140] [Relative Scaling Laws for LLMs](https://arxiv.org/abs/2510.24626)
*William Held,David Hall,Percy Liang,Diyi Yang*

Main category: cs.CL

TL;DR: 本文提出了相对扩展定律，用于追踪不同测试分布之间的性能差距如何随模型规模变化，并通过255个Transformer模型在相同计算预算下的实验，揭示了不同领域、语言变体和AI风险行为的多样化扩展轨迹，表明扩展并不总是能消除性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有的扩展定律通常基于整体测试集评估，掩盖了不同子群体间的性能差异，因此需要引入能够反映性能差距变化的相对扩展定律。

Method: 在匹配计算量（IsoFLOP）条件下训练255个仅解码器的Transformer模型，范围从10^18到10^20 FLOPs，在标准预训练数据集上分析不同测试分布间的性能差距演变。

Result: 发现学术领域性能趋于一致；区域英语方言表现因人群规模而异；AI风险行为出现分化，其中能力和影响力相关风险在预训练中增加，而对抗性风险未增加。

Conclusion: 尽管扩大规模能提升整体性能，但它并非普遍的均衡器，扩展过程中可能加剧某些不平等现象，因此需结合相对扩展定律来更好识别和应对鲁棒性挑战。

Abstract: Scaling laws describe how language models improve with additional data,
parameters, and compute. While widely used, they are typically measured on
aggregate test sets. Aggregate evaluations yield clean trends but average over
heterogeneous subpopulations, obscuring performance disparities. We introduce
relative scaling laws, which track how performance gaps between test
distributions evolve with scale rather than focusing solely on absolute error.
Using 255 decoder-only Transformers trained under matched-compute (IsoFLOP)
budgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we
find diverse trajectories: academic domains on MMLU converge toward parity;
regional English dialects shift depending on population size; and clusters of
AI risk behaviours split, with capability- and influence-related risks
increasing during pretraining while adversarial risks do not. These results
show that although scaling improves overall performance, it is not a universal
equalizer. To support further study, we release all model checkpoints from this
work to enable practitioners to measure relative alongside traditional scaling
laws, in order to better prioritize robustness challenges in light of the
bitter lesson.

</details>


### [141] ["Mm, Wat?" Detecting Other-initiated Repair Requests in Dialogue](https://arxiv.org/abs/2510.24628)
*Anh Ngo,Nicolas Rollet,Catherine Pelachaud,Chloe Clavel*

Main category: cs.CL

TL;DR: 提出一种多模态模型，通过结合语言和韵律特征来自动检测荷兰语对话中的修复启动，结果表明韵律线索能显著提升预训练文本和音频嵌入的效果。


<details>
  <summary>Details</summary>
Motivation: 当前对话代理难以识别用户的修复启动信号，导致对话中断或用户脱离，因此需要更有效的修复启动检测方法。

Method: 基于会话分析，融合语言特征和韵律特征构建多模态模型，用于检测荷兰语对话中的他人发起修复（OIR）。

Result: 韵律线索补充了语言特征，显著提升了预训练文本和音频嵌入的性能，揭示了不同特征间的交互作用。

Conclusion: 所提出的多模态模型有效提升了修复启动的检测效果，未来可引入视觉线索并扩展至多语言和跨场景语料以验证其鲁棒性和泛化能力。

Abstract: Maintaining mutual understanding is a key component in human-human
conversation to avoid conversation breakdowns, in which repair, particularly
Other-Initiated Repair (OIR, when one speaker signals trouble and prompts the
other to resolve), plays a vital role. However, Conversational Agents (CAs)
still fail to recognize user repair initiation, leading to breakdowns or
disengagement. This work proposes a multimodal model to automatically detect
repair initiation in Dutch dialogues by integrating linguistic and prosodic
features grounded in Conversation Analysis. The results show that prosodic cues
complement linguistic features and significantly improve the results of
pretrained text and audio embeddings, offering insights into how different
features interact. Future directions include incorporating visual cues,
exploring multilingual and cross-context corpora to assess the robustness and
generalizability.

</details>


### [142] [OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning](https://arxiv.org/abs/2510.24636)
*Ziyou Hu,Zhengliang Shi,Minghang Zhu,Haitao Li,Teng Sun,Pengjie Ren,Suzan Verberne,Zhaochun Ren*

Main category: cs.CL

TL;DR: 本文提出了OpenRM，一种工具增强的长文本奖励模型，通过调用外部工具获取证据来系统评估开放性回答，并在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型在知识密集型和长文本任务上表现不佳，难以准确判断需要外部证据支持的回答质量。

Method: 提出OpenRM模型，结合外部工具进行证据收集，并使用Group Relative Policy Optimization (GRPO) 在超过27K合成的成对样本上训练，联合监督工具使用和最终结果准确性。

Result: 在三个新构建的数据集和两个常用基准上实验表明，OpenRM显著优于现有的奖励模型方法；并在推理和训练阶段的应用中带来一致的性能提升。

Conclusion: 工具增强的奖励模型（如OpenRM）能够有效提升长文本、知识密集型任务中的评估可靠性，具有广泛的应用潜力。

Abstract: Reward models (RMs) have become essential for aligning large language models
(LLMs), serving as scalable proxies for human evaluation in both training and
inference. However, existing RMs struggle on knowledge-intensive and long-form
tasks, where evaluating correctness requires grounding beyond the model's
internal knowledge. This limitation hinders them from reliably discriminating
subtle quality differences, especially when external evidence is necessary. To
address this, we introduce OpenRM, a tool-augmented long-form reward model that
systematically judges open-ended responses by invoking external tools to gather
relevant evidence. We train OpenRM with Group Relative Policy Optimization
(GRPO) on over 27K synthesized pairwise examples generated through a
controllable data synthesis framework. The training objective jointly
supervises intermediate tool usage and final outcome accuracy, incentivizing
our reward model to learn effective evidence-based judgment strategies.
Extensive experiments on three newly-collected datasets and two widely-used
benchmarks demonstrate that OpenRM substantially outperforms existing reward
modeling approaches. As a further step, we integrate OpenRM into both
inference-time response selection and training-time data selection. This yields
consistent gains in downstream LLM alignment tasks, highlighting the potential
of tool-augmented reward models for scaling reliable long-form evaluation.

</details>


### [143] [Quantifying the Effects of Word Length, Frequency, and Predictability on Dyslexia](https://arxiv.org/abs/2510.24647)
*Hugo Rydel-Johnston,Alex Kafkas*

Main category: cs.CL

TL;DR: 该研究通过眼动追踪技术分析了阅读障碍者在自然阅读中的时间成本，发现词长、词频和可预测性均影响阅读时间，且阅读障碍者对这些特征更敏感，尤其是可预测性；通过反事实操纵这些特征可缩小约三分之一的阅读障碍与对照组差距。


<details>
  <summary>Details</summary>
Motivation: 探究阅读障碍者在真实阅读中产生额外阅读成本的具体条件和位置，以理解其认知机制并为干预提供依据。

Method: 使用眼动追踪数据结合词语层面特征（词长、词频、可预测性），建模分析这些特征如何影响典型读者与阅读障碍者的阅读时间，并进行反事实特征操纵以评估对组间差异的影响。

Result: 所有三个词语特征均显著影响两类读者的阅读时间，阅读障碍者对各特征更敏感，尤其对可预测性；反事实操纵使阅读障碍与对照组的差距缩小约三分之一，其中可预测性作用最强，其次为词长和词频。

Conclusion: 阅读障碍者的额外阅读成本主要源于对词汇特征更强的敏感性，尤其是语言可预测性，这支持了语言工作记忆和语音编码负担加重的理论，研究结果为阅读障碍干预措施和计算模型提供了量化依据和改进方向。

Abstract: We ask where, and under what conditions, dyslexic reading costs arise in a
large-scale naturalistic reading dataset. Using eye-tracking aligned to
word-level features (word length, frequency, and predictability), we model how
each feature influences dyslexic time costs. We find that all three features
robustly change reading times in both typical and dyslexic readers, and that
dyslexic readers show stronger sensitivities to each, especially
predictability. Counterfactual manipulations of these features substantially
narrow the dyslexic-control gap by about one third, with predictability showing
the strongest effect, followed by length and frequency. These patterns align
with dyslexia theories that posit heightened demands on linguistic working
memory and phonological encoding, and they motivate further work on lexical
complexity and parafoveal preview benefits to explain the remaining gap. In
short, we quantify when extra dyslexic costs arise, how large they are, and
offer actionable guidance for interventions and computational models for
dyslexics.

</details>


### [144] [Evolving Diagnostic Agents in a Virtual Clinical Environment](https://arxiv.org/abs/2510.24654)
*Pengcheng Qiu,Chaoyi Wu,Junwei Liu,Qiaoyu Zheng,Yusheng Liao,Haowen Wang,Yun Yue,Qianrui Fan,Shuai Zhen,Jian Wang,Jinjie Gu,Yanfeng Wang,Ya Zhang,Weidi Xie*

Main category: cs.CL

TL;DR: 本文提出了一种通过强化学习训练大语言模型作为诊断代理的框架DiagAgent，能够在交互式临床环境中学习动态诊断策略，在多项指标上显著优于现有LLM。


<details>
  <summary>Details</summary>
Motivation: 传统指令微调的模型依赖静态病例摘要，缺乏动态诊疗决策能力，难以模拟真实临床诊断过程。

Method: 构建了一个基于电子健康记录的诊断环境DiagGym，采用端到端多轮强化学习训练DiagAgent，并提出包含医师验证数据的诊断基准DiagBench进行评估。

Result: DiagAgent在单轮和端到端设置下分别提升诊断准确率9.34%和15.12%，检查推荐F1分数提高23.09%，在基于评分细则的评估中超过Claude-sonnet-4达7.1%。

Conclusion: 通过在交互式环境中学习诊断策略，可赋予LLM更动态、更具临床意义的诊疗管理能力，超越被动训练模型的性能上限。

Abstract: In this paper, we present a framework for training large language models
(LLMs) as diagnostic agents with reinforcement learning, enabling them to
manage multi-turn diagnostic processes, adaptively select examinations, and
commit to final diagnoses. Unlike instruction-tuned models trained on static
case summaries, our method acquires diagnostic strategies through interactive
exploration and outcome-based feedback. Our contributions are fourfold: (i) We
present DiagGym, a diagnostics world model trained with electronic health
records that emits examination outcomes conditioned on patient history and
recommended examination, serving as a virtual clinical environment for
realistic diagnosis training and evaluation; (ii) We train DiagAgent via
end-to-end, multi-turn reinforcement learning to learn diagnostic policies that
optimize both information yield and diagnostic accuracy; (iii) We introduce
DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated
examination recommendations and 99 cases annotated with 973 physician-written
rubrics on diagnosis process; (iv) we demonstrate superior performance across
diverse diagnostic settings. DiagAgent significantly outperforms 10
state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two
prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%
higher diagnostic accuracy and 44.03% improvement in examination recommendation
hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic
accuracy and 23.09% boost in examination recommendation F1 score. In
rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by
7.1% in weighted rubric score. These findings indicate that learning policies
in interactive clinical environments confers dynamic and clinically meaningful
diagnostic management abilities unattainable through passive training alone.

</details>


### [145] [MQM Re-Annotation: A Technique for Collaborative Evaluation of Machine Translation](https://arxiv.org/abs/2510.24664)
*Parker Riley,Daniel Deutsch,Mara Finkelstein,Colten DiIanni,Juraj Juraska,Markus Freitag*

Main category: cs.CL

TL;DR: 本文提出了一种名为MQM重注释的两阶段机器翻译评估方法，通过让标注者审查和编辑已有的MQM标注来提高标注质量，实验表明该方法能有效发现首次标注中遗漏的错误，从而提升评估质量。


<details>
  <summary>Details</summary>
Motivation: 随着机器翻译模型质量的提升，现有的人工评估方法面临挑战，评估噪声可能导致实际质量增益无法准确体现，因此需要改进评估方法以确保评估结果的可靠性。

Method: 采用两阶段的MQM重注释方法，由MQM标注者对已有标注（来自自身、其他人类标注者或自动标注系统）进行审查和修改，并分析标注者在重注释过程中的行为及其对标注质量的影响。

Result: 实验结果显示，重注释过程中的标注者行为符合预期目标，且重注释显著提高了标注质量，主要体现在发现了首次标注中遗漏的错误。

Conclusion: MQM重注释是一种有效的翻译评估改进方法，能够提升评估的一致性和准确性，适用于高质量机器翻译系统的评估需求。

Abstract: Human evaluation of machine translation is in an arms race with translation
model quality: as our models get better, our evaluation methods need to be
improved to ensure that quality gains are not lost in evaluation noise. To this
end, we experiment with a two-stage version of the current state-of-the-art
translation evaluation paradigm (MQM), which we call MQM re-annotation. In this
setup, an MQM annotator reviews and edits a set of pre-existing MQM
annotations, that may have come from themselves, another human annotator, or an
automatic MQM annotation system. We demonstrate that rater behavior in
re-annotation aligns with our goals, and that re-annotation results in
higher-quality annotations, mostly due to finding errors that were missed
during the first pass.

</details>


### [146] [InteractComp: Evaluating Search Agents With Ambiguous Queries](https://arxiv.org/abs/2510.24668)
*Mingyi Deng,Lijun Huang,Yani Fan,Jiayi Zhang,Fashen Ren,Jinyi Bai,Fuzhen Yang,Dayi Miao,Zhaoyang Yu,Yifan Wu,Yanfei Zhang,Fengwei Teng,Yingjia Wan,Song Hu,Yude Li,Xin Jin,Conghao Hu,Haoyu Li,Qirui Fu,Tai Zhong,Xinyu Wang,Xiangru Tang,Nan Tang,Chenglin Wu,Yuyu Luo*

Main category: cs.CL

TL;DR: 本文提出了InteractComp，一个用于评估搜索代理在面对模糊查询时能否主动交互以消除歧义的新基准。研究发现现有模型在处理不完整查询时表现不佳且过度自信，尽管在完全上下文下表现良好，但其交互能力在过去15个月中停滞不前。


<details>
  <summary>Details</summary>
Motivation: 现实中的用户查询往往是不完整或模糊的，需要通过交互澄清，但当前的语言代理缺乏这种互动机制，且缺少评估该能力的基准。

Method: 提出InteractComp基准，基于目标-干扰项方法构建210个跨9个领域的专家标注问题，确保歧义只能通过交互解决，并评估17种模型的表现。

Result: 最佳模型在模糊查询下准确率仅为13.73%，远低于在完整上下文下的71.50%；强制交互显著提升性能，显示模型具备潜在能力但未被激活；过去15个月交互能力无明显进展。

Conclusion: InteractComp揭示了当前语言代理在交互式搜索中的关键缺陷和系统性过度自信问题，为训练和评估搜索代理的交互能力提供了重要资源。

Abstract: Language agents have demonstrated remarkable potential in web search and
information retrieval. However, these search agents assume user queries are
complete and unambiguous, an assumption that diverges from reality where users
begin with incomplete queries requiring clarification through interaction. Yet
most agents lack interactive mechanisms during the search process, and existing
benchmarks cannot assess this capability. To address this gap, we introduce
InteractComp, a benchmark designed to evaluate whether search agents can
recognize query ambiguity and actively interact to resolve it during search.
Following the principle of easy to verify, interact to disambiguate, we
construct 210 expert-curated questions across 9 domains through a
target-distractor methodology that creates genuine ambiguity resolvable only
through interaction. Evaluation of 17 models reveals striking failure: the best
model achieves only 13.73% accuracy despite 71.50% with complete context,
exposing systematic overconfidence rather than reasoning deficits. Forced
interaction produces dramatic gains, demonstrating latent capability current
strategies fail to engage. Longitudinal analysis shows interaction capabilities
stagnated over 15 months while search performance improved seven-fold,
revealing a critical blind spot. This stagnation, coupled with the immediate
feedback inherent to search tasks, makes InteractComp a valuable resource for
both evaluating and training interaction capabilities in search agents. The
code is available at https://github.com/FoundationAgents/InteractComp.

</details>


### [147] [Dissecting Role Cognition in Medical LLMs via Neuronal Ablation](https://arxiv.org/abs/2510.24677)
*Xun Liang,Huayi Lai,Hanyu Wang,Wentao Zhang,Linfeng Zhang,Yanfang Chen,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 本研究提出RP-Neuron-Activated评价框架（RPNA），通过神经元消融和表征分析，评估角色提示是否引发大语言模型在医疗问答中产生特定认知过程。结果表明，角色提示主要影响语言风格，未显著提升医学推理能力，也未引发不同的推理路径，核心决策机制在不同角色间保持一致。


<details>
  <summary>Details</summary>
Motivation: 探究提示词中的角色设定是否真正影响大语言模型的推理机制，而非仅改变语言风格，以评估当前医学角色扮演方法的有效性。

Method: 提出RPNA框架，在三个医学问答数据集上使用神经元消融和表征分析技术，比较不同临床角色提示下模型的内部推理路径变化。

Result: 角色提示未引起显著的推理能力提升或认知分化，仅影响表层语言特征；模型的核心决策机制在不同角色下保持一致。

Conclusion: 当前基于提示的角色扮演方法未能模拟真实医疗实践中的认知复杂性，仅实现语言模仿，需开发能真正模拟专业认知过程的医疗AI模型。

Abstract: Large language models (LLMs) have gained significant traction in medical
decision support systems, particularly in the
  context of medical question answering and role-playing simulations. A common
practice, Prompt-Based Role Playing (PBRP),
  instructs models to adopt different clinical roles (e.g., medical students,
residents, attending physicians) to simulate varied
  professional behaviors. However, the impact of such role prompts on model
reasoning capabilities remains unclear. This
  study introduces the RP-Neuron-Activated Evaluation Framework(RPNA) to
evaluate whether role prompts induce distinct,
  role-specific cognitive processes in LLMs or merely modify linguistic style.
We test this framework on three medical QA
  datasets, employing neuron ablation and representation analysis techniques to
assess changes in reasoning pathways. Our
  results demonstrate that role prompts do not significantly enhance the
medical reasoning abilities of LLMs. Instead, they
  primarily affect surface-level linguistic features, with no evidence of
distinct reasoning pathways or cognitive differentiation
  across clinical roles. Despite superficial stylistic changes, the core
decision-making mechanisms of LLMs remain uniform
  across roles, indicating that current PBRP methods fail to replicate the
cognitive complexity found in real-world medical
  practice. This highlights the limitations of role-playing in medical AI and
emphasizes the need for models that simulate genuine
  cognitive processes rather than linguistic imitation.We have released the
related code in the following repository:https:
  //github.com/IAAR-Shanghai/RolePlay_LLMDoctor

</details>


### [148] [SPICE: Self-Play In Corpus Environments Improves Reasoning](https://arxiv.org/abs/2510.24684)
*Bo Liu,Chuanyang Jin,Seungone Kim,Weizhe Yuan,Wenting Zhao,Ilia Kulikov,Xian Li,Sainbayar Sukhbaatar,Jack Lanchantin,Jason Weston*

Main category: cs.CL

TL;DR: SPICE是一种基于语料库环境的自对弈强化学习框架，通过挑战者与推理者的对抗动态实现模型的持续自我提升。


<details>
  <summary>Details</summary>
Motivation: 现有无 grounding 的自对弈方法改进有限，缺乏持续自我进化所需的丰富外部信号。

Method: 提出SPICE框架，让同一模型担任挑战者（从大规模语料库中挖掘文档生成推理任务）和推理者（解决任务）两种角色，在语料库支持的环境中进行自对弈。

Result: 在数学（+8.9%）和通用推理（+9.8%）基准测试中，多个模型家族均取得持续提升；分析表明语料库 grounding 能持续生成更具挑战性的目标。

Conclusion: 语料库 grounding 是实现持续自我改进的关键，SPICE通过结合环境交互与对抗学习，显著优于传统自对弈方法。

Abstract: Self-improving systems require environmental interaction for continuous
adaptation. We introduce SPICE (Self-Play In Corpus Environments), a
reinforcement learning framework where a single model acts in two roles: a
Challenger that mines documents from a large corpus to generate diverse
reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,
the Challenger creates an automatic curriculum at the frontier of the
Reasoner's capability, while corpus grounding provides the rich,
near-inexhaustible external signal necessary for sustained improvement. Unlike
existing ungrounded self-play methods that offer more limited benefits, SPICE
achieves consistent gains across mathematical (+8.9%) and general reasoning
(+9.8%) benchmarks on multiple model families. Our analysis reveals how
document grounding is a key ingredient in SPICE to continuously generate its
own increasingly challenging goals and achieve them, enabling sustained
self-improvement.

</details>


### [149] [Repurposing Synthetic Data for Fine-grained Search Agent Supervision](https://arxiv.org/abs/2510.24694)
*Yida Zhao,Kuan Li,Xixi Wu,Liwen Zhang,Dingchu Zhang,Baixuan Li,Maojia Song,Zhuo Chen,Chenxi Wang,Xinyu Wang,Kewei Tu,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种面向实体的组相对策略优化（E-GRPO）方法，通过利用推理过程中识别出的实体信息构建密集奖励函数，使基于大模型的搜索代理能从“接近正确”的错误样本中学习，从而显著提升在知识密集型任务中的性能和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有训练方法如GRPO忽略了实体信息，仅依赖稀疏的结果奖励，无法区分推理过程基本正确但答案错误的‘近似成功’样本与完全失败的样本，导致浪费有价值的训练信号。

Method: 提出E-GRPO框架，引入基于真实实体匹配率的密集奖励机制，在训练中对错误样本根据其识别出的正确实体比例给予部分奖励，从而增强模型的学习能力。

Result: 在多个问答和深度研究基准上的实验表明，E-GRPO显著优于GRPO基线，不仅提高了准确率，还减少了工具调用次数，实现了更高效的推理策略。

Conclusion: E-GRPO通过利用被忽略的实体信息，提供了一种更有效、样本更高效的搜索代理对齐方法，能够更好地利用合成数据中的知识并提升复杂任务的性能。

Abstract: LLM-based search agents are increasingly trained on entity-centric synthetic
data to solve complex, knowledge-intensive tasks. However, prevailing training
methods like Group Relative Policy Optimization (GRPO) discard this rich entity
information, relying instead on sparse, outcome-based rewards. This critical
limitation renders them unable to distinguish informative "near-miss"
samples-those with substantially correct reasoning but a flawed final
answer-from complete failures, thus discarding valuable learning signals. We
address this by leveraging the very entities discarded during training. Our
empirical analysis reveals a strong positive correlation between the number of
ground-truth entities identified during an agent's reasoning process and final
answer accuracy. Building on this insight, we introduce Entity-aware Group
Relative Policy Optimization (E-GRPO), a novel framework that formulates a
dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect
samples proportional to their entity match rate, enabling the model to
effectively learn from these "near-misses". Experiments on diverse
question-answering (QA) and deep research benchmarks show that E-GRPO
consistently and significantly outperforms the GRPO baseline. Furthermore, our
analysis reveals that E-GRPO not only achieves superior accuracy but also
induces more efficient reasoning policies that require fewer tool calls,
demonstrating a more effective and sample-efficient approach to aligning search
agents.

</details>


### [150] [AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis](https://arxiv.org/abs/2510.24695)
*Xuanzhong Chen,Zile Qiao,Guoxin Chen,Liangcai Su,Zhen Zhang,Xinyu Wang,Pengjun Xie,Fei Huang,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: 提出一种基于最近发展区（ZPD）理论的数据合成方法，通过AgentFrontier引擎生成位于大语言模型能力前沿的高质量多学科训练数据，并构建ZPD Exam评估基准，显著提升模型在复杂推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 为了突破大语言模型在复杂任务上的推理能力，需要在其能力边界（即ZPD）内进行训练，但缺乏有效的方法来生成此类前沿任务数据。

Method: 提出AgentFrontier引擎，自动化合成处于LLM最近发展区内的知识密集型和复杂推理任务数据，支持持续预训练和针对性后训练；同时构建动态评估基准ZPD Exam。

Result: 训练出的AgentFrontier-30B-A3B模型在Humanity's Last Exam等高难度基准上达到SOTA性能，甚至超越一些领先的专有模型。

Conclusion: 基于ZPD的数据合成方法为构建更强大LLM代理提供了一条可扩展且有效的路径。

Abstract: Training large language model agents on tasks at the frontier of their
capabilities is key to unlocking advanced reasoning. We introduce a data
synthesis approach inspired by the educational theory of the Zone of Proximal
Development (ZPD), which defines this frontier as tasks an LLM cannot solve
alone but can master with guidance. To operationalize this, we present the
AgentFrontier Engine, an automated pipeline that synthesizes high-quality,
multidisciplinary data situated precisely within the LLM's ZPD. This engine
supports both continued pre-training with knowledge-intensive data and targeted
post-training on complex reasoning tasks. From the same framework, we derive
the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent
capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on
our synthesized data, which achieves state-of-the-art results on demanding
benchmarks like Humanity's Last Exam, even surpassing some leading proprietary
agents. Our work demonstrates that a ZPD-guided approach to data synthesis
offers a scalable and effective path toward building more capable LLM agents.

</details>


### [151] [WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking](https://arxiv.org/abs/2510.24697)
*Zhengwei Tao,Haiyang Shen,Baixuan Li,Wenbiao Yin,Jialong Wu,Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Liwen Zhang,Xinyu Wang,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: 本文提出WebLeaper框架，通过构建高覆盖率的信息检索任务和高效搜索路径，提升大模型代理在信息检索中的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索代理因目标实体稀疏导致搜索效率低下，限制了整体性能。

Method: 将信息检索建模为树结构推理问题，利用维基百科表格生成三类合成任务（Basic、Union、Reverse-Union），并筛选准确且高效的训练轨迹。

Result: 在五个基准测试上实验表明，该方法在有效性和效率方面均优于强基线。

Conclusion: WebLeaper通过高覆盖任务构造与高效轨迹学习，显著提升了LLM代理的信息检索能力。

Abstract: Large Language Model (LLM)-based agents have emerged as a transformative
approach for open-ended problem solving, with information seeking (IS) being a
core capability that enables autonomous reasoning and decision-making. While
prior research has largely focused on improving retrieval depth, we observe
that current IS agents often suffer from low search efficiency, which in turn
constrains overall performance. A key factor underlying this inefficiency is
the sparsity of target entities in training tasks, which limits opportunities
for agents to learn and generalize efficient search behaviors. To address these
challenges, we propose WebLeaper, a framework for constructing high-coverage IS
tasks and generating efficient solution trajectories. We formulate IS as a
tree-structured reasoning problem, enabling a substantially larger set of
target entities to be embedded within a constrained context. Leveraging curated
Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic,
Union, and Reverse-Union, to systematically increase both IS efficiency and
efficacy. Finally, we curate training trajectories by retaining only those that
are simultaneously accurate and efficient, ensuring that the model is optimized
for both correctness and search performance. Extensive experiments on both
basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,
GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method
consistently achieves improvements in both effectiveness and efficiency over
strong baselines.

</details>


### [152] [ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking](https://arxiv.org/abs/2510.24698)
*Baixuan Li,Dingchu Zhang,Jialong Wu,Wenbiao Yin,Zhengwei Tao,Yida Zhao,Liwen Zhang,Haiyang Shen,Runnan Fang,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: 提出了一种名为ParallelMuse的两阶段并行思维范式，通过功能指定的部分 rollout 和压缩推理聚合，提升信息探索代理的问题解决能力，显著提高性能并减少令牌消耗。


<details>
  <summary>Details</summary>
Motivation: 传统并行思维在信息探索代理中存在效率低下和难以整合长程推理轨迹的问题，受限于重复从头生成和上下文容量限制。

Method: 采用两阶段方法：第一阶段为功能指定的部分 rollout，通过不确定性引导的路径复用与分支提升探索效率；第二阶段为压缩推理聚合，利用推理冗余进行无损压缩并合成最终答案。

Result: 在多个开源代理和基准测试中，性能最高提升62%，探索性令牌消耗减少10%-30%。

Conclusion: ParallelMuse有效解决了并行思维中的效率与长程推理整合难题，显著提升了深度信息探索代理的问题解决能力和资源效率。

Abstract: Parallel thinking expands exploration breadth, complementing the deep
exploration of information-seeking (IS) agents to further enhance
problem-solving capability. However, conventional parallel thinking faces two
key challenges in this setting: inefficiency from repeatedly rolling out from
scratch, and difficulty in integrating long-horizon reasoning trajectories
during answer generation, as limited context capacity prevents full
consideration of the reasoning process. To address these issues, we propose
ParallelMuse, a two-stage paradigm designed for deep IS agents. The first
stage, Functionality-Specified Partial Rollout, partitions generated sequences
into functional regions and performs uncertainty-guided path reuse and
branching to enhance exploration efficiency. The second stage, Compressed
Reasoning Aggregation, exploits reasoning redundancy to losslessly compress
information relevant to answer derivation and synthesize a coherent final
answer. Experiments across multiple open-source agents and benchmarks
demonstrate up to 62% performance improvement with a 10--30% reduction in
exploratory token consumption.

</details>


### [153] [AgentFold: Long-Horizon Web Agents with Proactive Context Management](https://arxiv.org/abs/2510.24699)
*Rui Ye,Zhongwang Zhang,Kuan Li,Huifeng Yin,Zhengwei Tao,Yida Zhao,Liangcai Su,Liwen Zhang,Zile Qiao,Xinyu Wang,Pengjun Xie,Fei Huang,Siheng Chen,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: AgentFold是一种新型的LLM-based网络代理范式，通过受人类回溯性巩固启发的主动上下文管理机制，在长周期任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有ReAct-based代理在处理长周期任务时面临上下文饱和或关键信息丢失的问题，亟需更有效的上下文管理方法。

Method: 提出AgentFold，引入‘折叠’操作，动态地在多个尺度上压缩和整合历史信息，实现细粒度保留关键细节或抽象多步子任务。

Result: 在BrowseComp和BrowseComp-ZH基准上分别达到36.2%和47.3%的成绩，超越了更大规模的开源模型（如DeepSeek-V3.1-671B）和领先的专有代理（如OpenAI o4-mini）。

Conclusion: AgentFold通过类人认知的上下文塑形机制，有效解决了长周期任务中的上下文管理难题，展现出卓越的性能和应用潜力。

Abstract: LLM-based web agents show immense promise for information seeking, yet their
effectiveness on long-horizon tasks is hindered by a fundamental trade-off in
context management. Prevailing ReAct-based agents suffer from context
saturation as they accumulate noisy, raw histories, while methods that fixedly
summarize the full history at each step risk the irreversible loss of critical
details. Addressing these, we introduce AgentFold, a novel agent paradigm
centered on proactive context management, inspired by the human cognitive
process of retrospective consolidation. AgentFold treats its context as a
dynamic cognitive workspace to be actively sculpted, rather than a passive log
to be filled. At each step, it learns to execute a `folding' operation, which
manages its historical trajectory at multiple scales: it can perform granular
condensations to preserve vital, fine-grained details, or deep consolidations
to abstract away entire multi-step sub-tasks. The results on prominent
benchmarks are striking: with simple supervised fine-tuning (without continual
pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp
and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or
matches open-source models of a dramatically larger scale, such as the
DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like
OpenAI's o4-mini.

</details>


### [154] [Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents](https://arxiv.org/abs/2510.24702)
*Yueqi Song,Ketan Ramaneti,Zaid Sheikh,Ziru Chen,Boyu Gou,Tianbao Xie,Yiheng Xu,Danyang Zhang,Apurva Gandhi,Fan Yang,Joseph Liu,Tianyue Ou,Zhihao Yuan,Frank Xu,Shuyan Zhou,Xingyao Wang,Xiang Yue,Tao Yu,Huan Sun,Yu Su,Graham Neubig*

Main category: cs.CL

TL;DR: 本文提出了代理数据协议（ADP），一种轻量级的表示语言，旨在统一多种格式的代理训练数据集，以解决大规模监督微调中数据碎片化的问题。通过将13个现有数据集转换为ADP格式并进行实验，展示了平均性能提升约20%，并在多个标准基准上达到或接近最先进水平。


<details>
  <summary>Details</summary>
Motivation: 由于代理训练数据分散在不同的格式、工具和接口中，导致公开的大规模监督微调研究成果较少。本文旨在通过引入统一的数据协议来打破这一瓶颈。

Method: 设计了一种轻量级且表达能力强的代理数据协议（ADP），能够兼容多种任务类型（如API使用、浏览、编码等），并将13个现有代理数据集统一转换为ADP格式，进而适配多个代理训练框架。

Result: 将13个代理数据集成功统一为ADP格式，并用于监督微调，实验结果显示相比基础模型平均性能提升约20%，在编码、浏览、工具使用和研究等标准基准上达到SOTA或接近SOTA水平。

Conclusion: ADP有效解决了代理训练数据碎片化问题，显著提升了模型性能，且无需领域特定调优，有助于实现标准化、可扩展和可复现的代理训练。

Abstract: Public research results on large-scale supervised finetuning of AI agents
remain relatively rare, since the collection of agent training data presents
unique challenges. In this work, we argue that the bottleneck is not a lack of
underlying data sources, but that a large variety of data is fragmented across
heterogeneous formats, tools, and interfaces. To this end, we introduce the
agent data protocol (ADP), a light-weight representation language that serves
as an "interlingua" between agent datasets in diverse formats and unified agent
training pipelines downstream. The design of ADP is expressive enough to
capture a large variety of tasks, including API/tool use, browsing, coding,
software engineering, and general agentic workflows, while remaining simple to
parse and train on without engineering at a per-dataset level. In experiments,
we unified a broad collection of 13 existing agent training datasets into ADP
format, and converted the standardized ADP data into training-ready formats for
multiple agent frameworks. We performed SFT on these data, and demonstrated an
average performance gain of ~20% over corresponding base models, and delivers
state-of-the-art or near-SOTA performance on standard coding, browsing, tool
use, and research benchmarks, without domain-specific tuning. All code and data
are released publicly, in the hope that ADP could help lower the barrier to
standardized, scalable, and reproducible agent training.

</details>


### [155] [ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?](https://arxiv.org/abs/2510.24706)
*Shuqing Li,Jiayi Yan,Chenyu Niu,Jen-tse Huang,Yun Peng,Wenxuan Wang,Yepang Liu,Michael R. Lyu*

Main category: cs.CL

TL;DR: 本文提出了ComboBench，一个评估大语言模型（LLMs）在虚拟现实（VR）游戏中将语义动作转化为设备操作能力的基准，涵盖262个来自四款流行VR游戏的场景，并对七种主流LLM进行了评测，发现尽管顶级模型具备较强的任务分解能力，但在程序推理和空间理解方面仍逊于人类。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型是否能够像人类一样基于常识和具身理解，将高层语义动作转化为具体的VR设备操作序列。

Method: 构建名为ComboBench的基准测试，包含262个来自Half-Life: Alyx、Into the Radius、Moss: Book II和Vivecraft四个VR游戏的场景，评估GPT-3.5、GPT-4、GPT-4o、Gemini-1.5-Pro、LLaMA-3-8B、Mixtral-8x7B和GLM-4-Flash七种LLM的表现，并与人工标注的真实数据和人类表现进行对比。

Result: Gemini-1.5-Pro等顶级模型展现出较强的任务分解能力，但在程序推理和空间理解方面仍显著弱于人类；不同VR游戏中的表现差异显著，表明模型对交互复杂度敏感；少量示例（few-shot）能显著提升模型性能。

Conclusion: 当前大语言模型在VR动作转化任务中具有一定潜力，尤其通过few-shot学习可提升表现，但在复杂推理和空间感知方面仍有局限，未来需进一步增强其具身推理能力。

Abstract: Virtual Reality (VR) games require players to translate high-level semantic
actions into precise device manipulations using controllers and head-mounted
displays (HMDs). While humans intuitively perform this translation based on
common sense and embodied understanding, whether Large Language Models (LLMs)
can effectively replicate this ability remains underexplored. This paper
introduces a benchmark, ComboBench, evaluating LLMs' capability to translate
semantic actions into VR device manipulation sequences across 262 scenarios
from four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,
and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,
Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared against
annotated ground truth and human performance. Our results reveal that while
top-performing models like Gemini-1.5-Pro demonstrate strong task decomposition
capabilities, they still struggle with procedural reasoning and spatial
understanding compared to humans. Performance varies significantly across
games, suggesting sensitivity to interaction complexity. Few-shot examples
substantially improve performance, indicating potential for targeted
enhancement of LLMs' VR manipulation capabilities. We release all materials at
https://sites.google.com/view/combobench.

</details>


### [156] [MetricX-25 and GemSpanEval: Google Translate Submissions to the WMT25 Evaluation Shared Task](https://arxiv.org/abs/2510.24707)
*Juraj Juraska,Tobias Domhan,Mara Finkelstein,Tetsuji Nakagawa,Geza Kovacs,Daniel Deutsch,Pidong Wang,Markus Freitag*

Main category: cs.CL

TL;DR: 本文提出了用于WMT25翻译评估共享任务的两种新方法：改进版MetricX-25用于质量评分预测，以及新型生成式模型GemSpanEval用于错误片段检测。


<details>
  <summary>Details</summary>
Motivation: 为了提升翻译质量评估的准确性和可解释性，特别是在质量评分预测和错误片段检测两个子任务上超越现有方法。

Method: 基于Gemma 3模型，MetricX-25采用编码器-only架构并添加回归头进行质量分数预测；GemSpanEval为解码器-only模型，将错误片段检测视为生成任务，并输出错误上下文、严重程度和类别。

Result: MetricX-25在MQM和ESA质量评分预测上显著优于前代模型；GemSpanEval在错误片段检测上与强基线xCOMET相当，并能生成明确的错误上下文。

Conclusion: 所提出的方法在翻译评估任务中表现出色，尤其是通过生成式建模提升了错误检测的可读性与实用性。

Abstract: In this paper, we present our submissions to the unified WMT25 Translation
Evaluation Shared Task. For the Quality Score Prediction subtask, we create a
new generation of MetricX with improvements in the input format and the
training protocol, while for the Error Span Detection subtask we develop a new
model, GemSpanEval, trained to predict error spans along with their severities
and categories. Both systems are based on the state-of-the-art multilingual
open-weights model Gemma 3, fine-tuned on publicly available WMT data. We
demonstrate that MetricX-25, adapting Gemma 3 to an encoder-only architecture
with a regression head on top, can be trained to effectively predict both MQM
and ESA quality scores, and significantly outperforms its predecessor. Our
decoder-only GemSpanEval model, on the other hand, we show to be competitive in
error span detection with xCOMET, a strong encoder-only sequence-tagging
baseline. With error span detection formulated as a generative task, we
instruct the model to also output the context for each predicted error span,
thus ensuring that error spans are identified unambiguously.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [157] [RoboOmni: Proactive Robot Manipulation in Omni-modal Context](https://arxiv.org/abs/2510.23763)
*Siyin Wang,Jinlan Fu,Feihong Liu,Xinzhe He,Huangxuan Wu,Junhao Shi,Kexin Huang,Zhaoye Fei,Jingjing Gong,Zuxuan Wu,Yugang Jiang,See-Kiong Ng,Tat-Seng Chua,Xipeng Qiu*

Main category: cs.RO

TL;DR: 本文提出了RoboOmni，一种基于多模态大语言模型的框架，用于从对话、环境声音和视觉线索中推断用户意图，实现机器人主动协作。


<details>
  <summary>Details</summary>
Motivation: 现有机器人系统依赖明确指令，而人类交互中通常不直接下达命令，因此需要机器人能主动推断用户意图以实现有效协作。

Method: 提出跨模态上下文指令新场景，构建RoboOmni框架（感知-思考-对话-执行），融合视听信号进行意图识别，并构建包含14万段数据的OmniAction数据集。

Result: 在仿真和真实环境中实验表明，RoboOmni在成功率、推理速度、意图识别和主动辅助方面优于文本和ASR基线方法。

Conclusion: RoboOmni能够有效利用多模态上下文信息实现机器人对用户意图的主动识别与响应，推动了人机协作向更自然的交互方式发展。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid
progress in Vision-Language-Action (VLA) models for robotic manipulation.
Although effective in many scenarios, current approaches largely rely on
explicit instructions, whereas in real-world interactions, humans rarely issue
instructions directly. Effective collaboration requires robots to infer user
intentions proactively. In this work, we introduce cross-modal contextual
instructions, a new setting where intent is derived from spoken dialogue,
environmental sounds, and visual cues rather than explicit commands. To address
this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor
framework based on end-to-end omni-modal LLMs that unifies intention
recognition, interaction confirmation, and action execution. RoboOmni fuses
auditory and visual signals spatiotemporally for robust intention recognition,
while supporting direct speech interaction. To address the absence of training
data for proactive intention recognition in robotic manipulation, we build
OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640
backgrounds, and six contextual instruction types. Experiments in simulation
and real-world settings show that RoboOmni surpasses text- and ASR-based
baselines in success rate, inference speed, intention recognition, and
proactive assistance.

</details>


### [158] [Motivating Students' Self-study with Goal Reminder and Emotional Support](https://arxiv.org/abs/2510.23860)
*Hyung Chan Cho,Go-Eum Cha,Yanfu Liu,Sooyeon Jeong*

Main category: cs.RO

TL;DR: 该研究探讨了社交机器人作为大学生自学伙伴的潜力，通过提供目标提醒和情感支持来提升专注度、生产力和参与度。


<details>
  <summary>Details</summary>
Motivation: 尽管社交机器人在学习任务中的有效性已被广泛研究，但其在学生自学场景中的作用尚未充分探索。

Method: 采用探索性“傀儡”实验（Wizard-of-Oz）方法，比较提供目标提醒、情感支持的机器人与仅提供物理存在的对照组对学生的感知影响。

Result: 接受目标提醒和情感支持的学生报告更高的易用性感知，目标提醒组更愿意在未来使用机器人；用户对机器人的社会性感知与其目标达成水平呈正相关。

Conclusion: 社交机器人通过功能性和情感性互动，具备显著支持自学活动的潜力。

Abstract: While the efficacy of social robots in supporting people in learning tasks
has been extensively investigated, their potential impact in assisting students
in self-studying contexts has not been investigated much. This study explores
how a social robot can act as a peer study companion for college students
during self-study tasks by delivering task-oriented goal reminder and positive
emotional support. We conducted an exploratory Wizard-of-Oz study to explore
how these robotic support behaviors impacted students' perceived focus,
productivity, and engagement in comparison to a robot that only provided
physical presence (control). Our study results suggest that participants in the
goal reminder and the emotional support conditions reported greater ease of
use, with the goal reminder condition additionally showing a higher willingness
to use the robot in future study sessions. Participants' satisfaction with the
robot was correlated with their perception of the robot as a social other, and
this perception was found to be a predictor for their level of goal achievement
in the self-study task. These findings highlight the potential of socially
assistive robots to support self-study through both functional and emotional
engagement.

</details>


### [159] [Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped](https://arxiv.org/abs/2510.23902)
*Jans Solano,Diego Quiroz*

Main category: cs.RO

TL;DR: 提出了一种低成本的轮腿式四足机器人视觉-惯性导航系统，结合深度相机感知和深度强化学习策略，实现鲁棒运动与自主跌倒恢复。


<details>
  <summary>Details</summary>
Motivation: 现有轮腿机器人多依赖高成本传感器与执行器，且缺乏跌倒恢复能力，限制了其在低成本平台上的应用。

Method: 采用深度相机进行视觉-惯性导航，结合基于深度强化学习的运动与恢复策略，在仿真和实际环境中验证方法有效性。

Result: 在低扭矩执行器条件下实现了复杂地形下的敏捷运动，能可靠地从外部扰动和自引发失败中恢复，并完成室内目标导航任务。

Conclusion: 该方法显著降低了在低成本机器人平台上部署自主导航与鲁棒运动策略的门槛，提升了系统的实用性与适应性。

Abstract: Wheeled-legged robots combine the efficiency of wheels with the obstacle
negotiation of legs, yet many state-of-the-art systems rely on costly actuators
and sensors, and fall-recovery is seldom integrated, especially for
wheeled-legged morphologies. This work presents a recovery-aware
visual-inertial navigation system on a low-cost wheeled quadruped. The proposed
system leverages vision-based perception from a depth camera and deep
reinforcement learning policies for robust locomotion and autonomous recovery
from falls across diverse terrains. Simulation experiments show agile mobility
with low-torque actuators over irregular terrain and reliably recover from
external perturbations and self-induced failures. We further show goal directed
navigation in structured indoor spaces with low-cost perception. Overall, this
approach lowers the barrier to deploying autonomous navigation and robust
locomotion policies in budget-constrained robotic platforms.

</details>


### [160] [Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments](https://arxiv.org/abs/2510.23928)
*Raman Jha,Yang Zhou,Giuseppe Loianno*

Main category: cs.RO

TL;DR: 本文提出了一种自适应关键帧选择方法，通过误差和动量机制提升动态环境下的3D场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，传统固定间隔的关键帧选择方法难以应对场景变化，导致信息冗余或缺失，影响实时感知与重建质量。

Method: 结合基于误差（光度误差和SSIM）的关键帧选择模块与基于动量的阈值更新模块，动态调整关键帧选取策略。

Result: 在Spann3r和CUT3R两个先进3D重建网络上均显著提升了重建质量，优于固定时间间隔等传统方法，消融实验验证了各模块的有效性。

Conclusion: 所提自适应关键帧选择方法能有效应对动态场景变化，生成高质量3D表示，推动可扩展的机器人学习与部署。

Abstract: In this paper, we propose an adaptive keyframe selection method for improved
3D scene reconstruction in dynamic environments. The proposed method integrates
two complementary modules: an error-based selection module utilizing
photometric and structural similarity (SSIM) errors, and a momentum-based
update module that dynamically adjusts keyframe selection thresholds according
to scene motion dynamics. By dynamically curating the most informative frames,
our approach addresses a key data bottleneck in real-time perception. This
allows for the creation of high-quality 3D world representations from a
compressed data stream, a critical step towards scalable robot learning and
deployment in complex, dynamic environments. Experimental results demonstrate
significant improvements over traditional static keyframe selection strategies,
such as fixed temporal intervals or uniform frame skipping. These findings
highlight a meaningful advancement toward adaptive perception systems that can
dynamically respond to complex and evolving visual scenes. We evaluate our
proposed adaptive keyframe selection module on two recent state-of-the-art 3D
reconstruction networks, Spann3r and CUT3R, and observe consistent improvements
in reconstruction quality across both frameworks. Furthermore, an extensive
ablation study confirms the effectiveness of each individual component in our
method, underlining their contribution to the overall performance gains.

</details>


### [161] [A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons](https://arxiv.org/abs/2510.23954)
*Pejman Kheradmand,Behnam Moradkhani,Raghavasimhan Sankaranarayanan,Kent K. Yamamoto,Tanner J. Zachem,Patrick J. Codd,Yash Chitalia,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 提出了一种基于Cosserat杆理论的通用力学模型，用于描述肌腱驱动的同心管机器人，实验验证了其在不同构型下的高精度形状预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有肌腱驱动连续体机器人和同心管机器人存在自由度受限或失稳等问题，且缺乏统一的通用力学模型。

Method: 基于Cosserat杆理论建立n根同心管、每根由mi条肌腱驱动的通用模型，允许各管扭转和拉伸，同时约束弯曲时共享中心线。

Result: 实验显示末端位置预测误差小于机器人总长度的4%，在已有机器人上的应用最大偏差约5%。

Conclusion: 该模型为肌腱驱动同心管机器人的精确形状估计与控制提供了基础。

Abstract: Tendon-actuated concentric tube mechanisms combine the advantages of
tendon-driven continuum robots and concentric tube robots while addressing
their respective limitations. They overcome the restricted degrees of freedom
often seen in tendon-driven designs, and mitigate issues such as snapping
instability associated with concentric tube robots. However, a complete and
general mechanical model for these systems remains an open problem. In this
work, we propose a Cosserat rod-based framework for modeling the general case
of $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \{1,
\ldots, n\}$. The model allows each tube to twist and elongate while enforcing
a shared centerline for bending. We validate the proposed framework through
experiments with two-tube and three tube assemblies under various tendon
routing configurations, achieving tip prediction errors $<4\%$ of the robot's
total length. We further demonstrate the model's generality by applying it to
existing robots in the field, where maximum tip deviations remain around $5\%$
of the total length. This model provides a foundation for accurate shape
estimation and control of advanced tendon-actuated concentric tube robots.

</details>


### [162] [Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping](https://arxiv.org/abs/2510.23963)
*Hiroki Ishikawa,Kyosuke Ishibashi,Ko Yamamoto*

Main category: cs.RO

TL;DR: 本文提出了一种具有自适应扭转变形能力的软体机器人手指，可通过包裹方式抓取物体。


<details>
  <summary>Details</summary>
Motivation: 为了使软体手能够从密集堆放的多个物体中抓取单个物体，需要手指具备在平面内和平面外方向的自适应扭转功能，以便深入狭窄间隙并实现稳定抓取。

Method: 设计了一种可变刚度机制，通过单一驱动源实现压力升高时刚度自适应变化，并利用有限元分析优化设计参数。

Result: 仿真和实验结果验证了该软体手指能有效实现自适应扭转变形，并成功抓取多种不同形状的物体。

Conclusion: 所提出的软体手指结合可变刚度机制，能够在单一驱动下实现稳定的包裹式抓取，适用于复杂环境下的物体拾取任务。

Abstract: This paper presents a soft robot finger capable of adaptive-twist deformation
to grasp objects by wrapping them. For a soft hand to grasp and pick-up one
object from densely contained multiple objects, a soft finger requires the
adaptive-twist deformation function in both in-plane and out-of-plane
directions. The function allows the finger to be inserted deeply into a limited
gap among objects. Once inserted, the soft finger requires appropriate control
of grasping force normal to contact surface, thereby maintaining the twisted
deformation. In this paper, we refer to this type of grasping as grasping by
wrapping. To achieve these two functions by a single actuation source, we
propose a variable stiffness mechanism that can adaptively change the stiffness
as the pressure is higher. We conduct a finite element analysis (FEA) on the
proposed mechanism and determine its design parameter based on the FEA result.
Using the developed soft finger, we report basic experimental results and
demonstrations on grasping various objects.

</details>


### [163] [A Survey on Collaborative SLAM with 3D Gaussian Splatting](https://arxiv.org/abs/2510.23988)
*Phuc Nguyen Xuan,Thanh Nguyen Canh,Huu-Hung Nguyen,Nak Young Chong,Xiem HoangVan*

Main category: cs.RO

TL;DR: 本文综述了基于3D高斯点阵的多机器人协同SLAM技术，系统分类了集中式和分布式架构方法，并分析了一致性对齐、通信效率、语义蒸馏等关键技术，总结了数据集与评估指标，指出了终身建图、语义关联、多模态鲁棒性及Sim2Real迁移等未来方向。


<details>
  <summary>Details</summary>
Motivation: 3D高斯点阵（3DGS）在实时高保真渲染方面表现出色，适合机器人应用，但在多机器人系统中面临全局一致性、通信开销和异构数据融合等挑战，亟需系统性梳理与研究。

Method: 通过分类集中式与分布式架构，系统分析多智能体一致性、通信效率、高斯表示、语义蒸馏、数据融合与位姿优化等核心组件，并总结常用数据集与评估指标。

Result: 全面梳理了3DGS在多机器人SLAM中的应用现状，明确了各类方法的优势与局限，提供了现有技术的比较分析，并总结了关键数据集和性能度量标准。

Conclusion: 3DGS为多机器人协同SLAM带来新机遇，但仍面临一致性维护、通信负载和可扩展性等挑战，未来应关注语义增强、多模态融合、持续学习与真实场景部署。

Abstract: This survey comprehensively reviews the evolving field of multi-robot
collaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian
Splatting (3DGS). As an explicit scene representation, 3DGS has enabled
unprecedented real-time, high-fidelity rendering, ideal for robotics. However,
its use in multi-robot systems introduces significant challenges in maintaining
global consistency, managing communication, and fusing data from heterogeneous
sources. We systematically categorize approaches by their architecture --
centralized, distributed -- and analyze core components like multi-agent
consistency and alignment, communication-efficient, Gaussian representation,
semantic distillation, fusion and pose optimization, and real-time scalability.
In addition, a summary of critical datasets and evaluation metrics is provided
to contextualize performance. Finally, we identify key open challenges and
chart future research directions, including lifelong mapping, semantic
association and mapping, multi-model for robustness, and bridging the Sim2Real
gap.

</details>


### [164] [VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion](https://arxiv.org/abs/2510.23997)
*Stanley Wu,Mohamad H. Danesh,Simon Li,Hanna Yurchyk,Amin Abyaneh,Anas El Houssaini,David Meger,Hsiu-Chin Lin*

Main category: cs.RO

TL;DR: 提出VOCALoco，一种基于感知输入动态选择安全且节能的预训练运动策略的模块化框架，在楼梯行走任务中表现出优于端到端DRL方法的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有基于端到端深度强化学习的腿式机器人运动方法在安全性和可解释性方面存在不足，尤其难以泛化到新地形。

Method: 设计一个模块化技能选择框架VOCALoco，通过预测执行安全性和运输成本，评估多个预训练策略的可行性和能耗，实现局部地形依赖的动态策略选择。

Result: 在仿真和真实四足机器人上验证了VOCALoco在楼梯上下行任务中的有效性，相比传统端到端DRL策略表现出更高的鲁棒性和安全性。

Conclusion: VOCALoco通过结合感知与多策略评估，在保证运动安全性与能效的同时提升了复杂地形下的适应能力，为可解释、安全的机器人运动控制提供了有效方案。

Abstract: Recent advancements in legged robot locomotion have facilitated traversal
over increasingly complex terrains. Despite this progress, many existing
approaches rely on end-to-end deep reinforcement learning (DRL), which poses
limitations in terms of safety and interpretability, especially when
generalizing to novel terrains. To overcome these challenges, we introduce
VOCALoco, a modular skill-selection framework that dynamically adapts
locomotion strategies based on perceptual input. Given a set of pre-trained
locomotion policies, VOCALoco evaluates their viability and energy-consumption
by predicting both the safety of execution and the anticipated cost of
transport over a fixed planning horizon. This joint assessment enables the
selection of policies that are both safe and energy-efficient, given the
observed local terrain. We evaluate our approach on staircase locomotion tasks,
demonstrating its performance in both simulated and real-world scenarios using
a quadrupedal robot. Empirical results show that VOCALoco achieves improved
robustness and safety during stair ascent and descent compared to a
conventional end-to-end DRL policy

</details>


### [165] [Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model](https://arxiv.org/abs/2510.24029)
*Andrew Gerstenslager,Bekarys Dukenbaev,Ali A. Minai*

Main category: cs.RO

TL;DR: 提出了一种结合垂直角度敏感性的三维边界向量细胞（BVC）模型，利用LiDAR数据捕捉垂直轮廓，有效消除位置歧义，提升复杂环境中的空间定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有BVC模型多局限于二维环境，在存在水平对称性时易产生空间歧义，难以准确表征真实三维空间。

Method: 在传统BVC框架中引入垂直角度敏感性，通过处理LiDAR数据提取垂直轮廓，实现三维空间中的边界检测与定位。

Result: 实验表明，在垂直变化小的环境中，该3D模型性能与2D基线相当；而在三维复杂度较高的环境中，能生成更清晰的位场，显著减少空间混叠。

Conclusion: 将垂直维度引入BVC模型可显著提升真实三维环境中的导航与建图能力，同时保持在近平面场景中的性能。

Abstract: Boundary Vector Cells (BVCs) are a class of neurons in the brains of
vertebrates that encode environmental boundaries at specific distances and
allocentric directions, playing a central role in forming place fields in the
hippocampus. Most computational BVC models are restricted to two-dimensional
(2D) environments, making them prone to spatial ambiguities in the presence of
horizontal symmetries in the environment. To address this limitation, we
incorporate vertical angular sensitivity into the BVC framework, thereby
enabling robust boundary detection in three dimensions, and leading to
significantly more accurate spatial localization in a biologically-inspired
robot model.
  The proposed model processes LiDAR data to capture vertical contours, thereby
disambiguating locations that would be indistinguishable under a purely 2D
representation. Experimental results show that in environments with minimal
vertical variation, the proposed 3D model matches the performance of a 2D
baseline; yet, as 3D complexity increases, it yields substantially more
distinct place fields and markedly reduces spatial aliasing. These findings
show that adding a vertical dimension to BVC-based localization can
significantly enhance navigation and mapping in real-world 3D spaces while
retaining performance parity in simpler, near-planar scenarios.

</details>


### [166] [SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration](https://arxiv.org/abs/2510.24052)
*Jongsuk Kim,Jaeyoung Lee,Gyojin Han,Dongjae Lee,Minki Jeong,Junmo Kim*

Main category: cs.RO

TL;DR: 本文提出了SynAD框架，首次将合成数据用于增强端到端自动驾驶模型，通过在多智能体合成场景中指定信息最全的智能体为自车，并利用地图到鸟瞰图网络提取特征，有效融合合成数据与真实驾驶数据，显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 仅依赖真实世界数据限制了训练场景的多样性，而现有合成数据难以直接应用于端到端自动驾驶模型，因缺乏明确的自车及传感器输入。

Method: 在多智能体合成场景中选择信息最完整的智能体作为自车；将路径级场景投影到地图上；使用新设计的Map-to-BEV网络从地图生成鸟瞰图特征，无需依赖传感器输入；提出一种融合合成数据与真实数据的训练策略。

Result: 实验结果表明，SynAD能有效整合各组件，在安全性能方面有显著提升。

Conclusion: SynAD填补了合成场景生成与端到端自动驾驶之间的空白，为构建更全面、更鲁棒的自动驾驶模型提供了新路径。

Abstract: Recent advancements in deep learning and the availability of high-quality
real-world driving datasets have propelled end-to-end autonomous driving.
Despite this progress, relying solely on real-world data limits the variety of
driving scenarios for training. Synthetic scenario generation has emerged as a
promising solution to enrich the diversity of training data; however, its
application within E2E AD models remains largely unexplored. This is primarily
due to the absence of a designated ego vehicle and the associated sensor
inputs, such as camera or LiDAR, typically provided in real-world scenarios. To
address this gap, we introduce SynAD, the first framework designed to enhance
real-world E2E AD models using synthetic data. Our method designates the agent
with the most comprehensive driving information as the ego vehicle in a
multi-agent synthetic scenario. We further project path-level scenarios onto
maps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view
features without relying on sensor inputs. Finally, we devise a training
strategy that effectively integrates these map-based synthetic data with real
driving data. Experimental results demonstrate that SynAD effectively
integrates all components and notably enhances safety performance. By bridging
synthetic scenario generation and E2E AD, SynAD paves the way for more
comprehensive and robust autonomous driving models.

</details>


### [167] [Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation](https://arxiv.org/abs/2510.24055)
*Xiucheng Zhang,Yang Jiang,Hongwei Qing,Jiashuo Bai*

Main category: cs.RO

TL;DR: 提出结合语言条件视觉表示（LCVR）和语言条件专家混合密度策略（LMoE-DP）的框架，以解决模仿学习中多任务机器人操作的感知模糊和任务冲突问题，显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 感知模糊和任务冲突限制了通过模仿学习实现多任务机器人操作的性能，需要更鲁棒的方法来区分相似任务并处理多模态动作分布。

Method: 设计LCVR模块，利用语言指令对视觉特征进行语义接地以消除感知歧义；采用稀疏专家结构的LMoE-DP策略，通过梯度调制稳定训练，实现对不同任务的动作分布专业化。

Result: 在真实机器人基准上，LCVR使ACT和DP的成功率分别提升33.75%和25%；完整框架达到79%的平均成功率，比先进基线高21%。

Conclusion: 结合语义接地与专家专业化可有效提升多任务模仿学习的鲁棒性与效率，适用于复杂机器人操作场景。

Abstract: Perceptual ambiguity and task conflict limit multitask robotic manipulation
via imitation learning. We propose a framework combining a Language-Conditioned
Visual Representation (LCVR) module and a Language-conditioned
Mixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual
ambiguities by grounding visual features with language instructions, enabling
differentiation between visually similar tasks. To mitigate task conflict,
LMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal
action distributions, stabilized by gradient modulation. On real-robot
benchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion
Policy (DP) success rates by 33.75% and 25%, respectively. The full framework
achieves a 79% average success, outperforming the advanced baseline by 21%. Our
work shows that combining semantic grounding and expert specialization enables
robust, efficient multi-task manipulation

</details>


### [168] [Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition](https://arxiv.org/abs/2510.24067)
*Tianyi Ding,Ronghao Zheng,Senlin Zhang,Meiqin Liu*

Main category: cs.RO

TL;DR: 提出一种基于拓扑地图的分布式多机器人在线探索方法，通过加权图Voronoi分割实现均衡的任务分配，在复杂环境中提高了探索效率、完整性和负载均衡。


<details>
  <summary>Details</summary>
Motivation: 在障碍物密集的非凸环境中，现有方法难以实现多机器人团队的动态平衡探索区域划分与任务分配，导致探索效率低和工作负载不均。

Method: 构建一种新型拓扑地图结构，增量更新可达空间并基于全局覆盖指导规划探索目标；提出分布式加权拓扑图Voronoi算法进行均衡空间划分；设计局部规划器优化访问顺序并生成安全平滑的运动轨迹。

Result: 理论证明了分布式共识收敛性和均衡图划分的有界性；实验表明该方法在探索效率、完整性和团队负载均衡方面显著优于现有最先进方法。

Conclusion: 所提方法有效解决了非凸障碍环境下的多机器人协同探索问题，实现了高效、完整且负载均衡的分布式在线探索。

Abstract: This work addresses the collaborative multi-robot autonomous online
exploration problem, particularly focusing on distributed exploration planning
for dynamically balanced exploration area partition and task allocation among a
team of mobile robots operating in obstacle-dense non-convex environments.
  We present a novel topological map structure that simultaneously
characterizes both spatial connectivity and global exploration completeness of
the environment. The topological map is updated incrementally to utilize known
spatial information for updating reachable spaces, while exploration targets
are planned in a receding horizon fashion under global coverage guidance.
  A distributed weighted topological graph Voronoi algorithm is introduced
implementing balanced graph space partitions of the fused topological maps.
Theoretical guarantees are provided for distributed consensus convergence and
equitable graph space partitions with constant bounds.
  A local planner optimizes the visitation sequence of exploration targets
within the balanced partitioned graph space to minimize travel distance, while
generating safe, smooth, and dynamically feasible motion trajectories.
  Comprehensive benchmarking against state-of-the-art methods demonstrates
significant improvements in exploration efficiency, completeness, and workload
balance across the robot team.

</details>


### [169] [Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition](https://arxiv.org/abs/2510.24069)
*Sangmin Kim,Hajun Kim,Gijeong Kim,Min-Gyu Kim,Hae-Won Park*

Main category: cs.RO

TL;DR: 本文提出了一种基于相位的轨迹优化方法，用于生成具有动态可靠性的腿式机器人运动，通过贝塞尔多项式的性质确保动力学和摩擦锥约束的满足。


<details>
  <summary>Details</summary>
Motivation: 为了在轨迹优化中同时计算机器人的路径和接触序列，并准确考虑动力学，以生成可靠的腿部机器人运动。

Method: 利用线性微分方程的叠加性质解耦各接触点的平移动力学，并结合贝塞尔多项式的微分矩阵建立位置与力之间的解析关系，同时利用其凸闭包性质保证摩擦锥约束的满足。

Result: 所提出的方法能够为腿式机器人生成包含多种步态序列的动态可靠的运动，并在四足机器人模型上验证了动力学可行性和运动生成效果。

Conclusion: 该框架能有效生成满足动力学和摩擦约束的可靠运动，适用于多种步态的腿式机器人轨迹优化。

Abstract: To generate reliable motion for legged robots through trajectory
optimization, it is crucial to simultaneously compute the robot's path and
contact sequence, as well as accurately consider the dynamics in the problem
formulation. In this paper, we present a phase-based trajectory optimization
that ensures the feasibility of translational dynamics and friction cone
constraints throughout the entire trajectory. Specifically, our approach
leverages the superposition properties of linear differential equations to
decouple the translational dynamics for each contact point, which operates
under different phase sequences. Furthermore, we utilize the differentiation
matrix of B{\'e}zier polynomials to derive an analytical relationship between
the robot's position and force, thereby ensuring the consistent satisfaction of
translational dynamics. Additionally, by exploiting the convex closure property
of B{\'e}zier polynomials, our method ensures compliance with friction cone
constraints. Using the aforementioned approach, the proposed trajectory
optimization framework can generate dynamically reliable motions with various
gait sequences for legged robots. We validate our framework using a quadruped
robot model, focusing on the feasibility of dynamics and motion generation.

</details>


### [170] [ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring](https://arxiv.org/abs/2510.24108)
*Zhenxin Li,Wenhao Yao,Zi Wang,Xinglong Sun,Jingde Chen,Nadine Chang,Maying Shen,Jingyu Song,Zuxuan Wu,Shiyi Lan,Jose M. Alvarez*

Main category: cs.RO

TL;DR: 本文提出了ZTRS，首个完全基于强化学习、无需模仿学习、直接从高维传感器数据进行端到端自动驾驶的框架。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法依赖模仿学习，受限于专家演示质量与部署时的协变量偏移；而强化学习多用于低维输入，难以应用于原始传感器数据。

Method: 提出ZTRS框架，结合强化学习与高维传感器输入，采用离线强化学习和专为可枚举动作设计的 Exhaustive Policy Optimization (EPO) 算法。

Result: 在Navtest、Navhard和HUGSIM三个基准上表现优异，尤其在Navhard上达到SOTA，并在HUGSIM上优于基于模仿学习的方法。

Conclusion: ZTRS首次实现了完全无需模仿学习、直接从传感器数据进行强化学习的端到端自动驾驶框架，验证了其在复杂场景下的有效性与鲁棒性。

Abstract: End-to-end autonomous driving maps raw sensor inputs directly into
ego-vehicle trajectories to avoid cascading errors from perception modules and
to leverage rich semantic cues. Existing frameworks largely rely on Imitation
Learning (IL), which can be limited by sub-optimal expert demonstrations and
covariate shift during deployment. On the other hand, Reinforcement Learning
(RL) has recently shown potential in scaling up with simulations, but is
typically confined to low-dimensional symbolic inputs (e.g. 3D objects and
maps), falling short of full end-to-end learning from raw sensor data. We
introduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory
Scoring), a framework that combines the strengths of both worlds: sensor inputs
without losing information and RL training for robust planning. To the best of
our knowledge, ZTRS is the first framework that eliminates IL entirely by only
learning from rewards while operating directly on high-dimensional sensor data.
ZTRS utilizes offline reinforcement learning with our proposed Exhaustive
Policy Optimization (EPO), a variant of policy gradient tailored for enumerable
actions and rewards. ZTRS demonstrates strong performance across three
benchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop
planning in challenging real-world and synthetic scenarios), and HUGSIM
(simulated closed-loop driving). Specifically, ZTRS achieves the
state-of-the-art result on Navhard and outperforms IL-based baselines on
HUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.

</details>


### [171] [PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI](https://arxiv.org/abs/2510.24109)
*Wenbin Ding,Jun Chen,Mingjia Chen,Fei Xie,Qi Mao,Philip Dames*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉-语言模型的具身智能体框架，用于实现能够在物理世界中执行复杂自然语言指令的机器人操作代理，并在模拟和真实环境中显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的具身代理在在线规划和执行复杂自然语言控制任务方面能力不足，难以满足人类中心人工智能对机器人智能水平的需求。

Method: 提出一种包含人机语音交互模块、视觉-语言代理模块和动作执行模块的新型具身代理框架；其中视觉-语言代理包括基于视觉的任务规划器、自然语言指令转换器和任务执行反馈评估器。

Result: 实验结果表明，该代理在模拟和真实环境中的平均任务成功率比仅依赖LLM+CLIP的方法高出28%，显著提高了高层自然语言指令任务的执行成功率。

Conclusion: 所提出的基于VLM的具身代理框架有效增强了机器人对复杂自然语言指令的理解与执行能力，推动了人类中心人工智能的发展。

Abstract: The rapid advancement of Large Language Models (LLMs) has marked a
significant breakthrough in Artificial Intelligence (AI), ushering in a new era
of Human-centered Artificial Intelligence (HAI). HAI aims to better serve human
welfare and needs, thereby placing higher demands on the intelligence level of
robots, particularly in aspects such as natural language interaction, complex
task planning, and execution. Intelligent agents powered by LLMs have opened up
new pathways for realizing HAI. However, existing LLM-based embodied agents
often lack the ability to plan and execute complex natural language control
tasks online. This paper explores the implementation of intelligent robotic
manipulating agents based on Vision-Language Models (VLMs) in the physical
world. We propose a novel embodied agent framework for robots, which comprises
a human-robot voice interaction module, a vision-language agent module and an
action execution module. The vision-language agent itself includes a
vision-based task planner, a natural language instruction converter, and a task
performance feedback evaluator. Experimental results demonstrate that our agent
achieves a 28\% higher average task success rate in both simulated and real
environments compared to approaches relying solely on LLM+CLIP, significantly
improving the execution success rate of high-level natural language instruction
tasks.

</details>


### [172] [LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation](https://arxiv.org/abs/2510.24118)
*Haotian Zhou,Xiaole Wang,He Li,Fusheng Sun,Shengyu Guo,Guolei Qi,Jianghuan Xu,Huijing Zhao*

Main category: cs.RO

TL;DR: 提出LagMemo，一种利用语言3D高斯点阵记忆的视觉导航系统，支持多模态、开放词汇和多目标导航，通过构建统一的3D语言记忆并结合局部感知验证机制，在新基准GOAT-Core上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统视觉导航方法受限于单目标、单模态和封闭集设置，难以应对实际应用中多模态、开放词汇和多目标的复杂查询需求。

Method: LagMemo在探索过程中构建统一的3D语言记忆，通过查询记忆预测候选目标位置，并结合局部感知验证机制在导航过程中动态匹配和验证目标。

Result: 实验表明，LagMemo的记忆模块能有效实现多模态开放词汇目标定位，并在多目标视觉导航任务中优于当前最先进方法。同时提出了用于公平评估的高质量基准GOAT-Core。

Conclusion: LagMemo通过语言3D高斯点阵记忆实现了高效的多模态、开放词汇、多目标视觉导航，为智能机器人在复杂环境中的导航提供了更灵活和强大的解决方案。

Abstract: Navigating to a designated goal using visual information is a fundamental
capability for intelligent robots. Most classical visual navigation methods are
restricted to single-goal, single-modality, and closed set goal settings. To
address the practical demands of multi-modal, open-vocabulary goal queries and
multi-goal visual navigation, we propose LagMemo, a navigation system that
leverages a language 3D Gaussian Splatting memory. During exploration, LagMemo
constructs a unified 3D language memory. With incoming task goals, the system
queries the memory, predicts candidate goal locations, and integrates a local
perception-based verification mechanism to dynamically match and validate goals
during navigation. For fair and rigorous evaluation, we curate GOAT-Core, a
high-quality core split distilled from GOAT-Bench tailored to multi-modal
open-vocabulary multi-goal visual navigation. Experimental results show that
LagMemo's memory module enables effective multi-modal open-vocabulary goal
localization, and that LagMemo outperforms state-of-the-art methods in
multi-goal visual navigation. Project page:
https://weekgoodday.github.io/lagmemo

</details>


### [173] [Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames](https://arxiv.org/abs/2510.24194)
*Ev Zisselman,Mirco Mutti,Shelly Francis-Meretzki,Elisei Shafer,Aviv Tamar*

Main category: cs.RO

TL;DR: 本文提出了一种通过“蒙眼”专家（即隐藏部分任务信息）进行行为克隆的新方法，以提升在未见任务上的泛化能力。实验和理论分析均表明，相比完全知情的专家，克隆“蒙眼”专家能在更少的任务演示下实现更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 为了提升行为克隆在多样化任务中的泛化能力，尤其是在演示数据有限的情况下，作者提出故意隐藏任务信息，迫使专家进行探索性行为，从而学习更具通用性的策略。

Method: 通过限制示范者对任务信息的访问（即‘盲folded’设置），使其必须通过探索来完成任务，然后使用行为克隆学习该策略，并在真实机器人插 peg 任务和Procgen游戏环境中进行验证，同时提供了理论分析支持。

Result: 实验表明，克隆‘盲folded’专家的行为在未见任务上具有更好的泛化能力；理论分析显示，泛化误差随任务信息量I的平方根减小而降低，且所需演示任务数m更少。

Conclusion: 隐藏任务信息给示范者反而能提升行为克隆的泛化性能，表明适度的信息缺失可促进更鲁棒和通用策略的学习，为构建物理世界的通用基础模型提供了新思路。

Abstract: Behavioral cloning is a simple yet effective technique for learning
sequential decision-making from demonstrations. Recently, it has gained
prominence as the core of foundation models for the physical world, where
achieving generalization requires countless demonstrations of a multitude of
tasks. Typically, a human expert with full information on the task demonstrates
a (nearly) optimal behavior. In this paper, we propose to hide some of the
task's information from the demonstrator. This ``blindfolded'' expert is
compelled to employ non-trivial exploration to solve the task. We show that
cloning the blindfolded expert generalizes better to unseen tasks than its
fully-informed counterpart. We conduct experiments of real-world robot peg
insertion tasks with (limited) human demonstrations, alongside videogames from
the Procgen benchmark. Additionally, we support our findings with theoretical
analysis, which confirms that the generalization error scales with
$\sqrt{I/m}$, where $I$ measures the amount of task information available to
the demonstrator, and $m$ is the number of demonstrated tasks. Both theory and
practice indicate that cloning blindfolded experts generalizes better with
fewer demonstrated tasks. Project page with videos and code:
https://sites.google.com/view/blindfoldedexperts/home

</details>


### [174] [Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors](https://arxiv.org/abs/2510.24257)
*Ziqi Ma,Changda Tian,Yue Gao*

Main category: cs.RO

TL;DR: 本文提出了一种基于对抗运动先验的新型方法HMAMP，用于学习人类风格的操作技能，并在锤击任务中表现出优于现有基线方法的效果。


<details>
  <summary>Details</summary>
Motivation: 使机器人和自主系统能够以类似人类的方式操作物体和工具，从而实现更自然、直观的人机交互。

Method: 利用对抗网络建模工具和物体操作的复杂动态及任务目标，结合真实世界数据和模拟数据训练判别器，以生成符合人类运动统计特性的逼真运动轨迹。

Result: 在锤击任务中，HMAMP成功学习到人类风格的操作技能，并在仿真和真实机器人实验中表现良好，优于当前基线方法。

Conclusion: HMAMP是实现类人操作能力的重要一步，有助于推动机器人以更自然的方式与人类交互。

Abstract: In recent years, there has been growing interest in developing robots and
autonomous systems that can interact with human in a more natural and intuitive
way. One of the key challenges in achieving this goal is to enable these
systems to manipulate objects and tools in a manner that is similar to that of
humans. In this paper, we propose a novel approach for learning human-style
manipulation skills by using adversarial motion priors, which we name HMAMP.
The approach leverages adversarial networks to model the complex dynamics of
tool and object manipulation, as well as the aim of the manipulation task. The
discriminator is trained using a combination of real-world data and simulation
data executed by the agent, which is designed to train a policy that generates
realistic motion trajectories that match the statistical properties of human
motion. We evaluated HMAMP on one challenging manipulation task: hammering, and
the results indicate that HMAMP is capable of learning human-style manipulation
skills that outperform current baseline methods. Additionally, we demonstrate
that HMAMP has potential for real-world applications by performing real robot
arm hammering tasks. In general, HMAMP represents a significant step towards
developing robots and autonomous systems that can interact with humans in a
more natural and intuitive way, by learning to manipulate tools and objects in
a manner similar to how humans do.

</details>


### [175] [DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation](https://arxiv.org/abs/2510.24261)
*Jingyi Tian,Le Wang,Sanping Zhou,Sen Wang,Jiayi Li,Gang Hua*

Main category: cs.RO

TL;DR: 本文提出了DynaRend，一种通过多视角RGB-D视频数据预训练的3D感知且动态信息融合的表示学习框架，能够统一捕捉空间几何、未来动态和任务语义，并在机器人操作任务中显著提升策略成功率、泛化能力和实际应用性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏多样化的现实世界训练数据，学习可泛化的机器人操作策略仍具挑战性；现有自监督方法大多基于2D视觉预训练或视频预测，难以同时建模几何、语义和动态信息。

Method: 提出DynaRend框架，利用可微体积渲染，通过掩码重建和未来状态预测，在三平面表示中联合学习3D感知且动态感知的特征。

Result: 在RLBench、Colosseum基准测试及真实机器人实验中，DynaRend在策略成功率、环境扰动下的泛化能力以及实际应用方面均显著优于现有方法。

Conclusion: DynaRend通过联合建模几何、语义与动态信息，有效提升了机器人操作策略的泛化性和实用性，验证了3D感知动态表示在真实场景中的优势。

Abstract: Learning generalizable robotic manipulation policies remains a key challenge
due to the scarcity of diverse real-world training data. While recent
approaches have attempted to mitigate this through self-supervised
representation learning, most either rely on 2D vision pretraining paradigms
such as masked image modeling, which primarily focus on static semantics or
scene geometry, or utilize large-scale video prediction models that emphasize
2D dynamics, thus failing to jointly learn the geometry, semantics, and
dynamics required for effective manipulation. In this paper, we present
DynaRend, a representation learning framework that learns 3D-aware and
dynamics-informed triplane features via masked reconstruction and future
prediction using differentiable volumetric rendering. By pretraining on
multi-view RGB-D video data, DynaRend jointly captures spatial geometry, future
dynamics, and task semantics in a unified triplane representation. The learned
representations can be effectively transferred to downstream robotic
manipulation tasks via action value map prediction. We evaluate DynaRend on two
challenging benchmarks, RLBench and Colosseum, as well as in real-world robotic
experiments, demonstrating substantial improvements in policy success rate,
generalization to environmental perturbations, and real-world applicability
across diverse manipulation tasks.

</details>


### [176] [Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation](https://arxiv.org/abs/2510.24315)
*Baozhe Zhang,Xinwei Chen,Qingcheng Chen,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 提出了一种名为CoNi-OA的新型避障算法，专为无人机-无人车协同任务设计，仅依赖相对状态和单帧LiDAR数据，实现实时、低计算开销的避障。


<details>
  <summary>Details</summary>
Motivation: CoNi-MPC框架虽能有效实现空地协同控制，但缺乏环境信息导致避障困难，需在不依赖全局状态估计的情况下解决该问题。

Method: 利用无人机单帧原始LiDAR数据构建调制矩阵，在非惯性系下直接调整四旋翼速度以实现避障，无需障碍物建模或预测。

Result: 实现了低于5毫秒每次迭代的实时轨迹生成，能在动态和未知环境中安全运行，并适用于无特征或未知场景。

Conclusion: CoNi-OA为无人机-无人车协同提供了高效、实时且适应性强的避障解决方案，显著降低了对全局状态和环境建模的依赖。

Abstract: CoNi-MPC provides an efficient framework for UAV control in air-ground
cooperative tasks by relying exclusively on relative states, eliminating the
need for global state estimation. However, its lack of environmental
information poses significant challenges for obstacle avoidance. To address
this issue, we propose a novel obstacle avoidance algorithm, Cooperative
Non-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for
UAV-UGV cooperative scenarios without reliance on global state estimation or
obstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data
from the UAV to generate a modulation matrix, which directly adjusts the
quadrotor's velocity to achieve obstacle avoidance. This modulation-based
method enables real-time generation of collision-free trajectories within the
UGV's non-inertial frame, significantly reducing computational demands (less
than 5 ms per iteration) while maintaining safety in dynamic and unpredictable
environments. The key contributions of this work include: (1) a
modulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV
cooperation in non-inertial frames without global states; (2) rapid, real-time
trajectory generation based solely on single-frame LiDAR data, removing the
need for obstacle modeling or prediction; and (3) adaptability to both static
and dynamic environments, thus extending applicability to featureless or
unknown scenarios.

</details>


### [177] [NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation](https://arxiv.org/abs/2510.24335)
*Mingyu Jeong,Eunsung Kim,Sehun Park,Andrew Jaeyong Choi*

Main category: cs.RO

TL;DR: 本文提出了一种名为NVSim的框架，能够仅通过普通图像序列自动生成大规模可导航的室内模拟器，克服了传统3D扫描在成本和扩展性上的限制。


<details>
  <summary>Details</summary>
Motivation: 传统3D扫描方法成本高且难以扩展，无法高效构建大规模可导航室内环境模拟器。

Method: 采用3D高斯点阵技术，并提出地板感知的高斯点阵方法以消除稀疏观测地板上的视觉伪影，同时引入一种无需网格的可通行性检测算法，通过直接分析渲染视图构建拓扑图。

Result: 成功从真实世界数据中生成有效的、大规模的导航图，验证了系统的可行性与实用性。

Conclusion: NVSim能够在低成本条件下实现高质量、可导航的大规模室内环境重建，为机器人导航研究提供了新的工具。

Abstract: We present NVSim, a framework that automatically constructs large-scale,
navigable indoor simulators from only common image sequences, overcoming the
cost and scalability limitations of traditional 3D scanning. Our approach
adapts 3D Gaussian Splatting to address visual artifacts on sparsely observed
floors a common issue in robotic traversal data. We introduce Floor-Aware
Gaussian Splatting to ensure a clean, navigable ground plane, and a novel
mesh-free traversability checking algorithm that constructs a topological graph
by directly analyzing rendered views. We demonstrate our system's ability to
generate valid, large-scale navigation graphs from real-world data. A video
demonstration is avilable at https://youtu.be/tTiIQt6nXC8

</details>


### [178] [Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance](https://arxiv.org/abs/2510.24457)
*Jorge Vicente-Martinez,Edgar Ramirez-Laboreo*

Main category: cs.RO

TL;DR: 提出了一种基于微分平坦性的3D桥式起重机最优轨迹生成方法，能够考虑复杂的物理和动态约束（如非线性摩擦和避障），并通过仿真验证了摩擦建模对快速安全轨迹的重要性。


<details>
  <summary>Details</summary>
Motivation: 为了实现3D桥式起重机在复杂环境下的快速、安全运动，需考虑非线性摩擦和碰撞等实际约束，传统方法常忽略这些因素导致性能下降。

Method: 利用微分平坦性理论直接将非线性摩擦和负载与绳索的避障等动态约束纳入轨迹优化框架，并仅在终点约束负载摆动，以实现激进但安全的运动。

Result: 仿真对比表明，忽略干摩擦会导致执行器饱和和碰撞，而所提方法能有效避免这些问题，证明了摩擦建模对高速安全轨迹生成至关重要。

Conclusion: 准确的摩擦建模是实现3D桥式起重机快速、安全轨迹生成的关键，所提基于微分平坦性的方法能有效集成复杂约束并提升控制性能。

Abstract: This paper presents an optimal trajectory generation method for 3D overhead
cranes by leveraging differential flatness. This framework enables the direct
inclusion of complex physical and dynamic constraints, such as nonlinear
friction and collision avoidance for both payload and rope. Our approach allows
for aggressive movements by constraining payload swing only at the final point.
A comparative simulation study validates our approach, demonstrating that
neglecting dry friction leads to actuator saturation and collisions. The
results show that friction modeling is a fundamental requirement for fast and
safe crane trajectories.

</details>


### [179] [Supervisory Measurement-Guided Noise Covariance Estimation](https://arxiv.org/abs/2510.24508)
*Haoying Li,Yifan Peng,Junfeng Wu*

Main category: cs.RO

TL;DR: 提出一种双层优化方法，通过贝叶斯视角分解联合似然来估计传感器噪声协方差，提升状态估计的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 准确的传感器噪声协方差对可靠的状态估计至关重要，但在实际中由于环境变化和前端处理等因素难以确定。

Method: 将噪声协方差估计建模为双层优化问题，利用贝叶斯方法分解里程计和监督测量的联合似然，采用带状态增广的不变扩展卡尔曼滤波器估计轨迹，并用导数滤波器并行计算梯度以更新上层协方差。

Result: 在合成和真实数据集上的实验表明，该方法相比现有基线具有更高的计算效率。

Conclusion: 所提双层优化框架能有效平衡信息利用与计算效率，显著提升噪声协方差估计的性能。

Abstract: Reliable state estimation hinges on accurate specification of sensor noise
covariances, which weigh heterogeneous measurements. In practice, these
covariances are difficult to identify due to environmental variability,
front-end preprocessing, and other reasons. We address this by formulating
noise covariance estimation as a bilevel optimization that, from a Bayesian
perspective, factorizes the joint likelihood of so-called odometry and
supervisory measurements, thereby balancing information utilization with
computational efficiency. The factorization converts the nested Bayesian
dependency into a chain structure, enabling efficient parallel computation: at
the lower level, an invariant extended Kalman filter with state augmentation
estimates trajectories, while a derivative filter computes analytical gradients
in parallel for upper-level gradient updates. The upper level refines the
covariance to guide the lower-level estimation. Experiments on synthetic and
real-world datasets show that our method achieves higher efficiency over
existing baselines.

</details>


### [180] [Stochastic Prize-Collecting Games: Strategic Planning in Multi-Robot Systems](https://arxiv.org/abs/2510.24515)
*Malintha Fernando,Petter Ögren,Silun Zhang*

Main category: cs.RO

TL;DR: 提出了随机奖励收集游戏（SPCG）作为团队定向问题（TOP）的扩展，用于在存在自利机器人、能量限制和随机转移的情况下进行规划，并设计了ORS和FORL算法以实现高效学习和策略泛化。


<details>
  <summary>Details</summary>
Motivation: 现有团队定向问题假设所有机器人合作，无法适用于奖励稀缺环境中竞争性机器人的场景，因此需要扩展以处理非合作性多机器人系统。

Method: 提出Stochastic Prize-Collecting Games（SPCG）模型，结合图上的能量约束与随机转移；设计Ordinal Rank Search（ORS）确定局部序数排名，以及Fictitious Ordinal Response Learning（FORL）学习针对高优先级对手的最佳响应策略。

Result: 理论分析表明SPCG在完全图和星型图中存在唯一的纯纳什均衡，且与等效TOP的最优解一致；实验显示ORS条件下的状态聚合提升了大规模团队的可扩展性，FORL在不平衡奖励分布下具有更好的泛化能力，且SPCG策略达到TOP最优解87%-95%的性能。

Conclusion: SPCG为非合作多机器人路径规划提供了有效建模框架，ORS和FORL算法实现了高效学习与良好泛化，接近集中式TOP的最优性能。

Abstract: The Team Orienteering Problem (TOP) generalizes many real-world multi-robot
scheduling and routing tasks that occur in autonomous mobility, aerial
logistics, and surveillance applications. While many flavors of the TOP exist
for planning in multi-robot systems, they assume that all the robots cooperate
toward a single objective; thus, they do not extend to settings where the
robots compete in reward-scarce environments. We propose Stochastic
Prize-Collecting Games (SPCG) as an extension of the TOP to plan in the
presence of self-interested robots operating on a graph, under energy
constraints and stochastic transitions. A theoretical study on complete and
star graphs establishes that there is a unique pure Nash equilibrium in SPCGs
that coincides with the optimal routing solution of an equivalent TOP given a
rank-based conflict resolution rule. This work proposes two algorithms: Ordinal
Rank Search (ORS) to obtain the ''ordinal rank'' --one's effective rank in
temporarily-formed local neighborhoods during the games' stages, and Fictitious
Ordinal Response Learning (FORL) to obtain best-response policies against one's
senior-rank opponents. Empirical evaluations conducted on road networks and
synthetic graphs under both dynamic and stationary prize distributions show
that 1) the state-aliasing induced by OR-conditioning enables learning policies
that scale more efficiently to large team sizes than those trained with the
global index, and 2) Policies trained with FORL generalize better to imbalanced
prize distributions than those with other multi-agent training methods.
Finally, the learned policies in the SPCG achieved between 87% and 95%
optimality compared to an equivalent TOP solution obtained by mixed-integer
linear programming.

</details>


### [181] [GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots](https://arxiv.org/abs/2510.24533)
*Yuan Shen,Yuze Hong,Guangyang Zeng,Tengfei Zhang,Pui Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: 本文提出了一种重力增强的双目视觉惯性SLAM系统GeVI-SLAM，用于解决水下机器人中常见的视觉退化和IMU运动激励不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于水下环境中频繁的视觉退化和IMU运动激励不足，现有的视觉惯性SLAM系统难以实现稳定准确的定位与建图。

Method: 利用双目相机的直接深度估计能力，消除IMU初始化中的尺度估计需求；通过精确的重力初始化，将姿态估计解耦为4自由度PnP问题，并采用最小3点求解器提升计算效率；提出一种无偏的4-DOF PnP估计器，并在动态运动中联合估计IMU协方差以自适应调整重力先验权重。

Result: 在模拟和真实数据上的实验表明，GeVI-SLAM相比现有最先进方法具有更高的精度和稳定性。

Conclusion: GeVI-SLAM通过引入重力约束和优化姿态估计流程，在低加速度和动态水下环境中实现了更准确、更稳定的SLAM性能。

Abstract: Accurate visual inertial simultaneous localization and mapping (VI SLAM) for
underwater robots remains a significant challenge due to frequent visual
degeneracy and insufficient inertial measurement unit (IMU) motion excitation.
In this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system
designed to address these issues. By leveraging the stereo camera's direct
depth estimation ability, we eliminate the need to estimate scale during IMU
initialization, enabling stable operation even under low acceleration dynamics.
With precise gravity initialization, we decouple the pitch and roll from the
pose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point
(PnP) problem for pose tracking. This allows the use of a minimal 3-point
solver, which significantly reduces computational time to reject outliers
within a Random Sample Consensus framework. We further propose a
bias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the
relative pose converges to the true value as the feature number increases. To
handle dynamic motion, we refine the full 6-DOF pose while jointly estimating
the IMU covariance, enabling adaptive weighting of the gravity prior. Extensive
experiments on simulated and real-world data demonstrate that GeVI-SLAM
achieves higher accuracy and greater stability compared to state-of-the-art
methods.

</details>


### [182] [An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments](https://arxiv.org/abs/2510.24554)
*Vignesh Kottayam Viswanathan,Yifan Bai,Scott Fredriksson,Sumeet Satpute,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出了一种分层框架，用于在环境不确定性下支持机器人检测，通过全局视图规划和局部视图重规划相结合，确保在真实复杂环境中实现鲁棒性和全覆盖。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖已知环境模型进行路径规划，但实际环境与模型之间的差异可能导致表面形态变化或路径阻塞，影响检测任务的完成。

Method: 将检测任务分为两个层次：首先基于历史地图生成初始全局视图计划，然后在局部进行实时重规划以适应当前环境形态，结合全局覆盖目标与局部反应性调整。

Result: 该分层框架能够在环境不确定的情况下保持全局覆盖并实现局部自适应，提升机器人系统的鲁棒性，并在真实地下矿山环境中通过四足机器人进行了验证。

Conclusion: 所提出的分层框架有效解决了环境不确定性对机器人检测任务的影响，能够在复杂动态环境中实现安全、完整的检测。

Abstract: In this work, we present a hierarchical framework designed to support robotic
inspection under environment uncertainty. By leveraging a known environment
model, existing methods plan and safely track inspection routes to visit points
of interest. However, discrepancies between the model and actual site
conditions, caused by either natural or human activities, can alter the surface
morphology or introduce path obstructions. To address this challenge, the
proposed framework divides the inspection task into: (a) generating the initial
global view-plan for region of interests based on a historical map and (b)
local view replanning to adapt to the current morphology of the inspection
scene. The proposed hierarchy preserves global coverage objectives while
enabling reactive adaptation to the local surface morphology. This enables the
local autonomy to remain robust against environment uncertainty and complete
the inspection tasks. We validate the approach through deployments in
real-world subterranean mines using quadrupedal robot.

</details>


### [183] [Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots](https://arxiv.org/abs/2510.24571)
*Hongxu Zhao,Guangyang Zeng,Yunling Shao,Tengfei Zhang,Junfeng Wu*

Main category: cs.RO

TL;DR: 提出了一种统一的迭代校准（UIC）框架，用于一般性DVL传感器设置的外参和时钟偏移联合估计，基于最大后验与高斯过程运动先验，并通过仿真和实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有DVL校准方法受限于特定传感器配置或过于简化的假设，且无法同时估计平移外参和时间偏移，导致水下SLAM系统精度不足。

Method: 提出UIC框架，采用最大后验估计结合高斯过程运动先验进行高保真运动插值，交替执行高效的GP运动状态更新和基于梯度的校准参数优化，并设计了统计一致的序列初始化方案。

Result: UIC在仿真和真实实验中均表现出良好的校准性能，可推广至IMU、相机等多传感器系统，并开源了DVL-相机校准工具箱。

Conclusion: UIC为水下SLAM中的多传感器校准提供了通用、鲁棒的解决方案，其引入高斯过程先验和可靠初始化的方法对其他多传感器校准问题具有广泛借鉴意义。

Abstract: The calibration of extrinsic parameters and clock offsets between sensors for
high-accuracy performance in underwater SLAM systems remains insufficiently
explored. Existing methods for Doppler Velocity Log (DVL) calibration are
either constrained to specific sensor configurations or rely on oversimplified
assumptions, and none jointly estimate translational extrinsics and time
offsets. We propose a Unified Iterative Calibration (UIC) framework for general
DVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a
Gaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC
alternates between efficient GP-based motion state updates and gradient-based
calibration variable updates, supported by a provably statistically consistent
sequential initialization scheme. The proposed UIC can be applied to IMU,
cameras and other modalities as co-sensors. We release an open-source
DVL-camera calibration toolbox. Beyond underwater applications, several aspects
of UIC-such as the integration of GP priors for MAP-based calibration and the
design of provably reliable initialization procedures-are broadly applicable to
other multi-sensor calibration problems. Finally, simulations and real-world
tests validate our approach.

</details>


### [184] [Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning](https://arxiv.org/abs/2510.24584)
*Jørgen Anker Olsen,Lars Rønhaug Pettersen,Kostas Alexis*

Main category: cs.RO

TL;DR: 提出了一种基于课程学习的强化学习框架，用于训练机器人“Olympus”的精确跳跃策略，实现了超越以往工作的水平和垂直跳跃性能，并有效跨越了Sim2Real差距。


<details>
  <summary>Details</summary>
Motivation: 为了提升机器人在复杂环境下的动态跳跃能力，并解决稀疏奖励和现实迁移（Sim2Real）的挑战。

Method: 采用课程学习强化学习框架，分别设计垂直与水平跳跃策略；通过抛体运动定律稠密化稀疏跳跃奖励，并使用参考状态初始化加速动态行为探索，无需依赖参考轨迹。

Result: 实验验证了机器人可在多种地形上行走，并实现最高1.25米的水平跳跃和1.0米的垂直跳跃，精度达厘米级；稍作修改即可扩展至全向跳跃。

Conclusion: 所提方法显著提升了机器人跳跃性能，结合行走策略实现了多功能、高动态的移动能力，成功实现从仿真到现实的迁移。

Abstract: This paper presents a curriculum-based reinforcement learning framework for
training precise and high-performance jumping policies for the robot `Olympus'.
Separate policies are developed for vertical and horizontal jumps, leveraging a
simple yet effective strategy. First, we densify the inherently sparse jumping
reward using the laws of projectile motion. Next, a reference state
initialization scheme is employed to accelerate the exploration of dynamic
jumping behaviors without reliance on reference trajectories. We also present a
walking policy that, when combined with the jumping policies, unlocks versatile
and dynamic locomotion capabilities. Comprehensive testing validates walking on
varied terrain surfaces and jumping performance that exceeds previous works,
effectively crossing the Sim2Real gap. Experimental validation demonstrates
horizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to
1.0 m. Additionally, we show that with only minor modifications, the proposed
method can be used to learn omnidirectional jumping.

</details>


### [185] [GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization](https://arxiv.org/abs/2510.24623)
*Nicolai Steinke,Daniel Goehring*

Main category: cs.RO

TL;DR: 本文提出了一种名为GroundLoc的纯LiDAR定位流水线，能够在大规模室外环境中实现高精度、低存储需求的机器人定位。


<details>
  <summary>Details</summary>
Motivation: 为了在大规模室外环境中实现高效、鲁棒且低资源消耗的LiDAR-only定位。

Method: 采用鸟瞰图（BEV）投影关注地面区域，并结合R2D2网络或SIFT算法提取关键点进行地图配准。

Result: 在SemanticKITTI和HeLiPR数据集上优于现有方法，多会话定位平均轨迹误差（ATE）低于50厘米，支持多种传感器，每平方公里仅需4MB存储。

Conclusion: GroundLoc是一种高效、通用且适用于多种LiDAR传感器的高精度定位系统，具备在线运行能力和低存储开销。

Abstract: In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline
designed to localize a mobile robot in large-scale outdoor environments using
prior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing
on the perceived ground area and utilizes the place recognition network R2D2,
or alternatively, the non-learning approach Scale-Invariant Feature Transform
(SIFT), to identify and select keypoints for BEV image map registration. Our
results demonstrate that GroundLoc outperforms state-of-the-art methods on the
SemanticKITTI and HeLiPR datasets across various sensors. In the multi-session
localization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)
well below 50 cm on all Ouster OS2 128 sequences while meeting online runtime
requirements. The system supports various sensor models, as evidenced by
evaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,
and Livox Avia sensors. The prior maps are stored as 2D raster image maps,
which can be created from a single drive and require only 4 MB of storage per
square kilometer. The source code is available at
https://github.com/dcmlr/groundloc.

</details>


### [186] [Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder](https://arxiv.org/abs/2510.24671)
*Li Li,Tobias Brinkmann,Till Temmen,Markus Eisenbarth,Jakob Andert*

Main category: cs.RO

TL;DR: 本文提出了一种基于Transformer增强的条件变分自编码器（CVAE-T）模型，用于生成环岛中的多智能体交通场景，具有高真实性和多样性，并可用于智能驾驶功能的验证与数据增强。


<details>
  <summary>Details</summary>
Motivation: 随着智能驾驶功能在量产车中的广泛应用，确保其功能性和鲁棒性面临更大挑战；传统道路测试成本高、可重复性差，而基于场景的虚拟测试在效率和边缘案例探索方面更具优势。

Method: 提出一种Transformer增强的条件变分自编码器（CVAE-T）模型，用于生成环岛中多智能体交通场景，并利用两个关键性能指标（KPIs）评估生成场景中的交互行为，同时分析潜在空间的解耦特性。

Result: 该模型能够准确重建原始场景并生成逼真且多样化的合成场景；潜在空间表现出部分解耦，多个隐变量维度对车辆进入时间、离开时间和速度分布等场景属性具有可解释的影响。

Conclusion: CVAE-T模型能有效生成用于验证涉及多智能体交互的智能驾驶功能的交通场景，同时可作为数据增强手段支持其开发与迭代优化。

Abstract: With the increasing integration of intelligent driving functions into
serial-produced vehicles, ensuring their functionality and robustness poses
greater challenges. Compared to traditional road testing, scenario-based
virtual testing offers significant advantages in terms of time and cost
efficiency, reproducibility, and exploration of edge cases. We propose a
Transformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for
generating multi-agent traffic scenarios in roundabouts, which are
characterized by high vehicle dynamics and complex layouts, yet remain
relatively underexplored in current research. The results show that the
proposed model can accurately reconstruct original scenarios and generate
realistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators
(KPIs) are employed to evaluate the interactive behavior in the generated
scenarios. Analysis of the latent space reveals partial disentanglement, with
several latent dimensions exhibiting distinct and interpretable effects on
scenario attributes such as vehicle entry timing, exit timing, and velocity
profiles. The results demonstrate the model's capability to generate scenarios
for the validation of intelligent driving functions involving multi-agent
interactions, as well as to augment data for their development and iterative
improvement.

</details>


### [187] [Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis](https://arxiv.org/abs/2510.24676)
*Jiaxuan Zhang,Yuquan Leng,Yixuan Guo,Chenglong Fu*

Main category: cs.RO

TL;DR: 该研究提出一种基于健全侧脚踝惯性传感器的神经网络方法，用于预测截肢者穿越障碍时所需的股骨和膝关节角度，结合遗传算法优化网络结构及步态进展预测算法，实现高精度步态相位估计与关节角度预测，有效提升动力型大腿假肢在复杂地形中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 帮助使用动力型大腿假肢的截肢者更安全、准确地跨越障碍或应对复杂地形，解决当前控制策略在实时性和准确性上的不足。

Method: 利用健全侧脚踝的惯性传感器数据，采用遗传算法优化神经网络结构以预测股骨和膝关节角度，并结合步态进展预测算法确定假肢膝关节电机的驱动角度索引，从而实现对步态阶段和关节角度的实时预测。

Result: 在150 Hz采样频率下，当股骨角度数据添加的高斯噪声标准差小于1时，步态相位估计准确率达100%，股骨角度预测误差为8.71%，膝关节角度预测误差为6.78%，能有效抑制噪声干扰。

Conclusion: 该方法能够准确预测步态进展和关节角度，具有较高的鲁棒性和实用性，为动力型大腿假肢在障碍穿越场景中的智能控制提供了有效解决方案。

Abstract: For amputees with powered transfemoral prosthetics, navigating obstacles or
complex terrain remains challenging. This study addresses this issue by using
an inertial sensor on the sound ankle to guide obstacle-crossing movements. A
genetic algorithm computes the optimal neural network structure to predict the
required angles of the thigh and knee joints. A gait progression prediction
algorithm determines the actuation angle index for the prosthetic knee motor,
ultimately defining the necessary thigh and knee angles and gait progression.
Results show that when the standard deviation of Gaussian noise added to the
thigh angle data is less than 1, the method can effectively eliminate noise
interference, achieving 100\% accuracy in gait phase estimation under 150 Hz,
with thigh angle prediction error being 8.71\% and knee angle prediction error
being 6.78\%. These findings demonstrate the method's ability to accurately
predict gait progression and joint angles, offering significant practical value
for obstacle negotiation in powered transfemoral prosthetics.

</details>


### [188] [Fare: Failure Resilience in Learned Visual Navigation Control](https://arxiv.org/abs/2510.24680)
*Zishuo Wang,Joel Loo,David Hsu*

Main category: cs.RO

TL;DR: 提出Fare框架，通过嵌入OOD检测与识别机制及恢复启发式方法，实现无需显式故障数据的仿效学习策略故障弹性。


<details>
  <summary>Details</summary>
Motivation: 解决模仿学习在分布外场景中易发生不可预测故障的问题，提升策略的鲁棒性和自主恢复能力。

Method: 设计Fare框架，在IL策略中嵌入无需显式故障数据的OOD检测与失败原因识别模块，并结合恢复启发式方法实现自动恢复。

Result: 在真实环境中验证了Fare在两种不同策略架构上的有效性，实现了复杂环境下的长距离稳健导航。

Conclusion: Fare框架能有效构建故障可恢复的IL策略，显著提升在分布外场景中的导航鲁棒性。

Abstract: While imitation learning (IL) enables effective visual navigation, IL
policies are prone to unpredictable failures in out-of-distribution (OOD)
scenarios. We advance the notion of failure-resilient policies, which not only
detect failures but also recover from them automatically. Failure recognition
that identifies the factors causing failure is key to informing recovery: e.g.
pinpointing image regions triggering failure detections can provide cues to
guide recovery. We present Fare, a framework to construct failure-resilient IL
policies, embedding OOD-detection and recognition in them without using
explicit failure data, and pairing them with recovery heuristics. Real-world
experiments show that Fare enables failure recovery across two different policy
architectures, enabling robust long-range navigation in complex environments.

</details>


### [189] [A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers](https://arxiv.org/abs/2510.24683)
*Caleb Escobedo,Nataliya Nechyporenko,Shreyas Kadekodi,Alessandro Roncone*

Main category: cs.RO

TL;DR: 提出一种用于分析物体感知控制器的框架，重点关注运动学、运动轨迹和虚拟约束，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了提升机器人在动态环境中的实时控制安全性，需要系统化分析和改进物体感知控制器的设计。

Method: 构建一个基于运动学、运动轨迹和虚拟约束的分析框架，并通过基础的机器人-障碍物实验场景验证行为表现，比较三种典型物体感知控制器。

Result: 发现现有物体感知控制器常缺乏运动学考虑、控制点连续性和运动稳定性；所提框架能有效支持控制器设计与评估。

Conclusion: 该框架可用于未来避障方法的设计、比较和基准测试。

Abstract: Real-time control is an essential aspect of safe robot operation in the real
world with dynamic objects. We present a framework for the analysis of
object-aware controllers, methods for altering a robot's motion to anticipate
and avoid possible collisions. This framework is focused on three design
considerations: kinematics, motion profiles, and virtual constraints.
Additionally, the analysis in this work relies on verification of robot
behaviors using fundamental robot-obstacle experimental scenarios. To showcase
the effectiveness of our method we compare three representative object-aware
controllers. The comparison uses metrics originating from the design
considerations. From the analysis, we find that the design of object-aware
controllers often lacks kinematic considerations, continuity of control points,
and stability in movement profiles. We conclude that this framework can be used
in the future to design, compare, and benchmark obstacle avoidance methods.

</details>


### [190] [Embodying Physical Computing into Soft Robots](https://arxiv.org/abs/2510.24692)
*Jun Wang,Ziyang Zhou,Ardalan Kahak,Suyi Li*

Main category: cs.RO

TL;DR: 本文提出了一种将物理计算嵌入软体机器人的框架，探讨了模拟振荡器、物理储备池计算和物理算法计算三种策略，使软体机器人无需传统电子元件即可实现复杂行为。


<details>
  <summary>Details</summary>
Motivation: 为了提升软体机器人在日常应用中的鲁棒性和智能性，需要发展无需传统电子器件的新型计算方式。

Method: 提出一个将物理计算嵌入软体机器人的框架，并综述基于模拟振荡器、物理储备池计算和物理算法计算的三种方法。

Result: 实现了包括避障协调运动、负载分类和基于逻辑规则的可编程操作等复杂行为。

Conclusion: 嵌入式物理计算为软体机器人提供了替代传统CMOS电子器件的新途径，具有广阔的发展前景。

Abstract: Softening and onboarding computers and controllers is one of the final
frontiers in soft robotics towards their robustness and intelligence for
everyday use. In this regard, embodying soft and physical computing presents
exciting potential. Physical computing seeks to encode inputs into a mechanical
computing kernel and leverage the internal interactions among this kernel's
constituent elements to compute the output. Moreover, such input-to-output
evolution can be re-programmable. This perspective paper proposes a framework
for embodying physical computing into soft robots and discusses three unique
strategies in the literature: analog oscillators, physical reservoir computing,
and physical algorithmic computing. These embodied computers enable the soft
robot to perform complex behaviors that would otherwise require CMOS-based
electronics -- including coordinated locomotion with obstacle avoidance,
payload weight and orientation classification, and programmable operation based
on logical rules. This paper will detail the working principles of these
embodied physical computing methods, survey the current state-of-the-art, and
present a perspective for future development.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [191] [Logic-based Task Representation and Reward Shaping in Multiagent Reinforcement Learning](https://arxiv.org/abs/2510.23615)
*Nishant Doshi*

Main category: cs.MA

TL;DR: 提出一种基于线性时序逻辑（LTL）任务的多智能体系统加速学习最优策略的方法，通过Buchi自动机与选项机制结合，采用无模型强化学习和新颖的奖励塑形，实现正确性保证的控制器并显著减少收敛时间。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，学习满足复杂任务规范（如LTL）的最优策略通常面临样本复杂度高和收敛慢的问题，尤其是当状态和动作空间较大时。因此，需要一种能加速学习且保证正确性的方法。

Method: 将LTL任务转换为Büchi自动机，结合各智能体的选项（option）机制，构建产品半马尔可夫决策过程（SMDP）；采用无模型的基于值的强化学习算法，在不学习环境转移模型的情况下进行学习，并提出一种新的奖励塑形方法以降低多智能体带来的指数级样本复杂度。

Result: 在确定性网格世界仿真中测试了算法，结果显示所提出的奖励塑形显著减少了收敛时间；同时发现，随着状态和动作空间的增大，使用选项机制对提升学习效率更为重要。

Conclusion: 该方法能够在不依赖环境模型的前提下，高效学习满足LTL任务的最优策略，通过奖励塑形有效缓解多智能体系统中的样本复杂度问题，且选项的使用在大规模问题中更具优势。

Abstract: This paper presents an approach for accelerated learning of optimal plans for
a given task represented using Linear Temporal Logic (LTL) in multi-agent
systems. Given a set of options (temporally abstract actions) available to each
agent, we convert the task specification into the corresponding Buchi Automaton
and proceed with a model-free approach which collects transition samples and
constructs a product Semi Markov Decision Process (SMDP) on-the-fly.
Value-based Reinforcement Learning algorithms can then be used to synthesize a
correct-by-design controller without learning the underlying transition model
of the multi-agent system. The exponential sample complexity due to multiple
agents is dealt with using a novel reward shaping approach. We test the
proposed algorithm in a deterministic gridworld simulation for different tasks
and find that the reward shaping results in significant reduction in
convergence times. We also infer that using options becomes increasing more
relevant as the state and action space increases in multi-agent systems.

</details>


### [192] [Coordinated Autonomous Drones for Human-Centered Fire Evacuation in Partially Observable Urban Environments](https://arxiv.org/abs/2510.23899)
*Maria G. Mendoza,Addison Kalanther,Daniel Bostwick,Emma Stephan,Chinmay Maheshwari,Shankar Sastry*

Main category: cs.MA

TL;DR: 本文提出了一种基于多智能体协调的自主无人机框架，用于在火灾等不确定环境中实时引导人员疏散，通过POMDP建模和PPO算法实现高效搜救。


<details>
  <summary>Details</summary>
Motivation: 现有疏散支持模型常忽视人在极端压力下的心理与行为复杂性，且缺乏动态实时响应能力，导致实际应用受限。

Method: 将问题建模为部分可观测马尔可夫决策过程（POMDP），采用高低层级两个异构无人机协同工作，并结合基于实证心理学的人类行为模型；使用带循环策略的近端策略优化（PPO）算法进行训练。

Result: 仿真结果表明，该无人机系统能快速定位并拦截受困人员，显著缩短其到达安全区域的时间，相比无无人机辅助场景表现更优。

Conclusion: 所提出的多智能体无人机协调框架在应对复杂、动态的疏散场景中具有有效性与实用性，提升了紧急救援中的实时决策与人类引导能力。

Abstract: Autonomous drone technology holds significant promise for enhancing search
and rescue operations during evacuations by guiding humans toward safety and
supporting broader emergency response efforts. However, their application in
dynamic, real-time evacuation support remains limited. Existing models often
overlook the psychological and emotional complexity of human behavior under
extreme stress. In real-world fire scenarios, evacuees frequently deviate from
designated safe routes due to panic and uncertainty. To address these
challenges, this paper presents a multi-agent coordination framework in which
autonomous Unmanned Aerial Vehicles (UAVs) assist human evacuees in real-time
by locating, intercepting, and guiding them to safety under uncertain
conditions. We model the problem as a Partially Observable Markov Decision
Process (POMDP), where two heterogeneous UAV agents, a high-level rescuer (HLR)
and a low-level rescuer (LLR), coordinate through shared observations and
complementary capabilities. Human behavior is captured using an agent-based
model grounded in empirical psychology, where panic dynamically affects
decision-making and movement in response to environmental stimuli. The
environment features stochastic fire spread, unknown evacuee locations, and
limited visibility, requiring UAVs to plan over long horizons to search for
humans and adapt in real-time. Our framework employs the Proximal Policy
Optimization (PPO) algorithm with recurrent policies to enable robust
decision-making in partially observable settings. Simulation results
demonstrate that the UAV team can rapidly locate and intercept evacuees,
significantly reducing the time required for them to reach safety compared to
scenarios without UAV assistance.

</details>


### [193] [Human Machine Social Hybrid Intelligence:A Collaborative Decision Making Framework for Large Model Agent Groups and Human Experts](https://arxiv.org/abs/2510.24030)
*Ahmet Akkaya Melih,Yamuna Singh,Kunal L. Agarwal,Priya Mukherjee,Kiran Pattnaik,Hanuman Bhatia*

Main category: cs.MA

TL;DR: 提出了一种名为“人机社会混合智能”（HMS-HI）的新框架，通过共享认知空间、动态任务分配和跨物种信任校准，显著提升人类与AI在高风险环境中的协同决策效率与信任度。


<details>
  <summary>Details</summary>
Motivation: 现有“人在回路”（HiTL）范式未能有效整合人类专业知识，易导致认知过载和决策瓶颈，尤其在复杂高风险场景中表现不足。

Method: 设计了HMS-HI框架，包含三个核心模块：共享认知空间（SCS）、动态角色与任务分配（DRTA）和跨物种信任校准（CSTC），并在高保真城市应急响应模拟中进行验证。

Result: 相比传统HiTL方法，HMS-HI将平民伤亡减少72%，认知负荷降低70%，并经消融实验证明各模块对系统性能的关键作用。

Conclusion: 共享情境认知与工程化信任机制是实现可扩展、协同式人机智能协作的基础，HMS-HI为未来多智能体协同系统提供了新架构方向。

Abstract: The rapid advancements in large foundation models and multi-agent systems
offer unprecedented capabilities, yet current Human-in-the-Loop (HiTL)
paradigms inadequately integrate human expertise, often leading to cognitive
overload and decision-making bottlenecks in complex, high-stakes environments.
We propose the "Human-Machine Social Hybrid Intelligence" (HMS-HI) framework, a
novel architecture designed for deep, collaborative decision-making between
groups of human experts and LLM-powered AI agents. HMS-HI is built upon three
core pillars: (1) a \textbf{Shared Cognitive Space (SCS)} for unified,
multi-modal situational awareness and structured world modeling; (2) a
\textbf{Dynamic Role and Task Allocation (DRTA)} module that adaptively assigns
tasks to the most suitable agent (human or AI) based on capabilities and
workload; and (3) a \textbf{Cross-Species Trust Calibration (CSTC)} protocol
that fosters transparency, accountability, and mutual adaptation through
explainable declarations and structured feedback. Validated in a high-fidelity
urban emergency response simulation, HMS-HI significantly reduced civilian
casualties by 72\% and cognitive load by 70\% compared to traditional HiTL
approaches, demonstrating superior decision quality, efficiency, and human-AI
trust. An ablation study confirms the critical contribution of each module,
highlighting that engineered trust and shared context are foundational for
scalable, synergistic human-AI collaboration.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [194] [Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents](https://arxiv.org/abs/2510.23691)
*Zihao Wang,Xujing Li,Yining Ye,Junjie Fang,Haoming Wang,Longxiang Liu,Shihao Liang,Junting Lu,Zhiyong Wu,Jiazhan Feng,Wanjun Zhong,Zili Li,Yu Wang,Yu Miao,Bo Zhou,Yuanfan Li,Hao Wang,Zhongkai Zhao,Faming Wu,Zhengxuan Jiang,Weihao Tan,Heyuan Yao,Shi Yan,Xiangyang Li,Yitao Liang,Yujia Qin,Guang Shi*

Main category: cs.AI

TL;DR: Game-TARS 是一种基于统一、可扩展动作空间的通用游戏智能体，使用键盘鼠标输入进行大规模持续预训练，在多种游戏任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 为了实现跨异构领域（如操作系统、网页和模拟游戏）的通用智能体，需要一种可扩展且与人类操作对齐的动作表示方法。

Method: 采用以原生键盘鼠标输入为锚点的统一动作空间，结合超过5000亿token的多样化轨迹和多模态数据进行预训练；引入衰减的持续损失减少因果混淆，并使用高效的Sparse-Thinking策略平衡推理深度与推断成本。

Result: 在开放世界Minecraft任务中成功率是先前最先进模型的约两倍，在未见过的网页3D游戏中接近新手人类水平，并在FPS基准上优于GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet；训练和测试时的扩展性实验验证了该方法的有效性。

Conclusion: 简单且可扩展的动作表示结合大规模预训练，为具备广泛计算机使用能力的通用智能体提供了可行路径。

Abstract: We present Game-TARS, a generalist game agent trained with a unified,
scalable action space anchored to human-aligned native keyboard-mouse inputs.
Unlike API- or GUI-based approaches, this paradigm enables large-scale
continual pre-training across heterogeneous domains, including OS, web, and
simulation games. Game-TARS is pre-trained on over 500B tokens with diverse
trajectories and multimodal data. Key techniques include a decaying continual
loss to reduce causal confusion and an efficient Sparse-Thinking strategy that
balances reasoning depth and inference cost. Experiments show that Game-TARS
achieves about 2 times the success rate over the previous sota model on
open-world Minecraft tasks, is close to the generality of fresh humans in
unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet
in FPS benchmarks. Scaling results on training-time and test-time confirm that
the unified action space sustains improvements when scaled to cross-game and
multimodal data. Our results demonstrate that simple, scalable action
representations combined with large-scale pre-training provide a promising path
toward generalist agents with broad computer-use abilities.

</details>


### [195] [AI and the Decentering of Disciplinary Creativity](https://arxiv.org/abs/2510.23734)
*Eamon Duede*

Main category: cs.AI

TL;DR: 本文探讨了人工智能在科学问题解决中的作用，特别是其对学科创造力的影响。通过数学领域的两个案例，表明计算虽可扩展学科创造力，但某些AI方法可能取代它，从而改变甚至削弱科学探索的价值。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能如何影响科学领域内的创造性思维，特别是在学科特定背景下创造力的表现形式和价值。

Method: 基于创造力哲学的最新研究，区分创造性方法与创造性成果，并引入‘学科创造力’概念，通过数学中的两个案例进行分析。

Result: 发现计算能够增强学科创造力，但某些AI应用可能取代人类的创造性过程，导致科学探索的价值发生变化或降低。

Conclusion: AI在科学问题解决中具有双重作用：既可扩展也可取代学科创造力，需谨慎对待其对科学价值的潜在影响。

Abstract: This paper examines the role of artificial intelligence in scientific
problem-solving, with a focus on its implications for disciplinary creativity.
Drawing on recent work in the philosophy of creativity, I distinguish between
creative approaches and creative products, and introduce the concept of
disciplinary creativity -the creative application of discipline-specific
expertise to a valued problem within that field. Through two cases in
mathematics, I show that while computation can extend disciplinary creativity,
certain approaches involving AI can serve to displace it. This displacement has
the potential to alter (and, perhaps, diminish) the value of scientific
pursuit.

</details>


### [196] [Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability](https://arxiv.org/abs/2510.23744)
*Eline M. Bovy,Caleb Probine,Marnix Suilen,Ufuk Topcu,Nils Jansen*

Main category: cs.AI

TL;DR: 本文提出了多环境POMDP（ME-POMDP）及其扩展对抗信念POMDP（AB-POMDP），旨在寻找在多种可能模型下均能最大化最坏情况奖励的鲁棒策略，并设计了精确与近似算法求解。


<details>
  <summary>Details</summary>
Motivation: 当多个领域专家对问题建模存在分歧时，传统POMDP难以处理模型不确定性，因此需要一种能应对离散模型不确定性的框架以提升策略鲁棒性。

Method: 将ME-POMDP推广到具有初始信念集合的AB-POMDP，证明ME-POMDP可约简为仅在转移与奖励或观测与奖励上变化的形式，并提出精确和基于点的近似算法计算鲁棒策略。

Result: 实现了对标准POMDP基准问题在多环境设置下的扩展，能够有效计算出鲁棒策略。

Conclusion: AB-POMDP框架统一并扩展了ME-POMDP，所提算法能在多环境不确定性下生成最优或近似最优的鲁棒策略。

Abstract: Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete
model uncertainty. ME-POMDPs represent a finite set of POMDPs that share the
same state, action, and observation spaces, but may arbitrarily vary in their
transition, observation, and reward models. Such models arise, for instance,
when multiple domain experts disagree on how to model a problem. The goal is to
find a single policy that is robust against any choice of POMDP within the set,
i.e., a policy that maximizes the worst-case reward across all POMDPs. We
generalize and expand on existing work in the following way. First, we show
that ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which
we call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any
arbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its
transition and reward functions or only in its observation and reward
functions, while preserving (optimal) policies. We then devise exact and
approximate (point-based) algorithms to compute robust policies for AB-POMDPs,
and thus ME-POMDPs. We demonstrate that we can compute policies for standard
POMDP benchmarks extended to the multi-environment setting.

</details>


### [197] [Policy Cards: Machine-Readable Runtime Governance for Autonomous AI Agents](https://arxiv.org/abs/2510.24383)
*Juraj Mavračić*

Main category: cs.AI

TL;DR: Policy Cards 是一种机器可读的部署层标准，用于表达AI代理的操作、监管和伦理约束，支持自动验证、版本控制，并与多种合规框架对接，实现可验证的合规性和分布式保证。


<details>
  <summary>Details</summary>
Motivation: 为了在AI代理的实际部署中有效实施操作、监管和伦理约束，提升透明度与合规性，解决现有透明度文档缺乏规范性规则的问题。

Method: 提出Policy Cards框架，作为Model Card等现有透明度工具的扩展，定义包含允许/禁止规则、义务、证据要求及与NIST AI RMF、ISO/IEC 42001、欧盟AI法案等框架映射的规范层，并支持与运行时执行和持续审计管道集成。

Result: Policy Cards 可被自动验证、版本控制，并链接到运行时 enforcement 或审计流程，为自主代理提供可验证的合规机制，促进多代理系统中的分布式保证。

Conclusion: Policy Cards 构建了连接高层治理与工程实践的桥梁，为大规模实现可问责的自主性提供了可行的技术路径。

Abstract: Policy Cards are introduced as a machine-readable, deployment-layer standard
for expressing operational, regulatory, and ethical constraints for AI agents.
The Policy Card sits with the agent and enables it to follow required
constraints at runtime. It tells the agent what it must and must not do. As
such, it becomes an integral part of the deployed agent. Policy Cards extend
existing transparency artifacts such as Model, Data, and System Cards by
defining a normative layer that encodes allow/deny rules, obligations,
evidentiary requirements, and crosswalk mappings to assurance frameworks
including NIST AI RMF, ISO/IEC 42001, and the EU AI Act. Each Policy Card can
be validated automatically, version-controlled, and linked to runtime
enforcement or continuous-audit pipelines. The framework enables verifiable
compliance for autonomous agents, forming a foundation for distributed
assurance in multi-agent ecosystems. Policy Cards provide a practical mechanism
for integrating high-level governance with hands-on engineering practice and
enabling accountable autonomy at scale.

</details>


### [198] [Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra](https://arxiv.org/abs/2510.23746)
*Laura Mismetti,Marvin Alberts,Andreas Krause,Mara Graziani*

Main category: cs.AI

TL;DR: 提出一种基于测试时调优的预训练Transformer模型框架，直接从串联质谱和分子式实现端到端的从头分子结构生成，无需人工标注和中间步骤，在NPLIB1和MassSpecGym两个基准上分别超越现有最先进方法DiffMS 100%和20%。


<details>
  <summary>Details</summary>
Motivation: 现有串联质谱分析方法依赖数据库匹配或复杂的多步流程，难以识别数据库中未见的新化合物。

Method: 利用测试时调优（test-time tuning）增强预训练Transformer模型，直接从串联质谱和分子式生成分子结构，实现端到端的从头预测。

Result: 在NPLIB1和MassSpecGym基准上分别比DiffMS提升100%和20%，测试时调优相比传统微调在MassSpecGym上性能提升62%；即使预测结果偏离真实结构，生成的分子候选仍具有结构准确性。

Conclusion: 该方法能有效适应新型质谱数据，显著提升未知化合物的识别能力，为人工解析提供可靠指导。

Abstract: Tandem Mass Spectrometry enables the identification of unknown compounds in
crucial fields such as metabolomics, natural product discovery and
environmental analysis. However, current methods rely on database matching from
previously observed molecules, or on multi-step pipelines that require
intermediate fragment or fingerprint prediction. This makes finding the correct
molecule highly challenging, particularly for compounds absent from reference
databases. We introduce a framework that, by leveraging test-time tuning,
enhances the learning of a pre-trained transformer model to address this gap,
enabling end-to-end de novo molecular structure generation directly from the
tandem mass spectra and molecular formulae, bypassing manual annotations and
intermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on
two popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.
Test-time tuning on experimental spectra allows the model to dynamically adapt
to novel spectra, and the relative performance gain over conventional
fine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground
truth, the generated molecular candidates remain structurally accurate,
providing valuable guidance for human interpretation and more reliable
identification.

</details>


### [199] [Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions](https://arxiv.org/abs/2510.23772)
*Vivek Veeriah,Federico Barbero,Marcus Chiam,Xidong Feng,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Johan Obando-Ceron,Jiaxin Shi,Shaobo Hou,Satinder Singh,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 本文介绍了一种生成具有美学吸引力、新颖性和反直觉解法的国际象棋谜题的AI系统，并通过三位国际象棋专家的评估来验证其创造性。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI是否能够产生具有创造性和新颖性的输出，特别是在国际象棋谜题设计这一需要高度创意的领域。

Method: 设计一个AI系统来自动生成国际象棋谜题，并将生成的谜题集呈现给三位世界知名的国际象棋专家进行评估，收集他们对谜题创意性、挑战性和美学设计的反馈。

Result: 三位专家对AI生成的谜题给予了积极评价，认为其中一些谜题具有高度的创造性、挑战性和美学价值。

Conclusion: 该研究表明生成式AI能够在特定领域（如国际象棋谜题）中产生具有人类水平创造力和审美价值的作品。

Abstract: The rapid advancement of Generative AI has raised significant questions
regarding its ability to produce creative and novel outputs. Our recent work
investigates this question within the domain of chess puzzles and presents an
AI system designed to generate puzzles characterized by aesthetic appeal,
novelty, counter-intuitive and unique solutions. We briefly discuss our method
below and refer the reader to the technical paper for more details. To assess
our system's creativity, we presented a curated booklet of AI-generated puzzles
to three world-renowned experts: International Master for chess compositions
Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All
three are noted authors on chess aesthetics and the evolving role of computers
in the game. They were asked to select their favorites and explain what made
them appealing, considering qualities such as their creativity, level of
challenge, or aesthetic design.

</details>


### [200] [Law in Silico: Simulating Legal Society with LLM-Based Agents](https://arxiv.org/abs/2510.24442)
*Yiding Wang,Yuxuan Chen,Fanxu Meng,Xifan Chen,Xiaolei Yang,Muhan Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（LLM）的法律社会仿真框架“Law in Silico”，用于模拟立法、裁决和执法等法律机制，实验表明该框架能有效复现宏观犯罪趋势，并揭示透明、适应性强的法律体系对弱势群体权利保护更优。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界中的法律实验往往成本高昂或不可行，需要一种有效的替代方法来验证和发展法律理论，并支持法律管理。

Method: 构建一个基于大语言模型（LLM）的智能体框架——Law in Silico，模拟个体决策以及立法、裁决和执法制度机制，并通过比较模拟犯罪率与真实数据来评估其效果。

Result: LLM-based agent能够较好地复现宏观层面的犯罪趋势，微观模拟显示运作良好的透明且自适应的法律系统更能保护弱势个体的权利。

Conclusion: LLM为法律社会的仿真提供了有力工具，有助于法律理论的发展和法律制度的设计优化。

Abstract: Since real-world legal experiments are often costly or infeasible, simulating
legal societies with Artificial Intelligence (AI) systems provides an effective
alternative for verifying and developing legal theory, as well as supporting
legal administration. Large Language Models (LLMs), with their world knowledge
and role-playing capabilities, are strong candidates to serve as the foundation
for legal society simulation. However, the application of LLMs to simulate
legal systems remains underexplored. In this work, we introduce Law in Silico,
an LLM-based agent framework for simulating legal scenarios with individual
decision-making and institutional mechanisms of legislation, adjudication, and
enforcement. Our experiments, which compare simulated crime rates with
real-world data, demonstrate that LLM-based agents can largely reproduce
macro-level crime trends and provide insights that align with real-world
observations. At the same time, micro-level simulations reveal that a
well-functioning, transparent, and adaptive legal system offers better
protection of the rights of vulnerable individuals.

</details>


### [201] [Why Foundation Models in Pathology Are Failing](https://arxiv.org/abs/2510.23807)
*Hamid R. Tizhoosh*

Main category: cs.AI

TL;DR: 本文探讨了当前病理学基础模型在应用于人类组织复杂性时存在的根本性缺陷，指出其低诊断准确率、鲁棒性差等问题源于通用人工智能假设与生物组织真实特性之间的概念错配，并提出了七个相互关联的原因，呼吁对该范式进行根本性重构。


<details>
  <summary>Details</summary>
Motivation: 由于基础模型在非医学领域取得巨大成功，人们期望其在计算病理学中也能推动癌症诊断和预后等重大进展，但近期研究暴露了其性能不足，因此有必要探究其背后的根本原因。

Method: 通过系统分析当前病理学基础模型的表现，识别出影响其性能的七个关键因素，并从概念层面剖析通用基础模型假设与组织病理学复杂性之间的不匹配。

Result: 发现了病理学基础模型存在诊断准确性低、鲁棒性差、几何不稳定、计算开销大及安全隐患等问题，其根源在于生物复杂性、无效的自监督、过度泛化、架构过于复杂、缺乏领域创新、数据不足以及组织切片尺寸设计缺陷等七个方面。

Conclusion: 当前的病理学基础模型在概念上与组织形态的真实特性不匹配，必须重新思考其建模范式，发展更符合病理学本质的专用模型。

Abstract: In non-medical domains, foundation models (FMs) have revolutionized computer
vision and language processing through large-scale self-supervised and
multimodal learning. Consequently, their rapid adoption in computational
pathology was expected to deliver comparable breakthroughs in cancer diagnosis,
prognostication, and multimodal retrieval. However, recent systematic
evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor
robustness, geometric instability, heavy computational demands, and concerning
safety vulnerabilities. This short paper examines these shortcomings and argues
that they stem from deeper conceptual mismatches between the assumptions
underlying generic foundation modeling in mainstream AI and the intrinsic
complexity of human tissue. Seven interrelated causes are identified:
biological complexity, ineffective self-supervision, overgeneralization,
excessive architectural complexity, lack of domain-specific innovation,
insufficient data, and a fundamental design flaw related to tissue patch size.
These findings suggest that current pathology foundation models remain
conceptually misaligned with the nature of tissue morphology and call for a
fundamental rethinking of the paradigm itself.

</details>


### [202] [Affordance Representation and Recognition for Autonomous Agents](https://arxiv.org/abs/2510.24459)
*Habtom Kahsay Gidey,Niklas Huber,Alexander Lenz,Alois Knoll*

Main category: cs.AI

TL;DR: 本文提出了一个用于从结构化数据构建世界模型的模式语言，包括DOM转导模式和超媒体功能识别模式，以解决软件代理在处理冗长HTML和动态集成Web服务时的挑战。


<details>
  <summary>Details</summary>
Motivation: 软件代理需要从结构化数据中构建可操作的内部世界模型，但原始HTML的冗长性和静态API集成限制了其自主性和适应性。

Method: 提出两种互补的架构模式：DOM转导模式将冗长的原始DOM压缩为紧凑且与任务相关的表示；超媒体功能识别模式通过解析标准化语义描述，在运行时发现并集成未知Web服务的功能。

Result: 这两种模式共同提供了一个强大的框架，使代理能够高效地构建和维护准确的世界模型，实现跨Web及其扩展资源的可扩展、自适应和互操作的自动化。

Conclusion: 所提出的模式语言有效解决了代理在复杂和动态环境中构建世界模型的关键挑战，提升了自动化系统的灵活性和适应能力。

Abstract: The autonomy of software agents is fundamentally dependent on their ability
to construct an actionable internal world model from the structured data that
defines their digital environment, such as the Document Object Model (DOM) of
web pages and the semantic descriptions of web services. However, constructing
this world model from raw structured data presents two critical challenges: the
verbosity of raw HTML makes it computationally intractable for direct use by
foundation models, while the static nature of hardcoded API integrations
prevents agents from adapting to evolving services.
  This paper introduces a pattern language for world modeling from structured
data, presenting two complementary architectural patterns. The DOM Transduction
Pattern addresses the challenge of web page complexity by distilling} a
verbose, raw DOM into a compact, task-relevant representation or world model
optimized for an agent's reasoning core. Concurrently, the Hypermedia
Affordances Recognition Pattern enables the agent to dynamically enrich its
world model by parsing standardized semantic descriptions to discover and
integrate the capabilities of unknown web services at runtime. Together, these
patterns provide a robust framework for engineering agents that can efficiently
construct and maintain an accurate world model, enabling scalable, adaptive,
and interoperable automation across the web and its extended resources.

</details>


### [203] [ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents](https://arxiv.org/abs/2510.23822)
*Zhenyu Zhang,Tianyi Chen,Weiran Xu,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: 本文提出了ReCAP，一种用于大语言模型的分层推理与规划框架，通过计划预分解、父级计划的结构化重注入和内存高效执行机制，有效解决了长时程任务中的上下文漂移和冗余提示问题，显著提升了多步推理任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理需要多步推理和动态重规划的长时程任务时面临挑战，现有方法存在上下文漂移、目标信息丢失或运行开销大的问题。

Method: 提出ReCAP框架，包含三个关键机制：(i) 预先规划分解，生成完整子任务列表并逐步执行与优化；(ii) 父级计划的结构化重注入，保持递归过程中的多层级上下文一致性；(iii) 内存高效的执行方式，使提示成本随任务深度线性增长。

Result: 实验表明，ReCAP在多个长时程推理基准上显著提升子目标对齐性和成功率，在同步和异步Robotouille任务中分别取得32%和29%的性能提升（严格pass@1协议下）。

Conclusion: ReCAP通过共享上下文的分层设计，有效对齐高层目标与底层动作，减少冗余提示，并在整个递归过程中保持连贯的上下文更新，为大语言模型的复杂任务规划提供了高效解决方案。

Abstract: Long-horizon tasks requiring multi-step reasoning and dynamic re-planning
remain challenging for large language models (LLMs). Sequential prompting
methods are prone to context drift, loss of goal information, and recurrent
failure cycles, while hierarchical prompting methods often weaken cross-level
continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive
Context-Aware Reasoning and Planning), a hierarchical framework with shared
context for reasoning and planning in LLMs. ReCAP combines three key
mechanisms: (i) plan-ahead decomposition, in which the model generates a full
subtask list, executes the first item, and refines the remainder; (ii)
structured re-injection of parent plans, maintaining consistent multi-level
context during recursive return; and (iii) memory-efficient execution, bounding
the active prompt so costs scale linearly with task depth. Together these
mechanisms align high-level goals with low-level actions, reduce redundant
prompting, and preserve coherent context updates across recursion. Experiments
demonstrate that ReCAP substantially improves subgoal alignment and success
rates on various long-horizon reasoning benchmarks, achieving a 32% gain on
synchronous Robotouille and a 29% improvement on asynchronous Robotouille under
the strict pass@1 protocol.

</details>


### [204] [Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models](https://arxiv.org/abs/2510.23824)
*Murad Ismayilov,Edwin Meriaux,Shuo Wen,Gregory Dudek*

Main category: cs.AI

TL;DR: 该论文研究了在去中心化条件下多智能体路径规划中的目标分配问题，通过比较贪婪启发式、最优分配和基于大语言模型（LLM）的智能体，发现合理设计提示和提供定量信息的LLM智能体可实现接近最优的完成时间并优于传统启发式方法。


<details>
  <summary>Details</summary>
Motivation: 在去中心化环境中协调多个自主智能体是一个长期挑战，尤其是在无需协商或迭代协调的情况下实现高效的目标分配。

Method: 智能体基于环境的结构化表示（如网格可视化和场景数据）独立生成对目标的排序偏好，随后交换排序结果，并通过固定的确定性冲突解决规则（如智能体索引顺序）确定分配。

Result: 实验表明，在全观测的网格世界中，使用良好设计提示和定量信息的LLM智能体能实现接近最优的makespan，并持续优于传统启发式方法。

Conclusion: 研究表明，大语言模型在去中心化多智能体路径规划的目标分配中具有潜力，且信息结构的设计对系统性能至关重要。

Abstract: Coordinating multiple autonomous agents in shared environments under
decentralized conditions is a long-standing challenge in robotics and
artificial intelligence. This work addresses the problem of decentralized goal
assignment for multi-agent path planning, where agents independently generate
ranked preferences over goals based on structured representations of the
environment, including grid visualizations and scenario data. After this
reasoning phase, agents exchange their goal rankings, and assignments are
determined by a fixed, deterministic conflict-resolution rule (e.g., agent
index ordering), without negotiation or iterative coordination. We
systematically compare greedy heuristics, optimal assignment, and large
language model (LLM)-based agents in fully observable grid-world settings. Our
results show that LLM-based agents, when provided with well-designed prompts
and relevant quantitative information, can achieve near-optimal makespans and
consistently outperform traditional heuristics. These findings underscore the
potential of language models for decentralized goal assignment in multi-agent
path planning and highlight the importance of information structure in such
systems.

</details>


### [205] [From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production](https://arxiv.org/abs/2510.23856)
*Segev Shlomov,Alon Oved,Sami Marreed,Ido Levy,Offer Akrabi,Avi Yaeli,Łukasz Strąk,Elizabeth Koumpan,Yinon Goldshtein,Eilam Shapira,Nir Mashkif,Asaf Adi*

Main category: cs.AI

TL;DR: 本文介绍了IBM开发的通用代理CUGA，其采用分层规划-执行架构，在AppWorld和WebArena上达到最先进性能，并在企业级业务流程外包人才招聘领域进行了试点，展示了通用代理在企业规模应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管通用代理在学术基准上表现出色，但在企业生产环境中的实际应用仍有限，缺乏标准化评估实践，企业难以将原型转化为可衡量商业价值的部署系统。

Method: CUGA采用分层的规划-执行架构，具备强大的分析基础，并在AppWorld、WebArena以及新提出的BPO-TA基准（包含26项任务）上进行评估，同时在真实企业场景中开展试点。

Result: CUGA在标准基准上达到最先进水平，在BPO-TA试点中接近专用代理的准确率，同时显示出降低开发时间与成本的潜力，满足企业对可扩展性、可审计性、安全性和治理的要求。

Conclusion: 本研究提供了通用代理在企业规模部署的早期证据，总结了技术和组织层面的经验教训，并提出了将研究级架构发展为稳健企业级系统的下一步方向。

Abstract: Agents are rapidly advancing in automating digital work, but enterprises face
a harder challenge: moving beyond prototypes to deployed systems that deliver
measurable business value. This path is complicated by fragmented frameworks,
slow development, and the absence of standardized evaluation practices.
Generalist agents have emerged as a promising direction, excelling on academic
benchmarks and offering flexibility across task types, applications, and
modalities. Yet, evidence of their use in production enterprise settings
remains limited. This paper reports IBM's experience developing and piloting
the Computer Using Generalist Agent (CUGA), which has been open-sourced for the
community (https://github.com/cuga-project/cuga-agent). CUGA adopts a
hierarchical planner--executor architecture with strong analytical foundations,
achieving state-of-the-art performance on AppWorld and WebArena. Beyond
benchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing
talent acquisition domain, addressing enterprise requirements for scalability,
auditability, safety, and governance. To support assessment, we introduce
BPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary
evaluations, CUGA approached the accuracy of specialized agents while
indicating potential for reducing development time and cost. Our contribution
is twofold: presenting early evidence of generalist agents operating at
enterprise scale, and distilling technical and organizational lessons from this
initial pilot. We outline requirements and next steps for advancing
research-grade architectures like CUGA into robust, enterprise-ready systems.

</details>


### [206] [Generating Creative Chess Puzzles](https://arxiv.org/abs/2510.23881)
*Xidong Feng,Vivek Veeriah,Marcus Chiam,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Federico Barbero,Johan Obando-Ceron,Jiaxin Shi,Satinder Singh,Shaobo Hou,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习和新型奖励机制的生成式AI方法，用于生成更具创造性、反直觉性和美学价值的国际象棋谜题，显著提升了生成质量和人类评价。


<details>
  <summary>Details</summary>
Motivation: 生成真正具有创造性、美学和反直觉性的内容仍是生成式AI的挑战，本文旨在通过国际象棋谜题这一领域推动生成质量的提升。

Method: 通过基准测试生成式AI架构，引入基于国际象棋引擎搜索统计信息的新型奖励机制的强化学习框架，以增强谜题的独特性、反直觉性、多样性和真实性。

Result: 该方法将反直觉谜题的生成率从监督学习的0.22%大幅提升至2.5%，超过现有数据集（2.1%）和最佳Lichess训练模型（0.4%）；生成的谜题在新颖性、多样性、美学主题方面表现良好，并被人类专家评为比传统书籍谜题更富创造性和趣味性。

Conclusion: 所提出的强化学习框架有效提升了生成国际象棋谜题的质量，最终产出的AI生成谜题手册获得了三位世界级专家对创造性的认可。

Abstract: While Generative AI rapidly advances in various domains, generating truly
creative, aesthetic, and counter-intuitive outputs remains a challenge. This
paper presents an approach to tackle these difficulties in the domain of chess
puzzles. We start by benchmarking Generative AI architectures, and then
introduce an RL framework with novel rewards based on chess engine search
statistics to overcome some of those shortcomings. The rewards are designed to
enhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.
Our RL approach dramatically increases counter-intuitive puzzle generation by
10x, from 0.22\% (supervised) to 2.5\%, surpassing existing dataset rates
(2.1\%) and the best Lichess-trained model (0.4\%). Our puzzles meet novelty
and diversity benchmarks, retain aesthetic themes, and are rated by human
experts as more creative, enjoyable, and counter-intuitive than composed book
puzzles, even approaching classic compositions. Our final outcome is a curated
booklet of these AI-generated puzzles, which is acknowledged for creativity by
three world-renowned experts.

</details>


### [207] [Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins](https://arxiv.org/abs/2510.23882)
*Adil Rasheed,Oscar Ravik,Omer San*

Main category: cs.AI

TL;DR: 本研究探讨了数字孪生在动态系统建模与控制中的应用，结合物理模型、数据驱动和混合方法，并在微型温室平台上评估了不同建模与控制策略的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在比较不同建模方法（物理、数据驱动、混合）和控制器（传统与AI驱动）在精度、泛化能力与实施成本之间的权衡，推动数字孪生技术在复杂系统中的有效应用。

Method: 开发了线性模型、基于物理的模型（PBM）、长短期记忆网络（LSTM）和混合分析建模（HAM）四种预测模型，并结合模型预测控制（MPC）、强化学习（RL）和大语言模型（LLM）控制三种策略，在插值与外推场景下进行实验对比。

Result: 在建模方面，HAM在准确性、泛化性和计算效率之间表现最均衡，LSTM精度高但资源消耗大；在控制方面，MPC性能稳定可预测，RL适应性强，LLM控制在结合预测工具时展现出良好的人机交互灵活性。

Conclusion: 混合建模方法（如HAM）在综合性能上优于纯数据或物理模型，而多种控制器各有优势，MPC适合高可靠性需求场景，RL适合动态变化环境，LLM则潜力在于人机协同控制。

Abstract: This work investigates the use of digital twins for dynamical system modeling
and control, integrating physics-based, data-driven, and hybrid approaches with
both traditional and AI-driven controllers. Using a miniature greenhouse as a
test platform, four predictive models Linear, Physics-Based Modeling (PBM),
Long Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are
developed and compared under interpolation and extrapolation scenarios. Three
control strategies Model Predictive Control (MPC), Reinforcement Learning (RL),
and Large Language Model (LLM) based control are also implemented to assess
trade-offs in precision, adaptability, and implementation effort. Results show
that in modeling HAM provides the most balanced performance across accuracy,
generalization, and computational efficiency, while LSTM achieves high
precision at greater resource cost. Among controllers, MPC delivers robust and
predictable performance, RL demonstrates strong adaptability, and LLM-based
controllers offer flexible human-AI interaction when coupled with predictive
tools.

</details>


### [208] [Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges](https://arxiv.org/abs/2510.23883)
*Shrestha Datta,Shahriar Kabir Nahin,Anshuman Chhabra,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 本文综述了由大语言模型驱动的智能体AI系统所面临的安全威胁，提出了针对此类系统的威胁分类体系，回顾了现有的评测方法与防御策略，并从技术和治理角度指出了构建安全优先的智能体系统的研究挑战与发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统在规划、工具使用、记忆和自主性方面的发展，其在网页、软件和物理环境中的自主任务执行能力带来了区别于传统AI安全和软件安全的新安全风险，亟需系统性分析与应对。

Method: 通过文献综述的方式，建立面向智能体AI的威胁分类法，总结近期基准测试与评估方法，并从技术与治理双重视角梳理现有防御策略。

Result: 提出了一套针对智能体AI系统的安全威胁分类体系，总结了当前的评估基准与方法，归纳了多维度的防御策略，并识别出若干开放性研究挑战。

Conclusion: 为保障智能体AI系统的安全性，需推动安全设计（secure-by-design）理念，结合技术手段与治理机制共同应对其独特且放大的安全风险。

Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with
planning, tool use, memory, and autonomy, are emerging as powerful, flexible
platforms for automation. Their ability to autonomously execute tasks across
web, software, and physical environments creates new and amplified security
risks, distinct from both traditional AI safety and conventional software
security. This survey outlines a taxonomy of threats specific to agentic AI,
reviews recent benchmarks and evaluation methodologies, and discusses defense
strategies from both technical and governance perspectives. We synthesize
current research and highlight open challenges, aiming to support the
development of secure-by-design agent systems.

</details>


### [209] [Latent Chain-of-Thought for Visual Reasoning](https://arxiv.org/abs/2510.23925)
*Guohao Sun,Hang Hua,Jian Wang,Jiebo Luo,Sohail Dianat,Majid Rabbani,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.AI

TL;DR: 提出一种基于摊销变分推断的可扩展训练算法，通过多样性强化学习和稀疏奖励函数提升大视觉语言模型在推理任务中的泛化性、解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有训练方法如SFT、PPO和GRPO在未见推理任务上泛化能力差，且依赖有偏奖励模型，难以生成多样化且可靠的思维链（CoT）推理过程。

Method: 将LVLM中的推理重新表述为后验推断问题，采用摊销变分推断框架；引入基于多样性强化学习的稀疏奖励函数，提供token级学习信号以鼓励高似然且多样的潜在CoT；设计贝叶斯推断-扩展策略，用边缘似然替代昂贵的Best-of-N和Beam Search来排序最优理由和答案。

Result: 在七个推理基准上验证了该方法的有效性，显著提升了现有最先进LVLM的性能，尤其在效果、泛化能力和解释性方面表现突出。

Conclusion: 所提方法通过变分推理与稀疏、多样化的奖励机制，有效解决了传统训练算法在推理任务中泛化性不足和奖励偏差的问题，为LVLM的可靠推理提供了新方向。

Abstract: Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.

</details>


### [210] [Decentralized Causal Discovery using Judo Calculus](https://arxiv.org/abs/2510.23942)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 提出了一种基于j-稳定因果推断的直觉主义去中心化因果发现框架（judo calculus），利用层论中的j-操作符形式化因果关系的上下文依赖性，并结合传统方法在多个领域实现了更高效和优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中因果效应依赖于不同情境（如年龄、国家、基因型等），传统因果推断难以处理这种上下文依赖性，因此需要一种能形式化局部真实性的新框架。

Method: 提出judo calculus，基于层论中的j-操作符定义局部因果有效性，采用j-稳定性保证跨情境的一致性，并结合得分法、约束法和梯度法实现算法框架。

Result: 在合成数据及生物、经济学的真实数据上验证了该方法的有效性，显示出比经典方法更好的性能和计算效率。

Conclusion: judo calculus为情境依赖的因果发现提供了一个去中心化、构造性强且一致的理论与实践框架，具有广泛的应用潜力。

Abstract: We describe a theory and implementation of an intuitionistic decentralized
framework for causal discovery using judo calculus, which is formally defined
as j-stable causal inference using j-do-calculus in a topos of sheaves. In
real-world applications -- from biology to medicine and social science --
causal effects depend on regime (age, country, dose, genotype, or lab
protocol). Our proposed judo calculus formalizes this context dependence
formally as local truth: a causal claim is proven true on a cover of regimes,
not everywhere at once. The Lawvere-Tierney modal operator j chooses which
regimes are relevant; j-stability means the claim holds constructively and
consistently across that family. We describe an algorithmic and implementation
framework for judo calculus, combining it with standard score-based,
constraint-based, and gradient-based causal discovery methods. We describe
experimental results on a range of domains, from synthetic to real-world
datasets from biology and economics. Our experimental results show the
computational efficiency gained by the decentralized nature of sheaf-theoretic
causal discovery, as well as improved performance over classical causal
discovery methods.

</details>


### [211] [The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity](https://arxiv.org/abs/2510.23965)
*Aymane El Gadarri,Ali Aouad,Vivek F. Farias*

Main category: cs.AI

TL;DR: 提出一种名为“符号估计器”的新方法，通过将交叉熵替换为二元分类损失，实现对大规模语言模型（LLM）偏好的一致且高效的对齐，显著减少偏好偏差，并在模拟中优于传统RLHF和复杂异质性建模方法。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对齐方法在面对人类偏好异质性时表现脆弱，导致群体平均效用估计不一致，影响社会福利衡量的准确性。

Method: 引入符号估计器，在聚合步骤中用二元分类损失替代交叉熵损失，基于成对比较数据进行偏好建模，在 mild 假设下实现一致的序数对齐，并提供有限样本误差界。

Result: 在数字孪生模拟中，相比标准RLHF，符号估计器将角度估计误差降低近35%，与真实群体偏好的分歧从12%降至8%，且性能优于显式建模用户异质性的面板数据启发式方法。

Conclusion: 符号估计器是一种简单、一致且高效的方法，能有效应对人类偏好异质性问题，在保持实现简便的同时显著提升LLM对齐性能。

Abstract: Traditional LLM alignment methods are vulnerable to heterogeneity in human
preferences. Fitting a na\"ive probabilistic model to pairwise comparison data
(say over prompt-completion pairs) yields an inconsistent estimate of the
population-average utility -a canonical measure of social welfare. We propose a
new method, dubbed the sign estimator, that provides a simple, provably
consistent, and efficient estimator by replacing cross-entropy with binary
classification loss in the aggregation step. This simple modification recovers
consistent ordinal alignment under mild assumptions and achieves the first
polynomial finite-sample error bounds in this setting. In realistic simulations
of LLM alignment using digital twins, the sign estimator substantially reduces
preference distortion over a panel of simulated personas, cutting (angular)
estimation error by nearly 35% and decreasing disagreement with true population
preferences from 12% to 8% compared to standard RLHF. Our method also compares
favorably to panel data heuristics that explicitly model user heterogeneity and
require tracking individual-level preference data-all while maintaining the
implementation simplicity of existing LLM alignment pipelines.

</details>


### [212] [Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance](https://arxiv.org/abs/2510.23989)
*Shangde Gao,Zelin Xu,Zhe Jiang*

Main category: cs.AI

TL;DR: 该研究通过将个体的社会基础设施韧性（SIR）纳入条件深度学习模型，利用大规模稀疏个体数据，提升对突发事件后个体移动模式的预测能力。


<details>
  <summary>Details</summary>
Motivation: 预测突发事件后的个体移动模式变化具有挑战性，主要由于缺乏衡量个体SIR的指标、空间背景的复杂交互未被充分捕捉，以及传统方法难以处理稀疏的个体级移动数据。

Method: 提出一种融合个体SIR和局部空间背景的条件深度学习模型，使用大规模个体移动数据进行训练与验证。

Result: 实验表明，结合SIR和空间背景能显著提升模型预测性能，且能识别出具有相似事前模式但SIR不同的个体在事件后移动行为的差异性变化。

Conclusion: 将SIR引入移动预测模型有助于理解个体在扰动事件后的行为响应，为城市规划和应急响应提供更精准的支持。

Abstract: Shifts in individual movement patterns following disruptive events can reveal
changing demands for community resources. However, predicting such shifts
before disruptive events remains challenging for several reasons. First,
measures are lacking for individuals' heterogeneous social infrastructure
resilience (SIR), which directly influences their movement patterns, and
commonly used features are often limited or unavailable at scale, e.g.,
sociodemographic characteristics. Second, the complex interactions between
individual movement patterns and spatial contexts have not been sufficiently
captured. Third, individual-level movement may be spatially sparse and not
well-suited to traditional decision-making methods for movement predictions.
This study incorporates individuals' SIR into a conditioned deep learning model
to capture the complex relationships between individual movement patterns and
local spatial context using large-scale, sparse individual-level data. Our
experiments demonstrate that incorporating individuals' SIR and spatial context
can enhance the model's ability to predict post-event individual movement
patterns. The conditioned model can capture the divergent shifts in movement
patterns among individuals who exhibit similar pre-event patterns but differ in
SIR.

</details>


### [213] [Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling](https://arxiv.org/abs/2510.24013)
*İbrahim Oğuz Çetinkaya,İ. Esra Büyüktahtakın,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 本研究利用大语言模型（LLM）发现了两种新的启发式算法（EDDC和MDDC），用于解决单机总延迟问题（SMTT），在大规模实例上优于传统方法，展示了人与LLM协作在组合优化中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法在求解NP难的组合优化问题时性能有限，且难以扩展。本文探索如何利用大语言模型发现更优、可扩展的启发式规则，以提升调度问题的求解效率与质量。

Method: 基于大语言模型设计了两种新启发式算法：EDD Challenger（EDDC）和MDD Challenger（MDDC），并使用混合整数规划（MIP）模型计算最优性差距和求解时间，与现有启发式和精确方法在不同规模任务实例上进行对比实验。

Result: 在超过100个任务的实例中，精确方法变得不可行；EDDC优于经典EDD和其他常用算法；MDDC始终优于传统启发式，在大规模复杂实例上与精确方法具有竞争力。

Conclusion: 通过合理配置，人与大语言模型的协作能够生成高效、可扩展的启发式算法，有效解决资源受限下的NP难组合优化问题，为调度领域提供了新路径。

Abstract: Our study contributes to the scheduling and combinatorial optimization
literature with new heuristics discovered by leveraging the power of Large
Language Models (LLMs). We focus on the single-machine total tardiness (SMTT)
problem, which aims to minimize total tardiness by sequencing n jobs on a
single processor without preemption, given processing times and due dates. We
develop and benchmark two novel LLM-discovered heuristics, the EDD Challenger
(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date
(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that
employed simpler rule-based heuristics, we evaluate our LLM-discovered
algorithms using rigorous criteria, including optimality gaps and solution time
derived from a mixed-integer programming (MIP) formulation of SMTT. We compare
their performance against state-of-the-art heuristics and exact methods across
various job sizes (20, 100, 200, and 500 jobs). For instances with more than
100 jobs, exact methods such as MIP and dynamic programming become
computationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD
rule and another widely used algorithm in the literature. MDDC consistently
outperforms traditional heuristics and remains competitive with exact
approaches, particularly on larger and more complex instances. This study shows
that human-LLM collaboration can produce scalable, high-performing heuristics
for NP-hard constrained combinatorial optimization, even under limited
resources when effectively configured.

</details>


### [214] [OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting](https://arxiv.org/abs/2510.24028)
*Tingyue Pan,Mingyue Cheng,Shilong Zhang,Zhiding Liu,Xiaoyu Tao,Yucong Luo,Jintao Zhang,Qi Liu*

Main category: cs.AI

TL;DR: OneCast 是一种用于跨域时间序列预测的结构化框架，通过分解季节性和趋势成分来提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理异构时间序列数据时难以有效泛化，尤其面对领域特定的趋势变化和不一致的周期模式。主要问题在于未对时间序列的内在结构进行显式解耦。

Method: 提出 OneCast 框架，将时间序列分解为季节性和趋势成分：季节性部分通过可解释的基函数重建周期模式；趋势部分通过语义感知分词器转换为离散片段，并利用掩码离散扩散机制进行推断。两分支输出融合生成最终预测。

Result: 在八个不同领域的实验中，OneCast 在大多数情况下优于现有的最先进基线模型。

Conclusion: 通过显式解耦时间序列的结构成分并采用模块化生成路径，OneCast 提升了跨域时间序列预测的泛化能力与准确性。

Abstract: Cross-domain time series forecasting is a valuable task in various web
applications. Despite its rapid advancement, achieving effective generalization
across heterogeneous time series data remains a significant challenge. Existing
methods have made progress by extending single-domain models, yet often fall
short when facing domain-specific trend shifts and inconsistent periodic
patterns. We argue that a key limitation lies in treating temporal series as
undifferentiated sequence, without explicitly decoupling their inherent
structural components. To address this, we propose OneCast, a structured and
modular forecasting framework that decomposes time series into seasonal and
trend components, each modeled through tailored generative pathways.
Specifically, the seasonal component is captured by a lightweight projection
module that reconstructs periodic patterns via interpretable basis functions.
In parallel, the trend component is encoded into discrete tokens at segment
level via a semantic-aware tokenizer, and subsequently inferred through a
masked discrete diffusion mechanism. The outputs from both branches are
combined to produce a final forecast that captures seasonal patterns while
tracking domain-specific trends. Extensive experiments across eight domains
demonstrate that OneCast mostly outperforms state-of-the-art baselines.

</details>


### [215] [LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models](https://arxiv.org/abs/2510.24031)
*Peng Cai,Reza Ryan,Nickson M. Karie*

Main category: cs.AI

TL;DR: 本文提出了一种基于聚类的日志分析聊天机器人LLMLogAnalyzer，结合大语言模型和机器学习算法，有效提升日志摘要、模式提取和异常检测的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 日志数据分析对网络安全至关重要，但高昂成本、缺乏专业人才和时间限制使得许多组织难以进行有效分析。现有大语言模型在处理结构化文本和上下文窗口方面存在局限，亟需改进。

Method: 提出LLMLogAnalyzer，采用模块化架构（包括路由器、日志识别器、解析器和搜索工具），结合聚类方法与大语言模型，克服上下文限制和结构化文本处理难题，提升日志分析效率。

Result: 在四个不同领域日志数据上评估显示，相比ChatGPT、ChatPDF和NotebookLM等先进LLM聊天机器人，性能提升39%至68%，且ROUGE-1评分的四分位距减少93%，结果更稳定。

Conclusion: LLMLogAnalyzer通过模块化设计显著增强了大语言模型在结构化日志分析中的准确性与鲁棒性，适用于专业人员和非技术用户，具有广泛应用前景。

Abstract: System logs are a cornerstone of cybersecurity, supporting proactive breach
prevention and post-incident investigations. However, analyzing vast amounts of
diverse log data remains significantly challenging, as high costs, lack of
in-house expertise, and time constraints make even basic analysis difficult for
many organizations. This study introduces LLMLogAnalyzer, a clustering-based
log analysis chatbot that leverages Large Language Models (LLMs) and Machine
Learning (ML) algorithms to simplify and streamline log analysis processes.
This innovative approach addresses key LLM limitations, including context
window constraints and poor structured text handling capabilities, enabling
more effective summarization, pattern extraction, and anomaly detection tasks.
LLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.
Results demonstrate significant performance improvements over state-of-the-art
LLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent
gains ranging from 39% to 68% across different tasks. The system also exhibits
strong robustness, achieving a 93% reduction in interquartile range (IQR) when
using ROUGE-1 scores, indicating significantly lower result variability. The
framework's effectiveness stems from its modular architecture comprising a
router, log recognizer, log parser, and search tools. This design enhances LLM
capabilities for structured text analysis while improving accuracy and
robustness, making it a valuable resource for both cybersecurity experts and
non-technical users.

</details>


### [216] [Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach](https://arxiv.org/abs/2510.24085)
*Md. Shihab Uddin,Md Nazmus Shakib,Rahul Bhadani*

Main category: cs.AI

TL;DR: 本研究比较了经典模型与机器学习模型在电动汽车跟车行为建模中的表现，发现随机森林回归器在真实数据下显著优于传统物理模型。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车的普及，理解其驾驶行为对提升交通安全和开发智能驾驶系统至关重要。

Method: 采用真实世界数据集，对比智能驾驶员模型（IDM）、最优速度模型（OVM）、OVRV和简化CACC等经典模型与随机森林回归器的表现，通过最小化RMSE进行参数标定。

Result: 随机森林模型在中距、长距和超长距跟车场景下的RMSE分别为0.0046、0.0016和0.0025，显著优于最佳物理模型CACC（长距RMSE为2.67）。

Conclusion: 机器学习模型在电动汽车跟车行为预测中表现出更高精度，适用于电动汽车环境下的交通仿真与混合自动驾驶交通动态分析。

Abstract: The increasing adoption of electric vehicles (EVs) necessitates an
understanding of their driving behavior to enhance traffic safety and develop
smart driving systems. This study compares classical and machine learning
models for EV car following behavior. Classical models include the Intelligent
Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative
Velocity (OVRV), and a simplified CACC model, while the machine learning
approach employs a Random Forest Regressor. Using a real world dataset of an EV
following an internal combustion engine (ICE) vehicle under varied driving
conditions, we calibrated classical model parameters by minimizing the RMSE
between predictions and real data. The Random Forest model predicts
acceleration using spacing, speed, and gap type as inputs. Results demonstrate
the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),
0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,
CACC performed best, with an RMSE of 2.67 for long gaps. These findings
highlight the machine learning model's performance across all scenarios. Such
models are valuable for simulating EV behavior and analyzing mixed autonomy
traffic dynamics in EV integrated environments.

</details>


### [217] [HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology](https://arxiv.org/abs/2510.24115)
*Sandeep Vissapragada,Vikrant Sahu,Gagan Raj Gupta,Vandita Singh*

Main category: cs.AI

TL;DR: HistoLens 是一个透明、协作的人工智能系统，使病理学家能用自然语言提问，并获得带有可视化证据的清晰分析报告，增强对AI诊断的信任。


<details>
  <summary>Details</summary>
Motivation: 为了提升医生对人工智能的信任，需要打破AI作为黑箱的局限，实现可解释、可协作的诊断辅助系统。

Method: 开发 HistoLens 系统，将自然语言问题转化为AI可处理的查询，生成结构化报告，并提供基于热图的‘视觉证明’以解释AI决策；同时训练AI忽略背景噪声，专注于组织区域。

Result: 实现了病理学家与AI之间的高效协作，AI提供可解释的分析结果，帮助医生更快、更自信地做出诊断，同时保持医生在诊断中的主导地位。

Conclusion: HistoLens 通过透明化AI推理过程，建立了医生对AI的信任，为临床中可解释人工智能的应用提供了可行方案。

Abstract: For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.

</details>


### [218] [From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems](https://arxiv.org/abs/2510.24145)
*Yu Luo,Jiamin Jiang,Jingfei Feng,Lei Tao,Qingliang Zhang,Xidao Wen,Yongqian Sun,Shenglin Zhang,Jielong Huang,Nan Qi,Dan Pei*

Main category: cs.AI

TL;DR: OpsAgent是一个轻量级、自进化的多智能体系统，用于自动化云系统的事件管理，通过无需训练的数据处理器和多智能体协作框架实现高效、可解释且可持续的故障诊断。


<details>
  <summary>Details</summary>
Motivation: 现有自动化事件管理方法在跨系统泛化、可解释性和部署成本方面存在不足，难以在实际中广泛应用。

Method: 提出OpsAgent，采用无需训练的数据处理器将异构可观测数据转换为结构化文本描述，并设计多智能体协作框架实现透明和可审计的诊断推理；引入双重自进化机制，结合内部模型更新和外部经验积累，实现能力持续增长。

Result: 在OPENRCA基准上的实验表明，OpsAgent在性能上达到先进水平，具备良好的可推广性、可解释性、成本效益和自进化能力。

Conclusion: OpsAgent是一种实用、可部署且可持续的解决方案，适用于真实云环境中的长期运维。

Abstract: Incident management (IM) is central to the reliability of large-scale cloud
systems. Yet manual IM, where on-call engineers examine metrics, logs, and
traces is labor-intensive and error-prone in the face of massive and
heterogeneous observability data. Existing automated IM approaches often
struggle to generalize across systems, provide limited interpretability, and
incur high deployment costs, which hinders adoption in practice. In this paper,
we present OpsAgent, a lightweight, self-evolving multi-agent system for IM
that employs a training-free data processor to convert heterogeneous
observability data into structured textual descriptions, along with a
multi-agent collaboration framework that makes diagnostic inference transparent
and auditable. To support continual capability growth, OpsAgent also introduces
a dual self-evolution mechanism that integrates internal model updates with
external experience accumulation, thereby closing the deployment loop.
Comprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art
performance and show that OpsAgent is generalizable, interpretable,
cost-efficient, and self-evolving, making it a practically deployable and
sustainable solution for long-term operation in real-world cloud systems.

</details>


### [219] [BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data](https://arxiv.org/abs/2510.24151)
*Bingsen Qiu,Zijian Liu,Xiao Liu,Haoshen Yang,Zeren Gao,Bingjie Wang,Feier Zhang,Yixuan Qin,Chunyan Li*

Main category: cs.AI

TL;DR: 提出了一种自动化框架，用于从半结构化知识源生成高难度、适合训练的多跳问答数据，通过NLI关系分类、反向问题构建和多模型验证，实现可扩展且低人工成本的数据生成。


<details>
  <summary>Details</summary>
Motivation: 现有高质量多跳问答数据集稀缺，主要用于评估而非训练，且人工构建复杂多跳问题成本高、难以扩展，导致检索与推理模型的训练数据瓶颈。

Method: 1) 基于自然语言推断（NLI）进行关系分类并多样性扩展，构建多样化的证据簇；2) 采用反向问题构造方法，组合模糊线索使单独线索信息不足但联合后唯一确定答案；3) 通过多模型共识过滤和结构化约束分解进行两步质量控制。

Result: 实现了可扩展的自动化多跳问题生成流程，生成的问题具有高检索难度但仍可验证，适用于监督微调和强化学习训练，同时保持与强评估基准相当的难度水平。

Conclusion: 该框架显著降低了人工标注成本，解决了训练数据匮乏的问题，为训练具备强检索与推理能力的模型提供了高质量、可扩展的数据来源。

Abstract: Building training-ready multi-hop question answering (QA) datasets that truly
stress a model's retrieval and reasoning abilities remains highly challenging
recently. While there have been a few recent evaluation datasets that capture
the characteristics of hard-to-search but easy-to-verify problems -- requiring
the integration of ambiguous, indirect, and cross-domain cues -- these data
resources remain scarce and are mostly designed for evaluation, making them
unsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).
Meanwhile, manually curating non-trivially retrievable questions -- where
answers cannot be found through a single direct query but instead require
multi-hop reasoning over oblique and loosely connected evidence -- incurs
prohibitive human costs and fails to scale, creating a critical data bottleneck
for training high-capability retrieval-and-reasoning agents.
  To address this, we present an automated framework for generating
high-difficulty, training-ready multi-hop questions from semi-structured
knowledge sources. The system (i) grows diverse, logically labeled evidence
clusters through Natural Language Inference (NLI)-based relation typing and
diversity-aware expansion; (ii) applies reverse question construction to
compose oblique cues so that isolated signals are underinformative but their
combination uniquely identifies the target entity; and (iii) enforces quality
with a two-step evaluation pipeline that combines multi-model consensus
filtering with structured constraint decomposition and evidence-based matching.
The result is a scalable process that yields complex, retrieval-resistant yet
verifiable questions suitable for SFT/RL training as well as challenging
evaluation, substantially reducing human curation effort while preserving the
difficulty profile of strong evaluation benchmarks.

</details>


### [220] [BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning](https://arxiv.org/abs/2510.24161)
*Wentao Tan,Bowen Wang,Heng Zhi,Chenyu Liu,Zhe Li,Jian Liu,Zengrong Lin,Yukun Dai,Yipeng Chen,Wenjie Yang,Enci Xie,Hao Xue,Baixu Ji,Chen Xu,Zhibin Wang,Tianshi Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.AI

TL;DR: 本文提出了Boundless Large Model (BLM₁)，一种支持跨空间、跨任务和跨具身泛化的多模态空间基础模型，通过两阶段训练在数字与物理环境中实现统一控制。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs、VLAs和ELLMs在跨数字-物理空间、跨具身及高层推理方面泛化能力不足，缺乏能无缝适应多种环境与任务的统一模型。

Method: 采用两阶段训练：第一阶段通过数字数据注入具身知识并保持语言能力；第二阶段通过意图桥接接口提取MLLM的高层语义指导策略模块训练，不微调MLLM主干。

Result: 在涵盖四个机器人形态和六项任务的跨具身演示集上训练，BLM₁在数字任务上提升约6%，物理任务上提升约3%，优于MLLMs、ELLMs、VLAs和GMLMs四类模型。

Conclusion: BLM₁实现了在数字与物理空间间的无缝操作，具备良好的跨任务、跨具身泛化能力，为通用具身智能提供了有效框架。

Abstract: Multimodal large language models (MLLMs) have advanced vision-language
reasoning and are increasingly deployed in embodied agents. However,
significant limitations remain: MLLMs generalize poorly across digital-physical
spaces and embodiments; vision-language-action models (VLAs) produce low-level
actions yet lack robust high-level embodied reasoning; and most embodied large
language models (ELLMs) are constrained to digital-space with poor
generalization to the physical world. Thus, unified models that operate
seamlessly across digital and physical spaces while generalizing across
embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
instruction following and reasoning, incorporates embodied knowledge, and
supports robust cross-embodiment control. BLM$_1$ integrates three key
capabilities -- \textit{cross-space transfer, cross-task learning, and
cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
injects embodied knowledge into the MLLM through curated digital corpora while
maintaining language competence. Stage II trains a policy module through an
intent-bridging interface that extracts high-level semantics from the MLLM to
guide control, without fine-tuning the MLLM backbone. This process is supported
by a self-collected cross-embodiment demonstration suite spanning four robot
embodiments and six progressively challenging tasks. Evaluations across digital
and physical benchmarks show that a single BLM$_1$ instance outperforms four
model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical
tasks.

</details>


### [221] [UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration](https://arxiv.org/abs/2510.24166)
*Xin Yang,Yuhang Zhang,Wei Li,Xin Lin,Wenbin Zou,Chen Xu*

Main category: cs.AI

TL;DR: 本文提出了UniPlanner，首个用于自动驾驶决策中多数据集融合的规划框架，通过跨数据集学习提升轨迹规划的鲁棒性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法局限于单数据集训练，导致规划鲁棒性不足；不同数据集中轨迹分布和历史-未来关联具有一致性，为多数据集联合训练提供了可能。

Method: 提出UniPlanner，包含三个核心模块：历史-未来轨迹字典网络（HFTDN）聚合多数据集轨迹对并提供规划指导；梯度无关轨迹映射器（GFTM）学习通用规划先验；稀疏到稠密（S2D）范式实现训练时选择性抑制先验、推理时充分利用先验。

Result: 实现了跨数据集的统一学习，增强了历史-未来关联建模的鲁棒性，有效防止了捷径学习，提升了规划的安全性和性能。

Conclusion: UniPlanner首次实现了多数据集集成的自动驾驶规划框架，通过HFTDN、GFTM和S2D协同机制，显著提升了规划系统的泛化能力与实际表现。

Abstract: Motion planning is a critical component of autonomous vehicle decision-making
systems, directly determining trajectory safety and driving efficiency. While
deep learning approaches have advanced planning capabilities, existing methods
remain confined to single-dataset training, limiting their robustness in
planning.
  Through systematic analysis, we discover that vehicular trajectory
distributions and history-future correlations demonstrate remarkable
consistency across different datasets. Based on these findings, we propose
UniPlanner, the first planning framework designed for multi-dataset integration
in autonomous vehicle decision-making. UniPlanner achieves unified
cross-dataset learning through three synergistic innovations.
  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates
history-future trajectory pairs from multiple datasets, using historical
trajectory similarity to retrieve relevant futures and generate cross-dataset
planning guidance.
  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust
history-future correlations from multiple datasets, transforming historical
trajectories into universal planning priors. Its gradient-free design ensures
the introduction of valuable priors while preventing shortcut learning, making
the planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)
paradigm implements adaptive dropout to selectively suppress planning priors
during training for robust learning, while enabling full prior utilization
during inference to maximize planning performance.

</details>


### [222] [MGA: Memory-Driven GUI Agent for Observation-Centric Interaction](https://arxiv.org/abs/2510.24168)
*Weihua Cheng,Ersheng Ni,Wenlong Wang,Yifei Sun,Junming Liu,Wangyu Shen,Yirong Chen,Botian Shi,Ding Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于记忆驱动的GUI智能体MGA，采用“先观察，后决策”的范式，通过当前截图、任务无关的空间信息和动态更新的结构化记忆三元组来建模每一步环境状态，有效提升了在复杂界面中的鲁棒性、泛化性和执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体方法依赖历史轨迹导致误差累积，并因“先决策后观察”的机制产生局部探索偏差，难以在复杂多变的桌面和网页界面中实现稳健和泛化的交互。

Method: MGA将每个步骤建模为独立且上下文丰富的环境状态，使用三元组（当前截图、任务无关的空间信息、动态更新的结构化记忆）进行表示，摒弃对历史轨迹的依赖，强调先充分观察再决策。

Result: 在OSWorld基准、真实桌面应用（Chrome、VSCode、VLC）及跨任务迁移实验中，MGA在成功率、鲁棒性和泛化能力上显著优于现有最先进方法。

Conclusion: MGA通过引入记忆驱动的‘先观察后决策’框架，有效缓解了误差传播和局部探索偏差问题，为GUI智能体提供了更高效、通用的交互范式。

Abstract: The rapid progress of Large Language Models (LLMs) and their multimodal
extensions (MLLMs) has enabled agentic systems capable of perceiving and acting
across diverse environments. A challenging yet impactful frontier is the
development of GUI agents, which must navigate complex desktop and web
interfaces while maintaining robustness and generalization. Existing paradigms
typically model tasks as long-chain executions, concatenating historical
trajectories into the context. While approaches such as Mirage and GTA1 refine
planning or introduce multi-branch action selection, they remain constrained by
two persistent issues: Dependence on historical trajectories, which amplifies
error propagation. And Local exploration bias, where "decision-first,
observation-later" mechanisms overlook critical interface cues. We introduce
the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the
principle of observe first, then decide. MGA models each step as an
independent, context-rich environment state represented by a triad: current
screenshot, task-agnostic spatial information, and a dynamically updated
structured memory. Experiments on OSworld benchmarks, real desktop applications
(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves
substantial gains in robustness, generalization, and efficiency compared to
state-of-the-art baselines. The code is publicly available at:
{https://anonymous.4open.science/r/MGA-3571}.

</details>


### [223] [MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools](https://arxiv.org/abs/2510.24284)
*Wenhao Wang,Peizhi Niu,Zhao Xu,Zhaoyu Chen,Jian Du,Yaxin Du,Xianghe Pang,Keduan Huang,Yanfeng Wang,Qiang Yan,Siheng Chen*

Main category: cs.AI

TL;DR: 本文提出了MCP-Flow，一个自动化网络代理驱动的管道，用于大规模发现服务器、合成数据和模型训练，显著提升了大型语言模型在真实世界Model Contextual Protocol（MCP）环境中的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有的MCP研究覆盖服务器少、依赖高成本的手动整理，并缺乏训练支持，限制了其在现实场景中的应用。因此，需要一种可扩展、自动化的解决方案来推动LLM代理在复杂任务中有效利用MCP生态系统。

Method: 提出MCP-Flow，通过自动化web代理进行大规模服务器发现，从1166个服务器和11536个工具中收集数据，生成68733条高质量指令-函数调用对和6439条轨迹，构建用于训练的数据集。

Result: 实验证明MCP-Flow在MCP工具选择、函数调用生成以及代理任务性能方面均优于先前方法，显著提升了LLM在MCP环境下的表现。

Conclusion: MCP-Flow为提升大型语言模型在现实世界MCP环境中的工具使用能力提供了可扩展的基础，推动了LLM代理的发展，并已公开发布以促进后续研究。

Abstract: Large Language Models (LLMs) increasingly rely on external tools to perform
complex, realistic tasks, yet their ability to utilize the rapidly expanding
Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP
research covers few servers, depends on costly manual curation, and lacks
training support, hindering progress toward real-world deployment. To overcome
these limitations, we introduce MCP-Flow, an automated web-agent-driven
pipeline for large-scale server discovery, data synthesis, and model training.
MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing
68733 high-quality instruction-function call pairs and 6439 trajectories, far
exceeding prior work in scale and diversity. Extensive experiments demonstrate
MCP-Flow's effectiveness in driving superior MCP tool selection, function-call
generation, and enhanced agentic task performance. MCP-Flow thus provides a
scalable foundation for advancing LLM agents' proficiency in real-world MCP
environments. MCP-Flow is publicly available at
\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.

</details>


### [224] [Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms](https://arxiv.org/abs/2510.24297)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文提出并评估了多种改进的MCTS抽象策略中的组内选择策略，以替代随机平局打破规则，实验表明其中多个策略在多数环境下优于随机策略。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛树搜索（MCTS）的样本效率较低，使用状态或动作抽象可提升其性能，但现有方法未充分考虑同一抽象节点中多个动作共享相同UCB值的问题，导致需依赖随机平局打破规则，影响决策质量。

Method: 提出多种替代随机选择的组内抽象策略（intra-abstraction policies），并在不同环境和参数设置下进行实证评估，与现有方法如剪枝的On the Go抽象（pruned OGA）结合使用。

Result: 多个提出的组内策略在大多数环境和参数配置下表现优于随机平局打破策略，提升了MCTS在使用抽象时的性能和稳定性。

Conclusion: 通过设计更合理的组内抽象策略，可以有效改善基于抽象的MCTS算法性能，避免随机平局打破带来的不确定性，为MCTS中的抽象机制提供了优化方向。

Abstract: One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which
can be addressed by building and using state and/or action abstractions in
parallel to the tree search such that information can be shared among nodes of
the same layer. The primary usage of abstractions for MCTS is to enhance the
Upper Confidence Bound (UCB) value during the tree policy by aggregating visits
and returns of an abstract node. However, this direct usage of abstractions
does not take the case into account where multiple actions with the same parent
might be in the same abstract node, as these would then all have the same UCB
value, thus requiring a tiebreak rule. In state-of-the-art abstraction
algorithms such as pruned On the Go Abstractions (pruned OGA), this case has
not been noticed, and a random tiebreak rule was implicitly chosen. In this
paper, we propose and empirically evaluate several alternative
intra-abstraction policies, several of which outperform the random policy
across a majority of environments and parameter settings.

</details>


### [225] [Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank](https://arxiv.org/abs/2510.24299)
*Jiayu Liu,Wei Dai,Zhenya Huang,Ning Miao,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出了一种名为Self-Indicator的简单且即插即用的方法，利用大语言模型（LLM）内部行为中的相关性矩阵秩来评估其推理路径的正确性，无需依赖外部资源如训练验证器或复杂提示，显著降低了计算开销，并在多个LLM上实现了超过75%的正确推理路径识别准确率，使三项推理基准测试的准确率提升了8%以上。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽然具备强大的推理能力，但容易产生错误和幻觉，现有输出验证方法依赖外部资源，导致高计算开销且适用范围有限。因此，探索是否可利用LLM内部行为自身来判断其推理可信度成为一个关键问题。

Method: 研究发现输入问题与输出推理路径之间的相关性矩阵的秩是推理正确性的稳健指标。基于此，提出Self-Indicator方法，通过该矩阵秩对候选推理路径进行重加权，实现自我验证，仅依赖LLM自身信息，不需额外模型或复杂提示设计。

Result: Self-Indicator在多个不同规模和模型家族的LLM上验证有效，推理路径正确性识别准确率超75%，并在三个推理基准上平均提升准确率8%以上，相比其他投票和验证方法性能更优且计算开销极低。

Conclusion: LLM内部行为中的相关性矩阵秩可作为有效的自我验证指标，Self-Indicator提供了一种通用、高效且低开销的推理路径评估方法，具有广泛的应用潜力。

Abstract: Despite the strong reasoning ability of large language models~(LLMs), they
are prone to errors and hallucinations. As a result, how to check their outputs
effectively and efficiently has become a critical problem in their
applications. Existing checking methods heavily rely on external resources,
such as trained verifiers (e.g., process/outcome reward models) or elaborate
prompts, which lead to high computational overhead and are only applicable to
specific domains. In this paper, we investigate whether the internal behaviors
of LLMs have already implied the credibility of their reasoning paths.
Specifically, we find that the rank of the correlation matrix between the input
problem and the output reasoning path is a robust indicator of reasoning
correctness. Different from other correctness indicators for LLMs, the
calculation of the correlation matrix only relies on the LLM itself, which
avoids the hassle of training a separate model or designing complicated
prompts. Based on it, we design a simple, plug-and-play Self-Indicator method
to reweight candidate reasoning paths, which achieves significant performance
improvements than other voting and verification methods with very few
computational overhead. Our experiments across multiple LLMs of varying scales
and model families have further shown the effectiveness of Self-Indicator. It
achieves over 75% accuracy in distinguishing correct reasoning paths from
incorrect ones, and, in turn, improves the accuracies on three reasoning
benchmarks by more than 8%.

</details>


### [226] [Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting](https://arxiv.org/abs/2510.24303)
*Deniz Gorur,Antoni Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体框架的判断性预测方法，利用大型语言模型生成支持和反对主张的证据，并通过量化双极论证框架（QBAF）进行主张验证，实验表明多智能体结合能提升预测准确性并提供可解释的证据融合。


<details>
  <summary>Details</summary>
Motivation: 判断性预测依赖人类主观判断，存在不一致性和缺乏可解释性，因此需要一种能够整合不同观点并提供透明推理过程的自动化主张验证框架。

Method: 提出一个多智能体框架，使用大型语言模型（LLM）实现三类智能体：ArgLLM、RbAM和RAG-ArgLLM，各自从不同方式生成QBAF用于主张验证，并在两个标准数据集上评估不同LLM和智能体组合的效果。

Result: 实验结果显示，结合多个智能体（尤其是三个智能体）的证据可以提高判断性预测的准确性，同时提供可解释的论证支持。

Conclusion: 该多智能体框架有效提升了主张验证的准确性和可解释性，适用于判断性预测任务，且通过不同LLM和智能体配置展示了良好的泛化能力。

Abstract: Judgmental forecasting is the task of making predictions about future events
based on human judgment. This task can be seen as a form of claim verification,
where the claim corresponds to a future event and the task is to assess the
plausibility of that event. In this paper, we propose a novel multi-agent
framework for claim verification, whereby different agents may disagree on
claim veracity and bring specific evidence for and against the claims,
represented as quantitative bipolar argumentation frameworks (QBAFs). We then
instantiate the framework for supporting claim verification, with a variety of
agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an
existing approach for claim verification that generates and evaluates QBAFs;
(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)
from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,
extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of
arguments from external sources. Finally, we conduct experiments with two
standard judgmental forecasting datasets, with instances of our framework with
two or three agents, empowered by six different base LLMs. We observe that
combining evidence from agents can improve forecasting accuracy, especially in
the case of three agents, while providing an explainable combination of
evidence for claim verification.

</details>


### [227] [Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research](https://arxiv.org/abs/2510.24337)
*Daria Kravets-Meinke,Hannah Schmid-Petri,Sonja Niemann,Ute Schmid*

Main category: cs.AI

TL;DR: 本文探讨了生成式大语言模型（gLLM）在传播学定量内容分析中的应用潜力，并提出了应对七项关键挑战的最佳实践指南，以提升研究的效度、信度、可重复性和伦理标准。


<details>
  <summary>Details</summary>
Motivation: 尽管gLLMs在内容分析中展现出优越性能和成本效率，其在传播学研究方法论中的整合仍不充分，亟需系统指导以确保研究质量。

Method: 综合现有关于gLLM辅助定量内容分析的研究，识别出影响结果质量的七个关键挑战：代码本开发、提示工程、模型选择、参数调优、迭代优化、模型可靠性验证和性能增强，并提出相应的最佳实践建议。

Result: 提出了一套全面的gLLM辅助内容分析最佳实践框架，帮助研究人员有效应对各项技术挑战，提升自动化内容分析的质量与规范性。

Conclusion: 该指南有助于将gLLM技术更广泛地引入传播学研究，同时确保符合学科内的质量与伦理标准，推动方法论的范式转变。

Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly
being used in communication research for content analysis. Studies show that
gLLMs can outperform both crowd workers and trained coders, such as research
assistants, on various coding tasks relevant to communication science, often at
a fraction of the time and cost. Additionally, gLLMs can decode implicit
meanings and contextual information, be instructed using natural language,
deployed with only basic programming skills, and require little to no annotated
data beyond a validation dataset - constituting a paradigm shift in automated
content analysis. Despite their potential, the integration of gLLMs into the
methodological toolkit of communication research remains underdeveloped. In
gLLM-assisted quantitative content analysis, researchers must address at least
seven critical challenges that impact result quality: (1) codebook development,
(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)
iterative refinement, (6) validation of the model's reliability, and
optionally, (7) performance enhancement. This paper synthesizes emerging
research on gLLM-assisted quantitative content analysis and proposes a
comprehensive best-practice guide to navigate these challenges. Our goal is to
make gLLM-based content analysis more accessible to a broader range of
communication researchers and ensure adherence to established disciplinary
quality standards of validity, reliability, reproducibility, and research
ethics.

</details>


### [228] [VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation](https://arxiv.org/abs/2510.24339)
*Yunxuan Jiang,Silan Hu,Xiaoning Wang,Yuanyuan Zhang,Xiangyu Chang*

Main category: cs.AI

TL;DR: 本文提出了VDSAgents，一个基于可预测性-可计算性-稳定性（PCS）原则的多智能体系统，用于提升大语言模型驱动的数据科学自动化系统的可信度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的数据科学系统缺乏科学理论指导，依赖模型内部推理，难以应对复杂真实数据，限制了其可靠性与可审计性。

Method: 基于Veridical Data Science框架中的PCS原则，设计了一个模块化多智能体系统，涵盖数据清洗、特征工程、建模和评估，并在各阶段引入扰动分析、单元测试和模型验证。

Result: 在九个不同特性的数据集上评估，使用DeepSeek-V3和GPT-4o作为后端，VDSAgents consistently优于AutoKaggle和DataInterpreter等先进系统。

Conclusion: 将PCS原则嵌入LLM驱动的数据科学自动化是可行且有效的，能显著提升系统的科学性、稳健性和可审计性。

Abstract: Large language models (LLMs) become increasingly integrated into data science
workflows for automated system design. However, these LLM-driven data science
systems rely solely on the internal reasoning of LLMs, lacking guidance from
scientific and theoretical principles. This limits their trustworthiness and
robustness, especially when dealing with noisy and complex real-world datasets.
This paper provides VDSAgents, a multi-agent system grounded in the
Predictability-Computability-Stability (PCS) principles proposed in the
Veridical Data Science (VDS) framework. Guided by PCS principles, the system
implements a modular workflow for data cleaning, feature engineering, modeling,
and evaluation. Each phase is handled by an elegant agent, incorporating
perturbation analysis, unit testing, and model validation to ensure both
functionality and scientific auditability. We evaluate VDSAgents on nine
datasets with diverse characteristics, comparing it with state-of-the-art
end-to-end data science systems, such as AutoKaggle and DataInterpreter, using
DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the
results of AutoKaggle and DataInterpreter, which validates the feasibility of
embedding PCS principles into LLM-driven data science automation.

</details>


### [229] [A Unified Geometric Space Bridging AI Models and the Human Brain](https://arxiv.org/abs/2510.24342)
*Silin Chen,Yuzhong Chen,Zifan Wang,Junhao Wang,Zifeng Jia,Keith M Kendrick,Tuo Zhang,Lin Zhao,Dezhong Yao,Tianming Liu,Xi Jiang*

Main category: cs.AI

TL;DR: 本文提出了“类脑空间”这一新概念，通过将AI模型的内在空间注意力拓扑结构映射到人类大脑功能网络，实现了跨模态、跨任务的AI与大脑组织方式的统一比较。分析151个Transformer模型发现，类脑程度呈现连续弧形几何分布，受预训练范式和位置编码方式影响，且类脑程度与下游任务性能并不完全相关。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在语言、感知和推理上已接近人类水平，但其信息组织方式是否与大脑相似仍不清楚。现有脑-AI对齐研究受限于特定输入和任务，缺乏跨模态的统一比较框架。

Method: 提出“类脑空间”概念，将不同模态的AI模型（视觉、语言、多模态）的内在空间注意力拓扑结构映射到人类功能性脑网络，构建一个统一的几何空间进行比较。分析了151个基于Transformer的模型。

Result: 发现类脑空间呈现连续弧形几何结构，反映类脑程度的渐进变化；模型分布模式与其模态、是否强调全局语义抽象以及位置编码是否支持深度跨模态融合有关；类脑程度与下游任务性能不完全一致。

Conclusion: 类脑空间为跨领域智能体的组织结构提供了首个统一的量化与比较框架，揭示了连接机器与大脑的深层组织原则。

Abstract: For decades, neuroscientists and computer scientists have pursued a shared
ambition: to understand intelligence and build it. Modern artificial neural
networks now rival humans in language, perception, and reasoning, yet it is
still largely unknown whether these artificial systems organize information as
the brain does. Existing brain-AI alignment studies have shown the striking
correspondence between the two systems, but such comparisons remain bound to
specific inputs and tasks, offering no common ground for comparing how AI
models with different kinds of modalities-vision, language, or multimodal-are
intrinsically organized. Here we introduce a groundbreaking concept of
Brain-like Space: a unified geometric space in which every AI model can be
precisely situated and compared by mapping its intrinsic spatial attention
topological organization onto canonical human functional brain networks,
regardless of input modality, task, or sensory domain. Our extensive analysis
of 151 Transformer-based models spanning state-of-the-art large vision models,
large language models, and large multimodal models uncovers a continuous
arc-shaped geometry within this space, reflecting a gradual increase of
brain-likeness; different models exhibit distinct distribution patterns within
this geometry associated with different degrees of brain-likeness, shaped not
merely by their modality but by whether the pretraining paradigm emphasizes
global semantic abstraction and whether the positional encoding scheme
facilitates deep fusion across different modalities. Moreover, the degree of
brain-likeness for a model and its downstream task performance are not
"identical twins". The Brain-like Space provides the first unified framework
for situating, quantifying, and comparing intelligence across domains,
revealing the deep organizational principles that bridge machines and the
brain.

</details>


### [230] [An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine](https://arxiv.org/abs/2510.24359)
*Pedram Fard,Alaleh Azhir,Neguine Rezaii,Jiazi Tian,Hossein Estiri*

Main category: cs.AI

TL;DR: 提出一种基于多智能体生态系统的N-of-1医疗决策支持框架，通过器官系统、人群和分析模式划分智能体，提升个体化医疗的准确性与公平性。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI系统过于依赖平均患者模型，导致在罕见变异、多重疾病或代表性不足的人群中表现不佳，损害了医疗公平与信任。

Method: 构建一个由多个智能体组成的生态系统，这些智能体按器官系统、患者群体和分析方式进行聚类，并共享模型库和证据合成工具；通过协调层整合结果，提供包含风险估计、异常标记和证据链接的决策支持包。

Result: 系统将验证重点从总体准确率转向个体可靠性，关注低数据密度区域的误差、小样本校准性和风险-覆盖权衡，提升对边缘病例的支持能力。

Conclusion: 通过从单一模型向协同智能转变，该方法致力于实现以个体为中心、透明且公平的医学AI，契合医学的第一原则。

Abstract: Artificial intelligence in medicine is built to serve the average patient. By
minimizing error across large datasets, most systems deliver strong aggregate
accuracy yet falter at the margins: patients with rare variants,
multimorbidity, or underrepresented demographics. This average patient fallacy
erodes both equity and trust. We propose a different design: a multi-agent
ecosystem for N-of-1 decision support. In this environment, agents clustered by
organ systems, patient populations, and analytic modalities draw on a shared
library of models and evidence synthesis tools. Their results converge in a
coordination layer that weighs reliability, uncertainty, and data density
before presenting the clinician with a decision-support packet: risk estimates
bounded by confidence ranges, outlier flags, and linked evidence. Validation
shifts from population averages to individual reliability, measured by error in
low-density regions, calibration in the small, and risk--coverage trade-offs.
Anticipated challenges include computational demands, automation bias, and
regulatory fit, addressed through caching strategies, consensus checks, and
adaptive trial frameworks. By moving from monolithic models to orchestrated
intelligence, this approach seeks to align medical AI with the first principle
of medicine: care that is transparent, equitable, and centered on the
individual.

</details>


### [231] [Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion](https://arxiv.org/abs/2510.24390)
*Xianjun Gao,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.AI

TL;DR: Orion 是一种新颖高效的推理框架，通过依赖感知的查询分解和逻辑并行内容扩展，提升大语言模型在实时Web应用中的推理效率与质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型推理方法在效率与质量之间难以兼顾，无法满足现代Web服务对低延迟、高吞吐和高质量推理的双重需求。

Method: Orion 将查询推理过程分解为两个阶段：关键点生成（通过检索增强的少样本提示提取结构化关键点）和内容并行扩展（基于依赖图并发展开各关键点）。同时引入流水线调度机制，利用两阶段不同的计算特性实现跨查询并行。

Result: 实验表明，Orion 相比基线方法最高可提升4.33倍的token生成速度、降低3.42倍的回答延迟，并通过显式建模关键点间依赖关系使推理质量最高提升18.75%。

Conclusion: Orion 成功解决了大语言模型在实时Web应用中效率与质量难以兼顾的问题，显著提升了推理性能，适用于高要求的交互式AI服务。

Abstract: The integration of Large Language Models (LLMs) into real-time Web
applications, such as AI-powered search and conversational agents, presents a
fundamental Web infrastructure challenge: reconciling the demand for
high-quality, complex reasoning with the stringent low-latency and
high-throughput requirements of interactive services. Current LLM reasoning,
hindered by computationally inefficient sequential generation and rigid
reasoning strategies, creates a critical bottleneck for the Web services.
Existing approaches typically optimize the LLM reasoning for either efficiency
or quality but struggle to achieve both, and thus fail to meet the dual
requirements of modern Web platforms. To overcome these limitations, we propose
Orion, a novel and efficient reasoning framework that enables dependency-aware
query decomposition and logic-parallel content expansion. Concretely, Orion
decomposes a single query reasoning process into two synergistic phases: (1)
\textit{key point generation}, which distills logically structured key points
through retrieval-augmented few-shot prompting, and (2) \textit{content
parallel expansion}, which concurrently elaborates on these points based on a
dependency graph to ensure logical consistency. Furthermore, Orion introduces a
pipeline scheduling mechanism that exploits the complementary computational
characteristics of the two phases (generation imposes pressure on GPU computing
and expansion stresses on GPU memory) across multiple queries, enabling
cross-query parallelism and dramatically improving reasoning performance (\ie,
efficiency and quality). Experiments on diverse benchmarks show that Orion not
only delivers up to 4.33x higher token generation speed and 3.42x lower answer
latency over the baselines but also improves reasoning quality by up to 18.75%
through explicitly modeling inter-point dependencies.

</details>


### [232] [APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training](https://arxiv.org/abs/2510.24397)
*Jiarui Qin,Yunjia Xi,Junjie Huang,Renting Rui,Di Yin,Weiwen Liu,Yong Yu,Weinan Zhang,Xing Sun*

Main category: cs.AI

TL;DR: 提出了APTBench，一个针对LLM预训练阶段的轻量级基准，用于评估模型的代理能力，涵盖规划与行动等核心能力，并覆盖软件工程和深度研究等关键场景。


<details>
  <summary>Details</summary>
Motivation: 现有预训练基准主要关注静态技能，无法反映模型的代理能力；而代理基准通常面向后训练模型，不适合基础模型。因此需要一个能在预训练阶段评估代理潜力的基准。

Method: 将真实世界中的代理任务和成功轨迹转化为适合基础模型的多选题或文本补全题，聚焦规划、行动等核心代理能力。

Result: APTBench能更准确预测模型作为代理在下游任务中的表现，同时比后训练的端到端代理评估更轻量且成本更低。

Conclusion: APTBench为LLM预训练阶段提供了有效的代理能力评估方案，有助于指导模型朝更强的自主任务执行能力发展。

Abstract: With the rapid development of LLM-based agents, there is a growing trend to
incorporate agent-specific data into the pre-training stage of LLMs, aiming to
better align LLMs with real-world autonomous task execution. However, current
pre-training benchmarks primarily focus on isolated and static skills, e.g.,
common knowledge or mathematical/code reasoning, and fail to reflect model's
agentic capabilities. On the other hand, agent benchmarks are typically
designed for post-trained models, requiring multi-turn task execution abilities
that base models struggle to support. Thus, there is a compelling need for a
benchmark that can evaluate agentic potentials during pre-training and guide
the model training more effectively. To address this gap, we propose APTBench,
a framework that converts real-world agent tasks and successful trajectories
into multiple-choice or text completion questions tailored for base models. It
focuses on core agentic abilities, e.g., planning and action, and covers key
agent scenarios, software engineering and deep research. Compared to existing
general-purpose benchmarks, APTBench offers a more predictive signal of a
model's downstream performance as an agent, while remaining significantly more
lightweight and cost-effective than full-scale, end-to-end agent evaluations
after post-training.

</details>


### [233] [OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows](https://arxiv.org/abs/2510.24411)
*Qiushi Sun,Mukai Li,Zhoumianze Liu,Zhihui Xie,Fangzhi Xu,Zhangyue Yin,Kanzhi Cheng,Zehao Li,Zichen Ding,Qi Liu,Zhiyong Wu,Zhuosheng Zhang,Ben Kao,Lingpeng Kong*

Main category: cs.AI

TL;DR: 提出了一种名为OS-Sentinel的混合安全检测框架，结合形式化验证和基于视觉语言模型的上下文判断，在动态沙箱环境MobileRisk-Live上实现了比现有方法高10%-30%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型驱动的计算机代理在移动环境中展现出类人操作能力，但其潜在的不安全操作（如系统入侵和隐私泄露）引发关注，而当前缺乏有效的安全检测机制。

Method: 构建了包含真实轨迹和细粒度标注的动态沙箱环境MobileRisk-Live，并提出OS-Sentinel框架，结合形式化验证器检测系统级违规和基于VLM的上下文判断器评估情境风险。

Result: 实验显示OS-Sentinel在多个指标上比现有方法提升10%-30%，并提供了关于移动代理安全性的关键分析见解。

Conclusion: 该研究为移动代理安全奠定了基础，所提出的混合框架有效提升了安全检测能力，有助于开发更安全可靠的自主移动代理。

Abstract: Computer-using agents powered by Vision-Language Models (VLMs) have
demonstrated human-like capabilities in operating digital environments like
mobile platforms. While these agents hold great promise for advancing digital
automation, their potential for unsafe operations, such as system compromise
and privacy leakage, is raising significant concerns. Detecting these safety
concerns across the vast and complex operational space of mobile environments
presents a formidable challenge that remains critically underexplored. To
establish a foundation for mobile agent safety research, we introduce
MobileRisk-Live, a dynamic sandbox environment accompanied by a safety
detection benchmark comprising realistic trajectories with fine-grained
annotations. Built upon this, we propose OS-Sentinel, a novel hybrid safety
detection framework that synergistically combines a Formal Verifier for
detecting explicit system-level violations with a VLM-based Contextual Judge
for assessing contextual risks and agent actions. Experiments show that
OS-Sentinel achieves 10%-30% improvements over existing approaches across
multiple metrics. Further analysis provides critical insights that foster the
development of safer and more reliable autonomous mobile agents.

</details>


### [234] [Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning](https://arxiv.org/abs/2510.24435)
*Benjamin Grando Moreira*

Main category: cs.AI

TL;DR: 该研究评估了多个大语言模型（LLM）在逻辑和抽象推理任务上的表现，并与人类表现进行对比，揭示了LLM在演绎推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型的推理能力对于推动人工智能发展至关重要，这超越了简单的语言任务表现，涉及模型是否真正理解信息、进行推断并以合乎逻辑的方式得出结论。

Method: 使用八个自定义设计的推理问题，对包括GPT、Claude、DeepSeek、Gemini等在内的多个LLM进行测试，并将其表现与人类在相同任务上的表现进行基准比较。

Result: 实验结果显示，LLM与人类在逻辑推理任务上存在显著差异，表明当前LLM在演绎推理方面仍面临挑战。

Conclusion: 尽管LLM在语言任务中表现出色，但在逻辑和抽象推理方面仍不及人类，需进一步改进其推理能力。

Abstract: Evaluating reasoning ability in Large Language Models (LLMs) is important for
advancing artificial intelligence, as it transcends mere linguistic task
performance. It involves understanding whether these models truly understand
information, perform inferences, and are able to draw conclusions in a logical
and valid way. This study compare logical and abstract reasoning skills of
several LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,
Perplexity, and Sabi\'a - using a set of eight custom-designed reasoning
questions. The LLM results are benchmarked against human performance on the
same tasks, revealing significant differences and indicating areas where LLMs
struggle with deduction.

</details>


### [235] [Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks](https://arxiv.org/abs/2510.24461)
*Korneel Van den Berghe,Stein Stroobants,Vijay Janapa Reddi,G. C. H. E. de Croon*

Main category: cs.AI

TL;DR: 本文提出了一种针对脉冲神经网络（SNN）在强化学习中训练难题的新型训练方法，通过分析代理梯度斜率设置并引入特权引导策略，在真实无人机控制任务中显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决SNN在复杂控制任务中的两个关键挑战：脉冲神经元不可微导致的梯度优化问题，以及状态动态特性在强化学习中因短序列训练难以跨越预热期的问题。

Method: 系统分析代理梯度斜率的影响，并提出一种结合特权引导策略的训练方法，利用引导策略启动学习过程，同时结合自适应斜率调度进行在线环境交互训练。

Result: 在真实无人机位置控制任务中，所提方法平均回报达到400分，显著优于行为克隆和TD3BC等基线方法（最多-200分）。

Conclusion: 该工作推进了对SNN中代理梯度学习的理论理解，并为实际机器人系统中的神经形态控制器提供了有效的训练方法。

Abstract: Neuromorphic computing systems are set to revolutionize energy-constrained
robotics by achieving orders-of-magnitude efficiency gains, while enabling
native temporal processing. Spiking Neural Networks (SNNs) represent a
promising algorithmic approach for these systems, yet their application to
complex control tasks faces two critical challenges: (1) the non-differentiable
nature of spiking neurons necessitates surrogate gradients with unclear
optimization properties, and (2) the stateful dynamics of SNNs require training
on sequences, which in reinforcement learning (RL) is hindered by limited
sequence lengths during early training, preventing the network from bridging
its warm-up period.
  We address these challenges by systematically analyzing surrogate gradient
slope settings, showing that shallower slopes increase gradient magnitude in
deeper layers but reduce alignment with true gradients. In supervised learning,
we find no clear preference for fixed or scheduled slopes. The effect is much
more pronounced in RL settings, where shallower slopes or scheduled slopes lead
to a 2.1x improvement in both training and final deployed performance. Next, we
propose a novel training approach that leverages a privileged guiding policy to
bootstrap the learning process, while still exploiting online environment
interactions with the spiking policy. Combining our method with an adaptive
slope schedule for a real-world drone position control task, we achieve an
average return of 400 points, substantially outperforming prior techniques,
including Behavioral Cloning and TD3BC, which achieve at most --200 points
under the same conditions. This work advances both the theoretical
understanding of surrogate gradient learning in SNNs and practical training
methodologies for neuromorphic controllers demonstrated in real-world robotic
systems.

</details>


### [236] [From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning](https://arxiv.org/abs/2510.24528)
*Zihan Chen,Song Wang,Xingbo Fu,Chengshuai Shi,Zhenyu Lei,Cong Shen,Jundong Li*

Main category: cs.AI

TL;DR: 提出了一种成本高效的两阶段流程，利用跨任务示例和图基标签传播方法，减少大语言模型在数据标注中的依赖，实验证明该方法在降低标注成本的同时保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 减少在新或复杂任务中收集高质量标注数据的成本和人力投入，降低对大语言模型（LLM）进行数据标注的依赖。

Method: 首先利用现成的跨任务示例通过LLM对少量目标任务实例进行伪标注；然后采用基于图的标签传播方法，将标签信息扩展到其余目标样本，无需额外查询LLM；最后使用完全伪标注的数据集构建任务内示例用于上下文学习（ICL）。

Result: 在五个任务上的实验表明，该方法在显著降低标注成本的同时，取得了具有竞争力的性能表现。

Conclusion: 所提出的两阶段伪标注与标签传播 pipeline 能有效结合跨任务监督的灵活性和无LLM传播的可扩展性，为低成本上下文学习提供了可行方案。

Abstract: The capability of in-context learning (ICL) enables large language models
(LLMs) to perform novel tasks without parameter updates by conditioning on a
few input-output examples. However, collecting high-quality examples for new or
challenging tasks can be costly and labor-intensive. In this work, we propose a
cost-efficient two-stage pipeline that reduces reliance on LLMs for data
labeling. Our approach first leverages readily available cross-task examples to
prompt an LLM and pseudo-label a small set of target task instances. We then
introduce a graph-based label propagation method that spreads label information
to the remaining target examples without additional LLM queries. The resulting
fully pseudo-labeled dataset is used to construct in-task demonstrations for
ICL. This pipeline combines the flexibility of cross-task supervision with the
scalability of LLM-free propagation. Experiments across five tasks demonstrate
that our method achieves strong performance while lowering labeling costs.

</details>


### [237] [Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives](https://arxiv.org/abs/2510.24551)
*Gang Chen,Changshuo Liu,Gene Anne Ooi,Marcus Tan,Zhongle Xie,Jianwei Yin,James Wei Luen Yip,Wenqiao Zhang,Jiaqi Zhu,Beng Chin Ooi*

Main category: cs.AI

TL;DR: 本文提出了一种以数据为中心的范式，用于设计和部署医疗领域的生成式人工智能（GenAI）系统，强调将医疗数据生态系统作为生成式医疗系统的基础，以支持多模态数据整合与知识检索，从而提升医疗服务质量。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗领域具有巨大潜力，但其有效部署需要深入理解医疗任务及数据的复杂性，因此需要一个可持续、高效的数据生态系统来支撑。

Method: 重新定位数据生命周期，构建以医疗数据生态系统为核心的架构，集成语义向量搜索和上下文查询等高效数据处理管道，支持上游模型训练和下游临床应用。

Result: 该生态系统可为基座模型提供高质量多模态数据用于预训练和微调，同时作为知识检索后端支持任务特定推理，增强GenAI在医疗中的实用性与可靠性。

Conclusion: 以数据为中心的范式能够有效支撑生成式AI在医疗领域的部署，提升临床决策质量和医疗服务效率。

Abstract: Generative Artificial Intelligence (GenAI) is taking the world by storm. It
promises transformative opportunities for advancing and disrupting existing
practices, including healthcare. From large language models (LLMs) for clinical
note synthesis and conversational assistance to multimodal systems that
integrate medical imaging, electronic health records, and genomic data for
decision support, GenAI is transforming the practice of medicine and the
delivery of healthcare, such as diagnosis and personalized treatments, with
great potential in reducing the cognitive burden on clinicians, thereby
improving overall healthcare delivery. However, GenAI deployment in healthcare
requires an in-depth understanding of healthcare tasks and what can and cannot
be achieved. In this paper, we propose a data-centric paradigm in the design
and deployment of GenAI systems for healthcare. Specifically, we reposition the
data life cycle by making the medical data ecosystem as the foundational
substrate for generative healthcare systems. This ecosystem is designed to
sustainably support the integration, representation, and retrieval of diverse
medical data and knowledge. With effective and efficient data processing
pipelines, such as semantic vector search and contextual querying, it enables
GenAI-powered operations for upstream model components and downstream clinical
applications. Ultimately, it not only supplies foundation models with
high-quality, multimodal data for large-scale pretraining and domain-specific
fine-tuning, but also serves as a knowledge retrieval backend to support
task-specific inference via the agentic layer. The ecosystem enables the
deployment of GenAI for high-quality and effective healthcare delivery.

</details>


### [238] [FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling](https://arxiv.org/abs/2510.24645)
*Zengzhuang Xu,Bingguang Hao,Zechuan Wang,Yuntao Wen,Maolin Wang,Yang Liu,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Chenyi Zhuang,Jinjie Gu,Leilei Gan,Xiangyu Zhao,Shi Gu*

Main category: cs.AI

TL;DR: FunReason-MT 是一种用于真实场景中多轮工具调用的数据合成框架，通过环境-API图交互、高级工具查询合成和引导迭代链，显著提升大模型在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法难以生成高质量的多轮函数调用数据，无法满足实际应用中对目标训练、工具架构隔离和多轮逻辑依赖的需求。

Method: 提出 FunReason-MT 框架，包含三项关键技术：1）环境-API图交互以收集多样化高质量轨迹；2）高级工具-查询合成以简化复杂查询构建；3）引导迭代链以生成复杂的思维链（CoT）。

Result: 在 BFCLv3 基准上，基于 FunReason-MT 数据训练的 4B 模型达到同规模模型中的最先进性能，超越多数闭源模型；在 BFCLv4 上进一步验证了其性能提升的有效性。

Conclusion: FunReason-MT 能够有效解决现实世界中多轮函数调用数据生成的挑战，为智能体学习提供可靠且鲁棒的数据来源。

Abstract: Function calling (FC) empowers large language models (LLMs) and autonomous
agents to interface with external tools, a critical capability for solving
complex, real-world problems. As this ability becomes increasingly central to
advanced AI systems, the need for high-quality, multi-turn training data to
develop and refine it cannot be overstated. Existing data synthesis methods,
such as random environment sampling or multi-agent role-playing, are not
powerful enough to generate high-quality data in real-world environments.
Practical challenges come in three folds: targeted model training, isolation of
tool architecture, and multi-turn logical dependency. To address these
structural deficiencies, we present FunReason-MT, a novel data synthesis
framework for real-world multi-turn tool use. FunReason-MT resolves the
complexity barrier in multi-turn FC data by employing 1) Environment-API Graph
Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query
Synthesis to simplify hard query construction, and 3) Guided Iterative Chain
for sophisticated CoT generation. Evaluations on Berkeley Function-Calling
Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built
upon FunReason-MT generated data achieves state-of-the-art performance among
comparable-sized models, outperforming most close-source models. Further
performance improvements on BFCLv4 confirm that FunReason-MT provides a
reliable and robust source for agentic learning.

</details>


### [239] [Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning](https://arxiv.org/abs/2510.24650)
*Nitin Rai,Daeun,Choi,Nathan S. Boyd,Arnold W. Schumann*

Main category: cs.AI

TL;DR: 本文综述了基础模型（FMs）在作物病害定点管理（SSDM）中的应用进展，强调视觉-语言模型（VLMs）和大语言模型（LLMs）在自适应学习、强化学习与数字孪生中的潜力，指出多模态FMs结合实时反馈将是下一代SSDM的关键方向。


<details>
  <summary>Details</summary>
Motivation: 提升作物病害管理的智能化与精准化水平，利用FMs整合图文信息、实现症状理解与管理决策支持，推动农业人工智能从实验室向田间应用转化。

Method: 筛选约40篇关于FMs在SSDM中应用的文献，重点分析LLMs和VLMs在自适应学习、强化学习及数字孪生框架下的作用，并探讨其在智能喷洒与人机协作中的应用前景。

Result: 发现：(a) FMs相关研究在2023-24年快速增长；(b) VLMs发展速度远超LLMs（5-10倍）；(c) 强化学习与自适应学习在智能喷洒中仍处于初期阶段；(d) 结合RL的数字孪生可模拟靶向喷洒；(e) 仿真到现实的迁移是部署关键；(f) 人机协同有限，尤其缺乏人在回路的验证机制；(g) 多模态FMs与实时反馈将驱动下一代SSDM。

Conclusion: 多模态基础模型为SSDM带来新机遇，未来需加强仿真-现实衔接、人机协同设计及开放数据共享以推动实际应用。

Abstract: Site-specific disease management (SSDM) in crops has advanced rapidly through
machine and deep learning (ML and DL) for real-time computer vision. Research
evolved from handcrafted feature extraction to large-scale automated feature
learning. With foundation models (FMs), crop disease datasets are now processed
in fundamentally new ways. Unlike traditional neural networks, FMs integrate
visual and textual data, interpret symptoms in text, reason about
symptom-management relationships, and support interactive QA for growers and
educators. Adaptive and imitation learning in robotics further enables
field-based disease management. This review screened approx. 40 articles on FM
applications for SSDM, focusing on large-language models (LLMs) and
vision-language models (VLMs), and discussing their role in adaptive learning
(AL), reinforcement learning (RL), and digital twin frameworks for targeted
spraying. Key findings: (a) FMs are gaining traction with surging literature in
2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL
and AL are still nascent for smart spraying; (d) digital twins with RL can
simulate targeted spraying virtually; (e) addressing the sim-to-real gap is
critical for real-world deployment; (f) human-robot collaboration remains
limited, especially in human-in-the-loop approaches where robots detect early
symptoms and humans validate uncertain cases; (g) multi-modal FMs with
real-time feedback will drive next-gen SSDM. For updates, resources, and
contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to
submit papers, code, or datasets.

</details>


### [240] [OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs](https://arxiv.org/abs/2510.24663)
*Yifu Lu,Shengjie Liu,Li Dong*

Main category: cs.AI

TL;DR: 本文提出了OrchDAG，一个用于生成具有可控复杂度的多轮工具交互合成数据的管道，并引入基于图结构的奖励机制来增强强化学习训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多忽视了多轮工具交互的复杂性，缺乏对代理在复杂任务中工具调用能力的有效评估和训练方法。

Method: 提出OrchDAG数据生成管道，将工具执行建模为有向无环图（DAG），并设计基于图结构的奖励信号，结合GRPO风格算法进行训练。

Result: 实验表明该数据集构成具有挑战性但可解的基准，所提出的图奖励机制能有效提升模型在多轮工具使用中的表现。

Conclusion: 利用拓扑结构和数据复杂性对于提升多轮工具调用的代理性能至关重要。

Abstract: Agentic tool use has gained traction with the rise of agentic tool calling,
yet most existing work overlooks the complexity of multi-turn tool
interactions. We introduce OrchDAG, a synthetic data generation pipeline that
models tool execution as directed acyclic graphs (DAGs) with controllable
complexity. Using this dataset, we benchmark model performance and propose a
graph-based reward to enhance RLVR training. Experiments show that the dataset
presents a challenging but solvable benchmark, and the proposed reward is
effective when combined with GRPO-style algorithms, highlighting the importance
of leveraging topological structure and data complexity in multi-turn tool use.

</details>


### [241] [Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning](https://arxiv.org/abs/2510.24690)
*Shengjie Liu,Li Dong,Zhenyu Zhang*

Main category: cs.AI

TL;DR: 提出了一种通过构建和融合工具知识图谱与文档知识图谱来增强示例工件生成的框架，实验表明该方法能有效建模工具交互并提升计划生成效果。


<details>
  <summary>Details</summary>
Motivation: 为了提升工具增强推理与规划中的工具交互建模能力，解决现有方法中工具与领域知识分离的问题。

Method: 首先从工具schema构建工具知识图谱，同时从内部文档和SOP中提取知识图谱，然后将两者融合；采用深度稀疏集成策略对齐工具结构依赖与过程知识以生成示例计划。

Result: 实验证明该统一框架能有效建模工具间交互，显著提升计划生成质量，特别是在复杂任务场景下表现优于基线方法。

Conclusion: 将工具知识图谱与领域知识图谱结合可有效支持工具依赖挖掘与推理规划，为自动化计划生成提供了新的解决方案。

Abstract: We present a framework for uncovering and exploiting dependencies among tools
and documents to enhance exemplar artifact generation. Our method begins by
constructing a tool knowledge graph from tool schemas,including descriptions,
arguments, and output payloads, using a DeepResearch-inspired analysis. In
parallel, we derive a complementary knowledge graph from internal documents and
SOPs, which is then fused with the tool graph. To generate exemplar plans, we
adopt a deep-sparse integration strategy that aligns structural tool
dependencies with procedural knowledge. Experiments demonstrate that this
unified framework effectively models tool interactions and improves plan
generation, underscoring the benefits of linking tool graphs with domain
knowledge graphs for tool-augmented reasoning and planning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [242] [An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.23617)
*Phuong Q. Dao,Mark Roantree,Vuong M. Ngo*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的多模态情感分析模型BERT-ViT-EF，并进一步扩展为DTCN，通过早期融合和对比学习提升文本与图像表征的对齐，显著提高了在TumEmo和MVSA-Single数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服单模态情感分析的局限性，需要一种能够深度融合文本和图像信息的方法，以实现更准确的情感理解。

Method: 提出BERT-ViT-EF模型，结合BERT和ViT分别处理文本和图像，并采用早期融合策略；进一步设计DTCN，在文本端增加Transformer层并引入对比学习，以增强跨模态对齐和上下文建模。

Result: 在TumEmo数据集上，DTCN达到78.4%的准确率和78.3%的F1分数，在MVSA-Single上也取得76.6%准确率和75.9% F1的竞争力表现。

Conclusion: 早期融合与深层上下文建模能有效提升Transformer架构下的多模态情感分析性能，DTCN在多个基准上验证了其有效性。

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by
jointly analyzing data from multiple modalities typically text and images
offering a richer and more accurate interpretation than unimodal approaches. In
this paper, we first propose BERT-ViT-EF, a novel model that combines powerful
Transformer-based encoders BERT for textual input and ViT for visual input
through an early fusion strategy. This approach facilitates deeper cross-modal
interactions and more effective joint representation learning. To further
enhance the model's capability, we propose an extension called the Dual
Transformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN
incorporates an additional Transformer encoder layer after BERT to refine
textual context (before fusion) and employs contrastive learning to align text
and image representations, fostering robust multimodal feature learning.
Empirical results on two widely used MSA benchmarks MVSA-Single and TumEmo
demonstrate the effectiveness of our approach. DTCN achieves best accuracy
(78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on
MVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements
highlight the benefits of early fusion and deeper contextual modeling in
Transformer-based multimodal sentiment analysis.

</details>


### [243] [Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields](https://arxiv.org/abs/2510.23621)
*Alexandre Benoit*

Main category: cs.LG

TL;DR: 本论文研究了在保持物理保真度的同时，通过降低精度和优化GPU内核来加速SO(3)-等变机器学习力场MACE的方法。


<details>
  <summary>Details</summary>
Motivation: 为了降低MACE模型的高计算成本，探索低精度算术和GPU优化内核是否能在不损害准确性的情况下提升效率。

Method: 对MACE进行端到端及模块级性能分析，比较e3nn与cuEquivariance后端，并评估FP64/FP32/BF16/FP16在推理、短NVT和长NPT水模拟以及训练中的表现。

Result: cuEquivariance使推理延迟降低约3倍；在线性层使用BF16/FP16（FP32累积）带来额外约4倍加速，且MD能量和热力学量保持稳定；训练中使用半精度会恶化力的RMSE；混合e3nn与cuEq模块会导致表示不匹配。

Conclusion: 推荐默认使用cuEquivariance + FP32，线性层启用BF16/FP16（保持FP32累积）以最大化吞吐量，训练仍用FP32；未来在Ampere/Hopper架构上有望进一步加速。

Abstract: Machine-learning force fields can deliver accurate molecular dynamics (MD) at
high computational cost. For SO(3)-equivariant models such as MACE, there is
little systematic evidence on whether reduced-precision arithmetic and
GPU-optimized kernels can cut this cost without harming physical fidelity. This
thesis aims to make MACE cheaper and faster while preserving accuracy by
identifying computational bottlenecks and evaluating low-precision execution
policies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA
cuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32
accumulation) for inference, short NVT and long NPT water simulations, and toy
training runs under reproducible, steady-state timing. cuEquivariance reduces
inference latency by about $3\times$. Casting only linear layers to BF16/FP16
within an FP32 model yields roughly 4x additional speedups, while energies and
thermodynamic observables in NVT/NPT MD remain within run-to-run variability.
Half-precision weights during training degrade force RMSE. Mixing e3nn and cuEq
modules without explicit adapters causes representation mismatches. Fused
equivariant kernels and mixed-precision inference can substantially accelerate
state-of-the-art force fields with negligible impact on downstream MD. A
practical policy is to use cuEquivariance with FP32 by default and enable
BF16/FP16 for linear layers (keeping FP32 accumulations) for maximum
throughput, while training remains in FP32. Further gains are expected on
Ampere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and
pipeline fusion.

</details>


### [244] [Adversarially-Aware Architecture Design for Robust Medical AI Systems](https://arxiv.org/abs/2510.23622)
*Alyssa Gerhart,Balaji Iyangar*

Main category: cs.LG

TL;DR: 该研究通过实证实验揭示了医疗AI系统在皮肤病学数据集上易受对抗性攻击的脆弱性，评估了防御方法的效果，并呼吁采取技术、伦理与政策相结合的综合措施以构建更安全、公平的医疗AI系统。


<details>
  <summary>Details</summary>
Motivation: 对抗性攻击可能对医疗AI系统造成严重威胁，导致误诊或治疗延误，尤其影响弱势群体，因此亟需评估其风险并开发有效防御机制。

Method: 通过威胁建模、实验基准测试和模型评估，在皮肤病学数据集上进行对抗性攻击实验，并测试对抗训练和蒸馏等防御方法的有效性。

Result: 对抗性攻击显著降低了分类准确性；防御方法虽能部分降低攻击成功率，但需权衡其对正常数据性能的影响。

Conclusion: 应结合技术、伦理与政策手段，共同提升医疗AI系统的鲁棒性和公平性，以应对对抗性攻击带来的安全挑战。

Abstract: Adversarial attacks pose a severe risk to AI systems used in healthcare,
capable of misleading models into dangerous misclassifications that can delay
treatments or cause misdiagnoses. These attacks, often imperceptible to human
perception, threaten patient safety, particularly in underserved populations.
Our study explores these vulnerabilities through empirical experimentation on a
dermatological dataset, where adversarial methods significantly reduce
classification accuracy. Through detailed threat modeling, experimental
benchmarking, and model evaluation, we demonstrate both the severity of the
threat and the partial success of defenses like adversarial training and
distillation. Our results show that while defenses reduce attack success rates,
they must be balanced against model performance on clean data. We conclude with
a call for integrated technical, ethical, and policy-based approaches to build
more resilient, equitable AI in healthcare.

</details>


### [245] [DiNo and RanBu: Lightweight Predictions from Shallow Random Forests](https://arxiv.org/abs/2510.23624)
*Tiago Mendonça dos Santos,Rafael Izbicki,Luís Gustavo Esteves*

Main category: cs.LG

TL;DR: 本文提出了两种浅层森林方法DiNo和RanBu，用于提升随机森林在表格数据预测中的效率，在保持甚至超越准确率的同时显著降低训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 传统的随机森林虽然性能强大，但由于依赖大量深度树，导致推理延迟高、内存需求大，难以部署在资源受限或对延迟敏感的环境中。因此需要更高效的替代方案。

Method: 提出DiNo（基于最近公共祖先的共表型距离）和RanBu（对Breiman的邻近度量应用核平滑）两种方法，均在森林训练后进行转换，无需生成新树，仅通过轻量级矩阵运算调整带宽参数h。

Result: 在3个合成基准和25个公开数据集上，RanBu在高噪声环境下准确率优于或等于全深度随机森林，且训练加推理时间最多减少95%；DiNo在低噪声下具有最佳偏差-方差权衡。两种方法均可直接扩展到分位数回归，并保持精度与速度优势。

Conclusion: DiNo和RanBu为随机森林提供了高效、可扩展的改进方案，在不牺牲准确率的前提下大幅降低计算开销，适用于资源受限场景。

Abstract: Random Forest ensembles are a strong baseline for tabular prediction tasks,
but their reliance on hundreds of deep trees often results in high inference
latency and memory demands, limiting deployment in latency-sensitive or
resource-constrained environments. We introduce DiNo (Distance with Nodes) and
RanBu (Random Bushes), two shallow-forest methods that convert a small set of
depth-limited trees into efficient, distance-weighted predictors. DiNo measures
cophenetic distances via the most recent common ancestor of observation pairs,
while RanBu applies kernel smoothing to Breiman's classical proximity measure.
Both approaches operate entirely after forest training: no additional trees are
grown, and tuning of the single bandwidth parameter $h$ requires only
lightweight matrix-vector operations. Across three synthetic benchmarks and 25
public datasets, RanBu matches or exceeds the accuracy of full-depth random
forests-particularly in high-noise settings-while reducing training plus
inference time by up to 95\%. DiNo achieves the best bias-variance trade-off in
low-noise regimes at a modest computational cost. Both methods extend directly
to quantile regression, maintaining accuracy with substantial speed gains. The
implementation is available as an open-source R/C++ package at
https://github.com/tiagomendonca/dirf. We focus on structured tabular random
samples (i.i.d.), leaving extensions to other modalities for future work.

</details>


### [246] [From Detection to Discovery: A Closed-Loop Approach for Simultaneous and Continuous Medical Knowledge Expansion and Depression Detection on Social Media](https://arxiv.org/abs/2510.23626)
*Shuang Geng,Wenli Zhang,Jiaheng Xie,Rui Wang,Sudha Ram*

Main category: cs.LG

TL;DR: 提出一种闭环大语言模型-知识图谱框架，通过迭代学习循环实现抑郁症检测与医学知识扩展的协同进化。


<details>
  <summary>Details</summary>
Motivation: 现有研究利用医学知识提升抑郁预测准确率，但忽视了在预测过程中同步扩展知识的机会。

Method: 构建一个包含知识感知的抑郁检测和知识精炼扩展两个阶段的闭环框架：大语言模型同时进行抑郁检测与实体抽取，知识图谱表征并加权实体以优化预测；新提取的实体、关系和类型在专家监督下融入知识图谱，实现知识持续演化。

Result: 基于大规模用户生成内容，该框架提升了预测准确性，并被专家验证发现了文献中未涵盖的临床有意义的症状、共病及社会诱因。

Conclusion: 实现了预测与学习的相互强化，推动了预测分析的方法论与理论发展，展示了计算模型与领域知识的协同进化潜力。

Abstract: Social media user-generated content (UGC) provides real-time, self-reported
indicators of mental health conditions such as depression, offering a valuable
source for predictive analytics. While prior studies integrate medical
knowledge to improve prediction accuracy, they overlook the opportunity to
simultaneously expand such knowledge through predictive processes. We develop a
Closed-Loop Large Language Model (LLM)-Knowledge Graph framework that
integrates prediction and knowledge expansion in an iterative learning cycle.
In the knowledge-aware depression detection phase, the LLM jointly performs
depression detection and entity extraction, while the knowledge graph
represents and weights these entities to refine prediction performance. In the
knowledge refinement and expansion phase, new entities, relationships, and
entity types extracted by the LLM are incorporated into the knowledge graph
under expert supervision, enabling continual knowledge evolution. Using
large-scale UGC, the framework enhances both predictive accuracy and medical
understanding. Expert evaluations confirmed the discovery of clinically
meaningful symptoms, comorbidities, and social triggers complementary to
existing literature. We conceptualize and operationalize
prediction-through-learning and learning-through-prediction as mutually
reinforcing processes, advancing both methodological and theoretical
understanding in predictive analytics. The framework demonstrates the
co-evolution of computational models and domain knowledge, offering a
foundation for adaptive, data-driven knowledge systems applicable to other
dynamic risk monitoring contexts.

</details>


### [247] [Chain of Execution Supervision Promotes General Reasoning in Large Language Models](https://arxiv.org/abs/2510.23629)
*Nuo Chen,Zehua Li,Keqin Bao,Junyang Lin,Dayiheng Liu*

Main category: cs.LG

TL;DR: TracePile是一个大规模语料库，将代码执行转化为显式的逐步推理链（CoE），用于提升大语言模型的推理能力，在多个基准测试中表现出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 代码蕴含丰富的逻辑结构和多种推理范式，但其推理过程常隐式表达并混杂噪声，直接训练效果不佳，因此需要更清晰、结构化的推理数据。

Method: 构建包含260万样本的TracePile语料库，将代码执行过程转化为显式的Chain of Execution（CoE）推理链，并加入变量追踪问题和代码重写以增强逻辑细粒度和多样性；在三种训练设置下进行评估。

Result: 在LLaMA 3、LLaMA 3.1、Qwen-2.5及其Coder版本上，TracePile在数学、代码、逻辑和算法共20个基准测试中均带来一致提升；在两阶段微调下，LLaMA3.1-8B在九个数学数据集上平均提升7.1%，并在LiveCodeBench、CRUX和MMLU上表现显著改善。

Conclusion: TracePile通过显式化代码中的推理过程，有效增强了大语言模型的推理能力和泛化性，是一种有前景的训练范式。

Abstract: Building robust and general reasoning ability is a central goal in the
development of large language models (LLMs). Recent efforts increasingly turn
to code as a rich training source, given its inherent logical structure and
diverse reasoning paradigms such as divide-and-conquer, topological ordering,
and enumeration. However, reasoning in code is often expressed implicitly and
entangled with syntactic or implementation noise, making direct training on raw
code suboptimal.To address this, we introduce TracePile, a large-scale corpus
of 2.6 million samples that transforms code execution into explicit,
step-by-step chain-of-thought-style rationales, which we call Chain of
Execution (CoE). The corpus spans domains including mathematics, classical
algorithms and algorithmic competition, and is enriched with variable-tracing
questions and code rewritings to enhance logical granularity and code
diversity. We evaluate TracePile using three training setups:
continue-pretraining, instruction tuning after pretraining, and two-stage
finetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,
and Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and
algorithms demonstrate consistent improvements. Notably, TracePile boosts
LLaMA3.1-8B by 7.1\% on average across nine math datasets and delivers clear
gains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.

</details>


### [248] [NUM2EVENT: Interpretable Event Reasoning from Numerical time-series](https://arxiv.org/abs/2510.23630)
*Ninghui Feng,Yiyan Qi*

Main category: cs.LG

TL;DR: 本文提出了从纯数值时间序列中推断可解释事件的“数值到事件”推理任务，提出了一种结合代理引导事件提取器和基于Hawkes过程的合成生成器的框架，并通过两阶段微调实现对数值变化的显式推理与结构化事件输出。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在多模态推理上表现优异，但对纯数值信号的理解有限，主要局限于预测或趋势描述，缺乏对驱动数值变化的潜在事件及其推理过程的解释。因此需要一种能从无文本标注的数值数据中解码出语义事件的方法。

Method: 提出一个包含代理引导事件提取器（AGE）、基于标记多变量Hawkes过程的合成生成器（EveDTS）以及结合时间序列编码器与结构化解码器的两阶段微调流程的框架，实现对数值变化的显式推理、中间解释生成和结构化事件假设输出。

Result: 在多领域数据集上的实验表明，该方法在事件级别的精确率和召回率上显著优于强大的大语言模型基线。

Conclusion: 该研究为连接定量推理与语义理解提供了新方向，使大模型能够直接从数值动态中解释和预测事件。

Abstract: Large language models (LLMs) have recently demonstrated impressive multimodal
reasoning capabilities, yet their understanding of purely numerical time-series
signals remains limited. Existing approaches mainly focus on forecasting or
trend description, without uncovering the latent events that drive numerical
changes or explaining the reasoning process behind them. In this work, we
introduce the task of number-to-event reasoning and decoding, which aims to
infer interpretable structured events from numerical inputs, even when current
text is unavailable. To address the data scarcity and semantic alignment
challenges, we propose a reasoning-aware framework that integrates an
agent-guided event extractor (AGE), a marked multivariate Hawkes-based
synthetic generator (EveDTS), and a two-stage fine-tuning pipeline combining a
time-series encoder with a structured decoder. Our model explicitly reasons
over numerical changes, generates intermediate explanations, and outputs
structured event hypotheses. Experiments on multi-domain datasets show that our
method substantially outperforms strong LLM baselines in event-level precision
and recall. These results suggest a new direction for bridging quantitative
reasoning and semantic understanding, enabling LLMs to explain and predict
events directly from numerical dynamics.

</details>


### [249] [Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments](https://arxiv.org/abs/2510.24503)
*Mortesa Hussaini,Jan Theiß,Anthony Stein*

Main category: cs.LG

TL;DR: 本文研究了联邦学习中异构数据环境下的模型个性化问题，提出了一种新的FedAvg改进方法FLIU，通过自适应个性化因子提升本地性能和泛化能力，并在多种非独立同分布条件下进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 由于联邦学习中客户端数据分布差异大，局部模型容易偏离全局最优，现有方法多关注本地表现而忽视对分布外样本的泛化能力，因此需要更全面的评估与改进方法。

Method: 提出Federated Learning with Individualized Updates (FLIU)，在FedAvg基础上引入自适应个性化因子进行个体化更新，并在单轮通信的不同阶段进行细粒度分析，以更好理解算法行为。

Result: 在MNIST和CIFAR-10上验证了FLIU在IID、病理性和Dirichlet非IID等不同数据分布下的表现，结果显示其在本地性能和泛化能力方面均优于传统方法。

Conclusion: FLIU通过简单的个性化机制有效缓解了客户端漂移问题，在保持良好泛化的同时提升了个性化性能，适用于高度异构的联邦学习环境。

Abstract: In the context of Federated Learning with heterogeneous data environments,
local models tend to converge to their own local model optima during local
training steps, deviating from the overall data distributions. Aggregation of
these local updates, e.g., with FedAvg, often does not align with the global
model optimum (client drift), resulting in an update that is suboptimal for
most clients. Personalized Federated Learning approaches address this challenge
by exclusively focusing on the average local performances of clients' models on
their own data distribution. Generalization to out-of-distribution samples,
which is a substantial benefit of FedAvg and represents a significant component
of robustness, appears to be inadequately incorporated into the assessment and
evaluation processes. This study involves a thorough evaluation of Federated
Learning approaches, encompassing both their local performance and their
generalization capabilities. Therefore, we examine different stages within a
single communication round to enable a more nuanced understanding of the
considered metrics. Furthermore, we propose and incorporate a modified approach
of FedAvg, designated as Federated Learning with Individualized Updates (FLIU),
extending the algorithm by a straightforward individualization step with an
adaptive personalization factor. We evaluate and compare the approaches
empirically using MNIST and CIFAR-10 under various distributional conditions,
including benchmark IID and pathological non-IID, as well as additional novel
test environments with Dirichlet distribution specifically developed to stress
the algorithms on complex data heterogeneity.

</details>


### [250] [Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling](https://arxiv.org/abs/2510.23631)
*Yuxuan Tang,Yifan Feng*

Main category: cs.LG

TL;DR: 提出了一种统一的偏好优化框架RCPO，支持多选和排名反馈，优于现有配对方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法主要依赖于成对偏好优化，忽略了更丰富的人类反馈形式，如多选比较和top-k排名。

Method: 通过最大似然估计将偏好优化与（排序）选择建模结合，提出了支持效用和排序模型的RCPO框架，并实例化了Multinomial Logit和Mallows-RMJ两种模型。

Result: 在Llama-3-8B-Instruct和Gemma-2-9B-it模型上，RCPO在AlpacaEval 2和Arena-Hard基准测试中持续优于基线方法。

Conclusion: 直接利用排序偏好数据并结合合适的选择模型，能更有效地实现大模型对齐，RCPO为LLM训练提供了灵活且可扩展的基础。

Abstract: Alignment of large language models (LLMs) has predominantly relied on
pairwise preference optimization, where annotators select the better of two
responses to a prompt. While simple, this approach overlooks the opportunity to
learn from richer forms of human feedback, such as multiwise comparisons and
top-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a
unified framework that bridges preference optimization with (ranked) choice
modeling via maximum likelihood estimation. The framework is flexible,
supporting both utility-based and rank-based choice models. It subsumes several
existing pairwise methods (e.g., DPO, SimPO), while providing principled
training objectives for richer feedback formats. We instantiate this framework
with two representative ranked choice models (Multinomial Logit and
Mallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across
AlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms
competitive baselines. RCPO shows how directly leveraging ranked preference
data, combined with the right choice models, yields more effective alignment.
It offers a versatile and extensible foundation for incorporating (ranked)
choice modeling into LLM training.

</details>


### [251] [LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression](https://arxiv.org/abs/2510.23632)
*Guozhong Li,Muhannad Alhumaidi,Spiros Skiadopoulos,Panos Kalnis*

Main category: cs.LG

TL;DR: 本文提出LLMCOMP，一种利用仅解码器大语言模型进行科学数据压缩的新方法，在严格误差限制下显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 高分辨率科学模拟和观测系统产生海量时空数据，亟需高效、有误差界限的压缩方法。

Method: 将3D场量化为离散令牌，使用Z-order曲线排列并结合覆盖引导采样，训练带时空嵌入的自回归Transformer模型，通过top-k预测并存储秩索引和回退校正实现压缩。

Result: 在多个再分析数据集上的实验表明，LLMCOMP在严格误差约束下压缩比最高提升30%，优于当前最先进的压缩器。

Conclusion: LLM具有作为高保真科学数据通用压缩器的潜力。

Abstract: The rapid growth of high-resolution scientific simulations and observation
systems is generating massive spatiotemporal datasets, making efficient,
error-bounded compression increasingly important. Meanwhile, decoder-only large
language models (LLMs) have demonstrated remarkable capabilities in modeling
complex sequential data. In this paper, we propose LLMCOMP, a novel lossy
compression paradigm that leverages decoder-only large LLMs to model scientific
data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via
Z-order curves to preserve locality, and applies coverage-guided sampling to
enhance training efficiency. An autoregressive transformer is then trained with
spatial-temporal embeddings to model token transitions. During compression, the
model performs top-k prediction, storing only rank indices and fallback
corrections to ensure strict error bounds. Experiments on multiple reanalysis
datasets show that LLMCOMP consistently outperforms state-of-the-art
compressors, achieving up to 30% higher compression ratios under strict error
bounds. These results highlight the potential of LLMs as general-purpose
compressors for high-fidelity scientific data.

</details>


### [252] [Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models](https://arxiv.org/abs/2510.23633)
*Xun Su,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: 提出了一种名为Noise Combination Sampling的新方法，通过从噪声子空间合成最优噪声向量来近似测量得分，从而在无需逐步调整超参数的情况下自然嵌入条件信息，有效解决预训练扩散模型在零样本反问题求解中的固有困境。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型在零样本反问题求解中面临过度或不足融合观测信息的两难问题：过度融合会破坏生成过程，而融合不足则无法充分强调反问题的约束条件。

Method: 提出Noise Combination Sampling方法，利用噪声子空间合成最优噪声向量以替代标准去噪扩散概率模型中的噪声项，逼近测量得分，实现条件信息的自然嵌入，且无需逐歩超参数调节。

Result: 该方法适用于多种反问题求解任务（如图像压缩），尤其在生成步数T较小时表现出优越性能，计算开销几乎可忽略，并显著提升鲁棒性和稳定性。

Conclusion: Noise Combination Sampling为扩散模型在反问题中的应用提供了一种高效、稳定的解决方案，克服了传统方法对超参数调优的依赖，在少步生成场景下具有显著优势。

Abstract: Pretrained diffusion models have demonstrated strong capabilities in
zero-shot inverse problem solving by incorporating observation information into
the generation process of the diffusion models. However, this presents an
inherent dilemma: excessive integration can disrupt the generative process,
while insufficient integration fails to emphasize the constraints imposed by
the inverse problem. To address this, we propose \emph{Noise Combination
Sampling}, a novel method that synthesizes an optimal noise vector from a noise
subspace to approximate the measurement score, replacing the noise term in the
standard Denoising Diffusion Probabilistic Models process. This enables
conditional information to be naturally embedded into the generation process
without reliance on step-wise hyperparameter tuning. Our method can be applied
to a wide range of inverse problem solvers, including image compression, and,
particularly when the number of generation steps $T$ is small, achieves
superior performance with negligible computational overhead, significantly
improving robustness and stability.

</details>


### [253] [Monotone and Separable Set Functions: Characterizations and Neural Models](https://arxiv.org/abs/2510.23634)
*Soutrik Sarangi,Yonatan Sverdlov,Nadav Dym,Abir De*

Main category: cs.LG

TL;DR: 本文研究了保持集合包含关系的集合到向量函数的设计问题，提出了“单调分离（MAS）”函数的概念，并分析了其存在性与维度界限；针对无限基集情形，提出具有弱MAS性质和Hölder连续性的替代模型，实验证明该模型在集合包含任务中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决集合包含问题中的实际应用需求，需要设计能够保持集合包含顺序的集合到向量映射函数。

Method: 提出并研究了单调分离（MAS）函数的性质，推导了其实现所需的向量维度上下界；对于无限基集情况，设计了一种满足弱MAS性质且具有Hölder稳定性的近似模型。

Result: 证明了在有限情况下MAS函数的维度界限，在无限基集下证明其不存在但提出了具有弱MAS性质的可实现模型，并展示了该模型在多种集合包含任务中的优越性能。

Conclusion: MAS函数能有效保留集合包含结构，所提出的弱MAS模型在无限基集下具有良好的理论性质和实验表现，适用于构建通用的单调集函数模型。

Abstract: Motivated by applications for set containment problems, we consider the
following fundamental problem: can we design set-to-vector functions so that
the natural partial order on sets is preserved, namely $S\subseteq T \text{ if
and only if } F(S)\leq F(T) $. We call functions satisfying this property
Monotone and Separating (MAS) set functions. % We establish lower and upper
bounds for the vector dimension necessary to obtain MAS functions, as a
function of the cardinality of the multisets and the underlying ground set. In
the important case of an infinite ground set, we show that MAS functions do not
exist, but provide a model called our which provably enjoys a relaxed MAS
property we name "weakly MAS" and is stable in the sense of Holder continuity.
We also show that MAS functions can be used to construct universal models that
are monotone by construction and can approximate all monotone set functions.
Experimentally, we consider a variety of set containment tasks. The experiments
show the benefit of using our our model, in comparison with standard set models
which do not incorporate set containment as an inductive bias. Our code is
available in https://github.com/yonatansverdlov/Monotone-Embedding.

</details>


### [254] [Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning](https://arxiv.org/abs/2510.23635)
*Andrea Bontempelli,Matteo Busso,Leonardo Javier Malcotti,Fausto Giunchiglia*

Main category: cs.LG

TL;DR: 本研究在真实环境下评估了Skeptical Learning（SKEL）在减少用户标注负担并提升数据质量方面的性能，通过大学生使用iLog应用四周的实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 用户生成的标注常存在噪声和错误，而现有方法缺乏来自最终用户的反馈验证，因此需要结合真实用户参与来评估和改进标注质量。

Method: 采用Skeptical Learning（SKEL）方法，结合主动标注与被动数据比较，并让真实用户在其上下文中动态修正标签，通过iLog移动应用在四周期间收集大学学生的实际使用数据进行评估。

Result: 结果显示SKEL能在降低用户标注努力的同时提高数据质量，但需权衡用户投入与数据准确性之间的平衡。

Conclusion: SKEL在真实用户参与下能有效改善标注质量并减少标注负担，具备在个人助手系统中应用的潜力。

Abstract: Any digital personal assistant, whether used to support task performance,
answer questions, or manage work and daily life, including fitness schedules,
requires high-quality annotations to function properly. However, user
annotations, whether actively produced or inferred from context (e.g., data
from smartphone sensors), are often subject to errors and noise. Previous
research on Skeptical Learning (SKEL) addressed the issue of noisy labels by
comparing offline active annotations with passive data, allowing for an
evaluation of annotation accuracy. However, this evaluation did not include
confirmation from end-users, the best judges of their own context. In this
study, we evaluate SKEL's performance in real-world conditions with actual
users who can refine the input labels based on their current perspectives and
needs. The study involves university students using the iLog mobile application
on their devices over a period of four weeks. The results highlight the
challenges of finding the right balance between user effort and data quality,
as well as the potential benefits of using SKEL, which include reduced
annotation effort and improved quality of collected data.

</details>


### [255] [Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation](https://arxiv.org/abs/2510.23636)
*Thaweerath Phisannupawong,Joshua Julian Damanik,Han-Lim Choi*

Main category: cs.LG

TL;DR: 提出一种基于轻量级大语言模型的多模态航班延误预测方法，结合轨迹数据与航空文本信息，实现亚分钟级预测误差。


<details>
  <summary>Details</summary>
Motivation: 航班延误影响空中交通网络效率，需更精准、实时的预测方法以支持空中交通管制决策。

Method: 将轨迹数据转换为语言模态，与航班信息、气象报告和机场通告等文本信息融合，利用轻量级大语言模型进行多模态建模。

Result: 模型在实验中 consistently 实现亚分钟级的预测误差，能有效利用延迟相关上下文信息提升预测精度。

Conclusion: 结合语言理解与跨模态轨迹适配的框架可显著提升航班延误预测性能，具备实际应用中的实用性与可扩展性。

Abstract: Flight delay prediction has become a key focus in air traffic management, as
delays highlight inefficiencies that impact overall network performance. This
paper presents a lightweight large language model-based multimodal flight delay
prediction, formulated from the perspective of air traffic controllers
monitoring aircraft delay after entering the terminal area. The approach
integrates trajectory representations with textual aeronautical information,
including flight information, weather reports, and aerodrome notices, by
adapting trajectory data into the language modality to capture airspace
conditions. Experimental results show that the model consistently achieves
sub-minute prediction error by effectively leveraging contextual information
related to the sources of delay. The framework demonstrates that linguistic
understanding, when combined with cross-modality adaptation of trajectory
information, enhances delay prediction. Moreover, the approach shows
practicality and scalability for real-world operations, supporting real-time
updates that refine predictions upon receiving new operational information.

</details>


### [256] [Combining Textual and Structural Information for Premise Selection in Lean](https://arxiv.org/abs/2510.23637)
*Job Petrovčič,David Eliecer Narvaez Denis,Ljupčo Todorovski*

Main category: cs.LG

TL;DR: 提出一种结合文本嵌入和图神经网络的图增强方法，用于改进Lean中的前提选择，显著优于语言模型基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言的方法在前提选择中忽略前提之间的依赖关系，导致在大型形式化库中定理证明的扩展受限。

Method: 将Lean形式化的密集文本嵌入与异构依赖图上的图神经网络相结合，该图捕捉状态-前提和前提-前提关系。

Result: 在LeanDojo基准上，该方法在标准检索指标上比ReProver基线高出25%以上。

Conclusion: 利用关系信息（如依赖结构）能显著提升前提选择的效果，验证了图增强方法的优势。

Abstract: Premise selection is a key bottleneck for scaling theorem proving in large
formal libraries. Yet existing language-based methods often treat premises in
isolation, ignoring the web of dependencies that connects them. We present a
graph-augmented approach that combines dense text embeddings of Lean
formalizations with graph neural networks over a heterogeneous dependency graph
capturing both state--premise and premise--premise relations. On the LeanDojo
Benchmark, our method outperforms the ReProver language-based baseline by over
25% across standard retrieval metrics. These results demonstrate the power of
relational information for more effective premise selection.

</details>


### [257] [Integrating Genomics into Multimodal EHR Foundation Models](https://arxiv.org/abs/2510.23639)
*Jonathan Amar,Edward Liu,Alessandra Breschi,Liangliang Zhang,Pouya Kheradpour,Sylvia Li,Lisa Soleymani Lehmann,Alessandro Giulianelli,Matt Edwards,Yugang Jia,David Nola,Raghav Mani,Pankaj Vats,Jesse Tetreault,T. J. Chen,Cory Y. McLean*

Main category: cs.LG

TL;DR: 提出了一种结合多基因风险评分（PRS）的电子健康记录（EHR）基础模型，利用All of Us数据通过生成式AI提升疾病预测能力，尤其在2型糖尿病预测中表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统EHR模型忽略遗传信息，限制了健康风险预测的全面性；本文旨在融合PRS与EHR，构建更全面、个性化的健康分析框架。

Method: 构建一个融合PRS和EHR的多模态基础模型，采用生成式AI技术，在All of Us大规模数据上进行训练，并探索迁移学习以适应特定分类任务。

Result: 模型在多种疾病预测中表现良好，尤其在2型糖尿病预测中展现出高预测价值，并揭示了PRS与临床数据之间的交互作用。

Conclusion: 该方法显著提升了EHR模型的预测性能与可解释性，为个性化医疗、风险分层和真实世界证据生成提供了新路径。

Abstract: This paper introduces an innovative Electronic Health Record (EHR) foundation
model that integrates Polygenic Risk Scores (PRS) as a foundational data
modality, moving beyond traditional EHR-only approaches to build more holistic
health profiles. Leveraging the extensive and diverse data from the All of Us
(AoU) Research Program, this multimodal framework aims to learn complex
relationships between clinical data and genetic predispositions. The
methodology extends advancements in generative AI to the EHR foundation model
space, enhancing predictive capabilities and interpretability. Evaluation on
AoU data demonstrates the model's predictive value for the onset of various
conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay
between PRS and EHR data. The work also explores transfer learning for custom
classification tasks, showcasing the architecture's versatility and efficiency.
This approach is pivotal for unlocking new insights into disease prediction,
proactive health management, risk stratification, and personalized treatment
strategies, laying the groundwork for more personalized, equitable, and
actionable real-world evidence generation in healthcare.

</details>


### [258] [Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning](https://arxiv.org/abs/2510.23640)
*Zihao Jing,Yan Sun,Yan Yi Li,Sugitha Janarthanan,Alana Deng,Pingzhao Hu*

Main category: cs.LG

TL;DR: MuMo是一种结构化的多模态融合框架，通过结合2D拓扑和3D几何信息，并采用渐进式注入机制，有效解决了分子表示中的3D构象不稳定和模态崩溃问题，在多个基准任务中表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 多模态分子模型常受3D构象不可靠和模态崩溃影响，导致鲁棒性和泛化能力受限，因此需要更稳定的融合方法。

Method: 提出结构化融合管道（SFP）整合2D和3D结构信息作为稳定先验，并设计渐进式注入（PI）机制将该先验非对称地融入序列流，避免模态崩溃。模型基于状态空间架构，支持长程依赖建模。

Result: 在TDC和MoleculeNet的29个基准任务上，MuMo平均比最佳基线提升2.7%，在22个任务中排名第一，其中LD50任务提升达27%，验证了其对3D噪声的鲁棒性和多模态融合的有效性。

Conclusion: MuMo通过结构化融合策略显著提升了分子表示的稳定性与性能，为多模态分子建模提供了有效解决方案。

Abstract: Multimodal molecular models often suffer from 3D conformer unreliability and
modality collapse, limiting their robustness and generalization. We propose
MuMo, a structured multimodal fusion framework that addresses these challenges
in molecular representation through two key strategies. To reduce the
instability of conformer-dependent fusion, we design a Structured Fusion
Pipeline (SFP) that combines 2D topology and 3D geometry into a unified and
stable structural prior. To mitigate modality collapse caused by naive fusion,
we introduce a Progressive Injection (PI) mechanism that asymmetrically
integrates this prior into the sequence stream, preserving modality-specific
modeling while enabling cross-modal enrichment. Built on a state space
backbone, MuMo supports long-range dependency modeling and robust information
propagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) and
MoleculeNet, MuMo achieves an average improvement of 2.7% over the
best-performing baseline on each task, ranking first on 22 of them, including a
27% improvement on the LD50 task. These results validate its robustness to 3D
conformer noise and the effectiveness of multimodal fusion in molecular
representation. The code is available at: github.com/selmiss/MuMo.

</details>


### [259] [Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging](https://arxiv.org/abs/2510.23641)
*Aaron Wang,Zihan Zhao,Subash Katel,Vivekanand Gyanchand Sahu,Elham E Khoda,Abhijith Gandrakota,Jennifer Ngadiuba,Richard Cavanaugh,Javier Duarte*

Main category: cs.LG

TL;DR: 提出了空间感知线性Transformer（SAL-T），一种基于物理启发的线性注意力机制模型，用于高效处理高能粒子碰撞数据，在保持较低资源消耗和延迟的同时，实现与全注意力Transformer相当的分类性能。


<details>
  <summary>Details</summary>
Motivation: Transformer在高能粒子碰撞中表现优异，但其二次复杂度导致在高数据吞吐场景（如CERN LHC）中部署困难，需要更高效的模型。

Method: 基于Linformer架构，引入基于运动学特征的空间感知粒子分区，并在物理意义区域间计算注意力；结合卷积层捕捉局部相关性，利用喷注物理知识指导模型设计。

Result: 在喷注分类任务中优于标准Linformer，性能接近全注意力Transformer，且推理资源消耗更低、延迟更小；在ModelNet10点云分类任务上也验证了有效性。

Conclusion: SAL-T在保持线性复杂度的同时，通过物理启发的设计提升了分类性能，适用于高吞吐量科学应用场景。

Abstract: Transformers are very effective in capturing both global and local
correlations within high-energy particle collisions, but they present
deployment challenges in high-data-throughput environments, such as the CERN
LHC. The quadratic complexity of transformer models demands substantial
resources and increases latency during inference. In order to address these
issues, we introduce the Spatially Aware Linear Transformer (SAL-T), a
physics-inspired enhancement of the linformer architecture that maintains
linear attention. Our method incorporates spatially aware partitioning of
particles based on kinematic features, thereby computing attention between
regions of physical significance. Additionally, we employ convolutional layers
to capture local correlations, informed by insights from jet physics. In
addition to outperforming the standard linformer in jet classification tasks,
SAL-T also achieves classification results comparable to full-attention
transformers, while using considerably fewer resources with lower latency
during inference. Experiments on a generic point cloud classification dataset
(ModelNet10) further confirm this trend. Our code is available at
https://github.com/aaronw5/SAL-T4HEP.

</details>


### [260] [Efficient Low Rank Attention for Long-Context Inference in Large Language Models](https://arxiv.org/abs/2510.23649)
*Tenghui Li,Guoxu Zhou,Xuyang Zhao,Yuning Qiu,Qibin Zhao*

Main category: cs.LG

TL;DR: 本文提出了LRQK，一种通过低秩分解查询和键矩阵来减少大模型推理过程中KV缓存内存开销的方法，结合混合GPU-CPU缓存机制，在保持准确性的前提下显著降低内存使用和数据传输开销。


<details>
  <summary>Details</summary>
Motivation: 随着输入文本增长，LLM中的KV缓存导致GPU内存成本过高，限制了资源受限设备上的长上下文推理。现有方法如量化和剪枝存在精度损失或保留关键值对不充分的问题。

Method: 提出两阶段框架LRQK：在prefill阶段将全精度的查询和键矩阵联合分解为紧凑的低秩因子；在每个decode步骤中用这些低维投影计算代理注意力分数，并采用top-k和最近token策略，结合混合GPU-CPU缓存与命中/未命中机制，仅传输缺失的全精度KV对。

Result: 在RULER和LongBench基准测试中，使用LLaMA-3-8B和Qwen2.5-7B模型验证了LRQK在长上下文场景下性能达到或超过主流稀疏注意力方法，同时显著节省内存且精度损失极小。

Conclusion: LRQK有效平衡了长上下文推理中的内存效率与模型准确性，通过低秩近似与混合缓存机制为实际部署提供了可扩展的解决方案。

Abstract: As the length of input text grows, the key-value (KV) cache in LLMs imposes
prohibitive GPU memory costs and limits long-context inference on resource
constrained devices. Existing approaches, such as KV quantization and pruning,
reduce memory usage but suffer from numerical precision loss or suboptimal
retention of key-value pairs. We introduce Low Rank Query and Key attention
(LRQK), a two-stage framework that jointly decomposes the full-precision query
and key matrices into compact rank-\(r\) factors during the prefill stage, and
then uses these low-dimensional projections to compute proxy attention scores
in \(\mathcal{O}(lr)\) time at each decode step. By selecting only the
top-\(k\) tokens and a small fixed set of recent tokens, LRQK employs a mixed
GPU-CPU cache with a hit-and-miss mechanism that transfers only missing
full-precision KV pairs, thereby preserving exact attention outputs while
reducing CPU-GPU data movement. Extensive experiments on the RULER and
LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK
matches or surpasses leading sparse-attention methods in long context settings,
while delivering significant memory savings with minimal loss in accuracy. Our
code is available at https://github.com/tenghuilee/LRQK.

</details>


### [261] [Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs](https://arxiv.org/abs/2510.23650)
*Wei Xia*

Main category: cs.LG

TL;DR: 提出两种零样本logits层去偏方法（Static和Dynamic），其中Dynamic方法在最小影响流畅性的情况下减少高达70%的偏见，且语义感知的logits干预对对齐的大型语言模型稳定有效。


<details>
  <summary>Details</summary>
Motivation: 为了减轻大型语言模型中的偏见，尤其是在不依赖额外训练数据的零样本设置下，探索更有效的去偏方法。

Method: 提出Static和Dynamic两种零样本logits层去偏方法，通过直接干预logits输出，并结合语义感知策略进行偏差校正。

Result: Dynamic方法最多可减少70%的偏见，且对生成流畅性影响极小；logits层干预优于隐藏层干预方法，在对齐的LLMs中表现出稳定性和有效性。

Conclusion: 语义感知的logits层干预是一种高效、稳定的零样本去偏策略，特别适用于已对齐的大型语言模型。

Abstract: We proposed Static and Dynamic -- two zero-shot logits-layer debiasing
methods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits
intervention outperforms hidden-layer approaches. We show semantic-aware logits
intervention is stable and effective for debiasing aligned LLMs.

</details>


### [262] [The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models](https://arxiv.org/abs/2510.23652)
*Yao Lu,Yuqi Li,Wenbin Xie,Shanqing Yu,Qi Xuan,Zhaowei Zhu,Shiping Wen*

Main category: cs.LG

TL;DR: 提出了一种新的连续层剪枝框架CLP，通过可微凹门算法和截断端点调优策略，有效提升大语言模型在边缘设备上的压缩效率与性能保持。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝方法依赖手工指标且忽略层间依赖关系，易破坏模型信息流并导致性能下降。

Method: 引入可微分的凹门算法自动识别最优连续剪枝层段，并采用截断端点调优策略仅微调邻近层以恢复性能。

Result: 在多个大模型（如LLaMA2、LLaMA3、Qwen）上实验表明，CLP在20%剪枝率下平均性能保持率达95.34%，显著优于现有方法。此外，CLP可与量化结合进一步压缩模型。

Conclusion: CLP通过考虑层间依赖关系实现高效连续剪枝，在高剪枝率下仍能保持优异性能，具备良好的通用性和实用性。

Abstract: Although large language models (LLMs) have achieved revolutionary
breakthroughs in many fields, their large model size and high computational
cost pose significant challenges for practical deployment on
resource-constrained edge devices. To this end, layer pruning has been proposed
to reduce the computational overhead by directly removing redundant layers.
However, existing layer pruning methods typically rely on hand-crafted metrics
to evaluate and remove individual layers, while ignoring the dependencies
between layers. This can disrupt the model's information flow and severely
degrade performance. To address these issues, we propose CLP, a novel
continuous layer pruning framework that introduces two key innovations: a
differentiable concave gate algorithm that automatically identifies the best
continuous layer segments for pruning via gradient-based optimization; and a
cutoff endpoint tuning strategy that effectively restores model performance by
fine-tuning only the layers adjacent to the pruned segments. Extensive
experiments across multiple model architectures (including LLaMA2, LLaMA3 and
Qwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly
outperforms existing state-of-the-art baselines. For example, at a pruning rate
of $20\%$, CLP achieves an average performance retention of $95.34\%$ on
LLaMA3-70B, outperforming baselines by $4.29\%$-$30.52\%$. Furthermore, CLP can
be seamlessly combined with quantization to further compress the model with
only a slight performance loss.

</details>


### [263] [Error Adjustment Based on Spatiotemporal Correlation Fusion for Traffic Forecasting](https://arxiv.org/abs/2510.23656)
*Fuqiang Liu,Weiping Ding,Luis Miranda-Moreno,Lijun Sun*

Main category: cs.LG

TL;DR: 本文提出了一种名为Spatiotemporally Autocorrelated Error Adjustment (SAEA)的新型框架，用于系统调整交通预测中的自相关预测误差。


<details>
  <summary>Details</summary>
Motivation: 现有的深度神经网络在交通预测中通常假设预测误差在时间和空间上是不相关的，但这一假设因交通数据的时空自相关性而不成立，限制了模型性能。

Method: SAEA将预测误差建模为时空向量自回归（VAR）过程，通过系数矩阵捕捉时空误差相关性，并引入结构稀疏正则化以融合先验空间信息，同时设计了测试时误差调整的推理过程来动态修正预测。

Result: 在多个交通数据集上的实验表明，SAEA在广泛的预测模型中均能提升预测性能，几乎在所有情况下都有效减少了自相关误差的影响。

Conclusion: SAEA能够有效捕捉并校正交通预测中的时空自相关误差，显著提升DNN-based模型的预测精度，具有良好的通用性和实用性。

Abstract: Deep neural networks (DNNs) play a significant role in an increasing body of
research on traffic forecasting due to their effectively capturing
spatiotemporal patterns embedded in traffic data. A general assumption of
training the said forecasting models via mean squared error estimation is that
the errors across time steps and spatial positions are uncorrelated. However,
this assumption does not really hold because of the autocorrelation caused by
both the temporality and spatiality of traffic data. This gap limits the
performance of DNN-based forecasting models and is overlooked by current
studies. To fill up this gap, this paper proposes Spatiotemporally
Autocorrelated Error Adjustment (SAEA), a novel and general framework designed
to systematically adjust autocorrelated prediction errors in traffic
forecasting. Unlike existing approaches that assume prediction errors follow a
random Gaussian noise distribution, SAEA models these errors as a
spatiotemporal vector autoregressive (VAR) process to capture their intrinsic
dependencies. First, it explicitly captures both spatial and temporal error
correlations by a coefficient matrix, which is then embedded into a newly
formulated cost function. Second, a structurally sparse regularization is
introduced to incorporate prior spatial information, ensuring that the learned
coefficient matrix aligns with the inherent road network structure. Finally, an
inference process with test-time error adjustment is designed to dynamically
refine predictions, mitigating the impact of autocorrelated errors in real-time
forecasting. The effectiveness of the proposed approach is verified on
different traffic datasets. Results across a wide range of traffic forecasting
models show that our method enhances performance in almost all cases.

</details>


### [264] [A machine learning framework integrating seed traits and plasma parameters for predicting germination uplift in crops](https://arxiv.org/abs/2510.23657)
*Saklain Niam,Tashfiqur Rahman,Md. Amjad Patwary,Mukarram Hossain*

Main category: cs.LG

TL;DR: 本研究首次提出基于机器学习的框架，用于预测介质阻挡放电冷等离子体处理对大豆、大麦、向日葵、萝卜和番茄种子萌发率的提升效果，其中Extra Trees模型表现最佳，并揭示了电压、时间与萌发间的 hormetic 响应关系。


<details>
  <summary>Details</summary>
Motivation: 冷等离子体虽可促进种子萌发，但因种子-等离子体-环境相互作用复杂，导致效果难以预测，因此需要一种可量化并预测不同条件下萌发提升的方法。

Method: 采用多种机器学习模型（GB、XGB、ET及混合模型）构建预测框架，使用DBD冷等离子体处理五种作物种子，结合处理参数（如电压、时间、功率）与物种/品种特征进行建模，并通过特征工程优化模型性能。

Result: Extra Trees模型表现最优（R²=0.919，RMSE=3.21，MAE=2.62），经特征筛选后进一步提升至R²=0.925；发现萌发率存在hormetic响应：7–15 kV、200–500 s为最佳区间，过高或过低均不利；放电功率≥100 W且暴露时间短时萌发率最高；萝卜和大豆预测精度高（MAE分别为1.46和2.05），而向日葵变异较大（MAE=3.80）；部分品种如Williams（MAE=1.23）预测良好，Arian和Nyírségi fekete则较差。

Conclusion: 该机器学习框架能有效预测冷等离子体对种子萌发的影响，已集成至MLflow平台，可作为精准农业中优化等离子体处理参数的决策支持工具。

Abstract: Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet
outcomes remain difficult to predict due to complex seed--plasma--environment
interactions. This study introduces the first machine learning framework to
forecast germination uplift in soybean, barley, sunflower, radish, and tomato
under dielectric barrier discharge (DBD) plasma. Among the models tested (GB,
XGB, ET, and hybrids), Extra Trees (ET) performed best (R\textsuperscript{2} =
0.919; RMSE = 3.21; MAE = 2.62), improving to R\textsuperscript{2} = 0.925
after feature reduction. Engineering analysis revealed a hormetic response:
negligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for
200--500 s, and reduced germination beyond 20 kV or prolonged exposures.
Discharge power was also a dominant factor, with germination rate maximizing at
$\geq$100 W with low exposure time. Species and cultivar-level predictions
showed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high
consistency, while sunflower remained slightly higher variable (MAE = 3.80).
Among cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,
while Arian (2.86) and Ny\'{\i}rs\'{e}gi fekete (3.74) were comparatively
poorly captured. This framework was also embedded into MLflow, providing a
decision-support tool for optimizing CP seed germination in precision
agriculture.

</details>


### [265] [Aligning Diffusion Language Models via Unpaired Preference Optimization](https://arxiv.org/abs/2510.23658)
*Vaibhav Jindal,Hejian Sang,Chun-Mao Lai,Yanning Chen,Zhipeng Wang*

Main category: cs.LG

TL;DR: 本文提出了ELBO-KTO方法，结合ELBO代理和非配对偏好优化（KTO），用于在扩散语言模型中进行无需成对标注的人类偏好对齐，在多个基准上表现优于基础模型。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型难以通过传统方式对齐人类偏好，因为序列似然不可计算且成对偏好数据收集成本高。

Method: 提出ELBO-KTO，使用ELBO作为扩散模型似然的代理，并引入基于前景理论的非配对偏好优化目标（KTO），同时采用方差缩减技术稳定训练梯度。

Result: 在LLaDA-8B-Instruct上应用ELBO-KTO后，在kto-mix-14k和UltraFeedback-Binary上的调整胜率分别为65.9%和62.3%，并在GSM8K、MMLU等下游任务中表现优于或相当于基础模型。

Conclusion: ELBO-KTO证明了非配对偏好优化是扩散语言模型中一种可行且有效的对齐方法。

Abstract: Diffusion language models (dLLMs) are an emerging alternative to
autoregressive (AR) generators, but aligning them to human preferences is
challenging because sequence log-likelihoods are intractable and pairwise
preference data are costly to collect. We introduce ELBO-KTO, which combines an
ELBO surrogate for diffusion log-likelihoods with a prospect-theoretic,
unpaired preference objective (Kahneman Tversky Optimization, KTO). We analyze
the bias and variance induced by the ELBO substitution and employ
variance-reduction practices that stabilize gradients during training. Applied
to LLaDA-8B-Instruct, ELBO-KTO yields \textbf{65.9\%} and \textbf{62.3\%}
adjusted win rates on kto-mix-14k and UltraFeedback-Binary, respectively,
versus the base model under an automatic LLM judge. Across downstream tasks,
including GSM8K, MMLU, and additional reasoning/knowledge benchmarks, ELBO-KTO
trained on UltraFeedback-Binary performs on par with or better than the base
model under identical decoding. This establishes unpaired preference
optimization as a viable alternative to pairwise alignment in diffusion LLMs.

</details>


### [266] [Quantum Machine Learning for Image Classification: A Hybrid Model of Residual Network with Quantum Support Vector Machine](https://arxiv.org/abs/2510.23659)
*Md. Farhan Shahriyar,Gazi Tanbhir,Abdullah Md Raihan Chy*

Main category: cs.LG

TL;DR: 本研究提出了一种结合ResNet-50与量子支持向量机（QSVM）的混合方法，用于马铃薯病害检测，在Z特征映射下QSVM准确率达到99.23%，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 经典机器学习和深度学习在处理高维复杂数据集时存在局限性，亟需引入量子计算等先进方法提升图像分类效率。

Method: 采用ResNet-50提取马铃薯病害图像的深层特征，通过主成分分析（PCA）降维后，利用不同量子特征映射（如ZZ、Z、Pauli-X）的QSVM进行分类，并与SVM和随机森林进行对比。

Result: Z特征映射的QSVM表现最佳，准确率达99.23%，超过传统SVM和随机森林模型。

Conclusion: 量子-经典混合模型在图像分类中具有显著优势，为植物病害检测提供了新思路和技术路径。

Abstract: Recently, there has been growing attention on combining quantum machine
learning (QML) with classical deep learning approaches, as computational
techniques are key to improving the performance of image classification tasks.
This study presents a hybrid approach that uses ResNet-50 (Residual Network)
for feature extraction and Quantum Support Vector Machines (QSVM) for
classification in the context of potato disease detection. Classical machine
learning as well as deep learning models often struggle with high-dimensional
and complex datasets, necessitating advanced techniques like quantum computing
to improve classification efficiency. In our research, we use ResNet-50 to
extract deep feature representations from RGB images of potato diseases. These
features are then subjected to dimensionality reduction using Principal
Component Analysis (PCA). The resulting features are processed through QSVM
models which apply various quantum feature maps such as ZZ, Z, and Pauli-X to
transform classical data into quantum states. To assess the model performance,
we compared it with classical machine learning algorithms such as Support
Vector Machine (SVM) and Random Forest (RF) using five-fold stratified
cross-validation for comprehensive evaluation. The experimental results
demonstrate that the Z-feature map-based QSVM outperforms classical models,
achieving an accuracy of 99.23 percent, surpassing both SVM and RF models. This
research highlights the advantages of integrating quantum computing into image
classification and provides a potential disease detection solution through
hybrid quantum-classical modeling.

</details>


### [267] [Quanvolutional Neural Networks for Pneumonia Detection: An Efficient Quantum-Assisted Feature Extraction Paradigm](https://arxiv.org/abs/2510.23660)
*Gazi Tanbhir,Md. Farhan Shahriyar,Abdullah Md Raihan Chy*

Main category: cs.LG

TL;DR: 本文提出了一种基于量子卷积神经网络（QNN）的混合量子-经典模型，用于肺炎检测，在PneumoniaMNIST数据集上实现了83.33%的准确率，优于传统CNN的73.33%，展示了QNN在医学图像分析中更高的收敛性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 为解决传统卷积神经网络（CNN）在肺炎检测中计算成本高、特征表示能力有限以及小数据集上泛化能力差的问题，探索量子计算在医学图像分析中的应用潜力。

Method: 提出一种新型混合量子-经典模型，使用参数化量子电路（PQC）构成的量子卷积层处理2x2图像块，通过旋转Y门进行数据编码，并利用纠缠层生成非经典特征表示，提取的特征由经典神经网络完成分类。

Result: 实验结果表明，所提出的QNN模型验证准确率达到83.33%，显著高于对比的经典CNN模型的73.33%，且表现出更强的收敛性和样本效率。

Conclusion: 量子卷积神经网络在医学图像分析中具有巨大潜力，尤其适用于标注数据有限的场景，为将量子计算集成到深度学习驱动的医疗诊断系统提供了可行路径和基础支持。

Abstract: Pneumonia poses a significant global health challenge, demanding accurate and
timely diagnosis. While deep learning, particularly Convolutional Neural
Networks (CNNs), has shown promise in medical image analysis for pneumonia
detection, CNNs often suffer from high computational costs, limitations in
feature representation, and challenges in generalizing from smaller datasets.
To address these limitations, we explore the application of Quanvolutional
Neural Networks (QNNs), leveraging quantum computing for enhanced feature
extraction. This paper introduces a novel hybrid quantum-classical model for
pneumonia detection using the PneumoniaMNIST dataset. Our approach utilizes a
quanvolutional layer with a parameterized quantum circuit (PQC) to process 2x2
image patches, employing rotational Y-gates for data encoding and entangling
layers to generate non-classical feature representations. These
quantum-extracted features are then fed into a classical neural network for
classification. Experimental results demonstrate that the proposed QNN achieves
a higher validation accuracy of 83.33 percent compared to a comparable
classical CNN which achieves 73.33 percent. This enhanced convergence and
sample efficiency highlight the potential of QNNs for medical image analysis,
particularly in scenarios with limited labeled data. This research lays the
foundation for integrating quantum computing into deep-learning-driven medical
diagnostic systems, offering a computationally efficient alternative to
traditional approaches.

</details>


### [268] [AI-Driven Carbon Monitoring: Transformer-Based Reconstruction of Atmospheric CO2 in Canadian Poultry Regions](https://arxiv.org/abs/2510.23663)
*Padmanabhan Jagannathan Prajesh,Kaliaperumal Ragunath,Miriam Gordon,Bruce Rathgeber,Suresh Neethirajan*

Main category: cs.LG

TL;DR: 提出一种基于小波和时空视觉Transformer的框架ST-ViWT，用于重建加拿大南部高精度、带不确定性量化的XCO2场，有效融合多源数据，在稀疏观测下实现全年连续制图，支持精准碳排放评估与政策制定。


<details>
  <summary>Details</summary>
Motivation: 准确绘制农业景观区柱浓度CO2（XCO2）对指导减排策略至关重要，但卫星观测稀疏且传统方法难以生成连续、可靠的地图。

Method: 提出Spatiotemporal Vision Transformer with Wavelets（ST-ViWT）框架，结合小波时频分析与Transformer注意力机制，融合气象、植被指数、地形和土地覆盖数据，重构OCO-2卫星的XCO2场并量化不确定性。

Result: 在2024年OCO-2数据上R2达0.984，RMSE为0.468 ppm，92.3%的插值预测误差在±1 ppm内；独立TCCON验证显示偏差-0.14 ppm，相关系数0.928，能准确再现夏末下降趋势；14个家禽养殖区分析显示设施密度与XCO2呈中等正相关（r=0.43），高密度区季节振幅更大（9.57 ppm）且夏季变异性增强。

Conclusion: ST-ViWT能生成无缝、高精度、带不确定性的0.25度XCO2地图，优于传统插值和机器学习基线方法，支持将卫星观测与国家清单及精准畜牧平台结合，推动可扩展、透明、空间显式的碳核算、热点识别和政策评估。

Abstract: Accurate mapping of column-averaged CO2 (XCO2) over agricultural landscapes
is essential for guiding emission mitigation strategies. We present a
Spatiotemporal Vision Transformer with Wavelets (ST-ViWT) framework that
reconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 across
southern Canada, emphasizing poultry-intensive regions. The model fuses wavelet
time-frequency representations with transformer attention over meteorology,
vegetation indices, topography, and land cover. On 2024 OCO-2 data, ST-ViWT
attains R2 = 0.984 and RMSE = 0.468 ppm; 92.3 percent of gap-filled predictions
lie within +/-1 ppm. Independent validation with TCCON shows robust
generalization (bias = -0.14 ppm; r = 0.928), including faithful reproduction
of the late-summer drawdown. Spatial analysis across 14 poultry regions reveals
a moderate positive association between facility density and XCO2 (r = 0.43);
high-density areas exhibit larger seasonal amplitudes (9.57 ppm) and enhanced
summer variability. Compared with conventional interpolation and standard
machine-learning baselines, ST-ViWT yields seamless 0.25 degree CO2 surfaces
with explicit uncertainties, enabling year-round coverage despite sparse
observations. The approach supports integration of satellite constraints with
national inventories and precision livestock platforms to benchmark emissions,
refine region-specific factors, and verify interventions. Importantly,
transformer-based Earth observation enables scalable, transparent, spatially
explicit carbon accounting, hotspot prioritization, and policy-relevant
mitigation assessment.

</details>


### [269] [Transformers from Compressed Representations](https://arxiv.org/abs/2510.23665)
*Juan C. Leon Alcazar,Mattia Soldan,Mohammad Saatialsoruji,Alejandro Pardo,Hani Itani,Juan Camilo Perez,Bernard Ghanem*

Main category: cs.LG

TL;DR: 本文提出了TEMPEST方法，利用压缩文件的字节流结构进行高效tokenization和编码，使标准Transformer能直接从压缩数据中学习语义表示，无需解码或处理原始字节，显著降低计算和内存开销，同时在多种数据集上达到与现有方法相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 压缩文件格式广泛用于数据存储与传输，但其在表示学习中的潜力尚未被充分挖掘。传统方法需对数据完全解码或处理原始字节，效率低下，因此需要一种更高效的语义学习方法。

Method: 提出TEMPEST方法，利用压缩文件固有的字节流结构设计新的tokenization和编码策略，使Transformer模型可直接在压缩数据上学习语义表示，避免了原始字节处理和完整解码过程。

Result: 在多个数据集、编码方案和模态上的实验表明，TEMPEST在显著减少token数量的同时，保持了与当前最先进方法相当的分类准确率，并有效降低了内存和计算资源消耗。

Conclusion: TEMPEST成功将压缩数据直接用于语义表示学习，为高效模型训练提供了新路径，在保持性能的同时大幅提升效率，具有广泛的应用前景。

Abstract: Compressed file formats are the corner stone of efficient data storage and
transmission, yet their potential for representation learning remains largely
underexplored. We introduce TEMPEST (TransformErs froM comPressed
rEpreSenTations), a method that exploits the inherent byte-stream structure of
compressed files to design an effective tokenization and encoding strategy. By
leveraging this compact encoding, a standard transformer can directly learn
semantic representations from compressed data streams, bypassing the need for
raw byte-level processing or full media decoding. Our proposal substantially
reduces the number of tokens required for semantic classification, thereby
lowering both computational complexity and memory usage. Through extensive
experiments across diverse datasets, coding schemes, and modalities, we show
that TEMPEST achieves accuracy competitive wit the state-of-the-art while
delivering efficiency gains in memory and compute.

</details>


### [270] [Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization](https://arxiv.org/abs/2510.23667)
*Amin Heyrani Nobari,Lyle Regenwetter,Cyril Picard,Ligong Han,Faez Ahmed*

Main category: cs.LG

TL;DR: 本文提出了Optimize Any Topology (OAT)，一种用于物理感知拓扑优化的通用、快速且与分辨率无关的基础模型框架，能够在任意长宽比、分辨率和边界条件下实现高效布局预测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法受限于固定网格、有限边界条件和后处理优化，难以广泛部署，因此需要一个更通用、灵活且高效的拓扑优化框架。

Method: OAT结合了分辨率和形状无关的自编码器、隐式神经场解码器以及基于大规模OpenTO数据集（包含220万个优化结构）训练的条件潜在扩散模型，直接预测最小柔度布局。

Result: 在四个公开基准和两个未见测试上，OAT相比先前最佳模型将平均柔度降低高达90%，并在64x64到256x256分辨率及高达10:1的长宽比下实现单GPU亚秒级推理。

Conclusion: OAT是一个通用、快速且分辨率无关的拓扑优化框架，显著提升了性能与泛化能力，并通过OpenTO数据集推动生成模型在逆向设计中的进一步研究。

Abstract: Structural topology optimization (TO) is central to engineering design but
remains computationally intensive due to complex physics and hard constraints.
Existing deep-learning methods are limited to fixed square grids, a few
hand-coded boundary conditions, and post-hoc optimization, preventing general
deployment. We introduce Optimize Any Topology (OAT), a foundation-model
framework that directly predicts minimum-compliance layouts for arbitrary
aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines
a resolution- and shape-agnostic autoencoder with an implicit neural-field
decoder and a conditional latent-diffusion model trained on OpenTO, a new
corpus of 2.2 million optimized structures covering 2 million unique
boundary-condition configurations. On four public benchmarks and two
challenging unseen tests, OAT lowers mean compliance up to 90% relative to the
best prior models and delivers sub-1 second inference on a single GPU across
resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These
results establish OAT as a general, fast, and resolution-free framework for
physics-aware topology optimization and provide a large-scale dataset to spur
further research in generative modeling for inverse design. Code & data can be
found at https://github.com/ahnobari/OptimizeAnyTopology.

</details>


### [271] [Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems](https://arxiv.org/abs/2510.23668)
*Fujiang Yuan,Yangrui Fan,Xiaohuan Bing,Zhen Tian,Chunhong Yuan,Yankang Li*

Main category: cs.LG

TL;DR: 提出了一种基于STL分解的LSTM-ARIMA-XGBoost混合模型，用于交通流预测，通过分解时间序列并分别建模趋势、季节性和残差成分，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 单一模型难以捕捉交通流数据中复杂的非线性及多尺度时序模式，因此需要更有效的建模方法。

Method: 采用STL将原始时间序列分解为趋势、季节和残差成分，分别用LSTM、ARIMA和XGBoost进行建模，并通过乘法方式融合各子模型预测结果。

Result: 在纽约市交叉口的真实数据上验证，该混合模型在MAE、RMSE和R²指标上均显著优于单独的LSTM、ARIMA和XGBoost模型。

Conclusion: 分解驱动的混合框架能有效分离时序特征，提升交通流预测的准确性、可解释性和鲁棒性。

Abstract: Accurate traffic flow forecasting is essential for intelligent transportation
systems and urban traffic management. However, single model approaches often
fail to capture the complex, nonlinear, and multi scale temporal patterns in
traffic flow data. This study proposes a decomposition driven hybrid framework
that integrates Seasonal Trend decomposition using Loess (STL) with three
complementary predictive models. STL first decomposes the original time series
into trend, seasonal, and residual components. Then, a Long Short Term Memory
(LSTM) network models long term trends, an Autoregressive Integrated Moving
Average (ARIMA) model captures seasonal periodicity, and an Extreme Gradient
Boosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The
final forecast is obtained through multiplicative integration of the sub model
predictions. Using 998 traffic flow records from a New York City intersection
between November and December 2015, results show that the LSTM ARIMA XGBoost
hybrid model significantly outperforms standalone models including LSTM, ARIMA,
and XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy
effectively isolates temporal characteristics, allowing each model to
specialize, thereby improving prediction accuracy, interpretability, and
robustness.

</details>


### [272] [Sparsity and Superposition in Mixture of Experts](https://arxiv.org/abs/2510.23671)
*Marmik Chaudhari,Jeremi Nuer,Rome Thorstenson*

Main category: cs.LG

TL;DR: 本文研究了MoE模型与稠密网络在机制上的差异，提出网络稀疏性（active专家与总专家之比）是刻画MoE的关键因素，并发现更高的网络稀疏性带来更强的单义性（monosemanticity），进而提出基于特征专门化的专家定义，表明适当初始化下专家会自然围绕一致的特征组合组织，有助于提升模型可解释性而不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE模型在扩展大语言模型中起核心作用，但其与稠密网络的机制差异尚不清楚。已有研究用“叠加”（superposition）解释稠密模型如何表示多于维度数的特征，但这不适用于MoE模型，因此需要新的理论框架来理解MoE的表示机制。

Method: 作者引入新的度量方法来评估专家间的叠加现象，分析特征稀疏性和重要性对相变的影响，并提出以网络稀疏性作为MoE的核心表征变量；同时定义基于单义性特征表示的专家专门化，而非依赖负载均衡。

Result: 发现特征稀疏性和重要性不会在MoE中引起不连续的相变，而网络稀疏性与更高的单义性正相关；专家在适当初始化下会自然形成对连贯特征组合的专门化。

Conclusion: 网络稀疏性是理解MoE模型表示机制的关键，高稀疏性有助于提升模型的可解释性，且无需牺牲性能，挑战了可解释性与模型能力不可兼得的传统观点。

Abstract: Mixture of Experts (MoE) models have become central to scaling large language
models, yet their mechanistic differences from dense networks remain poorly
understood. Previous work has explored how dense models use
\textit{superposition} to represent more features than dimensions, and how
superposition is a function of feature sparsity and feature importance. MoE
models cannot be explained mechanistically through the same lens. We find that
neither feature sparsity nor feature importance cause discontinuous phase
changes, and that network sparsity (the ratio of active to total experts)
better characterizes MoEs. We develop new metrics for measuring superposition
across experts. Our findings demonstrate that models with greater network
sparsity exhibit greater \emph{monosemanticity}. We propose a new definition of
expert specialization based on monosemantic feature representation rather than
load balancing, showing that experts naturally organize around coherent feature
combinations when initialized appropriately. These results suggest that network
sparsity in MoEs may enable more interpretable models without sacrificing
performance, challenging the common assumption that interpretability and
capability are fundamentally at odds.

</details>


### [273] [DBLoss: Decomposition-based Loss Function for Time Series Forecasting](https://arxiv.org/abs/2510.23672)
*Xiangfei Qiu,Xingjian Wu,Hanyin Cheng,Xvyuan Liu,Chenjuan Guo,Jilin Hu,Bin Yang*

Main category: cs.LG

TL;DR: 提出了一种新的基于分解的损失函数DBLoss，用于提升时间序列预测中对季节性和趋势的建模能力，显著提高了现有模型在多个真实世界数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MSE损失函数难以准确捕捉预测范围内的季节性和趋势，即使使用了分解模块。

Method: 利用指数移动平均将时间序列分解为季节性和趋势成分，分别计算两者的损失并加权求和，形成DBLoss。该损失函数可与任何深度学习预测模型结合使用。

Result: 在多个真实世界数据集上，DBLoss显著提升了先进模型的预测性能，验证了其有效性。

Conclusion: DBLoss是一种简单而有效的通用损失函数，为时间序列预测中的损失函数设计提供了新思路。

Abstract: Time series forecasting holds significant value in various domains such as
economics, traffic, energy, and AIOps, as accurate predictions facilitate
informed decision-making. However, the existing Mean Squared Error (MSE) loss
function sometimes fails to accurately capture the seasonality or trend within
the forecasting horizon, even when decomposition modules are used in the
forward propagation to model the trend and seasonality separately. To address
these challenges, we propose a simple yet effective Decomposition-Based Loss
function called DBLoss. This method uses exponential moving averages to
decompose the time series into seasonal and trend components within the
forecasting horizon, and then calculates the loss for each of these components
separately, followed by weighting them. As a general loss function, DBLoss can
be combined with any deep learning forecasting model. Extensive experiments
demonstrate that DBLoss significantly improves the performance of
state-of-the-art models across diverse real-world datasets and provides a new
perspective on the design of time series loss functions.

</details>


### [274] [Informed Initialization for Bayesian Optimization and Active Learning](https://arxiv.org/abs/2510.23681)
*Carl Hvarfner,David Eriksson,Eytan Bakshy,Max Balandat*

Main category: cs.LG

TL;DR: 提出了一种新的贝叶斯优化初始化策略HIPE，通过信息论原则平衡预测不确定性的减少和超参数学习，在少量样本设置下显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在少样本贝叶斯优化中，初始化对代理模型的预测质量和后续优化至关重要，但现有方法忽视了空间填充设计可能不利于减少预测不确定性以及高效超参数学习的需求。

Method: 提出Hyperparameter-Informed Predictive Exploration (HIPE) 作为新采集策略，推导出其在高斯过程下的闭式表达，结合信息论原则平衡预测不确定性降低与超参数学习。

Result: 实验表明，HIPE在预测准确性、超参数识别和后续优化性能方面均优于标准初始化方法，尤其在大批次、少样本场景下表现突出。

Conclusion: HIPE通过兼顾超参数学习和预测不确定性减少，显著提升了贝叶斯优化在少样本设置下的性能，适用于许多实际应用场景。

Abstract: Bayesian Optimization is a widely used method for optimizing expensive
black-box functions, relying on probabilistic surrogate models such as Gaussian
Processes. The quality of the surrogate model is crucial for good optimization
performance, especially in the few-shot setting where only a small number of
batches of points can be evaluated. In this setting, the initialization plays a
critical role in shaping the surrogate's predictive quality and guiding
subsequent optimization. Despite this, practitioners typically rely on
(quasi-)random designs to cover the input space. However, such approaches
neglect two key factors: (a) space-filling designs may not be desirable to
reduce predictive uncertainty, and (b) efficient hyperparameter learning during
initialization is essential for high-quality prediction, which may conflict
with space-filling designs. To address these limitations, we propose
Hyperparameter-Informed Predictive Exploration (HIPE), a novel acquisition
strategy that balances predictive uncertainty reduction with hyperparameter
learning using information-theoretic principles. We derive a closed-form
expression for HIPE in the Gaussian Process setting and demonstrate its
effectiveness through extensive experiments in active learning and few-shot BO.
Our results show that HIPE outperforms standard initialization strategies in
terms of predictive accuracy, hyperparameter identification, and subsequent
optimization performance, particularly in large-batch, few-shot settings
relevant to many real-world Bayesian Optimization applications.

</details>


### [275] [Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents](https://arxiv.org/abs/2510.23682)
*Gokturk Aytug Akarlar*

Main category: cs.LG

TL;DR: 提出Chimera，一种结合神经、符号和因果组件的架构，显著提升大语言模型代理在高风险环境中的鲁棒性和收益表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险领域应用时因提示敏感性而表现出严重脆弱性，需通过架构设计提升其决策可靠性。

Method: 构建Chimera架构，集成大语言模型策略模块、形式化验证的符号约束引擎和因果反事实推理模块，并在真实电商环境中进行52周仿真测试。

Result: 相比纯LLM或仅加符号约束的模型，Chimera在利润（最高达196万美元）和品牌信任度（提升最多20.86%）上均显著领先，且零约束违规。

Conclusion: 架构设计而非提示工程决定了自主代理在生产环境中的可靠性，Chimera实现了提示无关的鲁棒决策。

Abstract: Large language models show promise as autonomous decision-making agents, yet
their deployment in high-stakes domains remains fraught with risk. Without
architectural safeguards, LLM agents exhibit catastrophic brittleness:
identical capabilities produce wildly different outcomes depending solely on
prompt framing. We present Chimera, a neuro-symbolic-causal architecture that
integrates three complementary components - an LLM strategist, a formally
verified symbolic constraint engine, and a causal inference module for
counterfactual reasoning. We benchmark Chimera against baseline architectures
(LLM-only, LLM with symbolic constraints) across 52-week simulations in a
realistic e-commerce environment featuring price elasticity, trust dynamics,
and seasonal demand. Under organizational biases toward either volume or margin
optimization, LLM-only agents fail catastrophically (total loss of \$99K in
volume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding
symbolic constraints prevents disasters but achieves only 43-87% of Chimera's
profit. Chimera consistently delivers the highest returns (\$1.52M and \$1.96M
respectively, some cases +\$2.2M) while improving brand trust (+1.8% and
+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+
formal verification proves zero constraint violations across all scenarios.
These results establish that architectural design not prompt engineering
determines the reliability of autonomous agents in production environments. We
provide open-source implementations and interactive demonstrations for
reproducibility.

</details>


### [276] [Parallel BiLSTM-Transformer networks for forecasting chaotic dynamics](https://arxiv.org/abs/2510.23685)
*Junwen Ma,Mingyu Ge,Yisen Wang,Yong Zhang,Weicheng Fu*

Main category: cs.LG

TL;DR: 提出了一种结合Transformer和BiLSTM的并行预测框架，用于提升混沌系统时间序列预测的准确性，在洛伦兹系统中验证了其在自主演化预测和未观测变量推断任务上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以同时捕捉混沌时间序列的局部特征和全局依赖关系，导致预测精度受限。

Method: 构建双分支并行架构，其中Transformer分支捕获长距离依赖，BiLSTM分支提取局部时序特征，并通过专门的特征融合层整合两种表征。

Result: 在洛伦兹系统的两个典型任务（自主演化预测和未观测变量推断）中，该模型均优于单一分支结构，表现出更高的预测准确性和稳定性。

Conclusion: 所提出的Transformer-BiLSTM混合框架能有效融合局部与全局特征，显著提升混沌系统预测的鲁棒性与性能。

Abstract: The nonlinear nature of chaotic systems results in extreme sensitivity to
initial conditions and highly intricate dynamical behaviors, posing fundamental
challenges for accurately predicting their evolution. To overcome the
limitation that conventional approaches fail to capture both local features and
global dependencies in chaotic time series simultaneously, this study proposes
a parallel predictive framework integrating Transformer and Bidirectional Long
Short-Term Memory (BiLSTM) networks. The hybrid model employs a dual-branch
architecture, where the Transformer branch mainly captures long-range
dependencies while the BiLSTM branch focuses on extracting local temporal
features. The complementary representations from the two branches are fused in
a dedicated feature-fusion layer to enhance predictive accuracy. As
illustrating examples, the model's performance is systematically evaluated on
two representative tasks in the Lorenz system. The first is autonomous
evolution prediction, in which the model recursively extrapolates system
trajectories from the time-delay embeddings of the state vector to evaluate
long-term tracking accuracy and stability. The second is inference of
unmeasured variable, where the model reconstructs the unobserved states from
the time-delay embeddings of partial observations to assess its
state-completion capability. The results consistently indicate that the
proposed hybrid framework outperforms both single-branch architectures across
tasks, demonstrating its robustness and effectiveness in chaotic system
prediction.

</details>


### [277] [On the Societal Impact of Machine Learning](https://arxiv.org/abs/2510.23693)
*Joachim Baumann*

Main category: cs.LG

TL;DR: 该论文研究了机器学习对社会的影响，提出了衡量公平性、分解系统偏差和减少算法歧视的方法，旨在使机器学习的发展符合社会价值观。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习在关键决策中日益广泛应用，但常缺乏公平性考虑，可能导致歧视性后果，因此需要系统性方法来评估和改善其社会影响。

Method: 提出了一套方法，包括更合适的公平性度量、系统分解以预测偏见动态，以及在保持系统效用的同时减少算法歧视的有效干预措施。

Result: 实现了对机器学习系统中公平性的更好测量与控制，能够有效识别和减轻算法歧视，同时维持系统性能。

Conclusion: 随着机器学习（包括生成式AI）越来越深入社会，需持续应对公平性挑战，未来的研究应致力于使技术发展与社会价值相一致。

Abstract: This PhD thesis investigates the societal impact of machine learning (ML). ML
increasingly informs consequential decisions and recommendations, significantly
affecting many aspects of our lives. As these data-driven systems are often
developed without explicit fairness considerations, they carry the risk of
discriminatory effects. The contributions in this thesis enable more
appropriate measurement of fairness in ML systems, systematic decomposition of
ML systems to anticipate bias dynamics, and effective interventions that reduce
algorithmic discrimination while maintaining system utility. I conclude by
discussing ongoing challenges and future research directions as ML systems,
including generative artificial intelligence, become increasingly integrated
into society. This work offers a foundation for ensuring that ML's societal
impact aligns with broader social values.

</details>


### [278] [MUStReason: A Benchmark for Diagnosing Pragmatic Reasoning in Video-LMs for Multimodal Sarcasm Detection](https://arxiv.org/abs/2510.23727)
*Anisha Saha,Varsha Suresh,Timothy Hospedales,Vera Demberg*

Main category: cs.LG

TL;DR: 本文提出了MUStReason基准和PragCoT框架，用于提升视频语言模型在多模态讽刺检测中的表现，通过分离感知与推理过程来更好地捕捉言外之意。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型在讽刺检测等复杂任务上表现不佳，难以跨模态识别关键线索并进行语用推理。

Method: 构建包含模态特定线索和推理步骤标注的诊断性基准MUStReason，并提出PragCoT框架引导模型关注隐含意图而非字面意思。

Result: 在讽刺分类性能上进行了定量与定性评估，验证了PragCoT在提升模型推理能力方面的有效性。

Conclusion: 通过显式建模相关感知线索与分步推理，可增强视频语言模型对讽刺等需深层语用理解任务的处理能力。

Abstract: Sarcasm is a specific type of irony which involves discerning what is said
from what is meant. Detecting sarcasm depends not only on the literal content
of an utterance but also on non-verbal cues such as speaker's tonality, facial
expressions and conversational context. However, current multimodal models
struggle with complex tasks like sarcasm detection, which require identifying
relevant cues across modalities and pragmatically reasoning over them to infer
the speaker's intention. To explore these limitations in VideoLMs, we introduce
MUStReason, a diagnostic benchmark enriched with annotations of
modality-specific relevant cues and underlying reasoning steps to identify
sarcastic intent. In addition to benchmarking sarcasm classification
performance in VideoLMs, using MUStReason we quantitatively and qualitatively
evaluate the generated reasoning by disentangling the problem into perception
and reasoning, we propose PragCoT, a framework that steers VideoLMs to focus on
implied intentions over literal meaning, a property core to detecting sarcasm.

</details>


### [279] [Debiasing Reward Models by Representation Learning with Guarantees](https://arxiv.org/abs/2510.23751)
*Ignavier Ng,Patrick Blöbaum,Siddharth Bhandari,Kun Zhang,Shiva Kasiviswanathan*

Main category: cs.LG

TL;DR: 提出一种新框架，通过变分推断分离并消除奖励模型中的虚假相关性，提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（如基于人类反馈的强化学习）易受响应长度、奉承、偏见等虚假相关性影响，导致奖励模型偏差，需有效缓解此类问题。

Method: 构建数据生成过程模型，假设观测数据由虚假和非虚假潜在变量共同生成；利用变分推断恢复非虚假变量，并用于训练奖励模型。

Result: 在合成和真实数据集上的实验表明，该方法能有效减轻虚假相关性问题，得到更鲁棒的奖励模型。

Conclusion: 该框架能在理论上识别出反映真实偏好的潜在因素，并有效消除多种虚假关联，为构建更可靠的对齐模型提供了新思路。

Abstract: Recent alignment techniques, such as reinforcement learning from human
feedback, have been widely adopted to align large language models with human
preferences by learning and leveraging reward models. In practice, these models
often exploit spurious correlations, involving, e.g., response length,
discrimination, sycophancy, and conceptual bias, which is a problem that has
received increasing attention. In this work, we propose a principled framework
that mitigates these biases in reward models while preserving the underlying
factors that reflect intended preferences. We first provide a formulation of
the data-generating process, assuming that the observed data (e.g., text) is
generated from both spurious and non-spurious latent variables. We show that,
interestingly, these non-spurious latent variables can be theoretically
identified from data, regardless of whether a surrogate for the spurious latent
variables is available. This further inspires a practical method that uses
variational inference to recover these variables and leverages them to train
reward models. Experiments on synthetic and real-world datasets demonstrate
that our method effectively mitigates spurious correlation issues and yields
more robust reward models.

</details>


### [280] [Learning Parameterized Skills from Demonstrations](https://arxiv.org/abs/2510.24095)
*Vedant Gupta,Haotian Fu,Calvin Luo,Yiding Jiang,George Konidaris*

Main category: cs.LG

TL;DR: DEPS是一种从专家演示中发现参数化技能的端到端算法，通过联合学习参数化技能策略和元策略，提升在未见任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决潜在变量模型中的退化问题，学习具有时间延伸性、语义明确且可适应的参数化技能。

Method: 结合时间变分推断和信息理论正则化方法，联合训练参数化技能策略与选择离散技能及连续参数的元策略。

Result: 在LIBERO和MetaWorld基准上优于多任务和技能学习基线，能发现可解释的参数化技能（如定义抓取位置的抓取技能）。

Conclusion: DEPS能有效从多任务专家演示中学习参数化技能，显著提升跨任务泛化性能，并具备良好的可解释性。

Abstract: We present DEPS, an end-to-end algorithm for discovering parameterized skills
from expert demonstrations. Our method learns parameterized skill policies
jointly with a meta-policy that selects the appropriate discrete skill and
continuous parameters at each timestep. Using a combination of temporal
variational inference and information-theoretic regularization methods, we
address the challenge of degeneracy common in latent variable models, ensuring
that the learned skills are temporally extended, semantically meaningful, and
adaptable. We empirically show that learning parameterized skills from
multitask expert demonstrations significantly improves generalization to unseen
tasks. Our method outperforms multitask as well as skill learning baselines on
both LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers
interpretable parameterized skills, such as an object grasping skill whose
continuous arguments define the grasp location.

</details>


### [281] [Explaining Robustness to Catastrophic Forgetting Through Incremental Concept Formation](https://arxiv.org/abs/2510.23756)
*Nicki Barari,Edward Kim,Christopher MacLellan*

Main category: cs.LG

TL;DR: 本文研究了Cobweb/4V模型在持续学习中抵抗灾难性遗忘的机制，提出了三个假设：自适应结构重组、稀疏更新和基于充分统计量的信息论学习，并通过与神经网络基线的比较验证了这些因素的有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨Cobweb/4V在视觉领域中对灾难性遗忘具有鲁棒性的原因，分析其背后的关键机制。

Method: 提出并验证三个假设：(1) 自适应结构重组增强知识保持；(2) 稀疏且选择性更新减少干扰；(3) 基于充分统计量的信息论学习优于基于梯度的反向传播。使用MNIST、Fashion-MNIST、MedMNIST和CIFAR-10数据集进行实验，并引入CobwebNN作为神经网络基线进行对比。

Result: 实验表明，自适应重构提升了学习可塑性，稀疏更新有效减轻了干扰，信息论学习能在不重访历史数据的情况下保留先前知识。Cobweb/4V在多种数据集上表现出更强的稳定性。

Conclusion: 自适应结构、稀疏更新和信息论学习是缓解灾难性遗忘的关键机制，概念化、基于信息论的方法在构建稳定且自适应的持续学习系统中具有潜力。

Abstract: Catastrophic forgetting remains a central challenge in continual learning,
where models are required to integrate new knowledge over time without losing
what they have previously learned. In prior work, we introduced Cobweb/4V, a
hierarchical concept formation model that exhibited robustness to catastrophic
forgetting in visual domains. Motivated by this robustness, we examine three
hypotheses regarding the factors that contribute to such stability: (1)
adaptive structural reorganization enhances knowledge retention, (2) sparse and
selective updates reduce interference, and (3) information-theoretic learning
based on sufficiency statistics provides advantages over gradient-based
backpropagation. To test these hypotheses, we compare Cobweb/4V with neural
baselines, including CobwebNN, a neural implementation of the Cobweb framework
introduced in this work. Experiments on datasets of varying complexity (MNIST,
Fashion-MNIST, MedMNIST, and CIFAR-10) show that adaptive restructuring
enhances learning plasticity, sparse updates help mitigate interference, and
the information-theoretic learning process preserves prior knowledge without
revisiting past data. Together, these findings provide insight into mechanisms
that can mitigate catastrophic forgetting and highlight the potential of
concept-based, information-theoretic approaches for building stable and
adaptive continual learning systems.

</details>


### [282] [Relaxed Sequence Sampling for Diverse Protein Design](https://arxiv.org/abs/2510.23786)
*Joohwan Ko,Aristofanis Rontogiannis,Yih-En Andrew Ban,Axel Elaldi,Nicholas Franklin*

Main category: cs.LG

TL;DR: 提出了一种基于马尔可夫链蒙特卡洛的蛋白设计方法RSS，结合AlphaFold2和ESM2，在保持计算成本相当的同时显著提升设计性和结构多样性。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白设计方法如RSO依赖单路径梯度下降且忽略序列空间约束，限制了设计的多样性和可设计性。

Method: 提出Relaxed Sequence Sampling (RSS)，在连续logit空间中结合梯度引导探索与蛋白语言模型驱动的跳跃，通过AlphaFold2结构目标与ESM2序列先验耦合的能量函数进行优化。

Result: 在虚拟蛋白结合体设计任务中，RSS相比RSO基线产生5倍更多可设计结构和2-3倍更高的结构多样性，计算成本相当。

Conclusion: RSS是一种高效、有理论依据的蛋白设计框架，能更好平衡结构准确性和生物学合理性，显著提升设计性能。

Abstract: Protein design using structure prediction models such as AlphaFold2 has shown
remarkable success, but existing approaches like relaxed sequence optimization
(RSO) rely on single-path gradient descent and ignore sequence-space
constraints, limiting diversity and designability. We introduce Relaxed
Sequence Sampling (RSS), a Markov chain Monte Carlo (MCMC) framework that
integrates structural and evolutionary information for protein design. RSS
operates in continuous logit space, combining gradient-guided exploration with
protein language model-informed jumps. Its energy function couples
AlphaFold2-derived structural objectives with ESM2-derived sequence priors,
balancing accuracy and biological plausibility. In an in silico protein binder
design task, RSS produces 5$\times$ more designable structures and 2-3$\times$
greater structural diversity than RSO baselines, at equal computational cost.
These results highlight RSS as a principled approach for efficiently exploring
the protein design landscape.

</details>


### [283] [Revealing the Potential of Learnable Perturbation Ensemble Forecast Model for Tropical Cyclone Prediction](https://arxiv.org/abs/2510.23794)
*Jun Liu,Tao Zhou,Jiarui Li,Xiaohui Zhong,Peng Zhang,Jie Feng,Lei Chen,Hao Li*

Main category: cs.LG

TL;DR: FuXi-ENS是一种基于AI的新型集合预报方法，通过可学习的扰动方案生成集合，在热带气旋（TC）相关物理变量和路径预报方面优于ECMWF-ENS，且集合离散度更小，但在强度预报上仍低估。动力和热力分析表明，FuXi-ENS能更好地捕捉大尺度环流和TC暖核周围的湿湍流能量分布。


<details>
  <summary>Details</summary>
Motivation: 传统集合预报系统受限于高计算成本和对大气非线性的表征能力不足，尤其是在热带气旋这类高度破坏性且不确定性强的天气系统中，亟需更高效的预报方法。

Method: 提出并应用了FuXi-ENS，采用可学习的扰动方案进行集合生成，并与ECMWF-ENS在2018年全球90个热带气旋事件中系统比较，评估其在TC物理变量、路径、强度及相关的动力热力学场的预报性能。

Result: FuXi-ENS在TC相关物理变量和路径预报上表现更优，集合离散度更小；在动力热力学结构上更准确地集中了湿湍流能量于TC暖核区域，而ECMWF-ENS分布较分散；但FuXi-ENS仍存在强度低估问题。

Conclusion: 可学习扰动方案有助于提升热带气旋的预报技巧，展示了AI在极端天气集合预报中的潜力，为未来AI驱动的气象预测提供了新方向。

Abstract: Tropical cyclones (TCs) are highly destructive and inherently uncertain
weather systems. Ensemble forecasting helps quantify these uncertainties, yet
traditional systems are constrained by high computational costs and limited
capability to fully represent atmospheric nonlinearity. FuXi-ENS introduces a
learnable perturbation scheme for ensemble generation, representing a novel
AI-based forecasting paradigm. Here, we systematically compare FuXi-ENS with
ECMWF-ENS using all 90 global TCs in 2018, examining their performance in
TC-related physical variables, track and intensity forecasts, and the
associated dynamical and thermodynamical fields. FuXi-ENS demonstrates clear
advantages in predicting TC-related physical variables, and achieves more
accurate track forecasts with reduced ensemble spread, though it still
underestimates intensity relative to observations. Further dynamical and
thermodynamical analyses reveal that FuXi-ENS better captures large-scale
circulation, with moisture turbulent energy more tightly concentrated around
the TC warm core, whereas ECMWF-ENS exhibits a more dispersed distribution.
These findings highlight the potential of learnable perturbations to improve TC
forecasting skill and provide valuable insights for advancing AI-based ensemble
prediction of extreme weather events that have significant societal impacts.

</details>


### [284] [Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders](https://arxiv.org/abs/2510.23802)
*Nathan Paek,Yongyi Zang,Qihui Yang,Randal Leistikow*

Main category: cs.LG

TL;DR: 提出了一种通过稀疏自编码器（SAE）与线性映射结合的框架，将音频生成模型的潜在表示映射到可解释的声学概念（如音高、振幅、音色），实现了对AI音乐生成过程的可控操作与分析。


<details>
  <summary>Details</summary>
Motivation: 音频数据密集且难以压缩后保留语义，现有方法在自动特征表征方面有限，因此需要一种能解释音频生成模型中潜在表示的方法。

Method: 在音频自编码器的潜在空间上训练稀疏自编码器（SAE），然后学习SAE特征到离散化声学属性（音高、振幅、音色）的线性映射，从而实现语义层面的解析与控制。

Result: 该方法在连续（DiffRhythm-VAE）和离散（EnCodec, WavTokenizer）音频潜在空间上均有效，并用于分析前沿文本到音乐模型DiffRhythm，揭示了音高、音色和响度在生成过程中的演化。

Conclusion: 所提框架能够有效解析音频生成模型中的声学属性演化，支持可控生成与模型理解，且具备扩展至视觉生成模型的潜力。

Abstract: While sparse autoencoders (SAEs) successfully extract interpretable features
from language models, applying them to audio generation faces unique
challenges: audio's dense nature requires compression that obscures semantic
meaning, and automatic feature characterization remains limited. We propose a
framework for interpreting audio generative models by mapping their latent
representations to human-interpretable acoustic concepts. We train SAEs on
audio autoencoder latents, then learn linear mappings from SAE features to
discretized acoustic properties (pitch, amplitude, and timbre). This enables
both controllable manipulation and analysis of the AI music generation process,
revealing how acoustic properties emerge during synthesis. We validate our
approach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer)
audio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music
model, to demonstrate how pitch, timbre, and loudness evolve throughout
generation. While our work is only done on audio modality, our framework can be
extended to interpretable analysis of visual latent space generation models.

</details>


### [285] [How do simple rotations affect the implicit bias of Adam?](https://arxiv.org/abs/2510.23804)
*Adela DePavia,Vasileios Charisopoulos,Rebecca Willett*

Main category: cs.LG

TL;DR: Adam等自适应梯度方法在机器学习中广泛应用，但其对模型泛化能力的影响尚不明确；研究表明，尽管Adam在二分类中表现出“丰富性偏好”，但其对特征空间正交变换敏感，可能导致性能反转，而通过正交重参数化方法可恢复其优势。


<details>
  <summary>Details</summary>
Motivation: 理解自适应梯度方法（如Adam）相对于梯度下降在模型泛化上的差异，尤其是其在非线性决策边界学习中的表现及其对数据旋转的敏感性问题。

Method: 分析Adam在不同数据旋转下的决策边界收敛行为，并引入一种正交重参数化方法以实现对数据旋转的一阶优化方法的等变性。

Result: 发现小幅度的数据旋转可使Adam失去其丰富性偏好，收敛到比梯度下降更远离Bayes最优边界的线性边界；重参数化方法能有效恢复Adam的非线性偏好。

Conclusion: Adam的性能受数据坐标系影响显著，通过适当的重参数化可增强其鲁棒性并保留其在复杂决策边界学习中的优势。

Abstract: Adaptive gradient methods such as Adam and Adagrad are widely used in machine
learning, yet their effect on the generalization of learned models -- relative
to methods like gradient descent -- remains poorly understood. Prior work on
binary classification suggests that Adam exhibits a ``richness bias,'' which
can help it learn nonlinear decision boundaries closer to the Bayes-optimal
decision boundary relative to gradient descent. However, the coordinate-wise
preconditioning scheme employed by Adam renders the overall method sensitive to
orthogonal transformations of feature space. We show that this sensitivity can
manifest as a reversal of Adam's competitive advantage: even small rotations of
the underlying data distribution can make Adam forfeit its richness bias and
converge to a linear decision boundary that is farther from the Bayes-optimal
decision boundary than the one learned by gradient descent. To alleviate this
issue, we show that a recently proposed reparameterization method -- which
applies an orthogonal transformation to the optimization objective -- endows
any first-order method with equivariance to data rotations, and we empirically
demonstrate its ability to restore Adam's bias towards rich decision
boundaries.

</details>


### [286] [A Physics-informed Multi-resolution Neural Operator](https://arxiv.org/abs/2510.23810)
*Sumanta Roy,Bahador Bahmani,Ioannis G. Kevrekidis,Michael D. Shields*

Main category: cs.LG

TL;DR: 提出一种无需数据的物理信息算子学习方法，通过扩展RINO框架解决训练数据不足和多分辨率离散化问题。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习依赖大量高质量、高保真数据，且数据常因采样分辨率不一而难以获取，限制了在实际工程中的应用。

Method: 将输入函数投影到由预训练基函数构建的潜在嵌入空间，使用MLP近似PDE相关算子，并结合物理空间中的有限差分法施加PDE约束。

Result: 在多个多分辨率数据的数值实验中验证了方法的有效性，能够在不同粗细离散化的输入下准确预测解。

Conclusion: 该方法成功实现了无需高保真训练数据的算子学习，兼具对不规则、多分辨率输入的鲁棒性，适用于现实工程场景。

Abstract: The predictive accuracy of operator learning frameworks depends on the
quality and quantity of available training data (input-output function pairs),
often requiring substantial amounts of high-fidelity data, which can be
challenging to obtain in some real-world engineering applications. These
datasets may be unevenly discretized from one realization to another, with the
grid resolution varying across samples. In this study, we introduce a
physics-informed operator learning approach by extending the Resolution
Independent Neural Operator (RINO) framework to a fully data-free setup,
addressing both challenges simultaneously. Here, the arbitrarily (but
sufficiently finely) discretized input functions are projected onto a latent
embedding space (i.e., a vector space of finite dimensions), using pre-trained
basis functions. The operator associated with the underlying partial
differential equations (PDEs) is then approximated by a simple multi-layer
perceptron (MLP), which takes as input a latent code along with spatiotemporal
coordinates to produce the solution in the physical space. The PDEs are
enforced via a finite difference solver in the physical space. The validation
and performance of the proposed method are benchmarked on several numerical
examples with multi-resolution data, where input functions are sampled at
varying resolutions, including both coarse and fine discretizations.

</details>


### [287] [Sample-efficient and Scalable Exploration in Continuous-Time RL](https://arxiv.org/abs/2510.24482)
*Klemens Iten,Lenart Treven,Bhavya Sukhija,Florian Dörfler,Andreas Krause*

Main category: cs.LG

TL;DR: 本文提出了一种用于连续时间强化学习的算法COMBRL，利用高斯过程和贝叶斯神经网络建模非线性常微分方程，并通过最大化外部奖励与模型认知不确定性的加权和实现高效、可扩展的学习。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的控制系统通常是连续时间的，但大多数强化学习算法针对离散时间设计，缺乏对连续时间动态的有效处理方法。

Method: 使用高斯过程和贝叶斯神经网络等概率模型学习带有不确定性估计的连续时间系统动力学（非线性ODE），并通过贪婪策略最大化外部奖励与模型认知不确定性的加权和。

Result: COMBRL在奖励驱动设定下实现了次线性遗憾，在无监督强化学习中提供了样本复杂度上界；实验表明其在多个深度强化学习任务中比现有方法更具可扩展性和样本效率。

Conclusion: COMBRL是一种可扩展且样本高效的连续时间基于模型的强化学习方法，在标准和无监督设置下均优于基线方法。

Abstract: Reinforcement learning algorithms are typically designed for discrete-time
dynamics, even though the underlying real-world control systems are often
continuous in time. In this paper, we study the problem of continuous-time
reinforcement learning, where the unknown system dynamics are represented using
nonlinear ordinary differential equations (ODEs). We leverage probabilistic
models, such as Gaussian processes and Bayesian neural networks, to learn an
uncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily
maximizes a weighted sum of the extrinsic reward and model epistemic
uncertainty. This yields a scalable and sample-efficient approach to
continuous-time model-based RL. We show that COMBRL achieves sublinear regret
in the reward-driven setting, and in the unsupervised RL setting (i.e., without
extrinsic rewards), we provide a sample complexity bound. In our experiments,
we evaluate COMBRL in both standard and unsupervised RL settings and
demonstrate that it scales better, is more sample-efficient than prior methods,
and outperforms baselines across several deep RL tasks.

</details>


### [288] [Combining SHAP and Causal Analysis for Interpretable Fault Detection in Industrial Processes](https://arxiv.org/abs/2510.23817)
*Pedro Cortes dos Santos,Matheus Becali Rocha,Renato A Krohling*

Main category: cs.LG

TL;DR: 提出了一种结合SHAP和因果分析的故障检测框架，用于提升工业过程中的检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在复杂工业过程中存在性能和可解释性不足的问题，需要更透明、可理解的故障检测方法。

Method: 使用SHAP识别关键特征，并结合多种算法生成的有向无环图（DAGs）进行因果分析，以揭示故障传播机制。

Result: SHAP与因果结构结果高度一致，突出冷却和分离系统等关键部件在故障发展中的作用，提升了检测精度和可解释性。

Conclusion: 该方法将预测能力与因果理解相结合，为复杂工业系统的故障检测提供了更智能、更可解释的工具。

Abstract: Industrial processes generate complex data that challenge fault detection
systems, often yielding opaque or underwhelming results despite advanced
machine learning techniques. This study tackles such difficulties using the
Tennessee Eastman Process, a well-established benchmark known for its intricate
dynamics, to develop an innovative fault detection framework. Initial attempts
with standard models revealed limitations in both performance and
interpretability, prompting a shift toward a more tractable approach. By
employing SHAP (SHapley Additive exPlanations), we transform the problem into a
more manageable and transparent form, pinpointing the most critical process
features driving fault predictions. This reduction in complexity unlocks the
ability to apply causal analysis through Directed Acyclic Graphs, generated by
multiple algorithms, to uncover the underlying mechanisms of fault propagation.
The resulting causal structures align strikingly with SHAP findings,
consistently highlighting key process elements-like cooling and separation
systems-as pivotal to fault development. Together, these methods not only
enhance detection accuracy but also provide operators with clear, actionable
insights into fault origins, a synergy that, to our knowledge, has not been
previously explored in this context. This dual approach bridges predictive
power with causal understanding, offering a robust tool for monitoring complex
manufacturing environments and paving the way for smarter, more interpretable
fault detection in industrial systems.

</details>


### [289] [ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank Fine-Tuning](https://arxiv.org/abs/2510.23818)
*Yilang Zhang,Xiaodong Yang,Yiwei Cai,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 提出一种通过累积低秩增量来逼近全量微调的高秩权重更新方法，优化了LoRA的收敛速度和性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在任务特定微调时计算开销大，LoRA虽降低开销但受限于低秩假设，影响效果和收敛速度。

Method: 通过逐步累积低秩更新构建高秩权重更新，利用分析方法确定最优缩放以逼近全微调。

Result: 在多种任务和多达120亿参数的LLM上验证，相比现有LoRA变体具有更快收敛和一致性能提升。

Conclusion: 该方法有效缓解了LoRA的秩限制问题，在不重启优化的前提下实现了高效、稳定的微调。

Abstract: As large language models (LLMs) continue to scale in size, the computational
overhead has become a major bottleneck for task-specific fine-tuning. While
low-rank adaptation (LoRA) effectively curtails this cost by confining the
weight updates to a low-dimensional subspace, such a restriction can hinder
effectiveness and slow convergence. This contribution deals with these
limitations by accumulating progressively a high-rank weight update from
consecutive low-rank increments. Specifically, the per update optimal low-rank
matrix is identified to minimize the loss function and closely approximate full
fine-tuning. To endow efficient and seamless optimization without restarting,
this optimal choice is formed by appropriately scaling the columns of the
original low-rank matrix. Rigorous performance guarantees reveal that the
optimal scaling can be found analytically. Extensive numerical tests with
popular LLMs scaling up to 12 billion parameters demonstrate a consistent
performance gain and fast convergence relative to state-of-the-art LoRA
variants on diverse tasks including natural language understanding, commonsense
reasoning, and mathematical problem solving.

</details>


### [290] [A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling](https://arxiv.org/abs/2510.23866)
*Paul Rosu,Muchang Bahng,Erick Jiang,Rico Zhu,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出一种物理条件下的潜在扩散模型，用于大气数据的动力降尺度，重点重建高分辨率2米温度场。


<details>
  <summary>Details</summary>
Motivation: 提高生成气象场的物理一致性与精度，解决传统方法在高分辨率重建中的不足。

Method: 基于预训练扩散模型，采用残差UNet结构，并引入PDE损失项，在全分辨率空间中通过有限差分近似有效平流-扩散方程来约束物理规律。

Result: 实验表明常规扩散训练已具有较低PDE残差，加入PDE损失后进一步提升了生成结果的物理合理性与稳定性。

Conclusion: 所提出的物理引导细调策略能有效增强潜在扩散模型在气象降尺度任务中的物理一致性，且代码已开源供后续研究使用。

Abstract: This work presents a physics-conditioned latent diffusion model tailored for
dynamical downscaling of atmospheric data, with a focus on reconstructing
high-resolution 2-m temperature fields. Building upon a pre-existing diffusion
architecture and employing a residual formulation against a reference UNet, we
integrate a partial differential equation (PDE) loss term into the model's
training objective. The PDE loss is computed in the full resolution (pixel)
space by decoding the latent representation and is designed to enforce physical
consistency through a finite-difference approximation of an effective
advection-diffusion balance. Empirical observations indicate that conventional
diffusion training already yields low PDE residuals, and we investigate how
fine-tuning with this additional loss further regularizes the model and
enhances the physical plausibility of the generated fields. The entirety of our
codebase is available on Github, for future reference and development.

</details>


### [291] [GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA](https://arxiv.org/abs/2510.23868)
*Zhichao Wang*

Main category: cs.LG

TL;DR: GIFT是一种新的强化学习框架，通过最小化隐式与显式奖励模型之间的差异来对齐大语言模型，结合了GRPO、DPO和UNA的优点，具有更快的收敛速度和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法如PPO和GRPO直接最大化累积奖励，难以有效利用隐式奖励，且存在训练过拟合、收敛慢等问题，需要更稳定高效的对齐方法。

Method: GIFT结合了GRPO的在线多响应生成与归一化、DPO的隐式奖励形式以及UNA的隐式-显式奖励对齐原则，通过对隐式和显式奖励进行联合归一化，将复杂的奖励最大化目标转化为归一化奖励函数间的均方误差（MSE）损失。

Result: GIFT将非凸优化问题转化为凸的、稳定的、解析可微的形式，相比GRPO超参数更少、收敛更快、泛化更好，训练过拟合显著减少，并在数学推理任务上表现出更优的性能和计算效率。

Conclusion: GIFT提供了一种高效稳定的大模型对齐方法，兼顾探索能力和优化稳定性，在推理与对齐性能上优于现有方法。

Abstract: I propose \textbf{G}roup-relative \textbf{I}mplicit \textbf{F}ine
\textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning
LLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT
minimizes the discrepancy between implicit and explicit reward models. It
combines three key ideas: (1) the online multi-response generation and
normalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the
implicit-explicit reward alignment principle of UNA. By jointly normalizing the
implicit and explicit rewards, GIFT eliminates an otherwise intractable term
that prevents effective use of implicit rewards. This normalization transforms
the complex reward maximization objective into a simple mean squared error
(MSE) loss between the normalized reward functions, converting a non-convex
optimization problem into a convex, stable, and analytically differentiable
formulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy
and thus retains exploration capability. Compared to GRPO, it requires fewer
hyperparameters, converges faster, and generalizes better with significantly
reduced training overfitting. Empirically, GIFT achieves superior reasoning and
alignment performance on mathematical benchmarks while remaining
computationally efficient.

</details>


### [292] [Artificial Intelligence Based Predictive Maintenance for Electric Buses](https://arxiv.org/abs/2510.23879)
*Ayse Irmak Ercevik,Ahmet Murat Ozbayoglu*

Main category: cs.LG

TL;DR: 本研究提出了一种基于图的特征选择方法，结合统计过滤与社区检测算法，用于分析电动公交车CAN总线数据，并利用机器学习模型预测车辆报警，有效提升了预测性能与特征可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统基于定期检查的维护方法难以捕捉电动公交车多维实时CAN总线数据中的异常，且复杂电驱和电池系统给预测性维护带来挑战。

Method: 采用混合图基特征选择方法（结合Pearson相关性、Cramer's V、ANOVA F检验与InfoMap、Leiden等社区检测算法），对两年采集的CAN数据进行预处理，并使用SVM、随机森林和XGBoost模型，结合SMOTEEN与二分下采样进行数据平衡，通过网格搜索与随机搜索优化模型，利用LIME提升可解释性。

Result: 所提系统能有效预测车辆报警，显著提升特征选择的可解释性，并在真实数据上验证了预测性能的提升。

Conclusion: 该方法支持基于数据驱动的主动维护策略，符合工业4.0理念，为电动公交车的预测性维护提供了可行解决方案。

Abstract: Predictive maintenance (PdM) is crucial for optimizing efficiency and
minimizing downtime of electric buses. While these vehicles provide
environmental benefits, they pose challenges for PdM due to complex electric
transmission and battery systems. Traditional maintenance, often based on
scheduled inspections, struggles to capture anomalies in multi-dimensional
real-time CAN Bus data. This study employs a graph-based feature selection
method to analyze relationships among CAN Bus parameters of electric buses and
investigates the prediction performance of targeted alarms using artificial
intelligence techniques. The raw data collected over two years underwent
extensive preprocessing to ensure data quality and consistency. A hybrid
graph-based feature selection tool was developed by combining statistical
filtering (Pearson correlation, Cramer's V, ANOVA F-test) with
optimization-based community detection algorithms (InfoMap, Leiden, Louvain,
Fast Greedy). Machine learning models, including SVM, Random Forest, and
XGBoost, were optimized through grid and random search with data balancing via
SMOTEEN and binary search-based down-sampling. Model interpretability was
achieved using LIME to identify the features influencing predictions. The
results demonstrate that the developed system effectively predicts vehicle
alarms, enhances feature interpretability, and supports proactive maintenance
strategies aligned with Industry 4.0 principles.

</details>


### [293] [RS-ORT: A Reduced-Space Branch-and-Bound Algorithm for Optimal Regression Trees](https://arxiv.org/abs/2510.23901)
*Cristobal Heredia,Pedro Chumpitaz-Flores,Kaixun Hua*

Main category: cs.LG

TL;DR: 提出了一种名为RS-ORT的专用分支定界算法，用于高效训练包含连续和二值特征的大规模最优回归树。


<details>
  <summary>Details</summary>
Motivation: 现有MIP方法在处理连续、大规模数据时计算复杂度高，或需将连续特征二值化而牺牲全局最优性。

Method: 将最优回归树训练重构为两阶段优化问题，仅对树结构变量进行分支，并引入多种边界收紧技术加速求解。

Result: 在多个包含二值和连续特征的回归基准上，RS-ORT在训练和测试性能上优于现有最先进方法；可在四小时内处理高达200万样本的数据集，获得更简洁的树结构和更好的泛化能力。

Conclusion: RS-ORT通过专有的分支定界设计和结构优化技术，实现了对大规模混合特征数据的高效、可扩展且全局最优的回归树学习。

Abstract: Mixed-integer programming (MIP) has emerged as a powerful framework for
learning optimal decision trees. Yet, existing MIP approaches for regression
tasks are either limited to purely binary features or become computationally
intractable when continuous, large-scale data are involved. Naively binarizing
continuous features sacrifices global optimality and often yields needlessly
deep trees. We recast the optimal regression-tree training as a two-stage
optimization problem and propose Reduced-Space Optimal Regression Trees
(RS-ORT) - a specialized branch-and-bound (BB) algorithm that branches
exclusively on tree-structural variables. This design guarantees the
algorithm's convergence and its independence from the number of training
samples. Leveraging the model's structure, we introduce several bound
tightening techniques - closed-form leaf prediction, empirical threshold
discretization, and exact depth-1 subtree parsing - that combine with
decomposable upper and lower bounding strategies to accelerate the training.
The BB node-wise decomposition enables trivial parallel execution, further
alleviating the computational intractability even for million-size datasets.
Based on the empirical studies on several regression benchmarks containing both
binary and continuous features, RS-ORT also delivers superior training and
testing performance than state-of-the-art methods. Notably, on datasets with up
to 2,000,000 samples with continuous features, RS-ORT can obtain guaranteed
training performance with a simpler tree structure and a better generalization
ability in four hours.

</details>


### [294] [Group Interventions on Deep Networks for Causal Discovery in Subsystems](https://arxiv.org/abs/2510.23906)
*Wasim Ahmad,Maha Shadaydeh,Joachim Denzler*

Main category: cs.LG

TL;DR: 本文提出了一种新的多组因果发现方法gCDMI，通过在深度神经网络上施加组级干预并结合模型不变性测试来推断变量组之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法主要关注成对变量间的因果关系，忽略了变量组（子系统）之间的集体因果影响，难以刻画复杂系统中的群体交互。

Method: 首先使用深度学习联合建模多变量时间序列的组间结构关系，然后对训练好的模型进行组级别干预，最后通过模型不变性测试判断变量组之间的因果联系。

Result: 在模拟数据上表现出优于现有方法的因果识别性能，并在脑网络和气候生态系统等真实数据上验证了有效性。

Conclusion: 结合组级干预与不变性测试的深度学习框架能有效揭示复杂系统的群体因果结构，为神经科学和气候科学等领域提供新洞察。

Abstract: Causal discovery uncovers complex relationships between variables, enhancing
predictions, decision-making, and insights into real-world systems, especially
in nonlinear multivariate time series. However, most existing methods primarily
focus on pairwise cause-effect relationships, overlooking interactions among
groups of variables, i.e., subsystems and their collective causal influence. In
this study, we introduce gCDMI, a novel multi-group causal discovery method
that leverages group-level interventions on trained deep neural networks and
employs model invariance testing to infer causal relationships. Our approach
involves three key steps. First, we use deep learning to jointly model the
structural relationships among groups of all time series. Second, we apply
group-wise interventions to the trained model. Finally, we conduct model
invariance testing to determine the presence of causal links among variable
groups. We evaluate our method on simulated datasets, demonstrating its
superior performance in identifying group-level causal relationships compared
to existing methods. Additionally, we validate our approach on real-world
datasets, including brain networks and climate ecosystems. Our results
highlight that applying group-level interventions to deep learning models,
combined with invariance testing, can effectively reveal complex causal
structures, offering valuable insights for domains such as neuroscience and
climate science.

</details>


### [295] [Preference Learning with Response Time: Robust Losses and Guarantees](https://arxiv.org/abs/2505.22820)
*Ayush Sawarni,Sahasrajit Sarmasarkar,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: 本文提出了一种将响应时间数据整合到人类偏好学习框架中的新方法，利用漂移扩散模型（EZ）来增强奖励模型的学习效率。


<details>
  <summary>Details</summary>
Motivation: 二元偏好数据虽广泛用于大模型微调，但用户决策中的时间信息未被充分利用。本文旨在通过引入响应时间来更准确地捕捉偏好强度。

Method: 结合二元选择与响应时间数据，基于证据累积漂移扩散模型（EZ），设计了具有Neyman正交性的损失函数，以实现对奖励模型的高效学习。

Result: 理论分析表明，传统方法的误差随奖励幅度指数增长，而新方法将其降至多项式增长；实验验证了该方法在图像偏好学习中的有效性。

Conclusion: 引入响应时间可显著提升偏好学习的样本效率，尤其在线性及非参数奖励函数空间中均表现出优越的收敛性能。

Abstract: This paper investigates the integration of response time data into human
preference learning frameworks for more effective reward model elicitation.
While binary preference data has become fundamental in fine-tuning foundation
models, generative AI systems, and other large-scale models, the valuable
temporal information inherent in user decision-making remains largely
unexploited. We propose novel methodologies to incorporate response time
information alongside binary choice data, leveraging the Evidence Accumulation
Drift Diffusion (EZ) model, under which response time is informative of the
preference strength. We develop Neyman-orthogonal loss functions that achieve
oracle convergence rates for reward model learning, matching the theoretical
optimal rates that would be attained if the expected response times for each
query were known a priori. Our theoretical analysis demonstrates that for
linear reward functions, conventional preference learning suffers from error
rates that scale exponentially with reward magnitude. In contrast, our response
time-augmented approach reduces this to polynomial scaling, representing a
significant improvement in sample efficiency. We extend these guarantees to
non-parametric reward function spaces, establishing convergence properties for
more complex, realistic reward models. Our extensive experiments validate our
theoretical findings in the context of preference learning over images.

</details>


### [296] [Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers](https://arxiv.org/abs/2510.23912)
*Marko Karbevski,Antonij Mijoski*

Main category: cs.LG

TL;DR: 本文研究了当前大语言模型中注意力机制的查询、键、值权重三元组，理论上证明在简化假设下查询权重是冗余的，从而减少了超过8%的非嵌入/语言模型头参数。通过在完整的GPT-3小型架构上验证，结果表明减少后的模型与标准基线具有相当的验证损失。


<details>
  <summary>Details</summary>
Motivation: 探索注意力机制中Query权重是否冗余，以减少模型参数量并提升效率。

Method: 在简化假设下进行理论分析，并在包含层归一化、跳跃连接和权重衰减的完整GPT-3小型架构上从零开始训练验证。

Result: 理论证明Query权重冗余，实验显示减少Query权重后的模型在验证损失上与标准模型相当。

Conclusion: Query权重在注意力机制中可能是冗余的，该发现支持进一步在更大规模模型中探究其去除的可行性与影响。

Abstract: The Query, Key, Value weight triplet is a building block of current attention
mechanisms in state-of-the-art LLMs. We theoretically investigate whether this
triplet can be reduced, proving under simplifying assumptions that the Query
weights are redundant, thereby reducing the number of non-embedding/lm-head
parameters by over 8%. We validate the theory on full-complexity GPT-3 small
architectures (with layer normalization, skip connections, and weight decay)
trained from scratch, demonstrating that the reduced model achieves comparable
validation loss to standard baselines. These findings motivate the
investigation of the Query weight redundancy at scale.

</details>


### [297] [Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs](https://arxiv.org/abs/2510.23914)
*Arsenii Mustafin,Xinyi Sheng,Dominik Baumann*

Main category: cs.LG

TL;DR: 本文扩展了马尔可夫决策过程（MDP）的几何解释，将折扣奖励情况下的分析方法推广到平均奖励情况，统一了两种情形。由此，证明了在唯一且遍历的最优策略下，值迭代算法在平均奖励情况下也具有几何收敛速度。


<details>
  <summary>Details</summary>
Motivation: 尽管平均奖励和折扣奖励的MDP在理论上常被分别分析，但二者有相似性，因此需要一种统一的框架来更好地理解它们之间的联系。

Method: 通过扩展最近提出的折扣奖励MDP的几何解释方法，并将其应用于平均奖励MDP，建立统一的分析框架。

Result: 成功将折扣奖励情况下的几何收敛性结果推广到平均奖励情况，证明值迭代算法在满足唯一且遍历最优政策时具有几何收敛率。

Conclusion: 该工作实现了对两类MDP问题的统一分析，并扩展了已知的重要理论结果，增强了对值迭代算法在不同奖励设定下收敛行为的理解。

Abstract: The theoretical analysis of Markov Decision Processes (MDPs) is commonly
split into two cases - the average-reward case and the discounted-reward case -
which, while sharing similarities, are typically analyzed separately. In this
work, we extend a recently introduced geometric interpretation of MDPs for the
discounted-reward case to the average-reward case, thereby unifying both. This
allows us to extend a major result known for the discounted-reward case to the
average-reward case: under a unique and ergodic optimal policy, the Value
Iteration algorithm achieves a geometric convergence rate.

</details>


### [298] [Improving the Straight-Through Estimator with Zeroth-Order Information](https://arxiv.org/abs/2510.23926)
*Ningfeng Yang,Tor M. Aamodt*

Main category: cs.LG

TL;DR: 提出了一种名为FOGZO（First-Order-Guided Zeroth-Order Gradient Descent）的新方法，结合了STE的高效一阶梯度和零阶优化的无偏性，显著提升了量化感知预训练的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的量化神经网络训练中，STE虽然能提供高效的梯度估计但存在偏差，而零阶方法虽无偏却计算昂贵，因此需要一种兼顾效率与准确性的新方法。

Method: 提出FOGZO方法，利用一阶梯度指导零阶梯度的估计过程，在减少STE偏差的同时降低计算开销。

Result: 在DeiT、ResNet和LLaMA等模型上，相比STE在相同迭代次数下取得1-8%的准确率提升或1-22个困惑度点的改善；在相同损失下，相比n-SPSA减少796倍计算量。

Conclusion: FOGZO在量化感知预训练中实现了更优的性能-计算权衡，是训练低精度量化网络的有效新策略。

Abstract: We study the problem of training neural networks with quantized parameters.
Learning low-precision quantized parameters by enabling computation of
gradients via the Straight-Through Estimator (STE) can be challenging. While
the STE enables back-propagation, which is a first-order method, recent works
have explored the use of zeroth-order (ZO) gradient descent for fine-tuning. We
note that the STE provides high-quality biased gradients, and ZO gradients are
unbiased but can be expensive. We thus propose First-Order-Guided Zeroth-Order
Gradient Descent (FOGZO) that reduces STE bias while reducing computations
relative to ZO methods. Empirically, we show FOGZO improves the tradeoff
between quality and training time in Quantization-Aware Pre-Training.
Specifically, versus STE at the same number of iterations, we show a 1-8\%
accuracy improvement for DeiT Tiny/Small, 1-2\% accuracy improvement on ResNet
18/50, and 1-22 perplexity point improvement for LLaMA models with up to 0.3
billion parameters. For the same loss, FOGZO yields a 796$\times$ reduction in
computation versus n-SPSA for a 2-layer MLP on MNIST. Code is available at
https://github.com/1733116199/fogzo.

</details>


### [299] [Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments](https://arxiv.org/abs/2510.23931)
*Miguel Fernandez-de-Retana,Unai Zulaika,Rubén Sánchez-Corcuera,Aitor Almeida*

Main category: cs.LG

TL;DR: 本研究探讨了差分隐私机制（DP-SGD和PDP-SGD）在联邦学习中抵御梯度泄露攻击的有效性，发现DP-SGD能显著降低隐私泄露风险，但牺牲一定模型性能；而PDP-SGD虽保持良好性能，却无法有效防御重构攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽保护数据隐私，但仍易受梯度泄露攻击，需评估实际防御效果。

Method: 在模拟联邦学习环境中，通过不同隐私水平训练多个计算机视觉模型，并分析从梯度中重建私有数据的质量。

Result: DP-SGD显著减轻了梯度泄露风险，但带来一定模型性能下降；PDP-SGD保持较强分类性能，但对重构攻击防御无效。

Conclusion: 应基于实证评估隐私机制的实际效果，尤其在分布式学习中，信息泄露可能构成严重安全威胁。

Abstract: Federated Learning (FL) allows for the training of Machine Learning models in
a collaborative manner without the need to share sensitive data. However, it
remains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private
information from the shared model updates. In this work, we investigate the
effectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD
and a variant based on explicit regularization (PDP-SGD) - as defenses against
GLAs. To this end, we evaluate the performance of several computer vision
models trained under varying privacy levels on a simple classification task,
and then analyze the quality of private data reconstructions obtained from the
intercepted gradients in a simulated FL environment. Our results demonstrate
that DP-SGD significantly mitigates the risk of gradient leakage attacks,
albeit with a moderate trade-off in model utility. In contrast, PDP-SGD
maintains strong classification performance but proves ineffective as a
practical defense against reconstruction attacks. These findings highlight the
importance of empirically evaluating privacy mechanisms beyond their
theoretical guarantees, particularly in distributed learning scenarios where
information leakage may represent an unassumable critical threat to data
security and privacy.

</details>


### [300] [A data free neural operator enabling fast inference of 2D and 3D Navier Stokes equations](https://arxiv.org/abs/2510.23936)
*Junho Choi,Teng-Yuan Chang,Namjung Kim,Youngjoon Hong*

Main category: cs.LG

TL;DR: 提出一种无需数据的神经算子，用于高效、高保真求解高维Navier-Stokes方程，支持实时大规模集合预测。


<details>
  <summary>Details</summary>
Motivation: 传统集合模拟计算成本高，现有神经算子依赖大量训练数据且难以推广到三维流动。

Method: 设计基于物理机制的无数据神经网络架构，以初边值条件和外力函数为输入，直接生成流场解。

Result: 在2D基准和3D案例中精度超过现有神经算子，集合模拟效率高于传统数值求解器，并首次实现无数据神经算子对三维Navier-Stokes方程的准确求解。

Conclusion: 该方法结合数值方法的稳定性和机器学习的可扩展性，为科学模拟提供了一种实用的无数据、高保真PDE代理模型路径。

Abstract: Ensemble simulations of high-dimensional flow models (e.g., Navier Stokes
type PDEs) are computationally prohibitive for real time applications. Neural
operators enable fast inference but are limited by costly data requirements and
poor generalization to 3D flows. We present a data-free operator network for
the Navier Stokes equations that eliminates the need for paired solution data
and enables robust, real time inference for large ensemble forecasting. The
physics-grounded architecture takes initial and boundary conditions as well as
forcing functions, yielding solutions robust to high variability and
perturbations. Across 2D benchmarks and 3D test cases, the method surpasses
prior neural operators in accuracy and, for ensembles, achieves greater
efficiency than conventional numerical solvers. Notably, it delivers accurate
solutions of the three dimensional Navier Stokes equations, a regime not
previously demonstrated for data free neural operators. By uniting a
numerically grounded architecture with the scalability of machine learning,
this approach establishes a practical pathway toward data free, high fidelity
PDE surrogates for end to end scientific simulation and prediction.

</details>


### [301] [Modeling Biological Multifunctionality with Echo State Networks](https://arxiv.org/abs/2510.23940)
*Anastasia-Maria Leventi-Peetz,Jörg-Volker Peetz,Kai Weber,Nikolaos Zacharis*

Main category: cs.LG

TL;DR: 提出了一种结合激发系统动力学与扩散过程的三维多组分反应-扩散模型，并利用回声状态网络（ESN）成功模拟了生物系统的时空动态行为。


<details>
  <summary>Details</summary>
Motivation: 旨在捕捉生物系统（特别是电生理过程）的时空行为，提供一种数据驱动的方法来模拟复杂生物动力学。

Method: 基于FitzHugh-Nagumo模型构建三维多组分反应-扩散模型，通过数值求解生成时间序列数据，并用这些数据训练和评估回声状态网络（ESN）。

Result: ESN能够准确复现系统的动态行为，验证了该建模方法在模拟生物动力学方面的可行性与有效性。

Conclusion: 结合反应-扩散模型与ESN的数据驱动方法可有效模拟复杂生物系统的动态，为生物过程建模提供了新途径。

Abstract: In this work, a three-dimensional multicomponent reaction-diffusion model has
been developed, combining excitable-system dynamics with diffusion processes
and sharing conceptual features with the FitzHugh-Nagumo model. Designed to
capture the spatiotemporal behavior of biological systems, particularly
electrophysiological processes, the model was solved numerically to generate
time-series data. These data were subsequently used to train and evaluate an
Echo State Network (ESN), which successfully reproduced the system's dynamic
behavior. The results demonstrate that simulating biological dynamics using
data-driven, multifunctional ESN models is both feasible and effective.

</details>


### [302] [ChessQA: Evaluating Large Language Models for Chess Understanding](https://arxiv.org/abs/2510.23948)
*Qianfeng Wen,Zhenwei Tang,Ashton Anderson*

Main category: cs.LG

TL;DR: ChessQA是一个全面的基准测试，用于评估大语言模型在国际象棋理解方面的表现，涵盖五个任务类别，提供更全面的能力分析和动态更新机制。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型国际象棋能力的评估零散且范围狭窄，难以准确衡量其理解水平及其随规模、训练方法或架构变化的表现。

Method: 提出ChessQA基准，包含结构、模式、短战术、局面判断和语义五个任务类别，对应棋手逐步掌握的抽象层次，并设计可动态更新的提示、答案和构建脚本。

Result: 在多个现代大语言模型上的评估显示，模型在所有五个类别中均存在持续弱点，提供了按类别划分的结果与错误分析。

Conclusion: ChessQA能更全面地评估大语言模型的国际象棋理解能力，超越以往仅评估走子质量的方法，为模型诊断与比较提供了可控且一致的环境。

Abstract: Chess provides an ideal testbed for evaluating the reasoning, modeling, and
abstraction capabilities of large language models (LLMs), as it has
well-defined structure and objective ground truth while admitting a wide
spectrum of skill levels. However, existing evaluations of LLM ability in chess
are ad hoc and narrow in scope, making it difficult to accurately measure LLM
chess understanding and how it varies with scale, post-training methodologies,
or architecture choices. We present ChessQA, a comprehensive benchmark that
assesses LLM chess understanding across five task categories (Structural,
Motifs, Short Tactics, Position Judgment, and Semantic), which approximately
correspond to the ascending abstractions that players master as they accumulate
chess knowledge, from understanding basic rules and learning tactical motifs to
correctly calculating tactics, evaluating positions, and semantically
describing high-level concepts. In this way, ChessQA captures a more
comprehensive picture of chess ability and understanding, going significantly
beyond the simple move quality evaluations done previously, and offers a
controlled, consistent setting for diagnosis and comparison. Furthermore,
ChessQA is inherently dynamic, with prompts, answer keys, and construction
scripts that can evolve as models improve. Evaluating a range of contemporary
LLMs, we find persistent weaknesses across all five categories and provide
results and error analyses by category. We will release the code, periodically
refreshed datasets, and a public leaderboard to support further research.

</details>


### [303] [A Pragmatic Way to Measure Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.23966)
*Scott Emmons,Roland S. Zimmermann,David K. Elson,Rohin Shah*

Main category: cs.LG

TL;DR: 提出了一种衡量链式思维（CoT）可监控性的实用方法，包括可读性和覆盖性两个指标，并通过自动评分提示在前沿模型上验证其有效性，作为开发人员评估设计决策影响的工具。


<details>
  <summary>Details</summary>
Motivation: 为了在训练实践或模型架构变化中保持AI安全中的CoT监控能力，需要一种可量化的手段来评估其可监控性。

Method: 提出了衡量CoT可监控性的两个维度：可读性和覆盖性，并设计了一个基于提示的自动评分器，使任何能力强的语言模型都能评估现有CoT的这两个指标。

Result: 在多个前沿模型和挑战性基准上应用该自动评分器后，发现这些模型表现出较高的可监控性。

Conclusion: 所提出的方法为开发者提供了一个实用工具，用于跟踪设计决策对CoT默认可监控性的影响，但应作为对抗性压力测试的补充而非替代。

Abstract: While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI
safety, this opportunity could be lost through shifts in training practices or
model architecture. To help preserve monitorability, we propose a pragmatic way
to measure two components of it: legibility (whether the reasoning can be
followed by a human) and coverage (whether the CoT contains all the reasoning
needed for a human to also produce the final output). We implement these
metrics with an autorater prompt that enables any capable LLM to compute the
legibility and coverage of existing CoTs. After sanity-checking our prompted
autorater with synthetic CoT degradations, we apply it to several frontier
models on challenging benchmarks, finding that they exhibit high
monitorability. We present these metrics, including our complete autorater
prompt, as a tool for developers to track how design decisions impact
monitorability. While the exact prompt we share is still a preliminary version
under ongoing development, we are sharing it now in the hopes that others in
the community will find it useful. Our method helps measure the default
monitorability of CoT - it should be seen as a complement, not a replacement,
for the adversarial stress-testing needed to test robustness against
deliberately evasive models.

</details>


### [304] [An efficient probabilistic hardware architecture for diffusion-like models](https://arxiv.org/abs/2510.23972)
*Andraž Jelinčič,Owen Lockwood,Akhil Garlapati,Guillaume Verdon,Trevor McCourt*

Main category: cs.LG

TL;DR: 提出了一种全晶体管概率计算机，通过在硬件层面实现强大的去噪模型，解决了现有随机计算方案因建模技术局限和硬件不可扩展性而难以推广的问题，系统级分析显示其能效比GPU高约10000倍。


<details>
  <summary>Details</summary>
Motivation: 现有的概率AI专用随机计算机由于依赖有限的建模技术和不可扩展的异构硬件，未能广泛应用，因此需要一种更高效、可扩展的解决方案。

Method: 设计了一种基于全晶体管架构的概率计算机，在硬件层面直接实现强大的去噪模型，并进行系统级性能与能效分析。

Result: 系统级分析表明，该架构在简单图像基准测试中可达到与GPU相当的性能，同时能耗降低约10000倍。

Conclusion: 所提出的全晶体管概率计算机在保持高性能的同时极大提升了能效，为概率AI的高效计算提供了一种可行且可扩展的硬件方案。

Abstract: The proliferation of probabilistic AI has promoted proposals for specialized
stochastic computers. Despite promising efficiency gains, these proposals have
failed to gain traction because they rely on fundamentally limited modeling
techniques and exotic, unscalable hardware. In this work, we address these
shortcomings by proposing an all-transistor probabilistic computer that
implements powerful denoising models at the hardware level. A system-level
analysis indicates that devices based on our architecture could achieve
performance parity with GPUs on a simple image benchmark using approximately
10,000 times less energy.

</details>


### [305] [Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models](https://arxiv.org/abs/2510.23974)
*Byeonghu Na,Minsang Park,Gyuwon Sim,Donghyeok Shin,HeeSun Bae,Mina Kang,Se Jung Kwon,Wanmo Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出了一种动态更新文本嵌入的方法DATE，能够在每个扩散时间步根据中间扰动数据优化文本-图像对齐，无需额外训练，提升了文本到图像生成的适应性和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型使用固定的文本编码，限制了其在生成过程中的适应性，因此需要一种能够动态调整文本嵌入以更好对齐图像生成过程的方法。

Method: 提出Diffusion Adaptive Text Embedding (DATE)，通过构建优化问题，在每个采样步骤中基于中间扰动数据动态更新文本嵌入，从而增强文本与生成图像之间的对齐和一致性。

Result: 理论分析和实验结果表明，DATE在保持模型生成能力的同时，显著优于固定文本嵌入方法，在多概念生成和文本引导图像编辑等任务中表现出更强的文本-图像对齐能力。

Conclusion: DATE通过动态调整文本嵌入，有效提升了文本到图像扩散模型的灵活性和生成质量，且无需额外训练，具有良好的通用性和应用前景。

Abstract: Text-to-image diffusion models rely on text embeddings from a pre-trained
text encoder, but these embeddings remain fixed across all diffusion timesteps,
limiting their adaptability to the generative process. We propose Diffusion
Adaptive Text Embedding (DATE), which dynamically updates text embeddings at
each diffusion timestep based on intermediate perturbed data. We formulate an
optimization problem and derive an update rule that refines the text embeddings
at each sampling step to improve alignment and preference between the mean
predicted image and the text. This allows DATE to dynamically adapts the text
conditions to the reverse-diffused images throughout diffusion sampling without
requiring additional model training. Through theoretical analysis and empirical
results, we show that DATE maintains the generative capability of the model
while providing superior text-image alignment over fixed text embeddings across
various tasks, including multi-concept generation and text-guided image
editing. Our code is available at https://github.com/aailab-kaist/DATE.

</details>


### [306] [Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling](https://arxiv.org/abs/2510.23977)
*Yohan Abeysinghe,Muhammad Akhtar Munir,Sanoojan Baliah,Ron Sarafian,Fahad Shahbaz Khan,Yinon Rudich,Salman Khan*

Main category: cs.LG

TL;DR: 本文提出了一种名为SynCast的高分辨率神经网络预测模型，结合气象和空气质量数据，利用区域适应性Transformer架构和基于扩散的随机细化模块，显著提升了对细颗粒物（PM）浓度（尤其是极端污染事件）的预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有空气污染预测模型往往低估罕见但危害严重的污染事件，难以满足对准确公共健康预警的需求，尤其是在易受野火、城市雾霾和沙尘暴影响的脆弱地区。

Method: SynCast采用区域适应的Transformer架构，并引入基于扩散的随机细化模块，结合ERA5和CAMS数据集，使用领域感知损失函数和极值理论来优化对分布尾部（极端事件）的建模能力。

Result: 模型在多种PM指标（PM₁、PM₂.₅、PM₁₀）上均表现出更高的预测保真度，尤其在极端污染条件下显著优于现有方法，同时保持全球范围内的预测准确性。

Conclusion: SynCast为下一代空气质量预警系统提供了可扩展的基础，有助于在脆弱地区加强气候与健康风险的应对能力。

Abstract: Air pollution remains a leading global health and environmental risk,
particularly in regions vulnerable to episodic air pollution spikes due to
wildfires, urban haze and dust storms. Accurate forecasting of particulate
matter (PM) concentrations is essential to enable timely public health warnings
and interventions, yet existing models often underestimate rare but hazardous
pollution events. Here, we present SynCast, a high-resolution neural
forecasting model that integrates meteorological and air composition data to
improve predictions of both average and extreme pollution levels. Built on a
regionally adapted transformer backbone and enhanced with a diffusion-based
stochastic refinement module, SynCast captures the nonlinear dynamics driving
PM spikes more accurately than existing approaches. Leveraging on harmonized
ERA5 and CAMS datasets, our model shows substantial gains in forecasting
fidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$),
especially under extreme conditions. We demonstrate that conventional loss
functions underrepresent distributional tails (rare pollution events) and show
that SynCast, guided by domain-aware objectives and extreme value theory,
significantly enhances performance in highly impacted regions without
compromising global accuracy. This approach provides a scalable foundation for
next-generation air quality early warning systems and supports climate-health
risk mitigation in vulnerable regions.

</details>


### [307] [HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing](https://arxiv.org/abs/2510.23980)
*Guojing Cong,Tom Potok,Hamed Poursiami,Maryam Parsa*

Main category: cs.LG

TL;DR: 提出了一种结合图卷积与超维计算的新型算法HDGC，在同质和异质图上均优于主流图神经网络和超维计算方法，且在GPU上比GCNII和HDGL分别快9561倍和144.5倍，具有优异的能效潜力。


<details>
  <summary>Details</summary>
Motivation: 为了提升图学习的预测准确性和计算效率，尤其是在同质和异质图上的表现，并探索低功耗硬件平台的适用性。

Method: 将图卷积与超维计算中的绑定和捆绑操作相结合，使用二值向量进行主要计算，实现高效的图表示学习。

Result: HDGC在多个数据集上超越了主流图神经网络（如GCNII）和超维计算方法（如HDGL），在相同GPU平台上平均比GCNII快9561倍，比HDGL快144.5倍。

Conclusion: HDGC是一种高效、快速且准确的图学习算法，特别适合在神经形态和存内计算等低功耗硬件上部署。

Abstract: We present a novel algorithm, \hdgc, that marries graph convolution with
binding and bundling operations in hyperdimensional computing for transductive
graph learning. For prediction accuracy \hdgc outperforms major and popular
graph neural network implementations as well as state-of-the-art
hyperdimensional computing implementations for a collection of homophilic
graphs and heterophilic graphs. Compared with the most accurate learning
methodologies we have tested, on the same target GPU platform, \hdgc is on
average 9561.0 and 144.5 times faster than \gcnii, a graph neural network
implementation and HDGL, a hyperdimensional computing implementation,
respectively. As the majority of the learning operates on binary vectors, we
expect outstanding energy performance of \hdgc on neuromorphic and emerging
process-in-memory devices.

</details>


### [308] [STNet: Spectral Transformation Network for Solving Operator Eigenvalue Problem](https://arxiv.org/abs/2510.23986)
*Hong Wang,Jiang Yixuan,Jie Wang,Xinyi Li,Jian Luo,Huanshuo Dong*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的谱变换网络（STNet），通过迭代对算子进行谱变换，有效提升高维算子特征值问题的求解精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在高维算子特征值问题中受限于维度灾难，且现有深度学习方法性能依赖于算子谱分布，缺乏对谱结构的有效利用。

Method: 提出STNet，在每次迭代中利用已估计的特征值和特征函数对原算子进行谱变换：采用deflation投影排除已求解的子空间，缩小搜索范围；通过滤波变换放大目标区域特征值、抑制其他区域，使问题更易求解。

Result: 大量实验表明，STNet在各类算子特征值问题上均优于现有的基于学习的方法，在精度上达到最先进水平。

Conclusion: STNet通过动态谱变换有效利用算子谱结构信息，显著提升了高维特征值问题的求解性能，为基于深度学习的数值求解提供了新思路。

Abstract: Operator eigenvalue problems play a critical role in various scientific
fields and engineering applications, yet numerical methods are hindered by the
curse of dimensionality. Recent deep learning methods provide an efficient
approach to address this challenge by iteratively updating neural networks.
These methods' performance relies heavily on the spectral distribution of the
given operator: larger gaps between the operator's eigenvalues will improve
precision, thus tailored spectral transformations that leverage the spectral
distribution can enhance their performance. Based on this observation, we
propose the Spectral Transformation Network (STNet). During each iteration,
STNet uses approximate eigenvalues and eigenfunctions to perform spectral
transformations on the original operator, turning it into an equivalent but
easier problem. Specifically, we employ deflation projection to exclude the
subspace corresponding to already solved eigenfunctions, thereby reducing the
search space and avoiding converging to existing eigenfunctions. Additionally,
our filter transform magnifies eigenvalues in the desired region and suppresses
those outside, further improving performance. Extensive experiments demonstrate
that STNet consistently outperforms existing learning-based methods, achieving
state-of-the-art performance in accuracy.

</details>


### [309] [Optimal Arm Elimination Algorithms for Combinatorial Bandits](https://arxiv.org/abs/2510.23992)
*Yuxiao Wen,Yanjun Han,Zhengyuan Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新的组合多臂赌博机框架下的消除算法，通过将臂分为确认、活跃和淘汰三类，并引入显式探索机制，在一般图反馈和线性上下文组合带环境中实现了近似最优的遗憾性能。


<details>
  <summary>Details</summary>
Motivation: 现有的上置信界（UCB）方法在组合带中自然适用，但消除类方法面临挑战，尤其因缺乏显式探索而导致性能不足。

Method: 提出一种新的消除机制，将臂划分为三类（确认、活跃、淘汰），并结合显式探索策略来更新这些集合，应用于组合多臂赌博机与图反馈及线性上下文组合带两种设定。

Result: 所提算法在两种设定下均实现近似最优的后悔界，理论证明UCB类方法可能因探索不足而失败，且给出了匹配的下界。

Conclusion: 该三类划分与显式探索的消除策略有效克服了传统消除方法的局限，在组合带环境中具有优越理论性能。

Abstract: Combinatorial bandits extend the classical bandit framework to settings where
the learner selects multiple arms in each round, motivated by applications such
as online recommendation and assortment optimization. While extensions of upper
confidence bound (UCB) algorithms arise naturally in this context, adapting arm
elimination methods has proved more challenging. We introduce a novel
elimination scheme that partitions arms into three categories (confirmed,
active, and eliminated), and incorporates explicit exploration to update these
sets. We demonstrate the efficacy of our algorithm in two settings: the
combinatorial multi-armed bandit with general graph feedback, and the
combinatorial linear contextual bandit. In both cases, our approach achieves
near-optimal regret, whereas UCB-based methods can provably fail due to
insufficient explicit exploration. Matching lower bounds are also provided.

</details>


### [310] [Predicting Barge Tow Size on Inland Waterways Using Vessel Trajectory Derived Features: Proof of Concept](https://arxiv.org/abs/2510.23994)
*Geoffery Agorku,Sarah Hernandez,Hayley Hames,Cade Wagner*

Main category: cs.LG

TL;DR: 提出一种基于AIS数据和机器学习的驳船数量预测新方法，通过卫星图像标注和时空匹配构建训练数据，使用递归特征消除筛选关键特征，泊松回归模型在12个特征下达到平均绝对误差1.92艘的最优性能，航向熵、速度变异性等机动性指标对预测最为重要，该方法可扩展应用于内河航运管理。


<details>
  <summary>Details</summary>
Motivation: 准确实时估计内河航道上的驳船数量存在挑战，因驳船无自推进能力且现有监测系统有限，亟需更有效的监测手段以提升 maritime domain awareness。

Method: 利用AIS船舶追踪数据，结合卫星影像手动标注的驳船实例，通过时空匹配建立训练数据集；提取30个AIS衍生特征（涵盖船舶几何、动态运动和轨迹模式），采用递归特征消除（RFE）筛选关键变量，并训练六种回归模型（包括集成、核方法和广义线性模型），最终评估各模型性能。

Result: 泊松回归模型表现最佳，使用12个特征实现1.92艘驳船的平均绝对误差（MAE）；特征重要性分析显示航向熵、速度变异性及航程长度等反映船舶机动性的指标最具预测力。

Conclusion: 所提方法为基于AIS数据的驳船数量预测提供了可扩展且易于实施的解决方案，有望应用于船闸调度、港口管理和货运规划；未来将验证该模型在不同内河环境下的可迁移性。

Abstract: Accurate, real-time estimation of barge quantity on inland waterways remains
a critical challenge due to the non-self-propelled nature of barges and the
limitations of existing monitoring systems. This study introduces a novel
method to use Automatic Identification System (AIS) vessel tracking data to
predict the number of barges in tow using Machine Learning (ML). To train and
test the model, barge instances were manually annotated from satellite scenes
across the Lower Mississippi River. Labeled images were matched to AIS vessel
tracks using a spatiotemporal matching procedure. A comprehensive set of 30
AIS-derived features capturing vessel geometry, dynamic movement, and
trajectory patterns were created and evaluated using Recursive Feature
Elimination (RFE) to identify the most predictive variables. Six regression
models, including ensemble, kernel-based, and generalized linear approaches,
were trained and evaluated. The Poisson Regressor model yielded the best
performance, achieving a Mean Absolute Error (MAE) of 1.92 barges using 12 of
the 30 features. The feature importance analysis revealed that metrics
capturing vessel maneuverability such as course entropy, speed variability and
trip length were most predictive of barge count. The proposed approach provides
a scalable, readily implementable method for enhancing Maritime Domain
Awareness (MDA), with strong potential applications in lock scheduling, port
management, and freight planning. Future work will expand the proof of concept
presented here to explore model transferability to other inland rivers with
differing operational and environmental conditions.

</details>


### [311] [Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion Models](https://arxiv.org/abs/2510.24012)
*Byeonghu Na,Mina Kang,Jiseok Kwak,Minsang Park,Jiwoo Shin,SeJoon Jun,Gayoung Lee,Jin-Hwa Kim,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出了一种无需训练的安全文本嵌入引导方法（STG），通过在采样过程中调整文本嵌入来提升扩散模型生成图像的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型可能因训练数据中的不当或偏见内容而在恶意提示下生成有害输出，亟需有效且灵活的安全机制。

Method: 设计安全函数评估预期去噪图像的安全性，并据此动态调整文本嵌入，实现生成过程中的安全引导，无需重新训练模型。

Result: 在裸露、暴力和艺术家风格移除等多个安全场景中，STG在去除不安全内容的同时更好保持了提示的语义意图，性能优于基于训练和无需训练的基线方法。

Conclusion: STG能有效将模型分布与安全约束对齐，在几乎不影响生成质量的前提下显著提升文本到图像模型的安全性，具备良好的实用性和泛化能力。

Abstract: Text-to-image models have recently made significant advances in generating
realistic and semantically coherent images, driven by advanced diffusion models
and large-scale web-crawled datasets. However, these datasets often contain
inappropriate or biased content, raising concerns about the generation of
harmful outputs when provided with malicious text prompts. We propose Safe Text
embedding Guidance (STG), a training-free approach to improve the safety of
diffusion models by guiding the text embeddings during sampling. STG adjusts
the text embeddings based on a safety function evaluated on the expected final
denoised image, allowing the model to generate safer outputs without additional
training. Theoretically, we show that STG aligns the underlying model
distribution with safety constraints, thereby achieving safer outputs while
minimally affecting generation quality. Experiments on various safety
scenarios, including nudity, violence, and artist-style removal, show that STG
consistently outperforms both training-based and training-free baselines in
removing unsafe content while preserving the core semantic intent of input
prompts. Our code is available at https://github.com/aailab-kaist/STG.

</details>


### [312] [NeuroPathNet: Dynamic Path Trajectory Learning for Brain Functional Connectivity Analysis](https://arxiv.org/abs/2510.24025)
*Guo Tianqi Guo,Chen Liping,Peng Ciyuan,Guo Jingjing,Ren Jing*

Main category: cs.LG

TL;DR: 本文提出了一种新的路径级轨迹建模框架NeuroPathNet，用于刻画脑功能分区间连接路径的动态行为，并在三个公开fMRI数据集上验证了其优于现有主流方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉特定功能社区之间连接的时间演化特征，而理解脑功能网络随时间的演变对认知机制分析和神经系统疾病诊断具有重要意义。

Method: 基于医学支持的静态分区方案（如Yeo和Smith ICA），提取每对功能分区间连接强度的时间序列，并利用时序神经网络进行建模。

Result: 在三个公开fMRI数据集上的实验表明，该模型在多个指标上优于现有的主流方法。

Conclusion: 本研究推动了脑网络分析中动态图学习方法的发展，并为神经系统疾病的诊断提供了潜在的临床应用价值。

Abstract: Understanding the evolution of brain functional networks over time is of
great significance for the analysis of cognitive mechanisms and the diagnosis
of neurological diseases. Existing methods often have difficulty in capturing
the temporal evolution characteristics of connections between specific
functional communities. To this end, this paper proposes a new path-level
trajectory modeling framework (NeuroPathNet) to characterize the dynamic
behavior of connection pathways between brain functional partitions. Based on
medically supported static partitioning schemes (such as Yeo and Smith ICA), we
extract the time series of connection strengths between each pair of functional
partitions and model them using a temporal neural network. We validate the
model performance on three public functional Magnetic Resonance Imaging (fMRI)
datasets, and the results show that it outperforms existing mainstream methods
in multiple indicators. This study can promote the development of dynamic graph
learning methods for brain network analysis, and provide possible clinical
applications for the diagnosis of neurological diseases.

</details>


### [313] [Efficient Global-Local Fusion Sampling for Physics-Informed Neural Networks](https://arxiv.org/abs/2510.24026)
*Jiaqi Luo,Shixin Xu,Zhouwang Yang*

Main category: cs.LG

TL;DR: 提出了一种全局-局部融合（GLF）采样策略，通过残差自适应采样和基于残差的近似方法，在保持稳定性的同时提升物理信息神经网络（PINNs）的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统全局采样计算成本高，局部采样易忽略已学习良好的区域，影响模型鲁棒性，因此需要一种兼顾稳定性和效率的采样策略。

Method: 提出GLF采样策略：利用高斯噪声扰动训练点生成新配置点，噪声幅度与残差成反比，集中于难学区域；引入轻量线性代理模型近似全局残差分布以降低计算开销。

Result: 在多个基准PDE实验中，GLF相比纯全局或局部采样策略，在相同计算资源下显著提升了PINNs的求解精度和训练效率。

Conclusion: GLF采样策略有效平衡了全局探索与局部精细采样的优势，为复杂高维PDE问题提供了可靠且可扩展的PINNs优化框架。

Abstract: The accuracy of Physics-Informed Neural Networks (PINNs) critically depends
on the placement of collocation points, as the PDE loss is approximated through
sampling over the solution domain. Global sampling ensures stability by
covering the entire domain but requires many samples and is computationally
expensive, whereas local sampling improves efficiency by focusing on
high-residual regions but may neglect well-learned areas, reducing robustness.
We propose a Global-Local Fusion (GLF) Sampling Strategy that combines the
strengths of both approaches. Specifically, new collocation points are
generated by perturbing training points with Gaussian noise scaled inversely to
the residual, thereby concentrating samples in difficult regions while
preserving exploration. To further reduce computational overhead, a lightweight
linear surrogate is introduced to approximate the global residual-based
distribution, achieving similar effectiveness at a fraction of the cost.
Together, these components, residual-adaptive sampling and residual-based
approximation, preserve the stability of global methods while retaining the
efficiency of local refinement. Extensive experiments on benchmark PDEs
demonstrate that GLF consistently improves both accuracy and efficiency
compared with global and local sampling strategies. This study provides a
practical and scalable framework for enhancing the reliability and efficiency
of PINNs in solving complex and high-dimensional PDEs.

</details>


### [314] [Spatio-temporal Multivariate Time Series Forecast with Chosen Variables](https://arxiv.org/abs/2510.24027)
*Zibo Liu,Zhe Jiang,Zelin Xu,Tingsong Xiao,Yupu Zhang,Zhengkun Xiao,Haibo Wang,Shigang Chen*

Main category: cs.LG

TL;DR: 本文提出了一种用于时空多变量时间序列预测的统一框架，通过联合变量选择和模型优化来提高预测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设输入模型的传感器位置是预先确定的，但如何选择最优的m个变量（即传感器位置）尚未被研究。本文旨在填补这一空白。

Method: 提出了一个包含三个关键技术组件的统一框架：基于分位数的掩码变量-参数剪枝、优先级变量-参数回放以及动态外推机制，实现变量选择与模型优化的联合学习。

Result: 在五个真实世界数据集上的实验表明，该方法在预测准确性和模型效率方面均显著优于现有最先进基线方法。

Conclusion: 通过联合优化变量选择和预测模型，能够有效提升时空多变量时间序列预测的性能，为传感器部署受限场景下的预测任务提供了新的解决方案。

Abstract: Spatio-Temporal Multivariate time series Forecast (STMF) uses the time series
of $n$ spatially distributed variables in a period of recent past to forecast
their values in a period of near future. It has important applications in
spatio-temporal sensing forecast such as road traffic prediction and air
pollution prediction. Recent papers have addressed a practical problem of
missing variables in the model input, which arises in the sensing applications
where the number $m$ of sensors is far less than the number $n$ of locations to
be monitored, due to budget constraints. We observe that the state of the art
assumes that the $m$ variables (i.e., locations with sensors) in the model
input are pre-determined and the important problem of how to choose the $m$
variables in the input has never been studied. This paper fills the gap by
studying a new problem of STMF with chosen variables, which optimally selects
$m$-out-of-$n$ variables for the model input in order to maximize the forecast
accuracy. We propose a unified framework that jointly performs variable
selection and model optimization for both forecast accuracy and model
efficiency. It consists of three novel technical components: (1) masked
variable-parameter pruning, which progressively prunes less informative
variables and attention parameters through quantile-based masking; (2)
prioritized variable-parameter replay, which replays low-loss past samples to
preserve learned knowledge for model stability; (3) dynamic extrapolation
mechanism, which propagates information from variables selected for the input
to all other variables via learnable spatial embeddings and adjacency
information. Experiments on five real-world datasets show that our work
significantly outperforms the state-of-the-art baselines in both accuracy and
efficiency, demonstrating the effectiveness of joint variable selection and
model optimization.

</details>


### [315] [GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler Research](https://arxiv.org/abs/2510.24035)
*Xinqi Li,Yiqun Liu,Shan Jiang,Enrong Zheng,Huaijin Zheng,Wenhao Dai,Haodong Deng,Dianhai Yu,Yanjun Ma*

Main category: cs.LG

TL;DR: 本文介绍了GraphNet，一个包含2700个真实世界深度学习计算图的数据集，并提出了Speedup Score S(t)和Error-aware Speedup Score ES(t)作为评估张量编译器性能的指标，验证了其在CV和NLP任务上的实用性。


<details>
  <summary>Details</summary>
Motivation: 为了更真实地评估张量编译器在实际深度学习任务中的优化能力，需要一个涵盖多框架、多任务的大规模计算图数据集及更全面的评估指标。

Method: 构建了一个包含2.7K真实计算图的GraphNet数据集，提出Speedup Score S(t)和ES(t)指标，用于综合评估编译器的运行速度提升、执行正确性和错误感知性能，并对CINN和TorchInductor进行了基准测试。

Result: 成功构建了GraphNet数据集及其完整构建流程工具链，提出的S(t)和ES(t)指标有效衡量了编译器性能，实验验证了其在CV和NLP任务上的实用性和可靠性。

Conclusion: GraphNet为张量编译器的研究提供了高质量的真实数据支持，S(t)和ES(t)为性能评估提供了更全面的度量标准，有助于推动编译器优化技术的发展。

Abstract: We introduce GraphNet, a dataset of 2.7K real-world deep learning
computational graphs with rich metadata, spanning six major task categories
across multiple deep learning frameworks. To evaluate tensor compiler
performance on these samples, we propose the benchmark metric Speedup Score
S(t), which jointly considers runtime speedup and execution correctness under
tunable tolerance levels, offering a reliable measure of general optimization
capability. Furthermore, we extend S(t) to the Error-aware Speedup Score ES(t),
which incorporates error information and helps compiler developers identify key
performance bottlenecks. In this report, we benchmark the default tensor
compilers, CINN for PaddlePaddle and TorchInductor for PyTorch, on computer
vision (CV) and natural language processing (NLP) samples to demonstrate the
practicality of GraphNet. The full construction pipeline with graph extraction
and compiler evaluation tools is available at
https://github.com/PaddlePaddle/GraphNet .

</details>


### [316] [Geometric Algorithms for Neural Combinatorial Optimization with Constraints](https://arxiv.org/abs/2510.24039)
*Nikolaos Karalias,Akbar Rafiey,Yifei Xu,Zhishang Luo,Behrooz Tahmasebi,Connie Jiang,Stefanie Jegelka*

Main category: cs.LG

TL;DR: 提出一种基于凸组合分解的端到端可微框架，用于解决带离散约束的组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法难以处理组合优化中的离散约束问题。

Method: 利用凸几何和Carathéodory定理，将神经网络输出分解为对应可行解的多面体顶点的凸组合。

Result: 在基数约束优化任务上优于现有神经网络基线，并可扩展至独立集和拟阵约束等问题。

Conclusion: 该方法实现了对离散约束组合优化问题的有效建模与高质量求解。

Abstract: Self-Supervised Learning (SSL) for Combinatorial Optimization (CO) is an
emerging paradigm for solving combinatorial problems using neural networks. In
this paper, we address a central challenge of SSL for CO: solving problems with
discrete constraints. We design an end-to-end differentiable framework that
enables us to solve discrete constrained optimization problems with neural
networks. Concretely, we leverage algorithmic techniques from the literature on
convex geometry and Carath\'eodory's theorem to decompose neural network
outputs into convex combinations of polytope corners that correspond to
feasible sets. This decomposition-based approach enables self-supervised
training but also ensures efficient quality-preserving rounding of the neural
net output into feasible solutions. Extensive experiments in
cardinality-constrained optimization show that our approach can consistently
outperform neural baselines. We further provide worked-out examples of how our
method can be applied beyond cardinality-constrained problems to a diverse set
of combinatorial optimization tasks, including finding independent sets in
graphs, and solving matroid-constrained problems.

</details>


### [317] [Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection](https://arxiv.org/abs/2510.24043)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 提出了一种名为Two-Stage LKPLO的新型多阶段异常检测框架，结合基于损失的异常度量、全局核PCA和局部聚类，显著提升了在复杂数据结构上的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于投影的方法依赖固定统计指标且假设单一数据结构，难以应对复杂、多模态或非线性数据，因此需要更灵活、适应性强的异常检测方法。

Method: 提出Two-Stage LKPLO框架：首先使用广义损失基异常度量（PLO）替代固定指标；其次通过全局核PCA处理非线性结构；最后引入局部聚类应对多模态分布。采用5折交叉验证与自动超参数优化进行评估，并进行消融研究验证各模块贡献。

Result: 在10个基准数据集上，Two-Stage LKPLO实现了最先进的性能，尤其在多簇（如Optdigits）和高维复杂数据（如Arrhythmia）上显著优于现有方法，消融实验证明核化与局部化阶段的协同作用至关重要。

Conclusion: Two-Stage LKPLO通过融合灵活的损失函数、核方法与局部聚类，构建了强大的多阶段异常检测框架，验证了混合架构在处理复杂数据异常检测问题中的有效性与必要性。

Abstract: This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection
framework that overcomes the coexisting limitations of conventional
projection-based methods: their reliance on a fixed statistical metric and
their assumption of a single data structure. Our framework uniquely synthesizes
three key concepts: (1) a generalized loss-based outlyingness measure (PLO)
that replaces the fixed metric with flexible, adaptive loss functions like our
proposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear
data structures; and (3) a subsequent local clustering stage to handle
multi-modal distributions. Comprehensive 5-fold cross-validation experiments on
10 benchmark datasets, with automated hyperparameter optimization, demonstrate
that Two-Stage LKPLO achieves state-of-the-art performance. It significantly
outperforms strong baselines on datasets with challenging structures where
existing methods fail, most notably on multi-cluster data (Optdigits) and
complex, high-dimensional data (Arrhythmia). Furthermore, an ablation study
empirically confirms that the synergistic combination of both the kernelization
and localization stages is indispensable for its superior performance. This
work contributes a powerful new tool for a significant class of outlier
detection problems and underscores the importance of hybrid, multi-stage
architectures.

</details>


### [318] [Mitigating Negative Transfer via Reducing Environmental Disagreement](https://arxiv.org/abs/2510.24044)
*Hui Sun,Zheng Xie,Hao-Yuan He,Ming Li*

Main category: cs.LG

TL;DR: 本文提出了一种通过因果解耦学习来减少非因果环境特征上的判别分歧，从而缓解无监督域适应中的负迁移问题的新方法RED，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于显著的域差异导致知识迁移困难，容易引发负迁移，影响模型性能，因此需要有效缓解负迁移问题。

Method: 通过对抗训练分离出域不变的因果特征和域特定的非因果环境特征，并基于后者估计并减少环境分歧。

Result: RED方法能有效减轻负迁移，在多个实验中表现出优于现有方法的性能。

Conclusion: 通过因果解耦视角分析并减少环境分歧，是缓解无监督域适应中负迁移的有效途径。

Abstract: Unsupervised Domain Adaptation~(UDA) focuses on transferring knowledge from a
labeled source domain to an unlabeled target domain, addressing the challenge
of \emph{domain shift}. Significant domain shifts hinder effective knowledge
transfer, leading to \emph{negative transfer} and deteriorating model
performance. Therefore, mitigating negative transfer is essential. This study
revisits negative transfer through the lens of causally disentangled learning,
emphasizing cross-domain discriminative disagreement on non-causal
environmental features as a critical factor. Our theoretical analysis reveals
that overreliance on non-causal environmental features as the environment
evolves can cause discriminative disagreements~(termed \emph{environmental
disagreement}), thereby resulting in negative transfer. To address this, we
propose Reducing Environmental Disagreement~(RED), which disentangles each
sample into domain-invariant causal features and domain-specific non-causal
environmental features via adversarially training domain-specific environmental
feature extractors in the opposite domains. Subsequently, RED estimates and
reduces environmental disagreement based on domain-specific non-causal
environmental features. Experimental results confirm that RED effectively
mitigates negative transfer and achieves state-of-the-art performance.

</details>


### [319] [Causal-Aware Generative Adversarial Networks with Reinforcement Learning](https://arxiv.org/abs/2510.24046)
*Tu Anh Hoang Nguyen,Dang Nguyen,Tri-Nhan Vo,Thuc Duy Le,Sunil Gupta*

Main category: cs.LG

TL;DR: CA-GAN是一种面向真实场景表格数据的新型生成框架，通过因果图提取与条件WGAN-GP结合，并引入强化学习目标，有效保持因果性、数据效用并提供隐私保障。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据生成方法在捕捉复杂因果关系、保持数据效用和提供可证明的隐私保护方面存在不足，难以满足企业级应用需求。

Method: 采用两步法：首先提取数据中的因果图，然后基于该图结构设计条件WGAN-GP生成器；生成器通过强化学习目标对齐真实与生成数据的因果图。

Result: 在14个表格数据集上优于6种SOTA方法，在因果保持、数据效用和隐私保护三项核心指标上表现优异。

Conclusion: CA-GAN为企业提供了高性能、实用的合成数据生成方案，适用于数据库基准测试、软件开发加速和安全的数据驱动研究。

Abstract: The utility of tabular data for tasks ranging from model training to
large-scale data analysis is often constrained by privacy concerns or
regulatory hurdles. While existing data generation methods, particularly those
based on Generative Adversarial Networks (GANs), have shown promise, they
frequently struggle with capturing complex causal relationship, maintaining
data utility, and providing provable privacy guarantees suitable for enterprise
deployment. We introduce CA-GAN, a novel generative framework specifically
engineered to address these challenges for real-world tabular datasets. CA-GAN
utilizes a two-step approach: causal graph extraction to learn a robust,
comprehensive causal relationship in the data's manifold, followed by a custom
Conditional WGAN-GP (Wasserstein GAN with Gradient Penalty) that operates
exclusively as per the structure of nodes in the causal graph. More
importantly, the generator is trained with a new Reinforcement Learning-based
objective that aligns the causal graphs constructed from real and fake data,
ensuring the causal awareness in both training and sampling phases. We
demonstrate CA-GAN superiority over six SOTA methods across 14 tabular
datasets. Our evaluations, focused on core data engineering metrics: causal
preservation, utility preservation, and privacy preservation. Our method offers
a practical, high-performance solution for data engineers seeking to create
high-quality, privacy-compliant synthetic datasets to benchmark database
systems, accelerate software development, and facilitate secure data-driven
research.

</details>


### [320] [Learning from History: A Retrieval-Augmented Framework for Spatiotemporal Prediction](https://arxiv.org/abs/2510.24049)
*Hao Jia,Penghao Zhao,Hao Wu,Yuan Gao,Yangyu Tao,Bin Cui*

Main category: cs.LG

TL;DR: 提出了一种检索增强预测（RAP）框架，结合深度学习模型与历史数据，提升复杂物理系统的长期时空预测准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在长期自回归预测中容易积累误差，导致物理上不合理的预测结果，难以准确捕捉系统内在动力学约束。

Method: 提出检索增强预测（RAP）框架，通过从大规模数据库中检索最相似的历史状态作为非参数估计，并将其真实演化作为参考目标，引入双流架构提供动态引导，而非硬性损失约束。

Result: 在气象、湍流和火灾模拟等多个领域测试中，RAP优于现有最先进方法，并显著超过类比预测基线，有效抑制长期预测中的误差发散。

Conclusion: RAP通过融合历史数据的动态指导与深度学习的预测能力，实现了更物理可信的长期预测，为科学计算中的复杂系统建模提供了新范式。

Abstract: Accurate and long-term spatiotemporal prediction for complex physical systems
remains a fundamental challenge in scientific computing. While deep learning
models, as powerful parametric approximators, have shown remarkable success,
they suffer from a critical limitation: the accumulation of errors during
long-term autoregressive rollouts often leads to physically implausible
artifacts. This deficiency arises from their purely parametric nature, which
struggles to capture the full constraints of a system's intrinsic dynamics. To
address this, we introduce a novel \textbf{Retrieval-Augmented Prediction
(RAP)} framework, a hybrid paradigm that synergizes the predictive power of
deep networks with the grounded truth of historical data. The core philosophy
of RAP is to leverage historical evolutionary exemplars as a non-parametric
estimate of the system's local dynamics. For any given state, RAP efficiently
retrieves the most similar historical analog from a large-scale database. The
true future evolution of this analog then serves as a \textbf{reference
target}. Critically, this target is not a hard constraint in the loss function
but rather a powerful conditional input to a specialized dual-stream
architecture. It provides strong \textbf{dynamic guidance}, steering the
model's predictions towards physically viable trajectories. In extensive
benchmarks across meteorology, turbulence, and fire simulation, RAP not only
surpasses state-of-the-art methods but also significantly outperforms a strong
\textbf{analog-only forecasting baseline}. More importantly, RAP generates
predictions that are more physically realistic by effectively suppressing error
divergence in long-term rollouts.

</details>


### [321] [Low-N Protein Activity Optimization with FolDE](https://arxiv.org/abs/2510.24053)
*Jacob B. Roberts,Catherine R. Ji,Isaac Donnell,Thomas D. Young,Allison N. Pearson,Graham A. Hudson,Leah S. Keiser,Mia Wesselkamper,Peter H. Winegar,Janik Ludwig,Sarah H. Klass,Isha V. Sheth,Ezechinyere C. Ukabiala,Maria C. T. Astolfi,Benjamin Eysenbach,Jay D. Keasling*

Main category: cs.LG

TL;DR: FolDE是一种新的主动学习辅助定向进化方法，通过自然性启发的预训练和多样化的批量选择策略，在蛋白质优化中显著提高了发现高活性突变体的效率。


<details>
  <summary>Details</summary>
Motivation: 传统蛋白质优化方法成本高、效率低，现有主动学习辅助方法因训练数据同质化导致预测模型准确性不足，需要更有效的算法来提升定向进化的效果和效率。

Method: 提出FolDE方法，采用基于自然性的warm-starting策略，结合蛋白质语言模型输出增强活性预测；引入constant-liar批量选择器以提高突变体批次的多样性，并在20个蛋白质靶标上进行模拟实验验证。

Result: 在20个蛋白质靶标的模拟中，FolDE比最佳基线方法多发现了23%的前10%高活性突变体（p=0.005），发现前1%突变体的可能性高出55%；批量选择器对多突变实验有益，但在当前基准中效果有限。

Conclusion: FolDE通过改进预测模型的初始化和训练数据多样性，显著提升了定向进化后期的成功率，且其开源工作流可广泛应用于各类实验室的高效蛋白质优化。

Abstract: Proteins are traditionally optimized through the costly construction and
measurement of many mutants. Active Learning-assisted Directed Evolution (ALDE)
alleviates that cost by predicting the best improvements and iteratively
testing mutants to inform predictions. However, existing ALDE methods face a
critical limitation: selecting the highest-predicted mutants in each round
yields homogeneous training data insufficient for accurate prediction models in
subsequent rounds. Here we present FolDE, an ALDE method designed to maximize
end-of-campaign success. In simulations across 20 protein targets, FolDE
discovers 23% more top 10% mutants than the best baseline ALDE method (p=0.005)
and is 55% more likely to find top 1% mutants. FolDE achieves this primarily
through naturalness-based warm-starting, which augments limited activity
measurements with protein language model outputs to improve activity
prediction. We also introduce a constant-liar batch selector, which improves
batch diversity; this is important in multi-mutation campaigns but had limited
effect in our benchmarks. The complete workflow is freely available as
open-source software, making efficient protein optimization accessible to any
laboratory.

</details>


### [322] [FALQON: Accelerating LoRA Fine-tuning with Low-Bit Floating-Point Arithmetic](https://arxiv.org/abs/2510.24061)
*Kanghyun Choi,Hyeyoon Lee,SunJong Park,Dain Kwon,Jinho Lee*

Main category: cs.LG

TL;DR: 提出FALQON框架，通过在微调期间将LoRA适配器直接合并到FP8量化主干中，消除低秩适应中的量化开销，实现约3倍的训练加速。


<details>
  <summary>Details</summary>
Motivation: 现有的FP8量化在低秩适应（LoRA）中因小维度矩阵导致量化开销大，难以发挥加速优势。

Method: 提出FALQON框架，将LoRA适配器直接合并到FP8量化主干；重构前向和反向计算以减少量化开销；引入行级代理更新机制，高效集成更新到量化主干。

Result: 实验表明，FALQON相比现有量化LoRA方法实现约3倍训练速度提升，且保持相似精度，并支持端到端FP8流程，无需后训练量化。

Conclusion: FALQON为大规模模型的高效微调提供了实用解决方案，显著提升训练效率并简化部署流程。

Abstract: Low-bit floating-point (FP) formats, such as FP8, provide significant
acceleration and memory savings in model training thanks to native hardware
support on modern GPUs and NPUs. However, we analyze that FP8 quantization
offers speedup primarily for large-dimensional matrix multiplications, while
inherent quantization overheads diminish speedup when applied to low-rank
adaptation (LoRA), which uses small-dimensional matrices for efficient
fine-tuning of large language models (LLMs). To address this limitation, we
propose FALQON, a novel framework that eliminates the quantization overhead
from separate LoRA computational paths by directly merging LoRA adapters into
an FP8-quantized backbone during fine-tuning. Furthermore, we reformulate the
forward and backward computations for merged adapters to significantly reduce
quantization overhead, and introduce a row-wise proxy update mechanism that
efficiently integrates substantial updates into the quantized backbone.
Experimental evaluations demonstrate that FALQON achieves approximately a
3$\times$ training speedup over existing quantized LoRA methods with a similar
level of accuracy, providing a practical solution for efficient large-scale
model fine-tuning. Moreover, FALQON's end-to-end FP8 workflow removes the need
for post-training quantization, facilitating efficient deployment. Code is
available at https://github.com/iamkanghyunchoi/falqon.

</details>


### [323] [Information-Theoretic Discrete Diffusion](https://arxiv.org/abs/2510.24088)
*Moongyu Jeon,Sangwoo Shin,Dongjae Jeon,Albert No*

Main category: cs.LG

TL;DR: 提出了一种基于信息论的离散扩散模型框架，通过得分匹配损失来估计对数似然，并推导出I-MDSE和I-MDCE关系，将互信息与最小去噪得分熵/交叉熵损失联系起来，证明了常用损失函数是精确且有原则的对数似然估计器。


<details>
  <summary>Details</summary>
Motivation: 受高斯情形下I-MMSE恒等式的启发，希望在离散扩散模型中建立类似的理论基础，以提供更原理性的对数似然估计方法。

Method: 引入信息-最小去噪得分熵（I-MDSE）关系及其在掩码扩散下的扩展I-MDCE关系，利用互信息与得分匹配损失之间的联系，构建对数似然的时间积分分解。

Result: 建立了离散扩散模型中互信息与最优得分损失之间的等价关系，证明DSE和DCE损失是紧致且无偏的对数似然估计；提出了时间自由公式、条件似然估计和蒙特卡洛似然比估计等实用扩展；实验验证了估计器的准确性与稳定性。

Conclusion: 所提出的I-MDSE和I-MDCE关系为离散扩散模型提供了坚实的理论基础，表明常用损失函数可作为原理性、可解释的对数似然估计工具，具有良好的实际性能。

Abstract: We present an information-theoretic framework for discrete diffusion models
that yields principled estimators of log-likelihood using score-matching
losses. Inspired by the I-MMSE identity for the Gaussian setup, we derive
analogous results for the discrete setting. Specifically, we introduce the
Information-Minimum Denoising Score Entropy (I-MDSE) relation, which links
mutual information between data and its diffused version to the minimum
denoising score entropy (DSE) loss. We extend this theory to masked diffusion
and establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)
relation, connecting cross-entropy losses to mutual information in discrete
masked processes. These results provide a time-integral decomposition of the
log-likelihood of the data in terms of optimal score-based losses, showing that
commonly used losses such as DSE and DCE are not merely variational bounds but
tight and principled estimators of log-likelihood. The I-MDCE decomposition
further enables practical extensions, including time-free formula, conditional
likelihood estimation in prompt-response tasks, and coupled Monte Carlo
estimation of likelihood ratios. Experiments on synthetic and real-world data
confirm the accuracy, variance stability, and utility of our estimators. The
code is publicly available at https://github.com/Dongjae0324/infodis.

</details>


### [324] [Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2510.24120)
*Ziyu Liu,Yijing Liu,Jianfei Yuan,Minzhi Yan,Le Yue,Honghui Xiong,Yi Yang*

Main category: cs.LG

TL;DR: 提出了一种名为G2ConS的图引导概念选择方法，通过减少知识图谱构建成本并弥补知识缺口，在多跳推理任务中实现了更高效的检索和问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的知识增强检索（Graph-based RAG）方法在构建知识图谱时需要大量调用大语言模型，导致高昂的成本，尤其是在大规模应用时。

Method: 设计了块选择方法和无需大语言模型的概念图：前者选择重要的文档块以降低构建成本，后者以零成本弥补因块选择导致的知识缺失。

Result: 在多个真实世界数据集上的实验表明，G2ConS在构建成本、检索效果和回答质量方面均优于所有基线方法。

Conclusion: G2ConS通过有选择性地构建知识图谱，在显著降低成本的同时保持甚至提升了检索与问答性能，适用于需要多跳推理的专业领域。

Abstract: Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance
retrieval in Large Language Model (LLM)-based question answering. It is
especially beneficial in domains such as biomedicine, law, and political
science, where effective retrieval often involves multi-hop reasoning over
proprietary documents. However, these methods demand numerous LLM calls to
extract entities and relations from text chunks, incurring prohibitive costs at
scale. Through a carefully designed ablation study, we observe that certain
words (termed concepts) and their associated documents are more important.
Based on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its
core comprises a chunk selection method and an LLM-independent concept graph.
The former selects salient document chunks to reduce KG construction costs; the
latter closes knowledge gaps introduced by chunk selection at zero cost.
Evaluations on multiple real-world datasets show that G2ConS outperforms all
baselines in construction cost, retrieval effectiveness, and answering quality.

</details>


### [325] [Causal Convolutional Neural Networks as Finite Impulse Response Filters](https://arxiv.org/abs/2510.24125)
*Kiran Bacsa,Wei Liu,Xudong Jian,Huangbin Liang,Eleni Chatzi*

Main category: cs.LG

TL;DR: 该研究发现，具有准线性激活函数的因果卷积神经网络在处理多模态频率内容的时间序列数据时，训练后表现出类似有限冲激响应（FIR）滤波器的特性，尤其当卷积核较长时。通过卷积的结合律，整个网络可简化为一个等效的单层FIR滤波器，从而提升对动态系统频谱学习行为的可解释性，并在模拟和真实振动数据上得到验证。


<details>
  <summary>Details</summary>
Motivation: 提高因果CNN在处理复杂时间序列（如多模态频率信号）时的可解释性，并揭示其频谱学习机制。

Method: 分析训练后的因果CNN在长卷积核下的行为，利用卷积的结合性将其简化为等效的单层FIR滤波器，并从最小二乘优化角度解释其频谱特征提取机制。

Result: 证明了因果CNN可显式和隐式捕捉频谱特征，网络整体等效于一个经最小二乘优化的FIR滤波器，尤其适用于稀疏频率信号的学习。

Conclusion: 因果CNN在处理动态系统时间序列时具有类似FIR滤波器的行为，该等效模型增强了网络的可解释性，为物理系统的建模与识别提供了理论支持。

Abstract: This study investigates the behavior of Causal Convolutional Neural Networks
(CNNs) with quasi-linear activation functions when applied to time-series data
characterized by multimodal frequency content. We demonstrate that, once
trained, such networks exhibit properties analogous to Finite Impulse Response
(FIR) filters, particularly when the convolutional kernels are of extended
length exceeding those typically employed in standard CNN architectures. Causal
CNNs are shown to capture spectral features both implicitly and explicitly,
offering enhanced interpretability for tasks involving dynamic systems.
Leveraging the associative property of convolution, we further show that the
entire network can be reduced to an equivalent single-layer filter resembling
an FIR filter optimized via least-squares criteria. This equivalence yields new
insights into the spectral learning behavior of CNNs trained on signals with
sparse frequency content. The approach is validated on both simulated beam
dynamics and real-world bridge vibration datasets, underlining its relevance
for modeling and identifying physical systems governed by dynamic responses.

</details>


### [326] [What do vision-language models see in the context? Investigating multimodal in-context learning](https://arxiv.org/abs/2510.24331)
*Gabriel O. dos Santos,Esther Colombini,Sandra Avila*

Main category: cs.LG

TL;DR: 本研究系统分析了视觉-语言模型（VLMs）中的上下文学习（ICL），发现当前VLMs在多模态ICL中主要依赖文本线索，难以有效整合视觉信息，且指令微调可能削弱对上下文示例的依赖。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文学习在大语言模型中已被广泛研究，但其在视觉-语言模型中的有效性仍缺乏探索。因此，本文旨在系统评估VLMs在多模态场景下的ICL能力。

Method: 研究评估了七种涵盖四种架构的VLM，在三个图像描述基准上分析提示设计、架构选择和训练策略对ICL的影响，并首次研究了随着上下文示例增加，VLM中注意力模式的变化。

Result: 训练于图文交错数据可提升ICL表现，但未实现有效的多模态信息融合；指令微调增强了指令遵循能力，却降低了对上下文示例的依赖；注意力分析显示当前VLM主要关注文本线索，未能充分利用视觉信息。

Conclusion: 当前VLMs在多模态上下文学习中存在显著局限，尤其在视觉与文本信息整合方面不足，未来需改进模型以更好利用多模态上下文示例进行学习。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks
from demonstration examples without parameter updates. Although it has been
extensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)
remains underexplored. In this work, we present a systematic study of ICL in
VLMs, evaluating seven models spanning four architectures on three image
captioning benchmarks. We analyze how prompt design, architectural choices, and
training strategies influence multimodal ICL. To our knowledge, we are the
first to analyze how attention patterns in VLMs vary with an increasing number
of in-context demonstrations. Our results reveal that training on imag-text
interleaved data enhances ICL performance but does not imply effective
integration of visual and textual information from demonstration examples. In
contrast, instruction tuning improves instruction-following but can reduce
reliance on in-context demonstrations, suggesting a trade-off between
instruction alignment and in-context adaptation. Attention analyses further
show that current VLMs primarily focus on textual cues and fail to leverage
visual information, suggesting a limited capacity for multimodal integration.
These findings highlight key limitations in the ICL abilities of current VLMs
and provide insights for enhancing their ability to learn from multimodal
in-context examples.

</details>


### [327] [Fixed Point Neural Acceleration and Inverse Surrogate Model for Battery Parameter Identification](https://arxiv.org/abs/2510.24135)
*Hojin Cheon,Hyeongseok Seo,Jihun Jeon,Wooju Lee,Dohyun Jeong,Hongseok Kim*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的电化学电池模型参数识别框架，结合神经代理模型和固定点迭代方法，显著提高了锂离子电池诊断的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的元启发式方法计算成本高、收敛慢，而机器学习方法依赖恒定电流数据，在实际应用中受限。因此需要一种高效且适用于动态工况的电池参数识别方法。

Method: 提出NeuralSPMe作为单粒子模型的神经代理模型，并结合参数更新网络（PUNet）进行固定点迭代优化。模型在真实电动汽车负载条件下训练，以实现对锂浓度动态的准确预测和快速参数识别。

Result: 该框架比传统元启发式算法加速超过2000倍，样本效率更高，精度提升10倍以上，尤其在动态负载场景下表现优异。

Conclusion: 所提深度学习框架显著提升了锂离子电池电化学模型参数识别的速度与精度，适用于实际电动汽车运行条件下的电池健康状态评估。

Abstract: The rapid expansion of electric vehicles has intensified the need for
accurate and efficient diagnosis of lithium-ion batteries. Parameter
identification of electrochemical battery models is widely recognized as a
powerful method for battery health assessment. However, conventional
metaheuristic approaches suffer from high computational cost and slow
convergence, and recent machine learning methods are limited by their reliance
on constant current data, which may not be available in practice. To overcome
these challenges, we propose deep learning-based framework for parameter
identification of electrochemical battery models. The proposed framework
combines a neural surrogate model of the single particle model with electrolyte
(NeuralSPMe) and a deep learning-based fixed-point iteration method. NeuralSPMe
is trained on realistic EV load profiles to accurately predict lithium
concentration dynamics under dynamic operating conditions while a parameter
update network (PUNet) performs fixed-point iterative updates to significantly
reduce both the evaluation time per sample and the overall number of iterations
required for convergence. Experimental evaluations demonstrate that the
proposed framework accelerates the parameter identification by more than 2000
times, achieves superior sample efficiency and more than 10 times higher
accuracy compared to conventional metaheuristic algorithms, particularly under
dynamic load scenarios encountered in practical applications.

</details>


### [328] [Identifiable learning of dissipative dynamics](https://arxiv.org/abs/2510.24160)
*Aiqing Zhu,Beatrice W. Soh,Grigorios A. Pavliotis,Qianxiao Li*

Main category: cs.LG

TL;DR: 本文提出了I-OnsagerNet，一种用于从数据中学习复杂耗散随机动力学的神经网络框架，兼具可解释性与唯一性，适用于非平衡系统的建模与不可逆性的量化。


<details>
  <summary>Details</summary>
Motivation: 复杂耗散系统广泛存在于科学与工程中，但其远离平衡态的动力学难以从数据中准确量化，现有模型在表达力与物理合理性之间难以平衡。

Method: 基于Onsager原理扩展，结合Helmholtz分解，确保学习到的势函数来自稳态密度，并将漂移项分解为时间可逆与不可逆部分，从而保证模型的物理一致性与唯一性。

Result: 成功应用于聚合物拉伸和随机梯度Langevin动力学，揭示了势垒高度的超线性增长、熵产生率的次线性增长以及批量增大对不可逆性的抑制效应。

Conclusion: I-OnsagerNet为发现和解释非平衡动力学提供了一个通用且数据驱动的框架，能够有效量化不可逆性并保持模型的物理意义。

Abstract: Complex dissipative systems appear across science and engineering, from
polymers and active matter to learning algorithms. These systems operate far
from equilibrium, where energy dissipation and time irreversibility are key to
their behavior, but are difficult to quantify from data. Learning accurate and
interpretable models of such dynamics remains a major challenge: the models
must be expressive enough to describe diverse processes, yet constrained enough
to remain physically meaningful and mathematically identifiable. Here, we
introduce I-OnsagerNet, a neural framework that learns dissipative stochastic
dynamics directly from trajectories while ensuring both interpretability and
uniqueness. I-OnsagerNet extends the Onsager principle to guarantee that the
learned potential is obtained from the stationary density and that the drift
decomposes cleanly into time-reversible and time-irreversible components, as
dictated by the Helmholtz decomposition. Our approach enables us to calculate
the entropy production and to quantify irreversibility, offering a principled
way to detect and quantify deviations from equilibrium. Applications to polymer
stretching in elongational flow and to stochastic gradient Langevin dynamics
reveal new insights, including super-linear scaling of barrier heights and
sub-linear scaling of entropy production rates with the strain rate, and the
suppression of irreversibility with increasing batch size. I-OnsagerNet thus
establishes a general, data-driven framework for discovering and interpreting
non-equilibrium dynamics.

</details>


### [329] [EddyFormer: Accelerated Neural Simulations of Three-Dimensional Turbulence at Scale](https://arxiv.org/abs/2510.24173)
*Yiheng Du,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的谱元法架构EddyFormer，用于大规模湍流模拟，结合了谱方法的精度和注意力机制的可扩展性，在256^3分辨率下达到DNS级精度并实现30倍加速，且在未见域中表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于湍流的多尺度相互作用，通过直接数值模拟（DNS）完全解析大尺度湍流在计算上是不可行的，因此需要数据驱动的机器学习替代方案。

Method: 提出EddyFormer，一种基于Transformer的谱元法（SEM）架构，引入SEM分词技术将流动分解为网格尺度和亚网格尺度成分，以捕捉局部和全局特征。

Result: 在新构建的三维各向同性湍流数据集上训练，EddyFormer在256^3分辨率下达到DNS级精度，比DNS快30倍；在比训练域大4倍的未见域中仍保持物理不变量指标（如能谱、关联函数和结构函数）的准确性，并在The Well基准套件上成功模拟了先前ML模型无法收敛的案例。

Conclusion: EddyFormer有效结合了谱方法的高精度与Transformer的可扩展性，实现了高效、准确且具有良好泛化能力的大规模湍流模拟，展示了其在复杂湍流预测中的潜力。

Abstract: Computationally resolving turbulence remains a central challenge in fluid
dynamics due to its multi-scale interactions. Fully resolving large-scale
turbulence through direct numerical simulation (DNS) is computationally
prohibitive, motivating data-driven machine learning alternatives. In this
work, we propose EddyFormer, a Transformer-based spectral-element (SEM)
architecture for large-scale turbulence simulation that combines the accuracy
of spectral methods with the scalability of the attention mechanism. We
introduce an SEM tokenization that decomposes the flow into grid-scale and
subgrid-scale components, enabling capture of both local and global features.
We create a new three-dimensional isotropic turbulence dataset and train
EddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x
speedup over DNS. When applied to unseen domains up to 4x larger than in
training, EddyFormer preserves accuracy on physics-invariant metrics-energy
spectra, correlation functions, and structure functions-showing domain
generalization. On The Well benchmark suite of diverse turbulent flows,
EddyFormer resolves cases where prior ML models fail to converge, accurately
reproducing complex dynamics across a wide range of physical conditions.

</details>


### [330] [V-SAT: Video Subtitle Annotation Tool](https://arxiv.org/abs/2510.24180)
*Arpita Kundu,Joyita Chakraborty,Anindita Desarkar,Aritra Sen,Srushti Anil Patil,Vishwanathan Raman*

Main category: cs.LG

TL;DR: 本文提出了一种名为V-SAT的视频字幕标注工具，结合大语言模型、视觉-语言模型、图像处理和语音识别技术，统一解决现有字幕生成中的多种质量问题，显著提升了字幕的准确性和可读性。


<details>
  <summary>Details</summary>
Motivation: 现有字幕生成方法在同步性、文本准确性、格式一致性及适应动态音视频上下文方面存在不足，导致后期编辑耗时耗力，亟需一种自动化、综合性的解决方案。

Method: 提出V-SAT框架，融合大语言模型（LLMs）、视觉-语言模型（VLMs）、图像处理和自动语音识别（ASR），利用音视频上下文信息，自动检测并修复多种字幕质量问题，并引入人机协同验证机制确保输出质量。

Result: 在语言模式问题修复后，SUBER评分从9.6降至3.54；图像模式问题的F1得分约为0.80，显著提升字幕质量。

Conclusion: V-SAT是首个能够全面解决多模态字幕质量问题的综合性框架，通过多技术融合与人机协作，实现了高质量、鲁棒的自动字幕标注。

Abstract: The surge of audiovisual content on streaming platforms and social media has
heightened the demand for accurate and accessible subtitles. However, existing
subtitle generation methods primarily speech-based transcription or OCR-based
extraction suffer from several shortcomings, including poor synchronization,
incorrect or harmful text, inconsistent formatting, inappropriate reading
speeds, and the inability to adapt to dynamic audio-visual contexts. Current
approaches often address isolated issues, leaving post-editing as a
labor-intensive and time-consuming process. In this paper, we introduce V-SAT
(Video Subtitle Annotation Tool), a unified framework that automatically
detects and corrects a wide range of subtitle quality issues. By combining
Large Language Models(LLMs), Vision-Language Models (VLMs), Image Processing,
and Automatic Speech Recognition (ASR), V-SAT leverages contextual cues from
both audio and video. Subtitle quality improved, with the SUBER score reduced
from 9.6 to 3.54 after resolving all language mode issues and F1-scores of
~0.80 for image mode issues. Human-in-the-loop validation ensures high-quality
results, providing the first comprehensive solution for robust subtitle
annotation.

</details>


### [331] [SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary Learning](https://arxiv.org/abs/2510.24200)
*Alexander Bakarsky,Dimitar I. Dimitrov,Maximilian Baader,Martin Vechev*

Main category: cs.LG

TL;DR: 本文提出了SPEAR++，一种基于稀疏字典学习技术的梯度反演攻击方法，解决了原有SPEAR攻击在批量大小上指数级运行时间的问题，显著提升了对线性层与ReLU激活函数的梯度反演效率，同时保持了对差分隐私噪声和FedAvg聚合的鲁棒性，并能处理十倍更大的批量大小。


<details>
  <summary>Details</summary>
Motivation: 现有的梯度反演攻击多依赖于无理论保证的直接数据优化，导致实际系统隐私风险难以评估；SPEAR虽有理论突破但因在批量大小上运行时间呈指数增长而实用性受限，因此需要更高效且具可扩展性的攻击方法来评估联邦学习系统的实际隐私风险。

Method: 本文采用稀疏字典学习领域的最新技术，将线性层中带ReLU激活的梯度反演问题转化为可解的稀疏恢复问题，从而实现对较大批量数据的高效反演，使攻击复杂度大幅降低。

Result: 实验表明，SPEAR++在保持SPEAR对差分隐私噪声和FedAvg聚合鲁棒性的同时，能够处理比原方法大10倍的批量大小，显著提升了攻击的实用性和可扩展性。

Conclusion: 通过引入稀疏字典学习技术，SPEAR++成功地将理论驱动的梯度反演攻击变得实际可行，为评估联邦学习系统的隐私泄露风险提供了更强大的工具，并揭示了现有防御机制在面对先进攻击时的潜在不足。

Abstract: Federated Learning has seen an increased deployment in real-world scenarios
recently, as it enables the distributed training of machine learning models
without explicit data sharing between individual clients. Yet, the introduction
of the so-called gradient inversion attacks has fundamentally challenged its
privacy-preserving properties. Unfortunately, as these attacks mostly rely on
direct data optimization without any formal guarantees, the vulnerability of
real-world systems remains in dispute and requires tedious testing for each new
federated deployment. To overcome these issues, recently the SPEAR attack was
introduced, which is based on a theoretical analysis of the gradients of linear
layers with ReLU activations. While SPEAR is an important theoretical
breakthrough, the attack's practicality was severely limited by its exponential
runtime in the batch size b. In this work, we fill this gap by applying
State-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the
problem of gradient inversion on linear layers with ReLU activations tractable.
Our experiments demonstrate that our new attack, SPEAR++, retains all desirable
properties of SPEAR, such as robustness to DP noise and FedAvg aggregation,
while being applicable to 10x bigger batch sizes.

</details>


### [332] [Unlocking Out-of-Distribution Generalization in Dynamics through Physics-Guided Augmentation](https://arxiv.org/abs/2510.24216)
*Fan Xu,Hao Wu,Kun Wang,Nan Wang,Qingsong Wen,Xian Wu,Wei Gong,Xibin Zhao*

Main category: cs.LG

TL;DR: 提出SPARK，一种物理引导的定量增强插件，通过在潜在空间中进行有原则的插值生成新的物理上合理的训练样本，并结合傅里叶增强的图ODE进行下游预测，在数据稀缺和分布外场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法计算成本高，现代数据驱动方法在数据稀缺和分布变化时表现不佳。

Method: 利用重建自编码器将物理参数集成到物理丰富的离散状态字典中，并通过潜在空间中的插值生成新样本；结合傅里叶增强的图ODE进行预测。

Result: 在多个基准测试中，SPARK在分布外场景和数据稀缺情况下显著优于现有最先进方法。

Conclusion: SPARK通过物理引导的增强范式有效提升了动力系统建模的性能和鲁棒性。

Abstract: In dynamical system modeling, traditional numerical methods are limited by
high computational costs, while modern data-driven approaches struggle with
data scarcity and distribution shifts. To address these fundamental
limitations, we first propose SPARK, a physics-guided quantitative augmentation
plugin. Specifically, SPARK utilizes a reconstruction autoencoder to integrate
physical parameters into a physics-rich discrete state dictionary. This state
dictionary then acts as a structured dictionary of physical states, enabling
the creation of new, physically-plausible training samples via principled
interpolation in the latent space. Further, for downstream prediction, these
augmented representations are seamlessly integrated with a Fourier-enhanced
Graph ODE, a combination designed to robustly model the enriched data
distribution while capturing long-term temporal dependencies. Extensive
experiments on diverse benchmarks demonstrate that SPARK significantly
outperforms state-of-the-art baselines, particularly in challenging
out-of-distribution scenarios and data-scarce regimes, proving the efficacy of
our physics-guided augmentation paradigm.

</details>


### [333] [Closing Gaps: An Imputation Analysis of ICU Vital Signs](https://arxiv.org/abs/2510.24217)
*Alisher Turubayev,Anna Shopova,Fabian Lange,Mahmut Kamalak,Paul Mattes,Victoria Ayvasky,Bert Arnrich,Bjarne Pfitzner,Robin P. van de Water*

Main category: cs.LG

TL;DR: 本文介绍了一个可扩展和可重用的基准，用于比较15种插补方法和4种缺失方法在主要ICU数据集上的表现，以指导研究人员选择最准确的插补技术来提高临床预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 由于ICU数据中存在大量缺失值，影响了机器学习在临床预测中的应用，因此需要系统地比较不同时间序列插补方法的效果，以确定最佳实践。

Method: 建立一个包含15种插补方法和4种人为制造缺失的方法的基准框架，并在主要ICU数据集上进行评估与比较。

Result: 提供了一个系统的比较结果，揭示了不同插补方法在ICU生命体征数据上的表现差异，表明常用的经验性插补方法（如零值插补）可能降低预测准确性。

Conclusion: 通过所提出的基准，研究者可以更好地选择合适的插补方法，从而提升临床预测模型的性能，推动更多机器学习模型进入临床实践。

Abstract: As more Intensive Care Unit (ICU) data becomes available, the interest in
developing clinical prediction models to improve healthcare protocols
increases. However, the lack of data quality still hinders clinical prediction
using Machine Learning (ML). Many vital sign measurements, such as heart rate,
contain sizeable missing segments, leaving gaps in the data that could
negatively impact prediction performance. Previous works have introduced
numerous time-series imputation techniques. Nevertheless, more comprehensive
work is needed to compare a representative set of methods for imputing ICU
vital signs and determine the best practice. In reality, ad-hoc imputation
techniques that could decrease prediction accuracy, like zero imputation, are
still used. In this work, we compare established imputation techniques to guide
researchers in improving the performance of clinical prediction models by
selecting the most accurate imputation technique. We introduce an extensible
and reusable benchmark with currently 15 imputation and 4 amputation methods,
created for benchmarking on major ICU datasets. We hope to provide a
comparative basis and facilitate further ML development to bring more models
into clinical practice.

</details>


### [334] [PRIVET: Privacy Metric Based on Extreme Value Theory](https://arxiv.org/abs/2510.24233)
*Antoine Szatkownik,Aurélien Decelle,Beatriz Seoane,Nicolas Bereux,Léo Planche,Guillaume Charpiat,Burak Yelmen,Flora Jay,Cyril Furtlehner*

Main category: cs.LG

TL;DR: 提出了一种名为PRIVET的通用、基于样本、模态无关的算法，通过极端值统计和最近邻距离为每个合成样本分配个体隐私泄露分数，能够可靠地检测多种数据模态下的记忆化和隐私泄露现象。


<details>
  <summary>Details</summary>
Motivation: 现有隐私风险评估方法多依赖全局指标，缺乏对单个样本隐私泄露的可解释性评估，限制了合成数据在实际场景中的应用。需要一种能在样本级别量化隐私泄露的严谨方法。

Method: 利用极端值统计结合最近邻距离，设计PRIVET算法，为每个合成样本计算隐私泄露分数，实现模态无关的样本级隐私评估，并在不同数据模态和训练状态下验证其有效性。

Result: PRIVET能有效检测高维、小样本（如基因数据）甚至欠拟合情况下的隐私泄露，相比现有方法可同时提供数据集级别和样本级别的定性与定量评估结果，并揭示了现有计算机视觉嵌入在识别近似重复样本时的感知距离局限性。

Conclusion: PRIVET是一种有效的样本级隐私评估工具，具有跨模态适用性和鲁棒性，有助于提升合成数据在敏感领域的可信度和实际部署能力。

Abstract: Deep generative models are often trained on sensitive data, such as genetic
sequences, health data, or more broadly, any copyrighted, licensed or protected
content. This raises critical concerns around privacy-preserving synthetic
data, and more specifically around privacy leakage, an issue closely tied to
overfitting. Existing methods almost exclusively rely on global criteria to
estimate the risk of privacy failure associated to a model, offering only
quantitative non interpretable insights. The absence of rigorous evaluation
methods for data privacy at the sample-level may hinder the practical
deployment of synthetic data in real-world applications. Using extreme value
statistics on nearest-neighbor distances, we propose PRIVET, a generic
sample-based, modality-agnostic algorithm that assigns an individual privacy
leak score to each synthetic sample. We empirically demonstrate that PRIVET
reliably detects instances of memorization and privacy leakage across diverse
data modalities, including settings with very high dimensionality, limited
sample sizes such as genetic data and even under underfitting regimes. We
compare our method to existing approaches under controlled settings and show
its advantage in providing both dataset level and sample level assessments
through qualitative and quantitative outputs. Additionally, our analysis
reveals limitations in existing computer vision embeddings to yield
perceptually meaningful distances when identifying near-duplicate samples.

</details>


### [335] [Sparse Optimistic Information Directed Sampling](https://arxiv.org/abs/2510.24234)
*Ludovic Schwartz,Hamish Flynn,Gergely Neu*

Main category: cs.LG

TL;DR: 本文提出了稀疏乐观信息导向采样（SOIDS）算法，首次在数据丰富和数据匮乏两种情况下同时实现了最优的最坏情况遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有算法难以在数据丰富和数据匮乏两种情况下同时达到最优性能，本文旨在设计一种能在两种情况下自适应并实现最优遗憾的算法。

Method: 通过引入时间依赖的学习率进行新颖分析，采用稀疏乐观信息导向采样（SOIDS）方法，在非贝叶斯设定下实现信息与遗憾的最优平衡。

Result: SOIDS算法在最坏情况下同时达到了数据丰富和数据匮乏两种情形下的最优遗憾界，并通过实验验证了其良好性能。

Conclusion: SOIDS是首个在两种数据条件下均实现最优最坏情况遗憾的算法，扩展了IDS的理论保证，具有更强的适应性和理论优势。

Abstract: Many high-dimensional online decision-making problems can be modeled as
stochastic sparse linear bandits. Most existing algorithms are designed to
achieve optimal worst-case regret in either the data-rich regime, where
polynomial dependence on the ambient dimension is unavoidable, or the data-poor
regime, where dimension-independence is possible at the cost of worse
dependence on the number of rounds. In contrast, the sparse Information
Directed Sampling (IDS) algorithm satisfies a Bayesian regret bound that has
the optimal rate in both regimes simultaneously. In this work, we explore the
use of Sparse Optimistic Information Directed Sampling (SOIDS) to achieve the
same adaptivity in the worst-case setting, without Bayesian assumptions.
Through a novel analysis that enables the use of a time-dependent learning
rate, we show that SOIDS can optimally balance information and regret. Our
results extend the theoretical guarantees of IDS, providing the first algorithm
that simultaneously achieves optimal worst-case regret in both the data-rich
and data-poor regimes. We empirically demonstrate the good performance of
SOIDS.

</details>


### [336] [PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling](https://arxiv.org/abs/2510.24235)
*Ai Jian,Jingqing Ruan,Xing Ma,Dailin Li,QianLin Zhou,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 本文提出了PaTaRM，一种结合偏好感知奖励机制与动态评分标准自适应的统一奖励模型框架，通过利用成对数据中的相对偏好信息生成鲁棒的点式训练信号，无需显式点式标签，并在多个基准上显著提升了奖励建模和下游RLHF性能。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式奖励模型训练范式存在局限：成对方法依赖二元标签，导致推理不匹配；点式方法需要复杂的绝对标注，适应性差且标注成本高。因此需要一种更高效、通用且可解释的奖励建模方法。

Method: 提出Preference-Aware Task-Adaptive Reward Model (PaTaRM)，包含两个核心组件：1) 偏好感知奖励（PAR）机制，利用成对数据中的相对偏好构建点式训练信号；2) 任务自适应评分标准系统，动态生成全局任务一致性和实例细粒度推理的评估准则。

Result: 在Qwen3-8B和Qwen3-14B模型上，PaTaRM在RewardBench和RMBench上平均相对提升4.7%；在下游RLHF任务中，在IFEval和InFoBench上平均提升13.6%，验证了其有效性与鲁棒性。

Conclusion: PaTaRM通过融合相对偏好信息与动态评分标准，实现了无需显式点式标注的高效、通用且可解释的奖励建模，显著提升了奖励模型性能及下游RLHF效果。

Abstract: Reward models (RMs) are central to reinforcement learning from human feedback
(RLHF), providing the critical supervision signals that align large language
models (LLMs) with human preferences. While generative reward models (GRMs)
offer greater interpretability than traditional scalar RMs, current training
paradigms remain limited. Pair-wise methods rely on binary good-versus-bad
labels, which cause mismatches for point-wise inference and necessitate complex
pairing strategies for effective application in RLHF. On the other hand,
point-wise methods require more elaborate absolute labeling with rubric-driven
criteria, resulting in poor adaptability and high annotation costs. In this
work, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a
unified framework that integrates a preference-aware reward (PAR) mechanism
with dynamic rubric adaptation. PaTaRM leverages relative preference
information from pairwise data to construct robust point-wise training signals,
eliminating the need for explicit point-wise labels. Simultaneously, it employs
a task-adaptive rubric system that flexibly generates evaluation criteria for
both global task consistency and instance-specific fine-grained reasoning. This
design enables efficient, generalizable, and interpretable reward modeling for
RLHF. Extensive experiments show that PaTaRM achieves an average relative
improvement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B
models. Furthermore, PaTaRM boosts downstream RLHF performance, with an average
improvement of 13.6% across IFEval and InFoBench benchmarks, confirming its
effectiveness and robustness. Our code is available at
https://github.com/JaneEyre0530/PaTaRM.

</details>


### [337] [Temporal Knowledge Graph Hyperedge Forecasting: Exploring Entity-to-Category Link Prediction](https://arxiv.org/abs/2510.24240)
*Edward Markai,Sina Molavipour*

Main category: cs.LG

TL;DR: 本文提出了一种扩展的基于规则的框架（TLogic），通过引入实体类别来提高时序知识图谱推理的准确性和可解释性，并利用大语言模型数据驱动地生成未知类别，同时研究了类别预测中的聚合方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于嵌入的方法缺乏可解释性，难以让用户理解预测依据；同时，如何有效利用实体类别信息以提升推理性能仍需探索。

Method: 扩展TLogic框架，引入包含实体类别的新规则格式；对于未知类别，采用基于大语言模型的数据驱动方法进行生成；研究不同聚合方法在实体类别预测中的效果。

Result: 所提方法在保持高预测准确性的同时，提供了可解释的规则基础；通过引入实体类别限制规则应用范围，提升了推理效率与合理性；LLM生成类别和合适的聚合策略对性能有积极影响。

Conclusion: 结合实体类别的规则扩展框架在时序知识图谱推理中实现了高精度与可解释性的平衡，且通过LLM生成类别增强了实用性，为未来事件预测提供了透明可信的解决方案。

Abstract: Temporal Knowledge Graphs have emerged as a powerful way of not only modeling
static relationships between entities but also the dynamics of how relations
evolve over time. As these informational structures can be used to store
information from a real-world setting, such as a news flow, predicting future
graph components to a certain extent equates predicting real-world events. Most
of the research in this field focuses on embedding-based methods, often
leveraging convolutional neural net architectures. These solutions act as black
boxes, limiting insight. In this paper, we explore an extension to an
established rule-based framework, TLogic, that yields a high accuracy in
combination with explainable predictions. This offers transparency and allows
the end-user to critically evaluate the rules applied at the end of the
prediction stage. The new rule format incorporates entity category as a key
component with the purpose of limiting rule application only to relevant
entities. When categories are unknown for building the graph, we propose a
data-driven method to generate them with an LLM-based approach. Additionally,
we investigate the choice of aggregation method for scores of retrieved
entities when performing category prediction.

</details>


### [338] [SALS: Sparse Attention in Latent Space for KV cache Compression](https://arxiv.org/abs/2510.24273)
*Junlin Mu,Hantao Huang,Jihang Zhang,Minghui Yu,Tao Wang,Yidong Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为SALS的新框架，通过在潜在空间中进行稀疏注意力计算，有效压缩KV缓存并加速大语言模型的长序列推理，同时保持较高的精度。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理长上下文时面临KV缓存过大和内存带宽需求高的挑战，而传统低秩压缩方法因RoPE机制导致精度下降或速度瓶颈。

Method: 基于RoPE增加键向量秩和变换后键在潜在空间中表示稳定的观察，SALS将KV缓存投影到低秩潜在空间，并在此空间内进行无RoPE的查询-键交互以实现稀疏token选择，仅重构重要token避免全重建开销。

Result: 在LLaMA2-7b-chat、Mistral-7b和LLaMA3.1-8B-Instruct等模型上验证了SALS的有效性，在4K序列上实现了6.4倍KV缓存压缩和5.7倍注意力运算加速，端到端吞吐量在4K和32K序列上分别提升1.4倍和4.5倍。

Conclusion: SALS通过在潜在空间中结合低秩投影与稀疏注意力，解决了RoPE带来的压缩与效率难题，显著提升了长序列推理效率，具备良好的可扩展性和实际应用价值。

Abstract: Large Language Models capable of handling extended contexts are in high
demand, yet their inference remains challenging due to substantial Key-Value
cache size and high memory bandwidth requirements. Previous research has
demonstrated that KV cache exhibits low-rank characteristics within the hidden
dimension, suggesting the potential for effective compression. However, due to
the widely adopted Rotary Position Embedding mechanism in modern LLMs, naive
low-rank compression suffers severe accuracy degradation or creates a new speed
bottleneck, as the low-rank cache must first be reconstructed in order to apply
RoPE. In this paper, we introduce two key insights: first, the application of
RoPE to the key vectors increases their variance, which in turn results in a
higher rank; second, after the key vectors are transformed into the latent
space, they largely maintain their representation across most layers. Based on
these insights, we propose the Sparse Attention in Latent Space framework. SALS
projects the KV cache into a compact latent space via low-rank projection, and
performs sparse token selection using RoPE-free query-key interactions in this
space. By reconstructing only a small subset of important tokens, it avoids the
overhead of full KV cache reconstruction. We comprehensively evaluate SALS on
various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and
additionally verify its scalability on the RULER-128k benchmark with
LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA
performance by maintaining competitive accuracy. Under different settings, SALS
achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention
operator compared to FlashAttention2 on the 4K sequence. For the end-to-end
throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared
to GPT-fast on 4k and 32K sequences, respectively.

</details>


### [339] [EDC: Equation Discovery for Classification](https://arxiv.org/abs/2510.24310)
*Guus Toussaint,Arno Knobbe*

Main category: cs.LG

TL;DR: 本文提出了一种基于方程发现（ED）的二分类框架EDC，能够发现简洁且可解释的决策边界函数，在人工和真实数据上表现优于现有ED方法，并达到与当前最佳二分类方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方程发现多用于回归任务，缺乏在分类任务中的有效应用，因此需要一种能自动发现分类决策边界的符号化方法。

Method: 提出EDC方法，采用适度复杂度的语法生成包含线性、二次、指数及特征乘积项的可加性表达式，以灵活建模决策边界，并通过进化算法搜索最优方程结构与参数。

Result: 在多个数据集上，EDC不仅成功发现了目标方程的结构和参数，分类性能优于现有ED方法，且与主流分类器相当，同时保持模型简洁、可解释，不易过拟合。

Conclusion: EDC为二分类任务提供了一种有效的符号化建模范式，兼具高性能与可解释性，且语法可配置，适用于领域定制。

Abstract: Equation Discovery techniques have shown considerable success in regression
tasks, where they are used to discover concise and interpretable models
(\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary
classification framework. Our proposed method EDC finds analytical functions of
manageable size that specify the location and shape of the decision boundary.
In extensive experiments on artificial and real-life data, we demonstrate how
EDC is able to discover both the structure of the target equation as well as
the value of its parameters, outperforming the current state-of-the-art
ED-based classification methods in binary classification and achieving
performance comparable to the state of the art in binary classification. We
suggest a grammar of modest complexity that appears to work well on the tested
datasets but argue that the exact grammar -- and thus the complexity of the
models -- is configurable, and especially domain-specific expressions can be
included in the pattern language, where that is required. The presented grammar
consists of a series of summands (additive terms) that include linear,
quadratic and exponential terms, as well as products of two features (producing
hyperbolic curves ideal for capturing XOR-like dependencies). The experiments
demonstrate that this grammar allows fairly flexible decision boundaries while
not so rich to cause overfitting.

</details>


### [340] [Transformers can do Bayesian Clustering](https://arxiv.org/abs/2510.24318)
*Prajit Bhaskaran,Tom Viering*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的模型Cluster-PFN，用于实现可扩展且灵活的贝叶斯聚类，能有效处理缺失数据并准确估计聚类数量。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯聚类在大规模数据上计算开销大，且现有方法在处理缺失值时忽略不确定性，导致结果不理想。

Method: 基于Prior-Data Fitted Networks (PFNs) 构建Transformer模型，通过从有限高斯混合模型（GMM）先验生成的合成数据进行训练，学习对聚类数和聚类分配的后验分布进行估计，并支持包含缺失数据的复杂先验。

Result: Cluster-PFN在估计聚类数量上优于AIC、BIC和变分推断（VI），聚类质量与VI相当但速度快几个数量级，在高缺失率的真实基因组数据上优于基于插补的方法。

Conclusion: Cluster-PFN实现了高效、灵活且可扩展的贝叶斯聚类，能够有效处理缺失数据并提供可靠的不确定性估计。

Abstract: Bayesian clustering accounts for uncertainty but is computationally demanding
at scale. Furthermore, real-world datasets often contain missing values, and
simple imputation ignores the associated uncertainty, resulting in suboptimal
results. We present Cluster-PFN, a Transformer-based model that extends
Prior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained
entirely on synthetic datasets generated from a finite Gaussian Mixture Model
(GMM) prior, Cluster-PFN learns to estimate the posterior distribution over
both the number of clusters and the cluster assignments. Our method estimates
the number of clusters more accurately than handcrafted model selection
procedures such as AIC, BIC and Variational Inference (VI), and achieves
clustering quality competitive with VI while being orders of magnitude faster.
Cluster-PFN can be trained on complex priors that include missing data,
outperforming imputation-based baselines on real-world genomic datasets, at
high missingness. These results show that the Cluster-PFN can provide scalable
and flexible Bayesian clustering.

</details>


### [341] [Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning](https://arxiv.org/abs/2510.24356)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 本文提出了感知学习（PeL）范式，通过任务无关信号优化代理的感官接口，分离感知与决策过程，并定义了独立于目标的感知属性。


<details>
  <summary>Details</summary>
Motivation: 为了提升智能体在复杂环境中的感知能力，需要一种能够独立于具体任务优化感知模块的方法。

Method: 提出感知学习（PeL）框架，利用任务无关信号优化感官接口f_ϕ，并通过表示不变的客观度量评估感知特性；形式化感知与决策的分离，证明保持充分不变性的PeL更新与贝叶斯任务风险梯度正交。

Result: 成功形式化了感知与决策的分离，定义了稳定的感知属性，并提供了一套任务无关的评估指标来验证感知质量。

Conclusion: PeL为优化感知系统提供了新范式，能够在不依赖下游任务的情况下提升感知性能，具有广泛的应用潜力。

Abstract: We introduce Perception Learning (PeL), a paradigm that optimizes an agent's
sensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnostic
signals, decoupled from downstream decision learning
$g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-free
perceptual properties, such as stability to nuisances, informativeness without
collapse, and controlled geometry, assessed via objective
representation-invariant metrics. We formalize the separation of perception and
decision, define perceptual properties independent of objectives or
reparameterizations, and prove that PeL updates preserving sufficient
invariants are orthogonal to Bayes task-risk gradients. Additionally, we
provide a suite of task-agnostic evaluation metrics to certify perceptual
quality.

</details>


### [342] [Filtering instances and rejecting predictions to obtain reliable models in healthcare](https://arxiv.org/abs/2510.24368)
*Maria Gabriela Valeriano,David Kohan Marzagão,Alfredo Montelongo,Carlos Roberto Veiga Kiffer,Natan Katz,Ana Carolina Lorena*

Main category: cs.LG

TL;DR: 提出一种基于数据的两步方法，通过实例难度过滤和置信度过滤提升机器学习模型在医疗等高风险领域的可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在高风险领域（如医疗）中常忽略预测的不确定性，导致低置信度预测仍被输出，影响可靠性。

Method: 第一步在训练时使用实例难度（IH）过滤有问题的样本以提升数据质量；第二步在推理时引入基于置信度的拒绝机制，仅保留高置信度预测。同时使用影响值和不确定性作为基线对比。

Result: 在三个真实医疗数据集上验证表明，该方法能有效提升模型可靠性，在保持较高预测性能的同时控制拒绝率，且优于基于影响值和不确定性的基线方法。

Conclusion: 结合实例难度过滤与置信度拒绝的两步法能有效增强模型性能与安全性，适用于安全关键场景下的机器学习部署。

Abstract: Machine Learning (ML) models are widely used in high-stakes domains such as
healthcare, where the reliability of predictions is critical. However, these
models often fail to account for uncertainty, providing predictions even with
low confidence. This work proposes a novel two-step data-centric approach to
enhance the performance of ML models by improving data quality and filtering
low-confidence predictions. The first step involves leveraging Instance
Hardness (IH) to filter problematic instances during training, thereby refining
the dataset. The second step introduces a confidence-based rejection mechanism
during inference, ensuring that only reliable predictions are retained. We
evaluate our approach using three real-world healthcare datasets, demonstrating
its effectiveness at improving model reliability while balancing predictive
performance and rejection rate. Additionally, we use alternative criteria -
influence values for filtering and uncertainty for rejection - as baselines to
evaluate the efficiency of the proposed method. The results demonstrate that
integrating IH filtering with confidence-based rejection effectively enhances
model performance while preserving a large proportion of instances. This
approach provides a practical method for deploying ML systems in
safety-critical applications.

</details>


### [343] [A Comprehensive Evaluation Framework for Synthetic Trip Data Generation in Public Transport](https://arxiv.org/abs/2510.24375)
*Yuanyuan Wu,Zhenlin Qin,Zhenliang Ma*

Main category: cs.LG

TL;DR: 提出了一种用于评估公共交通中合成出行数据质量的代表性-隐私-效用（RPU）框架，系统地衡量了12种生成方法在三个维度和三个层次上的表现，发现不存在“一刀切”的模型，CTGAN在权衡中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有对合成数据的评估通常局限于整体代表性或个体隐私，缺乏对群体差异和任务效用的综合考量，导致难以全面评估其可靠性、安全性和实用性。

Method: 提出了一个包含代表性、隐私和效用三个维度及记录、群体、总体三个层次的RPU评估框架，并采用一致的指标集对12种典型生成方法（包括统计模型、深度生成网络和隐私增强变体）进行基准测试。

Result: 实证结果显示合成数据并不天然保证隐私，且在隐私与代表性/效用之间存在明显权衡；不同方法表现各异，无单一最优模型；CTGAN在三者间提供了最均衡的权衡，推荐用于实际应用。

Conclusion: RPU框架为研究人员和实践者提供了一个系统化、可复现的工具，可用于比较和选择适用于公共交通场景的合成数据生成方法。

Abstract: Synthetic data offers a promising solution to the privacy and accessibility
challenges of using smart card data in public transport research. Despite rapid
progress in generative modeling, there is limited attention to comprehensive
evaluation, leaving unclear how reliable, safe, and useful synthetic data truly
are. Existing evaluations remain fragmented, typically limited to
population-level representativeness or record-level privacy, without
considering group-level variations or task-specific utility. To address this
gap, we propose a Representativeness-Privacy-Utility (RPU) framework that
systematically evaluates synthetic trip data across three complementary
dimensions and three hierarchical levels (record, group, population). The
framework integrates a consistent set of metrics to quantify similarity,
disclosure risk, and practical usefulness, enabling transparent and balanced
assessment of synthetic data quality. We apply the framework to benchmark
twelve representative generation methods, spanning conventional statistical
models, deep generative networks, and privacy-enhanced variants. Results show
that synthetic data do not inherently guarantee privacy and there is no
"one-size-fits-all" model, the trade-off between privacy and
representativeness/utility is obvious. Conditional Tabular generative
adversarial network (CTGAN) provide the most balanced trade-off and is
suggested for practical applications. The RPU framework provides a systematic
and reproducible basis for researchers and practitioners to compare synthetic
data generation techniques and select appropriate methods in public transport
applications.

</details>


### [344] [APEX: Approximate-but-exhaustive search for ultra-large combinatorial synthesis libraries](https://arxiv.org/abs/2510.24380)
*Aryan Pedawi,Jordi Silvestre-Ryan,Bradley Worley,Darren J Hsu,Kushal S Shah,Elias Stehle,Jingrong Zhang,Izhar Wallach*

Main category: cs.LG

TL;DR: 提出了一种名为APEX的近似但穷举搜索协议，利用神经网络代理模型在消费者级GPU上实现对大规模组合合成库的快速全枚举虚拟筛选。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟筛选方法难以在有限计算预算下全面搜索超大规模化合物库（如数十亿级别），且难以适应变化的目标和约束条件，导致高分化合物可能被遗漏。

Method: 设计一个神经网络代理模型，利用组合合成库的结构特征，快速预测化合物的目标函数值和约束条件，实现在消费级GPU上几分钟内完成全库枚举，并精确检索近似top-k化合物集。

Result: 在包含一千万以上化合物的基准库上验证，APEX在检索准确性和运行时间方面均优于现有方法，能高效应对不同目标和约束组合下的虚拟筛选任务。

Conclusion: APEX为大规模组合合成库提供了一种高效、灵活且可扩展的虚拟筛选新范式，显著提升了药物发现中先导化合物的识别效率。

Abstract: Make-on-demand combinatorial synthesis libraries (CSLs) like Enamine REAL
have significantly enabled drug discovery efforts. However, their large size
presents a challenge for virtual screening, where the goal is to identify the
top compounds in a library according to a computational objective (e.g.,
optimizing docking score) subject to computational constraints under a limited
computational budget. For current library sizes -- numbering in the tens of
billions of compounds -- and scoring functions of interest, a routine virtual
screening campaign may be limited to scoring fewer than 0.1% of the available
compounds, leaving potentially many high scoring compounds undiscovered.
Furthermore, as constraints (and sometimes objectives) change during the course
of a virtual screening campaign, existing virtual screening algorithms
typically offer little room for amortization. We propose the
approximate-but-exhaustive search protocol for CSLs, or APEX. APEX utilizes a
neural network surrogate that exploits the structure of CSLs in the prediction
of objectives and constraints to make full enumeration on a consumer GPU
possible in under a minute, allowing for exact retrieval of approximate top-$k$
sets. To demonstrate APEX's capabilities, we develop a benchmark CSL comprised
of more than 10 million compounds, all of which have been annotated with their
docking scores on five medically relevant targets along with physicohemical
properties measured with RDKit such that, for any objective and set of
constraints, the ground truth top-$k$ compounds can be identified and compared
against the retrievals from any virtual screening algorithm. We show APEX's
consistently strong performance both in retrieval accuracy and runtime compared
to alternative methods.

</details>


### [345] [Fill in the Blanks: Accelerating Q-Learning with a Handful of Demonstrations in Sparse Reward Settings](https://arxiv.org/abs/2510.24432)
*Seyed Mahdi Basiri Azad,Joschka Boedecker*

Main category: cs.LG

TL;DR: 提出一种利用少量成功示范初始化强化学习智能体价值函数的方法，通过离线预计算价值估计并作为早期学习目标，有效提升稀疏奖励环境下的样本效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境中的强化学习因缺乏有效反馈而面临巨大挑战，导致探索困难和学习效率低下。

Method: 使用少量成功的离线示范预计算价值函数估计，并将其作为在线强化学习初期的训练目标，结合标准在线交互逐步优化价值函数。

Result: 在基准任务上的实验表明，该方法显著减少了探索负担，加速了收敛过程，并优于标准基线方法，即使示范数据极少或不够理想也表现良好。

Conclusion: 这种结合离线示范与在线学习的混合范式能有效提升稀疏奖励环境下强化学习的样本效率和性能。

Abstract: Reinforcement learning (RL) in sparse-reward environments remains a
significant challenge due to the lack of informative feedback. We propose a
simple yet effective method that uses a small number of successful
demonstrations to initialize the value function of an RL agent. By precomputing
value estimates from offline demonstrations and using them as targets for early
learning, our approach provides the agent with a useful prior over promising
actions. The agent then refines these estimates through standard online
interaction. This hybrid offline-to-online paradigm significantly reduces the
exploration burden and improves sample efficiency in sparse-reward settings.
Experiments on benchmark tasks demonstrate that our method accelerates
convergence and outperforms standard baselines, even with minimal or suboptimal
demonstration data.

</details>


### [346] [Methodology for Comparing Machine Learning Algorithms for Survival Analysis](https://arxiv.org/abs/2510.24473)
*Lucas Buk Cardoso,Simone Aldrey Angelo,Yasmin Pacheco Gil Bonilha,Fernando Maia,Adeylson Guimarães Ribeiro,Maria Paula Curado,Gisele Aparecida Fernandes,Vanderlei Cunha Parro,Flávio Almeida de Magalhães Cipparrone,Alexandre Dias Porto Chiavegatto Filho,Tatiana Natasha Toporcov*

Main category: cs.LG

TL;DR: 本研究比较了六种机器学习生存分析模型在结直肠癌患者生存预测中的表现，XGB-AFT模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了提高生存预测的准确性并支持临床决策，需要系统评估不同机器学习生存分析模型在真实世界大规模数据上的表现。

Method: 采用来自圣保罗医院癌症登记系统的近45,000名结直肠癌患者数据，评估了RSF、GBSA、SSVM、XGB-Cox、XGB-AFT和LGBM六种模型；通过多种采样器进行超参数优化，并使用C-Index、C-Index IPCW、时变AUC和IBS等指标评估模型性能；利用SHAP和排列重要性进行特征解释。

Result: XGB-AFT模型表现最优（C-Index = 0.7618；IPCW = 0.7532），其次为GBSA和RSF；生成的生存曲线优于分类算法的预测结果。

Conclusion: 机器学习生存分析模型，特别是XGB-AFT，在处理大规模真实世界数据时具有优越的预测性能，具备临床辅助决策的应用潜力。

Abstract: This study presents a comparative methodological analysis of six machine
learning models for survival analysis (MLSA). Using data from nearly 45,000
colorectal cancer patients in the Hospital-Based Cancer Registries of S\~ao
Paulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for
Survival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),
XGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival
considering censored data. Hyperparameter optimization was performed with
different samplers, and model performance was assessed using the Concordance
Index (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score
(IBS). Survival curves produced by the models were compared with predictions
from classification algorithms, and predictor interpretation was conducted
using SHAP and permutation importance. XGB-AFT achieved the best performance
(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results
highlight the potential and applicability of MLSA to improve survival
prediction and support decision making.

</details>


### [347] [MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis Trajectories in the ICU](https://arxiv.org/abs/2510.24500)
*Yong Huang,Zhongqi Yang,Amir Rahmani*

Main category: cs.LG

TL;DR: MIMIC-Sepsis是一个基于MIMIC-IV数据库的脓毒症研究队列和基准框架，提供标准化的临床数据和治疗信息，支持可重复的脓毒症轨迹建模。


<details>
  <summary>Details</summary>
Motivation: 现有脓毒症研究依赖过时数据集、不可复现的预处理流程且对临床干预覆盖有限，限制了模型的可靠性和实用性。

Method: 基于Sepsis-3标准构建包含35,239名患者的数据队列，采用透明的预处理流程，包括结构化插补策略和治疗变量整合，并发布用于早期死亡率预测、住院时长估计和休克 onset 分类的基准任务。

Result: 实验证明，引入治疗变量显著提升模型性能，尤其是基于Transformer的架构。

Conclusion: MIMIC-Sepsis为重症监护中的预测和序列模型评估提供了一个可靠且可复现的研究平台。

Abstract: Sepsis is a leading cause of mortality in intensive care units (ICUs), yet
existing research often relies on outdated datasets, non-reproducible
preprocessing pipelines, and limited coverage of clinical interventions. We
introduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from
the MIMIC-IV database, designed to support reproducible modeling of sepsis
trajectories. Our cohort includes 35,239 ICU patients with time-aligned
clinical variables and standardized treatment data, including vasopressors,
fluids, mechanical ventilation and antibiotics. We describe a transparent
preprocessing pipeline-based on Sepsis-3 criteria, structured imputation
strategies, and treatment inclusion-and release it alongside benchmark tasks
focused on early mortality prediction, length-of-stay estimation, and shock
onset classification. Empirical results demonstrate that incorporating
treatment variables substantially improves model performance, particularly for
Transformer-based architectures. MIMIC-Sepsis serves as a robust platform for
evaluating predictive and sequential models in critical care research.

</details>


### [348] [LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis](https://arxiv.org/abs/2510.24561)
*Qingyue Zhang,Chang Chu,Tianren Peng,Qi Li,Xiangyang Luo,Zhihao Jiang,Shao-Lun Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于渐近分析的理论框架，用于数据感知的LoRA初始化，并在此基础上开发了LoRA-DA算法，通过小样本估计优化项，实现了更优的初始化策略，在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA初始化方法存在缺乏目标域数据利用、理论基础薄弱或依赖强假设等问题，且基于单步梯度分解的方法因基础模型性能弱而效果不佳，因此需要一种更具理论支持且能有效利用数据的初始化方法。

Method: 从最小化微调模型与目标模型参数差异期望的通用优化目标出发，推导出包含偏差项（通过Fisher梯度公式保留各向异性）和方差项（通过Fisher信息衡量采样随机性带来的不确定性）的优化问题，并求解得到最优LoRA初始化策略；基于此提出LoRA-DA算法，利用少量目标域样本估计优化项以实现高效初始化。

Result: 在多个基准上的实验表明，LoRA-DA在最终准确率上持续优于现有初始化方法，具有更快更稳定的收敛性、对不同秩的鲁棒性以及较小的初始化开销。

Conclusion: 本文建立了一个有理论依据的数据感知LoRA初始化框架，提出的LoRA-DA算法能够有效提升微调性能，为PEFT中的LoRA应用提供了更优的初始化方案。

Abstract: With the widespread adoption of LLMs, LoRA has become a dominant method for
PEFT, and its initialization methods have attracted increasing attention.
However, existing methods have notable limitations: many methods do not
incorporate target-domain data, while gradient-based methods exploit data only
at a shallow level by relying on one-step gradient decomposition, which remains
unsatisfactory due to the weak empirical performance of the one-step
fine-tuning model that serves as their basis, as well as the fact that these
methods either lack a rigorous theoretical foundation or depend heavily on
restrictive isotropic assumptions. In this paper, we establish a theoretical
framework for data-aware LoRA initialization based on asymptotic analysis.
Starting from a general optimization objective that minimizes the expectation
of the parameter discrepancy between the fine-tuned and target models, we
derive an optimization problem with two components: a bias term, which is
related to the parameter distance between the fine-tuned and target models, and
is approximated using a Fisher-gradient formulation to preserve anisotropy; and
a variance term, which accounts for the uncertainty introduced by sampling
stochasticity through the Fisher information. By solving this problem, we
obtain an optimal initialization strategy for LoRA. Building on this
theoretical framework, we develop an efficient algorithm, LoRA-DA, which
estimates the terms in the optimization problem from a small set of target
domain samples and obtains the optimal LoRA initialization. Empirical results
across multiple benchmarks demonstrate that LoRA-DA consistently improves final
accuracy over existing initialization methods. Additional studies show faster,
more stable convergence, robustness across ranks, and only a small
initialization overhead for LoRA-DA. The source code will be released upon
publication.

</details>


### [349] [DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment](https://arxiv.org/abs/2510.24574)
*Hao Wang,Licheng Pan,Yuan Lu,Zhixuan Chu,Xiaoxi Li,Shuting He,Zhichao Chen,Haoxuan Li,Qingsong Wen,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间序列预测方法DistDF，通过最小化预测分布与标签分布之间的联合分布Wasserstein差异来实现条件分布对齐，解决了传统直接预测方法在标签自相关时的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 标准的直接预测方法在存在标签自相关时，使用均方误差估计条件负对数似然会产生偏差，因此需要一种更准确的分布对齐方法。

Method: 提出DistDF方法，利用新设计的联合分布Wasserstein差异来替代难以估计的条件差异，该差异可从有限样本中进行可微且可计算的估计，并与基于梯度的训练兼容。

Result: 实验表明，DistDF在多种预测模型上均提升了性能，并达到了最先进的预测效果。

Conclusion: DistDF通过有效的分布对齐机制显著提高了时间序列预测的准确性，具有广泛的应用潜力。

Abstract: Training time-series forecast models requires aligning the conditional
distribution of model forecasts with that of the label sequence. The standard
direct forecast (DF) approach resorts to minimize the conditional negative
log-likelihood of the label sequence, typically estimated using the mean
squared error. However, this estimation proves to be biased in the presence of
label autocorrelation. In this paper, we propose DistDF, which achieves
alignment by alternatively minimizing a discrepancy between the conditional
forecast and label distributions. Because conditional discrepancies are
difficult to estimate from finite time-series observations, we introduce a
newly proposed joint-distribution Wasserstein discrepancy for time-series
forecasting, which provably upper bounds the conditional discrepancy of
interest. This discrepancy admits tractable, differentiable estimation from
empirical samples and integrates seamlessly with gradient-based training.
Extensive experiments show that DistDF improves the performance diverse
forecast models and achieves the state-of-the-art forecasting performance. Code
is available at https://anonymous.4open.science/r/DistDF-F66B.

</details>


### [350] [Physics-Informed Extreme Learning Machine (PIELM): Opportunities and Challenges](https://arxiv.org/abs/2510.24577)
*He Yang,Fei Ren,Hai-Sui Yu,Xiaohui Chen,Pei-Zhi Zhuang*

Main category: cs.LG

TL;DR: 本文综述了物理信息极端学习机（PIELM）的最新进展，强调其在提高计算效率和精度方面的优势，并探讨了其在求解复杂偏微分方程和多物理场耦合等问题中的应用潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 由于目前尚无关于PIELM的系统性综述，作者基于自身研究经验，旨在为这一新兴且有前景的研究方向提供视角和总结。

Method: 通过回顾近年来PIELM在物理信息机器学习中的发展，分析其在处理尖锐梯度、非线性、高频行为、强约束、不确定性及多物理场耦合问题中的表现。

Result: 总结了PIELM在各类复杂物理问题求解中的成功应用，同时指出现有方法仍面临诸多挑战。

Conclusion: 尽管PIELM已取得显著进展，但仍需进一步发展更鲁棒、可解释且通用的框架，以推动其在科学与工程领域的广泛应用。

Abstract: We are very delighted to see the fast development of physics-informed extreme
learning machine (PIELM) in recent years for higher computation efficiency and
accuracy in physics-informed machine learning. As a summary or review on PIELM
is currently not available, we would like to take this opportunity to show our
perspective and experience for this promising research direction. We can see
many efforts are made to solve PDEs with sharp gradients, nonlinearities,
high-frequency behavior, hard constraints, uncertainty, multiphysics coupling.
Despite the success, many urgent challenges remain to be tackled, which also
provides us opportunities to develop more robust, interpretable, and
generalizable PIELM frameworks with applications in science and engineering.

</details>


### [351] [A Novel XAI-Enhanced Quantum Adversarial Networks for Velocity Dispersion Modeling in MaNGA Galaxies](https://arxiv.org/abs/2510.24598)
*Sathwik Narkedimilli,N V Saran Kumar,Aswath Babu H,Manjunath K Vanahalli,Manish M,Vinija Jain,Aman Chadha*

Main category: cs.LG

TL;DR: 提出了一种结合量子神经网络与经典深度学习的量子对抗框架，通过LIME可解释性指导和对抗评估器优化，实现了高精度、可解释的轻量级预测模型。


<details>
  <summary>Details</summary>
Motivation: 当前量子机器学习在准确性、鲁棒性和可解释性之间难以平衡，需要更有效的融合方法。

Method: 提出一种新型量子对抗框架，结合混合量子神经网络（QNN）与经典深度学习层，引入基于LIME的可解释性评估模型，并扩展了量子GAN和自监督变体；对抗评估器通过反馈损失同时优化预测准确性和模型可解释性。

Result: Vanilla模型表现最稳定，RMSE=0.27，MSE=0.071，MAE=0.21，R²=0.59，在回归指标上优于对抗变体。

Conclusion: 该框架展示了量子与经典方法融合的潜力，推动了可解释、高性能轻量级量子机器学习模型的发展。

Abstract: Current quantum machine learning approaches often face challenges balancing
predictive accuracy, robustness, and interpretability. To address this, we
propose a novel quantum adversarial framework that integrates a hybrid quantum
neural network (QNN) with classical deep learning layers, guided by an
evaluator model with LIME-based interpretability, and extended through quantum
GAN and self-supervised variants. In the proposed model, an adversarial
evaluator concurrently guides the QNN by computing feedback loss, thereby
optimizing both prediction accuracy and model explainability. Empirical
evaluations show that the Vanilla model achieves RMSE = 0.27, MSE = 0.071, MAE
= 0.21, and R^2 = 0.59, delivering the most consistent performance across
regression metrics compared to adversarial counterparts. These results
demonstrate the potential of combining quantum-inspired methods with classical
architectures to develop lightweight, high-performance, and interpretable
predictive models, advancing the applicability of QML beyond current
limitations.

</details>


### [352] [Semi-supervised and unsupervised learning for health indicator extraction from guided waves in aerospace composite structures](https://arxiv.org/abs/2510.24614)
*James Josep Perry,Pablo Garcia-Conde Ortiz,George Konstantinou,Cornelie Vergouwen,Edlyn Santha Kumaran,Morteza Moradi*

Main category: cs.LG

TL;DR: 本研究提出了一种基于数据驱动的综合框架，通过多域信号处理和两种学习方法提取航空航天复合材料结构的健康指标（HIs），解决了因材料变异性、损伤演化随机性和多种损伤模式导致的HI提取难题。


<details>
  <summary>Details</summary>
Motivation: 由于材料特性差异、损伤演化不确定及多种损伤模式的存在，传统方法难以可靠提取健康指标（HIs），且缺乏真实标签，仅依赖二元状态无法反映中间退化过程。

Method: 采用半监督与无监督学习相结合的方法：一是引入连续辅助标签的多样性深度半监督异常检测（Diversity-DeepSAD）；二是嵌入单调性趋势约束的退化趋势约束变分自编码器（DTC-VAE）。结合多频率导波信号，提取时域、频域和时频域特征，并通过无监督集成学习融合各频率下的HIs以降低方差。

Result: 在单加劲肋复合材料结构疲劳实验中，基于快速傅里叶变换特征，DTC-VAE实现了92.3%的性能表现，优于基线方法，且生成的HIs一致性更高；Diversity-DeepSAD达到81.6%的性能。

Conclusion: 所提出的DTC-VAE和Diversity-DeepSAD框架能有效克服标签缺失和退化过程建模难题，显著提升健康指标的可靠性与稳定性，适用于复杂复合材料结构的诊断与预测。

Abstract: Health indicators (HIs) are central to diagnosing and prognosing the
condition of aerospace composite structures, enabling efficient maintenance and
operational safety. However, extracting reliable HIs remains challenging due to
variability in material properties, stochastic damage evolution, and diverse
damage modes. Manufacturing defects (e.g., disbonds) and in-service incidents
(e.g., bird strikes) further complicate this process. This study presents a
comprehensive data-driven framework that learns HIs via two learning approaches
integrated with multi-domain signal processing. Because ground-truth HIs are
unavailable, a semi-supervised and an unsupervised approach are proposed: (i) a
diversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approach
augmented with continuous auxiliary labels used as hypothetical damage proxies,
which overcomes the limitation of prior binary labels that only distinguish
healthy and failed states while neglecting intermediate degradation, and (ii) a
degradation-trend-constrained variational autoencoder (DTC-VAE), in which the
monotonicity criterion is embedded via an explicit trend constraint. Guided
waves with multiple excitation frequencies are used to monitor single-stiffener
composite structures under fatigue loading. Time, frequency, and time-frequency
representations are explored, and per-frequency HIs are fused via unsupervised
ensemble learning to mitigate frequency dependence and reduce variance. Using
fast Fourier transform features, the augmented Diversity-DeepSAD model achieved
81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3%
performance, outperforming existing baselines.

</details>


### [353] [Symbolic Snapshot Ensembles](https://arxiv.org/abs/2510.24633)
*Mingyue Liu,Andrew Cropper*

Main category: cs.LG

TL;DR: 提出一种新的归纳逻辑编程方法，通过一次训练生成多个中间假设，并使用最小描述长度加权方案组合，显著提高预测准确率且计算开销极低。


<details>
  <summary>Details</summary>
Motivation: 传统ILP算法单次训练只学习一个假设，限制了模型性能，而集成方法需多次训练，计算成本高。

Method: 在一次训练过程中保存多个中间假设，并采用最小描述长度（MDL）原则进行加权组合。

Result: 在多个基准任务（包括游戏和视觉推理）上实验显示，该方法将预测准确率提高了4%，且计算开销低于1%。

Conclusion: 该方法在几乎不增加计算成本的前提下，有效提升了ILP模型的预测性能，具有实际应用价值。

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. Most
ILP algorithms learn a single hypothesis from a single training run. Ensemble
methods train an ILP algorithm multiple times to learn multiple hypotheses. In
this paper, we train an ILP algorithm only once and save intermediate
hypotheses. We then combine the hypotheses using a minimum description length
weighting scheme. Our experiments on multiple benchmarks, including game
playing and visual reasoning, show that our approach improves predictive
accuracy by 4% with less than 1% computational overhead.

</details>


### [354] [Causal Ordering for Structure Learning From Time Series](https://arxiv.org/abs/2510.24639)
*Pedro P. Sanchez,Damian Machlanski,Steven McDonagh,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: 本文提出了一种名为DOTS的扩散有序时间结构模型，用于时间序列因果发现。该方法通过整合多个有效的因果排序，克服了传统单排序方法的局限性，能有效恢复底层有向无环图的传递闭包，减少虚假关联。基于标准假设和扩散过程的得分匹配，DOTS实现了高效的Hessian估计，并在合成和真实数据集上优于现有最先进方法，具有更高的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统基于排序的时间序列因果发现方法因仅依赖单一因果排序而限制了模型表达能力，难以应对多变量长时间序列中的复杂因果关系。因此，需要一种能够利用多个有效因果排序的方法来提升因果结构推断的准确性和鲁棒性。

Method: 提出DOTS（Diffusion Ordered Temporal Structure）模型，结合扩散机制与因果发现，利用多个合法的因果排序而非单一排序；在标准假设（如平稳性和加性噪声模型）下，采用基于得分匹配的扩散过程进行高效Hessian估计，从而实现对时间序列因果结构的精确推断。

Result: 在合成数据集（d=3-6, T=200-5,000）上，DOTS将平均窗口图F1从基线最佳的0.63提升至0.81；在真实世界CausalTime基准（d=20-36）上，DOTS取得最高的平均汇总图F1，且运行时间比图优化方法减少一半。

Conclusion: DOTS通过融合多因果排序和扩散模型，提供了一种可扩展、高效且准确的时间序列因果发现解决方案，显著优于现有方法，适用于高维和长序列场景。

Abstract: Predicting causal structure from time series data is crucial for
understanding complex phenomena in physiology, brain connectivity, climate
dynamics, and socio-economic behaviour. Causal discovery in time series is
hindered by the combinatorial complexity of identifying true causal
relationships, especially as the number of variables and time points grow. A
common approach to simplify the task is the so-called ordering-based methods.
Traditional ordering methods inherently limit the representational capacity of
the resulting model. In this work, we fix this issue by leveraging multiple
valid causal orderings, instead of a single one as standard practice. We
propose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based
causal discovery for temporal data. By integrating multiple orderings, DOTS
effectively recovers the transitive closure of the underlying directed acyclic
graph, mitigating spurious artifacts inherent in single-ordering approaches. We
formalise the problem under standard assumptions such as stationarity and the
additive noise model, and leverage score matching with diffusion processes to
enable efficient Hessian estimation. Extensive experiments validate the
approach. Empirical evaluations on synthetic and real-world datasets
demonstrate that DOTS outperforms state-of-the-art baselines, offering a
scalable and robust approach to temporal causal discovery. On synthetic
benchmarks ($d{=}\!3-\!6$ variables, $T{=}200\!-\!5{,}000$ samples), DOTS
improves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the
CausalTime real-world benchmark ($d{=}20\!-\!36$), while baselines remain the
best on individual datasets, DOTS attains the highest average summary-graph
$F1$ while halving runtime relative to graph-optimisation methods. These
results establish DOTS as a scalable and accurate solution for temporal causal
discovery.

</details>


### [355] [The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets](https://arxiv.org/abs/2510.24643)
*Yujun Kim,Chaewon Moon,Chulhee Yun*

Main category: cs.LG

TL;DR: 研究了ReLU网络在具有标签分离的数据集上进行鲁棒记忆化的参数复杂度，分析了在整个鲁棒性比率范围内的参数需求，并给出了更紧的上下界。


<details>
  <summary>Details</summary>
Motivation: 理解在保证预测鲁棒性的前提下，神经网络需要多少参数来记忆数据，尤其是在不同鲁棒性要求下的复杂度变化。

Method: 通过理论分析建立了参数数量的上下界，作为鲁棒性比率ρ=μ/ε的函数，并覆盖整个ρ∈(0,1)范围。

Result: 发现当ρ较小时，鲁棒记忆化的参数复杂度与非鲁棒情况相同，但随ρ增大而增长；且上下界比现有结果更紧。

Conclusion: 鲁棒性要求对参数复杂度的影响是渐进的，在低鲁棒性需求下无需额外参数开销。

Abstract: We study the parameter complexity of robust memorization for $\mathrm{ReLU}$
networks: the number of parameters required to interpolate any given dataset
with $\epsilon$-separation between differently labeled points, while ensuring
predictions remain consistent within a $\mu$-ball around each training sample.
We establish upper and lower bounds on the parameter count as a function of the
robustness ratio $\rho = \mu / \epsilon$. Unlike prior work, we provide a
fine-grained analysis across the entire range $\rho \in (0,1)$ and obtain
tighter upper and lower bounds that improve upon existing results. Our findings
reveal that the parameter complexity of robust memorization matches that of
non-robust memorization when $\rho$ is small, but grows with increasing $\rho$.

</details>


### [356] [Pearl: A Foundation Model for Placing Every Atom in the Right Location](https://arxiv.org/abs/2510.24670)
*Genesis Research Team,Alejandro Dobles,Nina Jovic,Kenneth Leidal,Pranav Murugan,David C. Williams,Drausin Wulsin,Nate Gruver,Christina X. Ji,Korrawat Pruegsanusak,Gianluca Scarpellini,Ansh Sharma,Wojciech Swiderski,Andrea Bootsma,Richard Strong Bowen,Charlotte Chen,Jamin Chen,Marc André Dämgen,Roy Tal Dew,Benjamin DiFrancesco,J. D. Fishman,Alla Ivanova,Zach Kagin,David Li-Bland,Zuli Liu,Igor Morozov,Jeffrey Ouyang-Zhang,Frank C. Pickard IV,Kushal S. Shah,Ben Shor,Gabriel Monteiro da Silva,Maxx Tessmer,Carl Tilbury,Cyr Vetcher,Daniel Zeng,Maruan Al-Shedivat,Aleksandra Faust,Evan N. Feinberg,Michael V. LeVine,Matteus Pan*

Main category: cs.LG

TL;DR: Pearl 是一个用于蛋白质-配体共折叠的基础模型，通过大规模合成数据、SO(3)-等变扩散模块和可控推理机制，在结构预测准确性与物理有效性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前蛋白质-配体复合物结构预测中数据稀缺、架构低效、生成构象物理无效以及无法有效利用推理时辅助信息的问题。

Method: 提出 Pearl 模型，采用三大创新：大规模合成数据训练策略、SO(3)-等变扩散架构以保持三维旋转对称性，以及支持多链模板和条件/非条件模式的可控推理系统。

Result: 在 Runs N' Poses 和 PoseBusters 公开基准上，Pearl 在 RMSD < 2 Å 的准确且物理有效的构象生成上分别比现有最佳模型提升 14.5% 和 14.2%；在更严格的 RMSD < 1 Å 条件下，对真实药物靶标实现 3.6 倍性能提升。模型性能随合成数据量增加而提升。

Conclusion: Pearl 在蛋白质-配体共折叠任务中达到新的SOTA水平，展示了合成数据、对称性感知架构和灵活推理在药物发现中的巨大潜力。

Abstract: Accurately predicting the three-dimensional structures of protein-ligand
complexes remains a fundamental challenge in computational drug discovery that
limits the pace and success of therapeutic design. Deep learning methods have
recently shown strong potential as structural prediction tools, achieving
promising accuracy across diverse biomolecular systems. However, their
performance and utility are constrained by scarce experimental data,
inefficient architectures, physically invalid poses, and the limited ability to
exploit auxiliary information available at inference. To address these issues,
we introduce Pearl (Placing Every Atom in the Right Location), a foundation
model for protein-ligand cofolding at scale. Pearl addresses these challenges
with three key innovations: (1) training recipes that include large-scale
synthetic data to overcome data scarcity; (2) architectures that incorporate an
SO(3)-equivariant diffusion module to inherently respect 3D rotational
symmetries, improving generalization and sample efficiency, and (3)
controllable inference, including a generalized multi-chain templating system
supporting both protein and non-polymeric components as well as dual
unconditional/conditional modes. Pearl establishes a new state-of-the-art
performance in protein-ligand cofolding. On the key metric of generating
accurate (RMSD < 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold
3 and other open source baselines on the public Runs N' Poses and PoseBusters
benchmarks, delivering 14.5% and 14.2% improvements, respectively, over the
next best model. In the pocket-conditional cofolding regime, Pearl delivers
$3.6\times$ improvement on a proprietary set of challenging, real-world drug
targets at the more rigorous RMSD < 1 \r{A} threshold. Finally, we demonstrate
that model performance correlates directly with synthetic dataset size used in
training.

</details>


### [357] [Eigenfunction Extraction for Ordered Representation Learning](https://arxiv.org/abs/2510.24672)
*Burak Varıcı,Che-Ping Tsai,Ritabrata Ray,Nicholas M. Boffi,Pradeep Ravikumar*

Main category: cs.LG

TL;DR: 本文提出了一种提取有序且可识别特征函数的通用框架，通过模块化设计满足上下文核兼容性和可扩展性，并验证了其在合成核和真实图像数据上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有表示学习方法只能恢复上下文核前几个特征函数的线性空间，缺乏精确谱分解以理解特征排序与重要性。

Method: 基于低秩近似和瑞利商优化两种范式，构建满足关键需求（如与上下文核兼容、可扩展）的模块化框架进行特征函数提取。

Result: 在合成核和真实图像数据集上验证了该方法的有效性，恢复的特征值可作为有效的特征重要性评分，用于特征选择。

Conclusion: 所提框架能够提取有序特征函数，支持基于特征重要性的自适应维度表示，在效率与准确性之间实现权衡。

Abstract: Recent advances in representation learning reveal that widely used
objectives, such as contrastive and non-contrastive, implicitly perform
spectral decomposition of a contextual kernel, induced by the relationship
between inputs and their contexts. Yet, these methods recover only the linear
span of top eigenfunctions of the kernel, whereas exact spectral decomposition
is essential for understanding feature ordering and importance. In this work,
we propose a general framework to extract ordered and identifiable
eigenfunctions, based on modular building blocks designed to satisfy key
desiderata, including compatibility with the contextual kernel and scalability
to modern settings. We then show how two main methodological paradigms,
low-rank approximation and Rayleigh quotient optimization, align with this
framework for eigenfunction extraction. Finally, we validate our approach on
synthetic kernels and demonstrate on real-world image datasets that the
recovered eigenvalues act as effective importance scores for feature selection,
enabling principled efficiency-accuracy tradeoffs via adaptive-dimensional
representations.

</details>


### [358] [Learning to Drive Safely with Hybrid Options](https://arxiv.org/abs/2510.24674)
*Bram De Cooman,Johan Suykens*

Main category: cs.LG

TL;DR: 本文将选项框架应用于高速公路自动驾驶任务，提出了一种结合纵向和横向控制的分层强化学习方法，通过引入安全与舒适约束，提升了策略的可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度强化学习在自动驾驶中的应用较少使用选项（技能）框架，尽管该框架天然适合分层控制任务。因此，本文旨在探索并改进这一框架在自动驾驶中的应用。

Method: 设计了针对纵向和横向操作的专用选项，并嵌入安全与舒适性约束；提出了多种基于选项的分层控制架构，并结合最先进的强化学习技术推导出实用算法。

Result: 实验表明，基于混合选项的灵活策略在不同交通条件下表现最优，优于传统的动作级策略。

Conclusion: 使用选项框架可以有效提升自动驾驶策略的性能、可解释性和对领域知识的融合能力，尤其适用于高速公路场景。

Abstract: Out of the many deep reinforcement learning approaches for autonomous
driving, only few make use of the options (or skills) framework. That is
surprising, as this framework is naturally suited for hierarchical control
applications in general, and autonomous driving tasks in specific. Therefore,
in this work the options framework is applied and tailored to autonomous
driving tasks on highways. More specifically, we define dedicated options for
longitudinal and lateral manoeuvres with embedded safety and comfort
constraints. This way, prior domain knowledge can be incorporated into the
learning process and the learned driving behaviour can be constrained more
easily. We propose several setups for hierarchical control with options and
derive practical algorithms following state-of-the-art reinforcement learning
techniques. By separately selecting actions for longitudinal and lateral
control, the introduced policies over combined and hybrid options obtain the
same expressiveness and flexibility that human drivers have, while being easier
to interpret than classical policies over continuous actions. Of all the
investigated approaches, these flexible policies over hybrid options perform
the best under varying traffic conditions, outperforming the baseline policies
over actions.

</details>


### [359] [Greedy Sampling Is Provably Efficient for RLHF](https://arxiv.org/abs/2510.24700)
*Di Wu,Chengshuai Shi,Jing Yang,Cong Shen*

Main category: cs.LG

TL;DR: 本论文研究了基于人类反馈的强化学习（RLHF）中的一般偏好模型，提出通过经验估计（即贪婪采样）即可实现显著性能提升的方法，揭示了KL正则化目标下最优策略类的独特结构特性。


<details>
  <summary>Details</summary>
Motivation: 尽管RLHF在实践中成功，但其理论理解仍有限，尤其是在仅依赖偏好反馈学习KL正则化目标时面临挑战。现有工作多集中于奖励驱动的Bradley-Terry模型并采用乐观或悲观设计，缺乏对更一般偏好模型的研究。

Method: 本文考虑了一般偏好模型，分析其理论性质，并提出直接使用经验估计（贪婪采样）的算法，无需构造乐观或悲观估计，从而简化了学习过程。

Result: 所提方法在一般偏好模型下获得了比现有结果更优的性能保证，且在BT模型特例中也表现出贪婪采样的充分性，实现了量级上的改进。

Conclusion: 贪婪采样在RLHF中具有理论合理性，得益于KL正则化目标下最优策略类的特殊结构，该发现为RLHF算法设计提供了新的视角和简化路径。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a key
technique for post-training large language models. Despite its empirical
success, the theoretical understanding of RLHF is still limited, as learning
the KL-regularized target with only preference feedback poses additional
challenges compared with canonical RL. Existing works mostly study the
reward-based Bradley-Terry (BT) preference model, and extend classical designs
utilizing optimism or pessimism. This work, instead, considers the general
preference model (whose practical relevance has been observed recently) and
obtains performance guarantees with major, order-wise improvements over
existing ones. Surprisingly, these results are derived from algorithms that
directly use the empirical estimates (i.e., greedy sampling), as opposed to
constructing optimistic or pessimistic estimates in previous works. This
insight has a deep root in the unique structural property of the optimal policy
class under the KL-regularized target, and we further specialize it to the BT
model, highlighting the surprising sufficiency of greedy sampling in RLHF.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [360] [MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images](https://arxiv.org/abs/2510.24136)
*Ovi Sarkar,Md Shafiuzzaman,Md. Faysal Ahamed,Golam Mahmud,Muhammad E. H. Chowdhury*

Main category: eess.IV

TL;DR: 提出一种名为MSRANetV2的卷积神经网络，用于结直肠组织图像分类，结合ResNet50V2、注意力机制和SE模块，实现高精度分类。


<details>
  <summary>Details</summary>
Motivation: 传统结直肠癌诊断方法主观性强、耗时长且易变，需更精确高效的自动诊断方案。

Method: 基于ResNet50V2引入残差注意力机制和squeeze-and-excitation模块，结合多尺度特征融合策略，并使用Grad-CAM提升可解释性。

Result: 在CRC-VAL-HE-7K和NCT-CRC-HE-100K数据集上均取得接近99%的准确率、召回率和F1分数，AUC超过0.999。

Conclusion: MSRANetV2是一种可靠、可解释且高性能的结直肠组织分类模型，具有临床辅助诊断潜力。

Abstract: Colorectal cancer (CRC) is a leading worldwide cause of cancer-related
mortality, and the role of prompt precise detection is of paramount interest in
improving patient outcomes. Conventional diagnostic methods such as colonoscopy
and histological examination routinely exhibit subjectivity, are extremely
time-consuming, and are susceptible to variation. Through the development of
digital pathology, deep learning algorithms have become a powerful approach in
enhancing diagnostic precision and efficiency. In our work, we proposed a
convolutional neural network architecture named MSRANetV2, specially optimized
for the classification of colorectal tissue images. The model employs a
ResNet50V2 backbone, extended with residual attention mechanisms and
squeeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained
spatial features. With channel alignment and upsampling operations, MSRANetV2
effectively fuses multi-scale representations, thereby enhancing the robustness
of the classification. We evaluated our model on a five-fold stratified
cross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and
NCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,
recall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900
plus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and
0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were
0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,
0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM
visualizations were incorporated to enhance model interpretability by
highlighting tissue areas that are medically relevant. These findings validate
that MSRANetV2 is a reliable, interpretable, and high-performing architectural
model for classifying CRC tissues.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [361] [Resource-Efficient LLM Application for Structured Transformation of Unstructured Financial Contracts](https://arxiv.org/abs/2510.23990)
*Maruf Ahmed Mridul,Oshani Seneviratne*

Main category: cs.IR

TL;DR: 本文提出并扩展了CDMizer框架，用于将复杂的法律合同（如CSA）准确转换为Common Domain Model（CDM）格式，实验表明该方法结合小型开源大语言模型即可在准确性与效率上媲美大型专有模型。


<details>
  <summary>Details</summary>
Motivation: 将非结构化的法律合同自动转换为标准化、机器可读的格式对金融流程自动化至关重要，但现有方法在处理复杂合同时面临挑战，尤其是确保符合CDM标准的问题。

Method: 采用基于模板的CDMizer框架扩展方案，确保语法正确性和对CDM模式的遵循，并在真实场景中应用于信用支持附件（CSA）条款提取任务，与ISDA基准进行对比评估。

Result: 扩展后的CDMizer框架结合小型开源大语言模型，在CSA条款提取任务中表现出与大型专有模型相当的准确性和效率。

Conclusion: 资源高效的解决方案（如轻量级开源模型结合模板驱动方法）在法律合同自动化转换中具有巨大潜力，可为资源受限或数据隐私要求严格的金融机构提供成本低、可扩展的替代方案。

Abstract: The transformation of unstructured legal contracts into standardized,
machine-readable formats is essential for automating financial workflows. The
Common Domain Model (CDM) provides a standardized framework for this purpose,
but converting complex legal documents like Credit Support Annexes (CSAs) into
CDM representations remains a significant challenge. In this paper, we present
an extension of the CDMizer framework, a template-driven solution that ensures
syntactic correctness and adherence to the CDM schema during contract-to-CDM
conversion. We apply this extended framework to a real-world task, comparing
its performance with a benchmark developed by the International Swaps and
Derivatives Association (ISDA) for CSA clause extraction. Our results show that
CDMizer, when integrated with a significantly smaller, open-source Large
Language Model (LLM), achieves competitive performance in terms of accuracy and
efficiency against larger, proprietary models. This work underscores the
potential of resource-efficient solutions to automate legal contract
transformation, offering a cost-effective and scalable approach that can meet
the needs of financial institutions with constrained resources or strict data
privacy requirements.

</details>


### [362] [DUET: Dual Model Co-Training for Entire Space CTR Prediction](https://arxiv.org/abs/2510.24369)
*Yutian Xiao,Meng Yuan,Fuzhen Zhuang,Wei Chen,Shukuan Wang,Shanqi Liu,Chao Feng,Wenhui Yu,Xiang Li,Lantao Hu,Han Li,Zhao Zhang*

Main category: cs.IR

TL;DR: 本文提出了DUET，一种用于大规模推荐系统预排序阶段的集式预测框架，通过双模型协同训练和全空间CTR预测，在保证计算效率的同时提升了模型表达能力，并有效缓解了样本选择偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有预排序模型因使用轻量化的双塔结构而在估计能力上受限，难以捕捉候选项目间的复杂交互关系，且加剧了样本选择偏差问题。

Method: DUET采用集级别预测方式，在单次前向传播中对整个候选子集进行预测，实现候选间的信息交互；同时引入双模型协同训练机制，通过互动生成伪标签来扩展未曝光项目的监督信号。

Result: 在离线实验和在线A/B测试中，DUET consistently 优于现有先进基线模型，并在多个核心业务指标上取得提升。

Conclusion: DUET在保持计算高效的同时显著提升了预排序阶段的模型性能，已成功部署于快手及快手极速版应用，服务于数亿用户。

Abstract: The pre-ranking stage plays a pivotal role in large-scale recommender systems
but faces an intrinsic trade-off between model expressiveness and computational
efficiency. Owing to the massive candidate pool and strict latency constraints,
industry systems often rely on lightweight two-tower architectures, which are
computationally efficient yet limited in estimation capability. As a result,
they struggle to capture the complex synergistic and suppressive relationships
among candidate items, which are essential for producing contextually coherent
and diverse recommendation lists. Moreover, this simplicity further amplifies
the Sample Selection Bias (SSB) problem, as coarse-grained models trained on
biased exposure data must generalize to a much larger candidate space with
distinct distributions.
  To address these issues, we propose \textbf{DUET} (\textbf{DU}al Model
Co-Training for \textbf{E}ntire Space C\textbf{T}R Prediction), a set-wise
pre-ranking framework that achieves expressive modeling under tight
computational budgets. Instead of scoring items independently, DUET performs
set-level prediction over the entire candidate subset in a single forward pass,
enabling information-aware interactions among candidates while amortizing the
computational cost across the set. Moreover, a dual model co-training mechanism
extends supervision to unexposed items via mutual pseudo-label refinement,
effectively mitigating SSB. Validated through extensive offline experiments and
online A/B testing, DUET consistently outperforms state-of-the-art baselines
and achieves improvements across multiple core business metrics. At present,
DUET has been fully deployed in Kuaishou and Kuaishou Lite Apps, serving the
main traffic for hundreds of millions of users.

</details>


### [363] [Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering](https://arxiv.org/abs/2510.24402)
*Michail Dadopoulos,Anestis Ladas,Stratos Moschidis,Ioannis Negkakis*

Main category: cs.IR

TL;DR: 本文提出了一种基于元数据驱动的多阶段检索增强生成（RAG）架构，通过将LLM生成的元数据与文本结合（“上下文块”），在金融文档分析中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在长且结构化的财务文件中表现不佳，因相关信息稀疏且存在交叉引用，需提高检索精度与效率。

Method: 设计了一个多阶段RAG架构，包含LLM生成元数据、上下文感知分块、预检索过滤、后检索重排序和嵌入增强，并在FinanceBench数据集上进行评估。

Result: 实验表明，将元数据直接嵌入文本块的‘上下文块’方法带来最大性能提升；结合预检索优化与强大重排序器可实现最优效果，同时提出的自定义元数据重排序器在成本效益上优于商业方案。

Conclusion: 该研究为构建高效、鲁棒的元数据感知RAG系统提供了可行蓝图，特别适用于复杂金融文档的分析场景。

Abstract: Retrieval-Augmented Generation (RAG) struggles on long, structured financial
filings where relevant evidence is sparse and cross-referenced. This paper
presents a systematic investigation of advanced metadata-driven
Retrieval-Augmented Generation (RAG) techniques, proposing and evaluating a
novel, multi-stage RAG architecture that leverages LLM-generated metadata. We
introduce a sophisticated indexing pipeline to create contextually rich
document chunks and benchmark a spectrum of enhancements, including
pre-retrieval filtering, post-retrieval reranking, and enriched embeddings,
benchmarked on the FinanceBench dataset. Our results reveal that while a
powerful reranker is essential for precision, the most significant performance
gains come from embedding chunk metadata directly with text ("contextual
chunks"). Our proposed optimal architecture combines LLM-driven pre-retrieval
optimizations with these contextual embeddings to achieve superior performance.
Additionally, we present a custom metadata reranker that offers a compelling,
cost-effective alternative to commercial solutions, highlighting a practical
trade-off between peak performance and operational efficiency. This study
provides a blueprint for building robust, metadata-aware RAG systems for
financial document analysis.

</details>


### [364] [From Time and Place to Preference: LLM-Driven Geo-Temporal Context in Recommendations](https://arxiv.org/abs/2510.24430)
*Yejin Kim,Shaghayegh Agah,Mayur Nankani,Neeraj Sharma,Feifei Peng,Maria Peifer,Sardar Hamidian,H Howie Huang*

Main category: cs.IR

TL;DR: 提出一种基于大语言模型的可扩展框架，通过时间戳和粗略位置生成地理时间嵌入，捕捉节假日、季节性趋势和本地/全球事件，用于改进推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统通常忽略真实世界的时间上下文（如节假日、事件和季节模式），仅将时间戳视为数值或周期性变量，导致上下文信息利用不足。

Method: 利用大语言模型（LLMs）从时间戳和粗略地理位置生成地理时间嵌入，并设计轻量级诊断测试评估嵌入的信息量；将嵌入通过特征融合或辅助损失方式集成到序列模型中。

Result: 在MovieLens、LastFM和生产数据集上验证了地理时间嵌入具有显著预测信号，且与完整模型集成结果一致。

Conclusion: 地理时间嵌入能有效捕捉现实世界时间上下文，提升推荐系统性能，支持自适应或混合推荐策略。

Abstract: Most recommender systems treat timestamps as numeric or cyclical values,
overlooking real-world context such as holidays, events, and seasonal patterns.
We propose a scalable framework that uses large language models (LLMs) to
generate geo-temporal embeddings from only a timestamp and coarse location,
capturing holidays, seasonal trends, and local/global events. We then introduce
a geo-temporal embedding informativeness test as a lightweight diagnostic,
demonstrating on MovieLens, LastFM, and a production dataset that these
embeddings provide predictive signal consistent with the outcomes of full model
integrations. Geo-temporal embeddings are incorporated into sequential models
through (1) direct feature fusion with metadata embeddings or (2) an auxiliary
loss that enforces semantic and geo-temporal alignment. Our findings highlight
the need for adaptive or hybrid recommendation strategies, and we release a
context-enriched MovieLens dataset to support future research.

</details>


### [365] [MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation](https://arxiv.org/abs/2510.24431)
*Xiaoyu Kong,Leheng Sheng,Junfei Tan,Yuxin Chen,Jiancan Wu,An Zhang,Xiang Wang,Xiangnan He*

Main category: cs.IR

TL;DR: MiniOneRec是首个开源的生成式推荐框架，通过语义ID序列和轻量级后训练策略，在扩大模型规模时展现出良好的参数效率，并提升了推荐的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 探索生成式推荐系统在公开基准上的可扩展性规律，并寻找最小化后训练方案以实现有竞争力的性能。

Method: 提出MiniOneRec框架，使用残差量化VAE生成语义ID（SID）序列，基于Qwen模型进行监督微调和面向推荐的强化学习，引入全流程SID对齐和约束解码的混合奖励机制。

Result: 实验显示随着模型规模增大，训练和评估损失持续下降；所提后训练方法显著提升排名准确性和候选多样性。

Conclusion: 生成式推荐范式具有良好的扩展性，MiniOneRec为该领域提供了有效的开源基础框架。

Abstract: The recent success of large language models (LLMs) has renewed interest in
whether recommender systems can achieve similar scaling benefits. Conventional
recommenders, dominated by massive embedding tables, tend to plateau as
embedding dimensions grow. In contrast, the emerging generative paradigm
replaces embeddings with compact Semantic ID (SID) sequences produced by
autoregressive Transformers. Yet most industrial deployments remain
proprietary, leaving two fundamental questions open: (1) Do the expected
scaling laws hold on public benchmarks? (2) What is the minimal post-training
recipe that enables competitive performance?
  We present MiniOneRec, to the best of our knowledge, the first fully
open-source generative recommendation framework, which provides an end-to-end
workflow spanning SID construction, supervised fine-tuning, and
recommendation-oriented reinforcement learning. We generate SIDs via a Residual
Quantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters
on the Amazon Review dataset. Our experiments reveal a consistent downward
trend in both training and evaluation losses with increasing model size,
validating the parameter efficiency of the generative approach. To further
enhance performance, we propose a lightweight yet effective post-training
pipeline that (1) enforces full-process SID alignment and (2) applies
reinforcement learning with constrained decoding and hybrid rewards. Together,
these techniques yield significant improvements in both ranking accuracy and
candidate diversity.

</details>
