<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.CL](#cs.CL) [Total: 84]
- [cs.IR](#cs.IR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.RO](#cs.RO) [Total: 29]
- [cs.LG](#cs.LG) [Total: 57]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Self-Supervised Masked Autoencoders with Dense-Unet for Coronary Calcium Removal in limited CT Data](https://arxiv.org/abs/2601.02392)
*Mo Chen*

Main category: cs.CV

TL;DR: 本文提出Dense-MAE，一种基于掩码自编码器的自监督学习框架，用于提升冠状动脉钙化伪影去除效果，尤其在标注数据稀缺的少样本场景下显著优于从头训练。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉钙化在CTA中产生晕染伪影，严重影响管腔狭窄诊断；而现有基于DCNN的去伪影方法依赖大量标注数据，但在医学领域难以获取。

Method: 提出Dense-MAE框架：在Dense-Unet基础上，采用3D体素块随机掩码策略进行自监督预训练，让模型重建被掩码的血管管腔几何结构，从而学习动脉拓扑的高层次隐式特征。

Result: 在临床CTA数据集上验证表明，使用Dense-MAE预训练权重初始化钙化去除网络，相比从零训练，在少样本条件下显著提升了伪影去除精度与狭窄程度估计准确性。

Conclusion: Dense-MAE是一种有效的自监督预训练方法，缓解了医学图像修复任务中标注数据稀缺问题，并为三维医学影像分析提供了新思路。

Abstract: Coronary calcification creates blooming artifacts in Computed Tomography Angiography (CTA), severely hampering the diagnosis of lumen stenosis. While Deep Convolutional Neural Networks (DCNNs) like Dense-Unet have shown promise in removing these artifacts via inpainting, they often require large labeled datasets which are scarce in the medical domain. Inspired by recent advancements in Masked Autoencoders (MAE) for 3D point clouds, we propose \textbf{Dense-MAE}, a novel self-supervised learning framework for volumetric medical data. We introduce a pre-training strategy that randomly masks 3D patches of the vessel lumen and trains the Dense-Unet to reconstruct the missing geometry. This forces the encoder to learn high-level latent features of arterial topology without human annotation. Experimental results on clinical CTA datasets demonstrate that initializing the Calcium Removal network with our MAE-based weights significantly improves inpainting accuracy and stenosis estimation compared to training from scratch, specifically in few-shot scenarios.

</details>


### [2] [MIAR: Modality Interaction and Alignment Representation Fuison for Multimodal Emotion](https://arxiv.org/abs/2601.02414)
*Jichao Zhu,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MIAR的新方法，通过模态交互与对齐表征来提升多模态情感识别性能，解决了模态间分布差异大、贡献度不均及泛化能力弱等问题，在CMU-MOSI和CMU-MOSEI数据集上超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分解决多模态间显著的分布差异、各模态对任务贡献度不同，以及在不同文本特征上的泛化能力不足问题。

Method: 提出Modality Interaction and Alignment Representation（MIAR）网络，利用特征交互生成代表各模态从其他模态提取信息的全局表征token，并结合对比学习与归一化策略实现模态对齐。

Result: 在CMU-MOSI和CMU-MOSEI两个基准数据集上，MIAR优于当前最先进的多模态情感识别方法。

Conclusion: MIAR有效缓解了模态异质性与贡献不平衡问题，提升了模型泛化能力与整体识别性能。

Abstract: Multimodal Emotion Recognition (MER) aims to perceive human emotions through three modes: language, vision, and audio. Previous methods primarily focused on modal fusion without adequately addressing significant distributional differences among modalities or considering their varying contributions to the task. They also lacked robust generalization capabilities across diverse textual model features, thus limiting performance in multimodal scenarios. Therefore, we propose a novel approach called Modality Interaction and Alignment Representation (MIAR). This network integrates contextual features across different modalities using a feature interaction to generate feature tokens to represent global representations of this modality extracting information from other modalities. These four tokens represent global representations of how each modality extracts information from others. MIAR aligns different modalities using contrastive learning and normalization strategies. We conduct experiments on two benchmarks: CMU-MOSI and CMU-MOSEI datasets, experimental results demonstrate the MIAR outperforms state-of-the-art MER methods.

</details>


### [3] [Multimodal Sentiment Analysis based on Multi-channel and Symmetric Mutual Promotion Feature Fusion](https://arxiv.org/abs/2601.02415)
*Wangyuan Zhu,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于多通道特征提取和对称互促（SMP）跨模态融合方法的多模态情感分析模型，通过双通道视觉/听觉特征增强模态内表征，并结合对称跨模态注意力与自注意力机制提升模态间交互与信息互补，实验验证了其在两个基准数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在单模态特征提取不足、忽视模态间特征差异导致融合不充分的问题。

Method: 1）在视觉和听觉模态中分别提取双通道特征以增强模态内表征；2）提出对称互促（SMP）跨模态融合方法，结合对称跨模态注意力（捕获其他模态有用信息）与自注意力（建模上下文）；3）联合利用模态内特征与跨模态融合特征，兼顾互补性与差异性。

Result: 在两个基准数据集上实验结果表明，所提方法在多模态情感分析任务中具有有效性与优越性。

Conclusion: 多通道特征提取与SMP融合策略能有效缓解单模态特征贫乏及模态间信息融合不充分问题，提升情感识别性能。

Abstract: Multimodal sentiment analysis is a key technology in the fields of human-computer interaction and affective computing. Accurately recognizing human emotional states is crucial for facilitating smooth communication between humans and machines. Despite some progress in multimodal sentiment analysis research, numerous challenges remain. The first challenge is the limited and insufficiently rich features extracted from single modality data. Secondly, most studies focus only on the consistency of inter-modal feature information, neglecting the differences between features, resulting in inadequate feature information fusion. In this paper, we first extract multi-channel features to obtain more comprehensive feature information. We employ dual-channel features in both the visual and auditory modalities to enhance intra-modal feature representation. Secondly, we propose a symmetric mutual promotion (SMP) inter-modal feature fusion method. This method combines symmetric cross-modal attention mechanisms and self-attention mechanisms, where the cross-modal attention mechanism captures useful information from other modalities, and the self-attention mechanism models contextual information. This approach promotes the exchange of useful information between modalities, thereby strengthening inter-modal interactions. Furthermore, we integrate intra-modal features and inter-modal fused features, fully leveraging the complementarity of inter-modal feature information while considering feature information differences. Experiments conducted on two benchmark datasets demonstrate the effectiveness and superiority of our proposed method.

</details>


### [4] [Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning](https://arxiv.org/abs/2601.02422)
*Wenting Lu,Didi Zhu,Tao Shen,Donglin Zhu,Ayong Ye,Chao Wu*

Main category: cs.CV

TL;DR: 本文提出CoCoT框架，通过动态多区域定位和关系感知推理解决多模态推理中单区域依赖和语义碎片化问题，并构建了74,691样本的CoCoT-70K数据集，在多个基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维方法在跨模态场景中存在对单一粗粒度图像区域过度依赖及推理步骤间语义断裂两大缺陷。

Method: 提出CoCoT框架，包含动态多区域定位（根据问题自适应选择最相关图像区域）和关系感知推理（通过迭代对齐视觉线索实现多区域协同，形成连贯逻辑链）。

Result: 在六个挑战性基准上，CoCoT使LLaVA-1.5平均准确率提升15.4%，Qwen2-VL提升4.0%；并构建了含74,691个高质量样本的CoCoT-70K数据集。

Conclusion: CoCoT有效缓解了跨模态推理中的区域依赖与语义碎片问题，显著提升了复杂视觉推理能力，具备良好的可扩展性与实用性。

Abstract: Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) framework, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively aligning visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual reasoning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.

</details>


### [5] [ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios](https://arxiv.org/abs/2601.03011)
*Yihan Wei,Shenghai Yuan,Tianchen Deng,Boyang Lou,Enwen Hu*

Main category: cs.CV

TL;DR: ReCCur 是一种低计算开销的递归框架，利用多智能体流程将噪声网络图像转化为可审计的细粒度标签，用于高效构建高质量corner-case数据集。


<details>
  <summary>Details</summary>
Motivation: corner cases在现实中易引发系统失效，但因网络数据噪声大、标签脆弱、边缘部署难以大规模重训练，导致其难以大规模构建。

Method: 提出ReCCur框架：1）基于VLM进行多模态（图像、描述、关键词）一致性过滤与领域词汇扩展；2）采用混合专家知识蒸馏（CLIP/DINOv2/BEiT等编码器），结合kNN投票、双置信激活与不确定性采样；3）区域证据驱动的VLM对抗标注，由提议者（多粒度区域+语义线索）与验证者（全局-局部链式一致性）协同生成可解释标签。

Result: 在真实corner-case场景（如水淹车辆检测）中，ReCCur可在消费级GPU上运行，持续提升标签纯度与类间可分性，且仅需极少人工监督。

Conclusion: ReCCur为资源受限下的corner-case数据构建提供了实用、可审计、可解释的低开销新范式。

Abstract: Corner cases are rare or extreme scenarios that drive real-world failures, but they are difficult to curate at scale: web data are noisy, labels are brittle, and edge deployments preclude large retraining. We present ReCCur (Recursive Corner-Case Curation), a low-compute framework that converts noisy web imagery into auditable fine-grained labels via a multi-agent recursive pipeline. First, large-scale data acquisition and filtering expands a domain vocabulary with a vision-language model (VLM), crawls the web, and enforces tri-modal (image, description, keyword) consistency with light human spot checks to yield refined candidates. Next, mixture-of-experts knowledge distillation uses complementary encoders (e.g., CLIP, DINOv2, BEiT) for kNN voting with dual-confidence activation and uncertainty sampling, converging to a high-precision set. Finally, region-evidence VLM adversarial labeling pairs a proposer (multi-granularity regions and semantic cues) with a validator (global and local chained consistency) to produce explainable labels and close the loop. On realistic corner-case scenarios (e.g., flooded-car inspection), ReCCur runs on consumer-grade GPUs, steadily improves purity and separability, and requires minimal human supervision, providing a practical substrate for downstream training and evaluation under resource constraints. Code and dataset will be released.

</details>


### [6] [NitroGen: An Open Foundation Model for Generalist Gaming Agents](https://arxiv.org/abs/2601.02427)
*Loïc Magne,Anas Awadalla,Guanzhi Wang,Yinzhen Xu,Joshua Belofsky,Fengyuan Hu,Joohwan Kim,Ludwig Schmidt,Georgia Gkioxari,Jan Kautz,Yisong Yue,Yejin Choi,Yuke Zhu,Linxi "Jim" Fan*

Main category: cs.CV

TL;DR: NitroGen is a vision-action foundation model trained on 40,000 hours of gameplay across 1,000+ games, enabling strong cross-game generalization for embodied gaming agents.


<details>
  <summary>Details</summary>
Motivation: To build generalist gaming agents capable of transferring knowledge across diverse games, overcoming limitations of game-specific models.

Method: Constructing an internet-scale video-action dataset via automatic action extraction from gameplay videos; developing a multi-game benchmark for cross-game evaluation; training a unified vision-action model via large-scale behavior cloning.

Result: NitroGen achieves strong performance across varied game genres (3D action, 2D platformers, procedurally generated worlds) and transfers to unseen games with up to 52% relative improvement in task success over from-scratch models.

Conclusion: Large-scale, diverse behavioral data and unified modeling enable effective generalization in vision-action agents for gaming, and the released resources support future research on generalist embodied agents.

Abstract: We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.

</details>


### [7] [TAP-ViTs: Task-Adaptive Pruning for On-Device Deployment of Vision Transformers](https://arxiv.org/abs/2601.02437)
*Zhibo Wang,Zuoyuan Zhang,Xiaoyi Pang,Qile Zhang,Xuanyi Hao,Shuguo Zhuo,Peng Sun*

Main category: cs.CV

TL;DR: 本文提出TAP-ViTs框架，实现无需原始本地数据的设备自适应ViT剪枝，通过GMM建模私有数据分布、构建代理度量数据集，并设计双粒度重要性评估策略，兼顾任务特性与设备算力约束。


<details>
  <summary>Details</summary>
Motivation: 现有ViT剪枝方法无法兼顾设备异构性与隐私保护需求：要么采用统一剪枝模型忽略设备差异，要么依赖本地数据微调，而后者在边缘设备上受限于资源与隐私。

Method: 提出TAP-ViTs框架：1）各设备用轻量高斯混合模型（GMM）近似本地数据分布并上传参数；2）云端基于GMM参数从公开数据中选取分布一致样本构建设备级任务代表度量数据集；3）在此代理数据集上，设计融合复合神经元重要性与自适应层重要性的双粒度剪枝策略。

Result: 在多个ViT骨干网络和数据集上的实验表明，TAP-ViTs在相同压缩比下持续优于现有最先进剪枝方法。

Conclusion: TAP-ViTs成功实现了隐私保护前提下的任务自适应、设备定制化ViT剪枝，为资源受限边缘场景部署高性能ViT提供了新范式。

Abstract: Vision Transformers (ViTs) have demonstrated strong performance across a wide range of vision tasks, yet their substantial computational and memory demands hinder efficient deployment on resource-constrained mobile and edge devices. Pruning has emerged as a promising direction for reducing ViT complexity. However, existing approaches either (i) produce a single pruned model shared across all devices, ignoring device heterogeneity, or (ii) rely on fine-tuning with device-local data, which is often infeasible due to limited on-device resources and strict privacy constraints. As a result, current methods fall short of enabling task-customized ViT pruning in privacy-preserving mobile computing settings. This paper introduces TAP-ViTs, a novel task-adaptive pruning framework that generates device-specific pruned ViT models without requiring access to any raw local data. Specifically, to infer device-level task characteristics under privacy constraints, we propose a Gaussian Mixture Model (GMM)-based metric dataset construction mechanism. Each device fits a lightweight GMM to approximate its private data distribution and uploads only the GMM parameters. Using these parameters, the cloud selects distribution-consistent samples from public data to construct a task-representative metric dataset for each device. Based on this proxy dataset, we further develop a dual-granularity importance evaluation-based pruning strategy that jointly measures composite neuron importance and adaptive layer importance, enabling fine-grained, task-aware pruning tailored to each device's computational budget. Extensive experiments across multiple ViT backbones and datasets demonstrate that TAP-ViTs consistently outperforms state-of-the-art pruning methods under comparable compression ratios.

</details>


### [8] [Understanding Pure Textual Reasoning for Blind Image Quality Assessment](https://arxiv.org/abs/2601.02441)
*Yuan Li,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本文从信息流角度分析了文本信息在盲图像质量评估（BIQA）中的作用，比较了Chain-of-Thought、Self-Consistency和Autoencoder三种范式对图像-文本-分数关系建模的效果，发现Self-Consistency能显著缩小仅用图像与仅用文本预测之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前盲图像质量评估（BIQA）中广泛使用文本推理，但尚不清楚文本信息如何贡献于质量预测，以及文本能在多大程度上表征与评分相关的图像内容。

Method: 从信息流视角出发，设计并比较三种学习图像-文本-分数关系的范式：Chain-of-Thought、Self-Consistency 和 Autoencoder，并在现有BIQA模型上进行实验验证。

Result: 仅用文本预测时，现有模型性能显著下降；Chain-of-Thought提升有限；Self-Consistency显著缩小图像与文本条件下的预测差距（PLCC/SRCC差降至0.02/0.03）；Autoencoder范式效果较弱但提示优化方向。

Conclusion: Self-Consistency范式更利于文本推理在BIQA中的有效应用，该发现为提升BIQA及高层视觉任务中的文本推理能力提供了新思路。

Abstract: Textual reasoning has recently been widely adopted in Blind Image Quality Assessment (BIQA). However, it remains unclear how textual information contributes to quality prediction and to what extent text can represent the score-related image contents. This work addresses these questions from an information-flow perspective by comparing existing BIQA models with three paradigms designed to learn the image-text-score relationship: Chain-of-Thought, Self-Consistency, and Autoencoder. Our experiments show that the score prediction performance of the existing model significantly drops when only textual information is used for prediction. Whereas the Chain-of-Thought paradigm introduces little improvement in BIQA performance, the Self-Consistency paradigm significantly reduces the gap between image- and text-conditioned predictions, narrowing the PLCC/SRCC difference to 0.02/0.03. The Autoencoder-like paradigm is less effective in closing the image-text gap, yet it reveals a direction for further optimization. These findings provide insights into how to improve the textual reasoning for BIQA and high-level vision tasks.

</details>


### [9] [Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative](https://arxiv.org/abs/2601.02443)
*Li Wang,Xi Chen,XiangWen Deng,HuaHui Yi,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: 本文研究了多模态大语言模型（MLLMs）在膝骨关节炎（OA）X光片分类任务中的表现，发现其诊断准确率不如单独优化的视觉编码器；LLM微调对分类提升有限，而数据平衡性比数据规模更重要；结论是MLLM更适合作为解释器和报告生成器，而非高确定性医学图像诊断分类器。


<details>
  <summary>Details</summary>
Motivation: 膝骨关节炎影响全球3–4亿人，但在现有医学MLLM基准中严重缺乏代表性；当前MLLM在VQA和报告生成上表现良好，但其能力难以可靠迁移到疾病特异性分类任务，亟需系统评估。

Method: 通过系统消融实验，分别操控视觉编码器、连接模块和大语言模型（LLM），并在不同训练策略（全参数微调、LoRA、提示工程）下评估其在膝OA X光分类任务中的贡献；对比了不同规模与平衡性的数据集效果。

Result: 单独训练的视觉编码器在分类准确率上优于完整MLLM流程；LLM微调无明显增益，提示式引导即足够；小规模但类别平衡的数据集（500张）经LoRA微调效果优于大规模不平衡数据集（5778张）。

Conclusion: 对于高确定性要求的医学图像诊断分类任务，MLLM架构并不理想；LLM更适合作为解释器和报告生成器；临床应用系统应优先优化视觉编码器并审慎构建高质量、平衡的数据集。

Abstract: Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.

</details>


### [10] [A Spatio-Temporal Deep Learning Approach For High-Resolution Gridded Monsoon Prediction](https://arxiv.org/abs/2601.02445)
*Parashjyoti Borah,Sanghamitra Sarkar,Ranjan Phukan*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的印度夏季风（ISM）高分辨率格点化预测新框架，将多变量大气海洋场视为多通道图像序列，利用CNN从5个月前兆数据预测未来季风期逐月及季节总降雨分布。


<details>
  <summary>Details</summary>
Motivation: 传统长期预报仅预测单一空间平均值，缺乏区域资源管理所需的精细化空间信息。

Method: 将1936–2020年ERA5再分析数据（预测因子）和IMD降水数据（目标）构建成视频式多通道时空张量，采用CNN架构建模1月–5月前兆场到6月–9月逐月及季节总降水的空间分布映射。

Result: 模型成功生成四个月份（6–9月）及整个季风期的高分辨率格点降水预测，显著提升空间细节和时效性。

Conclusion: 该框架将季风预测转化为时空计算机视觉任务，为区域水文与农业决策提供了更实用、更具解释性的长程预测工具。

Abstract: The Indian Summer Monsoon (ISM) is a critical climate phenomenon, fundamentally impacting the agriculture, economy, and water security of over a billion people. Traditional long-range forecasting, whether statistical or dynamical, has predominantly focused on predicting a single, spatially-averaged seasonal value, lacking the spatial detail essential for regional-level resource management. To address this gap, we introduce a novel deep learning framework that reframes gridded monsoon prediction as a spatio-temporal computer vision task. We treat multi-variable, pre-monsoon atmospheric and oceanic fields as a sequence of multi-channel images, effectively creating a video-like input tensor. Using 85 years of ERA5 reanalysis data for predictors and IMD rainfall data for targets, we employ a Convolutional Neural Network (CNN)-based architecture to learn the complex mapping from the five-month pre-monsoon period (January-May) to a high-resolution gridded rainfall pattern for the subsequent monsoon season. Our framework successfully produces distinct forecasts for each of the four monsoon months (June-September) as well as the total seasonal average, demonstrating its utility for both intra-seasonal and seasonal outlooks.

</details>


### [11] [Don't Mind the Gaps: Implicit Neural Representations for Resolution-Agnostic Retinal OCT Analysis](https://arxiv.org/abs/2601.02447)
*Bennet Kahrs,Julia Andresen,Fenja Falta,Monty Santarossa,Heinz Handels,Timo Kepp*

Main category: cs.CV

TL;DR: 本文提出两种基于隐式神经表示（INR）的框架，用于对高各向异性、稀疏扫描的视网膜OCT体积数据进行密集三维分析：一是结合en-face模态信息实现B-scan间插值；二是构建分辨率无关的视网膜图谱，支持泛化分析。二者均利用INR的分辨率无关性和连续性优势，提升三维结构表征能力，并适用于不同成像协议的数据。


<details>
  <summary>Details</summary>
Motivation: 常规OCT临床成像切片间距大，导致图像高度各向异性与视网膜采样稀疏；现有2D深度学习方法易造成相邻B-scan结果不一致（如层分割表面不光滑），且模型受限于训练分辨率，难以泛化至不同成像协议。

Method: 提出两种基于隐式神经表示（INR）的框架：1）融合en-face模态信息的B-scan间插值；2）分辨率无关的通用视网膜图谱构建；二者均采用坐标输入的INR建模，通过群体数据训练实现泛化。

Result: 所提方法在保持结构一致性的同时实现了高保真3D插值，并构建出可跨设备、跨协议迁移的视网膜图谱；实验验证其在大间距OCT数据上显著提升三维层结构表征质量。

Conclusion: INR为解决OCT各向异性与分辨率依赖问题提供了新范式；所提两个框架推动了稀疏OCT数据的可靠、通用三维定量分析，具有临床转化潜力。

Abstract: Routine clinical imaging of the retina using optical coherence tomography (OCT) is performed with large slice spacing, resulting in highly anisotropic images and a sparsely scanned retina. Most learning-based methods circumvent the problems arising from the anisotropy by using 2D approaches rather than performing volumetric analyses. These approaches inherently bear the risk of generating inconsistent results for neighboring B-scans. For example, 2D retinal layer segmentations can have irregular surfaces in 3D. Furthermore, the typically used convolutional neural networks are bound to the resolution of the training data, which prevents their usage for images acquired with a different imaging protocol. Implicit neural representations (INRs) have recently emerged as a tool to store voxelized data as a continuous representation. Using coordinates as input, INRs are resolution-agnostic, which allows them to be applied to anisotropic data. In this paper, we propose two frameworks that make use of this characteristic of INRs for dense 3D analyses of retinal OCT volumes. 1) We perform inter-B-scan interpolation by incorporating additional information from en-face modalities, that help retain relevant structures between B-scans. 2) We create a resolution-agnostic retinal atlas that enables general analysis without strict requirements for the data. Both methods leverage generalizable INRs, improving retinal shape representation through population-based training and allowing predictions for unseen cases. Our resolution-independent frameworks facilitate the analysis of OCT images with large B-scan distances, opening up possibilities for the volumetric evaluation of retinal structures and pathologies.

</details>


### [12] [PatchAlign3D: Local Feature Alignment for Dense 3D Shape understanding](https://arxiv.org/abs/2601.02457)
*Souhail Hadgi,Bingchen Gong,Ramana Sundararaman,Emery Pierson,Lei Li,Peter Wonka,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: 本文提出了一种无需多视角渲染、直接从点云生成语言对齐的局部特征的3D编码器，通过两阶段预训练（2D特征蒸馏+多正样本对比对齐）实现零样本3D部件分割，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D基础模型擅长全局任务但难以迁移到局部部件级推理；基于多视角渲染和大语言模型提示工程的方法计算昂贵、依赖文本提示且未充分利用3D几何信息。

Method: 构建一个仅含编码器的3D模型，以点云为输入；采用两阶段预训练：(1) 将DINOv2等视觉编码器的密集2D特征蒸馏到3D patch；(2) 通过多正样本对比损失对齐patch嵌入与部件级文本嵌入；数据来自多视角SAM区域与VLM字幕配对的部件标注3D形状。

Result: 在多个3D部件分割基准上显著超越先前基于渲染和前馈的方法，支持零样本分割，且仅需单次前向推理、无需测试时多视角渲染。

Conclusion: 该工作证明了直接在3D空间中学习语言对齐的局部表征是可行且高效的，为3D基础模型向细粒度理解迈进提供了新范式。

Abstract: Current foundation models for 3D shapes excel at global tasks (retrieval, classification) but transfer poorly to local part-level reasoning. Recent approaches leverage vision and language foundation models to directly solve dense tasks through multi-view renderings and text queries. While promising, these pipelines require expensive inference over multiple renderings, depend heavily on large language-model (LLM) prompt engineering for captions, and fail to exploit the inherent 3D geometry of shapes. We address this gap by introducing an encoder-only 3D model that produces language-aligned patch-level features directly from point clouds. Our pre-training approach builds on existing data engines that generate part-annotated 3D shapes by pairing multi-view SAM regions with VLM captioning. Using this data, we train a point cloud transformer encoder in two stages: (1) distillation of dense 2D features from visual encoders such as DINOv2 into 3D patches, and (2) alignment of these patch embeddings with part-level text embeddings through a multi-positive contrastive objective. Our 3D encoder achieves zero-shot 3D part segmentation with fast single-pass inference without any test-time multi-view rendering, while significantly outperforming previous rendering-based and feed-forward approaches across several 3D part segmentation benchmarks. Project website: https://souhail-hadgi.github.io/patchalign3dsite/

</details>


### [13] [CT Scans As Video: Efficient Intracranial Hemorrhage Detection Using Multi-Object Tracking](https://arxiv.org/abs/2601.02521)
*Amirreza Parvahan,Mohammad Hoseyni,Javad Khoramdel,Amirhossein Nikoofard*

Main category: cs.CV

TL;DR: 本文提出一种轻量级视频视角框架，将3D CT数据转化为视频流，结合YOLO Nano系列检测器与ByteTrack跟踪算法，并引入混合推理与时空一致性滤波，提升颅内出血检测精度（Precision从0.703升至0.779），兼顾实时性与3D上下文建模能力。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上3D CNN因高内存与计算开销难以部署，而2D方法缺乏必要3D上下文；亟需在资源受限场景（如移动卒中单元）中实现高效、精准的实时ICH检测。

Method: 将CT体积数据重构为沿z轴的视频流；选用YOLO v8–v12 Nano中mAP@50最高的版本作切片级检测器；引入ByteTrack保障z轴解剖一致性；设计混合推理策略与时空一致性滤波以缓解跟踪初始化延迟并抑制假阳性。

Result: 在独立测试集上Precision提升至0.779（+7.6%），保持高敏感性；显著降低计算开销，支持边缘端实时运行。

Conclusion: 该视频视角范式可在极低计算成本下近似3D推理，为资源受限医疗场景提供可扩展、高精度的实时优先分诊方案。

Abstract: Automated analysis of volumetric medical imaging on edge devices is severely constrained by the high memory and computational demands of 3D Convolutional Neural Networks (CNNs). This paper develops a lightweight computer vision framework that reconciles the efficiency of 2D detection with the necessity of 3D context by reformulating volumetric Computer Tomography (CT) data as sequential video streams. This video-viewpoint paradigm is applied to the time-sensitive task of Intracranial Hemorrhage (ICH) detection using the Hemorica dataset. To ensure operational efficiency, we benchmarked multiple generations of the YOLO architecture (v8, v10, v11 and v12) in their Nano configurations, selecting the version with the highest mAP@50 to serve as the slice-level backbone. A ByteTrack algorithm is then introduced to enforce anatomical consistency across the $z$-axis. To address the initialization lag inherent in video trackers, a hybrid inference strategy and a spatiotemporal consistency filter are proposed to distinguish true pathology from transient prediction noise. Experimental results on independent test data demonstrate that the proposed framework serves as a rigorous temporal validator, increasing detection Precision from 0.703 to 0.779 compared to the baseline 2D detector, while maintaining high sensitivity. By approximating 3D contextual reasoning at a fraction of the computational cost, this method provides a scalable solution for real-time patient prioritization in resource-constrained environments, such as mobile stroke units and IoT-enabled remote clinics.

</details>


### [14] [MovieRecapsQA: A Multimodal Open-Ended Video Question-Answering Benchmark](https://arxiv.org/abs/2601.02536)
*Shaden Shaar,Bradon Thymes,Sirawut Chaixanien,Claire Cardie,Bharath Hariharan*

Main category: cs.CV

TL;DR: 本文提出了一个名为MovieRecapsQA的新型开放性多模态视频问答（VideoQA）基准，利用电影回顾视频（同步视觉与文本摘要）构建约8.2K个问答对，并提供可验证答案的显式事实依据；该基准支持细粒度分析，实验表明当前MLLM在纯视觉问题、视频事实提取等方面仍面临显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有VideoQA基准难以捕捉真实世界视频（如电影）中视觉与对话线索的多模态推理能力，且大多非开放性，难以评估自由生成答案；亟需一个具备显式文本上下文、支持开放回答和参考无关评估的新基准。

Method: 基于YouTube上的电影回顾视频（含同步 recap 视频与文本摘要），利用文本摘要生成约8.2K个与字幕对齐的开放性QA对，并为每个答案提供可验证的‘事实’依据；设计多长度视频输入（recap-segments / movie-segments）与多维度问题分类（按模态和类型），用于细粒度评估；在7个SOTA多模态大语言模型上进行系统评测。

Result: 1）纯视觉问题最难；2）模型倾向依赖文本输入；3）从视频中提取准确事实仍困难；4）闭源与开源模型在视频依赖型问题上表现相当。

Conclusion: MovieRecapsQA是首个提供显式文本上下文的开放性VideoQA基准，揭示了当前MLLM在多模态理解尤其是视频语义提取方面的关键短板，为未来研究提供了新评测范式与分析维度。

Abstract: Understanding real-world videos such as movies requires integrating visual and dialogue cues to answer complex questions. Yet existing VideoQA benchmarks struggle to capture this multimodal reasoning and are largely not open-ended, given the difficulty of evaluating free-form answers. In this paper, we introduce a novel open-ended multi-modal VideoQA benchmark, MovieRecapsQA created using movie recap videos--a distinctive type of YouTube content that summarizes a film by presenting its key events through synchronized visual (recap video) and textual (recap summary) modalities. Using the recap summary, we generate $\approx 8.2$ K question-answer (QA) pairs (aligned with movie-subtitles) and provide the necessary "facts" needed to verify an answer in a reference-free manner. To our knowledge, this is the first open-ended VideoQA benchmark that supplies explicit textual context of the input (video and/or text); which we use for evaluation. Our benchmark provides videos of multiple lengths (i.e., recap-segments, movie-segments) and categorizations of questions (by modality and type) to enable fine-grained analysis. We evaluate the performance of seven state-of-the-art MLLMs using our benchmark and observe that: 1) visual-only questions remain the most challenging; 2) models default to textual inputs whenever available; 3) extracting factually accurate information from video content is still difficult for all models; and 4) proprietary and open-source models perform comparably on video-dependent questions.

</details>


### [15] [Shallow- and Deep-fake Image Manipulation Localization Using Vision Mamba and Guided Graph Neural Network](https://arxiv.org/abs/2601.02566)
*Junbin Zhang,Hamid Reza Tohidypour,Yixiao Wang,Panos Nasiopoulos*

Main category: cs.CV

TL;DR: 本文提出了一种基于Vision Mamba与新型引导图神经网络（G-GNN）的统一方法，用于定位浅层伪造（shallowfakes）和深层伪造（deepfakes）图像中的篡改区域，显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多仅针对浅伪造或深伪造视频，缺乏能同时处理浅伪造和深伪造图像的通用定位方法，而伪造图像的社会影响重大，亟需统一、鲁棒的定位技术。

Method: 采用Vision Mamba网络提取高判别性特征图以清晰刻画篡改边界，并设计引导图神经网络（G-GNN）模块增强真实像素与篡改像素之间的区分能力。

Result: 在多个基准数据集上，所提方法在定位精度上优于当前主流方法，验证了其对浅伪造和深伪造图像的统一有效性。

Conclusion: Vision Mamba与G-GNN的结合为图像篡改定位提供了新范式，证明了单一深度学习框架可有效应对异构伪造类型，具备实际部署潜力。

Abstract: Image manipulation localization is a critical research task, given that forged images may have a significant societal impact of various aspects. Such image manipulations can be produced using traditional image editing tools (known as "shallowfakes") or advanced artificial intelligence techniques ("deepfakes"). While numerous studies have focused on image manipulation localization on either shallowfake images or deepfake videos, few approaches address both cases. In this paper, we explore the feasibility of using a deep learning network to localize manipulations in both shallow- and deep-fake images, and proposed a solution for such purpose. To precisely differentiate between authentic and manipulated pixels, we leverage the Vision Mamba network to extract feature maps that clearly describe the boundaries between tampered and untouched regions. To further enhance this separation, we propose a novel Guided Graph Neural Network (G-GNN) module that amplifies the distinction between manipulated and authentic pixels. Our evaluation results show that our proposed method achieved higher inference accuracy compared to other state-of-the-art methods.

</details>


### [16] [DreamLoop: Controllable Cinemagraph Generation from a Single Photograph](https://arxiv.org/abs/2601.02646)
*Aniruddha Mahapatra,Long Mai,Cusuh Ham,Feng Liu*

Main category: cs.CV

TL;DR: DreamLoop是一种无需cinemagraph训练数据、从单张照片可控生成高质量cinemagraph的新方法，通过改造通用视频扩散模型实现时间衔接与运动条件控制。


<details>
  <summary>Details</summary>
Motivation: 现有图像动画技术仅支持简单低频运动且适用域窄；大规模视频扩散模型未针对cinemagraph的无缝循环与可控性进行优化，缺乏专用训练数据。

Method: 提出DreamLoop框架，基于通用视频扩散模型，引入两个训练目标：时间桥接（temporal bridging）和运动条件控制（motion conditioning）；推理时以输入图像同时作为首尾帧条件以保证循环无缝，用静态轨迹保持背景静止，并通过用户指定运动路径控制目标物体的轨迹与时序。

Result: 实现了通用场景下高质量、复杂、符合用户意图的cinemagraph生成，在可控性与视觉质量上优于现有方法。

Conclusion: DreamLoop是首个支持通用场景、具备灵活直观控制能力的cinemagraph生成方法，无需领域特定训练数据，有效 bridged 通用视频生成与专业cinemagraph需求之间的鸿沟。

Abstract: Cinemagraphs, which combine static photographs with selective, looping motion, offer unique artistic appeal. Generating them from a single photograph in a controllable manner is particularly challenging. Existing image-animation techniques are restricted to simple, low-frequency motions and operate only in narrow domains with repetitive textures like water and smoke. In contrast, large-scale video diffusion models are not tailored for cinemagraph constraints and lack the specialized data required to generate seamless, controlled loops. We present DreamLoop, a controllable video synthesis framework dedicated to generating cinemagraphs from a single photo without requiring any cinemagraph training data. Our key idea is to adapt a general video diffusion model by training it on two objectives: temporal bridging and motion conditioning. This strategy enables flexible cinemagraph generation. During inference, by using the input image as both the first- and last- frame condition, we enforce a seamless loop. By conditioning on static tracks, we maintain a static background. Finally, by providing a user-specified motion path for a target object, our method provides intuitive control over the animation's trajectory and timing. To our knowledge, DreamLoop is the first method to enable cinemagraph generation for general scenes with flexible and intuitive controls. We demonstrate that our method produces high-quality, complex cinemagraphs that align with user intent, outperforming existing approaches.

</details>


### [17] [GRRE: Leveraging G-Channel Removed Reconstruction Error for Robust Detection of AI-Generated Images](https://arxiv.org/abs/2601.02709)
*Shuman He,Xiehua Li,Xioaju Yang,Yang Xiong,Keqin Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于通道移除重建的新型AI生成图像检测方法GRRE，通过分析绿色通道移除后的重建误差差异，实现了对多种（包括未见过的）生成模型的高精度、强泛化检测。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法在面对新型或未见过的生成模型时泛化能力差，难以准确区分合成图像与真实图像。

Method: 提出G通道移除重建误差（GRRE）方法，利用真实图像与AI生成图像在绿色通道移除后重建误差上的显著差异进行检测。

Result: GRRE在多个生成模型（含训练中未见模型）上均保持高检测精度，且对各类扰动和后处理操作具有强鲁棒性及优越的跨模型泛化能力。

Conclusion: 基于通道移除重建的策略是一种有潜力的图像鉴伪新范式，可有效应对生成式AI带来的真实性挑战。

Abstract: The rapid progress of generative models, particularly diffusion models and GANs, has greatly increased the difficulty of distinguishing synthetic images from real ones. Although numerous detection methods have been proposed, their accuracy often degrades when applied to images generated by novel or unseen generative models, highlighting the challenge of achieving strong generalization. To address this challenge, we introduce a novel detection paradigm based on channel removal reconstruction. Specifically, we observe that when the green (G) channel is removed from real images and reconstructed, the resulting reconstruction errors differ significantly from those of AI-generated images. Building upon this insight, we propose G-channel Removed Reconstruction Error (GRRE), a simple yet effective method that exploits this discrepancy for robust AI-generated image detection. Extensive experiments demonstrate that GRRE consistently achieves high detection accuracy across multiple generative models, including those unseen during training. Compared with existing approaches, GRRE not only maintains strong robustness against various perturbations and post-processing operations but also exhibits superior cross-model generalization. These results highlight the potential of channel-removal-based reconstruction as a powerful forensic tool for safeguarding image authenticity in the era of generative AI.

</details>


### [18] [CAMO: Category-Agnostic 3D Motion Transfer from Monocular 2D Videos](https://arxiv.org/abs/2601.02716)
*Taeyeon Kim,Youngju Na,Jumin Lee,Minhyuk Sung,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: CAMO是一种无需预定义模板或显式3D监督的类别无关运动迁移框架，通过形态参数化的3D高斯点阵与密集语义对应联合优化形状和姿态，有效缓解形状-姿态歧义，实现从单目2D视频到多样化3D网格的高保真运动迁移。


<details>
  <summary>Details</summary>
Motivation: 运动从2D视频迁移到3D资产面临姿态模糊性和物体形状多样性挑战，现有方法常依赖类别特定参数化模板，泛化性差。

Method: 提出CAMO框架，核心是形态参数化的可动3D高斯点阵模型结合密集语义对应关系，通过联合优化同时适配目标网格的形状和姿态。

Result: 实验表明CAMO在运动精度、效率和视觉一致性上优于现有方法，在多种物体类别和日常视频场景中显著提升了运动迁移性能。

Conclusion: CAMO实现了类别无关、模板无关、无显式3D监督的高质量运动迁移，为通用3D运动重建提供了新范式。

Abstract: Motion transfer from 2D videos to 3D assets is a challenging problem, due to inherent pose ambiguities and diverse object shapes, often requiring category-specific parametric templates. We propose CAMO, a category-agnostic framework that transfers motion to diverse target meshes directly from monocular 2D videos without relying on predefined templates or explicit 3D supervision. The core of CAMO is a morphology-parameterized articulated 3D Gaussian splatting model combined with dense semantic correspondences to jointly adapt shape and pose through optimization. This approach effectively alleviates shape-pose ambiguities, enabling visually faithful motion transfer for diverse categories. Experimental results demonstrate superior motion accuracy, efficiency, and visual coherence compared to existing methods, significantly advancing motion transfer in varied object categories and casual video scenarios.

</details>


### [19] [Robust Mesh Saliency GT Acquisition in VR via View Cone Sampling and Geometric Smoothing](https://arxiv.org/abs/2601.02721)
*Guoquan Zheng,Jie Hao,Huiyu Duan,Yongming Han,Liang Yuan,Dong Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种面向3D网格显著性真值获取的鲁棒框架，通过视锥采样（VCS）和混合流形-欧氏约束扩散（HCD）算法，提升复杂拓扑下的采样鲁棒性与显著性传播的一致性，更贴合人类自然感知。


<details>
  <summary>Details</summary>
Motivation: 现有3D网格显著性真值获取方法沿用2D图像思路，忽视3D几何拓扑与2D图像阵列的本质差异；VR眼动追踪中单射线采样和欧氏平滑导致纹理注意力偏差与跨间隙信号泄漏。

Method: 提出视锥采样（VCS）策略，模拟人眼中央凹感受野，采用高斯分布射线束增强复杂拓扑采样鲁棒性；设计混合流形-欧氏约束扩散（HCD）算法，融合流形测地约束与欧氏尺度，保障拓扑一致的显著性传播。

Result: 有效缓解‘拓扑短路’与混叠现象，实现高保真的3D注意力获取，显著提升显著性真值质量与鲁棒性。

Conclusion: 该框架为3D网格显著性研究提供了更准确、符合人类感知的基准范式，推动人本视觉建模在虚拟现实中的发展。

Abstract: Reliable 3D mesh saliency ground truth (GT) is essential for human-centric visual modeling in virtual reality (VR). However, current 3D mesh saliency GT acquisition methods are generally consistent with 2D image methods, ignoring the differences between 3D geometry topology and 2D image array. Current VR eye-tracking pipelines rely on single ray sampling and Euclidean smoothing, triggering texture attention and signal leakage across gaps. This paper proposes a robust framework to address these limitations. We first introduce a view cone sampling (VCS) strategy, which simulates the human foveal receptive field via Gaussian-distributed ray bundles to improve sampling robustness for complex topologies. Furthermore, a hybrid Manifold-Euclidean constrained diffusion (HCD) algorithm is developed, fusing manifold geodesic constraints with Euclidean scales to ensure topologically-consistent saliency propagation. By mitigating "topological short-circuits" and aliasing, our framework provides a high-fidelity 3D attention acquisition paradigm that aligns with natural human perception, offering a more accurate and robust baseline for 3D mesh saliency research.

</details>


### [20] [Foreground-Aware Dataset Distillation via Dynamic Patch Selection](https://arxiv.org/abs/2601.02727)
*Longzhen Li,Guang Li,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出了一种前景感知的数据集蒸馏方法，利用Grounded SAM2识别前景并动态选择图像块或全图，以提升合成数据的信息量和泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统数据集蒸馏方法存在计算开销大、生成图像不真实、泛化性差等问题；现有非优化方法虽用真实图像块，但固定选择策略易丢失关键目标信息。

Method: 基于Grounded SAM2提取每张图像的前景占用率，据此设定类别级图像块选择阈值，并设计双路径动态选择策略：前景占比高时直接缩放全图，否则从多个候选块中选最信息量大的块。

Result: 在多个基准上显著优于现有方法，蒸馏数据更具信息量和代表性，且在不同网络架构和图像组成下鲁棒性更强。

Conclusion: 前景感知的动态图像块选择策略能更有效地保留主要对象的关键信息，提升数据集蒸馏的质量与泛化能力。

Abstract: In this paper, we propose a foreground-aware dataset distillation method that enhances patch selection in a content-adaptive manner. With the rising computational cost of training large-scale deep models, dataset distillation has emerged as a promising approach for constructing compact synthetic datasets that retain the knowledge of their large original counterparts. However, traditional optimization-based methods often suffer from high computational overhead, memory constraints, and the generation of unrealistic, noise-like images with limited architectural generalization. Recent non-optimization methods alleviate some of these issues by constructing distilled data from real image patches, but the used rigid patch selection strategies can still discard critical information about the main objects. To solve this problem, we first leverage Grounded SAM2 to identify foreground objects and compute per-image foreground occupancy, from which we derive a category-wise patch decision threshold. Guided by these thresholds, we design a dynamic patch selection strategy that, for each image, either selects the most informative patch from multiple candidates or directly resizes the full image when the foreground dominates. This dual-path mechanism preserves more key information about the main objects while reducing redundant background content. Extensive experiments on multiple benchmarks show that the proposed method consistently improves distillation performance over existing approaches, producing more informative and representative distilled datasets and enhancing robustness across different architectures and image compositions.

</details>


### [21] [HOLO: Homography-Guided Pose Estimator Network for Fine-Grained Visual Localization on SD Maps](https://arxiv.org/abs/2601.02730)
*Xuchang Zhong,Xu Cao,Jinke Feng,Hao Fang*

Main category: cs.CV

TL;DR: 本文提出了一种基于单应性引导的位姿估计网络，用于多视角图像与标准分辨率（SD）地图之间的细粒度视觉定位，通过在BEV域中投影并语义对齐特征，并利用单应性关系指导特征融合和约束位姿输出，显著提升了训练效率和定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于回归的视觉定位方法忽视了几何先验，导致训练效率低、定位精度受限；同时缺乏对跨分辨率输入的支持。

Method: 构建满足单应性约束的输入对（将地视图特征投影至BEV域并与地图特征语义对齐），利用单应性关系指导特征融合，并将位姿输出限制在有效可行区域内；统一BEV语义推理与单应性学习。

Result: 在nuScenes数据集上显著超越现有SOTA视觉定位方法；支持跨分辨率输入，提升模型灵活性；代码与预训练模型将开源。

Conclusion: 这是首个将BEV语义推理与单应性学习统一用于图像到地图视觉定位的工作，通过显式建模单应性变换，兼顾几何一致性与语义对齐，有效提升定位性能与泛化能力。

Abstract: Visual localization on standard-definition (SD) maps has emerged as a promising low-cost and scalable solution for autonomous driving. However, existing regression-based approaches often overlook inherent geometric priors, resulting in suboptimal training efficiency and limited localization accuracy. In this paper, we propose a novel homography-guided pose estimator network for fine-grained visual localization between multi-view images and standard-definition (SD) maps. We construct input pairs that satisfy a homography constraint by projecting ground-view features into the BEV domain and enforcing semantic alignment with map features. Then we leverage homography relationships to guide feature fusion and restrict the pose outputs to a valid feasible region, which significantly improves training efficiency and localization accuracy compared to prior methods relying on attention-based fusion and direct 3-DoF pose regression. To the best of our knowledge, this is the first work to unify BEV semantic reasoning with homography learning for image-to-map localization. Furthermore, by explicitly modeling homography transformations, the proposed framework naturally supports cross-resolution inputs, enhancing model flexibility. Extensive experiments on the nuScenes dataset demonstrate that our approach significantly outperforms existing state-of-the-art visual localization methods. Code and pretrained models will be publicly released to foster future research.

</details>


### [22] [Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench](https://arxiv.org/abs/2601.02737)
*Zanting Ye,Xiaolong Niu,Xuanbin Wu,Xu Han,Shengyuan Liu,Jing Hao,Zhihao Peng,Hao Sun,Jieqin Lv,Fanghu Wang,Yanchao Huang,Hubing Wu,Yixuan Yuan,Habib Zaidi,Arman Rahmim,Yefeng Zheng,Lijun Lu*

Main category: cs.CV

TL;DR: 本文揭示了多模态大语言模型（MLLMs）在功能影像（如PET）理解中的根本性感知缺陷，提出了首个大规模功能影像基准PET-Bench，并发现标准思维链（CoT）提示会引发临床误导性幻觉；为此提出原子视觉对齐（AVA）微调方法，显著提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在解剖影像任务中表现优异，但在功能影像（如PET）的理解能力尚未被系统探索，存在功能感知与形态先验混淆的根本缺陷。

Method: 构建首个大规模功能影像基准PET-Bench（含52,308组QA对）；通过评估19种SOTA MLLMs识别出CoT幻觉陷阱；提出Atomic Visual Alignment（AVA）微调策略，强制模型先掌握低层功能感知再进行高层诊断推理。

Result: AVA成功弥合功能感知鸿沟，将CoT从幻觉源转变为可靠推理工具，诊断准确率最高提升14.83%。

Conclusion: 功能影像理解需以底层功能特征感知为前提，脱离形态先验的视觉对齐是保障临床可信推理的关键；PET-Bench和AVA为安全可靠的医学MLLM发展提供了新范式。

Abstract: While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.

</details>


### [23] [D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images](https://arxiv.org/abs/2601.02747)
*Zixiao Wen,Zhen Yang,Xianjie Bao,Lei Zhang,Xiantai Xiang,Wenshuai Li,Yuhan Liu*

Main category: cs.CV

TL;DR: 本文提出D^3R-DETR，一种基于DETR的新型检测器，通过融合空间与频域信息来优化低层特征图并生成更精确的物体密度图，从而提升遥感图像中微小物体的检测精度。


<details>
  <summary>Details</summary>
Motivation: 主流基于Transformer的检测器在遥感图像中检测微小物体时存在收敛慢、查询-物体匹配不准确的问题，主要由于像素信息极度有限及物体密度变化大。

Method: 提出D^3R-DETR，引入双域密度精炼（Dual-Domain Density Refinement），融合空间域与频率域信息以增强低层特征图，并利用其丰富细节预测更准确的物体密度图，指导微小物体精确定位。

Result: 在AI-TOD-v2数据集上的大量实验表明，D^3R-DETR在微小物体检测任务上优于现有最先进方法。

Conclusion: D^3R-DETR有效缓解了微小物体检测中的特征不足与密度变化问题，提升了定位精度与模型收敛速度，为遥感智能解译提供了新思路。

Abstract: Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.

</details>


### [24] [Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups](https://arxiv.org/abs/2601.02759)
*Hyungtae Lim,Minkyun Seo,Luca Carlone,Jaesik Park*

Main category: cs.CV

TL;DR: 本文提出BUFFER-X，一种无需训练的点云配准框架，通过几何引导的超参数估计、分布感知的最远点采样和局部坐标归一化，实现零样本泛化；并进一步提出轻量版BUFFER-X-Lite，在显著提速的同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习点云配准方法在零样本泛化能力上存在不足，需针对新环境重新调参或训练；其根源在于固定超参数、学习型关键点检测器跨域迁移性差、以及绝对坐标导致尺度不一致。

Method: 提出BUFFER-X框架：(a) 几何自举法自动估计超参数（如体素大小、搜索半径）；(b) 分布感知的最远点采样替代学习型关键点检测；(c) 补丁级坐标归一化保障尺度一致性；并采用分层多尺度匹配（局部/中观/全局感受野）提取对应关系；另设计BUFFER-X-Lite，引入早停策略与快速位姿求解器以提升效率。

Result: 在涵盖物体级、室内外及跨传感器（异构LiDAR）的12个数据集组成的综合基准上验证，BUFFER-X无需任何手动调参或目标域先验知识即可实现有效泛化；BUFFER-X-Lite相较原版计算时间减少43%，精度保持不变。

Conclusion: BUFFER-X实现了真正意义上的零样本、免训练点云配准，解决了尺度适应性、关键点迁移性与坐标表示三大瓶颈，为实际部署提供了鲁棒、高效且通用的新范式。

Abstract: Some deep learning-based point cloud registration methods struggle with zero-shot generalization, often requiring dataset-specific hyperparameter tuning or retraining for new environments. We identify three critical limitations: (a) fixed user-defined parameters (e.g., voxel size, search radius) that fail to generalize across varying scales, (b) learned keypoint detectors exhibit poor cross-domain transferability, and (c) absolute coordinates amplify scale mismatches between datasets. To address these three issues, we present BUFFER-X, a training-free registration framework that achieves zero-shot generalization through: (a) geometric bootstrapping for automatic hyperparameter estimation, (b) distribution-aware farthest point sampling to replace learned detectors, and (c) patch-level coordinate normalization to ensure scale consistency. Our approach employs hierarchical multi-scale matching to extract correspondences across local, middle, and global receptive fields, enabling robust registration in diverse environments. For efficiency-critical applications, we introduce BUFFER-X-Lite, which reduces total computation time by 43% (relative to BUFFER-X) through early exit strategies and fast pose solvers while preserving accuracy. We evaluate on a comprehensive benchmark comprising 12 datasets spanning object-scale, indoor, and outdoor scenes, including cross-sensor registration between heterogeneous LiDAR configurations. Results demonstrate that our approach generalizes effectively without manual tuning or prior knowledge of test domains. Code: https://github.com/MIT-SPARK/BUFFER-X.

</details>


### [25] [AnyDepth: Depth Estimation Made Easy](https://arxiv.org/abs/2601.02760)
*Zeyu Ren,Zeyu Zhang,Wukai Li,Qingxiang Liu,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级、数据驱动的零样本单目深度估计框架，采用DINOv3作为视觉编码器，并设计了结构更简、参数更少的Simple Depth Transformer（SDT）解码器，同时引入基于质量的数据筛选策略，在提升精度的同时大幅降低计算开销和数据依赖。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模数据集和复杂解码器，导致效率低、泛化能力差。

Method: 采用DINOv3提取密集特征；设计轻量单路径Transformer解码器SDT替代DPT；提出基于质量的数据过滤策略优化训练集。

Result: 在五个基准上精度超越DPT，SDT参数减少85%-89%，计算开销显著降低。

Conclusion: 模型结构精简与高质量数据协同优化，是实现高效、泛化强的零样本深度估计的关键。

Abstract: Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.

</details>


### [26] [ClearAIR: A Human-Visual-Perception-Inspired All-in-One Image Restoration](https://arxiv.org/abs/2601.02763)
*Xu Zhang,Huan Zhang,Guoli Wang,Qian Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: 本文提出ClearAIR框架，受人类视觉感知启发，采用粗到细的分层修复策略，结合多模态大语言模型评估、区域感知与任务识别、以及内部线索重用机制，显著提升全合一图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 现有全合一图像恢复方法依赖于退化特定表示，导致过度平滑和伪影问题。

Method: 提出ClearAIR框架：1）基于多模态大语言模型的图像质量评估；2）区域感知与任务识别管道，含语义交叉注意力与退化感知模块；3）自监督的内部线索重用机制以恢复细节。

Result: ClearAIR在多种合成与真实世界数据集上均取得优越性能。

Conclusion: ClearAIR通过模仿人类视觉感知并融合多阶段协同策略，有效解决了复杂退化下的图像恢复难题，提升了整体与局部修复质量。

Abstract: All-in-One Image Restoration (AiOIR) has advanced significantly, offering promising solutions for complex real-world degradations. However, most existing approaches rely heavily on degradation-specific representations, often resulting in oversmoothing and artifacts. To address this, we propose ClearAIR, a novel AiOIR framework inspired by Human Visual Perception (HVP) and designed with a hierarchical, coarse-to-fine restoration strategy. First, leveraging the global priority of early HVP, we employ a Multimodal Large Language Model (MLLM)-based Image Quality Assessment (IQA) model for overall evaluation. Unlike conventional IQA, our method integrates cross-modal understanding to more accurately characterize complex, composite degradations. Building upon this overall assessment, we then introduce a region awareness and task recognition pipeline. A semantic cross-attention, leveraging semantic guidance unit, first produces coarse semantic prompts. Guided by this regional context, a degradation-aware module implicitly captures region-specific degradation characteristics, enabling more precise local restoration. Finally, to recover fine details, we propose an internal clue reuse mechanism. It operates in a self-supervised manner to mine and leverage the intrinsic information of the image itself, substantially enhancing detail restoration. Experimental results show that ClearAIR achieves superior performance across diverse synthetic and real-world datasets.

</details>


### [27] [AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs](https://arxiv.org/abs/2601.02771)
*Boyu Chang,Qi Wang,Xi Guo,Zhixiong Nan,Yazhou Yao,Tianfei Zhou*

Main category: cs.CV

TL;DR: 本文提出AbductiveMLLM，通过结合语言推理（REASONER）与图像生成（IMAGINER）双模块，模拟人类的言语与图像溯因能力，显著提升多模态大模型在视觉溯因推理（VAR）任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视觉溯因推理（VAR）任务中表现弱于人类，因其缺乏对言语与图像双通道溯因协同机制的建模。

Method: 提出双组件框架AbductiveMLLM：REASONER在语言域内先广搜后基于跨模态因果对齐剪枝假设，并将筛选后的假设作为先验引导MLLM；IMAGINER则利用文本到图像扩散模型，以输入视频和REASONER输出为条件生成对应视觉场景，增强上下文感知。两模块端到端联合训练。

Result: 在标准VAR基准上达到SOTA性能，持续优于传统方法及先进MLLM。

Conclusion: 融合言语溯因与图像想象的双模态协同机制，可有效提升MLLM的视觉溯因推理能力，为构建类人多模态推理系统提供新范式。

Abstract: Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER's output embeddings to "imagine" plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs' contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.

</details>


### [28] [EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework](https://arxiv.org/abs/2601.02783)
*Junjue Wang,Yanfei Zhong,Zihang Chen,Zhuo Zheng,Ailong Ma,Liangpei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种面向地理场景理解的地球视觉-语言联合框架EarthVLNet及配套多任务数据集EarthVLSet，旨在提升遥感图像中对象关系推理能力，尤其服务于城市规划应用。


<details>
  <summary>Details</summary>
Motivation: 地球视觉在地物识别上已取得进展，但在对象关系推理方面缺乏探索，限制了对场景的全面理解。

Method: 构建了包含10.9k高分辨率遥感影像、地表覆盖掩膜和761.5k图文对的EarthVLSet数据集；提出对象中心的EarthVLNet，分阶段实现地表覆盖语义分割、基于像素语义引导的对象感知大语言模型关系推理与知识总结，并引入数值差异损失进行优化。

Result: 在语义分割、多选题VQA和开放题VQA三个基准上均表现优越；发现分割特征可跨数据集提升VQA性能；多选题更依赖视觉编码器，开放题则需强视觉与语言解码器协同。

Conclusion: EarthVLSet与EarthVLNet为连接‘图像-掩膜-文本’提供了新范式，推动地球视觉在地理应用中的深入发展，并指明了三个未来方向。

Abstract: Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects' statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects ''image-mask-text'', advancing geographical applications for Earth vision.

</details>


### [29] [DreamStyle: A Unified Framework for Video Stylization](https://arxiv.org/abs/2601.02785)
*Mengtian Li,Jinshu Chen,Songtao Zhao,Wanquan Feng,Pengqi Tu,Qian He*

Main category: cs.CV

TL;DR: DreamStyle是一个支持文本、风格图像和首帧三种条件引导的统一视频风格化框架，通过改进的LoRA微调策略和高质量数据集提升风格一致性和视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频风格化方法局限于单一风格条件输入，且缺乏高质量数据集，导致风格不一致和时间闪烁问题。

Method: 基于基础Image-to-Video模型，采用面向token的低秩自适应（LoRA）微调，并构建专用数据清洗流程以获取高质量配对视频数据。

Result: 在三种风格化任务上均表现优异，显著优于现有方法，在风格一致性和视频质量方面取得提升。

Conclusion: DreamStyle实现了多条件统一视频风格化，验证了其泛化能力与实用性，为视频生成下游任务提供了新范式。

Abstract: Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.

</details>


### [30] [Textile IR: A Bidirectional Intermediate Representation for Physics-Aware Fashion CAD](https://arxiv.org/abs/2601.02792)
*Petteri Teikari,Neliana Fuenmayor*

Main category: cs.CV

TL;DR: 本文提出Textile IR，一种连接CAD、物理仿真和生命周期评估的双向中间表示，通过七层验证阶梯实现时尚设计中制造可行性、物理行为预测与可持续性评估的集成，并支持双向反馈与不确定性传播。


<details>
  <summary>Details</summary>
Motivation: 现有工具孤立运作：版型软件无法理解悬垂性，物理仿真无法自动修正版型；同时，材料测试误差、仿真近似和LCA数据缺口导致可持续性声明不可靠，亟需显式不确定性追踪与跨域协同。

Method: 构建七层Verification Ladder（从语法检查到物理验证）的Textile IR中间表示，采用场景图（scene-graph）建模，将服装形式化为结构化程序；定义三域约束满足问题，支持双向反馈、实时可持续性更新与不确定性传播。

Result: 实现了CAD、仿真与LCA的语义集成；支持仿真失败驱动版型修改、材料替换实时更新可持续性指标、跨流程不确定性显式建模；验证了AI可将服装作为结构化程序而非像素数组进行操作。

Conclusion: Textile IR首次为时尚工程提供了形式化、可操作、具工程后果的统一表示，使设计师能在早期同步权衡可持续性、可制造性与美学，避免高成本实物打样后才发现冲突。

Abstract: We introduce Textile IR, a bidirectional intermediate representation that connects manufacturing-valid CAD, physics-based simulation, and lifecycle assessment for fashion design. Unlike existing siloed tools where pattern software guarantees sewable outputs but understands nothing about drape, and physics simulation predicts behaviour but cannot automatically fix patterns, Textile IR provides the semantic glue for integration through a seven-layer Verification Ladder -- from cheap syntactic checks (pattern closure, seam compatibility) to expensive physics validation (drape simulation, stress analysis). The architecture enables bidirectional feedback: simulation failures suggest pattern modifications; material substitutions update sustainability estimates in real time; uncertainty propagates across the pipeline with explicit confidence bounds. We formalise fashion engineering as constraint satisfaction over three domains and demonstrate how Textile IR's scene-graph representation enables AI systems to manipulate garments as structured programs rather than pixel arrays. The framework addresses the compound uncertainty problem: when measurement errors in material testing, simulation approximations, and LCA database gaps combine, sustainability claims become unreliable without explicit uncertainty tracking. We propose six research priorities and discuss deployment considerations for fashion SMEs where integrated workflows reduce specialised engineering requirements. Key contribution: a formal representation that makes engineering constraints perceptible, manipulable, and immediately consequential -- enabling designers to navigate sustainability, manufacturability, and aesthetic tradeoffs simultaneously rather than discovering conflicts after costly physical prototyping.

</details>


### [31] [StableDPT: Temporal Stable Monocular Video Depth Estimation](https://arxiv.org/abs/2601.02793)
*Ivan Sobko,Hayko Riemenschneider,Markus Gross,Christopher Schroers*

Main category: cs.CV

TL;DR: 本文提出StableDPT方法，通过在Dense Prediction Transformer（DPT）头部引入可训练的时序模块（基于高效跨注意力机制），将单图单目深度估计模型适配至视频序列，显著提升深度预测的时序稳定性与精度，同时支持任意长度视频的高效推理。


<details>
  <summary>Details</summary>
Motivation: 单张图像的单目深度估计（MDE）模型直接应用于视频时存在严重的时序不稳定和闪烁伪影问题，亟需一种能建模帧间关系、兼顾精度与效率的视频深度估计方案。

Method: 基于现成ViT编码器与DPT解码头，设计新型时序模块嵌入DPT头部，利用跨注意力机制融合全局采样关键帧信息；并提出无重叠、免尺度错位的新型视频推理策略。

Result: 在多个基准数据集上验证了更优的时序一致性、具有竞争力的SOTA性能，并在实际场景中实现2倍加速。

Conclusion: StableDPT有效解决了视频深度估计中的时序不稳定性问题，其轻量级时序模块和高效推理策略为图像模型向视频任务迁移提供了通用且实用的新范式。

Abstract: Applying single image Monocular Depth Estimation (MDE) models to video sequences introduces significant temporal instability and flickering artifacts. We propose a novel approach that adapts any state-of-the-art image-based (depth) estimation model for video processing by integrating a new temporal module - trainable on a single GPU in a few days. Our architecture StableDPT builds upon an off-the-shelf Vision Transformer (ViT) encoder and enhances the Dense Prediction Transformer (DPT) head. The core of our contribution lies in the temporal layers within the head, which use an efficient cross-attention mechanism to integrate information from keyframes sampled across the entire video sequence. This allows the model to capture global context and inter-frame relationships leading to more accurate and temporally stable depth predictions. Furthermore, we propose a novel inference strategy for processing videos of arbitrary length avoiding the scale misalignment and redundant computations associated with overlapping windows used in other methods. Evaluations on multiple benchmark datasets demonstrate improved temporal consistency, competitive state-of-the-art performance and on top 2x faster processing in real-world scenarios.

</details>


### [32] [Topology-aware Pathological Consistency Matching for Weakly-Paired IHC Virtual Staining](https://arxiv.org/abs/2601.02806)
*Mingzhou Jiang,Jiaying Zhou,Nan Zeng,Mickael Li,Qijie Tang,Chao He,Huazhu Fu,Honghui He*

Main category: cs.CV

TL;DR: 本文提出了一种拓扑感知的H&E到IHC虚拟染色框架，通过图对比学习和拓扑扰动解决空间错位问题，提升病理一致性与生成质量。


<details>
  <summary>Details</summary>
Motivation: IHC染色流程复杂、耗时且昂贵，限制其临床广泛应用；而基于相邻切片的虚拟染色常因空间错位和局部形变导致弱配对，影响监督学习效果。

Method: 提出拓扑感知一致性匹配（TACM）机制（结合图对比学习与拓扑扰动）和拓扑约束病理匹配（TCPM）机制（依据节点重要性对齐病理阳性区域）。

Result: 在两个基准数据集、四种染色任务上显著优于现有方法，生成图像质量更高、临床相关性更强。

Conclusion: 该拓扑感知框架有效缓解了弱配对与空间错位问题，为低成本、高可靠性的虚拟IHC染色提供了新范式。

Abstract: Immunohistochemical (IHC) staining provides crucial molecular characterization of tissue samples and plays an indispensable role in the clinical examination and diagnosis of cancers. However, compared with the commonly used Hematoxylin and Eosin (H&E) staining, IHC staining involves complex procedures and is both time-consuming and expensive, which limits its widespread clinical use. Virtual staining converts H&E images to IHC images, offering a cost-effective alternative to clinical IHC staining. Nevertheless, using adjacent slides as ground truth often results in weakly-paired data with spatial misalignment and local deformations, hindering effective supervised learning. To address these challenges, we propose a novel topology-aware framework for H&E-to-IHC virtual staining. Specifically, we introduce a Topology-aware Consistency Matching (TACM) mechanism that employs graph contrastive learning and topological perturbations to learn robust matching patterns despite spatial misalignments, ensuring structural consistency. Furthermore, we propose a Topology-constrained Pathological Matching (TCPM) mechanism that aligns pathological positive regions based on node importance to enhance pathological consistency. Extensive experiments on two benchmarks across four staining tasks demonstrate that our method outperforms state-of-the-art approaches, achieving superior generation quality with higher clinical relevance.

</details>


### [33] [SketchThinker-R1: Towards Efficient Sketch-Style Reasoning in Large Multimodal Models](https://arxiv.org/abs/2601.02825)
*Ruiyang Zhang,Dongzhan Zhou,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本文提出SketchThinker-R1，通过模仿人类草图式推理（简洁、目标导向、聚焦关键信息），提升多模态大模型的推理效率，在显著降低token消耗的同时保持答案准确率。


<details>
  <summary>Details</summary>
Motivation: 现有长链推理虽有效但计算开销大（高token成本与响应延迟），而人类常采用更高效、简明的草图式推理，因此需在多模态大模型中建模并激发此类能力。

Method: 三阶段方法：1）Sketch-Mode冷启动——将长推理蒸馏为草图式推理并微调基础模型；2）训练SketchJudge奖励模型，显式评估并奖励草图式推理过程；3）基于SketchJudge进行强化学习，进一步泛化草图式推理能力。

Result: 在四个基准上实现超64%的推理token成本下降，且最终答案准确率未下降；定性分析表明模型更关注解题中的关键线索。

Conclusion: 草图式推理可显著提升多模态大模型的推理效率与实用性，SketchThinker-R1为高效、可控、可解释的多模态推理提供了新范式。

Abstract: Despite the empirical success of extensive, step-by-step reasoning in large multimodal models, long reasoning processes inevitably incur substantial computational overhead, i.e., in terms of higher token costs and increased response time, which undermines inference efficiency. In contrast, humans often employ sketch-style reasoning: a concise, goal-directed cognitive process that prioritizes salient information and enables efficient problem-solving. Inspired by this cognitive efficiency, we propose SketchThinker-R1, which incentivizes sketch-style reasoning ability in large multimodal models. Our method consists of three primary stages. In the Sketch-Mode Cold Start stage, we convert standard long reasoning process into sketch-style reasoning and finetune base multimodal model, instilling initial sketch-style reasoning capability. Next, we train SketchJudge Reward Model, which explicitly evaluates thinking process of model and assigns higher scores to sketch-style reasoning. Finally, we conduct Sketch-Thinking Reinforcement Learning under supervision of SketchJudge to further generalize sketch-style reasoning ability. Experimental evaluation on four benchmarks reveals that our SketchThinker-R1 achieves over 64% reduction in reasoning token cost without compromising final answer accuracy. Qualitative analysis further shows that sketch-style reasoning focuses more on key cues during problem solving.

</details>


### [34] [DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection](https://arxiv.org/abs/2601.02831)
*Yuetong Li,Qing Zhang,Yilin Zhao,Gongyang Li,Zeming Liu*

Main category: cs.CV

TL;DR: 本文提出了DGA-Net，一种通过新型“深度提示”范式适配Segment Anything Model（SAM）的伪装物体检测框架，利用跨模态图增强和锚点引导细化模块，有效融合RGB语义与深度几何信息，提升检测精度与一致性。


<details>
  <summary>Details</summary>
Motivation: 为充分利用深度线索提升伪装物体检测（COD）性能，克服现有方法仅依赖稀疏提示（如点、框）的局限性。

Method: 提出DGA-Net框架，包含：1）Cross-modal Graph Enhancement（CGE）模块，在异构图中融合RGB语义与深度几何信息，生成统一指导信号；2）Anchor-Guided Refinement（AGR）模块，构建全局锚点并建立深层到浅层的非局部通路，缓解特征层级中的信息衰减。

Result: 在定量与定性实验中，DGA-Net均优于当前最先进的COD方法。

Conclusion: DGA-Net通过密集深度提示、跨模态图建模与锚点引导的特征传播机制，显著提升了COD任务中对深度线索的利用效率与分割精度。

Abstract: To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting" paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.

</details>


### [35] [Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection](https://arxiv.org/abs/2601.02837)
*Yuteng Liu,Duanni Meng,Maoxun Yuan,Xingxing Wei*

Main category: cs.CV

TL;DR: 本文提出SEF-DETR框架，通过频率引导的块筛选、动态嵌入增强和可靠性-一致性感知融合三个模块，改进红外小目标检测中DETR类模型的查询初始化问题，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于DETR的红外小目标检测方法因自注意力机制易被背景特征主导，导致查询初始化不可靠和定位不准。

Method: 提出SEF-DETR框架，包含：1）频率引导的块筛选（FPS）构建目标相关密度图；2）动态嵌入增强（DEE）进行目标感知的多尺度表征强化；3）可靠性-一致性感知融合（RCF）在空间-频率域约束查询优化。

Result: 在三个公开红外小目标数据集上，SEF-DETR显著优于当前最先进方法，展现出更强的鲁棒性与效率。

Conclusion: SEF-DETR有效缓解了DETR在红外小目标检测中因背景干扰导致的查询初始化失效问题，为该任务提供了新思路和实用解决方案。

Abstract: Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.

</details>


### [36] [Towards Agnostic and Holistic Universal Image Segmentation with Bit Diffusion](https://arxiv.org/abs/2601.02881)
*Jakob Lønborg Christensen,Morten Rieger Hannemose,Anders Bjorholm Dahl,Vedrana Andersen Dahl*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的通用图像分割框架，通过关键改进（如位置感知调色板、2D灰度码排序、tanh激活、sigmoid损失加权等）实现端到端、全图式的离散分割预测，虽暂未超越主流掩码模型，但具备建模分割不确定性的新能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像分割方法多依赖于掩码生成框架，缺乏对分割不确定性等原理性建模能力；本文旨在探索不依赖掩码、能以更统一和原则性方式完成通用图像分割的新范式。

Method: 提出基于扩散模型的通用图像分割框架，关键改进包括：设计位置感知的调色板与2D灰度码像素排序策略、引入tanh激活函数适配离散输出、采用sigmoid加权的损失函数并选择x-prediction作为预测目标。所有模型均从零训练。

Result: 所提方法在多个基准上缩小了与领先掩码模型（如Mask R-CNN等）的性能差距，并首次实现了对分割模糊/歧义区域的原理性建模；验证了各项改进（如灰度码排序、tanh、sigmoid加权）的有效性。

Conclusion: 扩散模型可作为通用图像分割的新基础架构；当前框架虽性能尚有提升空间，但其整体式预测范式和不确定性建模能力为未来结合大规模预训练或提示式条件化提供了新路径。

Abstract: This paper introduces a diffusion-based framework for universal image segmentation, making agnostic segmentation possible without depending on mask-based frameworks and instead predicting the full segmentation in a holistic manner. We present several key adaptations to diffusion models, which are important in this discrete setting. Notably, we show that a location-aware palette with our 2D gray code ordering improves performance. Adding a final tanh activation function is crucial for discrete data. On optimizing diffusion parameters, the sigmoid loss weighting consistently outperforms alternatives, regardless of the prediction type used, and we settle on x-prediction. While our current model does not yet surpass leading mask-based architectures, it narrows the performance gap and introduces unique capabilities, such as principled ambiguity modeling, that these models lack. All models were trained from scratch, and we believe that combining our proposed improvements with large-scale pretraining or promptable conditioning could lead to competitive models.

</details>


### [37] [TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors](https://arxiv.org/abs/2601.02908)
*Wei-Yuan Cheng,Kai-Po Chang,Chi-Pin Huang,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 本文提出TA-Prompting方法，通过引入时间锚点（Temporal Anchors）增强视频大语言模型（VideoLLMs）的时间定位能力，并设计事件一致性采样策略以提升密集视频描述生成的准确性和时序理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLMs在未剪辑视频中难以精确定位事件边界，导致生成的字幕缺乏时序接地性（temporal grounding）。

Method: 提出TA-Prompting框架：1）利用Temporal Anchors学习精确事件定位；2）设计事件一致性采样策略，兼顾事件间时序连贯性与跨模态视频相似性。

Result: 在多个基准数据集上显著优于当前SOTA VideoLLMs，在密集视频描述、时刻检索和TemporalQA等任务中均取得更优性能。

Conclusion: TA-Prompting有效提升了VideoLLMs的时序感知与事件定位能力，为密集视频理解提供了新思路。

Abstract: Dense video captioning aims to interpret and describe all temporally localized events throughout an input video. Recent state-of-the-art methods leverage large language models (LLMs) to provide detailed moment descriptions for video data. However, existing VideoLLMs remain challenging in identifying precise event boundaries in untrimmed videos, causing the generated captions to be not properly grounded. In this paper, we propose TA-Prompting, which enhances VideoLLMs via Temporal Anchors that learn to precisely localize events and prompt the VideoLLMs to perform temporal-aware video event understanding. During inference, in order to properly determine the output caption sequence from an arbitrary number of events presented within a video, we introduce an event coherent sampling strategy to select event captions with sufficient coherence across temporal events and cross-modal similarity with the given video. Through extensive experiments on benchmark datasets, we show that our TA-Prompting is favorable against state-of-the-art VideoLLMs, yielding superior performance on dense video captioning and temporal understanding tasks including moment retrieval and temporalQA.

</details>


### [38] [Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning](https://arxiv.org/abs/2601.02918)
*Guoqiang Liang,Jianyi Wang,Zhonghua Wu,Shangchen Zhou*

Main category: cs.CV

TL;DR: 本文提出Zoom-IQA，一种基于视觉语言模型的图像质量评估方法，通过两阶段训练（监督微调与带KL-Coverage正则和渐进重采样的强化学习）模拟不确定性感知、区域推理和迭代优化等认知行为，显著提升鲁棒性、可解释性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的IQA方法在融合视觉与文本线索方面能力有限，导致推理不可靠；且缺乏对关键区域的定位能力与认知层面建模。

Method: 提出Zoom-IQA模型，包含两阶段训练：1）在自建GR-IQA数据集上进行监督微调，实现评估结果与关键图像区域的对齐；2）采用强化学习，引入KL-Coverage正则防止推理与评分多样性坍缩，并结合渐进重采样策略缓解标注偏差。

Result: Zoom-IQA在鲁棒性、可解释性和泛化性上均优于现有方法，并在图像恢复等下游任务中验证了其有效性。

Conclusion: 显式建模人类认知行为（如不确定性感知、区域推理、迭代优化）可显著提升VLM-based IQA的可靠性与实用性，为可解释图像质量评估提供了新范式。

Abstract: Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.

</details>


### [39] [DCG ReID: Disentangling Collaboration and Guidance Fusion Representations for Multi-modal Vehicle Re-Identification](https://arxiv.org/abs/2601.02924)
*Aihua Zheng,Ya Gao,Shihao Li,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出DCG-ReID方法，通过动态置信度加权解耦多模态数据，并针对质量均衡与不均衡场景分别设计协作融合模块（CFM）和引导融合模块（GFM），以兼顾类内一致性与模态间异质性，显著提升多模态车辆重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法用单一融合模型处理所有多模态数据，忽视了模态质量分布平衡与不平衡两类情形的不同融合需求，难以协调类内一致性与模态间异质性的冲突。

Method: 提出DCG-ReID框架，包含：1）动态置信度驱动的解耦加权机制（DCDW），依据模态交互置信度动态调整RGB/NIR/TIR三模态贡献；2）针对质量均衡场景的协作融合模块（CFM），挖掘模态对间共识特征；3）针对质量不均衡场景的引导融合模块（GFM），差异化增强主导模态优势并引导辅助模态挖掘互补判别信息。

Result: 在WMVeID863、MSVR310和RGBNT100三个多模态ReID基准上验证了方法有效性，性能优于现有方法。

Conclusion: DCG-ReID通过解耦建模与场景自适应融合策略，有效缓解了多模态车辆ReID中因模态质量分布不确定性引发的融合冲突，提升了跨模态判别能力与鲁棒性。

Abstract: Multi-modal vehicle Re-Identification (ReID) aims to leverage complementary information from RGB, Near Infrared (NIR), and Thermal Infrared (TIR) modalities to retrieve the same vehicle. The challenges of multi-modal vehicle ReID arise from the uncertainty of modality quality distribution induced by inherent discrepancies across modalities, resulting in distinct conflicting fusion requirements for data with balanced and unbalanced quality distributions. Existing methods handle all multi-modal data within a single fusion model, overlooking the different needs of the two data types and making it difficult to decouple the conflict between intra-class consistency and inter-modal heterogeneity. To this end, we propose Disentangle Collaboration and Guidance Fusion Representations for Multi-modal Vehicle ReID (DCG-ReID). Specifically, to disentangle heterogeneous quality-distributed modal data without mutual interference, we first design the Dynamic Confidence-based Disentangling Weighting (DCDW) mechanism: dynamically reweighting three-modal contributions via interaction-derived modal confidence to build a disentangled fusion framework. Building on DCDW, we develop two scenario-specific fusion strategies: (1) for balanced quality distributions, Collaboration Fusion Module (CFM) mines pairwise consensus features to capture shared discriminative information and boost intra-class consistency; (2) for unbalanced distributions, Guidance Fusion Module (GFM) implements differential amplification of modal discriminative disparities to reinforce dominant modality advantages, guide auxiliary modalities to mine complementary discriminative info, and mitigate inter-modal divergence to boost multi-modal joint decision performance. Extensive experiments on three multi-modal ReID benchmarks (WMVeID863, MSVR310, RGBNT100) validate the effectiveness of our method. Code will be released upon acceptance.

</details>


### [40] [PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding](https://arxiv.org/abs/2601.02927)
*Iñaki Erregue,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: PrismVAU是一种轻量级实时视频异常理解系统，仅用一个现成的多模态大语言模型（MLLM），通过文本锚点相似性评分与自动提示优化，实现无需精细标注和外部模块的高效异常检测与可解释推理。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常理解方法依赖微调的多模态大语言模型或外部模块（如视频描述器），导致标注成本高、训练复杂、推理开销大，难以实用化。

Method: 提出两阶段框架：1）基于文本锚点相似性的粗粒度帧级异常评分；2）利用MLLM进行上下文化异常解释与提示优化；全程采用弱监督自动提示工程（APE）优化锚点和提示。

Result: 在标准VAD基准上达到有竞争力的检测性能，同时提供可解释的异常描述，且无需指令微调、帧级标注、外部模块或密集计算。

Conclusion: PrismVAU是一种高效、实用、无需额外标注与复杂组件的实时视频异常理解方案，兼顾性能与部署可行性。

Abstract: Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.

</details>


### [41] [HybridSolarNet: A Lightweight and Explainable EfficientNet-CBAM Architecture for Real-Time Solar Panel Fault Detection](https://arxiv.org/abs/2601.02928)
*Md. Asif Hossain,G M Mota-Tahrin Tayef,Nabil Subhan*

Main category: cs.CV

TL;DR: 本文提出了一种轻量高效、适用于无人机边缘计算的太阳能板故障检测模型HybridSolarNet，结合EfficientNet-B0与CBAM模块，并采用焦点损失和余弦退火策略，在Kaggle数据集上实现92.37%准确率、54.9 FPS推理速度及优异可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统人工巡检太阳能板费时费力且易出错；现有深度学习方法要么模型过大不适用于边缘设备，要么因训练策略不当导致准确率评估偏差。

Method: 提出HybridSolarNet：融合EfficientNet-B0主干与CBAM注意力机制；采用split-before-augmentation防止数据泄露；引入focal loss处理类别不平衡，cosine annealing优化训练；使用Grad-CAM进行可视化分析。

Result: 在Kaggle Solar Panel Images数据集上，5折分层交叉验证平均准确率达92.37%±0.41%，F1-score为0.9226±0.39；模型仅16.3MB（比VGG19小32倍），GPU推理达54.9 FPS；Grad-CAM显示模型关注真实故障区域。

Conclusion: HybridSolarNet兼顾高精度、轻量化与实时性，显著优于VGG19等基线模型，是面向无人机边缘部署的太阳能板智能巡检的有效解决方案。

Abstract: Manual inspections for solar panel systems are a tedious, costly, and error-prone task, making it desirable for Unmanned Aerial Vehicle (UAV) based monitoring. Though deep learning models have excellent fault detection capabilities, almost all methods either are too large and heavy for edge computing devices or involve biased estimation of accuracy due to ineffective learning techniques. We propose a new solar panel fault detection model called HybridSolarNet. It integrates EfficientNet-B0 with Convolutional Block Attention Module (CBAM). We implemented it on the Kaggle Solar Panel Images competition dataset with a tight split-before-augmentation protocol. It avoids leakage in accuracy estimation. We introduced focal loss and cosine annealing. Ablation analysis validates that accuracy boosts due to added benefits from CBAM (+1.53%) and that there are benefits from recognition of classes with imbalanced samples via focal loss. Overall average accuracy on 5-fold stratified cross-validation experiments on the given competition dataset topped 92.37% +/- 0.41 and an F1-score of 0.9226 +/- 0.39 compared to baselines like VGG19, requiring merely 16.3 MB storage, i.e., 32 times less. Its inference speed measured at 54.9 FPS with GPU support makes it a successful candidate for real-time UAV implementation. Moreover, visualization obtained from Grad-CAM illustrates that HybridSolarNet focuses on actual locations instead of irrelevant ones.

</details>


### [42] [VTONQA: A Multi-Dimensional Quality Assessment Dataset for Virtual Try-on](https://arxiv.org/abs/2601.02945)
*Xinyi Wei,Sijing Wu,Zitong Xu,Yunhao Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了首个专为虚拟试衣（VTON）图像质量评估设计的多维数据集VTONQA，包含8132张由11种主流VTON模型生成的图像及24396条人工评分，涵盖服装贴合度、身体兼容性和整体质量三个维度，并基于该数据集对现有VTON模型和图像质量评估指标进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有VTON模型常存在服装扭曲、身体不一致等伪影，缺乏可靠的质量评估方法，亟需面向VTON任务的专用质量评估数据集与基准。

Method: 构建了VTONQA数据集，涵盖11种VTON模型生成的8132张图像及对应24396条三维度人工评分（MOS），并在此基础上对VTON模型与多种IQA指标进行系统性基准评测。

Result: 揭示了当前VTON模型与通用IQA指标在VTON质量评估任务上的局限性，验证了VTONQA在推动感知对齐评估方面的价值。

Conclusion: VTONQA是首个面向虚拟试衣的多维质量评估数据集，为VTON质量评估方法研究与VTON模型优化提供了坚实基础。

Abstract: With the rapid development of e-commerce and digital fashion, image-based virtual try-on (VTON) has attracted increasing attention. However, existing VTON models often suffer from artifacts such as garment distortion and body inconsistency, highlighting the need for reliable quality evaluation of VTON-generated images. To this end, we construct VTONQA, the first multi-dimensional quality assessment dataset specifically designed for VTON, which contains 8,132 images generated by 11 representative VTON models, along with 24,396 mean opinion scores (MOSs) across three evaluation dimensions (i.e., clothing fit, body compatibility, and overall quality). Based on VTONQA, we benchmark both VTON models and a diverse set of image quality assessment (IQA) metrics, revealing the limitations of existing methods and highlighting the value of the proposed dataset. We believe that the VTONQA dataset and corresponding benchmarks will provide a solid foundation for perceptually aligned evaluation, benefiting both the development of quality assessment methods and the advancement of VTON models.

</details>


### [43] [LAMS-Edit: Latent and Attention Mixing with Schedulers for Improved Content Preservation in Diffusion-Based Image and Style Editing](https://arxiv.org/abs/2601.02987)
*Wingwa Fu,Takayuki Okatani*

Main category: cs.CV

TL;DR: 本文提出LAMS-Edit方法，利用扩散模型反演过程中的中间状态（潜在表示和注意力图），通过调度器控制的加权插值融合，结合Prompt-to-Prompt框架，实现高保真文本驱动图像编辑，支持区域掩码编辑与LoRA风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像编辑方法在内容保持与编辑效果之间难以平衡，且对真实图像编辑支持不足。

Method: 提出Latent and Attention Mixing with Schedulers（LAMS）技术，在反演与生成过程中对齐并混合潜在表示与注意力图；将其集成至Prompt-to-Prompt框架，扩展支持区域掩码编辑与LoRA风格迁移。

Result: 实验表明LAMS-Edit在保持原始图像内容的同时显著提升编辑准确性与可控性，优于现有基线方法。

Conclusion: LAMS-Edit是一种灵活、可扩展的扩散模型图像编辑框架，有效缓解内容破坏与编辑弱化之间的矛盾。

Abstract: Text-to-Image editing using diffusion models faces challenges in balancing content preservation with edit application and handling real-image editing. To address these, we propose LAMS-Edit, leveraging intermediate states from the inversion process--an essential step in real-image editing--during edited image generation. Specifically, latent representations and attention maps from both processes are combined at each step using weighted interpolation, controlled by a scheduler. This technique, Latent and Attention Mixing with Schedulers (LAMS), integrates with Prompt-to-Prompt (P2P) to form LAMS-Edit--an extensible framework that supports precise editing with region masks and enables style transfer via LoRA. Extensive experiments demonstrate that LAMS-Edit effectively balances content preservation and edit application.

</details>


### [44] [ULS+: Data-driven Model Adaptation Enhances Lesion Segmentation](https://arxiv.org/abs/2601.02988)
*Rianne Weber,Niels Rocholl,Max de Grauw,Mathias Prokop,Ewoud Smit,Alessa Hering*

Main category: cs.CV

TL;DR: ULS+ 是 ULS 模型的增强版本，通过引入新公开数据集和减小输入图像尺寸，在准确性和推理速度上均优于原模型，并在 ULS23 挑战赛中排名第一。


<details>
  <summary>Details</summary>
Motivation: 原始 ULS 模型虽能对 CT 扫描中的全身病灶进行分割，但随着多个新公开数据集的出现，存在进一步提升性能的空间。

Method: ULS+ 在 ULS 基础上整合了新增的公共数据集，并采用更小的输入图像尺寸进行训练与推理。

Result: ULS+ 在 Dice 分数和对点击点位置的鲁棒性方面均显著优于 ULS，并在 ULS23 Challenge 测试阶段排行榜上排名第一。

Conclusion: ULS+ 通过持续的数据驱动更新与临床验证，为构建鲁棒且临床实用的病灶分割模型奠定了基础。

Abstract: In this study, we present ULS+, an enhanced version of the Universal Lesion Segmentation (ULS) model. The original ULS model segments lesions across the whole body in CT scans given volumes of interest (VOIs) centered around a click-point. Since its release, several new public datasets have become available that can further improve model performance. ULS+ incorporates these additional datasets and uses smaller input image sizes, resulting in higher accuracy and faster inference.
  We compared ULS and ULS+ using the Dice score and robustness to click-point location on the ULS23 Challenge test data and a subset of the Longitudinal-CT dataset. In all comparisons, ULS+ significantly outperformed ULS. Additionally, ULS+ ranks first on the ULS23 Challenge test-phase leaderboard. By maintaining a cycle of data-driven updates and clinical validation, ULS+ establishes a foundation for robust and clinically relevant lesion segmentation models.

</details>


### [45] [Towards Faithful Reasoning in Comics for Small MLLMs](https://arxiv.org/abs/2601.02991)
*Chengcheng Feng,Haojie Yin,Yucheng Jin,Kaizhu Huang*

Main category: cs.CV

TL;DR: 本文提出了一种针对漫画视觉问答（CVQA）任务的新型推理框架，解决了标准思维链（CoT）在该任务中因状态纠缠、虚假转移和探索低效而导致性能下降的问题，尤其提升了小规模多模态大语言模型（MLLMs）的表现。


<details>
  <summary>Details</summary>
Motivation: 漫画视觉问答（CVQA）具有符号抽象、叙事逻辑和幽默等独特挑战，标准Chain-of-Thought（CoT） prompting 在此任务中反而常导致性能下降，尤其对小模型更明显。

Method: 提出一种新型漫画推理框架，结合模块化CoT生成、基于GRPO的强化微调及新型结构化奖励机制。

Result: 在五个CVQA及相关幽默/抽象视觉推理基准上，3B模型超越现有最优方法；插件实验使不同MLLM平均提升12.1%。

Conclusion: 所提框架能生成更忠实、可迁移的推理链，显著提升小规模MLLM在CVQA等抽象幽默视觉推理任务中的表现。

Abstract: Comic-based visual question answering (CVQA) poses distinct challenges to multimodal large language models (MLLMs) due to its reliance on symbolic abstraction, narrative logic, and humor, which differ from conventional VQA tasks. Although Chain-of-Thought (CoT) prompting is widely used to enhance MLLM reasoning, surprisingly, its direct application to CVQA often degrades performance, especially in small-scale models. Our theoretical and empirical analyses reveal that standard CoT in CVQA suffers from state entanglement, spurious transitions, and exploration inefficiency, with small models particularly vulnerable in resource-constrained settings. To address these issues, we propose a novel comic reasoning framework, designed to produce more faithful and transferable reasoning chains in small MLLMs. Specifically, our framework combines modular CoT generation with GRPO-based reinforcement fine-tuning and a novel structured reward. Beyond comic VQA, we further evaluate our approach on a broader class of humor-centric and abstract visual reasoning tasks, including meme understanding and editorial cartoon interpretation. Across five challenging benchmarks, our 3B model outperforms state-of-the-art methods, and plug-in experiments yield an additional average improvement of $\mathbf{12.1\%}$ across different MLLMs.

</details>


### [46] [Towards Efficient 3D Object Detection for Vehicle-Infrastructure Collaboration via Risk-Intent Selection](https://arxiv.org/abs/2601.03001)
*Li Wang,Boqi Li,Hang Chen,Xingjian Wu,Yichen Wang,Jiewen Tan,Xinyu Zhang,Huaping Liu*

Main category: cs.CV

TL;DR: 本文提出RiSe框架，通过风险-意图选择性检测，在车路协同感知中实现高精度与低带宽的平衡：利用势场-轨迹相关模型评估运动风险，结合意图驱动区域预测模块筛选关键BEV区域，仅传输高交互区域的高质量特征，通信量降至全特征共享的0.71%，同时保持SOTA检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有车路协同感知方法在中间融合阶段仍存在空间冗余问题，尤其非关键背景区域特征传输效率低，难以兼顾通信带宽与感知性能。

Method: 提出Risk-intent Selective detection（RiSe）框架，包含两个核心模块：1）基于势场理论的Potential Field-Trajectory Correlation Model（PTCM），定量评估动态风险；2）利用自车运动先验的Intention-Driven Area Prediction Module（IDAPM），主动预测并筛选关键BEV区域；二者联合实现语义选择性特征融合。

Result: 在DeepAccident数据集上，通信量仅为全特征共享的0.71%，同时检测精度达SOTA，构建了带宽效率与感知性能间的优越Pareto前沿。

Conclusion: RiSe通过从‘可见性优先’转向‘风险-意图优先’的范式转变，有效解决了VICP中特征冗余与带宽受限的核心矛盾，为高效车路协同感知提供了新思路。

Abstract: Vehicle-Infrastructure Collaborative Perception (VICP) is pivotal for resolving occlusion in autonomous driving, yet the trade-off between communication bandwidth and feature redundancy remains a critical bottleneck. While intermediate fusion mitigates data volume compared to raw sharing, existing frameworks typically rely on spatial compression or static confidence maps, which inefficiently transmit spatially redundant features from non-critical background regions. To address this, we propose Risk-intent Selective detection (RiSe), an interaction-aware framework that shifts the paradigm from identifying visible regions to prioritizing risk-critical ones. Specifically, we introduce a Potential Field-Trajectory Correlation Model (PTCM) grounded in potential field theory to quantitatively assess kinematic risks. Complementing this, an Intention-Driven Area Prediction Module (IDAPM) leverages ego-motion priors to proactively predict and filter key Bird's-Eye-View (BEV) areas essential for decision-making. By integrating these components, RiSe implements a semantic-selective fusion scheme that transmits high-fidelity features only from high-interaction regions, effectively acting as a feature denoiser. Extensive experiments on the DeepAccident dataset demonstrate that our method reduces communication volume to 0.71\% of full feature sharing while maintaining state-of-the-art detection accuracy, establishing a competitive Pareto frontier between bandwidth efficiency and perception performance.

</details>


### [47] [SA-ResGS: Self-Augmented Residual 3D Gaussian Splatting for Next Best View Selection](https://arxiv.org/abs/2601.03024)
*Kim Jun-Seong,Tae-Hyun Oh,Eduardo Pérez-Pellitero,Youngkyoon Jang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Self-Augmented Residual 3D Gaussian Splatting（SA-ResGS）的新框架，用于主动场景重建中的不确定性量化与下一次最佳视角（NBV）选择，通过自增强点云生成和残差学习策略提升不确定性估计的稳定性与监督有效性。


<details>
  <summary>Details</summary>
Motivation: 解决主动场景重建中因稀疏、大基线视角导致的高斯分布监督不足、不确定性估计不可靠及NBV规划中探索与模糊性冲突等问题。

Method: 提出SA-ResGS框架：1）通过训练视角与外推渲染视角三角化生成Self-Augmented点云（SA-Points）以估计场景覆盖；2）引入首个面向3D高斯泼溅的残差学习策略，结合不确定性过滤与类dropout/难负样本挖掘采样，增强高不确定性高斯的梯度流。

Result: 在主动视角选择实验中，SA-ResGS在重建质量与视角选择鲁棒性上均优于当前最优方法。

Conclusion: SA-ResGS通过物理引导的视角选择、不确定性感知的残差监督及隐式不确定性去偏机制，显著提升了3D高斯泼溅在主动重建中的训练稳定性与不确定性建模能力。

Abstract: We propose Self-Augmented Residual 3D Gaussian Splatting (SA-ResGS), a novel framework to stabilize uncertainty quantification and enhancing uncertainty-aware supervision in next-best-view (NBV) selection for active scene reconstruction. SA-ResGS improves both the reliability of uncertainty estimates and their effectiveness for supervision by generating Self-Augmented point clouds (SA-Points) via triangulation between a training view and a rasterized extrapolated view, enabling efficient scene coverage estimation. While improving scene coverage through physically guided view selection, SA-ResGS also addresses the challenge of under-supervised Gaussians, exacerbated by sparse and wide-baseline views, by introducing the first residual learning strategy tailored for 3D Gaussian Splatting. This targeted supervision enhances gradient flow in high-uncertainty Gaussians by combining uncertainty-driven filtering with dropout- and hard-negative-mining-inspired sampling. Our contributions are threefold: (1) a physically grounded view selection strategy that promotes efficient and uniform scene coverage; (2) an uncertainty-aware residual supervision scheme that amplifies learning signals for weakly contributing Gaussians, improving training stability and uncertainty estimation across scenes with diverse camera distributions; (3) an implicit unbiasing of uncertainty quantification as a consequence of constrained view selection and residual supervision, which together mitigate conflicting effects of wide-baseline exploration and sparse-view ambiguity in NBV planning. Experiments on active view selection demonstrate that SA-ResGS outperforms state-of-the-art baselines in both reconstruction quality and view selection robustness.

</details>


### [48] [Flow Matching and Diffusion Models via PointNet for Generating Fluid Fields on Irregular Geometries](https://arxiv.org/abs/2601.03030)
*Ali Kashefi*

Main category: cs.CV

TL;DR: 本文提出了两种基于PointNet的生成式几何深度学习框架（Flow Matching PointNet和Diffusion PointNet），用于在不规则几何体上直接预测流体流动变量，避免了传统像素化方法的限制，并在精度与鲁棒性上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在不规则几何上建模流场时受限于像素化投影或图神经网络引入的高频噪声，且几何条件依赖复杂辅助网络，亟需更简洁、鲁棒、几何本征的生成建模方法。

Method: 将PointNet嵌入流匹配（Flow Matching）和扩散模型（Diffusion Models）的反向生成过程，以点云表示的计算域（如有限体积网格顶点）为输入，直接从高斯噪声重建物理场；无需图结构或额外几何编码网络。

Result: 在圆柱绕流数据集（变化截面形状与朝向）上验证，两种框架在速度/压力场及升阻力预测上均优于同等参数量的普通PointNet，且对不完整几何更具鲁棒性，同时避免了图扩散模型的高频噪声问题。

Conclusion: PointNet与流匹配/扩散模型的结合提供了一种简洁、统一、几何本征的生成式流场建模范式，在精度、鲁棒性和架构简洁性方面具有显著优势。

Abstract: We present two novel generative geometric deep learning frameworks, termed Flow Matching PointNet and Diffusion PointNet, for predicting fluid flow variables on irregular geometries by incorporating PointNet into flow matching and diffusion models, respectively. In these frameworks, a reverse generative process reconstructs physical fields from standard Gaussian noise conditioned on unseen geometries. The proposed approaches operate directly on point-cloud representations of computational domains (e.g., grid vertices of finite-volume meshes) and therefore avoid the limitations of pixelation used to project geometries onto uniform lattices. In contrast to graph neural network-based diffusion models, Flow Matching PointNet and Diffusion PointNet do not exhibit high-frequency noise artifacts in the predicted fields. Moreover, unlike such approaches, which require auxiliary intermediate networks to condition geometry, the proposed frameworks rely solely on PointNet, resulting in a simple and unified architecture. The performance of the proposed frameworks is evaluated on steady incompressible flow past a cylinder, using a geometric dataset constructed by varying the cylinder's cross-sectional shape and orientation across samples. The results demonstrate that Flow Matching PointNet and Diffusion PointNet achieve more accurate predictions of velocity and pressure fields, as well as lift and drag forces, and exhibit greater robustness to incomplete geometries compared to a vanilla PointNet with the same number of trainable parameters.

</details>


### [49] [Motion Blur Robust Wheat Pest Damage Detection with Dynamic Fuzzy Feature Fusion](https://arxiv.org/abs/2601.03046)
*Han Zhang,Yanwei Wang,Fang Li,Hongjun Wang*

Main category: cs.CV

TL;DR: 本文提出DFRCP（动态模糊鲁棒卷积金字塔），作为YOLOv11的即插即用模块，通过融合多尺度特征、引入动态鲁棒开关单元及模糊特征合成机制，在不显著增加延迟的前提下提升运动模糊图像中的目标检测鲁棒性，并在小麦害虫损伤数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 运动模糊导致边缘失真和鬼影伪影，严重损害目标检测性能；现有方法要么将模糊视为噪声而丢失结构信息，要么进行全图去模糊导致高延迟、难部署于边缘设备。

Method: 提出DFRCP模块：1）增强YOLOv11特征金字塔，融合大/中尺度特征并保留原生表征；2）设计动态鲁棒开关单元，自适应注入经旋转与非线性插值生成的模糊特征以增强全局感知；3）采用透明卷积融合原始与模糊线索；4）开发CUDA并行旋转插值核，避免边界溢出且提速超400倍。

Result: 在模糊测试集上，集成DFRCP的YOLOv11比基线准确率提升约10.4%，训练开销 modest，显著减少采集后人工筛选需求。

Conclusion: DFRCP是一种高效、轻量、可部署于边缘设备的模糊鲁棒检测增强方案，兼顾精度提升与实时性，在农业害虫检测等实际场景中具有实用价值。

Abstract: Motion blur caused by camera shake produces ghosting artifacts that substantially degrade edge side object detection. Existing approaches either suppress blur as noise and lose discriminative structure, or apply full image restoration that increases latency and limits deployment on resource constrained devices. We propose DFRCP, a Dynamic Fuzzy Robust Convolutional Pyramid, as a plug in upgrade to YOLOv11 for blur robust detection. DFRCP enhances the YOLOv11 feature pyramid by combining large scale and medium scale features while preserving native representations, and by introducing Dynamic Robust Switch units that adaptively inject fuzzy features to strengthen global perception under jitter. Fuzzy features are synthesized by rotating and nonlinearly interpolating multiscale features, then merged through a transparency convolution that learns a content adaptive trade off between original and fuzzy cues. We further develop a CUDA parallel rotation and interpolation kernel that avoids boundary overflow and delivers more than 400 times speedup, making the design practical for edge deployment. We train with paired supervision on a private wheat pest damage dataset of about 3,500 images, augmented threefold using two blur regimes, uniform image wide motion blur and bounding box confined rotational blur. On blurred test sets, YOLOv11 with DFRCP achieves about 10.4 percent higher accuracy than the YOLOv11 baseline with only a modest training time overhead, reducing the need for manual filtering after data collection.

</details>


### [50] [On the Intrinsic Limits of Transformer Image Embeddings in Non-Solvable Spatial Reasoning](https://arxiv.org/abs/2601.03048)
*Siyi Lyu,Quan Liu,Feng Yan*

Main category: cs.CV

TL;DR: 本文指出ViT在空间推理任务（如心理旋转）上的失败源于其内在电路复杂度限制，而非数据规模；通过将空间理解形式化为群同态学习，并证明非可解群（如SO(3)）的结构保持嵌入存在NC¹-完全下界，而常数深度ViT仅能实现TC⁰能力，从而在复杂性上无法胜任此类任务。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers（ViTs）虽在语义识别上表现优异，但在空间推理任务（如心理旋转）中系统性失败；作者认为该问题根源不在数据量，而在于ViT架构固有的电路复杂度限制。

Method: 将空间理解形式化为学习图像序列到隐空间的群同态映射，以保持底层变换群（如SO(3)）的代数结构；理论分析该映射的计算复杂度下界（Word Problem, NC¹-完全），并与ViT的电路复杂度类（TC⁰）进行比较；辅以隐空间探针实验验证理论预测。

Result: 证明常数深度ViT在多项式精度下被TC⁰严格限制；在NC¹ ⊈ TC⁰假设下，确立其无法高效建模非可解群对应的空间结构；实验证实ViT隐表示在非可解任务上随层数增加出现结构性坍塌。

Conclusion: ViT的空间推理缺陷是其计算复杂度本质限制所致，需超越TC⁰能力的新架构或机制（如更深电路、显式群归纳偏置）来突破该边界。

Abstract: Vision Transformers (ViTs) excel in semantic recognition but exhibit systematic failures in spatial reasoning tasks such as mental rotation. While often attributed to data scale, we propose that this limitation arises from the intrinsic circuit complexity of the architecture. We formalize spatial understanding as learning a Group Homomorphism: mapping image sequences to a latent space that preserves the algebraic structure of the underlying transformation group. We demonstrate that for non-solvable groups (e.g., the 3D rotation group $\mathrm{SO}(3)$), maintaining such a structure-preserving embedding is computationally lower-bounded by the Word Problem, which is $\mathsf{NC^1}$-complete. In contrast, we prove that constant-depth ViTs with polynomial precision are strictly bounded by $\mathsf{TC^0}$. Under the conjecture $\mathsf{TC^0} \subsetneq \mathsf{NC^1}$, we establish a complexity boundary: constant-depth ViTs fundamentally lack the logical depth to efficiently capture non-solvable spatial structures. We validate this complexity gap via latent-space probing, demonstrating that ViT representations suffer a structural collapse on non-solvable tasks as compositional depth increases.

</details>


### [51] [IBISAgent: Reinforcing Pixel-Level Visual Reasoning in MLLMs for Universal Biomedical Object Referring and Segmentation](https://arxiv.org/abs/2601.03054)
*Yankai Jiang,Qiaoru Li,Binlu Xu,Haoran Sun,Chao Ding,Junting Dong,Yuxiang Cai,Xuhong Zhang,Jianwei Yin*

Main category: cs.CV

TL;DR: 本文提出了一种名为IBISAgent的新型代理式医学多模态大语言模型（MLLM），将图像分割建模为视觉为中心的多步决策过程，通过文本点击指令调用分割工具、迭代推理与掩码优化，无需修改模型结构，在医学像素级分割任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学MLLM在像素级分割中存在两大问题：一是需联合微调MLLM与外部解码器，易导致灾难性遗忘且泛化性差；二是依赖单次推理，缺乏迭代优化能力，影响分割精度。

Method: 提出IBISAgent，将分割重构为多步视觉决策过程，支持生成文本点击动作、调用分割工具、基于掩码图像特征进行多步视觉推理；设计两阶段训练框架：冷启动监督微调 + 面向细粒度任务的代理式强化学习。

Result: 在多个医学 referring segmentation 任务上显著超越现有闭源与开源SOTA方法，具备更强的鲁棒性与泛化能力。

Conclusion: IBISAgent验证了将分割作为代理式多步视觉推理任务的有效性，为医学MLLM发展像素级视觉推理能力提供了新范式，且所有数据、代码与模型将开源。

Abstract: Recent research on medical MLLMs has gradually shifted its focus from image-level understanding to fine-grained, pixel-level comprehension. Although segmentation serves as the foundation for pixel-level understanding, existing approaches face two major challenges. First, they introduce implicit segmentation tokens and require simultaneous fine-tuning of both the MLLM and external pixel decoders, which increases the risk of catastrophic forgetting and limits generalization to out-of-domain scenarios. Second, most methods rely on single-pass reasoning and lack the capability to iteratively refine segmentation results, leading to suboptimal performance. To overcome these limitations, we propose a novel agentic MLLM, named IBISAgent, that reformulates segmentation as a vision-centric, multi-step decision-making process. IBISAgent enables MLLMs to generate interleaved reasoning and text-based click actions, invoke segmentation tools, and produce high-quality masks without architectural modifications. By iteratively performing multi-step visual reasoning on masked image features, IBISAgent naturally supports mask refinement and promotes the development of pixel-level visual reasoning capabilities. We further design a two-stage training framework consisting of cold-start supervised fine-tuning and agentic reinforcement learning with tailored, fine-grained rewards, enhancing the model's robustness in complex medical referring and reasoning segmentation tasks. Extensive experiments demonstrate that IBISAgent consistently outperforms both closed-source and open-source SOTA methods. All datasets, code, and trained models will be released publicly.

</details>


### [52] [Fine-Grained Generalization via Structuralizing Concept and Feature Space into Commonality, Specificity and Confounding](https://arxiv.org/abs/2601.03056)
*Zhen Wang,Jiaojiao Zhao,Qilong Wang,Yongfeng Dong,Wenlong Yu*

Main category: cs.CV

TL;DR: 本文提出Concept-Feature Structuralized Generalization (CFSG)方法，通过结构化解耦概念与特征空间为共性、特性和混淆三部分，并引入自适应机制动态调整各部分比例，显著提升细粒度领域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 细粒度领域泛化（FGDG）因类间差异细微、类内变化显著而更具挑战；现有模型未有效模拟人类利用共性与特性属性进行分类的认知机制。

Method: 提出CFSG模型，显式地将概念和特征空间解耦为共性、特性和混淆三个结构化部分，并设计自适应机制动态调节三者比例，在预测时对各组件对赋予显式权重。

Result: 在三个单源基准数据集上，CFSG平均比基线模型提升9.87%，超越现有SOTA方法平均3.08%；可解释性分析证实其能融合多粒度结构化知识，且特征结构化促进概念结构化。

Conclusion: CFSG通过模仿人类认知机制实现更鲁棒的细粒度领域泛化，验证了结构化解耦与自适应融合在FGDG中的有效性。

Abstract: Fine-Grained Domain Generalization (FGDG) presents greater challenges than conventional domain generalization due to the subtle inter-class differences and relatively pronounced intra-class variations inherent in fine-grained recognition tasks. Under domain shifts, the model becomes overly sensitive to fine-grained cues, leading to the suppression of critical features and a significant drop in performance. Cognitive studies suggest that humans classify objects by leveraging both common and specific attributes, enabling accurate differentiation between fine-grained categories. However, current deep learning models have yet to incorporate this mechanism effectively. Inspired by this mechanism, we propose Concept-Feature Structuralized Generalization (CFSG). This model explicitly disentangles both the concept and feature spaces into three structured components: common, specific, and confounding segments. To mitigate the adverse effects of varying degrees of distribution shift, we introduce an adaptive mechanism that dynamically adjusts the proportions of common, specific, and confounding components. In the final prediction, explicit weights are assigned to each pair of components. Extensive experiments on three single-source benchmark datasets demonstrate that CFSG achieves an average performance improvement of 9.87% over baseline models and outperforms existing state-of-the-art methods by an average of 3.08%. Additionally, explainability analysis validates that CFSG effectively integrates multi-granularity structured knowledge and confirms that feature structuralization facilitates the emergence of concept structuralization.

</details>


### [53] [Understanding Multi-Agent Reasoning with Large Language Models for Cartoon VQA](https://arxiv.org/abs/2601.03073)
*Tong Wu,Thanet Markchom*

Main category: cs.CV

TL;DR: 本文提出了一种面向卡通图像的多智能体大语言模型框架，通过视觉、语言和批评三个专用智能体协同工作，提升视觉问答（VQA）性能，并在Pororo和Simpsons数据集上进行了系统评估。


<details>
  <summary>Details</summary>
Motivation: 标准大语言模型在处理风格化卡通图像时面临挑战，如夸张的视觉抽象和叙事驱动的上下文理解不足。

Method: 设计了一个由视觉智能体、语言智能体和批评智能体组成的多智能体LLM框架，协同整合视觉线索与叙事上下文以支持结构化推理。

Result: 在Pororo和Simpsons两个卡通VQA数据集上进行了系统评估，详细分析了各智能体对最终预测的贡献。

Conclusion: 该多智能体框架有效提升了卡通图像VQA任务的表现，并加深了对LLM在卡通VQA及多模态推理中行为的理解。

Abstract: Visual Question Answering (VQA) for stylised cartoon imagery presents challenges, such as interpreting exaggerated visual abstraction and narrative-driven context, which are not adequately addressed by standard large language models (LLMs) trained on natural images. To investigate this issue, a multi-agent LLM framework is introduced, specifically designed for VQA tasks in cartoon imagery. The proposed architecture consists of three specialised agents: visual agent, language agent and critic agent, which work collaboratively to support structured reasoning by integrating visual cues and narrative context. The framework was systematically evaluated on two cartoon-based VQA datasets: Pororo and Simpsons. Experimental results provide a detailed analysis of how each agent contributes to the final prediction, offering a deeper understanding of LLM-based multi-agent behaviour in cartoon VQA and multimodal inference.

</details>


### [54] [LesionTABE: Equitable AI for Skin Lesion Detection](https://arxiv.org/abs/2601.03090)
*Rocio Mexia Diaz,Yasmin Greenway,Petru Manescu*

Main category: cs.CV

TL;DR: 本文提出了LesionTABE框架，通过结合对抗去偏和皮肤科基础模型嵌入，显著提升了AI在深色皮肤人群中的诊断公平性，同时提高了整体诊断准确率。


<details>
  <summary>Details</summary>
Motivation: AI在皮肤科临床应用中面临的主要障碍是偏见问题，尤其是在深色皮肤人群中的诊断性能较差。

Method: 提出LesionTABE框架，将对抗去偏技术与皮肤科专用基础模型嵌入相结合，并在多个涵盖良恶性及炎症性皮肤病的数据集上进行评估。

Result: 相较于ResNet-152基线模型，LesionTABE在公平性指标上提升超过25%，且优于现有去偏方法，同时提升了整体诊断准确率。

Conclusion: 基础模型去偏是推动公平临床AI落地的重要方向。

Abstract: Bias remains a major barrier to the clinical adoption of AI in dermatology, as diagnostic models underperform on darker skin tones. We present LesionTABE, a fairness-centric framework that couples adversarial debiasing with dermatology-specific foundation model embeddings. Evaluated across multiple datasets covering both malignant and inflammatory conditions, LesionTABE achieves over a 25\% improvement in fairness metrics compared to a ResNet-152 baseline, outperforming existing debiasing methods while simultaneously enhancing overall diagnostic accuracy. These results highlight the potential of foundation model debiasing as a step towards equitable clinical AI adoption.

</details>


### [55] [Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs](https://arxiv.org/abs/2601.03100)
*Chenchen Lin,Sanbao Su,Rachel Luo,Yuxiao Chen,Yan Wang,Marco Pavone,Fei Miao*

Main category: cs.CV

TL;DR: 本文提出TGIF（Text-Guided Inter-layer Fusion）模块，通过文本引导的、查询依赖的视觉编码器多层特征融合，增强多模态大模型（MLLMs）的视觉接地能力，减少幻觉，且无需更新视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs仅使用冻结视觉编码器的单一层特征，未能充分利用其丰富的层次化视觉线索，导致视觉不 grounded 的幻觉；已有方法多在文本侧缓解，或采用静态多层融合，缺乏对查询的适应性。

Method: 提出轻量级TGIF模块，将视觉编码器各层视为深度方向的'专家'，根据文本提示动态预测各层特征的融合权重，实现直接外部融合，不修改视觉编码器。

Result: 在LLaVA-1.5-7B上集成TGIF后，在幻觉、OCR和VQA基准上均取得一致提升，同时在ScienceQA、GQA和MMBench上保持或提升性能。

Conclusion: 查询条件驱动、层次感知的视觉特征融合是增强现代MLLMs视觉接地与抑制幻觉的有效途径。

Abstract: Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder's rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise "experts" and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.

</details>


### [56] [LeafLife: An Explainable Deep Learning Framework with Robustness for Grape Leaf Disease Recognition](https://arxiv.org/abs/2601.03124)
*B. M. Shahria Alam,Md. Nasim Ahmed*

Main category: cs.CV

TL;DR: 本文提出了一种基于Xception模型的葡萄叶病害分类方法，结合对抗训练和Grad-CAM可视化提升鲁棒性与可解释性，并通过Streamlit部署了带热力图和置信度的Web应用。


<details>
  <summary>Details</summary>
Motivation: 葡萄叶病害检测对提高农作物产量和农业生产力至关重要，而现有方法在准确率、鲁棒性和可解释性方面仍有提升空间。

Method: 采用Xception和InceptionV3两个预训练模型，在经预处理和划分（70%训练、20%验证、10%测试）的9032张四类葡萄叶图像数据集上进行训练；引入对抗训练增强鲁棒性，使用Grad-CAM进行病灶区域可视化解释；最终用Streamlit构建交互式Web应用。

Result: Xception模型达到96.23%的准确率，优于InceptionV3；系统支持热力图可视化和带置信度的预测结果。

Conclusion: Xception结合对抗训练与Grad-CAM是一种高效、鲁棒且可解释的葡萄叶病害诊断方案，具备实际农业应用潜力。

Abstract: Plant disease diagnosis is essential to farmers' management choices because plant diseases frequently lower crop yield and product quality. For harvests to flourish and agricultural productivity to boost, grape leaf disease detection is important. The plant disease dataset contains grape leaf diseases total of 9,032 images of four classes, among them three classes are leaf diseases, and the other one is healthy leaves. After rigorous pre-processing dataset was split (70% training, 20% validation, 10% testing), and two pre-trained models were deployed: InceptionV3 and Xception. Xception shows a promising result of 96.23% accuracy, which is remarkable than InceptionV3. Adversarial Training is used for robustness, along with more transparency. Grad-CAM is integrated to confirm the leaf disease. Finally deployed a web application using Streamlit with a heatmap visualization and prediction with confidence level for robust grape leaf disease classification.

</details>


### [57] [Unified Thinker: A General Reasoning Modular Core for Image Generation](https://arxiv.org/abs/2601.03127)
*Sashuai Zhou,Qiang Zhou,Jijin Hu,Hanqing Yang,Yue Cao,Junpeng Ma,Yinchao Ma,Jun Song,Tiezheng Ge,Cheng Yu,Bo Zheng,Zhou Zhao*

Main category: cs.CV

TL;DR: 本文提出Unified Thinker，一种任务无关的推理架构，通过将推理（Thinker）与生成（Generator）解耦，并引入两阶段训练范式，显著提升图像生成中的逻辑推理能力与质量。


<details>
  <summary>Details</summary>
Motivation: 当前开源生成模型在逻辑密集型指令遵循方面表现不足，存在推理-执行鸿沟；而闭源系统（如Nano Banana）展现出更强的推理驱动生成能力，凸显了性能差距。

Method: 提出Unified Thinker架构，解耦专用Thinker与图像Generator；构建结构化规划接口，并采用强化学习结合像素级反馈进行训练，使规划策略聚焦于视觉正确性而非仅文本合理性。

Result: 在文本到图像生成和图像编辑任务上，Unified Thinker显著提升了图像推理能力与生成质量。

Conclusion: 闭合推理-执行鸿沟的关键在于可执行的推理——即生成可验证、可落地的规划；Unified Thinker作为通用规划核心，支持模块化升级，为开放生成模型提供了新范式。

Abstract: Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.

</details>


### [58] [LSP-DETR: Efficient and Scalable Nuclei Segmentation in Whole Slide Images](https://arxiv.org/abs/2601.03163)
*Matěj Pekár,Vít Musil,Rudolf Nenutil,Petr Holub,Tomáš Brázdil*

Main category: cs.CV

TL;DR: LSP-DETR是一种端到端的轻量级Transformer模型，用星形凸多边形表示细胞核，并引入径向距离损失函数，无需重叠标注或后处理即可高效、精准地完成大规模全切片图像中的细胞核实例分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖补丁处理和高成本后处理，牺牲上下文信息与效率，难以应对千兆像素级全切片图像的计算挑战。

Method: 提出LSP-DETR框架，采用线性复杂度的轻量Transformer；将细胞核建模为星形凸多边形，并设计新型径向距离损失函数，使重叠核分割自然涌现。

Result: 在PanNuke和MoNuSeg数据集上展现出跨组织泛化能力与最先进效率，速度比次快方法快五倍以上。

Conclusion: LSP-DETR实现了高精度、高效率、端到端的细胞核实例分割，显著提升了计算病理学中大规模图像分析的可行性与可扩展性。

Abstract: Precise and scalable instance segmentation of cell nuclei is essential for computational pathology, yet gigapixel Whole-Slide Images pose major computational challenges. Existing approaches rely on patch-based processing and costly post-processing for instance separation, sacrificing context and efficiency. We introduce LSP-DETR (Local Star Polygon DEtection TRansformer), a fully end-to-end framework that uses a lightweight transformer with linear complexity to process substantially larger images without additional computational cost. Nuclei are represented as star-convex polygons, and a novel radial distance loss function allows the segmentation of overlapping nuclei to emerge naturally, without requiring explicit overlap annotations or handcrafted post-processing. Evaluations on PanNuke and MoNuSeg show strong generalization across tissues and state-of-the-art efficiency, with LSP-DETR being over five times faster than the next-fastest leading method. Code and models are available at https://github.com/RationAI/lsp-detr.

</details>


### [59] [DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation](https://arxiv.org/abs/2601.03178)
*Jiajun jiao,Haowei Zhu,Puyuan Yang,Jianghui Wang,Ji Liu,Ziqiong Liu,Dong Li,Yuejian Fang,Junhai Yong,Bin Wang,Emad Barsoum*

Main category: cs.CV

TL;DR: 本文提出了一种基于大语言模型（LLM）的自动化扩散模型加速框架，包括评估基准DiffBench和智能体DiffAgent，显著提升了加速策略生成的有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型推理步骤多、计算开销大，难以实际部署；而多种加速技术的组合优化缺乏系统方法。

Method: 构建了涵盖多架构、多优化组合与多部署场景的自动化评估基准DiffBench；设计了具备规划、代码生成、调试与遗传反馈闭环机制的智能体DiffAgent。

Result: DiffBench能全面评估生成代码性能；DiffAgent在生成有效加速策略方面显著优于现有LLM方法。

Conclusion: 基于LLM的自动化框架可高效协同多种加速技术，为扩散模型的实际部署提供新范式。

Abstract: Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion models is therefore essential, yet determining how to combine multiple model acceleration techniques remains a significant challenge. To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation. First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios. Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models. DiffAgent employs a closed-loop workflow in which a planning component and a debugging component iteratively refine the output of a code generation component, while a genetic algorithm extracts performance feedback from the execution environment to guide subsequent code refinements. We provide a detailed explanation of the DiffBench construction and the design principles underlying DiffAgent. Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.

</details>


### [60] [AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation](https://arxiv.org/abs/2601.03191)
*Anees Ur Rehman Hashmi,Numan Saeed,Christoph Lippert*

Main category: cs.CV

TL;DR: 本文提出了AnatomiX，一种专为解剖学引导的胸部X光片解释而设计的多任务多模态大语言模型，通过两阶段方法显著提升了空间推理和解剖理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态医学大语言模型在胸部X光解读中仍存在空间推理和解剖理解不足的问题，尤其在建立真实解剖对应关系方面表现不佳。

Method: 提出AnatomiX模型，采用受放射科工作流程启发的两阶段方法：先识别解剖结构并提取其特征，再利用大语言模型完成短语定位、报告生成、视觉问答和图像理解等下游任务。

Result: 在多个基准测试中，AnatomiX在解剖推理方面表现优异，在解剖定位、短语定位、定位诊断和定位描述等任务上相比现有方法提升超25%。

Conclusion: AnatomiX有效弥补了当前模型在解剖学一致性上的缺陷，为可解释、可靠的医学AI提供了新范式。

Abstract: Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at https://github.com/aneesurhashmi/anatomix

</details>


### [61] [UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision](https://arxiv.org/abs/2601.03193)
*Ruiyan Han,Zhen Fang,XinYu Sun,Yuchen Ma,Ziheng Wang,Yu Zeng,Zehui Chen,Lin Chen,Wenxuan Huang,Wei-Jie Xu,Yi Cao,Feng Zhao*

Main category: cs.CV

TL;DR: 本文提出UniCorn框架，通过自提升方式解决统一多模态模型在跨模态理解强但生成质量弱的问题（称为传导性失语症），无需外部数据或教师监督，在多个图像生成基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型（UMMs）虽在跨模态理解上表现优异，但在将内部知识转化为高质量、可控生成方面存在明显短板，作者将此现象定义为'传导性失语症'。

Method: 提出UniCorn自改进框架：将单个UMM划分为Proposer、Solver和Judge三个协作角色，通过自博弈生成高质量交互，并利用认知模式重构将隐含理解蒸馏为显式生成信号；同时构建UniCycle循环一致性评测基准（Text→Image→Text）验证多模态一致性恢复效果。

Result: 在六个通用图像生成基准上全面显著超越基线模型，尤其在TIIF(73.8)、DPG(86.8)、CompBench(88.5)和UniCycle上达SOTA，并在WISE(+5.0)与OneIG(+6.5)上取得大幅增益。

Conclusion: UniCorn证明了完全自监督优化可有效提升统一多模态模型的生成能力，同时保持强理解力，为统一多模态智能的可扩展自优化提供了新范式。

Abstract: While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.

</details>


### [62] [LTX-2: Efficient Joint Audio-Visual Foundation Model](https://arxiv.org/abs/2601.03233)
*Yoav HaCohen,Benny Brazowski,Nisan Chiprut,Yaki Bitterman,Andrew Kvochko,Avishai Berkowitz,Daniel Shalem,Daphna Lifschitz,Dudu Moshe,Eitan Porat,Eitan Richardson,Guy Shiran,Itay Chachy,Jonathan Chetboun,Michael Finkelson,Michael Kupchick,Nir Zabari,Nitzan Guetta,Noa Kotler,Ofir Bibi,Ori Gordon,Poriya Panet,Roi Benita,Shahar Armon,Victor Kulikov,Yaron Inger,Yonatan Shiftan,Zeev Melumian,Zeev Farbman*

Main category: cs.CV

TL;DR: LTX-2 是一个开源的双流扩散模型，能同步生成高质量音视频，通过不对称双流Transformer和模态感知的CFG机制实现高效音视频对齐与可控生成。


<details>
  <summary>Details</summary>
Motivation: 现有文生视频模型缺乏音频，无法提供语义、情感与氛围线索；需统一生成高质量、时序同步的音视频内容。

Method: 提出不对称双流Transformer（14B视频流 + 5B音频流），引入双向音视频交叉注意力、时间位置编码与跨模态AdaLN；采用多语言文本编码器与模态感知的CFG（modality-CFG）机制。

Result: 在开源系统中达到SOTA音视频质量与提示遵循能力，性能媲美闭源模型，但计算成本与推理时间显著更低。

Conclusion: LTX-2验证了统一音视频生成的可行性与高效性，推动开放、可控、高质量多模态内容生成的发展。

Abstract: Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.

</details>


### [63] [A Versatile Multimodal Agent for Multimedia Content Generation](https://arxiv.org/abs/2601.03250)
*Daoan Zhang,Wenlin Yao,Xiaoyang Wang,Yebowen Hu,Jiebo Luo,Dong Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MultiMedia-Agent的多模态内容生成代理系统，通过技能获取理论、两阶段相关性策略和三阶段训练方法，提升了AIGC在真实复杂场景下的端到端多媒体内容生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC模型多为单任务、单模态组件，难以应对真实世界中需融合图像、视频、音频、文本等多模态输入与输出的复杂编辑任务；而代理系统为解决该问题提供了新路径。

Method: 提出MultiMedia-Agent系统，包含数据生成管道、内容创作工具库和偏好对齐评估指标；引入技能获取理论指导数据构建与代理训练；设计自相关与模型偏好相关的两阶段计划优化策略；采用基础计划微调、成功计划微调和偏好优化的三阶段训练方式。

Result: 实验表明，所提方法有效，MultiMedia-Agent在多媒体内容生成质量上优于当前新型模型。

Conclusion: MultiMedia-Agent为实现端到端、多模态、高集成度的AIGC应用提供了可行框架，推动AIGC从单一工具向协同智能体演进。

Abstract: With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs -- a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks. To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.

</details>


### [64] [InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields](https://arxiv.org/abs/2601.03252)
*Hao Yu,Haotong Lin,Jiawei Wang,Jiaxin Li,Yida Wang,Xueyang Zhang,Yue Wang,Xiaowei Zhou,Ruizhen Hu,Sida Peng*

Main category: cs.CV

TL;DR: 本文提出InfiniDepth，利用神经隐式场表示深度，支持任意分辨率和细粒度深度估计，并在合成与真实数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计方法受限于离散图像网格表示，难以扩展到任意输出分辨率且影响几何细节恢复。

Method: 提出InfiniDepth，将深度表示为神经隐式场，通过简单有效的局部隐式解码器，在连续2D坐标上查询深度值。

Result: 在自建的4K高质量合成基准及多个真实世界基准上均取得SOTA性能，尤其在细粒度区域表现突出；同时提升大视角变化下的新视角合成质量，减少空洞与伪影。

Conclusion: 神经隐式深度表示能有效突破传统离散网格限制，兼顾高分辨率、高细节与泛化能力，为深度估计与下游任务提供新范式。

Abstract: Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.

</details>


### [65] [Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training](https://arxiv.org/abs/2601.03256)
*Hexiao Lu,Xiaokun Sun,Zeyu Cai,Hao Guo,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: Muses是一种无需训练的3D生物生成方法，通过3D骨架引导结构化设计、体素组装和图像引导外观建模，实现高质量、文本对齐的3D内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于部件优化、人工组装或2D图像生成的方法常因部件级操作复杂和域外生成能力有限而产生不真实或不连贯的3D资产。

Method: Muses利用3D骨架作为基础表示，分三步进行：1）图约束推理构建创意性3D骨架；2）在结构化潜在空间中进行骨架引导的体素组装；3）在骨骼条件下进行图像引导的外观建模以生成风格一致纹理。

Result: 实验表明Muses在视觉保真度和文本对齐方面达到SOTA，并展现出灵活3D物体编辑潜力。

Conclusion: Muses首次实现了训练无关、前馈式的高质量3D生物生成，为结构感知的3D内容创作提供了新范式。

Abstract: We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [66] [WearVox: An Egocentric Multichannel Voice Assistant Benchmark for Wearables](https://arxiv.org/abs/2601.02391)
*Zhaojiang Lin,Yong Xu,Kai Sun,Jing Zheng,Yin Huang,Surya Teja Appini,Krish Narang,Renjie Tao,Ishan Kapil Jain,Siddhant Arora,Ruizhi Li,Yiteng Huang,Kaushik Patnaik,Wenfang Xu,Suwon Shon,Yue Liu,Ahmed A Aly,Anuj Kumar,Florian Metze,Xin Luna Dong*

Main category: cs.CL

TL;DR: 本文提出了WearVox，首个面向可穿戴设备（如AI眼镜）语音助手的现实场景基准测试，涵盖5类任务、3842条多通道第一人称音频，评估显示现有SLLM性能受限，尤其在嘈杂户外环境；多通道音频显著提升噪声鲁棒性与指令识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有语音基准忽视可穿戴设备特有的挑战，如运动引起的自中心音频失真、微交互、设备定向语音与背景对话区分难等问题，亟需更贴近真实场景的评估基准。

Method: 构建WearVox基准：采集3842段多通道、自中心音频，覆盖5类任务（搜索增强问答、闭卷问答、旁听拒绝、工具调用、语音翻译），涵盖室内外多种声学环境；配套丰富元数据；评估主流SLLM，并开展单/多通道音频输入的对比案例研究。

Result: 主流实时SLLM在WearVox上准确率仅29%–59%，户外噪声下性能显著下降；多通道输入相较单通道明显提升噪声鲁棒性及设备定向语音识别能力。

Conclusion: 空间音频线索对上下文感知型语音助手至关重要；WearVox为可穿戴语音AI研究提供了首个全面、真实的评测平台。

Abstract: Wearable devices such as AI glasses are transforming voice assistants into always-available, hands-free collaborators that integrate seamlessly with daily life, but they also introduce challenges like egocentric audio affected by motion and noise, rapid micro-interactions, and the need to distinguish device-directed speech from background conversations. Existing benchmarks largely overlook these complexities, focusing instead on clean or generic conversational audio. To bridge this gap, we present WearVox, the first benchmark designed to rigorously evaluate voice assistants in realistic wearable scenarios. WearVox comprises 3,842 multi-channel, egocentric audio recordings collected via AI glasses across five diverse tasks including Search-Grounded QA, Closed-Book QA, Side-Talk Rejection, Tool Calling, and Speech Translation, spanning a wide range of indoor and outdoor environments and acoustic conditions. Each recording is accompanied by rich metadata, enabling nuanced analysis of model performance under real-world constraints. We benchmark leading proprietary and open-source speech Large Language Models (SLLMs) and find that most real-time SLLMs achieve accuracies on WearVox ranging from 29% to 59%, with substantial performance degradation on noisy outdoor audio, underscoring the difficulty and realism of the benchmark. Additionally, we conduct a case study with two new SLLMs that perform inference with single-channel and multi-channel audio, demonstrating that multi-channel audio inputs significantly enhance model robustness to environmental noise and improve discrimination between device-directed and background speech. Our results highlight the critical importance of spatial audio cues for context-aware voice assistants and establish WearVox as a comprehensive testbed for advancing wearable voice AI research.

</details>


### [67] [PCEval: A Benchmark for Evaluating Physical Computing Capabilities of Large Language Models](https://arxiv.org/abs/2601.02404)
*Inpyo Song,Eunji Jeon,Jangwon Lee*

Main category: cs.CL

TL;DR: 本文提出了首个面向物理计算的自动评估基准PCEval，用于全面评测大语言模型（LLMs）在逻辑设计与物理实现两方面的能力；实验表明LLMs在代码和逻辑电路生成上表现良好，但在面包板物理布线（如引脚连接、避免短路等）上存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有LLM研究多集中于纯软件场景，而其在需与物理硬件交互的物理计算（如嵌入式开发、创客教育）中的能力尚未系统评估，尤其缺乏兼顾逻辑与物理层面的自动化评测基准。

Method: 构建PCEval基准：包含多层级物理计算项目（从简单LED控制到复杂传感器系统），支持自动验证生成的电路图、代码及面包板布局的正确性；在仿真环境中对13个主流LLM进行端到端测试。

Result: 实验发现LLMs在代码生成和逻辑电路设计任务上准确率较高，但在物理面包板布局任务中错误率高，尤其难以处理引脚冲突、电源/地线连接和信号完整性等硬件约束。

Conclusion: 当前LLMs在物理计算中仍存在关键短板，PCEval为该领域提供了首个可复现、全自动的评估框架，并为未来提升AI在硬件教育与开发中的实用性指明方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including software development, education, and technical assistance. Among these, software development is one of the key areas where LLMs are increasingly adopted. However, when hardware constraints are considered-for instance, in physical computing, where software must interact with and control physical hardware -their effectiveness has not been fully explored. To address this gap, we introduce \textsc{PCEval} (Physical Computing Evaluation), the first benchmark in physical computing that enables a fully automatic evaluation of the capabilities of LLM in both the logical and physical aspects of the projects, without requiring human assessment. Our evaluation framework assesses LLMs in generating circuits and producing compatible code across varying levels of project complexity. Through comprehensive testing of 13 leading models, \textsc{PCEval} provides the first reproducible and automatically validated empirical assessment of LLMs' ability to reason about fundamental hardware implementation constraints within a simulation environment. Our findings reveal that while LLMs perform well in code generation and logical circuit design, they struggle significantly with physical breadboard layout creation, particularly in managing proper pin connections and avoiding circuit errors. \textsc{PCEval} advances our understanding of AI assistance in hardware-dependent computing environments and establishes a foundation for developing more effective tools to support physical computing education.

</details>


### [68] [EvoRoute: Experience-Driven Self-Routing LLM Agent Systems](https://arxiv.org/abs/2601.02695)
*Guibin Zhang,Haiyang Yu,Kaiming Yang,Bingli Wu,Fei Huang,Yongbin Li,Shuicheng Yan*

Main category: cs.CL

TL;DR: 本文提出EvoRoute，一种自演化的模型路由范式，旨在解决复杂代理AI系统中的性能、成本和延迟三难困境。通过动态选择帕累托最优的LLM主干模型并持续优化策略，EvoRoute在保持甚至提升性能的同时，显著降低成本（最高80%）和延迟（超70%）。


<details>
  <summary>Details</summary>
Motivation: 复杂代理AI系统虽能力强，但面临高昂经济成本和严重延迟问题，存在性能、成本与延迟之间的未被充分研究的三难困境。

Method: 提出EvoRoute——一种基于经验知识库、动态选择帕累托最优LLM主干模型并利用环境反馈持续优化路由策略的自演化模型路由方法。

Result: 在GAIA和BrowseComp+等基准测试中，EvoRoute集成到现有代理系统后，性能不降反升，执行成本降低最多80%，延迟降低超70%。

Conclusion: EvoRoute有效破解了代理系统三难困境，为构建高效、低成本、低延迟的下一代智能代理提供了新范式。

Abstract: Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\%$ and latency by over $70\%$.

</details>


### [69] [Losses that Cook: Topological Optimal Transport for Structured Recipe Generation](https://arxiv.org/abs/2601.02531)
*Mattia Ottoborgo,Daniele Rege Cambrin,Paolo Garza*

Main category: cs.CL

TL;DR: 本文提出一种新的拓扑损失函数，将食谱中的食材列表表示为嵌入空间中的点云，以最小化预测与真实食材之间的差异，并结合多种复合目标优化食谱生成效果，在成分、动作、时间/温度精度等指标上均有提升，人类评估显示62%偏好该模型。


<details>
  <summary>Details</summary>
Motivation: 标准食谱生成方法仅关注文本流畅性（基于交叉熵），忽视了时间、温度、步骤连贯性和食材组成等关键因素。

Method: 基于RECIPE-NLG数据集，引入新提出的拓扑损失（将食材列表建模为嵌入空间点云）、Dice损失（提升时间/温度精度）及混合损失，并结合标准NLG与食谱专用评估指标进行训练与验证。

Result: 所提拓扑损失显著提升食材和动作层面指标；Dice损失在时间/温度精度上表现最优；混合损失在数量与时间维度实现协同增益；人工偏好测试中，该模型被选中的比例达62%。

Conclusion: 多目标联合优化（尤其是拓扑损失）能有效提升食谱生成的准确性与实用性，兼顾语言质量与烹饪约束。

Abstract: Cooking recipes are complex procedures that require not only a fluent and factual text, but also accurate timing, temperature, and procedural coherence, as well as the correct composition of ingredients. Standard training procedures are primarily based on cross-entropy and focus solely on fluency. Building on RECIPE-NLG, we investigate the use of several composite objectives and present a new topological loss that represents ingredient lists as point clouds in embedding space, minimizing the divergence between predicted and gold ingredients. Using both standard NLG metrics and recipe-specific metrics, we find that our loss significantly improves ingredient- and action-level metrics. Meanwhile, the Dice loss excels in time/temperature precision, and the mixed loss yields competitive trade-offs with synergistic gains in quantity and time. A human preference analysis supports our finding, showing our model is preferred in 62% of the cases.

</details>


### [70] [Image, Word and Thought: A More Challenging Language Task for the Iterated Learning Model](https://arxiv.org/abs/2601.02911)
*Hyoyeon Lee,Seth Bullock,Conor Houghton*

Main category: cs.CL

TL;DR: 本文提出了一种半监督迭代学习模型，结合了自监督与无监督学习，在自动编码器架构中成功应用于七段数码管图像的复杂语义通信任务，结果表明该模型能生成具有表达性、组合性和稳定性的语言。


<details>
  <summary>Details</summary>
Motivation: 探索语言在代际传递过程中如何在传输约束下自发形成结构（如无歧义性、语法性、稳定性），并扩展到更大、更贴近现实的意义-信号空间。

Method: 采用一种计算上更可行且生态效度更高的半监督迭代学习模型，将监督学习与无监督学习结合于自动编码器架构中，并应用于七段数码管（128种glyph）图像的语言学习任务。

Result: 模型成功习得并代际稳定传递一种语言：对全部128个glyph使用不同编码（表达性），信号成分与意义成分保持一致映射（组合性），且语言在多代间不发生漂变（稳定性）。

Conclusion: 半监督迭代学习框架可有效支持复杂语义空间中的结构化语言涌现，验证了语言结构可通过代际学习瓶颈自然产生，无需先天语法知识。

Abstract: The iterated learning model simulates the transmission of language from generation to generation in order to explore how the constraints imposed by language transmission facilitate the emergence of language structure. Despite each modelled language learner starting from a blank slate, the presence of a bottleneck limiting the number of utterances to which the learner is exposed can lead to the emergence of language that lacks ambiguity, is governed by grammatical rules, and is consistent over successive generations, that is, one that is expressive, compositional and stable. The recent introduction of a more computationally tractable and ecologically valid semi supervised iterated learning model, combining supervised and unsupervised learning within an autoencoder architecture, has enabled exploration of language transmission dynamics for much larger meaning-signal spaces. Here, for the first time, the model has been successfully applied to a language learning task involving the communication of much more complex meanings: seven-segment display images. Agents in this model are able to learn and transmit a language that is expressive: distinct codes are employed for all 128 glyphs; compositional: signal components consistently map to meaning components, and stable: the language does not change from generation to generation.

</details>


### [71] [ModeX: Evaluator-Free Best-of-N Selection for Open-Ended Generation](https://arxiv.org/abs/2601.02535)
*Hyeong Kyu Choi,Sharon Li*

Main category: cs.CL

TL;DR: 本文提出了一种无需外部评估器的Best-of-N选择框架ModeX，通过语义相似性图与谱聚类识别生成文本中的模态（主导语义共识）输出，在摘要、代码生成和数学推理等开放生成任务中显著提升性能且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有Best-of-N和自一致性方法依赖外部评估器、奖励模型或精确字符串匹配，在开放性任务中适用性差、效率低。

Method: 提出Mode Extraction（ModeX）：构建候选生成文本的相似性图，递归应用谱聚类选取语义中心（模态输出）；并设计轻量版ModeX-Lite，引入早期剪枝提升效率。

Result: 在文本摘要、代码生成、数学推理等开放任务上，ModeX及ModeX-Lite持续优于单路径与多路径基线方法，具备高鲁棒性与计算效率。

Conclusion: ModeX为开放文本生成提供了一种无需额外模型或推理、基于语义共识的高效、通用的Best-of-N选择新范式。

Abstract: Selecting a single high-quality output from multiple stochastic generations remains a fundamental challenge for large language models (LLMs), particularly in open-ended tasks where no canonical answer exists. While Best-of-N and self-consistency methods show that aggregating multiple generations can improve performance, existing approaches typically rely on external evaluators, reward models, or exact string-match voting, limiting their applicability and efficiency. We propose Mode Extraction (ModeX), an evaluator-free Best-of-N selection framework that generalizes majority voting to open-ended text generation by identifying the modal output representing the dominant semantic consensus among generated texts. ModeX constructs a similarity graph over candidate generations and recursively applies spectral clustering to select a representative centroid, without requiring additional inference or auxiliary models. We further instantiate this selection principle as ModeX-Lite, an improved version of ModeX with early pruning for efficiency. Across open-ended tasks -- including text summarization, code generation, and mathematical reasoning -- our approaches consistently outperform standard single- and multi-path baselines, providing a computationally efficient solution for robust open-ended text generation. Code is released in https://github.com/deeplearning-wisc/ModeX.

</details>


### [72] [LoRA-Drop: Temporal LoRA Decoding for Efficient LLM Inference](https://arxiv.org/abs/2601.02569)
*Hossein Rajabzadeh,Maryam Dialameh,Chul B. Park,Il-Min Kim,Hyock Ju Kwon*

Main category: cs.CL

TL;DR: LoRA-Drop是一种无需额外路由网络的插件式推理加速方法，通过在部分中间层复用前一token隐藏状态并施加低秩LoRA校正、周期性全模型刷新，实现显著解码加速与KV缓存压缩，同时保持几乎无损精度。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型受限于逐token顺序解码，现有动态深度或跳层方法常依赖辅助路由机制或导致精度下降。

Method: 提出LoRA-Drop框架：对固定子集中间层实施时间计算调度——多数步复用前一token隐藏状态并叠加LoRA修正，周期性执行完整前向传播以防止误差漂移；跳过可丢弃层的KV更新，仅在刷新步更新全部KV缓存。

Result: 在多个主流模型（LLaMA2-7B/LLaMA3-8B/Qwen2.5-7B/Qwen2.5-14B）上实现最高2.6倍解码加速和45–55% KV缓存缩减，精度损失≤0.5个百分点；在推理、代码、长文本及多语言任务上验证了稳定有效的调度‘安全区’。

Conclusion: LoRA-Drop提供了一种简单、兼容性强、高性价比的自适应计算容量推理路径，无需修改模型结构或训练流程，易于部署。

Abstract: Autoregressive large language models (LLMs) are bottlenecked by sequential decoding, where each new token typically requires executing all transformer layers. Existing dynamic-depth and layer-skipping methods reduce this cost, but often rely on auxiliary routing mechanisms or incur accuracy degradation when bypassed layers are left uncompensated. We present \textbf{LoRA-Drop}, a plug-and-play inference framework that accelerates decoding by applying a \emph{temporal compute schedule} to a fixed subset of intermediate layers: on most decoding steps, selected layers reuse the previous-token hidden state and apply a low-rank LoRA correction, while periodic \emph{refresh} steps execute the full model to prevent drift. LoRA-Drop requires no routing network, is compatible with standard KV caching, and can reduce KV-cache footprint by skipping KV updates in droppable layers during LoRA steps and refreshing periodically. Across \textbf{LLaMA2-7B}, \textbf{LLaMA3-8B}, \textbf{Qwen2.5-7B}, and \textbf{Qwen2.5-14B}, LoRA-Drop achieves up to \textbf{2.6$\times$ faster decoding} and \textbf{45--55\% KV-cache reduction} while staying within \textbf{0.5 percentage points (pp)} of baseline accuracy. Evaluations on reasoning (GSM8K, MATH, BBH), code generation (HumanEval, MBPP), and long-context/multilingual benchmarks (LongBench, XNLI, XCOPA) identify a consistent \emph{safe zone} of scheduling configurations that preserves quality while delivering substantial efficiency gains, providing a simple path toward adaptive-capacity inference in LLMs. Codes are available at https://github.com/hosseinbv/LoRA-Drop.git.

</details>


### [73] [Fact-Checking with Large Language Models via Probabilistic Certainty and Consistency](https://arxiv.org/abs/2601.02574)
*Haoran Wang,Maryam Khalid,Qiong Wu,Jian Gao,Cheng Cao*

Main category: cs.CL

TL;DR: 本文提出PCC框架，通过联合建模大语言模型的概率确定性和推理一致性来估计事实置信度，并据此自适应地决定是否依赖内部知识、触发目标检索或升级深度搜索，从而提升事实核查的效率与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查方法通常不加区分地检索外部证据，忽略了模型的内部知识，可能引入无关噪声，且缺乏针对推理中特定不确定性的解决机制。

Method: 提出概率确定性与一致性（PCC）框架，联合建模LLM的概率确定性和推理一致性以估计事实置信度，并设计基于置信度的自适应验证策略和路由机制。

Result: 在三个具有挑战性的基准上，PCC在不确定性量化方面优于口头化置信度方法，并持续超越强LLM基线；同时展现出跨多种LLM的良好泛化能力。

Conclusion: PCC通过将内部置信度评估与外部检索有机结合，实现了更高效、更可靠的事实核查，为LLM可信推理提供了新范式。

Abstract: Large language models (LLMs) are increasingly used in applications requiring factual accuracy, yet their outputs often contain hallucinated responses. While fact-checking can mitigate these errors, existing methods typically retrieve external evidence indiscriminately, overlooking the model's internal knowledge and potentially introducing irrelevant noise. Moreover, current systems lack targeted mechanisms to resolve specific uncertainties in the model's reasoning. Inspired by how humans fact-check, we argue that LLMs should adaptively decide whether to rely on internal knowledge or initiate retrieval based on their confidence in a given claim. We introduce Probabilistic Certainty and Consistency (PCC), a framework that estimates factual confidence by jointly modeling an LLM's probabilistic certainty and reasoning consistency. These confidence signals enable an adaptive verification strategy: the model answers directly when confident, triggers targeted retrieval when uncertain or inconsistent, and escalates to deep search when ambiguity is high. Our confidence-guided routing mechanism ensures that retrieval is invoked only when necessary, improving both efficiency and reliability. Extensive experiments across three challenging benchmarks show that PCC achieves better uncertainty quantification than verbalized confidence and consistently outperforms strong LLM-based fact-checking baselines. Furthermore, we demonstrate that PCC generalizes well across various LLMs.

</details>


### [74] [DataParasite Enables Scalable and Repurposable Online Data Curation](https://arxiv.org/abs/2601.02578)
*Mengyi Sun*

Main category: cs.CL

TL;DR: DataParasite 是一个开源、模块化的网络数据采集管道，通过轻量级配置和自然语言指令实现灵活、可复用的结构化数据提取，显著降低计算社会科学中在线数据收集的成本与门槛。


<details>
  <summary>Details</summary>
Motivation: 现有网络数据收集方法劳动密集、成本高、难以复现，且当前基于大语言模型的系统常缺乏透明性、灵活性或不适用于科研数据整理。

Method: 提出 DataParasite 管道，将表格型数据整理任务分解为独立的实体级搜索，由轻量配置文件定义、统一无任务依赖的 Python 脚本执行；支持仅凭自然语言指令适配新任务（包括无预定义实体列表的任务）。

Result: 在多个计算社会科学典型任务（如教师聘任史、精英人物逝世事件、政治生涯轨迹）上验证，准确率高，数据收集成本较人工降低一个数量级。

Conclusion: DataParasite 降低了技术与人力门槛，为计算社会科学及更广泛领域提供了可扩展、透明、可复用的数据整理基础。

Abstract: Many questions in computational social science rely on datasets assembled from heterogeneous online sources, a process that is often labor-intensive, costly, and difficult to reproduce. Recent advances in large language models enable agentic search and structured extraction from the web, but existing systems are frequently opaque, inflexible, or poorly suited to scientific data curation. Here we introduce DataParasite, an open-source, modular pipeline for scalable online data collection. DataParasite decomposes tabular curation tasks into independent, entity-level searches defined through lightweight configuration files and executed through a shared, task-agnostic python script. Crucially, the same pipeline can be repurposed to new tasks, including those without predefined entity lists, using only natural-language instructions. We evaluate the pipeline on multiple canonical tasks in computational social science, including faculty hiring histories, elite death events, and political career trajectories. Across tasks, DataParasite achieves high accuracy while reducing data-collection costs by an order of magnitude relative to manual curation. By lowering the technical and labor barriers to online data assembly, DataParasite provides a practical foundation for scalable, transparent, and reusable data curation in computational social science and beyond.

</details>


### [75] [Reconstructing Item Characteristic Curves using Fine-Tuned Large Language Models](https://arxiv.org/abs/2601.02580)
*Christopher Ormerod*

Main category: cs.CL

TL;DR: 本文提出一种利用微调大语言模型（LLM）模拟不同能力学生作答行为、从而无需真实测试数据即可估计IRT项目参数（如难度、区分度）的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统IRT项目参数标定依赖昂贵的实地测试收集学生作答数据，成本高、周期长，亟需低成本、高效替代方案。

Method: 基于Qwen-3系列模型与LoRA微调技术，让LLM根据离散能力描述生成多项选择题作答；通过建模正确作答概率随能力变化的函数，重建项目特征曲线（ICC），进而估计IRT参数。

Result: 在六年级英语语言艺术（ELA）题库和BEA 2024共享任务数据集上的实验表明，该方法性能媲美甚至优于基线方法，尤其在区分度建模上表现突出。

Conclusion: LLM可作为可控的心理测量模拟器，为IRT参数估计提供一种可行、有效的合成数据驱动新范式。

Abstract: Traditional methods for determining assessment item parameters, such as difficulty and discrimination, rely heavily on expensive field testing to collect student performance data for Item Response Theory (IRT) calibration. This study introduces a novel approach that implicitly models these psychometric properties by fine-tuning Large Language Models (LLMs) to simulate student responses across a spectrum of latent abilities. Leveraging the Qwen-3 dense model series and Low-Rank Adaptation (LoRA), we train models to generate responses to multiple choice questions conditioned on discrete ability descriptors. We reconstruct the probability of a correct response as a function of student ability, effectively generating synthetic Item Characteristic Curves (ICCs) to estimate IRT parameters. Evaluation on a dataset of Grade 6 English Language Arts (ELA) items and the BEA 2024 Shared Task dataset demonstrates that this method competes with or outperforms baseline approaches. This simulation-based technique seems particularly effective at modeling item discrimination.

</details>


### [76] [FlowPlan-G2P: A Structured Generation Framework for Transforming Scientific Papers into Patent Descriptions](https://arxiv.org/abs/2601.02589)
*Kris W Pan,Yongmin Yoo*

Main category: cs.CL

TL;DR: 本文提出FlowPlan-G2P框架，将科研论文转化为专利文本分为三阶段：概念图构建、段落与章节规划、图条件生成，显著提升逻辑连贯性与法律合规性。


<details>
  <summary>Details</summary>
Motivation: 科研论文与专利在修辞风格和法律要求上差异大，现有黑箱式文本生成方法难以建模结构化推理与法律约束。

Method: 提出FlowPlan-G2P三阶段框架：(1) 概念图归纳，提取技术实体与关系构建有向图；(2) 段落与章节规划，将图聚类对齐专利标准结构；(3) 图条件生成，基于子图与定制提示生成合规段落。

Result: 实验表明该框架在逻辑连贯性和法律合规性上显著优于端到端大语言模型基线。

Conclusion: FlowPlan-G2P建立了论文到专利生成的新范式，推动了专业领域结构化文本生成的发展。

Abstract: Over 3.5 million patents are filed annually, with drafting patent descriptions requiring deep technical and legal expertise. Transforming scientific papers into patent descriptions is particularly challenging due to their differing rhetorical styles and stringent legal requirements. Unlike black-box text-to-text approaches that struggle to model structural reasoning and legal constraints, we propose FlowPlan-G2P, a novel framework that mirrors the cognitive workflow of expert drafters by reformulating this task into three stages: (1) Concept Graph Induction, extracting technical entities and relationships into a directed graph via expert-like reasoning; (2) Paragraph and Section Planning, reorganizing the graph into coherent clusters aligned with canonical patent sections; and (3) Graph-Conditioned Generation, producing legally compliant paragraphs using section-specific subgraphs and tailored prompts. Experiments demonstrate that FlowPlan-G2P significantly improves logical coherence and legal compliance over end-to-end LLM baselines. Our framework establishes a new paradigm for paper-to-patent generation and advances structured text generation for specialized domains.

</details>


### [77] [Scalable Construction of a Lung Cancer Knowledge Base: Profiling Semantic Reasoning in LLMs](https://arxiv.org/abs/2601.02604)
*Cesar Felipe Martínez Cisneros,Jesús Ulises Quiroz Bautista,Claudia Anahí Guzmán Solano,Bogdan Kaleb García Rivera,Iván García Pacheco,Yalbi Itzel Balderas Martínez,Kolawole John Adebayoc,Ignacio Arroyo Fernández*

Main category: cs.CL

TL;DR: 本文提出了一种基于OpenIE的肺部癌症知识库构建流程，用于提升大语言模型在生物医学领域的微调效果。


<details>
  <summary>Details</summary>
Motivation: 在肿瘤学中，需要高精度和可解释性的知识表示，而现有LLM性能受限于训练数据的语义质量，亟需可扩展的结构化知识库构建方法。

Method: 采用Open Information Extraction（OpenIE）构建肺癌知识库，包括：1）使用MeSH词表识别医学概念；2）筛选CC0许可的开放PubMed文献；3）用OpenIE抽取（主语，关系，宾语）三元组；4）结合NER增强三元组的生物医学相关性。

Result: 构建了领域特定、大规模、抗噪声的知识三元组资源，并通过监督语义微调（SSFT）在T5模型上验证，ROUGE与BERTScore评估显示语义一致性和性能显著提升。

Conclusion: 基于OpenIE的知识库构建方法是一种可扩展、低成本的方案，能有效增强生物医学NLP任务中的大语言模型性能。

Abstract: The integration of Large Language Models (LLMs) into biomedical research offers new opportunities for domainspecific reasoning and knowledge representation. However, their performance depends heavily on the semantic quality of training data. In oncology, where precision and interpretability are vital, scalable methods for constructing structured knowledge bases are essential for effective fine-tuning. This study presents a pipeline for developing a lung cancer knowledge base using Open Information Extraction (OpenIE). The process includes: (1) identifying medical concepts with the MeSH thesaurus; (2) filtering open-access PubMed literature with permissive licenses (CC0); (3) extracting (subject, relation, object) triplets using OpenIE method; and (4) enriching triplet sets with Named Entity Recognition (NER) to ensure biomedical relevance. The resulting triplet sets provide a domain-specific, large-scale, and noise-aware resource for fine-tuning LLMs. We evaluated T5 models finetuned on this dataset through Supervised Semantic Fine-Tuning. Comparative assessments with ROUGE and BERTScore show significantly improved performance and semantic coherence, demonstrating the potential of OpenIE-derived resources as scalable, low-cost solutions for enhancing biomedical NLP.

</details>


### [78] [Improved Evidence Extraction for Document Inconsistency Detection with LLMs](https://arxiv.org/abs/2601.02627)
*Nelvin Tan,Yaowen Zhang,James Asikin Cheung,Fusheng Liu,Yu-Ching Shih,Dong Yang*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的文档不一致性检测新方法，聚焦于提取不一致句子的证据，并引入了新的证据提取评估指标及‘遮蔽-重试’框架，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究对基于大语言模型的文档不一致性检测关注较少，尤其缺乏对不一致证据提取的系统性方法和评估标准。

Method: 提出红掩-重试（redact-and-retry）框架与约束过滤机制，并设计综合性的证据提取评估指标。

Result: 实验结果表明，所提方法在证据提取任务上显著优于直接提示（direct prompting）基线。

Conclusion: 该工作为LLM在文档不一致性检测中的应用提供了更可靠、可解释的证据提取路径，推动了该方向的实用化发展。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their impressive abilities that arise from large training datasets and large model sizes. However, research on LLM-based approaches to document inconsistency detection is relatively limited. There are two key aspects of document inconsistency detection: (i) classification of whether there exists any inconsistency, and (ii) providing evidence of the inconsistent sentences. We focus on the latter, and introduce new comprehensive evidence-extraction metrics and a redact-and-retry framework with constrained filtering that substantially improves LLM-based document inconsistency detection over direct prompting. We back our claims with promising experimental results.

</details>


### [79] [Empirical Comparison of Encoder-Based Language Models and Feature-Based Supervised Machine Learning Approaches to Automated Scoring of Long Essays](https://arxiv.org/abs/2601.02659)
*Kuo Wang,Haowei Hua,Pengfei Yan,Hong Jiao,Dan Song*

Main category: cs.CL

TL;DR: 本研究探讨了长文本对编码器-only语言模型在自动作文评分任务中的影响，通过对比多种BERT类模型及其集成方法，发现基于多预训练语言模型嵌入的集成模型结合梯度提升分类器在长作文评分中表现最优。


<details>
  <summary>Details</summary>
Motivation: 长上下文可能给仅编码器的语言模型在文本处理（尤其是自动作文评分）中带来挑战，需探索更有效的建模方法。

Method: 训练并比较多种BERT类编码器模型（BERT、RoBERTa、DistilBERT、DeBERTa）、多模型嵌入集成模型、以及基于特征的传统机器学习集成模型（如GBDT、XGBoost、LightGBM），在17307篇作文数据集上进行80/10/10划分，并用二次加权Kappa评估性能。

Result: 基于多语言模型嵌入并以梯度提升分类器为融合器的集成模型显著优于各单个语言模型。

Conclusion: 对于长作文自动评分任务，融合多源预训练嵌入与强分类器的集成策略比单一编码器模型更有效。

Abstract: Long context may impose challenges for encoder-only language models in text processing, specifically for automated scoring of essays. This study trained several commonly used encoder-based language models for automated scoring of long essays. The performance of these trained models was evaluated and compared with the ensemble models built upon the base language models with a token limit of 512?. The experimented models include BERT-based models (BERT, RoBERTa, DistilBERT, and DeBERTa), ensemble models integrating embeddings from multiple encoder models, and ensemble models of feature-based supervised machine learning models, including Gradient-Boosted Decision Trees, eXtreme Gradient Boosting, and Light Gradient Boosting Machine. We trained, validated, and tested each model on a dataset of 17,307 essays, with an 80%/10%/10% split, and evaluated model performance using Quadratic Weighted Kappa. This study revealed that an ensemble-of-embeddings model that combines multiple pre-trained language model representations with gradient-boosting classifier as the ensemble model significantly outperforms individual language models at scoring long essays.

</details>


### [80] [When Do Tools and Planning Help LLMs Think? A Cost- and Latency-Aware Benchmark](https://arxiv.org/abs/2601.02663)
*Subha Ghoshal,Ali Al-Bustami*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLM）在推理时使用规划与外部工具（如SPARQL查询、维基百科检索、网络搜索）对两类真实任务——基于事件的图知识问答（Event-QA）和Reddit说服性回复生成（CMV）——的影响；结果表明：工具增强显著提升Event-QA准确率但大幅增加延迟，而CMV中简单提示更优；模型大小与工具复杂度需依任务权衡。


<details>
  <summary>Details</summary>
Motivation: 现代大模型日益依赖推理时规划与外部工具提升推理能力，但其在真实任务中的实际效益、成本与适用边界尚不清晰，亟需实证基准分析。

Method: 在Event-QA和CMV两个真实任务上，使用LangChain/LangGraph构建‘计划-执行-重计划’智能体，配备DBpedia SPARQL查询、维基百科检索和主题网络搜索等任务定制工具，并与单次提示（one-shot）基线对比；在60个样本（每任务3×20）上评估GPT-4o与GPT-4o-mini的准确率、端到端延迟及token开销。

Result: 在Event-QA上，工具增强使GPT-4o准确率从47.5%升至67.5%，但延迟从~8秒增至~317秒；在CMV上，one-shot表现最优（GPT-4o-mini达75%准确率、~6秒），规划+搜索未带来稳定增益且显著增耗；小模型在多工具协同中易失效。

Conclusion: 任务特性决定是否采用推理时规划与工具调用；应进行任务导向、成本敏感的模型尺寸与智能体复杂度联合选择，避免盲目堆叠工具。

Abstract: Modern large language models (LLMs) increasingly rely on inference-time planning and external tools to improve reasoning. We benchmark this behavior on two real-world settings: event-centric question answering over graph-structured knowledge (Event-QA) and persuasive response generation in Reddit ChangeMyView (CMV). Using LangChain and LangGraph, we compare a one-shot baseline against a plan-execute-replan agent equipped with task-specific tools (DBpedia SPARQL/lookup/schema exploration, Wikipedia-focused retrieval, and topical web search). We evaluate on 60 examples each from Event-QA and CMV (3 splits of 20), and report both mean end-to-end latency and per-example token cost estimates. We evaluate GPT-4o and GPT-4o-mini under identical workflows and report accuracy and end-to-end latency. On Event-QA, the best tool-augmented configuration improves accuracy (e.g., 47.5\% $\rightarrow$ 67.5\% for GPT-4o) while increasing latency by orders of magnitude ($\sim$8s $\rightarrow$ $\sim$317s per example). On CMV, one-shot prompting is strongest (e.g., GPT-4o-mini achieves 75\% at $\sim$6s), and planning+search increases latency substantially without consistent gains. However, complex multi-tool orchestration exposes failure modes where the smaller model degrades. Overall, the findings highlight the need for task-specific, cost-aware choices of both model size and agent/tooling complexity.

</details>


### [81] [Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking](https://arxiv.org/abs/2601.02669)
*Hongzhan Lin,Zixin Chen,Zhiqi Shen,Ziyang Luo,Zhen Ye,Jing Ma,Tat-Seng Chua,Guandong Xu*

Main category: cs.CL

TL;DR: 本文提出了FactArena，一个全自动的竞技场式评估框架，用于对大语言模型（LLMs）在完整事实核查流程（包括主张提取、证据检索与验证判断）中的各阶段能力进行系统性、分阶段评测。


<details>
  <summary>Details</summary>
Motivation: 现有评估集中于主张验证，忽视了完整的事实核查流程，无法揭示LLM在推理、事实盲区和鲁棒性方面的系统性缺陷。

Method: 构建FactArena框架，包含三部分：(i) LLM驱动的标准化工件分解、工具增强型证据检索与基于理由的判决预测；(ii) 基于统一指南的竞技场式多裁判成对比较机制；(iii) 自适应生成更具挑战性、语义可控主张的主张演化模块。

Result: 在16个SOTA LLM（7个模型家族）上实现稳定可解释的排名，并发现静态主张验证准确率与端到端事实核查能力存在显著差距。

Conclusion: 全链路、阶段式、自适应的事实核查评估范式更真实反映LLM的事实推理能力，对模型开发与安全关键场景部署具有重要指导意义。

Abstract: Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.

</details>


### [82] [Multi-Turn Jailbreaking of Aligned LLMs via Lexical Anchor Tree Search](https://arxiv.org/abs/2601.02670)
*Devang Kulshreshtha,Hang Su,Chinmay Hegde,Haohan Wang*

Main category: cs.CL

TL;DR: 本文提出了一种无需攻击者大语言模型（attacker LLM）的 jailbreak 方法 Lexical Anchor Tree Search（LATS），通过词法锚点注入和广度优先树搜索，在极低查询预算（平均仅约6.4次）下实现高达97–100%的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有 jailbreak 方法依赖 attacker LLM 生成难以解释的随机前缀，且查询开销大、成本高，缺乏高效、可解释、低资源的替代方案。

Method: LATS 将 jailbreaking 建模为多轮对话上的广度优先树搜索，每个节点通过逐步向良性提示注入攻击目标中缺失的内容词（lexical anchors）来构造对抗性查询，全程无需 attacker LLM。

Result: 在 AdvBench 和 HarmBench 上，LATS 在最新 GPT、Claude 和 Llama 模型上达到 97–100% 攻击成功率，平均仅需 ~6.4 次查询，显著优于需 20+ 次查询的其他方法。

Conclusion: LATS 揭示了对话结构本身是一个强大但未被充分防护的攻击面，并证明在高攻击成功率已成常态的当下，查询效率成为更关键的评估维度。

Abstract: Most jailbreak methods achieve high attack success rates (ASR) but require attacker LLMs to craft adversarial queries and/or demand high query budgets. These resource limitations make jailbreaking expensive, and the queries generated by attacker LLMs often consist of non-interpretable random prefixes. This paper introduces Lexical Anchor Tree Search (), addressing these limitations through an attacker-LLM-free method that operates purely via lexical anchor injection. LATS reformulates jailbreaking as a breadth-first tree search over multi-turn dialogues, where each node incrementally injects missing content words from the attack goal into benign prompts. Evaluations on AdvBench and HarmBench demonstrate that LATS achieves 97-100% ASR on latest GPT, Claude, and Llama models with an average of only ~6.4 queries, compared to 20+ queries required by other methods. These results highlight conversational structure as a potent and under-protected attack surface, while demonstrating superior query efficiency in an era where high ASR is readily achievable. Our code will be released to support reproducibility.

</details>


### [83] [Extracting books from production language models](https://arxiv.org/abs/2601.02671)
*Ahmed Ahmed,A. Feder Cooper,Sanmi Koyejo,Percy Liang*

Main category: cs.CL

TL;DR: 本文研究了在生产级大语言模型（LLMs）中是否仍可提取受版权保护的训练数据，发现即使有安全措施，部分模型（如Claude 3.7 Sonnet）仍存在高比例文本提取风险，而其他模型（如GPT-4.1）则抵抗较强。


<details>
  <summary>Details</summary>
Motivation: 当前关于LLM与版权的法律争议核心在于模型是否记忆并可提取训练数据，尤其关注生产级模型在部署安全机制后是否仍存在此类风险。

Method: 采用两阶段方法：第一阶段用Best-of-N（BoN）等探针测试提取可行性；第二阶段用迭代续写提示尝试完整提取整本书；评估指标为基于块的最长公共子串近似得分（nv-recall）。

Result: 在Claude 3.7 Sonnet上 jailbreak 后可达95.8% nv-recall（近乎逐字还原整书）；Gemini 2.5 Pro和Grok 3无需jailbreak即达76.8%和70.3%；GPT-4.1需大量BoN尝试且最终拒绝输出（仅4.0%）。

Conclusion: 尽管存在模型级和系统级防护，生产LLM仍可能泄露受版权保护的训练数据，构成现实法律与合规风险。

Abstract: Many unresolved legal questions over LLMs and copyright center on memorization: whether specific training data have been encoded in the model's weights during training, and whether those memorized data can be extracted in the model's outputs. While many believe that LLMs do not memorize much of their training data, recent work shows that substantial amounts of copyrighted text can be extracted from open-weight models. However, it remains an open question if similar extraction is feasible for production LLMs, given the safety measures these systems implement. We investigate this question using a two-phase procedure: (1) an initial probe to test for extraction feasibility, which sometimes uses a Best-of-N (BoN) jailbreak, followed by (2) iterative continuation prompts to attempt to extract the book. We evaluate our procedure on four production LLMs -- Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3 -- and we measure extraction success with a score computed from a block-based approximation of longest common substring (nv-recall). With different per-LLM experimental configurations, we were able to extract varying amounts of text. For the Phase 1 probe, it was unnecessary to jailbreak Gemini 2.5 Pro and Grok 3 to extract text (e.g, nv-recall of 76.8% and 70.3%, respectively, for Harry Potter and the Sorcerer's Stone), while it was necessary for Claude 3.7 Sonnet and GPT-4.1. In some cases, jailbroken Claude 3.7 Sonnet outputs entire books near-verbatim (e.g., nv-recall=95.8%). GPT-4.1 requires significantly more BoN attempts (e.g., 20X), and eventually refuses to continue (e.g., nv-recall=4.0%). Taken together, our work highlights that, even with model- and system-level safeguards, extraction of (in-copyright) training data remains a risk for production LLMs.

</details>


### [84] [Iterative Structured Pruning for Large Language Models with Multi-Domain Calibration](https://arxiv.org/abs/2601.02674)
*Guangxin Wu,Hao Zhang,Zhang Zhibin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种基于混合多领域校准集和迭代校准策略的新型结构化剪枝框架，用于压缩大语言模型（LLMs），在保持标准硬件兼容性的同时实现高效压缩与低性能损失。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模持续扩大，导致部署时计算开销、内存占用和推理延迟过高；现有非结构化剪枝方法因稀疏模式不规则而依赖专用软硬件，亟需兼容通用硬件的结构化剪枝方案。

Method: 提出一种结构化剪枝框架，通过构建混合多领域校准集，并采用迭代校准策略识别并移除冗余通道，从而实现对整个架构组件的剪枝。

Result: 在多种模型和下游任务上的大量实验表明，该方法能在显著压缩模型的同时，仅造成极小的性能下降。

Conclusion: 所提结构化剪枝方法在不牺牲硬件兼容性的前提下，有效缓解了大语言模型部署中的资源瓶颈问题，为实际应用提供了可行路径。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide spectrum of natural language processing tasks. However, their ever-growing scale introduces significant barriers to real-world deployment, including substantial computational overhead, memory footprint, and inference latency. While model pruning presents a viable solution to these challenges, existing unstructured pruning techniques often yield irregular sparsity patterns that necessitate specialized hardware or software support. In this work, we explore structured pruning, which eliminates entire architectural components and maintains compatibility with standard hardware accelerators. We introduce a novel structured pruning framework that leverages a hybrid multi-domain calibration set and an iterative calibration strategy to effectively identify and remove redundant channels. Extensive experiments on various models across diverse downstream tasks show that our approach achieves significant compression with minimal performance degradation.

</details>


### [85] [Boosting Accuracy and Interpretability in Multilingual Hate Speech Detection Through Layer Freezing and Explainable AI](https://arxiv.org/abs/2601.02697)
*Meysam Shirdel Bilehsavar,Negin Mahmoudi,Mohammad Jalili Torkamani,Kiana Kiashemshaki*

Main category: cs.CL

TL;DR: 本文研究了三种基于Transformer的模型（BERT-base-multilingual-cased、RoBERTa-base和XLM-RoBERTa-base）在五种语言上的多语言情感分析与仇恨言论检测任务中的性能，并结合LIME方法提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 情感分析与仇恨言论检测对在线内容审核至关重要，但多语言场景下模型性能与可解释性仍需提升。

Method: 采用三种冻结前八层的多语言Transformer模型，在英语、韩语、日语、中文和法语数据上进行训练与评估，并引入LIME框架进行模型预测的局部可解释性分析。

Result: 报告了各模型在准确率、精确率、召回率和F1分数上的对比结果，同时通过LIME识别出影响预测的关键词汇。

Conclusion: 结合先进Transformer架构与LIME可解释性技术，可有效提升多语言情感分析与仇恨言论检测系统的性能与透明度。

Abstract: Sentiment analysis focuses on identifying the emotional polarity expressed in textual data, typically categorized as positive, negative, or neutral. Hate speech detection, on the other hand, aims to recognize content that incites violence, discrimination, or hostility toward individuals or groups based on attributes such as race, gender, sexual orientation, or religion. Both tasks play a critical role in online content moderation by enabling the detection and mitigation of harmful or offensive material, thereby contributing to safer digital environments. In this study, we examine the performance of three transformer-based models: BERT-base-multilingual-cased, RoBERTa-base, and XLM-RoBERTa-base with the first eight layers frozen, for multilingual sentiment analysis and hate speech detection. The evaluation is conducted across five languages: English, Korean, Japanese, Chinese, and French. The models are compared using standard performance metrics, including accuracy, precision, recall, and F1-score. To enhance model interpretability and provide deeper insight into prediction behavior, we integrate the Local Interpretable Model-agnostic Explanations (LIME) framework, which highlights the contribution of individual words to the models decisions. By combining state-of-the-art transformer architectures with explainability techniques, this work aims to improve both the effectiveness and transparency of multilingual sentiment analysis and hate speech detection systems.

</details>


### [86] [Adversarial Question Answering Robustness: A Multi-Level Error Analysis and Mitigation Study](https://arxiv.org/abs/2601.02700)
*Agniv Roy Choudhury,Vignesh Ponselvan Rajasingh*

Main category: cs.CL

TL;DR: 本文系统研究了Transformer模型在AddSent对抗数据集上的鲁棒性，通过多层级错误分析识别主要失败模式，并提出基于命名实体识别（NER）引导的对比学习等针对性缓解策略，在AddSent和SQuAD上显著缩小对抗差距，首次实现清洁与对抗性能接近一致。


<details>
  <summary>Details</summary>
Motivation: 尽管问答系统在标准基准（如SQuAD）上表现优异，但在对抗样本（如AddSent）面前仍脆弱；需深入理解失败机制并设计有效鲁棒性提升方法。

Method: 采用多级错误分析（五种分类方案）、对抗微调比例调优、数据增强实验、模型缩放分析（ELECTRA-small到base），并提出三种缓解策略，其中Entity-Aware对比学习为核心创新。

Result: 确定80%清洁+20%对抗数据为最优微调配比；发现小模型存在容量瓶颈；大模型（ELECTRA-base）消除鲁棒性-准确性权衡；Entity-Aware对比学习达到AddSent EM 89.89%、SQuAD EM 90.73%，闭合94.9%对抗差距。

Conclusion: 结合语言学错误分析与NER引导的对比学习可显著提升QA模型对抗鲁棒性，使清洁与对抗性能趋近一致，为鲁棒问答提供了新范式。

Abstract: Question answering (QA) systems achieve impressive performance on standard benchmarks like SQuAD, but remain vulnerable to adversarial examples. This project investigates the adversarial robustness of transformer models on the AddSent adversarial dataset through systematic experimentation across model scales and targeted mitigation strategies. We perform comprehensive multi-level error analysis using five complementary categorization schemes, identifying negation confusion and entity substitution as the primary failure modes. Through systematic evaluation of adversarial fine-tuning ratios, we identify 80% clean + 20% adversarial data as optimal. Data augmentation experiments reveal a capacity bottleneck in small models. Scaling from ELECTRA-small (14M parameters) to ELECTRA-base (110M parameters) eliminates the robustness-accuracy trade-off, achieving substantial improvements on both clean and adversarial data. We implement three targeted mitigation strategies, with Entity-Aware contrastive learning achieving best performance: 89.89% AddSent Exact Match (EM) and 90.73% SQuAD EM, representing 94.9% closure of the adversarial gap. To our knowledge, this is the first work integrating comprehensive linguistic error analysis with Named Entity Recognition (NER)-guided contrastive learning for adversarial QA, demonstrating that targeted mitigation can achieve near-parity between clean and adversarial performance.

</details>


### [87] [Mitigating Prompt-Induced Hallucinations in Large Language Models via Structured Reasoning](https://arxiv.org/abs/2601.02739)
*Jinbo Hao,Kai Yang,Qingzhen Su,Yang Chen,Yifan Li,Chao Jiang*

Main category: cs.CL

TL;DR: 本文提出一种通过代码模块引导知识图谱探索并将其融入链式思维提示的方法，以缓解大语言模型中的提示诱导幻觉问题，显著提升了推理准确性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）中存在的幻觉问题，特别是提示诱导型幻觉。

Method: 基于知识蒸馏链式模型，引入代码模块引导知识图谱探索，并将代码作为链式思维提示的一部分，形成外部结构化知识输入，用以分析和约束LLM的推理过程。

Result: 在GPT-4和LLaMA-3.3上实验表明，该方法显著提升HIT@1（+15.64%）、HIT@3（+13.38%）、HIT@5（+13.28%），多个设置下HIT@1/3/5均超95%。

Conclusion: 所提方法能有效缓解提示诱导幻觉，提高大语言模型的推理准确性与结果可验证性。

Abstract: To address hallucination issues in large language models (LLMs), this paper proposes a method for mitigating prompt-induced hallucinations. Building on a knowledge distillation chain-style model, we introduce a code module to guide knowledge-graph exploration and incorporate code as part of the chain-of-thought prompt, forming an external knowledge input that provides more accurate and structured information to the model. Based on this design, we develop an improved knowledge distillation chain-style model and leverage it to analyze and constrain the reasoning process of LLMs, thereby improving inference accuracy. We empirically evaluate the proposed approach using GPT-4 and LLaMA-3.3 on multiple public datasets. Experimental results demonstrate that incorporating code modules significantly enhances the model's ability to capture contextual information and effectively mitigates prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 improve by 15.64%, 13.38%, and 13.28%, respectively. Moreover, the proposed method achieves HIT@1, HIT@3, and HIT@5 scores exceeding 95% across several evaluation settings. These results indicate that the proposed approach substantially reduces hallucination behavior while improving the accuracy and verifiability of large language models.

</details>


### [88] [Language Hierarchization Provides the Optimal Solution to Human Working Memory Limits](https://arxiv.org/abs/2601.02740)
*Luyao Chen,Weibo Gao,Junjie Wu,Jinshan Wu,Angela D. Friederici*

Main category: cs.CL

TL;DR: 本文提出层次化结构是人类语言为适应工作记忆容量限制而演化出的最优解，通过似然函数和模拟验证了层次处理比线性处理更能有效约束记忆负荷。


<details>
  <summary>Details</summary>
Motivation: 解释人类语言为何具有普遍的层次结构，核心动机是探究层次化是否源于对有限工作记忆容量（WMC）的适应性优化。

Method: 构建一个似然函数，以量化语言处理中平均单元数与人类工作记忆容量的一致性；通过计算最大似然估计（theta_MLE），并结合符号序列的计算模拟及自然语言句子的实证分析进行验证。

Result: 层次化处理相比线性处理能更有效地将theta_MLE约束在人类WMC限制内，且该优势随序列长度增加而增强；还观察到与儿童WMC发育相关的收敛模式。

Conclusion: 语言的层次结构本质上是一种适应性机制，旨在优化顺序语言输入的处理效率，同时严格服从人类工作记忆的生理限制，从而真正解释了语言的普遍层次性。

Abstract: Language is a uniquely human trait, conveying information efficiently by organizing word sequences in sentences into hierarchical structures. A central question persists: Why is human language hierarchical? In this study, we show that hierarchization optimally solves the challenge of our limited working memory capacity. We established a likelihood function that quantifies how well the average number of units according to the language processing mechanisms aligns with human working memory capacity (WMC) in a direct fashion. The maximum likelihood estimate (MLE) of this function, tehta_MLE, turns out to be the mean of units. Through computational simulations of symbol sequences and validation analyses of natural language sentences, we uncover that compared to linear processing, hierarchical processing far surpasses it in constraining the tehta_MLE values under the human WMC limit, along with the increase of sequence/sentence length successfully. It also shows a converging pattern related to children's WMC development. These results suggest that constructing hierarchical structures optimizes the processing efficiency of sequential language input while staying within memory constraints, genuinely explaining the universal hierarchical nature of human language.

</details>


### [89] [SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation](https://arxiv.org/abs/2601.02744)
*Hanqi Jiang,Junhao Chen,Yi Pan,Ling Chen,Weihang You,Yifan Zhou,Ruidong Zhang,Yohannes Abate,Tianming Liu*

Main category: cs.CL

TL;DR: 本文提出Synapse，一种受认知科学启发的动态图记忆架构，通过扩散激活、侧抑制和时间衰减机制实现关联式语义检索，显著提升长程多跳推理性能。


<details>
  <summary>Details</summary>
Motivation: 标准检索增强方法无法解决长时程智能体记忆的离散性问题，导致上下文隧道效应（Contextual Tunneling）；现有向量检索依赖静态相似度，缺乏动态关联建模能力。

Method: 提出Synapse统一记忆架构：将记忆建模为动态图，采用扩散激活机制生成相关性；引入侧抑制与时间衰减调控节点激活强度；设计三重混合检索策略，融合几何嵌入与基于激活的图遍历。

Result: 在LoCoMo基准测试中，Synapse在复杂时序与多跳推理任务上显著超越当前最优方法，有效缓解上下文隧道问题。

Conclusion: Synapse为大模型代理提供了更符合认知机制的动态记忆范式，证明了基于激活的图检索比传统向量检索更适合长程、关联性强的推理任务。

Abstract: While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the "Contextual Tunneling" problem. Our code and data will be made publicly available upon acceptance.

</details>


### [90] [Window-based Membership Inference Attacks Against Fine-tuned Large Language Models](https://arxiv.org/abs/2601.02751)
*Yuetian Chen,Yuntao Du,Kaiyuan Zhang,Ashish Kundu,Charles Fleming,Bruno Ribeiro,Ninghui Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于滑动窗口的成员推断攻击方法WBC，通过在文本序列上使用不同大小的滑动窗口进行局部损失比较和符号聚合，显著提升了对微调大语言模型的成员推断效果，优于现有全局平均方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对大语言模型的成员推断攻击多依赖全局信号（如平均损失），但会稀释局部记忆信号，导致攻击效果下降；作者认为成员信号在局部上下文中更显著。

Method: 提出WBC（Window-Based Comparison）方法：在文本序列上滑动多种尺寸窗口，每个窗口基于目标模型与参考模型的损失比较进行二元投票；再对几何级数分布的窗口尺寸结果进行集成，以捕获从词元到短语层级的记忆模式。

Result: 在11个数据集上的实验表明，WBC显著超越现有基线方法，在AUC指标和低误报率下的检测率（提升2-3倍）上均表现更优。

Conclusion: 聚合局部证据比全局平均更有效，揭示了微调大语言模型中存在的关键隐私漏洞。

Abstract: Most membership inference attacks (MIAs) against Large Language Models (LLMs) rely on global signals, like average loss, to identify training data. This approach, however, dilutes the subtle, localized signals of memorization, reducing attack effectiveness. We challenge this global-averaging paradigm, positing that membership signals are more pronounced within localized contexts. We introduce WBC (Window-Based Comparison), which exploits this insight through a sliding window approach with sign-based aggregation. Our method slides windows of varying sizes across text sequences, with each window casting a binary vote on membership based on loss comparisons between target and reference models. By ensembling votes across geometrically spaced window sizes, we capture memorization patterns from token-level artifacts to phrase-level structures. Extensive experiments across eleven datasets demonstrate that WBC substantially outperforms established baselines, achieving higher AUC scores and 2-3 times improvements in detection rates at low false positive thresholds. Our findings reveal that aggregating localized evidence is fundamentally more effective than global averaging, exposing critical privacy vulnerabilities in fine-tuned LLMs.

</details>


### [91] [EComStage: Stage-wise and Orientation-specific Benchmarking for Large Language Models in E-commerce](https://arxiv.org/abs/2601.02752)
*Kaiyan Zhao,Zijie Meng,Zheyong Xie,Jin Duan,Yao Hu,Zuozhu Liu,Shaosheng Cao*

Main category: cs.CL

TL;DR: 本文提出了EComStage，一个面向电商场景的LLM智能体阶段式推理评估基准，覆盖感知、规划、执行三阶段，并兼顾顾客与商家双视角任务。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅关注最终任务完成度，忽视中间推理阶段，且缺乏对商家端场景的覆盖，难以支撑真实电商场景中LLM智能体的优化设计。

Method: 构建EComStage基准，涵盖感知、规划、执行三个阶段，包含7个多样化电商任务（含顾客与商家双视角），所有样本由人工标注并质检；对30+个参数量从1B到200B的开源与闭源LLM进行系统性评测。

Result: 揭示了不同LLM在各推理阶段及顾客/商家导向任务上的显著能力差异，提供了细粒度、可落地的性能洞察。

Conclusion: EComStage填补了电商领域LLM智能体阶段式、多视角评估的空白，为实际应用中的智能体设计与优化提供了可靠评估框架与实证依据。

Abstract: Large Language Model (LLM)-based agents are increasingly deployed in e-commerce applications to assist customer services in tasks such as product inquiries, recommendations, and order management. Existing benchmarks primarily evaluate whether these agents successfully complete the final task, overlooking the intermediate reasoning stages that are crucial for effective decision-making. To address this gap, we propose EComStage, a unified benchmark for evaluating agent-capable LLMs across the comprehensive stage-wise reasoning process: Perception (understanding user intent), Planning (formulating an action plan), and Action (executing the decision). EComStage evaluates LLMs through seven separate representative tasks spanning diverse e-commerce scenarios, with all samples human-annotated and quality-checked. Unlike prior benchmarks that focus only on customer-oriented interactions, EComStage also evaluates merchant-oriented scenarios, including promotion management, content review, and operational support relevant to real-world applications. We evaluate a wide range of over 30 LLMs, spanning from 1B to over 200B parameters, including open-source models and closed-source APIs, revealing stage/orientation-specific strengths and weaknesses. Our results provide fine-grained, actionable insights for designing and optimizing LLM-based agents in real-world e-commerce settings.

</details>


### [92] [MiMo-V2-Flash Technical Report](https://arxiv.org/abs/2601.02780)
*Bangjun Xiao,Bingquan Xia,Bo Yang,Bofei Gao,Bowen Shen,Chen Zhang,Chenhong He,Chiheng Lou,Fuli Luo,Gang Wang,Gang Xie,Hailin Zhang,Hanglong Lv,Hanyu Li,Heyu Chen,Hongshen Xu,Houbin Zhang,Huaqiu Liu,Jiangshan Duo,Jianyu Wei,Jiebao Xiao,Jinhao Dong,Jun Shi,Junhao Hu,Kainan Bao,Kang Zhou,Lei Li,Liang Zhao,Linghao Zhang,Peidian Li,Qianli Chen,Shaohui Liu,Shihua Yu,Shijie Cao,Shimao Chen,Shouqiu Yu,Shuo Liu,Tianling Zhou,Weijiang Su,Weikun Wang,Wenhan Ma,Xiangwei Deng,Bohan Mao,Bowen Ye,Can Cai,Chenghua Wang,Chengxuan Zhu,Chong Ma,Chun Chen,Chunan Li,Dawei Zhu,Deshan Xiao,Dong Zhang,Duo Zhang,Fangyue Liu,Feiyu Yang,Fengyuan Shi,Guoan Wang,Hao Tian,Hao Wu,Heng Qu,Hongfei Yi,Hongxu An,Hongyi Guan,Xing Zhang,Yifan Song,Yihan Yan,Yihao Zhao,Yingchun Lai,Yizhao Gao,Yu Cheng,Yuanyuan Tian,Yudong Wang,Zhen Tang,Zhengju Tang,Zhengtao Wen,Zhichao Song,Zhixian Zheng,Zihan Jiang,Jian Wen,Jiarui Sun,Jiawei Li,Jinlong Xue,Jun Xia,Kai Fang,Menghang Zhu,Nuo Chen,Qian Tu,Qihao Zhang,Qiying Wang,Rang Li,Rui Ma,Shaolei Zhang,Shengfan Wang,Shicheng Li,Shuhao Gu,Shuhuai Ren,Sirui Deng,Tao Guo,Tianyang Lu,Weiji Zhuang,Weikang Zhang,Weimin Xiong,Wenshan Huang,Wenyu Yang,Xin Zhang,Xing Yong,Xu Wang,Xueyang Xie,Yilin Jiang,Yixin Yang,Yongzhe He,Yu Tu,Yuanliang Dong,Yuchen Liu,Yue Ma,Yue Yu,Yuxing Xiang,Zhaojun Huang,Zhenru Lin,Zhipeng Xu,Zhiyang Chen,Zhonghua Deng,Zihan Zhang,Zihao Yue*

Main category: cs.CL

TL;DR: MiMo-V2-Flash 是一个309B参数、15B激活参数的MoE大模型，采用混合注意力（滑动窗口+全局）、多令牌预测预训练（27T tokens，原生32k上下文，扩展至256k），并提出多教师在线策略蒸馏（MOPD）提升后训练效率；推理中复用MTP实现高效推测解码（3.6平均接受长度，2.6倍加速），性能媲美DeepSeek-V3.2和Kimi-K2但参数更少，全部权重开源。


<details>
  <summary>Details</summary>
Motivation: 在保持强大推理与智能体能力的同时，降低大模型参数量与计算开销，并提升后训练效率与推理速度。

Method: 提出MiMo-V2-Flash模型：采用混合注意力（128-token滑动窗口 + 5:1全局/滑动比）、Multi-Token Prediction（MTP）预训练（27万亿token，32k→256k上下文扩展）；引入Multi-Teacher On-Policy Distillation（MOPD）框架，利用领域专用强化学习教师提供细粒度token级奖励进行知识蒸馏；推理阶段将MTP模块复用为draft model实现推测解码。

Result: 在参数量仅为DeepSeek-V3.2的1/2、Kimi-K2的1/3下，性能达到同等水平；推测解码实现最高3.6接受长度与2.6倍解码加速；开源全部模型权重及三层MTP权重。

Conclusion: MiMo-V2-Flash通过架构创新（混合注意力、MTP）、训练范式革新（MOPD）与推理优化（MTP复用），实现了高性能、高效率与强可扩展性的统一，推动开源大模型向轻量化、专业化与实用化发展。

Abstract: We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.

</details>


### [93] [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练、无需标签的短文本聚类方法，基于迭代向量更新和大语言模型（LLM）引导，在无先验簇数和标注数据的商业场景中表现优异，且具有模型无关性、低资源需求和良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在面向客户的聊天机器人等实际商业场景中，企业需对大量无标注的用户语句按意图聚类，但通常缺乏标注数据且簇数未知，现有方法难以适用。

Method: 提出一种训练-free、标签-free的短文本聚类方法：首先基于代表性文本构建稀疏向量，再通过LLM指导进行迭代向量更新；可适配任意嵌入模型、小型LLM及不同下游聚类算法。

Result: 在多个数据集上达到或超越依赖对比学习的SOTA方法性能，验证了其模型无关性、对小规模LLM的兼容性、对不同聚类算法的适应性，以及在大规模数据上的可扩展性和更低的LLM计算开销。

Conclusion: 该方法因无需训练与标注、不预设簇数、资源消耗低、易扩展，更契合真实世界中的短文本聚类需求，为无监督意图发现提供了实用新范式。

Abstract: In this paper, we propose a training-free and label-free method for short text clustering that can be used on top of any existing embedder. In the context of customer-facing chatbots, companies are dealing with large amounts of user utterances that need to be clustered according to their intent. In these commercial settings, no labeled data is typically available, and the number of clusters is not known. Our method is based on iterative vector updating: it constructs sparse vectors based on representative texts, and then iteratively refines them through LLM guidance. Our method achieves comparable or superior results to state-of-the-art methods that use contrastive learning, but without assuming prior knowledge of clusters or labels. Experiments on diverse datasets and smaller LLMs show that our method is model agnostic and can be applied to any embedder, with relatively small LLMs, and different clustering methods. We also show that our method scales to large datasets, reducing the computational cost of the LLM. These low-resource, adaptable settings and the scalability of our method make it more aligned with real-world scenarios than existing clustering methods.

</details>


### [94] [Punctuation-aware Hybrid Trainable Sparse Attention for Large Language Models](https://arxiv.org/abs/2601.02819)
*Junxiang Qiu,Shuo Wang,Zhengsu Chen,Hengheng Zhang,Jinda Lu,Changcheng Li,Qi Tian*

Main category: cs.CL

TL;DR: 本文提出了一种标点感知的混合稀疏注意力机制（PHSA），利用标点符号作为语义边界锚点，通过双分支聚合和极稀疏自适应策略，在保持低计算开销的同时显著减少长上下文建模中的信息损失。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法在块选择中依赖粗粒度语义表示，模糊了块内语义边界，导致关键信息丢失。

Method: 提出PHSA框架：(1) 设计融合全局语义与标点增强边界特征的双分支聚合机制；(2) 引入极端稀疏自适应训练与推理策略，以稳定极低token激活比下的模型行为。

Result: 在通用基准和长上下文评测中，PHSA持续优于密集注意力及SOTA稀疏注意力方法（如InfLLM v2）；在32k-token输入、97.3%稀疏率下，0.6B参数模型信息损失降低10.8%。

Conclusion: PHSA是一种可端到端训练、高效且鲁棒的稀疏注意力方法，能有效缓解长上下文建模中的信息损失问题。

Abstract: Attention serves as the fundamental mechanism for long-context modeling in large language models (LLMs), yet dense attention becomes structurally prohibitive for long sequences due to its quadratic complexity. Consequently, sparse attention has received increasing attention as a scalable alternative. However, existing sparse attention methods rely on coarse-grained semantic representations during block selection, which blur intra-block semantic boundaries and lead to the loss of critical information. To address this issue, we propose \textbf{P}unctuation-aware \textbf{H}ybrid \textbf{S}parse \textbf{A}ttention \textbf{(PHSA)}, a natively trainable sparse attention framework that leverages punctuation tokens as semantic boundary anchors. Specifically, (1) we design a dual-branch aggregation mechanism that fuses global semantic representations with punctuation-enhanced boundary features, preserving the core semantic structure while introducing almost no additional computational overhead; (2) we introduce an extreme-sparsity-adaptive training and inference strategy that stabilizes model behavior under very low token activation ratios; Extensive experiments on general benchmarks and long-context evaluations demonstrate that PHSA consistently outperforms dense attention and state-of-the-art sparse attention baselines, including InfLLM v2. Specifically, for the 0.6B-parameter model with 32k-token input sequences, PHSA can reduce the information loss by 10.8\% at a sparsity ratio of 97.3\%.

</details>


### [95] [The performances of the Chinese and U.S. Large Language Models on the Topic of Chinese Culture](https://arxiv.org/abs/2601.02830)
*Feiyan Liu,Chenxun Zhuo,Siyan Zhao,Bao Ge,Tianming Liu*

Main category: cs.CL

TL;DR: 本研究比较了中美开发的大语言模型（LLM）在中文文化理解任务上的表现，发现中国模型整体优于美国模型，可能源于训练数据分布、本地化策略及对中华文化内容重视程度的差异。


<details>
  <summary>Details</summary>
Motivation: 探究中美开发的大语言模型在中文语境下是否表现出文化差异，特别是对中国传统文化（历史、文学、诗词等）的理解能力。

Method: 采用直接提问范式，评估GPT-5.1、DeepSeek-V3.2、Qwen3-Max、Gemini2.5Pro等模型在中文文化相关问题上的表现，并进行中美模型间的对比分析。

Result: 中国开发的模型在中文文化理解任务上普遍优于美国开发的模型；其中Gemini 2.5Pro和GPT-5.1在美国模型中准确率相对较高。

Conclusion: LLM的文化表现受其开发背景影响显著，训练数据分布、本地化策略及文化内容侧重是造成性能差异的关键因素。

Abstract: Cultural backgrounds shape individuals' perspectives and approaches to problem-solving. Since the emergence of GPT-1 in 2018, large language models (LLMs) have undergone rapid development. To date, the world's ten leading LLM developers are primarily based in China and the United States. To examine whether LLMs released by Chinese and U.S. developers exhibit cultural differences in Chinese-language settings, we evaluate their performance on questions about Chinese culture. This study adopts a direct-questioning paradigm to evaluate models such as GPT-5.1, DeepSeek-V3.2, Qwen3-Max, and Gemini2.5Pro. We assess their understanding of traditional Chinese culture, including history, literature, poetry, and related domains. Comparative analyses between LLMs developed in China and the U.S. indicate that Chinese models generally outperform their U.S. counterparts on these tasks. Among U.S.-developed models, Gemini 2.5Pro and GPT-5.1 achieve relatively higher accuracy. The observed performance differences may potentially arise from variations in training data distribution, localization strategies, and the degree of emphasis on Chinese cultural content during model development.

</details>


### [96] [TiMem: Temporal-Hierarchical Memory Consolidation for Long-Horizon Conversational Agents](https://arxiv.org/abs/2601.02845)
*Kai Li,Xuanqing Yu,Ziyi Ni,Yi Zeng,Yao Xu,Zheqing Zhang,Xin Li,Jitao Sang,Xiaogang Duan,Xuelei Wang,Chengbao Liu,Jie Tan*

Main category: cs.CL

TL;DR: TiMem is a temporal-hierarchical memory framework for long-horizon conversational agents, using a Temporal Memory Tree (TMT) to consolidate conversation history into abstracted persona representations, achieving SOTA accuracy while reducing memory length by over 52%.


<details>
  <summary>Details</summary>
Motivation: Long-horizon conversational agents face challenges due to finite LLM context windows and lack of temporally structured, hierarchical memory support, leading to fragmented memories and unstable personalization.

Method: TiMem introduces a Temporal Memory Tree (TMT) for hierarchical memory organization, semantic-guided consolidation without fine-tuning, and complexity-aware recall balancing precision and efficiency.

Result: TiMem achieves 75.30% on LoCoMo and 76.88% on LongMemEval-S, outperforms all baselines, reduces recalled memory length by 52.20% on LoCoMo, and shows improved persona separation and reduced dispersion in manifold analysis.

Conclusion: TiMem establishes temporal continuity as a foundational principle for long-horizon memory in conversational agents, enabling scalable, structured, and personalized interactions.

Abstract: Long-horizon conversational agents have to manage ever-growing interaction histories that quickly exceed the finite context windows of large language models (LLMs). Existing memory frameworks provide limited support for temporally structured information across hierarchical levels, often leading to fragmented memories and unstable long-horizon personalization. We present TiMem, a temporal--hierarchical memory framework that organizes conversations through a Temporal Memory Tree (TMT), enabling systematic memory consolidation from raw conversational observations to progressively abstracted persona representations. TiMem is characterized by three core properties: (1) temporal--hierarchical organization through TMT; (2) semantic-guided consolidation that enables memory integration across hierarchical levels without fine-tuning; and (3) complexity-aware memory recall that balances precision and efficiency across queries of varying complexity. Under a consistent evaluation setup, TiMem achieves state-of-the-art accuracy on both benchmarks, reaching 75.30% on LoCoMo and 76.88% on LongMemEval-S. It outperforms all evaluated baselines while reducing the recalled memory length by 52.20% on LoCoMo. Manifold analysis indicates clear persona separation on LoCoMo and reduced dispersion on LongMemEval-S. Overall, TiMem treats temporal continuity as a first-class organizing principle for long-horizon memory in conversational agents.

</details>


### [97] [To Generate or Discriminate? Methodological Considerations for Measuring Cultural Alignment in LLMs](https://arxiv.org/abs/2601.02858)
*Saurabh Kumar Pandey,Sougata Saha,Monojit Choudhury*

Main category: cs.CL

TL;DR: 本文提出逆社会人口统计提示（ISDP）方法，通过让大语言模型（LLM）从真实或模拟用户行为中反推其人口统计标签，以更可靠地评估其文化理解能力，避免传统社会人口统计提示（SDP）中因提示敏感性、解码参数等带来的混淆；在Goodreads-CSI数据集上对四类LLM的实验表明，模型对真实行为识别优于模拟行为，但在个体层面二者性能趋同，揭示个性化建模的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统社会人口统计提示（SDP）易受提示敏感性、解码参数及生成任务固有难度等混淆因素影响，难以区分LLM表现差是源于偏见还是任务设计缺陷，亟需更鲁棒的文化能力评估方法。

Method: 提出逆社会人口统计提示（ISDP），即让LLM根据真实或模拟的用户行为（如书评）预测其对应的社会人口统计标签（如国籍）；在Goodreads-CSI数据集（含印度、墨西哥、美国用户英文书评）上，对Aya-23、Gemma-2、GPT-4o和LLaMA-3.1四个模型进行评估。

Result: 模型在识别真实用户行为时表现优于模拟行为，与SDP结论相反；但在个体粒度上，两类行为的识别性能均下降且趋于一致，表明模型个性化文化理解能力存在瓶颈。

Conclusion: ISDP是一种更稳健的文化能力评估范式，能规避SDP的混淆问题；实验揭示当前LLM虽具备一定群体级文化判别能力，但难以实现细粒度个体级文化适配，凸显个性化建模的挑战。

Abstract: Socio-demographic prompting (SDP) - prompting Large Language Models (LLMs) using demographic proxies to generate culturally aligned outputs - often shows LLM responses as stereotypical and biased. While effective in assessing LLMs' cultural competency, SDP is prone to confounding factors such as prompt sensitivity, decoding parameters, and the inherent difficulty of generation over discrimination tasks due to larger output spaces. These factors complicate interpretation, making it difficult to determine if the poor performance is due to bias or the task design. To address this, we use inverse socio-demographic prompting (ISDP), where we prompt LLMs to discriminate and predict the demographic proxy from actual and simulated user behavior from different users. We use the Goodreads-CSI dataset (Saha et al., 2025), which captures difficulty in understanding English book reviews for users from India, Mexico, and the USA, and test four LLMs: Aya-23, Gemma-2, GPT-4o, and LLaMA-3.1 with ISDP. Results show that models perform better with actual behaviors than simulated ones, contrary to what SDP suggests. However, performance with both behavior types diminishes and becomes nearly equal at the individual level, indicating limits to personalization.

</details>


### [98] [Limited Linguistic Diversity in Embodied AI Datasets](https://arxiv.org/abs/2601.03136)
*Selma Wanna,Agnes Luhtaru,Jonathan Salfity,Ryan Barron,Juston Moore,Cynthia Matuszek,Mitch Pryor*

Main category: cs.CL

TL;DR: 本文对多个广泛使用的视觉-语言-动作（VLA）数据集进行了系统性语言特征审计，发现其指令普遍存在重复性高、句法结构单一、词汇和语义多样性不足等问题，旨在推动更规范的数据集报告、选择与增强。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型依赖语言输入，但训练与评测所用数据集的语言特性缺乏系统记录与分析。

Method: 从词汇多样性、指令重复与重叠、语义相似性、句法复杂度等互补维度，对主流VLA数据集进行量化语言审计。

Result: 多数VLA数据集指令高度模板化、结构变化少，语言分布狭窄，缺乏真实语言的丰富性。

Conclusion: 该研究为VLA数据集提供了描述性语言特征文档，有助于提升数据集报告质量、指导数据选择，并支持针对性的语言覆盖扩展策略。

Abstract: Language plays a critical role in Vision-Language-Action (VLA) models, yet the linguistic characteristics of the datasets used to train and evaluate these systems remain poorly documented. In this work, we present a systematic dataset audit of several widely used VLA corpora, aiming to characterize what kinds of instructions these datasets actually contain and how much linguistic variety they provide. We quantify instruction language along complementary dimensions-including lexical variety, duplication and overlap, semantic similarity, and syntactic complexity. Our analysis shows that many datasets rely on highly repetitive, template-like commands with limited structural variation, yielding a narrow distribution of instruction forms. We position these findings as descriptive documentation of the language signal available in current VLA training and evaluation data, intended to support more detailed dataset reporting, more principled dataset selection, and targeted curation or augmentation strategies that broaden language coverage.

</details>


### [99] [Training Language Models with homotokens Leads to Delayed Overfitting](https://arxiv.org/abs/2601.02867)
*Adrian Cosma,Stefan Ruseti,Emilian Radoi,Mihai Dascalu*

Main category: cs.CL

TL;DR: 本文提出homotokens（同义子词）作为数据增强方法，通过在训练中引入同一词汇的不同合法子词切分方式，提升语言模型对子词切分的鲁棒性与泛化能力，无需修改模型结构或训练目标。


<details>
  <summary>Details</summary>
Motivation: 子词切分存在非唯一性：多个不同token序列可对应相同表面形式且语义不变，但模型通常仅用单一最长前缀切分进行训练，忽略了这种语义等价的多样性。

Method: 定义homotokens为同一词汇的多种合法、语义不变的子词切分；设计轻量架构，利用辅助因果编码器和块因果交叉注意力，在不改变原始训练目标和token接口的前提下，使模型在预测下一个token时条件化于采样的homotoken变体。

Result: 在数据受限的预训练中，homotoken增强能持续延缓过拟合并提升多任务泛化性能；在多语言微调中，其效果依赖分词器质量：分词器越压缩（切分越粗），增益越显著；若已过度切分，则增益减弱。

Conclusion: homotokens是一种简单、模块化的机制，可有效提升语言模型对子词切分的不变性，增强鲁棒性与泛化能力。

Abstract: Subword tokenization introduces a computational layer in language models where many distinct token sequences decode to the same surface form and preserve meaning, yet induce different internal computations. Despite this non-uniqueness, language models are typically trained using a single canonical longest-prefix tokenization. We formalize homotokens-alternative valid subword segmentations of the same lexical item-as a strictly meaning-preserving form of data augmentation. We introduce a lightweight training architecture that conditions canonical next-token prediction on sampled homotoken variants via an auxiliary causal encoder and block-causal cross-attention, without modifying the training objective or token interface. In data-constrained pretraining, homotoken augmentation consistently delays overfitting under repeated data exposure and improves generalization across diverse evaluation datasets. In multilingual fine-tuning, we find that the effectiveness of homotokens depends on tokenizer quality: gains are strongest when canonical tokens are highly compressed and diminish when the tokenizer already over-fragments the input. Overall, homotokens provide a simple and modular mechanism for inducing tokenization invariance in language models.

</details>


### [100] [LongBench Pro: A More Realistic and Comprehensive Bilingual Long-Context Evaluation Benchmark](https://arxiv.org/abs/2601.02872)
*Ziyang Chen,Xing Wu,Junlong Jia,Chaochen Gao,Qi Fu,Debing Zhang,Songlin Hu*

Main category: cs.CL

TL;DR: 本文提出了LongBench Pro，一个更真实、全面的双语长上下文评测基准，包含1500个自然产生的长文本样本（8k–256k tokens），覆盖11个主任务和25个子任务，并引入人机协同构建流程与多维分析框架；实验揭示了长上下文优化比参数扩展更重要、实际有效上下文长度常低于宣称长度且存在跨语言偏差、以及‘思考’范式对原生推理训练模型更有效等关键发现。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评测基准在可扩展性与真实性之间难以兼顾：合成任务缺乏现实复杂性，而全人工标注难以扩展至超长文本和多样场景。

Method: 提出LongBench Pro基准，含1500个英汉双语自然长文本样本（8k–256k tokens），覆盖11主任务/25子任务；设计多维分类体系（依赖类型、长度六级、难度四级）；采用人机协同构建流程（前沿LLM生成题目与解析，专家验证与修正）。

Result: 在46个主流长上下文LLM上评估得出三点发现：(1) 长上下文优化对理解能力提升作用大于参数缩放；(2) 实际有效上下文长度普遍短于宣称长度，且中英文间存在显著不对齐；(3) ‘思考’范式主要利好原生推理训练模型，混合思考设计提供更优帕累托权衡。

Conclusion: LongBench Pro为推进长上下文理解提供了更真实、可扩展、细粒度的评测平台，其构建方法与实证发现对模型研发与评估具有重要指导意义。

Abstract: The rapid expansion of context length in large language models (LLMs) has outpaced existing evaluation benchmarks. Current long-context benchmarks often trade off scalability and realism: synthetic tasks underrepresent real-world complexity, while fully manual annotation is costly to scale to extreme lengths and diverse scenarios. We present LongBench Pro, a more realistic and comprehensive bilingual benchmark of 1,500 naturally occurring long-context samples in English and Chinese spanning 11 primary tasks and 25 secondary tasks, with input lengths from 8k to 256k tokens. LongBench Pro supports fine-grained analysis with task-specific metrics and a multi-dimensional taxonomy of context requirement (full vs. partial dependency), length (six levels), and difficulty (four levels calibrated by model performance). To balance quality with scalability, we propose a Human-Model Collaborative Construction pipeline: frontier LLMs draft challenging questions and reference answers, along with design rationales and solution processes, to reduce the cost of expert verification. Experts then rigorously validate correctness and refine problematic cases. Evaluating 46 widely used long-context LLMs on LongBench Pro yields three findings: (1) long-context optimization contributes more to long-context comprehension than parameter scaling; (2) effective context length is typically shorter than the claimed context length, with pronounced cross-lingual misalignment; and (3) the "thinking" paradigm helps primarily models trained with native reasoning, while mixed-thinking designs offer a promising Pareto trade-off. In summary, LongBench Pro provides a robust testbed for advancing long-context understanding.

</details>


### [101] [Revisiting Data Compression with Language Modeling](https://arxiv.org/abs/2601.02875)
*Chen-Han Tsai*

Main category: cs.CL

TL;DR: 本文探讨了大语言模型（LLM）在数据压缩任务中的应用，尤其关注其在文本、多模态及非英语/代码/字节流等数据上的压缩性能；在无需额外训练的前提下，在enwik9数据集上实现了约18%的SOTA调整压缩率。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在数据压缩中展现出潜力，但其能否实际替代传统压缩算法仍存在若干实践挑战，如压缩率、泛化性与适用数据类型等。

Method: 探索多种基于LLM的无训练或轻量配置压缩方法，评估其在enwik9、非英语文本、代码和字节流等不同数据类型上的调整压缩率。

Result: 在enwik9上达成约18%的SOTA调整压缩率（无需额外训练）；验证LLM在文本主导数据上优势显著，在非自然文本（如代码、字节流）上经合理配置仍具竞争力。

Conclusion: LLM作为通用数据压缩器具有可行性与扩展潜力，尤其在文本类数据上表现突出，但需进一步优化以适配更广泛的数据形态和实际部署需求。

Abstract: In this report, we investigate the potential use of large language models (LLM's) in the task of data compression. Previous works have demonstrated promising results in applying LLM's towards compressing not only text, but also a wide range of multi-modal data. Despite the favorable performance achieved, there still remains several practical questions that pose a challenge towards replacing existing data compression algorithms with LLM's. In this work, we explore different methods to achieve a lower adjusted compression rate using LLM's as data compressors. In comparison to previous works, we were able to achieve a new state-of-the-art (SOTA) adjusted compression rate of around $18\%$ on the enwik9 dataset without additional model training. Furthermore, we explore the use of LLM's in compressing non-English data, code data, byte stream sequences. We show that while LLM's excel in compressing data in text-dominant domains, their ability in compressing non-natural text sequences still remain competitive if configured in the right way.

</details>


### [102] [Transparent Semantic Change Detection with Dependency-Based Profiles](https://arxiv.org/abs/2601.02891)
*Bach Phan-Tat,Kris Heylen,Dirk Geeraerts,Stefano De Pascale,Dirk Speelman*

Main category: cs.CL

TL;DR: 本文提出了一种基于依存共现模式的可解释方法用于词汇语义变化检测，并证明其效果优于部分分布语义模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于嵌入的语义变化检测方法性能强但缺乏可解释性，本文旨在探索更透明、可解释的替代方案。

Method: 提出一种纯基于词的依存共现模式的方法，不依赖词嵌入或神经网络。

Result: 该方法在语义变化检测任务上有效，且表现优于多个分布语义模型；预测结果经定量与定性分析验证为合理且可解释。

Conclusion: 依存共现模式是一种有效、可解释的语义变化检测途径，为替代黑箱嵌入方法提供了可行方案。

Abstract: Most modern computational approaches to lexical semantic change detection (LSC) rely on embedding-based distributional word representations with neural networks. Despite the strong performance on LSC benchmarks, they are often opaque. We investigate an alternative method which relies purely on dependency co-occurrence patterns of words. We demonstrate that it is effective for semantic change detection and even outperforms a number of distributional semantic models. We provide an in-depth quantitative and qualitative analysis of the predictions, showing that they are plausible and interpretable.

</details>


### [103] [Linear Script Representations in Speech Foundation Models Enable Zero-Shot Transliteration](https://arxiv.org/abs/2601.02906)
*Ryan Soh-Eun Shim,Kwanghee Choi,Kalvin Chang,Ming-Hao Hsu,Florian Eichin,Zhizheng Wu,Alane Suhr,Michael A. Hedderich,David Harwath,David R. Mortensen,Barbara Plank*

Main category: cs.CL

TL;DR: 本文提出了一种在多语言语音模型（如Whisper）中通过修改激活向量来实现输出文字脚本（script）可控的方法，发现脚本信息在线性可分的激活空间中编码，并可在推理时通过添加脚本向量实现跨语言-脚本组合的脚本切换。


<details>
  <summary>Details</summary>
Motivation: 多语言语音模型（如Whisper）训练数据包含同一语言的不同地区变体，而这些变体常使用不同文字系统（脚本），导致语音识别输出脚本不一致、不可控。

Method: 分析Whisper等模型激活空间中脚本的线性可分性，提取脚本方向向量，并在推理时将该向量加到中间层激活上以控制输出脚本。

Result: 在Whisper各尺寸模型上均验证了该方法的有效性：能稳定诱导脚本切换（包括非常规配对，如意大利语用西里尔字母、日语用拉丁字母），且性能与基线相当。

Conclusion: 脚本信息在多语言语音模型激活空间中呈线性结构，支持无需重训练的后置脚本控制，为语音识别输出标准化和本地化提供了新路径。

Abstract: Multilingual speech foundation models such as Whisper are trained on web-scale data, where data for each language consists of a myriad of regional varieties. However, different regional varieties often employ different scripts to write the same language, rendering speech recognition output also subject to non-determinism in the output script. To mitigate this problem, we show that script is linearly encoded in the activation space of multilingual speech models, and that modifying activations at inference time enables direct control over output script. We find the addition of such script vectors to activations at test time can induce script change even in unconventional language-script pairings (e.g. Italian in Cyrillic and Japanese in Latin script). We apply this approach to inducing post-hoc control over the script of speech recognition output, where we observe competitive performance across all model sizes of Whisper.

</details>


### [104] [Beyond the Black Box: Theory and Mechanism of Large Language Models](https://arxiv.org/abs/2601.02907)
*Zeyu Gan,Ruifeng Ren,Wei Yao,Xiaolin Hu,Gengze Xu,Chen Qian,Huayi Tang,Zixuan Gong,Xinhao Yao,Pengwei Tang,Zhenxing Dou,Yong Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于生命周期的统一分类法，将大语言模型（LLM）研究划分为数据准备、模型准备、训练、对齐、推理和评估六个阶段，并系统梳理各阶段的基础理论与内在机制，旨在推动LLM从工程实践迈向理论驱动的科学学科。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在实践中取得了巨大成功，但其理论基础仍十分薄弱，被广泛视为‘黑箱’，亟需统一框架来整合碎片化的理论研究。

Method: 构建基于LLM全生命周期的六阶段分类体系（数据准备、模型准备、训练、对齐、推理、评估），并在此框架下系统综述各阶段的核心理论问题与机制分析。

Result: 明确了当前理论研究的关键议题（如数据混合的数学依据、架构表征极限、对齐优化动力学）及前沿挑战（如合成数据自提升的理论极限、安全保证的数学边界、智能涌现的机制起源）。

Conclusion: 该综述为LLM研究提供了结构化理论图谱，有助于弥合理论与实践鸿沟，推动LLM发展成为一门原理清晰、可验证、可预测的科学学科。

Abstract: The rapid emergence of Large Language Models (LLMs) has precipitated a profound paradigm shift in Artificial Intelligence, delivering monumental engineering successes that increasingly impact modern society. However, a critical paradox persists within the current field: despite the empirical efficacy, our theoretical understanding of LLMs remains disproportionately nascent, forcing these systems to be treated largely as ``black boxes''. To address this theoretical fragmentation, this survey proposes a unified lifecycle-based taxonomy that organizes the research landscape into six distinct stages: Data Preparation, Model Preparation, Training, Alignment, Inference, and Evaluation. Within this framework, we provide a systematic review of the foundational theories and internal mechanisms driving LLM performance. Specifically, we analyze core theoretical issues such as the mathematical justification for data mixtures, the representational limits of various architectures, and the optimization dynamics of alignment algorithms. Moving beyond current best practices, we identify critical frontier challenges, including the theoretical limits of synthetic data self-improvement, the mathematical bounds of safety guarantees, and the mechanistic origins of emergent intelligence. By connecting empirical observations with rigorous scientific inquiry, this work provides a structured roadmap for transitioning LLM development from engineering heuristics toward a principled scientific discipline.

</details>


### [105] [RAL2M: Retrieval Augmented Learning-To-Match Against Hallucination in Compliance-Guaranteed Service Systems](https://arxiv.org/abs/2601.02917)
*Mengze Hong,Di Jiang,Jiangtao Wen,Zhiyang Su,Yawen Li,Yanjie Sun,Guan Wang,Chen Jason Zhang*

Main category: cs.CL

TL;DR: 本文提出RAL2M框架，将大语言模型（LLM）作为检索系统中的查询-响应匹配判别器，避免生成式幻觉；并设计查询自适应的潜在集成策略，建模多个LLM的能力差异与依赖关系，实现校准后的共识决策，在大规模基准上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在服务系统中存在严重幻觉问题，需通过显式知识 grounding 保障响应合规性。

Method: 提出检索增强型学习匹配（RAL2M）框架，将LLM用作检索系统中的查询-响应匹配判别器；进一步引入查询自适应的潜在集成策略，建模LLM间异质能力与相互依赖，生成校准共识决策。

Result: 在大规模基准测试中，该方法有效利用‘群体智慧’，显著优于强基线方法。

Conclusion: RAL2M为抑制LLM幻觉提供了稳健的检索式替代方案；潜在表征建模是未来值得深入探索的方向。

Abstract: Hallucination is a major concern in LLM-driven service systems, necessitating explicit knowledge grounding for compliance-guaranteed responses. In this paper, we introduce Retrieval-Augmented Learning-to-Match (RAL2M), a novel framework that eliminates generation hallucination by repositioning LLMs as query-response matching judges within a retrieval-based system, providing a robust alternative to purely generative approaches. To further mitigate judgment hallucination, we propose a query-adaptive latent ensemble strategy that explicitly models heterogeneous model competence and interdependencies among LLMs, deriving a calibrated consensus decision. Extensive experiments on large-scale benchmarks demonstrate that the proposed method effectively leverages the "wisdom of the crowd" and significantly outperforms strong baselines. Finally, we discuss best practices and promising directions for further exploiting latent representations in future work.

</details>


### [106] [Memorization, Emergence, and Explaining Reversal Failures: A Controlled Study of Relational Semantics in LLMs](https://arxiv.org/abs/2601.02931)
*Yihua Zhu,Qianying Liu,Jiaxin Wang,Fei Cheng,Chaoran Liu,Akiko Aizawa,Sadao Kurohashi,Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱的合成框架，用于研究自回归大语言模型（LLMs）是否真正学习了关系词的逻辑语义（如对称性、逆关系），并通过从头训练GPT式模型进行验证；结果表明：关系语义可在少量逻辑监督下涌现，且反转失败主要源于自回归的顺序偏差，而非语义缺失。


<details>
  <summary>Details</summary>
Motivation: 探究自回归大语言模型是否真正习得关系词的逻辑语义（如对称性、逆关系），以及其在关系反转任务中的失败根源是语义缺失还是自回归的左到右顺序偏差。

Method: 构建基于知识图谱的可控合成框架，生成具有对称/逆关系的三元组文本；从零训练GPT风格自回归模型；评估其在记忆、逻辑推理和对未见实体的上下文泛化能力。

Result: 发现关系语义在足够逻辑监督下会出现突变式涌现（即使在2-3层浅层模型中）；成功泛化与中间层稳定信号相关；前向/反向顺序匹配测试及扩散基线表明，反转失败主因是自回归顺序偏差，而非逆关系语义缺失。

Conclusion: 自回归LLMs能在适度逻辑监督下习得基本关系逻辑语义；其反转类错误本质上是建模结构（顺序生成）导致的偏差，而非语义表征缺陷。

Abstract: Autoregressive LLMs perform well on relational tasks that require linking entities via relational words (e.g., father/son, friend), but it is unclear whether they learn the logical semantics of such relations (e.g., symmetry and inversion logic) and, if so, whether reversal-type failures arise from missing relational semantics or left-to-right order bias. We propose a controlled Knowledge Graph-based synthetic framework that generates text from symmetric/inverse triples, train GPT-style autoregressive models from scratch, and evaluate memorization, logical inference, and in-context generalization to unseen entities to address these questions. We find a sharp phase transition in which relational semantics emerge with sufficient logic-bearing supervision, even in shallow (2-3 layer) models, and that successful generalization aligns with stable intermediate-layer signals. Finally, order-matched forward/reverse tests and a diffusion baseline indicate that reversal failures are primarily driven by autoregressive order bias rather than deficient inversion semantics.

</details>


### [107] [Pearmut: Human Evaluation of Translation Made Trivial](https://arxiv.org/abs/2601.02933)
*Vilém Zouhar,Tom Kocmi*

Main category: cs.CL

TL;DR: 本文介绍了Pearmut，一个轻量级但功能丰富的平台，旨在简化多语言自然语言处理（NLP）中的人类评估流程，使其像自动评估一样易于运行。


<details>
  <summary>Details</summary>
Motivation: 人类评估是多语言NLP的金标准，但由于现有工具设置复杂、耗时且工程与运维开销大，实践中常被自动指标替代。

Method: 设计并实现了Pearmut平台，支持标准评估协议（如DA、ESA、MQM）及可扩展的新协议原型；集成文档级上下文、绝对与对比评估、注意力检查、ESAAI预标注、静态与主动学习分配策略等功能。

Result: Pearmut显著降低了人类评估的门槛，使可靠的人类评估成为模型开发与诊断中常规、实用的环节。

Conclusion: Pearmut为多语言NLP（尤其是机器翻译）提供了高效、灵活、易用的人类评估解决方案，推动人类评估从偶发性工作转变为日常实践。

Abstract: Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.

</details>


### [108] [Enhancing Multilingual RAG Systems with Debiased Language Preference-Guided Query Fusion](https://arxiv.org/abs/2601.02956)
*Jeonghyun Park,Byeongjeong Kim,Seojin Hwang,Hwanhee Lee*

Main category: cs.CL

TL;DR: 本文揭示了多语言检索增强生成（mRAG）系统中所谓‘英语偏好’实为评估基准中的结构性偏差所致，提出去偏指标DeLP，并基于单语对齐新发现设计轻量框架DELTA，显著提升跨语言性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究将mRAG系统的英语偏好归因于大模型的英语中心性，但作者指出该结论受评估基准中暴露偏差、黄金答案可用性偏差和文化主题偏差等结构性先验严重扭曲。

Method: 提出去偏语言偏好指标DeLP以校正评估偏差；通过DeLP分析发现检索器本质偏好查询与文档的单语对齐；据此设计轻量mRAG框架DELTA，利用单语对齐优化跨语言检索与生成。

Result: DeLP分析表明，先前报告的英语偏好主要源于证据分布偏差，而非模型固有偏见；DELTA在多种语言上持续优于英语中转及主流mRAG基线。

Conclusion: mRAG中的语言偏好问题本质是评估方法缺陷与检索机制特性共同导致，而非LLM本身英语偏向；强调需在公平基准下重新审视多语言能力，并倡导基于单语对齐的设计范式。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems often exhibit a perceived preference for high-resource languages, particularly English, resulting in the widespread adoption of English pivoting. While prior studies attribute this advantage to the superior English-centric capabilities of Large Language Models (LLMs), we find that such measurements are significantly distorted by structural priors inherent in evaluation benchmarks. Specifically, we identify exposure bias and a gold availability prior-both driven by the disproportionate concentration of resources in English-as well as cultural priors rooted in topic locality, as factors that hinder accurate assessment of genuine language preference. To address these biases, we propose DeLP (Debiased Language Preference), a calibrated metric designed to explicitly factor out these structural confounds. Our analysis using DeLP reveals that the previously reported English preference is largely a byproduct of evidence distribution rather than an inherent model bias. Instead, we find that retrievers fundamentally favor monolingual alignment between the query and the document language. Building on this insight, we introduce DELTA (DEbiased Language preference-guided Text Augmentation), a lightweight and efficient mRAG framework that strategically leverages monolingual alignment to optimize cross-lingual retrieval and generation. Experimental results demonstrate that DELTA consistently outperforms English pivoting and mRAG baselines across diverse languages.

</details>


### [109] [LLM-Augmented Changepoint Detection: A Framework for Ensemble Detection and Automated Explanation](https://arxiv.org/abs/2601.02957)
*Fabian Lukassen,Christoph Weisser,Michael Schlee,Manish Kumar,Anton Thielmann,Benjamin Saefken,Thomas Kneib*

Main category: cs.CL

TL;DR: 本文提出了一种结合集成统计方法与大语言模型（LLMs）的新型变点检测框架，旨在提升时间序列中变点检测的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有变点检测方法中两个关键问题：一是单一方法性能依赖数据特性，难以普适选择；二是缺乏对检测结果的自动化、上下文相关的解释。

Method: 集成10种不同的变点检测算法，并引入LLM驱动的解释流水线；对私有或领域数据，采用检索增强生成（RAG）技术实现基于用户文档的定制化解释。

Result: 该集成方法在准确性和鲁棒性上优于各单一算法；LLM解释模块能自动关联变点与真实历史事件，RAG支持领域知识注入；开源Python框架已在金融、政治学和环境科学中验证有效。

Conclusion: 该框架将统计检测结果转化为可操作的业务洞察，显著提升了变点分析的实用性与可解释性，为分析师和决策者提供有力支持。

Abstract: This paper introduces a novel changepoint detection framework that combines ensemble statistical methods with Large Language Models (LLMs) to enhance both detection accuracy and the interpretability of regime changes in time series data. Two critical limitations in the field are addressed. First, individual detection methods exhibit complementary strengths and weaknesses depending on data characteristics, making method selection non-trivial and prone to suboptimal results. Second, automated, contextual explanations for detected changes are largely absent. The proposed ensemble method aggregates results from ten distinct changepoint detection algorithms, achieving superior performance and robustness compared to individual methods. Additionally, an LLM-powered explanation pipeline automatically generates contextual narratives, linking detected changepoints to potential real-world historical events. For private or domain-specific data, a Retrieval-Augmented Generation (RAG) solution enables explanations grounded in user-provided documents. The open source Python framework demonstrates practical utility in diverse domains, including finance, political science, and environmental science, transforming raw statistical output into actionable insights for analysts and decision-makers.

</details>


### [110] [Low-Resource Heuristics for Bahnaric Optical Character Recognition Improvement](https://arxiv.org/abs/2601.02965)
*Phat Tran,Phuoc Pham,Hung Trinh,Tho Quan*

Main category: cs.CL

TL;DR: 本文提出了一种结合表格与非表格检测及基于概率的后处理启发式方法，以提升濒危少数民族语言Bahnar文档OCR识别准确率，准确率从72.86%提升至79.26%。


<details>
  <summary>Details</summary>
Motivation: Bahnar语缺乏研究和数据，其纸质文档因图像质量差（如破损、模糊）导致OCR识别错误多，影响信息检索，亟需高精度OCR方案。

Method: 融合先进表格与非表格检测技术，并在OCR输出上应用基于概率的后处理纠错启发式方法。

Result: OCR识别准确率从72.86%显著提升至79.26%。

Conclusion: 该方法不仅助力Bahnar语保护，也为其他少数民族语言数字化提供了可复用框架。

Abstract: Bahnar, a minority language spoken across Vietnam, Cambodia, and Laos, faces significant preservation challenges due to limited research and data availability. This study addresses the critical need for accurate digitization of Bahnar language documents through optical character recognition (OCR) technology. Digitizing scanned paper documents poses significant challenges, as degraded image quality from broken or blurred areas introduces considerable OCR errors that compromise information retrieval systems. We propose a comprehensive approach combining advanced table and non-table detection techniques with probability-based post-processing heuristics to enhance recognition accuracy. Our method first applies detection algorithms to improve input data quality, then employs probabilistic error correction on OCR output. Experimental results indicate a substantial improvement, with recognition accuracy increasing from 72.86% to 79.26%. This work contributes valuable resources for Bahnar language preservation and provides a framework applicable to other minority language digitization efforts.

</details>


### [111] [Reliability-Aware Adaptive Self-Consistency for Efficient Sampling in LLM Reasoning](https://arxiv.org/abs/2601.02970)
*Junseok Kim,Nakyeong Yang,Kyungmin Min,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出了一种可靠性感知的自一致性方法（ReASC），通过将自适应采样从响应计数转向证据充分性，并利用响应级置信度进行信息聚合，从而在保持准确性的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有自一致性方法虽能提升推理可靠性，但推理开销大；自适应方法依赖于等同对待所有响应的计数型停止规则，导致冗余采样。

Method: ReASC包含两个阶段：单样本决策阶段（对高置信度问题直接给出答案）和可靠性感知累积阶段（联合利用响应频率与置信度进行聚合）。

Result: 在五个模型和四个数据集上，ReASC始终取得最优的准确率-成本权衡效果；在Gemma-3-4B-it上处理GSM8K时，相较标准自一致性可降低最多70%推理成本且精度不变。

Conclusion: ReASC通过引入响应级置信度驱动的证据充分性判断，有效提升了自一致性方法的推理效率，适用于不同规模的大语言模型。

Abstract: Self-Consistency improves reasoning reliability through multi-sample aggregation, but incurs substantial inference cost. Adaptive self-consistency methods mitigate this issue by adjusting the sampling budget; however, they rely on count-based stopping rules that treat all responses equally, often leading to unnecessary sampling. We propose Reliability-Aware Adaptive Self-Consistency (ReASC), which addresses this limitation by reframing adaptive sampling from response counting to evidence sufficiency, leveraging response-level confidence for principled information aggregation. ReASC operates in two stages: a single-sample decision stage that resolves instances confidently answerable from a single response, and a reliability-aware accumulation stage that aggregates responses by jointly leveraging their frequency and confidence. Across five models and four datasets, ReASC consistently achieves the best accuracy-cost trade-off compared to existing baselines, yielding improved inference efficiency across model scales from 3B to 27B parameters. As a concrete example, ReASC reduces inference cost by up to 70\% relative to self-consistency while preserving accuracy on GSM8K using Gemma-3-4B-it.

</details>


### [112] [Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning](https://arxiv.org/abs/2601.02972)
*Nathanaël Carraz Rakotonirina,Ren Pang,Neha Anna John,Michael Bohlke-Schneider,Momchil Hardalov*

Main category: cs.CL

TL;DR: 本文提出了一种多阶段高效推理方法，通过监督微调（如拒绝采样或推理轨迹重格式化）结合带自适应长度惩罚的强化学习，减少大语言模型链式思维（CoT）中的冗余生成（即'过度思考'），在显著压缩响应长度的同时仅带来轻微准确率下降。


<details>
  <summary>Details</summary>
Motivation: 链式思维（CoT）虽提升LLM推理能力，但常导致冗余、低效甚至有害的长推理路径（'过度思考'），增加计算开销且未必提升性能。

Method: 结合监督微调（拒绝采样/推理轨迹重格式化）与强化学习；设计轻量级奖励函数：对首个正确答案后的生成token施加惩罚，仅在有益时鼓励自我验证；引入自适应长度惩罚机制。

Result: 在7个推理任务上，8B和32B模型响应长度分别平均减少28%和40%，准确率仅下降1.6和2.5分；AUC_OAA达76.6，比基线高5分、比次优方法高2.5分。

Conclusion: 该方法以概念简洁性实现了优于现有复杂高效推理方法的精度-长度权衡，有效缓解过度思考问题。

Abstract: The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking''. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy-response length trade-off. Our approach reduces response length by an average of 28\% for 8B models and 40\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\text{AUC}_{\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach.

</details>


### [113] [Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders](https://arxiv.org/abs/2601.02978)
*Ruikang Zhang,Shuo Wang,Qi Su*

Main category: cs.CL

TL;DR: 本文提出了一种基于稀疏自编码器的框架，用于识别和调控大语言模型中与高层语义行为（如人格特质）相关的可解释内部特征，实现了比现有方法更稳定、精准的行为控制，并发现‘功能忠实性’现象。


<details>
  <summary>Details</summary>
Motivation: 现有机制可解释性研究虽能识别内部特征，但难以将其可靠地关联到复杂、行为级的语义属性控制上。

Method: 提出基于稀疏自编码器的对比特征检索框架：利用受控语义对立构建对比激活分析 pipeline，结合统计激活分析与生成式验证，从稀疏激活空间中提取单义性功能特征。

Result: 在Big Five人格特质案例中，实现了精确双向行为调控，稳定性与性能优于CAA等现有激活引导方法；发现‘功能忠实性’现象——干预特定内部特征可引发多个语言维度上一致且可预测的变化。

Conclusion: LLM内部存在高度整合的高阶概念表征，该方法为复杂AI行为的机制化调控提供了新路径。

Abstract: Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. In this paper, we propose a Sparse Autoencoder-based framework for retrieving and steering semantically interpretable internal features associated with high-level linguistic behaviors. Our method employs a contrastive feature retrieval pipeline based on controlled semantic oppositions, combing statistical activation analysis and generation-based validation to distill monosemantic functional features from sparse activation spaces. Using the Big Five personality traits as a case study, we demonstrate that our method enables precise, bidirectional steering of model behavior while maintaining superior stability and performance compared to existing activation steering methods like Contrastive Activation Addition (CAA). We further identify an empirical effect, which we term Functional Faithfulness, whereby intervening on a specific internal feature induces coherent and predictable shifts across multiple linguistic dimensions aligned with the target semantic attribute. Our findings suggest that LLMs internalize deeply integrated representations of high-order concepts, and provide a novel, robust mechanistic path for the regulation of complex AI behaviors.

</details>


### [114] [P-Check: Advancing Personalized Reward Model via Learning to Generate Dynamic Checklist](https://arxiv.org/abs/2601.02986)
*Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 本文提出P-Check框架，通过动态生成个性化评估清单并引入偏好对比准则加权策略，提升个性化奖励建模的准确性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有个性化奖励建模方法将用户上下文视为静态或隐式条件信号，难以捕捉人类判断的动态性和多维性。

Method: 提出P-Check框架，包含可插拔的动态检查清单生成器，并引入偏好对比准则加权（Preference-Contrastive Criterion Weighting）策略，依据各准则对个性化判断的区分能力分配显著性得分。

Result: 实验表明P-Check不仅提升了奖励预测准确率，还增强了下游个性化生成效果，并在分布外（OOD）场景中保持鲁棒性。

Conclusion: P-Check通过显式建模动态、多维的个性化判断标准，为个性化奖励建模提供了更有效且鲁棒的新范式。

Abstract: Recent approaches in personalized reward modeling have primarily focused on leveraging user interaction history to align model judgments with individual preferences. However, existing approaches largely treat user context as a static or implicit conditioning signal, failing to capture the dynamic and multi-faceted nature of human judgment. In this paper, we propose P-Check, a novel personalized reward modeling framework, designed to train a plug-and-play checklist generator that synthesizes dynamic evaluation criteria for guiding the reward prediction. To better align these checklists with personalized nuances, we introduce Preference-Contrastive Criterion Weighting, a training strategy that assigns saliency scores to criteria based on their discriminative power for personalized judgment. We conduct extensive experiments and demonstrate that P-Check not only improves reward accuracy but also enhances downstream personalized generation, and remains robust in OOD scenarios.

</details>


### [115] [Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy](https://arxiv.org/abs/2601.02989)
*Hosein Hasani,Mohammadali Banayeeanzade,Ali Nafisi,Sadegh Mohammadian,Fatemeh Askari,Mobin Bagherian,Amirmohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: 本文提出了一种受System-2认知过程启发的测试时策略，通过将大计数任务分解为小的独立子问题，克服了LLM在计数任务中因Transformer架构深度限制导致的精度下降问题，并通过机制分析揭示了其内部工作原理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在计数任务中存在系统性局限，源于Transformer架构在多层中执行计数所导致的大规模问题精度退化。

Method: 提出一种测试时的System-2式策略，将大计数任务分解为可可靠求解的小子问题；并采用观察性与因果中介分析来解析该策略的内在机制。

Result: 机制分析发现：潜在计数结果存储于各子部分末项表征中，经专用注意力头传递至中间步骤，并在最终阶段聚合得出总数；实验表明该策略显著提升LLM在大规模计数任务上的准确率。

Conclusion: 该工作不仅揭示了LLM中System-2式计数的机制，还提供了一种通用、可推广的方法以增强和理解其推理行为。

Abstract: Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.

</details>


### [116] [Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.02993)
*Qianchi Zhang,Hainan Zhang,Liang Pang,Hongwei Zheng,Zhiming Zheng*

Main category: cs.CL

TL;DR: 本文提出Stable-RAG方法，通过估计检索文档顺序敏感性，利用多顺序生成、隐藏状态聚类与中心表示解码，缓解RAG中因检索顺序变化导致的幻觉问题，显著提升答案准确性与推理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法未关注检索文档顺序变化对LLM输出稳定性的影响，即即使包含正确文档，不同排列仍导致答案差异，该‘排列敏感性’尚未被系统研究和解决。

Method: Stable-RAG在多个检索顺序下运行生成器，对各次前向过程中的隐藏状态进行聚类，选取簇中心表征作为主导推理模式，并基于此对齐并修正幻觉输出，提升跨排列的一致性。

Result: 在三个QA数据集上，Stable-RAG显著优于基线，在答案准确率、推理一致性及跨检索器、跨输入长度的泛化能力方面均有提升。

Conclusion: 检索文档的排列顺序显著影响RAG性能；Stable-RAG通过建模和缓解排列敏感性，有效抑制幻觉，增强RAG系统的稳定性与可靠性。

Abstract: Retrieval-Augmented Generation (RAG) has become a key paradigm for reducing factual hallucinations in large language models (LLMs), yet little is known about how the order of retrieved documents affects model behavior. We empirically show that under Top-5 retrieval with the gold document included, LLM answers vary substantially across permutations of the retrieved set, even when the gold document is fixed in the first position. This reveals a previously underexplored sensitivity to retrieval permutations. Although robust RAG methods primarily focus on enhancing LLM robustness to low-quality retrieval and mitigating positional bias to distribute attention fairly over long contexts, neither approach directly addresses permutation sensitivity. In this paper, we propose Stable-RAG, which exploits permutation sensitivity estimation to mitigate permutation-induced hallucinations. Stable-RAG runs the generator under multiple retrieval orders, clusters hidden states, and decodes from a cluster-center representation that captures the dominant reasoning pattern. It then uses these reasoning results to align hallucinated outputs toward the correct answer, encouraging the model to produce consistent and accurate predictions across document permutations. Experiments on three QA datasets show that Stable-RAG significantly improves answer accuracy, reasoning consistency and robust generalization across datasets, retrievers, and input lengths compared with baselines.

</details>


### [117] [Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners](https://arxiv.org/abs/2601.02996)
*Yihong Liu,Raoyuan Zhao,Hinrich Schütze,Michael A. Hedderich*

Main category: cs.CL

TL;DR: 本文系统研究了大推理模型（LRMs）在11种语言中的多语言潜在推理现象，发现其虽存在跨语言差异（资源丰富语言更强、低资源语言较弱、难题上更不明显），但内部预测演化高度一致且以英语为中心。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究发现大推理模型存在非文本的潜在推理能力，但其在多语言场景下的表现尚不清楚，亟需系统探究。

Method: 采用截断策略分析部分推理轨迹下答案的逐步生成，并结合表征分析方法考察不同语言间内部预测演化的异同。

Result: 在11种语言中均观测到潜在推理现象，但强度不均：高资源语言强、低资源语言弱、难题上更难观测；表征分析显示各语言内部预测演化高度一致，且与英语高度对齐。

Conclusion: 多语言潜在推理普遍存在，但受语言资源和任务难度影响；其内在机制具有跨语言一致性，呈现英语中心化特征。

Abstract: Large reasoning models (LRMs) achieve strong performance on mathematical reasoning tasks, often attributed to their capability to generate explicit chain-of-thought (CoT) explanations. However, recent work shows that LRMs often arrive at the correct answer before completing these textual reasoning steps, indicating the presence of latent reasoning -- internal, non-verbal computation encoded in hidden states. While this phenomenon has been explored in English, its multilingual behavior remains largely unknown. In this paper, we conduct a systematic investigation of multilingual latent reasoning in LRMs across 11 languages. Using a truncation-based strategy, we examine how the correct answer emerges as the model is given only partial reasoning traces, allowing us to measure stepwise latent prediction formation. Our results reveal clear evidence of multilingual latent reasoning, though unevenly: strong in resource-rich languages, weaker in low-resource ones, and broadly less observable on harder benchmarks. To understand whether these differences reflect distinct internal mechanisms, we further perform representational analyses. Despite surface-level disparities, we find that the internal evolution of predictions is highly consistent across languages and broadly aligns with English -- a pattern suggesting an English-centered latent reasoning pathway.

</details>


### [118] [SentGraph: Hierarchical Sentence Graph for Multi-hop Retrieval-Augmented Question Answering](https://arxiv.org/abs/2601.03014)
*Junli Liang,Pengfei Zhou,Wangqiu Zhou,Wenjie Qing,Qi Zhao,Ziwen Wang,Qi Song,Xiangyang Li*

Main category: cs.CL

TL;DR: 本文提出SentGraph，一种基于句子级图结构的检索增强生成（RAG）框架，用于提升多跳问答性能。它通过引入修辞结构理论构建分层句子图，并在检索中进行图引导的证据选择与路径扩展，显著改善了多文档逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于块的RAG在多跳问答中难以获取逻辑连贯、跨文档的细粒度证据链，导致推理错误。

Method: 提出SentGraph框架：离线构建分层句子图（结合修辞结构理论区分核心/卫星句，并以实体为桥组织主题子图）；在线阶段进行图引导的句子级证据检索与路径扩展。

Result: 在四个多跳问答基准上实验验证了SentGraph的有效性，证明显式建模句子级逻辑依赖对多跳推理至关重要。

Conclusion: 句子级图结构能更精准捕捉跨文档逻辑关系，显著提升RAG在多-hop QA任务中的推理能力与答案准确性。

Abstract: Traditional Retrieval-Augmented Generation (RAG) effectively supports single-hop question answering with large language models but faces significant limitations in multi-hop question answering tasks, which require combining evidence from multiple documents. Existing chunk-based retrieval often provides irrelevant and logically incoherent context, leading to incomplete evidence chains and incorrect reasoning during answer generation. To address these challenges, we propose SentGraph, a sentence-level graph-based RAG framework that explicitly models fine-grained logical relationships between sentences for multi-hop question answering. Specifically, we construct a hierarchical sentence graph offline by first adapting Rhetorical Structure Theory to distinguish nucleus and satellite sentences, and then organizing them into topic-level subgraphs with cross-document entity bridges. During online retrieval, SentGraph performs graph-guided evidence selection and path expansion to retrieve fine-grained sentence-level evidence. Extensive experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of SentGraph, validating the importance of explicitly modeling sentence-level logical dependencies for multi-hop reasoning.

</details>


### [119] [MMFormalizer: Multimodal Autoformalization in the Wild](https://arxiv.org/abs/2601.03017)
*Jing Xiong,Qi Han,Yunta Hsieh,Hui Shen,Huajian Xin,Chaofan Tao,Chenyang Zhao,Hengyuan Zhang,Taiqiang Wu,Zhen Zhang,Haochen Wang,Zhongwei Wan,Lingpeng Kong,Ngai Wong*

Main category: cs.CL

TL;DR: 本文提出了MMFormalizer，一种支持多模态（尤其是视觉与物理约束结合）的自动形式化方法，通过自适应实体锚定、递归形式命题构建和基于感知的公理组合，实现从自然语言数学到形式化表达的转换，并在新基准PhyX-AF上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统自动形式化局限于文本，难以处理现实世界中需从视觉元素推断隐含物理约束（如质量、能量）的多模态数学问题。

Method: MMFormalizer融合自适应接地机制，将真实世界数学与物理实体（如经典力学、相对论、量子力学、热力学中的概念）纳入形式化过程；通过递归接地与公理组合生成形式命题，并以视觉证据和维度/公理 grounding 保障抽象合理性。

Result: 在新基准PhyX-AF（含115个样本）上，GPT-5和Gemini-3-Pro表现最优，GPT-5在物理推理任务中尤为突出，几何任务仍最具挑战性；MMFormalizer是首个能处理经典力学（哈密顿量导出）、相对论、量子力学和热力学的形式化方法。

Conclusion: MMFormalizer为统一多模态自动形式化提供了可扩展框架，有效弥合了感知与形式化推理之间的鸿沟。

Abstract: Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io

</details>


### [120] [Dementia-R1: Reinforced Pretraining and Reasoning from Unstructured Clinical Notes for Real-World Dementia Prognosis](https://arxiv.org/abs/2601.03018)
*Choonghan Kim,Hyunmin Hwang,Hangeol Chang,Jaemin Kim,Jinse Park,Jae-Sung Lim,Jong Chul Ye*

Main category: cs.CL

TL;DR: 本文提出Dementia-R1，一种基于强化学习的框架，用于从非结构化临床笔记中进行纵向痴呆预后预测；通过冷启动RL策略预训练模型以预测可验证临床指标，提升疾病进展推理能力，在真实数据集上F1达77.03%，并在ADNI基准上媲美GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床文本理解上表现良好，但在需推理复杂、非单调症状轨迹的纵向预测任务（如痴呆预后）上存在困难；标准监督训练缺乏症状演化的显式标注，而直接强化学习受限于稀疏的二元奖励。

Method: 提出Dementia-R1框架，采用冷启动强化学习策略：先预训练模型预测从病史中提取的可验证临床指标，再用于最终临床状态判断。

Result: 在真实世界非结构化临床数据集上F1分数达77.03%；在ADNI基准上，7B参数模型性能媲美GPT-4o，能有效捕捉波动性认知轨迹。

Conclusion: Dementia-R1通过引入临床指标引导的冷启动RL范式，显著提升了LLM在纵向痴呆预后任务中的推理能力，为基于非结构化临床文本的疾病进展建模提供了新思路。

Abstract: While Large Language Models (LLMs) have shown strong performance on clinical text understanding, they struggle with longitudinal prediction tasks such as dementia prognosis, which require reasoning over complex, non-monotonic symptom trajectories across multiple visits. Standard supervised training lacks explicit annotations for symptom evolution, while direct Reinforcement Learning (RL) is hindered by sparse binary rewards. To address this challenge, we introduce Dementia-R1, an RL-based framework for longitudinal dementia prognosis from unstructured clinical notes. Our approach adopts a Cold-Start RL strategy that pre-trains the model to predict verifiable clinical indices extracted from patient histories, enhancing the capability to reason about disease progression before determining the final clinical status. Extensive experiments demonstrate that Dementia-R1 achieves an F1 score of 77.03% on real-world unstructured clinical datasets. Notably, on the ADNI benchmark, our 7B model rivals GPT-4o, effectively capturing fluctuating cognitive trajectories. Code is available at https://anonymous.4open.science/r/dementiar1-CDB5

</details>


### [121] [MedDialogRubrics: A Comprehensive Benchmark and Evaluation Framework for Multi-turn Medical Consultations in Large Language Models](https://arxiv.org/abs/2601.03023)
*Lecheng Gong,Weimin Fang,Ting Yang,Dongjie Tao,Chunxiao Guo,Peng Wei,Bo Xie,Jinqun Guan,Zixiao Chen,Fang Shi,Jinjie Gu,Junwei Liu*

Main category: cs.CL

TL;DR: 本文提出了MedDialogRubrics，一个用于评估医疗大语言模型多轮诊断能力的新基准，包含5200个合成患者病例和6万余条细粒度评估细则，并设计了多智能体系统与防幻觉机制以保障病例真实性与临床合理性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗对话AI的评估基准和框架缺乏对信息收集与诊断推理能力的严格评测，且存在隐私与数据治理问题。

Method: 构建MedDialogRubrics基准：采用多智能体系统合成患者病例；设计受限于原子医学事实并具备动态纠偏能力的Patient Agent以抑制幻觉；提出基于循证医学指南与拒绝采样的结构化rubric生成流程，并由临床专家精修。

Result: 对当前主流模型的全面评估表明，其在多维度诊断能力上仍存在显著不足，尤其在对话管理架构层面。

Conclusion: 提升医疗对话系统性能需突破对话管理架构设计，而非仅依赖基础模型的微调。

Abstract: Medical conversational AI (AI) plays a pivotal role in the development of safer and more effective medical dialogue systems. However, existing benchmarks and evaluation frameworks for assessing the information-gathering and diagnostic reasoning abilities of medical large language models (LLMs) have not been rigorously evaluated. To address these gaps, we present MedDialogRubrics, a novel benchmark comprising 5,200 synthetically constructed patient cases and over 60,000 fine-grained evaluation rubrics generated by LLMs and subsequently refined by clinical experts, specifically designed to assess the multi-turn diagnostic capabilities of LLM. Our framework employs a multi-agent system to synthesize realistic patient records and chief complaints from underlying disease knowledge without accessing real-world electronic health records, thereby mitigating privacy and data-governance concerns. We design a robust Patient Agent that is limited to a set of atomic medical facts and augmented with a dynamic guidance mechanism that continuously detects and corrects hallucinations throughout the dialogue, ensuring internal coherence and clinical plausibility of the simulated cases. Furthermore, we propose a structured LLM-based and expert-annotated rubric-generation pipeline that retrieves Evidence-Based Medicine (EBM) guidelines and utilizes the reject sampling to derive a prioritized set of rubric items ("must-ask" items) for each case. We perform a comprehensive evaluation of state-of-the-art models and demonstrate that, across multiple assessment dimensions, current models face substantial challenges. Our results indicate that improving medical dialogue will require advances in dialogue management architectures, not just incremental tuning of the base-model.

</details>


### [122] [LittiChoQA: Literary Texts in Indic Languages Chosen for Question Answering](https://arxiv.org/abs/2601.03025)
*Aarya Khandelwal,Ritwik Mishra,Rajiv Ratn Shah*

Main category: cs.CL

TL;DR: 本文介绍了LittiChoQA，一个针对印度北部甘吉斯平原多种语言的大型文学问答数据集，用于解决长文本问答在低资源语言中的挑战，并评估了多个多语言大模型在不同上下文长度下的表现。


<details>
  <summary>Details</summary>
Motivation: 解决现代大语言模型在低资源语言中长文本问答任务的资源稀缺问题。

Method: 构建了包含27万多个自动生成功能问答对的LittiChoQA数据集，并在全上下文和截断上下文设置下评估多个多语言大语言模型在非事实性、抽象式问答任务上的性能。

Result: 全上下文微调效果最佳（Krutrim-2语义得分76.1），但上下文截断可显著提升吞吐量；Krutrim-2在段落选择和向量检索两种截断方式下分别得分为74.9和71.4。

Conclusion: 存在性能与效率的权衡，全上下文更优但计算开销大，而上下文截断是提升效率的有效折中方案。

Abstract: Long-context question answering (QA) over literary texts poses significant challenges for modern large language models, particularly in low-resource languages. We address the scarcity of long-context QA resources for Indic languages by introducing LittiChoQA, the largest literary QA dataset to date covering many languages spoken in the Gangetic plains of India. The dataset comprises over 270K automatically generated question-answer pairs with a balanced distribution of factoid and non-factoid questions, generated from naturally authored literary texts collected from the open web. We evaluate multiple multilingual LLMs on non-factoid, abstractive QA, under both full-context and context-shortened settings. Results demonstrate a clear trade-off between performance and efficiency: full-context fine-tuning yields the highest token-level and semantic-level scores, while context shortening substantially improves throughput. Among the evaluated models, Krutrim-2 achieves the strongest performance, obtaining a semantic score of 76.1 with full context. While, in shortened context settings it scores 74.9 with answer paragraph selection and 71.4 with vector-based retrieval. Qualitative evaluations further corroborate these findings.

</details>


### [123] [Reducing Hallucinations in LLMs via Factuality-Aware Preference Learning](https://arxiv.org/abs/2601.03027)
*Sindhuja Chaduvula,Ahmed Y. Radwan,Azib Farooq,Yani Ioannou,Shaina Raza*

Main category: cs.CL

TL;DR: 本文提出F-DPO（事实性感知直接偏好优化），一种仅需二元事实性标签的DPO简单扩展，通过标签翻转和事实性感知间隔提升大模型的事实性并降低幻觉率，无需额外奖励模型或复杂标注。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法（如RLHF、DPO）易因偏好判断偏向流畅性和自信度而强化幻觉，忽视事实正确性。

Method: F-DPO包含两部分：(i) 标签翻转变换，确保选中响应的事实性不低于被拒响应；(ii) 事实性感知间隔，增强事实性差异显著的样本权重，并在两者事实性相同时退化为标准DPO；数据上通过添加二元事实性标签与合成幻觉变体构建事实性感知偏好数据。

Result: 在7个开源LLM（1B–14B）上，F-DPO一致提升事实性、降低幻觉率；Qwen3-8B上幻觉率降为原来的1/5（0.424→0.084），事实性得分提升50%（5.26→7.90）；在TruthfulQA上Qwen2.5-14B的MC1和MC2准确率分别提升17%和49%。

Conclusion: F-DPO是一种轻量、高效、无需额外模块或标注的事实性对齐方法，显著缓解大模型幻觉问题，并具备跨分布泛化能力。

Abstract: Preference alignment methods such as RLHF and Direct Preference Optimization (DPO) improve instruction following, but they can also reinforce hallucinations when preference judgments reward fluency and confidence over factual correctness. We introduce F-DPO (Factuality-aware Direct Preference Optimization), a simple extension of DPO that uses only binary factuality labels. F-DPO (i) applies a label-flipping transformation that corrects misordered preference pairs so the chosen response is never less factual than the rejected one, and (ii) adds a factuality-aware margin that emphasizes pairs with clear correctness differences, while reducing to standard DPO when both responses share the same factuality. We construct factuality-aware preference data by augmenting DPO pairs with binary factuality indicators and synthetic hallucinated variants. Across seven open-weight LLMs (1B-14B), F-DPO consistently improves factuality and reduces hallucination rates relative to both base models and standard DPO. On Qwen3-8B, F-DPO reduces hallucination rates by five times (from 0.424 to 0.084) while improving factuality scores by 50 percent (from 5.26 to 7.90). F-DPO also generalizes to out-of-distribution benchmarks: on TruthfulQA, Qwen2.5-14B achieves plus 17 percent MC1 accuracy (0.500 to 0.585) and plus 49 percent MC2 accuracy (0.357 to 0.531). F-DPO requires no auxiliary reward model, token-level annotations, or multi-stage training.

</details>


### [124] [NorwAI's Large Language Models: Technical Report](https://arxiv.org/abs/2601.03034)
*Jon Atle Gulla,Peng Liu,Lemei Zhang*

Main category: cs.CL

TL;DR: NorLLM团队开发了专为挪威语及其他斯堪的纳维亚语言定制的大语言模型系列，基于多种Transformer架构（如GPT、Mistral、Llama2等），通过大规模预训练与指令微调，显著提升其在真实任务中的性能与适应性，并开源供北欧研究与实验使用。


<details>
  <summary>Details</summary>
Motivation: 解决挪威语在自然语言处理重大进展中长期被忽视、资源匮乏的问题，弥补其在NLP领域代表性不足的空白。

Method: 基于GPT、Mistral、Llama2、Mixtral和Magistral等多种Transformer架构，从头预训练或持续预训练（25B–88.45B tokens），采用挪威语扩展分词器及先进后训练策略（包括指令微调），并针对交互式与领域应用优化模型能力。

Result: 成功构建多个高性能挪威语大模型，尤其指令微调变体（如Mistral-7B-Instruct、Mixtral-8x7B-Instruct）展现出优异的助手式能力；模型已开源，面向北欧机构、企业与学生开放使用。

Conclusion: NorLLM系列模型有效提升了挪威语在NLP中的技术能力与可用性，为低资源语言大模型建设提供了可复用的方法论与实践范例。

Abstract: Norwegian, spoken by approximately five million people, remains underrepresented in many of the most significant breakthroughs in Natural Language Processing (NLP). To address this gap, the NorLLM team at NorwAI has developed a family of models specifically tailored to Norwegian and other Scandinavian languages, building on diverse Transformer-based architectures such as GPT, Mistral, Llama2, Mixtral and Magistral. These models are either pretrained from scratch or continually pretrained on 25B - 88.45B tokens, using a Norwegian-extended tokenizer and advanced post-training strategies to optimize performance, enhance robustness, and improve adaptability across various real-world tasks. Notably, instruction-tuned variants (e.g., Mistral-7B-Instruct and Mixtral-8x7B-Instruct) showcase strong assistant-style capabilities, underscoring their potential for practical deployment in interactive and domain-specific applications. The NorwAI large language models are openly available to Nordic organizations, companies and students for both research and experimental use. This report provides detailed documentation of the model architectures, training data, tokenizer design, fine-tuning strategies, deployment, and evaluations.

</details>


### [125] [BaseCal: Unsupervised Confidence Calibration via Base Model Signals](https://arxiv.org/abs/2601.03042)
*Hexiang Tan,Wanli Yang,Junwei Zhang,Xin Chen,Rui Tang,Du Su,Jingang Wang,Yuanzhuo Wang,Fei Sun,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出BaseCal方法，利用基础大语言模型（base LLM）作为参考来校准后训练大语言模型（PoLLM）的置信度，包含ReEval和Proj两种策略，无需监督信号或修改模型，显著降低校准误差。


<details>
  <summary>Details</summary>
Motivation: 广泛部署的后训练大语言模型（PoLLMs）存在严重过度自信问题，而其对应的基础模型（base LLM）通常校准良好，因此可利用base LLM校准PoLLM的置信度。

Method: 提出两种无监督校准方法：BaseCal-ReEval——将PoLLM输出送入base LLM获取平均概率作为置信度；BaseCal-Proj——训练轻量投影网络，将PoLLM最后层隐状态映射回base LLM空间，再经base LLM输出层生成校准置信度。

Result: 在五个数据集和三类LLM上实验表明，BaseCal平均降低期望校准误差（ECE）42.90%，优于最佳无监督基线。

Conclusion: BaseCal是一种即插即用、无需标签或模型修改的通用置信度校准方案，有效缓解PoLLM的过自信问题，提升可信度。

Abstract: Reliable confidence is essential for trusting the outputs of LLMs, yet widely deployed post-trained LLMs (PoLLMs) typically compromise this trust with severe overconfidence. In contrast, we observe that their corresponding base LLMs often remain well-calibrated. This naturally motivates us to calibrate PoLLM confidence using the base LLM as a reference. This work proposes two ways to achieve this. A straightforward solution, BaseCal-ReEval, evaluates PoLLM's responses by feeding them into the base LLM to get average probabilities as confidence. While effective, this approach introduces additional inference overhead. To address this, we propose BaseCal-Proj, which trains a lightweight projection to map the final-layer hidden states of PoLLMs back to those of their base LLMs. These projected states are then processed by the base LLM's output layer to derive base-calibrated confidence for PoLLM's responses. Notably, BaseCal is an unsupervised, plug-and-play solution that operates without human labels or LLM modifications. Experiments across five datasets and three LLM families demonstrate the effectiveness of BaseCal, reducing Expected Calibration Error (ECE) by an average of 42.90\% compared to the best unsupervised baselines.

</details>


### [126] [Lil: Less is Less When Applying Post-Training Sparse-Attention Algorithms in Long-Decode Stage](https://arxiv.org/abs/2601.03043)
*Junhao Hu,Fangze Li,Mingtao Xu,Feifan Meng,Shiju Zhao,Tiancheng Hu,Ting Peng,Anmin Liu,Wenrui Huang,Chenxu Liu,Ziyue Hua,Tao Xie*

Main category: cs.CL

TL;DR: 本文揭示了稀疏注意力在大语言模型解码阶段可能导致信息损失，进而引发序列长度异常增加（即'Less is Less'现象），并提出一种早期停止算法来缓解该问题，在显著降低token消耗的同时仅带来轻微精度下降。


<details>
  <summary>Details</summary>
Motivation: 稀疏注意力虽旨在降低解码阶段的时间与内存复杂度，但其潜在的信息损失可能反而增加端到端延迟，亟需识别并缓解这一悖论现象。

Method: 通过理论分析与实证验证揭示'Less is Less'现象，并设计一种早期停止算法，动态检测稀疏解码中信息损失超过信息增益的临界点。

Result: 所提早期停止算法在推理密集型基准上最多减少90%的token消耗，且精度下降小于2%。

Conclusion: 稀疏注意力并非总能提升效率；需兼顾信息保真度与计算效率，早期停止是一种有效缓解信息损失引发冗余生成的实用策略。

Abstract: Large language models (LLMs) demonstrate strong capabilities across a wide range of complex tasks and are increasingly deployed at scale, placing significant demands on inference efficiency. Prior work typically decomposes inference into prefill and decode stages, with the decode stage dominating total latency. To reduce time and memory complexity in the decode stage, a line of work introduces sparse-attention algorithms. In this paper, we show, both empirically and theoretically, that sparse attention can paradoxically increase end-to-end complexity: information loss often induces significantly longer sequences, a phenomenon we term ``Less is Less'' (Lil). To mitigate the Lil problem, we propose an early-stopping algorithm that detects the threshold where information loss exceeds information gain during sparse decoding. Our early-stopping algorithm reduces token consumption by up to 90% with a marginal accuracy degradation of less than 2% across reasoning-intensive benchmarks.

</details>


### [127] [Temporal Graph Network: Hallucination Detection in Multi-Turn Conversation](https://arxiv.org/abs/2601.03051)
*Vidhi Rathore,Sambu Aneesh,Himanshu Singh*

Main category: cs.CL

TL;DR: 本文提出了一种基于时序图的对话级幻觉检测方法，将对话建模为节点，通过共享实体边和时序边构建图结构，并利用消息传递与注意力池化进行上下文感知的嵌入聚合，最终分类幻觉类型；该方法性能略优于现有方法，且注意力机制可提供决策解释。


<details>
  <summary>Details</summary>
Motivation: 对话式AI系统在多轮对话中易产生幻觉，尤其当上下文变化或出现矛盾时，亟需有效检测对话级幻觉的方法。

Method: 将整段对话建模为时序图，每轮对话为节点，用句子Transformer编码；构建两类边——共享实体边（指代相同实体的轮次间）和时序边（相邻轮次间）；通过消息传递更新节点嵌入，再经注意力池化融合为单向量，输入分类器识别幻觉存在及类型。

Result: 所提方法在幻觉检测任务上性能略优于现有方法，且注意力权重可解释模型决策依据。

Conclusion: 基于图结构与注意力机制的对话建模能有效捕捉跨轮次语义依赖，提升对话级幻觉检测效果并增强可解释性。

Abstract: Hallucinations can be produced by conversational AI systems, particularly in multi-turn conversations where context changes and contradictions may eventually surface. By representing the entire conversation as a temporal graph, we present a novel graph-based method for detecting dialogue-level hallucinations. Our framework models each dialogue as a node, encoding it using a sentence transformer. We explore two different ways of connectivity: i) shared-entity edges, which connect turns that refer to the same entities; ii) temporal edges, which connect contiguous turns in the conversation. Message-passing is used to update the node embeddings, allowing flow of information between related nodes. The context-aware node embeddings are then combined using attention pooling into a single vector, which is then passed on to a classifier to determine the presence and type of hallucinations. We demonstrate that our method offers slightly improved performance over existing methods. Further, we show the attention mechanism can be used to justify the decision making process. The code and model weights are made available at: https://github.com/sambuaneesh/anlp-project.

</details>


### [128] [Detecting Hallucinations in Retrieval-Augmented Generation via Semantic-level Internal Reasoning Graph](https://arxiv.org/abs/2601.03052)
*Jianpeng Hu,Yanzeng Li,Jialun Zhong,Wenfa Qi,Lei Zou*

Main category: cs.CL

TL;DR: 本文提出了一种基于语义级内部推理图的方法，用于检测RAG系统中大语言模型的忠实性幻觉，通过扩展层相关传播算法并构建语义级归因图，结合小型预训练语言模型框架实现更精准的幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测忠实性幻觉时忽视或粗略处理大语言模型内部推理过程，导致判别器难以有效学习。

Method: 将层-wise相关传播算法从词元级扩展到语义级，构建基于归因向量的内部推理图；设计基于小型预训练语言模型的通用框架，利用LLM推理依赖进行训练与检测，并通过阈值动态调节正确样本通过率。

Result: 在RAGTruth和Dolly-15k数据集上，该方法整体性能优于当前最优基线。

Conclusion: 语义级内部推理图能更真实地表征依赖关系，所提框架可有效提升RAG系统中忠实性幻觉的检测能力。

Abstract: The Retrieval-augmented generation (RAG) system based on Large language model (LLM) has made significant progress. It can effectively reduce factuality hallucinations, but faithfulness hallucinations still exist. Previous methods for detecting faithfulness hallucinations either neglect to capture the models' internal reasoning processes or handle those features coarsely, making it difficult for discriminators to learn. This paper proposes a semantic-level internal reasoning graph-based method for detecting faithfulness hallucination. Specifically, we first extend the layer-wise relevance propagation algorithm from the token level to the semantic level, constructing an internal reasoning graph based on attribution vectors. This provides a more faithful semantic-level representation of dependency. Furthermore, we design a general framework based on a small pre-trained language model to utilize the dependencies in LLM's reasoning for training and hallucination detection, which can dynamically adjust the pass rate of correct samples through a threshold. Experimental results demonstrate that our method achieves better overall performance compared to state-of-the-art baselines on RAGTruth and Dolly-15k.

</details>


### [129] [Do LLMs Encode Functional Importance of Reasoning Tokens?](https://arxiv.org/abs/2601.03066)
*Janvijay Singh,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 本文提出了一种名为贪婪剪枝（greedy pruning）的方法，用于在保持模型似然的前提下，系统性地删除推理链中功能重要性较低的token，从而生成长度可控、更紧凑的推理链；实验表明，基于剪枝后链训练的学生模型优于前沿模型监督的压缩基线，并揭示了模型内部存在token级的功能重要性结构。


<details>
  <summary>Details</summary>
Motivation: 现有紧凑推理方法（如概率采样、启发式或前沿模型监督）未能揭示大语言模型是否在内部编码了token级别的功能重要性信息，本文旨在填补这一诊断性空白。

Method: 提出贪婪剪枝：一种似然保持的迭代删除策略，每次移除对指定目标下模型似然下降最小的推理token；并在知识蒸馏框架中评估剪枝链的效果；同时分析剪枝模式及注意力分数与剪枝顺序的相关性。

Result: 在匹配推理长度下，基于贪婪剪枝链训练的学生模型性能优于前沿模型监督的压缩基线；注意力分数可预测剪枝顺序，表明模型隐含编码了token功能重要性结构。

Conclusion: 大语言模型确实在推理过程中隐式编码了token级的功能重要性，贪婪剪枝不仅能有效压缩推理链，还为理解模型内部推理机制提供了新视角。

Abstract: Large language models solve complex tasks by generating long reasoning chains, achieving higher accuracy at the cost of increased computational cost and reduced ability to isolate functionally relevant reasoning. Prior work on compact reasoning shortens such chains through probabilistic sampling, heuristics, or supervision from frontier models, but offers limited insight into whether models internally encode token-level functional importance for answer generation. We address this gap diagnostically and propose greedy pruning, a likelihood-preserving deletion procedure that iteratively removes reasoning tokens whose removal minimally degrades model likelihood under a specified objective, yielding length-controlled reasoning chains. We evaluate pruned reasoning in a distillation framework and show that students trained on pruned chains outperform a frontier-model-supervised compression baseline at matched reasoning lengths. Finally, our analysis reveals systematic pruning patterns and shows that attention scores can predict greedy pruning ranks, further suggesting that models encode a nontrivial functional importance structure over reasoning tokens.

</details>


### [130] [Learning to Diagnose and Correct Moral Errors: Towards Enhancing Moral Sensitivity in Large Language Models](https://arxiv.org/abs/2601.03079)
*Bocheng Chen,Han Zi,Xi Chen,Xitong Zhang,Kristen Johnson,Guangliang Liu*

Main category: cs.CL

TL;DR: 本文提出两种实用的推理方法，以增强大语言模型（LLM）的道德敏感性，使其能识别道德无害与有害输入并纠正道德错误，基于统一的推理负荷视角，实证表明其在道德相关基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种方法尝试使大语言模型对齐人类道德价值观，但提升其道德敏感性仍极具挑战；本文旨在探索如何增强LLM对道德情境的感知与响应能力。

Method: 提出两种基于推理负荷的实用推理方法，使LLM能够诊断输入的道德属性（良性/有害）并主动纠正道德错误，强调统一、原则性的推理设计而非处理语义繁杂的表层道德话语。

Result: 实验表明所提方法显著提升了LLM的道德敏感性，在代表性道德相关基准测试中取得优异性能。

Conclusion: 基于推理负荷的统一视角可有效增强LLM的道德敏感性，为构建更具道德意识的AI系统提供了可行路径。

Abstract: Moral sensitivity is fundamental to human moral competence, as it guides individuals in regulating everyday behavior. Although many approaches seek to align large language models (LLMs) with human moral values, how to enable them morally sensitive has been extremely challenging. In this paper, we take a step toward answering the question: how can we enhance moral sensitivity in LLMs? Specifically, we propose two pragmatic inference methods that faciliate LLMs to diagnose morally benign and hazardous input and correct moral errors, whereby enhancing LLMs' moral sensitivity. A central strength of our pragmatic inference methods is their unified perspective: instead of modeling moral discourses across semantically diverse and complex surface forms, they offer a principled perspective for designing pragmatic inference procedures grounded in their inferential loads. Empirical evidence demonstrates that our pragmatic methods can enhance moral sensitivity in LLMs and achieves strong performance on representative morality-relevant benchmarks.

</details>


### [131] [Grad-ELLM: Gradient-based Explanations for Decoder-only LLMs](https://arxiv.org/abs/2601.03089)
*Xin Huang,Antoni B. Chan*

Main category: cs.CL

TL;DR: 本文提出Grad-ELLM，一种面向解码器-only Transformer大语言模型的梯度驱动输入归因方法，通过融合通道重要性（来自输出logit对注意力层梯度）与空间重要性（来自注意力图），在不修改模型结构前提下生成每步生成的归因热图；同时设计了两个更公平的忠实性评估指标π-Soft-NC/NS，并在多种任务上验证其优于现有归因方法。


<details>
  <summary>Details</summary>
Motivation: 现有输入归因方法多为模型无关，未充分利用Transformer架构特性，导致对LLM归因的忠实性不足。

Method: 提出Grad-ELLM：基于梯度的方法，聚合注意力层输出logit梯度（表征通道重要性）和注意力图（表征空间重要性），逐生成步生成归因热图；并设计π-Soft-NC和π-Soft-NS两个新忠实性评估指标。

Result: 在情感分类、问答和开放生成任务上，Grad-ELLM在多个LLM上均展现出比其他归因方法更高的忠实性。

Conclusion: Grad-ELLM是一种无需修改模型结构、专为decoder-only Transformer LLM设计的高效且高忠实性的输入归因方法，所提评估指标也提升了归因方法比较的公平性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks, yet their black-box nature raises concerns about transparency and faithfulness. Input attribution methods aim to highlight each input token's contributions to the model's output, but existing approaches are typically model-agnostic, and do not focus on transformer-specific architectures, leading to limited faithfulness. To address this, we propose Grad-ELLM, a gradient-based attribution method for decoder-only transformer-based LLMs. By aggregating channel importance from gradients of the output logit with respect to attention layers and spatial importance from attention maps, Grad-ELLM generates heatmaps at each generation step without requiring architectural modifications. Additionally, we introduce two faithfulneses metrics $π$-Soft-NC and $π$-Soft-NS, which are modifications of Soft-NC/NS that provide fairer comparisons by controlling the amount of information kept when perturbing the text. We evaluate Grad-ELLM on sentiment classification, question answering, and open-generation tasks using different models. Experiment results show that Grad-ELLM consistently achieves superior faithfulness than other attribution methods.

</details>


### [132] [Who Laughs with Whom? Disentangling Influential Factors in Humor Preferences across User Clusters and LLMs](https://arxiv.org/abs/2601.03103)
*Soichiro Murakami,Hidetaka Kamigaito,Hiroya Takamura,Manabu Okumura*

Main category: cs.CL

TL;DR: 本文研究了日本创意回应游戏Oogiri中用户幽默偏好的异质性，通过聚类用户投票日志并使用Bradley-Terry-Luce模型估计各簇的偏好因子权重，同时利用大语言模型（LLM）模拟不同用户簇的幽默判断，并通过角色提示（persona prompting）引导LLM偏好对齐特定用户簇。


<details>
  <summary>Details</summary>
Motivation: 幽默偏好在个体和文化间差异显著，使得用大语言模型（LLMs）评估幽默面临挑战；需建模用户偏好异质性以提升LLM幽默判断的可解释性与适用性。

Method: 基于Oogiri游戏的用户投票日志进行用户聚类，构建Bradley-Terry-Luce模型估计各簇在可解释幽默偏好因子上的权重；通过提示LLM选择‘更有趣’的回应获取其偏好判断；进一步采用persona prompting调控LLM偏好倾向。

Result: 发现用户存在显著不同的幽默偏好簇；LLM的偏好判断可近似某类真实用户簇；通过persona prompting可有效将LLM偏好定向至特定用户簇。

Conclusion: LLM不仅能反映部分人类幽默偏好，还可通过可控提示机制适配多样化用户群体，为个性化幽默生成与评估提供了新路径。

Abstract: Humor preferences vary widely across individuals and cultures, complicating the evaluation of humor using large language models (LLMs). In this study, we model heterogeneity in humor preferences in Oogiri, a Japanese creative response game, by clustering users with voting logs and estimating cluster-specific weights over interpretable preference factors using Bradley-Terry-Luce models. We elicit preference judgments from LLMs by prompting them to select the funnier response and found that user clusters exhibit distinct preference patterns and that the LLM results can resemble those of particular clusters. Finally, we demonstrate that, by persona prompting, LLM preferences can be directed toward a specific cluster. The scripts for data collection and analysis will be released to support reproducibility.

</details>


### [133] [Discovering and Causally Validating Emotion-Sensitive Neurons in Large Audio-Language Models](https://arxiv.org/abs/2601.03115)
*Xiutian Zhao,Björn Schuller,Berrak Sisman*

Main category: cs.CL

TL;DR: 本文首次在神经元层面研究了大型音频-语言模型（LALMs）中情绪敏感神经元（ESNs）的存在与作用，通过干预实验证明其对特定情绪识别具有因果性影响，并发现ESNs具有层间非均匀聚类和部分跨数据集可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对现代大型音频-语言模型（LALMs）如何内在编码情绪的机制性理解。

Method: 在Qwen2.5-Omni、Kimi-Audio和Audio Flamingo 3三个开源LALM上，采用频率、熵、幅值和对比度四种神经元选择方法，在多个情绪识别基准上进行评估；并通过推理时的神经元消融与增益干预，分析其对情绪识别性能的影响。

Result: 证实三模型中均存在情绪敏感神经元（ESNs）；消融特定情绪对应的ESNs会显著降低该情绪识别准确率，而放大则增强该情绪预测；效应在少量识别数据下即显现，且随干预强度系统变化；ESNs呈现非均匀层分布并具备部分跨数据集迁移能力。

Conclusion: 本研究提供了LALMs中情绪决策的因果性、神经元级解释，并表明定向神经元干预是实现可控情感行为的有效手段。

Abstract: Emotion is a central dimension of spoken communication, yet, we still lack a mechanistic account of how modern large audio-language models (LALMs) encode it internally. We present the first neuron-level interpretability study of emotion-sensitive neurons (ESNs) in LALMs and provide causal evidence that such units exist in Qwen2.5-Omni, Kimi-Audio, and Audio Flamingo 3. Across these three widely used open-source models, we compare frequency-, entropy-, magnitude-, and contrast-based neuron selectors on multiple emotion recognition benchmarks. Using inference-time interventions, we reveal a consistent emotion-specific signature: ablating neurons selected for a given emotion disproportionately degrades recognition of that emotion while largely preserving other classes, whereas gain-based amplification steers predictions toward the target emotion. These effects arise with modest identification data and scale systematically with intervention strength. We further observe that ESNs exhibit non-uniform layer-wise clustering with partial cross-dataset transfer. Taken together, our results offer a causal, neuron-level account of emotion decisions in LALMs and highlight targeted neuron interventions as an actionable handle for controllable affective behaviors.

</details>


### [134] [ToxiGAN: Toxic Data Augmentation via LLM-Guided Directional Adversarial Generation](https://arxiv.org/abs/2601.03121)
*Peiran Li,Jan Fillies,Adrian Paschke*

Main category: cs.CL

TL;DR: 本文提出ToxiGAN，一种结合对抗生成与大语言模型（LLM）语义引导的类别感知文本增强框架，用于提升毒性分类鲁棒性；通过两步定向训练和动态选择中性样本来缓解模式坍缩与语义漂移，实验表明其在多个仇恨言论基准上性能最优。


<details>
  <summary>Details</summary>
Motivation: 毒性语言数据增强需可控且类别特异，但受限于标注稀疏和分布偏斜，现有GAN方法易出现模式坍缩和语义漂移。

Method: 提出ToxiGAN框架：1）两步定向训练策略；2）利用LLM生成中性文本作为语义压舱石，并动态选择中性样例；3）显式优化毒样本远离中性样例以强化类别对比信号。

Result: 在四个仇恨言论基准上，ToxiGAN在macro-F1和hate-F1上均取得最优平均性能，显著优于传统及LLM-based增强方法；消融与敏感性分析验证了语义压舱石和定向训练的有效性。

Conclusion: ToxiGAN通过引入动态语义引导与定向对抗训练，有效提升了毒性分类器的鲁棒性，为低资源、高偏斜场景下的可控文本增强提供了新范式。

Abstract: Augmenting toxic language data in a controllable and class-specific manner is crucial for improving robustness in toxicity classification, yet remains challenging due to limited supervision and distributional skew. We propose ToxiGAN, a class-aware text augmentation framework that combines adversarial generation with semantic guidance from large language models (LLMs). To address common issues in GAN-based augmentation such as mode collapse and semantic drift, ToxiGAN introduces a two-step directional training strategy and leverages LLM-generated neutral texts as semantic ballast. Unlike prior work that treats LLMs as static generators, our approach dynamically selects neutral exemplars to provide balanced guidance. Toxic samples are explicitly optimized to diverge from these exemplars, reinforcing class-specific contrastive signals. Experiments on four hate speech benchmarks show that ToxiGAN achieves the strongest average performance in both macro-F1 and hate-F1, consistently outperforming traditional and LLM-based augmentation methods. Ablation and sensitivity analyses further confirm the benefits of semantic ballast and directional training in enhancing classifier robustness.

</details>


### [135] [The Anatomy of Conversational Scams: A Topic-Based Red Teaming Analysis of Multi-Turn Interactions in LLMs](https://arxiv.org/abs/2601.03134)
*Xiangzhe Yuan,Zhenhao Zhang,Haoming Tang,Siying Hu*

Main category: cs.CL

TL;DR: 本文通过LLM-to-LLM多轮模拟框架系统研究大语言模型在多轮对话诈骗场景中的交互安全风险，发现其存在可复现的升级模式、防御机制及由安全护栏触发和角色不稳定导致的失败模式，强调多轮交互安全是LLM行为中一个关键且独立的维度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在多轮对话中展现出更强的说服性与代理能力，单轮安全评估已无法捕捉其在多轮对话诈骗等复杂交互场景中引入的新风险。

Method: 构建可控的LLM-to-LLM多轮模拟框架，在多种诈骗场景下对八种英中文先进模型进行系统评估，并对攻击策略、防御响应和失败模式进行定性标注与分析。

Result: 发现诈骗交互呈现反复升级模式；防御主要依赖验证与延迟机制；交互失败常源于安全护栏误触发和角色不稳定。

Conclusion: 多轮交互安全性是LLM行为中一个关键且区别于单轮评估的独立安全维度，亟需专门建模与评测。

Abstract: As LLMs gain persuasive agentic capabilities through extended dialogues, they introduce novel risks in multi-turn conversational scams that single-turn safety evaluations fail to capture. We systematically study these risks using a controlled LLM-to-LLM simulation framework across multi-turn scam scenarios. Evaluating eight state-of-the-art models in English and Chinese, we analyze dialogue outcomes and qualitatively annotate attacker strategies, defensive responses, and failure modes. Results reveal that scam interactions follow recurrent escalation patterns, while defenses employ verification and delay mechanisms. Furthermore, interactional failures frequently stem from safety guardrail activation and role instability. Our findings highlight multi-turn interactional safety as a critical, distinct dimension of LLM behavior.

</details>


### [136] [Improving Indigenous Language Machine Translation with Synthetic Data and Language-Specific Preprocessing](https://arxiv.org/abs/2601.03135)
*Aashish Dhawan,Christopher Driggers-Ellis,Christan Grant,Daisy Zhe Wang*

Main category: cs.CL

TL;DR: 本文通过使用多语言翻译模型生成合成数据，增强美洲土著语言的平行语料库，并结合语言特定预处理提升神经机器翻译性能，尤其在瓜拉尼语-西班牙语和克丘亚语-西班牙语任务中效果显著，但在艾马拉语上揭示了通用预处理的局限性。


<details>
  <summary>Details</summary>
Motivation: 低资源土著语言缺乏足够的平行语料库，难以支撑有效的神经机器翻译（NMT），亟需实用的数据增强策略。

Method: 利用高容量多语言翻译模型生成合成句对，扩充美洲土著语言的精选平行语料；在此基础上微调mBART模型；并引入语言特定的正字法标准化与噪声感知过滤等预处理技术。

Result: 在瓜拉尼语-西班牙语和克丘亚语-西班牙语翻译任务中，chrF++指标均实现稳定提升；而在艾马拉语上的诊断实验表明，通用预处理方法难以适配高度黏着型语言。

Conclusion: 合成数据增强配合语言定制化预处理可有效提升低资源土著语言的翻译质量，但需针对高度黏着结构的语言进一步优化预处理策略。

Abstract: Low-resource indigenous languages often lack the parallel corpora required for effective neural machine translation (NMT). Synthetic data generation offers a practical strategy for mitigating this limitation in data-scarce settings. In this work, we augment curated parallel datasets for indigenous languages of the Americas with synthetic sentence pairs generated using a high-capacity multilingual translation model. We fine-tune a multilingual mBART model on curated-only and synthetically augmented data and evaluate translation quality using chrF++, the primary metric used in recent AmericasNLP shared tasks for agglutinative languages.
  We further apply language-specific preprocessing, including orthographic normalization and noise-aware filtering, to reduce corpus artifacts. Experiments on Guarani--Spanish and Quechua--Spanish translation show consistent chrF++ improvements from synthetic data augmentation, while diagnostic experiments on Aymara highlight the limitations of generic preprocessing for highly agglutinative languages.

</details>


### [137] [Self-Verification is All You Need To Pass The Japanese Bar Examination](https://arxiv.org/abs/2601.03144)
*Andrew Shin*

Main category: cs.CL

TL;DR: 本文提出了一种在真实日语司法考试格式和评分标准下训练的自验证模型，首次实现了LLM在不修改题目结构和评分规则的前提下通过日本司法考试。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽通过分解问题提升性能，但未在原始考试格式和评分体系下系统评估，无法确认是否真正达到考试级能力。

Method: 构建了忠实还原考试格式与评分尺度的新数据集，并在此基础上训练自验证模型。

Result: 该模型在真实考试评分尺度下超过官方及格线；相比多智能体推理和分解式监督等替代策略，性能更优。

Conclusion: 格式保真监督与一致性验证至关重要；精心设计的单模型方法可在高风险专业推理任务中超越复杂系统。

Abstract: Despite rapid advances in large language models (LLMs), achieving reliable performance on highly professional and structured examinations remains a significant challenge. The Japanese bar examination is a particularly demanding benchmark, requiring not only advanced legal reasoning but also strict adherence to complex answer formats that involve joint evaluation of multiple propositions. While recent studies have reported improvements by decomposing such questions into simpler true--false judgments, these approaches have not been systematically evaluated under the original exam format and scoring scheme, leaving open the question of whether they truly capture exam-level competence. In this paper, we present a self-verification model trained on a newly constructed dataset that faithfully replicates the authentic format and evaluation scale of the exam. Our model is able to exceed the official passing score when evaluated on the actual exam scale, marking the first demonstration, to our knowledge, of an LLM passing the Japanese bar examination without altering its original question structure or scoring rules. We further conduct extensive comparisons with alternative strategies, including multi-agent inference and decomposition-based supervision, and find that these methods fail to achieve comparable performance. Our results highlight the importance of format-faithful supervision and consistency verification, and suggest that carefully designed single-model approaches can outperform more complex systems in high-stakes professional reasoning tasks. Our dataset and codes are publicly available.

</details>


### [138] [Decoupling the Effect of Chain-of-Thought Reasoning: A Human Label Variation Perspective](https://arxiv.org/abs/2601.03154)
*Beiduo Chen,Tiancheng Hu,Caiqi Zhang,Robert Litschko,Anna Korhonen,Barbara Plank*

Main category: cs.CL

TL;DR: 本文探讨了推理调优的大语言模型（LLM）在建模人类标签变异（Human Label Variation）方面的能力，发现长链式思维（CoT）虽能提升分布对齐，但最终准确率主要由CoT内容决定，而分布排序则主要依赖模型内在先验。


<details>
  <summary>Details</summary>
Motivation: 现有推理调优的LLM擅长单答案任务，但在需建模概率性模糊性（而非消除模糊性）的人类标签变异任务上能力尚不明确。

Method: 通过系统性的解耦实验，在基于分布的任务上开展Cross-CoT实验，分离推理文本与模型内在先验的影响，并进行逐步分析。

Result: 发现‘解耦机制’：CoT内容贡献99%的准确率方差，而模型先验主导超80%的分布排序；CoT对准确率的影响随推理步骤单调增强，但分布结构主要由模型先验决定。

Conclusion: 长CoT可作为LLM在确定首选答案时的关键决策器，但无法精细校准模糊任务下的概率分布。

Abstract: Reasoning-tuned LLMs utilizing long Chain-of-Thought (CoT) excel at single-answer tasks, yet their ability to model Human Label Variation--which requires capturing probabilistic ambiguity rather than resolving it--remains underexplored. We investigate this through systematic disentanglement experiments on distribution-based tasks, employing Cross-CoT experiments to isolate the effect of reasoning text from intrinsic model priors. We observe a distinct "decoupled mechanism": while CoT improves distributional alignment, final accuracy is dictated by CoT content (99% variance contribution), whereas distributional ranking is governed by model priors (over 80%). Step-wise analysis further shows that while CoT's influence on accuracy grows monotonically during the reasoning process, distributional structure is largely determined by LLM's intrinsic priors. These findings suggest that long CoT serves as a decisive LLM decision-maker for the top option but fails to function as a granular distribution calibrator for ambiguous tasks.

</details>


### [139] [WebAnchor: Anchoring Agent Planning to Stabilize Long-Horizon Web Reasoning](https://arxiv.org/abs/2601.03164)
*Yu Xinmiao,Zhang Liwen,Feng Xiaocheng,Jiang Yong,Qin Bing,Xie Pengjun,Zhou Jingren*

Main category: cs.CL

TL;DR: 本文提出Anchor-GRPO，一种两阶段强化学习框架，通过解耦规划与执行，并利用首步‘计划锚点’效应提升LLM智能体在长周期网页推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLM网页信息检索智能体在长周期规划中表现不佳，核心问题在于首步推理（计划锚点）对后续行为影响巨大，但传统RL均匀分配奖励，未能针对性优化该关键步骤。

Method: 提出两阶段RL框架Anchor-GRPO：第一阶段利用自博弈和人工校准生成细粒度评分标准，专门优化首步规划；第二阶段采用稀疏奖励对齐执行与初始计划，保障工具使用的稳定性与效率。

Result: 在BrowseComp、BrowseComp-Zh、GAIA和XBench-DeepSearch四个基准上，Anchor-GRPO在3B至30B模型上均优于GRPO和First-step GRPO；WebAnchor-30B在BrowseComp和GAIA上分别达到46.0%和76.4%的pass@1；且随模型规模和上下文长度增加，性能持续提升。

Conclusion: 计划锚点是长周期网页推理的关键瓶颈，Anchor-GRPO通过分阶段、差异化奖励机制有效缓解该问题，显著提升任务成功率与工具使用效率，并具备良好可扩展性。

Abstract: Large Language Model(LLM)-based agents have shown strong capabilities in web information seeking, with reinforcement learning (RL) becoming a key optimization paradigm. However, planning remains a bottleneck, as existing methods struggle with long-horizon strategies. Our analysis reveals a critical phenomenon, plan anchor, where the first reasoning step disproportionately impacts downstream behavior in long-horizon web reasoning tasks. Current RL algorithms, fail to account for this by uniformly distributing rewards across the trajectory. To address this, we propose Anchor-GRPO, a two-stage RL framework that decouples planning and execution. In Stage 1, the agent optimizes its first-step planning using fine-grained rubrics derived from self-play experiences and human calibration. In Stage 2, execution is aligned with the initial plan through sparse rewards, ensuring stable and efficient tool usage. We evaluate Anchor-GRPO on four benchmarks: BrowseComp, BrowseComp-Zh, GAIA, and XBench-DeepSearch. Across models from 3B to 30B, Anchor-GRPO outperforms baseline GRPO and First-step GRPO, improving task success and tool efficiency. Notably, WebAnchor-30B achieves 46.0% pass@1 on BrowseComp and 76.4% on GAIA. Anchor-GRPO also demonstrates strong scalability, getting higher accuracy as model size and context length increase.

</details>


### [140] [Can Embedding Similarity Predict Cross-Lingual Transfer? A Systematic Study on African Languages](https://arxiv.org/abs/2601.03168)
*Tewodros Kederalah Idris,Prasenjit Mitra,Roald Eiselen*

Main category: cs.CL

TL;DR: 本文系统评估了五种嵌入相似性度量在跨语言迁移中的预测能力，发现余弦差距和基于检索的指标（P@1、CSLS）能可靠预测迁移效果，而CKA效果差；同时指出需按模型单独验证，避免辛普森悖论。


<details>
  <summary>Details</summary>
Motivation: 缺乏针对低资源非洲语言的可靠源语言选择方法，亟需评估不同嵌入相似性指标对跨语言迁移效果的预测能力。

Method: 在816次迁移实验中，涵盖3个NLP任务、3个非洲中心多语言模型和12种来自4个语系的语言，系统评估5种嵌入相似性度量（如余弦差距、P@1、CSLS、CKA等）的预测性能，并分析其与URIEL语言类型学指标的对比及跨模型聚合引发的辛普森悖论。

Result: 余弦差距和CSLS、P@1等检索类指标预测效果较好（ρ=0.4–0.6），CKA几乎无效（ρ≈0.1）；跨模型聚合时相关性符号反转（辛普森悖论）；嵌入指标预测力与URIEL语言类型学相当。

Conclusion: 嵌入相似性指标可有效指导源语言选择，但必须按模型单独验证；研究为低资源非洲语言NLP提供了实证依据与实践指南。

Abstract: Cross-lingual transfer is essential for building NLP systems for low-resource African languages, but practitioners lack reliable methods for selecting source languages. We systematically evaluate five embedding similarity metrics across 816 transfer experiments spanning three NLP tasks, three African-centric multilingual models, and 12 languages from four language families. We find that cosine gap and retrieval-based metrics (P@1, CSLS) reliably predict transfer success ($ρ= 0.4-0.6$), while CKA shows negligible predictive power ($ρ\approx 0.1$). Critically, correlation signs reverse when pooling across models (Simpson's Paradox), so practitioners must validate per-model. Embedding metrics achieve comparable predictive power to URIEL linguistic typology. Our results provide concrete guidance for source language selection and highlight the importance of model-specific analysis.

</details>


### [141] [Maximizing Local Entropy Where It Matters: Prefix-Aware Localized LLM Unlearning](https://arxiv.org/abs/2601.03190)
*Naixin Zhai,Pengyang Shao,Binbin Zheng,Fei Shen,Long Bai,Xun Yang*

Main category: cs.CL

TL;DR: 本文提出PALU框架，通过前缀感知的局部化遗忘，在时间和词汇维度上进行局部熵最大化，仅抑制敏感前缀和top-k logits，从而在保证模型效用的同时实现高效遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的机器遗忘方法对所有token一视同仁，强制整个词汇表的不确定性，导致不必要的效用下降和内容无关区域的冗余优化。

Method: 提出PALU（Prefix-Aware Localized Unlearning）框架，采用局部熵最大化目标，聚焦于时间维度（敏感前缀）和词汇维度（top-k logits）进行针对性遗忘。

Result: 实验表明，PALU在遗忘效果和效用保持方面均优于当前最优方法。

Conclusion: 局部化、前缀感知的遗忘策略可显著减少对模型整体性能的损害，同时提升遗忘精准性与效率。

Abstract: Machine unlearning aims to forget sensitive knowledge from Large Language Models (LLMs) while maintaining general utility. However, existing approaches typically treat all tokens in a response indiscriminately and enforce uncertainty over the entire vocabulary. This global treatment results in unnecessary utility degradation and extends optimization to content-agnostic regions. To address these limitations, we propose PALU (Prefix-Aware Localized Unlearning), a framework driven by a local entropy maximization objective across both temporal and vocabulary dimensions. PALU reveals that (i) suppressing the sensitive prefix alone is sufficient to sever the causal generation link, and (ii) flattening only the top-$k$ logits is adequate to maximize uncertainty in the critical subspace. These findings allow PALU to avoid redundant optimization across the full vocabulary and parameter space while minimizing collateral damage to general model performance. Extensive experiments validate that PALU achieves superior forgetting efficacy and utility preservation compared to state-of-the-art baselines.

</details>


### [142] [MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory](https://arxiv.org/abs/2601.03192)
*Shengtao Zhang,Jiaqian Wang,Ruiwen Zhou,Junwei Liao,Yuchen Feng,Weinan Zhang,Ying Wen,Zhiyu Li,Feiyu Xiong,Yutao Qi,Bo Tang,Muning Wen*

Main category: cs.CL

TL;DR: 本文提出MemRL框架，通过非参数强化学习在情景记忆上实现智能体的自我进化，解决了大语言模型在持续学习中稳定性与可塑性难以兼顾的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽具备强大推理能力，但在模仿人类通过情景模拟掌握新技能方面存在不足：微调计算成本高且易发生灾难性遗忘，现有基于记忆的方法依赖被动语义匹配，易检索到噪声。

Method: MemRL将冻结的LLM的稳定推理能力与可塑的情景记忆显式分离，并采用两阶段检索机制：先按语义相关性筛选候选，再依据学习得到的Q值（效用）选择；效用值通过环境反馈在试错中持续更新。

Result: 在HLE、BigCodeBench、ALFWorld和Lifelong Agent Bench等多个基准上，MemRL显著优于当前最优方法；分析实验验证其能有效协调稳定性与可塑性，在不更新权重的前提下实现持续运行时提升。

Conclusion: MemRL为大语言模型提供了无需参数更新即可持续自我进化的可行路径，是迈向真正类人自主学习的重要一步。

Abstract: The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory. Unlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.

</details>


### [143] [X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework](https://arxiv.org/abs/2601.03194)
*Mohammad Zia Ur Rehman,Sai Kartheek Reddy Kasu,Shashivardhan Reddy Koppula,Sai Rithwik Reddy Chirra,Shwetank Shekhar Singh,Nagendra Kumar*

Main category: cs.CL

TL;DR: 本文提出X-MuTeST框架，结合大语言模型的高层语义推理与传统注意力增强技术，提升印地语、泰卢固语和英语等多语言仇恨言论检测的准确性与可解释性，并提供带人工标注词级理由的新基准数据集。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论检测在准确性和可解释性方面面临挑战，尤其在资源匮乏的印度语言中；现有方法缺乏高质量、细粒度（如词级）的人类可解释依据。

Method: 提出X-MuTeST框架：1）利用LLM生成高层次解释；2）设计X-MuTeST解释方法，通过对比原始文本与n-gram掩码后的预测概率差异计算词重要性；3）融合LLM与X-MuTeST解释；4）将人工标注的词级理由融入训练以引导注意力机制优化。

Result: 在印地语、泰卢固语和英语上验证了该方法有效性：分类性能与可解释性（Plausibility：Token-F1/IOU-F1；Faithfulness：Comprehensiveness/Sufficiency）均显著提升；融合人类理由进一步优化注意力。

Conclusion: X-MuTeST通过融合人类理性标注与模型驱动解释，有效提升了多语言（尤其低资源语言）仇恨言论检测的性能与可解释性，为跨语言可解释NLP提供了新范式与实用基准资源。

Abstract: Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST

</details>


### [144] [DIP: Dynamic In-Context Planner For Diffusion Language Models](https://arxiv.org/abs/2601.03199)
*Yang Li,Han Meng,Chenan Wang,Haipeng Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为DIP的动态上下文规划方法，用于优化扩散语言模型（DLMs）的推理效率，通过在生成过程中动态选择和插入上下文示例，显著提升推理速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）因双向注意力机制导致长上下文下的计算开销大，亟需高效上下文利用方法。

Method: 基于DLMs扩散生成范式支持上下文动态调整的发现，提出DIP方法，在生成过程中动态选择和插入上下文示例，而非一次性提供全部示例。

Result: DIP在保持生成质量的同时，相比标准推理提速最高达12.9倍，相比KV缓存增强推理提速1.17倍。

Conclusion: DIP验证了动态上下文优化在DLMs中的有效性，为高效扩散语言建模提供了新思路。

Abstract: Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases. This work addresses this issue with a key discovery: unlike the sequential generation in autoregressive language models (ARLMs), the diffusion generation paradigm in DLMs allows \textit{efficient dynamic adjustment of the context} during generation. Building on this insight, we propose \textbf{D}ynamic \textbf{I}n-Context \textbf{P}lanner (DIP), a context-optimization method that dynamically selects and inserts in-context examples during generation, rather than providing all examples in the prompt upfront. Results show DIP maintains generation quality while achieving up to 12.9$\times$ inference speedup over standard inference and 1.17$\times$ over KV cache-enhanced inference.

</details>


### [145] [UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward](https://arxiv.org/abs/2601.03205)
*Yile Liu,Yixian Liu,Zongwei Li,Yufei Huang,Xinhua Feng,Zhichao Hu,Jinglu Hu,Jianfeng Yan,Fengzong Lian,Yuhong Liu*

Main category: cs.CL

TL;DR: 本文提出UltraLogic框架，通过代码化求解方法解耦逻辑核心与自然语言表达，自动生成高质量、多难度的通用推理数据，并引入双极浮点奖励（BFR）机制缓解奖励稀疏性与非负奖励陷阱，显著提升大模型多步逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂通用推理（如多步逻辑、规划与验证）上仍存在瓶颈，且缺乏大规模、高质量、难度标定的通用推理训练数据。

Method: 提出UltraLogic框架：1）采用基于代码的求解方法，将问题逻辑核心与自然语言表达解耦，实现高质量数据自动化生成；2）构建覆盖百种任务类型、十级难度的自动标定流水线；3）设计双极浮点奖励（BFR）机制，用分级惩罚区分完全正确与含逻辑缺陷的回答。

Result: 实验证明任务多样性是提升推理能力的主因；BFR结合难度匹配策略可显著提高训练效率，引导模型收敛至全局逻辑最优解。

Conclusion: UltraLogic为通用推理能力的规模化训练提供了可扩展的数据生成范式与更有效的奖励机制，有效突破当前LLM在复杂逻辑推理上的关键瓶颈。

Abstract: While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.

</details>


### [146] [MalruleLib: Large-Scale Executable Misconception Reasoning with Step Traces for Modeling Student Thinking in Mathematics](https://arxiv.org/abs/2601.03217)
*Xinghe Chen,Naiming Liu,Shashank Sonkar*

Main category: cs.CL

TL;DR: 本文提出了MalruleLib框架，将数学学习中的系统性错误（即错误但一致的解题程序）转化为可执行的‘错误规则’（malrules），并生成学生错误解题的逐步追踪，以支持教育AI对学生误解的跨情境诊断与反馈。


<details>
  <summary>Details</summary>
Motivation: 学生在数学学习中常表现出系统性错误，即重复应用某种内在一致但错误的解题程序；现有AI模型难以准确识别和泛化此类误解，尤其在题目形式变化时性能显著下降。

Method: 基于67项学习科学与数学教育文献构建MalruleLib框架，形式化定义‘错误规则推理准确率’（MRA）任务；编码101条malrule与498个参数化题目模板，生成正确与错误双路径解题步骤追踪；在9个不同规模语言模型上评估跨模板误解预测能力。

Result: 语言模型在直接解题任务中准确率为66%，但在跨模板误解预测任务中降至40%；使用MalruleLib提供的学生步骤追踪可使预测准确率提升3–15%；观察到跨模板性能下降10–21%。

Conclusion: MalruleLib为教育AI提供了可扩展、可控制、可解释的学生建模基础设施，支持基于深层误解的精准诊断与反馈，推动AI从‘答对与否’走向‘为何错’的建模范式。

Abstract: Student mistakes in mathematics are often systematic: a learner applies a coherent but wrong procedure and repeats it across contexts. We introduce MalruleLib, a learning-science-grounded framework that translates documented misconceptions into executable procedures, drawing on 67 learning-science and mathematics education sources, and generates step-by-step traces of malrule-consistent student work. We formalize a core student-modeling problem as Malrule Reasoning Accuracy (MRA): infer a misconception from one worked mistake and predict the student's next answer under cross-template rephrasing. Across nine language models (4B-120B), accuracy drops from 66% on direct problem solving to 40% on cross-template misconception prediction. MalruleLib encodes 101 malrules over 498 parameterized problem templates and produces paired dual-path traces for both correct reasoning and malrule-consistent student reasoning. Because malrules are executable and templates are parameterizable, MalruleLib can generate over one million instances, enabling scalable supervision and controlled evaluation. Using MalruleLib, we observe cross-template degradations of 10-21%, while providing student step traces improves prediction by 3-15%. We release MalruleLib as infrastructure for educational AI that models student procedures across contexts, enabling diagnosis and feedback that targets the underlying misconception.

</details>


### [147] [Multi-RADS Synthetic Radiology Report Dataset and Head-to-Head Benchmarking of 41 Open-Weight and Proprietary Language Models](https://arxiv.org/abs/2601.03232)
*Kartik Bose,Abhinandan Kumar,Raghuraman Soundararajan,Priya Mudgil,Samonee Ralmilay,Niharika Dutta,Manphool Singhal,Arun Kumar,Saugata Sen,Anurima Patra,Priya Ghosh,Abanti Das,Amit Gupta,Ashish Verma,Dipin Sudhakaran,Ekta Dhamija,Himangi Unde,Ishan Kumar,Krithika Rangarajan,Prerna Garg,Rachel Sequeira,Sudhin Shylendran,Taruna Yadav,Tej Pal,Pankaj Gupta*

Main category: cs.CL

TL;DR: 本文构建了RXL-RADSet——首个经放射科医生验证的多RADS合成基准数据集，并系统评估了41种开源小语言模型（SLMs）与GPT-5.2在RADS自动分类任务上的有效性与准确性，发现20–32B参数的顶级SLMs在引导式提示下可接近专有模型性能，但对高复杂度RADS方案仍有差距。


<details>
  <summary>Details</summary>
Motivation: 现有RADS自动分配面临指南复杂、输出格式受限及跨RADS框架和模型规模缺乏统一基准等挑战。

Method: 构建含1600份合成报告的RXL-RADSet数据集（覆盖10种RADS、多模态），经两阶段放射科医生验证；在固定引导式提示下评估41个量化SLMs（0.135–32B）与GPT-5.2，并对比引导式与零样本提示效果。

Result: GPT-5.2达99.8%有效性与81.1%准确率；SLMs整体为96.8%有效性与61.1%准确率；20–32B顶级SLMs可达~99%有效性与70%+准确率；性能随模型规模提升（拐点在1B与10B之间），受RADS分类难度影响大于无效输出；引导式提示显著优于零样本提示。

Conclusion: RXL-RADSet为首个放射科医生验证的多RADS基准；20–32B级SLMs在引导式提示下可逼近专有模型性能，但在高复杂度RADS方案上仍存差距。

Abstract: Background: Reporting and Data Systems (RADS) standardize radiology risk communication but automated RADS assignment from narrative reports is challenging because of guideline complexity, output-format constraints, and limited benchmarking across RADS frameworks and model sizes. Purpose: To create RXL-RADSet, a radiologist-verified synthetic multi-RADS benchmark, and compare validity and accuracy of open-weight small language models (SLMs) with a proprietary model for RADS assignment. Materials and Methods: RXL-RADSet contains 1,600 synthetic radiology reports across 10 RADS (BI-RADS, CAD-RADS, GB-RADS, LI-RADS, Lung-RADS, NI-RADS, O-RADS, PI-RADS, TI-RADS, VI-RADS) and multiple modalities. Reports were generated by LLMs using scenario plans and simulated radiologist styles and underwent two-stage radiologist verification. We evaluated 41 quantized SLMs (12 families, 0.135-32B parameters) and GPT-5.2 under a fixed guided prompt. Primary endpoints were validity and accuracy; a secondary analysis compared guided versus zero-shot prompting. Results: Under guided prompting GPT-5.2 achieved 99.8% validity and 81.1% accuracy (1,600 predictions). Pooled SLMs (65,600 predictions) achieved 96.8% validity and 61.1% accuracy; top SLMs in the 20-32B range reached ~99% validity and mid-to-high 70% accuracy. Performance scaled with model size (inflection between <1B and >=10B) and declined with RADS complexity primarily due to classification difficulty rather than invalid outputs. Guided prompting improved validity (99.2% vs 96.7%) and accuracy (78.5% vs 69.6%) compared with zero-shot. Conclusion: RXL-RADSet provides a radiologist-verified multi-RADS benchmark; large SLMs (20-32B) can approach proprietary-model performance under guided prompting, but gaps remain for higher-complexity schemes.

</details>


### [148] [STReasoner: Empowering LLMs for Spatio-Temporal Reasoning in Time Series via Spatial-Aware Reinforcement Learning](https://arxiv.org/abs/2601.03248)
*Juntong Ni,Shiyu Wang,Ming Jin,Qi He,Wei Jin*

Main category: cs.CL

TL;DR: 本文提出ST-Bench基准和STReasoner模型，旨在提升时间序列中时空推理能力，通过S-GRPO强化学习算法增强空间信息利用，显著提升推理准确率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有工作过于关注预测精度而忽视推理能力，导致时空推理在关键系统（如交通、电网、疾病传播）中发展不足。

Method: 构建包含四类推理任务的ST-Bench基准；提出STReasoner模型融合时间序列、图结构与文本；设计S-GRPO强化学习算法，专门奖励空间信息带来的性能增益。

Result: STReasoner在各项任务上平均准确率提升17%–135%，成本仅为专有模型的0.004X，并在真实数据上表现出强泛化能力。

Conclusion: 显式建模时空推理是可行且高效的，STReasoner与S-GRPO为高风险决策场景提供了可解释、低成本、鲁棒的推理框架。

Abstract: Spatio-temporal reasoning in time series involves the explicit synthesis of temporal dynamics, spatial dependencies, and textual context. This capability is vital for high-stakes decision-making in systems such as traffic networks, power grids, and disease propagation. However, the field remains underdeveloped because most existing works prioritize predictive accuracy over reasoning. To address the gap, we introduce ST-Bench, a benchmark consisting of four core tasks, including etiological reasoning, entity identification, correlation reasoning, and in-context forecasting, developed via a network SDE-based multi-agent data synthesis pipeline. We then propose STReasoner, which empowers LLM to integrate time series, graph structure, and text for explicit reasoning. To promote spatially grounded logic, we introduce S-GRPO, a reinforcement learning algorithm that rewards performance gains specifically attributable to spatial information. Experiments show that STReasoner achieves average accuracy gains between 17% and 135% at only 0.004X the cost of proprietary models and generalizes robustly to real-world data.

</details>


### [149] [Automated Semantic Rules Detection (ASRD) for Emergent Communication Interpretation](https://arxiv.org/abs/2601.03254)
*Bastien Vanderplaetse,Xavier Siebert,Stéphane Dupont*

Main category: cs.CL

TL;DR: 本文提出了一种自动语义规则检测（ASRD）算法，用于提升多智能体系统中涌现通信语言的可解释性，通过在Lewis游戏中分析代理间消息模式，将其与输入数据属性关联。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注多智能体系统中涌现通信语言的可解释性问题。

Method: 提出自动语义规则检测（ASRD）算法，在Lewis游戏任务中，基于两种不同数据集训练的智能体所交换的消息提取语义模式，并将这些模式映射到输入数据的具体属性上。

Result: ASRD能有效提取消息中的相关模式，并将其与输入数据属性建立联系，显著简化后续对涌现通信的解释与分析。

Conclusion: ASRD为提升涌现通信的可解释性提供了可行方法，有助于深入理解自主智能体如何形成有意义的通信策略。

Abstract: The field of emergent communication within multi-agent systems examines how autonomous agents can independently develop communication strategies, without explicit programming, and adapt them to varied environments. However, few studies have focused on the interpretability of emergent languages. The research exposed in this paper proposes an Automated Semantic Rules Detection (ASRD) algorithm, which extracts relevant patterns in messages exchanged by agents trained with two different datasets on the Lewis Game, which is often studied in the context of emergent communication. ASRD helps at the interpretation of the emergent communication by relating the extracted patterns to specific attributes of the input data, thereby considerably simplifying subsequent analysis.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [150] [GCRank: A Generative Contextual Comprehension Paradigm for Takeout Ranking Model](https://arxiv.org/abs/2601.02361)
*Ziheng Ni,Congcong Liu,Cai Shang,Yiming Sun,Junjie Li,Zhiwei Fang,Guangpeng Chen,Jian Li,Zehua Zhang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.IR

TL;DR: 本文提出了一种新的生成式排序框架，将广告排序任务重构为上下文理解任务，通过统一架构建模异构信号，在位置服务（如外卖）场景中显著提升点击率和平台收入。


<details>
  <summary>Details</summary>
Motivation: 现有广告排序模型依赖碎片化模块和人工特征，难以理解复杂用户意图，尤其在受动态时空与个体因素影响的本地生活服务（如外卖）中表现受限。

Method: 提出生成式上下文编码器（GCE）和生成式上下文融合（GCF）两模块框架；GCE包含个性化（PCE）、群体性（CCE）和动态性（DCE）三个上下文增强模块；GCF采用低秩适配融合多源表征。

Result: 在关键业务指标（如CTR、平台营收）上取得显著提升，并已在大型外卖广告平台成功落地部署。

Conclusion: 该工作开创了生成式推荐在工业广告系统中的新范式，验证了其实际应用价值与潜力。

Abstract: The ranking stage serves as the central optimization and allocation hub in advertising systems, governing economic value distribution through eCPM and orchestrating the user-centric blending of organic and advertising content. Prevailing ranking models often rely on fragmented modules and hand-crafted features, limiting their ability to interpret complex user intent. This challenge is further amplified in location-based services such as food delivery, where user decisions are shaped by dynamic spatial, temporal, and individual contexts. To address these limitations, we propose a novel generative framework that reframes ranking as a context comprehension task, modeling heterogeneous signals in a unified architecture. Our architecture consists of two core components: the Generative Contextual Encoder (GCE) and the Generative Contextual Fusion (GCF). The GCE comprises three specialized modules: a Personalized Context Enhancer (PCE) for user-specific modeling, a Collective Context Enhancer (CCE) for group-level patterns, and a Dynamic Context Enhancer (DCE) for real-time situational adaptation. The GCF module then seamlessly integrates these contextual representations through low-rank adaptation. Extensive experiments confirm that our method achieves significant gains in critical business metrics, including click-through rate and platform revenue. We have successfully deployed our method on a large-scale food delivery advertising platform, demonstrating its substantial practical impact. This work pioneers a new perspective on generative recommendation and highlights its practical potential in industrial advertising systems.

</details>


### [151] [The Impact of LLM-Generated Reviews on Recommender Systems: Textual Shifts, Performance Effects, and Strategic Platform Control](https://arxiv.org/abs/2601.02362)
*Itzhak Ziv,Moshe Unger,Hilah Geva*

Main category: cs.IR

TL;DR: 本研究探讨了AI生成评论对基于内容的推荐系统（RS）性能和商业结果的影响，发现人类撰写的评论在训练推荐系统时仍具有不可替代的高质量优势，而AI生成的评论虽能提升无文本数据模型的表现，但无法超越人类数据训练的效果；同时，平台主导的AI评论若采用合适的语气策略（如鼓励性、建设性或批判性），可显著提升其有效性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术的兴起使得推荐系统越来越多地面临AI生成内容与人工撰写内容共存的局面，亟需理解AI生成评论如何影响推荐系统性能及商业价值。

Method: 基于TripAdvisor酒店评论的大规模数据集，利用大语言模型（LLM）生成合成评论，区分用户中心（用户用AI润色自身评论）和平台中心（平台基于结构化元数据自动生成评论）两种AI内容引入路径，并在推荐系统的训练与部署阶段评估其影响。

Result: AI生成评论在文本特征上系统性区别于人工评论；两类AI评论均能提升无文本基线模型性能，但人工评论训练的模型始终表现最优；人工训练模型对AI内容泛化能力强，而AI训练模型在两类内容上均表现较差；语气策略（鼓励/建设/批判）显著提升平台生成评论的有效性。

Conclusion: 人工内容质量仍是推荐系统训练的黄金标准；平台应主动管控AI生成内容的生成与整合方式，以确保其增强而非削弱推荐鲁棒性与长期商业价值。

Abstract: The rise of generative AI technologies is reshaping content-based recommender systems (RSes), which increasingly encounter AI-generated content alongside human-authored content. This study examines how the introduction of AI-generated reviews influences RS performance and business outcomes. We analyze two distinct pathways through which AI content can enter RSes: user-centric, in which individuals use AI tools to refine their reviews, and platform-centric, in which platforms generate synthetic reviews directly from structured metadata. Using a large-scale dataset of hotel reviews from TripAdvisor, we generate synthetic reviews using LLMs and evaluate their impact across the training and deployment phases of RSes. We find that AI-generated reviews differ systematically from human-authored reviews across multiple textual dimensions. Although both user- and platform-centric AI reviews enhance RS performance relative to models without textual data, models trained on human reviews consistently achieve superior performance, underscoring the quality of authentic human data. Human-trained models generalize robustly to AI content, whereas AI-trained models underperform on both content types. Furthermore, tone-based framing strategies (encouraging, constructive, or critical) substantially enhance platform-generated review effectiveness. Our findings highlight the strategic importance of platform control in governing the generation and integration of AI-generated reviews, ensuring that synthetic content complements recommendation robustness and sustainable business value.

</details>


### [152] [Towards Trustworthy LLM-Based Recommendation via Rationale Integration](https://arxiv.org/abs/2601.02364)
*Chung Park,Taesan Kim,Hyeongjun Yun,Dongjoon Hong,Junui Hong,Kijung Park,MinCheol Cho,Mira Myong,Jihoon Oh,Min sung Choi*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型的推荐系统LLM-Rec，强调在推荐物品前先生成逻辑严谨、链式思维（CoT）风格的解释，以提升可解释性与推荐性能，并公开了带理由标注的推荐数据集。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统侧重准确性和短期参与度，忽视透明性与可信度；当前平台虽开始提供推荐理由，但多为事后生成，缺乏与推荐过程的深度整合。

Method: 构建基于大语言模型的推荐系统LLM-Rec，采用‘理由优先’（rationale-first）的指令微调范式，利用自标注的理由数据集，将理由生成置于物品预测之前，并以链式思维（CoT）形式建模理由。

Result: 在Amazon评论数据集的Fashion和Scientific子领域上，LLM-Rec显著优于多个强基线模型，同时提升了推荐效果与可解释性。

Conclusion: 将理由生成深度嵌入推荐流程（尤其是理由优先+CoT结构）是提升推荐系统可信性与性能的有效路径，所发布数据集有助于推动可解释推荐研究。

Abstract: Traditional recommender systems (RS) have been primarily optimized for accuracy and short-term engagement, often overlooking transparency and trustworthiness. Recently, platforms such as Amazon and Instagram have begun providing recommendation rationales to users, acknowledging their critical role in fostering trust and enhancing engagement; however, most existing systems still treat them as post-hoc artifacts. We propose an LLM-based recommender (LLM-Rec) that not only predicts items but also generates logically grounded rationales. Our approach leverages a self-annotated rationale dataset and instruction tuning in a rationale-first format, where the model generates an explanation before outputting the recommended item. By adopting this strategy and representing rationales in a chain-of-thought (CoT) style, LLM-Rec strengthens both interpretability and recommendation performance. Experiments on the Fashion and Scientific domains of the Amazon Review dataset demonstrate significant improvements over well-established baselines. To encourage reproducibility and future research, we publicly release a rationale-augmented recommendation dataset containing user histories, rationales, and recommended items.

</details>


### [153] [FUSE : Failure-aware Usage of Subagent Evidence for MultiModal Search and Recommendation](https://arxiv.org/abs/2601.02365)
*Tushar Vatsa,Vibha Belavadi,Priya Shanmugasundaram,Suhas Suresha,Dewang Sultania*

Main category: cs.IR

TL;DR: 本文提出FUSE框架，通过引入紧凑的接地设计表示（GDR）和七种上下文预算策略，提升多模态创意助手中的检索质量与系统效率，尤其在上下文压缩策略下取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 多模态创意助手中检索质量至关重要，但存在意图理解、内容类型选择、候选召回与结果排序等多个失败环节；同时原始图像处理成本高，需更高效的方法。

Method: 提出FUSE框架：1）用紧凑的接地设计表示（GDR）替代大部分原始图像提示；2）实现七种上下文预算策略（如上下文压缩、思维链推理、两阶段处理等）；3）构建管道归因层，对子代理信号进行多维度健康检查。

Result: 在788个评估查询上验证，上下文压缩策略表现最优：意图准确率93.3%，路由成功率86.8%（含回退），召回率99.4%，NDCG@5达88.5%。

Conclusion: 战略性上下文摘要（如上下文压缩）显著优于全面或极简的上下文策略，能有效提升多模态搜索与推荐的整体性能与鲁棒性。

Abstract: Multimodal creative assistants decompose user goals and route tasks to subagents for layout, styling, retrieval, and generation. Retrieval quality is pivotal, yet failures can arise at several stages: understanding user intent, choosing content types, finding candidates (recall), or ranking results. Meanwhile, sending and processing images is costly, making naive multimodal approaches impractical. We present FUSE: Failure-aware Usage of Subagent Evidence for MultiModal Search and Recommendation. FUSE replaces most raw-image prompting with a compact Grounded Design Representation (GDR): a selection aware JSON of canvas elements (image, text, shape, icon, video, logo), structure, styles, salient colors, and user selection provided by the Planner team. FUSE implements seven context budgeting strategies: comprehensive baseline prompting, context compression, chain-of-thought reasoning, mini-shot optimization, retrieval-augmented context, two-stage processing, and zero-shot minimalism. Finally, a pipeline attribution layer monitors system performance by converting subagent signals into simple checks: intent alignment, content-type/routing sanity, recall health (e.g., zero-hit and top-match strength), and ranking displacement analysis. We evaluate the seven context budgeting variants across 788 evaluation queries from diverse users and design templates (refer Figure 3). Our systematic evaluation reveals that Context Compression achieves optimal performance across all pipeline stages, with 93.3% intent accuracy, 86.8% routing success(with fallbacks), 99.4% recall, and 88.5% NDCG@5. This approach demonstrates that strategic context summarization outperforms both comprehensive and minimal contextualization strategies.

</details>


### [154] [TextBridgeGNN: Pre-training Graph Neural Network for Cross-Domain Recommendation via Text-Guided Transfer](https://arxiv.org/abs/2601.02366)
*Yiwen Chen,Yiqing Wu,Huishi Luo,Fuzhen Zhuang,Deqing Wang*

Main category: cs.IR

TL;DR: 本文提出TextBridgeGNN框架，利用文本作为跨域语义桥梁，通过多级图传播实现预训练图神经网络在推荐任务中的知识迁移，解决了ID嵌入不可迁移和图结构异构问题。


<details>
  <summary>Details</summary>
Motivation: ID嵌入因域隔离和图结构异构而难以跨域迁移，阻碍了图推荐模型的预训练与复用。

Method: 提出TextBridgeGNN：以文本为语义桥，结合多级图传播进行预训练（融合文本特征学习域特异与全局知识），并在微调阶段设计相似性迁移机制，将源域语义相关节点的ID嵌入迁移到目标域。

Result: 在跨域、多域及免训练设定下均优于现有方法，兼顾PLM语义增强与图协同过滤，无需微调语言模型或实时推理开销。

Conclusion: TextBridgeGNN有效打通了图推荐中ID嵌入与文本语义的壁垒，为可迁移、轻量化的预训练图推荐提供了新范式。

Abstract: Graph-based recommendation has achieved great success in recent years. The classical graph recommendation model utilizes ID embedding to store essential collaborative information. However, this ID-based paradigm faces challenges in transferring to a new domain, making it hard to build a pre-trained graph recommendation model. This phenomenon primarily stems from two inherent challenges: (1) the non-transferability of ID embeddings due to isolated domain-specific ID spaces, and (2) structural incompatibility between heterogeneous interaction graphs across domains.
  To address these issues, we propose TextBridgeGNN, a pre-training and fine-tuning framework that can effectively transfer knowledge from a pre-trained GNN to downstream tasks. We believe the key lies in how to build the relationship between domains. Specifically, TextBridgeGNN uses text as a semantic bridge to connect domains through multi-level graph propagation. During the pre-training stage, textual information is utilized to break the data islands formed by multiple domains, and hierarchical GNNs are designed to learn both domain-specific and domain-global knowledge with text features, ensuring the retention of collaborative signals and the enhancement of semantics. During the fine-tuning stage, a similarity transfer mechanism is proposed. This mechanism initializes ID embeddings in the target domain by transferring from semantically related nodes, successfully transferring the ID embeddings and graph pattern.
  Experiments demonstrate that TextBridgeGNN outperforms existing methods in cross-domain, multi-domain, and training-free settings, highlighting its ability to integrate Pre-trained Language Model (PLM)-driven semantics with graph-based collaborative filtering without costly language model fine-tuning or real-time inference overhead.

</details>


### [155] [Distillation-based Scenario-Adaptive Mixture-of-Experts for the Matching Stage of Multi-scenario Recommendation](https://arxiv.org/abs/2601.02368)
*Ruibing Wang,Shuhan Guo,Haotong Du,Quanming Yao*

Main category: cs.IR

TL;DR: 本文提出DSMOE模型，通过场景自适应投影模块和跨架构知识蒸馏框架，解决多场景推荐中匹配阶段的专家坍塌和头部场景参数主导问题，显著提升长尾场景的检索质量。


<details>
  <summary>Details</summary>
Motivation: Multi-gate Mixture-of-Experts (MMOE) 在排序阶段表现优异，但在匹配阶段因独立双塔结构的盲目优化和头部场景参数主导而受限，难以应对多场景尤其是长尾场景的挑战。

Method: 提出Distillation-based Scenario-Adaptive Mixture-of-Experts (DSMOE)，包含两个核心组件：1）场景自适应投影（SAP）模块，生成轻量、上下文相关的参数以防止长尾场景专家坍塌；2）跨架构知识蒸馏框架，用交互式教师模型指导双塔学生模型学习复杂匹配模式。

Result: 在真实数据集上的大量实验表明，DSMOE显著提升了稀疏、低表示度场景下的检索质量，优于现有方法。

Conclusion: DSMOE有效缓解了多场景匹配中的结构性与分布性瓶颈，尤其增强了对长尾场景的建模能力，为多场景推荐匹配阶段提供了新思路。

Abstract: Multi-scenario recommendation is pivotal for optimizing user experience across diverse contexts. While Multi-gate Mixture-of-Experts (MMOE) thrives in ranking, its transfer to the matching stage is hindered by the blind optimization inherent to independent two-tower architectures and the parameter dominance of head scenarios. To address these structural and distributional bottlenecks, we propose Distillation-based Scenario-Adaptive Mixture-of-Experts (DSMOE). Specially, we devise a Scenario-Adaptive Projection (SAP) module to generate lightweight, context-specific parameters, effectively preventing expert collapse in long-tail scenarios. Concurrently, we introduce a cross-architecture knowledge distillation framework, where an interaction-aware teacher guides the two-tower student to capture complex matching patterns. Extensive experiments on real-world datasets demonstrate DSMOE's superiority, particularly in significantly improving retrieval quality for under-represented, data-sparse scenarios.

</details>


### [156] [Improving News Recommendations through Hybrid Sentiment Modelling and Reinforcement Learning](https://arxiv.org/abs/2601.02372)
*Eunice Kingenga,Mike Wa Nkongolo*

Main category: cs.IR

TL;DR: 本文提出了一种结合混合情感分析与强化学习的自适应情感感知新闻推荐框架，以提升个性化推荐效果。


<details>
  <summary>Details</summary>
Motivation: 传统新闻推荐系统在处理多源新闻时存在情感歧义、词典不一致和上下文理解有限等问题，且常将情感作为次要特征，难以适应用户的情感偏好。

Method: 构建混合情感分析模型（融合VADER、AFINN、TextBlob和SentiWordNet），对BBC News数据集中的文章进行情感分类（正/负/中性），并将情感状态嵌入Q-learning架构中实现推荐策略学习。

Result: 该框架能有效识别并推荐符合用户情感偏好的新闻，并通过迭代Q-learning持续优化个性化效果；实验表明其具备可行性、可解释性和自适应性。

Conclusion: 将混合情感建模与强化学习结合，为以用户为中心的新闻推荐提供了一种新颖且有效的解决方案。

Abstract: News recommendation systems rely on automated sentiment analysis to personalise content and enhance user engagement. Conventional approaches often struggle with ambiguity, lexicon inconsistencies, and limited contextual understanding, particularly in multi-source news environments. Existing models typically treat sentiment as a secondary feature, reducing their ability to adapt to users' affective preferences. To address these limitations, this study develops an adaptive, sentiment-aware news recommendation framework by integrating hybrid sentiment analysis with reinforcement learning. Using the BBC News dataset, a hybrid sentiment model combines VADER, AFINN, TextBlob, and SentiWordNet scores to generate robust article-level sentiment estimates. Articles are categorised as positive, negative, or neutral, and these sentiment states are embedded within a Q-learning architecture to guide the agent in learning optimal recommendation policies. The proposed system effectively identifies and recommends articles with aligned emotional profiles while continuously improving personalisation through iterative Q-learning updates. The results demonstrate that coupling hybrid sentiment modelling with reinforcement learning provides a feasible, interpretable, and adaptive approach for user-centred news recommendation.

</details>


### [157] [A Lay User Explainable Food Recommendation System Based on Hybrid Feature Importance Extraction and Large Language Models](https://arxiv.org/abs/2601.02374)
*Melissa Tessa,Diderot D. Cidjeu,Rachele Carli,Sarah Abchiche,Ahmad Aldarwishd,Igor Tchappi,Amro Najjar*

Main category: cs.IR

TL;DR: This paper proposes a post-hoc explanation method for food recommendation systems by integrating LLMs with SHAP-based key variable extraction, aiming to generate dynamic, understandable, and trustworthy explanations for lay users.


<details>
  <summary>Details</summary>
Motivation: To improve user trust and transparency in food recommendation systems by providing more elaborated, dynamic, and comprehensible explanations tailored for lay users.

Method: Combines Large Language Models (LLMs) with a hybrid SHAP-based extraction of key variables to generate post-hoc explanations for food recommendation results.

Result: The proposed approach yields dynamic, convincing, and more comprehensive explanations compared to existing methods in the literature, enhancing understandability for lay users.

Conclusion: Integrating LLMs with SHAP improves the quality and accessibility of explanations in food recommendation systems, thereby boosting user trust and system transparency.

Abstract: Large Language Models (LLM) have experienced strong development in recent years, with varied applications. This paper uses LLMs to develop a post-hoc process that provides more elaborated explanations of the results of food recommendation systems. By combining LLM with a hybrid extraction of key variables using SHAP, we obtain dynamic, convincing and more comprehensive explanations to lay user, compared to those in the literature. This approach enhances user trust and transparency by making complex recommendation outcomes easier to understand for a lay user.

</details>


### [158] [TAG-HGT: A Scalable and Cost-Effective Framework for Inductive Cold-Start Academic Recommendation](https://arxiv.org/abs/2601.02381)
*Zhexiang Li*

Main category: cs.IR

TL;DR: 本文提出TAG-HGT框架，结合冻结大语言模型（DeepSeek-V3）的语义能力与轻量级异构图Transformer（HGT），通过跨视图对比学习实现高效冷启动学术推荐，在保持高精度（Recall@10达91.97%）的同时，将推理延迟降低5个数量级、成本降低99.9%。


<details>
  <summary>Details</summary>
Motivation: 工业级学术平台面临大量无交互历史的新学者冷启动推荐难题；现有生成式图模型虽语义能力强，但推理延迟高、计算成本大，难以部署于实时百万级场景。

Method: 提出神经符号融合的TAG-HGT框架，采用‘语义优先、结构精修’解耦范式：用冻结LLM（DeepSeek-V3）离线提取语义，再通过跨视图对比学习（CVCL）将其知识蒸馏至轻量HGT；强调语义提供全局召回、结构提供局部判别。

Result: 在OpenAlex数据集严格时间机器协议下，Recall@10达91.97%，较纯结构基线提升20.7%；推理延迟从780秒降至1.73毫秒（加速4.5×10⁵倍），每千次查询成本从约1.50美元降至<0.001美元。

Conclusion: TAG-HGT成功弥合了生成质量与工业可扩展性之间的鸿沟，以极低成本实现高精度冷启动推荐，推动高质量学术推荐技术普惠化落地。

Abstract: Inductive cold-start recommendation remains the "Achilles' Heel" of industrial academic platforms, where thousands of new scholars join daily without historical interaction records. While recent Generative Graph Models (e.g., HiGPT, OFA) demonstrate promising semantic capabilities, their prohibitive inference latency (often exceeding 13 minutes per 1,000 requests) and massive computational costs render them practically undeployable for real-time, million-scale applications. To bridge this gap between generative quality and industrial scalability, we propose TAG-HGT, a cost-effective neuro-symbolic framework. Adopting a decoupled "Semantics-First, Structure-Refined" paradigm, TAG-HGT utilizes a frozen Large Language Model (DeepSeek-V3) as an offline semantic factory and distills its knowledge into a lightweight Heterogeneous Graph Transformer (HGT) via Cross-View Contrastive Learning (CVCL). We present a key insight: while LLM semantics provide necessary global recall, structural signals offer the critical local discrimination needed to distinguish valid collaborators from semantically similar but socially unreachable strangers in dense embedding spaces. Validated under a strict Time-Machine Protocol on the massive OpenAlex dataset, TAG-HGT achieves a SOTA System Recall@10 of 91.97%, outperforming structure-only baselines by 20.7%. Most significantly, from an industrial perspective, TAG-HGT reduces inference latency by five orders of magnitude ($4.5 \times 10^{5}\times$) compared to generative baselines (from 780s down to 1.73 ms), and slashes inference costs from $\sim$$1.50 to $<$$0.001 per 1k queries. This 99.9% cost reduction democratizes high-precision academic recommendation.

</details>


### [159] [Tree of Preferences for Diversified Recommendation](https://arxiv.org/abs/2601.02386)
*Hanyang Yuan,Ning Tang,Tongya Zheng,Jiarong Xu,Xintong Hu,Renhong Huang,Shunyu Liu,Jiacong Hu,Jiawei Chen,Mingli Song*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型（LLM）的新方法，通过构建‘偏好树’（ToP）来挖掘用户未充分展现的偏好，从而提升推荐系统的多样性与相关性。


<details>
  <summary>Details</summary>
Motivation: 现有多样化推荐方法受限于观测数据的偏差，难以捕捉用户未显化的潜在偏好，导致推荐多样性不足。

Method: 提出‘偏好树’（ToP）结构建模用户从粗到细的偏好；利用LLM推理用户行为背后的潜在动机；生成匹配未探索偏好的合成交互数据，并用于训练通用推荐模型；引入动态用户选择机制提升效率。

Result: 在多样性和相关性评估中，该方法在大多数情况下优于现有方法，部分指标接近最优，且推理延迟合理。

Conclusion: 从数据偏差视角出发，结合LLM的世界知识与推理能力，可有效挖掘隐性偏好，显著提升多样化推荐效果。

Abstract: Diversified recommendation has attracted increasing attention from both researchers and practitioners, which can effectively address the homogeneity of recommended items. Existing approaches predominantly aim to infer the diversity of user preferences from observed user feedback. Nonetheless, due to inherent data biases, the observed data may not fully reflect user interests, where underexplored preferences can be overwhelmed or remain unmanifested. Failing to capture these preferences can lead to suboptimal diversity in recommendations. To fill this gap, this work aims to study diversified recommendation from a data-bias perspective. Inspired by the outstanding performance of large language models (LLMs) in zero-shot inference leveraging world knowledge, we propose a novel approach that utilizes LLMs' expertise to uncover underexplored user preferences from observed behavior, ultimately providing diverse and relevant recommendations. To achieve this, we first introduce Tree of Preferences (ToP), an innovative structure constructed to model user preferences from coarse to fine. ToP enables LLMs to systematically reason over the user's rationale behind their behavior, thereby uncovering their underexplored preferences. To guide diversified recommendations using uncovered preferences, we adopt a data-centric approach, identifying candidate items that match user preferences and generating synthetic interactions that reflect underexplored preferences. These interactions are integrated to train a general recommender for diversification. Moreover, we scale up overall efficiency by dynamically selecting influential users during optimization. Extensive evaluations of both diversity and relevance show that our approach outperforms existing methods in most cases and achieves near-optimal performance in others, with reasonable inference latency.

</details>


### [160] [Socially-Aware Recommender Systems Mitigate Opinion Clusterization](https://arxiv.org/abs/2601.02412)
*Lukas Schüepp,Carmen Amo Alonso,Florian Dörfler,Giulia De Pasquale*

Main category: cs.IR

TL;DR: 本文提出了一种社交网络感知的推荐系统，通过建模用户-创作者-算法之间的反馈循环，并利用用户社交网络拓扑结构来促进内容多样性，从而缓解信息茧房和观点极化问题。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统加剧了用户偏好演化、创作者内容调整与算法推荐之间的反馈循环，导致信息茧房和观点极化。

Method: 构建一个社交网络感知的推荐系统，显式建模用户-创作者反馈交互，并策略性利用用户社交网络拓扑以提升内容多样性。

Result: 理论证明推荐内容对用户观点的影响正向关联于观点聚类；实验表明该方法能有效缓解观点极化与聚类现象。

Conclusion: 在推荐系统中纳入并利用用户社交网络结构，对缓解信息茧房、平衡个性化与多样性至关重要。

Abstract: Recommender systems shape online interactions by matching users with creators content to maximize engagement. Creators, in turn, adapt their content to align with users preferences and enhance their popularity. At the same time, users preferences evolve under the influence of both suggested content from the recommender system and content shared within their social circles. This feedback loop generates a complex interplay between users, creators, and recommender algorithms, which is the key cause of filter bubbles and opinion polarization. We develop a social network-aware recommender system that explicitly accounts for this user-creators feedback interaction and strategically exploits the topology of the user's own social network to promote diversification. Our approach highlights how accounting for and exploiting user's social network in the recommender system design is crucial to mediate filter bubble effects while balancing content diversity with personalization. Provably, opinion clusterization is positively correlated with the influence of recommended content on user opinions. Ultimately, the proposed approach shows the power of socially-aware recommender systems in combating opinion polarization and clusterization phenomena.

</details>


### [161] [A Dynamic Retrieval-Augmented Generation System with Selective Memory and Remembrance](https://arxiv.org/abs/2601.02428)
*Okan Bursa*

Main category: cs.IR

TL;DR: 本文提出了一种名为Adaptive RAG Memory（ARM）的动态检索增强生成框架，通过模拟人类记忆的巩固与遗忘机制，实现对检索记忆的自适应管理，在保持高精度的同时显著提升参数效率与系统可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG依赖静态向量索引，缺乏对知识重要性的动态评估与记忆更新能力；受认知科学中选择性记忆巩固与遗忘机制启发，需构建更高效、可解释、低开销的动态记忆机制。

Method: 提出ARM框架，用受认知原理启发的动态记忆基质替代静态向量索引：支持基于检索频率的选择性记忆巩固（强化常用项）与渐进式遗忘（弱化冷门项）；实现嵌入权重运行时可配置、可调节，并保障鲁棒性。

Result: 在轻量级检索基准上，ARM以仅约22M嵌入参数达到近SOTA性能（NDCG@5≈0.940，Recall@5=1.000），为超高效模型（<25M参数）中效率最优；Llama 3.1+静态RAG覆盖率达67.2%，GPT-4o+动态策略响应最快（8.2s），覆盖率达58.7%；ARM无需重训练生成器即具自正则化记忆增长与可解释保留动力学。

Conclusion: ARM在准确性、延迟与内存效率间提供实用权衡，兼具竞争性性能、自我调节的记忆扩展能力及无需生成器重训练的可解释性，适用于生产与研究场景的RAG系统。

Abstract: We introduce \emph{Adaptive RAG Memory} (ARM), a retrieval-augmented generation (RAG) framework that replaces a static vector index with a \emph{dynamic} memory substrate governed by selective remembrance and decay. Frequently retrieved items are consolidated and protected from forgetting, while rarely used items gradually decay, inspired by cognitive consolidation and forgetting principles. On a lightweight retrieval benchmark, ARM reaches near state-of-the-art performance (e.g., NDCG@5 $\approx$ 0.940, Recall@5 $=1.000$) with only $\sim$22M parameters in the embedding layer, achieving the best efficiency among ultra-efficient models ($<$25M parameters). In addition, we compare static vs. dynamic RAG combinations across Llama 3.1 and GPT-4o. Llama 3.1 with static RAG achieves the highest key-term coverage (67.2\%) at moderate latency, while GPT-4o with a dynamic selective retrieval policy attains the fastest responses (8.2s on average) with competitive coverage (58.7\%). We further present an engineering optimization of the DynamicRAG implementation, making embedding weights configurable, adjustable at runtime, and robust to invalid settings.
  ARM yields competitive accuracy, self-regularizing memory growth, and interpretable retention dynamics without retraining the generator\color{black} and provides practical trade-off between quality, latency and memory efficiency for production and research RAG system.

</details>


### [162] [CREAM: Continual Retrieval on Dynamic Streaming Corpora with Adaptive Soft Memory](https://arxiv.org/abs/2601.02708)
*HuiJeong Son,Hyeongu Kang,Sunho Kim,Subeen Ho,SeongKu Kang,Dongha Lee,Susik Yoon*

Main category: cs.IR

TL;DR: 本文提出CREAM框架，一种用于内存式持续检索的自监督方法，通过动态软内存建模流式查询与文档语义演化，无需真实标签即可适应新主题，在无监督设置下显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的持续信息检索方法依赖固定查询及其标注相关文档，泛化能力差，难以应对现实场景中无标注、新主题的动态数据流。

Method: 提出CREAM自监督框架，包含细粒度相似性估计、正则化聚类原型构建和分层核心集采样三项关键技术，将查询与文档语义映射到动态结构化软内存中，实现对已见与未见主题的无监督适应。

Result: 在两个基准数据集上，CREAM在无标签设置下平均提升最强基线方法27.79%（Success@5）和44.5%（Recall@10），性能媲美甚至超越监督方法。

Conclusion: CREAM有效解决了动态数据流中无监督持续检索的泛化与适应性难题，为实际部署提供了可行方案。

Abstract: Information retrieval (IR) in dynamic data streams is emerging as a challenging task, as shifts in data distribution degrade the performance of AI-powered IR systems. To mitigate this issue, memory-based continual learning has been widely adopted for IR. However, existing methods rely on a fixed set of queries with ground-truth relevant documents, which limits generalization to unseen queries and documents, making them impractical for real-world applications. To enable more effective learning with unseen topics of a new corpus without ground-truth labels, we propose CREAM, a self-supervised framework for memory-based continual retrieval. CREAM captures the evolving semantics of streaming queries and documents into dynamically structured soft memory and leverages it to adapt to both seen and unseen topics in an unsupervised setting. We realize this through three key techniques: fine-grained similarity estimation, regularized cluster prototyping, and stratified coreset sampling. Experiments on two benchmark datasets demonstrate that CREAM exhibits superior adaptability and retrieval accuracy, outperforming the strongest method in a label-free setting by 27.79\% in Success@5 and 44.5\% in Recall@10 on average, and achieving performance comparable to or even exceeding that of supervised methods.

</details>


### [163] [Ahead of the Spread: Agent-Driven Virtual Propagation for Early Fake News Detection](https://arxiv.org/abs/2601.02750)
*Bincheng Gu,Min Gao,Junliang Yu,Zongwei Wang,Zhiyi Liu,Kai Shu,Hongyu Zhang*

Main category: cs.IR

TL;DR: 本文提出AVOID框架，通过LLM驱动的多角色智能体模拟虚假新闻早期传播行为，生成虚拟传播信号以增强内容分析，从而实现更早、更准的虚假新闻检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于传播动态的虚假新闻检测方法在早期阶段受限于缺乏可观测的传播信号，难以有效应用。

Method: 提出AVOID框架，利用具有差异化角色和数据驱动人设的LLM智能体，主动模拟虚假新闻早期传播轨迹，生成虚拟传播证据，并通过去噪引导的融合策略将虚拟传播信号与文本语义对齐。

Result: 在多个基准数据集上的实验表明，AVOID持续优于当前最优方法，验证了虚拟传播增强的有效性。

Conclusion: 虚拟传播模拟是一种可行且有效的早期虚假新闻检测新范式，为缺乏真实传播数据的场景提供了实用解决方案。

Abstract: Early detection of fake news is critical for mitigating its rapid dissemination on social media, which can severely undermine public trust and social stability. Recent advancements show that incorporating propagation dynamics can significantly enhance detection performance compared to previous content-only approaches. However, this remains challenging at early stages due to the absence of observable propagation signals. To address this limitation, we propose AVOID, an \underline{a}gent-driven \underline{v}irtual pr\underline{o}pagat\underline{i}on for early fake news \underline{d}etection. AVOID reformulates early detection as a new paradigm of evidence generation, where propagation signals are actively simulated rather than passively observed. Leveraging LLM-powered agents with differentiated roles and data-driven personas, AVOID realistically constructs early-stage diffusion behaviors without requiring real propagation data. The resulting virtual trajectories provide complementary social evidence that enriches content-based detection, while a denoising-guided fusion strategy aligns simulated propagation with content semantics. Extensive experiments on benchmark datasets demonstrate that AVOID consistently outperforms state-of-the-art baselines, highlighting the effectiveness and practical value of virtual propagation augmentation for early fake news detection. The code and data are available at https://github.com/Ironychen/AVOID.

</details>


### [164] [Netflix Artwork Personalization via LLM Post-training](https://arxiv.org/abs/2601.02764)
*Hyunji Nam,Sejoon Oh,Emma Kong,Yesu Feng,Moumita Bhattacharya*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型（LLM）的个性化封面图推荐方法，针对用户偏好异质性，通过后训练LLM实现为每个用户选择最契合其偏好的影视标题封面，显著提升推荐满意度与参与度。


<details>
  <summary>Details</summary>
Motivation: 娱乐平台（如Netflix）中用户偏好高度异质，同一影视标题的多种艺术封面对不同用户吸引力差异大，现有‘一刀切’推荐策略效果受限，亟需细粒度个性化封面推荐。

Method: 对预训练大语言模型（如Llama 3.1 8B）进行后训练，建模用户-标题-封面三元关系，使模型能根据用户历史偏好和标题多维语义（主题、情绪、风格等），为每个用户从多个候选封面中选出最优匹配项。

Result: 在11万样本训练、5千对用户-标题测试集上，后训练LLM相较Netflix线上生产模型提升3–5%推荐准确率/满意度指标。

Conclusion: LLM具备建模用户与视觉内容细粒度偏好的潜力，后训练范式可有效支撑个性化艺术封面推荐，为下一代沉浸式、多模态个性化系统提供新路径。

Abstract: Large language models (LLMs) have demonstrated success in various applications of user recommendation and personalization across e-commerce and entertainment. On many entertainment platforms such as Netflix, users typically interact with a wide range of titles, each represented by an artwork. Since users have diverse preferences, an artwork that appeals to one type of user may not resonate with another with different preferences. Given this user heterogeneity, our work explores the novel problem of personalized artwork recommendations according to diverse user preferences. Similar to the multi-dimensional nature of users' tastes, titles contain different themes and tones that may appeal to different viewers. For example, the same title might feature both heartfelt family drama and intense action scenes. Users who prefer romantic content may like the artwork emphasizing emotional warmth between the characters, while those who prefer action thrillers may find high-intensity action scenes more intriguing. Rather than a one-size-fits-all approach, we conduct post-training of pre-trained LLMs to make personalized artwork recommendations, selecting the most preferred visual representation of a title for each user and thereby improving user satisfaction and engagement. Our experimental results with Llama 3.1 8B models (trained on a dataset of 110K data points and evaluated on 5K held-out user-title pairs) show that the post-trained LLMs achieve 3-5\% improvements over the Netflix production model, suggesting a promising direction for granular personalized recommendations using LLMs.

</details>


### [165] [COFFEE: COdesign Framework for Feature Enriched Embeddings in Ads-Ranking Systems](https://arxiv.org/abs/2601.02807)
*Sohini Roychowdhury,Doris Wang,Qian Ge,Joy Mu,Srihari Reddy*

Main category: cs.IR

TL;DR: 本文提出了一种三维框架，通过融合多源用户行为数据（如内容观看与广告曝光）、延长用户历史序列、以及引入多模态属性增强事件表征，在不增加推理开销的前提下显著提升广告推荐效果。实验表明，该方法在AUC和CTR预测上均有明显提升。


<details>
  <summary>Details</summary>
Motivation: 商业广告推荐模型需要多样化、丰富的数据源来准确刻画用户兴趣（尤其是互动前后），同时兼顾表征的新鲜度与可扩展性；现有方法在多源融合、历史长度与事件丰富度三方面存在不足。

Method: 提出一个三维框架：1）融合多样化的用户行为事件源（如内容观看 vs 广告曝光）；2）利用更长的用户历史序列；3）为事件添加属性信息及多模态嵌入。通过ROI分析比较不同源的效果，并在不增加模型推理复杂度前提下实现表征增强。

Result: 相比有机行为源（如内容观看），广告曝光源经本框架增强后，AUC提升1.56–2倍，缩放曲线斜率显著提高；在短序列（100–10K）下仍有效；CTR预测AUC相对基线提升0.56%，并改善长序列与离线用户-广告表征的缩放分辨率。

Conclusion: 多源广告曝光事件的结构化增强是提升广告推荐模型性能与可扩展性的高效途径，三维协同设计可在零推理开销增长下达成显著收益。

Abstract: Diverse and enriched data sources are essential for commercial ads-recommendation models to accurately assess user interest both before and after engagement with content. While extended user-engagement histories can improve the prediction of user interests, it is equally important to embed activity sequences from multiple sources to ensure freshness of user and ad-representations, following scaling law principles. In this paper, we present a novel three-dimensional framework for enhancing user-ad representations without increasing model inference or serving complexity. The first dimension examines the impact of incorporating diverse event sources, the second considers the benefits of longer user histories, and the third focuses on enriching data with additional event attributes and multi-modal embeddings. We assess the return on investment (ROI) of our source enrichment framework by comparing organic user engagement sources, such as content viewing, with ad-impression sources. The proposed method can boost the area under curve (AUC) and the slope of scaling curves for ad-impression sources by 1.56 to 2 times compared to organic usage sources even for short online-sequence lengths of 100 to 10K. Additionally, click-through rate (CTR) prediction improves by 0.56% AUC over the baseline production ad-recommendation system when using enriched ad-impression event sources, leading to improved sequence scaling resolutions for longer and offline user-ad representations.

</details>


### [166] [HarmonRank: Ranking-aligned Multi-objective Ensemble for Live-streaming E-commerce Recommendation](https://arxiv.org/abs/2601.02955)
*Boyang Xia,Zhou Yu,Zhiliang Zhu,Hanxiao Sun,Biyun Han,Jun Wang,Runnan Liu,Wenwu Ou*

Main category: cs.IR

TL;DR: 本文提出HarmonRank框架，解决直播电商推荐中多目标排序的对齐问题，通过可微分排序技术和两步关系感知集成方案，在快手平台实现超2%的购买转化提升。


<details>
  <summary>Details</summary>
Motivation: 直播电商推荐需平衡购买与用户-主播互动等多目标，但现有基于独立二分类损失的集成模型存在优化目标（AUC）不一致和忽略目标间行为依赖关系两大问题。

Method: 提出HarmonRank框架：1）将AUC建模为秩和问题，采用可微分排序技术实现面向排序的优化；2）设计两步关系感知集成方案，显式建模评论与购买等目标间的行为相关性。

Result: 在两个工业数据集上离线实验及线上A/B测试均显著优于SOTA方法；已在快手（4亿DAU）直播电商推荐平台全量部署，带来超2%的购买转化提升。

Conclusion: HarmonRank通过同时实现‘与排序任务对齐’和‘多目标间对齐’，有效提升了直播电商多目标推荐效果，具备强实用性与落地价值。

Abstract: Recommendation for live-streaming e-commerce is gaining increasing attention due to the explosive growth of the live streaming economy. Different from traditional e-commerce, live-streaming e-commerce shifts the focus from products to streamers, which requires ranking mechanism to balance both purchases and user-streamer interactions for long-term ecology. To trade off multiple objectives, a popular solution is to build an ensemble model to integrate multi-objective scores into a unified score. The ensemble model is usually supervised by multiple independent binary classification losses of all objectives. However, this paradigm suffers from two inherent limitations. First, the optimization direction of the binary classification task is misaligned with the ranking task (evaluated by AUC). Second, this paradigm overlooks the alignment between objectives, e.g., comment and buy behaviors are partially dependent which can be revealed in labels correlations. The model can achieve better trade-offs if it learns the aligned parts of ranking abilities among different objectives.
  To mitigate these limitations, we propose a novel multi-objective ensemble framework HarmonRank to fulfill both alignment to the ranking task and alignment among objectives. For alignment to ranking, we formulate ranking metric AUC as a rank-sum problem and utilize differentiable ranking techniques for ranking-oriented optimization. For inter-objective alignment, we change the original one-step ensemble paradigm to a two-step relation-aware ensemble scheme.
  Extensive offline experiments results on two industrial datasets and online experiments demonstrate that our approach significantly outperforms existing state-of-the-art methods. The proposed method has been fully deployed in Kuaishou's live-streaming e-commerce recommendation platform with 400 million DAUs, contributing over 2% purchase gain.

</details>


### [167] [Auditing Search Query Suggestion Bias Through Recursive Algorithm Interrogation](https://arxiv.org/abs/2601.02962)
*Fabian Haak,Philipp Schaer*

Main category: cs.IR

TL;DR: 本文提出了一种新的搜索查询建议偏差识别方法，通过递归算法生成建议树，挖掘更深层、次级的查询建议，以增强数据基础并分析政治领域人物相关搜索中的主题群体偏差。


<details>
  <summary>Details</summary>
Motivation: 现有搜索查询建议偏差研究受限于上下文稀疏和每查询仅10条建议的数据量不足，导致偏差识别困难。

Method: 采用递归算法探查技术构建搜索建议树，获取更深层的次级建议，并基于这些建议分析政治人物搜索中的主题群体偏差。

Result: 该方法扩展了可用于偏差分析的数据基础，支持对更细微、次级建议中潜在偏见的识别与评估。

Conclusion: 相比依赖后续搜索的传统方法，本文提出的建议树方法为搜索建议偏差研究提供了新路径，尤其适用于提升人物类政治搜索中群体偏差分析的深度与可靠性。

Abstract: Despite their important role in online information search, search query suggestions have not been researched as much as most other aspects of search engines. Although reasons for this are multi-faceted, the sparseness of context and the limited data basis of up to ten suggestions per search query pose the most significant problem in identifying bias in search query suggestions. The most proven method to reduce sparseness and improve the validity of bias identification of search query suggestions so far is to consider suggestions from subsequent searches over time for the same query. This work presents a new, alternative approach to search query bias identification that includes less high-level suggestions to deepen the data basis of bias analyses. We employ recursive algorithm interrogation techniques and create suggestion trees that enable access to more subliminal search query suggestions. Based on these suggestions, we investigate topical group bias in person-related searches in the political domain.

</details>


### [168] [Parallel Latent Reasoning for Sequential Recommendation](https://arxiv.org/abs/2601.03153)
*Jiakai Tang,Xu Chen,Wen Chen,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 本文提出了一种名为Parallel Latent Reasoning (PLR)的新框架，通过在潜在空间中并行探索多条推理路径来提升序列推荐中的用户偏好建模能力，克服了传统基于深度扩展的推理方法的边际效益递减问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度扩展的隐式推理方法在序列推荐中存在推理深度增加时收益递减的问题，难以有效捕捉稀疏行为序列背后的复杂用户偏好。

Method: PLR框架通过可学习的触发令牌在连续潜在空间中构建并行推理流，利用全局推理正则化保持各流多样性，并通过混合多流输出的聚合机制自适应融合结果。

Result: 在三个真实数据集上的大量实验表明，PLR显著优于当前最优基线方法，同时保持实时推理效率；理论分析也证实其能提升泛化能力。

Conclusion: PLR首次将宽度级计算扩展引入序列推荐的隐式推理，为超越现有深度扩展范式、增强推理能力开辟了新方向。

Abstract: Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose \textbf{Parallel Latent Reasoning (PLR)}, a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.

</details>


### [169] [Fine-tuning Small Language Models as Efficient Enterprise Search Relevance Labelers](https://arxiv.org/abs/2601.03211)
*Yue Kang,Zhuoyi Huang,Benji Schussheim,Diana Licon,Dina Atia,Shixing Cao,Jacob Danovitch,Kunho Kim,Billy Norcilien,Jonah Karpman,Mahmound Sayed,Mike Taylor,Tao Sun,Pavel Metrikov,Vipul Agarwal,Chris Quirk,Ye-Yi Wang,Nick Craswell,Irene Shaffer,Tianwei Chen,Sulaiman Vesal,Soundar Srinivasan*

Main category: cs.IR

TL;DR: 本文提出了一种利用合成数据蒸馏小型语言模型（SLM）用于企业搜索中高效、低成本、高质量相关性标注的方法，性能媲美甚至优于教师大模型（LLM），吞吐量提升17倍，成本降低19倍。


<details>
  <summary>Details</summary>
Motivation: 企业搜索中缺乏高质量、可获取的标注数据，人工标注成本高、规模受限，而直接使用大模型（LLM）进行标注又存在效率低、成本高的问题。

Method: 利用教师LLM从种子文档生成真实企业查询，用BM25检索难负样本，并由同一教师LLM打相关性分数，构建合成数据集；再将该数据集蒸馏训练小型语言模型（SLM），得到轻量级相关性标注器。

Result: 在923对人工标注的企业查询-文档对基准上，蒸馏后的SLM与人类判断的一致性媲美或优于教师LLM；吞吐量提升17倍，成本降低19倍。

Conclusion: 该方法实现了企业级检索中可扩展、高性价比的相关性标注，支持真实场景下的快速离线评估与迭代。

Abstract: In enterprise search, building high-quality datasets at scale remains a central challenge due to the difficulty of acquiring labeled data. To resolve this challenge, we propose an efficient approach to fine-tune small language models (SLMs) for accurate relevance labeling, enabling high-throughput, domain-specific labeling comparable or even better in quality to that of state-of-the-art large language models (LLMs). To overcome the lack of high-quality and accessible datasets in the enterprise domain, our method leverages on synthetic data generation. Specifically, we employ an LLM to synthesize realistic enterprise queries from a seed document, apply BM25 to retrieve hard negatives, and use a teacher LLM to assign relevance scores. The resulting dataset is then distilled into an SLM, producing a compact relevance labeler. We evaluate our approach on a high-quality benchmark consisting of 923 enterprise query-document pairs annotated by trained human annotators, and show that the distilled SLM achieves agreement with human judgments on par with or better than the teacher LLM. Furthermore, our fine-tuned labeler substantially improves throughput, achieving 17 times increase while also being 19 times more cost-effective. This approach enables scalable and cost-effective relevance labeling for enterprise-scale retrieval applications, supporting rapid offline evaluation and iteration in real-world settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [170] [Textual Explanations and Their Evaluations for Reinforcement Learning Policy](https://arxiv.org/abs/2601.02514)
*Ahmad Terra,Mohit Ahmed,Rafia Inam,Elena Fersman,Martin Törngren*

Main category: cs.AI

TL;DR: 本文提出了一种新的可解释强化学习（XRL）框架，利用大语言模型（LLM）和聚类生成文本解释，并将其转化为透明规则，结合专家知识与自动谓词生成器提升解释质量与可评估性，在多个环境及工业场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有XRL中文本解释的正确性难以保障，且缺乏系统、定量的评估方法；同时，自主策略解释（APE）等方法存在局限性。

Method: 提出一个XRL框架：1）用LLM和状态聚类生成文本解释；2）将解释条件转化为透明规则；3）引入专家知识和自动谓词生成器提取状态语义；4）设计两种规则精炼技术以减少冲突、提升保真度；5）在开源环境与电信工业案例中开展实验验证。

Result: 在三个开源环境和一个电信实际用例中验证了框架的可复现性与工业适用性；生成的透明规则在部分任务上达到满意性能；实现了对文本解释的系统化、定量评估。

Conclusion: 该框架弥补了现有XRL方法在解释正确性、可评估性与工业落地方面的不足，为XRL提供了更可靠、可验证、可集成专家知识的新范式。

Abstract: Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.

</details>


### [171] [SimpleMem: Efficient Lifelong Memory for LLM Agents](https://arxiv.org/abs/2601.02553)
*Jiaqi Liu,Yaofeng Su,Peng Xia,Siwei Han,Zeyu Zheng,Cihang Xie,Mingyu Ding,Huaxiu Yao*

Main category: cs.AI

TL;DR: 本文提出SimpleMem，一种基于语义无损压缩的高效记忆框架，通过三阶段流程（语义结构化压缩、递归记忆整合、自适应查询感知检索）提升LLM智能体在长期交互中的记忆效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆方法存在冗余高或推理开销大等问题，难以兼顾信息保留与效率。

Method: 提出SimpleMem框架，包含语义结构化压缩（熵感知过滤）、递归记忆整合（异步抽象聚合）和自适应查询感知检索（动态范围调整）三个阶段。

Result: 在基准数据集上，相比基线方法，F1值平均提升26.4%，推理token消耗最多降低30倍，显著提升准确率、检索效率与推理成本平衡性。

Conclusion: SimpleMem实现了语义保真度与计算效率的协同优化，为复杂环境中LLM智能体的长期可靠交互提供了可扩展的记忆解决方案。

Abstract: To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.

</details>


### [172] [Orchestral AI: A Framework for Agent Orchestration](https://arxiv.org/abs/2601.02577)
*Alexander Roman,Jacob Roman*

Main category: cs.AI

TL;DR: Orchestral 是一个轻量级 Python 框架，旨在为多 LLM 提供商提供统一、类型安全的代理构建接口，解决工具调用碎片化、格式不兼容和可移植性差等问题。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 代理框架存在厂商锁定、多包生态复杂、API 和消息格式碎片化、工具调用行为不一致等问题，导致可移植性、可复现性和工程可靠性下降。

Method: 设计统一的消息/工具/LLM 使用抽象表示；基于 Python 类型提示自动生成工具 schema；采用同步执行模型支持流式响应；模块化架构分离 provider、tool、orchestration 和 interface 层。

Result: 实现了跨主流 LLM 提供商（如 OpenAI、Anthropic 等）的无缝工具调用；支持上下文压缩、沙盒工作区、用户审批、子代理、记忆管理及 MCP 集成等高级能力；显著降低框架引入的复杂性。

Conclusion: Orchestral 在保持轻量与简洁的同时，提供了媲美大型框架的代理能力，兼顾科研可复现性与生产部署需求，是构建可移植、可靠、易调试 LLM 代理系统的实用解决方案。

Abstract: The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration.

</details>


### [173] [An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices](https://arxiv.org/abs/2601.02641)
*Jeiyoon Park,Daehwan Lee,Changmin Yeo,Yongshin Han,Minseop Kim*

Main category: cs.AI

TL;DR: 本文通过大量实验研究了在移动设备上部署AI模型的两个关键问题：模型选择与资源消耗，以及模型的领域自适应能力，并构建了LiveChatBench基准测试集，在多个移动设备上验证了所提方法在实时直播聊天翻译任务中可媲美GPT-5.1等商用模型。


<details>
  <summary>Details</summary>
Motivation: 尽管边缘AI高效，但其实际部署中涉及的CPU利用率、热条件等现实问题缺乏研究。

Method: 通过在五种移动设备上开展大量实验，构建LiveChatBench（含1000对韩英平行句对）基准，评估不同模型的资源消耗与领域自适应能力。

Result: 在目标任务（直播聊天消息翻译）上，所提方法性能媲美GPT-5.1等商用模型，同时兼顾设备资源约束。

Conclusion: 面向特定任务优化的轻量级模型可在资源受限的移动设备上实现高性能，为端侧AI部署提供实用指导。

Abstract: Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community.

</details>


### [174] [AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents](https://arxiv.org/abs/2601.02643)
*Mehmet Kurmaz*

Main category: cs.AI

TL;DR: 本文提出了一种偏好感知的查询修复方法，用于解决工具调用型对话代理在查询结构化数据库时面临的欠指定与不可行问题，通过三种基于大语言模型的约束重要性推断方法（局部加权、全局单次加权、成对排序）提升修复准确性，并构建了AWARE-US基准测试集。


<details>
  <summary>Details</summary>
Motivation: 现有工具调用型对话代理在面对查询无结果（infeasibility）或条件不足（underspecification）时，常简单返回空结果或采用随意规则放松约束，易违背用户真实意图。

Method: 提出将不可行查询修复建模为偏好感知的查询修复问题；设计三种LLM驱动的约束重要性推断方法：局部加权、全局单次加权和成对排序；构建AWARE-US基准，包含人格驱动、需对话消歧与偏好一致修复的查询场景。

Result: 实验表明局部加权在偏好对齐上最优，全局加权在正确约束松弛上表现最佳；AWARE-US验证了方法在人格一致性修复任务上的有效性。

Conclusion: 约束重要性应依据用户偏好动态建模，而非静态或启发式放松；LLM具备从对话中隐式推断偏好并指导精准查询修复的能力。

Abstract: Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with "no results" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences.

</details>


### [175] [Inferring Causal Graph Temporal Logic Formulas to Expedite Reinforcement Learning in Temporally Extended Tasks](https://arxiv.org/abs/2601.02666)
*Hadi Partovi Aria,Zhe Xu*

Main category: cs.AI

TL;DR: 本文提出GT L-CIRL框架，结合因果图时序逻辑与高斯过程驱动的贝叶斯优化，在图结构时空动态系统中实现高效、可解释、可验证的闭环强化学习。


<details>
  <summary>Details</summary>
Motivation: 黑箱强化学习忽视局部变化在网络结构中的传播机制，导致样本效率低、可解释性差。

Method: 提出GT L-CIRL闭环框架：联合学习策略与挖掘因果图时序逻辑（Causal GTL）规范；以鲁棒性塑造奖励；在效果失败时收集反例；利用高斯过程建模时空相关性，驱动贝叶斯优化来精调参数化因果模板。

Result: 在基因网络和电力网络案例中，相比标准RL基线，学习速度更快，行为更清晰且可形式化验证。

Conclusion: GT L-CIRL通过融合因果推理、时序逻辑与贝叶斯优化，提升了图上时空决策任务的样本效率、可解释性与可验证性。

Abstract: Decision-making tasks often unfold on graphs with spatial-temporal dynamics. Black-box reinforcement learning often overlooks how local changes spread through network structure, limiting sample efficiency and interpretability. We present GTL-CIRL, a closed-loop framework that simultaneously learns policies and mines Causal Graph Temporal Logic (Causal GTL) specifications. The method shapes rewards with robustness, collects counterexamples when effects fail, and uses Gaussian Process (GP) driven Bayesian optimization to refine parameterized cause templates. The GP models capture spatial and temporal correlations in the system dynamics, enabling efficient exploration of complex parameter spaces. Case studies in gene and power networks show faster learning and clearer, verifiable behavior compared to standard RL baselines.

</details>


### [176] [InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents](https://arxiv.org/abs/2601.03204)
*Chenglin Yu,Yuchen Wang,Songmiao Wang,Hongxia Yang,Ming Li*

Main category: cs.AI

TL;DR: InfiAgent 是一种通用框架，通过将持久状态外部化为基于文件的状态抽象，严格限制 LLM 代理的推理上下文长度，从而提升长周期任务的稳定性与覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 代理在长周期任务中因上下文无限增长和错误累积而失效；常用压缩或检索增强方法存在信息保真度与推理稳定性之间的权衡。

Method: 提出 InfiAgent 框架，将状态持久化到外部文件系统，每步仅基于工作区状态快照和固定窗口内的最近动作重建上下文，实现上下文严格有界。

Result: 在 DeepResearch 和 80 篇论文综述任务中，未微调的 20B 开源模型表现媲美更大闭源系统，且长周期覆盖显著优于上下文中心型基线。

Conclusion: 显式状态外部化是构建稳定长周期 LLM 代理的一种实用且有效的基础范式。

Abstract: LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent

</details>


### [177] [Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization](https://arxiv.org/abs/2601.02683)
*Dongyu Chen,Jian Ma,Xianpeng Zhang,Lei Zhang,Haonan Lu,Chen Chen,Chuangchuang Wang,Kai Tang*

Main category: cs.AI

TL;DR: 本文提出了一种分层归因提示优化（HAPO）框架，通过动态归因机制、语义单元优化和多模态友好流程，解决现有提示优化中提示漂移和可解释性差的问题，在图像问答和复杂任务分析等场景中展现出更优的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法存在提示漂移（新提示损害原有任务性能）和从零生成提示导致可解释性差的问题，亟需一种结构化、可解释且高效的自动化提示优化方法。

Method: 提出HAPO框架，包含三个核心创新：（1）基于训练数据与提示历史错误模式的动态归因机制；（2）针对功能型提示片段的语义单元级编辑优化；（3）支持端到端LLM及LLM-MLLM工作流的多模态友好优化流程。

Result: 在单/多图像问答（如OCRV2）和复杂任务分析（如BBH）任务上，HAPO显著提升优化效率，优于现有自动化提示优化方法，并验证了其可扩展性和跨模态适用性。

Conclusion: HAPO为提示工程提供了可解释、高效且可扩展的新范式，有效缓解提示漂移问题，推动自动化提示优化向结构化与实用化发展。

Abstract: Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.

</details>


### [178] [Learning User Preferences Through Interaction for Long-Term Collaboration](https://arxiv.org/abs/2601.02702)
*Shuhaib Mehri,Priyanka Kargupta,Tal August,Dilek Hakkani-Tür*

Main category: cs.AI

TL;DR: 本文提出了MultiSessionCollab基准，用于评估对话代理在多轮会话中学习并利用用户偏好以提升长期协作质量的能力；设计了具备持久化与更新用户偏好记忆的长期协作代理，并通过用户模拟器行为提供学习信号以优化记忆更新和反思生成；实验证明该记忆机制显著提升了任务成功率、交互效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着对话代理与用户协作经验的积累，适应用户偏好对建立长期关系和持续提升协作质量至关重要，但缺乏系统性评估长期协作能力的基准和方法。

Method: 构建MultiSessionCollab多会话协作基准；设计具备持续更新用户偏好记忆的长期协作代理；利用用户模拟器行为生成学习信号，优化代理的反思生成与记忆更新机制；开展仿真实验与真实人类用户研究。

Result: 配备记忆机制的代理在多会话协作中显著提升任务成功率、交互效率并降低用户努力程度；人类用户研究证实该机制切实改善真实场景下的用户体验。

Conclusion: 持久化且可演化的用户偏好记忆是提升对话代理长期协作能力的关键，MultiSessionCollab为该方向提供了有效评估框架，所提方法具有实际应用价值。

Abstract: As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.

</details>


### [179] [Time-Scaling Is What Agents Need Now](https://arxiv.org/abs/2601.02714)
*Zhi Liu,Guangzhi Wang*

Main category: cs.AI

TL;DR: 本文提出“时间缩放”（Time-Scaling）概念，旨在通过扩展和优化智能体在时间维度上的推理能力，提升其深层语义推理与问题求解能力，而无需显著增加模型参数量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽能生成流畅文本，但在鲁棒的深层语义推理上仍不足；CoT、ToT等提示方法受限于搜索完整性与效率；人类在有限认知资源下依赖时序化逐步推理，亟需建模类似机制。

Method: 提出Time-Scaling框架，强调架构层面设计长时程推理路径，支持更深入的问题空间探索、动态策略调整与元认知控制，以模拟人类受认知约束的序列推理过程。

Result: 确立Time-Scaling为增强智能体深度推理能力的关键前沿方向，主张将显式的时间推理管理作为智能体设计的基础原则。

Conclusion: Time-Scaling是实现高效、可控、类人深层推理的核心路径，应成为下一代认知智能体架构设计的首要考量。

Abstract: Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on "perception-representation," Reinforcement Learning on "decision-making-behavior," and Symbolic AI on "knowledge-reasoning." With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop "perception-decision-action" capabilities.
  Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency.
  This highlights the need for "Time-Scaling"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.

</details>


### [180] [The Path Ahead for Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2601.02749)
*Nadia Sibai,Yara Ahmed,Serry Sibaee,Sawsan AlHalawani,Adel Ammar,Wadii Boulila*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）向自主、目标驱动的智能体（agentic AI）演进的技术路径与挑战，提出包含感知、记忆、规划与工具执行的整合框架，并强调安全性、对齐性、可靠性与可持续性等关键问题。


<details>
  <summary>Details</summary>
Motivation: LLMs正从被动文本生成转向自主行动，需系统梳理其迈向‘智能体’所需的核心能力与架构演进，填补从语言理解到自主行为的技术空白。

Method: 通过分析LLM架构演进（如Transformer），提炼支持智能体行为的关键能力（长程推理、上下文感知、自适应决策），构建以‘推理-行动-反思’循环为核心的整合框架，并系统评估应用与挑战。

Result: 提出了三大贡献：（1）LLM向智能体演进的推理-行动-反思机制；（2）涵盖感知、记忆、规划与工具执行的智能体架构框架；（3）对安全、对齐、可靠性和可持续性的批判性评估及关键技术优先级。

Conclusion: LLM迈向真正自主智能体仍面临重大技术与治理挑战，需在可验证规划、多智能体协同、持久化记忆与治理框架等方面同步突破，并兼顾技术鲁棒性、可解释性与伦理保障。

Abstract: The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.

</details>


### [181] [LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery](https://arxiv.org/abs/2601.02757)
*Zixuan Xiao,Jun Ma*

Main category: cs.AI

TL;DR: 本文提出了一个名为ChangeGPT的通用智能体框架，结合大语言模型（LLM）与视觉基础模型，用于遥感图像变化检测，具备多步推理与工具选择能力，在真实场景中展现出高准确率与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法缺乏应对多样化现实查询的灵活性和全面分析的智能性。

Method: 构建了基于LLM与视觉基础模型的分层智能体框架ChangeGPT，以缓解幻觉问题，并在140个真实场景问题构成的数据集上评估其工具选择能力（Precision/Recall）与整体查询准确率（Match）。

Result: ChangeGPT（尤其使用GPT-4-turbo后端）达到90.71%的Match率，在多步推理与工具选择任务中表现突出，并在深圳前海湾城市变化监测案例中验证了实际有效性。

Conclusion: ChangeGPT通过提供智能性、适应性与多类型变化分析能力，为遥感应用中的决策支持提供了强大解决方案。

Abstract: Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.

</details>


### [182] [HAL: Inducing Human-likeness in LLMs with Alignment](https://arxiv.org/abs/2601.02813)
*Masum Hasan,Junjie Zhao,Ehsan Hoque*

Main category: cs.AI

TL;DR: 本文提出HAL框架，通过可解释的数据驱动奖励机制，将语言模型对齐到对话中的人类相似性，提升模型在人类评估中的拟人化表现，同时保持其整体性能。


<details>
  <summary>Details</summary>
Motivation: 对话中的人类相似性在人机交互中至关重要，但其定义、度量和优化一直困难，导致当前改进主要依赖规模或宽泛的监督训练，缺乏针对性对齐。

Method: HAL框架从对比对话数据中提取显式对话特征，将其融合为紧凑标量得分，并以此作为透明奖励信号，结合标准偏好优化方法进行对齐。

Result: HAL成功对齐了不同规模的语言模型，在大规模人类评估中显著提升了模型被感知为人类-like的程度，且未损害其整体性能；同时支持对齐行为的可解释性检查与副作用诊断。

Conclusion: HAL证明了语言中软性、定性属性（如人类相似性）可通过可解释、可测量的方式实现有效对齐，为语言模型的拟人化对齐提供了新范式。

Abstract: Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way.

</details>


### [183] [Causal-Enhanced AI Agents for Medical Research Screening](https://arxiv.org/abs/2601.02814)
*Duc Ngo,Arya Rahgoza*

Main category: cs.AI

TL;DR: 本文提出了一种因果图增强的检索增强生成系统（CausalAgent），通过整合显式因果推理与双层知识图谱，显著提升了系统性综述任务中的准确性和可靠性，在 dementia 运动研究中实现95%准确率和零幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法在系统性综述任务中仍存在不可接受的幻觉问题（2%-15%），而人工审查海量医学文献又不现实，亟需更可靠、可解释的AI方案。

Method: 构建因果图增强的检索增强生成系统，融合显式因果推理与双层知识图谱，强制执行‘证据优先’原则，确保每个因果主张均可追溯至原始文献，并自动生成有向无环图（DAG）可视化干预-结局路径。

Result: 在234篇痴呆症运动相关摘要的评估中，CausalAgent达到95%准确率、100%检索成功率、0%幻觉；对比基线AI（34%准确率、10%幻觉）大幅提升；并支持自动因果图生成以实现机制建模与可视化合成。

Conclusion: 该方法为高风险医疗场景下的可信AI提供了可行架构，证明了因果推理在提升医学AI可靠性与可解释性方面的关键价值，具备跨领域迁移潜力。

Abstract: Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.
  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.
  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.

</details>


### [184] [Quantum-enhanced long short-term memory with attention for spatial permeability prediction in oilfield reservoirs](https://arxiv.org/abs/2601.02818)
*Muzhen Zhang,Yujie Cheng,Zhanxiang Lei*

Main category: cs.AI

TL;DR: 本文提出了一种量子增强的LSTM-Attention模型（QLSTMA），首次将变分量子电路（VQC）融入循环神经网络单元，用于提升储层渗透率等复杂地质参数的空间预测精度；实验表明8量子比特的QLSTMA-IG模型较传统LSTMA显著降低MAE（19%）和RMSE（20%），验证了量子-经典混合神经网络在石油工程中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以可靠预测具有宽范围和高变异性的储层渗透率，亟需提升复杂地质参数空间预测的准确性与鲁棒性。

Method: 提出量子增强的长短期记忆-注意力模型（QLSTMA），将变分量子电路（VQC）嵌入RNN单元，利用量子纠缠与叠加原理增强建模能力；设计两种量子化结构：共享门（QLSTMA-SG）与独立门（QLSTMA-IG），系统评估量子结构配置及量子比特数对性能的影响。

Result: 8量子比特的QLSTMA-IG模型在渗透率预测任务中显著优于传统LSTMA，MAE降低19%，RMSE降低20%，尤其在复杂测井数据区域表现突出；增加量子比特数可进一步提升精度，即使基于经典模拟器亦能体现量子优势。

Conclusion: 量子-经典混合神经网络在储层参数预测中具备实际应用潜力；本研究为未来在真实量子硬件上部署此类模型、并拓展至更广泛的石油工程与地球科学领域奠定了基础框架。

Abstract: Spatial prediction of reservoir parameters, especially permeability, is crucial for oil and gas exploration and development. However, the wide range and high variability of permeability prevent existing methods from providing reliable predictions. For the first time in subsurface spatial prediction, this study presents a quantum-enhanced long short-term memory with attention (QLSTMA) model that incorporates variational quantum circuits (VQCs) into the recurrent cell. Using quantum entanglement and superposition principles, the QLSTMA significantly improves the ability to predict complex geological parameters such as permeability. Two quantization structures, QLSTMA with Shared Gates (QLSTMA-SG) and with Independent Gates (QLSTMA-IG), are designed to investigate and evaluate the effects of quantum structure configurations and the number of qubits on model performance. Experimental results demonstrate that the 8-qubit QLSTMA-IG model significantly outperforms the traditional long short-term memory with attention (LSTMA), reducing Mean Absolute Error (MAE) by 19% and Root Mean Squared Error (RMSE) by 20%, with particularly strong performance in regions featuring complex well-logging data. These findings validate the potential of quantum-classical hybrid neural networks for reservoir prediction, indicating that increasing the number of qubits yields further accuracy gains despite the reliance on classical simulations. This study establishes a foundational framework for the eventual deployment of such models on real quantum hardware and their extension to broader applications in petroleum engineering and geoscience.

</details>


### [185] [Sample-Efficient Neurosymbolic Deep Reinforcement Learning](https://arxiv.org/abs/2601.02850)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 本文提出了一种结合符号知识的神经符号深度强化学习方法，通过将简单任务中获得的部分策略表示为逻辑规则，并在训练中利用在线推理来引导探索与利用，从而提升样本效率、泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习算法样本效率低、泛化能力差，尤其在稀疏奖励和长规划任务中表现不佳。

Method: 将简单任务中习得的部分策略编码为逻辑规则，通过在线符号推理在探索阶段偏置动作分布、在利用阶段重标Q值，实现神经与符号模块的协同。

Result: 在多种变体gridworld环境（全观测与部分观测）中，性能优于基于reward machine的先进基线方法。

Conclusion: 神经符号融合能有效提升DRL的样本效率、泛化性、可解释性与收敛速度，尤其适用于稀疏奖励与长时序任务。

Abstract: Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.

</details>


### [186] [M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?](https://arxiv.org/abs/2601.02854)
*Ao Li,Jinghui Zhang,Luyu Li,Yuxiang Duan,Lang Gao,Mingcai Chen,Weijun Qin,Shaopeng Li,Fengxian Ji,Ning Liu,Lizhen Cui,Xiuying Chen,Yuntao Du*

Main category: cs.AI

TL;DR: 本文提出了M3MAD-Bench，一个面向多领域、多模态、多维度评估的统一基准，用于系统评测多智能体辩论（MAD）方法，弥补了现有研究在评估设置不一致和单模态限制上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有MAD研究存在评估设置碎片化、不一致，且局限于单模态（仅文本）两大问题，亟需统一、可扩展、多模态的标准化基准。

Method: 构建M3MAD-Bench基准，涵盖五大任务领域（知识、数学、医学、自然科学、复杂推理），支持纯文本与视觉-语言数据，并在九种基础模型上评估，引入准确性与效率（token消耗、推理时间）等多维指标。

Result: 通过大规模实验，系统揭示了MAD在文本与多模态场景下的有效性、鲁棒性与效率权衡，验证了M3MAD-Bench作为标准化评估平台的可靠性。

Conclusion: M3MAD-Bench为MAD方法提供了首个统一、多模态、多维度的评估基准，推动未来研究向更公平、全面和实用的方向发展。

Abstract: As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench.

</details>


### [187] [SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection](https://arxiv.org/abs/2601.02871)
*Zhiyong Cao,Dunqiang Liu,Qi Dai,Haojun Xu,Huaiyan Xu,Huan He,Yafei Liu,Siyuan Liu,XiaoLin Lin,Ke Ma,Ruqian Shi,Sijia Yao,Hao Wang,Sicheng Zhou*

Main category: cs.AI

TL;DR: 本文提出SimRPD框架，通过高保真用户模拟器生成大规模对话数据，并结合Chain-of-Intention多维评估方法筛选高质量数据，最终训练出更优的招聘场景主动式对话代理。


<details>
  <summary>Details</summary>
Motivation: 任务导向的主动对话代理在招聘中至关重要，但受限于高质量、目标明确的领域训练数据稀缺。

Method: 提出三阶段SimRPD框架：1）构建高保真用户模拟器生成多轮在线对话数据；2）基于Chain-of-Intention（CoI）设计多维评估框架，融合全局与实例级指标筛选高质量数据；3）在筛选数据上训练招聘主动对话代理。

Result: 在真实招聘场景实验中，SimRPD优于现有基于模拟器的数据选择策略，展现出工业落地价值及向其他业务对话场景迁移的潜力。

Conclusion: SimRPD有效缓解了领域高质量训练数据稀缺问题，提升了主动对话代理性能，为业务导向对话系统提供了可扩展的数据合成与筛选范式。

Abstract: Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios.

</details>


### [188] [ReTreVal: Reasoning Tree with Validation -- A Hybrid Framework for Enhanced LLM Multi-Step Reasoning](https://arxiv.org/abs/2601.02880)
*Abhishek HS,Pavan C Shekar,Arpit Jain,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: 本文提出ReTreVal框架，结合思维树探索、自我精炼、LLM批判评分与反思记忆，实现有界且可验证的多步推理，在数学与创意写作任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多步推理仍是大语言模型在数学和创意写作等复杂领域中的关键挑战；现有方法如ReAct、Reflexion和Self-Refine缺乏对替代解路径的结构化探索及跨问题的持续学习能力。

Method: 提出ReTreVal（Reasoning Tree with Validation）：构建自适应深度的结构化推理树，每个节点经LLM生成反馈引导的迭代自批判与精炼；引入双验证机制评估节点质量并维护反思记忆缓冲区；采用基于批判分数的剪枝策略保留top-k节点。

Result: 在500道数学题与创意写作任务上，以Qwen 2.5 7B为基座模型，ReTreVal在各项指标上持续超越ReAct、Reflexion和Self-Refine。

Conclusion: ReTreVal通过结构化探索、批判驱动精炼与跨问题记忆的协同，显著提升了需探索性推理、严格验证与知识迁移的任务性能。

Abstract: Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.

</details>


### [189] [Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning](https://arxiv.org/abs/2601.02902)
*Xinglang Zhang,Yunyao Zhang,ZeLiang Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.AI

TL;DR: 本文发现大语言模型在符号逻辑推理中存在'逻辑相变'现象：性能在特定逻辑深度内稳定，但超过临界深度后骤降；为此提出神经符号课程调优方法，在相变边界处渐进式强化推理能力，显著提升高复杂度逻辑推理准确率。


<details>
  <summary>Details</summary>
Motivation: 符号逻辑推理是大语言模型的关键但未被充分探索的能力，尤其在数学和法律等高风险领域需要可靠、可验证的决策；现有研究缺乏对逻辑复杂度系统性增加下模型行为的深入分析。

Method: 提出'神经符号课程调优'框架：自适应地对齐自然语言与逻辑符号以建立共享表征，并围绕逻辑相变边界重塑训练动态，渐进式增强不同逻辑深度的推理能力。

Result: 在五个基准测试中，该方法有效缓解高复杂度下的逻辑推理崩溃问题，在朴素提示下平均准确率提升+1.26，在思维链提示下提升+3.95，并增强了对未见逻辑组合的泛化能力。

Conclusion: 逻辑推理性能并非随复杂度平滑下降，而是呈现类似物理相变的突变特性；利用该现象设计的课程学习策略可显著提升LLMs在深层逻辑推理任务中的鲁棒性与泛化性。

Abstract: Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.

</details>


### [190] [Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning](https://arxiv.org/abs/2601.02950)
*Xuan Yang,Furong Jia,Roy Xie,Xiong Xi,Hengwei Bian,Jian Li,Monica Agrawal*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的批处理式推理方法Batch-of-Thought（BoT），通过联合处理相关查询实现跨实例学习，提升准确性、置信度校准并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理系统独立处理每个查询，忽略了跨实例间共享的推理模式和一致性约束等有价值信号。

Method: 提出Batch-of-Thought（BoT）方法，不依赖训练，对相关查询进行批量联合处理；进一步构建多智能体反思架构BoT-R，由Reflector模块执行联合评估以挖掘跨实例互信息。

Result: 在三个模型家族和六个基准上实验表明，BoT-R持续提升准确率与置信度校准，并最多降低61%推理开销；理论与实验分析揭示了批感知推理有效性的条件与机制。

Conclusion: 批处理式推理能有效利用跨实例信号，在不增加训练成本前提下显著增强LLM推理性能与效率，为高效可信推理提供了新范式。

Abstract: Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.

</details>


### [191] [Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2601.02968)
*Qingxiang Liu,Zhiqing Cui,Xiaoliang Luo,Yuqian Wu,Zhuoyang Jiang,Huaiyu Wan,Sheng Sun,Lvchun Wang,Wei Yu,Yuxuan Liang*

Main category: cs.AI

TL;DR: 本文提出RationaleTS方法，通过引入基于理由的上下文学习框架，为时间序列推理提供可解释、可引导的推理路径，显著提升多模态大模型在该任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列推理模型表现不佳，因其缺乏将时序观测与下游结果关联起来的理由先验，导致模型仅依赖表面模式匹配而非原理性推理。

Method: 提出理由驱动的上下文学习框架RationaleTS：首先生成标签条件下的理由（即从观测证据到潜在结果的推理路径），再设计融合时间模式与语义上下文的混合检索机制，以选取相关理由先验用于新样本的上下文推理。

Result: 在三个领域的时间序列推理任务上进行了大量实验，验证了RationaleTS在有效性与效率上的优势。

Conclusion: RationaleTS通过将理由作为引导性推理单元而非事后解释，有效提升了时间序列推理的可解释性与准确性，为多模态大模型在时序任务中的理性推理提供了新范式。

Abstract: The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.

</details>


### [192] [Explainable Fuzzy GNNs for Leak Detection in Water Distribution Networks](https://arxiv.org/abs/2601.03062)
*Qusai Khaled,Pasquale De Marinis,Moez Louati,David Ferras,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 本文提出了一种可解释的图神经网络框架FGENConv，用于水管网泄漏检测与定位，结合互信息识别关键区域、模糊逻辑提供规则化解释，在保持较高性能的同时显著提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: GNN在水网泄漏检测中虽有效，但其黑箱特性及缺乏针对水网的图可解释模型限制了实际应用。

Method: 提出融合互信息（识别关键网络区域）和模糊逻辑（提供基于规则的节点分类解释）的可解释GNN框架；在多种GNN中选定GENConv为基础，构建模糊增强变体FGENConv。

Result: FGENConv在泄漏检测和定位任务上Graph F1分数分别为0.889和0.814，略低于基准GENConv（0.938/0.858），但能提供空间局部化、规则清晰的模糊解释。

Conclusion: FGENConv在精度与可解释性间取得良好平衡，有助于工程师验证预测结果、节省人力并优化运维策略。

Abstract: Timely leak detection in water distribution networks is critical for conserving resources and maintaining operational efficiency. Although Graph Neural Networks (GNNs) excel at capturing spatial-temporal dependencies in sensor data, their black-box nature and the limited work on graph-based explainable models for water networks hinder practical adoption. We propose an explainable GNN framework that integrates mutual information to identify critical network regions and fuzzy logic to provide clear, rule-based explanations for node classification tasks. After benchmarking several GNN architectures, we selected the generalized graph convolution network (GENConv) for its superior performance and developed a fuzzy-enhanced variant that offers intuitive explanations for classified leak locations. Our fuzzy graph neural network (FGENConv) achieved Graph F1 scores of 0.889 for detection and 0.814 for localization, slightly below the crisp GENConv 0.938 and 0.858, respectively. Yet it compensates by providing spatially localized, fuzzy rule-based explanations. By striking the right balance between precision and explainability, the proposed fuzzy network could enable hydraulic engineers to validate predicted leak locations, conserve human resources, and optimize maintenance strategies. The code is available at github.com/pasqualedem/GNNLeakDetection.

</details>


### [193] [A framework for assuring the accuracy and fidelity of an AI-enabled Digital Twin of en route UK airspace](https://arxiv.org/abs/2601.03120)
*Adam Keane,Nick Pepper,Chris Burr,Amy Hodgkin,Dewi Gould,John Korna,Marc Thomas*

Main category: cs.AI

TL;DR: 本文提出了一种面向航空领域数字孪生（特别是英国空域数字孪生）的可信保障框架，融合TEA方法构建保障案例，以支持AI空管代理的开发、验证及监管合规。


<details>
  <summary>Details</summary>
Motivation: 应对数字孪生与AI/ML在空管领域应用中日益增长的监管需求，缺乏通用、可操作的保障方法；需确保数字孪生准确表征物理系统并满足特定用例功能要求。

Method: 基于新兴数字孪生与AI/ML在空管领域的指导原则，采用可信与伦理保障（TEA）方法构建结构化保障案例（assurance case），定义可操作目标与对应证据要求，并通过多层论证与深度剖析（含证据、假设与依据）支撑顶层目标。

Result: 提出一个可复用、结构化的数字孪生保障框架，支持研究人员评估其局限性、提升保真度，并为与利益相关方及监管机构对话提供基础，同时为监管指南制定贡献实际范例。

Conclusion: 该框架不仅适用于Project Bluebird中的概率型空域数字孪生，也为其他航空AI应用的可信性与合规性验证提供了方法论基础和实践路径。

Abstract: Digital Twins combine simulation, operational data and Artificial Intelligence (AI), and have the potential to bring significant benefits across the aviation industry. Project Bluebird, an industry-academic collaboration, has developed a probabilistic Digital Twin of en route UK airspace as an environment for training and testing AI Air Traffic Control (ATC) agents. There is a developing regulatory landscape for this kind of novel technology. Regulatory requirements are expected to be application specific, and may need to be tailored to each specific use case.
  We draw on emerging guidance for both Digital Twin development and the use of Artificial Intelligence/Machine Learning (AI/ML) in Air Traffic Management (ATM) to present an assurance framework. This framework defines actionable goals and the evidence required to demonstrate that a Digital Twin accurately represents its physical counterpart and also provides sufficient functionality across target use cases. It provides a structured approach for researchers to assess, understand and document the strengths and limitations of the Digital Twin, whilst also identifying areas where fidelity could be improved. Furthermore, it serves as a foundation for engagement with stakeholders and regulators, supporting discussions around the regulatory needs for future applications, and contributing to the emerging guidance through a concrete, working example of a Digital Twin.
  The framework leverages a methodology known as Trustworthy and Ethical Assurance (TEA) to develop an assurance case. An assurance case is a nested set of structured arguments that provides justified evidence for how a top-level goal has been realised. In this paper we provide an overview of each structured argument and a number of deep dives which elaborate in more detail upon particular arguments, including the required evidence, assumptions and justifications.

</details>


### [194] [Automatic Prompt Engineering with No Task Cues and No Tuning](https://arxiv.org/abs/2601.03130)
*Faisal Chowdhury,Nandana Mihindukulasooriya,Niharika S D'Souza,Horst Samulowitz,Neeru Gupta,Tomasz Hanusiak,Michal Kapitonow*

Main category: cs.AI

TL;DR: 本文提出了一种更简单但同样有效的自动提示工程系统，无需调优和任务显式线索，并首次将其应用于密码式列名扩展（CNE）任务及非英语（德语）场景。


<details>
  <summary>Details</summary>
Motivation: 解决数据库表中密码式列名扩展（CNE）这一关键但研究不足的任务，并拓展自动提示工程至非英语语言场景。

Method: 设计了一种无需调优、无需任务显式线索的简单自动提示工程系统。

Result: 在英语和德语两种语言的CNE数据集上验证了该方法的有效性，是首个将自动提示工程应用于CNE及非英语语言的工作。

Conclusion: 所提出的自动提示工程系统在保持极简设计的同时，实现了与现有方法相当的效果，并成功拓展至新任务和新语言领域。

Abstract: This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.

</details>


### [195] [MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents](https://arxiv.org/abs/2601.03236)
*Dongming Jiang,Yi Li,Guanpeng Li,Bingzhe Li*

Main category: cs.AI

TL;DR: 本文提出MAGMA，一种多图代理记忆架构，通过将记忆项分别表示在语义、时间、因果和实体正交图中，并采用策略引导的遍历方式进行检索，从而提升长上下文推理的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有Memory-Augmented Generation（MAG）方法依赖单一语义相似性检索，混杂时间、因果和实体信息，导致可解释性差、检索与查询意图对齐不足、推理准确率受限。

Method: 提出MAGMA架构，将每个记忆项映射到语义、时间、因果和实体四个正交图中；检索建模为策略引导的多图遍历，实现查询自适应选择与结构化上下文构建；解耦记忆表示与检索逻辑。

Result: 在LoCoMo和LongMemEval基准上，MAGMA在长视野推理任务中持续优于当前最优的代理记忆系统。

Conclusion: MAGMA通过多图解耦表征与策略引导检索，显著提升了长上下文推理的准确性、可解释性与可控性，为记忆增强生成提供了新范式。

Abstract: Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [196] [Stroke Patches: Customizable Artistic Image Styling Using Regression](https://arxiv.org/abs/2601.03114)
*Ian Jaffray,John Bronskill*

Main category: cs.GR

TL;DR: 本文提出了一种基于回归的图像艺术化风格迁移新方法，通过可扩展的程序生成笔触贴片集，实现对笔触构成和细节程度的显式控制，并利用U-Net回归模型渲染多种可定制风格。


<details>
  <summary>Details</summary>
Motivation: 现有神经风格迁移或扩散模型缺乏对笔触构成与细节程度的显式可控性，需一种更灵活、可定制的艺术化渲染方法。

Method: 提出基于程序生成的可扩展笔触贴片集，结合U-Net架构的回归模型进行端到端训练，以实现输入图像到目标艺术风格的映射。

Result: 模型能对任意输入图像生成多种独特、富有表现力且高度可定制的艺术风格结果，支持对笔触形状、大小、方向、密度、颜色及噪声等参数的精细控制。

Conclusion: 该回归式方法在保持生成质量的同时显著提升了艺术风格控制的灵活性与可解释性，为可控图像艺术化提供了新范式。

Abstract: We present a novel, regression-based method for artistically styling images. Unlike recent neural style transfer or diffusion-based approaches, our method allows for explicit control over the stroke composition and level of detail in the rendered image through the use of an extensible set of stroke patches. The stroke patch sets are procedurally generated by small programs that control the shape, size, orientation, density, color, and noise level of the strokes in the individual patches. Once trained on a set of stroke patches, a U-Net based regression model can render any input image in a variety of distinct, evocative and customizable styles.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [197] [Stigmergic Swarming Agents for Fast Subgraph Isomorphism](https://arxiv.org/abs/2601.02449)
*H. Van Dyke Parunak*

Main category: cs.MA

TL;DR: 本文提出了一种名为ASSIST的启发式算法，用于解决最大部分子图同构问题，其搜索阶段时间复杂度为线性于查询图大小、常数级于数据图大小，并支持多种扩展匹配场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理最大部分子图同构问题时复杂度高（如O(d²)），且难以应对带有时序边、不精确匹配、缺失节点或边等复杂匹配需求。

Method: 受蚁群优化启发，ASSIST首先通过O(q·log(d))时间完成节点配对（peering），再通过基于信息素机制（stigmergy）的迭代群体搜索进行子图匹配，使核心搜索阶段复杂度降为O(q)。

Result: ASSIST在子图搜索阶段实现时间复杂度线性于查询图大小q、与数据图大小d无关；同时可扩展支持时序边、不精确匹配及缺失结构等挑战性匹配任务。

Conclusion: ASSIST显著提升了子图同构问题的求解效率与适用性，为图数据匹配提供了更高效、更鲁棒的新范式。

Abstract: Maximum partial subgraph isomorphism compares two graphs (nodes joined by edges) to find a largest common subgraph. A common use case, for graphs with labeled nodes, seeks to find instances of a \textit{query} graph with $q$ nodes in a (typically larger) \textit{data} graph with $d$ nodes. The problem is NP-complete, and naïve solutions are exponential in $q + d$. The fastest current heuristic has complexity $O(d^2)$. This paper outlines ASSIST (Approximate Swarming Subgraph Isomorphism through Stigmergy), inspired by the ant colony optimization approach to the traveling salesperson. After peering (identifying matching individual nodes in query and data) in time $O(q\cdot log(d))$, the time required for ASSIST's iterative subgraph search, the combinatorially complex part of the problem, is linear in query size and constant in data size. ASSIST can be extended to support matching problems (such as temporally ordered edges, inexact matches, and missing nodes or edges in the data graph) that frustrate other heuristics.

</details>


### [198] [Modellierung und Simulation der Dynamik von Fussgängerströmen](https://arxiv.org/abs/2601.02526)
*Péter Molnár*

Main category: cs.MA

TL;DR: 本文基于社会力理论构建了一个微观行人流模型，旨在设计行人友好型基础设施并验证社会科学理论；模型通过简单个体规则（朝目标移动、避让他人与障碍）涌现出复杂集体行为（如路径痕迹），并结合进化算法优化建筑布局、引入决策与学习机制提升避让能力，最终提出一种最小化绕行的自组织路径系统模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个可用于行人友好型基础设施设计的现实模型，并利用足够数据验证社会学理论。

Method: 基于社会力理论构建微观行人流模型，引入两条基本个体行为规则；结合进化算法优化建筑布局；集成决策模型、适应与学习机制；提出路径系统负载分布计算方法及最小绕行自组织路径模型。

Result: 发现简单个体规则可导致复杂时空结构和自组织集体行为（如单向路径痕迹）；揭示行人流特性对建筑几何形态的高度依赖性；证明缩减可行走区域可提升效率；实现了基于经验的学习型避让与决策优化。

Conclusion: 该模型不仅支持基础设施优化设计，还为社会力理论提供了计算验证；所提出的自组织路径模型类比自然运输网络，在总长度与材料成本间取得平衡，具有实际应用价值。

Abstract: This work presents a microscopic model to describe pedestrian flows based on the social force theory. The aim of this study is twofold: (1) developing a realistic model that can be used as a tool for designing pedestrian-friendly infrastructure, and (2) verifying a social science theory using a model with sufficient data. The investigation of the pedestrian model shows that despite simple individual behavior patterns, complex spatial and temporal structures emerge through the interactions in pedestrian flows. Collective behavior emerges from individuals following two basic rules: (1) moving directly towards their goal at a certain speed, and (2) maintaining a distance to other pedestrians and obstacles. This self-organized collective behavior manifests itself as trails that are formed by pedestrians moving in one direction. Furthermore, strong dependencies of the properties of pedestrian flows on geometric forms of buildings are shown, and the influence of geometric changes on performance characteristics is investigated. An example demonstrates how efficiency can be increased by reducing walkable areas. This work also presents an evolutionary algorithm for optimizing building layouts based on the social force model. Additionally, a decision-making model is integrated to describe alternative goal selection, and adaptation and learning capabilities are included to improve pedestrian avoidance behavior and decision strategies based on accumulated experience. A method for determining load distributions in individual sections of a path system considering subjective selection criteria is also developed. Finally, a model that describes the self-organization of path systems with minimal detours is presented, similar to natural transport networks where total length and material costs are optimized.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [199] [Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis](https://arxiv.org/abs/2601.02409)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh*

Main category: eess.IV

TL;DR: 本文提出了一种双框架方法（EGxFSL和xGAL），结合专家知识与可解释性，分别解决医学图像分析中少样本学习的可解释性问题和主动学习中的样本选择可解释性问题，在多个数据集上显著提升性能并验证跨模态泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析面临标注数据稀缺和模型缺乏可解释性两大挑战，现有少样本学习和主动学习方法分别未能兼顾透明性与样本选择的诊断相关性。

Method: 提出Expert-Guided Explainable Few-Shot Learning（EGxFSL）：利用放射科医生定义的感兴趣区域，通过Grad-CAM Dice损失提供空间监督，并与原型分类联合优化；提出Explainability-Guided Active Learning（xGAL）：在样本选取中同时考虑预测不确定性与注意力错位，构建可解释性驱动的闭环学习框架。

Result: 在BraTS、VinDr-CXR和SIIM-COVID-19数据集上分别达到92%、76%、62%准确率；xGAL仅用680样本即达76%准确率（随机采样为57%）；Grad-CAM可视化证实模型聚焦于诊断相关区域，并在乳腺超声数据上验证跨模态泛化性。

Conclusion: 将专家先验与可解释性深度融入少样本学习与主动学习流程，可协同提升模型性能、可信度与临床适用性。

Abstract: Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\%, 76\%, and 62\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\% accuracy with only 680 samples versus 57\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [200] [Trust in LLM-controlled Robotics: a Survey of Security Threats, Defenses and Challenges](https://arxiv.org/abs/2601.02377)
*Xinyu Huang,Shyam Karthick V B,Taozhao Chen,Mitch Bryson,Thomas Chaffey,Huaming Chen,Kim-Kwang Raymond Choo,Ian R. Manchester*

Main category: cs.RO

TL;DR: 本文系统综述了大语言模型（LLM）驱动机器人所面临的安全威胁与防御策略，提出针对‘具身鸿沟’引发的独特物理层风险的分类框架，并涵盖攻击类型、防御机制及评估基准。


<details>
  <summary>Details</summary>
Motivation: LLM在机器人中的应用引入了由‘具身鸿沟’导致的新安全风险——恶意输出可能转化为危险物理行为，而现有文本LLM安全方案难以适配具身场景。

Method: 采用系统性文献调研方法，构建攻击向量分类体系（如越狱、后门、多模态提示注入），并归纳防御机制（形式化安全规范、运行时监控、多LLM监督、提示加固等），同时梳理相关数据集与评测基准。

Result: 提出了首个面向LLM控制机器人的综合性安全威胁与防御分类框架，明确了当前研究缺口与关键挑战。

Conclusion: 亟需发展上下文感知的安全方案；本综述为构建安全、可靠、可信赖的LLM驱动机器人提供了基础性路线图。

Abstract: The integration of Large Language Models (LLMs) into robotics has revolutionized their ability to interpret complex human commands and execute sophisticated tasks. However, such paradigm shift introduces critical security vulnerabilities stemming from the ''embodiment gap'', a discord between the LLM's abstract reasoning and the physical, context-dependent nature of robotics. While security for text-based LLMs is an active area of research, existing solutions are often insufficient to address the unique threats for the embodied robotic agents, where malicious outputs manifest not merely as harmful text but as dangerous physical actions. In this work, we present a systematic survey, summarizing the emerging threat landscape and corresponding defense strategies for LLM-controlled robotics. Specifically, we discuss a comprehensive taxonomy of attack vectors, covering topics such as jailbreaking, backdoor attacks, and multi-modal prompt injection. In response, we analyze and categorize a range of defense mechanisms, from formal safety specifications and runtime enforcement to multi-LLM oversight and prompt hardening. Furthermore, we review key datasets and benchmarks used to evaluate the robustness of these embodied systems. By synthesizing current research, this work highlights the urgent need for context-aware security solutions and provides a foundational roadmap for the development of safe, secure, and reliable LLM-controlled robotics.

</details>


### [201] [Modeling the Mental World for Embodied AI: A Comprehensive Review](https://arxiv.org/abs/2601.02378)
*Biyuan Liu,Daigang Xu,Lei Jiang,Wenjun Guo,Ping Chen*

Main category: cs.RO

TL;DR: 本文综述了具身AI中心理世界模型（MWM）的研究进展，首次构建了完整的MWM理论框架，明确了其与物理世界模型（PWM）的本质区别，系统定义了MWM的关键组成，并分析了两种核心心智理论（ToM）推理范式及19种方法，同时梳理了26个ToM评估基准和神经符号混合架构的融合趋势。


<details>
  <summary>Details</summary>
Motivation: 具身AI代理在社会交互理解方面的需求日益增长，而传统物理世界模型（PWM）难以支撑社会智能建模；当前心理世界模型（MWM）研究存在概念模糊、推理机制割裂、评估与实践脱节等瓶颈。

Method: 系统综述100余篇权威文献，构建MWM理论框架，区分MWM与PWM；提出两种心智要素表征范式以定义MWM关键组件；全面分析两类ToM推理范式及19种具体方法；梳理26个ToM评估基准，并探讨神经符号混合架构的融合趋势。

Result: 首次建立完整的MWM理论体系；明确MWM与PWM的本质差异；系统界定MWM核心构成；归纳19种ToM方法与26个评估基准；指出神经符号混合是重要发展方向。

Conclusion: 该综述为具身AI的社会化发展提供了系统性认知基础与技术路线图，有望推动具身代理深度融入人类社会并实现自然、动态的人机协同。

Abstract: As the application of Embodied AI Agents in avatars, wearable devices, and robotic systems continues to deepen, their core research challenges have gradually shifted from physical environment interaction to the accurate understanding of social interactions. Traditional physical world models (PWM) focus on quantifiable physical attributes such as space and motion, failing to meet the needs of social intelligence modeling. In contrast, the Mental World Model (MWM), as a structured representation of humans' internal mental states, has become the critical cognitive foundation for embodied agents to achieve natural human-machine collaboration and dynamic social adaptation. However, current MWM research faces significant bottlenecks: such as fragmented conceptual framework with vague boundaries between MWM and PWM, disjointed reasoning mechanisms for the technical pathways and applicable scenarios of different Theory of Mind (ToM) reasoning paradigms, and detachment between evaluation and practice.
  To address these issues, this review systematically synthesizes over 100 authoritative studies to provide a comprehensive overview of MWM research for embodied AI. Its core contributions are threefold: First, it constructs a complete theoretical framework for MWM for the first time. Specifically, it distinguishes the essential differences between MWM and PWMs. Second, it systematically defines the key components of MWM through two paradigms for mental element representation. Third, it comprehensively analyzes two core ToM reasoning paradigms with 19 ToM methods. Finally, it also clarifies the integration trend of neuro-symbolic hybrid architectures, and synthesizes 26 ToM evaluation benchmarks. This work aims to promote the integration of embodied agents into human society and advance the in-depth development of human-machine collaborative interaction.

</details>


### [202] [Movement Primitives in Robotics: A Comprehensive Survey](https://arxiv.org/abs/2601.02379)
*Nolan B. Gutierrez,William J. Beksi*

Main category: cs.RO

TL;DR: This survey provides a comprehensive, chronological overview of movement primitive approaches and their applications in robotics, focusing on how they encode human demonstrations into robotic control trajectories, their analytical properties (e.g., spring-damper behavior, probabilistic coupling, neural network integration), and practical challenges in deployment.


<details>
  <summary>Details</summary>
Motivation: To systematically inform robotics practitioners about movement primitives—elementary motion building blocks—by reviewing frameworks, applications, strengths/weaknesses, and open challenges in real-world robotic deployment.

Method: Chronological, encyclopedic survey methodology: systematic classification and critical analysis of major movement primitive frameworks (e.g., DMPs, ProMPs, neural variants), their mathematical foundations, implementation strategies, and application case studies in robotics.

Result: A structured taxonomy of movement primitive frameworks; identification of key strengths (e.g., generalization, robustness, modularity) and limitations (e.g., scalability, adaptability to dynamic environments); summary of successful real-world applications (e.g., grasping, throwing); and enumeration of open research and engineering challenges.

Conclusion: Movement primitives are powerful and widely adopted tools for robot motor skill learning from demonstration, but their effective use requires careful framework selection and addressing persistent challenges in generalization, high-dimensional control, and real-time adaptation—making this survey a practical guide for researchers and engineers.

Abstract: Biological systems exhibit a continuous stream of movements, consisting of sequential segments, that allow them to perform complex tasks in a creative and versatile fashion. This observation has led researchers towards identifying elementary building blocks of motion known as movement primitives, which are well-suited for generating motor commands in autonomous systems, such as robots. In this survey, we provide an encyclopedic overview of movement primitive approaches and applications in chronological order. Concretely, we present movement primitive frameworks as a way of representing robotic control trajectories acquired through human demonstrations. Within the area of robotics, movement primitives can encode basic motions at the trajectory level, such as how a robot would grasp a cup or the sequence of motions necessary to toss a ball. Furthermore, movement primitives have been developed with the desirable analytical properties of a spring-damper system, probabilistic coupling of multiple demonstrations, using neural networks in high-dimensional systems, and more, to address difficult challenges in robotics. Although movement primitives have widespread application to a variety of fields, the goal of this survey is to inform practitioners on the use of these frameworks in the context of robotics. Specifically, we aim to (i) present a systematic review of major movement primitive frameworks and examine their strengths and weaknesses; (ii) highlight applications that have successfully made use of movement primitives; and (iii) examine open questions and discuss practical challenges when applying movement primitives in robotics.

</details>


### [203] [InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation](https://arxiv.org/abs/2601.02456)
*Junhao Cai,Zetao Cai,Jiafei Cao,Yilun Chen,Zeyu He,Lei Jiang,Hang Li,Hengjie Li,Yang Li,Yufei Liu,Yanan Lu,Qi Lv,Haoxiang Ma,Jiangmiao Pang,Yu Qiao,Zherui Qiu,Yanqing Shen,Xu Shi,Yang Tian,Bolun Wang,Hanqing Wang,Jiaheng Wang,Tai Wang,Xueyuan Wei,Chao Wu,Yiman Xie,Boyang Xing,Yuqiang Yang,Yuyin Yang,Qiaojun Yu,Feng Yuan,Jia Zeng,Jingjing Zhang,Shenghan Zhang,Shi Zhang,Zhuoma Zhaxi,Bowen Zhou,Yuanzhen Zhou,Yunsong Zhou,Hongrui Zhu,Yangkun Zhu,Yuchen Zhu*

Main category: cs.RO

TL;DR: 本文提出InternVLA-A1模型，融合语义理解与物理世界动态预测能力，采用统一的Mixture-of-Transformers架构，结合场景理解、视觉远见生成与动作执行三大专家模块，并在合成与真实数据混合训练下，在多项真实机器人任务中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型擅长语义理解但缺乏物理动态推断能力；而基于视频预测的世界模型又缺乏语义 grounding 且易受预测误差影响。因此需融合二者优势。

Method: 提出InternVLA-A1，基于InternVL3和Qwen3-VL构建2B/3B参数规模模型；采用统一Mixture-of-Transformers架构，含三个专家模块（场景理解、视觉远见生成、动作执行），通过统一masked self-attention机制协同；在InternData-A1与Agibot-World混合数据（5.33亿帧）上预训练。

Result: 在12项真实机器人任务及仿真基准测试中显著优于pi0和GR00T N1.5：日常任务提升14.5%，动态场景（如传送带分拣）提升40%–73.3%。

Conclusion: InternVLA-A1成功桥接语义理解与物理动态建模，验证了多专家协同与混合数据训练策略在VLA模型中的有效性，为具身智能提供了新范式。

Abstract: Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\% improvement in daily tasks and a 40\%-73.3\% boost in dynamic settings, such as conveyor belt sorting.

</details>


### [204] [Learning and Optimizing the Efficacy of Spatio-Temporal Task Allocation under Temporal and Resource Constraints](https://arxiv.org/abs/2601.02505)
*Jiazhen Liu,Glen Neville,Jinwoo Park,Sonia Chernova,Harish Ravichandar*

Main category: cs.RO

TL;DR: 本文提出了STEAM问题框架，用于异构多机器人系统在时空约束下的任务分配、调度与路径规划联合优化，并设计了E-ITAGS算法，结合实现实性感知的主动学习来估计特质-效能映射，在保证时间预算和资源约束前提下提升任务执行效能。


<details>
  <summary>Details</summary>
Motivation: 现有二元成功/失败建模无法刻画任务执行效能的连续变化；特质-效能映射难以先验指定；且任务分配、调度与路径规划常被割裂处理，难以满足复杂多机器人任务对时空与资源约束的联合优化需求。

Method: 提出STEAM问题形式化框架，定义基于特质的效能映射与时空约束；设计E-ITAGS算法，通过图搜索联合优化分配、调度与路径；引入实现实性感知的主动学习模块在线高效学习特质-效能映射。

Result: 在应急响应仿真与实验中，E-ITAGS相较基线方法显著提升任务效能，严格满足时间预算与资源约束；主动学习模块具备样本高效性，并在数据与计算开销间实现可解释权衡。

Conclusion: STEAM框架与E-ITAGS算法为异构多机器人协同提供了统一、可学习、可验证的效能优化范式，兼顾建模表达力、求解可行性与实际部署鲁棒性。

Abstract: Complex multi-robot missions often require heterogeneous teams to jointly optimize task allocation, scheduling, and path planning to improve team performance under strict constraints. We formalize these complexities into a new class of problems, dubbed Spatio-Temporal Efficacy-optimized Allocation for Multi-robot systems (STEAM). STEAM builds upon trait-based frameworks that model robots using their capabilities (e.g., payload and speed), but goes beyond the typical binary success-failure model by explicitly modeling the efficacy of allocations as trait-efficacy maps. These maps encode how the aggregated capabilities assigned to a task determine performance. Further, STEAM accommodates spatio-temporal constraints, including a user-specified time budget (i.e., maximum makespan). To solve STEAM problems, we contribute a novel algorithm named Efficacy-optimized Incremental Task Allocation Graph Search (E-ITAGS) that simultaneously optimizes task performance and respects time budgets by interleaving task allocation, scheduling, and path planning. Motivated by the fact that trait-efficacy maps are difficult, if not impossible, to specify, E-ITAGS efficiently learns them using a realizability-aware active learning module. Our approach is realizability-aware since it explicitly accounts for the fact that not all combinations of traits are realizable by the robots available during learning. Further, we derive experimentally-validated bounds on E-ITAGS' suboptimality with respect to efficacy. Detailed numerical simulations and experiments using an emergency response domain demonstrate that E-ITAGS generates allocations of higher efficacy compared to baselines, while respecting resource and spatio-temporal constraints. We also show that our active learning approach is sample efficient and establishes a principled tradeoff between data and computational efficiency.

</details>


### [205] [Making Infeasible Tasks Feasible: Planning to Reconfigure Disconnected 3D Environments with Movable Objects](https://arxiv.org/abs/2601.02645)
*Samarth Kalluraya,Yiannis Kantaros*

Main category: cs.RO

TL;DR: 本文提出BRiDGE方法，解决3D环境中机器人因支持面断开而无法到达目标区域的问题，通过重排可动物体（如箱子）来搭建连接桥梁，实现导航任务。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划器假设目标区域可达，但在实际3D环境中，因高度差、空洞或横向分离导致支持面断开，使目标不可达；需通过环境交互（重排可动物体）来建立新通路。

Method: 提出BRiDGE——一种基于采样的规划器，增量构建机器人与物体联合构型树，支持选择移动哪些物体、放置位置及顺序；引入非均匀采样策略加速规划，并保证概率完备性。

Result: 在数值仿真和真实硬件实验中验证了BRiDGE的有效性，能成功生成可行计划，在3D断连环境中实现导航与物体重配置。

Conclusion: BRiDGE有效拓展了NAMO问题至3D几何环境，克服了传统2D推动式方法的局限，为机器人在复杂结构化环境中自主搭建通路提供了可行框架。

Abstract: Several planners have been developed to compute dynamically feasible, collision-free robot paths from an initial to a goal configuration. A key assumption in these works is that the goal region is reachable; an assumption that often fails in practice when environments are disconnected. Motivated by this limitation, we consider known 3D environments comprising objects, also called blocks, that form distinct navigable support surfaces (planes), and that are either non-movable (e.g., tables) or movable (e.g., boxes). These surfaces may be mutually disconnected due to height differences, holes, or lateral separations. Our focus is on tasks where the robot must reach a goal region residing on an elevated plane that is unreachable. Rather than declaring such tasks infeasible, an effective strategy is to enable the robot to interact with the environment, rearranging movable objects to create new traversable connections; a problem known as Navigation Among Movable Objects (NAMO). Existing NAMO planners typically address 2D environments, where obstacles are pushed aside to clear a path. These methods cannot directly handle the considered 3D setting; in such cases, obstacles must be placed strategically to bridge these physical disconnections. We address this challenge by developing BRiDGE (Block-based Reconfiguration in Disconnected 3D Geometric Environments), a sampling-based planner that incrementally builds trees over robot and object configurations to compute feasible plans specifying which objects to move, where to place them, and in what order, while accounting for a limited number of movable objects. To accelerate planning, we introduce non-uniform sampling strategies. We show that our method is probabilistically complete and we provide extensive numerical and hardware experiments validating its effectiveness.

</details>


### [206] [Effective Online 3D Bin Packing with Lookahead Parcels Using Monte Carlo Tree Search](https://arxiv.org/abs/2601.02649)
*Jiangyi Fang,Bowen Zhou,Haotian Wang,Xin Zhu,Leye Wang*

Main category: cs.RO

TL;DR: 本文提出了一种结合模型预测控制（MPC）与改进蒙特卡洛树搜索（MCTS）的在线3D装箱方法，利用短时前瞻信息缓解分布偏移问题，并通过动态探索先验和辅助奖励提升鲁棒性与空间利用率。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在在线3D装箱中难以适应物流中因批次货物顺序到达引起的短期分布偏移，而现代物流系统具备的短时前瞻信息可被有效利用以缓解该问题。

Method: 将带前瞻包裹的在线3D装箱建模为模型预测控制（MPC）问题，适配蒙特卡洛树搜索（MCTS）框架；引入基于前瞻特征动态平衡学习策略与随机策略的探索先验；设计惩罚长期空间浪费的辅助奖励。

Result: 在真实数据集上实验表明，该方法在分布偏移下性能提升超10%，在线部署平均提升4%，最优情况下提升超8%，显著优于现有SOTA方法。

Conclusion: 利用短时前瞻信息并结合MPC与改进MCTS，能有效提升在线3D装箱在动态物流环境中的鲁棒性与效率。

Abstract: Online 3D Bin Packing (3D-BP) with robotic arms is crucial for reducing transportation and labor costs in modern logistics. While Deep Reinforcement Learning (DRL) has shown strong performance, it often fails to adapt to real-world short-term distribution shifts, which arise as different batches of goods arrive sequentially, causing performance drops. We argue that the short-term lookahead information available in modern logistics systems is key to mitigating this issue, especially during distribution shifts. We formulate online 3D-BP with lookahead parcels as a Model Predictive Control (MPC) problem and adapt the Monte Carlo Tree Search (MCTS) framework to solve it. Our framework employs a dynamic exploration prior that automatically balances a learned RL policy and a robust random policy based on the lookahead characteristics. Additionally, we design an auxiliary reward to penalize long-term spatial waste from individual placements. Extensive experiments on real-world datasets show that our method consistently outperforms state-of-the-art baselines, achieving over 10\% gains under distributional shifts, 4\% average improvement in online deployment, and up to more than 8\% in the best case--demonstrating the effectiveness of our framework.

</details>


### [207] [Learning to Nudge: A Scalable Barrier Function Framework for Safe Robot Interaction in Dense Clutter](https://arxiv.org/abs/2601.02686)
*Haixin Jin,Nikhil Uday Shinde,Soofiyan Atar,Hongzhan Yu,Dylan Hirsch,Sicun Gao,Michael C. Yip,Sylvia Herbert*

Main category: cs.RO

TL;DR: 本文提出Dense Contact Barrier Functions (DCBF)，一种学习型、可组合的对象中心安全函数，用于在密集杂乱环境中实现无需重训练即可跨任务迁移的实时安全导航与接触式操作。


<details>
  <summary>Details</summary>
Motivation: 传统安全框架将接触视为不安全，限制了机器人在日常密集环境中的功能；而基于模型的方法计算复杂度高，学习方法又缺乏任务泛化性。

Method: 提出Dense Contact Barrier Functions（DCBF），通过离线学习少量物体交互数据，构建可组合的对象中心安全函数，在运行时对任意物体集合生成线性可扩展的全局安全滤波器。

Result: 在密集杂乱环境的仿真实验中，DCBF成功实现了无碰撞导航与安全、接触丰富的交互。

Conclusion: DCBF绕过了显式建模多物体动力学的计算瓶颈，具备跨物体集合和跨任务的泛化能力，为密集环境中安全、鲁棒的机器人操作提供了新范式。

Abstract: Robots operating in everyday environments must navigate and manipulate within densely cluttered spaces, where physical contact with surrounding objects is unavoidable. Traditional safety frameworks treat contact as unsafe, restricting robots to collision avoidance and limiting their ability to function in dense, everyday settings. As the number of objects grows, model-based approaches for safe manipulation become computationally intractable; meanwhile, learned methods typically tie safety to the task at hand, making them hard to transfer to new tasks without retraining. In this work we introduce Dense Contact Barrier Functions(DCBF). Our approach bypasses the computational complexity of explicitly modeling multi-object dynamics by instead learning a composable, object-centric function that implicitly captures the safety constraints arising from physical interactions. Trained offline on interactions with a few objects, the learned DCBFcomposes across arbitrary object sets at runtime, producing a single global safety filter that scales linearly and transfers across tasks without retraining. We validate our approach through simulated experiments in dense clutter, demonstrating its ability to enable collision-free navigation and safe, contact-rich interaction in suitable settings.

</details>


### [208] [Analysis of Various Manipulator Configurations Based on Multi-Objective Black-Box Optimization](https://arxiv.org/abs/2601.02704)
*Kento Kawaharazuka,Keita Yoneda,Takahiro Hattori,Shintaro Inoue,Kei Okada*

Main category: cs.RO

TL;DR: 本文通过多目标优化（末端可达性与关节力矩）分析6/7-DOF机械臂的最优结构，并评估现有机械臂在优化解空间中的位置，为未来设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有6/7-DOF机械臂结构各异（关节顺序与连杆长度比不同），缺乏统一最优结构标准；同时机器人基础模型发展推动了新型机械臂需求，亟需系统性结构优化依据。

Method: 采用多目标优化方法，以末端执行器可达性和关节力矩为优化目标，对机械臂的关节配置和连杆长度比进行系统性搜索与分析。

Result: 获得了兼顾可达性与力矩性能的Pareto最优结构分布；定位了现有主流机械臂在该优化解空间中的相对位置，揭示其优势与不足。

Conclusion: 机械臂结构存在可量化的优化边界，未来设计应参考多目标优化结果，在可达性与力矩间取得平衡，而非仅依赖经验。

Abstract: Various 6-degree-of-freedom (DOF) and 7-DOF manipulators have been developed to date. Over a long history, their joint configurations and link length ratios have been determined empirically. In recent years, the development of robotic foundation models has become increasingly active, leading to the continuous proposal of various manipulators to support these models. However, none of these manipulators share exactly the same structure, as the order of joints and the ratio of link lengths differ among robots. Therefore, in order to discuss the optimal structure of a manipulator, we performed multi-objective optimization from the perspectives of end-effector reachability and joint torque. We analyze where existing manipulator structures stand within the sampling results of the optimization and provide insights for future manipulator design.

</details>


### [209] [Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM](https://arxiv.org/abs/2601.02723)
*Wenzheng Zhang,Kazuki Adachi,Yoshitaka Hara,Sousuke Nakamura*

Main category: cs.RO

TL;DR: 本文提出了一种基于AnyLoc深度学习方法替代传统BoVW的环路闭合检测方案，并引入自适应相似度阈值机制，显著提升了DPV-SLAM在不同环境下的环路闭合精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统BoVW方法依赖手工设计特征，在多视角和光照变化下性能受限；需提升SLAM系统中环路闭合的准确性与鲁棒性。

Method: 用AnyLoc深度特征替代BoVW进行视觉地点识别，并设计环境自适应的相似度阈值调整机制。

Result: 在室内外数据集上实验表明，该方法显著优于原始DPV-SLAM，环路闭合准确率与鲁棒性均提升。

Conclusion: 该方法为现代SLAM系统提供了一种实用、可扩展的环路闭合性能增强方案。

Abstract: Loop closure is crucial for maintaining the accuracy and consistency of visual SLAM. We propose a method to improve loop closure performance in DPV-SLAM. Our approach integrates AnyLoc, a learning-based visual place recognition technique, as a replacement for the classical Bag of Visual Words (BoVW) loop detection method. In contrast to BoVW, which relies on handcrafted features, AnyLoc utilizes deep feature representations, enabling more robust image retrieval across diverse viewpoints and lighting conditions. Furthermore, we propose an adaptive mechanism that dynamically adjusts similarity threshold based on environmental conditions, removing the need for manual tuning. Experiments on both indoor and outdoor datasets demonstrate that our method significantly outperforms the original DPV-SLAM in terms of loop closure accuracy and robustness. The proposed method offers a practical and scalable solution for enhancing loop closure performance in modern SLAM systems.

</details>


### [210] [Optimizing Control-Friendly Trajectories with Self-Supervised Residual Learning](https://arxiv.org/abs/2601.02738)
*Kexin Guo,Zihan Yang,Yuhang Liu,Jindou Jia,Xiang Yu*

Main category: cs.RO

TL;DR: 本文提出了一种自监督残差学习与轨迹优化框架，通过学习闭环模型中未知动力学效应作为名义动力学的残差，构建混合动力学模型，并设计轨迹优化器最小化残差影响，从而生成易于底层控制器精确跟踪的激进轨迹。


<details>
  <summary>Details</summary>
Motivation: 现实物理系统难以被精确解析建模，导致在控制器综合中存在残余动力学误差，影响激进轨迹的准确跟踪。

Method: 采用自监督残差学习方法，仅利用轨迹级数据、借助解析梯度进行训练，构建包含名义动力学与学习残差的混合模型；进而设计轨迹优化器，在优化过程中最小化沿轨迹的残差物理效应。

Result: 在四旋翼敏捷飞行任务中验证了该方法，结果表明基于混合动力学的优化器能生成可被精确跟踪的激进运动轨迹。

Conclusion: 所提框架有效提升了复杂机器人系统对激进轨迹的跟踪精度，兼具建模准确性与控制友好性。

Abstract: Real-world physics can only be analytically modeled with a certain level of precision for modern intricate robotic systems. As a result, tracking aggressive trajectories accurately could be challenging due to the existence of residual physics during controller synthesis. This paper presents a self-supervised residual learning and trajectory optimization framework to address the aforementioned challenges. At first, unknown dynamic effects on the closed-loop model are learned and treated as residuals of the nominal dynamics, jointly forming a hybrid model. We show that learning with analytic gradients can be achieved using only trajectory-level data while enjoying accurate long-horizon prediction with an arbitrary integration step size. Subsequently, a trajectory optimizer is developed to compute the optimal reference trajectory with the residual physics along it minimized. It ends up with trajectories that are friendly to the following control level. The agile flight of quadrotors illustrates that by utilizing the hybrid dynamics, the proposed optimizer outputs aggressive motions that can be precisely tracked.

</details>


### [211] [Unified Meta-Representation and Feedback Calibration for General Disturbance Estimation](https://arxiv.org/abs/2601.02762)
*Zihan Yang,Jindou Jia,Meng Wang,Yuhang Liu,Kexin Guo,Xiang Yu*

Main category: cs.RO

TL;DR: 本文提出了一种基于元学习与状态反馈校准在线自适应的通用扰动估计框架，用于解决机器人控制中未知时变扰动的精确估计问题。


<details>
  <summary>Details</summary>
Motivation: 现代机器人控制中，未知时变扰动导致精确控制困难；现有元学习方法依赖环境结构化表征，难以应对真实场景中的非结构化扰动，且易受表征误差和分布偏移影响。

Method: 构建基于元学习的扰动估计框架，通过有限时间窗口的历史观测提取特征，学习无需预设结构假设的统一扰动表征；引入状态反馈机制校准在线自适应过程，以抑制表征与泛化能力局限带来的学习残差。

Result: 理论证明了在线学习误差与扰动估计误差可同时收敛；实验表明该框架能有效估计多种快速变化的非结构化扰动，已在四旋翼飞行实验中验证。

Conclusion: 所提框架摆脱了对环境结构先验的依赖，提升了扰动估计的通用性与鲁棒性，为实际机器人系统提供了更可靠的实时扰动补偿方案。

Abstract: Precise control in modern robotic applications is always an open issue due to unknown time-varying disturbances. Existing meta-learning-based approaches require a shared representation of environmental structures, which lack flexibility for realistic non-structural disturbances. Besides, representation error and the distribution shifts can lead to heavy degradation in prediction accuracy. This work presents a generalizable disturbance estimation framework that builds on meta-learning and feedback-calibrated online adaptation. By extracting features from a finite time window of past observations, a unified representation that effectively captures general non-structural disturbances can be learned without predefined structural assumptions. The online adaptation process is subsequently calibrated by a state-feedback mechanism to attenuate the learning residual originating from the representation and generalizability limitations. Theoretical analysis shows that simultaneous convergence of both the online learning error and the disturbance estimation error can be achieved. Through the unified meta-representation, our framework effectively estimates multiple rapidly changing disturbances, as demonstrated by quadrotor flight experiments. See the project page for video, supplementary material and code: https://nonstructural-metalearn.github.io.

</details>


### [212] [Advancing Assistive Robotics: Multi-Modal Navigation and Biophysical Monitoring for Next-Generation Wheelchairs](https://arxiv.org/abs/2601.02766)
*Md. Anowar Hossain,Mohd. Ehsanul Hoque*

Main category: cs.RO

TL;DR: 本文提出了一种新型多模态电动轮椅（EPW）控制系统，整合了操纵杆、语音、手势和眼电图（EOG）四种控制接口，并结合连续生命体征监测（心率变异性、血氧饱和度、皮肤温度），兼顾患者自主性与护理人员实时监护能力；系统在精度、延迟和安全性方面均达到临床与工业标准。


<details>
  <summary>Details</summary>
Motivation: 针对ALS、中风后偏瘫及痴呆相关行动障碍患者，现有电动轮椅控制方式单一、缺乏健康状态协同监控，难以兼顾独立性与安全性需求。

Method: 设计并实现集成四种控制模态（操纵杆、语音、手势、EOG）与实时生理参数监测（HRV、SpO2、皮肤温度）的闭环EPW系统；采用双点校准提升传感器精度；通过20名残障参与者完成500次室内导航任务评估性能；生理数据经加密云端传输至Android监护端。

Result: 传感器校准误差：心率≤2 bpm、SpO2≤1%、皮肤温度≤0.5℃；控制识别准确率：操纵杆99%、语音97±2%、手势95±3%；平均闭环延迟20±0.5 ms；实现加密云传输与实时护理端告警。

Conclusion: 该系统成功融合多模态人机交互与远程健康监护，在满足ISO 7176-31与IEC 80601-2-78安全标准的同时，为自适应机器学习驱动的下一代智能辅具奠定基础。

Abstract: Assistive electric-powered wheelchairs (EPWs) have become essential mobility aids for people with disabilities such as amyotrophic lateral sclerosis (ALS), post-stroke hemiplegia, and dementia-related mobility impairment. This work presents a novel multi-modal EPW control system designed to prioritize patient needs while allowing seamless switching between control modes. Four complementary interfaces, namely joystick, speech, hand gesture, and electrooculography (EOG), are integrated with a continuous vital sign monitoring framework measuring heart rate variability, oxygen saturation (SpO2), and skin temperature. This combination enables greater patient independence while allowing caregivers to maintain real-time supervision and early intervention capability.
  Two-point calibration of the biophysical sensors against clinical reference devices resulted in root mean square errors of at most 2 bpm for heart rate, 0.5 degree Celsius for skin temperature, and 1 percent for SpO2. Experimental evaluation involved twenty participants with mobility impairments executing a total of 500 indoor navigation commands. The achieved command recognition accuracies were 99 percent for joystick control, 97 percent plus or minus 2 percent for speech, and 95 percent plus or minus 3 percent for hand gesture, with an average closed-loop latency of 20 plus or minus 0.5 milliseconds. Caregivers receive real-time alerts through an Android application following encrypted cloud transmission of physiological data. By integrating multi-modal mobility control with cloud-enabled health monitoring and reporting latency and energy budgets, the proposed prototype addresses key challenges in assistive robotics, contributes toward compliance with ISO 7176-31 and IEC 80601-2-78 safety standards, and establishes a foundation for future adaptive machine learning enhancements.

</details>


### [213] [M-SEVIQ: A Multi-band Stereo Event Visual-Inertial Quadruped-based Dataset for Perception under Rapid Motion and Challenging Illumination](https://arxiv.org/abs/2601.02777)
*Jingcheng Cao,Chaoran Xiong,Jianmin Song,Shang Yan,Jiachen Liu,Ling Pei*

Main category: cs.RO

TL;DR: 本文介绍了M-SEVIQ数据集，一个面向四足机器人敏捷运动场景的多波段立体事件视觉与惯性数据集，旨在弥补现有事件相机数据集在立体配置和多光谱感知方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统帧式相机在腿足机器人高速运动和低光照下易产生运动模糊，而事件相机虽具优势，但现有数据集缺乏立体配置和多波段、多光照条件下的覆盖。

Method: 构建了配备双目事件相机、帧式相机、IMU和关节编码器的Unitree Go2四足机器人平台，采集了30+组涵盖不同速度、光照波长和亮度的真实世界序列，并提供完整的内参、外参和时间同步标定数据。

Result: 发布了M-SEVIQ数据集，支持敏捷机器人感知、多传感器融合、语义分割及复杂环境下的多模态视觉研究。

Conclusion: M-SEVIQ填补了事件相机在立体与多波段感知数据上的空白，为动态、低光照等挑战性场景下的机器人视觉研究提供了高质量基准资源。

Abstract: Agile locomotion in legged robots poses significant challenges for visual perception. Traditional frame-based cameras often fail in these scenarios for producing blurred images, particularly under low-light conditions. In contrast, event cameras capture changes in brightness asynchronously, offering low latency, high temporal resolution, and high dynamic range. These advantages make them suitable for robust perception during rapid motion and under challenging illumination. However, existing event camera datasets exhibit limitations in stereo configurations and multi-band sensing domains under various illumination conditions. To address this gap, we present M-SEVIQ, a multi-band stereo event visual and inertial quadruped dataset collected using a Unitree Go2 equipped with stereo event cameras, a frame-based camera, an inertial measurement unit (IMU), and joint encoders. This dataset contains more than 30 real-world sequences captured across different velocity levels, illumination wavelengths, and lighting conditions. In addition, comprehensive calibration data, including intrinsic, extrinsic, and temporal alignments, are provided to facilitate accurate sensor fusion and benchmarking. Our M-SEVIQ can be used to support research in agile robot perception, sensor fusion, semantic segmentation and multi-modal vision in challenging environments.

</details>


### [214] [Closing the Reality Gap: Zero-Shot Sim-to-Real Deployment for Dexterous Force-Based Grasping and Manipulation](https://arxiv.org/abs/2601.02778)
*Haoyu Dong,Zhengmao He,Yang Li,Zhibin Li,Xinyu Yi,Zhe Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种实用的sim-to-real强化学习框架，利用密集触觉反馈与关节力矩传感，结合快速触觉仿真、电流-力矩标定和执行器动力学建模，实现了无需微调即可将纯仿真训练的控制策略零样本迁移到五指灵巧手，并成功实现可控抓握力跟踪与手内物体重定向。


<details>
  <summary>Details</summary>
Motivation: 灵巧手在真实硬件上部署直接训练的控制策略困难，主因是接触丰富的物理特性与不完美驱动。

Method: 构建基于密集触觉+关节力矩感知的sim-to-real RL框架；引入三项关键技术：(i) 基于并行正向运动学的高速触觉仿真；(ii) 电流到力矩的标定替代力矩传感器；(iii) 随机化建模执行器非理想动态（如回差、扭矩-转速饱和）；采用纯仿真训练的非对称Actor-Critic PPO策略。

Result: 策略零样本部署至五指灵巧手，成功实现可控抓握力跟踪与手内物体重定向，无需真实环境微调，表现鲁棒。

Conclusion: 本工作首次实现了完全在仿真中训练、零样本迁移至多指灵巧手并完成可控抓握的dexterous manipulation，验证了融合触觉与力矩感知及精准执行器建模对sim-to-real迁移的关键作用。

Abstract: Human-like dexterous hands with multiple fingers offer human-level manipulation capabilities, but training control policies that can directly deploy on real hardware remains difficult due to contact-rich physics and imperfect actuation. We close this gap with a practical sim-to-real reinforcement learning (RL) framework that utilizes dense tactile feedback combined with joint torque sensing to explicitly regulate physical interactions. To enable effective sim-to-real transfer, we introduce (i) a computationally fast tactile simulation that computes distances between dense virtual tactile units and the object via parallel forward kinematics, providing high-rate, high-resolution touch signals needed by RL; (ii) a current-to-torque calibration that eliminates the need for torque sensors on dexterous hands by mapping motor current to joint torque; and (iii) actuator dynamics modeling to bridge the actuation gaps with randomization of non-ideal effects such as backlash, torque-speed saturation. Using an asymmetric actor-critic PPO pipeline trained entirely in simulation, our policies deploy directly to a five-finger hand. The resulting policies demonstrated two essential skills: (1) command-based, controllable grasp force tracking, and (2) reorientation of objects in the hand, both of which were robustly executed without fine-tuning on the robot. By combining tactile and torque in the observation space with effective sensing/actuation modeling, our system provides a practical solution to achieve reliable dexterous manipulation. To our knowledge, this is the first demonstration of controllable grasping on a multi-finger dexterous hand trained entirely in simulation and transferred zero-shot on real hardware.

</details>


### [215] [Reinforcement Learning for Follow-the-Leader Robotic Endoscopic Navigation via Synthetic Data](https://arxiv.org/abs/2601.02798)
*Sicong Gao,Chen Qian,Laurence Xian,Liao Wu,Maurice Pagnucco,Yang Song*

Main category: cs.RO

TL;DR: 本文提出了一种基于单目深度估计与深度强化学习的跟随式内窥镜机器人导航方法，通过NVIDIA Omniverse仿真环境训练及Replicator合成数据微调Depth Anything模型，提升肠道三维感知精度，并引入几何感知奖惩机制实现低壁接触自主导航。


<details>
  <summary>Details</summary>
Motivation: 解决内窥镜机器人在狭窄管状环境中（如肠道）自主导航时频繁接触管壁、导致患者不适的问题，追求安全、高效、低接触的无持续人工干预导航。

Method: 构建基于柔性连续体结构的跟随式内窥镜机器人；在NVIDIA Omniverse中建立真实感肠道仿真环境；利用NVIDIA Replicator生成数千张合成腔内图像，微调Depth Anything模型以实现单目稠密深度估计；设计几何感知的奖励与惩罚机制用于强化学习导航策略训练。

Result: 相比原始Depth Anything模型，δ₁深度精度提升39.2%；导航J-index较次优方法降低0.67，验证了方法在精度与导航性能上的显著提升。

Conclusion: 所提视觉-强化学习联合框架能有效支持低壁接触、高鲁棒性的内窥镜自主导航，为医疗机器人在复杂解剖环境中的智能化应用提供了可行技术路径。

Abstract: Autonomous navigation is crucial for both medical and industrial endoscopic robots, enabling safe and efficient exploration of narrow tubular environments without continuous human intervention, where avoiding contact with the inner walls has been a longstanding challenge for prior approaches. We present a follow-the-leader endoscopic robot based on a flexible continuum structure designed to minimize contact between the endoscope body and intestinal walls, thereby reducing patient discomfort. To achieve this objective, we propose a vision-based deep reinforcement learning framework guided by monocular depth estimation. A realistic intestinal simulation environment was constructed in \textit{NVIDIA Omniverse} to train and evaluate autonomous navigation strategies. Furthermore, thousands of synthetic intraluminal images were generated using NVIDIA Replicator to fine-tune the Depth Anything model, enabling dense three-dimensional perception of the intestinal environment with a single monocular camera. Subsequently, we introduce a geometry-aware reward and penalty mechanism to enable accurate lumen tracking. Compared with the original Depth Anything model, our method improves $δ_{1}$ depth accuracy by 39.2% and reduces the navigation J-index by 0.67 relative to the second-best method, demonstrating the robustness and effectiveness of the proposed approach.

</details>


### [216] [Soft Responsive Materials Enhance Humanoid Safety](https://arxiv.org/abs/2601.02857)
*Chunzheng Wang,Yiyuan Zhang,Annan Tang,Ziqiu Zeng,Haoran Chen,Quan Gao,Zixuan Zhuang,Boyu Li,Zhilin Xiong,Aoqian Zhang,Ce Hao,Siyuan Luo,Tongyang Zhao,Cecilia Laschi,Fan Shi*

Main category: cs.RO

TL;DR: 本文提出了一种软硬协同设计框架，利用非牛顿流体基软响应材料提升人形机器人安全性，在正常交互时保持柔顺，受冲击时快速变硬以吸收和耗散跌倒力，并通过物理仿真指导防护结构设计与主动防跌策略学习，显著降低峰值冲击力，支持多次高风险跌落（如3米坠落、长楼梯翻滚）而不损坏硬件，提升了机器人鲁棒性与环境安全性。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在人类环境中部署受限于易跌倒及刚性金属-塑料结构对人和环境的安全风险。

Method: 提出软硬协同设计框架，采用非牛顿流体基软响应材料；结合物理仿真优化防护器位置与厚度，并学习主动防跌策略。

Result: 在42公斤全尺寸人形机器人上验证，显著降低峰值冲击力，支持3米坠落和长楼梯翻滚等多次跌落，无硬件损伤。

Conclusion: 融合响应材料、结构协同设计与基于学习的控制，推动了可交互安全、产业就绪的人形机器人发展。

Abstract: Humanoid robots are envisioned as general-purpose platforms in human-centered environments, yet their deployment is limited by vulnerability to falls and the risks posed by rigid metal-plastic structures to people and surroundings. We introduce a soft-rigid co-design framework that leverages non-Newtonian fluid-based soft responsive materials to enhance humanoid safety. The material remains compliant during normal interaction but rapidly stiffens under impact, absorbing and dissipating fall-induced forces. Physics-based simulations guide protector placement and thickness and enable learning of active fall policies. Applied to a 42 kg life-size humanoid, the protector markedly reduces peak impact and allows repeated falls without hardware damage, including drops from 3 m and tumbles down long staircases. Across diverse scenarios, the approach improves robot robustness and environmental safety. By uniting responsive materials, structural co-design, and learning-based control, this work advances interact-safe, industry-ready humanoid robots.

</details>


### [217] [Warm-Starting Collision-Free Model Predictive Control With Object-Centric Diffusion](https://arxiv.org/abs/2601.02873)
*Arthur Haffemayer,Alexandre Chapin,Armand Jordana,Krzysztof Wojciechowski,Florent Lamiraux,Nicolas Mansard,Vladimir Petrik*

Main category: cs.RO

TL;DR: 本文提出了一种结合扩散模型与碰撞感知模型预测控制（MPC）的混合运动规划方法，利用物体中心的槽注意力机制对场景建模，并以扩散Transformer生成初始轨迹、再由MPC实时优化，显著提升了在密集障碍环境中的成功率与实时性。


<details>
  <summary>Details</summary>
Motivation: 传统优化控制器在障碍物多时求解慢；扩散模型虽能生成避障轨迹，但缺乏对场景结构高效、通用的条件建模能力。

Method: 提出扩散Transformer作为warm-start生成器，以物体中心的slot attention提取紧凑障碍表征，并联合碰撞感知MPC进行实时轨迹优化，满足刚体动力学与符号距离碰撞约束。

Result: 在基准任务中，该混合方法相比采样式规划器或单一组件，成功率达更高、延迟更低；真实Panda机器人实验验证了其可靠性与安全性。

Conclusion: 扩散模型与MPC的协同设计可兼顾多样性与可行性，在严格时间限制下实现高效、安全的密集环境运动规划。

Abstract: Acting in cluttered environments requires predicting and avoiding collisions while still achieving precise control. Conventional optimization-based controllers can enforce physical constraints, but they struggle to produce feasible solutions quickly when many obstacles are present. Diffusion models can generate diverse trajectories around obstacles, yet prior approaches lacked a general and efficient way to condition them on scene structure. In this paper, we show that combining diffusion-based warm-starting conditioned with a latent object-centric representation of the scene and with a collision-aware model predictive controller (MPC) yields reliable and efficient motion generation under strict time limits. Our approach conditions a diffusion transformer on the system state, task, and surroundings, using an object-centric slot attention mechanism to provide a compact obstacle representation suitable for control. The sampled trajectories are refined by an optimal control problem that enforces rigid-body dynamics and signed-distance collision constraints, producing feasible motions in real time. On benchmark tasks, this hybrid method achieved markedly higher success rates and lower latency than sampling-based planners or either component alone. Real-robot experiments with a torque-controlled Panda confirm reliable and safe execution with MPC.

</details>


### [218] [LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments](https://arxiv.org/abs/2601.02905)
*Sara Micol Ferraina,Michele Brienza,Francesco Argenziano,Emanuele Musumeci,Vincenzo Suriani,Domenico D. Bloisi,Daniele Nardi*

Main category: cs.RO

TL;DR: LOST-3DSG 是一种轻量级、开放词汇的 3D 场景图方法，用于在动态环境中高效跟踪物体，避免使用高维视觉嵌入，转而采用 word2vec 和句子嵌入进行语义跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有基于重型基础模型的目标跟踪方法效率低下，亟需更轻量、高效的替代方案。

Method: 提出 LOST-3DSG，利用 word2vec 和句子嵌入构建开放词汇的 3D 场景图，不依赖 CLIP 等密集视觉特征。

Result: 在 TIAGo 机器人真实 3D 环境实验中，LOST-3DSG 在性能和效率上均优于依赖高维视觉嵌入的方法。

Conclusion: LOST-3DSG 为动态环境中的实时、轻量级物体跟踪提供了可行且有效的语义化新范式。

Abstract: Tracking objects that move within dynamic environments is a core challenge in robotics. Recent research has advanced this topic significantly; however, many existing approaches remain inefficient due to their reliance on heavy foundation models. To address this limitation, we propose LOST-3DSG, a lightweight open-vocabulary 3D scene graph designed to track dynamic objects in real-world environments. Our method adopts a semantic approach to entity tracking based on word2vec and sentence embeddings, enabling an open-vocabulary representation while avoiding the necessity of storing dense CLIP visual features. As a result, LOST-3DSG achieves superior performance compared to approaches that rely on high-dimensional visual embeddings. We evaluate our method through qualitative and quantitative experiments conducted in a real 3D environment using a TIAGo robot. The results demonstrate the effectiveness and efficiency of LOST-3DSG in dynamic object tracking. Code and supplementary material are publicly available on the project website at https://lab-rococo-sapienza.github.io/lost-3dsg/.

</details>


### [219] [Parameter-Robust MPPI for Safe Online Learning of Unknown Parameters](https://arxiv.org/abs/2601.02948)
*Matti Vahs,Jaeyoun Choi,Niklas Schmid,Jana Tumova,Chuchu Fan*

Main category: cs.RO

TL;DR: 本文提出了一种参数鲁棒的模型预测路径积分（PRMPPI）控制框架，结合在线参数学习与概率安全约束，通过粒子信念表示、共形预测评估安全约束，并并行优化性能主轨迹与安全备份轨迹，从而在动态不确定环境中实现安全且渐进优化的机器人控制。


<details>
  <summary>Details</summary>
Motivation: 机器人在动态环境中部署时，关键物理参数可能存在不确定性或随时间变化，需保证全程安全性。

Method: 采用基于Stein变分梯度下降的粒子信念更新参数分布，利用共形预测评估概率安全约束，并并行优化性能导向的标称轨迹与安全导向的备份轨迹。

Result: 仿真与实物实验表明，该方法相比基线方法具有更高的任务成功率、更低的跟踪误差和更准确的参数估计。

Conclusion: PRMPPI是一种兼顾安全性与性能渐进提升的鲁棒控制框架，适用于参数不确定且时变的动态环境。

Abstract: Robots deployed in dynamic environments must remain safe even when key physical parameters are uncertain or change over time. We propose Parameter-Robust Model Predictive Path Integral (PRMPPI) control, a framework that integrates online parameter learning with probabilistic safety constraints. PRMPPI maintains a particle-based belief over parameters via Stein Variational Gradient Descent, evaluates safety constraints using Conformal Prediction, and optimizes both a nominal performance-driven and a safety-focused backup trajectory in parallel. This yields a controller that is cautious at first, improves performance as parameters are learned, and ensures safety throughout. Simulation and hardware experiments demonstrate higher success rates, lower tracking error, and more accurate parameter estimates than baselines.

</details>


### [220] [Learning to Act Robustly with View-Invariant Latent Actions](https://arxiv.org/abs/2601.02994)
*Youngjoon Jeong,Junha Chun,Taesup Kim*

Main category: cs.RO

TL;DR: 本文提出VILA方法，通过建模基于物理动力学的潜在动作并利用真实动作序列进行动作引导对齐，学习视角不变的视觉表征，显著提升机器人策略在未见视角下的泛化能力和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 视觉机器人策略常因微小视角变化而失效，尤其在真实场景中视角多变且不可避免；现有方法仅从场景级多视角外观学习不变性，忽略了关键的物理动力学信息。

Method: 提出View-Invariant Latent Action（VILA），建模捕捉轨迹间转移模式的潜在动作，并基于真实动作序列设计动作引导目标，对齐不同视角下的潜在动作。

Result: 在仿真与真实世界实验中，VILA策略在未见视角下表现出强泛化能力，并能良好迁移到新任务，验证其作为预训练框架可提升鲁棒性与下游学习性能。

Conclusion: VILA通过将视角不变性建立在物理动力学基础上，克服了纯外观驱动方法的局限，为视觉机器人策略提供了更鲁棒、可迁移的表征学习范式。

Abstract: Vision-based robotic policies often struggle with even minor viewpoint changes, underscoring the need for view-invariant visual representations. This challenge becomes more pronounced in real-world settings, where viewpoint variability is unavoidable and can significantly disrupt policy performance. Existing methods typically learn invariance from multi-view observations at the scene level, but such approaches rely on visual appearance and fail to incorporate the physical dynamics essential for robust generalization. We propose View-Invariant Latent Action (VILA), which models a latent action capturing transition patterns across trajectories to learn view-invariant representations grounded in physical dynamics. VILA aligns these latent actions across viewpoints using an action-guided objective based on ground-truth action sequences. Experiments in both simulation and the real world show that VILA-based policies generalize effectively to unseen viewpoints and transfer well to new tasks, establishing VILA as a strong pretraining framework that improves robustness and downstream learning performance.

</details>


### [221] [A Bi-directional Adaptive Framework for Agile UAV Landing](https://arxiv.org/abs/2601.03037)
*Chunhui Zhao,Xirui Kao,Yilin Lu,Yang Lyu*

Main category: cs.RO

TL;DR: 本文提出了一种双向协同着陆框架，通过让移动平台主动倾斜以优化四旋翼着陆姿态，将传统单智能体跟踪问题转化为耦合系统优化问题，从而并行化对齐与下降阶段，提升动态场景下自主着陆的效率、精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统“先跟踪后下降”范式在高度动态场景下效率低下，将移动平台视为被动目标，导致复杂串行机动。

Method: 提出双向协同着陆框架：移动平台主动倾斜表面以提供稳定终端姿态；四旋翼规划时间最优、动力学可行且能耗最小的轨迹；二者协同实现对齐与下降的并行化。

Result: 在动态场景中验证有效，显著提升了自主四旋翼回收的效率、精度和鲁棒性，支持高敏捷性轨迹跟踪与瞬态窗口内快速状态同步。

Conclusion: 通过重新定义飞行器与平台的角色，将着陆建模为耦合系统优化问题，可突破传统范式限制，实现更高效、精准、鲁棒的移动平台自主着陆。

Abstract: Autonomous landing on mobile platforms is crucial for extending quadcopter operational flexibility, yet conventional methods are often too inefficient for highly dynamic scenarios. The core limitation lies in the prevalent ``track-then-descend'' paradigm, which treats the platform as a passive target and forces the quadcopter to perform complex, sequential maneuvers. This paper challenges that paradigm by introducing a bi-directional cooperative landing framework that redefines the roles of the vehicle and the platform. The essential innovation is transforming the problem from a single-agent tracking challenge into a coupled system optimization. Our key insight is that the mobile platform is not merely a target, but an active agent in the landing process. It proactively tilts its surface to create an optimal, stable terminal attitude for the approaching quadcopter. This active cooperation fundamentally breaks the sequential model by parallelizing the alignment and descent phases. Concurrently, the quadcopter's planning pipeline focuses on generating a time-optimal and dynamically feasible trajectory that minimizes energy consumption. This bi-directional coordination allows the system to execute the recovery in an agile manner, characterized by aggressive trajectory tracking and rapid state synchronization within transient windows. The framework's effectiveness, validated in dynamic scenarios, significantly improves the efficiency, precision, and robustness of autonomous quadrotor recovery in complex and time-constrained missions.

</details>


### [222] [Validating Generalist Robots with Situation Calculus and STL Falsification](https://arxiv.org/abs/2601.03038)
*Changwen Li,Rongjie Yan,Chih-Hong Cheng,Jian Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种两层验证框架，结合抽象推理与具体系统证伪，用于验证通用机器人在多样化自然语言任务下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 通用机器人能理解自然语言并执行多样任务，但其验证困难在于每项任务具有独特操作上下文和正确性规范，超出传统验证方法的假设。

Method: 构建两层验证框架：抽象层用情景演算建模世界、推导最弱前置条件，并进行约束感知的组合测试以生成语义有效且覆盖可控的世界-任务配置；具体层将配置实例化，在仿真中结合STL监控进行证伪。

Result: 在桌面操纵任务实验中，该框架成功发现了NVIDIA GR00T控制器中的多种失效案例。

Conclusion: 该框架为通用机器人自主性的验证提供了有效且有前景的新途径。

Abstract: Generalist robots are becoming a reality, capable of interpreting natural language instructions and executing diverse operations. However, their validation remains challenging because each task induces its own operational context and correctness specification, exceeding the assumptions of traditional validation methods. We propose a two-layer validation framework that combines abstract reasoning with concrete system falsification. At the abstract layer, situation calculus models the world and derives weakest preconditions, enabling constraint-aware combinatorial testing to systematically generate diverse, semantically valid world-task configurations with controllable coverage strength. At the concrete layer, these configurations are instantiated for simulation-based falsification with STL monitoring. Experiments on tabletop manipulation tasks show that our framework effectively uncovers failure cases in the NVIDIA GR00T controller, demonstrating its promise for validating general-purpose robot autonomy.

</details>


### [223] [PiDR: Physics-Informed Inertial Dead Reckoning for Autonomous Platforms](https://arxiv.org/abs/2601.03040)
*Arup Kumar Sahoo,Itzik Klein*

Main category: cs.RO

TL;DR: 本文提出了一种物理信息驱动的惯性航位推算框架PiDR，通过将惯性导航原理显式融入网络训练，提升纯惯性导航在无GNSS或视觉信息场景下的精度与鲁棒性，在真实机器人和水下平台数据上实现超29%定位精度提升。


<details>
  <summary>Details</summary>
Motivation: 纯惯性导航中传感器噪声导致轨迹漂移；现有深度学习方法为黑箱、依赖大量标注数据且难以保持物理一致性。

Method: 提出PiDR框架，引入物理信息残差组件，将惯性导航原理（如运动学方程）嵌入神经网络训练过程，实现可解释、低监督、物理一致的建模。

Result: 在移动机器人与自主水下航行器真实数据集上，定位精度提升超29%，具备跨平台、跨环境泛化能力，并支持资源受限平台实时部署。

Conclusion: PiDR是一种轻量、鲁棒且物理可解释的纯惯性导航解决方案，有效缓解黑箱模型缺陷，适用于GNSS拒止等恶劣场景。

Abstract: A fundamental requirement for full autonomy is the ability to sustain accurate navigation in the absence of external data, such as GNSS signals or visual information. In these challenging environments, the platform must rely exclusively on inertial sensors, leading to pure inertial navigation. However, the inherent noise and other error terms of the inertial sensors in such real-world scenarios will cause the navigation solution to drift over time. Although conventional deep-learning models have emerged as a possible approach to inertial navigation, they are inherently black-box in nature. Furthermore, they struggle to learn effectively with limited supervised sensor data and often fail to preserve physical principles. To address these limitations, we propose PiDR, a physics-informed inertial dead-reckoning framework for autonomous platforms in situations of pure inertial navigation. PiDR offers transparency by explicitly integrating inertial navigation principles into the network training process through the physics-informed residual component. PiDR plays a crucial role in mitigating abrupt trajectory deviations even under limited or sparse supervision. We evaluated PiDR on real-world datasets collected by a mobile robot and an autonomous underwater vehicle. We obtained more than 29% positioning improvement in both datasets, demonstrating the ability of PiDR to generalize different platforms operating in various environments and dynamics. Thus, PiDR offers a robust, lightweight, yet effective architecture and can be deployed on resource-constrained platforms, enabling real-time pure inertial navigation in adverse scenarios.

</details>


### [224] [SOP: A Scalable Online Post-Training System for Vision-Language-Action Models](https://arxiv.org/abs/2601.03044)
*Mingjie Pan,Siyuan Feng,Qinglin Zhang,Xinchen Li,Jianheng Song,Chendi Qu,Yi Wang,Chuankang Li,Ziyu Xiong,Zhi Chen,Yi Liu,Jianlan Luo*

Main category: cs.RO

TL;DR: 本文提出了一种可扩展的在线后训练（SOP）系统，支持通用视觉-语言-动作（VLA）模型在真实世界中进行在线、分布式、多任务后训练，显著提升性能并保持单策略泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型后训练方法多为离线、单机器人或任务特定，难以实现有效的在线策略适应和可扩展的真实世界交互学习。

Method: 提出SOP系统：通过闭环架构将机器人集群实时采集的on-policy数据与人工干预信号上传至云端学习器，异步下发更新策略；兼容多种后训练算法（如HG-DAgger和RECAP）。

Result: 在布料折叠、盒子组装、杂货补货等真实操作任务中，SOP数小时内显著提升预训练VLA模型性能，且性能随机器人数量近似线性增长，维持单一共享策略。

Conclusion: 紧耦合在线学习与大规模机器人部署，是实现在物理世界中高效、可靠、可扩展通用机器人策略后训练的关键路径。

Abstract: Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.

</details>


### [225] [A Fast Semidefinite Convex Relaxation for Optimal Control Problems With Spatio-Temporal Constraints](https://arxiv.org/abs/2601.03055)
*Shiying Dong,Zhipeng Shen,Rudolf Reiter,Hailong Huang,Bingzhao Gao,Hong Chen,Wen-Hua Chen*

Main category: cs.RO

TL;DR: 本文提出了一种基于时间缩放直接多段打靶法和稀疏性驱动的半定规划凸松弛的新方法，用于高效、准确地求解受时空约束的非线性最优控制问题。


<details>
  <summary>Details</summary>
Motivation: 解决自主智能体在时空约束下的最优控制问题（OCPs）对自动驾驶车辆节能驾驶、四旋翼导航等应用至关重要；但现有方法因动力学与事件时刻耦合导致非凸，常预设航点时间或仅用非凸轨迹优化，易得次优解。

Method: 提出一种时间缩放的直接多段打靶法，将预测时域按特征时间约束分段；并设计一种利用提升后公式稀疏结构的快速半定规划凸松弛方法。

Result: 仿真结果验证了所提方法在解的最优性和计算效率上的优势；真实四旋翼在开放时间窗约束下的航点飞行实验验证了其在复杂环境中的实用有效性。

Conclusion: 该方法显著改善了数值性质，在保证解质量的同时提升了求解速度，适用于实际部署的实时最优控制任务。

Abstract: Solving optimal control problems (OCPs) of autonomous agents operating under spatial and temporal constraints fast and accurately is essential in applications ranging from eco-driving of autonomous vehicles to quadrotor navigation. However, the nonlinear programs approximating the OCPs are inherently nonconvex due to the coupling between the dynamics and the event timing, and therefore, they are challenging to solve. Most approaches address this challenge by predefining waypoint times or just using nonconvex trajectory optimization, which simplifies the problem but often yields suboptimal solutions. To significantly improve the numerical properties, we propose a formulation with a time-scaling direct multiple shooting scheme that partitions the prediction horizon into segments aligned with characteristic time constraints. Moreover, we develop a fast semidefinite-programming-based convex relaxation that exploits the sparsity pattern of the lifted formulation. Comprehensive simulation studies demonstrate the solution optimality and computational efficiency. Furthermore, real-world experiments on a quadrotor waypoint flight task with constrained open time windows validate the practical applicability of the approach in complex environments.

</details>


### [226] [HEXAR: a Hierarchical Explainability Architecture for Robots](https://arxiv.org/abs/2601.03070)
*Tamlin Love,Ferran Gebellí,Pradip Pramanick,Antonio Andriella,Guillem Alenyà,Anais Garrell,Raquel Ros,Silvia Rossi*

Main category: cs.RO

TL;DR: 本文提出HEXAR框架，一种分层可解释性架构，通过模块化解释器和解释器选择器为机器人系统提供可插拔、分层的解释能力，并在TIAGo机器人上验证其在根因识别、错误信息排除和运行效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有机器人可解释性方法要么局限于单个模块、难以支持高层行为查询，要么采用整体式方法、无法利用机器人架构的模块化特性。

Method: 提出HEXAR（Hierarchical EXplainability Architecture for Robots）框架，包含面向不同机器人模块的专用解释器（如LLM推理、因果模型、特征重要性等）及一个根据查询动态选择最优解释器的解释器选择器。

Result: 在TIAGo机器人执行家庭辅助任务的180种场景-查询组合中，HEXAR在根因识别准确率、错误信息排除能力和运行时间方面显著优于端到端和聚合式基线方法。

Conclusion: HEXAR为构建透明、可信的自主机器人系统提供了可行且高效的新路径。

Abstract: As robotic systems become increasingly complex, the need for explainable decision-making becomes critical. Existing explainability approaches in robotics typically either focus on individual modules, which can be difficult to query from the perspective of high-level behaviour, or employ monolithic approaches, which do not exploit the modularity of robotic architectures. We present HEXAR (Hierarchical EXplainability Architecture for Robots), a novel framework that provides a plug-in, hierarchical approach to generate explanations about robotic systems. HEXAR consists of specialised component explainers using diverse explanation techniques (e.g., LLM-based reasoning, causal models, feature importance, etc) tailored to specific robot modules, orchestrated by an explainer selector that chooses the most appropriate one for a given query. We implement and evaluate HEXAR on a TIAGo robot performing assistive tasks in a home environment, comparing it against end-to-end and aggregated baseline approaches across 180 scenario-query variations. We observe that HEXAR significantly outperforms baselines in root cause identification, incorrect information exclusion, and runtime, offering a promising direction for transparent autonomous systems.

</details>


### [227] [Dual-quaternion learning control for autonomous vehicle trajectory tracking with safety guarantees](https://arxiv.org/abs/2601.03097)
*Omayra Yago Nieto,Alexandre Anahory Simoes,Juan I. Giribet,Leonardo Colombo*

Main category: cs.RO

TL;DR: 本文提出了一种基于高斯过程回归的SE(3)空间下双四元数框架的几何轨迹跟踪控制器，用于补偿未知状态相关扰动，同时保证姿态与位置的耦合特性及概率意义下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 针对自主机器人平台在SE(3)空间中运动时存在的建模误差、传感器干扰、执行器耦合及环境不确定性等未知状态相关扰动，需一种不依赖显式参数模型、能在线学习并保持几何结构的控制方法。

Method: 在双四元数框架下构建速度级几何反馈控制器，并嵌入高斯过程（GP）回归以在线学习和补偿姿态与位置通道中的未知扰动；采用李雅普诺夫分析证明在GP不确定性有界条件下姿态跟踪误差的概率最终有界性。

Result: 仿真验证了该方法在磁力计扰动等真实局部干扰（含旋转与平移相关性）下仍能实现精确、平滑的轨迹跟踪。

Conclusion: 融合几何建模与概率学习的策略可实现数据高效、鲁棒的自主机器人位姿控制，且无需先验扰动模型，具有良好的泛化性与形式稳定性保证。

Abstract: We propose a learning-based trajectory tracking controller for autonomous robotic platforms whose motion can be described kinematically on $\mathrm{SE}(3)$. The controller is formulated in the dual quaternion framework and operates at the velocity level, assuming direct command of angular and linear velocities, as is standard in many aerial vehicles and omnidirectional mobile robots. Gaussian Process (GP) regression is integrated into a geometric feedback law to learn and compensate online for unknown, state-dependent disturbances and modeling imperfections affecting both attitude and position, while preserving the algebraic structure and coupling properties inherent to rigid-body motion.
  The proposed approach does not rely on explicit parametric models of the unknown effects, making it well-suited for robotic systems subject to sensor-induced disturbances, unmodeled actuation couplings, and environmental uncertainties. A Lyapunov-based analysis establishes probabilistic ultimate boundedness of the pose tracking error under bounded GP uncertainty, providing formal stability guarantees for the learning-based controller.
  Simulation results demonstrate accurate and smooth trajectory tracking in the presence of realistic, localized disturbances, including correlated rotational and translational effects arising from magnetometer perturbations. These results illustrate the potential of combining geometric modeling and probabilistic learning to achieve robust, data-efficient pose control for autonomous robotic systems.

</details>


### [228] [A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting](https://arxiv.org/abs/2601.03200)
*Ziyang Sun,Lingfan Bao,Tianhu Peng,Jingcheng Sun,Chengxu Zhou*

Main category: cs.RO

TL;DR: 本文提出了一种基于3D高斯泼溅（3DGS）的快速、高保真数字孪生构建框架，支持从稀疏RGB图像在数分钟内生成兼具视觉真实感与规划可用碰撞几何的语义一致数字孪生，并在Franka机器人抓取任务中验证了其真实世界操作鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有数字孪生方法存在重建慢、视觉保真度低、难以生成规划可用的碰撞几何等问题，制约了sim-to-real迁移与闭环运动规划。

Method: 采用3D高斯泼溅（3DGS）作为统一场景表示实现快速光栅化重建；引入可见性感知的语义融合提升3D标签精度；设计基于滤波器的高效几何转换方法，生成可直接集成至Unity-ROS2-MoveIt物理引擎的碰撞模型。

Result: 在Franka Emika Panda机器人抓放任务实验中，该框架可在数分钟内构建高质量数字孪生，显著提升真实场景操作的鲁棒性与可靠性。

Conclusion: 融合语义与几何一致性的3DGS数字孪生，为非结构化环境中从感知到操作提供了快速、可靠且可扩展的解决方案。

Abstract: Developing high-fidelity, interactive digital twins is crucial for enabling closed-loop motion planning and reliable real-world robot execution, which are essential to advancing sim-to-real transfer. However, existing approaches often suffer from slow reconstruction, limited visual fidelity, and difficulties in converting photorealistic models into planning-ready collision geometry. We present a practical framework that constructs high-quality digital twins within minutes from sparse RGB inputs. Our system employs 3D Gaussian Splatting (3DGS) for fast, photorealistic reconstruction as a unified scene representation. We enhance 3DGS with visibility-aware semantic fusion for accurate 3D labelling and introduce an efficient, filter-based geometry conversion method to produce collision-ready models seamlessly integrated with a Unity-ROS2-MoveIt physics engine. In experiments with a Franka Emika Panda robot performing pick-and-place tasks, we demonstrate that this enhanced geometric accuracy effectively supports robust manipulation in real-world trials. These results demonstrate that 3DGS-based digital twins, enriched with semantic and geometric consistency, offer a fast, reliable, and scalable path from perception to manipulation in unstructured environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [229] [Physical Transformer](https://arxiv.org/abs/2601.02433)
*Tao Xu,Zhixin Hu,Li Luo,Momiao Xiong*

Main category: cs.LG

TL;DR: 本文提出了一种'物理Transformer'，将现代Transformer计算与几何表示和物理动力学相结合，构建了微观（自旋模型）、介观（神经微分流形+哈密顿/最优控制）和宏观（语义工作区+信息相图）三层物理化推理框架，在数值积分和动力系统任务中展现出更优的稳定性和长程精度。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统（如大语言模型、视觉模型等）主要在符号、语言或像素等虚拟空间中运行，缺乏物理世界的交互与可解释性，难以实现真正物理 grounded 的智能。

Method: 构建物理Transformer：微观层用有效哈密顿量与非哈密顿浴项建模注意力头和前馈块为相互作用自旋；介观层在学习的神经微分流形（NDM）上由哈密顿流与HJB最优控制驱动，采用辛离散层保持几何与能量不变性；宏观层维护生成式语义工作区与二维信息-相图以追踪推理过程中的不确定性与信息增益。

Result: 在简单数值积分与动力系统玩具任务中，物理Transformer在稳定性与长时域预测精度上优于朴素基线模型。

Conclusion: 该框架为‘物理AI’提供了新路径，有望统一数字推理与物理世界建模，提升AI的可解释性及与现实世界交互的能力。

Abstract: Digital AI systems spanning large language models, vision models, and generative architectures that operate primarily in symbolic, linguistic, or pixel domains. They have achieved striking progress, but almost all of this progress lives in virtual spaces. These systems transform embeddings and tokens, yet do not themselves touch the world and rarely admit a physical interpretation. In this work we propose a physical transformer that couples modern transformer style computation with geometric representation and physical dynamics. At the micro level, attention heads, and feed-forward blocks are modeled as interacting spins governed by effective Hamiltonians plus non-Hamiltonian bath terms. At the meso level, their aggregated state evolves on a learned Neural Differential Manifold (NDM) under Hamiltonian flows and Hamilton, Jacobi, Bellman (HJB) optimal control, discretized by symplectic layers that approximately preserve geometric and energetic invariants. At the macro level, the model maintains a generative semantic workspace and a two-dimensional information-phase portrait that tracks uncertainty and information gain over a reasoning trajectory. Within this hierarchy, reasoning tasks are formulated as controlled information flows on the manifold, with solutions corresponding to low cost trajectories that satisfy geometric, energetic, and workspace-consistency constraints. On simple toy problems involving numerical integration and dynamical systems, the physical transformer outperforms naive baselines in stability and long-horizon accuracy, highlighting the benefits of respecting underlying geometric and Hamiltonian structure. More broadly, the framework suggests a path toward physical AI that unify digital reasoning with physically grounded manifolds, opening a route to more interpretable and potentially unified models of reasoning, control, and interaction with the real world.

</details>


### [230] [WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks](https://arxiv.org/abs/2601.02439)
*Hao Bai,Alexey Taymanov,Tong Zhang,Aviral Kumar,Spencer Whitehead*

Main category: cs.LG

TL;DR: 本文介绍了WebGym——一个大规模开源视觉网页智能体训练环境，包含近30万真实网站任务，并提出高效异步采样系统与基于自交互轨迹的强化学习方法，显著提升模型在未见网站上的泛化成功率。


<details>
  <summary>Details</summary>
Motivation: 真实网站具有非平稳性和多样性，人工或小规模任务集难以支撑鲁棒的网页智能体策略学习。

Method: 构建WebGym环境，包含近30万基于评分标准评估的真实网站任务；采用基于自生成交互轨迹（rollouts）的简单强化学习范式；设计高吞吐异步 rollout 系统加速采样；在Qwen-3-VL-8B-Instruct上进行微调。

Result: WebGym rollout 系统实现4–5倍采样加速；微调后模型在未见网站的OOD测试集上成功率从26.2%提升至42.9%，显著超越GPT-4o（27.1%）和GPT-5-Thinking（29.8%）。

Conclusion: 大规模真实任务环境、高效采样机制与视觉语言模型微调相结合，可显著提升视觉网页智能体的泛化能力，为该领域提供可复现、可扩展的基准与方法论。

Abstract: We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.

</details>


### [231] [mHC-GNN: Manifold-Constrained Hyper-Connections for Graph Neural Networks](https://arxiv.org/abs/2601.02451)
*Subhankar Mishra*

Main category: cs.LG

TL;DR: 本文提出mHC-GNN，通过在多个并行流中扩展节点表示，并将流混合矩阵约束在Birkhoff多面体上，显著缓解GNN的过平滑问题并提升表达能力，超越1-WL测试限制，在极深层（128层）下仍保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络（GNNs）在深层架构中严重的过平滑问题，以及其表达能力受限于1-Weisfeiler-Leman（1-WL）测试的瓶颈。

Method: 将Manifold-Constrained Hyper-Connections（MHC）方法适配到GNN中，构建mHC-GNN：在n个并行流中扩展节点表示，并使用Sinkhorn-Knopp归一化将流间混合矩阵约束于Birkhoff多面体；理论证明其过平滑速率指数级减缓且表达力超越1-WL。

Result: 在10个数据集、4种GNN架构上均取得一致提升；深度从2增至128层时，标准GNN在16层后性能崩溃，而mHC-GNN在128层仍保持>74%准确率，极端深度下提升超50个百分点；消融实验表明去除流形约束导致最高达82%的性能下降。

Conclusion: mHC-GNN通过引入流形约束的超连接机制，有效突破了GNN深度与表达能力的双重限制，为构建更深、更强的图神经网络提供了新范式。

Abstract: Graph Neural Networks (GNNs) suffer from over-smoothing in deep architectures and expressiveness bounded by the 1-Weisfeiler-Leman (1-WL) test. We adapt Manifold-Constrained Hyper-Connections (\mhc)~\citep{xie2025mhc}, recently proposed for Transformers, to graph neural networks. Our method, mHC-GNN, expands node representations across $n$ parallel streams and constrains stream-mixing matrices to the Birkhoff polytope via Sinkhorn-Knopp normalization. We prove that mHC-GNN exhibits exponentially slower over-smoothing (rate $(1-γ)^{L/n}$ vs.\ $(1-γ)^L$) and can distinguish graphs beyond 1-WL. Experiments on 10 datasets with 4 GNN architectures show consistent improvements. Depth experiments from 2 to 128 layers reveal that standard GNNs collapse to near-random performance beyond 16 layers, while mHC-GNN maintains over 74\% accuracy even at 128 layers, with improvements exceeding 50 percentage points at extreme depths. Ablations confirm that the manifold constraint is essential: removing it causes up to 82\% performance degradation. Code is available at \href{https://github.com/smlab-niser/mhc-gnn}{https://github.com/smlab-niser/mhc-gnn}

</details>


### [232] [Polynomial Convergence of Riemannian Diffusion Models](https://arxiv.org/abs/2601.02499)
*Xingyu Xu,Ziyi Zhang,Yorie Nakahira,Guannan Qu,Yuejie Chi*

Main category: cs.LG

TL;DR: 本文改进了黎曼流形上扩散模型的理论分析，证明在L2精度得分估计下，多项式小步长即可保证总变差距离下的小采样误差，无需数据分布的光滑性或正定性假设。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型理论多基于欧氏空间假设，而实际数据常受限于流形结构；需建立更普适、更弱假设下的理论保障。

Method: 采用Li-Yau对热核对数梯度的估计与Minakshisundaram-Pleijel对扰动热方程的渐近展开作为核心分析工具。

Result: 在仅需流形曲率有温和标准假设、得分估计满足L2精度条件下，证明多项式量级小步长即可控制总变差距离下的采样误差。

Conclusion: 该工作显著强化了黎曼扩散模型的理论基础，为非欧空间上扩散模型的更精细分析开辟了新路径。

Abstract: Diffusion models have demonstrated remarkable empirical success in the recent years and are considered one of the state-of-the-art generative models in modern AI. These models consist of a forward process, which gradually diffuses the data distribution to a noise distribution spanning the whole space, and a backward process, which inverts this transformation to recover the data distribution from noise. Most of the existing literature assumes that the underlying space is Euclidean. However, in many practical applications, the data are constrained to lie on a submanifold of Euclidean space. Addressing this setting, De Bortoli et al. (2022) introduced Riemannian diffusion models and proved that using an exponentially small step size yields a small sampling error in the Wasserstein distance, provided the data distribution is smooth and strictly positive, and the score estimate is $L_\infty$-accurate. In this paper, we greatly strengthen this theory by establishing that, under $L_2$-accurate score estimate, a {\em polynomially small stepsize} suffices to guarantee small sampling error in the total variation distance, without requiring smoothness or positivity of the data distribution. Our analysis only requires mild and standard curvature assumptions on the underlying manifold. The main ingredients in our analysis are Li-Yau estimate for the log-gradient of heat kernel, and Minakshisundaram-Pleijel parametrix expansion of the perturbed heat equation. Our approach opens the door to a sharper analysis of diffusion models on non-Euclidean spaces.

</details>


### [233] [MixTTE: Multi-Level Mixture-of-Experts for Scalable and Adaptive Travel Time Estimation](https://arxiv.org/abs/2601.02943)
*Wenzhao Jiang,Jindong Han,Ruiqian Han,Hao Liu*

Main category: cs.LG

TL;DR: 本文提出MixTTE框架，通过结合链路级建模与路线级TTE系统，提升大规模城市交通场景下的行程时间估计精度与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有生产系统虽擅长路线级依赖建模，但难以捕捉城市级交通动态和长尾场景，导致在大型城市网络中预测不可靠。

Method: 提出混合式框架MixTTE，包含：1）时空外部注意力模块以高效捕获百万级路网全局交通动态；2）稳定化图混合专家网络处理异构交通模式；3）异步增量学习策略实现对交通分布变化的实时稳定适应。

Result: 在真实数据集上实验表明，MixTTE相较七个基线显著降低预测误差；已在滴滴平台部署，大幅提升了TTE服务的准确性与稳定性。

Conclusion: MixTTE是一种可扩展、自适应的TTE框架，有效弥补了工业级路线模型在城市尺度动态建模上的不足，具备实际落地价值。

Abstract: Accurate Travel Time Estimation (TTE) is critical for ride-hailing platforms, where errors directly impact user experience and operational efficiency. While existing production systems excel at holistic route-level dependency modeling, they struggle to capture city-scale traffic dynamics and long-tail scenarios, leading to unreliable predictions in large urban networks. In this paper, we propose \model, a scalable and adaptive framework that synergistically integrates link-level modeling with industrial route-level TTE systems. Specifically, we propose a spatio-temporal external attention module to capture global traffic dynamic dependencies across million-scale road networks efficiently. Moreover, we construct a stabilized graph mixture-of-experts network to handle heterogeneous traffic patterns while maintaining inference efficiency. Furthermore, an asynchronous incremental learning strategy is tailored to enable real-time and stable adaptation to dynamic traffic distribution shifts. Experiments on real-world datasets validate MixTTE significantly reduces prediction errors compared to seven baselines. MixTTE has been deployed in DiDi, substantially improving the accuracy and stability of the TTE service.

</details>


### [234] [GEM-Style Constraints for PEFT with Dual Gradient Projection in LoRA](https://arxiv.org/abs/2601.02500)
*Brian Tekmen,Jason Yin,Qianqian Tong*

Main category: cs.LG

TL;DR: 本文提出I-GEM，一种在LoRA适配器子空间中实现的轻量级梯度投影方法，显著降低Gradient Episodic Memory（GEM）的计算开销，同时保持相近的持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 全量微调大语言模型计算成本高，需更高效的持续学习方法；现有GEM在LLM上因投影开销大而难以实用，尤其在参数高效微调（如LoRA）场景下缺乏适配方案。

Method: 在LoRA低秩子空间内重构GEM的二次投影约束，设计固定预算、GPU内存驻留的双投影梯度近似算法（I-GEM），仅对适配器参数施加非干扰约束。

Result: 在AG News三任务漂移设定下，I-GEM以约千分之一的投影时间达到与GEM相当的平均准确率（误差<0.04），并比A-GEM高约1.4个百分点。

Conclusion: 将GEM约束限制在LoRA子空间是实现大规模语言模型实用化持续学习的有效且可行路径。

Abstract: Full fine-tuning of Large Language Models (LLMs) is computationally costly, motivating Continual Learning (CL) approaches that utilize parameter-efficient adapters. We revisit Gradient Episodic Memory (GEM) within the Low-Rank Adapter (LoRA) subspace and introduce I-GEM: a fixed-budget, GPU-resident dual projected-gradient approximation to GEM's quadratic projection. By constraining non-interference solely within the adapter parameters, I-GEM preserves GEM-like stability with orders-of-magnitude lower mean projection overhead. On a 3-task AG News split with induced domain drift, using GPT-2 (355M) and LoRA ($r=8$), I-GEM matches GEM's average accuracy (within $\sim\!0.04$ pts) and outperforms A-GEM by $\sim\!1.4$ pts. Crucially, it reduces projection time vs.\ GEM by a factor of $\sim\!10^3$. These results suggest that applying GEM constraints in the LoRA subspace is a practical pathway for continual learning at the LLM scale.

</details>


### [235] [hdlib 2.0: Extending Machine Learning Capabilities of Vector-Symbolic Architectures](https://arxiv.org/abs/2601.02509)
*Fabio Cumbo,Kabir Dhillon,Daniel Blankenberg*

Main category: cs.LG

TL;DR: 本文介绍了hdlib库的重大更新，增强了其在向量符号架构（VSA）中的机器学习能力，新增了监督分类（含特征选择）、回归、聚类和图学习模型，并首次实现了量子超维计算与量子机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 为满足VSA框架内对更先进、数据驱动建模的日益增长需求，扩展hdlib的机器学习功能。

Method: 提出四个新扩展：增强的监督分类模型（支持特征选择）、新的回归模型、无监督聚类模型、图学习模型；并首次实现量子超维计算（含量子算术运算）及量子机器学习监督模型。

Result: hdlib成功集成了多种机器学习模型，并首次实现了量子超维计算与量子机器学习模型，提升了VSA在实际任务中的适用性与前沿性。

Conclusion: 该扩展显著提升了hdlib作为VSA开发平台的实用性与先进性，推动了超维计算与量子机器学习的开源实践与应用发展。

Abstract: Following the initial publication of hdlib, a Python library for designing Vector-Symbolic Architectures (VSA), we introduce a major extension that significantly enhances its machine learning capabilities. VSA, also known as Hyperdimensional Computing, is a computing paradigm that represents and processes information using high-dimensional vectors. While the first version of hdlib established a robust foundation for creating and manipulating these vectors, this update addresses the growing need for more advanced, data-driven modeling within the VSA framework. Here, we present four extensions: significant enhancements to the existing supervised classification model also enabling feature selection, and a new regression model for predicting continuous variables, a clustering model for unsupervised learning, and a graph-based learning model. Furthermore, we propose the first implementation ever of Quantum Hyperdimensional Computing with quantum-powered arithmetic operations and a new Quantum Machine Learning model for supervised learning. hdlib remains open-source and available on GitHub at https://github.com/cumbof/hdlib under the MIT license, and distributed through the Python Package Index (pip install hdlib) and Conda (conda install -c conda-forge hdlib). Documentation and examples of these new features are available on the official Wiki at https://github.com/cumbof/hdlib/wiki.

</details>


### [236] [LLM-Enhanced Reinforcement Learning for Time Series Anomaly Detection](https://arxiv.org/abs/2601.02511)
*Bahareh Golchin,Banafsheh Rekabdar,Danielle Justo*

Main category: cs.LG

TL;DR: 本文提出了一种结合大语言模型（LLM）、强化学习（RL）与变分自编码器（VAE）的统一框架，用于解决标签稀疏、模式复杂的时间序列异常检测问题。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测面临标签稀疏、时序模式复杂及专家标注成本高等挑战。

Method: 融合LLM语义奖励引导的LSTM-RL代理、VAE重构误差增强的动态奖励缩放、主动学习与标签传播机制。

Result: 在Yahoo-A1和SMD基准上，该方法在有限标注预算下达到当前最优检测精度，并在数据受限场景中表现稳健。

Conclusion: LLM与RL及先进无监督技术的协同可提升现实场景中异常检测的鲁棒性与可扩展性。

Abstract: Detecting anomalies in time series data is crucial for finance, healthcare, sensor networks, and industrial monitoring applications. However, time series anomaly detection often suffers from sparse labels, complex temporal patterns, and costly expert annotation. We propose a unified framework that integrates Large Language Model (LLM)-based potential functions for reward shaping with Reinforcement Learning (RL), Variational Autoencoder (VAE)-enhanced dynamic reward scaling, and active learning with label propagation. An LSTM-based RL agent leverages LLM-derived semantic rewards to guide exploration, while VAE reconstruction errors add unsupervised anomaly signals. Active learning selects the most uncertain samples, and label propagation efficiently expands labeled data. Evaluations on Yahoo-A1 and SMD benchmarks demonstrate that our method achieves state-of-the-art detection accuracy under limited labeling budgets and operates effectively in data-constrained settings. This study highlights the promise of combining LLMs with RL and advanced unsupervised techniques for robust, scalable anomaly detection in real-world applications.

</details>


### [237] [Multi-scale Graph Autoregressive Modeling: Molecular Property Prediction via Next Token Prediction](https://arxiv.org/abs/2601.02530)
*Zhuoyang Jiang,Yaosen Min,Peiran Jin,Lei Chen*

Main category: cs.LG

TL;DR: 本文提出了Connection-Aware Motif Sequencing (CamS)，一种将分子图转化为结构丰富、因果有序序列的图到序列表示方法，使解码器-only Transformer可通过标准下一词预测（NTP）学习分子图；CamS通过数据驱动挖掘连接感知的基序，并采用支架根BFS序列化，支持多尺度层次建模；基于CamS预训练的CamS-LLaMA在MoleculeNet和MoleculeACE（活性悬崖基准）上达到SOTA性能，并展现出对关键结构差异的良好可解释性。


<details>
  <summary>Details</summary>
Motivation: SMILES-based NTP虽扩展性好但缺乏显式拓扑信息；图原生掩码建模虽能捕捉连通性，却易破坏关键化学细节（如活性悬崖）。需兼顾结构保真与序列建模优势。

Method: 提出CamS：1）数据驱动挖掘连接感知基序；2）以支架为根进行BFS序列化，形成稳定的核心-外围顺序；3）多尺度（细粒度到粗粒度）序列拼接实现层次建模；4）在CamS序列上预训练标准LLaMA模型（CamS-LLaMA）。

Result: CamS-LLaMA在MoleculeNet和MoleculeACE基准上均达SOTA；可解释性分析表明其注意力机制能有效聚焦于决定活性悬崖的关键结构差异。

Conclusion: CamS成功弥合了序列语言建模与图结构建模之间的鸿沟，通过因果、多尺度、连接感知的序列化策略，使纯解码器Transformer能高效、鲁棒且可解释地学习分子图。

Abstract: We present Connection-Aware Motif Sequencing (CamS), a graph-to-sequence representation that enables decoder-only Transformers to learn molecular graphs via standard next-token prediction (NTP). For molecular property prediction, SMILES-based NTP scales well but lacks explicit topology, whereas graph-native masked modeling captures connectivity but risks disrupting the pivotal chemical details (e.g., activity cliffs). CamS bridges this gap by serializing molecular graphs into structure-rich causal sequences. CamS first mines data-driven connection-aware motifs. It then serializes motifs via scaffold-rooted breadth-first search (BFS) to establish a stable core-to-periphery order. Crucially, CamS enables hierarchical modeling by concatenating sequences from fine to coarse motif scales, allowing the model to condition global scaffolds on dense, uncorrupted local structural evidence. We instantiate CamS-LLaMA by pre-training a vanilla LLaMA backbone on CamS sequences. It achieves state-of-the-art performance on MoleculeNet and the activity-cliff benchmark MoleculeACE, outperforming both SMILES-based language models and strong graph baselines. Interpretability analysis confirms that our multi-scale causal serialization effectively drives attention toward cliff-determining differences.

</details>


### [238] [Normalized Conditional Mutual Information Surrogate Loss for Deep Neural Classifiers](https://arxiv.org/abs/2601.02543)
*Linfeng Ye,Zhixiang Chi,Konstantinos N. Plataniotis,En-hui Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于信息论的新型代理损失函数——归一化条件互信息（NCMI），作为交叉熵（CE）的替代方案用于训练深度神经网络分类器，并在多个基准上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络分类器普遍使用交叉熵（CE）作为损失函数，但其存在优化目标与最终评估指标（如准确率）不一致的问题；作者观察到模型的归一化条件互信息（NCMI）与其准确率呈反比关系，因此希望设计一种更直接反映判别能力、且可高效优化的信息论损失。

Method: 提出归一化条件互信息（NCMI）作为新的监督学习损失函数；理论分析其与分类准确率的负相关性；设计一种交替优化算法以高效最小化NCMI；在图像识别（ImageNet）和全切片图像（CAMELYON-17）等任务上验证其有效性与鲁棒性。

Result: 在ImageNet上，ResNet-50使用NCMI训练相比CE提升2.77% top-1准确率；在CAMELYON-17上macro-F1提升8.6%；性能增益在不同网络结构和批量大小下均保持稳定；计算开销与CE相当。

Conclusion: NCMI是一种实用、高效且性能优越的交叉熵替代损失函数，在多种视觉分类任务中展现出显著优势，为基于信息论的深度学习优化提供了新思路。

Abstract: In this paper, we propose a novel information theoretic surrogate loss; normalized conditional mutual information (NCMI); as a drop in alternative to the de facto cross-entropy (CE) for training deep neural network (DNN) based classifiers. We first observe that the model's NCMI is inversely proportional to its accuracy. Building on this insight, we introduce an alternating algorithm to efficiently minimize the NCMI. Across image recognition and whole-slide imaging (WSI) subtyping benchmarks, NCMI-trained models surpass state of the art losses by substantial margins at a computational cost comparable to that of CE. Notably, on ImageNet, NCMI yields a 2.77% top-1 accuracy improvement with ResNet-50 comparing to the CE; on CAMELYON-17, replacing CE with NCMI improves the macro-F1 by 8.6% over the strongest baseline. Gains are consistent across various architectures and batch sizes, suggesting that NCMI is a practical and competitive alternative to CE.

</details>


### [239] [CutisAI: Deep Learning Framework for Automated Dermatology and Cancer Screening](https://arxiv.org/abs/2601.02562)
*Rohit Kaushik,Eva Kaushik*

Main category: cs.LG

TL;DR: 本文提出了Conformal Bayesian Dermatological Classifier (CBDC)，一个融合统计学习理论、拓扑数据分析（TDA）与贝叶斯共形推断的框架，旨在为皮肤科深度学习诊断模型提供理论保证和可校准的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 深度学习在皮肤科图像诊断中虽表现优异，但缺乏可靠的不确定性校准，限制了其临床部署；亟需兼具实证性能与理论保障的系统。

Method: 结合统计学习理论、拓扑数据分析（TDA）和贝叶斯共形推断，构建CBDC框架，推导分布依赖泛化界，证明CNN嵌入在光照与形态扰动下的拓扑稳定性，并提供有限样本共形覆盖保证。

Result: 在HAM10000、PH2和ISIC 2020数据集上验证，CBDC不仅保持高分类精度，还生成临床可解释、统计校准的预测结果。

Conclusion: CBDC实现了皮肤科AI诊断在理论严谨性与临床实用性上的双重突破，打通了机器学习理论与临床应用之间的接口。

Abstract: The rapid growth of dermatological imaging and mobile diagnostic tools calls for systems that not only demonstrate empirical performance but also provide strong theoretical guarantees. Deep learning models have shown high predictive accuracy; however, they are often criticized for lacking well, calibrated uncertainty estimates without which these models are hardly deployable in a clinical setting. To this end, we present the Conformal Bayesian Dermatological Classifier (CBDC), a well, founded framework that combines Statistical Learning Theory, Topological Data Analysis (TDA), and Bayesian Conformal Inference. CBDC offers distribution, dependent generalization bounds that reflect dermatological variability, proves a topological stability theorem that guarantees the invariance of convolutional neural network embeddings under photometric and morphological perturbations and provides finite conformal coverage guarantees for trustworthy uncertainty quantification.
  Through exhaustive experiments on the HAM10000, PH2, and ISIC 2020 datasets, we show that CBDC not only attains classification accuracy but also generates calibrated predictions that are interpretable from a clinical perspective. This research constitutes a theoretical and practical leap for deep dermatological diagnostics, thereby opening the machine learning theory clinical applicability interface.

</details>


### [240] [LendNova: Towards Automated Credit Risk Assessment with Language Models](https://arxiv.org/abs/2601.02573)
*Kiarash Shamsi,Danijel Novokmet,Joshua Peters,Mao Lin Liu,Paul K Edwards,Vahab Khoshdel*

Main category: cs.LG

TL;DR: LendNova 是首个面向信用风险评估的端到端自动化管道，利用大语言模型直接处理原始征信文本，无需人工特征工程，提升准确性、可扩展性与成本效益。


<details>
  <summary>Details</summary>
Motivation: 传统基于特征的信用风险模型成本高、信息利用率低，难以处理原始、术语密集的征信文本。

Method: 提出 LendNova 管道，采用先进 NLP 技术和语言模型，直接在原始征信文本上学习任务相关表征，自动提取风险信号，替代人工预处理与特征工程。

Result: 在真实数据上验证了其在准确性和效率方面的优越性能，确立了智能信用风险代理的基线。

Conclusion: 证明了语言模型在信用风险评估中的可行性，为构建更精准、自适应、自动化的金融决策基础系统奠定基础。

Abstract: Credit risk assessment is essential in the financial sector, but has traditionally depended on costly feature-based models that often fail to utilize all available information in raw credit records. This paper introduces LendNova, the first practical automated end-to-end pipeline for credit risk assessment, designed to utilize all available information in raw credit records by leveraging advanced NLP techniques and language models. LendNova transforms risk modeling by operating directly on raw, jargon-heavy credit bureau text using a language model that learns task-relevant representations without manual feature engineering. By automatically capturing patterns and risk signals embedded in the text, it replaces manual preprocessing steps, reducing costs and improving scalability. Evaluation on real-world data further demonstrates its strong potential in accurate and efficient risk assessment. LendNova establishes a baseline for intelligent credit risk agents, demonstrating the feasibility of language models in this domain. It lays the groundwork for future research toward foundation systems that enable more accurate, adaptable, and automated financial decision-making.

</details>


### [241] [Threat Detection in Social Media Networks Using Machine Learning Based Network Analysis](https://arxiv.org/abs/2601.02581)
*Aditi Sanjay Agrawal*

Main category: cs.LG

TL;DR: 本文提出了一种基于人工神经网络（ANN）的机器学习威胁检测框架，用于在社交媒体网络环境中根据网络流量特征识别恶意行为，通过大规模数据预处理与探索性分析解决数据不平衡、特征不一致和噪声问题，并在准确率、召回率、F1分数和ROC-AUC等指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的安全系统难以应对社交媒体快速发展带来的动态、复杂网络安全威胁（如入侵、异常流量、有组织攻击）；亟需可扩展、自适应的智能检测方法。

Method: 构建基于人工神经网络（ANN）的威胁检测模型；对大规模网络流量数据进行深度预处理与探索性数据分析，以解决数据不平衡、特征不一致和噪声问题。

Result: 模型在准确率、召回率、F1-score和ROC-AUC等常规性能指标上均表现出良好的检测能力与鲁棒性。

Conclusion: 基于神经网络的方法能有效识别大规模社交媒体网络中的潜在威胁动态，可作为现有入侵检测系统的有力补充，支撑主动式网络安全运维。

Abstract: The accelerated development of social media websites has posed intricate security issues in cyberspace, where these sites have increasingly become victims of criminal activities including attempts to intrude into them, abnormal traffic patterns, and organized attacks. The conventional rule-based security systems are not always scalable and dynamic to meet such a threat. This paper introduces a threat detection framework based on machine learning that can be used to classify malicious behavior in the social media network environment based on the nature of network traffic. Exploiting a rich network traffic dataset, the massive preprocessing and exploratory data analysis is conducted to overcome the problem of data imbalance, feature inconsistency, and noise. A model of artificial neural network (ANN) is then created to acquire intricate, non-linear tendencies of malicious actions. The proposed model is tested on conventional performance metrics, such as accuracy, accuracy, recall, F1-score, and ROC-AUC, and shows good detection and high levels of strength. The findings suggest that neural network-based solutions have the potential to be used effectively to identify the latent threat dynamics within the context of a large-scale social media network and that they can be employed to complement the existing intrusion detection system and better to conduct proactive cybersecurity operations.

</details>


### [242] [Chronicals: A High-Performance Framework for LLM Fine-Tuning with 3.51x Speedup over Unsloth](https://arxiv.org/abs/2601.02609)
*Arjun S. Nair*

Main category: cs.LG

TL;DR: Chronicals 是一个开源的 LLM 微调框架，通过四种协同优化（融合 Triton 内核、在线 Softmax、LoRA+ 自适应学习率、序列打包）显著降低内存占用并提升训练速度，在 A100-40GB 上实现最高 4.1x 加速，并指出前人工作存在未训练却误报高吞吐的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调受限于显存瓶颈（如 7B 模型需 84GB），远超主流 GPU（如 A100-40GB）容量，亟需高效内存与计算优化方案。

Method: 提出 Chronicals 框架，包含四项关键技术：(1) 融合 Triton 内核（RMSNorm/SwiGLU/QK-RoPE）减少 75% 内存访存；(2) Cut Cross-Entropy 实现在线 softmax，将 logits 内存从 5GB 压缩至 135MB；(3) LoRA+，基于梯度幅值分析推导出适配器矩阵间 16 倍差异学习率；(4) Best-Fit Decreasing 序列打包策略，回收 60–75% 因 padding 浪费的算力。

Result: 在 Qwen2.5-0.5B + A100-40GB 上：全参数微调达 41,184 tokens/s（vs Unsloth 11,736，+3.51x）；LoRA（rank=32）达 11,699 tokens/s（vs Unsloth MAX 2,857，+4.10x）；并发现 Unsloth 所报 46,000 tokens/s 实际梯度为零，模型未训练。

Conclusion: Chronicals 通过系统性软硬协同优化，突破 LLM 微调内存瓶颈，在保持数学严谨性（含在线 softmax 正确性证明、IO 复杂度界、LoRA+ 学习率推导、装箱近似保证）的同时，显著提升训练效率与实用性，代码与理论全部开源。

Abstract: Large language model fine-tuning is bottlenecked by memory: a 7B parameter model requires 84GB--14GB for weights, 14GB for gradients, and 56GB for FP32 optimizer states--exceeding even A100-40GB capacity. We present Chronicals, an open-source training framework achieving 3.51x speedup over Unsloth through four synergistic optimizations: (1) fused Triton kernels eliminating 75% of memory traffic via RMSNorm (7x), SwiGLU (5x), and QK-RoPE (2.3x) fusion; (2) Cut Cross-Entropy reducing logit memory from 5GB to 135MB through online softmax computation; (3) LoRA+ with theoretically-derived 16x differential learning rates between adapter matrices; and (4) Best-Fit Decreasing sequence packing recovering 60-75% of compute wasted on padding.
  On Qwen2.5-0.5B with A100-40GB, Chronicals achieves 41,184 tokens/second for full fine-tuning versus Unsloth's 11,736 tokens/second (3.51x). For LoRA at rank 32, we reach 11,699 tokens/second versus Unsloth MAX's 2,857 tokens/second (4.10x). Critically, we discovered that Unsloth's reported 46,000 tokens/second benchmark exhibited zero gradient norms--the model was not training.
  We provide complete mathematical foundations: online softmax correctness proofs, FlashAttention IO complexity bounds O(N^2 d^2 M^{-1}), LoRA+ learning rate derivations from gradient magnitude analysis, and bin-packing approximation guarantees. All implementations, benchmarks, and proofs are available at https://github.com/Ajwebdevs/Chronicals with pip installation via https://pypi.org/project/chronicals/.

</details>


### [243] [Credit Assignment via Neural Manifold Noise Correlation](https://arxiv.org/abs/2601.02636)
*Byungwoo Kang,Maceo Richards,Bernardo Sabatini*

Main category: cs.LG

TL;DR: 本文提出神经流形噪声相关（NMNC）方法，通过将扰动限制在神经流形上实现更高效、更符合生物学的信用分配，显著提升了多种网络的性能与样本效率，并使表征更接近灵长类视觉系统。


<details>
  <summary>Details</summary>
Motivation: 现有噪声相关法虽具生物合理性，但因需大量扰动而难以扩展，且各向同性噪声与神经活动位于低维流形的生物学事实不符。

Method: 提出神经流形噪声相关（NMNC），将扰动约束在神经流形内进行信用分配；理论与实验验证雅可比矩阵行空间与神经流形对齐，且流形维度随网络规模缓慢增长。

Result: NMNC在CIFAR-10、ImageNet尺度卷积网络及循环网络中显著优于标准噪声相关法，提升性能与样本效率，并产生更接近灵长类视觉系统的表征。

Conclusion: NMNC为生物回路如何实现信用分配提供了机制性假说，表明生物启发约束可能促进而非限制大规模有效学习。

Abstract: Credit assignment--how changes in individual neurons and synapses affect a network's output--is central to learning in brains and machines. Noise correlation, which estimates gradients by correlating perturbations of activity with changes in output, provides a biologically plausible solution to credit assignment but scales poorly as accurately estimating the Jacobian requires that the number of perturbations scale with network size. Moreover, isotropic noise conflicts with neurobiological observations that neural activity lies on a low-dimensional manifold. To address these drawbacks, we propose neural manifold noise correlation (NMNC), which performs credit assignment using perturbations restricted to the neural manifold. We show theoretically and empirically that the Jacobian row space aligns with the neural manifold in trained networks, and that manifold dimensionality scales slowly with network size. NMNC substantially improves performance and sample efficiency over vanilla noise correlation in convolutional networks trained on CIFAR-10, ImageNet-scale models, and recurrent networks. NMNC also yields representations more similar to the primate visual system than vanilla noise correlation. These findings offer a mechanistic hypothesis for how biological circuits could support credit assignment, and suggest that biologically inspired constraints may enable, rather than limit, effective learning at scale.

</details>


### [244] [Prioritized Replay for RL Post-training](https://arxiv.org/abs/2601.02648)
*Mehdi Fatemi*

Main category: cs.LG

TL;DR: 本文提出了一种面向问题级别的优先级框架，用于大语言模型的强化学习后训练，通过基于经验成功率的简单模型驱动优先级分数，自动聚焦于中等难度问题，无需预定义难度等级或外部标签。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习策略通常强调从简单任务开始，但中间成功率的问题往往能提供更强的学习信号；同时，传统方法依赖人工设计或额外预测器，缺乏灵活性和可扩展性。

Method: 基于优先回放思想，设计一种由实证成功率驱动的问题优先级评分机制；引入堆排序实现优先采样，并通过周期性重测已解/未解问题来缓解饥饿与遗忘问题。

Result: 该方法实现了自动、连续适应的问题优先级调度，在GRPO类后训练中提升了数据选择效率与模型性能，且无需人工设定难度层级或辅助模型。

Conclusion: 所提框架为大语言模型RL后训练提供了一种原理清晰、可扩展、免人工干预的动态课程学习替代方案，直接对齐GRPO优化动力学。

Abstract: We introduce a problem-level prioritization framework for RL post-training of large language models. Building on insights from prioritized replay in deep RL, as well as prior observations that rollouts with intermediate success rates tend to produce stronger learning signals under methods such as GRPO, our approach selects problems according to a simple, model-driven priority score derived from empirical success statistics. In contrast to conventional curriculum strategies that emphasize easier tasks early in training, the resulting schedule naturally focuses training on problems that are neither consistently solved nor consistently failed, while deprioritizing those that contribute little gradient information. The method yields a continuously adapting and automatic prioritization process that requires no predefined difficulty tiers, auxiliary predictors, or external labels. We further introduce lightweight mechanisms for practical deployment, including heap-based prioritized sampling and periodic retesting of solved and unsolved problems to mitigate starvation and forgetting. Overall, the approach offers a principled and scalable alternative to manually designed curricula while aligning data selection directly with the dynamics of GRPO-based post-training.

</details>


### [245] [When Prompting Meets Spiking: Graph Sparse Prompting via Spiking Graph Prompt Learning](https://arxiv.org/abs/2601.02662)
*Bo Jiang,Weijun Zhao,Beibei Wang,Jin Tang*

Main category: cs.LG

TL;DR: 本文提出SpikingGPF，一种基于脉冲神经元机制的稀疏图提示特征学习方法，通过选择性地在节点特征维度上进行提示，并利用稀疏表示理论提升鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有图提示特征（GPF）方法在所有节点特征维度上进行提示，存在冗余且对特征噪声敏感；而脉冲神经元具有低成本信息处理和天然稀疏输出特性，适合实现稀疏图提示。

Method: SpikingGPF包含两部分：1）采用脉冲神经元架构学习每个节点的稀疏提示向量，仅在关键特征维度上提示；2）基于稀疏表示理论，将节点提示建模为提示原子的稀疏线性组合。

Result: 在多个基准数据集上的实验表明，SpikingGPF在性能和抗噪鲁棒性方面均优于现有GPF方法。

Conclusion: SpikingGPF首次将脉冲神经元机制引入图提示学习，实现了更紧凑、轻量且鲁棒的稀疏提示设计，为图神经网络适配下游任务提供了新思路。

Abstract: Graph Prompt Feature (GPF) learning has been widely used in adapting pre-trained GNN model on the downstream task. GPFs first introduce some prompt atoms and then learns the optimal prompt vector for each graph node using the linear combination of prompt atoms. However, existing GPFs generally conduct prompting over node's all feature dimensions which is obviously redundant and also be sensitive to node feature noise. To overcome this issue, for the first time, this paper proposes learning sparse graph prompts by leveraging the spiking neuron mechanism, termed Spiking Graph Prompt Feature (SpikingGPF). Our approach is motivated by the observation that spiking neuron can perform inexpensive information processing and produce sparse outputs which naturally fits the task of our graph sparse prompting. Specifically, SpikingGPF has two main aspects. First, it learns a sparse prompt vector for each node by exploiting a spiking neuron architecture, enabling prompting on selective node features. This yields a more compact and lightweight prompting design while also improving robustness against node noise. Second, SpikingGPF introduces a novel prompt representation learning model based on sparse representation theory, i.e., it represents each node prompt as a sparse combination of prompt atoms. This encourages a more compact representation and also facilitates efficient computation. Extensive experiments on several benchmarks demonstrate the effectiveness and robustness of SpikingGPF.

</details>


### [246] [MAFS: Multi-head Attention Feature Selection for High-Dimensional Data via Deep Fusion of Filter Methods](https://arxiv.org/abs/2601.02668)
*Xiaoyan Sun,Qingyu Meng,Yalu Wen*

Main category: cs.LG

TL;DR: 本文提出MAFS（多头注意力机制特征选择）框架，结合统计先验与深度学习能力，在高维生物医学数据中实现可解释、稳定且高效的特征选择。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法在高维生物医学数据中存在局限：滤波法缺乏对复杂关系的建模能力；深度学习方法稳定性差、可解释性弱；单头注意力机制难以捕获多层次依赖且初始化敏感。亟需融合统计可解释性与深度表征能力的新方法。

Method: MAFS采用三阶段设计：1）基于滤波法的统计先验进行稳定初始化；2）利用多头注意力机制并行建模多视角特征交互与非线性关系；3）通过重排序模块整合各注意力头输出，缓解冲突并减少信息损失，生成鲁棒一致的特征重要性排序。

Result: 在模拟数据及真实癌症基因表达、阿尔茨海默病等数据集上，MAFS在特征覆盖度与稳定性方面均显著优于现有滤波法和深度学习方法，兼具可扩展性、可解释性与鲁棒性。

Conclusion: MAFS成功弥合了统计方法与深度学习在特征选择中的鸿沟，为精准医学中的高维生物医学数据分析提供了兼具性能与可解释性的新范式。

Abstract: Feature selection is essential for high-dimensional biomedical data, enabling stronger predictive performance, reduced computational cost, and improved interpretability in precision medicine applications. Existing approaches face notable challenges. Filter methods are highly scalable but cannot capture complex relationships or eliminate redundancy. Deep learning-based approaches can model nonlinear patterns but often lack stability, interpretability, and efficiency at scale. Single-head attention improves interpretability but is limited in capturing multi-level dependencies and remains sensitive to initialization, reducing reproducibility. Most existing methods rarely combine statistical interpretability with the representational power of deep learning, particularly in ultra-high-dimensional settings. Here, we introduce MAFS (Multi-head Attention-based Feature Selection), a hybrid framework that integrates statistical priors with deep learning capabilities. MAFS begins with filter-based priors for stable initialization and guide learning. It then uses multi-head attention to examine features from multiple perspectives in parallel, capturing complex nonlinear relationships and interactions. Finally, a reordering module consolidates outputs across attention heads, resolving conflicts and minimizing information loss to generate robust and consistent feature rankings. This design combines statistical guidance with deep modeling capacity, yielding interpretable importance scores while maximizing retention of informative signals. Across simulated and real-world datasets, including cancer gene expression and Alzheimer's disease data, MAFS consistently achieves superior coverage and stability compared with existing filter-based and deep learning-based alternatives, offering a scalable, interpretable, and robust solution for feature selection in high-dimensional biomedical data.

</details>


### [247] [Uni-FinLLM: A Unified Multimodal Large Language Model with Modular Task Heads for Micro-Level Stock Prediction and Macro-Level Systemic Risk Assessment](https://arxiv.org/abs/2601.02677)
*Gongao Zhang,Haijiang Zeng,Lu Jiang*

Main category: cs.LG

TL;DR: 本文提出Uni-FinLLM，一种统一的多模态大语言模型，通过共享Transformer主干和模块化任务头，联合处理金融文本、数值时间序列、基本面和视觉数据，显著提升股票预测、信用风险评估和系统性风险检测性能。


<details>
  <summary>Details</summary>
Motivation: 金融监管与机构需整合异构数据以评估从个股波动到系统性风险的多层次风险，但现有方法多孤立处理任务，难以建模跨尺度依赖关系。

Method: 提出Uni-FinLLM，采用共享Transformer骨干网络与模块化任务头，结合跨模态注意力机制和多任务优化，实现对金融文本、数值时序、基本面及视觉数据的联合建模。

Result: 在股票方向预测准确率提升至67.4%（基线61.7%），信用风险评估准确率达84.1%（基线79.6%），宏观系统性风险早期预警准确率达82.3%。

Conclusion: 统一多模态大语言模型可有效联合建模资产行为与系统性脆弱性，为金融领域提供可扩展的决策支持引擎。

Abstract: Financial institutions and regulators require systems that integrate heterogeneous data to assess risks from stock fluctuations to systemic vulnerabilities. Existing approaches often treat these tasks in isolation, failing to capture cross-scale dependencies. We propose Uni-FinLLM, a unified multimodal large language model that uses a shared Transformer backbone and modular task heads to jointly process financial text, numerical time series, fundamentals, and visual data. Through cross-modal attention and multi-task optimization, it learns a coherent representation for micro-, meso-, and macro-level predictions. Evaluated on stock forecasting, credit-risk assessment, and systemic-risk detection, Uni-FinLLM significantly outperforms baselines. It raises stock directional accuracy to 67.4% (from 61.7%), credit-risk accuracy to 84.1% (from 79.6%), and macro early-warning accuracy to 82.3%. Results validate that a unified multimodal LLM can jointly model asset behavior and systemic vulnerabilities, offering a scalable decision-support engine for finance.

</details>


### [248] [Topology-Independent Robustness of the Weighted Mean under Label Poisoning Attacks in Heterogeneous Decentralized Learning](https://arxiv.org/abs/2601.02682)
*Jie Peng,Weiyu Li,Stefan Vlaski,Qing Ling*

Main category: cs.LG

TL;DR: 本文分析了去中心化梯度下降在标签投毒攻击下的鲁棒性，发现加权平均聚合器虽常被视为脆弱基线，但在足够异质性条件下（如全局污染率低于局部污染率、正常节点网络不连通或稀疏）反而优于鲁棒聚合器，且其性能与网络拓扑无关，而鲁棒聚合器的误差则依赖于拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 提升去中心化信号处理和机器学习系统对恶意攻击（如标签投毒）的鲁棒性，挑战‘加权平均聚合器必然脆弱’的传统认知。

Method: 理论分析去中心化梯度下降在标签投毒攻击下使用鲁棒聚合器与加权平均聚合器的收敛行为与误差界，并结合网络拓扑特性（如连通性、稀疏性、污染率分布）进行比较。

Result: 加权平均聚合器的性能与网络拓扑无关，而鲁棒聚合器的学习误差受拓扑影响；在特定异质性条件下（如全局污染率<局部污染率、正则节点网络不连通或稀疏），加权平均聚合器可优于鲁棒聚合器。

Conclusion: 网络拓扑在标签投毒鲁棒性中起关键作用，简单加权平均聚合器在特定实际场景下具备意外优势，不应被一概视为脆弱基线。

Abstract: Robustness to malicious attacks is crucial for practical decentralized signal processing and machine learning systems. A typical example of such attacks is label poisoning, meaning that some agents possess corrupted local labels and share models trained on these poisoned data. To defend against malicious attacks, existing works often focus on designing robust aggregators; meanwhile, the weighted mean aggregator is typically considered a simple, vulnerable baseline. This paper analyzes the robustness of decentralized gradient descent under label poisoning attacks, considering both robust and weighted mean aggregators. Theoretical results reveal that the learning errors of robust aggregators depend on the network topology, whereas the performance of weighted mean aggregator is topology-independent. Remarkably, the weighted mean aggregator, although often considered vulnerable, can outperform robust aggregators under sufficient heterogeneity, particularly when: (i) the global contamination rate (i.e., the fraction of poisoned agents for the entire network) is smaller than the local contamination rate (i.e., the maximal fraction of poisoned neighbors for the regular agents); (ii) the network of regular agents is disconnected; or (iii) the network of regular agents is sparse and the local contamination rate is high. Empirical results support our theoretical findings, highlighting the important role of network topology in the robustness to label poisoning attacks.

</details>


### [249] [Scaling Laws of Machine Learning for Optimal Power Flow](https://arxiv.org/abs/2601.02706)
*Xinyi Liu,Xuan He,Yize Chen*

Main category: cs.LG

TL;DR: 本文首次系统研究了基于机器学习的最优潮流（OPF）在数据规模和计算规模两个维度上的缩放规律，发现预测误差、约束违反和求解速度均遵循幂律关系，为ML-OPF的实际部署提供了可预测、可量化的指导。


<details>
  <summary>Details</summary>
Motivation: 现有ML-OPF研究缺乏对训练数据量需求和模型复杂度与实时性权衡的定量分析，导致实际应用中依赖试错法。

Method: 开展跨数据规模（0.1K–40K样本）和计算规模（多种FLOPs的NN架构，含DNN和PINN）的系统性缩放实验，拟合各资源维度与性能指标（MAE、约束违反、速度）之间的幂律关系。

Result: 在ACOPF任务上，预测误差、约束违反和求解速度均随数据量和计算量呈现一致的幂律缩放；识别出精度与可行性之间的分歧，并刻画了计算最优前沿。

Conclusion: 揭示了ML-OPF中关键缩放规律，支持可预测、原则性的ML流水线设计，为工程部署提供定量依据。

Abstract: Optimal power flow (OPF) is one of the fundamental tasks for power system operations. While machine learning (ML) approaches such as deep neural networks (DNNs) have been widely studied to enhance OPF solution speed and performance, their practical deployment faces two critical scaling questions: What is the minimum training data volume required for reliable results? How should ML models' complexity balance accuracy with real-time computational limits? Existing studies evaluate discrete scenarios without quantifying these scaling relationships, leading to trial-and-error-based ML development in real-world applications. This work presents the first systematic scaling study for ML-based OPF across two dimensions: data scale (0.1K-40K training samples) and compute scale (multiple NN architectures with varying FLOPs). Our results reveal consistent power-law relationships on both DNNs and physics-informed NNs (PINNs) between each resource dimension and three core performance metrics: prediction error (MAE), constraint violations and speed. We find that for ACOPF, the accuracy metric scales with dataset size and training compute. These scaling laws enable predictable and principled ML pipeline design for OPF. We further identify the divergence between prediction accuracy and constraint feasibility and characterize the compute-optimal frontier. This work provides quantitative guidance for ML-OPF design and deployments.

</details>


### [250] [CRoPE: Efficient Parametrization of Rotary Positional Embedding](https://arxiv.org/abs/2601.02728)
*Beicheng Lou,Zifei Xu*

Main category: cs.LG

TL;DR: 本文提出了一种更自然、参数更少的复数线性变换方式来实现旋转位置编码，替代现有实现中的冗余操作，在几乎不损失性能的前提下提升了参数效率和表征可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有旋转位置编码（RoPE）在实际实现中对Q/K/V投影的操作并非真正的复数线性变换，存在参数冗余；作者认为复数线性变换是更自然的参数化方式，并可节省近50%注意力块内的参数。

Method: 用严格的复数线性变换重新参数化RoPE中的Q/K/V投影，消除原实现中的冗余自由度。

Result: 该修改在样本内和样本外性能上影响可忽略，同时提升参数利用效率并增强表示空间的可解释性。

Conclusion: 复数线性变换是一种更优的RoPE实现范式，兼顾效率、简洁性与性能。

Abstract: Rotary positional embedding has become the state-of-the-art approach to encode position information in transformer-based models. While it is often succinctly expressed in complex linear algebra, we note that the actual implementation of $Q/K/V$-projections is not equivalent to a complex linear transformation. We argue that complex linear transformation is a more natural parametrization and saves near 50\% parameters within the attention block. We show empirically that removing such redundancy has negligible impact on the model performance both in sample and out of sample. Our modification achieves more efficient parameter usage, as well as a cleaner interpretation of the representation space.

</details>


### [251] [Scalable Tree Ensemble Proximities in Python](https://arxiv.org/abs/2601.02735)
*Adrien Aumon,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: 本文提出了一种可扩展的树集成相似性度量计算框架，通过定义可分离加权叶节点碰撞相似性，实现稀疏矩阵分解，避免显式成对计算，显著降低时间和内存复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有基于树集成（如随机森林）的相似性度量方法存在二次时间或内存复杂度，难以扩展到大规模数据。

Method: 提出可分离加权叶节点碰撞相似性（Separable Weighted Leaf-Collision Proximities）家族，证明其具有精确稀疏矩阵分解形式，利用稀疏线性代数高效实现。

Result: 在标准CPU上实证表明，该方法在运行时间和内存占用上大幅优于传统方法，可高效处理数十万样本规模的数据集。

Conclusion: 所提框架为树集成相似性提供了可扩展、低内存、高效率的通用计算范式。

Abstract: Tree ensemble methods such as Random Forests naturally induce supervised similarity measures through their decision tree structure, but existing implementations of proximities derived from tree ensembles typically suffer from quadratic time or memory complexity, limiting their scalability. In this work, we introduce a general framework for efficient proximity computation by defining a family of Separable Weighted Leaf-Collision Proximities. We show that any proximity measure in this family admits an exact sparse matrix factorization, restricting computation to leaf-level collisions and avoiding explicit pairwise comparisons. This formulation enables low-memory, scalable proximity computation using sparse linear algebra in Python. Empirical benchmarks demonstrate substantial runtime and memory improvements over traditional approaches, allowing tree ensemble proximities to scale efficiently to datasets with hundreds of thousands of samples on standard CPU hardware.

</details>


### [252] [Q-Regularized Generative Auto-Bidding: From Suboptimal Trajectories to Optimal Policies](https://arxiv.org/abs/2601.02754)
*Mingming Zhang,Na Li,Zhuang Feiqing,Hongyang Zheng,Jiangbing Zhou,Wang Wuyin,Sheng-jie Sun,XiaoWei Chen,Junxiong Zhu,Lixin Zou,Chenliang Li*

Main category: cs.LG

TL;DR: 本文提出QGA方法，通过在决策Transformer中引入Q值正则化与双Q学习，并设计Q值引导的双重探索机制，以联合优化策略模仿与动作价值最大化，在真实广告系统中显著提升GMV和ROI。


<details>
  <summary>Details</summary>
Motivation: 现有自动竞价方法依赖强化学习和生成模型，但存在结构复杂、超参调优成本高、历史次优轨迹影响策略学习等问题。

Method: 提出Q-value正则化的生成式自动竞价方法QGA：1）在Decision Transformer骨干中嵌入双Q学习的Q值正则化模块；2）设计Q值引导的双重探索机制，支持多目标Return-to-go条件建模与局部动作扰动。

Result: 在公开基准和仿真环境上性能优于或媲美现有方法；大规模真实A/B测试中，广告GMV提升3.27%，广告ROI提升2.49%。

Conclusion: QGA通过联合优化策略模仿与动作价值估计，并辅以Q值引导的安全探索，有效缓解了离线数据中次优轨迹带来的偏差，提升了自动竞价策略的泛化性与实际效果。

Abstract: With the rapid development of e-commerce, auto-bidding has become a key asset in optimizing advertising performance under diverse advertiser environments. The current approaches focus on reinforcement learning (RL) and generative models. These efforts imitate offline historical behaviors by utilizing a complex structure with expensive hyperparameter tuning. The suboptimal trajectories further exacerbate the difficulty of policy learning.
  To address these challenges, we proposes QGA, a novel Q-value regularized Generative Auto-bidding method. In QGA, we propose to plug a Q-value regularization with double Q-learning strategy into the Decision Transformer backbone. This design enables joint optimization of policy imitation and action-value maximization, allowing the learned bidding policy to both leverage experience from the dataset and alleviate the adverse impact of the suboptimal trajectories. Furthermore, to safely explore the policy space beyond the data distribution, we propose a Q-value guided dual-exploration mechanism, in which the DT model is conditioned on multiple return-to-go targets and locally perturbed actions. This entire exploration process is dynamically guided by the aforementioned Q-value module, which provides principled evaluation for each candidate action. Experiments on public benchmarks and simulation environments demonstrate that QGA consistently achieves superior or highly competitive results compared to existing alternatives. Notably, in large-scale real-world A/B testing, QGA achieves a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.

</details>


### [253] [RadioDiff-Flux: Efficient Radio Map Construction via Generative Denoise Diffusion Model Trajectory Midpoint Reuse](https://arxiv.org/abs/2601.02790)
*Xiucheng Wang,Peilin Zheng,Honggang Jia,Nan Cheng,Ruijin Sun,Conghao Zhou,Xuemin Shen*

Main category: cs.LG

TL;DR: 本文提出RadioDiff-Flux，一种两阶段潜在扩散框架，通过解耦静态环境建模与动态细化，复用预计算中点以跳过冗余去噪，显著降低推理延迟，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 在6G高速、快变环境中，传统扩散模型因迭代特性导致推理延迟过高，难以满足实时无线电图（RM）构建需求。

Method: 利用扩散过程中语义相似场景下潜在中点高度一致的结构性质，设计两阶段框架：第一阶段仅用静态场景特征生成可缓存共享的粗粒度潜在表示；第二阶段基于预训练模型，结合动态条件与发射机位置进行自适应细化。

Result: 实验表明RadioDiff-Flux可实现最高50倍加速，且精度损失低于0.15%。

Conclusion: RadioDiff-Flux为未来6G网络中快速、可扩展的无线电图生成提供了实用有效的解决方案。

Abstract: Accurate radio map (RM) construction is essential to enabling environment-aware and adaptive wireless communication. However, in future 6G scenarios characterized by high-speed network entities and fast-changing environments, it is very challenging to meet real-time requirements. Although generative diffusion models (DMs) can achieve state-of-the-art accuracy with second-level delay, their iterative nature leads to prohibitive inference latency in delay-sensitive scenarios. In this paper, by uncovering a key structural property of diffusion processes: the latent midpoints remain highly consistent across semantically similar scenes, we propose RadioDiff-Flux, a novel two-stage latent diffusion framework that decouples static environmental modeling from dynamic refinement, enabling the reuse of precomputed midpoints to bypass redundant denoising. In particular, the first stage generates a coarse latent representation using only static scene features, which can be cached and shared across similar scenarios. The second stage adapts this representation to dynamic conditions and transmitter locations using a pre-trained model, thereby avoiding repeated early-stage computation. The proposed RadioDiff-Flux significantly reduces inference time while preserving fidelity. Experiment results show that RadioDiff-Flux can achieve up to 50 acceleration with less than 0.15% accuracy loss, demonstrating its practical utility for fast, scalable RM generation in future 6G networks.

</details>


### [254] [Stratified Hazard Sampling: Minimal-Variance Event Scheduling for CTMC/DTMC Discrete Diffusion and Flow Models](https://arxiv.org/abs/2601.02799)
*Seunghwan Jang,SooJean Han*

Main category: cs.LG

TL;DR: 本文提出分层风险采样（SHS）方法，用于改进基于CTMC/DTMC的离散生成模型的推理过程，通过分层累积风险实现低方差、高可复现的token编辑，且无需额外超参数。


<details>
  <summary>Details</summary>
Motivation: 现有基于CTMC/DTMC的离散生成模型（如D3PM、CTDD）在均匀噪声初始化下依赖独立伯努利决策进行逐步编辑，导致编辑次数与时机方差大，引发欠编辑或过编辑等失败模式，损害可复现性。

Method: 提出分层风险采样（SHS），将每个token的编辑建模为由累积风险（CTMC）或累积跳跃质量（DTMC）驱动的事件；通过为每位置分配单一随机相位，并在累积量跨越单位间隔阈值时触发跳跃，从而最小化编辑次数估计的方差；同时引入相位分配变体以支持黑名单式词汇约束，优先在高风险位置早期编辑。

Result: SHS作为即插即用、免调参的推理原则，在保持原有跳转目标采样不变（因而保留多模态性）的前提下，使编辑次数估计方差降至理论最小值（≤1/4），显著缓解欠/过编辑问题，提升生成可复现性；相位分配变体有效抑制晚期掩蔽效应。

Conclusion: SHS为离散时间/连续时间马尔可夫生成模型提供了一种通用、高效且理论最优的低方差推理机制，兼顾准确性、稳定性和约束可控性，是离散扩散与流匹配类方法的重要推理升级。

Abstract: CTMC/DTMC-based discrete generative models, including uniform-noise discrete diffusion (e.g., D3PM/CTDD) and discrete flow matching, enable non-autoregressive sequence generation by repeatedly replacing tokens through a time-inhomogeneous Markov process. Inference is typically implemented with step-based simulation: each token decides to jump via independent Bernoulli (or categorical) draws at every discretization step. Under uniform-noise initialization, where self-correction requires multiple edits per position, these independent decisions induce substantial variance in both the number and timing of edits, leading to characteristic failure modes such as under-editing (residual noise) or over-editing (cascading unnecessary substitutions), decreasing reproducibility.
  We propose Stratified Hazard Sampling (SHS), a drop-in and hyperparameter-free inference principle for any sampler that admits a stay-vs.-replace decomposition. SHS models per-token edits as events driven by cumulative hazard (CTMC) or cumulative jump mass (DTMC) and places events by stratifying this cumulative quantity: with a single random phase per position, a token jumps whenever its accumulated hazard crosses unit-spaced thresholds. This preserves the expected number of jumps while achieving the minimum possible variance among unbiased integer estimators (bounded by 1/4), without altering per-jump destination sampling and thus retaining multimodality. We also introduce a phase-allocation variant for blacklist-style lexical constraints that prioritizes early edits at high-risk positions to mitigate late-masking artifacts.

</details>


### [255] [Electricity Price Forecasting: Bridging Linear Models, Neural Networks and Online Learning](https://arxiv.org/abs/2601.02856)
*Btissame El Mahtout,Florian Ziel*

Main category: cs.LG

TL;DR: 本文提出了一种结合线性和非线性前馈神经网络结构的新型多变量神经网络方法，用于电力价格日前预测，在保证高精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 电力价格日前预测对投资组合管理、电厂运营决策、储能优化和需求响应规划至关重要，但市场不确定性与波动性使得高精度建模极具挑战；现有线性模型计算高效但难以捕捉非线性关系，非线性模型精度高但计算成本大。

Method: 提出一种融合线性与非线性前馈神经结构的多变量神经网络方法，创新性地集成在线学习与预测组合机制，并全面纳入风电/光伏出力、电力需求、能源燃料与碳市场、自回归动态及日历效应等关键特征。

Result: 在六年期欧洲主要电力市场实证中，相较当前最优基准模型，RMSE降低12–13%，MAE降低15–18%，同时显著降低计算成本。

Conclusion: 该混合神经网络架构在精度与效率之间实现了更优平衡，为电力市场预测提供了兼具实用性与先进性的新范式。

Abstract: Precise day-ahead forecasts for electricity prices are crucial to ensure efficient portfolio management, support strategic decision-making for power plant operations, enable efficient battery storage optimization, and facilitate demand response planning. However, developing an accurate prediction model is highly challenging in an uncertain and volatile market environment. For instance, although linear models generally exhibit competitive performance in predicting electricity prices with minimal computational requirements, they fail to capture relevant nonlinear relationships. Nonlinear models, on the other hand, can improve forecasting accuracy with a surge in computational costs. We propose a novel multivariate neural network approach that combines linear and nonlinear feed-forward neural structures. Unlike previous hybrid models, our approach integrates online learning and forecast combination for efficient training and accuracy improvement. It also incorporates all relevant characteristics, particularly the fundamental relationships arising from wind and solar generation, electricity demand patterns, related energy fuel and carbon markets, in addition to autoregressive dynamics and calendar effects. Compared to the current state-of-the-art benchmark models, the proposed forecasting method significantly reduces computational cost while delivering superior forecasting accuracy (12-13% RMSE and 15-18% MAE reductions). Our results are derived from a six-year forecasting study conducted on major European electricity markets.

</details>


### [256] [Quantum-Enhanced Neural Contextual Bandit Algorithms](https://arxiv.org/abs/2601.02870)
*Yuqi Huang,Vincent Y. F Tan,Sharu Theresa Jose*

Main category: cs.LG

TL;DR: 本文提出QNTK-UCB算法，利用量子神经正切核（QNTK）解决量子神经网络在随机上下文赌博机中的训练不稳定性与参数爆炸问题，理论证明其参数复杂度显著优于经典NeuralUCB，并在实验中展现出低数据下的高样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的随机上下文赌博机算法在扩展至量子神经网络（QNN）时面临严重挑战，包括过参数化、计算不稳定性和贫瘠高原现象。

Method: 提出QNTK-UCB算法：冻结随机初始化的QNN，用其静态QNTK作为岭回归核，规避显式参数优化的不稳定性，同时利用量子归纳偏置。

Result: 理论分析显示QNTK-UCB参数复杂度为Ω((TK)^3)，远优于NeuralUCB的Ω((TK)^8)；实验证明其在非线性合成任务和变分量子本征解算器任务中具备更高样本效率。

Conclusion: QNTK固有的隐式正则化与更锐利的谱衰减特性，为在线学习实现‘量子优势’提供了新路径。

Abstract: Stochastic contextual bandits are fundamental for sequential decision-making but pose significant challenges for existing neural network-based algorithms, particularly when scaling to quantum neural networks (QNNs) due to issues such as massive over-parameterization, computational instability, and the barren plateau phenomenon. This paper introduces the Quantum Neural Tangent Kernel-Upper Confidence Bound (QNTK-UCB) algorithm, a novel algorithm that leverages the Quantum Neural Tangent Kernel (QNTK) to address these limitations.
  By freezing the QNN at a random initialization and utilizing its static QNTK as a kernel for ridge regression, QNTK-UCB bypasses the unstable training dynamics inherent in explicit parameterized quantum circuit training while fully exploiting the unique quantum inductive bias. For a time horizon $T$ and $K$ actions, our theoretical analysis reveals a significantly improved parameter scaling of $Ω((TK)^3)$ for QNTK-UCB, a substantial reduction compared to $Ω((TK)^8)$ required by classical NeuralUCB algorithms for similar regret guarantees. Empirical evaluations on non-linear synthetic benchmarks and quantum-native variational quantum eigensolver tasks demonstrate QNTK-UCB's superior sample efficiency in low-data regimes. This work highlights how the inherent properties of QNTK provide implicit regularization and a sharper spectral decay, paving the way for achieving ``quantum advantage'' in online learning.

</details>


### [257] [Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction](https://arxiv.org/abs/2601.02884)
*Hana Yahia,Bruno Figliuzzi,Florent Di Meglio,Laurent Gerbaud,Stephane Menand,Mohamed Mahjoub*

Main category: cs.LG

TL;DR: This paper compares domain generalization methods (ADG, IRM, baseline) for predicting Stick-Slip Index from surface drilling data across unseen wells, showing ADG achieves best performance with 10% improvement and 60% severe event detection.


<details>
  <summary>Details</summary>
Motivation: To develop a robust regression model for predicting the Stick-Slip Index (SSI) that generalizes across different drilling wells — a critical challenge due to domain shifts in time-series drilling data.

Method: Trains models on 60-second, 1 Hz surface drilling sequences to predict continuous SSI; evaluates Adversarial Domain Generalization (ADG), Invariant Risk Minimization (IRM), and baseline models; applies grid search for hyperparameter tuning and assesses transfer learning (TL) enhancement.

Result: ADG and IRM improve performance by 10% and 8% over baseline; severe event detection rises from 20% (baseline) to 60% (ADG); TL further boosts performance; ADG slightly outperforms IRM.

Conclusion: Domain generalization—especially ADG—is highly effective for cross-well SSI prediction in drilling applications, offering improved robustness and early severe event detection.

Abstract: This paper provides a comprehensive comparison of domain generalization techniques applied to time series data within a drilling context, focusing on the prediction of a continuous Stick-Slip Index (SSI), a critical metric for assessing torsional downhole vibrations at the drill bit. The study aims to develop a robust regression model that can generalize across domains by training on 60 second labeled sequences of 1 Hz surface drilling data to predict the SSI. The model is tested in wells that are different from those used during training. To fine-tune the model architecture, a grid search approach is employed to optimize key hyperparameters. A comparative analysis of the Adversarial Domain Generalization (ADG), Invariant Risk Minimization (IRM) and baseline models is presented, along with an evaluation of the effectiveness of transfer learning (TL) in improving model performance. The ADG and IRM models achieve performance improvements of 10% and 8%, respectively, over the baseline model. Most importantly, severe events are detected 60% of the time, against 20% for the baseline model. Overall, the results indicate that both ADG and IRM models surpass the baseline, with the ADG model exhibiting a slight advantage over the IRM model. Additionally, applying TL to a pre-trained model further improves performance. Our findings demonstrate the potential of domain generalization approaches in drilling applications, with ADG emerging as the most effective approach.

</details>


### [258] [RPIQ: Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization for Visually Impaired Assistance](https://arxiv.org/abs/2601.02888)
*Xuanyu Wang,Haisen Su,Jingtao Zhang,Xiangxiang Wang,Yongbin Yu,Manping Fan,Bo Gong,Siqi Chen,Mingsheng Cao,Liyong Ren*

Main category: cs.LG

TL;DR: 本文提出了一种名为RPIQ的新型量化框架，通过多协作闭环补偿机制，在4比特量化下显著降低大模型内存消耗（60%-75%），同时保持接近全精度模型的性能，适用于视障辅助系统。


<details>
  <summary>Details</summary>
Motivation: 视障用户亟需低资源、高精度的智能辅助系统，但大模型因内存和推理开销大、现有量化方法忽略块间误差累积而难以部署。

Method: 提出Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization (RPIQ)框架，结合单样本校准与高斯-赛德尔迭代量化实现多协作闭环补偿。

Result: 在OPT、Qwen、LLaMA、CogVLM2等多类大模型上实现4-bit量化，峰值内存降低60%-75%，语言与视觉任务性能接近全精度，尤其在文本理解与复杂场景视觉问答中表现优异。

Conclusion: RPIQ有效提升了大模型在资源受限辅助设备上的部署可行性与稳定性，兼顾高效性与可靠性，为视障用户实时获取准确信息提供了新路径。

Abstract: Visually impaired users face significant challenges in daily information access and real-time environmental perception, and there is an urgent need for intelligent assistive systems with accurate recognition capabilities. Although large-scale models provide effective solutions for perception and reasoning, their practical deployment on assistive devices is severely constrained by excessive memory consumption and high inference costs. Moreover, existing quantization strategies often ignore inter-block error accumulation, leading to degraded model stability. To address these challenges, this study proposes a novel quantization framework -- Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization(RPIQ), whose quantization process adopts a multi-collaborative closed-loop compensation scheme based on Single Instance Calibration and Gauss-Seidel Iterative Quantization. Experiments on various types of large-scale models, including language models such as OPT, Qwen, and LLaMA, as well as vision-language models such as CogVLM2, demonstrate that RPIQ can compress models to 4-bit representation while significantly reducing peak memory consumption (approximately 60%-75% reduction compared to original full-precision models). The method maintains performance highly close to full-precision models across multiple language and visual tasks, and exhibits excellent recognition and reasoning capabilities in key applications such as text understanding and visual question answering in complex scenarios. While verifying the effectiveness of RPIQ for deployment in real assistive systems, this study also advances the computational efficiency and reliability of large models, enabling them to provide visually impaired users with the required information accurately and rapidly.

</details>


### [259] [Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control](https://arxiv.org/abs/2601.02896)
*Harshvardhan Saini,Yiming Tang,Dianbo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于梯度上升的可解释提示优化框架（RESGA和SAEGA），用于精准控制大语言模型中的行为人格（如谄媚、幻觉等），在多个模型和人格类型上验证了其有效性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在控制大语言模型行为人格时面临可扩展性与可解释性的权衡：手工提示工程直观但低效不精确，自动优化方法有效却缺乏对模型内部机制的可解释关联。

Method: 提出基于梯度上升的提示优化框架，包括RESGA和SAEGA两种方法，通过在识别出的人格方向上优化随机初始化提示，并引入流利度约束的梯度上升以提升提示质量。

Result: 在Llama 3.1、Qwen 2.5和Gemma 3上成功 steering 三种人格（谄媚、幻觉、短视奖励）；在谄媚人格上，自动发现的提示将性能从49.90%提升至79.24%。

Conclusion: 该方法将提示发现锚定于机制可解释的特征空间，为可控、可解释的行为调控提供了新范式。

Abstract: Controlling emergent behavioral personas (e.g., sycophancy, hallucination) in Large Language Models (LLMs) is critical for AI safety, yet remains a persistent challenge. Existing solutions face a dilemma: manual prompt engineering is intuitive but unscalable and imprecise, while automatic optimization methods are effective but operate as "black boxes" with no interpretable connection to model internals. We propose a novel framework that adapts gradient ascent to LLMs, enabling targeted prompt discovery. In specific, we propose two methods, RESGA and SAEGA, that both optimize randomly initialized prompts to achieve better aligned representation with an identified persona direction. We introduce fluent gradient ascent to control the fluency of discovered persona steering prompts. We demonstrate RESGA and SAEGA's effectiveness across Llama 3.1, Qwen 2.5, and Gemma 3 for steering three different personas,sycophancy, hallucination, and myopic reward. Crucially, on sycophancy, our automatically discovered prompts achieve significant improvement (49.90% compared with 79.24%). By grounding prompt discovery in mechanistically meaningful features, our method offers a new paradigm for controllable and interpretable behavior modification.

</details>


### [260] [ChemBART: A Pre-trained BART Model Assisting Organic Chemistry Analysis](https://arxiv.org/abs/2601.02915)
*Kenan Li,Yijian Zhang,Jin Wang,Haipeng Gan,Zeying Sun,Xiaoguang Lei,Hao Dong*

Main category: cs.LG

TL;DR: ChemBART是一种基于SMILES、以化学反应为预训练目标的大语言模型，支持多种下游化学任务（如前体预测、条件优化、性质分类等），并成功指导实验验证，显著提升合成路径效率和产率。


<details>
  <summary>Details</summary>
Motivation: 现有基于SMILES的LLM多针对单一化学任务（如前体预测），缺乏统一模型支持多任务协同的合成规划；需探索以反应为中心的预训练范式以提升模型泛化性与实用性。

Method: 提出ChemBART：基于化学反应数据进行掩码填充（mask-filling）预训练的SMILES语言模型；支持多任务微调，包括前体/试剂生成、温度-产率回归、分子性质分类，以及与蒙特卡洛树搜索结合的强化学习策略与价值函数优化。

Result: ChemBART在多个化学任务上表现优异；其设计的多步合成路线经湿实验验证，路径更短且产率较文献基准提升约30%。

Conclusion: 以反应为中心的预训练范式有效提升了LLM在化学合成规划中的实用性与泛化能力；ChemBART实现了‘一个模型、一次预训练、多任务适配’，推动了从计算预测到实验落地的完整合成规划闭环。

Abstract: Recent advances in large language models (LLMs) have demonstrated transformative potential across diverse fields. While LLMs have been applied to molecular simplified molecular input line entry system (SMILES) in computer-aided synthesis planning (CASP), existing methodologies typically address single tasks, such as precursor prediction. We introduce ChemBART, a SMILES-based LLM pre-trained on chemical reactions, which enables a unified model for multiple downstream chemical tasks--achieving the paradigm of "one model, one pre-training, multiple tasks." By leveraging outputs from a mask-filling pre-training task on reaction expressions, ChemBART effectively solves a variety of chemical problems, including precursor/reagent generation, temperature-yield regression, molecular property classification, and optimizing the policy and value functions within a reinforcement learning framework, integrated with Monte Carlo tree search for multi-step synthesis route design. Unlike single-molecule pre-trained LLMs constrained to specific applications, ChemBART addresses broader chemical challenges and integrates them for comprehensive synthesis planning. Crucially, ChemBART-designed multi-step synthesis routes and reaction conditions directly inspired wet-lab validation, which confirmed shorter pathways with ~30% yield improvement over literature benchmarks. Our work validates the power of reaction-focused pre-training and showcases the broad utility of ChemBART in advancing the complete synthesis planning cycle.

</details>


### [261] [From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures](https://arxiv.org/abs/2601.02997)
*Waleed Khalid,Dmitry Ignatov,Radu Timofte*

Main category: cs.LG

TL;DR: 本文提出了一种将代码导向的大语言模型（LLM）嵌入闭环合成框架的方法，通过22轮监督微调，使其能自主设计高性能、结构新颖的PyTorch卷积神经网络；模型在有效性、单轮准确率和高表现架构生成能力上显著提升，并能超越训练数据生成新架构。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在程序合成中表现出色，但在自主进行神经网络架构设计（需兼顾语法正确性、性能与结构新颖性）方面仍缺乏探索。

Method: 将代码导向LLM置于闭环合成框架中，经22轮监督微调；每轮生成PyTorch卷积网络，用单epoch精度作低保真性能信号验证，并用MinHash-Jaccard准则去重；高表现、新颖架构转为prompt-code对，采用LoRA进行参数高效微调，初始化自LEMUR数据集。

Result: 有效生成率稳定在50.6%（峰值74.5%），首epoch平均精度从28.06%升至50.99%，超40%精度的候选架构占比从2.04%升至96.81%；生成455个原语料库中不存在的高性能架构。

Conclusion: LLM可通过执行反馈内化经验性架构先验，成为鲁棒的自主神经架构设计者；该方法为将随机生成器转化为性能驱动的自主设计者提供了可扩展范式，并证实LLM能内化非文本的经验奖励，超越原始训练数据。

Abstract: Large language models (LLMs) excel in program synthesis, yet their ability to autonomously navigate neural architecture design--balancing syntactic reliability, performance, and structural novelty--remains underexplored. We address this by placing a code-oriented LLM within a closed-loop synthesis framework, analyzing its evolution over 22 supervised fine-tuning cycles. The model synthesizes PyTorch convolutional networks which are validated, evaluated via low-fidelity performance signals (single-epoch accuracy), and filtered using a MinHash-Jaccard criterion to prevent structural redundancy. High-performing, novel architectures are converted into prompt-code pairs for iterative fine-tuning via parameter-efficient LoRA adaptation, initialized from the LEMUR dataset. Across cycles, the LLM internalizes empirical architectural priors, becoming a robust generator. The valid generation rate stabilizes at 50.6 percent (peaking at 74.5 percent), while mean first-epoch accuracy rises from 28.06 percent to 50.99 percent, and the fraction of candidates exceeding 40 percent accuracy grows from 2.04 percent to 96.81 percent. Analyses confirm the model moves beyond replicating existing motifs, synthesizing 455 high-performing architectures absent from the original corpus. By grounding code synthesis in execution feedback, this work provides a scalable blueprint for transforming stochastic generators into autonomous, performance-driven neural designers, establishing that LLMs can internalize empirical, non-textual rewards to transcend their training data.

</details>


### [262] [Multi-Distribution Robust Conformal Prediction](https://arxiv.org/abs/2601.02998)
*Yuqi Yang,Ying Jin*

Main category: cs.LG

TL;DR: 本文提出了一种max-p聚合方案，用于在多个异质分布上构建具有统一覆盖率保证的共形预测集，并证明了其最优性和紧性；同时设计了学习高效一致性分数的通用算法，在合成与真实数据实验中显著减小预测集大小，同时保持最坏情况下的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 在公平性和分布鲁棒性问题中，测试数据可能来自多个源分布中的任意一个或其混合，需确保预测集在所有可能分布下均满足预设覆盖率，现有方法难以兼顾多分布覆盖保证与预测集效率。

Method: 提出max-p聚合方案，结合各分布对应的 conformity scores 实现有限样本下的多分布覆盖率保证；通过优化效率目标（如最小化预测集大小）并约束均匀覆盖率，证明该聚合方案的最优性与紧性；进一步提出通用算法学习适配该聚合框架的 conformity scores。

Result: 在合成和真实数据实验中，所提方法在保证多分布最坏情况覆盖率的同时，显著缩小预测集尺寸，优于直接对单源 conformity scores 应用 max-p 聚合的方法，且预测集大小可媲美使用主流标准 conformity scores 的单源共形预测。

Conclusion: max-p 聚合是实现多分布统一覆盖率保证的最优策略；所提框架为分布鲁棒预测、子群体偏移、公平性及多源学习提供了统一且实用的共形预测基础。

Abstract: In many fairness and distribution robustness problems, one has access to labeled data from multiple source distributions yet the test data may come from an arbitrary member or a mixture of them. We study the problem of constructing a conformal prediction set that is uniformly valid across multiple, heterogeneous distributions, in the sense that no matter which distribution the test point is from, the coverage of the prediction set is guaranteed to exceed a pre-specified level. We first propose a max-p aggregation scheme that delivers finite-sample, multi-distribution coverage given any conformity scores associated with each distribution. Upon studying several efficiency optimization programs subject to uniform coverage, we prove the optimality and tightness of our aggregation scheme, and propose a general algorithm to learn conformity scores that lead to efficient prediction sets after the aggregation under standard conditions. We discuss how our framework relates to group-wise distributionally robust optimization, sub-population shift, fairness, and multi-source learning. In synthetic and real-data experiments, our method delivers valid worst-case coverage across multiple distributions while greatly reducing the set size compared with naively applying max-p aggregation to single-source conformity scores, and can be comparable in size to single-source prediction sets with popular, standard conformity scores.

</details>


### [263] [In-Context Reinforcement Learning through Bayesian Fusion of Context and Value Prior](https://arxiv.org/abs/2601.03015)
*Anaïs Berkes,Vincent Taboga,Donna Vakalis,David Rolnick,Yoshua Bengio*

Main category: cs.LG

TL;DR: SPICE是一种贝叶斯式上下文内强化学习方法，通过深度集成学习Q值先验，并在测试时利用贝叶斯更新结合上置信界探索策略实现快速、鲁棒的零样本适应，即使在仅用次优数据预训练下仍具理论最优悔值保证。


<details>
  <summary>Details</summary>
Motivation: 现有上下文内强化学习（ICRL）方法难以超越训练分布泛化能力，或严重依赖近最优数据，实用性受限。

Method: 提出SPICE：基于深度集成构建Q值先验；测试时通过贝叶斯更新融合上下文信息；采用上置信界（UCB）驱动的在线推理以缓解次优先验影响。

Result: 理论证明在随机多臂老虎机和有限步长MDP中达到悔值最优；实验表明在各类基准任务中显著降低悔值、快速适应未见任务、对分布偏移鲁棒。

Conclusion: SPICE为ICRL提供了兼具理论保证与实用鲁棒性的新范式，尤其适用于次优数据场景下的零样本策略适应。

Abstract: In-context reinforcement learning (ICRL) promises fast adaptation to unseen environments without parameter updates, but current methods either cannot improve beyond the training distribution or require near-optimal data, limiting practical adoption. We introduce SPICE, a Bayesian ICRL method that learns a prior over Q-values via deep ensemble and updates this prior at test-time using in-context information through Bayesian updates. To recover from poor priors resulting from training on sub-optimal data, our online inference follows an Upper-Confidence Bound rule that favours exploration and adaptation. We prove that SPICE achieves regret-optimal behaviour in both stochastic bandits and finite-horizon MDPs, even when pretrained only on suboptimal trajectories. We validate these findings empirically across bandit and control benchmarks. SPICE achieves near-optimal decisions on unseen tasks, substantially reduces regret compared to prior ICRL and meta-RL approaches while rapidly adapting to unseen tasks and remaining robust under distribution shift.

</details>


### [264] [Causal Manifold Fairness: Enforcing Geometric Invariance in Representation Learning](https://arxiv.org/abs/2601.03032)
*Vidhi Rathore*

Main category: cs.LG

TL;DR: 本文提出因果流形公平性（CMF）框架，将因果推断与几何深度学习结合，在潜在表示中保持敏感属性干预下的黎曼几何不变性，从而实现更本质的公平性建模。


<details>
  <summary>Details</summary>
Motivation: 标准机器学习公平性方法常忽略数据的生成结构，而敏感属性实际上会因果地扭曲数据流形的几何结构，需从几何与因果联合视角建模。

Method: 提出CMF框架，通过约束解码器的雅可比矩阵和海森矩阵，在潜在空间中保持局部黎曼度量张量与曲率在敏感属性反事实干预下的不变性。

Result: 在合成结构因果模型（SCM）上验证了CMF能有效解耦敏感属性引起的几何扭曲，同时保持任务效用，并通过几何指标量化公平性-效用权衡。

Conclusion: CMF为公平性提供了新的几何-因果统一视角，表明保障流形几何不变性是实现深层公平性的关键路径。

Abstract: Fairness in machine learning is increasingly critical, yet standard approaches often treat data as static points in a high-dimensional space, ignoring the underlying generative structure. We posit that sensitive attributes (e.g., race, gender) do not merely shift data distributions but causally warp the geometry of the data manifold itself. To address this, we introduce Causal Manifold Fairness (CMF), a novel framework that bridges causal inference and geometric deep learning. CMF learns a latent representation where the local Riemannian geometry, defined by the metric tensor and curvature, remains invariant under counterfactual interventions on sensitive attributes. By enforcing constraints on the Jacobian and Hessian of the decoder, CMF ensures that the rules of the latent space (distances and shapes) are preserved across demographic groups. We validate CMF on synthetic Structural Causal Models (SCMs), demonstrating that it effectively disentangles sensitive geometric warping while preserving task utility, offering a rigorous quantification of the fairness-utility trade-off via geometric metrics.

</details>


### [265] [When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability](https://arxiv.org/abs/2601.03047)
*Raphael Ronge,Markus Maier,Frederick Eberhardt*

Main category: cs.LG

TL;DR: 本文对Anthropic提出的基于稀疏自编码器（SAE）的机制可解释性方法进行了压力测试，发现其特征操控能力存在显著脆弱性，难以满足AI安全所需的系统性可靠性，建议转向更注重输出预测与控制的研究方向。


<details>
  <summary>Details</summary>
Motivation: 验证Anthropic提出的基于稀疏自编码器（SAE）的机制可解释性方法在AI安全中是否具备足够可靠性和泛化能力。

Method: 复现Anthropic的关键结果，使用开源SAE在Llama 3.1上进行特征提取与操控实验，并系统评估其对层选择、操控强度和上下文的敏感性，以及特征区分能力。

Result: 成功复现基础特征提取与操控，但发现操控效果高度脆弱；观察到非标准激活行为；难以区分主题相似的特征；当前SAE方法缺乏安全关键应用所需的系统性可靠性。

Conclusion: SAE驱动的机制可解释性虽具启发性，但尚不能支撑AI安全所需的人类监督；需将重点从内部表征解释转向模型输出的可靠预测与控制。

Abstract: Recent work by Anthropic on Mechanistic interpretability claims to understand and control Large Language Models by extracting human-interpretable features from their neural activation patterns using sparse autoencoders (SAEs). If successful, this approach offers one of the most promising routes for human oversight in AI safety. We conduct an initial stress-test of these claims by replicating their main results with open-source SAEs for Llama 3.1. While we successfully reproduce basic feature extraction and steering capabilities, our investigation suggests that major caution is warranted regarding the generalizability of these claims. We find that feature steering exhibits substantial fragility, with sensitivity to layer selection, steering magnitude, and context. We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another. While SAE-based interpretability produces compelling demonstrations in selected cases, current methods often fall short of the systematic reliability required for safety-critical applications. This suggests a necessary shift in focus from prioritizing interpretability of internal representations toward reliable prediction and control of model output. Our work contributes to a more nuanced understanding of what mechanistic interpretability has achieved and highlights fundamental challenges for AI safety that remain unresolved.

</details>


### [266] [Joint Encoding of KV-Cache Blocks for Scalable LLM Serving](https://arxiv.org/abs/2601.03067)
*Joseph Kampeas,Emir Haleva*

Main category: cs.LG

TL;DR: 本文提出了一种联合编码KV缓存块的方法，通过融合不同请求和输入块中的相似KV块来减少内存占用，在保持标准缓存结构的同时显著提升大语言模型的推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型受制于键值（KV）缓存内存占用过大，尤其在高并发场景下限制了实时吞吐量；现有压缩方法存在依赖固定启发式、破坏张量布局或需专用硬件等问题。

Method: 提出KV缓存块的联合编码方法，融合跨请求与输入块的相似KV块为共享表示，保持标准缓存结构；并基于泊松过程建模分析其率失真权衡。

Result: 在多个LLM和基准测试中实现最高4.38× KV缓存压缩，精度损失可忽略；单机vLLM服务中token吞吐量提升约40%。

Conclusion: 该方法有效缓解KV缓存内存瓶颈，无需专用硬件即可支持高并发LLM服务，兼具理论严谨性与实际部署优势。

Abstract: Modern large language models (LLMs) drive interactive AI systems but are bottlenecked by the memory-heavy growth of key-value (KV) caches, which limits real-time throughput under concurrent loads. Existing KV-cache compression methods rely on rigid heuristics, disrupt tensor layouts, or require specialized compute, hindering scalability and deployment.
  We propose joint encoding of KV-cache blocks, which fuses similar blocks across requests and input chunks into shared representations while preserving standard cache structure. This alleviates the KV-cache memory bottleneck, supporting high-concurrency serving without specialized hardware. Theoretically, we analyze the rate-distortion tradeoff of fused cache blocks under a Poisson process model. Empirically, our method achieves up to 4.38 $\times$ KV-cache compression with negligible accuracy loss across diverse LLMs and benchmarks, outperforming recent structured and adaptive compression baselines. In real LLM serving, joint encoding improves the token throughput by $\sim$40\% on a single-machine vLLM benchmark, demonstrating substantial gains in inference throughput. Code is available at https://github.com/sef1/kv_fast_fusion  kv_joint_encoding.

</details>


### [267] [Real-Time Adaptive Anomaly Detection in Industrial IoT Environments](https://arxiv.org/abs/2601.03085)
*Mahsa Raeiszadeh,Amin Ebrahimzadeh,Roch H. Glitho,Johan Eker,Raquel A. F. Mini*

Main category: cs.LG

TL;DR: 本文提出了一种面向工业物联网（IIoT）流数据的自适应异常检测方法，融合多源预测模型与概念漂移适配机制，在保证效率与可扩展性的同时，AUC准确率达89.71%，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法难以有效应对IIoT中多维、动态、异构的数据流，而实时异常检测对预防故障至关重要。

Method: 提出一种结合多源预测模型与概念漂移自适应机制的新型异常检测算法，通过将预测模型嵌入漂移适配框架以提升准确性与可扩展性。

Result: 在真实轨迹驱动评估中，该方法AUC达89.71%，在准确性、效率和可扩展性方面均优于当前最优方法。

Conclusion: 所提方法能有效应对IIoT流数据的复杂性与动态性，为下一代网络提供高可靠、低延迟的自动化异常检测能力。

Abstract: To ensure reliability and service availability, next-generation networks are expected to rely on automated anomaly detection systems powered by advanced machine learning methods with the capability of handling multi-dimensional data. Such multi-dimensional, heterogeneous data occurs mostly in today's industrial Internet of Things (IIoT), where real-time detection of anomalies is critical to prevent impending failures and resolve them in a timely manner. However, existing anomaly detection methods often fall short of effectively coping with the complexity and dynamism of multi-dimensional data streams in IIoT. In this paper, we propose an adaptive method for detecting anomalies in IIoT streaming data utilizing a multi-source prediction model and concept drift adaptation. The proposed anomaly detection algorithm merges a prediction model into a novel drift adaptation method resulting in accurate and efficient anomaly detection that exhibits improved scalability. Our trace-driven evaluations indicate that the proposed method outperforms the state-of-the-art anomaly detection methods by achieving up to an 89.71% accuracy (in terms of Area under the Curve (AUC)) while meeting the given efficiency and scalability requirements.

</details>


### [268] [Audit Me If You Can: Query-Efficient Active Fairness Auditing of Black-Box LLMs](https://arxiv.org/abs/2601.03087)
*David Hartmann,Lena Pohlmann,Lelia Hanslik,Noah Gießing,Bettina Berendt,Pieter Delobelle*

Main category: cs.LG

TL;DR: 本文提出BAFA方法，通过有界主动公平审计框架，以更少的查询次数高效评估黑盒大语言模型（LLM）在不同人口统计群体上的系统性偏差，显著提升公平性审计的资源效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在不同人口统计群体中存在系统性偏差，而现有黑盒模型公平性审计方法依赖大量查询，资源开销大，亟需更高效的审计机制。

Method: 将审计建模为对目标公平性指标（如ΔAUC）的不确定性估计；提出BAFA框架，利用查询得分维护一致的代理模型版本空间，并通过约束经验风险最小化计算公平性指标的不确定性区间；采用主动查询选择策略逐步缩小区间以降低估计误差。

Result: 在CivilComments和Bias-in-Bios两个标准公平性数据集上，BAFA在ε=0.02误差阈值下仅需144次查询，比分层采样（5956次）减少达40倍；收敛更快、性能更优、运行方差更低。

Conclusion: BAFA能显著降低LLM公平性独立审计所需的查询资源，支持持续、高效的模型公平性评估，为实际部署中的问责机制提供可行技术路径。

Abstract: Large Language Models (LLMs) exhibit systematic biases across demographic groups. Auditing is proposed as an accountability tool for black-box LLM applications, but suffers from resource-intensive query access. We conceptualise auditing as uncertainty estimation over a target fairness metric and introduce BAFA, the Bounded Active Fairness Auditor for query-efficient auditing of black-box LLMs. BAFA maintains a version space of surrogate models consistent with queried scores and computes uncertainty intervals for fairness metrics (e.g., $Δ$ AUC) via constrained empirical risk minimisation. Active query selection narrows these intervals to reduce estimation error. We evaluate BAFA on two standard fairness dataset case studies: \textsc{CivilComments} and \textsc{Bias-in-Bios}, comparing against stratified sampling, power sampling, and ablations. BAFA achieves target error thresholds with up to 40$\times$ fewer queries than stratified sampling (e.g., 144 vs 5,956 queries at $\varepsilon=0.02$ for \textsc{CivilComments}) for tight thresholds, demonstrates substantially better performance over time, and shows lower variance across runs. These results suggest that active sampling can reduce resources needed for independent fairness auditing with LLMs, supporting continuous model evaluations.

</details>


### [269] [ATLAS: Adaptive Test-Time Latent Steering with External Verifiers for Enhancing LLMs Reasoning](https://arxiv.org/abs/2601.03093)
*Tuc Nguyen,Thai Le*

Main category: cs.LG

TL;DR: 本文提出ATLAS，一种在推理时自适应控制潜变量引导的框架，通过轻量级外部潜变量验证器动态决定是否及如何施加引导，从而提升大语言模型的数学推理准确率并减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有激活和潜变量引导方法依赖固定策略和静态干预强度，鲁棒性差，易出现过引导或欠引导问题。

Method: 提出ATLAS框架，引入一个轻量级、外部的潜变量验证器，在推理过程中根据中间隐状态预测当前推理质量，并动态决定是否及以多强力度进行潜变量引导。

Result: 在多个数学推理基准上，ATLAS显著优于基线方法，既提高了准确率，又大幅减少了测试时token使用量。

Conclusion: 基于学习的潜变量验证可有效指导推理时潜变量自适应调整，是一种高效、可扩展的推理效率控制机制，且不损害解题质量。

Abstract: Recent work on activation and latent steering has demonstrated that modifying internal representations can effectively guide large language models (LLMs) toward improved reasoning and efficiency without additional training. However, most existing approaches rely on fixed steering policies and static intervention strengths, which limit their robustness across problem instances and often result in over- or under-steering. We propose Adaptive Test-time Latent Steering, called (ATLAS), a task-specific framework that dynamically controls steering decisions at inference time using an external, lightweight latent verifier. Given intermediate hidden states, the verifier predicts the quality of ongoing reasoning and adaptively selects whether and how strongly to apply steering, enabling per-example and per-step adjustment with minimal overhead. To our knowledge, ATLAS is the first method to integrate learned latent verification into test-time steering for enhancing LLMs reasoning. Experiments on multiple mathematical reasoning benchmarks show that ATLAS consistently outperforms both vanilla decoding and fixed steering baselines, achieving higher accuracy while substantially reducing test-time token usage. These results demonstrate that verifier-guided latent adaptation provides an effective and scalable mechanism for controlling reasoning efficiency without sacrificing solution quality. All source code will be publicly available.

</details>


### [270] [From Muscle to Text with MyoText: sEMG to Text via Finger Classification and Transformer-Based Decoding](https://arxiv.org/abs/2601.03098)
*Meghna Roy Chowdhury,Shreyas Sen,Yi Ding*

Main category: cs.LG

TL;DR: MyoText is a hierarchical sEMG-to-text framework that decodes finger activations via CNN-BiLSTM-Attention, applies ergonomic priors for letter inference, and uses a fine-tuned T5 transformer for sentence reconstruction—achieving 85.4% finger accuracy, 5.4% CER, and 6.5% WER on 30 users.


<details>
  <summary>Details</summary>
Motivation: To advance keyboard-free text input in wearable and mixed-reality systems by moving beyond direct letter recognition to a physiologically and ergonomically grounded hierarchical decoding process.

Method: A three-stage hierarchical framework: (1) finger activation classification using CNN-BiLSTM-Attention on multichannel sEMG; (2) letter inference incorporating ergonomic typing priors; (3) full sentence reconstruction with a fine-tuned T5 transformer.

Result: On the emg2qwerty dataset with 30 users: 85.4% finger-classification accuracy, 5.4% character error rate (CER), and 6.5% word error rate (WER); outperforms prior baselines.

Conclusion: MyoText establishes a principled, modular pathway from neuromuscular signals to natural language, enabling high-accuracy, keyboard-free typing for virtual/augmented reality and ubiquitous computing.

Abstract: Surface electromyography (sEMG) provides a direct neural interface for decoding muscle activity and offers a promising foundation for keyboard-free text input in wearable and mixed-reality systems. Previous sEMG-to-text studies mainly focused on recognizing letters directly from sEMG signals, forming an important first step toward translating muscle activity into text. Building on this foundation, we present MyoText, a hierarchical framework that decodes sEMG signals to text through physiologically grounded intermediate stages. MyoText first classifies finger activations from multichannel sEMG using a CNN-BiLSTM-Attention model, applies ergonomic typing priors to infer letters, and reconstructs full sentences with a fine-tuned T5 transformer. This modular design mirrors the natural hierarchy of typing, linking muscle intent to language output and reducing the search space for decoding. Evaluated on 30 users from the emg2qwerty dataset, MyoText outperforms baselines by achieving 85.4% finger-classification accuracy, 5.4% character error rate (CER), and 6.5% word error rate (WER). Beyond accuracy gains, this methodology establishes a principled pathway from neuromuscular signals to text, providing a blueprint for virtual and augmented-reality typing interfaces that operate entirely without physical keyboards. By integrating ergonomic structure with transformer-based linguistic reasoning, MyoText advances the feasibility of seamless, wearable neural input for future ubiquitous computing environments.

</details>


### [271] [Time-Aware Synthetic Control](https://arxiv.org/abs/2601.03099)
*Saeyoung Rho,Cyrus Illick,Samhitha Narasipura,Alberto Abadie,Daniel Hsu,Vishal Misra*

Main category: cs.LG

TL;DR: 本文提出了时间感知合成控制（TASC）方法，通过结合状态空间模型与卡尔曼滤波及RTS平滑器，在保持信号低秩结构的同时建模恒定趋势，以提升存在强时间趋势和高观测噪声场景下的反事实推断效果。


<details>
  <summary>Details</summary>
Motivation: 现有合成控制方法将干预前时间索引视为可交换，忽略了时间序列中的时序结构，尤其在存在强趋势时表现不佳。

Method: 提出TASC方法，采用带恒定趋势的状态空间模型，利用EM算法拟合生成式时间序列模型，并通过卡尔曼滤波与Rauch-Tung-Striebel平滑器进行反事实推理。

Result: 在模拟数据和真实数据（政策评估、体育预测）上验证了TASC在强时间趋势和高观测噪声场景下优于现有方法。

Conclusion: TASC能更有效地利用时间结构，提升合成控制在动态、噪声大环境下的因果推断性能。

Abstract: The synthetic control (SC) framework is widely used for observational causal inference with time-series panel data. SC has been successful in diverse applications, but existing methods typically treat the ordering of pre-intervention time indices interchangeable. This invariance means they may not fully take advantage of temporal structure when strong trends are present. We propose Time-Aware Synthetic Control (TASC), which employs a state-space model with a constant trend while preserving a low-rank structure of the signal. TASC uses the Kalman filter and Rauch-Tung-Striebel smoother: it first fits a generative time-series model with expectation-maximization and then performs counterfactual inference. We evaluate TASC on both simulated and real-world datasets, including policy evaluation and sports prediction. Our results suggest that TASC offers advantages in settings with strong temporal trends and high levels of observation noise.

</details>


### [272] [One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling](https://arxiv.org/abs/2601.03111)
*Yiyuan Li,Zhen Huang,Yanan Wu,Weixun Wang,Xuefeng Li,Yijia Luo,Wenbo Su,Bo Zheng,Pengfei Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为'polymath learning'的一次性样本训练框架，证明高质量、精心设计的单个训练样本（尤其是融合多学科元素的合成样本）能显著提升大语言模型在数学及跨学科推理任务上的性能，挑战了强化学习依赖大量数据的传统假设。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）的强化学习（RL）方法通常依赖成千上万的高质量样本，本文旨在挑战这一对数据规模的依赖假设，探索极低数据量（甚至单样本）下提升推理能力的可能性。

Method: 提出'polymath learning'框架，通过设计一个具有多学科影响力的单一训练样本（特别是基于数学推理能力并融合物理、化学、生物等元素的合成样本），在RL训练中进行一次学习；系统评估该样本在多个推理基准上的泛化效果，并对比天然样本与工程化合成样本的效果差异。

Result: （1）单个精选数学推理样本即可显著提升模型在物理、化学、生物等多个学科的推理性能；（2）数学推理中凸显的关键技能可指导最优‘polymath样本’的设计；（3）人工构造的多学科合成样本优于多个天然样本的组合；整体性能超越使用更大规模数据集的传统训练方法。

Conclusion: 样本质量与设计（即‘样本工程’）比样本数量更关键；提升LLM推理能力的新范式应转向对训练样本的精准工程化设计，而非单纯扩大数据规模。

Abstract: The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.

</details>


### [273] [PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback](https://arxiv.org/abs/2601.03149)
*Dehao Yuan,Tyler Farnan,Stefan Tesliuc,Doron L Bergman,Yulun Wu,Xiaoyu Liu,Minghui Liu,James Montgomery,Nam H Nguyen,C. Bayan Bruss,Furong Huang*

Main category: cs.LG

TL;DR: PersonaLedger 是一种结合大语言模型与可配置规则引擎的合成交易数据生成框架，旨在兼顾行为多样性与金融逻辑正确性，发布包含3000万笔交易的公开数据集及基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 严格隐私法规限制真实金融交易数据的获取，阻碍金融AI开放研究；现有合成数据方法难以同时满足行为多样性与逻辑严谨性要求。

Method: 提出 PersonaLedger：利用大语言模型（LLM）基于丰富用户画像生成交易序列，并与专家设计的程序化规则引擎闭环协同——引擎实时更新用户状态、强制执行金融约束，并生成上下文感知的‘next-prompt’引导LLM生成下一步合法动作。

Result: 生成包含23,000名用户的3000万笔交易的公开数据集，配套 illiquidity 分类与 identity theft 分割两项任务的基准测试套件；支持可复现的预测与异常检测模型评估。

Conclusion: PersonaLedger 提供了一种现实、合规、可解释的合成数据生成范式，为金融AI研究提供了开源、可审计、隐私安全的基础设施。

Abstract: Strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Learning-based generators such as GANs capture correlations yet often violate hard financial constraints and still require training on private data. We introduce PersonaLedger, a generation engine that uses a large language model conditioned on rich user personas to produce diverse transaction streams, coupled with an expert configurable programmatic engine that maintains correctness. The LLM and engine interact in a closed loop: after each event, the engine updates the user state, enforces financial rules, and returns a context aware "nextprompt" that guides the LLM toward feasible next actions. With this engine, we create a public dataset of 30 million transactions from 23,000 users and a benchmark suite with two tasks, illiquidity classification and identity theft segmentation. PersonaLedger offers a realistic, privacy preserving resource that supports rigorous evaluation of forecasting and anomaly detection models. PersonaLedger offers the community a rich, realistic, and privacy preserving resource -- complete with code, rules, and generation logs -- to accelerate innovation in financial AI and enable rigorous, reproducible evaluation.

</details>


### [274] [Prompt-Counterfactual Explanations for Generative AI System Behavior](https://arxiv.org/abs/2601.03156)
*Sofie Goethals,Foster Provost,João Sedoc*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型（LLM）生成式AI系统的提示级可解释性框架——提示反事实解释（PCE），通过适配传统反事实解释方法，解决其在非确定性生成系统中的适用性问题，并在毒性、情感和政治倾向三个案例中验证了其在提示工程与红队测试中的实用性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在现实场景中广泛应用，决策者亟需理解输入提示（prompt）如何影响输出特性（如毒性、偏见、情感等），但现有可解释AI方法难以直接适用于非确定性、生成式的LLM系统。

Method: 基于可解释AI中的反事实解释思想，本文提出一个灵活框架以适配生成式AI的非确定性特点，并设计生成提示反事实解释（PCE）的算法，依赖下游分类器识别输出关键特性。

Result: 在政治倾向、毒性、情感三类输出特性的案例研究中成功生成PCE；验证了PCE可优化提示工程以抑制不良输出，并增强红队测试能力以发现更多诱发不良输出的提示。

Conclusion: 该工作建立了面向提示的生成式AI可解释性基础，为高风险应用与监管合规（如透明性、问责制）提供了关键技术支撑。

Abstract: As generative AI systems become integrated into real-world applications, organizations increasingly need to be able to understand and interpret their behavior. In particular, decision-makers need to understand what causes generative AI systems to exhibit specific output characteristics. Within this general topic, this paper examines a key question: what is it about the input -- the prompt -- that causes an LLM-based generative AI system to produce output that exhibits specific characteristics, such as toxicity, negative sentiment, or political bias. To examine this question, we adapt a common technique from the Explainable AI literature: counterfactual explanations. We explain why traditional counterfactual explanations cannot be applied directly to generative AI systems, due to several differences in how generative AI systems function. We then propose a flexible framework that adapts counterfactual explanations to non-deterministic, generative AI systems in scenarios where downstream classifiers can reveal key characteristics of their outputs. Based on this framework, we introduce an algorithm for generating prompt-counterfactual explanations (PCEs). Finally, we demonstrate the production of counterfactual explanations for generative AI systems with three case studies, examining different output characteristics (viz., political leaning, toxicity, and sentiment). The case studies further show that PCEs can streamline prompt engineering to suppress undesirable output characteristics and can enhance red-teaming efforts to uncover additional prompts that elicit undesirable outputs. Ultimately, this work lays a foundation for prompt-focused interpretability in generative AI: a capability that will become indispensable as these models are entrusted with higher-stakes tasks and subject to emerging regulatory requirements for transparency and accountability.

</details>


### [275] [Rapid Augmentations for Time Series (RATS): A High-Performance Library for Time Series Augmentation](https://arxiv.org/abs/2601.03159)
*Wadie Skaf,Felix Kern,Aryamaan Basu Roy,Tejas Pradhan,Roman Kalkreuth,Holger Hoos*

Main category: cs.LG

TL;DR: 本文介绍了RATS（Rapid Augmentations for Time Series），一个用Rust编写、支持Python绑定的高性能时间序列增强库，旨在解决现有Python库在大规模数据上性能瓶颈的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列增强库（主要为Python实现）在大数据集上运行时间呈指数增长，难以满足大规模生产系统需求。

Method: 使用Rust开发RATS库，并提供Python绑定（RATSpy），支持多种增强方法（基础变换、频域操作、时间扭曲等），通过统一管道接口和内置并行化实现高效计算。

Result: 在143个数据集上的基准测试表明，RATSpy相比tsaug平均提速74.5%，最大提速达94.8%；峰值内存使用最多减少47.9%。

Conclusion: RATS是一个显著提升时间序列增强效率与资源利用率的高性能解决方案，适用于数据稀缺且需高效训练的场景。

Abstract: Time series augmentation is critical for training robust deep learning models, particularly in domains where labelled data is scarce and expensive to obtain. However, existing augmentation libraries for time series, mainly written in Python, suffer from performance bottlenecks, where running time grows exponentially as dataset sizes increase -- an aspect limiting their applicability in large-scale, production-grade systems. We introduce RATS (Rapid Augmentations for Time Series), a high-performance library for time series augmentation written in Rust with Python bindings (RATSpy). RATS implements multiple augmentation methods spanning basic transformations, frequency-domain operations and time warping techniques, all accessible through a unified pipeline interface with built-in parallelisation. Comprehensive benchmarking of RATSpy versus a commonly used library (tasug) on 143 datasets demonstrates that RATSpy achieves an average speedup of 74.5\% over tsaug (up to 94.8\% on large datasets), with up to 47.9\% less peak memory usage.

</details>


### [276] [On the Convergence Behavior of Preconditioned Gradient Descent Toward the Rich Learning Regime](https://arxiv.org/abs/2601.03162)
*Shuai Jiang,Alexey Voronin,Eric Cyr,Ben Southworth*

Main category: cs.LG

TL;DR: 本文研究了预条件梯度下降（PGD）对神经网络谱偏差和'顿悟'（grokking）现象的影响，理论与实证表明PGD可缓解谱偏差，并加速从NTK懒惰训练向特征丰富学习的过渡，从而减轻顿悟延迟。


<details>
  <summary>Details</summary>
Motivation: 谱偏差虽利于泛化但阻碍科学任务中细尺度结构建模；顿悟现象则导致训练延迟。二者均限制神经网络高效学习，亟需优化策略加以缓解。

Method: 结合理论分析与实验验证，研究预条件梯度下降（如高斯-牛顿法）对谱偏差和顿悟的影响，并基于'顿悟源于NTK向富特征学习转变'的假设，分析PGD如何促进该过渡。

Result: PGD能有效缓解谱偏差，实现NTK阶段参数空间的均匀探索；实验验证PGD显著缩短顿悟延迟，支持顿悟是懒惰（NTK）与富特征学习相变过程的观点。

Conclusion: PGD通过克服谱偏差，促进神经网络更快跨越NTK懒惰训练阶段，进入富特征学习，从而统一解释其对谱偏差抑制与顿悟加速的双重作用，深化了对优化动力学与学习阶段关系的理解。

Abstract: Spectral bias, the tendency of neural networks to learn low frequencies first, can be both a blessing and a curse. While it enhances the generalization capabilities by suppressing high-frequency noise, it can be a limitation in scientific tasks that require capturing fine-scale structures. The delayed generalization phenomenon known as grokking is another barrier to rapid training of neural networks. Grokking has been hypothesized to arise as learning transitions from the NTK to the feature-rich regime. This paper explores the impact of preconditioned gradient descent (PGD), such as Gauss-Newton, on spectral bias and grokking phenomena. We demonstrate through theoretical and empirical results how PGD can mitigate issues associated with spectral bias. Additionally, building on the rich learning regime grokking hypothesis, we study how PGD can be used to reduce delays associated with grokking. Our conjecture is that PGD, without the impediment of spectral bias, enables uniform exploration of the parameter space in the NTK regime. Our experimental results confirm this prediction, providing strong evidence that grokking represents a transitional behavior between the lazy regime characterized by the NTK and the rich regime. These findings deepen our understanding of the interplay between optimization dynamics, spectral bias, and the phases of neural network learning.

</details>


### [277] [Dynamic Hyperparameter Importance for Efficient Multi-Objective Optimization](https://arxiv.org/abs/2601.03166)
*Daphne Theodorakopoulos,Marcel Wever,Marius Lindauer*

Main category: cs.LG

TL;DR: 本文提出了一种基于动态超参数重要性（HPI）的多目标优化方法，通过集成HyperSHAP评估HPI并结合ParEGO算法自适应调整搜索空间，在多个基准任务上提升了收敛速度与Pareto前沿质量。


<details>
  <summary>Details</summary>
Motivation: 现有多目标优化（MOO）方法忽略超参数重要性（HPI）随目标权衡而变化的事实，导致搜索效率低、解质量差。

Method: 将HyperSHAP用于量化HPI，结合ParEGO生成的目标权重动态识别重要超参数，并固定不重要超参数以精简配置空间，实现动态优化。

Result: 在PyMOO和YAHPO-Gym多个任务上验证，相较基线方法显著提升收敛速度与Pareto前沿质量。

Conclusion: 动态利用HPI可有效提升MOO效率与效果，为ML模型选择提供更优的多目标自动调参框架。

Abstract: Choosing a suitable ML model is a complex task that can depend on several objectives, e.g., accuracy, model size, fairness, inference time, or energy consumption. In practice, this requires trading off multiple, often competing, objectives through multi-objective optimization (MOO). However, existing MOO methods typically treat all hyperparameters as equally important, overlooking that hyperparameter importance (HPI) can vary significantly depending on the trade-off between objectives. We propose a novel dynamic optimization approach that prioritizes the most influential hyperparameters based on varying objective trade-offs during the search process, which accelerates empirical convergence and leads to better solutions. Building on prior work on HPI for MOO post-analysis, we now integrate HPI, calculated with HyperSHAP, into the optimization. For this, we leverage the objective weightings naturally produced by the MOO algorithm ParEGO and adapt the configuration space by fixing the unimportant hyperparameters, allowing the search to focus on the important ones. Eventually, we validate our method with diverse tasks from PyMOO and YAHPO-Gym. Empirical results demonstrate improvements in convergence speed and Pareto front quality compared to baselines.

</details>


### [278] [Predicting Time Pressure of Powered Two-Wheeler Riders for Proactive Safety Interventions](https://arxiv.org/abs/2601.03173)
*Sumit S. Shevtekar,Chandresh K. Maurya,Gourab Sil,Subasish Das*

Main category: cs.LG

TL;DR: 本文提出了一种基于多变量时间序列的大规模数据集与深度学习模型MotoTimePressure，用于预测摩托车骑手在不同时间压力下的行为状态，并验证其在碰撞风险预测和主动交通干预中的实用价值。


<details>
  <summary>Details</summary>
Motivation: 时间压力显著影响两轮车骑手的危险操作与事故倾向，但其在智能交通系统中尚未被有效建模与预测。

Method: 构建包含129,000+多变量时序样本的大规模数据集（含车辆运动学、操控输入、行为违规及环境特征共63维），并提出MotoTimePressure模型：融合卷积预处理、双阶段时间注意力机制与Squeeze-and-Excitation特征重校准。

Result: MotoTimePressure在时间压力分类任务中达91.53%准确率与98.93% ROC AUC；将其预测结果作为特征输入Informer模型后，碰撞风险预测准确率由91.25%提升至93.51%，接近Oracle性能（93.72%）；阈值化的时间压力状态可支持自适应预警、触觉反馈、V2I通信与限速引导等主动干预。

Conclusion: 时间压力可作为表征骑手认知负荷的关键隐变量，本工作为两轮车安全提供可部署的感知—预测—干预闭环，契合‘安全系统方法’理念。

Abstract: Time pressure critically influences risky maneuvers and crash proneness among powered two-wheeler riders, yet its prediction remains underexplored in intelligent transportation systems. We present a large-scale dataset of 129,000+ labeled multivariate time-series sequences from 153 rides by 51 participants under No, Low, and High Time Pressure conditions. Each sequence captures 63 features spanning vehicle kinematics, control inputs, behavioral violations, and environmental context. Our empirical analysis shows High Time Pressure induces 48% higher speeds, 36.4% greater speed variability, 58% more risky turns at intersections, 36% more sudden braking, and 50% higher rear brake forces versus No Time Pressure. To benchmark this dataset, we propose MotoTimePressure, a deep learning model combining convolutional preprocessing, dual-stage temporal attention, and Squeeze-and-Excitation feature recalibration, achieving 91.53% accuracy and 98.93% ROC AUC, outperforming eight baselines. Since time pressure cannot be directly measured in real time, we demonstrate its utility in collision prediction and threshold determination. Using MTPS-predicted time pressure as features, improves Informer-based collision risk accuracy from 91.25% to 93.51%, approaching oracle performance (93.72%). Thresholded time pressure states capture rider cognitive stress and enable proactive ITS interventions, including adaptive alerts, haptic feedback, V2I signaling, and speed guidance, supporting safer two-wheeler mobility under the Safe System Approach.

</details>


### [279] [Decentralized Autoregressive Generation](https://arxiv.org/abs/2601.03184)
*Stepan Maschan,Haoxuan Qu,Jun Liu*

Main category: cs.LG

TL;DR: 本文提出了一种去中心化自回归生成的理论分析框架，定义了去中心化离散流匹配目标，并通过实验验证了其与集中式训练在多模态语言模型上的等价性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态语言模型中自回归生成过程的去中心化问题，提升训练效率与可扩展性。

Method: 定义去中心化离散流匹配目标，将概率生成速度表示为多个专家流的线性组合，并在LLaVA和InternVL 2.5-1B等模型上进行去中心化与集中式训练对比实验。

Result: 实验证明去中心化与集中式训练在多个基准测试中性能等价，尤其在ViT+MLP+LLM全参数微调范式下表现一致。

Conclusion: 去中心化自回归生成在理论和实践上均可行，为多模态大模型的分布式训练提供了新思路。

Abstract: We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and performs full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.

</details>


### [280] [Sparse Knowledge Distillation: A Mathematical Framework for Probability-Domain Temperature Scaling and Multi-Stage Compression](https://arxiv.org/abs/2601.03195)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 本文提出了一种基于概率域软化算子的稀疏知识蒸馏统一理论框架，包含偏差-方差分解、同伦路径形式化、收敛性保证和等价类刻画，并给出软化算子的公理化定义，理论结果对多种算子族具有一致性，适用于黑盒蒸馏、部分访问及隐私保护压缩场景。


<details>
  <summary>Details</summary>
Motivation: 为稀疏知识蒸馏提供统一、严谨的理论基础，解释为何稀疏学生模型能超越密集教师模型、为何迭代剪枝优于一次性剪枝，并支持黑盒、部分访问和隐私保护等实际设定。

Method: 构建基于概率域软化算子的算子级分析框架，引入五条公理定义软化算子，推导偏差-方差分解、函数空间中的同伦路径、n阶段蒸馏的O(1/n)收敛率，以及在容量约束下生成相同学生模型的算子等价类。

Result: 获得四类核心理论结果：（i）算子无关的偏差-方差分解；（ii）多阶段剪枝的同伦路径解释；（iii）显式参数依赖的O(1/n)收敛保证；（iv）满足相同学生输出的软化算子等价类刻画；所有结论在所定义的公理化算子类上一致成立。

Conclusion: 该框架为稀疏知识蒸馏提供了首个系统性、算子无关的理论支撑，弥合理论与实践鸿沟，拓展至黑盒蒸馏、top-k截断、文本输出及隐私保护压缩等现实约束场景。

Abstract: We develop a unified theoretical framework for sparse knowledge distillation based on probability-domain softening operators. While the equivalence $p^{1/T} \propto \mathrm{softmax}(z/T)$ is well known, our contribution is an operator-level analytical framework built on this foundation rather than the equivalence itself.
  The framework comprises four core components: (i) operator-agnostic bias--variance decompositions that characterize when sparse students outperform dense teachers, (ii) a homotopy path formalization of multi-stage pruning in function space explaining why iterative compression succeeds where one-shot pruning fails, (iii) convergence guarantees establishing $O(1/n)$ rates for $n$-stage distillation with explicit parameter dependence, and (iv) equivalence class characterizations identifying distinct probability-domain operators that yield identical student models under capacity constraints.
  We introduce an axiomatic definition of probability-domain softening operators based on ranking preservation, continuity, entropy monotonicity, identity, and boundary behavior, and show that multiple non-equivalent operator families satisfy these axioms. All learning-theoretic guarantees are shown to hold uniformly across this operator class, independent of implementation details. These results provide theoretical grounding for black-box teacher distillation, partial-access settings such as top-$k$ truncation and text-only outputs, and privacy-preserving model compression.

</details>


### [281] [Empowering Reliable Visual-Centric Instruction Following in MLLMs](https://arxiv.org/abs/2601.03198)
*Weilei He,Feng Ju,Zhiyuan Fan,Rui Min,Minhao Cheng,Yi R. Fung*

Main category: cs.LG

TL;DR: 本文提出了VC-IFEval基准，用于评估多模态大语言模型（MLLMs）在视觉-文本联合指令下的遵循能力，弥补了现有纯文本指令评估的不足，并通过数据集微调显著提升了模型的视觉指令遵循准确率。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM指令遵循评估基准主要局限于文本模态，忽略了视觉模态中蕴含的丰富语义约束，导致评估不全面。

Method: 构建了包含视觉依赖约束的系统化指令数据集VC-IFEval，并基于该数据集对MLLMs进行微调与多模型评测。

Result: 在VC-IFEval上微调后，MLLMs的视觉指令遵循准确率和一致性显著提升；跨模型评测揭示了当前MLLMs在多模态指令遵循上的优势与局限。

Conclusion: VC-IFEval为多模态指令遵循能力提供了更严谨、细粒度的评估框架，推动MLLMs向更可靠、可控的多模态交互方向发展。

Abstract: Evaluating the instruction-following (IF) capabilities of Multimodal Large Language Models (MLLMs) is essential for rigorously assessing how faithfully model outputs adhere to user-specified intentions. Nevertheless, existing benchmarks for evaluating MLLMs' instruction-following capability primarily focus on verbal instructions in the textual modality. These limitations hinder a thorough analysis of instruction-following capabilities, as they overlook the implicit constraints embedded in the semantically rich visual modality. To address this gap, we introduce VC-IFEval, a new benchmark accompanied by a systematically constructed dataset that evaluates MLLMs' instruction-following ability under multimodal settings. Our benchmark systematically incorporates vision-dependent constraints into instruction design, enabling a more rigorous and fine-grained assessment of how well MLLMs align their outputs with both visual input and textual instructions. Furthermore, by fine-tuning MLLMs on our dataset, we achieve substantial gains in visual instruction-following accuracy and adherence. Through extensive evaluation across representative MLLMs, we provide new insights into the strengths and limitations of current models.

</details>


### [282] [Counterfactual Fairness with Graph Uncertainty](https://arxiv.org/abs/2601.03203)
*Davi Valério,Chrysoula Zerva,Mariana Pinto,Ricardo Santos,André Carreiro*

Main category: cs.LG

TL;DR: 本文提出CF-GU方法，将因果图的不确定性纳入反事实公平性（CF）审计中，通过因果发现算法生成多个可能的有向无环图（DAG），用归一化香农熵量化图不确定性，并为CF指标提供置信区间。


<details>
  <summary>Details</summary>
Motivation: 反事实公平性（CF）审计依赖于单一因果图，但在现实场景中该图往往无法确切获知，因此需将因果图的不确定性纳入评估框架。

Method: CF-GU方法包含三步：（i）在领域知识约束下对因果发现算法进行自助采样，生成一组合理的DAG；（ii）用归一化香农熵量化图的不确定性；（iii）为CF指标提供置信界。

Result: 在合成数据上验证了不同领域假设对CF审计结论的影响；在COMPAS和Adult真实数据集上成功识别出已知偏见，并在仅提供少量领域知识约束时仍保持高置信度。

Conclusion: CF-GU提升了CF审计在现实不确定环境下的鲁棒性与可信度，为模型偏见评估提供了更可靠的因果框架。

Abstract: Evaluating machine learning (ML) model bias is key to building trustworthy and robust ML systems. Counterfactual Fairness (CF) audits allow the measurement of bias of ML models with a causal framework, yet their conclusions rely on a single causal graph that is rarely known with certainty in real-world scenarios. We propose CF with Graph Uncertainty (CF-GU), a bias evaluation procedure that incorporates the uncertainty of specifying a causal graph into CF. CF-GU (i) bootstraps a Causal Discovery algorithm under domain knowledge constraints to produce a bag of plausible Directed Acyclic Graphs (DAGs), (ii) quantifies graph uncertainty with the normalized Shannon entropy, and (iii) provides confidence bounds on CF metrics. Experiments on synthetic data show how contrasting domain knowledge assumptions support or refute audits of CF, while experiments on real-world data (COMPAS and Adult datasets) pinpoint well-known biases with high confidence, even when supplied with minimal domain knowledge constraints.

</details>


### [283] [Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion](https://arxiv.org/abs/2601.03213)
*Mykola Vysotskyi,Zahar Kohut,Mariia Shpir,Taras Rumezhak,Volodymyr Karpiv*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的扩散模型概念遗忘新框架，通过引入时间步感知的 critic 和噪声步奖励机制，提升信用分配与训练稳定性，在保持图像质量和提示忠实度的同时实现更优的概念移除效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的机器遗忘方法依赖监督权重编辑或全局惩罚；而强化学习方法虽灵活，但受限于稀疏的终局奖励，导致高方差更新和弱信用分配。

Method: 将去噪过程建模为序列决策问题，构建基于CLIP的噪声潜在空间奖励预测器，生成每步奖励信号，并结合 timestep-aware critic 计算优势函数，用于策略梯度更新反向扩散核。支持离策略复用，可即插即用地集成到标准文生图主干中。

Result: 在多个目标概念上，本方法遗忘效果优于或媲美强基线，同时保持图像质量与良性提示生成能力；消融实验证实了每步 critic 和噪声条件化奖励对稳定性和有效性至关重要。

Conclusion: 该RL框架为扩散模型的高效、稳定概念遗忘提供了通用且实用的新范式，代码与评估脚本已开源以推动后续研究。

Abstract: Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a general RL framework for diffusion unlearning that treats denoising as a sequential decision process and introduces a timestep-aware critic with noisy-step rewards. Concretely, we train a CLIP-based reward predictor on noisy latents and use its per-step signal to compute advantage estimates for policy-gradient updates of the reverse diffusion kernel. Our algorithm is simple to implement, supports off-policy reuse, and plugs into standard text-to-image backbones. Across multiple concepts, the method achieves better or comparable forgetting to strong baselines while maintaining image quality and benign prompt fidelity; ablations show that (i) per-step critics and (ii) noisy-conditioned rewards are key to stability and effectiveness. We release code and evaluation scripts to facilitate reproducibility and future research on RL-based diffusion unlearning.

</details>


### [284] [From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence](https://arxiv.org/abs/2601.03220)
*Marc Finzi,Shikai Qiu,Yiding Jiang,Pavel Izmailov,J. Zico Kolter,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 本文提出'epiplexity'概念，用以量化计算受限观察者能从数据中学习到的信息量，解决传统信息论无法刻画数据实用价值的问题。


<details>
  <summary>Details</summary>
Motivation: 传统香农信息论和柯尔莫哥洛夫复杂度因假设观察者具有无限计算能力，难以刻画数据对实际学习系统的有用信息含量，无法解释现代机器学习实践中数据变换、顺序依赖和似然建模为何有效。

Method: 提出epiplexity作为新信息度量，区分结构性内容与时间受限熵（如伪随机数、混沌系统产生的不可预测随机性）；设计可估计epiplexity的实用方法。

Result: 证明信息可通过计算被‘创造’；信息依赖数据顺序；似然建模可生成比数据生成过程更复杂的程序；epiplexity估计值能区分不同数据源、与下游性能相关、揭示提升分布外泛化的数据干预方式。

Conclusion: epiplexity为数据选择、生成与变换提供了理论基础，弥补了模型选择原则在数据层面指导的缺失，推动以数据为中心的AI发展。

Abstract: Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? Can the learnable content in data be evaluated without considering a downstream task? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. In this work, we identify and exemplify three seeming paradoxes in information theory: (1) information cannot be increased by deterministic transformations; (2) information is independent of the order of data; (3) likelihood modeling is merely distribution matching. To shed light on the tension between these results and modern practice, and to quantify the value of data, we introduce epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. With these concepts, we demonstrate how information can be created with computation, how it depends on the ordering of the data, and how likelihood modeling can produce more complex programs than present in the data generating process itself. We also present practical procedures to estimate epiplexity which we show capture differences across data sources, track with downstream performance, and highlight dataset interventions that improve out-of-distribution generalization. In contrast to principles of model selection, epiplexity provides a theoretical foundation for data selection, guiding how to select, generate, or transform data for learning systems.

</details>


### [285] [PET-TURTLE: Deep Unsupervised Support Vector Machines for Imbalanced Data Clusters](https://arxiv.org/abs/2601.03237)
*Javier Salazar Cavazos*

Main category: cs.LG

TL;DR: 本文提出了PET-TURTLE，一种改进的深度聚类算法，通过引入幂律先验和稀疏logits来解决TURTLE在数据不平衡时性能下降的问题，提升了聚类准确率。


<details>
  <summary>Details</summary>
Motivation: TURTLE作为先进的无监督深度聚类算法，在数据不平衡时因假设簇平衡而产生次优超平面，导致聚类误差升高。

Method: PET-TURTLE通过在代价函数中引入幂律先验以适应不平衡数据分布，并在标签分配过程中采用稀疏logits，简化搜索空间并提升精度。

Result: 在合成与真实数据集上的实验表明，PET-TURTLE提高了不平衡数据下的聚类准确率，抑制了对少数簇的过预测，并整体增强了聚类性能。

Conclusion: PET-TURTLE有效缓解了TURTLE对数据平衡性的依赖，拓展了其在现实不平衡场景中的适用性与鲁棒性。

Abstract: Foundation vision, audio, and language models enable zero-shot performance on downstream tasks via their latent representations. Recently, unsupervised learning of data group structure with deep learning methods has gained popularity. TURTLE, a state of the art deep clustering algorithm, uncovers data labeling without supervision by alternating label and hyperplane updates, maximizing the hyperplane margin, in a similar fashion to support vector machines (SVMs). However, TURTLE assumes clusters are balanced; when data is imbalanced, it yields non-ideal hyperplanes that cause higher clustering error. We propose PET-TURTLE, which generalizes the cost function to handle imbalanced data distributions by a power law prior. Additionally, by introducing sparse logits in the labeling process, PET-TURTLE optimizes a simpler search space that in turn improves accuracy for balanced datasets. Experiments on synthetic and real data show that PET-TURTLE improves accuracy for imbalanced sources, prevents over-prediction of minority clusters, and enhances overall clustering.

</details>
